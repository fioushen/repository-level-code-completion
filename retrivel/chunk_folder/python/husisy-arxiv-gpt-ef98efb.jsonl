{"filename": "main_chat.py", "chunked_list": ["import os\n\timport dotenv\n\timport openai\n\tfrom app.controller import ArxivChatGPT\n\tdotenv.load_dotenv()\n\topenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\tif __name__=='__main__':\n\t    chatgpt = ArxivChatGPT()\n\t    chatgpt.list_arxiv(num_print=5)\n\t    chatgpt.select(2)\n", "    # chatgpt.add_arxiv_paper_to_db('2209.10934')\n\t    # chatgpt.select('2209.10934')\n\t    chatgpt.chat(\"What is the main contribution of this paper?\")\n\t    question = \"What is the main contribution of this paper?\"\n\t    question = 'What is the key references of this paper?'\n\t    question = 'To fully understand the content, can you list 5 necessary references to read?'\n\t# nohup autossh -NT -L 0.0.0.0:443:127.0.0.1:5001 zhangc@localhost > ~/autossh_443_to_5001.log 2>&1 &\n"]}
{"filename": "config.py", "chunked_list": ["import os\n\timport dotenv\n\tdotenv.load_dotenv()\n\tclass Config:\n\t    SECRET_KEY = os.environ['SECRET_KEY']\n\t    SQLALCHEMY_DATABASE_URI = 'sqlite:///'+os.path.abspath(os.environ['SQLITE3_DB_PATH'])\n\t    SQLALCHEMY_TRACK_MODIFICATIONS = False\n"]}
{"filename": "draft00.py", "chunked_list": ["import sqlite3\n\tsql_conn0 = sqlite3.connect('arxiv.sqlite3.bak')\n\tsql_conn0.execute(\"SELECT * FROM sqlite_master where type='table'\").fetchall()\n\tsql_conn0 = sqlite3.connect('/public_data/arxiv-gpt/arxiv.sqlite3')\n\tsql_conn1 = sqlite3.connect('test.sqlite3')\n\tsql_conn0.execute('SELECT * FROM paper').fetchall()\n\ttmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n\ttmp2 = \",\".join(tmp0)\n\tpaper_list = [x[1:] for x in sql_conn0.execute(f'SELECT * FROM paper').fetchall()]\n\tsql_conn1.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', paper_list)\n"]}
{"filename": "main_crawl.py", "chunked_list": ["# https://arxiv.org/list/cs.PF/recent\n\t# https://arxiv.org/list/quant-ph/recent\n\timport os\n\timport datetime\n\timport time\n\timport sqlite3\n\timport dotenv\n\timport openai\n\timport crawl_arxiv\n\tdotenv.load_dotenv()\n", "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\t# sqlite3 PaperParseQueue table\n\tdef insert_only_user():\n\t    from werkzeug.security import generate_password_hash\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    tmp0 = os.environ['ONLY_USER_NAME'], 'xxx@email.com', generate_password_hash(os.environ['ONLY_USER_PASS'])\n\t    sql_conn.execute('INSERT INTO User (username,email,password_hash) VALUES (?,?,?)', tmp0)\n\tif __name__=='__main__':\n\t    # TODO make it a crontab (run every day)\n\t    # crawl_arxiv_recent_paper()\n", "    # _update_existing_arxiv_data()\n\t    # crawl_arxiv.database.init_vector_database()\n\t    recent_url_list = ['https://arxiv.org/list/quant-ph/recent']\n\t    arxivID_time_list = []\n\t    last_query_time = None\n\t    num_paper_limit_one_day = int(os.environ['CRAWL_ONE_DAY_LIMIT'])\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    while True:\n\t        if (last_query_time is None) or (datetime.datetime.now()-last_query_time).days >= 1:\n\t            last_query_time = datetime.datetime.now()\n", "            for url in recent_url_list:\n\t                crawl_arxiv.crawl.crawl_arxiv_recent_paper(url)\n\t        time.sleep(10) #query every 10 seconds\n\t        arxiv_list = [x[0] for x in sql_conn.execute('SELECT arxivID FROM paper_parse_queue').fetchall()]\n\t        sql_conn.execute('DELETE FROM paper_parse_queue')\n\t        for x in arxiv_list:\n\t            sql_conn.execute('DELETE FROM paper_parse_queue WHERE arxivID = ?', (x,))\n\t        sql_conn.commit()\n\t        tmp0 = [sql_conn.execute('SELECT arxivID FROM paper WHERE arxivID = ?', (x,)).fetchone() for x in arxiv_list]\n\t        existed_list = [x[0] for x in tmp0 if x is not None]\n", "        arxiv_list = list(set(arxiv_list) - set(existed_list))\n\t        if len(arxiv_list)>0:\n\t            for arxivID in arxiv_list:\n\t                arxivID_time_list = [x for x in arxivID_time_list if (datetime.datetime.now() - x[1]).days < 1]\n\t                if len(arxivID_time_list)>num_paper_limit_one_day:\n\t                    print(f'Limit reached: {len(arxivID_time_list)}')\n\t                    tmp0 = max(1, 60*60*24 - (datetime.datetime.now()-arxivID_time_list[0][1]).total_seconds())\n\t                    time.sleep(tmp0)\n\t                tmp0 = crawl_arxiv.crawl.crawl_one_arxiv_paper(arxivID, tag_commit_sqlite3=True)\n\t                arxivID_time_list.append((arxivID, datetime.datetime.now()))\n", "    sql_conn.close()\n"]}
{"filename": "crawl_arxiv/database.py", "chunked_list": ["import os\n\timport math\n\timport sqlite3\n\timport weaviate\n\timport numpy as np\n\tfrom .utils import _MY_REQUEST_HEADERS, download_url_and_save\n\tdef sqlite_insert_paper_list(paper_list):\n\t    paper_dict = {x['arxivID']:x for x in paper_list} #remove duplicate arxivID\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    for arxivID, x in paper_dict.items():\n", "        sql_conn.execute('DELETE FROM paper WHERE arxivID = ?', (arxivID,)) #remove old first\n\t        # sql_conn.execute('SELECT * FROM paper').fetchall()\n\t    tmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n\t    tmp1 = [tuple(x[y] for y in tmp0) for x in paper_dict.values()]\n\t    tmp2 = \",\".join(tmp0)\n\t    sql_conn.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', tmp1)\n\t    sql_conn.commit()\n\t    sql_conn.close()\n\tdef sqlite_get_arxivID_by_paper_id(pid):\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n", "    arxivID = sql_conn.execute('SELECT arxivID FROM paper WHERE pid = ?', (pid,)).fetchone()[0]\n\t    sql_conn.close()\n\t    return arxivID\n\tdef sqlite3_load_all_paper_from():\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    paper_list = sql_conn.execute('SELECT * FROM paper').fetchall()\n\t    sql_conn.close()\n\t    tmp0 = 'pid arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n\t    ret = [{y0:y1 for y0,y1 in zip(tmp0,x)} for x in paper_list]\n\t    return ret\n", "# print('pid | arxivID | meta_info_json_path | pdf_path | tex_path | chunk_text_json_path | num_chunk')\n\t# for x in sqlite3_load_all_paper_from():\n\t#     print(x)\n\tdef init_sqlite3_paper_parse_queue(remove_if_exist=False):\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    if remove_if_exist:\n\t        sql_conn.execute('DROP TABLE IF EXISTS paper_parse_queue')\n\t    cmd = '''create table if not exists paper_parse_queue (\n\t        ppqid integer primary key,\n\t        arxivID text\n", "    )\n\t    '''\n\t    sql_conn.execute(cmd)\n\t    sql_conn.commit()\n\t    sql_conn.close()\n\tdef init_sqlite3_database(remove_if_exist=False):\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    if remove_if_exist:\n\t        sql_conn.execute('DROP TABLE IF EXISTS paper')\n\t    cmd = '''create table if not exists paper (\n", "        pid integer primary key,\n\t        arxivID text,\n\t        meta_info_json_path text,\n\t        pdf_path text,\n\t        tex_path text,\n\t        chunk_text_json_path text,\n\t        num_chunk integer\n\t    )\n\t    '''\n\t    sql_conn.execute(cmd)\n", "    sql_conn.commit()\n\t    sql_conn.close()\n\tWeaviate_Paper_schema = {\n\t    \"class\": \"Paper\",\n\t    \"description\": \"A collection of arxiv paper\",\n\t    \"vectorizer\": \"text2vec-openai\",\n\t    \"moduleConfig\": {\n\t        \"text2vec-openai\": {\n\t          \"model\": \"ada\",\n\t          \"modelVersion\": \"002\",\n", "          \"type\": \"text\"\n\t        }\n\t    },\n\t    \"properties\": [{\n\t        \"name\": \"chunk\",\n\t        \"description\": \"chunk contents of the paper\",\n\t        \"dataType\": [\"text\"]\n\t    },\n\t    {\n\t        \"name\": \"arxiv_id\",\n", "        \"description\": \"arxiv ID\",\n\t        \"dataType\": [\"string\"],\n\t        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    },\n\t    {\n\t        \"name\": \"num_chunk\",\n\t        \"description\": \"total number of chunk\",\n\t        \"dataType\": [\"int\"],\n\t        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    },\n", "    {\n\t        \"name\": \"num_token\",\n\t        \"description\": \"number of token\",\n\t        \"dataType\": [\"int\"],\n\t        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    },\n\t    {\n\t        \"name\": \"index\",\n\t        \"description\": \"index of the chunk\",\n\t        \"dataType\": [\"int\"],\n", "        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    }]\n\t}\n\tdef init_vector_database(remove_if_exist=False):\n\t    client = _get_vector_database(with_openai_api_key=False)\n\t    tag_exist = 'Paper' in {x['class'] for x in client.schema.get()['classes']}\n\t    if remove_if_exist and tag_exist:\n\t        client.schema.delete_class('Paper')\n\t        tag_exist = False\n\t    if not tag_exist:\n", "        client.schema.create_class(Weaviate_Paper_schema)\n\t'''\n\tchunk: text\n\tarxiv_id: string\n\tindex: int\n\tnum_token: int\n\tnum_chunk: int\n\tvector (ada-002)\n\t'''\n\tdef _get_vector_database(with_openai_api_key):\n", "    tmp0 = weaviate.auth.AuthApiKey(os.environ['WEAVIATE_API_KEY'])\n\t    if with_openai_api_key:\n\t        tmp1 = {\"X-OpenAI-Api-Key\": os.environ['OPENAI_API_KEY']} #optional\n\t    else:\n\t        tmp1 = None\n\t    client = weaviate.Client(url=os.environ['WEAVIATE_API_URL'], auth_client_secret=tmp0, additional_headers=tmp1)\n\t    return client\n\tdef vector_database_insert_paper(arxivID, text_chunk_list, vector_list=None):\n\t    # text_chunk_list(list,tuple(str,int))\n\t    client = _get_vector_database(with_openai_api_key=True)\n", "    num_chunk = len(text_chunk_list)\n\t    uuid_list = []\n\t    if vector_list is not None:\n\t        assert len(vector_list)==num_chunk\n\t    with client.batch as batch:\n\t        batch.batch_size = 20 #20-100\n\t        # https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate\n\t        # client.batch.configure(batch_size=10,  dynamic=True, timeout_retries=3)\n\t        for ind0 in range(num_chunk):\n\t            tmp0 = dict(arxiv_id=arxivID, num_chunk=num_chunk, index=ind0, chunk=text_chunk_list[ind0][0], num_token=text_chunk_list[ind0][1])\n", "            tmp1 = vector_list[ind0] if vector_list is not None else None\n\t            uuid_list.append(batch.add_data_object(tmp0, class_name='Paper', vector=tmp1))\n\t    return uuid_list\n\tdef vector_database_contains_paper(arxivID:str):\n\t    client = _get_vector_database(with_openai_api_key=False)\n\t    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t    num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n\t    return num_chunk>0\n\tdef vector_database_retrieve_paper(arxivID:str, index=None):\n\t    client = _get_vector_database(with_openai_api_key=False)\n", "    # TODO handle error\n\t    if index is None:\n\t        tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t        num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n\t        assert num_chunk>0\n\t        # TODO batch_size=100\n\t        response = client.query.get(\"Paper\", [\"chunk\", \"index\"]).with_where(tmp0).with_limit(num_chunk).do()\n\t        tmp1 = sorted(response['data']['Get']['Paper'], key=lambda x:x['index'])\n\t        assert tuple(x['index'] for x in tmp1)==tuple(range(num_chunk))\n\t        text_chunk_list = [x['chunk'] for x in tmp1]\n", "        # vector_np = np.zeros((1, 1536), dtype=np.float64)\n\t        ret = text_chunk_list\n\t    else:\n\t        raise NotImplementedError\n\t        # tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t        # text_chunk = ''\n\t        # vector_np = np.zeros(1536, dtype=np.float64)\n\t        # return text_chunk, vector_np\n\t    return ret\n\tdef vector_database_find_close_chunk(arxivID, message, max_context_len):\n", "    client = _get_vector_database(with_openai_api_key=True)\n\t    nearText = {\"concepts\": [message]}\n\t    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t    result = client.query.get(\"Paper\", [\"chunk\", \"num_token\"]).with_near_text(nearText).with_where(tmp0).with_additional(['certainty']).with_limit(10).do()\n\t    certainty = [x['_additional']['certainty'] for x in result['data']['Get']['Paper']] #in descending order\n\t    num_token_list = np.array([x['num_token'] for x in result['data']['Get']['Paper']])\n\t    chunk_text_str_list = [x['chunk'] for x in result['data']['Get']['Paper']]\n\t    np.nonzero((num_token_list + 4).cumsum() <= max_context_len)\n\t    tmp0 = np.nonzero((num_token_list + 4).cumsum() <= max_context_len)[0][-1] + 1\n\t    ret = chunk_text_str_list[:tmp0]\n", "    return ret\n"]}
{"filename": "crawl_arxiv/text.py", "chunked_list": ["import os\n\timport io\n\timport re\n\timport gzip\n\timport tarfile\n\timport sys\n\timport time\n\timport math\n\timport unicodedata\n\timport chardet\n", "import numpy as np\n\timport openai\n\timport tiktoken\n\timport magic\n\timport pylatexenc.latexwalker\n\timport pdfminer.high_level\n\tdef extract_unknown_arxiv_file(file, directory):\n\t    desc = magic.from_file(file)\n\t    if desc.startswith('gzip compressed data'):\n\t        with gzip.open(file, 'rb') as fid:\n", "            file_byte = fid.read()\n\t        desc = magic.from_buffer(file_byte[:2048])\n\t        if desc.startswith('POSIX tar archive'):\n\t            with tarfile.open(fileobj=io.BytesIO(file_byte), mode='r') as fid:\n\t                fid.extractall(directory)\n\t        elif desc.startswith('LaTeX 2e document, ASCII text'):\n\t            with open(os.path.join(directory, 'main.tex'), 'wb') as fid:\n\t                fid.write(file_byte)\n\t        else:\n\t            print(f'unknown file type \"{file}\": {desc}')\n", "    else:\n\t        print(f'unknown file type \"{file}\": {desc}')\n\t# TODO replace with re2\n\t_TEX_INPUT_RE = re.compile(r'\\\\input\\{(.*)\\}')\n\tdef resolve_tex_input(text, directory):\n\t    while True:\n\t        tmp0 = _TEX_INPUT_RE.search(text)\n\t        if tmp0 is None:\n\t            break\n\t        else:\n", "            ind0,ind1 = tmp0.span()\n\t            file_i = os.path.join(directory, tmp0.group(1))\n\t            if (not os.path.exists(file_i)) and (not file_i.endswith('.tex')):\n\t                file_i = os.path.join(directory, tmp0.group(1)+'.tex')\n\t            if os.path.exists(file_i):\n\t                with open(file_i, 'r', encoding='utf-8') as fid:\n\t                    text = text[:ind0] + '\\n' + fid.read() + '\\n' + text[ind1:]\n\t            else:\n\t                # replace \\input{xxx} with input{xxx}\n\t                text = text[:ind0] + text[(ind0+1):]\n", "    return text\n\t_TEX_COMMENT_RE = re.compile(r'(?<!\\\\)%.*')\n\tdef try_except_make_main_tex_file(directory):\n\t    texpath_list = [os.path.join(directory, x) for x in os.listdir(directory) if x.endswith('.tex')]\n\t    text_list = []\n\t    for texpath_i in texpath_list:\n\t        try:\n\t            with open(texpath_i, 'r', encoding='utf-8') as fid:\n\t                text = fid.read()\n\t        except UnicodeDecodeError:\n", "            with open(texpath_i, 'rb') as fid:\n\t                tmp0 = fid.read()\n\t            tmp1 = chardet.detect(tmp0)['encoding']\n\t            print(f'[{texpath_i}] chardet detect encoding: {tmp1}')\n\t            text = tmp0.decode(tmp1)\n\t        if r'\\begin{document}' in text:\n\t            try:\n\t                text_list.append(resolve_tex_input(text, os.path.dirname(texpath_i)))\n\t            except Exception as e:\n\t                print(e)\n", "                print(f'Error in resolving tex input \"{texpath_i}\"')\n\t    # TODO handle other .tex files except main.tex\n\t    if len(text_list)>=1:\n\t        tmp0 = max(text_list, key=len)\n\t        ret = _TEX_COMMENT_RE.sub('', tmp0)\n\t    else:\n\t        ret = None\n\t    return ret\n\tTIKTOKEN_tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\tdef split_text_into_chunks(text, max_token, tokenizer):\n", "    sentence_list = text.split('. ')\n\t    num_token_list = [len(tokenizer.encode(\" \" + x)) for x in sentence_list]\n\t    ret = []\n\t    num_token_current = 0\n\t    chunk = []\n\t    for sentence, num_token in zip(sentence_list, num_token_list):\n\t        if num_token_current + num_token > max_token:\n\t            ret.append(\". \".join(chunk) + \".\")\n\t            chunk = []\n\t            num_token_current = 0\n", "        if num_token > max_token: #TODO possible loss information here\n\t            continue\n\t        chunk.append(sentence)\n\t        num_token_current = num_token_current + num_token + 1\n\t    ret = [(x,len(tokenizer.encode(x))) for x in ret]\n\t    return ret\n\t# fail data/2304.03250/main.tex\n\tdef texpath_to_text_chunk(tex_file):\n\t    # if error, return None\n\t    with open(tex_file, 'r', encoding='utf-8') as fid:\n", "        tex_text = fid.read()\n\t    # split raw text by section\n\t    # TODO finer split: abstract author title paragraph etc.\n\t    tmp0 = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n\t    tmp1 = [x for x in tmp0 if x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='document']\n\t    if len(tmp1)==0:\n\t        # [bug] pylatexenc may fail to parse some tex file\n\t        ind0 = re.search(r'\\\\begin\\{document\\}', tex_text).span()[1]\n\t        ind1 = re.search(r'\\\\end\\{document\\}', tex_text).span()[0]\n\t        tex_text = tex_text[ind0:ind1]\n", "        tex_nodelist = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n\t    else:\n\t        assert len(tmp1)==1\n\t        tex_nodelist = tmp1[0].nodelist\n\t    # remove reference\n\t    tex_nodelist = [x for x in tex_nodelist if not (x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='thebibliography')]\n\t    pos_section = [x.pos for x in tex_nodelist if x.isNodeType(pylatexenc.latexwalker.LatexMacroNode) and x.macroname=='section']\n\t    # TODO split by paragraph\n\t    index_split = [tex_nodelist[0].pos] + pos_section + [tex_nodelist[-1].pos + tex_nodelist[-1].len]\n\t    text_split = [tex_text[x:y] for x,y in zip(index_split[:-1], index_split[1:])]\n", "    ARXIVGPT_MAX_TOKEN_PER_CHUNK = int(os.environ['ARXIVGPT_MAX_TOKEN_PER_CHUNK'])\n\t    text_chunk_list = [y for x in text_split for y in split_text_into_chunks(x, ARXIVGPT_MAX_TOKEN_PER_CHUNK, TIKTOKEN_tokenizer)]\n\t    return text_chunk_list\n\t# TODO make a cache\n\t# https://stackoverflow.com/a/93029/7290857\n\ttmp0 = (chr(i) for i in range(sys.maxunicode)) #all character\n\ttmp1 = ''.join(c for c in tmp0 if (unicodedata.category(c)=='Cc') and c not in '\\t\\n') #all control character\n\t_CONTROL_CHAR_RE = re.compile('[%s]' % re.escape(tmp1))\n\t_REMOVE_BEGIN_ARXIV_RE = re.compile(\"$(.?\\n)+\", flags=re.MULTILINE)\n\t_REMOVE_CONNECTING_RE = re.compile('-\\n', flags=re.MULTILINE)\n", "_REMOVE_LATEXIT_RE = re.compile('latexit(.*)/latexit', flags=re.MULTILINE)\n\t_REMOVE_NEWLINE_RE = re.compile(r'(\\S)\\n(\\S)', flags=re.MULTILINE)\n\t_MISC00_RE = re.compile('ﬀ')\n\tdef pdfpath_to_text_chunk(pdf_path):\n\t    # TODO see github/chatpaper how to cleanup pdf\n\t    assert pdf_path.endswith('.pdf')\n\t    text_path = pdf_path[:-4] + '.txt'\n\t    with open(pdf_path, 'rb') as fid:\n\t        text_ori = pdfminer.high_level.extract_text(fid)\n\t    text0 = _CONTROL_CHAR_RE.sub('', text_ori)\n", "    text1 = _REMOVE_BEGIN_ARXIV_RE.sub('', text0, count=1)\n\t    text2 = _REMOVE_LATEXIT_RE.sub('', text1)\n\t    text3 = _REMOVE_CONNECTING_RE.sub('', text2)\n\t    text4 = _REMOVE_NEWLINE_RE.sub(r'\\1 \\2', text3)\n\t    text4 = _REMOVE_NEWLINE_RE.sub(r'\\1 \\2', text4) #sometimes we need do this several time\n\t    text5 = _MISC00_RE.sub('ff', text4)\n\t    text = text5\n\t    ARXIVGPT_MAX_TOKEN_PER_CHUNK = int(os.environ['ARXIVGPT_MAX_TOKEN_PER_CHUNK'])\n\t    text_chunk_list = split_text_into_chunks(text, ARXIVGPT_MAX_TOKEN_PER_CHUNK, TIKTOKEN_tokenizer)\n\t    return text_chunk_list\n", "def text_chunk_list_to_numpy_vector(text_chunk_list, batch_size=20):\n\t    assert all(isinstance(x,str) for x in text_chunk_list)\n\t    num_chunk = len(text_chunk_list)\n\t    tmp0 = [x*batch_size for x in range(math.ceil(num_chunk/batch_size) + 1)]\n\t    text_chunk_list_batch = [text_chunk_list[x:y] for x,y in zip(tmp0,tmp0[1:])]\n\t    embedding_list = []\n\t    for batch_i in text_chunk_list_batch:\n\t        # rate limiting\n\t        # https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api\n\t        time.sleep(2.5) #make sure not exceed 20 requests per minutes\n", "        response = openai.Embedding.create(input=batch_i, engine='text-embedding-ada-002')\n\t        tmp0 = sorted(response['data'], key=lambda x:x.index)\n\t        embedding_list += [x.embedding for x in tmp0]\n\t        print(f'embedding progress: {len(embedding_list)}/{num_chunk}')\n\t    embedding_np = np.array(embedding_list, dtype=np.float64)\n\t    return embedding_np\n"]}
{"filename": "crawl_arxiv/__init__.py", "chunked_list": ["# nohup python &\n\tfrom . import database\n\tfrom . import text\n\tfrom . import crawl\n"]}
{"filename": "crawl_arxiv/utils.py", "chunked_list": ["import os\n\timport requests\n\tfrom tqdm import tqdm\n\t_MY_REQUEST_HEADERS = {'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'}\n\tdef download_url_and_save(url, filename=None, directory='.', headers=None, proxies=None):\n\t    assert os.path.exists(directory)\n\t    response = requests.get(url, headers=headers, proxies=proxies, stream=True)\n\t    response.raise_for_status()\n\t    if filename is None:\n\t        filepath = os.path.join(directory, url.rsplit('/',1)[1])\n", "    else:\n\t        filepath = os.path.join(directory, filename)\n\t    if not os.path.exists(filepath):\n\t        tmp_filepath = filepath + '.incomplete'\n\t        tmp0 = {'total':int(response.headers['content-length']), 'unit':'iB', 'unit_scale':True}\n\t        with open(tmp_filepath, 'wb') as fid, tqdm(**tmp0) as progress_bar:\n\t            for x in response.iter_content(chunk_size=1024): #1kiB\n\t                progress_bar.update(len(x))\n\t                fid.write(x)\n\t        os.rename(tmp_filepath, filepath)\n", "    return filepath\n"]}
{"filename": "crawl_arxiv/crawl.py", "chunked_list": ["import os\n\timport re\n\timport json\n\timport shutil\n\timport requests\n\timport time\n\timport numpy as np\n\timport lxml.etree\n\tfrom .utils import download_url_and_save, _MY_REQUEST_HEADERS\n\tfrom .text import (extract_unknown_arxiv_file, try_except_make_main_tex_file,\n", "            texpath_to_text_chunk, pdfpath_to_text_chunk, text_chunk_list_to_numpy_vector)\n\tfrom .database import sqlite_insert_paper_list, vector_database_insert_paper, vector_database_contains_paper\n\tdef crawl_arxiv_meta_info(arxivID):\n\t    url = 'https://arxiv.org/abs/' + arxivID\n\t    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n\t    response.raise_for_status()\n\t    time.sleep(1) #sleep 1 seconds to avoid being banned\n\t    html = lxml.etree.HTML(response.content)\n\t    tmp0 = html.xpath('//div[@id=\"abs\"]')\n\t    assert len(tmp0)==1\n", "    title = tmp0[0].xpath('./h1[@class=\"title mathjax\"]/text()')[0]\n\t    author_list = [str(x) for x in tmp0[0].xpath('./div[@class=\"authors\"]/a/text()')]\n\t    tmp1 = ''.join(tmp0[0].xpath('./blockquote[@class=\"abstract mathjax\"]/text()')).strip()\n\t    abstract = re.sub('\\n', ' ', tmp1)\n\t    tmp0 = ''.join(html.xpath('//td[@class=\"tablecell subjects\"]/span/text()'))\n\t    tmp1 = ''.join(html.xpath('//td[@class=\"tablecell subjects\"]/text()'))\n\t    subject = tmp0.strip() + tmp1.strip()\n\t    pdf_url = 'https://arxiv.org' + str(html.xpath('//a[@class=\"abs-button download-pdf\"]/@href')[0])\n\t    tmp0 = html.xpath('//a[@class=\"abs-button download-format\"]/@href')\n\t    if len(tmp0)>0:\n", "        assert str(tmp0[0]).startswith('/format')\n\t        targz_url = 'https://arxiv.org/e-print' + str(tmp0[0][7:])\n\t    else:\n\t        targz_url = ''\n\t    ret = {'title': title, 'author_list': author_list,'abstract': abstract, 'subject': subject, 'pdf_url': pdf_url, 'targz_url': targz_url}\n\t    return ret\n\tdef crawl_one_arxiv_paper(arxivID, tag_commit_sqlite3=False):\n\t    arxivID = str(arxivID).strip('/')\n\t    url_abs = 'https://arxiv.org/abs/' + arxivID\n\t    try:\n", "        response = requests.get(url_abs, headers=_MY_REQUEST_HEADERS)\n\t        response.raise_for_status()\n\t        time.sleep(1) #sleep 1 seconds to avoid being banned\n\t    except Exception as e:\n\t        print(e)\n\t        print(f'[Error][crawl_utils.py/crawl_one_arxiv_paper] fail to crawl url {url_abs}')\n\t        response = None\n\t    if response is not None:\n\t        html = lxml.etree.HTML(response.content)\n\t        hf_file = lambda *x: os.path.join(os.environ['ARXIV_DIRECTORY'], arxivID, *x)\n", "        if not os.path.exists(hf_file()):\n\t            os.makedirs(hf_file())\n\t        pdf_path = hf_file('main.pdf')\n\t        meta_info_json_path = hf_file('meta-info.json')\n\t        targz_path = hf_file('main.tar.gz')\n\t        tex_path = hf_file('main.tex')\n\t        chunk_text_json_path = hf_file('chunk-text.json')\n\t        vector_npy_path = hf_file('chunk-vector.npy')\n\t        if not os.path.exists(meta_info_json_path):\n\t            print(f'[{arxivID}] crawling meta information')\n", "            meta_info = crawl_arxiv_meta_info(arxivID)\n\t            with open(meta_info_json_path, 'w', encoding='utf-8') as fid:\n\t                json.dump(meta_info, fid, ensure_ascii=False)\n\t        else:\n\t            with open(meta_info_json_path, 'r', encoding='utf-8') as fid:\n\t                meta_info = json.load(fid)\n\t        pdf_url = meta_info['pdf_url']\n\t        if not os.path.exists(pdf_path):\n\t            print(f'[{arxivID}] downloading {pdf_url}')\n\t            download_url_and_save(pdf_url, filename=os.path.basename(pdf_path), directory=hf_file(), headers=_MY_REQUEST_HEADERS)\n", "            time.sleep(3) #sleep 3 seconds to avoid being banned\n\t        targz_url = meta_info['targz_url']\n\t        if targz_url!='':\n\t            if not os.path.exists(targz_path):\n\t                print(f'[{arxivID}] downloading targz file \"{targz_url}\"')\n\t                download_url_and_save(targz_url, filename=os.path.basename(targz_path), directory=hf_file(), headers=_MY_REQUEST_HEADERS)\n\t                time.sleep(3)\n\t            if not os.path.exists(tex_path):\n\t                if not os.path.exists(hf_file('untar')):\n\t                    print(f'[{arxivID}] extract targz file to untar folder')\n", "                    os.makedirs(hf_file('untar'))\n\t                    extract_unknown_arxiv_file(targz_path, hf_file('untar'))\n\t                print(f'[{arxivID}] make main.tex')\n\t                tex_text = try_except_make_main_tex_file(hf_file('untar'))\n\t                if tex_text is not None:\n\t                    with open(tex_path, 'w', encoding='utf-8') as fid:\n\t                        fid.write(tex_text)\n\t                shutil.rmtree(hf_file('untar'))\n\t        if not os.path.exists(chunk_text_json_path):\n\t            text_list = []\n", "            if os.path.exists(tex_path):\n\t                print(f'[{arxivID}] convert tex to chunk_text')\n\t                try:\n\t                    text_list = texpath_to_text_chunk(tex_path)\n\t                except Exception as e:\n\t                    print(e)\n\t                    print(f'[Error][crawl_utils.py/crawl_one_arxiv_paper] fail to convert tex to chunk_text')\n\t            if len(text_list)==0:\n\t                print(f'[{arxivID}] convert pdf to chunk_text')\n\t                text_list = pdfpath_to_text_chunk(pdf_path)\n", "            with open(chunk_text_json_path, 'w', encoding='utf-8') as fid:\n\t                json.dump(text_list, fid, ensure_ascii=False)\n\t        else:\n\t            with open(chunk_text_json_path, 'r', encoding='utf-8') as fid:\n\t                text_list = json.load(fid)\n\t        num_chunk = len(text_list)\n\t        if os.path.exists(chunk_text_json_path):\n\t            if (os.environ['ARXIVGPT_SAVE_NUMPY_VECTOR']=='1') and (not os.path.exists(vector_npy_path)):\n\t                print(f'[{arxivID}] converting chunk_text to numpy vector')\n\t                embedding_np = text_chunk_list_to_numpy_vector([x[0] for x in text_list])\n", "                np.save(vector_npy_path, embedding_np)\n\t            # if vector_database_contains_paper(arxivID):\n\t            #     print(f'[{arxivID}] vector_database already contains this paper')\n\t            # else:\n\t            #     if os.path.exists(vector_npy_path):\n\t            #         embedding_np = np.load(vector_npy_path)\n\t            #         assert embedding_np.shape[0] == num_chunk\n\t            #     else:\n\t            #         embedding_np = None\n\t            #     print(f'[{arxivID}] inserting paper into vector database')\n", "            #     uuid_list = vector_database_insert_paper(arxivID, text_list, embedding_np)\n\t            # TODO should we save uuid_list to json file?\n\t        ret = dict(arxivID=arxivID, num_chunk=num_chunk, meta_info_json_path=meta_info_json_path, pdf_path=pdf_path, tex_path=tex_path, chunk_text_json_path=chunk_text_json_path)\n\t        for key in list(ret.keys()):\n\t            if key.endswith('_path'):\n\t                value = ret[key]\n\t                if not os.path.exists(value):\n\t                    value = ''\n\t                assert value.startswith(os.environ['ARXIV_DIRECTORY']) or (value=='')\n\t                ret[key] = value[len(os.environ['ARXIV_DIRECTORY']):].lstrip(os.sep)\n", "        sqlite_insert_paper_list([ret])\n\t    else:\n\t        ret = None\n\t    return ret\n\tdef crawl_arxiv_recent_paper(url):\n\t    print(f'crawling {url}')\n\t    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n\t    response.raise_for_status()\n\t    time.sleep(1) #sleep 3 seconds to avoid being banned\n\t    html = lxml.etree.HTML(response.content)\n", "    arxivID_list = [str(x.rsplit('/',1)[1]) for x in html.xpath('//dt/span[@class=\"list-identifier\"]/a[@title=\"Abstract\"]/@href')]\n\t    for x in arxivID_list:\n\t        crawl_one_arxiv_paper(x, tag_commit_sqlite3=True)\n\tdef remove_all_intermidiate_data_in_arxiv(directory):\n\t    for x in os.listdir(directory):\n\t        folder = os.path.join(directory, x)\n\t        if os.path.isdir(folder):\n\t            tmp0 = os.path.join(folder, 'untar')\n\t            if os.path.exists(tmp0):\n\t                shutil.rmtree(tmp0)\n", "            tmp0 = os.path.join(folder, 'main.tex')\n\t            if os.path.exists(tmp0):\n\t                os.remove(tmp0)\n\t            tmp0 = os.path.join(folder, 'meta-info.json')\n\t            if os.path.exists(tmp0):\n\t                os.remove(tmp0)\n\t            tmp0 = os.path.join(folder, 'chunk-text.json')\n\t            if os.path.exists(tmp0):\n\t                os.remove(tmp0)\n\t            # tmp0 = os.path.join(folder, 'chunk-vector.npy')\n", "            # if os.path.exists(tmp0):\n\t            #     os.remove(tmp0)\n\tdef _update_existing_arxiv_data():\n\t    directory = os.environ['ARXIV_DIRECTORY']\n\t    arxivID_list = [x for x in os.listdir(directory) if os.path.isdir(os.path.join(directory,x))]\n\t    for x in arxivID_list:\n\t        crawl_one_arxiv_paper(x, tag_commit_sqlite3=True)\n"]}
{"filename": "history_file/draft00.py", "chunked_list": ["import os\n\timport time\n\timport pickle\n\timport dotenv\n\timport requests\n\tfrom tqdm import tqdm\n\timport lxml.etree\n\timport tiktoken\n\timport openai\n\timport openai.embeddings_utils\n", "import numpy as np\n\tfrom utils import download_url_and_save, convert_pdf_to_text, _MY_REQUEST_HEADERS, NaiveChatGPT\n\tdotenv.load_dotenv()\n\topenai.api_key = os.environ[\"OPENAI_API_KEY\"]\n\tif not os.path.exists('data'):\n\t    os.makedirs('data')\n\tchatgpt = NaiveChatGPT()\n\tdef get_arxiv_recent_url_pdf():\n\t    url = \"https://arxiv.org/list/quant-ph/recent\"\n\t    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n", "    response.raise_for_status()\n\t    z0 = lxml.etree.HTML(response.content)\n\t    ret = ['https://arxiv.org'+str(x) for x in z0.xpath('//a[@title=\"Download PDF\"]/@href')]\n\t    # TODO title author abstract\n\t    return ret\n\tdef download_arxiv_pdf(url_pdf_list, directory='data'):\n\t    # TODO download and read from tex file\n\t    for url_i in url_pdf_list:\n\t        print(url_i)\n\t        filename = url_i.rsplit('/',1)[1] + '.pdf'\n", "        filepath = os.path.join(directory, filename)\n\t        if not os.path.exists(filepath):\n\t            download_url_and_save(url_i, filename=filename, directory=directory, headers=_MY_REQUEST_HEADERS)\n\t            time.sleep(3) #sleep 3 seconds to avoid being banned\n\t        else:\n\t            print(f'{filename} already downloaded, skip it')\n\tdef split_text_into_chunks(text, max_token, tokenizer):\n\t    sentence_list = text.split('. ')\n\t    num_token_list = [len(tokenizer.encode(\" \" + x)) for x in sentence_list]\n\t    ret = []\n", "    num_token_current = 0\n\t    chunk = []\n\t    for sentence, num_token in zip(sentence_list, num_token_list):\n\t        if num_token_current + num_token > max_token:\n\t            ret.append(\". \".join(chunk) + \".\")\n\t            chunk = []\n\t            num_token_current = 0\n\t        if num_token > max_token: #TODO possible loss information here\n\t            continue\n\t        chunk.append(sentence)\n", "        num_token_current = num_token_current + num_token + 1\n\t    ret = [(x,len(tokenizer.encode(x))) for x in ret]\n\t    return ret\n\tdef get_arxiv_text_embedding(txt_file, max_tokens=None, tokenizer=None):\n\t    # (ret0) chunked_text_list (list,tuple(text:str, num_token:int))\n\t    # (ret1) embedding_np (np.float64, (N,1536))\n\t    assert txt_file.endswith('.txt')\n\t    pkl_file = txt_file[:-4] + '.pkl'\n\t    if os.path.exists(pkl_file):\n\t        with open(pkl_file, 'rb') as fid:\n", "            tmp0 = pickle.load(fid)\n\t            chunked_list = tmp0['chunked_list']\n\t            embedding_np = tmp0['embedding_np']\n\t    else:\n\t        assert max_tokens is not None\n\t        with open(txt_file, 'r', encoding='utf-8') as fid:\n\t            text = fid.read()\n\t        chunked_list = split_text_into_chunks(text, max_tokens, tokenizer)\n\t        embbeding_list = []\n\t        # TODO rate limiting\n", "        # https://platform.openai.com/docs/guides/rate-limits/what-are-the-rate-limits-for-our-api\n\t        for x,_ in tqdm(chunked_list):\n\t            time.sleep(2.5) #make sure not exceed 20 requests per minutes\n\t            tmp0 = np.array(openai.Embedding.create(input=x, engine='text-embedding-ada-002')['data'][0]['embedding'], dtype=np.float64)\n\t            embbeding_list.append(tmp0)\n\t        embedding_np = np.stack(embbeding_list, axis=0)\n\t        with open(pkl_file, 'wb') as fid:\n\t            tmp0 = dict(chunked_list=chunked_list, embedding_np=embedding_np)\n\t            pickle.dump(tmp0, fid)\n\t    return chunked_list, embedding_np\n", "_openai_qa_template = (\"Answer the question based on the context below, and if the question can't be answered based on the context, \"\n\t            \"say \\\"I don't know\\\"\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nAnswer:\")\n\tdef answer_question(question, text_list, embedding_np, max_context_len=1800, tag_print_context=False):\n\t    \"\"\"\n\t    Answer a question based on the most similar context from the dataframe texts\n\t    text_list(list, tuple(text:str, num_token:int))\n\t    TODO response_max_tokens=150\n\t    \"\"\"\n\t    assert all(len(x)==2 for x in text_list)\n\t    assert len(text_list) == embedding_np.shape[0]\n", "    text_len_list = np.array([x[1] for x in text_list])\n\t    text_str_list = [x[0] for x in text_list]\n\t    q_embedding = openai.Embedding.create(input=question, engine='text-embedding-ada-002')['data'][0]['embedding']\n\t    distance = np.array(openai.embeddings_utils.distances_from_embeddings(q_embedding, embedding_np, distance_metric='cosine')) # 0: cloest\n\t    ind0 = np.argsort(distance)\n\t    tmp0 = np.nonzero((text_len_list[ind0] + 4).cumsum() > max_context_len)[0].min()\n\t    context_text_list = [text_str_list[x] for x in ind0[:tmp0]]\n\t    context_text = \"\\n\\n###\\n\\n\".join(context_text_list)\n\t    if tag_print_context:\n\t        print(f\"Context:\\n{context_text}\\n\\n\")\n", "    prompt = _openai_qa_template.format(context=context_text, question=question)\n\t    try:\n\t        chatgpt.reset()\n\t        ret = chatgpt.chat(prompt, tag_print=False, tag_return=True)\n\t    except Exception as e:\n\t        print(e)\n\t        ret = \"\"\n\t    return ret\n\turl_i = 'https://arxiv.org/pdf/2209.10934'\n\turl_pdf_list = get_arxiv_recent_url_pdf()\n", "download_arxiv_pdf(url_pdf_list)\n\tpdf_file_list = [os.path.join('data',x) for x in os.listdir('data') if x.endswith('.pdf')]\n\tconvert_pdf_to_text(pdf_file_list)\n\ttokenizer = tiktoken.get_encoding(\"cl100k_base\")\n\tmax_token_per_chunk = 300\n\ttxt_file_list = [os.path.join('data',x) for x in os.listdir('data') if x.endswith('.txt')]\n\ttxt_file = 'data/2304.01190.txt'\n\tchunked_list, embedding_np = get_arxiv_text_embedding(txt_file, max_token_per_chunk, tokenizer)\n\tquestion = \"What is the main contribution of this paper?\"\n\tquestion = 'What is the key references of this paper?'\n", "question = 'To fully understand the content, can you list 5 necessary references to read?'\n\tanswer = answer_question(question, chunked_list, embedding_np, tag_print_context=True)\n\tprint(answer)\n"]}
{"filename": "history_file/draft02.py", "chunked_list": ["import os\n\timport io\n\timport magic\n\timport time\n\timport re\n\timport gzip\n\timport tarfile\n\timport lxml.etree\n\timport requests\n\tfrom tqdm import tqdm\n", "import pylatexenc.latexwalker\n\tfrom utils import _MY_REQUEST_HEADERS, download_url_and_save\n\t# TODO save to database\n\tdef get_arxiv_recent_targz_url():\n\t    # TODO title author abstract\n\t    url = \"https://arxiv.org/list/quant-ph/recent\"\n\t    response = requests.get(url, headers=_MY_REQUEST_HEADERS)\n\t    response.raise_for_status()\n\t    html = lxml.etree.HTML(response.content)\n\t    # TODO print if targz file is missing\n", "    tmp0 = [str(x) for x in html.xpath('//a[@title=\"Other formats\"]/@href')]\n\t    tmp1 = [str(x) for x in html.xpath('//a[@title=\"Abstract\"]/@href')]\n\t    assert all(x.startswith('/format/') for x in tmp0)\n\t    assert all(x.startswith('/abs/') for x in tmp1)\n\t    tmp2 = set(x[5:] for x in tmp1) - set(x[8:] for x in tmp0)\n\t    if len(tmp2)>0:\n\t        print('these arxiv keys are missing targz file:', tmp2)\n\t    ret = ['https://arxiv.org/e-print/'+x[8:] for x in tmp0]\n\t    return ret\n\tdef extract_unknown_arxiv_file(file, directory):\n", "    desc = magic.from_file(file)\n\t    if desc.startswith('gzip compressed data'):\n\t        with gzip.open(file, 'rb') as fid:\n\t            file_byte = fid.read()\n\t        desc = magic.from_buffer(file_byte[:2048])\n\t        if desc.startswith('POSIX tar archive'):\n\t            with tarfile.open(fileobj=io.BytesIO(file_byte), mode='r') as fid:\n\t                fid.extractall(directory)\n\t        elif desc.startswith('LaTeX 2e document, ASCII text'):\n\t            with open(os.path.join(directory, 'main.tex'), 'wb') as fid:\n", "                fid.write(file_byte)\n\t        else:\n\t            print(f'unknown file type \"{file}\": {desc}')\n\t    else:\n\t        print(f'unknown file type \"{file}\": {desc}')\n\tdef download_tex_targz_file(url_list, directory='data'):\n\t    tex_file_list = []\n\t    for url_i in url_list:\n\t        arxiv_key = url_i.rsplit('/',1)[1]\n\t        hf_path = lambda *x: os.path.join(directory, arxiv_key, *x)\n", "        if not os.path.exists(hf_path()):\n\t            os.makedirs(hf_path())\n\t        if not os.path.exists(hf_path('untar')):\n\t            os.makedirs(hf_path('untar'))\n\t        targz_filename = arxiv_key + '.tar.gz'\n\t        if os.path.exists(hf_path(targz_filename)):\n\t            print(f'{targz_filename} already exists, skip downloading')\n\t        else:\n\t            print(f'downloading {url_i}')\n\t            download_url_and_save(url_i, filename=targz_filename, directory=hf_path(), headers=_MY_REQUEST_HEADERS)\n", "            time.sleep(2.5) #avoid being banned\n\t        tag_has_tex = len([x for x in os.listdir(hf_path('untar')) if x.endswith('.tex')])>1\n\t        if not tag_has_tex:\n\t            extract_unknown_arxiv_file(hf_path(targz_filename), hf_path('untar'))\n\t        tmp0 = [hf_path('untar',x) for x in os.listdir(hf_path('untar')) if x.endswith('.tex')]\n\t        if len(tmp0)==0:\n\t            print(f'no tex file for {arxiv_key}')\n\t        else:\n\t            if len(tmp0)>1:\n\t                print(f'more than one tex file in {arxiv_key}, choose the largest one in size')\n", "                tmp0 = sorted(tmp0, key=os.path.getsize, reverse=True)[0]\n\t            else:\n\t                tmp0 = tmp0[0]\n\t            tex_file_list.append(tmp0)\n\t    return tex_file_list\n\tdef get_tex_split_text(tex_file):\n\t    # if error, return None\n\t    with open(tex_file, 'r', encoding='utf-8') as fid:\n\t        tex_text = fid.read()\n\t    # split raw text by section\n", "    # TODO finer split: abstract author title paragraph etc.\n\t    tmp0 = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n\t    tmp1 = [x for x in tmp0 if x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='document']\n\t    if len(tmp1)==0:\n\t        tmp0 = re.search(r'\\\\begin\\{document\\}', tex_text)\n\t        tmp1 = re.search(r'\\\\end\\{document\\}', tex_text)\n\t        if (tmp0 is None) or (tmp1 is None):\n\t            return None\n\t        ind0 = tmp0.span()[1]\n\t        ind1 = tmp1.span()[0]\n", "        tex_text = tex_text[ind0:ind1]\n\t        tex_nodelist = pylatexenc.latexwalker.LatexWalker(tex_text.strip()).get_latex_nodes(pos=0)[0]\n\t    else:\n\t        assert len(tmp1)==1\n\t        tex_nodelist = tmp1[0].nodelist\n\t    # remove reference\n\t    tex_nodelist = [x for x in tex_nodelist if not (x.isNodeType(pylatexenc.latexwalker.LatexEnvironmentNode) and x.environmentname=='thebibliography')]\n\t    pos_section = [x.pos for x in tex_nodelist if x.isNodeType(pylatexenc.latexwalker.LatexMacroNode) and x.macroname=='section']\n\t    # TODO split by paragraph\n\t    index_split = [tex_nodelist[0].pos] + pos_section + [tex_nodelist[-1].pos + tex_nodelist[-1].len]\n", "    text_split = [tex_text[x:y] for x,y in zip(index_split[:-1], index_split[1:])]\n\t    return text_split\n\t'''\n\tdata\n\t├── 2101.00001\n\t│   ├── 2101.00001.pdf\n\t│   ├── 2101.00001.txt\n\t│   ├── 2101.00001.tar.gz\n\t│   ├── untar\n\t│   │   ├── 2101.00001v1\n", "'''\n\turl_list = get_arxiv_recent_targz_url()\n\ttex_file_list = download_tex_targz_file(url_list)\n\tzc0 = [get_tex_split_text(x) for x in tqdm(tex_file_list)]\n\tzc0 = get_tex_split_text(tex_file_list[13])\n\t# TODO 2303.17399 input\n\ttex_file = tex_file_list[13]\n\t# TODO download 2209.10934 (PureB) tex\n\t# url_i = 'https://arxiv.org/e-print//2303.17565'\n\t# arxiv_key = url_i.rsplit('/',1)[1]\n", "# hf_path = lambda *x: os.path.join('data', arxiv_key, *x)\n\t# if not os.path.exists(hf_path()):\n\t#     os.makedirs(hf_path())\n\t# targz_filename = arxiv_key + '.tar.gz' #if exists, then skip all below\n\t# download_url_and_save(url_i, filename=targz_filename, directory=hf_path(), headers=_MY_REQUEST_HEADERS)\n\t# tmp0 = hf_path('untar')\n\t# if os.path.isdir(tmp0):\n\t#     os.rmdir(tmp0)\n\t# with tarfile.open(hf_path(targz_filename), 'r') as fid:\n\t#     fid.extractall(tmp0)\n", "# tmp0 = [x for x in os.listdir(hf_path('untar')) if x.endswith('.tex')]\n\t# assert len(tmp0) == 1\n\t# tex_file = hf_path('untar', tmp0[0])\n\t# with open(tex_file, 'r', encoding='utf-8') as fid:\n\t#     tex_text = fid.read()\n"]}
{"filename": "history_file/draft01.py", "chunked_list": ["# python chat_arxiv.py --query \"chatgpt robot\" --page_num 2 --max_results 3 --days 2\n\t# paper_list = reader1.get_arxiv_web(args=args, page_num=args.page_num, days=args.days)\n\t# titles, links, dates = self.get_all_titles_from_web(args.query, page_num=page_num, days=days)\n\tdef get_all_titles_from_web(self, keyword, page_num=1, days=1):\n\t    title_list, link_list, date_list = [], [], []\n\t    for page in range(page_num):\n\t        url = self.get_url(keyword, page)  # 根据关键词和页码生成链接\n\t        titles, links, dates = self.get_titles(url, days)  # 根据链接获取论文标题\n\t        if not titles:  # 如果没有获取到任何标题，说明已经到达最后一页，退出循环\n\t            break\n", "        for title_index, title in enumerate(titles):  # 遍历每个标题，并打印出来\n\t            print(page, title_index, title, links[title_index], dates[title_index])\n\t        title_list.extend(titles)\n\t        link_list.extend(links)\n\t        date_list.extend(dates)\n\t    print(\"-\" * 40)\n\t    return title_list, link_list, date_list\n\tdef get_url(self, keyword, page):\n\t    base_url = \"https://arxiv.org/search/?\"\n\t    params = {\n", "        \"query\": keyword,\n\t        \"searchtype\": \"all\",  # 搜索所有字段\n\t        \"abstracts\": \"show\",  # 显示摘要\n\t        \"order\": \"-announced_date_first\",  # 按日期降序排序\n\t        \"size\": 50  # 每页显示50条结果\n\t    }\n\t    if page > 0:\n\t        params[\"start\"] = page * 50  # 设置起始位置\n\t    return base_url + requests.compat.urlencode(params)\n"]}
{"filename": "app/Form.py", "chunked_list": ["from flask_wtf import FlaskForm\n\tfrom wtforms import StringField, PasswordField, BooleanField, SubmitField, SelectField, TextAreaField, IntegerField\n\tfrom wtforms.validators import ValidationError, DataRequired, Email, EqualTo,InputRequired\n\tfrom . import controller\n\tfrom .models import User\n\t# login form, used in login page\n\tclass LoginForm(FlaskForm):\n\t    username = StringField(\"Username\", validators = [DataRequired()], default='NOT REQUIRED')\n\t    password = PasswordField(\"Password\", validators = [DataRequired()])\n\t    remember_me = BooleanField(\"Remember Me\")\n", "    submit = SubmitField(\"Sign In\")\n\t# registration form, used to get registration information\n\t# class RegistrationForm(FlaskForm):\n\t#     username = StringField('Username', validators=[DataRequired()])\n\t#     email = StringField('Email', validators=[DataRequired(), Email()])\n\t#     password = PasswordField('Password', validators=[DataRequired()])\n\t#     password2 = PasswordField('Repeat Password', validators=[DataRequired(), EqualTo('password')])\n\t#     submit = SubmitField('Register')\n\t#     # make sure user name is not empty or duplicate\n\t#     def validate_username(self, username):\n", "#         user = User.query.filter_by(username=username.data).first()\n\t#         if user is not None:\n\t#             raise ValidationError('Please use a different username.')\n\t#     # make sure email is not empty or duplicate\n\t#     def validate_email(self, email):\n\t#         user = User.query.filter_by(email=email.data).first()\n\t#         if user is not None:\n\t#             raise ValidationError('Please use a different email address.')\n\t# form for chat room\n\tclass messageForm(FlaskForm):\n", "    message = TextAreaField('Message', validators=[DataRequired()])\n\t    submit = SubmitField('Send')\n"]}
{"filename": "app/_init.py", "chunked_list": ["from flask import Flask\n\tfrom flask_sqlalchemy import SQLAlchemy\n\tfrom flask_migrate import Migrate\n\tfrom flask_login import LoginManager\n\tfrom config import Config #root directory\n\tapp = Flask(__name__.split('.',1)[0])\n\tapp.config.from_object(Config)\n\tdb = SQLAlchemy(app)\n\tmigrate = Migrate(app, db)\n\tlogin = LoginManager(app)\n", "login.login_view = 'login'\n"]}
{"filename": "app/models.py", "chunked_list": ["import os\n\tfrom werkzeug.security import generate_password_hash, check_password_hash\n\tfrom flask_login import UserMixin\n\tfrom hashlib import md5\n\timport sqlite3\n\timport json\n\tfrom ._init import login, db\n\t#the tables of sql and related caculations are wirtten here\n\tclass User(UserMixin,db.Model):\n\t    uid = db.Column(db.Integer, primary_key=True)\n", "    username = db.Column(db.String(64), index=True, unique=True)\n\t    email = db.Column(db.String(120), index=True, unique=True)\n\t    password_hash = db.Column(db.String(128))\n\t    Is_adm = db.Column(db.Integer)\n\t    message = db.relationship('message', backref='User', lazy='dynamic')\n\t    def set_password(self, password):\n\t        self.password_hash = generate_password_hash(password)\n\t    def check_password(self, password):\n\t        return check_password_hash(self.password_hash, password)\n\t    def __repr__(self):\n", "        return '<User {}>'.format(self.username)\n\t    def get_id(self):\n\t        return self.uid\n\t    def get_user_name(self):\n\t        return self.username\n\t    def if_adm(self):\n\t        return self.Is_adm\n\t@login.user_loader\n\tdef load_user(id):\n\t    return User.query.get(int(id))\n", "# paper table\n\tclass paper(db.Model):\n\t    pid = db.Column(db.Integer, primary_key=True)\n\t    arxivID = db.Column(db.String(50), unique=True)\n\t    meta_info_json_path = db.Column(db.String(100), unique=True)\n\t    pdf_path = db.Column(db.String(100), unique=True, default='')\n\t    tex_path = db.Column(db.String(100), unique=True, default='')\n\t    chunk_text_json_path = db.Column(db.String(100), default='')\n\t    num_chunk = db.Column(db.Integer, default=0)\n\t    message = db.relationship('message', backref='paper', lazy='dynamic')\n", "    def __repr__(self):\n\t        return '<paper {}>'.format(self.arxivID)\n\t    def get_id(self):\n\t        return self.pid\n\t    def get_arxivID(self):\n\t        return self.arxivID\n\t    def get_title(self):\n\t        with open(os.path.join(os.environ['ARXIV_DIRECTORY'], self.meta_info_json_path), 'r') as fid:\n\t            meta_info = json.load(fid)\n\t        return meta_info['title']\n", "    def get_pdf_url(self):\n\t        with open(os.path.join(os.environ['ARXIV_DIRECTORY'], self.meta_info_json_path), 'r') as fid:\n\t            meta_info = json.load(fid)\n\t        return meta_info['pdf_url']\n\t# message table\n\tclass message(db.Model):\n\t    mid = db.Column(db.Integer, primary_key=True)\n\t    pid = db.Column(db.Integer, db.ForeignKey('paper.pid'))\n\t    uid = db.Column(db.Integer, db.ForeignKey('user.uid'))\n\t    content = db.Column(db.String(500))\n", "    time = db.Column(db.DateTime)\n\t    def __repr__(self):\n\t        return '<\"message\": {},'.format(self.content)+'\"time\": {}'.format(self.time)+\">\"\n\tclass paper_parse_queue(db.Model):\n\t    ppqid = db.Column(db.Integer, primary_key=True) #useless\n\t    arxivID = db.Column(db.String(50), unique=True)\n\t# init_db() is used to initialize the database, it should be called only once\n\tdef init_db():\n\t    # create the database and the db table\n\t    db.drop_all()\n", "    db.create_all()\n\t    db.session.commit()\n\t    # insert some test data\n\t    conn = sqlite3.connect('app.db')\n\t    print('opened database successfully')\n\t    c = conn.cursor()\n\t    sql_query = \"UPDATE paper SET  meta_info_json_path = 'arxiv//test_paper1//meta-info.json' where pid = 1;\"\n\t    c.execute(sql_query)\n\t    conn.commit()\n\t    sql_query = \"UPDATE paper SET  meta_info_json_path = 'arxiv//test_paper2//meta-info.json' where pid = 2;\"\n", "    c.execute(sql_query)\n\t    conn.commit()\n\t    # show the table\n\t    contents = c.execute(\"SELECT * FROM paper\")\n"]}
{"filename": "app/__init__.py", "chunked_list": ["from ._init import app,db,migrate,login\n\tfrom . import routes\n\tfrom . import models\n\tfrom . import Form\n\tfrom . import controller\n"]}
{"filename": "app/routes.py", "chunked_list": ["import os\n\tfrom flask import render_template, flash, redirect, url_for, request, send_from_directory, jsonify\n\tfrom flask_login import logout_user, login_user, current_user, login_required\n\tfrom werkzeug.urls import url_parse\n\tfrom flask_wtf import FlaskForm\n\tfrom wtforms import SelectField, SubmitField\n\tfrom wtforms.validators import ValidationError, DataRequired\n\timport sqlalchemy.sql.expression\n\tfrom .models import User, paper, paper_parse_queue\n\tfrom ._init import app, db\n", "from . import controller\n\tfrom .Form import LoginForm, messageForm #RegistrationForm\n\t# root page\n\t@app.route('/')\n\t@app.route('/index')\n\tdef index():\n\t    if current_user.is_authenticated:\n\t        return redirect(url_for('welcome'))\n\t    return render_template('Application.html')\n\t# route for login page\n", "@app.route('/Login', methods=['POST', 'GET'])\n\tdef login():\n\t    if current_user.is_authenticated:\n\t        return redirect(url_for('welcome'))\n\t    form = LoginForm()\n\t    if form.validate_on_submit():\n\t        user = User.query.filter_by(username=os.environ['ONLY_USER_NAME']).first()\n\t        if user is None or not user.check_password(form.password.data):\n\t            flash('Invalid username or password')\n\t            return redirect(url_for('login'))\n", "        login_user(user, remember=form.remember_me.data)\n\t        next_page = request.args.get('next')\n\t        if not next_page or url_parse(next_page).netloc != '':\n\t            next_page = url_for('welcome')\n\t        if user.Is_adm ==1:\n\t            next_page = url_for('AdminWelcome')\n\t        return redirect(next_page)\n\t    return render_template('Can_log.html', title='Sign In', form=form)\n\t# route for welcome page\n\t@app.route('/welcome', methods=['GET', 'POST'])\n", "@login_required\n\tdef welcome():\n\t    # tmp0 = list(paper.query.limit(10))\n\t    # https://stackoverflow.com/a/60815/7290857\n\t    arxivID_list = db.session.execute(db.select(paper.arxivID).order_by(sqlalchemy.sql.expression.func.random()).limit(10)).all()\n\t    tmp0 = [paper.query.filter_by(arxivID=x[0]).first() for x in arxivID_list]\n\t    return render_template('welcome.html', title='Home', user=current_user, paper_list=tmp0)\n\t# route for welcome page of administrator\n\t@app.route('/AdminWelcome', methods=['GET', 'POST'])\n\tdef AdminWelcome():\n", "    return render_template('AdminWelcome.html')\n\t# route for logout, redirect them to root page\n\t@app.route('/logout')\n\tdef logout():\n\t    logout_user()\n\t    return redirect(url_for('index'))\n\t# # route for register\n\t# @app.route('/register', methods=['GET', 'POST'])\n\t# def register():\n\t#     form = RegistrationForm()\n", "#     if form.validate_on_submit():\n\t#         user = User(username=form.username.data, email=form.email.data)\n\t#         user.set_password(form.password.data)\n\t#         db.session.add(user)\n\t#         db.session.commit()\n\t#         if current_user.is_authenticated:\n\t#             flash('Add a user')\n\t#         else:\n\t#             flash('Congratulations, you are now a registered user!')\n\t#         return redirect(url_for('login'))\n", "#     return render_template('Register.html', title='Register', form=form)\n\t# route for get paper\n\t@app.route('/paper/<arxivID>', methods=['GET', 'POST'])\n\t@login_required\n\tdef get_paper(arxivID):\n\t    tmp0 = paper.query.filter_by(arxivID=arxivID).first()\n\t    return render_template('paper.html', paper=tmp0, title='paper_chat')\n\t@app.route('/search', methods=['POST'])\n\t@login_required\n\tdef search_paper():\n", "    arxivID = str(request.form.get('arxivID')).strip()\n\t    tmp0 = paper.query.filter_by(arxivID=arxivID).first()\n\t    if tmp0 is None:\n\t        # TODO: add this arxivID to paper_parse_queue table\n\t        db.session.add(paper_parse_queue(arxivID=arxivID))\n\t        db.session.commit()\n\t        flash(\"No such paper yet! we will add it soon! please search again 1 minute later (usually).\")\n\t        ret = redirect(url_for('welcome'))\n\t    else:\n\t        ret = redirect(url_for('get_paper', arxivID=arxivID))\n", "    return ret\n\t# route for chat response\n\t@app.route('/chat_response', methods=['GET', 'POST'])\n\t@login_required\n\tdef chat_response():\n\t    if request.method == 'POST':\n\t        prompt = request.form['prompt']\n\t        res = {}\n\t        res['answer'] = controller.reply_message(current_user.get_id(), request.form['arxivID'], prompt)\n\t    return jsonify(res), 200\n", "# route to remove, add and make other users admin\n\t@app.route('/User_management', methods=['GET', 'POST'])\n\t@login_required\n\tdef User_management():\n\t    if current_user.if_adm() != 1:\n\t        return redirect(url_for('welcome'))\n\t    users = User.query.all()\n\t    return render_template('User_management.html', title='User_management', Users_list=users)\n\t# get user id for delete\n\t@app.route('/delete/<user_id>')\n", "@login_required\n\tdef delete_user(user_id):\n\t    if current_user.if_adm() != 1:\n\t        return redirect(url_for('welcome'))\n\t    controller.delete_user(user_id)\n\t    flash(\"User\"+str(user_id)+ \" has been deleted!\")\n\t    return redirect(url_for('User_management'))\n\t# make user an admin\n\t@app.route('/make_admin/<user_id>')\n\t@login_required\n", "def make_admin(user_id):\n\t    if current_user.if_adm() != 1:\n\t        return redirect(url_for('welcome'))\n\t    controller.make_admin(user_id)\n\t    flash(\"User\"+str(user_id)+ \" now is admin!\")\n\t    return redirect(url_for('User_management'))\n"]}
{"filename": "app/controller/_old_version.py", "chunked_list": ["import sqlite3\n\timport time\n\tfrom .gpt_utils import ArxivChatGPT\n\tfrom .database_utils import sqlite_get_arxivID_by_paper_id\n\t# The functions are used to interact with the database\n\tdef reply_message(user_id, arxivID, content):\n\t    # conn = sqlite3.connect('app.db')\n\t    # print('opened database successfully')\n\t    # c = conn.cursor()\n\t    # sql_query = \"INSERT INTO message (pid, uid, content, time) VALUES (\" + str(paper_id) + \", \" + str(user_id) + \", '\" + content + \"', datetime('now'))\"\n", "    # c.execute(sql_query)\n\t    # conn.commit()\n\t    # print('successfully inserted')\n\t    chatgpt = ArxivChatGPT(use_local_npy=True)\n\t    chatgpt.select(arxivID, print_meta_info=False)\n\t    ret = chatgpt.chat(content, tag_print=False, tag_return=True)\n\t    return ret\n\tdef get_papers(pid):\n\t    #connect to database\n\t    conn = sqlite3.connect('app.db')\n", "    print('opened database successfully')\n\t    c = conn.cursor()\n\t    cursor = c.execute(\"SELECT * FROM paper where pid = \" + str(pid))\n\t    for row in cursor:\n\t        cur_tup = (row[0])\n\t    return cur_tup\n\t# delete a user using his/her id\n\tdef delete_user(user_id):\n\t    conn = sqlite3.connect('app.db')\n\t    print('opened database successfully')\n", "    c = conn.cursor()\n\t    sql_query = \"DELETE FROM User where id = \" + str(user_id)\n\t    c.execute(sql_query)\n\t    conn.commit()\n\t    sql_query = \"DELETE FROM answer where user_id = \" + str(user_id)\n\t    c.execute(sql_query)\n\t    conn.commit()\n\t    print('successfully deleted')\n\t# make a user administrator\n\tdef make_admin(user_id):\n", "    conn = sqlite3.connect('app.db')\n\t    print('opened database successfully')\n\t    c = conn.cursor()\n\t    sql_query = \"UPDATE User SET Is_adm = 1 WHERE id = \" + str(user_id)\n\t    c.execute(sql_query)\n\t    conn.commit()\n\t    print('updated successfully')\n"]}
{"filename": "app/controller/__init__.py", "chunked_list": ["from ._old_version import get_papers, delete_user, make_admin, reply_message\n\tfrom .gpt_utils import ArxivChatGPT\n"]}
{"filename": "app/controller/gpt_utils.py", "chunked_list": ["import os\n\timport json\n\timport requests\n\tfrom tqdm import tqdm\n\timport openai\n\timport openai.embeddings_utils\n\timport numpy as np\n\tfrom .database_utils import sqlite3_load_all_paper_from, vector_database_find_close_chunk\n\t# from .crawl_utils import crawl_one_arxiv_paper\n\t# caller's duty to set openai.api_key\n", "class NaiveChatGPT:\n\t    def __init__(self) -> None:\n\t        self.message_list = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},]\n\t        self.response = None #for debug only\n\t    def reset(self):\n\t        self.message_list = self.message_list[:1]\n\t    def chat(self, message='', tag_print=True, tag_return=False):\n\t        message = str(message)\n\t        if message: #skip if empty\n\t            self.message_list.append({\"role\": \"user\", \"content\": str(message)})\n", "            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n\t            tmp0 = self.response.choices[0].message.content\n\t            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n\t            if tag_print:\n\t                print(tmp0)\n\t            if tag_return:\n\t                return tmp0\n\tclass ContextChatGPT:\n\t    def __init__(self):\n\t        self.message_list = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the context below, \"\n", "                              \"and if the question can't be answered based on the context, say \\\"I don't know\\\"\"},]\n\t        self.response = None\n\t    def reset(self):\n\t        self.message_list = self.message_list[:1]\n\t    def set_context(self, context, use_gpt_reply=False):\n\t        tmp0 = '\\nAbove is some context, no need to reply and just acknowledge it with \"...\"'\n\t        self.message_list.append({'role':'user', 'content': context+tmp0})\n\t        if use_gpt_reply:\n\t            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n\t            tmp0 = self.response.choices[0].message.content\n", "            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n\t        else:\n\t            self.message_list.append({\"role\": \"assistant\", \"content\": '...'})\n\t    def chat(self, message, tag_print=True, tag_return=False):\n\t        message = str(message)\n\t        if message: #skip if empty\n\t            self.message_list.append({\"role\": \"user\", \"content\": str(message)})\n\t            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n\t            tmp0 = self.response.choices[0].message.content\n\t            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n", "            if tag_print:\n\t                print(tmp0)\n\t            if tag_return:\n\t                return tmp0\n\tclass ArxivChatGPT:\n\t    def __init__(self, use_local_npy=False):\n\t        self.message_list = [{\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based on the context below, \"\n\t                              \"and if the question can't be answered based on the context, say \\\"I don't know\\\"\"},]\n\t        self.response = None\n\t        self._db_paper_list = sqlite3_load_all_paper_from()\n", "        self.arxivID_list = [x['arxivID'] for x in self._db_paper_list]\n\t        self._db_paper_i = None\n\t        self.use_local_npy = use_local_npy\n\t    # def add_arxiv_paper_to_db(self, arxivID):\n\t    #     assert isinstance(arxivID, str)\n\t    #     crawl_one_arxiv_paper(arxivID, tag_commit_sqlite3=True)\n\t    #     self._db_paper_list = sqlite3_load_all_paper_from()\n\t    #     self.arxivID_list = [x['arxivID'] for x in self._db_paper_list]\n\t    def list_arxiv(self, num_print=-1):\n\t        db_paper_list = self._db_paper_list\n", "        if num_print>0:\n\t            db_paper_list = db_paper_list[:num_print]\n\t        for ind0,x in enumerate(db_paper_list):\n\t            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], x['meta_info_json_path'])\n\t            with open(tmp0, 'r') as f:\n\t                meta_info = json.load(f)\n\t            print(f'[{ind0}]', x['arxivID'], meta_info['title'])\n\t    def select(self, index, print_meta_info=True):\n\t        if isinstance(index, str):\n\t            arxivID = index\n", "            if arxivID not in self.arxivID_list:\n\t                print(f'Error: {arxivID} not in arxivID_list')\n\t                return\n\t            index = self.arxivID_list.index(arxivID)\n\t        else:\n\t            if (index<0) or (index>=len(self.arxivID_list)):\n\t                print(f'Error: index {index} out of range [0, {len(self.arxivID_list)-1}]')\n\t                return\n\t        self._db_paper_i = self._db_paper_list[index]\n\t        if print_meta_info:\n", "            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], self._db_paper_i['meta_info_json_path'])\n\t            with open(tmp0, 'r') as f:\n\t                meta_info = json.load(f)\n\t            for key,value in meta_info.items():\n\t                print(f'[{key}]: {value}')\n\t        self.reset()\n\t    def reset(self):\n\t        self.message_list = self.message_list[:1]\n\t    def set_context(self, context, use_gpt_reply=False):\n\t        tmp0 = '\\nAbove is some context, no need to reply and just acknowledge it with \"...\"'\n", "        self.message_list.append({'role':'user', 'content': context+tmp0})\n\t        if use_gpt_reply:\n\t            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n\t            tmp0 = self.response.choices[0].message.content\n\t            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n\t        else:\n\t            self.message_list.append({\"role\": \"assistant\", \"content\": '...'})\n\t    def _find_related_chunk(self, message):\n\t        max_context_len = int(os.environ['ARXIVGPT_MAX_TOKEN_PER_QA'])\n\t        if self.use_local_npy:\n", "            q_embedding = openai.Embedding.create(input=message, engine='text-embedding-ada-002')['data'][0]['embedding']\n\t            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], self._db_paper_i['chunk_text_json_path'])\n\t            assert os.path.exists(tmp0)\n\t            with open(tmp0, 'r') as fid:\n\t                chunk_text_list = json.load(fid)\n\t                chunk_text_len_list = np.array([x[1] for x in chunk_text_list])\n\t                chunk_text_str_list = [x[0] for x in chunk_text_list]\n\t            tmp0 = os.path.join(os.environ['ARXIV_DIRECTORY'], self._db_paper_i['arxivID'], 'chunk-vector.npy')\n\t            assert os.path.exists(tmp0)\n\t            embedding_np = np.load(tmp0)\n", "            distance = np.array(openai.embeddings_utils.distances_from_embeddings(q_embedding, embedding_np, distance_metric='cosine')) # 0: cloest\n\t            ind0 = np.argsort(distance)\n\t            tmp0 = np.nonzero((chunk_text_len_list[ind0] + 4).cumsum() > max_context_len)[0].min()\n\t            context_text_list = [chunk_text_str_list[x] for x in ind0[:tmp0]]\n\t        else:\n\t            context_text_list = vector_database_find_close_chunk(self._db_paper_i['arxivID'], message, max_context_len)\n\t        for x in context_text_list:\n\t            self.set_context(x, use_gpt_reply=False)\n\t    def chat(self, message, tag_reset=True, tag_print=True, tag_return=False):\n\t        assert self._db_paper_i is not None\n", "        if tag_reset:\n\t            self.reset()\n\t        message = str(message)\n\t        if message: #skip if empty\n\t            self._find_related_chunk(message)\n\t            self.message_list.append({\"role\": \"user\", \"content\": str(message)})\n\t            self.response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=self.message_list)\n\t            tmp0 = self.response.choices[0].message.content\n\t            self.message_list.append({\"role\": \"assistant\", \"content\": tmp0})\n\t            if tag_print:\n", "                print(tmp0)\n\t            if tag_return:\n\t                return tmp0\n"]}
{"filename": "app/controller/database_utils.py", "chunked_list": ["import os\n\timport math\n\timport sqlite3\n\timport weaviate\n\timport numpy as np\n\t# TODO\n\tdef sqlite_insert_paper_list(paper_list):\n\t    paper_dict = {x['arxivID']:x for x in paper_list} #remove duplicate arxivID\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    for arxivID, x in paper_dict.items():\n", "        sql_conn.execute('DELETE FROM paper WHERE arxivID = ?', (arxivID,)) #remove old first\n\t        # sql_conn.execute('SELECT * FROM paper').fetchall()\n\t    tmp0 = 'arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n\t    tmp1 = [tuple(x[y] for y in tmp0) for x in paper_dict.values()]\n\t    tmp2 = \",\".join(tmp0)\n\t    sql_conn.executemany(f'INSERT INTO paper ({tmp2}) VALUES (?,?,?,?,?,?)', tmp1)\n\t    sql_conn.commit()\n\t    sql_conn.close()\n\tdef sqlite_get_arxivID_by_paper_id(pid):\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n", "    arxivID = sql_conn.execute('SELECT arxivID FROM paper WHERE pid = ?', (pid,)).fetchone()[0]\n\t    sql_conn.close()\n\t    return arxivID\n\tdef sqlite3_load_all_paper_from():\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    paper_list = sql_conn.execute('SELECT * FROM paper').fetchall()\n\t    sql_conn.close()\n\t    tmp0 = 'pid arxivID meta_info_json_path pdf_path tex_path chunk_text_json_path num_chunk'.split(' ')\n\t    ret = [{y0:y1 for y0,y1 in zip(tmp0,x)} for x in paper_list]\n\t    return ret\n", "# print('pid | arxivID | meta_info_json_path | pdf_path | tex_path | chunk_text_json_path | num_chunk')\n\t# for x in sqlite3_load_all_paper_from():\n\t#     print(x)\n\tdef init_sqlite3_database(remove_if_exist=False):\n\t    sql_conn = sqlite3.connect(os.environ['SQLITE3_DB_PATH'])\n\t    if remove_if_exist:\n\t        sql_conn.execute('DROP TABLE IF EXISTS paper')\n\t    cmd = '''create table if not exists paper (\n\t        pid integer primary key,\n\t        arxivID text,\n", "        meta_info_json_path text,\n\t        pdf_path text,\n\t        tex_path text,\n\t        chunk_text_json_path text,\n\t        num_chunk integer\n\t    )\n\t    '''\n\t    sql_conn.execute(cmd)\n\t    sql_conn.commit()\n\t    sql_conn.close()\n", "Weaviate_Paper_schema = {\n\t    \"class\": \"Paper\",\n\t    \"description\": \"A collection of arxiv paper\",\n\t    \"vectorizer\": \"text2vec-openai\",\n\t    \"moduleConfig\": {\n\t        \"text2vec-openai\": {\n\t          \"model\": \"ada\",\n\t          \"modelVersion\": \"002\",\n\t          \"type\": \"text\"\n\t        }\n", "    },\n\t    \"properties\": [{\n\t        \"name\": \"chunk\",\n\t        \"description\": \"chunk contents of the paper\",\n\t        \"dataType\": [\"text\"]\n\t    },\n\t    {\n\t        \"name\": \"arxiv_id\",\n\t        \"description\": \"arxiv ID\",\n\t        \"dataType\": [\"string\"],\n", "        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    },\n\t    {\n\t        \"name\": \"num_chunk\",\n\t        \"description\": \"total number of chunk\",\n\t        \"dataType\": [\"int\"],\n\t        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    },\n\t    {\n\t        \"name\": \"num_token\",\n", "        \"description\": \"number of token\",\n\t        \"dataType\": [\"int\"],\n\t        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    },\n\t    {\n\t        \"name\": \"index\",\n\t        \"description\": \"index of the chunk\",\n\t        \"dataType\": [\"int\"],\n\t        \"moduleConfig\": { \"text2vec-openai\": { \"skip\": True } }\n\t    }]\n", "}\n\tdef init_vector_database(remove_if_exist=False):\n\t    client = _get_vector_database(with_openai_api_key=False)\n\t    tag_exist = 'Paper' in {x['class'] for x in client.schema.get()['classes']}\n\t    if remove_if_exist and tag_exist:\n\t        client.schema.delete_class('Paper')\n\t        tag_exist = False\n\t    if not tag_exist:\n\t        client.schema.create_class(Weaviate_Paper_schema)\n\t'''\n", "chunk: text\n\tarxiv_id: string\n\tindex: int\n\tnum_token: int\n\tnum_chunk: int\n\tvector (ada-002)\n\t'''\n\tdef _get_vector_database(with_openai_api_key):\n\t    tmp0 = weaviate.auth.AuthApiKey(os.environ['WEAVIATE_API_KEY'])\n\t    if with_openai_api_key:\n", "        tmp1 = {\"X-OpenAI-Api-Key\": os.environ['OPENAI_API_KEY']} #optional\n\t    else:\n\t        tmp1 = None\n\t    client = weaviate.Client(url=os.environ['WEAVIATE_API_URL'], auth_client_secret=tmp0, additional_headers=tmp1)\n\t    return client\n\tdef vector_database_insert_paper(arxivID, text_chunk_list, vector_list=None):\n\t    # text_chunk_list(list,tuple(str,int))\n\t    client = _get_vector_database(with_openai_api_key=True)\n\t    num_chunk = len(text_chunk_list)\n\t    uuid_list = []\n", "    if vector_list is not None:\n\t        assert len(vector_list)==num_chunk\n\t    with client.batch as batch:\n\t        batch.batch_size = 20 #20-100\n\t        # https://github.com/openai/openai-cookbook/tree/main/examples/vector_databases/weaviate\n\t        # client.batch.configure(batch_size=10,  dynamic=True, timeout_retries=3)\n\t        for ind0 in range(num_chunk):\n\t            tmp0 = dict(arxiv_id=arxivID, num_chunk=num_chunk, index=ind0, chunk=text_chunk_list[ind0][0], num_token=text_chunk_list[ind0][1])\n\t            tmp1 = vector_list[ind0] if vector_list is not None else None\n\t            uuid_list.append(batch.add_data_object(tmp0, class_name='Paper', vector=tmp1))\n", "    return uuid_list\n\tdef vector_database_contains_paper(arxivID:str):\n\t    client = _get_vector_database(with_openai_api_key=False)\n\t    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t    num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n\t    return num_chunk>0\n\tdef vector_database_retrieve_paper(arxivID:str, index=None):\n\t    client = _get_vector_database(with_openai_api_key=False)\n\t    # TODO handle error\n\t    if index is None:\n", "        tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t        num_chunk = client.query.aggregate(\"Paper\").with_fields(\"meta {count}\").with_where(tmp0).do()['data']['Aggregate']['Paper'][0]['meta']['count']\n\t        assert num_chunk>0\n\t        # TODO batch_size=100\n\t        response = client.query.get(\"Paper\", [\"chunk\", \"index\"]).with_where(tmp0).with_limit(num_chunk).do()\n\t        tmp1 = sorted(response['data']['Get']['Paper'], key=lambda x:x['index'])\n\t        assert tuple(x['index'] for x in tmp1)==tuple(range(num_chunk))\n\t        text_chunk_list = [x['chunk'] for x in tmp1]\n\t        # vector_np = np.zeros((1, 1536), dtype=np.float64)\n\t        ret = text_chunk_list\n", "    else:\n\t        raise NotImplementedError\n\t        # tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t        # text_chunk = ''\n\t        # vector_np = np.zeros(1536, dtype=np.float64)\n\t        # return text_chunk, vector_np\n\t    return ret\n\tdef vector_database_find_close_chunk(arxivID, message, max_context_len):\n\t    client = _get_vector_database(with_openai_api_key=True)\n\t    nearText = {\"concepts\": [message]}\n", "    tmp0 = {\"path\": [\"arxiv_id\"], \"operator\": \"Equal\", \"valueString\": arxivID}\n\t    result = client.query.get(\"Paper\", [\"chunk\", \"num_token\"]).with_near_text(nearText).with_where(tmp0).with_additional(['certainty']).with_limit(10).do()\n\t    certainty = [x['_additional']['certainty'] for x in result['data']['Get']['Paper']] #in descending order\n\t    num_token_list = np.array([x['num_token'] for x in result['data']['Get']['Paper']])\n\t    chunk_text_str_list = [x['chunk'] for x in result['data']['Get']['Paper']]\n\t    np.nonzero((num_token_list + 4).cumsum() <= max_context_len)\n\t    tmp0 = np.nonzero((num_token_list + 4).cumsum() <= max_context_len)[0][-1] + 1\n\t    ret = chunk_text_str_list[:tmp0]\n\t    return ret\n"]}
