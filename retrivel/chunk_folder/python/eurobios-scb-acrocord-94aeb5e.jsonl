{"filename": "test/test_misc.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\tfrom acrocord.misc import execution\n", "import pytest\n\tdef test_deprecated():\n\t    @execution.deprecated\n\t    def foo():\n\t        return \"hello\"\n\t    with pytest.deprecated_call():\n\t        foo()\n\tdef test_print():\n\t    @execution.execution_time\n\t    def foo():\n", "        return \"hello\"\n\t    assert foo() == \"hello\"\n"]}
{"filename": "test/test_logger.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\timport pytest\n", "from acrocord.utils.logger import Logger\n\t@pytest.fixture\n\tdef get_logger(get_connection) -> Logger:\n\t    logger = Logger(connection=get_connection, table_name='log_table', schema='test')\n\t    assert 'test.log_table' == logger.table_name\n\t    return logger\n\tdef test_write_log_dataframe(get_logger: Logger, get_example_log_dataframe) -> None:\n\t    get_logger.write_log(get_example_log_dataframe)\n\tdef test_write_log_series(get_logger: Logger, get_example_log_series) -> None:\n\t    get_logger.write_log(get_example_log_series)\n", "def test_write_log_dict(get_logger: Logger, get_example_log_dict) -> None:\n\t    get_logger.write_log(get_example_log_dict)\n\tdef test_write_log_other_type(get_logger: Logger, get_example_log_other_type) -> None:\n\t    with pytest.raises(ValueError) as exc_info:\n\t        get_logger.write_log(get_example_log_other_type)\n\t        assert str(exc_info.value) == \"Input data should be type of [pd.Series, pd.DataFrame, or dict]\"\n"]}
{"filename": "test/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n"]}
{"filename": "test/test_constraints.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\timport pytest\n", "from acrocord.test import constraints\n\tdef test_example_span_table_constraint(\n\t        get_example_dataframe_building):\n\t    data = get_example_dataframe_building\n\t    example_constraints = {\n\t        \"first_constraint\": (constraints.unique, {\"columns\": [\"building_id\"]}),\n\t        \"second_constraint\": (\n\t            constraints.data_type,\n\t            {'columns': ['building_id'], 'dtype': 'int64'}),\n\t        \"forth_constraint\":\n", "            (constraints.nb_unique_index,\n\t             {\"columns\": [\"building_id\"], \"minimum\": 0,\n\t              \"maximum\": 10}),\n\t        \"fifth_constraint\": (constraints.quantile,\n\t                             {\"columns\": [\"building_id\"], \"q\": 0.99,\n\t                              \"threshold\": 1000}),\n\t        \"third_constraint\": (\n\t            constraints.not_nullable, {\"columns\": [\"building_id\"]})\n\t    }\n\t    dc = constraints.DataConstraints(data)\n", "    dc.add_constraint(example_constraints)\n\t    dc.test()\n\tdef test_example_span_table_constraint_raise(get_example_dataframe_building):\n\t    data = get_example_dataframe_building\n\t    example_constraints = {\n\t        \"first_constraint\": (constraints.eligible_data_type, {}),\n\t        \"second_constraint\": (\n\t            constraints.data_type, {'columns': 'building_id'}),\n\t    }\n\t    dc = constraints.DataConstraints(data)\n", "    dc.add_constraint(example_constraints)\n\t    with pytest.raises(AssertionError):\n\t        dc.test()\n"]}
{"filename": "test/test_databasepg.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\timport warnings\n", "import pandas as pd\n\tdef test_dataframe_equals_reading_writing(get_connection,\n\t                                          get_example_data_frame):\n\t    get_connection.write_table(get_example_data_frame, 'test.test_data')\n\t    dataframe_read = get_connection.read_table('test.test_data')\n\t    get_connection.drop_table('test.test_data')\n\t    assert get_example_data_frame.equals(\n\t        dataframe_read), \"dataframe and dataframe_read are the same\"\n\tdef test_warning_types(get_example_data_frame):\n\t    from acrocord.utils.types import warning_type\n", "    dataframe = get_example_data_frame.astype(int)\n\t    with warnings.catch_warnings(record=True) as w:\n\t        warnings.simplefilter(\"always\")\n\t        warning_type(dataframe, 3)\n\t        assert len(w) == dataframe.shape[1]\n\tdef test_warning_types_64(get_example_data_frame):\n\t    from acrocord.utils.types import warning_type\n\t    dataframe = get_example_data_frame.astype(\"int64\")\n\t    warning_type(dataframe, 3)\n\tdef test_get_metadata(get_connection, get_example_data_frame):\n", "    get_connection.write_table(get_example_data_frame, 'test.dataframe')\n\t    df_meta = get_connection.get_metadata('test.dataframe')\n\t    error_msg = \"The number of line in meta data does correspond to the number of columns \"\n\t    number_columns = len(get_connection.get_columns('test.dataframe'))\n\t    get_connection.drop_table('test.dataframe')\n\t    assert len(df_meta) == number_columns, error_msg\n\tdef test_foreign_keys(get_connection, get_example_data_frame,\n\t                      get_example_data_frame_other):\n\t    df1 = get_example_data_frame\n\t    df2 = get_example_data_frame_other\n", "    get_connection.write_table(df1, \"test.main\", primary_key='a')\n\t    get_connection.write_table(df2, \"test.other\",\n\t                               foreign_keys={'a': \"test.main\"})\n\tdef test_get_shape(get_connection, get_example_data_frame):\n\t    get_connection.write_table(get_example_data_frame, \"test.main\", primary_key='a')\n\t    s = get_connection.get_shape(\"test.main\")\n\t    assert (s == get_example_data_frame.shape)\n\tdef test_query_where(get_connection, get_example_data_frame):\n\t    get_connection.write_table(get_example_data_frame, \"test.main\",\n\t                               primary_key='a')\n", "    df = get_connection.read_table(\"test.main\", where=\"a in (155)\")\n\t    assert len(df) == 1\n\tdef test_query_limit(get_connection, get_example_data_frame):\n\t    get_connection.write_table(get_example_data_frame, \"test.main\",\n\t                               primary_key='a')\n\t    df = get_connection.read_table(\"test.main\", limit=2)\n\t    assert len(df) == 2\n\tdef test_create_insert(get_connection, get_example_data_frame):\n\t    get_connection.write_table(get_example_data_frame, \"test.main\")\n\t    get_connection.create_insert(get_example_data_frame, \"test.main\")\n", "    df = get_connection.read_table(\"test.main\")\n\t    assert len(df) == len(get_example_data_frame) * 2\n\tdef test_create_insert_create(get_connection, get_example_data_frame):\n\t    get_connection.drop_table(\"test.main\")\n\t    get_connection.create_insert(get_example_data_frame, \"test.main\")\n\t    df = get_connection.read_table(\"test.main\")\n\t    assert len(df) == len(get_example_data_frame)\n\tdef test_create_drop_create_column(get_connection, get_example_data_frame):\n\t    get_connection.write_table(get_example_data_frame, \"test.main\")\n\t    get_connection.drop_column(\"test.main\", \"b\")\n", "    df = get_connection.read_table(\"test.main\")\n\t    assert df.shape[1] == get_example_data_frame.shape[1] - 1\n\t    get_connection.add_columns(\n\t        \"test.main\",\n\t        get_example_data_frame[[\"a\", \"b\"]],\n\t        index=\"a\"\n\t    )\n\tdef test_copy(get_connection, get_example_data_frame_other):\n\t    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n\t    get_connection.copy(\"test.main\", \"test.main_copy\")\n", "    df1 = get_connection.read_table(\"test.main\")\n\t    df2 = get_connection.read_table(\"test.main_copy\")\n\t    assert df1.equals(df2)\n\tdef test_get_table_names(get_connection, get_example_data_frame_other):\n\t    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n\t    assert len(get_connection.get_table_names(\"test\")) > 0\n\tdef test_get_view_names(get_connection, get_example_data_frame_other):\n\t    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n\t    get_connection.execute(\"create view public.test as select * from test.main\")\n\t    assert len(get_connection.get_view_names(\"public\")) > 0\n", "def test_create_schema(get_connection, get_example_data_frame):\n\t    get_connection.create_schema(\"test_ci\")\n\t    get_connection.write_table(get_example_data_frame, \"test_ci.test_schema\")\n\t    assert \"test_ci\" in get_connection.get_schema_names()\n\t# def test_geopandas(get_connection, get_example_data_frame):\n\t#     import geopandas\n\t#     connection = get_connection\n\t#     dataframe = get_example_data_frame\n\t#     gdf = geopandas.GeoDataFrame(\n\t#         dataframe,\n", "#         geometry=geopandas.points_from_xy(dataframe.lon, dataframe.lat))\n\t#     connection.write_table(gdf, \"test.test_lat_lon\")\n\t#\n\t#\n\tdef test_list_table(get_connection):\n\t    list_table = get_connection.list_table(\"public\")\n\t    print(list_table)\n\tdef test_update(get_connection, get_example_data_frame_other):\n\t    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n\t    get_connection.update(\"test.other\", where=\"a=155\",\n", "                          value=\"150\", column=\"a\")\n\t    df_read = get_connection.read_table(\"test.other\")\n\t    assert 150 in df_read[\"a\"].values\n\tdef test_get_dtypes(get_connection, get_example_data_frame_other):\n\t    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n\t    res = get_connection.get_dtypes(\"test.other\")\n\t    assert isinstance(res, pd.DataFrame)\n\t    assert len(res) == len(get_connection.get_columns(\"test.other\"))\n\tdef test_drop(get_connection, get_example_data_frame, get_example_data_frame_other):\n\t    get_connection.write_table(get_example_data_frame_other, \"test.other\")\n", "    get_connection.write_table(get_example_data_frame, \"test.main\")\n\t    get_connection.drop(\"test.main\", \"table\", \"a\")\n\t    assert \"a\" not in get_connection.get_columns(\"test.main\")\n\t    get_connection.write_table(get_example_data_frame_other, \"test.main\")\n\tdef test_execute(get_connection, get_example_data_frame):\n\t    get_connection.write_table(get_example_data_frame, \"test.main\",\n\t                               primary_key='a')\n\t    get_connection.execute(\"select * from test.main\")\n\tdef test_names_empty_list(get_connection):\n\t    get_connection.execute(\"DROP SCHEMA if exists test_names\")\n", "    get_connection.create_schema(\"test_names\")\n\t    assert len(get_connection.get_table_names(\"test_names\")) == 0\n\t    get_connection.execute(\"DROP SCHEMA test_names\")\n"]}
{"filename": "test/test_connect.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\timport asyncio\n", "import pytest\n\tdef test_password_auth(get_connection):\n\t    from acrocord.utils.connect import password_auth\n\t    try:\n\t        assert isinstance(password_auth(), str)\n\t    except (PermissionError, FileNotFoundError):\n\t        pass\n\tdef test_connect_literal():\n\t    from acrocord.utils.connect import connect_psql_server\n\t    str_ = \"Engine(postgresql+psycopg2://user:***@localhost:5432/name)\"\n", "    assert str(connect_psql_server(\n\t        username=\"user\",\n\t        connection=dict(\n\t            password=\"pass\",\n\t            sslmode=\"require\",\n\t            host=\"localhost\",\n\t            port=\"5432\",\n\t            ssh=False, dbname=\"name\"))) == str_\n\t@pytest.mark.asyncio\n\tasync def test_connect_async_main(postgresql):\n", "    from acrocord.utils.connect import connect_psql_server\n\t    connect_psql_server(username=postgresql.info.user,\n\t                        connection=dict(\n\t                            password=\" \",\n\t                            sslmode=\"require\",\n\t                            host=postgresql.info.host,\n\t                            port=postgresql.info.port,\n\t                            ssh=False, dbname=postgresql.info.dbname), async_=True)\n"]}
{"filename": "test/test_table_factory.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\timport os\n", "from typing import Dict, Tuple\n\timport pandas as pd\n\timport pytest\n\tfrom acrocord.factory.table import TableFactory\n\tfrom acrocord.utils.types import EligibleDataType\n\tclass BuildingTableFactory(TableFactory):\n\t    # name of the table\n\t    @classmethod\n\t    def table_name(cls) -> str:\n\t        return 'buildings'\n", "    # name of the schema where to store the table\n\t    @classmethod\n\t    def schema_name(cls) -> str:\n\t        return 'test'\n\t    # column format statement and documentation.\n\t    @classmethod\n\t    def data_definition(cls) -> Dict[str, Tuple[EligibleDataType, str]]:\n\t        return {\n\t            'building_id': (EligibleDataType.INTEGER, 'Identification number'),\n\t            'architect': (EligibleDataType.STRING, 'Architect name'),\n", "            'height': (\"float16\", ''),\n\t            'construction_date': (\n\t                EligibleDataType.DATE_TIME,\n\t                'Construction Date of the building'),\n\t            # 'is_constructed': (\n\t            #     EligibleDataType.BOOLEAN, 'Does the building already construct') -> test\n\t        }\n\t    # this is the primary key of your table it must exist in\n\t    # BuildingTableFactory._doc_cols\n\t    # and as it is a row identifier, each row of your\n", "    # table must have a unique non-null value\n\t    @classmethod\n\t    def id_key(cls) -> str:\n\t        return 'building_id'\n\t    # This is the main table where you get data from other sources\n\t    # (possibly another TableFactory).\n\t    # The only restriction is to\n\t    def _create_table(self) -> None:\n\t        df = pd.DataFrame(data={'building_id': [11, 20, 14, 34, 61],\n\t                                'architect': [\"Durand\", \"Blanc\", \"Blanc\",\n", "                                              \"Dubois\", \"Martin\"],\n\t                                'height': [14.4, 24.4, 35.3, 12.3, 14.4],\n\t                                'construction_date': ['10/03/1957',\n\t                                                      '30/11/2087',\n\t                                                      '01/02/2070',\n\t                                                      '04/01/1989',\n\t                                                      '28/10/2003']\n\t                                })\n\t        df['construction_date'] = pd.to_datetime(df.construction_date,\n\t                                                 format='%d/%m/%Y')\n", "        from datetime import datetime\n\t        df['is_constructed'] = df.construction_date < datetime.now()\n\t        self._set_table(df)  # Required line to store the result\n\t    @classmethod\n\t    def get_db_connection(cls):\n\t        return cls.connection\n\t@pytest.fixture\n\tdef get_table_factory(get_connection) -> BuildingTableFactory:\n\t    BuildingTableFactory.connection = get_connection\n\t    table_factory = BuildingTableFactory(verbose=True)\n", "    return table_factory\n\tdef test_write(get_table_factory: BuildingTableFactory, get_connection) -> None:\n\t    get_table_factory.write_table()\n\t    get_table_factory.get_table()\n\t    assert \"buildings\" in get_connection.get_table_names(\"test\")\n\tdef test_read(get_table_factory: BuildingTableFactory) -> None:\n\t    get_table_factory.write_table()\n\t    get_table_factory.read_table()\n\tdef test_implement_foreign_keys(\n\t        get_table_factory: BuildingTableFactory) -> None:\n", "    get_table_factory.add_foreign_keys()\n\tdef test_get_table_name(get_table_factory: BuildingTableFactory) -> None:\n\t    assert \"test.buildings\" == get_table_factory.get_full_name()\n\tdef test_write_to_excel(get_table_factory: BuildingTableFactory) -> None:\n\t    import tempfile\n\t    get_table_factory.write_to_excel()\n\t    assert \"buildings.xlsx\" in os.listdir(tempfile.gettempdir())\n\tdef test_columns(get_table_factory: BuildingTableFactory) -> None:\n\t    print(get_table_factory.columns)\n\t    for c in get_table_factory.columns:\n", "        assert c in ['building_id', 'architect', 'height',\n\t                     'construction_date',\n\t                     'is_constructed'], f\"column {c} is not in columns\"\n\t    assert isinstance(get_table_factory.columns,\n\t                      list), \"columns attribute is not a list\"\n\tdef test_get_instance(get_table_factory):\n\t    inst = get_table_factory.get_instance()\n\t    assert inst._table is None\n\t    inst.get_table()\n\tdef test_check_table_doc(get_table_factory):\n", "    get_table_factory.check_table_doc(get_table_factory.get_table(),\n\t                                      get_table_factory.data_definition())\n\t# Test to add\n\t# TODO:  test if deployed and _table is freed\n\t# TODO:  test if second _get_data is faster\n"]}
{"filename": "test/conftest.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\timport pandas as pd\n", "import pytest\n\tfrom acrocord import ConnectDatabase\n\t@pytest.fixture(autouse=True)\n\tdef get_connection(postgresql):\n\t    db = ConnectDatabase()\n\t    connection = db.connect(verbose=3,\n\t                            connection={\n\t                                \"dbname\": postgresql.info.dbname,\n\t                                \"password\": \" \",\n\t                                \"user\": postgresql.info.user,\n", "                                \"port\": postgresql.info.port,\n\t                                \"host\": postgresql.info.host\n\t                            })\n\t    connection.create_schema(\"test\")\n\t    return connection\n\t@pytest.fixture(scope=\"module\")\n\tdef get_example_data_frame():\n\t    dataframe = pd.DataFrame({'a': [155, 20, 3],\n\t                              'b': [11, 299, 45],\n\t                              'c': [73, 3, 39],\n", "                              'd': [783, 488, 739],\n\t                              \"lat\": [45, 45, 45],\n\t                              \"lon\": [0, 1, 2]\n\t                              })\n\t    return dataframe\n\t@pytest.fixture(scope=\"module\")\n\tdef get_example_data_frame_other():\n\t    dataframe = pd.DataFrame({'a': [155, 20, 155],\n\t                              'e': [\"a\", \"b\", \"b\"]\n\t                              })\n", "    return dataframe\n\t@pytest.fixture(scope=\"module\")\n\tdef db_connector(get_connection):\n\t    return get_connection\n\t@pytest.fixture(scope=\"module\")\n\tdef get_example_dataframe_building():\n\t    df = pd.DataFrame(data={'building_id': [11, 20, 14, 34, 61],\n\t                            'architect': [\"Durand\", \"Blanc\", \"Blanc\",\n\t                                          \"Dubois\", \"Martin\"],\n\t                            'height': [14.4, 24.4, 35.3, 12.3, 14.4],\n", "                            'construction_date': ['10/03/1957',\n\t                                                  '30/11/2087',\n\t                                                  '01/02/2070',\n\t                                                  '04/01/1989',\n\t                                                  '28/10/2003']\n\t                            })\n\t    return df\n\t@pytest.fixture(scope=\"module\")\n\tdef get_example_log_dataframe():\n\t    dataframe = pd.DataFrame({'value': [1, 2, 3],\n", "                              'message': 'test log with pd.DataFrame type',\n\t                              'other': 'tmp',\n\t                              })\n\t    return dataframe\n\t@pytest.fixture(scope=\"module\")\n\tdef get_example_log_series():\n\t    return pd.Series([1, 3, 4, 5])\n\t@pytest.fixture(scope=\"module\")\n\tdef get_example_log_dict():\n\t    return {'value': 1, 'message': 'test log with dict type'}\n", "@pytest.fixture(scope=\"module\")\n\tdef get_example_log_other_type():\n\t    return [1, 2, 3]\n"]}
{"filename": "test/test_auxiliaries.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n\t# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and limitations under the License.\n\timport pytest\n", "from sqlalchemy.exc import ProgrammingError\n\tfrom acrocord.utils import auxiliaries\n\tdef test_execute_sql_cmd_raise_error(get_connection):\n\t    sql_cmd_with_error = \"select *\"\n\t    with pytest.raises(ProgrammingError) as e_info:\n\t        auxiliaries.execute_sql_cmd(get_connection, sql_cmd_with_error)\n\tdef test_drop_table():\n\t    assert auxiliaries.drop_table(\"test\") == \"DROP TABLE IF \" \\\n\t                                             \"EXISTS test CASCADE;\"\n\tdef test_count():\n", "    assert auxiliaries.count(\"test\") == \"SELECT COUNT(*) FROM test;\"\n\tdef test_list_table():\n\t    assert auxiliaries.list_table(\n\t        \"test\") == f\"SELECT * FROM information_schema.tables \" \\\n\t                   f\"WHERE table_schema = 'test'\"\n\tdef test_data_types():\n\t    a, b, c, d = \"int32\", \"int4\", \"datetime64[s]\", \"timestamp\"\n\t    assert auxiliaries.db_data_type(a) == b\n\t    assert auxiliaries.db_data_type(b, invert=True) == a\n\t    assert auxiliaries.db_data_type(c) == d\n", "    assert auxiliaries.db_data_type(d, invert=True) == c\n"]}
{"filename": "acrocord/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .databasepg import ConnectDatabase\n"]}
{"filename": "acrocord/databasepg.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\t\"\"\"\n\tThis module contains the main class to be used\n\tIt solely contains the class ConnectDatabase\n\t\"\"\"\n\timport io\n\timport logging\n\timport typing\n", "from typing import Union, Iterable, Dict\n\timport pandas as pd\n\tfrom sqlalchemy import exc, text\n\tfrom acrocord.misc import execution\n\tfrom acrocord.utils import auxiliaries\n\tfrom acrocord.utils import connect\n\tfrom acrocord.utils import insert\n\tfrom acrocord.utils.types import warning_type\n\tLOGGER = logging.getLogger(__name__)\n\tlogging.basicConfig(format='%(levelname)s:%(message)s')\n", "class ConnectDatabase:\n\t    def __init__(self):\n\t        self.username = \"\"\n\t        self.engine = None\n\t        self.connection = None\n\t        self.replace = self.rename\n\t        self.rename_ = {}\n\t        self.active_execution_time = False\n\t        self.active_sql_printing = False\n\t        self.connections = connect.load_available_connections()\n", "        self.verbose = 1\n\t    def connect(self, verbose: int = 1,\n\t                connection: Union[dict, str] = None,\n\t                **kwargs) -> \"ConnectDatabase\":\n\t        \"\"\"\n\t        Instantiate the connexion with the database. This function has to be\n\t        called to access other methods.\n\t        Return a :obj:`ConnectDatabase` object.\n\t        Parameters\n\t        ----------\n", "        verbose: int,\n\t            level of verbosity.\n\t            - 0 nothing\n\t            - 1 metadata\n\t            - 2 sql command\n\t        connection: (str or dict),\n\t            connection parameters. If a string is\n\t            provided the file `~/.postgresql/connections.cfg` will be loaded\n\t            as connection parameters. If a dict is provided, it must contain\n\t            'host', 'port', 'dbname' and 'user' keys.\n", "        Returns\n\t        -------\n\t        :obj:`ConnectDatabase`\n\t        \"\"\"\n\t        if isinstance(connection, str):\n\t            connection = self.connections[connection]\n\t        self.engine = connect.connect_psql_server(\n\t            connection=connection, **kwargs.copy())\n\t        self.connection = self.engine.raw_connection()\n\t        self.active_execution_time = verbose >= 1\n", "        self.active_sql_printing = verbose >= 2\n\t        self.verbose = verbose\n\t        self.username = connection[\"user\"]\n\t        return self\n\t    def close(self) -> None:\n\t        \"\"\"\n\t        Close the connection\n\t        Returns\n\t        -------\n\t            None\n", "        \"\"\"\n\t        self.engine.dispose()\n\t        self.connection.detach()\n\t        self.connection.close()\n\t    # ===========================================================================\n\t    #                           READ AND WRITE\n\t    # ==========================================================================\n\t    @execution.execution_time\n\t    def read_table(\n\t            self, table_name: str,\n", "            columns: iter = None,\n\t            where: str = None,\n\t            **kwargs) -> pd.DataFrame:\n\t        \"\"\"\n\t        Read a table from server\n\t        Parameters\n\t        ---------\n\t        table_name: str\n\t            Name of the table. Naming convention \"schema.name\" if no schema\n\t            is specified the \"main\" schema will be used\n", "        columns: iter\n\t            Specific columns to request.\n\t            If None all columns are requested.\n\t            If errors are made or columns are missed, they will be ignored\n\t        where: str\n\t            Request specific rows in postgresql fashion\n\t        Returns\n\t        -------\n\t            Data as :obj:`pandas.DataFrame`\n\t        \"\"\"\n", "        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n\t        query = self._select(table_name, columns=columns, where=where, **kwargs)\n\t        copy_sql = f\"COPY ({query}) TO STDOUT WITH CSV HEADER\"\n\t        connection = self.engine.connect()\n\t        with connection.connection.cursor() as cursor:\n\t            storage = io.StringIO()\n\t            cursor.copy_expert(copy_sql, storage)\n\t            storage.seek(0)\n\t        data_frame = pd.read_csv(storage, true_values=[\"t\"], false_values=[\"f\"])\n\t        return data_frame\n", "    @execution.execution_time\n\t    def write_table(self, data: typing.Union[pd.DataFrame, \"GeoDataFrame\"],\n\t                    table_name: str = \"tmp\",\n\t                    primary_key=None,\n\t                    if_exists=\"replace_cascade\",\n\t                    foreign_keys=None,\n\t                    column_comments=None,\n\t                    **kwargs) -> None:\n\t        \"\"\"\n\t        Create table based on panda's DataFrame. This function upload the data\n", "        on server and insert all data in the table.\n\t        Parameters\n\t        ----------\n\t        data: pandas.DataFrame\n\t            Data (GeoDataFrame or DataFrame) to insert into the database.\n\t        table_name: str\n\t            Name of the table. Naming convention \"schema.name\" if no schema\n\t            is specified the \"main\" schema will be used\n\t        primary_key: list or str\n\t            List of string specifying all keys or single string\n", "        if_exists: str\n\t            In case of existing table\n\t            - \"replace_cascade\" : will delete all table with dependency\n\t            - \"replace\" : will delete this table. Error will raise if there are dependencies\n\t        foreign_keys: dict\n\t            specify foreign keys in the following fashion\n\t            {column name in the current table : foreign table name}\n\t            Note that if the name of the foreign key is different in the\n\t            foreign table, use the `add_key` method.\n\t        column_comments: dict\n", "            Columns comments of type `{column_name: column_comment}`\n\t        Returns\n\t        -------\n\t            None\n\t        >>> from acrocord import ConnectDatabase\n\t        >>> dataframe = pd.DataFrame({'a': [155, 20, 3],\n\t        >>>                           'b': [11, 299, 45],\n\t        >>>                           'c': [73, 3, 39],\n\t        >>>                           'd': [783, 488, 739],\n\t        >>>                           \"lat\": [45, 45, 45],\n", "        >>>                           \"lon\": [0, 1, 2]\n\t        >>>                       })\n\t        >>> df = ConnectDatabase().connect(\n\t        >>>      connection=\"connection-name\"\n\t        >>>      ).write_table( dataframe, \"test.test\")\n\t        \"\"\"\n\t        data = data.rename(columns=str.lower)\n\t        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n\t        warning_type(data, verbose=self.verbose)\n\t        if \"geometry\" in data.dtypes.values and self.has_postgis_extension:\n", "            if_exists_pg = \"replace\" if if_exists == \"replace_cascade\" else if_exists\n\t            # set postgis extension in database if not already set\n\t            data.to_postgis(\n\t                con=self.engine,\n\t                name=self._get_name(table_name),\n\t                schema=self._get_schema(table_name),\n\t                if_exists=if_exists_pg)\n\t        else:\n\t            if if_exists == \"replace_cascade\":\n\t                self.drop_table(table_name, option=\"cascade\")\n", "                self.create_table(table_name, data.columns,\n\t                                  data.dtypes.values.astype(str),\n\t                                  column_comments)\n\t            with execution.silence_stdout():\n\t                self.insert(data, table_name)\n\t        if primary_key is not None:\n\t            self.add_key(table_name, primary_key)\n\t        if foreign_keys is not None:\n\t            for key in foreign_keys.keys():\n\t                self.add_key(table_name, key, type_=\"foreign\",\n", "                             table_foreign=foreign_keys[key])\n\t    def insert(self, data: pd.DataFrame, table_name: str,\n\t               chunksize=1000) -> None:\n\t        \"\"\"\n\t        This function insert data in existing postgresql table\n\t        Parameters\n\t        ---------\n\t        data: pandas.DataFrame ,\n\t            Data to insert\n\t        table_name: str\n", "            Name of the table. Note that the column of `data` and the columns\n\t            of the table must be the same\n\t        chunksize: int\n\t            Size of chunk in insert function\n\t        Returns\n\t        -------\n\t            None\n\t        \"\"\"\n\t        insert.insert(self, data, table_name, chunksize)\n\t    def create_insert(self, data: pd.DataFrame, table_name: str,\n", "                      *args, **kwargs):\n\t        \"\"\"\n\t        This function creates table if it does not exist and insert in the\n\t        table otherwise.\n\t        Parameters\n\t        ----------\n\t        data: pandas.DataFrame,\n\t            Data to insert.\n\t        table_name: str\n\t            Name of the table. Naming convention \"schema.name\" if no schema\n", "            is specified the \"main\" schema will be used\n\t        \"\"\"\n\t        if self._get_name(table_name) in self.get_table_names(\n\t                self._get_schema(table_name)):\n\t            self.insert(data, table_name)\n\t        else:\n\t            self.write_table(data, table_name, *args, **kwargs)\n\t    def add_key(self, table_name: str, key: str, type_=\"primary\",\n\t                table_foreign=\"\",\n\t                key_foreign=\"\") -> None:\n", "        \"\"\"\n\t        Add key (reference between tables).\n\t        Parameters\n\t        ----------\n\t        table_name: str\n\t            The name of the table for which to add key\n\t        key: str\n\t            The name of the key (corresponds to a column in the table)\n\t        type_: str\n\t            The type of key. By default, the key type is a primary key. The\n", "            possible values are\n\t                - \"primary\" to define primary key. The primary key must be\n\t                 composed of unique values\n\t                - \"foreign\" to define foreign key. If type is set to foreign,\n\t                the argument `table_foreign` must be provided\n\t        table_foreign: str\n\t            The name of the foreign table. Argument is ignored if key type\n\t            is set to \"primary\".\n\t        key_foreign: str\n\t            The name of the foreign key (column) in the foreign table table\n", "        Returns\n\t        -------\n\t            None\n\t        \"\"\"\n\t        table_name = f\"{self._get_schema(table_name)}.\" \\\n\t                     f\"{self._get_name(table_name)}\"\n\t        try:\n\t            if key_foreign == \"\":\n\t                key_foreign = key\n\t            if table_foreign != \"\":\n", "                df_type = self.get_dtypes(table_foreign)\n\t                typ = df_type.loc[\n\t                    df_type[\"column_name\"].values == key_foreign,\n\t                    \"data_type\"].values[0]\n\t                sql_cmd = f\"ALTER TABLE {table_name} ALTER COLUMN \" \\\n\t                          f\"{key} TYPE {typ} USING {key}::{typ};\"\n\t                auxiliaries.execute_sql_cmd(self, sql_cmd, fetch=False)\n\t            key = _iterable_or_str(key)\n\t            sql_cmd = f\"ALTER TABLE {table_name} ADD {type_.upper()} KEY {key}\"\n\t            if type_ == \"foreign\":\n", "                sql_cmd += f\" REFERENCES {self._get_schema(table_foreign)}.\" \\\n\t                           f\"{self._get_name(table_foreign)} ({key_foreign});\"\n\t            auxiliaries.execute_sql_cmd(self, sql_cmd, fetch=False)\n\t        except exc.ProgrammingError as error:\n\t            print(error)\n\t    # ==========================================================================\n\t    #                          ALTER DATA\n\t    # ==========================================================================\n\t    def add_columns(self, table_name: str, data: pd.DataFrame,\n\t                    index: str) -> None:\n", "        \"\"\"\n\t        Add columns to `table_name` based on the data provided on the index\n\t        given in argument. The name of the index column given must be the same\n\t        in table (on postgres server) and data (the data frame object)\n\t        Parameters\n\t        ----------\n\t        table_name: str\n\t            The name of table\n\t        data: pandas.DataFrame\n\t            Data to add in the table.\n", "        index:\n\t            The name of the column in the table and the dataframe on which\n\t            to perform the merge\n\t        Returns\n\t        -------\n\t            None\n\t        \"\"\"\n\t        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n\t        table_tmp = self._get_schema(table_name) + \".tmp\"\n\t        self.write_table(data, table_tmp)\n", "        if isinstance(index, str):\n\t            index = (index,)\n\t        index = [id_.lower() for id_ in index]\n\t        auxiliaries.merge(self, table_name, table_tmp,\n\t                          out=table_name, on=index, out_type=\"table\")\n\t        self.drop_table(table_tmp)\n\t    def drop_column(self,\n\t                    table_name: str, columns: Iterable[str],\n\t                    option: str = \"cascade\", type_: str = \"TABLE\"):\n\t        \"\"\"\n", "        Remove some columns\n\t        Parameters\n\t        ----------\n\t        table_name: str\n\t            The name of the table to alter\n\t        columns: iter[str]\n\t            The columns to drop\n\t        option: str\n\t            Option can be either\n\t            - 'cascade' : all dependent objects will be removed\n", "            - '': only the columns\n\t            Default is 'cascade'\n\t        type_:str\n\t            The type of object can be either\n\t                - a table and type_=\"TABLE\"\n\t                - a view and type_=\"VIEW\"\n\t        \"\"\"\n\t        sql_cmd = f\"ALTER {type_} {table_name}\"\n\t        for col in columns:\n\t            sql_cmd += f\" DROP COLUMN {col} {option},\"\n", "        auxiliaries.execute_sql_cmd(self, sql_cmd[:-1], hide=True, fetch=False)\n\t    @auxiliaries.execute\n\t    def drop_table(self, table_name: str, option: str = \"cascade\",\n\t                   type_: str = \"TABLE\"):\n\t        \"\"\"\n\t        Delete a table\n\t        Parameters\n\t        ----------\n\t        table_name: str\n\t            The name of the table to drop (delete)\n", "        option: str\n\t            Option can be either\n\t                - 'cascade' : all depend objects will be removed\n\t                - '': only the columns\n\t            Default is 'cascade'\n\t        type_:str\n\t            The type of object can be either\n\t                - a table and type_=\"TABLE\"\n\t                - a view and type_=\"VIEW\"\n\t        \"\"\"\n", "        sql_cmd = auxiliaries.drop_table(table_name, option, type_)\n\t        return sql_cmd\n\t    @auxiliaries.execute\n\t    def rename(self, table_name: str, new_table_name: str,\n\t               type_: str = \"TABLE\"):\n\t        \"\"\"\n\t        Rename a table\n\t        Parameters\n\t        ----------\n\t        table_name: str\n", "            The current name of the table\n\t        new_table_name: str\n\t            The new name of the table\n\t        type_:str\n\t            The type of table, either\n\t                - a table and type_=\"TABLE\"\n\t                - a view and type_=\"VIEW\"\n\t            Default is \"TABLE\"\n\t        Returns\n\t        -------\n", "            SQL query\n\t        \"\"\"\n\t        sql_cmd = f\"ALTER {type_} {table_name} \" \\\n\t                  f\"RENAME TO {self._get_name(new_table_name)}\"\n\t        return sql_cmd\n\t    @auxiliaries.execute\n\t    def copy(self, table_name: str, new_table_name: str, type_=\"TABLE\") -> str:\n\t        \"\"\"\n\t        Copy the table and create another table (or view)\n\t        Parameters\n", "        ----------\n\t        table_name: str\n\t            The name of the table to copy\n\t        new_table_name: str\n\t            The name of the target table\n\t        type_: str\n\t            The target type object, it can be either\n\t            - \"view\" does not create new data\n\t            - \"table\" creates new data\n\t        Returns\n", "        -------\n\t            SQL query\n\t        \"\"\"\n\t        table_name = f\"{self._get_schema(table_name)}.\" \\\n\t                     f\"{self._get_name(table_name)}\"\n\t        new_table_name = f\"{self._get_schema(new_table_name)}.\" \\\n\t                         f\"{self._get_name(new_table_name)}\"\n\t        self.drop_table(new_table_name, type_=type_)\n\t        sql_cmd = f\"CREATE {type_} {new_table_name} AS {type_} {table_name} ;\"\n\t        return sql_cmd\n", "    # ==========================================================================\n\t    #                           FETCH INFORMATION\n\t    # ==========================================================================\n\t    def get_dtypes(\n\t            self,\n\t            table_name: str,\n\t            format: str = \"dataframe\") -> typing.Union[dict, pd.DataFrame]:\n\t        \"\"\"\n\t        Get the column type of the table\n\t        Parameters\n", "        ----------\n\t        table_name: str\n\t            The name of the table to get type from\n\t        format: str\n\t            Format of the output\n\t        Returns\n\t        -------\n\t           Dict or pandas.DataFrame\n\t        \"\"\"\n\t        sql_cmd = f\"select udt_name, column_name from \" \\\n", "                  f\"information_schema.columns where table_name=\" \\\n\t                  f\"'{self._get_name(table_name)}' AND table_schema=\" \\\n\t                  f\"'{self._get_schema(table_name)}'\"\n\t        ret = auxiliaries.execute_sql_cmd(self, sql_cmd, hide=True, fetch=True)\n\t        df_type = pd.DataFrame(\n\t            ret,\n\t            columns=[\"data_type\", \"column_name\"])\n\t        return df_type\n\t    def get_columns(self, table_name: str) -> list:\n\t        \"\"\"\n", "        Get the list of columns of the given table name\n\t        Parameters\n\t        ----------\n\t        table_name: str\n\t            The name of the table\n\t        Returns\n\t        -------\n\t            list of column names\n\t        \"\"\"\n\t        dtypes = self.get_dtypes(table_name)\n", "        return list(dtypes[\"column_name\"])\n\t    def get_metadata(self, table_name: str) -> pd.DataFrame:\n\t        \"\"\"\n\t        Get metadata of a given table\n\t        Parameters\n\t        ----------\n\t        table_name: str\n\t            The name of the table\n\t        Returns\n\t        -------\n", "            pandas.DataFrame with metadata\n\t        \"\"\"\n\t        sql_cmd = auxiliaries.get_metadata(table_name)\n\t        res = auxiliaries.execute_sql_cmd(self, sql_cmd, hide=True, fetch=True)\n\t        df_meta = pd.DataFrame(res)\n\t        df_type = self.get_dtypes(table_name)\n\t        return pd.merge(df_meta, df_type, on='column_name')\n\t    def get_shape(self, table_name: str) -> typing.Tuple[int, int]:\n\t        \"\"\"\n\t        Get shape of a table\n", "        Parameters\n\t        ----------\n\t        table_name:str\n\t            The name of the table\n\t        Returns\n\t        -------\n\t            tuple of row, column shape\n\t        \"\"\"\n\t        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n\t        row = auxiliaries.execute_sql_cmd(\n", "            self,\n\t            auxiliaries.count(table_name),\n\t            fetch=True,\n\t            hide=True\n\t        )[0][0]\n\t        col = len(self.get_columns(table_name))\n\t        return row, col\n\t    def get_schema_names(self) -> list:\n\t        \"\"\"\n\t        Get the names of schemas for a given connection.\n", "        The connection is associated with a database.\n\t        Returns\n\t        -------\n\t            List of schema names\n\t        \"\"\"\n\t        with self.engine.connect() as cursor:\n\t            res = cursor.execute(text('SELECT * FROM pg_catalog.pg_tables'))\n\t            data_frame = pd.DataFrame(res.fetchall())\n\t        list_ = list(set(data_frame.iloc[:, 0]))\n\t        list_ = [schema for schema in list_ if\n", "                 \"pg\" not in schema and \"schema\" not in schema]\n\t        return list_\n\t    def get_table_names(self, schema: str = \"public\") -> list:\n\t        \"\"\"\n\t        Get a list of table for a given schema\n\t        Parameters\n\t        ----------\n\t        schema: str\n\t            The name of the schema\n\t        Returns\n", "        -------\n\t            List of table names\n\t        \"\"\"\n\t        res = auxiliaries.execute_sql_cmd(\n\t            self,\n\t            f\"SELECT * FROM pg_catalog.pg_tables where schemaname='{schema}'\",\n\t            hide=True, fetch=True)\n\t        df_names = pd.DataFrame(res)\n\t        if len(df_names) == 0:\n\t            return []\n", "        return list(df_names.iloc[:, 1])\n\t    def get_view_names(self, schema=\"public\"):\n\t        \"\"\"\n\t        Get a list of views for a given schema\n\t        Parameters\n\t        ----------\n\t        schema: str\n\t            The name of the schema\n\t        Returns\n\t        -------\n", "            List of view names\n\t        \"\"\"\n\t        res = auxiliaries.execute_sql_cmd(\n\t            self, 'SELECT * FROM pg_catalog.pg_views',\n\t            hide=True, fetch=True)\n\t        df_names = pd.DataFrame(res)\n\t        df_names = df_names[df_names.iloc[:, 0] == schema]\n\t        return list(df_names.iloc[:, 1])\n\t    def _select(self, table_name: str, columns: Iterable[str] = None,\n\t                where: str = None,\n", "                limit: int = None):\n\t        \"\"\"\n\t        Select type query\n\t        Parameters\n\t        ----------\n\t        table_name: str\n\t            The name of the table on which to apply a select\n\t        columns: iterable of str\n\t            The name of the selected columns\n\t        where: str\n", "            Selection conditions according to the indexes\n\t        limit: int\n\t            The maximal number of row to select\n\t        Returns\n\t        -------\n\t            SQL query\n\t        \"\"\"\n\t        table_name = f\"{self._get_schema(table_name)}.{self._get_name(table_name)}\"\n\t        if columns is not None:\n\t            columns = [col.lower() for col in columns if\n", "                       col in self.get_columns(table_name)]\n\t            columns = pd.Series(columns).drop_duplicates(keep='first')\n\t            query = \",\\n  \".join(columns)\n\t            query = \"SELECT \" + query + \" FROM \" + table_name\n\t        else:\n\t            query = \"SELECT * FROM \" + table_name\n\t        if where is not None:\n\t            query += \" WHERE \" + where\n\t        if limit is not None:\n\t            query += \" LIMIT \" + str(limit)\n", "        if self.active_sql_printing:\n\t            execution.print_(query)\n\t        return query\n\t    def update(self, table_name: str, where: str = \"\",\n\t               column: str = \"\", value: str = \"NULL\") -> None:\n\t        \"\"\"\n\t        Update values in table\n\t        Parameters\n\t        ----------\n\t        table_name: str\n", "            The name of the table\n\t        where: str\n\t            The location according to the indexes of the values to be modified\n\t        column: str\n\t            The column concerned\n\t        value\n\t            The value\n\t        Returns\n\t        -------\n\t            None\n", "        \"\"\"\n\t        if isinstance(value, str):\n\t            value = f\"'{value}'\"\n\t        sql_cmd = f\"UPDATE {table_name} SET {column}={value}\"\n\t        if where != \"\":\n\t            sql_cmd += f\" where {where}\"\n\t        auxiliaries.execute_sql_cmd(self, sql_cmd, fetch=False)\n\t    @staticmethod\n\t    def _get_schema(name):\n\t        return name.split(\".\")[0].lower() if \".\" in name else \"main\"\n", "    @staticmethod\n\t    def _get_name(name):\n\t        return name.split(\".\")[1].lower() if \".\" in name else name.lower()\n\t    @auxiliaries.execute\n\t    def create_table(self, name: str, columns: Iterable[str],\n\t                     dtypes: Iterable[str],\n\t                     column_comments: Dict[str, str] = None) -> str:\n\t        \"\"\"\n\t        Parameters\n\t        ----------\n", "        name: str\n\t            The name of the table\n\t        columns: iterable of str\n\t            The names of the columns\n\t        dtypes: iterable of str\n\t            The types of columns\n\t        column_comments : dict\n\t            The comment on columns `{column1: comment 1, ...}`\n\t        Returns\n\t        -------\n", "            SQL query\n\t        \"\"\"\n\t        return auxiliaries.create_table(name, columns, dtypes, column_comments)\n\t    @auxiliaries.execute\n\t    def drop(self, name: str, type_: str, column_name: str) -> str:\n\t        \"\"\"\n\t        Drop a column or object in table\n\t        Parameters\n\t        ----------\n\t        name: str\n", "            Object to alter\n\t        type_: str\n\t            Type of object to alter\n\t            - VIEW\n\t            - TABLE (default)\n\t        column_name: str\n\t            Column or object to drop\n\t        Returns\n\t        -------\n\t            SQL query\n", "        \"\"\"\n\t        return auxiliaries.drop(name, type_, column_name)\n\t    @auxiliaries.execute\n\t    def create_schema(self, name: str) -> str:\n\t        \"\"\"\n\t        Create a schema\n\t        Parameters\n\t        ----------\n\t        name: str\n\t            Name of the schema\n", "        Returns\n\t        -------\n\t            None\n\t        \"\"\"\n\t        return auxiliaries.create_schema(name)\n\t    def list_table(self, schema: str) -> pd.DataFrame:\n\t        \"\"\"\n\t        List tables provided a postregsql schema\n\t        Parameters\n\t        ----------\n", "        schema: str\n\t            Schema name\n\t        Returns\n\t        -------\n\t            :obj: `pandas.DataFrame` with table list and properties\n\t        \"\"\"\n\t        return pd.DataFrame(auxiliaries.execute_sql_cmd(\n\t            self, auxiliaries.list_table(schema),\n\t            fetch=True))\n\t    def execute(self, sql: str) -> None:\n", "        \"\"\"\n\t        Provided a query as string,\n\t        execute the query using SQLalchemy engine with psycopg2 driver\n\t        Parameters\n\t        ----------\n\t        sql: str\n\t            Some query to execute\n\t        Returns\n\t        -------\n\t            None\n", "        \"\"\"\n\t        auxiliaries.execute_sql_cmd(self, sql, hide=False)\n\t    # ==========================================================================\n\t    #                       Property of connexion\n\t    # ==========================================================================\n\t    @property\n\t    def has_postgis_extension(self) -> bool:\n\t        with self.engine.connect() as con:\n\t            out_ = con.execute(\n\t                text(\"SELECT COUNT(1) FROM information_schema.routines\"\n", "                     \" WHERE routine_name = 'postgis_version'\"))\n\t            has_postgis = next(out_)[0]\n\t        return has_postgis\n\tdef _iterable_or_str(arg) -> str:\n\t    if isinstance(arg, str):\n\t        arg = f\"({arg})\"\n\t    else:\n\t        arg = str(tuple(arg)).replace(\"'\", \"\")\n\t    return arg\n"]}
{"filename": "acrocord/test/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n"]}
{"filename": "acrocord/test/constraints.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\tfrom typing import Type, Iterable\n\timport pandas as pd\n\tfrom acrocord import ConnectDatabase\n\tfrom acrocord.utils.types import EligibleDataType\n\tdef not_nullable(data: pd.DataFrame, columns: Iterable[str]):\n\t    \"\"\" Function that verify if a column does not contains null values \"\"\"\n\t    for column in columns:\n", "        assert not data[column].isnull().values.any(), f\"Column\" \\\n\t            f\" {column} \" \\\n\t            f\"contains null values\"\n\tdef data_type(data: pd.DataFrame, columns: Iterable[str],\n\t              dtype: Type):\n\t    \"\"\" Function that verify if the type of column is correct \"\"\"\n\t    for column in columns:\n\t        assert data[column].dtype == dtype, \\\n\t            f\"Column {column} is {data[column].dtype}\"\n\tdef eligible_data_type(data: pd.DataFrame):\n", "    result = data.dtypes\n\t    for column, type_ in result.items():\n\t        msg = f'The type of the column {column} is {type_} ' \\\n\t              f'which is not an eligible type.'\n\t        assert type_ in EligibleDataType.get_list(), msg\n\tdef quantile(data: pd.DataFrame, columns: Iterable[str], q, threshold):\n\t    \"\"\" Function that verifies the quantile according to a threshold  \"\"\"\n\t    for column in columns:\n\t        assert data[column].quantile(q) < threshold\n\tdef unique(data: pd.DataFrame, columns: Iterable[str]):\n", "    \"\"\" Function that checks if the values of a column is unique \"\"\"\n\t    for column in columns:\n\t        assert data[\n\t            column].nunique() == data.__len__(), f\"Column {column} is not unique\"\n\tdef nb_unique_index(data: pd.DataFrame, columns: Iterable[str], minimum,\n\t                    maximum):\n\t    \"\"\" Function that checks if the number of unique objects is included in a given interval \"\"\"\n\t    for column in columns:\n\t        msg = f\"Column {column} est compris dans l'intervalle \"\n\t        assert (data[column].nunique() < maximum) and (\n", "            data[column].nunique() >= minimum), msg\n\tclass DataConstraints:\n\t    def __init__(self, data: pd.DataFrame = None,\n\t                 connection: ConnectDatabase = None):\n\t        self.data = data.copy()\n\t        self.constraints = {}\n\t    def add_constraint(self, constraint: dict):\n\t        self.constraints = {**self.constraints, **constraint}\n\t    def test(self):\n\t        for name in self.constraints.keys():\n", "            function, args = self.constraints[name]\n\t            args[\"data\"] = self.data\n\t            function(**args)\n"]}
{"filename": "acrocord/utils/auxiliaries.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\tfrom functools import wraps\n\tfrom typing import Iterable, Dict\n\tfrom sqlalchemy import text\n\tfrom acrocord.misc import execution\n\tdef execute(cmd, hide=False, fetch=False):\n\t    @wraps(cmd)\n\t    def wrapper(*args, **kwargs):\n", "        cmd_sql = cmd(*args, **kwargs)\n\t        execute_sql_cmd(args[0], cmd_sql, hide=hide, fetch=fetch)\n\t    return wrapper\n\tdef execute_sql_cmd(connection, cmd: str, hide=False, fetch=False):\n\t    if not hide:\n\t        execution.print_(cmd)\n\t    with connection.engine.connect() as cursor:\n\t        res = cursor.execute(text(cmd))\n\t        cursor.commit()\n\t        if fetch:\n", "            ret = res.fetchall()\n\t            return ret\n\tdef merge(connection, table1, table2, out=\"tmp_merge\", out_type=\"VIEW\",\n\t          suffixe=(\"_x\", \"_y\"), on=\"\", left_on=\"\", right_on=\"\"):\n\t    out = f\"{connection._get_schema(out)}.{connection._get_name(out)}\"\n\t    out_save = out\n\t    if out in [table1, table2]:\n\t        out = f\"{connection._get_schema(out)}.tmp_merge\"\n\t    if left_on == \"\":\n\t        left_on = on\n", "        right_on = on\n\t    if isinstance(left_on, str):\n\t        right_on, left_on = [right_on], [left_on]\n\t    left_on, right_on = [l.lower() for l in left_on], [r.lower() for r in\n\t                                                       right_on]\n\t    all_columns = connection.get_columns(table1) + connection.get_columns(\n\t        table2)\n\t    col_intersect = set(connection.get_columns(table1)\n\t                        ).intersection(connection.get_columns(table2))\n\t    if suffixe != (\"_x\", \"_y\"):\n", "        col_intersect = all_columns\n\t    db_str = f\"CREATE {out_type} {out} AS (\\n  SELECT \"\n\t    for col in connection.get_columns(table1):\n\t        db_str += table1 + \".\" + col\n\t        if col in col_intersect and col not in left_on:\n\t            db_str += \" AS \" + col + suffixe[0]\n\t        db_str += \",\"\n\t    for col in connection.get_columns(table2):\n\t        if col not in left_on:\n\t            db_str += table2 + \".\" + col\n", "            if col in col_intersect:\n\t                db_str += \" AS \" + col + suffixe[1]\n\t            db_str += \",\"\n\t    db_str = db_str[:-1]\n\t    db_str += f\" FROM {table1},{table2}\"\n\t    left_on_ = [f\"{table1}.{l}\" for l in left_on]\n\t    right_on_ = [f\"{table1}.{r}\" for r in right_on]\n\t    left_on_ = \", \".join(left_on_)\n\t    right_on_ = \", \".join(right_on_)\n\t    db_str += \"\\n  WHERE (\" + left_on_ + \")=(\" + right_on_ + \"));\"\n", "    if connection.active_sql_printing:\n\t        print(db_str)\n\t    connection.drop_table(out, type_=out_type)\n\t    execute_sql_cmd(connection, db_str, hide=True)\n\t    if out_save in [table1, table2]:\n\t        connection.drop_table(out_save)\n\t        connection.replace(out, out_save, type_=out_type)\n\tdef count(table_name):\n\t    sql_cmd = f\"SELECT COUNT(*) FROM {table_name};\"\n\t    return sql_cmd\n", "def drop_table(table_name, option=\"CASCADE\", type_=\"TABLE\"):\n\t    return f\"DROP {type_} IF EXISTS {table_name} {option};\"\n\tdef list_table(schema: str):\n\t    \"\"\"\n\t    Parameters\n\t    ----------\n\t    schema: str\n\t        Schema name\n\t    Returns\n\t    -------\n", "        SQL Query\n\t    \"\"\"\n\t    sql_cmd = f\"SELECT * FROM information_schema.tables \" \\\n\t              f\"WHERE table_schema = '{schema}'\"\n\t    return sql_cmd\n\tdef add(name, type_, column_name, dtype) -> str:\n\t    return f\"ALTER {type_} {name} ADD {column_name} {dtype}\"\n\tdef drop(name: str, type_: str, column_name: str):\n\t    \"\"\"\n\t    Drop a column or object in table\n", "    Parameters\n\t    ----------\n\t    name: str\n\t        Object to alter\n\t    type_: str\n\t        Type of object to alter\n\t        - VIEW\n\t        - TABLE (default)\n\t    column_name: str\n\t        Column or object to drop\n", "    Returns\n\t    -------\n\t        SQL Query\n\t    \"\"\"\n\t    return f\"ALTER {type_} {name} DROP {column_name} \"\n\tdef create_schema(name: str) -> str:\n\t    \"\"\"\n\t    Parameters\n\t    ----------\n\t    name: str\n", "        Name of the schema to create\n\t    Returns\n\t    -------\n\t        SQL Query\n\t    \"\"\"\n\t    return f\"CREATE SCHEMA IF NOT EXISTS {name}\"\n\tdef create_table(name: str, columns: Iterable[str], dtypes: Iterable[str],\n\t                 column_comments: Dict[str, str] = None) -> str:\n\t    \"\"\"\n\t    Parameters\n", "    ----------\n\t    name: str\n\t        The name of the table\n\t    columns: iterable of str\n\t        The names of the columns\n\t    dtypes: iterable of str\n\t        The types of columns\n\t    column_comments : dict\n\t        The comment on columns `{column1: comment 1, ...}`\n\t    Returns\n", "    -------\n\t        SQL Query\n\t    \"\"\"\n\t    col_zip = [f'{\" \" * 4}\"{c}\"{\" \" * max(1, (15 - len(c)))}' + db_data_type(\n\t        dtypes[i])\n\t        for i, c in enumerate(columns)]\n\t    sql_cmd = f\"CREATE TABLE {name}( \\n\"\n\t    sql_cmd += \",\\n\".join(col_zip) + \"\\n)\"\n\t    sql_cmd += '\\n;'\n\t    if column_comments is not None:\n", "        for col_name, col_comment in column_comments.items():\n\t            col_comment = col_comment.replace(\"'\", \"''\")\n\t            sql_cmd += f\"COMMENT ON COLUMN {name}\" \\\n\t                       f\".{col_name} IS '{col_comment}';\\n\"\n\t    return sql_cmd\n\t_conversion_table = {\"uint8\": \"int2\",\n\t                     \"uint16\": \"int4\",\n\t                     \"uint32\": \"int8\",\n\t                     \"int8\": \"int2\",\n\t                     \"Int8\": \"int2\",\n", "                     \"int16\": \"int2\",\n\t                     \"Int16\": \"int2\",\n\t                     \"float16\": \"float4\",\n\t                     \"float32\": \"float8\",\n\t                     \"int32\": \"int4\",\n\t                     \"Int32\": \"int4\",\n\t                     \"int64\": \"int8\",\n\t                     \"Int64\": \"int8\",\n\t                     \"float64\": \"float8\",\n\t                     \"Float64\": \"float8\",\n", "                     \"float\": \"float8\",\n\t                     \"bytes_\": \"text\",\n\t                     \"object\": \"text\",\n\t                     \"nan\": \"text\",\n\t                     \"str\": \"text\",\n\t                     \"string\": \"text\",\n\t                     \"bool\": \"bool\",\n\t                     \"boolean\": \"bool\",\n\t                     \"datetime64\": \"timestamp\",\n\t                     \"category\": 'int4'}\n", "_conversion_table_inv = {\n\t    'int2': 'int16', 'float4': 'float16', 'int4': 'int32', 'int8': 'int64',\n\t    'float8': 'float64', 'text': 'str', 'bool': 'bool',\n\t    'datetime64': 'timestamp'}\n\tdef db_data_type(data_type: str, invert: bool = False) -> str:\n\t    \"\"\"\n\t    Parameters\n\t    ----------\n\t    data_type: str\n\t        data type to translate into postgresql one\n", "    invert: bool\n\t        reciprocal conversion\n\t    Returns\n\t    -------\n\t    \"\"\"\n\t    if \"datetime\" in data_type:\n\t        return \"timestamp\"\n\t    if \"timestamp\" in data_type:\n\t        return \"datetime64[s]\"\n\t    data_type = str(data_type)\n", "    if invert:\n\t        return _conversion_table_inv[data_type]\n\t    return _conversion_table[data_type]\n\tdef get_metadata(table_name: str) -> str:\n\t    \"\"\"\n\t    Parameters\n\t    ----------\n\t    table_name: str\n\t        Name of the table from which the metadata should be retrieved\n\t    Returns\n", "    -------\n\t        SQL Query\n\t    \"\"\"\n\t    schema, table = table_name.split('.')\n\t    sql_cmd = f\"\"\"\n\t        select\n\t            table_cols.table_schema,\n\t            table_cols.table_name,\n\t            table_cols.column_name,\n\t            pgd.description\n", "        from pg_catalog.pg_statio_all_tables as st\n\t        full outer join pg_catalog.pg_description pgd on (\n\t            pgd.objoid = st.relid\n\t        )\n\t        full outer join information_schema.columns table_cols on (\n\t            pgd.objsubid   = table_cols .ordinal_position and\n\t            table_cols.table_schema = st.schemaname and\n\t            table_cols.table_name   = st.relname\n\t        )\n\t        where\n", "        table_cols.table_schema = '{schema}' and\n\t        table_cols.table_name = '{table}' \"\"\"\n\t    return sql_cmd\n"]}
{"filename": "acrocord/utils/types.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\t\"\"\"\n\tThis module aims  to specify data types that are consistent with\n\tthe usage of :object:`pypgsql`\n\t\"\"\"\n\timport warnings\n\tfrom typing import List\n\timport numpy as np\n", "import pandas as pd\n\tclass EligibleDataType:\n\t    # nullable integers\n\t    INTEGER_64 = pd.Int64Dtype()  # [-9_223_372_036_854_775_808   to      9_223_372_036_854_775_807]\n\t    INTEGER_32 = pd.Int32Dtype()  # [-2_147_483_648               to      2_147_483_647]\n\t    INTEGER_16 = pd.Int16Dtype()  # [-32_768                      to      32_767]\n\t    INTEGER_8 = pd.Int8Dtype()  # [-128                         to      127]\n\t    INTEGER = INTEGER_64\n\t    # float\n\t    FLOAT_16 = np.float16\n", "    FLOAT_32 = np.float32\n\t    FLOAT_64 = np.float64\n\t    FLOAT = FLOAT_64\n\t    # nullable boolean\n\t    BOOLEAN = pd.BooleanDtype()\n\t    STRING = pd.StringDtype()\n\t    # time\n\t    DATE_TIME = np.dtype('datetime64[ns]')\n\t    @classmethod\n\t    def get_list(cls) -> List:\n", "        \"\"\"\n\t        Returns\n\t        -------\n\t            list of eligible data types\n\t        \"\"\"\n\t        return [\n\t            cls.BOOLEAN,\n\t            cls.INTEGER,\n\t            cls.INTEGER_8,\n\t            cls.INTEGER_16,\n", "            cls.INTEGER_32,\n\t            cls.INTEGER_64,\n\t            cls.FLOAT_16,\n\t            cls.FLOAT_32,\n\t            cls.FLOAT_64,\n\t            cls.FLOAT,\n\t            cls.BOOLEAN,\n\t            cls.STRING,\n\t            cls.DATE_TIME\n\t        ]\n", "def warning_type(dataframe: pd.DataFrame, verbose: int):\n\t    for column, column_type in dataframe.dtypes.items():\n\t        if verbose == 2:\n\t            if str(column_type) not in EligibleDataType.get_list():\n\t                return warnings.warn(\n\t                    \"The types of this dataframe are not eligible types\")\n\t        if verbose > 2:\n\t            if str(column_type) not in EligibleDataType.get_list():\n\t                msg = f\"The type of the column {column} \" \\\n\t                      f\"is {column_type} which is not an eligible type.\"\n", "                warnings.warn(msg)\n"]}
{"filename": "acrocord/utils/logger.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\tfrom datetime import datetime\n\tfrom typing import Union\n\tfrom typing import Optional\n\tfrom acrocord.misc.execution import execution_time\n\tfrom acrocord.misc.execution import print_\n\tfrom acrocord.databasepg import ConnectDatabase\n\timport warnings\n", "import pandas as pd\n\twarnings.filterwarnings('ignore')\n\t__DEFAULT_COL_NAME__ = ['date', 'value', 'message', 'other_info']\n\t__DEFAULT_COL_TYPE__ = ['datetime', 'str', 'str', 'str']\n\tclass Logger:\n\t    def __init__(self, connection: ConnectDatabase, table_name: str, schema: str = 'public', columns=None,\n\t                 column_types=None):\n\t        \"\"\"\n\t        Initialize class logger to write log messages\n\t        Create a new table to write logs if it doesn't already exist\n", "        Parameters\n\t        ----------\n\t        connection: ConnectDatabase's instance\n\t            The connection instance, allowing to connect to a specific database\n\t        table_name: str\n\t            The table name in the database\n\t        schema: str\n\t            The schema name in the database, default is 'public'\n\t        columns: Iterable of str\n\t            columns names\n", "        column_types: Iterable if str\n\t            The columns types\n\t        \"\"\"\n\t        if column_types is None:\n\t            column_types = __DEFAULT_COL_TYPE__\n\t        if columns is None:\n\t            columns = __DEFAULT_COL_NAME__\n\t        self.db_connection = connection\n\t        self.table_name = f\"{schema}.{table_name}\"\n\t        print_(f\"{self.table_name = }\", color='OKBLUE')\n", "        if table_name in self.db_connection.get_table_names(schema):\n\t            print_(f\"\\nTable {table_name} ALREADY EXIST, do not create a new one\", color='OKBLUE')\n\t            columns = self.db_connection.get_columns(self.table_name)\n\t            column_types = self.db_connection.get_dtypes(self.table_name)\n\t        else:\n\t            print_(f\"CREATE new table with {table_name = }\", color='OKGREEN')\n\t            self.db_connection.create_table(name=self.table_name, columns=columns, dtypes=column_types)\n\t        self.columns = columns\n\t        self.column_types = column_types\n\t    @execution_time\n", "    def write_log(self, data: Union[pd.Series, pd.DataFrame, dict]) -> Optional[ValueError]:\n\t        \"\"\"\n\t        Main function to write log messages\n\t        Parameters\n\t        ----------\n\t        data: pd.Series or pd.Dataframe or dictionary\n\t            The log to be written to the table, specifically:\n\t                - pd.Series: all values are written into 'value' column\n\t                - pd.DataFrame: only data belonging to predefined columns are written\n\t                - dict: only one log (one value for each key) is written\n", "        Returns\n\t        -------\n\t            None\n\t        \"\"\"\n\t        if isinstance(data, pd.Series):\n\t            return self._write_log(self._series2df(data))\n\t        elif isinstance(data, dict):\n\t            return self._write_log(self._dict2df(data))\n\t        elif isinstance(data, pd.DataFrame):\n\t            return self._write_log(data.copy())\n", "        else:\n\t            print_(f\"\\nError of data type = {type(data)}\", color='WARNING')\n\t            raise ValueError(\"Input data should be type of [pd.Series, pd.DataFrame, or dict]\")\n\t    @staticmethod\n\t    def _series2df(data: pd.Series) -> pd.DataFrame:\n\t        \"\"\"\n\t        Convert pd.Series into pd.Dataframe\n\t        Parameters\n\t        ----------\n\t        data: pd.Series\n", "            The value to be converted\n\t        Returns\n\t        -------\n\t            pd.DataFrame\n\t        \"\"\"\n\t        return data.to_frame(name='value')\n\t    @staticmethod\n\t    def _dict2df(data: dict) -> pd.DataFrame:\n\t        \"\"\"\n\t        Convert a dictionary into pd.Dataframe\n", "        Parameters\n\t        ----------\n\t        data: dict\n\t            The data to be converted, only one value for each key\n\t        Returns\n\t        -------\n\t            pd.DataFrame\n\t        \"\"\"\n\t        return pd.DataFrame(data, index=[0])\n\t    def _write_log(self, log_message: pd.DataFrame) -> None:\n", "        \"\"\"\n\t        Parameters\n\t        ----------\n\t        log_message: pd.DataFrame\n\t            The log message to be written to the predefined table\n\t        Returns\n\t        -------\n\t            None\n\t        \"\"\"\n\t        input_cols = list(log_message.columns)\n", "        cols_write = list(set(self.columns) & set(input_cols))\n\t        cols_null = list(set(self.columns) - set(cols_write))\n\t        print_(f\"\\n{cols_write = }\\n{cols_null = }\", color='BOLD')\n\t        df_write = pd.DataFrame()\n\t        df_write[cols_write] = log_message[cols_write]\n\t        df_write[cols_null] = ''\n\t        if 'date' in cols_write:\n\t            df_write['date'] = pd.to_datetime(df_write['date']).strftime(\"%Y-%m-%d %H:%M:%S\")\n\t        else:\n\t            df_write['date'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n", "        df_write.reset_index(drop=True, inplace=True)\n\t        self.db_connection.insert(data=df_write, table_name=self.table_name)\n\t    def print_info(self):\n\t        print_(f\"\\ntable_name: {self.table_name}\"\n\t               f\"\\ncolumns: {self.columns}\"\n\t               f\"\\ncolumns types: \\n{self.column_types}\", color='Magenta')\n"]}
{"filename": "acrocord/utils/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n"]}
{"filename": "acrocord/utils/monitoring.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\tdef print_date(connection, schema: str, table: str):\n\t    from acrocord.misc import execution\n\t    with execution.silence_stdout():\n\t        data = connection.read_table(\"public.informations_table\").query(\n\t            f\"nom=='{table}'\").query(f\"schema=='{schema}'\")\n\t    if len(data) > 0:\n\t        try:\n", "            msg = \"[info] creation date :\"\n\t            msg += \"-\" * (40 - len(msg)) + \"  \"\n\t            msg += data[\"date_creation\"].iloc[0][:-10]\n\t            execution.print_(msg, \"Grey\")\n\t            msg = \"[info] author name :\"\n\t            msg += \"-\" * (40 - len(msg)) + \"  \"\n\t            msg += data[\"auteur_creation\"].iloc[0]\n\t            execution.print_(msg, \"Grey\")\n\t        except TypeError as error:\n\t            execution.print_(error, color=\"Red\")\n"]}
{"filename": "acrocord/utils/connect.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\timport getpass\n\timport glob\n\timport os\n\timport subprocess\n\timport sys\n\tfrom os.path import expanduser, abspath\n\timport asyncpg\n", "import pandas as pd\n\tfrom sqlalchemy import create_engine\n\thome = expanduser(\"~\")\n\tpath_postgresql = f\"{home}/.postgresql/\"\n\tpath_acr = f\"{home}/.acrocord/\"\n\tdef load_available_connections():\n\t    import configparser\n\t    default = configparser.ConfigParser()\n\t    connection_dict = {}\n\t    for path in [path_acr, path_postgresql]:\n", "        abs_path = abspath(path)\n\t        dir_path = f\"{abs_path}/connections.cfg\"\n\t        default.read(dir_path)\n\t        connection_dict = {**connection_dict, **default._sections}\n\t        try:\n\t            _ = pd.DataFrame(connection_dict).drop(\"password\").T\n\t        except KeyError:\n\t            pass\n\t    for connection in connection_dict.keys():\n\t        try:\n", "            if connection_dict[connection][\"password\"] == '':\n\t                connection_dict[connection][\"password\"] = ' '\n\t        except KeyError:\n\t            pass\n\t    return connection_dict\n\tdef password_auth():\n\t    path = os.path.abspath(__file__).replace(os.path.basename(__file__), \"\")\n\t    cmd = \"python \" + path + \"getpassword.py \" + path_postgresql\n\t    if not os.path.exists(path_postgresql):\n\t        os.mkdir(path_postgresql)\n", "    if sys.platform == \"win32\":\n\t        with subprocess.Popen(cmd, shell=True,\n\t                              stderr=subprocess.PIPE,\n\t                              stdout=subprocess.PIPE):\n\t            pass\n\t    elif sys.platform == 'linux':\n\t        subprocess.call(['xterm', \"-e\", cmd])\n\t    list_of_files = glob.glob(path_postgresql + '*')\n\t    latest_file = max(list_of_files, key=os.path.getctime)\n\t    with open(latest_file) as file:\n", "        password = file.read()\n\t        file.close()\n\t    os.remove(latest_file)\n\t    return password\n\tdef connect_psql_server(username=getpass.getuser(), async_=False, **kwargs):\n\t    connection_param = kwargs[\"connection\"].copy()\n\t    if \"password\" not in connection_param.keys():\n\t        connection_param[\"password\"] = \"\"\n\t    if username != getpass.getuser():\n\t        connection_param[\"user\"] = username\n", "    if connection_param[\n\t            \"password\"] == \"\" and \"sslmode\" not in connection_param.keys():\n\t        connection_param[\"password\"] = password_auth()\n\t    if \"sslmode\" in connection_param.keys():\n\t        connect_args = {'sslmode': connection_param[\"sslmode\"]}\n\t    else:\n\t        connect_args = {}\n\t    sql_cmd = 'postgresql+psycopg2://'\n\t    sql_cmd += connection_param[\"user\"] + ':' + connection_param[\n\t        \"password\"] + '@' + connection_param[\"host\"] + ':'\n", "    sql_cmd += str(connection_param[\"port\"]) + '/' + connection_param[\"dbname\"]\n\t    if async_:\n\t        return connect_async(sql_cmd)\n\t    return create_engine(sql_cmd, connect_args=connect_args)\n\tasync def connect_async(sql_cmd):\n\t    conn = await asyncpg.connect(\n\t        sql_cmd.replace(\"postgresql+psycopg2\", \"postgresql\"))\n\t    return conn\n"]}
{"filename": "acrocord/utils/getpassword.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\timport getpass\n\timport sys\n\timport tempfile\n\tdef pass_(path=\".\"):\n\t    \"\"\"\n\t    Parameters\n\t    ----------\n", "    path\n\t    Returns\n\t    -------\n\t    \"\"\"\n\t    password = getpass.getpass(prompt=\"Enter the Password:\")\n\t    tmp = tempfile.NamedTemporaryFile(dir=path, delete=False)\n\t    tmp.write(bytearray(password, encoding=\"utf-8\"))\n\tdef pass_win():\n\t    return getpass.win_getpass(prompt=\"Enter the Password:\", stream=sys.stdout)\n\tif __name__ == \"__main__\":\n", "    pass_(sys.argv[1])\n"]}
{"filename": "acrocord/utils/insert.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\timport io\n\tfrom typing import Iterator, Optional\n\timport pandas as pd\n\tfrom acrocord.misc import execution\n\t@execution.execution_time\n\tdef insert(connection: \"ConnectDatabase\", data: pd.DataFrame, table_name: str,\n\t           chunksize: int = 1000):\n", "    storage = io.StringIO()\n\t    data.to_csv(storage, sep='\\t', header=False,\n\t                index=False, encoding=\"utf8\", chunksize=chunksize)\n\t    storage.seek(0)\n\t    sii = StringIteratorIO(storage)\n\t    connection = connection.engine.connect()\n\t    columns = tuple(f'{c}' for c in data.columns)\n\t    with connection.connection.cursor() as cursor:\n\t        cursor.execute(f'SET search_path TO {table_name.split(\".\")[0]}')\n\t        cursor.copy_from(sii,\n", "                         table_name.split(\".\")[1], null=\"\",\n\t                         columns=columns, sep=\"\\t\")\n\t        cursor.connection.commit()\n\t        cursor.close()\n\tclass StringIteratorIO(io.TextIOBase):\n\t    def __init__(self, iter_: Iterator[str]):\n\t        self._iter = iter_\n\t        self._buff = ''\n\t    def readable(self) -> bool:\n\t        return True\n", "    def _read1(self, n_line: Optional[int] = None) -> str:\n\t        while not self._buff:\n\t            try:\n\t                self._buff = next(self._iter)\n\t            except StopIteration:\n\t                break\n\t        ret = self._buff[:n_line]\n\t        self._buff = self._buff[len(ret):]\n\t        return ret\n\t    def read(self, n: Optional[int] = None) -> str:\n", "        line = []\n\t        if n is None or n < 0:\n\t            while True:\n\t                line_str = self._read1()\n\t                if not line_str:\n\t                    break\n\t                line.append(line_str)\n\t        else:\n\t            while n > 0:\n\t                line_str = self._read1(n)\n", "                if not line_str:\n\t                    break\n\t                n -= len(line_str)\n\t                line.append(line_str)\n\t        return ''.join(line)\n"]}
{"filename": "acrocord/factory/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\tfrom .table import TableFactory\n\t__all__ = [\"TableFactory\"]\n"]}
{"filename": "acrocord/factory/table.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport os.path\n\tfrom abc import ABC\n\tfrom abc import abstractmethod\n\tfrom typing import Dict\n\tfrom typing import Optional\n\tfrom typing import Sequence\n\tfrom typing import Tuple\n", "from typing import Type\n\tfrom typing import Union\n\timport pandas as pd\n\timport xlsxwriter\n\tfrom pandas.core.dtypes.base import ExtensionDtype\n\tfrom termcolor import colored\n\tfrom termcolor import cprint\n\tfrom acrocord import ConnectDatabase\n\tclass TableFactory(ABC):\n\t    \"\"\"\n", "    Description\n\t    -----------\n\t    Abstract class for table creation and interaction with database.\n\t    Usage\n\t    -----\n\t    If you want to use the static class you can call :\n\t    >>> table_factory = TableFactory.get_instance()\n\t    Otherwise you can create your own instance:\n\t    >>> table_factory= TableFactory()\n\t    Then you can access to the table by calling\n", "    >>> table_factory.get_table()\n\t    If you want to save the table to the database you can call:\n\t    >>> table_factory.write_table(...)\n\t    Then you can read the table from the same database using:\n\t    >>> table_factory.read_table(...)\n\t    \"\"\"\n\t    _instance: \"TableFactory\" = None\n\t    @classmethod\n\t    @abstractmethod\n\t    def data_definition(cls) -> Dict[str, Tuple[ExtensionDtype, str]]:\n", "        \"\"\"\n\t        Returns\n\t        -------\n\t        dict\n\t            Column datatypes and description. The dict follow this structure\n\t            {'column_name': (dtype, ' description')}\n\t        \"\"\"\n\t    @classmethod\n\t    @abstractmethod\n\t    def get_db_connection(cls):\n", "        ...\n\t    @classmethod\n\t    @abstractmethod\n\t    def table_name(cls) -> str:\n\t        \"\"\"\n\t        Returns\n\t        -------\n\t        str\n\t            the database table name\n\t        \"\"\"\n", "    @classmethod\n\t    @abstractmethod\n\t    def schema_name(cls) -> str:\n\t        \"\"\"\n\t        Returns\n\t        -------\n\t        str\n\t            the database schema name\n\t        \"\"\"\n\t    @classmethod\n", "    def read_table(cls,\n\t                   connection: Union[dict, str, ConnectDatabase] = None,\n\t                   schema_name: str = None,\n\t                   table_name: str = None,\n\t                   columns: iter = None,\n\t                   where: str = None\n\t                   ) -> pd.DataFrame:\n\t        \"\"\"\n\t        if exists, read the table from database.\n\t        Returns\n", "        -------\n\t        pd.DataFrame\n\t            the table\n\t        \"\"\"\n\t        schema_name: str = cls.schema_name() if schema_name is None else schema_name\n\t        table_name: str = cls.table_name() if table_name is None else table_name\n\t        columns = list(\n\t            cls.data_definition().keys()) if columns is None else columns\n\t        if isinstance(connection, ConnectDatabase):\n\t            connection_ = connection\n", "            close_db = False\n\t        else:\n\t            connection_ = cls.get_db_connection()\n\t            close_db = True\n\t        table = connection_.read_table(\n\t            f'{schema_name}.{table_name}', columns=columns,\n\t            where=where)\n\t        # assert all columns and oly columns are retrieved\n\t        assert table.columns.isin(columns).all() and pd.Index(columns).isin(\n\t            table.columns).all()\n", "        # convert to correct datatype\n\t        for col in columns:\n\t            dtype = cls.data_definition()[col][0]\n\t            table[col] = table[col].astype(dtype)\n\t        if close_db:\n\t            connection_.close()\n\t        return table\n\t    @classmethod\n\t    @abstractmethod\n\t    def id_key(cls) -> Optional[Union[str, Sequence[str]]]:\n", "        \"\"\"\n\t        Return Primary key column name, or None\n\t        \"\"\"\n\t        ...\n\t    @classmethod\n\t    def get_column_description(cls) -> dict:\n\t        \"\"\"\n\t        Returns\n\t        -------\n\t        dict\n", "            dictionary with name of column as key and its description as value\n\t        \"\"\"\n\t        doc = cls.data_definition()\n\t        return {col_name: col_data[1] for col_name, col_data in doc.items()}\n\t    @classmethod\n\t    def get_instance(cls) -> \"TableFactory\":\n\t        \"\"\"\n\t        Static instance getter to avoid recomputing the table for multiple usages.\n\t        Returns\n\t        -------\n", "        TableFactory\n\t            static table factory instance\n\t        \"\"\"\n\t        if cls._instance is None:\n\t            cls._instance = cls()\n\t        return cls._instance\n\t    @classmethod\n\t    def data_description(cls) -> pd.DataFrame:\n\t        \"\"\"\n\t        Returns\n", "        -------\n\t        pd.DataFrame\n\t            a dataframe with table documentation\n\t        \"\"\"\n\t        return pd.DataFrame(cls.data_definition(),\n\t                            index=['data type', 'column description']).T\n\t    @classmethod\n\t    def get_foreign_keys(cls) -> Dict[str, Tuple[Type['TableFactory'], str]]:\n\t        \"\"\"\n\t        return each foreign keys to be implemented in the following format:\n", "        {key in current TableFactory columns (str) :\n\t        (foreign table (TableFactory), foreign key (str) )}\n\t        \"\"\"\n\t        return {}\n\t    @classmethod\n\t    def get_full_name(cls):\n\t        return f\"{cls.schema_name()}.{cls.table_name()}\"\n\t    def __init__(self, verbose: bool = True):\n\t        self._verbose: bool = verbose\n\t        # does the table have been saved into the deployment database\n", "        self._is_deployed: bool = False\n\t        self._table: Optional[pd.DataFrame] = None\n\t        self.columns = list(self.data_definition().keys())\n\t    @abstractmethod\n\t    def _create_table(self) -> None:\n\t        \"\"\"\n\t        compute and set the table in the self._table attribute\n\t        Returns\n\t        -------\n\t        None\n", "        \"\"\"\n\t        self._set_table(pd.DataFrame())\n\t    def get_table(self, columns=None) -> pd.DataFrame:\n\t        \"\"\"\n\t        Returns\n\t        -------\n\t        pd.DataFrame\n\t            table générée par la fabrique\n\t        \"\"\"\n\t        if self._is_deployed:\n", "            return self.read_table(self.get_db_connection(), columns=columns)\n\t        if self._table is None:\n\t            self._create_table()\n\t        return self._table.copy() if columns is None else self._table[\n\t            columns].copy()\n\t    def _set_table(self, table) -> None:\n\t        \"\"\"\n\t        save table in self._table variable after checking its consistency with documentation\n\t        Parameters\n\t        ----------\n", "        table\n\t        Returns\n\t        -------\n\t        \"\"\"\n\t        table_ = table.rename(columns=str.lower)\n\t        data_definition = self.data_definition()\n\t        self.check_table_doc(table_, data_definition, self.__class__)\n\t        self._table = table_[data_definition.keys()]\n\t    def write_table(self, db: Optional[ConnectDatabase] = None,\n\t                    schema: str = None, table_name: str = None,\n", "                    **kwargs) -> None:\n\t        \"\"\"\n\t        Write table to database\n\t        Parameters\n\t        ----------\n\t        db : ConnectDatabase\n\t            the database connection\n\t        schema : str\n\t            name of the schema\n\t        table_name : str\n", "            name of the tabler\n\t        kwargs\n\t            additional arguments for ConnectDatabase.write_table\n\t        Returns\n\t        -------\n\t        \"\"\"\n\t        if db is None:\n\t            db = self.get_db_connection()\n\t        schema = self.schema_name() if schema is None else schema\n\t        table_name = self.table_name() if table_name is None else table_name\n", "        full_name = f\"{schema}.{table_name}\"\n\t        if self._verbose:\n\t            cprint(f\"[write_table] {full_name}\", 'white', attrs=['reverse', ])\n\t        data = self.get_table()\n\t        db.write_table(data, table_name=full_name,\n\t                       column_comments=self.get_column_description(),\n\t                       opt_dtype=False,\n\t                       primary_key=self.id_key(), **kwargs)\n\t        self._is_deployed = True\n\t        self._table = None\n", "    @staticmethod\n\t    def check_table_doc(table: pd.DataFrame, doc: dict, class_warning=None):\n\t        \"\"\"\n\t        check table and documentation consistency\n\t        Parameters\n\t        ----------\n\t        table\n\t        doc\n\t        class_warning\n\t            the name of the class to print for warnings\n", "        Returns\n\t        -------\n\t        \"\"\"\n\t        if class_warning is None:\n\t            class_warning = TableFactory.__class__\n\t        warning_header = f\"WARNING: {class_warning} \"\n\t        doc_cols = doc.keys()\n\t        table_cols = table.columns\n\t        if table_cols.isin(doc_cols).all() and len(doc_cols) == len(table_cols):\n\t            for col_name, val in doc.items():\n", "                col_dtype, col_doc = val\n\t                try:\n\t                    table[col_name] = table[col_name].astype(col_dtype)\n\t                except Exception as e:\n\t                    cprint(f\"{warning_header} {col_name}: \"\n\t                           f\"cannot convert {col_name} \"\n\t                           f\"from {table[col_name].dtype} to {col_dtype}.\",\n\t                           'yellow')\n\t                    cprint(f\"{e}\", 'yellow')\n\t                if col_doc is None or col_doc == \"\":\n", "                    cprint(warning_header +\n\t                           f\"missing description of {col_name}.\",\n\t                           'yellow')\n\t        else:\n\t            doc_cols = pd.Series(doc_cols)\n\t            table_cols = pd.Series(table_cols)\n\t            missing_doc = table_cols[~(table_cols.isin(doc_cols))]\n\t            if len(missing_doc) > 0:\n\t                cprint(\n\t                    warning_header + f\" column statement is missing:\\n{missing_doc}\",\n", "                    'yellow')\n\t            missing_col = doc_cols[~(doc_cols.isin(table_cols))]\n\t            if len(missing_col) > 0:\n\t                cprint(warning_header + f\" data is missing :\\n{missing_col}\",\n\t                       'yellow')\n\t    def write_to_excel(self, save: bool = True, path: str = None,\n\t                       file_name: str = None, verbose=True) -> 'xlsxwriter':\n\t        if file_name is None:\n\t            file_name = self.table_name() + '.xlsx'\n\t        elif len(file_name) <= 5 or file_name[-5:] != '.xlsx':\n", "            file_name += '.xlsx'\n\t        if path is None:\n\t            import tempfile\n\t            path = tempfile.gettempdir()\n\t        data = self.get_table()\n\t        full_path = os.path.join(path, file_name)\n\t        writer = pd.ExcelWriter(full_path, engine='xlsxwriter')\n\t        doc = self.data_description()[\n\t            ['column description']].reset_index().rename(\n\t            columns={'index': 'column name'})\n", "        for df, sheet_name, save_index in (\n\t                (doc, 'Column description', False), (data, 'Data', False)):\n\t            if verbose:\n\t                print(f\"Writing '{sheet_name}' Excel sheet...\", end='')\n\t            df.to_excel(writer, sheet_name=sheet_name, index=save_index)\n\t            if verbose:\n\t                print(f\"\\rWriting '{sheet_name}' Excel sheet\" + colored(\" ok\",\n\t                                                                        'green'))\n\t            if verbose:\n\t                print(\"adjust column width \" + sheet_name + \"...\", end='')\n", "            worksheet = writer.sheets[sheet_name]\n\t            worksheet.autofilter(0, 0, 0, len(df.columns) - 1)\n\t            for idx, col in enumerate(df):  # loop through all columns\n\t                series = df[col]\n\t                max_len = max((\n\t                    series.astype(str).map(len).max(),  # len of largest item\n\t                    len(str(col)) + 2  # len of column name/header +2 for bold\n\t                )) + 1  # adding a little extra space\n\t                worksheet.set_column(idx, idx, max_len)\n\t            if verbose:\n", "                print(\"\\radjust column width \" + sheet_name + colored(\" ok\",\n\t                                                                      'green'))\n\t        if save:\n\t            if verbose:\n\t                print(\"save excel...\", end='')\n\t            if hasattr(writer, \"save\"):\n\t                writer.save()\n\t            else:\n\t                writer._save()\n\t            if verbose:\n", "                print(\n\t                    f\"ok -> {colored(full_path, 'blue', attrs=['underline'])}\")\n\t        return writer\n\t    def add_foreign_keys(self, db: ConnectDatabase = None):\n\t        if db is None:\n\t            db = self.get_db_connection()\n\t        full_table_name = f\"{self.schema_name()}.{self.table_name()}\"\n\t        for key, (\n\t                foreign_table, foreign_key) in self.get_foreign_keys().items():\n\t            foreign_table_full_name = f\"{foreign_table.schema_name()}\" \\\n", "                                      f\".{foreign_table.table_name()}\"\n\t            print(\n\t                f\"{full_table_name} ({key}) -- > ({foreign_key}) \"\n\t                f\"{foreign_table_full_name} \")\n\t            db.add_key(type_='foreign',\n\t                       table_name=full_table_name, key=key,\n\t                       table_foreign=foreign_table_full_name,\n\t                       key_foreign=foreign_key,\n\t                       )\n"]}
{"filename": "acrocord/misc/execution.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n\timport datetime\n\timport os\n\timport sys\n\timport time\n\timport warnings\n\tfrom contextlib import contextmanager\n\tfrom functools import wraps\n", "from memory_profiler import memory_usage\n\tclass ColorsOut:\n\t    HEADER = '\\033[95m'\n\t    OKBLUE = '\\033[94m'\n\t    OKGREEN = '\\033[92m'\n\t    WARNING = '\\033[93m'\n\t    FAIL = '\\033[91m'\n\t    ENDC = '\\033[0m'\n\t    BOLD = '\\033[1m'\n\t    UNDERLINE = '\\033[4m'\n", "    Red = '\\033[91m'\n\t    Green = '\\033[92m'\n\t    Blue = '\\033[94m'\n\t    Cyan = '\\033[96m'\n\t    White = '\\033[97m'\n\t    Yellow = '\\033[93m'\n\t    Magenta = '\\033[95m'\n\t    Grey = '\\033[90m'\n\t    Black = '\\033[90m'\n\t    Default = '\\033[99m'\n", "    DEFAULT = Blue\n\t    TIME = Grey\n\tdef print_(output, color=\"DEFAULT\"):\n\t    print(\n\t        f\"\"\"{ColorsOut.__getattribute__(ColorsOut, color)}{output}{ColorsOut.ENDC}\"\"\")\n\tdef execution_time(method):\n\t    @wraps(method)\n\t    def timed(*args, **kw):\n\t        starting_time = time.time()\n\t        mem, result = memory_usage((method, args, kw), retval=True, timeout=200,\n", "                                   interval=1e-7)\n\t        stopping_time = time.time()\n\t        msg = \"[\" + method.__name__ + \"] execution time :\"\n\t        msg += \"-\" * (40 - len(msg)) + \"  \"\n\t        msg += str(datetime.timedelta(milliseconds=(stopping_time - starting_time) * 1000))\n\t        msg += \"  \" + f'Memory {int(max(mem) - min(mem))}' + \" MiB\"\n\t        if len(args) > 0 and hasattr(args[0], \"active_execution_time\"):\n\t            if not args[0].active_execution_time:\n\t                return result\n\t            else:\n", "                print_(msg, color=\"TIME\")\n\t        else:\n\t            print_(msg, color=\"TIME\")\n\t        return result\n\t    return timed\n\t@contextmanager\n\tdef silence_stdout():\n\t    new_target = open(os.devnull, \"w\")\n\t    old_target = sys.stdout\n\t    sys.stdout = new_target\n", "    try:\n\t        yield new_target\n\t    finally:\n\t        sys.stdout = old_target\n\tdef deprecated(func):\n\t    \"\"\"This is a decorator which can be used to mark functions\n\t    as deprecated. It will result in a warning being emitted\n\t    when the function is used.\"\"\"\n\t    @wraps(func)\n\t    def new_func(*args, **kwargs):\n", "        warnings.simplefilter('always', DeprecationWarning)  # turn off filter\n\t        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__),\n\t                      category=DeprecationWarning,\n\t                      stacklevel=2)\n\t        warnings.simplefilter('default', DeprecationWarning)  # reset filter\n\t        return func(*args, **kwargs)\n\t    return new_func\n"]}
{"filename": "acrocord/misc/__init__.py", "chunked_list": ["# Copyright 2023 Eurobios\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.#\n"]}
