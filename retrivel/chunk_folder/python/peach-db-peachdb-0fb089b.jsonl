{"filename": "setup.py", "chunked_list": ["from pathlib import Path\n\tfrom setuptools import find_packages, setup  # type: ignore\n\trequirements = []\n\t# Read requirements from files\n\twith open(\"requirements.txt\", \"r\") as f:\n\t    requirements.extend(f.read().splitlines())\n\t# read the contents README\n\tthis_directory = Path(__file__).parent\n\tlong_description = (this_directory / \"README.md\").read_text()\n\tsetup(\n", "    name=\"peachdb\",\n\t    version=\"0.1.0\",\n\t    packages=find_packages(),\n\t    install_requires=requirements,\n\t    dependency_links=[\"https://download.pytorch.org/whl/cu113\"],\n\t    long_description=long_description,\n\t    long_description_content_type=\"text/markdown\",\n\t    license=\"Apache License 2.0\",\n\t    project_urls={\n\t        \"Source on GitHub\": \"https://github.com/peach-db/peachdb\",\n", "        \"Documentation\": \"https://github.com/peach-db/peachdb/blob/master/README.md\",\n\t    },\n\t    python_requires=\">=3.10\",\n\t)\n"]}
{"filename": "deploy_api.py", "chunked_list": ["import os\n\timport shelve\n\timport tempfile\n\timport traceback\n\tfrom uuid import uuid4\n\timport openai\n\timport pandas as pd\n\timport uvicorn\n\tfrom fastapi import FastAPI, WebSocket\n\tfrom fastapi.middleware.cors import CORSMiddleware\n", "from fastapi.requests import Request\n\tfrom fastapi.responses import Response\n\tfrom pydantic import BaseModel\n\tfrom rich import print\n\tfrom peachdb import EmptyNamespace, PeachDB\n\tfrom peachdb.bots.qa import BadBotInputError, ConversationNotFoundError, QABot\n\tfrom peachdb.constants import SHELVE_DB\n\tapp = FastAPI()\n\tapp.add_middleware(\n\t    CORSMiddleware,\n", "    allow_origins=[\"*\"],\n\t    allow_credentials=True,\n\t    allow_methods=[\"*\"],\n\t    allow_headers=[\"*\"],\n\t)\n\tEMBEDDING_GENERATOR = \"openai_ada\"\n\tEMBEDDING_BACKEND = \"exact_cpu\"\n\tdef _validate_data(texts, ids, metadatas_list):\n\t    assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n\t    assert all(isinstance(i, str) for i in ids), \"All IDs must be strings\"\n", "    assert all(isinstance(m, dict) for m in metadatas_list), \"All metadata must be dicts\"\n\t    assert len(set([str(x.keys()) for x in metadatas_list])) == 1, \"All metadata must have the same keys\"\n\tdef _validate_metadata_key_names_dont_conflict(metadatas_dict, namespace):\n\t    assert \"texts\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'texts'\"\n\t    assert \"ids\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'ids'\"\n\t    if namespace is not None:\n\t        assert \"namespace\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'namespace'\"\n\tdef _process_input_data(request_json: dict) -> pd.DataFrame:\n\t    input_data = request_json\n\t    namespace = request_json.get(\"namespace\", None)\n", "    # TODO: make metadata optional?\n\t    # tuples of (id: str, text: list[float], metadata: dict[str, str]])\n\t    data = input_data[\"data\"]\n\t    assert len(set([len(d) for d in data])) == 1, \"All data must have the same length\"\n\t    if len(data[0]) == 3:\n\t        # we got (ids, texts, metadata)\n\t        pass\n\t    elif len(data[0]) == 2:\n\t        # we got (ids, texts)\n\t        data = [(d[0], d[1], {}) for d in data]\n", "    else:\n\t        raise ValueError(\"Data must be of the form (ids, texts) or (ids, texts, metadata)\")\n\t    ids = [str(d[0]) for d in data]\n\t    texts = [d[1] for d in data]\n\t    metadatas_list: list = [d[2] for d in data]\n\t    _validate_data(texts, ids, metadatas_list)\n\t    # convert metadata from input format to one we can create a dataframe from.\n\t    metadatas_dict: dict[str, list] = {\n\t        key: [metadata[key] for metadata in metadatas_list] for key in metadatas_list[0].keys()\n\t    }\n", "    _validate_metadata_key_names_dont_conflict(metadatas_dict, namespace)\n\t    if namespace is None:\n\t        metadatas_dict[\"namespace\"] = [None] * len(ids)\n\t    else:\n\t        metadatas_dict[\"namespace\"] = [namespace] * len(ids)\n\t    df = pd.DataFrame(\n\t        data={\n\t            \"ids\": ids,\n\t            \"texts\": texts,\n\t            **metadatas_dict,\n", "        }\n\t    )\n\t    return df\n\t@app.post(\"/upsert-text\")\n\tasync def upsert_handler(request: Request):\n\t    \"\"\"\n\t    Takes texts as input rather than vectors (unlike Pinecone).\n\t    \"\"\"\n\t    input_data = await request.json()\n\t    project_name = input_data.get(\"project_name\", None)\n", "    if project_name is None:\n\t        return Response(content=\"Project name must be specified.\", status_code=400)\n\t    peach_db = PeachDB(\n\t        project_name=project_name,\n\t        embedding_generator=EMBEDDING_GENERATOR,\n\t        embedding_backend=EMBEDDING_BACKEND,\n\t    )\n\t    new_data_df = _process_input_data(input_data)\n\t    namespace = input_data.get(\"namespace\", None)\n\t    with shelve.open(SHELVE_DB) as shelve_db:\n", "        project_info = shelve_db.get(project_name, None)\n\t        assert project_info is not None, \"Project not found\"\n\t        assert not project_info[\"lock\"], \"Some other process is currently reading/writing to this project\"\n\t        # TODO: replace with a lock we can await.\n\t        project_info[\"lock\"] = True\n\t        shelve_db[project_name] = project_info\n\t    if os.path.exists(project_info[\"exp_compound_csv_path\"]):\n\t        data_df = pd.read_csv(project_info[\"exp_compound_csv_path\"])\n\t        if namespace is None:\n\t            data_df_namespace = data_df[data_df[\"namespace\"].isnull()]\n", "        else:\n\t            data_df_namespace = data_df[data_df[\"namespace\"] == namespace]\n\t        # Check for intersection between the \"ids\" column of data_df and new_data_df\n\t        # TODO: add support on backend for string ids.\n\t        if len(set(data_df_namespace[\"ids\"].apply(str)).intersection(set(new_data_df[\"ids\"].apply(str)))) != 0:\n\t            with shelve.open(SHELVE_DB) as shelve_db:\n\t                project_info = shelve_db.get(project_name, None)\n\t                project_info[\"lock\"] = False\n\t                shelve_db[project_name] = project_info\n\t            return Response(\n", "                content=\"New data contains IDs that already exist in the database for this namespace. This is not allowed.\",\n\t                status_code=400,\n\t            )\n\t    # We use unique csv_name to avoid conflicts in the stored data.\n\t    with tempfile.NamedTemporaryFile(suffix=f\"{uuid4()}.csv\") as tmp:\n\t        # TODO: what happens if ids' conflict?\n\t        new_data_df.to_csv(tmp.name, index=False)  # TODO: check it won't cause an override.\n\t        peach_db.upsert_text(\n\t            csv_path=tmp.name,\n\t            column_to_embed=\"texts\",\n", "            id_column_name=\"ids\",\n\t            # TODO: below is manually set, this might cause issues!\n\t            embeddings_output_s3_bucket_uri=\"s3://metavoice-vector-db/deployed_solution/\",\n\t            namespace=namespace,\n\t        )\n\t    if os.path.exists(project_info[\"exp_compound_csv_path\"]):\n\t        # Update the data_df with the new data, and save to disk.\n\t        data_df = pd.concat([data_df, new_data_df], ignore_index=True)\n\t        data_df.to_csv(project_info[\"exp_compound_csv_path\"], index=False)\n\t    else:\n", "        new_data_df.to_csv(project_info[\"exp_compound_csv_path\"], index=False)\n\t    # release lock\n\t    with shelve.open(SHELVE_DB) as shelve_db:\n\t        project_info = shelve_db.get(project_name, None)\n\t        project_info[\"lock\"] = False\n\t        shelve_db[project_name] = project_info\n\t@app.get(\"/query\")\n\tasync def query_embeddings_handler(request: Request):\n\t    data = await request.json()\n\t    project_name = data.get(\"project_name\", None)\n", "    if project_name is None:\n\t        return Response(content=\"Project name must be specified.\", status_code=400)\n\t    peach_db = PeachDB(\n\t        project_name=project_name,\n\t        embedding_generator=EMBEDDING_GENERATOR,\n\t        embedding_backend=EMBEDDING_BACKEND,\n\t    )\n\t    text = data[\"text\"]\n\t    top_k = int(data.get(\"top_k\", 5))\n\t    namespace = data.get(\"namespace\", None)\n", "    try:\n\t        ids, distances, metadata = peach_db.query(query_input=text, modality=\"text\", namespace=namespace, top_k=top_k)\n\t    except EmptyNamespace:\n\t        return Response(content=\"Empty namespace.\", status_code=400)\n\t    result = []\n\t    # TODO: we're aligning distances, ids, and metadata from different sources which could cause bugs.\n\t    # Fix this.\n\t    for id, dist in zip(ids, distances):\n\t        values = metadata[metadata[\"ids\"] == id].values[0]\n\t        columns = list(metadata.columns)\n", "        columns = [\n\t            c for c in columns if c != \"namespace\"\n\t        ]  # Causes an error with JSON encoding when \"namespace\" is None and ends up as NaN here.\n\t        result_dict = {columns[i]: values[i] for i in range(len(columns))}\n\t        result_dict[\"distance\"] = dist\n\t        result.append(result_dict)\n\t    return {\"result\": result}\n\t@app.post(\"/create-bot\")\n\tasync def create_bot_handler(request: Request):\n\t    try:\n", "        request_json = await request.json()\n\t        try:\n\t            bot = QABot(\n\t                bot_id=request_json[\"bot_id\"],\n\t                system_prompt=request_json[\"system_prompt\"],\n\t                llm_model_name=request_json[\"llm_model_name\"] if \"llm_model_name\" in request_json else \"gpt-3.5-turbo\",\n\t                embedding_model=request_json[\"embedding_model_name\"]\n\t                if \"embedding_model_name\" in request_json\n\t                else \"openai_ada\",\n\t            )\n", "        except BadBotInputError as e:\n\t            return Response(content=str(e), status_code=400)\n\t        try:\n\t            bot.add_data(documents=request_json[\"documents\"])\n\t            return Response(content=\"Bot created successfully.\", status_code=200)\n\t        except openai.error.RateLimitError:\n\t            return Response(\n\t                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n\t            )\n\t        except openai.error.AuthenticationError:\n", "            return Response(content=\"There's been an authentication error. Please contact the team.\", status_code=400)\n\t        except openai.error.ServiceUnavailableError:\n\t            return Response(\n\t                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n\t            )\n\t    except Exception as e:\n\t        traceback.print_exc()\n\t        return Response(content=\"An unknown error occured. Please contact the team.\", status_code=500)\n\t@app.post(\"/create-conversation\")\n\tasync def create_conversation_handler(request: Request):\n", "    try:\n\t        request_json = await request.json()\n\t        if \"bot_id\" not in request_json:\n\t            return Response(content=\"bot_id must be specified.\", status_code=400)\n\t        if \"query\" not in request_json:\n\t            return Response(content=\"query must be specified.\", status_code=400)\n\t        bot_id = request_json[\"bot_id\"]\n\t        query = request_json[\"query\"]\n\t        bot = QABot(bot_id=bot_id)\n\t        try:\n", "            for cid, response in bot.create_conversation_with_query(query=query):\n\t                break\n\t        except openai.error.RateLimitError:\n\t            return Response(\n\t                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n\t            )\n\t        except openai.error.ServiceUnavailableError:\n\t            return Response(\n\t                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n\t            )\n", "        return {\n\t            \"conversation_id\": cid,\n\t            \"response\": response,\n\t        }\n\t    except Exception as e:\n\t        traceback.print_exc()\n\t        return Response(content=\"An unknown error occured. Please contact the team.\", status_code=500)\n\t@app.websocket_route(\"/ws-create-conversation\")\n\tasync def ws_create_conversation_handler(websocket: WebSocket):\n\t    try:\n", "        await websocket.accept()\n\t        request_json = await websocket.receive_json()\n\t        if \"bot_id\" not in request_json:\n\t            await websocket.close(reason=\"bot_id must be specified.\", code=1003)\n\t        if \"query\" not in request_json:\n\t            await websocket.close(reason=\"query must be specified.\", code=1003)\n\t        bot_id = request_json[\"bot_id\"]\n\t        query = request_json[\"query\"]\n\t        try:\n\t            bot = QABot(bot_id=bot_id)\n", "            for cid, response in bot.create_conversation_with_query(query=query, stream=True):\n\t                await websocket.send_json(\n\t                    {\n\t                        \"conversation_id\": cid,\n\t                        \"response\": response,\n\t                    }\n\t                )\n\t        except openai.error.RateLimitError:\n\t            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n\t        except BadBotInputError as e:\n", "            await websocket.close(reason=str(e), code=1003)\n\t        # TODO: add below error message to add endpoints... Can we handle all these exceptions together somehow?\n\t        except openai.error.ServiceUnavailableError:\n\t            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n\t    except Exception as e:\n\t        traceback.print_exc()\n\t        await websocket.close(reason=\"An unknown error occured. Please contact the team.\", code=1003)\n\t@app.post(\"/continue-conversation\")\n\tasync def continue_conversation_handler(request: Request):\n\t    try:\n", "        request_json = await request.json()\n\t        if \"bot_id\" not in request_json:\n\t            return Response(content=\"bot_id must be specified.\", status_code=400)\n\t        if \"conversation_id\" not in request_json:\n\t            return Response(content=\"conversation_id must be specified.\", status_code=400)\n\t        if \"query\" not in request_json:\n\t            return Response(content=\"query must be specified.\", status_code=400)\n\t        bot_id = request_json[\"bot_id\"]\n\t        conversation_id = request_json[\"conversation_id\"]\n\t        query = request_json[\"query\"]\n", "        bot = QABot(bot_id=bot_id)\n\t        try:\n\t            for response in bot.continue_conversation_with_query(conversation_id=conversation_id, query=query):\n\t                break\n\t        except ConversationNotFoundError:\n\t            return Response(content=\"Conversation not found. Please check `conversation_id`\", status_code=400)\n\t        except openai.error.RateLimitError:\n\t            return Response(\n\t                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n\t            )\n", "        except openai.error.ServiceUnavailableError:\n\t            return Response(\n\t                content=\"OpenAI's server are currently overloaded. Please try again later.\", status_code=400\n\t            )\n\t        return {\n\t            \"response\": response,\n\t        }\n\t    except Exception as e:\n\t        traceback.print_exc()\n\t        return Response(content=\"An unknown error occured. Please contact the team.\", status_code=500)\n", "@app.websocket_route(\"/ws-continue-conversation\")\n\tasync def ws_continue_conversation_handler(websocket: WebSocket):\n\t    try:\n\t        await websocket.accept()\n\t        request_json = await websocket.receive_json()\n\t        if \"bot_id\" not in request_json:\n\t            await websocket.close(reason=\"bot_id must be specified.\", code=1003)\n\t        if \"conversation_id\" not in request_json:\n\t            await websocket.close(reason=\"conversation_id must be specified.\", code=1003)\n\t        if \"query\" not in request_json:\n", "            await websocket.close(reason=\"query must be specified.\", code=1003)\n\t        bot_id = request_json[\"bot_id\"]\n\t        conversation_id = request_json[\"conversation_id\"]\n\t        query = request_json[\"query\"]\n\t        bot = QABot(bot_id=bot_id)\n\t        try:\n\t            for response in bot.continue_conversation_with_query(\n\t                conversation_id=conversation_id, query=query, stream=True\n\t            ):\n\t                await websocket.send_json(\n", "                    {\n\t                        \"response\": response,\n\t                    }\n\t                )\n\t        except ConversationNotFoundError:\n\t            await websocket.close(reason=\"Conversation not found. Please check `conversation_id`\", code=1003)\n\t        except openai.error.RateLimitError:\n\t            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n\t        # TODO: add below error message to add endpoints... Can we handle all these exceptions together somehow?\n\t        except openai.error.ServiceUnavailableError:\n", "            await websocket.close(reason=\"OpenAI's server are currently overloaded. Please try again later.\", code=1003)\n\t    except Exception as e:\n\t        traceback.print_exc()\n\t        await websocket.close(reason=\"An unknown error occured. Please contact the team.\", code=1003)\n\tif __name__ == \"__main__\":\n\t    port = 8000\n\t    uvicorn.run(\"deploy_api:app\", host=\"0.0.0.0\", port=port)  # , reload=True)\n"]}
{"filename": "run_bot.py", "chunked_list": ["import shelve\n\tfrom peachdb.bots.qa import QABot\n\tfrom peachdb.constants import CONVERSATIONS_DB\n\tbot = QABot(\n\t    bot_id=\"my_bot_12\",\n\t    embedding_model=\"openai_ada\",\n\t    llm_model_name=\"gpt-3.5-turbo\",\n\t    system_prompt=\"Please answer questions about the 1880 Greenback Party National Convention.\",\n\t)\n\tbot.add_data(\n", "    documents=[\n\t        \"The 1880 Greenback Party National Convention convened in Chicago from June 9 to June 11 to select presidential and vice presidential nominees and write a party platform for the Greenback Party in the United States presidential election of 1880. Delegates chose James B. Weaver of Iowa for President and Barzillai J. Chambers of Texas for Vice President.\",\n\t        'The Greenback Party was a newcomer to the political scene in 1880 having arisen, mostly in the nation\\'s West and South, as a response to the economic depression that followed the Panic of 1873. During the Civil War, Congress had authorized \"greenbacks\", a form of money redeemable in government bonds, rather than in then-traditional gold. After the war, many Democrats and Republicans in the East sought to return to the gold standard, and the government began to withdraw greenbacks from circulation. The reduction of the money supply, combined with the economic depression, made life harder for debtors, farmers, and industrial laborers; the Greenback Party hoped to draw support from these groups.',\n\t    ]\n\t)\n\tcid, answer = bot.create_conversation_with_query(\"Where did the convention convene?\")\n\tanswer_2 = bot.continue_conversation_with_query(cid, \"Who was the vice presidential nominee?\")\n\tanswer_3 = bot.continue_conversation_with_query(cid, \"What was The Greenback Party?\")\n"]}
{"filename": "peachdb_grpc/api_server.py", "chunked_list": ["\"\"\"\n\tModule docstring:\n\tThis module is a streaming gRPC server for the bot service.\n\tIt serves to handle requests related to bot creation, conversation creation and continuation.\n\t\"\"\"\n\timport asyncio\n\timport functools\n\timport traceback\n\tfrom typing import AsyncIterable, Iterator\n\timport api_pb2 as api_pb2  # type: ignore\n", "import api_pb2_grpc as api_pb2_grpc  # type: ignore\n\timport grpc  # type: ignore\n\timport openai\n\tfrom peachdb.bots.qa import BadBotInputError, ConversationNotFoundError, QABot, UnexpectedGPTRoleResponse\n\tdef grpc_error_handler_async_fn(fn):\n\t    \"\"\"\n\t    A decorator to handle any unhandled errors and map them to appropriate gRPC status codes.\n\t    :param fn: The function to be decorated.\n\t    :return: Decorated function.\n\t    \"\"\"\n", "    @functools.wraps(fn)\n\t    async def wrapper(self, request, context):\n\t        try:\n\t            return await fn(self, request, context)\n\t        except BadBotInputError as e:\n\t            await context.abort(grpc.StatusCode.INVALID_ARGUMENT, str(e))\n\t        except openai.error.RateLimitError:\n\t            await context.abort(\n\t                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n\t            )\n", "        except openai.error.AuthenticationError:\n\t            await context.abort(\n\t                grpc.StatusCode.UNAUTHENTICATED, \"There's been an authentication error. Please contact the team.\"\n\t            )\n\t        except openai.error.ServiceUnavailableError:\n\t            await context.abort(\n\t                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n\t            )\n\t        except UnexpectedGPTRoleResponse:\n\t            await context.abort(grpc.StatusCode.INTERNAL, \"GPT-3 responded with a role that was not expected.\")\n", "        except ConversationNotFoundError:\n\t            await context.abort(\n\t                grpc.StatusCode.INVALID_ARGUMENT, \"Conversation not found. Please check `conversation_id`\"\n\t            )\n\t        except Exception as e:\n\t            traceback.print_exc()\n\t            await context.abort(grpc.StatusCode.UNKNOWN, \"An unknown error occurred. Please contact the team.\")\n\t    return wrapper\n\tdef grpc_error_handler_async_gen(fn):\n\t    \"\"\"\n", "    A decorator to handle any unhandled errors and map them to appropriate gRPC status codes.\n\t    :param fn: The function to be decorated.\n\t    :return: Decorated function.\n\t    \"\"\"\n\t    @functools.wraps(fn)\n\t    async def wrapper(self, request, context):\n\t        try:\n\t            async for response in fn(self, request, context):\n\t                yield response\n\t        except BadBotInputError as e:\n", "            await context.abort(grpc.StatusCode.INVALID_ARGUMENT, str(e))\n\t        except openai.error.RateLimitError:\n\t            await context.abort(\n\t                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n\t            )\n\t        except openai.error.ServiceUnavailableError:\n\t            await context.abort(\n\t                grpc.StatusCode.RESOURCE_EXHAUSTED, \"OpenAI's servers are currently overloaded. Please try again later.\"\n\t            )\n\t        except openai.error.AuthenticationError:\n", "            await context.abort(\n\t                grpc.StatusCode.UNAUTHENTICATED, \"There's been an authentication error. Please contact the team.\"\n\t            )\n\t        except UnexpectedGPTRoleResponse:\n\t            await context.abort(grpc.StatusCode.INTERNAL, \"GPT-3 responded with a role that was not expected.\")\n\t        except ConversationNotFoundError:\n\t            await context.abort(\n\t                grpc.StatusCode.INVALID_ARGUMENT, \"Conversation not found. Please check `conversation_id`\"\n\t            )\n\t        except Exception as e:\n", "            traceback.print_exc()\n\t            await context.abort(grpc.StatusCode.UNKNOWN, \"An unknown error occurred. Please contact the team.\")\n\t    return wrapper\n\tclass BotServiceServicer(api_pb2_grpc.BotServiceServicer):\n\t    @grpc_error_handler_async_fn\n\t    async def CreateBot(self, request: api_pb2.CreateBotRequest, context) -> api_pb2.CreateBotResponse:\n\t        \"\"\"\n\t        RPC method to create a new bot instance.\n\t        :param request: Request instance for CreateBot.\n\t        :param context: Context instance for CreateBot.\n", "        :return: CreateBotResponse instance.\n\t        \"\"\"\n\t        bot = QABot(\n\t            bot_id=request.bot_id,\n\t            system_prompt=request.system_prompt,\n\t            llm_model_name=request.llm_model_name if request.HasField(\"llm_model_name\") else \"gpt-3.5-turbo\",\n\t            embedding_model=request.embedding_model_name if request.HasField(\"embedding_model_name\") else \"openai_ada\",\n\t        )\n\t        bot.add_data(documents=list(request.documents))\n\t        return api_pb2.CreateBotResponse(status=\"Bot created successfully.\")\n", "    @grpc_error_handler_async_gen\n\t    async def CreateConversation(\n\t        self, request: api_pb2.CreateConversationRequest, context\n\t    ) -> AsyncIterable[api_pb2.CreateConversationResponse]:\n\t        \"\"\"\n\t        RPC method to create a new conversation for a bot.\n\t        :param request: Request instance for CreateConversation.\n\t        :param context: Context instance for CreateConversation.\n\t        :return: CreateConversationResponse instance stream.\n\t        \"\"\"\n", "        await self._check(request, \"bot_id\", context)\n\t        await self._check(request, \"query\", context)\n\t        bot_id = request.bot_id\n\t        query = request.query\n\t        bot = QABot(bot_id=bot_id)\n\t        generator = bot.create_conversation_with_query(query=query, stream=True)\n\t        assert isinstance(generator, Iterator)\n\t        for cid, response in generator:\n\t            yield api_pb2.CreateConversationResponse(conversation_id=cid, response=response)\n\t    @grpc_error_handler_async_gen\n", "    async def ContinueConversation(\n\t        self, request: api_pb2.ContinueConversationRequest, context\n\t    ) -> AsyncIterable[api_pb2.ContinueConversationResponse]:\n\t        \"\"\"\n\t        RPC method to continue a conversation for a bot.\n\t        :param request: Request instance for ContinueConversation.\n\t        :param context: Context instance for ContinueConversation.\n\t        :return: ContinueConversationResponse instance stream.\n\t        \"\"\"\n\t        await self._check(request, \"bot_id\", context)\n", "        await self._check(request, \"conversation_id\", context)\n\t        await self._check(request, \"query\", context)\n\t        bot_id = request.bot_id\n\t        conversation_id = request.conversation_id\n\t        query = request.query\n\t        bot = QABot(bot_id=bot_id)\n\t        response_gen = bot.continue_conversation_with_query(conversation_id=conversation_id, query=query, stream=True)\n\t        for response in response_gen:\n\t            yield api_pb2.ContinueConversationResponse(response=response)\n\t    async def _check(self, request, field, context):\n", "        if not getattr(request, field):\n\t            await context.abort(grpc.StatusCode.INVALID_ARGUMENT, f\"{field} must be specified.\")\n\tasync def serve() -> None:\n\t    \"\"\"\n\t    Start a gRPC server.\n\t    :return: None\n\t    \"\"\"\n\t    server = grpc.aio.server()\n\t    api_pb2_grpc.add_BotServiceServicer_to_server(BotServiceServicer(), server)\n\t    server.add_insecure_port(\"[::]:50051\")\n", "    await server.start()\n\t    await server.wait_for_termination()\n\tif __name__ == \"__main__\":\n\t    asyncio.get_event_loop().run_until_complete(serve())\n"]}
{"filename": "peachdb_grpc/api_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: api.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\tfrom google.protobuf.internal import builder as _builder\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n\t    b'\\n\\tapi.proto\\x12\\x07peachdb\"\\xb8\\x01\\n\\x10\\x43reateBotRequest\\x12\\x0e\\n\\x06\\x62ot_id\\x18\\x01 \\x01(\\t\\x12\\x15\\n\\rsystem_prompt\\x18\\x02 \\x01(\\t\\x12\\x11\\n\\tdocuments\\x18\\x03 \\x03(\\t\\x12\\x1b\\n\\x0ellm_model_name\\x18\\x04 \\x01(\\tH\\x00\\x88\\x01\\x01\\x12!\\n\\x14\\x65mbedding_model_name\\x18\\x05 \\x01(\\tH\\x01\\x88\\x01\\x01\\x42\\x11\\n\\x0f_llm_model_nameB\\x17\\n\\x15_embedding_model_name\"#\\n\\x11\\x43reateBotResponse\\x12\\x0e\\n\\x06status\\x18\\x01 \\x01(\\t\":\\n\\x19\\x43reateConversationRequest\\x12\\x0e\\n\\x06\\x62ot_id\\x18\\x01 \\x01(\\t\\x12\\r\\n\\x05query\\x18\\x02 \\x01(\\t\"G\\n\\x1a\\x43reateConversationResponse\\x12\\x17\\n\\x0f\\x63onversation_id\\x18\\x01 \\x01(\\t\\x12\\x10\\n\\x08response\\x18\\x02 \\x01(\\t\"U\\n\\x1b\\x43ontinueConversationRequest\\x12\\x0e\\n\\x06\\x62ot_id\\x18\\x01 \\x01(\\t\\x12\\x17\\n\\x0f\\x63onversation_id\\x18\\x02 \\x01(\\t\\x12\\r\\n\\x05query\\x18\\x03 \\x01(\\t\"0\\n\\x1c\\x43ontinueConversationResponse\\x12\\x10\\n\\x08response\\x18\\x01 \\x01(\\t2\\x9e\\x02\\n\\nBotService\\x12\\x44\\n\\tCreateBot\\x12\\x19.peachdb.CreateBotRequest\\x1a\\x1a.peachdb.CreateBotResponse\"\\x00\\x12\\x61\\n\\x12\\x43reateConversation\\x12\".peachdb.CreateConversationRequest\\x1a#.peachdb.CreateConversationResponse\"\\x00\\x30\\x01\\x12g\\n\\x14\\x43ontinueConversation\\x12$.peachdb.ContinueConversationRequest\\x1a%.peachdb.ContinueConversationResponse\"\\x00\\x30\\x01\\x62\\x06proto3'\n\t)\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"api_pb2\", globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t    DESCRIPTOR._options = None\n\t    _CREATEBOTREQUEST._serialized_start = 23\n\t    _CREATEBOTREQUEST._serialized_end = 207\n\t    _CREATEBOTRESPONSE._serialized_start = 209\n", "    _CREATEBOTRESPONSE._serialized_end = 244\n\t    _CREATECONVERSATIONREQUEST._serialized_start = 246\n\t    _CREATECONVERSATIONREQUEST._serialized_end = 304\n\t    _CREATECONVERSATIONRESPONSE._serialized_start = 306\n\t    _CREATECONVERSATIONRESPONSE._serialized_end = 377\n\t    _CONTINUECONVERSATIONREQUEST._serialized_start = 379\n\t    _CONTINUECONVERSATIONREQUEST._serialized_end = 464\n\t    _CONTINUECONVERSATIONRESPONSE._serialized_start = 466\n\t    _CONTINUECONVERSATIONRESPONSE._serialized_end = 514\n\t    _BOTSERVICE._serialized_start = 517\n", "    _BOTSERVICE._serialized_end = 803\n\t# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "peachdb_grpc/__init__.py", "chunked_list": []}
{"filename": "peachdb_grpc/api_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport api_pb2 as api__pb2  # type: ignore\n\timport grpc  # type: ignore\n\tclass BotServiceStub(object):\n\t    \"\"\"The service definition\"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n\t            channel: A grpc.Channel.\n", "        \"\"\"\n\t        self.CreateBot = channel.unary_unary(\n\t            \"/peachdb.BotService/CreateBot\",\n\t            request_serializer=api__pb2.CreateBotRequest.SerializeToString,\n\t            response_deserializer=api__pb2.CreateBotResponse.FromString,\n\t        )\n\t        self.CreateConversation = channel.unary_stream(\n\t            \"/peachdb.BotService/CreateConversation\",\n\t            request_serializer=api__pb2.CreateConversationRequest.SerializeToString,\n\t            response_deserializer=api__pb2.CreateConversationResponse.FromString,\n", "        )\n\t        self.ContinueConversation = channel.unary_stream(\n\t            \"/peachdb.BotService/ContinueConversation\",\n\t            request_serializer=api__pb2.ContinueConversationRequest.SerializeToString,\n\t            response_deserializer=api__pb2.ContinueConversationResponse.FromString,\n\t        )\n\tclass BotServiceServicer(object):\n\t    \"\"\"The service definition\"\"\"\n\t    def CreateBot(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n", "        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\t    def CreateConversation(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\t    def ContinueConversation(self, request, context):\n\t        \"\"\"Missing associated documentation comment in .proto file.\"\"\"\n", "        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\tdef add_BotServiceServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n\t        \"CreateBot\": grpc.unary_unary_rpc_method_handler(\n\t            servicer.CreateBot,\n\t            request_deserializer=api__pb2.CreateBotRequest.FromString,\n\t            response_serializer=api__pb2.CreateBotResponse.SerializeToString,\n\t        ),\n", "        \"CreateConversation\": grpc.unary_stream_rpc_method_handler(\n\t            servicer.CreateConversation,\n\t            request_deserializer=api__pb2.CreateConversationRequest.FromString,\n\t            response_serializer=api__pb2.CreateConversationResponse.SerializeToString,\n\t        ),\n\t        \"ContinueConversation\": grpc.unary_stream_rpc_method_handler(\n\t            servicer.ContinueConversation,\n\t            request_deserializer=api__pb2.ContinueConversationRequest.FromString,\n\t            response_serializer=api__pb2.ContinueConversationResponse.SerializeToString,\n\t        ),\n", "    }\n\t    generic_handler = grpc.method_handlers_generic_handler(\"peachdb.BotService\", rpc_method_handlers)\n\t    server.add_generic_rpc_handlers((generic_handler,))\n\t# This class is part of an EXPERIMENTAL API.\n\tclass BotService(object):\n\t    \"\"\"The service definition\"\"\"\n\t    @staticmethod\n\t    def CreateBot(\n\t        request,\n\t        target,\n", "        options=(),\n\t        channel_credentials=None,\n\t        call_credentials=None,\n\t        insecure=False,\n\t        compression=None,\n\t        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n\t    ):\n\t        return grpc.experimental.unary_unary(\n", "            request,\n\t            target,\n\t            \"/peachdb.BotService/CreateBot\",\n\t            api__pb2.CreateBotRequest.SerializeToString,\n\t            api__pb2.CreateBotResponse.FromString,\n\t            options,\n\t            channel_credentials,\n\t            insecure,\n\t            call_credentials,\n\t            compression,\n", "            wait_for_ready,\n\t            timeout,\n\t            metadata,\n\t        )\n\t    @staticmethod\n\t    def CreateConversation(\n\t        request,\n\t        target,\n\t        options=(),\n\t        channel_credentials=None,\n", "        call_credentials=None,\n\t        insecure=False,\n\t        compression=None,\n\t        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n\t    ):\n\t        return grpc.experimental.unary_stream(\n\t            request,\n\t            target,\n", "            \"/peachdb.BotService/CreateConversation\",\n\t            api__pb2.CreateConversationRequest.SerializeToString,\n\t            api__pb2.CreateConversationResponse.FromString,\n\t            options,\n\t            channel_credentials,\n\t            insecure,\n\t            call_credentials,\n\t            compression,\n\t            wait_for_ready,\n\t            timeout,\n", "            metadata,\n\t        )\n\t    @staticmethod\n\t    def ContinueConversation(\n\t        request,\n\t        target,\n\t        options=(),\n\t        channel_credentials=None,\n\t        call_credentials=None,\n\t        insecure=False,\n", "        compression=None,\n\t        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n\t    ):\n\t        return grpc.experimental.unary_stream(\n\t            request,\n\t            target,\n\t            \"/peachdb.BotService/ContinueConversation\",\n\t            api__pb2.ContinueConversationRequest.SerializeToString,\n", "            api__pb2.ContinueConversationResponse.FromString,\n\t            options,\n\t            channel_credentials,\n\t            insecure,\n\t            call_credentials,\n\t            compression,\n\t            wait_for_ready,\n\t            timeout,\n\t            metadata,\n\t        )\n"]}
{"filename": "peachdb_grpc/test_api_client.py", "chunked_list": ["import asyncio\n\timport api_pb2  # type: ignore\n\timport api_pb2_grpc  # type: ignore\n\timport grpc  # type: ignore\n\tdef create_bot(stub: api_pb2_grpc.BotServiceStub):\n\t    repsonse = stub.CreateBot(\n\t        api_pb2.CreateBotRequest(\n\t            bot_id=\"grpc_test_4\", documents=[\"Hello\", \"World\"], system_prompt=\"Answer questions about this document.\"\n\t        )\n\t    )\n", "    return repsonse\n\tasync def create_conversation(stub: api_pb2_grpc.BotServiceStub):\n\t    responses = stub.CreateConversation(\n\t        api_pb2.CreateConversationRequest(bot_id=\"grpc_test_4\", query=\"What is this document about?\")\n\t    )\n\t    async for response in responses:\n\t        print(\"Create conversation response:\")\n\t        print(response)\n\tasync def continue_conversation(stub: api_pb2_grpc.BotServiceStub):\n\t    responses = stub.ContinueConversation(\n", "        api_pb2.ContinueConversationRequest(\n\t            bot_id=\"grpc_test_4\",\n\t            conversation_id=\"335700eb-0b4b-4094-a317-f03efe71e09e\",\n\t            query=\"What is this document about?\",\n\t        )\n\t    )\n\t    async for response in responses:\n\t        # TODO: make streamable given openAI.\n\t        print(\"Continue conversation response:\")\n\t        print(response)\n", "async def main() -> None:\n\t    async with grpc.aio.insecure_channel(\"localhost:50051\") as channel:\n\t        # async with grpc.aio.insecure_channel(\"1.tcp.ngrok.io:24448\") as channel:\n\t        stub = api_pb2_grpc.BotServiceStub(channel)\n\t        # print(await create_bot(stub))\n\t        # await create_conversation(stub)\n\t        await continue_conversation(stub)\n\t        # await asyncio.gather(\n\t        #     create_conversation(stub),\n\t        #     continue_conversation(stub)\n", "        # )\n\tif __name__ == \"__main__\":\n\t    asyncio.run(main())\n"]}
{"filename": "peachdb/validators.py", "chunked_list": ["import os\n\tfrom typing import Optional\n\timport numpy as np\n\timport pandas as pd\n\tfrom peachdb.embedder.utils import S3File, is_s3_uri\n\tdef validate_embedding_generator(engine: str):\n\t    supported_engines = [\"sentence_transformer_L12\", \"imagebind\", \"openai_ada\"]\n\t    assert (\n\t        engine in supported_engines\n\t    ), f\"Unsupported embedding generator. Currently supported engines are: {', '.join(supported_engines)}\"\n", "def validate_distance_metric(metric: str):\n\t    supported_metrics = [\"l2\", \"cosine\"]\n\t    assert (\n\t        metric in supported_metrics\n\t    ), f\"Unsupported distance metric. The metric should be one of the following: {', '.join(supported_metrics)}\"\n\tdef validate_embedding_backend(backend: str):\n\t    supported_backends = [\"exact_cpu\", \"exact_gpu\", \"approx\"]\n\t    assert (\n\t        backend in supported_backends\n\t    ), f\"Unsupported embedding backend. The backend should be one of the following: {', '.join(supported_backends)}\"\n", "def validate_csv_path(csv_path: str):\n\t    assert csv_path, \"csv_path parameter is missing. Please provide a valid local file path or an S3 URI.\"\n\t    # TODO: in case of S3 URI, check if the URI exists\n\t    if not is_s3_uri(csv_path):\n\t        assert os.path.exists(\n\t            csv_path\n\t        ), f\"The provided csv_path '{csv_path}' does not exist. Please ensure that the path is either a valid local file path or an S3 URI (e.g., s3://path/to/csv).\"\n\tdef validate_columns(column_to_embed: str, id_column_name: str, csv_path: str):\n\t    assert column_to_embed\n\t    assert id_column_name\n", "    def _check(data: pd.DataFrame):\n\t        assert column_to_embed in data.columns, f\"column_to_embed parameter is missing in {data.columns}\"\n\t        assert id_column_name in data.columns, f\"id_column_name parameter is missing in {data.columns}\"\n\t        try:\n\t            ids = np.array(data[id_column_name].values.tolist()).astype(\"int64\")\n\t        except:\n\t            print(\"[red]Only INTEGER datatype is supported for id column right now[/]\")\n\t    if is_s3_uri(csv_path):\n\t        with S3File(csv_path) as downloaded_csv:\n\t            data = pd.read_csv(downloaded_csv)\n", "            _check(data)\n\t    else:\n\t        data = pd.read_csv(csv_path)\n\t        _check(data)\n"]}
{"filename": "peachdb/__init__.py", "chunked_list": ["\"\"\"\n\tPeachDB Library\n\t\"\"\"\n\timport abc\n\timport datetime\n\timport os\n\timport shelve\n\timport shutil\n\tfrom typing import List, Optional, Tuple\n\timport numpy as np\n", "import pandas as pd\n\timport uvicorn\n\tfrom fastapi import FastAPI\n\tfrom fastapi.middleware.cors import CORSMiddleware\n\tfrom fastapi.requests import Request\n\tfrom pydantic import BaseModel\n\tfrom pyngrok import ngrok  # type: ignore\n\tfrom rich import print\n\tfrom rich.prompt import Prompt\n\timport peachdb.utils\n", "from peachdb.backends import get_backend\n\tfrom peachdb.backends.backend_base import BackendBase\n\tfrom peachdb.backends.numpy_backend import NumpyBackend\n\tfrom peachdb.constants import BLOB_STORE, SHELVE_DB\n\tfrom peachdb.embedder import EmbeddingProcessor\n\tfrom peachdb.embedder.utils import Modality, is_s3_uri\n\tfrom peachdb.validators import (\n\t    validate_columns,\n\t    validate_csv_path,\n\t    validate_distance_metric,\n", "    validate_embedding_backend,\n\t    validate_embedding_generator,\n\t)\n\tclass QueryResponse(BaseModel):\n\t    ids: list\n\t    distances: list\n\t    metadata: List[dict]\n\tclass _Base(abc.ABC):\n\t    @abc.abstractmethod\n\t    def query(\n", "        self,\n\t        query_input: str,\n\t        modality: Modality,\n\t        namespace: Optional[str],\n\t        store_modality: Optional[Modality] = None,\n\t        top_k: int = 5,\n\t    ):\n\t        pass\n\tclass EmptyNamespace(ValueError):\n\t    pass\n", "class PeachDB(_Base):\n\t    def __init__(\n\t        self,\n\t        project_name,\n\t        embedding_generator: str = \"sentence_transformer_L12\",\n\t        distance_metric: str = \"cosine\",\n\t        embedding_backend: str = \"exact_cpu\",\n\t    ):\n\t        validate_embedding_generator(embedding_generator)\n\t        validate_distance_metric(distance_metric)\n", "        validate_embedding_backend(embedding_backend)\n\t        super().__init__()\n\t        self._project_name = project_name\n\t        self._embedding_generator = embedding_generator\n\t        self._distance_metric = distance_metric\n\t        self._embedding_backend = embedding_backend\n\t        with shelve.open(SHELVE_DB) as shelve_db:\n\t            if self._project_name in shelve_db.keys():\n\t                assert set(shelve_db[self._project_name].keys()) == set(\n\t                    [\n", "                        \"embedding_generator\",\n\t                        \"exp_compound_csv_path\",\n\t                        \"query_logs\",\n\t                        \"upsertion_logs\",\n\t                        \"distance_metric\",\n\t                        \"embedding_backend\",\n\t                        \"lock\",\n\t                        \"init_logs\",\n\t                    ]\n\t                ), \"The project name already exists but the data is corrupted. Please delete the project and try again.\"\n", "                project_info = shelve_db[self._project_name]\n\t                project_info[\"init_logs\"].append({\"time\": datetime.datetime.now()})\n\t                shelve_db[project_name] = project_info\n\t            else:\n\t                shelve_db[project_name] = {\n\t                    \"embedding_generator\": embedding_generator,\n\t                    \"exp_compound_csv_path\": os.path.join(BLOB_STORE, project_name, \"exp_compound.csv\"),\n\t                    \"query_logs\": [],\n\t                    \"upsertion_logs\": [],\n\t                    \"distance_metric\": distance_metric,\n", "                    \"embedding_backend\": embedding_backend,\n\t                    \"lock\": False,\n\t                    \"init_logs\": [{\"time\": datetime.datetime.now()}],\n\t                }\n\t                print(f\"[u]PeachDB has been created for project: [bold green]{project_name}[/bold green][/u]\")\n\t        self._db: Optional[BackendBase] = None\n\t    def deploy(self):\n\t        if self._db is None:\n\t            self._get_db_backend()\n\t        app = FastAPI()\n", "        app.add_middleware(\n\t            CORSMiddleware,\n\t            allow_origins=[\"*\"],\n\t            allow_credentials=True,\n\t            allow_methods=[\"*\"],\n\t            allow_headers=[\"*\"],\n\t        )\n\t        @app.get(\"/query\", response_model=QueryResponse)\n\t        async def query_handler(query_input: str, modality: str | Modality, top_k: int = 5):\n\t            if isinstance(modality, str):\n", "                modality = Modality(modality)\n\t            ids, distances, metadata = self.query(query_input, modality=modality, top_k=top_k)\n\t            return {\n\t                \"ids\": ids.tolist(),\n\t                \"distances\": distances.tolist(),\n\t                \"metadata\": metadata.to_dict(orient=\"records\"),\n\t            }\n\t        port = 8000\n\t        url = ngrok.connect(port)\n\t        print(f\"[green]Public URL: {url}[/green]\")\n", "        try:\n\t            uvicorn.run(app, host=\"0.0.0.0\", port=port)\n\t        except KeyboardInterrupt:\n\t            self._db.cleanup()\n\t    def query(\n\t        self,\n\t        query_input: str,\n\t        modality: str | Modality,\n\t        namespace: Optional[str] = None,\n\t        store_modality: Optional[Modality] = None,\n", "        top_k: int = 5,\n\t    ) -> Tuple[np.ndarray, np.ndarray, pd.DataFrame]:\n\t        assert query_input and modality\n\t        with shelve.open(SHELVE_DB) as shelve_db:\n\t            project_info = shelve_db[self._project_name]\n\t            assert not project_info[\"lock\"], \"Please wait for the upsertion to finish before querying.\"\n\t        # TODO: add query logs.\n\t        if isinstance(modality, str):\n\t            modality = Modality(modality)\n\t        self._db = self._get_db_backend(namespace)\n", "        # was originally doing below, but now we just instantiate a new backend which loads everything into memory.\n\t        # # check insertion logs for any new upsertion, and download locally\n\t        # self._db.download_data_for_new_upsertions(project_info[\"upsertion_logs\"])\n\t        assert isinstance(self._db, NumpyBackend), \"Only NumpyBackend is supported for now.\"\n\t        ids, distances = self._db.process_query(query=query_input, top_k=top_k, modality=modality)\n\t        metadata = self._db.fetch_metadata(ids, namespace=namespace)\n\t        return ids, distances, metadata\n\t    # TODO: handle store_modality\n\t    def _get_db_backend(self, namespace: Optional[str] = None, store_modality: Optional[Modality] = None):\n\t        with shelve.open(SHELVE_DB) as shelve_db:\n", "            project_info = shelve_db[self._project_name]\n\t            metadata_path = project_info[\"exp_compound_csv_path\"]\n\t            upsertions_namespace = [x for x in project_info[\"upsertion_logs\"] if x[\"namespace\"] == namespace]\n\t            if len(upsertions_namespace) < 1:\n\t                raise EmptyNamespace(\"No embeddings in this namespace! Please upsert data before running your query\")\n\t            upsertion_embedding_dirs = [x[\"embeddings_dir\"] for x in upsertions_namespace]\n\t            assert (\n\t                len(set(upsertion_embedding_dirs)) == 1\n\t            ), \"All upsertions in a namespace must have the same embeddings_dir\"\n\t            last_upsertion = upsertions_namespace[-1]\n", "        embeddings_dir = last_upsertion[\"embeddings_dir\"]\n\t        id_column_name = last_upsertion[\"id_column_name\"]\n\t        # TODO: fix if we have multiple modalities stored. (#multi-modality)\n\t        store_modality = store_modality if store_modality is not None else Modality(last_upsertion[\"modality\"])\n\t        return get_backend(\n\t            embedding_generator=self._embedding_generator,\n\t            embedding_backend=self._embedding_backend,\n\t            distance_metric=self._distance_metric,\n\t            embeddings_dir=embeddings_dir,\n\t            metadata_path=metadata_path,\n", "            id_column_name=id_column_name,\n\t            modality=store_modality,\n\t        )\n\t    def _upsert(\n\t        self,\n\t        csv_path: str,\n\t        column_to_embed: str,\n\t        id_column_name: str,\n\t        modality: Modality,\n\t        namespace: Optional[str] = None,\n", "        embeddings_output_s3_bucket_uri: Optional[str] = None,\n\t        max_rows: Optional[int] = None,\n\t    ):\n\t        validate_csv_path(csv_path)\n\t        validate_columns(column_to_embed, id_column_name, csv_path)\n\t        if is_s3_uri(csv_path):\n\t            assert (\n\t                embeddings_output_s3_bucket_uri\n\t            ), \"Please provide `embeddings_output_s3_bucket_uri` for output embeddings when the `csv_path` is an S3 URI.\"\n\t            assert is_s3_uri(\n", "                embeddings_output_s3_bucket_uri\n\t            ), f\"The provided output_bucket_s3_uri {embeddings_output_s3_bucket_uri} is not a S3 URI\"\n\t        shelve_db = shelve.open(SHELVE_DB)\n\t        processor = EmbeddingProcessor(\n\t            csv_path=csv_path,\n\t            column_to_embed=column_to_embed,\n\t            id_column_name=id_column_name,\n\t            max_rows=max_rows,\n\t            embedding_model_name=self._embedding_generator,\n\t            project_name=self._project_name,\n", "            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n\t            modality=modality,\n\t            namespace=namespace,\n\t        )\n\t        processor.process()\n\t        _save = shelve_db[self._project_name]\n\t        _save[\"upsertion_logs\"].append(\n\t            {\n\t                \"embeddings_dir\": processor.embeddings_output_dir,\n\t                \"csv_path\": csv_path,\n", "                \"column_to_embed\": column_to_embed,\n\t                \"id_column_name\": id_column_name,\n\t                \"max_rows\": max_rows,\n\t                \"embeddings_output_s3_bucket_uri\": embeddings_output_s3_bucket_uri,\n\t                \"modality\": modality.value,\n\t                \"namespace\": namespace,\n\t            }\n\t        )\n\t        shelve_db[self._project_name] = _save\n\t        shelve_db.close()\n", "        peachdb.utils.sync_cache_dir_s3()\n\t    def upsert_text(\n\t        self,\n\t        csv_path: str,\n\t        column_to_embed: str,\n\t        id_column_name: str,\n\t        namespace: Optional[str] = None,\n\t        embeddings_output_s3_bucket_uri: Optional[str] = None,\n\t        max_rows: Optional[int] = None,\n\t    ):\n", "        self._upsert(\n\t            csv_path=csv_path,\n\t            column_to_embed=column_to_embed,\n\t            id_column_name=id_column_name,\n\t            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n\t            modality=Modality.TEXT,\n\t            namespace=namespace,\n\t            max_rows=max_rows,\n\t        )\n\t    def upsert_audio(\n", "        self,\n\t        csv_path: str,\n\t        column_to_embed: str,\n\t        id_column_name: str,\n\t        embeddings_output_s3_bucket_uri: Optional[str] = None,\n\t        max_rows: Optional[int] = None,\n\t    ):\n\t        self._upsert(\n\t            csv_path=csv_path,\n\t            column_to_embed=column_to_embed,\n", "            id_column_name=id_column_name,\n\t            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n\t            max_rows=max_rows,\n\t            modality=Modality.AUDIO,\n\t        )\n\t    def upsert_image(\n\t        self,\n\t        csv_path: str,\n\t        column_to_embed: str,\n\t        id_column_name: str,\n", "        embeddings_output_s3_bucket_uri: Optional[str] = None,\n\t        max_rows: Optional[int] = None,\n\t    ):\n\t        self._upsert(\n\t            csv_path=csv_path,\n\t            column_to_embed=column_to_embed,\n\t            id_column_name=id_column_name,\n\t            embeddings_output_s3_bucket_uri=embeddings_output_s3_bucket_uri,\n\t            max_rows=max_rows,\n\t            modality=Modality.IMAGE,\n", "        )\n\t    @staticmethod\n\t    def delete(project_name: str):\n\t        db = shelve.open(SHELVE_DB)\n\t        if project_name not in db.keys():\n\t            print(f\"Project: {project_name} does not exist.\")\n\t            return\n\t        answer = Prompt.ask(f\"[bold red]Would you like to delete the project: {project_name}? (y/N)[/]\")\n\t        delete_project = answer.lower() == \"y\"\n\t        if delete_project:\n", "            del db[project_name]\n\t            project_dir = os.path.join(BLOB_STORE, project_name)\n\t            if os.path.exists(project_dir):\n\t                shutil.rmtree(project_dir)\n\t        print(f\"[green]Successfully deleted project: {project_name}[/]\")\n\t    @staticmethod\n\t    def _ensure_unique_project_name(project_name: str):\n\t        with shelve.open(SHELVE_DB) as db:\n\t            assert (\n\t                project_name not in db.keys()\n", "            ), f\"The project name '{project_name}' already exists. Please choose a unique name.\"\n"]}
{"filename": "peachdb/utils.py", "chunked_list": ["import subprocess\n\tfrom peachdb.constants import _DISK_CACHE_DIR\n\tdef sync_cache_dir_s3():\n\t    subprocess.Popen([\"aws\", \"s3\", \"sync\", _DISK_CACHE_DIR, \"s3://metavoice-vector-db/peachdb-cache/\"])\n"]}
{"filename": "peachdb/constants.py", "chunked_list": ["import os\n\t_USER_DIR = os.path.expanduser(\"~/\")\n\t_DISK_CACHE_DIR = os.path.join(_USER_DIR, \".peachdb\")\n\tos.makedirs(_DISK_CACHE_DIR, exist_ok=True)\n\tSHELVE_DB = f\"{_DISK_CACHE_DIR}/db\"\n\tBLOB_STORE = f\"{_DISK_CACHE_DIR}/blobs\"\n\tCACHED_REQUIREMENTS_TXT = f\"{_DISK_CACHE_DIR}/requirements.txt\"\n\tBOTS_DB = f\"{_DISK_CACHE_DIR}/bots.db\"\n\tCONVERSATIONS_DB = f\"{_DISK_CACHE_DIR}/conversations.db\"\n\tGIT_REQUIREMENTS_TXT = \"https://raw.githubusercontent.com/peach-db/peachdb/master/requirements.txt\"\n"]}
{"filename": "peachdb/bots/qa.py", "chunked_list": ["import dotenv\n\tdotenv.load_dotenv()\n\timport shelve\n\timport tempfile\n\tfrom typing import Iterator, Optional, Union\n\tfrom uuid import uuid4\n\timport openai\n\timport pandas as pd\n\tfrom peachdb import PeachDB\n\tfrom peachdb.constants import BOTS_DB, CONVERSATIONS_DB, SHELVE_DB\n", "class ConversationNotFoundError(ValueError):\n\t    pass\n\tclass UnexpectedGPTRoleResponse(ValueError):\n\t    pass\n\tdef _validate_embedding_model(embedding_model: str):\n\t    assert embedding_model in [\"openai_ada\"]\n\tdef _validate_llm_model(llm_model):\n\t    assert llm_model in [\"gpt-3.5-turbo\", \"gpt-4\"]\n\tdef _process_input_data(namespace, ids: list[str], texts: list[str], metadatas_dict) -> pd.DataFrame:\n\t    assert all(isinstance(text, str) for text in texts), \"All texts must be strings\"\n", "    assert all(isinstance(i, str) for i in ids), \"All IDs must be strings\"\n\t    if metadatas_dict is not None:\n\t        assert all(isinstance(m, dict) for m in metadatas_dict), \"All metadata must be dicts\"\n\t        assert len(set([str(x.keys()) for x in metadatas_dict])) == 1, \"All metadata must have the same keys\"\n\t        # convert metadata from input format to one we can create a dataframe from.\n\t        metadatas_dict = {key: [metadata[key] for metadata in metadatas_dict] for key in metadatas_dict[0].keys()}\n\t        assert \"texts\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'texts'\"\n\t        assert \"ids\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'ids'\"\n\t        if namespace is not None:\n\t            assert \"namespace\" not in metadatas_dict.keys(), \"Metadata cannot contain a key called 'namespace'\"\n", "    else:\n\t        metadatas_dict = {}\n\t    if namespace is None:\n\t        metadatas_dict[\"namespace\"] = [None] * len(ids)\n\t    else:\n\t        metadatas_dict[\"namespace\"] = [namespace] * len(ids)\n\t    df = pd.DataFrame(\n\t        data={\n\t            \"ids\": ids,\n\t            \"texts\": texts,\n", "            **metadatas_dict,\n\t        }\n\t    )\n\t    return df\n\tdef _peachdb_upsert_wrapper(peach_db_instance, peach_db_project_name: str, namespace, ids, texts, metadatas_dict):\n\t    new_data_df = _process_input_data(namespace, ids, texts, metadatas_dict)\n\t    with tempfile.NamedTemporaryFile(suffix=f\"{uuid4()}.csv\") as tmp:\n\t        new_data_df.to_csv(tmp.name, index=False)  # TODO: check it won't cause an override.\n\t        peach_db_instance.upsert_text(\n\t            csv_path=tmp.name,\n", "            column_to_embed=\"texts\",\n\t            id_column_name=\"ids\",\n\t            # TODO: below is manually set, this might cause issues!\n\t            embeddings_output_s3_bucket_uri=\"s3://metavoice-vector-db/deployed_solution/\",\n\t            namespace=namespace,\n\t        )\n\t    with shelve.open(SHELVE_DB) as db:\n\t        project_info = db[peach_db_project_name]\n\t        new_data_df.to_csv(project_info[\"exp_compound_csv_path\"], index=False)\n\t    return True\n", "class BadBotInputError(ValueError):\n\t    pass\n\tclass QABot:\n\t    def __init__(\n\t        self,\n\t        bot_id: str,\n\t        embedding_model: Optional[str] = None,\n\t        llm_model_name: Optional[str] = None,\n\t        system_prompt: Optional[str] = None,\n\t    ):\n", "        with shelve.open(BOTS_DB) as db:\n\t            if bot_id in db:\n\t                if system_prompt is not None:\n\t                    raise BadBotInputError(\n\t                        \"System prompt cannot be changed for existing bot. Maybe you want to create a new bot?\"\n\t                    )\n\t                if embedding_model is not None:\n\t                    raise BadBotInputError(\n\t                        \"Embedding model cannot be changed for existing bot. Maybe you want to create a new bot?\"\n\t                    )\n", "                if llm_model_name is not None:\n\t                    raise BadBotInputError(\n\t                        \"LLM model cannot be changed for existing bot. Maybe you want to create a new bot?\"\n\t                    )\n\t                self._peachdb_project_id = db[bot_id][\"peachdb_project_id\"]\n\t                self._embedding_model = db[bot_id][\"embedding_model\"]\n\t                self._llm_model_name = db[bot_id][\"llm_model_name\"]\n\t                self._system_prompt = db[bot_id][\"system_prompt\"]\n\t            else:\n\t                if system_prompt is None:\n", "                    raise BadBotInputError(\"System prompt must be specified for new bot.\")\n\t                if embedding_model is None:\n\t                    raise BadBotInputError(\"Embedding model must be specified for new bot.\")\n\t                if llm_model_name is None:\n\t                    raise BadBotInputError(\"LLM model must be specified for new bot.\")\n\t                self._peachdb_project_id = f\"{uuid4()}_{bot_id}\"\n\t                self._embedding_model = embedding_model\n\t                self._llm_model_name = llm_model_name\n\t                self._system_prompt = system_prompt\n\t                db[bot_id] = {\n", "                    \"peachdb_project_id\": self._peachdb_project_id,\n\t                    \"embedding_model\": self._embedding_model,\n\t                    \"llm_model_name\": self._llm_model_name,\n\t                    \"system_prompt\": self._system_prompt,\n\t                }\n\t        _validate_embedding_model(self._embedding_model)\n\t        _validate_llm_model(self._llm_model_name)\n\t        self.peach_db = PeachDB(\n\t            project_name=self._peachdb_project_id,\n\t            embedding_generator=self._embedding_model,\n", "        )\n\t        if self._llm_model_name in [\"gpt-3.5-turbo\", \"gpt-4\"]:\n\t            self._llm_model = lambda messages, stream: openai.ChatCompletion.create(\n\t                messages=[{\"role\": \"system\", \"content\": self._system_prompt}] + messages,\n\t                model=self._llm_model_name,\n\t                stream=stream,\n\t            )\n\t        else:\n\t            raise ValueError(f\"Unknown/Unsupported LLM model: {self._llm_model_name}\")\n\t    def add_data(self, documents: list[str]):\n", "        _peachdb_upsert_wrapper(\n\t            peach_db_instance=self.peach_db,\n\t            peach_db_project_name=self._peachdb_project_id,\n\t            namespace=None,\n\t            ids=[str(i) for i in range(len(documents))],\n\t            texts=documents,\n\t            metadatas_dict=None,\n\t        )\n\t    def _llm_response(\n\t        self, conversation_id: str, messages: list[dict[str, str]], stream: bool = False\n", "    ) -> Iterator[tuple[str, str]]:\n\t        \"\"\"\n\t        Responds to the given messages with the LLM model. Additionally, it appends to the shelve db the current conversation (After response has been returned from GPT).\n\t        \"\"\"\n\t        response = self._llm_model(messages=messages, stream=stream)\n\t        if stream:\n\t            response_str = \"\"\n\t            for resp in response:\n\t                delta = resp.choices[0].delta\n\t                if \"role\" in delta:\n", "                    if delta.role != \"assistant\":\n\t                        raise UnexpectedGPTRoleResponse(f\"Expected assistant response, got {delta.role} response.\")\n\t                if \"content\" in delta:\n\t                    response_str += delta[\"content\"]\n\t                    yield conversation_id, delta[\"content\"]\n\t                # keep updating shelve with current conversation.\n\t                with shelve.open(CONVERSATIONS_DB) as db:\n\t                    db[conversation_id] = messages + [{\"role\": \"assistant\", \"content\": response_str}]\n\t        else:\n\t            response_message = response[\"choices\"][0][\"message\"]\n", "            if response_message.role != \"assistant\":\n\t                raise UnexpectedGPTRoleResponse(f\"Expected assistant response, got {response_message.role} response.\")\n\t            with shelve.open(CONVERSATIONS_DB) as db:\n\t                db[conversation_id] = messages + [response_message]\n\t            yield conversation_id, response_message[\"content\"]\n\t    def _create_unique_conversation_id(self) -> str:\n\t        # get conversation id not in shelve.\n\t        id = str(uuid4())\n\t        with shelve.open(CONVERSATIONS_DB) as db:\n\t            while id in db:\n", "                id = str(uuid4())\n\t        return id\n\t    def create_conversation_with_query(\n\t        self, query: str, top_k: int = 3, stream: bool = False\n\t    ) -> Iterator[tuple[str, str]]:\n\t        _, _, context_metadata = self.peach_db.query(query, top_k=top_k, modality=\"text\")\n\t        assert \"texts\" in context_metadata\n\t        contextual_query = \"Use the below snippets to answer the subsequent questions. If the answer can't be found, write \\\"I don't know.\\\"\"\n\t        for text in context_metadata[\"texts\"]:\n\t            contextual_query += f\"\\n\\nSnippet:\\n{text}\"\n", "        contextual_query += f\"\\n\\nQuestion:{query}\"\n\t        # add context to query\n\t        messages = [\n\t            {\"role\": \"user\", \"content\": contextual_query},\n\t        ]\n\t        conversation_id = self._create_unique_conversation_id()\n\t        if stream:\n\t            for x in self._llm_response(conversation_id, messages, stream=True):\n\t                yield x\n\t        else:\n", "            for x in self._llm_response(conversation_id, messages, stream=False):\n\t                yield x\n\t    def continue_conversation_with_query(\n\t        self, conversation_id: str, query: str, top_k: int = 3, stream: bool = False\n\t    ) -> Iterator[str]:\n\t        with shelve.open(CONVERSATIONS_DB) as db:\n\t            if conversation_id not in db:\n\t                raise ConversationNotFoundError(\"Conversation ID not found.\")\n\t            messages = db[conversation_id]\n\t        messages.append({\"role\": \"user\", \"content\": query})\n", "        if stream:\n\t            for _, response in self._llm_response(conversation_id, messages, stream=True):\n\t                yield response\n\t        else:\n\t            for _, response in self._llm_response(conversation_id, messages, stream=False):\n\t                yield response\n"]}
{"filename": "peachdb/embedder/openai_ada.py", "chunked_list": ["from dotenv import load_dotenv\n\tload_dotenv()\n\timport os\n\timport openai\n\tif os.environ.get(\"OPENAI_API_KEY\") == None:\n\t    raise ValueError(\"OPENAI_API_KEY environment variable not set\")\n\tclass OpenAIAdaEmbedder:\n\t    def __init__(self):\n\t        self.embedder = openai.Embedding\n\t    def calculate_embeddings(self, text_inputs: list) -> list:\n", "        response = self.embedder.create(input=text_inputs, model=\"text-embedding-ada-002\")\n\t        embeddings = [data[\"embedding\"] for data in response[\"data\"]]\n\t        return embeddings\n\tif __name__ == \"__main__\":\n\t    embedder = OpenAIAdaEmbedder()\n\t    import numpy as np\n\t    print(np.max(embedder.calculate_embeddings([\"hello\"] * 5), axis=1))\n\t    import time\n\t    time.sleep(10)\n\t    print(np.max(embedder.calculate_embeddings([\"hello\"] * 5), axis=1))\n"]}
{"filename": "peachdb/embedder/__init__.py", "chunked_list": ["import os\n\timport shutil\n\tfrom collections import namedtuple\n\tfrom typing import List, Optional, Type\n\timport duckdb\n\timport modal\n\timport pandas as pd\n\timport pyarrow as pa  # type: ignore\n\timport pyarrow.parquet as pq  # type: ignore\n\tfrom rich import print\n", "import peachdb.embedder.containers.base\n\tfrom peachdb.constants import BLOB_STORE\n\tfrom peachdb.embedder.utils import Modality, S3File, is_s3_uri\n\tChunk = namedtuple(\"Chunk\", [\"texts_or_paths\", \"ids\"])\n\t# TODO: split into two separate classes LocalEmbeddingsProcessor & S3EmbeddingsProcessor\n\tclass EmbeddingProcessor:\n\t    def __init__(\n\t        self,\n\t        csv_path: str,\n\t        column_to_embed: str,\n", "        id_column_name: str,\n\t        embedding_model_name: str,\n\t        project_name: str,\n\t        modality: Modality,\n\t        namespace: Optional[str] = None,\n\t        embeddings_output_s3_bucket_uri: Optional[str] = None,\n\t        max_rows: Optional[int] = None,\n\t    ):\n\t        self._csv_path = csv_path\n\t        self._column_to_embed = column_to_embed\n", "        self._id_column_name = id_column_name\n\t        self._embedding_model_name = embedding_model_name\n\t        self._max_rows = max_rows\n\t        self._project_name = project_name\n\t        self._embeddings_output_s3_bucket_uri = (\n\t            embeddings_output_s3_bucket_uri.strip(\"/\") + \"/\" if embeddings_output_s3_bucket_uri else None\n\t        )\n\t        self._namespace = namespace\n\t        self._modality = modality\n\t        if self._embedding_model_name == \"sentence_transformer_L12\":\n", "            from peachdb.embedder.containers.sentence_transformer import SentenceTransformerEmbedder, sbert_stub\n\t            self._embedding_model: Optional[\n\t                Type[peachdb.embedder.containers.base.EmbeddingModelBase]\n\t            ] = SentenceTransformerEmbedder\n\t            self._embedding_model_stub: Optional[modal.Stub] = sbert_stub\n\t            self._embedding_model_chunk_size = 10000\n\t        elif self._embedding_model_name == \"imagebind\":\n\t            from peachdb.embedder.containers.multimodal_imagebind import ImageBindEmbdedder, imagebind_stub\n\t            self._embedding_model = ImageBindEmbdedder\n\t            self._embedding_model_stub = imagebind_stub\n", "            self._embedding_model_chunk_size = 1000\n\t        elif self._embedding_model_name == \"openai_ada\":\n\t            from peachdb.embedder.openai_ada import OpenAIAdaEmbedder\n\t            self._embedding_model = None\n\t            self._embedding_model_stub = None\n\t            # TODO: use this\n\t            self._embedding_model_chunk_size = 1000\n\t            self._openai_ada_embedder = OpenAIAdaEmbedder()\n\t        else:\n\t            raise ValueError(f\"Invalid embedding model name: {self._embedding_model_name}\")\n", "    @property\n\t    def embeddings_output_dir(self):\n\t        if is_s3_uri(self._csv_path):\n\t            return f\"{self._embeddings_output_s3_bucket_uri}{self._project_name}/embeddings/{self._namespace}\"\n\t        dir = f\"{BLOB_STORE}/{self._project_name}/embeddings/{self._namespace}\"\n\t        os.makedirs(dir, exist_ok=True)\n\t        return dir\n\t    def process(self):\n\t        dataset = self._download_and_read_dataset(self._csv_path)\n\t        print(\"[bold]Chunking data into batches...[/bold]\")\n", "        chunked_data = self._chunk_data(dataset)\n\t        print(\"[bold]Running embedding model on each chunk in parallel...[/bold]\")\n\t        self._run_model(chunked_data)\n\t    def _download_and_read_dataset(self, csv_path: str) -> str:\n\t        \"\"\"Supports a local/s3 reference to the csv formatted dataset\"\"\"\n\t        if not is_s3_uri(self._csv_path):\n\t            print(\"[bold]Loading data into memory...[/bold]\")\n\t            # local ref has been provided. make a copy within .peachdb for persistence\n\t            project_blob_dir = f\"{BLOB_STORE}/{self._project_name}\"\n\t            os.makedirs(project_blob_dir, exist_ok=True)\n", "            fname = self._csv_path.split(\"/\")[-1]\n\t            dataset_path = f\"{project_blob_dir}/{self._namespace}_{fname}\"\n\t            shutil.copy2(self._csv_path, dataset_path)\n\t            return self._read_dataset(dataset_path)\n\t        else:\n\t            print(\"[bold]Downloading data from S3...[/bold]\")\n\t            with S3File(self._csv_path) as downloaded_dataset:\n\t                print(\"[bold]Loading data into memory...[/bold]\")\n\t                return self._read_dataset(downloaded_dataset)\n\t    def _read_dataset(self, dataset_path: str):\n", "        data = duckdb.read_csv(dataset_path, header=True)\n\t        sql_query = f\"SELECT {self._column_to_embed}, {self._id_column_name} FROM data\"\n\t        if self._max_rows:\n\t            sql_query += f\" LIMIT {self._max_rows}\"\n\t        return duckdb.sql(sql_query).fetchall()  # NOTE: takes 2 mins 10 seconds for a large dataset\n\t    def _chunk_data(self, fetched_data) -> List[Chunk]:\n\t        chunk_size = self._embedding_model_chunk_size\n\t        chunked_data = [fetched_data[i : i + chunk_size] for i in range(0, len(fetched_data), chunk_size)]\n\t        print(f\"[bold]...{len(fetched_data)} rows were split into {len(chunked_data)} chunks[/bold]\")\n\t        inputs = []\n", "        for chunk in chunked_data:\n\t            texts_or_paths = []\n\t            ids = []\n\t            for text_or_path, id in chunk:\n\t                texts_or_paths.append(text_or_path)\n\t                ids.append(str(id))\n\t            inputs += [Chunk(texts_or_paths, ids)]\n\t        return inputs\n\t    def _run_model(self, chunks: List[Chunk]):\n\t        if self._embedding_model_name == \"openai_ada\":\n", "            assert not is_s3_uri(self._csv_path)\n\t            assert self._modality == Modality.TEXT\n\t            for idx, chunk in enumerate(chunks):\n\t                embeddings_dict = {}\n\t                embeddings_dict[\"ids\"] = chunk.ids\n\t                embeddings_dict[\"text_embeddings\"] = self._openai_ada_embedder.calculate_embeddings(\n\t                    chunk.texts_or_paths\n\t                )\n\t                df = pd.DataFrame(embeddings_dict)\n\t                result = pa.Table.from_pandas(df)\n", "                fname = self._csv_path.split(\"/\")[-1].split(\".\")[0]\n\t                pq.write_table(\n\t                    result, f\"{self.embeddings_output_dir}/{fname}_{idx}_{self._embedding_model_name}.parquet\"\n\t                )\n\t        else:\n\t            assert self._embedding_model_stub is not None and self._embedding_model is not None\n\t            with self._embedding_model_stub.run():\n\t                st = self._embedding_model()\n\t                # NOTE: a unique fname here is being used to avoid conflicts in the stored data!\n\t                # Be careful about changing this logic.\n", "                fname = self._csv_path.split(\"/\")[-1].split(\".\")[0]\n\t                input_tuples = [\n\t                    # expected: (ids, output_path, texts, audio_paths, image_paths, show_progress)\n\t                    (\n\t                        chunk.ids,\n\t                        f\"{self.embeddings_output_dir}/{fname}_{idx}_{self._embedding_model_name}.parquet\",\n\t                        # TODO: enable support of using multiple modalities at the same time here (#multi-modality)\n\t                        chunk.texts_or_paths if self._modality == Modality.TEXT else None,\n\t                        chunk.texts_or_paths if self._modality == Modality.AUDIO else None,\n\t                        chunk.texts_or_paths if self._modality == Modality.IMAGE else None,\n", "                        True,\n\t                    )\n\t                    for idx, chunk in enumerate(chunks)\n\t                ]\n\t                results = list(st.calculate_embeddings.starmap(input_tuples))  # type: ignore\n\t                if not is_s3_uri(self._csv_path):\n\t                    for idx, result in enumerate(results):\n\t                        pq.write_table(\n\t                            result, f\"{self.embeddings_output_dir}/{fname}_{idx}_{self._embedding_model_name}.parquet\"\n\t                        )\n"]}
{"filename": "peachdb/embedder/utils.py", "chunked_list": ["import abc\n\timport concurrent.futures\n\timport datetime\n\timport logging\n\timport os\n\timport subprocess\n\timport tempfile\n\timport uuid\n\tfrom enum import Enum\n\tfrom functools import cache, wraps\n", "from types import SimpleNamespace\n\tfrom typing import List, Optional, Type, Union\n\timport tqdm  # type: ignore\n\tlogger = logging.getLogger(__name__)\n\tclass Modality(Enum):\n\t    TEXT = \"text\"\n\t    AUDIO = \"audio\"\n\t    IMAGE = \"image\"\n\tdef handle_s3_download_error(func):\n\t    @wraps(func)\n", "    def wrapper(*args, **kwargs):\n\t        try:\n\t            return func(*args, **kwargs)\n\t        except Exception as e:\n\t            message = f\"Failed to download {args[0]} from S3. Error: {str(e)}\"\n\t            logger.exception(message)\n\t            raise FileNotFoundError(message)\n\t    return wrapper\n\t@cache\n\tdef _verify_aws_cli_installed() -> None:\n", "    try:\n\t        subprocess.check_output([\"aws\", \"--version\"], stderr=subprocess.STDOUT)\n\t    except subprocess.CalledProcessError:\n\t        raise EnvironmentError(\"AWS CLI not installed. Please install it and try again.\")\n\tclass S3Entity(metaclass=abc.ABCMeta):\n\t    \"\"\"Abstract base class for S3 Entities\"\"\"\n\t    def __init__(self, s3_path: str):\n\t        _verify_aws_cli_installed()\n\t        self.s3_path = s3_path\n\t        self.temp_resource: Optional[Union[tempfile._TemporaryFileWrapper, tempfile.TemporaryDirectory]] = None\n", "    @abc.abstractmethod\n\t    def download(self):\n\t        pass\n\t    @abc.abstractmethod\n\t    def cleanup(self):\n\t        pass\n\t    def __enter__(self) -> str:\n\t        self.download()\n\t        assert self.temp_resource is not None\n\t        return self.temp_resource.name\n", "    def __exit__(self, exc_type, exc_val, exc_tb):\n\t        self.cleanup()\n\t    @handle_s3_download_error\n\t    def _download_from_s3(self, command: str) -> None:\n\t        assert self.temp_resource is not None\n\t        with subprocess.Popen(\n\t            [\"aws\", \"s3\", command, self.s3_path, self.temp_resource.name],\n\t            stdout=subprocess.PIPE,\n\t            stderr=subprocess.STDOUT,\n\t            bufsize=1,\n", "            universal_newlines=True,\n\t        ) as download_process:\n\t            assert download_process.stdout is not None\n\t            for line in download_process.stdout:\n\t                print(\"\\r\" + line.strip().ljust(120), end=\"\", flush=True)\n\t            print()\n\tclass S3Files(S3Entity):\n\t    def __init__(self, s3_paths: list[str]):\n\t        _verify_aws_cli_installed()\n\t        self.s3_paths = s3_paths\n", "        self.temp_resources: Optional[List[tempfile._TemporaryFileWrapper]] = None\n\t    @handle_s3_download_error\n\t    def copy_file(self, path, resource_name) -> None:\n\t        command = [\"aws\", \"s3\", \"cp\", path, resource_name.name]\n\t        with subprocess.Popen(\n\t            command,\n\t            stdout=subprocess.PIPE,\n\t            stderr=subprocess.STDOUT,\n\t            bufsize=1,\n\t            universal_newlines=True,\n", "        ) as download_process:\n\t            assert download_process.stdout is not None\n\t            for line in download_process.stdout:\n\t                pass\n\t    def _download_from_s3(self) -> None:\n\t        assert self.temp_resources is not None\n\t        print(\"CPU count: \", os.cpu_count())\n\t        futures = []\n\t        with concurrent.futures.ThreadPoolExecutor(max_workers=len(self.s3_paths)) as executor:\n\t            for path, resource in tqdm.tqdm(\n", "                zip(self.s3_paths, self.temp_resources), total=len(self.s3_paths), desc=\"Scheduling download threads\"\n\t            ):\n\t                futures.append(executor.submit(self.copy_file, path, resource))\n\t            for future in tqdm.tqdm(\n\t                concurrent.futures.as_completed(futures),\n\t                total=len(futures),\n\t                desc=\"Waiting for threads to finish\",\n\t            ):\n\t                future.result()\n\t    def cleanup(self):\n", "        [x.cleanup() for x in self.temp_resources]\n\t    def download(self) -> list[str]:\n\t        self.temp_resources = [tempfile.NamedTemporaryFile(delete=True) for _ in tqdm.tqdm(self.s3_paths)]\n\t        self._download_from_s3()\n\t        return [x.name for x in self.temp_resources]\n\tclass S3File(S3Entity):\n\t    \"\"\"Represents an S3 File\"\"\"\n\t    def download(self) -> str:\n\t        self.temp_resource = tempfile.NamedTemporaryFile(delete=True)\n\t        self._download_from_s3(\"cp\")\n", "        return self.temp_resource.name\n\t    def cleanup(self):\n\t        self.temp_resource.close()\n\tclass S3Folder(S3Entity):\n\t    \"\"\"Represents an S3 Folder\"\"\"\n\t    def download(self) -> str:\n\t        self.temp_resource = tempfile.TemporaryDirectory()\n\t        self._download_from_s3(\"sync\")\n\t        return self.temp_resource.name\n\t    def cleanup(self):\n", "        self.temp_resource.cleanup()\n\tdef create_unique_id() -> str:\n\t    now = datetime.datetime.now()\n\t    now_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n\t    return now_str + \"_\" + str(uuid.uuid4())\n\tdef create_unique_parquet_name() -> str:\n\t    return f\"{create_unique_id()}.parquet\"\n\tdef is_s3_uri(path: str) -> bool:\n\t    return \"s3://\" in path\n"]}
{"filename": "peachdb/embedder/containers/sentence_transformer.py", "chunked_list": ["from typing import Optional, Union\n\timport modal\n\timport pyarrow as pa  # type: ignore\n\tfrom peachdb.embedder.containers.base import EmbeddingModelBase, base_container_image, modal_compute_spec_decorator\n\tfrom peachdb.embedder.models.sentence_transformer import SentenceTransformerModel\n\tSENTENCE_TRANSFORMER_BATCH_SIZE = 64\n\tsbert_stub = modal.Stub(\"SBERT\")\n\tdef download_model():\n\t    SentenceTransformerModel.download_model()\n\timage = base_container_image.run_function(download_model)\n", "@modal_compute_spec_decorator(stub=sbert_stub, image=image)\n\tclass SentenceTransformerEmbedder(EmbeddingModelBase):\n\t    def __enter__(self):\n\t        self.model = SentenceTransformerModel()\n\t    def _calculate_text_embeddings(self, texts, show_progress_bar: bool):\n\t        return self.model.encode_texts(\n\t            texts,\n\t            batch_size=SENTENCE_TRANSFORMER_BATCH_SIZE,\n\t            show_progress_bar=show_progress_bar,\n\t        )\n", "    # TODO: are defining these functions necessary?\n\t    def _calculate_audio_embeddings(self, audio_paths, show_progress_bar: bool):\n\t        raise NotImplementedError\n\t    def _calculate_image_embeddings(self, image_paths: list, show_progress_bar: bool):\n\t        raise NotImplementedError\n\t    @property\n\t    def _can_take_text_input(cls):\n\t        return True\n\t    @property\n\t    def _can_take_audio_input(cls):\n", "        return False\n\t    @property\n\t    def _can_take_image_input(cls):\n\t        # return True - once implemented!\n\t        return False\n\t    # We need to rewrite this function in all the inherited class so we can use the @modal method decorator.\n\t    # TODO: check if above statement is true / if we can factor this out.\n\t    @modal.method()\n\t    def calculate_embeddings(  # type: ignore\n\t        self,\n", "        ids: list,\n\t        output_path: str,\n\t        texts: Optional[list] = None,\n\t        audio_paths: Optional[list] = None,\n\t        image_paths: Optional[list] = None,\n\t        show_progress_bar: bool = False,\n\t    ) -> Union[None, pa.Table]:\n\t        return super().calculate_embeddings(\n\t            ids=ids,\n\t            output_path=output_path,\n", "            texts=texts,\n\t            audio_paths=audio_paths,\n\t            image_paths=image_paths,\n\t            show_progress_bar=show_progress_bar,\n\t        )\n\t# We have a function here instead of putting it into __main__ so that `modal shell` works\n\t@sbert_stub.function(image=image)\n\tdef test(s3_bucket_path: str):\n\t    st = SentenceTransformerEmbedder()\n\t    embeddings = st.calculate_embeddings.call(\n", "        texts=[\"hello\", \"world\"],\n\t        ids=[1, 2],\n\t        output_path=s3_bucket_path,\n\t        show_progress_bar=True,\n\t    )\n\tif __name__ == \"__main__\":\n\t    # Run as \"python sentence_transformer.py s3://<bucket_name>/test_mainfn/\"\n\t    import sys\n\t    with sbert_stub.run():\n\t        test.call(sys.argv[1])\n"]}
{"filename": "peachdb/embedder/containers/multimodal_imagebind.py", "chunked_list": ["from typing import Optional, Union\n\timport modal\n\timport numpy as np\n\timport pyarrow as pa  # type: ignore\n\tfrom peachdb.embedder.containers.base import EmbeddingModelBase, base_container_image, modal_compute_spec_decorator\n\tfrom peachdb.embedder.models.multimodal_imagebind import ImageBindModel\n\tfrom peachdb.embedder.utils import S3File, is_s3_uri\n\tIMAGEBIND_BATCH_SIZE = 1024\n\timagebind_stub = modal.Stub(\"ImageBind\")\n\tdef download_model():\n", "    ImageBindModel.download_model()\n\timage = base_container_image.run_function(download_model)\n\t@modal_compute_spec_decorator(stub=imagebind_stub, image=image)\n\tclass ImageBindEmbdedder(EmbeddingModelBase):\n\t    def __enter__(self):\n\t        self.model = ImageBindModel()\n\t    def _calculate_text_embeddings(self, texts, show_progress_bar: bool) -> np.ndarray:\n\t        return self.model.encode_texts(texts, IMAGEBIND_BATCH_SIZE, show_progress_bar)\n\t    def _calculate_audio_embeddings(self, audio_paths, show_progress_bar: bool) -> np.ndarray:\n\t        # TODO: add handling of different batch sizes to EmbeddingProcessor. (#multi-modality)\n", "        return self.model.encode_audio(audio_paths, IMAGEBIND_BATCH_SIZE // 8, show_progress_bar)\n\t    def _calculate_image_embeddings(self, image_paths: list, show_progress_bar: bool) -> np.ndarray:\n\t        # TODO: add handling of different batch sizes to EmbeddingProcessor. (#multi-modality)\n\t        return self.model.encode_image(image_paths, IMAGEBIND_BATCH_SIZE // 8, show_progress_bar)\n\t    @property\n\t    def _can_take_text_input(cls) -> bool:\n\t        return True\n\t    @property\n\t    def _can_take_audio_input(cls) -> bool:\n\t        return True\n", "    @property\n\t    def _can_take_image_input(cls) -> bool:\n\t        return True\n\t    # We need to rewrite this function in all the inherited class so we can use the @modal method decorator.\n\t    # TODO: check if above statement is true / if we can factor this out.\n\t    @modal.method()\n\t    def calculate_embeddings(  # type: ignore\n\t        self,\n\t        ids: list,\n\t        output_path: str,\n", "        texts: Optional[list] = None,\n\t        audio_paths: Optional[list] = None,\n\t        image_paths: Optional[list] = None,\n\t        show_progress_bar: bool = False,\n\t    ) -> Union[None, pa.Table]:\n\t        return super().calculate_embeddings(\n\t            ids=ids,\n\t            output_path=output_path,\n\t            texts=texts,\n\t            audio_paths=audio_paths,\n", "            image_paths=image_paths,\n\t            show_progress_bar=show_progress_bar,\n\t        )\n\t### Test functions ###\n\t# We have a function here instead of putting it into __main__ so that `modal shell` works\n\t@imagebind_stub.function(image=image)\n\tdef test_texts(s3_bucket_path: str):\n\t    ib = ImageBindEmbdedder()\n\t    embeddings = ib.calculate_embeddings.call(\n\t        texts=[\"hello\", \"world\"],\n", "        ids=[1, 2],\n\t        output_path=s3_bucket_path,\n\t        show_progress_bar=True,\n\t    )\n\t@imagebind_stub.function(image=image)\n\tdef test_audio(s3_bucket_path: str):\n\t    ib = ImageBindEmbdedder()\n\t    embeddings = ib.calculate_embeddings.call(\n\t        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 2,\n\t        ids=list(range(2)),\n", "        output_path=s3_bucket_path,\n\t        show_progress_bar=True,\n\t    )\n\t@imagebind_stub.function(image=image)\n\tdef test_texts_audio(s3_bucket_path: str):\n\t    ib = ImageBindEmbdedder()\n\t    embeddings = ib.calculate_embeddings.call(\n\t        texts=[\"hello\", \"world\"],\n\t        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 2,\n\t        ids=list(range(2)),\n", "        output_path=s3_bucket_path,\n\t        show_progress_bar=True,\n\t    )\n\t@imagebind_stub.function(image=image)\n\tdef test_texts_audio_batched(s3_bucket_path: str):\n\t    ib = ImageBindEmbdedder()\n\t    embeddings = ib.calculate_embeddings.call(\n\t        texts=[\"hello\"] * 1025,\n\t        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 1025,\n\t        ids=list(range(1025)),\n", "        output_path=s3_bucket_path,\n\t        show_progress_bar=True,\n\t    )\n\t@imagebind_stub.function(image=image)\n\tdef test_audio_batched(s3_bucket_path: str):\n\t    ib = ImageBindEmbdedder()\n\t    embeddings = ib.calculate_embeddings.call(\n\t        # texts=[\"hello\"] * 1025,\n\t        audio_paths=[\"s3://clip-audio-deploy/audioset/---1_cCGK4M.flac\"] * 10000,\n\t        ids=list(range(1025)),\n", "        output_path=s3_bucket_path,\n\t        show_progress_bar=True,\n\t    )\n\t@imagebind_stub.function(image=image)\n\tdef test_image(s3_bucket_path: str):\n\t    ib = ImageBindEmbdedder()\n\t    NUM_IMAGES = 1025\n\t    embeddings = ib.calculate_embeddings.call(\n\t        image_paths=[\"s3://metavoice-vector-db/bird_image.jpg\"] * NUM_IMAGES,\n\t        ids=list(range(NUM_IMAGES)),\n", "        output_path=s3_bucket_path,\n\t        show_progress_bar=True,\n\t    )\n\tif __name__ == \"__main__\":\n\t    # Run as \"python multimodal_imagebind.py s3://<bucket_name>/test_mainfn/\"\n\t    import sys\n\t    with imagebind_stub.run():\n\t        # test_texts.call(sys.argv[1])\n\t        # test_audio.call(sys.argv[1])\n\t        # test_texts_audio.call(sys.argv[1])\n", "        # test_texts_audio_batched.call(sys.argv[1])\n\t        # test_audio_batched.call(sys.argv[1])\n\t        test_image.call(sys.argv[1])\n"]}
{"filename": "peachdb/embedder/containers/base.py", "chunked_list": ["import os\n\timport subprocess\n\tfrom abc import ABC, abstractmethod\n\tfrom pathlib import Path\n\tfrom typing import Optional, Union\n\timport boto3  # type: ignore\n\timport modal\n\timport numpy as np\n\timport pandas as pd\n\timport pyarrow as pa  # type: ignore\n", "import pyarrow.parquet as pq  # type: ignore\n\timport requests\n\tfrom peachdb.constants import CACHED_REQUIREMENTS_TXT, GIT_REQUIREMENTS_TXT\n\tfrom peachdb.embedder.utils import S3File, S3Files, is_s3_uri\n\t# Logic to get a requirements.txt file for the base image when package is on PyPI.\n\tdev_requirements_path = Path(__file__).parents[3] / \"requirements.txt\"\n\tif os.path.exists(dev_requirements_path):\n\t    requirements_path: Union[Path, str] = dev_requirements_path\n\telse:\n\t    response = requests.get(GIT_REQUIREMENTS_TXT)\n", "    response.raise_for_status()\n\t    with open(CACHED_REQUIREMENTS_TXT, \"w\") as f:\n\t        f.write(response.text)\n\t    requirements_path = CACHED_REQUIREMENTS_TXT\n\t# Grab AWS credentials from ~/.aws/credentials using boto3.\n\t# This code is written this way as it ends up getting executed inside the container creation process as well,\n\t# and so ends up with empty credentials. Doing it this way means empty credentials get set in the container creation process,\n\t# but the actual model serving process will have the correct credentials.\n\t# TODO: fix scope for bad error handling here. really want to error if these don't exist locally!\n\t# But we don't have stub here to run `is_inside`.\n", "secrets = []\n\t_aws_boto_session = boto3.Session()\n\tif _aws_boto_session != None:\n\t    _aws_credentials = _aws_boto_session.get_credentials()\n\t    if _aws_credentials != None:\n\t        secrets = [\n\t            modal.Secret.from_dict(\n\t                {\"AWS_ACCESS_KEY_ID\": _aws_credentials.access_key, \"AWS_SECRET_ACCESS_KEY\": _aws_credentials.secret_key}\n\t            ),\n\t        ]\n", "# Requirements for the base image of models we want to serve.\n\t# We don't add the requirements.txt here as that contains requirements across ALL our models.\n\tbase_container_image = (\n\t    modal.Image.debian_slim()\n\t    .apt_install(\"curl\", \"zip\", \"git\")\n\t    .run_commands(\n\t        \"curl https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip -o awscliv2.zip\",\n\t        \"unzip awscliv2.zip\",\n\t        \"./aws/install\",\n\t        \"rm -rf awscliv2.zip aws\",\n", "    )\n\t    .pip_install_from_requirements(str(requirements_path))\n\t    # Container creation flow from inside a pypi package doesn't pick up it's own files.\n\t    .pip_install(\"git+https://github.com/peach-db/peachdb\")\n\t)\n\tmodal_compute_spec_decorator = lambda stub, image: stub.cls(\n\t    image=image,\n\t    gpu=\"T4\",\n\t    timeout=400,\n\t    secrets=secrets,\n", "    concurrency_limit=500,\n\t)\n\tclass EmbeddingModelBase(ABC):\n\t    @abstractmethod\n\t    def _calculate_text_embeddings(self, texts: list, show_progress_bar: bool) -> np.ndarray:\n\t        pass\n\t    @abstractmethod\n\t    def _calculate_audio_embeddings(self, audio_paths: list, show_progress_bar: bool) -> np.ndarray:\n\t        pass\n\t    @abstractmethod\n", "    def _calculate_image_embeddings(self, image_paths: list, show_progress_bar: bool) -> np.ndarray:\n\t        pass\n\t    @property\n\t    @abstractmethod\n\t    def _can_take_text_input(cls) -> bool:\n\t        raise NotImplementedError\n\t    @property\n\t    @abstractmethod\n\t    def _can_take_audio_input(cls) -> bool:\n\t        raise NotImplementedError\n", "    @property\n\t    @abstractmethod\n\t    def _can_take_image_input(cls) -> bool:\n\t        raise NotImplementedError\n\t    def _check_s3_credentials(self):\n\t        try:\n\t            subprocess.check_output([\"aws\", \"s3\", \"ls\", \"s3://\"], stderr=subprocess.STDOUT)\n\t        except subprocess.CalledProcessError:\n\t            raise EnvironmentError(\n\t                \"AWS CLI not configured. Please set credentials locally using `aws configure` and try again.\"\n", "            )\n\t    def calculate_embeddings(\n\t        self,\n\t        ids: list,\n\t        output_path: str,\n\t        texts: Optional[list] = None,\n\t        audio_paths: Optional[list] = None,\n\t        image_paths: Optional[list] = None,\n\t        show_progress_bar: bool = False,\n\t    ) -> Union[None, pa.Table]:\n", "        assert (\n\t            texts is not None or audio_paths is not None or image_paths is not None\n\t        ), \"Must provide at least one input.\"\n\t        if (\n\t            is_s3_uri(output_path)\n\t            # TODO: refactor\n\t            or (any([is_s3_uri(audio_path) for audio_path in audio_paths]) if audio_paths is not None else False)\n\t            or (any([is_s3_uri(image_path) for image_path in image_paths]) if image_paths is not None else False)\n\t        ):\n\t            self._check_s3_credentials()\n", "        embeddings_dict = {}\n\t        embeddings_dict[\"ids\"] = ids\n\t        if texts is not None:\n\t            print(\"Processing text input...\")\n\t            if self._can_take_text_input:\n\t                text_embeddings = self._calculate_text_embeddings(texts, show_progress_bar)\n\t                # TODO: update wherever this variable is used upstream\n\t                embeddings_dict[\"text_embeddings\"] = text_embeddings.tolist()\n\t            else:\n\t                raise Exception(\"This model cannot take text input.\")\n", "        # TODO: refactor below two if statements.\n\t        # TODO: think about if we want to error if a modality is not supported by a model, or just\n\t        # error, and then let things continue.\n\t        if audio_paths is not None:\n\t            print(\"Processing audio input...\")\n\t            if self._can_take_audio_input:\n\t                assert (\n\t                    len(set([is_s3_uri(x) for x in audio_paths])) == 1\n\t                ), \"All audio paths must be either local or S3 paths.\"\n\t                is_s3 = all([is_s3_uri(x) for x in audio_paths])\n", "                if is_s3:\n\t                    print(\"Downloading audio files from S3... Creating handlers...\")\n\t                    audio_files_handler = S3Files(audio_paths)\n\t                    print(\"Handlers created, downloading...\")\n\t                    local_audio_paths = audio_files_handler.download()\n\t                else:\n\t                    local_audio_paths = audio_paths\n\t                audio_embeddings = self._calculate_audio_embeddings(local_audio_paths, show_progress_bar)\n\t                embeddings_dict[\"audio_embeddings\"] = audio_embeddings.tolist()\n\t            else:\n", "                raise Exception(\"This model cannot take audio input.\")\n\t        if image_paths is not None:\n\t            if self._can_take_image_input:\n\t                assert (\n\t                    len(set([is_s3_uri(x) for x in image_paths])) == 1\n\t                ), \"All image paths must be either local or S3 paths.\"\n\t                is_s3 = all([is_s3_uri(x) for x in image_paths])\n\t                if is_s3:\n\t                    print(\"Downloading audio files from S3... Creating handlers...\")\n\t                    image_file_handlers = S3Files(image_paths)\n", "                    print(\"Handlers created, downloading...\")\n\t                    local_image_paths = image_file_handlers.download()\n\t                else:\n\t                    local_image_paths = image_paths\n\t                image_embeddings = self._calculate_image_embeddings(local_image_paths, show_progress_bar)\n\t                embeddings_dict[\"image_embeddings\"] = image_embeddings.tolist()\n\t            else:\n\t                raise Exception(\"This model cannot take image input.\")\n\t        tmp_output_path = \"/root/embeddings.parquet\"\n\t        df = pd.DataFrame(embeddings_dict)\n", "        table = pa.Table.from_pandas(df)\n\t        pq.write_table(table, tmp_output_path)\n\t        if is_s3_uri(output_path):\n\t            os.system(f\"aws s3 cp {tmp_output_path} {output_path}\")\n\t            return None\n\t        return table\n"]}
{"filename": "peachdb/embedder/containers/__init__.py", "chunked_list": []}
{"filename": "peachdb/embedder/models/sentence_transformer.py", "chunked_list": ["import torch\n\tfrom sentence_transformers import SentenceTransformer  # type: ignore\n\tfrom peachdb.embedder.models.base import BaseModel\n\tMODEL_NAME = \"all-MiniLM-L12-v2\"\n\tclass SentenceTransformerModel(BaseModel):\n\t    def __init__(self) -> None:\n\t        self.model = SentenceTransformer(MODEL_NAME, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t    def encode_texts(self, texts, batch_size, show_progress_bar):\n\t        return self.model.encode(texts, batch_size=batch_size, show_progress_bar=show_progress_bar)\n\t    def encode_audio(self, local_paths, batch_size, show_progress_bar):\n", "        raise NotImplementedError\n\t    def encode_image(self, local_paths, batch_size, show_progress_bar):\n\t        raise NotImplementedError\n\t    @staticmethod\n\t    def download_model():\n\t        SentenceTransformer(MODEL_NAME, device=\"cpu\")\n"]}
{"filename": "peachdb/embedder/models/multimodal_imagebind.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport tqdm  # type: ignore\n\tfrom imagebind.data import (  # type: ignore\n\t    load_and_transform_audio_data,\n\t    load_and_transform_text,\n\t    load_and_transform_vision_data,\n\t)\n\tfrom imagebind.models import imagebind_model  # type: ignore\n\tfrom imagebind.models.imagebind_model import ModalityType  # type: ignore\n", "from peachdb.embedder.models.base import BaseModel\n\tclass ImageBindModel(BaseModel):\n\t    def __init__(self) -> None:\n\t        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"  # TODO: can we factor to base model?\n\t        self.model = imagebind_model.imagebind_huge(pretrained=True).eval().to(self.device)\n\t    # TODO: we want all the encodings in one function so that we can get the embeddings in one go!\n\t    # Otherwise we are wasting compute. Refactor given this.\n\t    # HAVE A SINGLE ENCODE FUNCTION!\n\t    def encode_texts(self, texts, batch_size, show_progress_bar) -> np.ndarray:\n\t        embeddings = []\n", "        for start_index in tqdm.tqdm(range(0, len(texts), batch_size), desc=\"Batches\", disable=not show_progress_bar):\n\t            texts_batch = texts[start_index : start_index + batch_size]\n\t            inputs_batch = {ModalityType.TEXT: load_and_transform_text(texts_batch, self.device)}\n\t            with torch.no_grad():\n\t                embeddings.append(self.model(inputs_batch)[ModalityType.TEXT].cpu().numpy())\n\t        return np.concatenate(embeddings, axis=0)\n\t    def encode_audio(self, local_paths, batch_size, show_progress_bar) -> np.ndarray:\n\t        embeddings = []\n\t        for start_index in tqdm.tqdm(\n\t            range(0, len(local_paths), batch_size), desc=\"Batches\", disable=not show_progress_bar\n", "        ):\n\t            batch_local_paths = local_paths[start_index : start_index + batch_size]\n\t            batched_inputs = {ModalityType.AUDIO: load_and_transform_audio_data(batch_local_paths, self.device)}\n\t            with torch.no_grad():\n\t                embeddings.append(self.model(batched_inputs)[ModalityType.AUDIO].cpu().numpy())\n\t        return np.concatenate(embeddings, axis=0)\n\t    def encode_image(self, local_paths, batch_size, show_progress_bar) -> np.ndarray:\n\t        embeddings = []\n\t        for start_index in tqdm.tqdm(\n\t            range(0, len(local_paths), batch_size), desc=\"Batches\", disable=not show_progress_bar\n", "        ):\n\t            batch_local_paths = local_paths[start_index : start_index + batch_size]\n\t            batched_inputs = {ModalityType.VISION: load_and_transform_vision_data(batch_local_paths, self.device)}\n\t            with torch.no_grad():\n\t                embeddings.append(self.model(batched_inputs)[ModalityType.VISION].cpu().numpy())\n\t        return np.concatenate(embeddings, axis=0)\n\t    @staticmethod\n\t    def download_model():\n\t        imagebind_model.imagebind_huge(pretrained=True)\n"]}
{"filename": "peachdb/embedder/models/base.py", "chunked_list": ["import abc\n\tclass BaseModel(abc.ABC):\n\t    @abc.abstractmethod\n\t    def __init__(self) -> None:\n\t        pass\n\t    @abc.abstractmethod\n\t    def encode_texts(self, texts, batch_size, show_progress_bar):\n\t        pass\n\t    @abc.abstractmethod\n\t    def encode_audio(self, local_paths, batch_size, show_progress_bar):\n", "        pass\n\t    @abc.abstractmethod\n\t    def encode_image(self, local_paths, batch_size, show_progress_bar):\n\t        pass\n\t    @staticmethod\n\t    @abc.abstractmethod\n\t    def download_model():\n\t        pass\n"]}
{"filename": "peachdb/embedder/models/__init__.py", "chunked_list": []}
{"filename": "peachdb/backends/backend_base.py", "chunked_list": ["import abc\n\timport dataclasses\n\timport os\n\tfrom typing import Optional\n\timport duckdb\n\timport numpy as np\n\timport pandas as pd\n\tfrom rich import print\n\timport peachdb.embedder.models.base\n\tfrom peachdb.embedder.models.multimodal_imagebind import ImageBindModel\n", "from peachdb.embedder.models.sentence_transformer import SentenceTransformerModel\n\tfrom peachdb.embedder.openai_ada import OpenAIAdaEmbedder\n\tfrom peachdb.embedder.utils import Modality, S3File, S3Folder, is_s3_uri\n\t@dataclasses.dataclass\n\tclass BackendConfig:\n\t    embedding_generator: str\n\t    distance_metric: str\n\t    embeddings_dir: str\n\t    metadata_path: str\n\t    id_column_name: str\n", "    modality: Modality\n\tclass BackendBase(abc.ABC):\n\t    def __init__(\n\t        self,\n\t        backend_config: BackendConfig,\n\t    ):\n\t        # TODO: refactor below to clean up\n\t        embeddings_dir = backend_config.embeddings_dir\n\t        metadata_path = backend_config.metadata_path\n\t        embedding_generator = backend_config.embedding_generator\n", "        distance_metric = backend_config.distance_metric\n\t        id_column_name = backend_config.id_column_name\n\t        modality = backend_config.modality\n\t        self._distance_metric = distance_metric\n\t        self._id_column_name = id_column_name\n\t        self._metadata_filepath = self._get_metadata_filepath(metadata_path)\n\t        self._modality = modality\n\t        self._embedding_generator = embedding_generator\n\t        self._embeddings, self._ids = self._get_embeddings(embeddings_dir)\n\t        if len(set(self._ids)) != len(self._ids):\n", "            raise ValueError(\"Duplicate ids found in the embeddings file.\")\n\t        if self._embedding_generator == \"sentence_transformer_L12\":\n\t            self._encoder: peachdb.embedder.models.base.BaseModel = SentenceTransformerModel()\n\t        elif self._embedding_generator == \"imagebind\":\n\t            self._encoder = ImageBindModel()\n\t        elif self._embedding_generator == \"openai_ada\":\n\t            self._openai_encoder = OpenAIAdaEmbedder()\n\t        else:\n\t            raise ValueError(f\"Unknown embedding generator: {embedding_generator}\")\n\t    @abc.abstractmethod\n", "    def _process_query(self, query_embedding, top_k: int = 5) -> tuple:\n\t        pass\n\t    def process_query(self, query: str, modality: Modality, top_k: int = 5) -> tuple:\n\t        print(\"Embedding query...\")\n\t        if self._embedding_generator == \"openai_ada\":\n\t            assert modality == Modality.TEXT\n\t            query_embedding = np.asarray(self._openai_encoder.calculate_embeddings([query])[0:1])\n\t            return self._process_query(query_embedding, top_k)\n\t        else:\n\t            if modality == Modality.TEXT:\n", "                query_embedding = self._encoder.encode_texts(texts=[query], batch_size=1, show_progress_bar=True)\n\t            elif modality == Modality.AUDIO:\n\t                query_embedding = self._encoder.encode_audio(local_paths=[query], batch_size=1, show_progress_bar=True)\n\t            elif modality == Modality.IMAGE:\n\t                query_embedding = self._encoder.encode_image(local_paths=[query], batch_size=1, show_progress_bar=True)\n\t            else:\n\t                raise ValueError(f\"Unknown modality: {modality}\")\n\t            return self._process_query(query_embedding, top_k)\n\t    def fetch_metadata(self, ids, namespace: Optional[str]) -> pd.DataFrame:\n\t        print(\"Fetching metadata...\")\n", "        # NOTE: this is a hack, as we keep updating the metadata.\n\t        data = duckdb.read_csv(self._metadata_filepath, header=True)\n\t        id_str = \" OR \".join([f\"{self._id_column_name} = '{id}'\" for id in ids])\n\t        if namespace is None:\n\t            metadata = duckdb.sql(f\"SELECT * FROM data WHERE {id_str}\").df()\n\t        else:\n\t            metadata = duckdb.sql(f\"SELECT * FROM data WHERE ({id_str}) AND (namespace = '{namespace}')\").df()\n\t        return metadata\n\t    def _get_embeddings(self, embeddings_dir: str):\n\t        if not is_s3_uri(embeddings_dir):\n", "            return self._load_embeddings(embeddings_dir)\n\t        print(\"[bold]Downloading calculated embeddings...[/bold]\")\n\t        with S3Folder(embeddings_dir) as tmp_local_embeddings_dir:\n\t            return self._load_embeddings(tmp_local_embeddings_dir)\n\t    def _load_embeddings(self, embeddings_dir: str) -> tuple:\n\t        \"\"\"Loads and preprocesses the embeddings from a parquet file.\"\"\"\n\t        assert os.path.exists(embeddings_dir)\n\t        print(\"[bold]Loading embeddings from parquet file...[/bold]\")\n\t        df = pd.read_parquet(embeddings_dir, \"pyarrow\")\n\t        print(\"[bold]Converting embeddings to numpy array...[/bold]\")\n", "        if self._modality == Modality.TEXT:\n\t            # TODO: these keys name are used in embedder.containers.base, so we should refactor\n\t            embeddings = np.array(df[\"text_embeddings\"].values.tolist()).astype(\"float32\")\n\t        elif self._modality == Modality.AUDIO:\n\t            embeddings = np.array(df[\"audio_embeddings\"].values.tolist()).astype(\"float32\")\n\t        elif self._modality == Modality.IMAGE:\n\t            embeddings = np.array(df[\"image_embeddings\"].values.tolist()).astype(\"float32\")\n\t        else:\n\t            raise ValueError(f\"Unknown modality: {self._modality}\")\n\t        ids = np.asarray(df[\"ids\"].apply(str).values.tolist())\n", "        return embeddings, ids\n\t    def _get_metadata_filepath(self, metadata_path: str) -> str:\n\t        if not is_s3_uri(metadata_path):\n\t            return metadata_path\n\t        print(\"[bold]Downloading metadata file...[/bold]\")\n\t        self._metadata_fileref = S3File(metadata_path)\n\t        return self._metadata_fileref.download()\n\t    def cleanup(self):\n\t        if is_s3_uri(self._metadata_path):\n\t            self._metadata_fileref.cleanup()\n", "        if is_s3_uri(self._embeddings_dir):\n\t            self._embeddings_dir.cleanup()\n"]}
{"filename": "peachdb/backends/torch_backend.py", "chunked_list": ["import numpy as np\n\timport torch\n\tfrom peachdb.backends.backend_base import BackendBase, BackendConfig\n\tfrom peachdb.embedder.utils import Modality\n\tdef _check_dims(query_embed: torch.Tensor, embeds: torch.Tensor):\n\t    if query_embed.dim() == 1:\n\t        query_embed = query_embed.unsqueeze(0)\n\t    elif query_embed.dim() == 2:\n\t        if query_embed.size(0) != 1:\n\t            raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n", "    else:\n\t        raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n\t    if embeds.dim() != 2:\n\t        raise ValueError(\"embeds should be a 2-D matrix\")\n\t    return query_embed, embeds\n\tdef l2(query_embed: torch.Tensor, embeds: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"\n\t    Calculate l2 distance between a query embedding and a set of embeddings.\n\t    \"\"\"\n\t    query_embed, embeds = _check_dims(query_embed, embeds)\n", "    return torch.norm(query_embed - embeds, dim=1)\n\tdef cosine(query_embed: torch.Tensor, embeds: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"\n\t    Can be used to compute cosine \"distance\" between any number of query embeddings\n\t    and a set of embeddings.\n\t    result[i, j] = 1 - torch.dot(query_embed[i], embeds[j])\n\t    \"\"\"\n\t    query_embed, embeds = _check_dims(query_embed, embeds)\n\t    return (\n\t        1\n", "        - torch.mm(query_embed, embeds.t())\n\t        / (torch.norm(query_embed, dim=1).unsqueeze(1) * torch.norm(embeds, dim=1).unsqueeze(0))\n\t    )[0]\n\tclass TorchBackend(BackendBase):\n\t    def __init__(\n\t        self,\n\t        backend_config: BackendConfig,\n\t    ):\n\t        if not torch.cuda.is_available():\n\t            raise RuntimeError(\"CUDA is required to use TorchDB\")\n", "        super().__init__(\n\t            backend_config=backend_config,\n\t        )\n\t        self.device = torch.device(\"cuda\")\n\t        self._embeddings = torch.from_numpy(self._embeddings).to(self.device)  # Ensure the tensor is on the GPU\n\t    def _process_query(self, query_embedding, top_k: int = 5) -> tuple:\n\t        \"\"\"Compute query embedding, calculate distance of query embedding and get top k.\"\"\"\n\t        query_embedding = torch.from_numpy(query_embedding).to(self.device)\n\t        print(\"Calculating distances...\")\n\t        distances = (\n", "            l2(query_embedding, self._embeddings)\n\t            if self._distance_metric == \"l2\"\n\t            else cosine(query_embedding, self._embeddings)\n\t        )\n\t        print(\"Getting top results...\")\n\t        results = torch.argsort(distances)[:top_k].cpu().numpy()\n\t        return self._ids[results], distances[results].cpu().numpy()\n\tif __name__ == \"__main__\":\n\t    import scipy.spatial.distance as scipy_distance  # type: ignore\n\t    from sentence_transformers.util import cos_sim as st_cos_sim  # type: ignore\n", "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t    for dim in [3, 384, 1536]:\n\t        a = torch.rand(dim, device=device)\n\t        b = torch.rand(3, dim, device=device)\n\t        # cosine\n\t        cosine_result = cosine(a, b)\n\t        for i in range(b.shape[0]):\n\t            np.testing.assert_allclose(\n\t                scipy_distance.cosine(a.cpu().numpy(), b[i].cpu().numpy()),\n\t                cosine_result[i].cpu().numpy(),\n", "                rtol=1e-4,\n\t            )\n\t            np.testing.assert_allclose(\n\t                1 - st_cos_sim(a.cpu().numpy(), b[i].cpu().numpy()).numpy(),\n\t                cosine_result[i].cpu().numpy(),\n\t                rtol=1e-4,\n\t            )\n\t        # l2\n\t        l2_result = l2(a, b)\n\t        for i in range(b.shape[0]):\n", "            np.testing.assert_allclose(\n\t                scipy_distance.euclidean(a.cpu().numpy(), b[i].cpu().numpy()),\n\t                l2_result[i].cpu().numpy(),\n\t                rtol=1e-4,\n\t            )\n"]}
{"filename": "peachdb/backends/__init__.py", "chunked_list": ["from typing import Dict, Union\n\tfrom peachdb.backends.backend_base import BackendBase, BackendConfig\n\tfrom peachdb.backends.hnsw_backend import HNSWBackend\n\tfrom peachdb.backends.numpy_backend import NumpyBackend\n\tfrom peachdb.backends.torch_backend import TorchBackend\n\tfrom peachdb.embedder.utils import Modality\n\tdef get_backend(\n\t    embedding_generator: str,\n\t    distance_metric: str,\n\t    embedding_backend: str,\n", "    embeddings_dir: str,\n\t    metadata_path: str,\n\t    id_column_name: str,\n\t    modality: Modality,\n\t) -> BackendBase:\n\t    backend_config = BackendConfig(\n\t        embeddings_dir=embeddings_dir,\n\t        metadata_path=metadata_path,\n\t        embedding_generator=embedding_generator,\n\t        distance_metric=distance_metric,\n", "        id_column_name=id_column_name,\n\t        modality=modality,\n\t    )\n\t    if embedding_backend == \"exact_cpu\":\n\t        return NumpyBackend(backend_config)\n\t    elif embedding_backend == \"exact_gpu\":\n\t        return TorchBackend(backend_config)\n\t    elif embedding_backend == \"approx\":\n\t        return HNSWBackend(backend_config)\n\t    else:\n", "        raise ValueError(f\"Unknown value for embedding_backend, provided: {embedding_backend}\")\n"]}
{"filename": "peachdb/backends/numpy_backend.py", "chunked_list": ["import os\n\tfrom typing import Tuple\n\timport duckdb\n\timport numpy as np\n\timport pandas as pd\n\tfrom rich import print\n\tfrom peachdb.backends.backend_base import BackendBase\n\tdef _check_dims(query_embed: np.ndarray, embeds: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n\t    if query_embed.ndim == 1:\n\t        query_embed = query_embed[np.newaxis, :]\n", "    elif query_embed.ndim == 2:\n\t        if query_embed.shape[0] != 1:\n\t            raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n\t    else:\n\t        raise ValueError(\"query_embed should be a vector or a matrix with one row\")\n\t    if embeds.ndim != 2:\n\t        raise ValueError(\"embeds should be a 2-D matrix\")\n\t    return query_embed, embeds\n\tdef l2(query_embed: np.ndarray, embeds: np.ndarray) -> np.ndarray:\n\t    \"\"\"\n", "    Calculate l2 distance between a query embedding and a set of embeddings.\n\t    \"\"\"\n\t    query_embed, embeds = _check_dims(query_embed, embeds)\n\t    return np.linalg.norm(query_embed - embeds, axis=1)\n\tdef cosine(query_embed: np.ndarray, embeds: np.ndarray) -> np.ndarray:\n\t    \"\"\"\n\t    Can be used to compute cosine \"distance\" between any number of query embeddings\n\t    and a set of embeddings.\n\t    result[i, j] = 1 - np.dot(query_embed[i], embeds[j])\n\t    \"\"\"\n", "    query_embed, embeds = _check_dims(query_embed, embeds)\n\t    return (1 - (query_embed @ embeds.T) / (np.linalg.norm(query_embed, axis=1) * np.linalg.norm(embeds, axis=1)))[0]\n\tclass NumpyBackend(BackendBase):\n\t    def _process_query(self, query_embedding, top_k: int = 5) -> tuple:\n\t        \"\"\"Compute query embedding, calculate distance of query embedding and get top k.\"\"\"\n\t        print(\"Calculating distances...\")\n\t        distances = (\n\t            l2(query_embedding, self._embeddings)\n\t            if self._distance_metric == \"l2\"\n\t            else cosine(query_embedding, self._embeddings)\n", "        )\n\t        print(\"Getting top results...\")\n\t        results = np.argsort(distances)[:top_k]\n\t        return self._ids[results], distances[results]\n\t    def download_data_for_new_upsertions(self, upsertion_logs: list):\n\t        # TODO: add test that we're not loading any extra data!\n\t        # TODO: this is likely going to be quite slow?\n\t        self._embeddings, self._ids = self._get_embeddings(upsertion_logs[-1][\"embeddings_dir\"])\n\tif __name__ == \"__main__\":\n\t    import scipy.spatial.distance as scipy_distance  # type: ignore\n", "    from sentence_transformers.util import cos_sim as st_cos_sim  # type: ignore\n\t    for dim in [3, 384, 1536]:\n\t        a = np.random.rand(dim)\n\t        b = np.random.rand(3, dim)\n\t        # cosine\n\t        cosine_result = cosine(a, b)\n\t        for i in range(b.shape[0]):\n\t            np.testing.assert_allclose(scipy_distance.cosine(a, b[i]), cosine_result[i])\n\t            np.testing.assert_allclose(1 - st_cos_sim(a, b[i]).numpy(), cosine_result[i])\n\t        # l2\n", "        l2_result = l2(a, b)\n\t        for i in range(b.shape[0]):\n\t            np.testing.assert_allclose(scipy_distance.euclidean(a, b[i]), l2_result[i])\n"]}
{"filename": "peachdb/backends/hnsw_backend.py", "chunked_list": ["from typing import Tuple\n\timport hnswlib  # type: ignore\n\timport numpy as np\n\tfrom rich import print\n\tfrom peachdb.backends.backend_base import BackendBase, BackendConfig\n\tfrom peachdb.embedder.utils import Modality\n\tclass HNSWBackend(BackendBase):\n\t    def __init__(\n\t        self,\n\t        backend_config: BackendConfig,\n", "    ):\n\t        super().__init__(\n\t            backend_config=backend_config,\n\t        )\n\t        if self._embeddings.ndim != 2:\n\t            raise ValueError(\"embeddings should be a 2-D matrix\")\n\t        self._dim = self._embeddings.shape[1]\n\t        # create hnsw index.\n\t        self._hnsw_index = hnswlib.Index(space=self._distance_metric, dim=self._dim)\n\t        self._max_elements = self._embeddings.shape[0]\n", "        # initialise index.\n\t        # TODO: fix to support multiple upserts. (#multiple-upserts)\n\t        self._hnsw_index.init_index(\n\t            max_elements=self._max_elements,\n\t            ef_construction=min(200, self._embeddings.shape[0]),  # default param\n\t            M=16,  # default param\n\t            random_seed=100,\n\t        )\n\t        # add data points to index.\n\t        print(\"[bold]Adding data points to index...[/bold]\")\n", "        self._hnsw_index.add_items(self._embeddings, self._ids)\n\t        # set hnsw ef param\n\t        self._hnsw_index.set_ef(max(200, self._embeddings.shape[0]))\n\t    def _process_query(self, query_embedding, top_k: int = 5):\n\t        \"\"\"Compute query embedding, calculate distance of query embedding and get top k.\"\"\"\n\t        if query_embedding.ndim != 1 and not (query_embedding.ndim == 2 and query_embedding.shape[0] == 1):\n\t            raise ValueError(\"query_embedding should be a vector or a matrix with one row\")\n\t        print(\"Getting top results...\")\n\t        labels, distances = self._hnsw_index.knn_query(query_embedding, k=top_k)\n\t        return labels[0], distances[0]\n"]}
