{"filename": "conf.py", "chunked_list": ["extensions = ['sphinx_click']\n"]}
{"filename": "tests/dbt_project_fixtures.py", "chunked_list": ["from dbt.contracts.results import CatalogTable\n\tshared_model_catalog_entry = CatalogTable.from_dict(\n\t    {\n\t        \"metadata\": {\n\t            \"type\": \"BASE TABLE\",\n\t            \"schema\": \"main\",\n\t            \"name\": \"shared_model\",\n\t            \"database\": \"database\",\n\t            \"comment\": None,\n\t            \"owner\": None,\n", "        },\n\t        \"columns\": {\n\t            \"ID\": {\"type\": \"INTEGER\", \"index\": 1, \"name\": \"id\", \"comment\": None},\n\t            \"colleague\": {\"type\": \"VARCHAR\", \"index\": 2, \"name\": \"colleague\", \"comment\": None},\n\t        },\n\t        \"stats\": {\n\t            \"has_stats\": {\n\t                \"id\": \"has_stats\",\n\t                \"label\": \"Has Stats?\",\n\t                \"value\": False,\n", "                \"include\": False,\n\t                \"description\": \"Indicates whether there are statistics for this table\",\n\t            }\n\t        },\n\t        \"unique_id\": \"model.src_proj_a.shared_model\",\n\t    }\n\t)\n"]}
{"filename": "tests/sql_and_yml_fixtures.py", "chunked_list": ["shared_model_sql = \"\"\"\n\twith source_data as (\n\t    select 1 as id, 'grace' as colleague\n\t    union all\n\t    select 2 as id, 'dave' as colleague\n\t)\n\tselect *\n\tfrom source_data\n\t\"\"\"\n\tmodel_yml_no_col_no_version = \"\"\"\n", "models:\n\t  - name: shared_model\n\t    description: \"this is a test model\"\n\t\"\"\"\n\tmodel_yml_other_model = \"\"\"\n\tmodels:\n\t  - name: other_shared_model\n\t    description: \"this is a different test model\"\n\t  - name: shared_model\n\t\"\"\"\n", "model_yml_one_col = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    description: \"this is a test model\"\n\t    columns:\n\t      - name: id\n\t        description: \"this is the id column\"\n\t\"\"\"\n\tmodel_yml_one_col_one_test = \"\"\"\n\tmodels:\n", "  - name: shared_model\n\t    description: \"this is a test model\"\n\t    columns:\n\t      - name: id\n\t        description: \"this is the id column\"\n\t        tests:\n\t          - unique\n\t\"\"\"\n\tmodel_yml_all_col = \"\"\"\n\tmodels:\n", "  - name: shared_model\n\t    description: \"this is a test model\"\n\t    columns:\n\t      - name: id\n\t        description: \"this is the id column\"\n\t      - name: colleague\n\t        description: \"this is the colleague column\"\n\t\"\"\"\n\texpected_contract_yml_no_col = \"\"\"\n\tmodels:\n", "  - name: shared_model\n\t    config:\n\t      contract:\n\t        enforced: true\n\t    description: \"this is a test model\"\n\t    columns:\n\t      - name: id\n\t        data_type: integer\n\t      - name: colleague\n\t        data_type: varchar\n", "\"\"\"\n\texpected_contract_yml_sequential = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    config:\n\t      contract:\n\t        enforced: true\n\t    columns:\n\t      - name: id\n\t        data_type: integer\n", "      - name: colleague\n\t        data_type: varchar\n\t  - name: new_model\n\t    config:\n\t      contract:\n\t        enforced: true\n\t    columns:\n\t      - name: id\n\t        data_type: integer\n\t\"\"\"\n", "expected_contract_yml_one_col_one_test = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    config:\n\t      contract:\n\t        enforced: true\n\t    description: \"this is a test model\"\n\t    columns:\n\t      - name: id\n\t        description: \"this is the id column\"\n", "        data_type: integer\n\t        tests:\n\t          - unique\n\t      - name: colleague\n\t        data_type: varchar\n\t\"\"\"\n\texpected_contract_yml_one_col = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    config:\n", "      contract:\n\t        enforced: true\n\t    description: \"this is a test model\"\n\t    columns:\n\t      - name: id\n\t        description: \"this is the id column\"\n\t        data_type: integer\n\t      - name: colleague\n\t        data_type: varchar\n\t\"\"\"\n", "expected_contract_yml_all_col = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    config:\n\t      contract:\n\t        enforced: true\n\t    description: \"this is a test model\"\n\t    columns:\n\t      - name: id\n\t        description: \"this is the id column\"\n", "        data_type: integer\n\t      - name: colleague\n\t        description: \"this is the colleague column\"\n\t        data_type: varchar\n\t\"\"\"\n\texpected_contract_yml_no_entry = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    config:\n\t      contract:\n", "        enforced: true\n\t    columns:\n\t      - name: id\n\t        data_type: integer\n\t      - name: colleague\n\t        data_type: varchar\n\t\"\"\"\n\texpected_contract_yml_other_model = \"\"\"\n\tmodels:\n\t  - name: other_shared_model\n", "    description: \"this is a different test model\"\n\t  - name: shared_model\n\t    config:\n\t      contract:\n\t        enforced: true\n\t    columns:\n\t      - name: id\n\t        data_type: integer\n\t      - name: colleague\n\t        data_type: varchar\n", "\"\"\"\n\texpected_versioned_model_yml_no_yml = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: 1\n\t    versions:\n\t      - v: 1\n\t\"\"\"\n\texpected_versioned_model_yml_no_version = \"\"\"\n\tmodels:\n", "  - name: shared_model\n\t    latest_version: 1\n\t    description: \"this is a test model\"\n\t    versions:\n\t      - v: 1\n\t\"\"\"\n\tmodel_yml_increment_version = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: 1\n", "    description: \"this is a test model\"\n\t    versions:\n\t      - v: 1\n\t\"\"\"\n\texpected_versioned_model_yml_increment_version_no_prerelease = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: 2\n\t    description: \"this is a test model\"\n\t    versions:\n", "      - v: 1\n\t      - v: 2\n\t\"\"\"\n\texpected_versioned_model_yml_increment_version_with_prerelease = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: 1\n\t    description: \"this is a test model\"\n\t    versions:\n\t      - v: 1\n", "      - v: 2\n\t\"\"\"\n\texpected_versioned_model_yml_increment_prerelease_version_with_second_prerelease = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: 1\n\t    description: \"this is a test model\"\n\t    versions:\n\t      - v: 1\n\t      - v: 2\n", "      - v: 3\n\t\"\"\"\n\texpected_versioned_model_yml_increment_prerelease_version = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: 2\n\t    description: \"this is a test model\"\n\t    versions:\n\t      - v: 1\n\t      - v: 2\n", "      - v: 3\n\t\"\"\"\n\texpected_versioned_model_yml_increment_version_defined_in = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: 2\n\t    description: \"this is a test model\"\n\t    versions:\n\t      - v: 1\n\t      - v: 2\n", "        defined_in: daves_model\n\t\"\"\"\n\tmodel_yml_string_version = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    latest_version: john_olerud\n\t    description: \"this is a test model\"\n\t    versions:\n\t      - v: john_olerud\n\t\"\"\"\n", "# expected result when removing the shared_model entry from model_yml_no_col_no_version\n\texpected_remove_model_yml__model_yml_no_col_no_version = \"\"\"\n\tname: shared_model\n\tdescription: \"this is a test model\"\n\t\"\"\"\n\t# expected result when removing the shared_model entry from model_yml_one_col\n\texpected_remove_model_yml__model_yml_one_col = \"\"\"\n\tname: shared_model\n\tdescription: \"this is a test model\"\n\tcolumns:\n", "  - name: id\n\t    description: \"this is the id column\"\n\t\"\"\"\n\t# expected result when removing the shared_model entry from model_yml_other_model\n\texpected_remove_model_yml__default = \"\"\"\n\tname: shared_model\n\t\"\"\"\n\texpected_remainder_yml__model_yml_other_model = \"\"\"\n\tmodels:\n\t  - name: other_shared_model\n", "    description: \"this is a different test model\"\n\t\"\"\"\n\tsource_yml_one_table = \"\"\"\n\tsources:\n\t  - name: test_source\n\t    description: \"this is a test source\"\n\t    schema: bogus\n\t    database: bogus\n\t    tables:\n\t      - name: table\n", "        description: \"this is a test table\"\n\t\"\"\"\n\texpected_remove_source_yml__default = \"\"\"\n\tname: test_source\n\tdescription: \"this is a test source\"\n\tschema: bogus\n\tdatabase: bogus\n\ttables:\n\t  - name: table\n\t    description: \"this is a test table\"\n", "\"\"\"\n\tsource_yml_multiple_tables = \"\"\"\n\tsources:\n\t  - name: test_source\n\t    description: \"this is a test source\"\n\t    schema: bogus\n\t    database: bogus\n\t    tables:\n\t      - name: table\n\t        description: \"this is a test table\"\n", "      - name: other_table\n\t        description: \"this is a different test table\"\n\t\"\"\"\n\texpeceted_remainder_yml__source_yml_multiple_tables = \"\"\"\n\tsources:\n\t  - name: test_source\n\t    description: \"this is a test source\"\n\t    schema: bogus\n\t    database: bogus\n\t    tables:\n", "      - name: other_table\n\t        description: \"this is a different test table\"\n\t\"\"\"\n\texposure_yml_one_exposure = \"\"\"\n\texposures:\n\t  - name: shared_exposure\n\t    description: \"this is a test exposure\"\n\t    type: dashboard\n\t    url: yager.com/dashboard\n\t    maturity: high\n", "    owner:\n\t      name: nick yager\n\t    depends_on:\n\t      - ref('model')\n\t\"\"\"\n\texposure_yml_multiple_exposures = \"\"\"\n\texposures:\n\t  - name: shared_exposure\n\t    description: \"this is a test exposure\"\n\t    type: dashboard\n", "    url: yager.com/dashboard\n\t    maturity: high\n\t    owner:\n\t      name: nick yager\n\t    depends_on:\n\t      - ref('model')\n\t  - name: anotha_one\n\t    description: \"this is also a test exposure\"\n\t    type: dashboard\n\t    url: yager.com/dashboard2\n", "    maturity: high\n\t    owner:\n\t      name: nick yager\n\t    depends_on:\n\t      - ref('model')\n\t\"\"\"\n\texpected_remove_exposure_yml__default = \"\"\"\n\tname: shared_exposure\n\tdescription: \"this is a test exposure\"\n\ttype: dashboard\n", "url: yager.com/dashboard\n\tmaturity: high\n\towner:\n\t  name: nick yager\n\tdepends_on:\n\t  - ref('model')\n\t\"\"\"\n\texpected_remainder_yml__multiple_exposures = \"\"\"\n\texposures:\n\t  - name: anotha_one\n", "    description: \"this is also a test exposure\"\n\t    type: dashboard\n\t    url: yager.com/dashboard2\n\t    maturity: high\n\t    owner:\n\t      name: nick yager\n\t    depends_on:\n\t      - ref('model')\n\t\"\"\"\n\tmetric_yml_one_metric = \"\"\"\n", "metrics:\n\t  - name: real_good_metric\n\t    label: Real Good Metric\n\t    model: ref('model')\n\t    calculation_method: sum\n\t    expression: \"col\"\n\t\"\"\"\n\tmetric_yml_multiple_metrics = \"\"\"\n\tmetrics:\n\t  - name: real_good_metric\n", "    label: Real Good Metric\n\t    model: ref('model')\n\t    calculation_method: sum\n\t    expression: \"col\"\n\t  - name: real_bad_metric\n\t    label: Real Bad Metric\n\t    model: ref('model')\n\t    calculation_method: sum\n\t    expression: \"col2\"\n\t\"\"\"\n", "expected_remove_metric_yml__default = \"\"\"\n\tname: real_good_metric\n\tlabel: Real Good Metric\n\tmodel: ref('model')\n\tcalculation_method: sum\n\texpression: col\n\t\"\"\"\n\texpected_remainder_yml__multiple_metrics = \"\"\"\n\tmetrics:\n\t  - name: real_bad_metric\n", "    label: Real Bad Metric\n\t    model: ref('model')\n\t    calculation_method: sum\n\t    expression: col2\n\t\"\"\"\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/dbt_project_utils.py", "chunked_list": ["import shutil\n\tfrom pathlib import Path\n\tfrom dbt_meshify.dbt import Dbt\n\tdbt = Dbt()\n\tdef setup_test_project(src_project_path, dest_project_path):\n\t    src_path = Path(src_project_path)\n\t    dest_path = Path(dest_project_path)\n\t    shutil.copytree(src_path, dest_path)\n\t    dbt.invoke(directory=dest_project_path, runner_args=[\"deps\"])\n\t    dbt.invoke(directory=dest_project_path, runner_args=[\"seed\"])\n", "    dbt.invoke(directory=dest_project_path, runner_args=[\"run\"])\n\tdef teardown_test_project(dest_project_path):\n\t    dest_path = Path(dest_project_path)\n\t    shutil.rmtree(dest_path)\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import pytest\n\tfrom _pytest.logging import LogCaptureFixture\n\tfrom loguru import logger\n\t@pytest.fixture\n\tdef caplog(caplog: LogCaptureFixture):\n\t    \"\"\"\n\t    code taken from loguru docs site\n\t    https://loguru.readthedocs.io/en/stable/resources/migration.html#replacing-caplog-fixture-from-pytest-library\n\t    \"\"\"\n\t    handler_id = logger.add(\n", "        caplog.handler,\n\t        format=\"{message}\",\n\t        level=0,\n\t        filter=lambda record: record[\"level\"].no >= caplog.handler.level,\n\t        enqueue=False,  # Set to 'True' if your test is spawning child processes.\n\t    )\n\t    yield caplog\n\t    logger.remove(handler_id)\n"]}
{"filename": "tests/integration/test_split_command.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\timport yaml\n\tfrom click.testing import CliRunner\n\tfrom dbt_meshify.dbt import Dbt\n\tfrom dbt_meshify.main import split\n\tfrom tests.dbt_project_utils import setup_test_project, teardown_test_project\n\tsrc_project_path = \"test-projects/split/split_proj\"\n\tdest_project_path = \"test-projects/split/temp_proj\"\n\tclass TestSplitCommand:\n", "    def test_split_one_model_one_source(self):\n\t        setup_test_project(src_project_path, dest_project_path)\n\t        runner = CliRunner()\n\t        result = runner.invoke(\n\t            split,\n\t            [\n\t                \"my_new_project\",\n\t                \"--project-path\",\n\t                dest_project_path,\n\t                \"--select\",\n", "                \"+stg_orders\",\n\t            ],\n\t        )\n\t        assert result.exit_code == 0\n\t        assert (\n\t            Path(dest_project_path) / \"my_new_project\" / \"models\" / \"staging\" / \"stg_orders.sql\"\n\t        ).exists()\n\t        assert (\n\t            Path(dest_project_path) / \"my_new_project\" / \"models\" / \"staging\" / \"__sources.yml\"\n\t        ).exists()\n", "        source_yml = yaml.safe_load(\n\t            (Path(dest_project_path) / \"models\" / \"staging\" / \"__sources.yml\").read_text()\n\t        )\n\t        new_source_yml = yaml.safe_load(\n\t            (\n\t                Path(dest_project_path) / \"my_new_project\" / \"models\" / \"staging\" / \"__sources.yml\"\n\t            ).read_text()\n\t        )\n\t        assert len(source_yml[\"sources\"][0][\"tables\"]) == 5\n\t        assert len(new_source_yml[\"sources\"][0][\"tables\"]) == 1\n", "        x_proj_ref = \"{{ ref('my_new_project', 'stg_orders') }}\"\n\t        child_sql = (Path(dest_project_path) / \"models\" / \"marts\" / \"orders.sql\").read_text()\n\t        assert x_proj_ref in child_sql\n\t        teardown_test_project(dest_project_path)\n\t    def test_split_one_model_one_source_custom_macro(self):\n\t        setup_test_project(src_project_path, dest_project_path)\n\t        runner = CliRunner()\n\t        result = runner.invoke(\n\t            split,\n\t            [\n", "                \"my_new_project\",\n\t                \"--project-path\",\n\t                dest_project_path,\n\t                \"--select\",\n\t                \"+stg_order_items\",\n\t            ],\n\t        )\n\t        assert result.exit_code == 0\n\t        # moved model\n\t        assert (\n", "            Path(dest_project_path)\n\t            / \"my_new_project\"\n\t            / \"models\"\n\t            / \"staging\"\n\t            / \"stg_order_items.sql\"\n\t        ).exists()\n\t        # moved source\n\t        assert (\n\t            Path(dest_project_path) / \"my_new_project\" / \"models\" / \"staging\" / \"__sources.yml\"\n\t        ).exists()\n", "        # copied custom macro\n\t        assert (\n\t            Path(dest_project_path) / \"my_new_project\" / \"macros\" / \"cents_to_dollars.sql\"\n\t        ).exists()\n\t        # assert only one source moved\n\t        source_yml = yaml.safe_load(\n\t            (Path(dest_project_path) / \"models\" / \"staging\" / \"__sources.yml\").read_text()\n\t        )\n\t        new_source_yml = yaml.safe_load(\n\t            (\n", "                Path(dest_project_path) / \"my_new_project\" / \"models\" / \"staging\" / \"__sources.yml\"\n\t            ).read_text()\n\t        )\n\t        assert len(source_yml[\"sources\"][0][\"tables\"]) == 5\n\t        assert len(new_source_yml[\"sources\"][0][\"tables\"]) == 1\n\t        # assert cross project ref in child model\n\t        x_proj_ref = \"{{ ref('my_new_project', 'stg_order_items') }}\"\n\t        child_sql = (Path(dest_project_path) / \"models\" / \"marts\" / \"orders.sql\").read_text()\n\t        assert x_proj_ref in child_sql\n\t        teardown_test_project(dest_project_path)\n", "    def test_split_one_model_create_path(self):\n\t        setup_test_project(src_project_path, dest_project_path)\n\t        runner = CliRunner()\n\t        result = runner.invoke(\n\t            split,\n\t            [\n\t                \"my_new_project\",\n\t                \"--project-path\",\n\t                dest_project_path,\n\t                \"--select\",\n", "                \"+stg_orders\",\n\t                \"--create-path\",\n\t                \"test-projects/split/ham_sandwich\",\n\t            ],\n\t        )\n\t        assert result.exit_code == 0\n\t        assert (\n\t            Path(\"test-projects/split/ham_sandwich\") / \"models\" / \"staging\" / \"stg_orders.sql\"\n\t        ).exists()\n\t        x_proj_ref = \"{{ ref('my_new_project', 'stg_orders') }}\"\n", "        child_sql = (Path(dest_project_path) / \"models\" / \"marts\" / \"orders.sql\").read_text()\n\t        assert x_proj_ref in child_sql\n\t        teardown_test_project(dest_project_path)\n\t        teardown_test_project(\"test-projects/split/ham_sandwich\")\n\t    def test_split_child_project(self):\n\t        \"\"\"\n\t        Test that splitting out a project downstream of the base project splits as expected\n\t        \"\"\"\n\t        setup_test_project(src_project_path, dest_project_path)\n\t        runner = CliRunner()\n", "        result = runner.invoke(\n\t            split,\n\t            [\n\t                \"my_new_project\",\n\t                \"--project-path\",\n\t                dest_project_path,\n\t                \"--select\",\n\t                \"+stg_orders+\",\n\t            ],\n\t        )\n", "        assert result.exit_code == 0\n\t        # selected model is moved to subdirectory\n\t        assert (\n\t            Path(dest_project_path) / \"my_new_project\" / \"models\" / \"staging\" / \"stg_orders.sql\"\n\t        ).exists()\n\t        x_proj_ref = \"{{ ref('split_proj', 'stg_order_items') }}\"\n\t        child_sql = (\n\t            Path(dest_project_path) / \"my_new_project\" / \"models\" / \"marts\" / \"orders.sql\"\n\t        ).read_text()\n\t        # split model is updated to reference the parent project\n", "        assert x_proj_ref in child_sql\n\t        # dependencies.yml is moved to subdirectory, not the parent project\n\t        assert (Path(dest_project_path) / \"my_new_project\" / \"dependencies.yml\").exists()\n\t        assert not (Path(dest_project_path) / \"dependencies.yml\").exists()\n\t        teardown_test_project(dest_project_path)\n\t    def test_split_project_cycle(self):\n\t        \"\"\"\n\t        Test that splitting out a project downstream of the base project splits as expected\n\t        \"\"\"\n\t        setup_test_project(src_project_path, dest_project_path)\n", "        runner = CliRunner()\n\t        result = runner.invoke(\n\t            split,\n\t            [\n\t                \"my_new_project\",\n\t                \"--project-path\",\n\t                dest_project_path,\n\t                \"--select\",\n\t                \"orders\",\n\t            ],\n", "        )\n\t        assert result.exit_code == 1\n\t        teardown_test_project(dest_project_path)\n"]}
{"filename": "tests/integration/test_dependency_detection.py", "chunked_list": ["from pathlib import Path\n\timport pytest as pytest\n\tfrom dbt_meshify.dbt import Dbt\n\tfrom dbt_meshify.dbt_projects import DbtProject\n\tfrom dbt_meshify.linker import Linker, ProjectDependency, ProjectDependencyType\n\tclass TestLinkerSourceDependencies:\n\t    \"\"\"Test how Linker computes dependencies via the source hack.\"\"\"\n\t    @pytest.fixture\n\t    def src_proj_a(self) -> DbtProject:\n\t        \"\"\"Load the `src_proj_a` project.\"\"\"\n", "        return DbtProject.from_directory(\n\t            Path(\"test-projects/source-hack/src_proj_a/\").resolve(), read_catalog=False\n\t        )\n\t    @pytest.fixture\n\t    def src_proj_b(self) -> DbtProject:\n\t        \"\"\"Load the `src_proj_b` project.\"\"\"\n\t        return DbtProject.from_directory(\n\t            Path(\"test-projects/source-hack/src_proj_b/\").resolve(), read_catalog=False\n\t        )\n\t    @pytest.fixture\n", "    def dest_proj_a(self) -> DbtProject:\n\t        \"\"\"Load the `dest_proj_a` project.\"\"\"\n\t        path = Path(\"test-projects/source-hack/dest_proj_a/\").resolve()\n\t        # Run `dbt deps` for this project so upstream projects are loaded.\n\t        dbt = Dbt()\n\t        dbt.invoke(path, [\"deps\"])\n\t        return DbtProject.from_directory(path, read_catalog=False)\n\t    @pytest.fixture\n\t    def dest_proj_b(self) -> DbtProject:\n\t        \"\"\"Load the `dest_proj_b` project.\"\"\"\n", "        return DbtProject.from_directory(\n\t            Path(\"test-projects/source-hack/dest_proj_b/\").resolve(), read_catalog=False\n\t        )\n\t    def test_linker_detects_source_dependencies(self, src_proj_a, src_proj_b):\n\t        \"\"\"Verify that Linker detects source-hacked projects.\"\"\"\n\t        linker = Linker()\n\t        dependencies = linker.dependencies(src_proj_b, src_proj_a)\n\t        assert dependencies == {\n\t            ProjectDependency(\n\t                upstream_resource=\"model.src_proj_a.shared_model\",\n", "                upstream_project_name=\"src_proj_a\",\n\t                downstream_resource=\"source.src_proj_b.src_proj_a.shared_model\",\n\t                downstream_project_name=\"src_proj_b\",\n\t                type=ProjectDependencyType.Source,\n\t            )\n\t        }\n\t    def test_linker_detects_source_dependencies_bidirectionally(self, src_proj_a, src_proj_b):\n\t        \"\"\"Verify that Linker detects source-hacked projects when provided out of order.\"\"\"\n\t        linker = Linker()\n\t        dependencies = linker.dependencies(src_proj_a, src_proj_b)\n", "        assert dependencies == {\n\t            ProjectDependency(\n\t                upstream_resource=\"model.src_proj_a.shared_model\",\n\t                upstream_project_name=\"src_proj_a\",\n\t                downstream_resource=\"source.src_proj_b.src_proj_a.shared_model\",\n\t                downstream_project_name=\"src_proj_b\",\n\t                type=ProjectDependencyType.Source,\n\t            )\n\t        }\n\t    def test_linker_detects_package_import_dependencies(self, src_proj_a, dest_proj_a):\n", "        \"\"\"Verify that Linker detects package import dependencies.\"\"\"\n\t        linker = Linker()\n\t        dependencies = linker.dependencies(src_proj_a, dest_proj_a)\n\t        assert dependencies == {\n\t            ProjectDependency(\n\t                upstream_resource=\"model.src_proj_a.shared_model\",\n\t                upstream_project_name=\"src_proj_a\",\n\t                downstream_resource=\"model.dest_proj_a.downstream_model\",\n\t                downstream_project_name=\"dest_proj_a\",\n\t                type=ProjectDependencyType.Package,\n", "            )\n\t        }\n\t    # This doesn't exist yet as of 1.5.0. We'll test it out once it's a thing.\n\t    # def test_linker_detects_cross_project_reference_dependencies(self, src_proj_a, dest_proj_b):\n\t    #     \"\"\"Verify that Linker detects cross-project reference dependencies.\"\"\"\n\t    #\n\t    #     linker = Linker()\n\t    #     dependencies = linker.dependencies(src_proj_a, dest_proj_b)\n\t    #\n\t    #     assert dependencies == {\n", "    #         ProjectDependency(\n\t    #             upstream='source.src_proj_b.src_proj_a.shared_model',\n\t    #             downstream='model.src_proj_a.shared_model',\n\t    #             type=ProjectDependencyType.Reference\n\t    #         )\n\t    #     }\n"]}
{"filename": "tests/integration/test_create_group_command.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\timport yaml\n\tfrom click.testing import CliRunner\n\tfrom dbt_meshify.main import create_group\n\tfrom tests.unit.test_add_group_and_access_to_model_yml import (\n\t    expected_model_yml_multiple_models_multi_select,\n\t    model_yml_multiple_models,\n\t    model_yml_shared_model,\n\t)\n", "from tests.unit.test_add_group_to_yml import (\n\t    expected_group_yml_existing_groups,\n\t    expected_group_yml_no_group,\n\t    group_yml_empty_file,\n\t    group_yml_existing_groups,\n\t    group_yml_group_predefined,\n\t)\n\tproj_path_string = \"test-projects/source-hack/src_proj_a\"\n\tproj_path = Path(proj_path_string)\n\t@pytest.mark.parametrize(\n", "    \"model_yml,select,start_group_yml,end_group_yml\",\n\t    [\n\t        (\n\t            model_yml_shared_model,\n\t            \"shared_model\",\n\t            group_yml_empty_file,\n\t            expected_group_yml_no_group,\n\t        ),\n\t        (\n\t            model_yml_shared_model,\n", "            \"shared_model\",\n\t            group_yml_existing_groups,\n\t            expected_group_yml_existing_groups,\n\t        ),\n\t        (\n\t            model_yml_shared_model,\n\t            \"shared_model\",\n\t            group_yml_group_predefined,\n\t            expected_group_yml_no_group,\n\t        ),\n", "        (model_yml_shared_model, \"\", group_yml_empty_file, expected_group_yml_no_group),\n\t    ],\n\t    ids=[\"1\", \"2\", \"3\", \"4\"],\n\t)\n\tdef test_create_group_command(model_yml, select, start_group_yml, end_group_yml):\n\t    group_yml_file = proj_path / \"models\" / \"_groups.yml\"\n\t    model_yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    group_yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    model_yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    start_yml_content = yaml.safe_load(model_yml)\n", "    with open(model_yml_file, \"w\") as f:\n\t        yaml.safe_dump(start_yml_content, f, sort_keys=False)\n\t    start_group_yml_content = yaml.safe_load(start_group_yml)\n\t    with open(group_yml_file, \"w\") as f:\n\t        yaml.safe_dump(start_group_yml_content, f, sort_keys=False)\n\t    runner = CliRunner()\n\t    result = runner.invoke(\n\t        create_group,\n\t        [\n\t            \"test_group\",\n", "            \"--select\",\n\t            select,\n\t            \"--project-path\",\n\t            proj_path_string,\n\t            \"--owner-name\",\n\t            \"Shaina Fake\",\n\t            \"--owner-email\",\n\t            \"fake@example.com\",\n\t        ],\n\t    )\n", "    with open(group_yml_file, \"r\") as f:\n\t        actual = yaml.safe_load(f)\n\t    group_yml_file.unlink()\n\t    model_yml_file.unlink()\n\t    assert result.exit_code == 0\n\t    assert actual == yaml.safe_load(end_group_yml)\n\t@pytest.mark.parametrize(\n\t    \"start_model_yml, end_model_yml, start_group_yml, end_group_yml\",\n\t    [\n\t        (\n", "            model_yml_multiple_models,\n\t            expected_model_yml_multiple_models_multi_select,\n\t            group_yml_empty_file,\n\t            expected_group_yml_no_group,\n\t        ),\n\t    ],\n\t    ids=[\"1\"],\n\t)\n\tdef test_create_group_command_multi_select(\n\t    start_model_yml, end_model_yml, start_group_yml, end_group_yml\n", "):\n\t    group_yml_file = proj_path / \"models\" / \"_groups.yml\"\n\t    model_yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    other_model_file = proj_path / \"models\" / \"other_model.sql\"\n\t    group_yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    model_yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    other_model_file.parent.mkdir(parents=True, exist_ok=True)\n\t    start_yml_content = yaml.safe_load(start_model_yml)\n\t    with open(model_yml_file, \"w\") as f:\n\t        yaml.safe_dump(start_yml_content, f, sort_keys=False)\n", "    start_group_yml_content = yaml.safe_load(start_group_yml)\n\t    with open(group_yml_file, \"w\") as f:\n\t        yaml.safe_dump(start_group_yml_content, f, sort_keys=False)\n\t    with open(other_model_file, \"w\") as f:\n\t        f.write(\"select 1 as test\")\n\t    runner = CliRunner()\n\t    result = runner.invoke(\n\t        create_group,\n\t        [\n\t            \"test_group\",\n", "            \"--select\",\n\t            \"shared_model\",\n\t            \"other_model\",\n\t            \"--project-path\",\n\t            proj_path_string,\n\t            \"--owner-name\",\n\t            \"Shaina Fake\",\n\t            \"--owner-email\",\n\t            \"fake@example.com\",\n\t        ],\n", "    )\n\t    with open(group_yml_file, \"r\") as f:\n\t        actual_group_yml = yaml.safe_load(f)\n\t    with open(model_yml_file, \"r\") as f:\n\t        actual_model_yml = yaml.safe_load(f)\n\t    group_yml_file.unlink()\n\t    model_yml_file.unlink()\n\t    other_model_file.unlink()\n\t    assert result.exit_code == 0\n\t    assert actual_group_yml == yaml.safe_load(end_group_yml)\n", "    assert actual_model_yml == yaml.safe_load(end_model_yml)\n\t@pytest.mark.parametrize(\n\t    \"name,email,end_group_yml\",\n\t    [\n\t        (\"Tony Legitman\", None, expected_group_yml_no_group),\n\t        (None, \"tony@notacop.org\", expected_group_yml_no_group),\n\t        (\"Tony Legitman\", \"tony@notacop.org\", expected_group_yml_no_group),\n\t    ],\n\t    ids=[\"1\", \"2\", \"3\"],\n\t)\n", "def test_group_group_owner_properties(name, email, end_group_yml):\n\t    group_yml_file = proj_path / \"models\" / \"_groups.yml\"\n\t    model_yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    group_yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    model_yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    start_yml_content = yaml.safe_load(model_yml_shared_model)\n\t    with open(model_yml_file, \"w\") as f:\n\t        yaml.safe_dump(start_yml_content, f, sort_keys=False)\n\t    start_group_yml_content = yaml.safe_load(group_yml_empty_file)\n\t    with open(group_yml_file, \"w\") as f:\n", "        yaml.safe_dump(start_group_yml_content, f, sort_keys=False)\n\t    args = [\"test_group\", \"--select\", \"shared_model\", \"--project-path\", proj_path_string]\n\t    if name:\n\t        args += [\"--owner-name\", name]\n\t    if email:\n\t        args += [\"--owner-email\", email]\n\t    runner = CliRunner()\n\t    result = runner.invoke(create_group, args)\n\t    with open(group_yml_file, \"r\") as f:\n\t        actual = yaml.safe_load(f)\n", "    group_yml_file.unlink()\n\t    model_yml_file.unlink()\n\t    assert result.exit_code == 0\n\t    end_group_content = yaml.safe_load(end_group_yml)\n\t    if name:\n\t        end_group_content[\"groups\"][0][\"owner\"][\"name\"] = name\n\t    else:\n\t        del end_group_content[\"groups\"][0][\"owner\"][\"name\"]\n\t    if email:\n\t        end_group_content[\"groups\"][0][\"owner\"][\"email\"] = email\n", "    else:\n\t        del end_group_content[\"groups\"][0][\"owner\"][\"email\"]\n\t    assert actual == end_group_content\n"]}
{"filename": "tests/integration/test_connect_command.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\tfrom click.testing import CliRunner\n\tfrom dbt_meshify.main import connect\n\tfrom tests.dbt_project_utils import setup_test_project, teardown_test_project\n\tproducer_project_path = \"test-projects/source-hack/src_proj_a\"\n\tsource_consumer_project_path = \"test-projects/source-hack/src_proj_b\"\n\tpackage_consumer_project_path = \"test-projects/source-hack/dest_proj_a\"\n\tcopy_producer_project_path = producer_project_path + \"_copy\"\n\tcopy_source_consumer_project_path = source_consumer_project_path + \"_copy\"\n", "copy_package_consumer_project_path = package_consumer_project_path + \"_copy\"\n\t@pytest.fixture\n\tdef producer_project():\n\t    setup_test_project(producer_project_path, copy_producer_project_path)\n\t    setup_test_project(source_consumer_project_path, copy_source_consumer_project_path)\n\t    setup_test_project(package_consumer_project_path, copy_package_consumer_project_path)\n\t    # yield to the test. We'll come back here after the test returns.\n\t    yield\n\t    teardown_test_project(copy_producer_project_path)\n\t    teardown_test_project(copy_source_consumer_project_path)\n", "    teardown_test_project(copy_package_consumer_project_path)\n\tclass TestConnectCommand:\n\t    def test_connect_source_hack(self, producer_project):\n\t        runner = CliRunner()\n\t        result = runner.invoke(\n\t            connect,\n\t            [\"--project-paths\", copy_producer_project_path, copy_source_consumer_project_path],\n\t        )\n\t        assert result.exit_code == 0\n\t        # assert that the source is replaced with a ref\n", "        x_proj_ref = \"{{ ref('src_proj_a', 'shared_model') }}\"\n\t        source_func = \"{{ source('src_proj_a', 'shared_model') }}\"\n\t        child_sql = (\n\t            Path(copy_source_consumer_project_path) / \"models\" / \"downstream_model.sql\"\n\t        ).read_text()\n\t        assert x_proj_ref in child_sql\n\t        assert source_func not in child_sql\n\t        # assert that the source was deleted\n\t        # may want to add some nuance in the future for cases where we delete a single source out of a set\n\t        assert not (\n", "            Path(copy_source_consumer_project_path) / \"models\" / \"staging\" / \"_sources.yml\"\n\t        ).exists()\n\t        # assert that the dependecies yml was created with a pointer to the upstream project\n\t        assert (\n\t            \"src_proj_a\"\n\t            in (Path(copy_source_consumer_project_path) / \"dependencies.yml\").read_text()\n\t        )\n\t    def test_connect_package(self, producer_project):\n\t        runner = CliRunner()\n\t        result = runner.invoke(\n", "            connect,\n\t            [\"--project-paths\", copy_producer_project_path, copy_package_consumer_project_path],\n\t        )\n\t        assert result.exit_code == 0\n\t        # assert that the source is replaced with a ref\n\t        x_proj_ref = \"{{ ref('src_proj_a', 'shared_model') }}\"\n\t        child_sql = (\n\t            Path(copy_package_consumer_project_path) / \"models\" / \"downstream_model.sql\"\n\t        ).read_text()\n\t        assert x_proj_ref in child_sql\n", "        # assert that the dependecies yml was created with a pointer to the upstream project\n\t        assert (\n\t            \"src_proj_a\"\n\t            in (Path(copy_package_consumer_project_path) / \"dependencies.yml\").read_text()\n\t        )\n\t    def test_packages_dir_exclude_packages(self):\n\t        # copy all three packages into a subdirectory to ensure no repeat project names in one directory\n\t        subdir_producer_project = \"test-projects/source-hack/subdir/src_proj_a\"\n\t        subdir_source_consumer_project = \"test-projects/source-hack/subdir/src_proj_b\"\n\t        subdir_package_consumer_project = \"test-projects/source-hack/subdir/dest_proj_a\"\n", "        setup_test_project(producer_project_path, subdir_producer_project)\n\t        setup_test_project(source_consumer_project_path, subdir_source_consumer_project)\n\t        setup_test_project(package_consumer_project_path, subdir_package_consumer_project)\n\t        runner = CliRunner()\n\t        result = runner.invoke(\n\t            connect,\n\t            [\n\t                \"--projects-dir\",\n\t                \"test-projects/source-hack/subdir\",\n\t                \"--exclude-projects\",\n", "                \"src_proj_b\",\n\t            ],\n\t        )\n\t        assert result.exit_code == 0\n\t        # assert that the source is replaced with a ref\n\t        x_proj_ref = \"{{ ref('src_proj_a', 'shared_model') }}\"\n\t        child_sql = (\n\t            Path(subdir_package_consumer_project) / \"models\" / \"downstream_model.sql\"\n\t        ).read_text()\n\t        assert x_proj_ref in child_sql\n", "        # assert that the dependecies yml was created with a pointer to the upstream project\n\t        assert (\n\t            \"src_proj_a\"\n\t            in (Path(subdir_package_consumer_project) / \"dependencies.yml\").read_text()\n\t        )\n\t        teardown_test_project(subdir_producer_project)\n\t        teardown_test_project(subdir_package_consumer_project)\n\t        teardown_test_project(subdir_source_consumer_project)\n"]}
{"filename": "tests/integration/test_group_command.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\tfrom click.testing import CliRunner\n\tfrom dbt_meshify.dbt_projects import DbtProject\n\tfrom dbt_meshify.main import group\n\tfrom tests.dbt_project_utils import setup_test_project, teardown_test_project\n\tsrc_path_string = \"test-projects/split/split_proj\"\n\tdest_path_string = \"test-projects/split/temp_proj\"\n\tproj_path = Path(dest_path_string)\n\t# this test should encapsulate the following:\n", "# 1. group is created in the project with proper yml\n\t# 2. public models also have contracts\n\t# since we handle the creation and validation of yml in other tests, this shouldn't need many fixtures\n\t@pytest.mark.parametrize(\n\t    \"select,expected_public_contracted_models\",\n\t    [\n\t        (\n\t            \"+orders\",\n\t            [\"orders\"],\n\t        ),\n", "    ],\n\t    ids=[\"1\"],\n\t)\n\tdef test_group_command(select, expected_public_contracted_models):\n\t    setup_test_project(src_path_string, dest_path_string)\n\t    runner = CliRunner()\n\t    result = runner.invoke(\n\t        group,\n\t        [\n\t            \"test_group\",\n", "            \"--owner-name\",\n\t            \"Teenage Mutant Jinja Turtles\",\n\t            \"--select\",\n\t            select,\n\t            \"--project-path\",\n\t            dest_path_string,\n\t        ],\n\t    )\n\t    assert result.exit_code == 0\n\t    project = DbtProject.from_directory(proj_path, read_catalog=False)\n", "    # ensure that the correct set of public models is created\n\t    public_contracted_models = [\n\t        model.name\n\t        for model_key, model in project.models.items()\n\t        if model.access == \"public\" and model.config.contract.enforced\n\t    ]\n\t    assert public_contracted_models == expected_public_contracted_models\n\t    teardown_test_project(dest_path_string)\n"]}
{"filename": "tests/integration/test_version_command.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\timport yaml\n\tfrom click.testing import CliRunner\n\tfrom dbt_meshify.main import add_version\n\tfrom ..sql_and_yml_fixtures import (\n\t    expected_versioned_model_yml_increment_prerelease_version,\n\t    expected_versioned_model_yml_increment_prerelease_version_with_second_prerelease,\n\t    expected_versioned_model_yml_increment_version_defined_in,\n\t    expected_versioned_model_yml_increment_version_no_prerelease,\n", "    expected_versioned_model_yml_increment_version_with_prerelease,\n\t    expected_versioned_model_yml_no_version,\n\t    expected_versioned_model_yml_no_yml,\n\t    model_yml_increment_version,\n\t    model_yml_no_col_no_version,\n\t    model_yml_string_version,\n\t    shared_model_sql,\n\t)\n\tproj_path_string = \"test-projects/source-hack/src_proj_a\"\n\tproj_path = Path(proj_path_string)\n", "def reset_model_files(files_list):\n\t    models_dir = proj_path / \"models\"\n\t    for file in models_dir.glob(\"*\"):\n\t        if file.is_file():\n\t            file.unlink()\n\t    for file in files_list:\n\t        file_path = proj_path / \"models\" / file\n\t        if not file_path.is_file():\n\t            with file_path.open(\"w+\") as f:\n\t                f.write(shared_model_sql)\n", "@pytest.mark.parametrize(\n\t    \"start_yml,end_yml,start_files,expected_files,command_options\",\n\t    [\n\t        (\n\t            model_yml_no_col_no_version,\n\t            expected_versioned_model_yml_no_version,\n\t            [\"shared_model.sql\"],\n\t            [\"shared_model_v1.sql\"],\n\t            [],\n\t        ),\n", "        (\n\t            model_yml_increment_version,\n\t            expected_versioned_model_yml_increment_version_no_prerelease,\n\t            [\"shared_model.sql\"],\n\t            [\"shared_model_v1.sql\", \"shared_model_v2.sql\"],\n\t            [],\n\t        ),\n\t        (\n\t            model_yml_increment_version,\n\t            expected_versioned_model_yml_increment_version_with_prerelease,\n", "            [\"shared_model.sql\"],\n\t            [\"shared_model_v1.sql\", \"shared_model_v2.sql\"],\n\t            [\"--prerelease\"],\n\t        ),\n\t        (\n\t            model_yml_increment_version,\n\t            expected_versioned_model_yml_increment_version_defined_in,\n\t            [\"shared_model.sql\"],\n\t            [\"shared_model_v1.sql\", \"daves_model.sql\"],\n\t            [\"--defined-in\", \"daves_model\"],\n", "        ),\n\t        (\n\t            None,\n\t            expected_versioned_model_yml_no_yml,\n\t            [\"shared_model.sql\"],\n\t            [\"shared_model_v1.sql\"],\n\t            [],\n\t        ),\n\t        (\n\t            expected_versioned_model_yml_increment_version_with_prerelease,\n", "            expected_versioned_model_yml_increment_prerelease_version_with_second_prerelease,\n\t            [\"shared_model_v1.sql\", \"shared_model_v2.sql\"],\n\t            [\"shared_model_v1.sql\", \"shared_model_v2.sql\", \"shared_model_v3.sql\"],\n\t            [\"--prerelease\"],\n\t        ),\n\t        (\n\t            expected_versioned_model_yml_increment_version_with_prerelease,\n\t            expected_versioned_model_yml_increment_prerelease_version,\n\t            [\"shared_model_v1.sql\", \"shared_model_v2.sql\"],\n\t            [\"shared_model_v1.sql\", \"shared_model_v2.sql\", \"shared_model_v3.sql\"],\n", "            [],\n\t        ),\n\t    ],\n\t    ids=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"],\n\t)\n\tdef test_add_version_to_yml(start_yml, end_yml, start_files, expected_files, command_options):\n\t    yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    reset_model_files(start_files)\n\t    yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    runner = CliRunner()\n", "    # only create file if start_yml is not None\n\t    # in situations where models don't have a patch path, there isn't a yml file to read from\n\t    if start_yml:\n\t        yml_file.touch()\n\t        start_yml_content = yaml.safe_load(start_yml)\n\t        with open(yml_file, \"w+\") as f:\n\t            yaml.safe_dump(start_yml_content, f, sort_keys=False)\n\t    base_command = [\"--select\", \"shared_model\", \"--project-path\", proj_path_string]\n\t    base_command.extend(command_options)\n\t    result = runner.invoke(add_version, base_command)\n", "    assert result.exit_code == 0\n\t    # reset the read path to the default in the logic\n\t    with open(yml_file, \"r\") as f:\n\t        actual = yaml.safe_load(f)\n\t    for file in expected_files:\n\t        path = proj_path / \"models\" / file\n\t        try:\n\t            assert path.is_file()\n\t        except:\n\t            print(f\"File {file} not found\")\n", "        path.unlink()\n\t    yml_file.unlink()\n\t    reset_model_files([\"shared_model.sql\"])\n\t    assert actual == yaml.safe_load(end_yml)\n\t@pytest.mark.parametrize(\n\t    \"start_yml,start_files\",\n\t    [\n\t        (\n\t            model_yml_string_version,\n\t            [\"shared_model.sql\"],\n", "        ),\n\t    ],\n\t    ids=[\"1\"],\n\t)\n\tdef test_add_version_to_invalid_yml(start_yml, start_files):\n\t    yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    reset_model_files(start_files)\n\t    yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    runner = CliRunner()\n\t    # only create file if start_yml is not None\n", "    # in situations where models don't have a patch path, there isn't a yml file to read from\n\t    if start_yml:\n\t        yml_file.touch()\n\t        start_yml_content = yaml.safe_load(start_yml)\n\t        with open(yml_file, \"w+\") as f:\n\t            yaml.safe_dump(start_yml_content, f, sort_keys=False)\n\t    base_command = [\"--select\", \"shared_model\", \"--project-path\", proj_path_string]\n\t    result = runner.invoke(add_version, base_command, catch_exceptions=True)\n\t    assert result.exit_code == 1\n\t    # reset the read path to the default in the logic\n", "    yml_file.unlink()\n\t    reset_model_files([\"shared_model.sql\"])\n"]}
{"filename": "tests/integration/__init__.py", "chunked_list": []}
{"filename": "tests/integration/test_subproject_creator.py", "chunked_list": ["import os\n\timport subprocess\n\tfrom pathlib import Path\n\timport yaml\n\tfrom dbt_meshify.dbt import Dbt\n\tfrom dbt_meshify.dbt_projects import DbtProject\n\tfrom dbt_meshify.storage.dbt_project_editors import DbtSubprojectCreator\n\tfrom dbt_meshify.storage.file_content_editors import DbtMeshConstructor\n\ttest_project_profile = yaml.safe_load(\n\t    \"\"\"\n", "test:\n\t  outputs:\n\t    dev:\n\t      path: ./database.db\n\t      threads: 2\n\t      type: duckdb\n\t  target: dev\n\t\"\"\"\n\t)\n\ttest_package_yml = yaml.safe_load(\n", "    \"\"\"\n\tpackages:\n\t  - package: dbt-labs/dbt_utils\n\t    version: 1.0.0\n\t\"\"\"\n\t)\n\tmodel_unique_id = \"model.test.my_first_dbt_model\"\n\tdef setup_new_project(write_packages_yml: bool = False):\n\t    dbt = Dbt()\n\t    subprocess.run([\"poetry\", \"run\", \"dbt\", \"init\", \"test\", \"-s\"])\n", "    with open(\"test/profiles.yml\", \"w\") as f:\n\t        f.write(yaml.dump(test_project_profile))\n\t    if write_packages_yml:\n\t        with open(\"test/packages.yml\", \"w\") as f:\n\t            f.write(yaml.dump(test_package_yml))\n\t        dbt.invoke(directory=Path(\"test\"), runner_args=[\"deps\"])\n\tdef split_project(select: str = \"my_first_dbt_model\"):\n\t    project = DbtProject.from_directory(Path(\"test\"), read_catalog=False)\n\t    subproject = project.split(project_name='subdir', select=select)\n\t    return subproject\n", "def get_meshify_constructor(subproject, unique_id):\n\t    resource = subproject.get_manifest_node(unique_id)\n\t    if not resource:\n\t        raise KeyError(f\"Resource {unique_id} not found in manifest\")\n\t    meshify_constructor = DbtMeshConstructor(\n\t        project_path=subproject.path, node=resource, catalog=None\n\t    )\n\t    return meshify_constructor\n\tdef teardown_new_project():\n\t    os.system(\"rm -rf test-projects/test\")\n", "class TestDbtSubprojectCreator:\n\t    def test_move_model_file(self) -> None:\n\t        starting_directory = os.getcwd()\n\t        os.chdir(Path(\"test-projects\"))\n\t        setup_new_project()\n\t        original_contents = Path(\"test/models/example/my_first_dbt_model.sql\").read_text()\n\t        subproject = split_project()\n\t        meshify_constructor = get_meshify_constructor(subproject, model_unique_id)\n\t        creator = DbtSubprojectCreator(subproject)\n\t        creator.move_resource(meshify_constructor)\n", "        assert Path(\"test/subdir/models/example/my_first_dbt_model.sql\").exists()\n\t        assert not Path(\"test/models/example/my_first_dbt_model.sql\").exists()\n\t        assert (\n\t            Path(\"test/subdir/models/example/my_first_dbt_model.sql\").read_text()\n\t            == original_contents\n\t        )\n\t        os.chdir(starting_directory)\n\t        teardown_new_project()\n\t    def test_copy_model_file(self) -> None:\n\t        starting_directory = os.getcwd()\n", "        os.chdir(Path(\"test-projects\"))\n\t        setup_new_project()\n\t        subproject = split_project()\n\t        meshify_constructor = get_meshify_constructor(subproject, model_unique_id)\n\t        creator = DbtSubprojectCreator(subproject)\n\t        creator.copy_resource(meshify_constructor)\n\t        assert Path(\"test/subdir/models/example/my_first_dbt_model.sql\").exists()\n\t        assert Path(\"test/models/example/my_first_dbt_model.sql\").exists()\n\t        assert (\n\t            Path(\"test/subdir/models/example/my_first_dbt_model.sql\").read_text()\n", "            == Path(\"test/models/example/my_first_dbt_model.sql\").read_text()\n\t        )\n\t        os.chdir(starting_directory)\n\t        teardown_new_project()\n\t    def test_move_yml_entry(self) -> None:\n\t        starting_directory = os.getcwd()\n\t        os.chdir(Path(\"test-projects\"))\n\t        setup_new_project()\n\t        subproject = split_project()\n\t        meshify_constructor = get_meshify_constructor(subproject, model_unique_id)\n", "        creator = DbtSubprojectCreator(subproject)\n\t        creator.update_resource_yml_entry(meshify_constructor)\n\t        # the original path should still exist, since we take only the single model entry\n\t        assert Path(\"test/models/example/schema.yml\").exists()\n\t        assert Path(\"test/subdir/models/example/schema.yml\").exists()\n\t        os.chdir(starting_directory)\n\t        teardown_new_project()\n\t    def test_write_project_yml(self) -> None:\n\t        starting_directory = os.getcwd()\n\t        os.chdir(Path(\"test-projects\"))\n", "        setup_new_project()\n\t        subproject = split_project()\n\t        creator = DbtSubprojectCreator(subproject)\n\t        creator.write_project_file()\n\t        # the original path should still exist, since we take only the single model entry\n\t        assert Path(\"test/dbt_project.yml\").exists()\n\t        assert Path(\"test/subdir/dbt_project.yml\").exists()\n\t        os.chdir(starting_directory)\n\t        teardown_new_project()\n\t    def test_write_packages_yml(self) -> None:\n", "        starting_directory = os.getcwd()\n\t        os.chdir(Path(\"test-projects\"))\n\t        setup_new_project(write_packages_yml=True)\n\t        subproject = split_project()\n\t        creator = DbtSubprojectCreator(subproject)\n\t        creator.copy_packages_yml_file()\n\t        # the original path should still exist, since we take only the single model entry\n\t        assert Path(\"test/packages.yml\").exists()\n\t        assert Path(\"test/subdir/packages.yml\").exists()\n\t        os.chdir(starting_directory)\n", "        teardown_new_project()\n\t    def test_write_dependencies_yml(self) -> None:\n\t        starting_directory = os.getcwd()\n\t        os.chdir(Path(\"test-projects\"))\n\t        setup_new_project(write_packages_yml=True)\n\t        subproject = split_project()\n\t        creator = DbtSubprojectCreator(subproject)\n\t        creator.update_dependencies_yml()\n\t        # the original path should still exist, since we take only the single model entry\n\t        assert Path(\"test/dependencies.yml\").exists()\n", "        os.chdir(starting_directory)\n\t        teardown_new_project()\n"]}
{"filename": "tests/integration/test_contract_command.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\timport yaml\n\tfrom click.testing import CliRunner\n\tfrom dbt_meshify.dbt import Dbt\n\tfrom dbt_meshify.main import add_contract\n\tfrom ..sql_and_yml_fixtures import (\n\t    expected_contract_yml_all_col,\n\t    expected_contract_yml_no_col,\n\t    expected_contract_yml_no_entry,\n", "    expected_contract_yml_one_col,\n\t    expected_contract_yml_other_model,\n\t    expected_contract_yml_sequential,\n\t    model_yml_all_col,\n\t    model_yml_no_col_no_version,\n\t    model_yml_one_col,\n\t    model_yml_other_model,\n\t)\n\tproj_path_string = \"test-projects/source-hack/src_proj_a\"\n\tproj_path = Path(proj_path_string)\n", "dbt = Dbt()\n\t@pytest.mark.parametrize(\n\t    \"start_yml,end_yml\",\n\t    [\n\t        (model_yml_no_col_no_version, expected_contract_yml_no_col),\n\t        (model_yml_one_col, expected_contract_yml_one_col),\n\t        (model_yml_all_col, expected_contract_yml_all_col),\n\t        (None, expected_contract_yml_no_entry),\n\t        (model_yml_other_model, expected_contract_yml_other_model),\n\t    ],\n", "    ids=[\"1\", \"2\", \"3\", \"4\", \"5\"],\n\t)\n\tdef test_add_contract_to_yml(start_yml, end_yml):\n\t    yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    runner = CliRunner()\n\t    # do a dbt run to create the duckdb\n\t    # and enable the docs generate command to work\n\t    dbt.invoke(directory=proj_path, runner_args=[\"run\"])\n\t    # only create file if start_yml is not None\n", "    # in situations where models don't have a patch path, there isn't a yml file to read from\n\t    if start_yml:\n\t        yml_file.touch()\n\t        start_yml_content = yaml.safe_load(start_yml)\n\t        with open(yml_file, \"w+\") as f:\n\t            yaml.safe_dump(start_yml_content, f, sort_keys=False)\n\t    result = runner.invoke(\n\t        add_contract, [\"--select\", \"shared_model\", \"--project-path\", proj_path_string]\n\t    )\n\t    assert result.exit_code == 0\n", "    # reset the read path to the default in the logic\n\t    with open(yml_file, \"r\") as f:\n\t        actual = yaml.safe_load(f)\n\t    yml_file.unlink()\n\t    assert actual == yaml.safe_load(end_yml)\n\t@pytest.mark.parametrize(\n\t    \"start_yml,end_yml\",\n\t    [\n\t        (None, expected_contract_yml_sequential),\n\t    ],\n", "    ids=[\"1\"],\n\t)\n\tdef test_add_squential_contracts_to_yml(start_yml, end_yml):\n\t    yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    runner = CliRunner()\n\t    # do a dbt run to create the duckdb\n\t    # and enable the docs generate command to work\n\t    dbt.invoke(directory=proj_path, runner_args=[\"run\"])\n\t    result = runner.invoke(\n\t        add_contract, [\"--select\", \"shared_model\", \"--project-path\", proj_path_string]\n", "    )\n\t    assert result.exit_code == 0\n\t    result2 = runner.invoke(\n\t        add_contract, [\"--select\", \"new_model\", \"--project-path\", proj_path_string]\n\t    )\n\t    assert result2.exit_code == 0\n\t    # reset the read path to the default in the logic\n\t    with open(yml_file, \"r\") as f:\n\t        actual = yaml.safe_load(f)\n\t    yml_file.unlink()\n", "    assert actual == yaml.safe_load(end_yml)\n\t@pytest.mark.parametrize(\n\t    \"start_yml,end_yml,read_catalog\",\n\t    [\n\t        (model_yml_no_col_no_version, expected_contract_yml_no_col, True),\n\t        (model_yml_no_col_no_version, expected_contract_yml_no_col, False),\n\t    ],\n\t    ids=[\"1\", \"2\"],\n\t)\n\tdef test_add_contract_read_catalog(start_yml, end_yml, read_catalog, caplog):\n", "    yml_file = proj_path / \"models\" / \"_models.yml\"\n\t    yml_file.parent.mkdir(parents=True, exist_ok=True)\n\t    runner = CliRunner()\n\t    # do a dbt run to create the duckdb\n\t    # and enable the docs generate command to work\n\t    dbt.invoke(directory=proj_path, runner_args=[\"run\"])\n\t    # only create file if start_yml is not None\n\t    # in situations where models don't have a patch path, there isn't a yml file to read from\n\t    if start_yml:\n\t        yml_file.touch()\n", "        start_yml_content = yaml.safe_load(start_yml)\n\t        with open(yml_file, \"w+\") as f:\n\t            yaml.safe_dump(start_yml_content, f, sort_keys=False)\n\t    args = [\"--select\", \"shared_model\", \"--project-path\", proj_path_string]\n\t    if read_catalog:\n\t        args.append(\"--read-catalog\")\n\t    result = runner.invoke(add_contract, args)\n\t    assert result.exit_code == 0\n\t    # reset the read path to the default in the logic\n\t    with open(yml_file, \"r\") as f:\n", "        actual = yaml.safe_load(f)\n\t    yml_file.unlink()\n\t    assert actual == yaml.safe_load(end_yml)\n\t    if read_catalog:\n\t        assert \"Reading catalog from\" in caplog.text\n\t        assert \"Generating catalog with dbt docs generate...\" not in caplog.text\n\t    else:\n\t        assert \"Reading catalog from\" not in caplog.text\n\t        assert \"Generating catalog with dbt docs generate...\" in caplog.text\n"]}
{"filename": "tests/unit/test_move_yml_entries.py", "chunked_list": ["import yaml\n\tfrom dbt.node_types import NodeType\n\tfrom dbt_meshify.storage.file_content_editors import DbtMeshFileEditor\n\tfrom ..sql_and_yml_fixtures import (\n\t    expeceted_remainder_yml__source_yml_multiple_tables,\n\t    expected_remainder_yml__model_yml_other_model,\n\t    expected_remainder_yml__multiple_exposures,\n\t    expected_remainder_yml__multiple_metrics,\n\t    expected_remove_exposure_yml__default,\n\t    expected_remove_metric_yml__default,\n", "    expected_remove_model_yml__default,\n\t    expected_remove_model_yml__model_yml_no_col_no_version,\n\t    expected_remove_model_yml__model_yml_one_col,\n\t    expected_remove_source_yml__default,\n\t    exposure_yml_multiple_exposures,\n\t    exposure_yml_one_exposure,\n\t    metric_yml_multiple_metrics,\n\t    metric_yml_one_metric,\n\t    model_yml_no_col_no_version,\n\t    model_yml_one_col,\n", "    model_yml_other_model,\n\t    source_yml_multiple_tables,\n\t    source_yml_one_table,\n\t)\n\tmeshify = DbtMeshFileEditor()\n\tmodel_name = \"shared_model\"\n\tsource_name = \"test_source\"\n\tsource_table_name = \"table\"\n\texposure_name = \"shared_exposure\"\n\tmetric_name = \"real_good_metric\"\n", "def read_yml(yml_str):\n\t    return yaml.safe_load(yml_str)\n\tclass TestRemoveResourceYml:\n\t    def test_remove_model_yml_simple(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=model_name,\n\t            full_yml=read_yml(model_yml_no_col_no_version),\n\t            resource_type=NodeType.Model,\n\t            source_name=None,\n\t        )\n", "        assert resource_yml == read_yml(expected_remove_model_yml__model_yml_no_col_no_version)\n\t        assert full_yml == None\n\t    def test_remove_model_yml_simple_with_description(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=model_name,\n\t            full_yml=read_yml(model_yml_one_col),\n\t            resource_type=NodeType.Model,\n\t            source_name=None,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_model_yml__model_yml_one_col)\n", "        assert full_yml == None\n\t    def test_remove_model_yml_other_model(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=model_name,\n\t            full_yml=read_yml(model_yml_other_model),\n\t            resource_type=NodeType.Model,\n\t            source_name=None,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_model_yml__default)\n\t        assert full_yml == read_yml(expected_remainder_yml__model_yml_other_model)\n", "    def test_remove_source_yml_one_table(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=source_table_name,\n\t            full_yml=read_yml(source_yml_one_table),\n\t            resource_type=NodeType.Source,\n\t            source_name=source_name,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_source_yml__default)\n\t        assert full_yml == None\n\t    def test_remove_source_yml_multiple_table(self):\n", "        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=source_table_name,\n\t            full_yml=read_yml(source_yml_multiple_tables),\n\t            resource_type=NodeType.Source,\n\t            source_name=source_name,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_source_yml__default)\n\t        assert full_yml == read_yml(expeceted_remainder_yml__source_yml_multiple_tables)\n\t    def test_remove_exposure_yml_one_exposure(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n", "            resource_name=exposure_name,\n\t            full_yml=read_yml(exposure_yml_one_exposure),\n\t            resource_type=NodeType.Exposure,\n\t            source_name=None,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_exposure_yml__default)\n\t        assert full_yml == None\n\t    def test_remove_exposure_yml_multiple_exposures(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=exposure_name,\n", "            full_yml=read_yml(exposure_yml_multiple_exposures),\n\t            resource_type=NodeType.Exposure,\n\t            source_name=None,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_exposure_yml__default)\n\t        assert full_yml == read_yml(expected_remainder_yml__multiple_exposures)\n\t    def test_remove_metric_yml_one_metric(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=metric_name,\n\t            full_yml=read_yml(metric_yml_one_metric),\n", "            resource_type=NodeType.Metric,\n\t            source_name=None,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_metric_yml__default)\n\t        assert full_yml == None\n\t    def test_remove_metric_yml_multiple_metrics(self):\n\t        resource_yml, full_yml = meshify.get_yml_entry(\n\t            resource_name=metric_name,\n\t            full_yml=read_yml(metric_yml_multiple_metrics),\n\t            resource_type=NodeType.Metric,\n", "            source_name=None,\n\t        )\n\t        assert resource_yml == read_yml(expected_remove_metric_yml__default)\n\t        assert full_yml == read_yml(expected_remainder_yml__multiple_metrics)\n\tclass TestAddResourceYml:\n\t    def test_add_model_yml_simple(self):\n\t        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_model_yml__model_yml_no_col_no_version),\n\t            full_yml=None,\n\t            resource_type=NodeType.Model,\n", "        )\n\t        assert full_yml == read_yml(model_yml_no_col_no_version)\n\t    def test_add_model_yml_other_model(self):\n\t        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_model_yml__default),\n\t            full_yml=read_yml(expected_remainder_yml__model_yml_other_model),\n\t            resource_type=NodeType.Model,\n\t        )\n\t        assert full_yml == read_yml(model_yml_other_model)\n\t    def test_add_source_yml_one_table(self):\n", "        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_source_yml__default),\n\t            full_yml=None,\n\t            resource_type=NodeType.Source,\n\t        )\n\t        assert full_yml == read_yml(source_yml_one_table)\n\t    def test_add_source_yml_multiple_table(self):\n\t        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_source_yml__default),\n\t            full_yml=read_yml(expeceted_remainder_yml__source_yml_multiple_tables),\n", "            resource_type=NodeType.Source,\n\t        )\n\t        expected = read_yml(source_yml_multiple_tables)\n\t        source_entry = list(filter(lambda x: x[\"name\"] == source_name, full_yml['sources']))\n\t        expected_source_entry = list(\n\t            filter(lambda x: x[\"name\"] == source_name, expected['sources'])\n\t        )\n\t        source_tables = source_entry[0].pop(\"tables\")\n\t        expected_source_tables = expected_source_entry[0].pop(\"tables\")\n\t        assert source_entry == expected_source_entry\n", "        assert sorted(source_tables, key=lambda x: x[\"name\"]) == sorted(\n\t            expected_source_tables, key=lambda x: x[\"name\"]\n\t        )\n\t    def test_add_exposure_yml_one_exposure(self):\n\t        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_exposure_yml__default),\n\t            full_yml=None,\n\t            resource_type=NodeType.Exposure,\n\t        )\n\t        assert full_yml == read_yml(exposure_yml_one_exposure)\n", "    def test_add_exposure_yml_multiple_exposures(self):\n\t        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_exposure_yml__default),\n\t            full_yml=read_yml(expected_remainder_yml__multiple_exposures),\n\t            resource_type=NodeType.Exposure,\n\t        )\n\t        assert sorted(full_yml[\"exposures\"], key=lambda x: x[\"name\"]) == sorted(\n\t            read_yml(exposure_yml_multiple_exposures)[\"exposures\"], key=lambda x: x[\"name\"]\n\t        )\n\t    def test_add_metric_yml_one_metric(self):\n", "        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_metric_yml__default),\n\t            full_yml=None,\n\t            resource_type=NodeType.Metric,\n\t        )\n\t        assert full_yml == read_yml(metric_yml_one_metric)\n\t    def test_add_metric_yml_multiple_metrics(self):\n\t        full_yml = meshify.add_entry_to_yml(\n\t            resource_entry=read_yml(expected_remove_metric_yml__default),\n\t            full_yml=read_yml(expected_remainder_yml__multiple_metrics),\n", "            resource_type=NodeType.Metric,\n\t        )\n\t        assert sorted(full_yml[\"metrics\"], key=lambda x: x[\"name\"]) == sorted(\n\t            read_yml(metric_yml_multiple_metrics)[\"metrics\"], key=lambda x: x[\"name\"]\n\t        )\n"]}
{"filename": "tests/unit/test_add_group_and_access_to_model_yml.py", "chunked_list": ["import pytest\n\tfrom dbt.contracts.graph.nodes import Group\n\tfrom dbt.contracts.graph.unparsed import Owner\n\tfrom dbt.node_types import AccessType, NodeType\n\tfrom dbt_meshify.storage.file_content_editors import DbtMeshFileEditor\n\tfrom tests.unit import read_yml\n\tmeshify = DbtMeshFileEditor()\n\tmodel_name = \"shared_model\"\n\tmodel_yml_empty_file = \"\"\"\"\"\"\n\tmodel_yml_model_missing = \"\"\"\n", "models:\n\t - name: unrelated_model\n\t\"\"\"\n\tmodel_yml_shared_model = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t\"\"\"\n\texpected_model_yml_shared_model = \"\"\"\n\tmodels:\n\t  - name: shared_model\n", "    access: public\n\t    group: test_group\n\t\"\"\"\n\tmodel_yml_shared_model_with_group = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    access: private\n\t    group: old_group\n\t\"\"\"\n\tmodel_yml_multiple_models = \"\"\"\n", "models:\n\t  - name: shared_model\n\t  - name: other_model\n\t  - name: other_other_model\n\t\"\"\"\n\texpected_model_yml_multiple_models = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    access: public\n\t    group: test_group\n", "  - name: other_model\n\t  - name: other_other_model\n\t\"\"\"\n\texpected_model_yml_multiple_models_multi_select = \"\"\"\n\tmodels:\n\t  - name: shared_model\n\t    access: public\n\t    group: test_group\n\t  - name: other_model\n\t    access: public\n", "    group: test_group\n\t  - name: other_other_model\n\t\"\"\"\n\tclass TestAddGroupToModelYML:\n\t    @pytest.fixture\n\t    def owner(self) -> Owner:\n\t        return Owner(name=\"Shaina Fake\", email=\"fake@example.com\")\n\t    @pytest.fixture\n\t    def new_group(self, owner: Owner) -> Group:\n\t        return Group(\n", "            name=\"test_group\",\n\t            owner=owner,\n\t            package_name=\"test_package\",\n\t            original_file_path=\"fake_path\",\n\t            unique_id=f\"group.test_package.test_group\",\n\t            path=\"models/fake_path\",\n\t            resource_type=NodeType.Group,\n\t        )\n\t    def test_adds_group_to_model_file(self, new_group: Group):\n\t        yml_dict = meshify.add_group_and_access_to_model_yml(\n", "            model_name=model_name,\n\t            group=new_group,\n\t            access_type=AccessType.Public,\n\t            models_yml=read_yml(model_yml_shared_model),\n\t        )\n\t        assert yml_dict == read_yml(expected_model_yml_shared_model)\n\t    def test_adds_group_overwrites_existing_groups(self, new_group: Group):\n\t        yml_dict = meshify.add_group_and_access_to_model_yml(\n\t            model_name=model_name,\n\t            group=new_group,\n", "            access_type=AccessType.Public,\n\t            models_yml=read_yml(model_yml_shared_model_with_group),\n\t        )\n\t        assert yml_dict == read_yml(expected_model_yml_shared_model)\n\t    def test_preserves_existing_models(self, new_group: Group):\n\t        yml_dict = meshify.add_group_and_access_to_model_yml(\n\t            model_name=model_name,\n\t            group=new_group,\n\t            access_type=AccessType.Public,\n\t            models_yml=read_yml(model_yml_multiple_models),\n", "        )\n\t        assert yml_dict == read_yml(expected_model_yml_multiple_models)\n"]}
{"filename": "tests/unit/test_dbt_projects.py", "chunked_list": ["# TODO -- we don't really have a test suite for DbtProjects and their properties\n"]}
{"filename": "tests/unit/test_resource_grouper_classification.py", "chunked_list": ["import networkx\n\timport pytest\n\tfrom dbt.node_types import AccessType\n\tfrom dbt_meshify.utilities.grouper import ResourceGrouper\n\tclass TestResourceGrouper:\n\t    @pytest.fixture\n\t    def example_graph(self):\n\t        graph = networkx.DiGraph()\n\t        graph.add_edges_from([(\"a\", \"b\"), (\"b\", \"c\"), (\"b\", \"d\"), (\"d\", \"1\")])\n\t        return graph\n", "    @pytest.fixture\n\t    def example_graph_with_tests(self):\n\t        graph = networkx.DiGraph()\n\t        graph.add_edges_from(\n\t            [\n\t                (\"source.a\", \"model.b\"),\n\t                (\"model.b\", \"test.c\"),\n\t                (\"model.b\", \"model.d\"),\n\t                (\"model.d\", \"test.1\"),\n\t            ]\n", "        )\n\t        return graph\n\t    def test_resource_grouper_boundary_classification(self, example_graph):\n\t        nodes = {\"a\", \"b\", \"c\", \"d\"}\n\t        resources = ResourceGrouper.classify_resource_access(example_graph, nodes)\n\t        assert resources == {\n\t            \"a\": AccessType.Private,\n\t            \"b\": AccessType.Private,\n\t            \"c\": AccessType.Public,\n\t            \"d\": AccessType.Public,\n", "        }\n\t    def test_clean_graph_removes_test_nodes(self, example_graph_with_tests):\n\t        output_graph = ResourceGrouper.clean_subgraph(example_graph_with_tests)\n\t        assert set(output_graph.nodes) == {\"source.a\", \"model.b\", \"model.d\"}\n"]}
{"filename": "tests/unit/test_update_ref_functions.py", "chunked_list": ["import yaml\n\tfrom dbt.node_types import NodeType\n\tfrom dbt_meshify.storage.file_content_editors import DbtMeshFileEditor\n\tmeshify = DbtMeshFileEditor()\n\tupstream_project_name = \"upstream_project\"\n\tupstream_model_name = \"my_table\"\n\tsimple_model_sql = \"\"\"\n\tselect * from {{ ref('my_table') }}\n\t\"\"\"\n\texpected_simple_model_sql = \"\"\"\n", "select * from {{ ref('upstream_project', 'my_table') }}\n\t\"\"\"\n\tsimple_model_python = \"\"\"\n\tdef model(dbt, session):\n\t    my_sql_model_df = dbt.ref('my_table')\n\t    return my_sql_model_df\n\t\"\"\"\n\texpected_simple_model_python = \"\"\"\n\tdef model(dbt, session):\n\t    my_sql_model_df = dbt.ref('upstream_project', 'my_table')\n", "    return my_sql_model_df\n\t\"\"\"\n\tdef read_yml(yml_str):\n\t    return yaml.safe_load(yml_str)\n\tclass TestRemoveResourceYml:\n\t    def test_update_sql_ref_function__basic(self):\n\t        updated_sql = meshify.update_refs__sql(\n\t            model_code=simple_model_sql,\n\t            model_name=upstream_model_name,\n\t            project_name=upstream_project_name,\n", "        )\n\t        assert updated_sql == expected_simple_model_sql\n\t    def test_update_python_ref_function__basic(self):\n\t        updated_python = meshify.update_refs__python(\n\t            model_code=simple_model_python,\n\t            model_name=upstream_model_name,\n\t            project_name=upstream_project_name,\n\t        )\n\t        assert updated_python == expected_simple_model_python\n"]}
{"filename": "tests/unit/__init__.py", "chunked_list": ["import yaml\n\tdef read_yml(yml_str):\n\t    return yaml.safe_load(yml_str)\n"]}
{"filename": "tests/unit/test_add_contract_to_yml.py", "chunked_list": ["from dbt_meshify.storage.file_content_editors import DbtMeshFileEditor\n\tfrom ..dbt_project_fixtures import shared_model_catalog_entry\n\tfrom ..sql_and_yml_fixtures import (\n\t    expected_contract_yml_all_col,\n\t    expected_contract_yml_no_col,\n\t    expected_contract_yml_no_entry,\n\t    expected_contract_yml_one_col,\n\t    expected_contract_yml_one_col_one_test,\n\t    expected_contract_yml_other_model,\n\t    model_yml_all_col,\n", "    model_yml_no_col_no_version,\n\t    model_yml_one_col,\n\t    model_yml_one_col_one_test,\n\t    model_yml_other_model,\n\t)\n\tfrom . import read_yml\n\tmeshify = DbtMeshFileEditor()\n\tcatalog_entry = shared_model_catalog_entry\n\tmodel_name = \"shared_model\"\n\tclass TestAddContractToYML:\n", "    def test_add_contract_to_yml_no_col(self):\n\t        yml_dict = meshify.add_model_contract_to_yml(\n\t            models_yml=read_yml(model_yml_no_col_no_version),\n\t            model_catalog=catalog_entry,\n\t            model_name=model_name,\n\t        )\n\t        assert yml_dict == read_yml(expected_contract_yml_no_col)\n\t    def test_add_contract_to_yml_one_col(self):\n\t        yml_dict = meshify.add_model_contract_to_yml(\n\t            models_yml=read_yml(model_yml_one_col),\n", "            model_catalog=catalog_entry,\n\t            model_name=model_name,\n\t        )\n\t        assert yml_dict == read_yml(expected_contract_yml_one_col)\n\t    def test_add_contract_to_yml_one_col_one_test(self):\n\t        yml_dict = meshify.add_model_contract_to_yml(\n\t            models_yml=read_yml(model_yml_one_col_one_test),\n\t            model_catalog=catalog_entry,\n\t            model_name=model_name,\n\t        )\n", "        assert yml_dict == read_yml(expected_contract_yml_one_col_one_test)\n\t    def test_add_contract_to_yml_all_col(self):\n\t        yml_dict = meshify.add_model_contract_to_yml(\n\t            models_yml=read_yml(model_yml_all_col),\n\t            model_catalog=catalog_entry,\n\t            model_name=model_name,\n\t        )\n\t        assert yml_dict == read_yml(expected_contract_yml_all_col)\n\t    def test_add_contract_to_yml_no_entry(self):\n\t        yml_dict = meshify.add_model_contract_to_yml(\n", "            models_yml={}, model_catalog=catalog_entry, model_name=model_name\n\t        )\n\t        assert yml_dict == read_yml(expected_contract_yml_no_entry)\n\t    def test_add_contract_to_yml_other_model(self):\n\t        yml_dict = meshify.add_model_contract_to_yml(\n\t            models_yml=read_yml(model_yml_other_model),\n\t            model_catalog=catalog_entry,\n\t            model_name=model_name,\n\t        )\n\t        assert yml_dict == read_yml(expected_contract_yml_other_model)\n"]}
{"filename": "tests/unit/test_add_group_to_yml.py", "chunked_list": ["import pytest\n\tfrom dbt.contracts.graph.nodes import Group\n\tfrom dbt.contracts.graph.unparsed import Owner\n\tfrom dbt.node_types import NodeType\n\tfrom dbt_meshify.storage.file_content_editors import DbtMeshFileEditor\n\tfrom tests.unit import read_yml\n\tmeshify = DbtMeshFileEditor()\n\tgroup_yml_empty_file = \"\"\"\"\"\"\n\texpected_group_yml_no_group = \"\"\"\n\tgroups:\n", "  - name: test_group\n\t    owner:\n\t      name: Shaina Fake\n\t      email: fake@example.com\n\t\"\"\"\n\tgroup_yml_existing_groups = \"\"\"\n\tgroups:\n\t  - name: other_group\n\t    owner:\n\t      name: Ted Real\n", "      email: real@example.com\n\t\"\"\"\n\texpected_group_yml_existing_groups = \"\"\"\n\tgroups:\n\t  - name: other_group\n\t    owner:\n\t      name: Ted Real\n\t      email: real@example.com\n\t  - name: test_group\n\t    owner:\n", "      name: Shaina Fake\n\t      email: fake@example.com\n\t\"\"\"\n\tgroup_yml_group_predefined = \"\"\"\n\tgroups:\n\t  - name: test_group\n\t    owner:\n\t      name: Ted Real\n\t      email: real@example.com\n\t\"\"\"\n", "class TestAddGroupToYML:\n\t    @pytest.fixture\n\t    def owner(self) -> Owner:\n\t        return Owner(name=\"Shaina Fake\", email=\"fake@example.com\")\n\t    @pytest.fixture\n\t    def new_group(self, owner: Owner) -> Group:\n\t        return Group(\n\t            name=\"test_group\",\n\t            owner=owner,\n\t            package_name=\"test_package\",\n", "            original_file_path=\"fake_path\",\n\t            unique_id=f\"group.test_package.test_group\",\n\t            path=\"models/fake_path\",\n\t            resource_type=NodeType.Group,\n\t        )\n\t    def test_adds_groups_to_empty_file(self, new_group: Group):\n\t        yml_dict = meshify.add_group_to_yml(\n\t            group=new_group, groups_yml=read_yml(group_yml_empty_file)\n\t        )\n\t        assert yml_dict == read_yml(expected_group_yml_no_group)\n", "    def test_adds_groups_to_existing_list_of_groups(self, new_group: Group):\n\t        yml_dict = meshify.add_group_to_yml(\n\t            group=new_group, groups_yml=read_yml(group_yml_existing_groups)\n\t        )\n\t        assert yml_dict == read_yml(expected_group_yml_existing_groups)\n\t    def test_adds_groups_updates_predefined_group(self, new_group: Group):\n\t        yml_dict = meshify.add_group_to_yml(\n\t            group=new_group, groups_yml=read_yml(group_yml_group_predefined)\n\t        )\n\t        assert yml_dict == read_yml(expected_group_yml_no_group)\n"]}
{"filename": "tests/unit/test_add_version_to_yml.py", "chunked_list": ["import yaml\n\tfrom dbt_meshify.storage.file_content_editors import DbtMeshFileEditor\n\tfrom ..sql_and_yml_fixtures import (\n\t    expected_versioned_model_yml_increment_version_defined_in,\n\t    expected_versioned_model_yml_increment_version_no_prerelease,\n\t    expected_versioned_model_yml_increment_version_with_prerelease,\n\t    expected_versioned_model_yml_no_version,\n\t    expected_versioned_model_yml_no_yml,\n\t    model_yml_increment_version,\n\t    model_yml_no_col_no_version,\n", ")\n\tmeshify = DbtMeshFileEditor()\n\tmodel_name = \"shared_model\"\n\tdef read_yml(yml_str):\n\t    return yaml.safe_load(yml_str)\n\tclass TestAddContractToYML:\n\t    def test_add_version_to_model_yml_no_yml(self):\n\t        yml_dict = meshify.add_model_version_to_yml(models_yml={}, model_name=model_name)\n\t        assert yml_dict == read_yml(expected_versioned_model_yml_no_yml)\n\t    def test_add_version_to_model_yml_no_version(self):\n", "        yml_dict = meshify.add_model_version_to_yml(\n\t            models_yml=read_yml(model_yml_no_col_no_version), model_name=model_name\n\t        )\n\t        assert yml_dict == read_yml(expected_versioned_model_yml_no_version)\n\t    def test_add_version_to_model_yml_increment_version_no_prerelease(self):\n\t        yml_dict = meshify.add_model_version_to_yml(\n\t            models_yml=read_yml(model_yml_increment_version), model_name=model_name\n\t        )\n\t        assert yml_dict == read_yml(expected_versioned_model_yml_increment_version_no_prerelease)\n\t    def test_add_version_to_model_yml_increment_version_with_prerelease(self):\n", "        yml_dict = meshify.add_model_version_to_yml(\n\t            models_yml=read_yml(model_yml_increment_version),\n\t            model_name=model_name,\n\t            prerelease=True,\n\t        )\n\t        assert yml_dict == read_yml(expected_versioned_model_yml_increment_version_with_prerelease)\n\t    def test_add_version_to_model_yml_increment_version_defined_in(self):\n\t        yml_dict = meshify.add_model_version_to_yml(\n\t            models_yml=read_yml(model_yml_increment_version),\n\t            model_name=model_name,\n", "            defined_in=\"daves_model\",\n\t        )\n\t        assert yml_dict == read_yml(expected_versioned_model_yml_increment_version_defined_in)\n"]}
{"filename": "test-projects/split/split_proj/models/marts/customer_status_histories.py", "chunked_list": ["import pandas as pd\n\tdef model(dbt, session):\n\t    # set length of time considered a churn\n\t    pd.Timedelta(days=2)\n\t    dbt.config(enabled=False, materialized=\"table\", packages=[\"pandas==1.5.2\"])\n\t    orders_relation = dbt.ref(\"stg_orders\")\n\t    # converting a DuckDB Python Relation into a pandas DataFrame\n\t    orders_df = orders_relation.df()\n\t    orders_df.sort_values(by=\"ordered_at\", inplace=True)\n\t    orders_df[\"previous_order_at\"] = orders_df.groupby(\"customer_id\")[\"ordered_at\"].shift(1)\n", "    orders_df[\"next_order_at\"] = orders_df.groupby(\"customer_id\")[\"ordered_at\"].shift(-1)\n\t    return orders_df\n"]}
{"filename": "dbt_meshify/dbt.py", "chunked_list": ["# third party\n\timport os\n\tfrom typing import List, Optional\n\tfrom dbt.cli.main import dbtRunner\n\tfrom dbt.contracts.graph.manifest import Manifest\n\tfrom dbt.contracts.results import CatalogArtifact\n\tfrom loguru import logger\n\tclass Dbt:\n\t    def __init__(self, manifest: Optional[Manifest] = None):\n\t        self.dbt_runner = dbtRunner(manifest=manifest)  # type: ignore\n", "    def invoke(\n\t        self, directory: Optional[os.PathLike] = None, runner_args: Optional[List[str]] = None\n\t    ):\n\t        starting_directory = os.getcwd()\n\t        if directory:\n\t            os.chdir(directory)\n\t        result = self.dbt_runner.invoke(runner_args if runner_args else [])\n\t        os.chdir(starting_directory)\n\t        if not result.success and result.exception:\n\t            raise result.exception\n", "        return result.result\n\t    def parse(self, directory: os.PathLike):\n\t        logger.info(\"Executing dbt parse...\")\n\t        return self.invoke(directory, [\"--quiet\", \"parse\"])\n\t    def ls(\n\t        self,\n\t        directory: os.PathLike,\n\t        arguments: Optional[List[str]] = None,\n\t        output_key: Optional[str] = None,\n\t    ) -> List[str]:\n", "        \"\"\"\n\t        Execute dbt ls with the given arguments and return the result as a list of strings.\n\t        Log level is set to none to prevent dbt from printing to stdout.\n\t        \"\"\"\n\t        args = [\"--log-format\", \"json\", \"--log-level\", \"none\", \"ls\"]\n\t        if arguments:\n\t            args.extend(arguments)\n\t        if output_key:\n\t            args.extend([\"--output\", \"json\", \"--output-keys\", output_key])\n\t        return self.invoke(directory, args)\n", "    def docs_generate(self, directory: os.PathLike) -> CatalogArtifact:\n\t        \"\"\"\n\t        Excute dbt docs generate with the given arguments\n\t        \"\"\"\n\t        logger.info(\"Generating catalog with dbt docs generate...\")\n\t        args = [\"--quiet\", \"docs\", \"generate\"]\n\t        return self.invoke(directory, args)\n"]}
{"filename": "dbt_meshify/linker.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom enum import Enum\n\tfrom typing import Set, Union\n\tfrom dbt.node_types import AccessType\n\tfrom loguru import logger\n\tfrom dbt_meshify.dbt_projects import BaseDbtProject, DbtProject\n\tfrom dbt_meshify.exceptions import FatalMeshifyException, FileEditorException\n\tfrom dbt_meshify.storage.dbt_project_editors import DbtProjectEditor, YMLOperationType\n\tfrom dbt_meshify.storage.file_content_editors import DbtMeshConstructor\n\tclass ProjectDependencyType(str, Enum):\n", "    \"\"\"ProjectDependencyTypes define how the dependency relationship was defined.\"\"\"\n\t    Source = \"source\"\n\t    Package = \"package\"\n\t@dataclass\n\tclass ProjectDependency:\n\t    \"\"\"ProjectDependencies define shared resources between two different projects\"\"\"\n\t    upstream_resource: str\n\t    upstream_project_name: str\n\t    downstream_resource: str\n\t    downstream_project_name: str\n", "    type: ProjectDependencyType\n\t    def __key(self):\n\t        return self.upstream_resource, self.downstream_resource, self.type\n\t    def __hash__(self):\n\t        return hash(self.__key())\n\t    def __eq__(self, other):\n\t        if isinstance(other, ProjectDependency):\n\t            return self.__key() == other.__key()\n\t        return NotImplemented\n\tclass Linker:\n", "    \"\"\"\n\t    Linker computes the metagraph between separate DbtProjects. This includes\n\t    providing an interface for mapping the dependencies between projects be\n\t    it source-hacked models, imported packages, or explicit cross-project\n\t    references.\n\t    \"\"\"\n\t    @staticmethod\n\t    def _find_relation_dependencies(\n\t        source_relations: Set[str], target_relations: Set[str]\n\t    ) -> Set[str]:\n", "        \"\"\"\n\t        Identify dependencies between projects using shared relation names.\n\t        \"\"\"\n\t        return source_relations.intersection(target_relations)\n\t    def _source_dependencies(\n\t        self,\n\t        project: Union[BaseDbtProject, DbtProject],\n\t        other_project: Union[BaseDbtProject, DbtProject],\n\t    ) -> Set[ProjectDependency]:\n\t        \"\"\"\n", "        Identify source-hack dependencies between projects.\n\t        Source hacking occurs when Project A defines a model, and a downstream project (Project B)\n\t        defines a source for the materialization of that same model.\n\t        \"\"\"\n\t        relations = self._find_relation_dependencies(\n\t            source_relations={\n\t                model.relation_name\n\t                for model in project.models.values()\n\t                if model.relation_name is not None\n\t            },\n", "            target_relations={\n\t                source.relation_name\n\t                for source in other_project.sources().values()\n\t                if source.relation_name is not None\n\t            },\n\t        )\n\t        forward_dependencies = {\n\t            ProjectDependency(\n\t                upstream_resource=project.model_relation_names[relation],\n\t                upstream_project_name=project.name,\n", "                downstream_resource=other_project.source_relation_names[relation],\n\t                downstream_project_name=other_project.name,\n\t                type=ProjectDependencyType.Source,\n\t            )\n\t            for relation in relations\n\t        }\n\t        backwards_relations = self._find_relation_dependencies(\n\t            source_relations={\n\t                model.relation_name\n\t                for model in other_project.models.values()\n", "                if model.relation_name is not None\n\t            },\n\t            target_relations={\n\t                source.relation_name\n\t                for source in project.sources().values()\n\t                if source.relation_name is not None\n\t            },\n\t        )\n\t        backward_dependencies = {\n\t            ProjectDependency(\n", "                upstream_resource=other_project.model_relation_names[relation],\n\t                upstream_project_name=other_project.name,\n\t                downstream_resource=project.source_relation_names[relation],\n\t                downstream_project_name=project.name,\n\t                type=ProjectDependencyType.Source,\n\t            )\n\t            for relation in backwards_relations\n\t        }\n\t        return forward_dependencies | backward_dependencies\n\t    def _package_dependencies(\n", "        self,\n\t        project: Union[BaseDbtProject, DbtProject],\n\t        other_project: Union[BaseDbtProject, DbtProject],\n\t    ) -> Set[ProjectDependency]:\n\t        \"\"\"\n\t        Identify package-imported dependencies between projects.\n\t        Imported project dependencies occur when Project A defines a model, and a downstream\n\t        project (Project B) imports Project A and references the model.\n\t        \"\"\"\n\t        if (\n", "            project.project_id not in other_project.installed_packages()\n\t            and other_project.project_id not in project.installed_packages()\n\t        ):\n\t            return set()\n\t        # find which models are in both manifests\n\t        relations = self._find_relation_dependencies(\n\t            source_relations={\n\t                model.relation_name\n\t                for model in project.models.values()\n\t                if model.relation_name is not None\n", "            },\n\t            target_relations={\n\t                model.relation_name\n\t                for model in other_project.models.values()\n\t                if model.relation_name is not None\n\t            },\n\t        )\n\t        # find the children of the shared models in the downstream project\n\t        package_children = [\n\t            {\n", "                'upstream_resource': project.model_relation_names[relation],\n\t                'downstream_resource': child,\n\t            }\n\t            for relation in relations\n\t            for child in other_project.manifest.child_map[project.model_relation_names[relation]]\n\t        ]\n\t        forward_dependencies = {\n\t            ProjectDependency(\n\t                upstream_resource=child['upstream_resource'],\n\t                upstream_project_name=project.name,\n", "                downstream_resource=child['downstream_resource'],\n\t                downstream_project_name=other_project.name,\n\t                type=ProjectDependencyType.Package,\n\t            )\n\t            for child in package_children\n\t        }\n\t        # find the children of the shared models in the downstream project\n\t        backward_package_children = [\n\t            {\n\t                'upstream_resource': other_project.model_relation_names[relation],\n", "                'downstream_resource': child,\n\t            }\n\t            for relation in relations\n\t            for child in project.manifest.child_map[other_project.model_relation_names[relation]]\n\t        ]\n\t        backward_dependencies = {\n\t            ProjectDependency(\n\t                upstream_resource=child['upstream_resource'],\n\t                upstream_project_name=other_project.name,\n\t                downstream_resource=child['downstream_resource'],\n", "                downstream_project_name=project.name,\n\t                type=ProjectDependencyType.Package,\n\t            )\n\t            for child in backward_package_children\n\t        }\n\t        return forward_dependencies | backward_dependencies\n\t    def dependencies(\n\t        self,\n\t        project: Union[BaseDbtProject, DbtProject],\n\t        other_project: Union[BaseDbtProject, DbtProject],\n", "    ) -> Set[ProjectDependency]:\n\t        \"\"\"Detect dependencies between two projects and return a list of resources shared.\"\"\"\n\t        dependencies = set()\n\t        # Detect Source Dependencies\n\t        source_dependencies = self._source_dependencies(project, other_project)\n\t        dependencies.update(source_dependencies)\n\t        # Detect package-defined dependencies\n\t        package_dependencies = self._package_dependencies(project, other_project)\n\t        dependencies.update(package_dependencies)\n\t        return dependencies\n", "    def resolve_dependency(\n\t        self,\n\t        dependency: ProjectDependency,\n\t        upstream_project: DbtProject,\n\t        downstream_project: DbtProject,\n\t    ):\n\t        upstream_manifest_entry = upstream_project.get_manifest_node(dependency.upstream_resource)\n\t        if not upstream_manifest_entry:\n\t            raise ValueError(\n\t                f\"Could not find upstream resource {dependency.upstream_resource} in project {upstream_project.name}\"\n", "            )\n\t        downstream_manifest_entry = downstream_project.get_manifest_node(\n\t            dependency.downstream_resource\n\t        )\n\t        upstream_catalog_entry = upstream_project.get_catalog_entry(dependency.upstream_resource)\n\t        upstream_mesh_constructor = DbtMeshConstructor(\n\t            project_path=upstream_project.path,\n\t            node=upstream_manifest_entry,  # type: ignore\n\t            catalog=upstream_catalog_entry,\n\t        )\n", "        downstream_mesh_constructor = DbtMeshConstructor(\n\t            project_path=downstream_project.path,\n\t            node=downstream_manifest_entry,  # type: ignore\n\t            catalog=None,\n\t        )\n\t        downstream_editor = DbtProjectEditor(downstream_project)\n\t        # for either dependency type, add contracts and make model public\n\t        try:\n\t            upstream_mesh_constructor.add_model_access(AccessType.Public)\n\t            logger.success(\n", "                f\"Successfully update model access : {dependency.upstream_resource} is now {AccessType.Public}\"\n\t            )\n\t        except FatalMeshifyException as e:\n\t            logger.error(f\"Failed to update model access: {dependency.upstream_resource}\")\n\t            raise e\n\t        try:\n\t            upstream_mesh_constructor.add_model_contract()\n\t            logger.success(f\"Successfully added contract to model: {dependency.upstream_resource}\")\n\t        except FatalMeshifyException as e:\n\t            logger.error(f\"Failed to add contract to model: {dependency.upstream_resource}\")\n", "            raise e\n\t        if dependency.type == ProjectDependencyType.Source:\n\t            for child in downstream_project.manifest.child_map[dependency.downstream_resource]:\n\t                constructor = DbtMeshConstructor(\n\t                    project_path=downstream_project.path,\n\t                    node=downstream_project.get_manifest_node(child),  # type: ignore\n\t                    catalog=None,\n\t                )\n\t                try:\n\t                    constructor.replace_source_with_refs(\n", "                        source_unique_id=dependency.downstream_resource,\n\t                        model_unique_id=dependency.upstream_resource,\n\t                    )\n\t                    logger.success(\n\t                        f\"Successfully replaced source function with ref to upstream resource: {dependency.downstream_resource} now calls {dependency.upstream_resource} directly\"\n\t                    )\n\t                except FileEditorException as e:\n\t                    logger.error(\n\t                        f\"Failed to replace source function with ref to upstream resource\"\n\t                    )\n", "                    raise e\n\t                try:\n\t                    downstream_editor.update_resource_yml_entry(\n\t                        downstream_mesh_constructor, operation_type=YMLOperationType.Delete\n\t                    )\n\t                    logger.success(\n\t                        f\"Successfully deleted unnecessary source: {dependency.downstream_resource}\"\n\t                    )\n\t                except FatalMeshifyException as e:\n\t                    logger.error(f\"Failed to delete unnecessary source\")\n", "                    raise e\n\t        if dependency.type == ProjectDependencyType.Package:\n\t            try:\n\t                downstream_mesh_constructor.update_model_refs(\n\t                    model_name=upstream_manifest_entry.name,\n\t                    project_name=dependency.upstream_project_name,\n\t                )\n\t                logger.success(\n\t                    f\"Successfully updated model refs: {dependency.downstream_resource} now references {dependency.upstream_resource}\"\n\t                )\n", "            except FileEditorException as e:\n\t                logger.error(f\"Failed to update model refs\")\n\t                raise e\n\t        # for both types, add upstream project to downstream project's dependencies.yml\n\t        try:\n\t            downstream_editor.update_dependencies_yml(parent_project=upstream_project.name)\n\t            logger.success(\n\t                f\"Successfully added {dependency.upstream_project_name} to {dependency.downstream_project_name}'s dependencies.yml\"\n\t            )\n\t        except FileEditorException as e:\n", "            logger.error(\n\t                f\"Failed to add {dependency.upstream_project_name} to {dependency.downstream_project_name}'s dependencies.yml\"\n\t            )\n\t            raise e\n"]}
{"filename": "dbt_meshify/main.py", "chunked_list": ["import os\n\timport sys\n\tfrom itertools import combinations\n\tfrom pathlib import Path\n\tfrom typing import List, Optional\n\timport click\n\timport yaml\n\tfrom dbt.contracts.graph.unparsed import Owner\n\tfrom loguru import logger\n\tfrom dbt_meshify.storage.dbt_project_editors import DbtSubprojectCreator\n", "from .cli import (\n\t    TupleCompatibleCommand,\n\t    create_path,\n\t    exclude,\n\t    exclude_projects,\n\t    group_yml_path,\n\t    owner,\n\t    owner_email,\n\t    owner_name,\n\t    owner_properties,\n", "    project_path,\n\t    project_paths,\n\t    projects_dir,\n\t    read_catalog,\n\t    select,\n\t    selector,\n\t)\n\tfrom .dbt_projects import DbtProject, DbtProjectHolder\n\tfrom .exceptions import FatalMeshifyException\n\tfrom .linker import Linker\n", "from .storage.file_content_editors import DbtMeshConstructor\n\tlog_format = \"<white>{time:HH:mm:ss}</white> | <level>{level}</level> | <level>{message}</level>\"\n\tlogger.remove()  # Remove the default sink added by Loguru\n\tlogger.add(sys.stdout, format=log_format)\n\t# define cli group\n\t@click.group()\n\tdef cli():\n\t    pass\n\t@cli.group()\n\tdef operation():\n", "    \"\"\"\n\t    Set of subcommands for performing mesh operations on dbt projects\n\t    \"\"\"\n\t    pass\n\t@cli.command(name=\"connect\")\n\t@project_paths\n\t@projects_dir\n\t@exclude_projects\n\t@read_catalog\n\tdef connect(\n", "    project_paths: tuple, projects_dir: Path, exclude_projects: List[str], read_catalog: bool\n\t):\n\t    \"\"\"\n\t    Connects multiple dbt projects together by adding all necessary dbt Mesh constructs\n\t    \"\"\"\n\t    if project_paths and projects_dir:\n\t        raise click.BadOptionUsage(\n\t            option_name=\"project_paths\",\n\t            message=\"Cannot specify both project_paths and projects_dir\",\n\t        )\n", "    # 1. initialize all the projects supplied to the command\n\t    # 2. compute the dependency graph between each combination of 2 projects in that set\n\t    # 3. for each dependency, add the necessary dbt Mesh constructs to each project.\n\t    #    This includes:\n\t    #    - adding the dependency to the dependencies.yml file of the downstream project\n\t    #    - adding contracts and public access to the upstream models\n\t    #    - deleting the source definition of the upstream models in the downstream project\n\t    #    - updating the `{{ source }}` macro in the downstream project to a {{ ref }} to the upstream project\n\t    linker = Linker()\n\t    if project_paths:\n", "        dbt_projects = [\n\t            DbtProject.from_directory(project_path, read_catalog) for project_path in project_paths\n\t        ]\n\t    if projects_dir:\n\t        dbt_project_paths = [path.parent for path in Path(projects_dir).glob(\"**/dbt_project.yml\")]\n\t        all_dbt_projects = [\n\t            DbtProject.from_directory(project_path, read_catalog)\n\t            for project_path in dbt_project_paths\n\t        ]\n\t        dbt_projects = [\n", "            project for project in all_dbt_projects if project.name not in exclude_projects\n\t        ]\n\t    project_map = {project.name: project for project in dbt_projects}\n\t    dbt_project_combinations = [combo for combo in combinations(dbt_projects, 2)]\n\t    all_dependencies = set()\n\t    for dbt_project_combo in dbt_project_combinations:\n\t        dependencies = linker.dependencies(dbt_project_combo[0], dbt_project_combo[1])\n\t        if len(dependencies) == 0:\n\t            logger.info(\n\t                f\"No dependencies found between {dbt_project_combo[0].name} and {dbt_project_combo[1].name}\"\n", "            )\n\t            continue\n\t        noun = \"dependency\" if len(dependencies) == 1 else \"dependencies\"\n\t        logger.info(\n\t            f\"Found {len(dependencies)} {noun} between {dbt_project_combo[0].name} and {dbt_project_combo[1].name}\"\n\t        )\n\t        all_dependencies.update(dependencies)\n\t    if len(all_dependencies) == 0:\n\t        logger.info(\"No dependencies found between any of the projects\")\n\t        return\n", "    noun = \"dependency\" if len(all_dependencies) == 1 else \"dependencies\"\n\t    logger.info(f\"Found {len(all_dependencies)} unique {noun} between all projects.\")\n\t    for dependency in all_dependencies:\n\t        logger.info(\n\t            f\"Resolving dependency between {dependency.upstream_resource} and {dependency.downstream_resource}\"\n\t        )\n\t        try:\n\t            linker.resolve_dependency(\n\t                dependency,\n\t                project_map[dependency.upstream_project_name],\n", "                project_map[dependency.downstream_project_name],\n\t            )\n\t        except Exception as e:\n\t            raise FatalMeshifyException(f\"Error resolving dependency : {dependency} {e}\")\n\t@cli.command(\n\t    cls=TupleCompatibleCommand,\n\t    name=\"split\",\n\t)\n\t@create_path\n\t@click.argument(\"project_name\")\n", "@exclude\n\t@project_path\n\t@read_catalog\n\t@select\n\t@selector\n\t@click.pass_context\n\tdef split(ctx, project_name, select, exclude, project_path, selector, create_path, read_catalog):\n\t    \"\"\"\n\t    Splits out a new subproject from a dbt project by adding all necessary dbt Mesh constructs to the resources based on the selected resources.\n\t    \"\"\"\n", "    path = Path(project_path).expanduser().resolve()\n\t    project = DbtProject.from_directory(path, read_catalog)\n\t    subproject = project.split(\n\t        project_name=project_name, select=select, exclude=exclude, selector=selector\n\t    )\n\t    logger.info(f\"Selected {len(subproject.resources)} resources: {subproject.resources}\")\n\t    if subproject.is_project_cycle:\n\t        raise FatalMeshifyException(\n\t            f\"Cannot create subproject {project_name} from {project.name} because it would create a project dependency cycle. Try adding a `+` to your selection syntax to ensure all upstream resources are properly selected\"\n\t        )\n", "    if create_path:\n\t        create_path = Path(create_path).expanduser().resolve()\n\t        create_path.parent.mkdir(parents=True, exist_ok=True)\n\t    subproject_creator = DbtSubprojectCreator(project=subproject, target_directory=create_path)\n\t    logger.info(f\"Creating subproject {subproject.name}...\")\n\t    try:\n\t        subproject_creator.initialize()\n\t        logger.success(f\"Successfully created subproject {subproject.name}\")\n\t    except Exception as e:\n\t        raise FatalMeshifyException(f\"Error creating subproject {subproject.name}: error {e}\")\n", "@operation.command(name=\"add-contract\")\n\t@exclude\n\t@project_path\n\t@read_catalog\n\t@select\n\t@selector\n\tdef add_contract(select, exclude, project_path, selector, read_catalog, public_only=False):\n\t    \"\"\"\n\t    Adds a contract to all selected models.\n\t    \"\"\"\n", "    path = Path(project_path).expanduser().resolve()\n\t    logger.info(f\"Reading dbt project at {path}\")\n\t    project = DbtProject.from_directory(path, read_catalog)\n\t    resources = list(\n\t        project.select_resources(\n\t            select=select, exclude=exclude, selector=selector, output_key=\"unique_id\"\n\t        )\n\t    )\n\t    logger.info(f\"Selected {len(resources)} resources: {resources}\")\n\t    models = filter(lambda x: x.startswith(\"model\"), resources)\n", "    if public_only:\n\t        models = filter(lambda x: project.get_manifest_node(x).access == \"public\", models)\n\t    logger.info(f\"Adding contracts to models in selected resources...\")\n\t    for model_unique_id in models:\n\t        model_node = project.get_manifest_node(model_unique_id)\n\t        model_catalog = project.get_catalog_entry(model_unique_id)\n\t        meshify_constructor = DbtMeshConstructor(\n\t            project_path=project_path, node=model_node, catalog=model_catalog\n\t        )\n\t        logger.info(f\"Adding contract to model: {model_unique_id}\")\n", "        try:\n\t            meshify_constructor.add_model_contract()\n\t            logger.success(f\"Successfully added contract to model: {model_unique_id}\")\n\t        except Exception as e:\n\t            raise FatalMeshifyException(f\"Error adding contract to model: {model_unique_id}\")\n\t@operation.command(name=\"add-version\")\n\t@exclude\n\t@project_path\n\t@read_catalog\n\t@select\n", "@selector\n\t@click.option(\"--prerelease\", \"--pre\", default=False, is_flag=True)\n\t@click.option(\"--defined-in\", default=None)\n\tdef add_version(select, exclude, project_path, selector, prerelease, defined_in, read_catalog):\n\t    \"\"\"\n\t    Adds/increments model versions for all selected models.\n\t    \"\"\"\n\t    path = Path(project_path).expanduser().resolve()\n\t    logger.info(f\"Reading dbt project at {path}\")\n\t    project = DbtProject.from_directory(path, read_catalog)\n", "    resources = list(\n\t        project.select_resources(\n\t            select=select, exclude=exclude, selector=selector, output_key=\"unique_id\"\n\t        )\n\t    )\n\t    models = filter(lambda x: x.startswith(\"model\"), resources)\n\t    logger.info(f\"Selected {len(resources)} resources: {resources}\")\n\t    logger.info(f\"Adding version to models in selected resources...\")\n\t    for model_unique_id in models:\n\t        model_node = project.get_manifest_node(model_unique_id)\n", "        if model_node.version == model_node.latest_version:\n\t            meshify_constructor = DbtMeshConstructor(project_path=project_path, node=model_node)\n\t            try:\n\t                meshify_constructor.add_model_version(prerelease=prerelease, defined_in=defined_in)\n\t                logger.success(f\"Successfully added version to model: {model_unique_id}\")\n\t            except Exception as e:\n\t                raise FatalMeshifyException(\n\t                    f\"Error adding version to model: {model_unique_id}\"\n\t                ) from e\n\t@operation.command(\n", "    name=\"create-group\",\n\t    cls=TupleCompatibleCommand,\n\t)\n\t@click.argument(\"name\")\n\t@exclude\n\t@group_yml_path\n\t@owner\n\t@owner_email\n\t@owner_name\n\t@owner_properties\n", "@project_path\n\t@read_catalog\n\t@select\n\t@selector\n\tdef create_group(\n\t    name,\n\t    project_path: os.PathLike,\n\t    group_yml_path: os.PathLike,\n\t    select: str,\n\t    read_catalog: bool,\n", "    owner_name: Optional[str] = None,\n\t    owner_email: Optional[str] = None,\n\t    owner_properties: Optional[str] = None,\n\t    exclude: Optional[str] = None,\n\t    selector: Optional[str] = None,\n\t):\n\t    \"\"\"\n\t    Create a group and add selected resources to the group.\n\t    \"\"\"\n\t    from dbt_meshify.utilities.grouper import ResourceGrouper\n", "    path = Path(project_path).expanduser().resolve()\n\t    logger.info(f\"Reading dbt project at {path}\")\n\t    project = DbtProject.from_directory(path, read_catalog)\n\t    if group_yml_path is None:\n\t        group_yml_path = (path / Path(\"models/_groups.yml\")).resolve()\n\t    else:\n\t        group_yml_path = Path(group_yml_path).resolve()\n\t    logger.info(f\"Creating new model group in file {group_yml_path.name}\")\n\t    if not str(os.path.commonpath([group_yml_path, path])) == str(path):\n\t        raise FatalMeshifyException(\n", "            \"The provided group-yml-path is not contained within the provided dbt project.\"\n\t        )\n\t    group_owner: Owner = Owner(\n\t        name=owner_name, email=owner_email, _extra=yaml.safe_load(owner_properties or \"{}\")\n\t    )\n\t    grouper = ResourceGrouper(project)\n\t    try:\n\t        grouper.add_group(\n\t            name=name,\n\t            owner=group_owner,\n", "            select=select,\n\t            exclude=exclude,\n\t            selector=selector,\n\t            path=group_yml_path,\n\t        )\n\t        logger.success(f\"Successfully created group: {name}\")\n\t    except Exception as e:\n\t        raise FatalMeshifyException(f\"Error creating group: {name}\")\n\t@cli.command(name=\"group\", cls=TupleCompatibleCommand)\n\t@click.argument(\"name\")\n", "@exclude\n\t@group_yml_path\n\t@owner\n\t@owner_email\n\t@owner_name\n\t@owner_properties\n\t@project_path\n\t@read_catalog\n\t@select\n\t@selector\n", "@click.pass_context\n\tdef group(\n\t    ctx,\n\t    name,\n\t    project_path: os.PathLike,\n\t    group_yml_path: os.PathLike,\n\t    select: str,\n\t    read_catalog: bool,\n\t    owner_name: Optional[str] = None,\n\t    owner_email: Optional[str] = None,\n", "    owner_properties: Optional[str] = None,\n\t    exclude: Optional[str] = None,\n\t    selector: Optional[str] = None,\n\t):\n\t    \"\"\"\n\t    Creates a new dbt group based on the selection syntax\n\t    Detects the edges of the group, makes their access public, and adds contracts to them\n\t    \"\"\"\n\t    ctx.forward(create_group)\n\t    ctx.invoke(add_contract, select=f\"group:{name}\", project_path=project_path, public_only=True)\n"]}
{"filename": "dbt_meshify/dbt_projects.py", "chunked_list": ["import copy\n\timport hashlib\n\timport json\n\timport os\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict, MutableMapping, Optional, Set, Union\n\timport yaml\n\tfrom dbt.contracts.graph.manifest import Manifest\n\tfrom dbt.contracts.graph.nodes import (\n\t    Documentation,\n", "    Exposure,\n\t    Group,\n\t    Macro,\n\t    ManifestNode,\n\t    ModelNode,\n\t    Resource,\n\t    SourceDefinition,\n\t)\n\tfrom dbt.contracts.project import Project\n\tfrom dbt.contracts.results import CatalogArtifact, CatalogTable\n", "from dbt.graph import Graph\n\tfrom dbt.node_types import NodeType\n\tfrom loguru import logger\n\tfrom dbt_meshify.dbt import Dbt\n\tfrom dbt_meshify.storage.file_content_editors import (\n\t    DbtMeshConstructor,\n\t    filter_empty_dict_items,\n\t)\n\tfrom dbt_meshify.storage.file_manager import DbtFileManager\n\tclass BaseDbtProject:\n", "    \"\"\"A base-level representation of a dbt project.\"\"\"\n\t    def __init__(\n\t        self,\n\t        manifest: Manifest,\n\t        project: Project,\n\t        catalog: CatalogArtifact,\n\t        name: Optional[str] = None,\n\t    ) -> None:\n\t        self.manifest = manifest\n\t        self.project = project\n", "        self.catalog = catalog\n\t        self.name = name if name else project.name\n\t        self.relationships: Dict[str, Set[str]] = {}\n\t        self._models: Optional[Dict[str, ModelNode]] = None\n\t        self.model_relation_names: Dict[str, str] = {\n\t            model.relation_name: unique_id\n\t            for unique_id, model in self.models.items()\n\t            if model.relation_name is not None\n\t        }\n\t        self.source_relation_names: Dict[str, str] = {\n", "            source.relation_name: unique_id\n\t            for unique_id, source in self.sources().items()\n\t            if source.relation_name is not None\n\t        }\n\t        self._graph = None\n\t        self._changes: Dict[str, str] = {}\n\t    @staticmethod\n\t    def _load_graph(manifest: Manifest) -> Graph:\n\t        \"\"\"Generate a dbt Graph using a project manifest and the internal dbt Compiler and Linker.\"\"\"\n\t        from dbt.compilation import Compiler, Linker\n", "        compiler = Compiler(config={})\n\t        linker = Linker()\n\t        compiler.link_graph(linker=linker, manifest=manifest)\n\t        return Graph(linker.graph)\n\t    @property\n\t    def graph(self):\n\t        \"\"\"Get the dbt-core Graph for a given project Manifest\"\"\"\n\t        if self._graph:\n\t            return self._graph\n\t        self._graph = self._load_graph(self.manifest)\n", "        return self._graph\n\t    def register_relationship(self, project: str, resources: Set[str]) -> None:\n\t        \"\"\"Register the relationship between two projects\"\"\"\n\t        logger.debug(f\"Registering the relationship between {project} and its resources\")\n\t        entry = self.relationships.get(project, set())\n\t        self.relationships[project] = entry.union(resources)\n\t    def sources(self) -> MutableMapping[str, SourceDefinition]:\n\t        return self.manifest.sources\n\t    @property\n\t    def models(self) -> Dict[str, ModelNode]:\n", "        if self._models:\n\t            return self._models\n\t        self._models = {\n\t            node_name: node\n\t            for node_name, node in self.manifest.nodes.items()\n\t            if node.resource_type == \"model\" and isinstance(node, ModelNode)\n\t        }\n\t        return self._models\n\t    def installed_packages(self) -> Set[str]:\n\t        project_packages = []\n", "        for key in [\"nodes\", \"sources\", \"exposures\", \"metrics\", \"sources\", \"macros\"]:\n\t            items = getattr(self.manifest, key)\n\t            for key, item in items.items():\n\t                if item.package_name:\n\t                    _hash = hashlib.md5()\n\t                    _hash.update(item.package_name.encode(\"utf-8\"))\n\t                    if _hash.hexdigest() != self.manifest.metadata.project_id:\n\t                        project_packages.append(_hash.hexdigest())\n\t        return set(project_packages)\n\t    @property\n", "    def project_id(self) -> Optional[str]:\n\t        return self.manifest.metadata.project_id\n\t    def installs(self, other) -> bool:\n\t        \"\"\"\n\t        Returns true if this project installs the other project as a package\n\t        \"\"\"\n\t        return self.project_id in other.installed_packages()\n\t    def get_model_by_relation_name(self, relation_name: str) -> Optional[ModelNode]:\n\t        model_id = self.model_relation_names.get(relation_name)\n\t        if not model_id:\n", "            return None\n\t        return self.manifest.nodes.get(model_id)  # type: ignore\n\t    def shares_source_metadata(self, other) -> bool:\n\t        \"\"\"\n\t        Returns true if there is any shared metadata between this project's sources and the models in the other project\n\t        \"\"\"\n\t        my_sources = {k: v.relation_name for k, v in self.sources().items()}\n\t        their_models = {k: v.relation_name for k, v in other.models().items()}\n\t        return any(item in set(their_models.values()) for item in my_sources.values())\n\t    def overlapping_sources(self, other) -> Dict[str, Dict[str, Union[SourceDefinition, Any]]]:\n", "        \"\"\"\n\t        Returns any shared metadata between this sources in this project and the models in the other  project\n\t        \"\"\"\n\t        shared_sources = {}\n\t        for source_id, source in self.sources().items():\n\t            relation_name = source.relation_name\n\t            upstream_model = other.get_model_by_relation_name(relation_name)\n\t            shared_sources[source_id] = {\"source\": source, \"upstream_model\": upstream_model}\n\t        return shared_sources\n\t    def depends_on(self, other) -> bool:\n", "        \"\"\"\n\t        Returns true if this project depends on the other project as a package or via shared metadata\n\t        \"\"\"\n\t        return self.installs(other) or self.shares_source_metadata(other)\n\t    def get_catalog_entry(self, unique_id: str) -> Optional[CatalogTable]:\n\t        \"\"\"Returns the catalog entry for a model in the dbt project's catalog\"\"\"\n\t        return self.catalog.nodes.get(unique_id)\n\t    def get_manifest_node(self, unique_id: str) -> Optional[Resource]:\n\t        \"\"\"Returns the manifest entry for a resource in the dbt project's manifest\"\"\"\n\t        if unique_id.split(\".\")[0] in [\n", "            \"model\",\n\t            \"seed\",\n\t            \"snapshot\",\n\t            \"test\",\n\t            \"analysis\",\n\t            \"snapshot\",\n\t        ]:\n\t            return self.manifest.nodes.get(unique_id)\n\t        pluralized = NodeType(unique_id.split(\".\")[0]).pluralize()\n\t        resources = getattr(self.manifest, pluralized)\n", "        return resources.get(unique_id)\n\tclass DbtProject(BaseDbtProject):\n\t    @staticmethod\n\t    def _load_project(path) -> Project:\n\t        \"\"\"Load a dbt Project configuration\"\"\"\n\t        project_dict = yaml.load(open(os.path.join(path, \"dbt_project.yml\")), Loader=yaml.Loader)\n\t        return Project.from_dict(project_dict)\n\t    @classmethod\n\t    def from_directory(cls, directory: os.PathLike, read_catalog: bool) -> \"DbtProject\":\n\t        \"\"\"Create a new DbtProject using a dbt project directory\"\"\"\n", "        dbt = Dbt()\n\t        project = cls._load_project(directory)\n\t        def get_catalog(directory: os.PathLike, read_catalog: bool) -> CatalogArtifact:\n\t            catalog_path = Path(directory) / (project.target_path or \"target\") / \"catalog.json\"\n\t            if read_catalog:\n\t                logger.info(f\"Reading catalog from {catalog_path}\")\n\t                try:\n\t                    catalog_dict = json.loads(catalog_path.read_text())\n\t                    return CatalogArtifact.from_dict(catalog_dict)\n\t                except FileNotFoundError:\n", "                    logger.info(f\"Catalog not found at {catalog_path}, running dbt docs generate\")\n\t                    return dbt.docs_generate(directory)\n\t            else:\n\t                return dbt.docs_generate(directory)\n\t        return DbtProject(\n\t            manifest=dbt.parse(directory),\n\t            project=project,\n\t            catalog=get_catalog(directory, read_catalog),\n\t            dbt=dbt,\n\t            path=Path(directory),\n", "        )\n\t    def __init__(\n\t        self,\n\t        manifest: Manifest,\n\t        project: Project,\n\t        catalog: CatalogArtifact,\n\t        dbt: Dbt,\n\t        path: Path = Path(os.getcwd()),\n\t        name: Optional[str] = None,\n\t    ) -> None:\n", "        super().__init__(manifest, project, catalog, name)\n\t        self.path = path\n\t        self.dbt = dbt\n\t    def select_resources(\n\t        self,\n\t        select: str,\n\t        exclude: Optional[str] = None,\n\t        selector: Optional[str] = None,\n\t        output_key: Optional[str] = None,\n\t    ) -> Set[str]:\n", "        \"\"\"Select dbt resources using NodeSelection syntax\"\"\"\n\t        args = []\n\t        if select:\n\t            args = [\"--select\", select]\n\t        if exclude:\n\t            args.extend([\"--exclude\", exclude])\n\t        if selector:\n\t            args.extend([\"--selector\", selector])\n\t        results = self.dbt.ls(self.path, args, output_key=output_key)\n\t        if output_key:\n", "            results = [json.loads(resource).get(output_key) for resource in results]\n\t        return set(results)\n\t    def split(\n\t        self,\n\t        project_name: str,\n\t        select: str,\n\t        exclude: Optional[str] = None,\n\t        selector: Optional[str] = None,\n\t    ) -> \"DbtSubProject\":\n\t        \"\"\"Create a new DbtSubProject using NodeSelection syntax.\"\"\"\n", "        subproject_resources = self.select_resources(\n\t            select=select, exclude=exclude, selector=selector, output_key=\"unique_id\"\n\t        )\n\t        # Construct a new project and inject the new manifest\n\t        subproject = DbtSubProject(\n\t            name=project_name, parent_project=copy.deepcopy(self), resources=subproject_resources\n\t        )\n\t        # Record the subproject to create a cross-project dependency edge list\n\t        self.register_relationship(project_name, subproject_resources)\n\t        return subproject\n", "class DbtSubProject(BaseDbtProject):\n\t    \"\"\"\n\t    DbtSubProjects are a filtered representation of a dbt project that leverage\n\t    a parent DbtProject for their manifest and project definitions until a real DbtProject\n\t    is created on disk.\n\t    \"\"\"\n\t    def __init__(self, name: str, parent_project: DbtProject, resources: Set[str]):\n\t        self.name = name\n\t        self.resources = resources\n\t        self.parent_project = parent_project\n", "        self.path = parent_project.path / Path(name)\n\t        # self.manifest = parent_project.manifest.deepcopy()\n\t        # i am running into a bug with the core deepcopy -- checking with michelle\n\t        self.manifest = copy.deepcopy(parent_project.manifest)\n\t        self.project = copy.deepcopy(parent_project.project)\n\t        self.catalog = parent_project.catalog\n\t        super().__init__(self.manifest, self.project, self.catalog, self.name)\n\t        self.custom_macros = self._get_custom_macros()\n\t        self.groups = self._get_indirect_groups()\n\t        self._rename_project()\n", "        self.xproj_children_of_resources = self._get_xproj_children_of_selected_nodes()\n\t        self.xproj_parents_of_resources = self._get_xproj_parents_of_selected_nodes()\n\t        self.is_parent_of_parent_project = self._check_is_parent_of_parent_project()\n\t        self.is_child_of_parent_project = self._check_is_child_of_parent_project()\n\t        self.is_project_cycle = (\n\t            self.is_child_of_parent_project and self.is_parent_of_parent_project\n\t        )\n\t    def _get_xproj_children_of_selected_nodes(self) -> Set[str]:\n\t        return {\n\t            model.unique_id\n", "            for model in self.models.values()\n\t            if any(\n\t                parent\n\t                for parent in self.get_manifest_node(model.unique_id).depends_on.nodes\n\t                if parent in self.resources\n\t            )\n\t            and model.unique_id not in self.resources\n\t        }\n\t    def _get_xproj_parents_of_selected_nodes(self) -> Set[str]:\n\t        return {\n", "            node\n\t            for resource in self.resources\n\t            if self.get_manifest_node(resource).resource_type\n\t            in [NodeType.Model, NodeType.Snapshot, NodeType.Seed]\n\t            # ignore tests and other non buildable resources\n\t            for node in self.get_manifest_node(resource).depends_on.nodes\n\t            if node not in self.resources\n\t        }\n\t    def _check_is_parent_of_parent_project(self) -> bool:\n\t        \"\"\"\n", "        checks if the subproject is a child of the parent project\n\t        \"\"\"\n\t        return len(self.xproj_children_of_resources) > 0\n\t    def _check_is_child_of_parent_project(self) -> bool:\n\t        \"\"\"\n\t        checks if the subproject is a child of the parent project\n\t        \"\"\"\n\t        return len(self.xproj_parents_of_resources) > 0\n\t    def _rename_project(self) -> None:\n\t        \"\"\"\n", "        edits the project yml to take any instance of the parent project name and update it to the subproject name\n\t        \"\"\"\n\t        project_dict = self.project.to_dict()\n\t        for key in [resource.pluralize() for resource in NodeType]:\n\t            if self.parent_project.name in project_dict.get(key, {}).keys():\n\t                project_dict[key][self.name] = project_dict[key].pop(self.parent_project.name)\n\t        project_dict[\"name\"] = self.name\n\t        self.project = Project.from_dict(project_dict)\n\t    def _get_custom_macros(self) -> Set[str]:\n\t        \"\"\"\n", "        get a set of macro unique_ids for all the selected resources\n\t        \"\"\"\n\t        macros_set = set()\n\t        for unique_id in self.resources:\n\t            resource = self.get_manifest_node(unique_id)\n\t            if not resource or any(\n\t                isinstance(resource, class_) for class_ in [Documentation, Group]\n\t            ):\n\t                continue\n\t            macros = resource.depends_on.macros  # type: ignore\n", "            project_macros = [\n\t                macro\n\t                for macro in macros\n\t                if hashlib.md5((macro.split(\".\")[1]).encode()).hexdigest()\n\t                == self.manifest.metadata.project_id\n\t            ]\n\t            macros_set.update(project_macros)\n\t        return macros_set\n\t    def _get_indirect_groups(self) -> Set[str]:\n\t        \"\"\"\n", "        get a set of group unique_ids for all the selected resources\n\t        \"\"\"\n\t        groups = set()\n\t        for unique_id in self.resources:\n\t            resource = self.get_manifest_node(unique_id)  # type: ignore\n\t            if not resource or any(\n\t                isinstance(resource, class_)\n\t                for class_ in [Documentation, Group, Exposure, SourceDefinition, Macro]\n\t            ):\n\t                continue\n", "            group = resource.group  # type: ignore\n\t            if group:\n\t                group_unique_id = f\"group.{self.parent_project.name}.{group}\"\n\t                groups.update({group_unique_id})\n\t        return groups\n\t    def select_resources(\n\t        self,\n\t        select: str,\n\t        exclude: Optional[str] = None,\n\t        selector: Optional[str] = None,\n", "        output_key: Optional[str] = None,\n\t    ) -> Set[str]:\n\t        \"\"\"\n\t        Select resources using the parent DbtProject and filtering down to only include resources in this\n\t        subproject.\n\t        \"\"\"\n\t        results = self.parent_project.select_resources(\n\t            select=select, exclude=exclude, selector=selector, output_key=output_key\n\t        )\n\t        return set(results) - self.resources\n", "    def split(\n\t        self,\n\t        project_name: str,\n\t        select: str,\n\t        exclude: Optional[str] = None,\n\t    ) -> \"DbtSubProject\":\n\t        \"\"\"Create a new DbtSubProject using NodeSelection syntax.\"\"\"\n\t        subproject_resources = self.select_resources(select, exclude)\n\t        # Construct a new project and inject the new manifest\n\t        subproject = DbtSubProject(\n", "            name=project_name,\n\t            parent_project=copy.deepcopy(self.parent_project),\n\t            resources=subproject_resources,\n\t        )\n\t        # Record the subproject to create a cross-project dependency edge list\n\t        self.register_relationship(project_name, subproject_resources)\n\t        return subproject\n\tclass DbtProjectHolder:\n\t    def __init__(self) -> None:\n\t        self.projects: Dict[str, BaseDbtProject] = {}\n", "    def project_map(self) -> dict:\n\t        return self.projects\n\t    def register_project(self, project: BaseDbtProject) -> None:\n\t        if project.project_id:\n\t            self.projects[project.project_id] = project\n"]}
{"filename": "dbt_meshify/__init__.py", "chunked_list": []}
{"filename": "dbt_meshify/exceptions.py", "chunked_list": ["import click\n\tfrom loguru import logger\n\tclass FileEditorException(BaseException):\n\t    \"\"\"Exceptions relating to errors in file generation and loading.\"\"\"\n\tclass ModelFileNotFoundError(BaseException):\n\t    \"\"\"Unable to identify a local file that a defines Model. This indicates that the Manifest may not be valid.\"\"\"\n\tclass FatalMeshifyException(click.ClickException):\n\t    \"\"\"An unrecoverable error in Meshify.\"\"\"\n\t    def __init__(self, message):\n\t        super().__init__(message)\n", "    def show(self):\n\t        logger.error(self.message)\n\t        if self.__cause__ is not None:\n\t            logger.exception(self.__cause__)\n"]}
{"filename": "dbt_meshify/cli.py", "chunked_list": ["import functools\n\timport click\n\tfrom click import Context, HelpFormatter\n\tfrom dbt.cli.options import MultiOption\n\t# define common parameters\n\tproject_path = click.option(\n\t    \"--project-path\",\n\t    type=click.Path(exists=True),\n\t    default=\".\",\n\t    help=\"The path to the dbt project to operate on. Defaults to the current directory.\",\n", ")\n\tproject_paths = click.option(\n\t    \"--project-paths\",\n\t    cls=MultiOption,\n\t    multiple=True,\n\t    type=tuple,\n\t    default=None,\n\t    help=\"The paths to the set of dbt projects to connect. Must supply 2+ paths.\",\n\t)\n\tprojects_dir = click.option(\n", "    \"--projects-dir\",\n\t    type=click.Path(exists=True),\n\t    default=None,\n\t    help=\"The path to a directory containing multiple dbt projects. Directory must contain 2+ projects.\",\n\t)\n\texclude_projects = click.option(\n\t    \"--exclude-projects\",\n\t    \"-e\",\n\t    cls=MultiOption,\n\t    multiple=True,\n", "    type=tuple,\n\t    default=None,\n\t    help=\"The set of dbt projects to exclude from the operation when using the --projects-dir option.\",\n\t)\n\tcreate_path = click.option(\n\t    \"--create-path\",\n\t    type=click.Path(exists=False),\n\t    default=None,\n\t    help=\"The path to create the new dbt project. Defaults to the name argument supplied.\",\n\t)\n", "exclude = click.option(\n\t    \"--exclude\",\n\t    \"-e\",\n\t    cls=MultiOption,\n\t    multiple=True,\n\t    type=tuple,\n\t    default=None,\n\t    help=\"The dbt selection syntax specifying the resources to exclude in the operation\",\n\t)\n\tgroup_yml_path = click.option(\n", "    \"--group-yml-path\",\n\t    type=click.Path(exists=False),\n\t    help=\"An optional path to store the new group YAML definition.\",\n\t)\n\tselect = click.option(\n\t    \"--select\",\n\t    \"-s\",\n\t    cls=MultiOption,\n\t    multiple=True,\n\t    type=tuple,\n", "    default=None,\n\t    help=\"The dbt selection syntax specifying the resources to include in the operation\",\n\t)\n\tselector = click.option(\n\t    \"--selector\",\n\t    cls=MultiOption,\n\t    multiple=True,\n\t    type=tuple,\n\t    default=None,\n\t    help=\"The name(s) of the YML selector specifying the resources to include in the operation\",\n", ")\n\towner_name = click.option(\n\t    \"--owner-name\",\n\t    help=\"The group Owner's name.\",\n\t)\n\towner_email = click.option(\n\t    \"--owner-email\",\n\t    help=\"The group Owner's email address.\",\n\t)\n\towner_properties = click.option(\n", "    \"--owner-properties\",\n\t    help=\"Additional properties to assign to a group Owner.\",\n\t)\n\tread_catalog = click.option(\n\t    \"--read-catalog\",\n\t    \"-r\",\n\t    is_flag=True,\n\t    envvar=\"DBT_MESHIFY_READ_CATALOG\",\n\t    help=\"Skips the dbt docs generate step and reads the local catalog.json file.\",\n\t)\n", "def owner(func):\n\t    \"\"\"Add click options and argument validation for creating Owner objects.\"\"\"\n\t    @functools.wraps(func)\n\t    def wrapper_decorator(*args, **kwargs):\n\t        if kwargs.get('owner_name') is None and kwargs.get('owner_email') is None:\n\t            raise click.UsageError(\n\t                \"Groups require an Owner to be defined using --owner-name and/or --owner-email.\"\n\t            )\n\t        return func(*args, **kwargs)\n\t    return wrapper_decorator\n", "class TupleCompatibleCommand(click.Command):\n\t    \"\"\"\n\t    A Click Command with a custom formatter that adds metavar options after all arguments.\n\t    This is valuable for commands that use tuple-type options, since type types will eat\n\t    arguments.\n\t    \"\"\"\n\t    def format_usage(self, ctx: Context, formatter: HelpFormatter) -> None:\n\t        pieces = self.collect_usage_pieces(ctx)\n\t        pieces = pieces[1:] + [pieces[0]]\n\t        formatter.write_usage(ctx.command_path, \" \".join(pieces))\n"]}
{"filename": "dbt_meshify/storage/dbt_project_editors.py", "chunked_list": ["from enum import Enum\n\tfrom pathlib import Path\n\tfrom typing import Optional, Set, Union\n\tfrom dbt.contracts.graph.nodes import ManifestNode\n\tfrom dbt.contracts.util import Identifier\n\tfrom dbt.node_types import AccessType\n\tfrom loguru import logger\n\tfrom dbt_meshify.dbt_projects import DbtProject, DbtSubProject\n\tfrom dbt_meshify.storage.file_content_editors import (\n\t    DbtMeshConstructor,\n", "    filter_empty_dict_items,\n\t)\n\tfrom dbt_meshify.storage.file_manager import DbtFileManager\n\tfrom dbt_meshify.utilities.grouper import ResourceGrouper\n\tclass YMLOperationType(str, Enum):\n\t    \"\"\"ProjectDependencyTypes define how the dependency relationship was defined.\"\"\"\n\t    Move = \"move\"\n\t    Delete = \"delete\"\n\tclass DbtProjectEditor:\n\t    def __init__(self, project: Union[DbtSubProject, DbtProject]):\n", "        self.project = project\n\t        self.file_manager = DbtFileManager(\n\t            read_project_path=project.path,\n\t        )\n\t    def move_resource(self, meshify_constructor: DbtMeshConstructor) -> None:\n\t        \"\"\"\n\t        move a resource file from one project to another\n\t        \"\"\"\n\t        current_path = meshify_constructor.get_resource_path()\n\t        self.file_manager.move_file(current_path)\n", "    def copy_resource(self, meshify_constructor: DbtMeshConstructor) -> None:\n\t        \"\"\"\n\t        copy a resource file from one project to another\n\t        \"\"\"\n\t        resource_path = meshify_constructor.get_resource_path()\n\t        contents = self.file_manager.read_file(resource_path)\n\t        self.file_manager.write_file(resource_path, contents)\n\t    def update_resource_yml_entry(\n\t        self,\n\t        meshify_constructor: DbtMeshConstructor,\n", "        operation_type: YMLOperationType = YMLOperationType.Move,\n\t    ) -> None:\n\t        \"\"\"\n\t        move a resource yml entry from one project to another\n\t        \"\"\"\n\t        current_yml_path = meshify_constructor.get_patch_path()\n\t        new_yml_path = self.file_manager.write_project_path / current_yml_path\n\t        full_yml_entry = self.file_manager.read_file(current_yml_path)\n\t        source_name = (\n\t            meshify_constructor.node.source_name\n", "            if hasattr(meshify_constructor.node, \"source_name\")\n\t            else None\n\t        )\n\t        resource_entry, remainder = meshify_constructor.get_yml_entry(\n\t            resource_name=meshify_constructor.node.name,\n\t            full_yml=full_yml_entry,  # type: ignore\n\t            resource_type=meshify_constructor.node.resource_type,\n\t            source_name=source_name,\n\t        )\n\t        try:\n", "            existing_yml = self.file_manager.read_file(new_yml_path)\n\t        except FileNotFoundError:\n\t            existing_yml = None\n\t        if operation_type == YMLOperationType.Move:\n\t            new_yml_contents = meshify_constructor.add_entry_to_yml(\n\t                resource_entry, existing_yml, meshify_constructor.node.resource_type  # type: ignore\n\t            )\n\t            self.file_manager.write_file(current_yml_path, new_yml_contents)\n\t        if remainder:\n\t            self.file_manager.write_file(current_yml_path, remainder, writeback=True)\n", "        else:\n\t            self.file_manager.delete_file(current_yml_path)\n\t    def update_dependencies_yml(\n\t        self, subproject_is_parent: bool = True, parent_project: Union[str, None] = None\n\t    ) -> None:\n\t        contents = self.file_manager.read_file(Path(\"dependencies.yml\"))\n\t        if not contents:\n\t            contents = {\"projects\": []}\n\t        contents[\"projects\"].append({\"name\": str(Identifier(parent_project)) if parent_project else self.project.name})  # type: ignore\n\t        self.file_manager.write_file(\n", "            Path(\"dependencies.yml\"), contents, writeback=subproject_is_parent\n\t        )\n\tclass DbtSubprojectCreator(DbtProjectEditor):\n\t    \"\"\"\n\t    Takes a `DbtSubProject` and creates the directory structure and files for it.\n\t    \"\"\"\n\t    def __init__(self, project: DbtSubProject, target_directory: Optional[Path] = None):\n\t        if not isinstance(project, DbtSubProject):\n\t            raise TypeError(f\"DbtSubprojectCreator requires a DbtSubProject, got {type(project)}\")\n\t        super().__init__(project)\n", "        self.target_directory = target_directory if target_directory else project.path\n\t        self.file_manager = DbtFileManager(\n\t            read_project_path=project.parent_project.path,\n\t            write_project_path=self.target_directory,\n\t        )\n\t        self.project_boundary_models = self._get_subproject_boundary_models()\n\t    def _get_subproject_boundary_models(self) -> Set[str]:\n\t        \"\"\"\n\t        get a set of boundary model unique_ids for all the selected resources\n\t        \"\"\"\n", "        nodes = set(filter(lambda x: not x.startswith(\"source\"), self.project.resources))  # type: ignore\n\t        parent_project_name = self.project.parent_project.name  # type: ignore\n\t        interface = ResourceGrouper.identify_interface(\n\t            graph=self.project.graph.graph, selected_bunch=nodes\n\t        )\n\t        boundary_models = set(\n\t            filter(\n\t                lambda x: (x.startswith(\"model\") and x.split('.')[1] == parent_project_name),\n\t                interface,\n\t            )\n", "        )\n\t        return boundary_models\n\t    def write_project_file(self) -> None:\n\t        \"\"\"\n\t        Writes the dbt_project.yml file for the subproject in the specified subdirectory\n\t        \"\"\"\n\t        contents = self.project.project.to_dict()\n\t        # was gettinga  weird serialization error from ruamel on this value\n\t        # it's been deprecated, so no reason to keep it\n\t        contents.pop(\"version\")\n", "        # this one appears in the project yml, but i don't think it should be written\n\t        contents.pop(\"query-comment\")\n\t        contents = filter_empty_dict_items(contents)\n\t        # project_file_path = self.target_directory / Path(\"dbt_project.yml\")\n\t        self.file_manager.write_file(Path(\"dbt_project.yml\"), contents)\n\t    def copy_packages_yml_file(self) -> None:\n\t        \"\"\"\n\t        Writes the dbt_project.yml file for the subproject in the specified subdirectory\n\t        \"\"\"\n\t        self.file_manager.copy_file(Path(\"packages.yml\"))\n", "    def copy_packages_dir(self) -> None:\n\t        \"\"\"\n\t        Writes the dbt_packages directory to the subproject's subdirectory to avoid the need for an immediate `dbt deps` command\n\t        \"\"\"\n\t        raise NotImplementedError(\"copy_packages_dir not implemented yet\")\n\t    def update_child_refs(self, resource: ManifestNode) -> None:\n\t        for model in self.project.xproj_children_of_resources:\n\t            model_node = self.project.get_manifest_node(model)\n\t            if not model_node:\n\t                raise KeyError(f\"Resource {model} not found in manifest\")\n", "            meshify_constructor = DbtMeshConstructor(\n\t                project_path=self.project.parent_project.path, node=model_node, catalog=None  # type: ignore\n\t            )\n\t            meshify_constructor.update_model_refs(\n\t                model_name=resource.name, project_name=self.project.name\n\t            )\n\t    def update_parent_refs(self, resource: ManifestNode) -> None:\n\t        for model in [\n\t            parent\n\t            for parent in resource.depends_on.nodes\n", "            if parent in self.project.xproj_parents_of_resources\n\t        ]:\n\t            model_node = self.project.get_manifest_node(model)\n\t            if not model_node:\n\t                raise KeyError(f\"Resource {model} not found in manifest\")\n\t            meshify_constructor = DbtMeshConstructor(\n\t                project_path=self.project.parent_project.path, node=resource, catalog=None  # type: ignore\n\t            )\n\t            meshify_constructor.update_model_refs(\n\t                model_name=model_node.name, project_name=self.project.parent_project.name\n", "            )\n\t    def initialize(self) -> None:\n\t        \"\"\"Initialize this subproject as a full dbt project at the provided `target_directory`.\"\"\"\n\t        subproject = self.project\n\t        for unique_id in subproject.resources | subproject.custom_macros | subproject.groups:  # type: ignore\n\t            resource = subproject.get_manifest_node(unique_id)\n\t            catalog = subproject.get_catalog_entry(unique_id)\n\t            if not resource:\n\t                raise KeyError(f\"Resource {unique_id} not found in manifest\")\n\t            meshify_constructor = DbtMeshConstructor(\n", "                project_path=subproject.parent_project.path, node=resource, catalog=catalog  # type: ignore\n\t            )\n\t            if resource.resource_type in [\"model\", \"test\", \"snapshot\", \"seed\"]:\n\t                # ignore generic tests, as moving the yml entry will move the test too\n\t                if resource.resource_type == \"test\" and len(resource.unique_id.split(\".\")) == 4:\n\t                    continue\n\t                if resource.unique_id in self.project_boundary_models:\n\t                    logger.info(\n\t                        f\"Adding contract to and publicizing boundary node {resource.unique_id}\"\n\t                    )\n", "                    try:\n\t                        meshify_constructor.add_model_contract()\n\t                        meshify_constructor.add_model_access(access_type=AccessType.Public)\n\t                        logger.success(\n\t                            f\"Successfully added contract to and publicized boundary node {resource.unique_id}\"\n\t                        )\n\t                    except Exception as e:\n\t                        logger.error(\n\t                            f\"Failed to add contract to and publicize boundary node {resource.unique_id}\"\n\t                        )\n", "                        logger.exception(e)\n\t                    logger.info(f\"Updating ref functions for children of {resource.unique_id}...\")\n\t                    try:\n\t                        self.update_child_refs(resource)  # type: ignore\n\t                        logger.success(\n\t                            f\"Successfully updated ref functions for children of {resource.unique_id}\"\n\t                        )\n\t                    except Exception as e:\n\t                        logger.error(\n\t                            f\"Failed to update ref functions for children of {resource.unique_id}\"\n", "                        )\n\t                        logger.exception(e)\n\t                if any(\n\t                    node\n\t                    for node in resource.depends_on.nodes\n\t                    if node in self.project.xproj_parents_of_resources\n\t                ):\n\t                    logger.info(f\"Updating ref functions in {resource.unique_id} for ...\")\n\t                    try:\n\t                        self.update_parent_refs(resource)  # type: ignore\n", "                        logger.success(\n\t                            f\"Successfully updated ref functions in {resource.unique_id}\"\n\t                        )\n\t                    except Exception as e:\n\t                        logger.error(f\"Failed to update ref functions in {resource.unique_id}\")\n\t                        logger.exception(e)\n\t                logger.info(\n\t                    f\"Moving {resource.unique_id} and associated YML to subproject {subproject.name}...\"\n\t                )\n\t                try:\n", "                    self.move_resource(meshify_constructor)\n\t                    self.update_resource_yml_entry(meshify_constructor)\n\t                    logger.success(\n\t                        f\"Successfully moved {resource.unique_id} and associated YML to subproject {subproject.name}\"\n\t                    )\n\t                except Exception as e:\n\t                    logger.error(\n\t                        f\"Failed to move {resource.unique_id} and associated YML to subproject {subproject.name}\"\n\t                    )\n\t                    logger.exception(e)\n", "            elif resource.resource_type in [\"macro\", \"group\"]:\n\t                logger.info(f\"Copying {resource.unique_id} to subproject {subproject.name}...\")\n\t                try:\n\t                    self.copy_resource(meshify_constructor)\n\t                    logger.success(\n\t                        f\"Successfully copied {resource.unique_id} to subproject {subproject.name}\"\n\t                    )\n\t                except Exception as e:\n\t                    logger.error(\n\t                        f\"Failed to copy {resource.unique_id} to subproject {subproject.name}\"\n", "                    )\n\t                    logger.exception(e)\n\t            else:\n\t                logger.info(\n\t                    f\"Moving resource {resource.unique_id} to subproject {subproject.name}...\"\n\t                )\n\t                try:\n\t                    self.update_resource_yml_entry(meshify_constructor)\n\t                    logger.success(\n\t                        f\"Successfully moved resource {resource.unique_id} to subproject {subproject.name}\"\n", "                    )\n\t                except Exception as e:\n\t                    logger.error(\n\t                        f\"Failed to move resource {resource.unique_id} to subproject {subproject.name}\"\n\t                    )\n\t                    logger.exception(e)\n\t        # add contracts and access to parents of split models\n\t        for unique_id in subproject.xproj_parents_of_resources:\n\t            resource = subproject.get_manifest_node(unique_id)\n\t            catalog = subproject.get_catalog_entry(unique_id)\n", "            if not resource:\n\t                raise KeyError(f\"Resource {unique_id} not found in manifest\")\n\t            meshify_constructor = DbtMeshConstructor(\n\t                project_path=subproject.parent_project.path, node=resource, catalog=catalog  # type: ignore\n\t            )\n\t            try:\n\t                meshify_constructor.add_model_contract()\n\t                meshify_constructor.add_model_access(access_type=AccessType.Public)\n\t                logger.success(\n\t                    f\"Successfully added contract to and publicized boundary node {resource.unique_id}\"\n", "                )\n\t            except Exception as e:\n\t                logger.error(\n\t                    f\"Failed to add contract to and publicize boundary node {resource.unique_id}\"\n\t                )\n\t                logger.exception(e)\n\t        self.write_project_file()\n\t        self.copy_packages_yml_file()\n\t        parent_project = (\n\t            self.project.parent_project.name\n", "            if self.project.is_child_of_parent_project\n\t            else self.project.name\n\t        )\n\t        self.update_dependencies_yml(\n\t            subproject_is_parent=self.project.is_parent_of_parent_project,\n\t            parent_project=parent_project,\n\t        )\n\t        # self.copy_packages_dir()\n"]}
{"filename": "dbt_meshify/storage/file_content_editors.py", "chunked_list": ["import os\n\tfrom collections import OrderedDict\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict, List, Optional, Tuple, Union\n\tfrom dbt.contracts.graph.nodes import Group, ManifestNode\n\tfrom dbt.contracts.results import CatalogTable\n\tfrom dbt.node_types import AccessType, NodeType\n\tfrom loguru import logger\n\tfrom dbt_meshify.exceptions import FileEditorException, ModelFileNotFoundError\n\tfrom dbt_meshify.storage.file_manager import DbtFileManager\n", "def filter_empty_dict_items(dict_to_filter: Dict[str, Any]):\n\t    \"\"\"Filters out empty dictionary items\"\"\"\n\t    return {k: v for k, v in dict_to_filter.items() if v}\n\tdef process_model_yml(model_yml: Dict[str, Any]):\n\t    \"\"\"Processes the yml contents to be written back to a file\"\"\"\n\t    model_ordered_dict = OrderedDict.fromkeys(\n\t        [\n\t            \"name\",\n\t            \"description\",\n\t            \"latest_version\",\n", "            \"access\",\n\t            \"group\",\n\t            \"config\",\n\t            \"meta\",\n\t            \"tests\",\n\t            \"columns\",\n\t            \"versions\",\n\t        ]\n\t    )\n\t    model_ordered_dict.update(model_yml)\n", "    # remove any keys with None values\n\t    return filter_empty_dict_items(model_ordered_dict)\n\tdef resources_yml_to_dict(resources_yml: Optional[Dict], resource_type: NodeType = NodeType.Model):\n\t    \"\"\"Converts a yml dict to a named dictionary for easier operation\"\"\"\n\t    return (\n\t        {resource[\"name\"]: resource for resource in resources_yml[resource_type.pluralize()]}\n\t        if resources_yml\n\t        else {}\n\t    )\n\tclass DbtMeshFileEditor:\n", "    \"\"\"\n\t    Class to operate on the contents of a dbt project's files\n\t    to add the dbt mesh functionality\n\t    includes editing yml entries and sql file contents\n\t    \"\"\"\n\t    @staticmethod\n\t    def add_group_to_yml(group: Group, groups_yml: Dict[str, Any]):\n\t        \"\"\"Add a group to a yml file\"\"\"\n\t        if groups_yml is None:\n\t            groups_yml = {}\n", "        groups = resources_yml_to_dict(groups_yml, NodeType.Group)\n\t        group_yml = groups.get(group.name) or {}\n\t        group_yml.update({\"name\": group.name})\n\t        owner = group_yml.get(\"owner\", {})\n\t        owner.update(filter_empty_dict_items(group.owner.to_dict()))\n\t        group_yml[\"owner\"] = owner\n\t        groups[group.name] = filter_empty_dict_items(group_yml)\n\t        groups_yml[\"groups\"] = list(groups.values())\n\t        return groups_yml\n\t    @staticmethod\n", "    def add_access_to_model_yml(\n\t        model_name: str, access_type: AccessType, models_yml: Optional[Dict[str, Any]]\n\t    ):\n\t        \"\"\"Add group and access configuration to a model's YAMl properties.\"\"\"\n\t        # parse the yml file into a dictionary with model names as keys\n\t        models = resources_yml_to_dict(models_yml)\n\t        model_yml = models.get(model_name) or {\"name\": model_name, \"columns\": [], \"config\": {}}\n\t        model_yml.update({\"access\": access_type.value})\n\t        models[model_name] = process_model_yml(model_yml)\n\t        models_yml[\"models\"] = list(models.values())\n", "        return models_yml\n\t    @staticmethod\n\t    def add_group_to_model_yml(model_name: str, group: Group, models_yml: Dict[str, Any]):\n\t        \"\"\"Add group and access configuration to a model's YAMl properties.\"\"\"\n\t        # parse the yml file into a dictionary with model names as keys\n\t        models = resources_yml_to_dict(models_yml)\n\t        model_yml = models.get(model_name) or {\"name\": model_name, \"columns\": [], \"config\": {}}\n\t        model_yml.update({\"group\": group.name})\n\t        models[model_name] = process_model_yml(model_yml)\n\t        models_yml[\"models\"] = list(models.values())\n", "        return models_yml\n\t    @staticmethod\n\t    def add_group_and_access_to_model_yml(\n\t        model_name: str, group: Group, access_type: AccessType, models_yml: Dict[str, Any]\n\t    ):\n\t        \"\"\"Add group and access configuration to a model's YAMl properties.\"\"\"\n\t        # parse the yml file into a dictionary with model names as keys\n\t        models_yml = DbtMeshFileEditor.add_access_to_model_yml(model_name, access_type, models_yml)\n\t        models_yml = DbtMeshFileEditor.add_group_to_model_yml(model_name, group, models_yml)\n\t        return models_yml\n", "    def get_source_yml_entry(\n\t        self, resource_name: str, full_yml: Dict[str, Any], source_name: str\n\t    ) -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:\n\t        \"\"\"\n\t        Remove a single source entry from a source defintion block, return source definition with single source entry and the remainder of the original\n\t        \"\"\"\n\t        sources = resources_yml_to_dict(full_yml, NodeType.Source)\n\t        source_definition = sources.get(source_name)\n\t        tables = source_definition.get(\"tables\", [])\n\t        table = list(filter(lambda x: x[\"name\"] == resource_name, tables))\n", "        remaining_tables = list(filter(lambda x: x[\"name\"] != resource_name, tables))\n\t        resource_yml = source_definition.copy()\n\t        resource_yml[\"tables\"] = table\n\t        source_definition[\"tables\"] = remaining_tables\n\t        sources[source_name] = source_definition\n\t        if len(remaining_tables) == 0:\n\t            return resource_yml, None\n\t        full_yml[\"sources\"] = list(sources.values())\n\t        return resource_yml, full_yml\n\t    def get_yml_entry(\n", "        self,\n\t        resource_name: str,\n\t        full_yml: Dict[str, Any],\n\t        resource_type: NodeType = NodeType.Model,\n\t        source_name: Optional[str] = None,\n\t    ) -> Tuple[Dict[str, Any], Optional[Dict[str, Any]]]:\n\t        \"\"\"Remove a single resource entry from a yml file, return the single entry and the remainder of the yml file\"\"\"\n\t        # parse the yml file into a dictionary with model names as keys\n\t        if resource_type == NodeType.Source:\n\t            if not source_name:\n", "                raise ValueError('Missing source name')\n\t            return self.get_source_yml_entry(resource_name, full_yml, source_name)\n\t        else:\n\t            resources = resources_yml_to_dict(full_yml, resource_type)\n\t            resource_yml = resources.pop(resource_name, None)\n\t            if len(resources.keys()) == 0:\n\t                return resource_yml, None\n\t            else:\n\t                full_yml[resource_type.pluralize()] = (\n\t                    list(resources.values()) if len(resources) > 0 else None\n", "                )\n\t                return resource_yml, full_yml\n\t    def add_entry_to_yml(\n\t        self, resource_entry: Dict[str, Any], full_yml: Dict[str, Any], resource_type: NodeType\n\t    ):\n\t        \"\"\"\n\t        Adds a single resource yml entry to yml file\n\t        \"\"\"\n\t        if not full_yml:\n\t            full_yml = {resource_type.pluralize(): []}\n", "        if resource_type != NodeType.Source or resource_entry[\"name\"] not in [\n\t            source[\"name\"] for source in full_yml[resource_type.pluralize()]\n\t        ]:\n\t            full_yml[resource_type.pluralize()].append(resource_entry)\n\t            return full_yml\n\t        new_table = resource_entry[\"tables\"][0]\n\t        sources = {source[\"name\"]: source for source in full_yml[\"sources\"]}\n\t        sources[resource_entry[\"name\"]][\"tables\"].append(new_table)\n\t        full_yml[\"sources\"] = list(sources.values())\n\t        return full_yml\n", "    def add_model_contract_to_yml(\n\t        self,\n\t        model_name: str,\n\t        model_catalog: Optional[CatalogTable],\n\t        models_yml: Optional[Dict[str, Any]],\n\t    ) -> Dict[str, Any]:\n\t        \"\"\"Adds a model contract to the model's yaml\"\"\"\n\t        # set up yml order\n\t        # parse the yml file into a dictionary with model names as keys\n\t        models = resources_yml_to_dict(models_yml)\n", "        model_yml = models.get(model_name) or {\"name\": model_name, \"columns\": [], \"config\": {}}\n\t        # isolate the columns from the existing model entry\n\t        yml_cols: List[Dict] = model_yml.get(\"columns\", [])\n\t        catalog_cols = model_catalog.columns or {} if model_catalog else {}\n\t        catalog_cols = {k.lower(): v for k, v in catalog_cols.items()}\n\t        # add the data type to the yml entry for columns that are in yml\n\t        yml_cols = [\n\t            {**yml_col, \"data_type\": catalog_cols[yml_col[\"name\"]].type.lower()}\n\t            for yml_col in yml_cols\n\t            if yml_col.get(\"name\") in catalog_cols.keys()\n", "        ]\n\t        # append missing columns in the table to the yml entry\n\t        yml_col_names = [col[\"name\"].lower() for col in yml_cols]\n\t        for col_name, col in catalog_cols.items():\n\t            if col_name.lower() not in yml_col_names:\n\t                yml_cols.append({\"name\": col_name.lower(), \"data_type\": col.type.lower()})\n\t        # update the columns in the model yml entry\n\t        model_yml.update({\"columns\": yml_cols})\n\t        # add contract to the model yml entry\n\t        # this part should come from the same service as what we use for the standalone command when we get there\n", "        model_config = model_yml.get(\"config\", {})\n\t        model_config.update({\"contract\": {\"enforced\": True}})\n\t        model_yml[\"config\"] = model_config\n\t        # update the model entry in the full yml file\n\t        # if no entries exist, add the model entry\n\t        # otherwise, update the existing model entry in place\n\t        processed = process_model_yml(model_yml)\n\t        models[model_name] = processed\n\t        models_yml[\"models\"] = list(models.values())\n\t        return models_yml\n", "    def get_latest_yml_defined_version(self, model_yml: Dict[str, Any]):\n\t        \"\"\"\n\t        Returns the latest version defined in the yml file for a given model name\n\t        the format of `model_yml` should be a single model yml entry\n\t        if no versions, returns 0\n\t        \"\"\"\n\t        model_yml_versions = model_yml.get(\"versions\", [])\n\t        try:\n\t            return max([int(v.get(\"v\")) for v in model_yml_versions]) if model_yml_versions else 0\n\t        except ValueError:\n", "            raise ValueError(\n\t                f\"Version not an integer, can't increment version for {model_yml.get('name')}\"\n\t            )\n\t    def add_model_version_to_yml(\n\t        self,\n\t        model_name,\n\t        models_yml,\n\t        prerelease: Optional[bool] = False,\n\t        defined_in: Optional[os.PathLike] = None,\n\t    ) -> Dict[str, Any]:\n", "        \"\"\"Adds a model version to the model's yaml\"\"\"\n\t        # set up yml order\n\t        models = resources_yml_to_dict(models_yml)\n\t        model_yml = models.get(model_name) or {\n\t            \"name\": model_name,\n\t            \"latest_version\": 0,\n\t            \"versions\": [],\n\t        }\n\t        # add the version to the model yml entry\n\t        versions_list = model_yml.get(\"versions\") or []\n", "        latest_version = model_yml.get(\"latest_version\") or 0\n\t        latest_yml_version = self.get_latest_yml_defined_version(model_yml)\n\t        version_dict: Dict[str, Union[int, str, os.PathLike]] = {}\n\t        if not versions_list:\n\t            version_dict[\"v\"] = 1\n\t            latest_version += 1\n\t        # if the model has versions, add the next version\n\t        # if prerelease flag is true, do not increment the latest_version\n\t        elif prerelease:\n\t            version_dict = {\"v\": latest_yml_version + 1}\n", "        else:\n\t            version_dict = {\"v\": latest_yml_version + 1}\n\t            latest_version += 1\n\t        # add the defined_in key if it exists\n\t        if defined_in:\n\t            version_dict[\"defined_in\"] = defined_in\n\t        # add the version to the model yml entry\n\t        versions_list.append(version_dict)\n\t        # update the latest version in the model yml entry\n\t        model_yml[\"versions\"] = versions_list\n", "        model_yml[\"latest_version\"] = latest_version\n\t        processed = process_model_yml(model_yml)\n\t        models[model_name] = processed\n\t        models_yml[\"models\"] = list(models.values())\n\t        return models_yml\n\t    def update_refs__sql(self, model_code: str, model_name: str, project_name: str):\n\t        import re\n\t        # pattern to search for ref() with optional spaces and either single or double quotes\n\t        pattern = re.compile(r\"{{\\s*ref\\s*\\(\\s*['\\\"]\" + re.escape(model_name) + r\"['\\\"]\\s*\\)\\s*}}\")\n\t        # replacement string with the new format\n", "        replacement = f\"{{{{ ref('{project_name}', '{model_name}') }}}}\"\n\t        # perform replacement\n\t        new_code = re.sub(pattern, replacement, model_code)\n\t        return new_code\n\t    def replace_source_with_ref__sql(\n\t        self, model_code: str, source_unique_id: str, model_unique_id: str\n\t    ):\n\t        import re\n\t        source_parsed = source_unique_id.split(\".\")\n\t        model_parsed = model_unique_id.split(\".\")\n", "        # pattern to search for source() with optional spaces and either single or double quotes\n\t        pattern = re.compile(\n\t            r\"{{\\s*source\\s*\\(\\s*['\\\"]\"\n\t            + re.escape(source_parsed[2])\n\t            + r\"['\\\"]\\s*,\\s*['\\\"]\"\n\t            + re.escape(source_parsed[3])\n\t            + r\"['\\\"]\\s*\\)\\s*}}\"\n\t        )\n\t        # replacement string with the new format\n\t        replacement = f\"{{{{ ref('{model_parsed[1]}', '{model_parsed[2]}') }}}}\"\n", "        # perform replacement\n\t        new_code = re.sub(pattern, replacement, model_code)\n\t        return new_code\n\t    def update_refs__python(self, model_code: str, model_name: str, project_name: str):\n\t        import re\n\t        # pattern to search for ref() with optional spaces and either single or double quotes\n\t        pattern = re.compile(r\"dbt\\.ref\\s*\\(\\s*['\\\"]\" + re.escape(model_name) + r\"['\\\"]\\s*\\)\")\n\t        # replacement string with the new format\n\t        replacement = f\"dbt.ref('{project_name}', '{model_name}')\"\n\t        # perform replacement\n", "        new_code = re.sub(pattern, replacement, model_code)\n\t        return new_code\n\t    def replace_source_with_ref__python(\n\t        self, model_code: str, source_unique_id: str, model_unique_id: str\n\t    ):\n\t        import re\n\t        source_parsed = source_unique_id.split(\".\")\n\t        model_parsed = model_unique_id.split(\".\")\n\t        # pattern to search for source() with optional spaces and either single or double quotes\n\t        pattern = re.compile(\n", "            r\"dbt\\.source\\s*\\(\\s*['\\\"]\"\n\t            + re.escape(source_parsed[2])\n\t            + r\"['\\\"]\\s*,\\s*['\\\"]\"\n\t            + re.escape(source_parsed[3])\n\t            + r\"['\\\"]\\s*\\)\"\n\t        )\n\t        # replacement string with the new format\n\t        replacement = f'dbt.ref(\"{model_parsed[1]}\", \"{model_parsed[2]}\")'\n\t        # perform replacement\n\t        new_code = re.sub(pattern, replacement, model_code)\n", "        return new_code\n\tclass DbtMeshConstructor(DbtMeshFileEditor):\n\t    def __init__(\n\t        self, project_path: Path, node: ManifestNode, catalog: Optional[CatalogTable] = None\n\t    ):\n\t        self.project_path = project_path\n\t        self.node = node\n\t        self.model_catalog = catalog\n\t        self.name = node.name\n\t        self.file_manager = DbtFileManager(read_project_path=project_path)\n", "    def get_patch_path(self) -> Path:\n\t        \"\"\"Returns the path to the yml file where the resource is defined or described\"\"\"\n\t        if self.node.resource_type in [\n\t            NodeType.Model,\n\t            NodeType.Seed,\n\t            NodeType.Snapshot,\n\t            NodeType.Macro,\n\t            NodeType.Test,\n\t        ]:\n\t            # find yml path for resoruces that are not defined\n", "            yml_path = Path(self.node.patch_path.split(\"://\")[1]) if self.node.patch_path else None\n\t        else:\n\t            yml_path = self.get_resource_path()\n\t        # if the model doesn't have a patch path, create a new yml file in the models directory\n\t        if not yml_path:\n\t            resource_path = self.get_resource_path()\n\t            if resource_path is None:\n\t                # If this happens, then the model doesn't have a model file, either, which is cause for alarm.\n\t                raise ModelFileNotFoundError(\n\t                    f\"Unable to locate the file defining {self.node.name}. Aborting\"\n", "                )\n\t            filename = f\"_{self.node.resource_type.pluralize()}.yml\"\n\t            yml_path = resource_path.parent / filename\n\t        return yml_path\n\t    def get_resource_path(self) -> Path:\n\t        \"\"\"\n\t        Returns the path to the file where the resource is defined\n\t        for yml-only nodes (generic tests, metrics, exposures, sources)\n\t        this will be the path to the yml file where the definitions\n\t        for all others this will be the .sql or .py file for the resource\n", "        \"\"\"\n\t        return Path(self.node.original_file_path)\n\t    def add_model_contract(self) -> None:\n\t        \"\"\"Adds a model contract to the model's yaml\"\"\"\n\t        yml_path = self.get_patch_path()\n\t        logger.info(f\"Adding contract to {self.node.name} at {yml_path}\")\n\t        # read the yml file\n\t        # pass empty dict if no file contents returned\n\t        models_yml = self.file_manager.read_file(yml_path)\n\t        if isinstance(models_yml, str):\n", "            raise FileEditorException(\n\t                f\"Unexpected string values in dumped model data in {yml_path}.\"\n\t            )\n\t        updated_yml = self.add_model_contract_to_yml(\n\t            model_name=self.node.name,\n\t            model_catalog=self.model_catalog,\n\t            models_yml=models_yml,\n\t        )\n\t        # write the updated yml to the file\n\t        self.file_manager.write_file(yml_path, updated_yml)\n", "    def add_model_access(self, access_type: AccessType) -> None:\n\t        \"\"\"Adds a model contract to the model's yaml\"\"\"\n\t        yml_path = self.get_patch_path()\n\t        logger.info(f\"Adding {access_type} access to {self.node.name} at {yml_path}\")\n\t        # read the yml file\n\t        # pass empty dict if no file contents returned\n\t        models_yml = self.file_manager.read_file(yml_path)\n\t        if isinstance(models_yml, str):\n\t            raise FileEditorException(\n\t                f\"Unexpected string values in dumped model data in {yml_path}.\"\n", "            )\n\t        updated_yml = self.add_access_to_model_yml(\n\t            model_name=self.node.name,\n\t            access_type=access_type,\n\t            models_yml=models_yml,\n\t        )\n\t        # write the updated yml to the file\n\t        self.file_manager.write_file(yml_path, updated_yml)\n\t    def add_model_version(\n\t        self, prerelease: Optional[bool] = False, defined_in: Optional[os.PathLike] = None\n", "    ) -> None:\n\t        \"\"\"Adds a model version to the model's yaml\"\"\"\n\t        yml_path = self.get_patch_path()\n\t        # read the yml file\n\t        # pass empty dict if no file contents returned\n\t        models_yml = self.file_manager.read_file(yml_path) or {}\n\t        latest_yml_version = self.get_latest_yml_defined_version(\n\t            resources_yml_to_dict(models_yml).get(self.node.name, {})  # type: ignore\n\t        )\n\t        try:\n", "            updated_yml = self.add_model_version_to_yml(\n\t                model_name=self.node.name,\n\t                models_yml=models_yml,\n\t                prerelease=prerelease,\n\t                defined_in=defined_in,\n\t            )\n\t            # write the updated yml to the file\n\t            self.file_manager.write_file(yml_path, updated_yml)\n\t            logger.info(\"Model version added to model yml\")\n\t        except Exception as e:\n", "            logger.error(f\"Error adding model version to model yml: {e}\")\n\t            logger.exception(e)\n\t        # create the new version file\n\t        # if we're incrementing the version, write the new version file with a copy of the code\n\t        latest_version = int(self.node.latest_version) if self.node.latest_version else 0\n\t        last_version_file_name = f\"{self.node.name}_v{latest_version}.{self.node.language}\"\n\t        next_version_file_name = (\n\t            f\"{defined_in}.{self.node.language}\"\n\t            if defined_in\n\t            else f\"{self.node.name}_v{latest_yml_version + 1}.{self.node.language}\"\n", "        )\n\t        model_path = self.get_resource_path()\n\t        if model_path is None:\n\t            raise ModelFileNotFoundError(\n\t                f\"Unable to find path to model {self.node.name}. Aborting.\"\n\t            )\n\t        model_folder = model_path.parent\n\t        next_version_path = model_folder / next_version_file_name\n\t        last_version_path = model_folder / last_version_file_name\n\t        # if this is the first version, rename the original file to the next version\n", "        if not self.node.latest_version:\n\t            logger.info(f\"Creating first version of {self.node.name} at {next_version_path}\")\n\t            Path(self.project_path).joinpath(model_path).rename(\n\t                Path(self.project_path).joinpath(next_version_path)\n\t            )\n\t        else:\n\t            # if existing versions, create the new one\n\t            logger.info(f\"Creating new version of {self.node.name} at {next_version_path}\")\n\t            self.file_manager.write_file(next_version_path, self.node.raw_code)\n\t            # if the existing version doesn't use the _v{version} naming convention, rename it to the previous version\n", "            if not model_path.stem.endswith(f\"_v{latest_version}\"):\n\t                logger.info(\n\t                    f\"Renaming existing version of {self.node.name} from {model_path.name} to {last_version_path.name}\"\n\t                )\n\t                Path(self.project_path).joinpath(model_path).rename(\n\t                    Path(self.project_path).joinpath(last_version_path)\n\t                )\n\t    def update_model_refs(self, model_name: str, project_name: str) -> None:\n\t        \"\"\"Updates the model refs in the model's sql file\"\"\"\n\t        model_path = self.get_resource_path()\n", "        if model_path is None:\n\t            raise ModelFileNotFoundError(\n\t                f\"Unable to find path to model {self.node.name}. Aborting.\"\n\t            )\n\t        # read the model file\n\t        model_code = str(self.file_manager.read_file(model_path))\n\t        # This can be defined in the init for this clas.\n\t        ref_update_methods = {'sql': self.update_refs__sql, 'python': self.update_refs__python}\n\t        # Here, we're trusting the dbt-core code to check the languages for us. 🐉\n\t        updated_code = ref_update_methods[self.node.language](\n", "            model_name=model_name,\n\t            project_name=project_name,\n\t            model_code=model_code,\n\t        )\n\t        # write the updated model code to the file\n\t        self.file_manager.write_file(model_path, updated_code)\n\t    def replace_source_with_refs(self, source_unique_id: str, model_unique_id: str) -> None:\n\t        \"\"\"Updates the model refs in the model's sql file\"\"\"\n\t        model_path = self.get_resource_path()\n\t        if model_path is None:\n", "            raise ModelFileNotFoundError(\n\t                f\"Unable to find path to model {self.node.name}. Aborting.\"\n\t            )\n\t        # read the model file\n\t        model_code = str(self.file_manager.read_file(model_path))\n\t        # This can be defined in the init for this clas.\n\t        ref_update_methods = {\n\t            'sql': self.replace_source_with_ref__sql,\n\t            'python': self.replace_source_with_ref__python,\n\t        }\n", "        # Here, we're trusting the dbt-core code to check the languages for us. 🐉\n\t        updated_code = ref_update_methods[self.node.language](\n\t            model_code=model_code,\n\t            source_unique_id=source_unique_id,\n\t            model_unique_id=model_unique_id,\n\t        )\n\t        # write the updated model code to the file\n\t        self.file_manager.write_file(model_path, updated_code)\n"]}
{"filename": "dbt_meshify/storage/file_manager.py", "chunked_list": ["# classes that deal specifically with file manipulation\n\t# of dbt files to be used in the meshify dbt project\n\timport abc\n\tfrom abc import ABC\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict, Optional, Union\n\tfrom dbt.contracts.util import Identifier\n\tfrom ruamel.yaml import YAML\n\tfrom ruamel.yaml.compat import StringIO\n\tfrom ruamel.yaml.representer import Representer\n", "class DbtYAML(YAML):\n\t    \"\"\"dbt-compatible YAML class.\"\"\"\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.preserve_quotes = True\n\t        self.width = 4096\n\t        self.indent(mapping=2, sequence=4, offset=2)\n\t    def dump(self, data, stream=None, **kw):\n\t        inefficient = False\n\t        if stream is None:\n", "            inefficient = True\n\t            stream = StringIO()\n\t        super().dump(data, stream, **kw)\n\t        if inefficient:\n\t            return stream.getvalue()\n\tyaml = DbtYAML()\n\tyaml.register_class(Identifier)\n\tFileContent = Union[Dict[str, str], str]\n\tclass BaseFileManager(ABC):\n\t    @abc.abstractmethod\n", "    def read_file(self, path: Path) -> Union[Dict[str, Any], str, None]:\n\t        \"\"\"Returns the content from a file.\"\"\"\n\t        pass\n\t    @abc.abstractmethod\n\t    def write_file(self, path: Path, file_contents: Any) -> None:\n\t        \"\"\"Write content to a file.\"\"\"\n\t        pass\n\tclass DbtFileManager(BaseFileManager):\n\t    def __init__(\n\t        self,\n", "        read_project_path: Path,\n\t        write_project_path: Optional[Path] = None,\n\t    ) -> None:\n\t        self.read_project_path = read_project_path\n\t        self.write_project_path = write_project_path if write_project_path else read_project_path\n\t    def read_file(self, path: Path) -> Union[Dict[str, Any], str, None]:\n\t        \"\"\"Returns the file contents at a given path\"\"\"\n\t        full_path = self.read_project_path / path\n\t        is_yml = full_path.suffix == \".yml\"\n\t        if not full_path.exists():\n", "            return {} if is_yml else None\n\t        elif is_yml:\n\t            return yaml.load(full_path.read_text())\n\t        else:\n\t            return full_path.read_text()\n\t    def write_file(\n\t        self,\n\t        path: Path,\n\t        file_contents: Optional[Union[Dict[str, Any], str]] = None,\n\t        writeback=False,\n", "    ) -> None:\n\t        \"\"\"Returns the yaml for a model in the dbt project's manifest\"\"\"\n\t        # workaround to let the same DbtFileManager write back to the same project in the event that it's managing movements between two project paths\n\t        # let it be known I do not like this\n\t        if not writeback:\n\t            full_path = self.write_project_path / path\n\t        else:\n\t            full_path = self.read_project_path / path\n\t        full_path.parent.mkdir(parents=True, exist_ok=True)\n\t        if full_path.suffix == \".yml\":\n", "            full_path.write_text(yaml.dump(file_contents))\n\t        else:\n\t            full_path.write_text(file_contents)  # type: ignore\n\t    def copy_file(self, path: Path) -> None:\n\t        file_contents = self.read_file(path)\n\t        self.write_file(path, file_contents)\n\t    def move_file(self, path: Path) -> None:\n\t        \"\"\"\n\t        move a file from the read project to the write project\n\t        \"\"\"\n", "        old_path = self.read_project_path / path\n\t        new_path = self.write_project_path / path\n\t        new_path.parent.mkdir(parents=True, exist_ok=True)\n\t        old_path.rename(new_path)\n\t    def delete_file(self, path: Path) -> None:\n\t        \"\"\"deletes the specified file\"\"\"\n\t        delete_path = self.read_project_path / path\n\t        delete_path.unlink()\n"]}
{"filename": "dbt_meshify/storage/__init__.py", "chunked_list": []}
{"filename": "dbt_meshify/utilities/grouper.py", "chunked_list": ["import os\n\tfrom pathlib import Path\n\tfrom typing import Any, Dict, Optional, Set, Tuple, Union\n\timport networkx\n\tfrom dbt.contracts.graph.nodes import Group, ModelNode\n\tfrom dbt.contracts.graph.unparsed import Owner\n\tfrom dbt.node_types import AccessType, NodeType\n\tfrom loguru import logger\n\tfrom dbt_meshify.dbt_projects import DbtProject, DbtSubProject\n\tfrom dbt_meshify.exceptions import ModelFileNotFoundError\n", "from dbt_meshify.storage.file_content_editors import DbtMeshFileEditor\n\tfrom dbt_meshify.storage.file_manager import DbtFileManager\n\tclass ResourceGroupingException(BaseException):\n\t    \"\"\"Exceptions relating to grouping of resources.\"\"\"\n\tclass ResourceGrouper:\n\t    \"\"\"\n\t    The ResourceGrouper is responsible for generating a dbt-core Group for a\n\t    collection of resources in a DbtProject, and for providing access control\n\t    recommendations based on the reference characteristics for each resource.\n\t    \"\"\"\n", "    def __init__(self, project: Union[DbtProject, DbtSubProject]):\n\t        self.project = project\n\t        self.meshify = DbtMeshFileEditor()\n\t        self.file_manager = DbtFileManager(read_project_path=project.path)\n\t    @classmethod\n\t    def identify_interface(cls, graph: networkx.Graph, selected_bunch: Set[str]) -> Set[str]:\n\t        \"\"\"\n\t        Given a graph, find the interface nodes, where interface nodes are the nodes that\n\t        either have downstream consumers outside the selected bunch OR are nodes\n\t        with no downstream consumers.\n", "        \"\"\"\n\t        boundary_nodes = {edge[0] for edge in networkx.edge_boundary(graph, selected_bunch)}\n\t        leaf_nodes = {node for node, out_degree in graph.out_degree() if out_degree == 0}\n\t        return boundary_nodes | leaf_nodes\n\t    @classmethod\n\t    def classify_resource_access(cls, graph, nodes):\n\t        \"\"\"\n\t        Identify what access types each node should have. We can make some simplifying assumptions about\n\t        recommended access for a group.\n\t        For example, interfaces (nodes on the boundary of a subgraph or leaf nodes) should be public,\n", "        whereas nodes that are not a referenced are safe for a private access level.\n\t        \"\"\"\n\t        boundary_nodes = cls.identify_interface(graph, nodes)\n\t        resources = {\n\t            node: AccessType.Public if node in boundary_nodes else AccessType.Private\n\t            for node in nodes\n\t        }\n\t        logger.info(f\"Identified resource access types based on the graph: {resources}\")\n\t        return resources\n\t    @classmethod\n", "    def clean_subgraph(cls, graph: networkx.DiGraph) -> networkx.DiGraph:\n\t        \"\"\"Generate a subgraph that does not contain test resource types.\"\"\"\n\t        test_nodes = set(node for node in graph.nodes if node.startswith('test'))\n\t        return graph.subgraph(set(graph.nodes) - test_nodes)\n\t    def _generate_resource_group(\n\t        self,\n\t        name: str,\n\t        owner: Owner,\n\t        path: os.PathLike,\n\t        select: str,\n", "        exclude: Optional[str] = None,\n\t        selector: Optional[str] = None,\n\t    ) -> Tuple[Group, Dict[str, AccessType]]:\n\t        \"\"\"Generate the ResourceGroup that we want to apply to the project.\"\"\"\n\t        group = Group(\n\t            name=name,\n\t            owner=owner,\n\t            package_name=self.project.name,\n\t            original_file_path=os.path.relpath(path, self.project.path),\n\t            unique_id=f\"group.{self.project.name}.{name}\",\n", "            path=os.path.relpath(path, self.project.path / Path(\"models\")),\n\t            resource_type=NodeType.Group,\n\t        )\n\t        if not (select or exclude or selector):\n\t            nodes = set()\n\t        else:\n\t            nodes = self.project.select_resources(\n\t                select=select, exclude=exclude, selector=selector, output_key=\"unique_id\"\n\t            )\n\t        logger.info(f\"Selected {len(nodes)} resources: {nodes}\")\n", "        # Check if any of the selected nodes are already in a group of a different name. If so, raise an exception.\n\t        nodes = set(filter(lambda x: not x.startswith(\"source\"), nodes))\n\t        for node in nodes:\n\t            existing_group = self.project.manifest.nodes[node].config.group\n\t            if existing_group is None or existing_group == group.name:\n\t                continue\n\t            raise ResourceGroupingException(\n\t                f\"The node {node} has been selected for addition to a new group, however {node} is already part \"\n\t                f\"of the {existing_group} group. Please remove {node} from its group and try again.\"\n\t            )\n", "        cleaned_subgraph = self.clean_subgraph(self.project.graph.graph)\n\t        resources = self.classify_resource_access(cleaned_subgraph, nodes)\n\t        return group, resources\n\t    def add_group(\n\t        self,\n\t        name: str,\n\t        owner: Owner,\n\t        path: os.PathLike,\n\t        select: str,\n\t        exclude: Optional[str] = None,\n", "        selector: Optional[str] = None,\n\t    ) -> None:\n\t        \"\"\"Create a ResourceGroup for a dbt project.\"\"\"\n\t        group, resources = self._generate_resource_group(\n\t            name, owner, path, select, exclude, selector\n\t        )\n\t        group_path = Path(group.original_file_path)\n\t        try:\n\t            group_yml: Dict[str, str] = self.file_manager.read_file(group_path)  # type: ignore\n\t        except FileNotFoundError:\n", "            group_yml = {}\n\t        output_yml = self.meshify.add_group_to_yml(group, group_yml)\n\t        self.file_manager.write_file(group_path, output_yml)\n\t        logger.info(f\"Adding resources to group '{group.name}'...\")\n\t        for resource, access_type in resources.items():\n\t            # TODO: revisit this logic other resource types\n\t            if not resource.startswith(\"model\"):\n\t                continue\n\t            model: ModelNode = self.project.models[resource]\n\t            if model.patch_path:\n", "                path = Path(model.patch_path.split(\"://\")[1])\n\t            else:\n\t                if not model.original_file_path:\n\t                    raise ModelFileNotFoundError(\"Unable to locate model file. Failing.\")\n\t                path = Path(model.original_file_path).parent / '_models.yml'\n\t            try:\n\t                file_yml: Dict[str, Any] = self.file_manager.read_file(path)  # type: ignore\n\t            except FileNotFoundError:\n\t                file_yml = {}\n\t            logger.info(\n", "                f\"Adding model '{model.name}' to group '{group.name}' in file '{path.name}'\"\n\t            )\n\t            try:\n\t                output_yml = self.meshify.add_group_and_access_to_model_yml(\n\t                    model.name, group, access_type, file_yml\n\t                )\n\t                self.file_manager.write_file(path, output_yml)\n\t                logger.success(f\"Successfully added model '{model.name}' to group '{group.name}'\")\n\t            except Exception as e:\n\t                logger.error(f\"Failed to add model '{model.name}' to group '{group.name}'\")\n", "                logger.exception(e)\n"]}
{"filename": "dbt_meshify/utilities/__init__.py", "chunked_list": []}
