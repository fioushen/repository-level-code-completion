{"filename": "setup.py", "chunked_list": ["#############################################################################\n\t#\n\t#   Setup based on Labelme install script:\n\t#   https://github.com/wkentaro/labelme\n\t#\n\t##############################################################################\n\timport re\n\tfrom setuptools import setup, find_packages\n\tdef get_version():\n\t    filename = \"faceshine/__init__.py\"\n", "    with open(filename) as f:\n\t        match = re.search(\n\t            r\"\"\"^__version__ = ['\"]([^'\"]*)['\"]\"\"\", f.read(), re.M\n\t        )\n\t    if not match:\n\t        raise RuntimeError(\"{} doesn't contain __version__\".format(filename))\n\t    version = match.groups()[0]\n\t    return version\n\tdef get_install_requires():\n\t    install_requires = [\n", "        \"numpy>=1.24.3\",\n\t        \"torch>=2.0.0\",\n\t        \"opencv-python\",\n\t        \"flask>=2.3.2\",\n\t        \"flask_restful>=0.3.10\",\n\t        \"pillow>=9.5.0\",\n\t        \"torchvision\",\n\t        \"colorama\",\n\t        \"matplotlib\",\n\t        \"fastprogress\",\n", "        \"pandas\",\n\t        \"scipy\",\n\t        \"deoldify\",\n\t        \"realesrgan\",\n\t        \"zeroscratches\",\n\t        \"huggingface_hub\"\n\t    ]\n\t    return install_requires\n\tdef get_long_description():\n\t    with open(\"README.md\") as f:\n", "        long_description = f.read()\n\t    try:\n\t        # when this package is being released\n\t        import github2pypi\n\t        return github2pypi.replace_url(\n\t            slug=\"leonelhs/face-shine\", content=long_description, branch=\"main\"\n\t        )\n\t    except ImportError:\n\t        # when this package is being installed\n\t        return long_description\n", "def main():\n\t    setup(\n\t        name='faceshine',\n\t        version=get_version(),\n\t        long_description=get_long_description(),\n\t        long_description_content_type=\"text/markdown\",\n\t        packages=find_packages(),\n\t        url='https://github.com/leonelhs/faceshine',\n\t        license='Apache',\n\t        author='leonel hernandez',\n", "        author_email='leonelhs@gmail.com',\n\t        description='Photo image enhancer',\n\t        install_requires=get_install_requires(),\n\t        package_data={\"faceshine\": []},\n\t        entry_points={\"console_scripts\": [\"faceshine=faceshine.__main__:main\"]},\n\t        classifiers=[\n\t            \"Development Status :: 5 - Production/Stable\",\n\t            \"Intended Audience :: Developers\",\n\t            \"Intended Audience :: Science/Research\",\n\t            \"Natural Language :: English\",\n", "            \"Operating System :: OS Independent\",\n\t            \"Programming Language :: Python\",\n\t            \"Programming Language :: Python :: 3.5\",\n\t            \"Programming Language :: Python :: 3.6\",\n\t            \"Programming Language :: Python :: 3.7\",\n\t            \"Programming Language :: Python :: 3.8\",\n\t            \"Programming Language :: Python :: 3.9\",\n\t            \"Programming Language :: Python :: 3 :: Only\",\n\t        ],\n\t    )\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "playground.py", "chunked_list": ["#  python playground.py -i /home/leonel/damage.jpg -o test.jpg -g 0 -dn 0.2 --fp32 -s 2 -n realesr-general-x4v3\n\timport argparse\n\timport cv2\n\timport glob\n\timport os\n\tfrom basicsr.archs.rrdbnet_arch import RRDBNet\n\tfrom basicsr.utils.download_util import load_file_from_url\n\tfrom realesrgan import RealESRGANer\n\tfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\tdef main():\n", "    \"\"\"Inference demo for Real-ESRGAN.\n\t    \"\"\"\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('-i', '--input', type=str, default='inputs', help='Input image or folder')\n\t    parser.add_argument(\n\t        '-n',\n\t        '--model_name',\n\t        type=str,\n\t        default='RealESRGAN_x4plus',\n\t        help=('Model names: RealESRGAN_x4plus | RealESRNet_x4plus | RealESRGAN_x4plus_anime_6B | RealESRGAN_x2plus | '\n", "              'realesr-animevideov3 | realesr-general-x4v3'))\n\t    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder')\n\t    parser.add_argument(\n\t        '-dn',\n\t        '--denoise_strength',\n\t        type=float,\n\t        default=0.5,\n\t        help=('Denoise strength. 0 for weak denoise (keep noise), 1 for strong denoise ability. '\n\t              'Only used for the realesr-general-x4v3 model'))\n\t    parser.add_argument('-s', '--outscale', type=float, default=4, help='The final upsampling scale of the image')\n", "    parser.add_argument(\n\t        '--model_path', type=str, default=None, help='[Option] Model path. Usually, you do not need to specify it')\n\t    parser.add_argument('--suffix', type=str, default='out', help='Suffix of the restored image')\n\t    parser.add_argument('-t', '--tile', type=int, default=0, help='Tile size, 0 for no tile during testing')\n\t    parser.add_argument('--tile_pad', type=int, default=10, help='Tile padding')\n\t    parser.add_argument('--pre_pad', type=int, default=0, help='Pre padding size at each border')\n\t    parser.add_argument('--face_enhance', action='store_true', help='Use GFPGAN to enhance face')\n\t    parser.add_argument(\n\t        '--fp32', action='store_true', help='Use fp32 precision during inference. Default: fp16 (half precision).')\n\t    parser.add_argument(\n", "        '--alpha_upsampler',\n\t        type=str,\n\t        default='realesrgan',\n\t        help='The upsampler for the alpha channels. Options: realesrgan | bicubic')\n\t    parser.add_argument(\n\t        '--ext',\n\t        type=str,\n\t        default='auto',\n\t        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs')\n\t    parser.add_argument(\n", "        '-g', '--gpu-id', type=int, default=None, help='gpu device to use (default=None) can be 0,1,2 for multi-gpu')\n\t    args = parser.parse_args()\n\t    # determine models according to model names\n\t    args.model_name = args.model_name.split('.')[0]\n\t    if args.model_name == 'RealESRGAN_x4plus':  # x4 RRDBNet model\n\t        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n\t        netscale = 4\n\t        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth']\n\t    elif args.model_name == 'RealESRNet_x4plus':  # x4 RRDBNet model\n\t        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n", "        netscale = 4\n\t        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.1/RealESRNet_x4plus.pth']\n\t    elif args.model_name == 'RealESRGAN_x4plus_anime_6B':  # x4 RRDBNet model with 6 blocks\n\t        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n\t        netscale = 4\n\t        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth']\n\t    elif args.model_name == 'RealESRGAN_x2plus':  # x2 RRDBNet model\n\t        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n\t        netscale = 2\n\t        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth']\n", "    elif args.model_name == 'realesr-animevideov3':  # x4 VGG-style model (XS size)\n\t        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n\t        netscale = 4\n\t        file_url = ['https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth']\n\t    elif args.model_name == 'realesr-general-x4v3':  # x4 VGG-style model (S size)\n\t        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n\t        netscale = 4\n\t        file_url = [\n\t            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth',\n\t            'https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth'\n", "        ]\n\t    # determine model paths\n\t    if args.model_path is not None:\n\t        model_path = args.model_path\n\t    else:\n\t        model_path = os.path.join('weights-0', args.model_name + '.pth')\n\t        if not os.path.isfile(model_path):\n\t            ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n\t            for url in file_url:\n\t                # model_path will be updated\n", "                model_path = load_file_from_url(\n\t                    url=url, model_dir=os.path.join(ROOT_DIR, 'weights-0'), progress=True, file_name=None)\n\t    # use dni to control the denoise strength\n\t    dni_weight = None\n\t    if args.model_name == 'realesr-general-x4v3' and args.denoise_strength != 1:\n\t        wdn_model_path = model_path.replace('realesr-general-x4v3', 'realesr-general-wdn-x4v3')\n\t        model_path = [model_path, wdn_model_path]\n\t        dni_weight = [args.denoise_strength, 1 - args.denoise_strength]\n\t    print(model_path)\n\t    print(dni_weight)\n", "    # restorer\n\t    upsampler = RealESRGANer(\n\t        scale=netscale,\n\t        model_path=model_path,\n\t        dni_weight=dni_weight,\n\t        model=model,\n\t        tile=args.tile,\n\t        tile_pad=args.tile_pad,\n\t        pre_pad=args.pre_pad,\n\t        half=not args.fp32,\n", "        gpu_id=args.gpu_id)\n\t    if args.face_enhance:  # Use GFPGAN for face enhancement\n\t        from gfpgan import GFPGANer\n\t        face_enhancer = GFPGANer(\n\t            model_path='https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n\t            upscale=args.outscale,\n\t            arch='clean',\n\t            channel_multiplier=2,\n\t            bg_upsampler=upsampler)\n\t    os.makedirs(args.output, exist_ok=True)\n", "    if os.path.isfile(args.input):\n\t        paths = [args.input]\n\t    else:\n\t        paths = sorted(glob.glob(os.path.join(args.input, '*')))\n\t    for idx, path in enumerate(paths):\n\t        imgname, extension = os.path.splitext(os.path.basename(path))\n\t        print('Testing', idx, imgname)\n\t        img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n\t        if len(img.shape) == 3 and img.shape[2] == 4:\n\t            img_mode = 'RGBA'\n", "        else:\n\t            img_mode = None\n\t        try:\n\t            if args.face_enhance:\n\t                _, _, output = face_enhancer.enhance(img, has_aligned=False, only_center_face=False, paste_back=True)\n\t            else:\n\t                output, _ = upsampler.enhance(img, outscale=args.outscale)\n\t        except RuntimeError as error:\n\t            print('Error', error)\n\t            print('If you encounter CUDA out of memory, try to set --tile with a smaller number.')\n", "        else:\n\t            if args.ext == 'auto':\n\t                extension = extension[1:]\n\t            else:\n\t                extension = args.ext\n\t            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n\t                extension = 'png'\n\t            if args.suffix == '':\n\t                save_path = os.path.join(args.output, f'{imgname}.{extension}')\n\t            else:\n", "                save_path = os.path.join(args.output, f'{imgname}_{args.suffix}.{extension}')\n\t            cv2.imwrite(save_path, output)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "faceshine/__main__.py", "chunked_list": ["from colorama import Fore, Style\n\tfrom colorama import init as colorama_init\n\tfrom faceshine import appFaceShine\n\tcolorama_init()\n\tdef main():\n\t    print(Fore.BLUE)\n\t    print(\"*********************************\")\n\t    print(\"* Face Shine Service is running *\")\n\t    print(\"*********************************\")\n\t    print(Style.RESET_ALL)\n", "    appFaceShine.run(debug=False, port='5000')\n\t    print(Fore.GREEN)\n\t    print(\"******************************\")\n\t    print(\"*      Service finished      *\")\n\t    print(\"******************************\")\n\t    print(Style.RESET_ALL)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "faceshine/app.py", "chunked_list": ["from flask import Flask, jsonify\n\tfrom flask_restful import Resource, Api, reqparse\n\tfrom werkzeug.datastructures import FileStorage\n\tfrom faceshine import data2image\n\tfrom faceshine.tasks import TaskZeroBackground, \\\n\t    TaskImageColorizer, TaskLowLight, TaskEraseScratches, TaskSuperFace, TaskFaceSegmentation\n\tappFaceShine = Flask(__name__)\n\tapi = Api(appFaceShine)\n\ttaskFaceSegmentation = TaskFaceSegmentation()\n\ttaskSuperFace = TaskSuperFace()\n", "taskLowLight = TaskLowLight()\n\ttaskEraseScratches = TaskEraseScratches()\n\ttaskImageColorizer = TaskImageColorizer()\n\ttaskZeroBackground = TaskZeroBackground()\n\tparser = reqparse.RequestParser()\n\tparser.add_argument('image',\n\t                    type=FileStorage,\n\t                    location='files',\n\t                    required=False,\n\t                    help='Provide one image file')\n", "class Index(Resource):\n\t    def get(self):\n\t        return {'It works!': 'AI Remote Procedure Machine'}\n\tclass FaceSegmentation(Resource):\n\t    def post(self):\n\t        args = parser.parse_args()\n\t        stream = args['image'].read()\n\t        image = data2image(stream)\n\t        prediction = taskFaceSegmentation.executeTask(image)\n\t        return jsonify(prediction.tolist())\n", "class SuperFace(Resource):\n\t    def post(self):\n\t        args = parser.parse_args()\n\t        stream_a = args['image'].read()\n\t        image = data2image(stream_a)\n\t        prediction = taskSuperFace.executeTask(image)\n\t        return jsonify(prediction.tolist())\n\tclass ImageLowLight(Resource):\n\t    def post(self):\n\t        args = parser.parse_args()\n", "        stream_a = args['image'].read()\n\t        image = data2image(stream_a)\n\t        prediction = taskLowLight.executeTask(image)\n\t        return jsonify(prediction.tolist())\n\tclass EraseScratches(Resource):\n\t    def post(self):\n\t        args = parser.parse_args()\n\t        stream_a = args['image'].read()\n\t        image = data2image(stream_a)\n\t        prediction = taskEraseScratches.executeTask(image)\n", "        return jsonify(prediction.tolist())\n\tclass ImageColorizer(Resource):\n\t    def post(self):\n\t        args = parser.parse_args()\n\t        stream_a = args['image'].read()\n\t        image = data2image(stream_a)\n\t        prediction = taskImageColorizer.executeTask(image)\n\t        return jsonify(prediction.tolist())\n\tclass ZeroBackground(Resource):\n\t    def post(self):\n", "        args = parser.parse_args()\n\t        stream_a = args['image'].read()\n\t        image = data2image(stream_a)\n\t        prediction = taskZeroBackground.executeTask(image)\n\t        return jsonify(prediction.tolist())\n\tapi.add_resource(FaceSegmentation, '/segment_face')\n\tapi.add_resource(SuperFace, '/super_face')\n\tapi.add_resource(EraseScratches, '/erase_scratches')\n\tapi.add_resource(ImageLowLight, '/enhance_light')\n\tapi.add_resource(ImageColorizer, '/colorize')\n", "api.add_resource(ZeroBackground, '/zero_background')\n\tapi.add_resource(Index, '/')\n"]}
{"filename": "faceshine/__init__.py", "chunked_list": ["__appname__ = \"Face Shine\"\n\t__version__ = \"1.0.4\"\n\tfrom .utils import tensor_to_ndarray\n\tfrom .utils import image_to_tensor\n\tfrom .utils import array2image\n\tfrom .utils import data2image\n\tfrom .utils import config\n\tfrom .app import appFaceShine\n"]}
{"filename": "faceshine/utils.py", "chunked_list": ["import PIL\n\timport cv2\n\timport json\n\timport torch\n\timport numpy as np\n\tfrom PIL.Image import Image\n\timport torchvision.transforms as transforms\n\tfrom torchvision.utils import make_grid\n\tdef config():\n\t    data = open('faceshine/conf.json')\n", "    return json.load(data)\n\tdef array2image(ndarray):\n\t    return PIL.Image.fromarray(np.uint8(ndarray)).convert('RGB')\n\tdef data2image(stream):\n\t    np_img = np.fromstring(stream, np.uint8)\n\t    return cv2.imdecode(np_img, cv2.IMREAD_UNCHANGED)\n\tdef image_to_tensor(image):\n\t    to_tensor = transforms.Compose([\n\t        transforms.ToTensor(),\n\t        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n", "    ])\n\t    return to_tensor(image)\n\t# Method are common to:\n\t# TaskMaskScratches\n\t# TaskEraseScratches\n\t# TaskLowLight\n\tdef tensor_to_ndarray(tensor, nrow=1, padding=0, normalize=True):\n\t    grid = make_grid(tensor, nrow, padding, normalize)\n\t    return grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n"]}
{"filename": "faceshine/tasks/__init__.py", "chunked_list": ["from .base import Task\n\tfrom .segmentation import TaskZeroBackground\n\tfrom .colorizer import TaskImageColorizer\n\tfrom .lowlight import TaskLowLight\n\tfrom .zeroscratches import TaskEraseScratches\n\tfrom .superface import TaskSuperFace\n\tfrom .faceparser import TaskFaceSegmentation\n"]}
{"filename": "faceshine/tasks/superface/__init__.py", "chunked_list": ["from .task_super_face import TaskSuperFace\n"]}
{"filename": "faceshine/tasks/superface/task_super_face.py", "chunked_list": ["import os\n\timport numpy as np\n\tfrom basicsr.archs.rrdbnet_arch import RRDBNet\n\tfrom gfpgan import GFPGANer\n\tfrom huggingface_hub import snapshot_download\n\tfrom realesrgan import RealESRGANer\n\tfrom realesrgan.archs.srvgg_arch import SRVGGNetCompact\n\tfrom faceshine.tasks import Task\n\tROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n\tREALESRGAN_REPO_ID = 'leonelhs/realesrgan'\n", "GFPGAN_REPO_ID = 'leonelhs/gfpgan'\n\tdef select_model(model_name):\n\t    model = None\n\t    netscale = 4\n\t    dni_weight = None\n\t    snapshot_folder = snapshot_download(repo_id=REALESRGAN_REPO_ID)\n\t    model_path = os.path.join(snapshot_folder, model_name)\n\t    if model_name == 'RealESRGAN_x4plus.pth':  # x4 RRDBNet model\n\t        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n\t    if model_name == 'RealESRNet_x4plus.pth':  # x4 RRDBNet model\n", "        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=4)\n\t    if model_name == 'RealESRGAN_x4plus_anime_6B.pth':  # x4 RRDBNet model with 6 blocks\n\t        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=6, num_grow_ch=32, scale=4)\n\t    if model_name == 'RealESRGAN_x2plus.pth':  # x2 RRDBNet model\n\t        model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n\t        netscale = 2  # This is\n\t    if model_name == 'realesr-animevideov3.pth':  # x4 VGG-style model (XS size)\n\t        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=16, upscale=4, act_type='prelu')\n\t    if model_name == 'realesr-general-x4v3.pth':  # x4 VGG-style model (S size)\n\t        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n", "        model_path = [\n\t            os.path.join(snapshot_folder, \"realesr-general-wdn-x4v3.pth\"),\n\t            os.path.join(snapshot_folder, \"realesr-general-x4v3.pth'\")\n\t        ]\n\t        dni_weight = [0.2, 0.8]\n\t    return model, netscale, model_path, dni_weight\n\tclass TaskSuperFace(Task):\n\t    def __init__(self):\n\t        super().__init__()\n\t        scale = 2\n", "        model_name = \"RealESRGAN_x4plus.pth\"\n\t        model, netscale, model_path, dni_weight = select_model(model_name)\n\t        upsampler = RealESRGANer(\n\t            scale=netscale,\n\t            model_path=model_path,\n\t            dni_weight=dni_weight,\n\t            model=model,\n\t            tile=0,\n\t            tile_pad=10,\n\t            pre_pad=0,\n", "            half=False,\n\t            gpu_id=0)\n\t        # output, _ = upsampler.enhance(img, outscale=outscale)\n\t        snapshot_folder = snapshot_download(repo_id=GFPGAN_REPO_ID)\n\t        model_path = os.path.join(snapshot_folder, \"GFPGANv1.3.pth\")\n\t        self.face_enhancer = GFPGANer(\n\t            model_path=model_path,\n\t            upscale=scale,\n\t            arch='clean',\n\t            channel_multiplier=2,\n", "            bg_upsampler=upsampler)\n\t    def executeTask(self, image):\n\t        image = np.array(image)\n\t        _, _, output = self.face_enhancer.enhance(image, has_aligned=False, only_center_face=False, paste_back=True)\n\t        return output\n"]}
{"filename": "faceshine/tasks/segmentation/task_zero_background.py", "chunked_list": ["#############################################################################\n\t#\n\t#   Source from:\n\t#   https://github.com/leonelhs/face-makeup.PyTorch\n\t#   Forked from:\n\t#   https://github.com/zllrunning/face-makeup.PyTorch\n\t#   Reimplemented by: Leonel Hernández\n\t#\n\t##############################################################################\n\timport numpy as np\n", "import torch\n\tfrom faceshine import array2image, image_to_tensor\n\tfrom faceshine.tasks import Task\n\tclass TaskZeroBackground(Task):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', weights='DEFAULT')\n\t        self.model.eval()\n\t    def executeTask(self, image):\n\t        image = array2image(image)\n", "        input_tensor = image_to_tensor(image)\n\t        input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n\t        # move the input and model to GPU for speed if available\n\t        if torch.cuda.is_available():\n\t            input_batch = input_batch.to('cuda')\n\t            self.model.to('cuda')\n\t        with torch.no_grad():\n\t            output = self.model(input_batch)['out'][0]\n\t        output_predictions = output.argmax(0)\n\t        # create a binary (black and white) mask of the profile foreground\n", "        mask = output_predictions.byte().cpu().numpy()\n\t        background = np.zeros(mask.shape)\n\t        return np.where(mask, 255, background).astype(np.uint8)\n"]}
{"filename": "faceshine/tasks/segmentation/__init__.py", "chunked_list": ["from .task_zero_background import TaskZeroBackground\n"]}
{"filename": "faceshine/tasks/colorizer/__init__.py", "chunked_list": ["from .task_image_colorizer import TaskImageColorizer\n"]}
{"filename": "faceshine/tasks/colorizer/model_image_colorizer.py", "chunked_list": ["import torch\n\tfrom PIL import Image as PilImage\n\tfrom deoldify.filters import IFilter, BaseFilter\n\tfrom deoldify.visualize import ModelImageVisualizer\n\tfrom fastai.basic_train import Learner\n\tfrom fastai.vision import normalize_funcs\n\tstats = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\tresults_dir = \"./faceshine/tasks/colorizer/results\"\n\tclass ImageFilter(BaseFilter):\n\t    def __init__(self, learn: Learner):\n", "        super().__init__(learn)\n\t        self.render_base = 16\n\t        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\t        self.norm, self.denorm = normalize_funcs(*stats)\n\t    def filter(self, filtered_image: PilImage, render_factor=35) -> PilImage:\n\t        orig_image = filtered_image.copy()\n\t        render_sz = render_factor * self.render_base\n\t        model_image = self._model_process(orig=filtered_image, sz=render_sz)\n\t        raw_color = self._unsquare(model_image, orig_image)\n\t        return raw_color\n", "class ModelImageColorizer(ModelImageVisualizer):\n\t    def __init__(self, filter: IFilter):\n\t        super().__init__(filter, results_dir=results_dir)\n\t    def get_colored_image(self, image, render_factor: int = None) -> PilImage:\n\t        self._clean_mem()\n\t        return self.filter.filter(image, render_factor=render_factor)\n"]}
{"filename": "faceshine/tasks/colorizer/task_image_colorizer.py", "chunked_list": ["#############################################################################\n\t#\n\t#   Source from:\n\t#   https://github.com/jantic/DeOldify\n\t#   Forked from:\n\t#   https://github.com/leonelhs/DeOldify\n\t#   Reimplemented by: Leonel Hernández\n\t#\n\t##############################################################################\n\tfrom pathlib import Path\n", "import numpy as np\n\tfrom deoldify import device\n\tfrom deoldify.device_id import DeviceId\n\tfrom deoldify.generators import gen_inference_deep\n\tfrom huggingface_hub import snapshot_download\n\tfrom faceshine import array2image\n\tfrom faceshine.tasks import Task\n\tfrom .model_image_colorizer import ImageFilter, ModelImageColorizer\n\tdevice.set(device=DeviceId.CPU)\n\tREPO_ID = \"leonelhs/deoldify\"\n", "MODEL_NAME = \"ColorizeArtistic_gen\"\n\tclass TaskImageColorizer(Task):\n\t    def __init__(self):\n\t        super().__init__()\n\t        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n\t        learn = gen_inference_deep(root_folder=Path(snapshot_folder), weights_name=MODEL_NAME)\n\t        image_filter = ImageFilter(learn=learn)\n\t        self.colorizer = ModelImageColorizer(image_filter)\n\t    def executeTask(self, image):\n\t        image = array2image(image)\n", "        image = self.colorizer.get_colored_image(image, render_factor=35)\n\t        return np.array(image)\n"]}
{"filename": "faceshine/tasks/zeroscratches/task_erase_scratches.py", "chunked_list": ["#############################################################################\n\t#\n\t#   Source from:\n\t#   https://github.com/leonelhs/zeroscratches\n\t#   Forked from:\n\t#   https://github.com/leonelhs/zeroscratches\n\t#   Reimplemented by: Leonel Hernández\n\t#\n\t##############################################################################\n\tfrom zeroscratches import EraseScratches\n", "from faceshine import array2image\n\tfrom faceshine.tasks import Task\n\tclass TaskEraseScratches(Task):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.scratchEraser = EraseScratches()\n\t    def executeTask(self, image):\n\t        image = array2image(image)\n\t        return self.scratchEraser.erase(image)\n"]}
{"filename": "faceshine/tasks/zeroscratches/__init__.py", "chunked_list": ["from .task_erase_scratches import TaskEraseScratches\n"]}
{"filename": "faceshine/tasks/base/ai_enhancer.py", "chunked_list": ["import abc\n\timport numpy as np\n\tclass Enhancer(metaclass=abc.ABCMeta):\n\t    @classmethod\n\t    def __subclasshook__(cls, subclass):\n\t        return (hasattr(subclass, 'enhance') and\n\t                callable(subclass.enhance) or\n\t                NotImplemented)\n\t    @abc.abstractmethod\n\t    def enhance(self, image=None, outscale=None) -> np.array:\n", "        return NotImplementedError\n"]}
{"filename": "faceshine/tasks/base/task.py", "chunked_list": ["import abc\n\tclass Task(metaclass=abc.ABCMeta):\n\t    def __init__(self):\n\t        pass\n\t    @classmethod\n\t    def __subclasshook__(cls, subclass):\n\t        return (hasattr(subclass, 'executeTask') and\n\t                callable(subclass.executeTask) or\n\t                NotImplemented)\n\t    @abc.abstractmethod\n", "    def executeTask(self, image):\n\t        raise NotImplementedError\n"]}
{"filename": "faceshine/tasks/base/__init__.py", "chunked_list": ["from .task import Task\n\tfrom .ai_preprocces import Preprocess\n\tfrom .ai_enhancer import Enhancer\n\tfrom .ai_upsampler import Upsampler\n"]}
{"filename": "faceshine/tasks/base/ai_preprocces.py", "chunked_list": ["import abc\n\timport numpy as np\n\tclass Preprocess(metaclass=abc.ABCMeta):\n\t    @classmethod\n\t    def __subclasshook__(cls, subclass):\n\t        return (hasattr(subclass, 'enhance') and\n\t                callable(subclass.enhance) or\n\t                NotImplemented)\n\t    @abc.abstractmethod\n\t    def process(self, image=None) -> np.array:\n", "        return NotImplementedError\n"]}
{"filename": "faceshine/tasks/base/ai_upsampler.py", "chunked_list": ["import abc\n\timport numpy as np\n\tclass Upsampler(metaclass=abc.ABCMeta):\n\t    @classmethod\n\t    def __subclasshook__(cls, subclass):\n\t        return (hasattr(subclass, 'enhance') and\n\t                callable(subclass.enhance) or\n\t                NotImplemented)\n\t    @abc.abstractmethod\n\t    def enhance(self, image=None, outscale=None) -> np.array:\n", "        return NotImplementedError\n"]}
{"filename": "faceshine/tasks/lowlight/__init__.py", "chunked_list": ["from .model import enhance_net_nopool\n\tfrom .task_low_light import TaskLowLight\n"]}
{"filename": "faceshine/tasks/lowlight/task_low_light.py", "chunked_list": ["#############################################################################\n\t#\n\t#   Source from:\n\t#   https://github.com/Li-Chongyi/Zero-DCE/\n\t#   Forked from:\n\t#   https://github.com/Li-Chongyi/Zero-DCE/\n\t#   Reimplemented by: Leonel Hernández\n\t#\n\t##############################################################################\n\timport logging\n", "import os.path\n\timport numpy as np\n\timport torch\n\timport torch.optim\n\tfrom faceshine import array2image, tensor_to_ndarray\n\tfrom faceshine.tasks import Task\n\tfrom faceshine.tasks.lowlight.model import enhance_net_nopool\n\tfrom huggingface_hub import snapshot_download\n\tREPO_ID = \"leonelhs/lowlight\"\n\tMODEL_NAME = \"Epoch99.pth\"\n", "class TaskLowLight(Task):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.model = enhance_net_nopool().cpu()\n\t        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n\t        model_path = os.path.join(snapshot_folder, MODEL_NAME)\n\t        state = torch.load(model_path, map_location=device)\n\t        self.model.load_state_dict(state)\n\t    def executeTask(self, image):\n", "        logging.info(\"Running low light enhancement\")\n\t        image = array2image(image)\n\t        image = (np.asarray(image) / 255.0)\n\t        image = torch.from_numpy(image).float()\n\t        image = image.permute(2, 0, 1)\n\t        image = image.cpu().unsqueeze(0)\n\t        _, enhanced_image, _ = self.model(image)\n\t        return tensor_to_ndarray(enhanced_image, nrow=8, padding=2, normalize=False)\n"]}
{"filename": "faceshine/tasks/lowlight/model/model.py", "chunked_list": ["# TaskLowLight\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass enhance_net_nopool(nn.Module):\n\t    def __init__(self):\n\t        super(enhance_net_nopool, self).__init__()\n\t        self.relu = nn.ReLU(inplace=True)\n\t        number_f = 32\n\t        self.e_conv1 = nn.Conv2d(3, number_f, 3, 1, 1, bias=True)\n", "        self.e_conv2 = nn.Conv2d(number_f, number_f, 3, 1, 1, bias=True)\n\t        self.e_conv3 = nn.Conv2d(number_f, number_f, 3, 1, 1, bias=True)\n\t        self.e_conv4 = nn.Conv2d(number_f, number_f, 3, 1, 1, bias=True)\n\t        self.e_conv5 = nn.Conv2d(number_f * 2, number_f, 3, 1, 1, bias=True)\n\t        self.e_conv6 = nn.Conv2d(number_f * 2, number_f, 3, 1, 1, bias=True)\n\t        self.e_conv7 = nn.Conv2d(number_f * 2, 24, 3, 1, 1, bias=True)\n\t        self.maxpool = nn.MaxPool2d(2, stride=2, return_indices=False, ceil_mode=False)\n\t        self.upsample = nn.UpsamplingBilinear2d(scale_factor=2)\n\t    def forward(self, x):\n\t        x1 = self.relu(self.e_conv1(x))\n", "        # p1 = self.maxpool(x1)\n\t        x2 = self.relu(self.e_conv2(x1))\n\t        # p2 = self.maxpool(x2)\n\t        x3 = self.relu(self.e_conv3(x2))\n\t        # p3 = self.maxpool(x3)\n\t        x4 = self.relu(self.e_conv4(x3))\n\t        x5 = self.relu(self.e_conv5(torch.cat([x3, x4], 1)))\n\t        # x5 = self.upsample(x5)\n\t        x6 = self.relu(self.e_conv6(torch.cat([x2, x5], 1)))\n\t        x_r = F.tanh(self.e_conv7(torch.cat([x1, x6], 1)))\n", "        r1, r2, r3, r4, r5, r6, r7, r8 = torch.split(x_r, 3, dim=1)\n\t        x = x + r1 * (torch.pow(x, 2) - x)\n\t        x = x + r2 * (torch.pow(x, 2) - x)\n\t        x = x + r3 * (torch.pow(x, 2) - x)\n\t        enhance_image_1 = x + r4 * (torch.pow(x, 2) - x)\n\t        x = enhance_image_1 + r5 * (torch.pow(enhance_image_1, 2) - enhance_image_1)\n\t        x = x + r6 * (torch.pow(x, 2) - x)\n\t        x = x + r7 * (torch.pow(x, 2) - x)\n\t        enhance_image = x + r8 * (torch.pow(x, 2) - x)\n\t        r = torch.cat([r1, r2, r3, r4, r5, r6, r7, r8], 1)\n", "        return enhance_image_1, enhance_image, r\n"]}
{"filename": "faceshine/tasks/lowlight/model/dataloader.py", "chunked_list": ["# TaskLowLight\n\timport glob\n\timport random\n\timport test as np\n\timport torch\n\timport torch.utils.data as data\n\tfrom PIL import Image\n\trandom.seed(1143)\n\tdef populate_train_list(lowlight_images_path):\n\t    image_list_lowlight = glob.glob(lowlight_images_path + \"*.jpg\")\n", "    train_list = image_list_lowlight\n\t    random.shuffle(train_list)\n\t    return train_list\n\tclass lowlight_loader(data.Dataset):\n\t    def __init__(self, lowlight_images_path):\n\t        self.train_list = populate_train_list(lowlight_images_path)\n\t        self.size = 256\n\t        self.data_list = self.train_list\n\t        print(\"Total training examples:\", len(self.train_list))\n\t    def __getitem__(self, index):\n", "        data_lowlight_path = self.data_list[index]\n\t        data_lowlight = Image.open(data_lowlight_path)\n\t        data_lowlight = data_lowlight.resize((self.size, self.size), Image.ANTIALIAS)\n\t        data_lowlight = (np.asarray(data_lowlight) / 255.0)\n\t        data_lowlight = torch.from_numpy(data_lowlight).float()\n\t        return data_lowlight.permute(2, 0, 1)\n\t    def __len__(self):\n\t        return len(self.data_list)\n"]}
{"filename": "faceshine/tasks/lowlight/model/__init__.py", "chunked_list": ["# TaskLowLight\n\tfrom .model import enhance_net_nopool\n"]}
{"filename": "faceshine/tasks/faceparser/__init__.py", "chunked_list": ["from .bisnet.model import BiSeNet\n\tfrom .task_face_segmentation import TaskFaceSegmentation\n"]}
{"filename": "faceshine/tasks/faceparser/task_face_segmentation.py", "chunked_list": ["#############################################################################\n\t#\n\t#   Source from:\n\t#   https://github.com/leonelhs/face-makeup.PyTorch\n\t#   Forked from:\n\t#   https://github.com/zllrunning/face-makeup.PyTorch\n\t#   Reimplemented by: Leonel Hernández\n\t#\n\t##############################################################################\n\timport logging\n", "import os.path\n\timport torch\n\tfrom PIL import Image\n\tfrom faceshine.tasks import Task\n\tfrom faceshine import image_to_tensor, array2image\n\tfrom faceshine.tasks.faceparser import BiSeNet\n\tfrom huggingface_hub import snapshot_download\n\tREPO_ID = \"leonelhs/faceparser\"\n\tMODEL_NAME = \"79999_iter.pth\"\n\tclass TaskFaceSegmentation(Task):\n", "    def __init__(self):\n\t        super().__init__()\n\t        self.net = BiSeNet(n_classes=19)\n\t        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t        snapshot_folder = snapshot_download(repo_id=REPO_ID)\n\t        model_path = os.path.join(snapshot_folder, MODEL_NAME)\n\t        self.net.load_state_dict(torch.load(model_path, map_location=device))\n\t        self.net.eval()\n\t    \"\"\"\n\t        Predicts image segments needed to create an alpha blending mask. \n", "        Final mask will be created at client side using this process result\n\t        :param image: Image file (jpg, png, ...)\n\t        :returns: A numpy array.\n\t        \"\"\"\n\t    def executeTask(self, image):\n\t        logging.info(\"Running face segmentation.\")\n\t        with torch.no_grad():\n\t            image = array2image(image)\n\t            image = image.resize((512, 512), Image.BILINEAR)\n\t            input_tensor = image_to_tensor(image)\n", "            input_tensor = torch.unsqueeze(input_tensor, 0)\n\t            if torch.cuda.is_available():\n\t                input_tensor = input_tensor.cuda()\n\t            output = self.net(input_tensor)[0]\n\t            return output.squeeze(0).cpu().numpy().argmax(0)\n"]}
{"filename": "faceshine/tasks/faceparser/bisnet/model.py", "chunked_list": ["#!/usr/bin/python\n\t# -*- encoding: utf-8 -*-\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom .resnet import Resnet18\n\t# from modules.bn import InPlaceABNSync as BatchNorm2d\n\tclass ConvBNReLU(nn.Module):\n\t    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1, *args, **kwargs):\n\t        super(ConvBNReLU, self).__init__()\n", "        self.conv = nn.Conv2d(in_chan,\n\t                out_chan,\n\t                kernel_size = ks,\n\t                stride = stride,\n\t                padding = padding,\n\t                bias = False)\n\t        self.bn = nn.BatchNorm2d(out_chan)\n\t        self.init_weight()\n\t    def forward(self, x):\n\t        x = self.conv(x)\n", "        x = F.relu(self.bn(x))\n\t        return x\n\t    def init_weight(self):\n\t        for ly in self.children():\n\t            if isinstance(ly, nn.Conv2d):\n\t                nn.init.kaiming_normal_(ly.weight, a=1)\n\t                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\tclass BiSeNetOutput(nn.Module):\n\t    def __init__(self, in_chan, mid_chan, n_classes, *args, **kwargs):\n\t        super(BiSeNetOutput, self).__init__()\n", "        self.conv = ConvBNReLU(in_chan, mid_chan, ks=3, stride=1, padding=1)\n\t        self.conv_out = nn.Conv2d(mid_chan, n_classes, kernel_size=1, bias=False)\n\t        self.init_weight()\n\t    def forward(self, x):\n\t        x = self.conv(x)\n\t        x = self.conv_out(x)\n\t        return x\n\t    def init_weight(self):\n\t        for ly in self.children():\n\t            if isinstance(ly, nn.Conv2d):\n", "                nn.init.kaiming_normal_(ly.weight, a=1)\n\t                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\t    def get_params(self):\n\t        wd_params, nowd_params = [], []\n\t        for name, module in self.named_modules():\n\t            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n\t                wd_params.append(module.weight)\n\t                if not module.bias is None:\n\t                    nowd_params.append(module.bias)\n\t            elif isinstance(module, nn.BatchNorm2d):\n", "                nowd_params += list(module.parameters())\n\t        return wd_params, nowd_params\n\tclass AttentionRefinementModule(nn.Module):\n\t    def __init__(self, in_chan, out_chan, *args, **kwargs):\n\t        super(AttentionRefinementModule, self).__init__()\n\t        self.conv = ConvBNReLU(in_chan, out_chan, ks=3, stride=1, padding=1)\n\t        self.conv_atten = nn.Conv2d(out_chan, out_chan, kernel_size= 1, bias=False)\n\t        self.bn_atten = nn.BatchNorm2d(out_chan)\n\t        self.sigmoid_atten = nn.Sigmoid()\n\t        self.init_weight()\n", "    def forward(self, x):\n\t        feat = self.conv(x)\n\t        atten = F.avg_pool2d(feat, feat.size()[2:])\n\t        atten = self.conv_atten(atten)\n\t        atten = self.bn_atten(atten)\n\t        atten = self.sigmoid_atten(atten)\n\t        out = torch.mul(feat, atten)\n\t        return out\n\t    def init_weight(self):\n\t        for ly in self.children():\n", "            if isinstance(ly, nn.Conv2d):\n\t                nn.init.kaiming_normal_(ly.weight, a=1)\n\t                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\tclass ContextPath(nn.Module):\n\t    def __init__(self, *args, **kwargs):\n\t        super(ContextPath, self).__init__()\n\t        self.resnet = Resnet18()\n\t        self.arm16 = AttentionRefinementModule(256, 128)\n\t        self.arm32 = AttentionRefinementModule(512, 128)\n\t        self.conv_head32 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n", "        self.conv_head16 = ConvBNReLU(128, 128, ks=3, stride=1, padding=1)\n\t        self.conv_avg = ConvBNReLU(512, 128, ks=1, stride=1, padding=0)\n\t        self.init_weight()\n\t    def forward(self, x):\n\t        H0, W0 = x.size()[2:]\n\t        feat8, feat16, feat32 = self.resnet(x)\n\t        H8, W8 = feat8.size()[2:]\n\t        H16, W16 = feat16.size()[2:]\n\t        H32, W32 = feat32.size()[2:]\n\t        avg = F.avg_pool2d(feat32, feat32.size()[2:])\n", "        avg = self.conv_avg(avg)\n\t        avg_up = F.interpolate(avg, (H32, W32), mode='nearest')\n\t        feat32_arm = self.arm32(feat32)\n\t        feat32_sum = feat32_arm + avg_up\n\t        feat32_up = F.interpolate(feat32_sum, (H16, W16), mode='nearest')\n\t        feat32_up = self.conv_head32(feat32_up)\n\t        feat16_arm = self.arm16(feat16)\n\t        feat16_sum = feat16_arm + feat32_up\n\t        feat16_up = F.interpolate(feat16_sum, (H8, W8), mode='nearest')\n\t        feat16_up = self.conv_head16(feat16_up)\n", "        return feat8, feat16_up, feat32_up  # x8, x8, x16\n\t    def init_weight(self):\n\t        for ly in self.children():\n\t            if isinstance(ly, nn.Conv2d):\n\t                nn.init.kaiming_normal_(ly.weight, a=1)\n\t                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\t    def get_params(self):\n\t        wd_params, nowd_params = [], []\n\t        for name, module in self.named_modules():\n\t            if isinstance(module, (nn.Linear, nn.Conv2d)):\n", "                wd_params.append(module.weight)\n\t                if not module.bias is None:\n\t                    nowd_params.append(module.bias)\n\t            elif isinstance(module, nn.BatchNorm2d):\n\t                nowd_params += list(module.parameters())\n\t        return wd_params, nowd_params\n\t### This is not used, since I replace this with the resnet feature with the same size\n\tclass SpatialPath(nn.Module):\n\t    def __init__(self, *args, **kwargs):\n\t        super(SpatialPath, self).__init__()\n", "        self.conv1 = ConvBNReLU(3, 64, ks=7, stride=2, padding=3)\n\t        self.conv2 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n\t        self.conv3 = ConvBNReLU(64, 64, ks=3, stride=2, padding=1)\n\t        self.conv_out = ConvBNReLU(64, 128, ks=1, stride=1, padding=0)\n\t        self.init_weight()\n\t    def forward(self, x):\n\t        feat = self.conv1(x)\n\t        feat = self.conv2(feat)\n\t        feat = self.conv3(feat)\n\t        feat = self.conv_out(feat)\n", "        return feat\n\t    def init_weight(self):\n\t        for ly in self.children():\n\t            if isinstance(ly, nn.Conv2d):\n\t                nn.init.kaiming_normal_(ly.weight, a=1)\n\t                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\t    def get_params(self):\n\t        wd_params, nowd_params = [], []\n\t        for name, module in self.named_modules():\n\t            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n", "                wd_params.append(module.weight)\n\t                if not module.bias is None:\n\t                    nowd_params.append(module.bias)\n\t            elif isinstance(module, nn.BatchNorm2d):\n\t                nowd_params += list(module.parameters())\n\t        return wd_params, nowd_params\n\tclass FeatureFusionModule(nn.Module):\n\t    def __init__(self, in_chan, out_chan, *args, **kwargs):\n\t        super(FeatureFusionModule, self).__init__()\n\t        self.convblk = ConvBNReLU(in_chan, out_chan, ks=1, stride=1, padding=0)\n", "        self.conv1 = nn.Conv2d(out_chan,\n\t                out_chan//4,\n\t                kernel_size = 1,\n\t                stride = 1,\n\t                padding = 0,\n\t                bias = False)\n\t        self.conv2 = nn.Conv2d(out_chan//4,\n\t                out_chan,\n\t                kernel_size = 1,\n\t                stride = 1,\n", "                padding = 0,\n\t                bias = False)\n\t        self.relu = nn.ReLU(inplace=True)\n\t        self.sigmoid = nn.Sigmoid()\n\t        self.init_weight()\n\t    def forward(self, fsp, fcp):\n\t        fcat = torch.cat([fsp, fcp], dim=1)\n\t        feat = self.convblk(fcat)\n\t        atten = F.avg_pool2d(feat, feat.size()[2:])\n\t        atten = self.conv1(atten)\n", "        atten = self.relu(atten)\n\t        atten = self.conv2(atten)\n\t        atten = self.sigmoid(atten)\n\t        feat_atten = torch.mul(feat, atten)\n\t        feat_out = feat_atten + feat\n\t        return feat_out\n\t    def init_weight(self):\n\t        for ly in self.children():\n\t            if isinstance(ly, nn.Conv2d):\n\t                nn.init.kaiming_normal_(ly.weight, a=1)\n", "                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\t    def get_params(self):\n\t        wd_params, nowd_params = [], []\n\t        for name, module in self.named_modules():\n\t            if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n\t                wd_params.append(module.weight)\n\t                if not module.bias is None:\n\t                    nowd_params.append(module.bias)\n\t            elif isinstance(module, nn.BatchNorm2d):\n\t                nowd_params += list(module.parameters())\n", "        return wd_params, nowd_params\n\tclass BiSeNet(nn.Module):\n\t    def __init__(self, n_classes, *args, **kwargs):\n\t        super(BiSeNet, self).__init__()\n\t        self.cp = ContextPath()\n\t        ## here self.sp is deleted\n\t        self.ffm = FeatureFusionModule(256, 256)\n\t        self.conv_out = BiSeNetOutput(256, 256, n_classes)\n\t        self.conv_out16 = BiSeNetOutput(128, 64, n_classes)\n\t        self.conv_out32 = BiSeNetOutput(128, 64, n_classes)\n", "        self.init_weight()\n\t    def forward(self, x):\n\t        H, W = x.size()[2:]\n\t        feat_res8, feat_cp8, feat_cp16 = self.cp(x)  # here return res3b1 feature\n\t        feat_sp = feat_res8  # use res3b1 feature to replace spatial path feature\n\t        feat_fuse = self.ffm(feat_sp, feat_cp8)\n\t        feat_out = self.conv_out(feat_fuse)\n\t        feat_out16 = self.conv_out16(feat_cp8)\n\t        feat_out32 = self.conv_out32(feat_cp16)\n\t        feat_out = F.interpolate(feat_out, (H, W), mode='bilinear', align_corners=True)\n", "        feat_out16 = F.interpolate(feat_out16, (H, W), mode='bilinear', align_corners=True)\n\t        feat_out32 = F.interpolate(feat_out32, (H, W), mode='bilinear', align_corners=True)\n\t        return feat_out, feat_out16, feat_out32\n\t    def init_weight(self):\n\t        for ly in self.children():\n\t            if isinstance(ly, nn.Conv2d):\n\t                nn.init.kaiming_normal_(ly.weight, a=1)\n\t                if not ly.bias is None: nn.init.constant_(ly.bias, 0)\n\t    def get_params(self):\n\t        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n", "        for name, child in self.named_children():\n\t            child_wd_params, child_nowd_params = child.get_params()\n\t            if isinstance(child, FeatureFusionModule) or isinstance(child, BiSeNetOutput):\n\t                lr_mul_wd_params += child_wd_params\n\t                lr_mul_nowd_params += child_nowd_params\n\t            else:\n\t                wd_params += child_wd_params\n\t                nowd_params += child_nowd_params\n\t        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n\tif __name__ == \"__main__\":\n", "    net = BiSeNet(19)\n\t    net.cuda()\n\t    net.eval()\n\t    in_ten = torch.randn(16, 3, 640, 480).cuda()\n\t    out, out16, out32 = net(in_ten)\n\t    print(out.shape)\n\t    net.get_params()\n"]}
{"filename": "faceshine/tasks/faceparser/bisnet/__init__.py", "chunked_list": []}
{"filename": "faceshine/tasks/faceparser/bisnet/resnet.py", "chunked_list": ["#!/usr/bin/python\n\t# -*- encoding: utf-8 -*-\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torch.utils.model_zoo as modelzoo\n\t# from modules.bn import InPlaceABNSync as BatchNorm2d\n\tresnet18_url = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n\tdef conv3x3(in_planes, out_planes, stride=1):\n\t    \"\"\"3x3 convolution with padding\"\"\"\n", "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n\t                     padding=1, bias=False)\n\tclass BasicBlock(nn.Module):\n\t    def __init__(self, in_chan, out_chan, stride=1):\n\t        super(BasicBlock, self).__init__()\n\t        self.conv1 = conv3x3(in_chan, out_chan, stride)\n\t        self.bn1 = nn.BatchNorm2d(out_chan)\n\t        self.conv2 = conv3x3(out_chan, out_chan)\n\t        self.bn2 = nn.BatchNorm2d(out_chan)\n\t        self.relu = nn.ReLU(inplace=True)\n", "        self.downsample = None\n\t        if in_chan != out_chan or stride != 1:\n\t            self.downsample = nn.Sequential(\n\t                nn.Conv2d(in_chan, out_chan,\n\t                          kernel_size=1, stride=stride, bias=False),\n\t                nn.BatchNorm2d(out_chan),\n\t            )\n\t    def forward(self, x):\n\t        residual = self.conv1(x)\n\t        residual = F.relu(self.bn1(residual))\n", "        residual = self.conv2(residual)\n\t        residual = self.bn2(residual)\n\t        shortcut = x\n\t        if self.downsample is not None:\n\t            shortcut = self.downsample(x)\n\t        out = shortcut + residual\n\t        out = self.relu(out)\n\t        return out\n\tdef create_layer_basic(in_chan, out_chan, bnum, stride=1):\n\t    layers = [BasicBlock(in_chan, out_chan, stride=stride)]\n", "    for i in range(bnum - 1):\n\t        layers.append(BasicBlock(out_chan, out_chan, stride=1))\n\t    return nn.Sequential(*layers)\n\tclass Resnet18(nn.Module):\n\t    def __init__(self):\n\t        super(Resnet18, self).__init__()\n\t        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n\t                               bias=False)\n\t        self.bn1 = nn.BatchNorm2d(64)\n\t        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n", "        self.layer1 = create_layer_basic(64, 64, bnum=2, stride=1)\n\t        self.layer2 = create_layer_basic(64, 128, bnum=2, stride=2)\n\t        self.layer3 = create_layer_basic(128, 256, bnum=2, stride=2)\n\t        self.layer4 = create_layer_basic(256, 512, bnum=2, stride=2)\n\t        self.init_weight()\n\t    def forward(self, x):\n\t        x = self.conv1(x)\n\t        x = F.relu(self.bn1(x))\n\t        x = self.maxpool(x)\n\t        x = self.layer1(x)\n", "        feat8 = self.layer2(x)  # 1/8\n\t        feat16 = self.layer3(feat8)  # 1/16\n\t        feat32 = self.layer4(feat16)  # 1/32\n\t        return feat8, feat16, feat32\n\t    def init_weight(self):\n\t        state_dict = modelzoo.load_url(resnet18_url)\n\t        self_state_dict = self.state_dict()\n\t        for k, v in state_dict.items():\n\t            if 'fc' in k: continue\n\t            self_state_dict.update({k: v})\n", "        self.load_state_dict(self_state_dict)\n\t    def get_params(self):\n\t        wd_params, nowd_params = [], []\n\t        for name, module in self.named_modules():\n\t            if isinstance(module, (nn.Linear, nn.Conv2d)):\n\t                wd_params.append(module.weight)\n\t                if not module.bias is None:\n\t                    nowd_params.append(module.bias)\n\t            elif isinstance(module, nn.BatchNorm2d):\n\t                nowd_params += list(module.parameters())\n", "        return wd_params, nowd_params\n\tif __name__ == \"__main__\":\n\t    net = Resnet18()\n\t    x = torch.randn(16, 3, 224, 224)\n\t    out = net(x)\n\t    print(out[0].size())\n\t    print(out[1].size())\n\t    print(out[2].size())\n\t    net.get_params()\n"]}
