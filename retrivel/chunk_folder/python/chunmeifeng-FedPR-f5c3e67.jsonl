{"filename": "maskgen.py", "chunked_list": ["'''Create sampling patterns for Cartesian k-space trajectories.'''\n\timport numpy as np\n\tdef cartesian_pe(shape, undersample=.5, reflines=20):\n\t    '''Randomly collect Cartesian phase encodes (lines).\n\t    Parameters\n\t    ----------\n\t    shape : tuple\n\t        Shape of the image to be sampled.\n\t    undersample : float, optional\n\t        Undersampling factor (0 < undersample <= 1).\n", "    reflines : int, optional\n\t        Number of lines in the center to collect regardless.\n\t    Returns\n\t    -------\n\t    mask : array_like\n\t        Boolean mask of sample locations on Cartesian grid.\n\t    Raises\n\t    ------\n\t    AssertionError\n\t        If undersample factor is outside of interval (0, 1].\n", "    '''\n\t    assert 0 < undersample <= 1, (\n\t        'Undersampling factor must be in (0,1]!')\n\t    M, _N = shape[:]\n\t    k = int(undersample*M)\n\t    idx = np.random.permutation(M)[:k]\n\t    mask = np.zeros(shape)*False\n\t    mask[idx, :] = True\n\t    # Make sure we grab center of kspace regardless\n\t    mask[int(M/2-reflines/2):int(M/2+reflines/2), :] = True\n", "    return mask\n\tdef cartesian_gaussian(shape, undersample=(.5, .5), reflines=20):\n\t    '''Undersample in Gaussian pattern.\n\t    Parameters\n\t    ----------\n\t    shape : tuple\n\t        Shape of the image to be sampled.\n\t    undersample : tuple, optional\n\t        Undersampling factor in x and y (0 < ux, uy <= 1).\n\t    reflines : int, optional\n", "        Number of lines in the center to collect regardless.\n\t    Returns\n\t    -------\n\t    mask : array_like\n\t        Boolean mask of sample locations on Cartesian grid.\n\t    Raises\n\t    ------\n\t    AssertionError\n\t        If undersample factors are outside of interval (0, 1].\n\t    '''\n", "    assert 0 < undersample[0] <= 1 and 0 < undersample[1] <= 1, \\\n\t        'Undersampling factor must be in (0,1]!'\n\t    M, N = shape[:]\n\t    km = int(undersample[0]*M)\n\t    kn = int(undersample[1]*N)\n\t    mask = np.zeros(N*M).astype(bool)\n\t    idx = np.arange(mask.size)\n\t    np.random.shuffle(idx)\n\t    mask[idx[:km*kn]] = True\n\t    mask = mask.reshape(shape)\n", "    # Make sure we grab the reference lines in center of kspace\n\t    mask[int(M/2-reflines/2):int(M/2+reflines/2), :] = True\n\t    return mask\n\timport matplotlib.pyplot as plt\n\tdef imshow(img, title=\"\"):\n\t    \"\"\" Show image as grayscale.\n\t    imshow(np.linalg.norm(coilImages, axis=0))\n\t    \"\"\"\n\t    if img.dtype == np.complex64 or img.dtype == np.complex128:\n\t        print('img is complex! Take absolute value.')\n", "        img = np.abs(img)\n\t    plt.figure()\n\t    plt.imshow(img, cmap='gray', interpolation='nearest')\n\t    plt.axis('off')\n\t    plt.title(title)\n\t    #plt.show()\n\t    plt.savefig('mask_show.jpg')\n\tif __name__ == \"__main__\":\n\t    import scipy.io as sio\n\t    Accrate=5\n", "    H=256\n\t    W=256\n\t    undersample=1/Accrate\n\t    a=cartesian_pe((W,H),undersample=undersample, reflines=20)\n\t    sio.savemat('1D-Cartesian_{}X_{}{}.mat'.format(Accrate,H,W),{'mask':np.rot90(a)})\n"]}
{"filename": "train.py", "chunked_list": ["import os\n\timport shutil\n\timport torch\n\timport random\n\timport copy\n\timport argparse\n\timport logging\n\timport time\n\timport datetime\n\timport numpy as np\n", "from collections import defaultdict\n\timport warnings\n\twarnings.filterwarnings(action='ignore')\n\tfrom tensorboardX import SummaryWriter\n\tfrom models.loss import Criterion\n\tfrom pathlib import Path\n\tfrom engine import train_one_epoch_null_nohead, server_evaluate\n\tfrom util.misc import get_rank\n\tfrom data import build_different_dataloader, build_server_dataloader\n\tfrom config import build_config\n", "# from models.model_config import get_cfg\n\tfrom util.adam_svd import AdamSVD\n\tfrom models.vit_models import Swin\n\tdef average_model(server_model, client_models, sampled_client_indices, coefficients):\n\t    \"\"\"Average the updated and transmitted parameters from each selected client.\"\"\"\n\t    averaged_weights = {}\n\t    for k, v in client_models[0].state_dict().items():\n\t        if 'prompter' in k or 'running' in k:\n\t            averaged_weights[k] = torch.zeros_like(v.data)\n\t    for it, idx in enumerate(sampled_client_indices):\n", "        for k, v in client_models[idx].state_dict().items():\n\t            if k in averaged_weights.keys():\n\t                averaged_weights[k] += coefficients[it] * v.data\n\t    for k, v in server_model.state_dict().items():\n\t        if k in averaged_weights.keys():\n\t            v.data.copy_(averaged_weights[k].data.clone())\n\t    for client_idx in np.arange(len(client_models)):\n\t        for key, param in averaged_weights.items():\n\t            if 'prompter' in key:\n\t                client_models[client_idx].state_dict()[key].data.copy_(param)\n", "    return server_model, client_models\n\tdef create_all_model(cfg):\n\t    device = torch.device(cfg.SOLVER.DEVICE)\n\t    server_model = Swin(cfg).to(device)\n\t    checkpoint = torch.load(cfg.PRETRAINED_FASTMRI_CKPT, map_location='cpu')\n\t    state_dict = checkpoint['server_model']\n\t    server_model.load_state_dict(state_dict, strict=False)\n\t    for k, v in server_model.head.named_parameters():\n\t        v.requires_grad = False\n\t    models = [copy.deepcopy(server_model) for idx in range(cfg.FL.CLIENTS_NUM)]\n", "    return server_model, models\n\tdef make_logger(dirname):\n\t    logger = logging.getLogger('FedMRI_log')\n\t    logger.propagate = False\n\t    logger.setLevel(logging.INFO)\n\t    fmt = logging.Formatter(fmt='%(asctime)s %(filename)s [lineno: %(lineno)d] %(message)s')\n\t    filename = '{}/log.txt'.format(dirname)\n\t    fh = logging.FileHandler(filename=filename)\n\t    fh.setLevel(logging.INFO)\n\t    fh.setFormatter(fmt=fmt)\n", "    sh = logging.StreamHandler()\n\t    sh.setFormatter(fmt=fmt)\n\t    logger.addHandler(hdlr=fh)\n\t    logger.addHandler(hdlr=sh)\n\t    return logger\n\tdef main(cfg):\n\t    outputdir = os.path.join(cfg.OUTPUTDIR, cfg.FL.MODEL_NAME, cfg.DISTRIBUTION_TYPE)\n\t    experiments_num = max([int(k.split('_')[0]) + 1 for k in os.listdir(outputdir)]) if os.path.exists(outputdir) and not len(os.listdir(outputdir)) == 0 else 0\n\t    outputdir = os.path.join(outputdir, f'{experiments_num:02d}_' + time.strftime('%y-%m-%d_%H-%M') + f'local{cfg.TRAIN.LOCAL_EPOCHS}')\n\t    if outputdir:\n", "        os.makedirs(outputdir, exist_ok=True)\n\t    ckpt_root = Path(outputdir) / 'ckpt'\n\t    ckpt_root.mkdir(parents=True, exist_ok=True)\n\t    writer = SummaryWriter(os.path.join(outputdir, 'tensorboard'))\n\t    logger = make_logger(outputdir)\n\t    logger.info(logger.handlers[0].baseFilename)\n\t    logger.info('New job assigned {}'.format(datetime.datetime.now().strftime('%Y-%m-%d-%H:%M')))\n\t    logger.info('\\nconfig:\\n{}\\n'.format(cfg))\n\t    logger.info('=======' * 5 + '\\n')\n\t    server_model, models = create_all_model(cfg)\n", "    criterion = Criterion()\n\t    start_epoch = 0\n\t    seed = cfg.SEED + get_rank()\n\t    torch.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    device = torch.device(cfg.SOLVER.DEVICE)\n\t    n_parameters = sum(p.numel() for p in server_model.parameters() if p.requires_grad)\n\t    logger.info('TOTAL Trainable Params:  {:.2f} M'.format(n_parameters / 1000 / 1000))\n\t    dataloader_train, lens_train = build_different_dataloader(cfg, mode='train')\n", "    if cfg.DISTRIBUTION_TYPE == 'in-distribution':\n\t        dataloader_val, lens_val = build_different_dataloader(cfg, mode='val')\n\t    elif cfg.DISTRIBUTION_TYPE == 'out-of-distribution':\n\t        dataloader_val, lens_val = build_server_dataloader(cfg, mode='val')\n\t    else:\n\t        raise ValueError(\"cfg.DISTRIBUTION_TYPE should be in ['in-distribution', 'out-of-distribution']\")\n\t    logger.info(f'train dataset:{lens_train}')\n\t    logger.info(f'val   dataset:{lens_val}')\n\t    # build optimizer\n\t    trainable_prompt = []\n", "    for idx in range(len(models)):\n\t        m_param = [v for k, v in models[idx].enc.prompter.named_parameters() if v.requires_grad]\n\t        trainable_prompt.append(m_param)\n\t    optimizers = [AdamSVD(trainable_prompt[idx], lr=cfg.SOLVER.LR[idx], weight_decay=cfg.SOLVER.WEIGHT_DECAY, ratio=cfg.SOLVER.RATIO) for idx in range(cfg.FL.CLIENTS_NUM)]\n\t    # milestone = [30, ]\n\t    # lr_schedulers = [torch.optim.lr_scheduler.MultiStepLR(optimizers[idx], milestones=milestone, gamma=cfg.SOLVER.LR_GAMMA) for idx in range(cfg.FL.CLIENTS_NUM)]\n\t    cfg.RESUME = ''\n\t    if cfg.RESUME != '':\n\t        checkpoint = torch.load(cfg.RESUME, device)\n\t        server_model.load_state_dict(checkpoint['server_model'], strict=True)\n", "        for idx, client_name in enumerate(cfg.DATASET.CLIENTS):\n\t            models[idx].load_state_dict(checkpoint['server_model'])\n\t    start_time = time.time()\n\t    server_best_status = {'NMSE': 10000000, 'PSNR': 0, 'SSIM': 0, 'bestround': 0}\n\t    for com_round in range(start_epoch, cfg.TRAIN.EPOCHS):\n\t        logger.info('---------------- com_round {:<3d}/{:<3d}----------------'.format(com_round, cfg.TRAIN.EPOCHS))\n\t        sampled_client_indices = np.random.choice(a=range(cfg.FL.CLIENTS_NUM), size=cfg.meta_client_num, replace=False).tolist()\n\t        logger.info(f\"sampled clients: {sampled_client_indices}\")\n\t        for idx, client_idx in enumerate(sampled_client_indices):\n\t            for _ in range(cfg.TRAIN.LOCAL_EPOCHS):\n", "                train_one_epoch_null_nohead(model=models[client_idx], criterion=criterion, data_loader=dataloader_train[client_idx],\n\t                                            optimizer=optimizers[client_idx], device=device)\n\t            # #lr_schedulers[client_idx].step()\n\t        logger.info(f\"[Round: {str(com_round).zfill(4)}] Aggregate updated weights ...!\")\n\t        # calculate averaging coefficient of weights\n\t        selected_total_size = sum([lens_train[idx] for idx in sampled_client_indices])\n\t        mixing_coefficients = [lens_train[idx] / selected_total_size for idx in sampled_client_indices]\n\t        # Aggregation\n\t        server_model, models = average_model(server_model, models, sampled_client_indices, mixing_coefficients)\n\t        fea_in = defaultdict(dict)\n", "        for idx, (k, p) in enumerate(server_model.enc.prompter.named_parameters()):\n\t            fea_in[idx] = torch.bmm(p.transpose(1, 2), p)\n\t        for idx in sampled_client_indices:\n\t            optimizers[idx].get_eigens(fea_in=fea_in)\n\t            optimizers[idx].get_transforms()\n\t        # server evaluate\n\t        eval_status = server_evaluate(server_model, criterion, dataloader_val, device)\n\t        logger.info(f'**** Current_round: {com_round:03d}  server PSNR: {eval_status[\"PSNR\"]:.3f}  SSIM: {eval_status[\"SSIM\"]:.3f} '\n\t                    f'NMSE: {eval_status[\"NMSE\"]:.3f} val_loss: {eval_status[\"loss\"]:.3f}')\n\t        writer.add_scalar(tag='server psnr', scalar_value=eval_status[\"PSNR\"], global_step=com_round)\n", "        writer.add_scalar(tag='server ssim', scalar_value=eval_status[\"SSIM\"], global_step=com_round)\n\t        writer.add_scalar(tag='server loss', scalar_value=eval_status[\"loss\"], global_step=com_round)\n\t        if eval_status['PSNR'] > server_best_status['PSNR']:\n\t            server_best_status.update(eval_status)\n\t            server_best_status.update({'bestround': com_round})\n\t            server_best_checkpoint = {\n\t                'server_model': server_model.state_dict(),\n\t                'bestround': com_round,\n\t                'args': cfg,\n\t            }\n", "            if not os.path.exists(ckpt_root):\n\t                ckpt_root = Path(outputdir) / 'ckpt'\n\t                ckpt_root.mkdir(parents=True, exist_ok=True)\n\t            checkpoint_path = os.path.join(ckpt_root, f'checkpoint-epoch_{(com_round):04}.pth')\n\t            torch.save(server_best_checkpoint, checkpoint_path)\n\t        logger.info(f'********* Best_round: {server_best_status[\"bestround\"]}  '\n\t                    f'SERVER PSNR: {server_best_status[\"PSNR\"]:.3f} '\n\t                    f'SSIM: {server_best_status[\"SSIM\"]:.3f} '\n\t                    f'NMSE: {server_best_status[\"NMSE\"]:.3f} ')\n\t        logger.info('*************' * 5 + '\\n')\n", "    # log the best score!\n\t    logger.info(\"Best Results ----------\")\n\t    logger.info('The best round for Server  is {}'.format(server_best_status['bestround']))\n\t    logger.info(\"PSNR: {:.4f}\".format(server_best_status['PSNR']))\n\t    logger.info(\"NMSE: {:.4f}\".format(server_best_status['NMSE']))\n\t    logger.info(\"SSIM: {:.4f}\".format(server_best_status['SSIM']))\n\t    logger.info(\"------------------\")\n\t    checkpoint_final_path = os.path.join(ckpt_root, 'best.pth')\n\t    shutil.copy(checkpoint_path, checkpoint_final_path)\n\t    total_time = time.time() - start_time\n", "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t    logger.info('Training time {}'.format(total_time_str))\n\t    logger.info(logger.handlers[0].baseFilename)\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser(description=\"a unit Cross Multi modity transformer\")\n\t    parser.add_argument(\n\t        \"--config\", default=\"different_dataset\", help=\"choose a experiment to do\")\n\t    args = parser.parse_args()\n\t    cfg = build_config(args.config)\n\t    main(cfg)\n", "    print('OK!')\n"]}
{"filename": "engine.py", "chunked_list": ["import os\n\timport time\n\timport hashlib\n\tfrom typing import Iterable\n\timport imageio\n\timport util.misc as utils\n\timport datetime\n\timport numpy as np\n\timport matplotlib.pyplot as plt\n\tfrom util.metric import nmse, psnr, ssim, AverageMeter\n", "from collections import defaultdict\n\timport torch\n\timport torch.nn.functional as F\n\tdef train_one_epoch_null_nohead(model, criterion,data_loader, optimizer, device):\n\t    model.train()\n\t    loss_all = 0\n\t    count=0\n\t    for _, data in enumerate(data_loader):\n\t        count+=1\n\t        image, target, mean, std, fname, slice_num = data  # NOTE\n", "        image = image.unsqueeze(1)  # (8,1,320,320)\n\t        target = target.unsqueeze(1)\n\t        image = image.to(device)\n\t        target = target.to(device)\n\t        outputs = model(image)\n\t        loss = criterion(outputs, target)\n\t        optimizer.zero_grad()\n\t        loss['loss'].backward()\n\t        optimizer.step()\n\t        loss_all += loss['loss'].item()\n", "    loss_avg = loss_all / len(data_loader)\n\t    global_step = count\n\t    return {\"loss\": loss_avg, \"global_step\": global_step}\n\t@torch.no_grad()\n\tdef server_evaluate(model, criterion, data_loaders, device):\n\t    model.eval()\n\t    criterion.eval()\n\t    criterion.to(device)\n\t    nmse_meter = AverageMeter()\n\t    psnr_meter = AverageMeter()\n", "    ssim_meter = AverageMeter()\n\t    output_dic = defaultdict(dict)\n\t    target_dic = defaultdict(dict)\n\t    start_time = time.time()\n\t    loss_all = 0\n\t    count = 0\n\t    for idx, data_loader in enumerate(data_loaders):\n\t        for i, data in enumerate(data_loader):\n\t            count += 1\n\t            image, target, mean, std, fname, slice_num = data\n", "            image = image.unsqueeze(1)  # (8,1,320,320)\n\t            image = image.to(device)\n\t            target = target.to(device)\n\t            mean = mean.unsqueeze(1).unsqueeze(2)\n\t            std = std.unsqueeze(1).unsqueeze(2)\n\t            mean = mean.to(device)\n\t            std = std.to(device)\n\t            outputs = model(image)\n\t            outputs = outputs.squeeze(1)\n\t            outputs = outputs * std + mean\n", "            target = target * std + mean\n\t            loss = criterion(outputs, target)\n\t            loss_all += loss['loss'].item()\n\t            for k, f in enumerate(fname):\n\t                output_dic[f][slice_num[k].item()] = outputs[k]\n\t                target_dic[f][slice_num[k].item()] = target[k]\n\t        for name in output_dic.keys():\n\t            f_output = torch.stack([v for _, v in output_dic[name].items()])  # (34,320,320)\n\t            f_target = torch.stack([v for _, v in target_dic[name].items()])  # (34,320,320)\n\t            our_nmse = nmse(f_target.cpu().numpy(), f_output.cpu().numpy())\n", "            our_psnr = psnr(f_target.cpu().numpy(), f_output.cpu().numpy())\n\t            our_ssim = ssim(f_target.cpu().numpy(), f_output.cpu().numpy())\n\t            nmse_meter.update(our_nmse, 1)\n\t            psnr_meter.update(our_psnr, 1)\n\t            ssim_meter.update(our_ssim, 1)\n\t    total_time = time.time() - start_time\n\t    total_time = str(datetime.timedelta(seconds=int(total_time)))\n\t    loss_avg = loss_all / count\n\t    return {'total_time': total_time, 'loss': loss_avg, 'PSNR': psnr_meter.avg, 'SSIM': ssim_meter.avg, 'NMSE': nmse_meter.avg}\n"]}
{"filename": "convertnii2mat.py", "chunked_list": ["import h5py\n\timport os\n\timport scipy.io as sio\n\tfrom os.path import splitext\n\tfrom tqdm import tqdm\n\timport argparse\n\timport nibabel as nib\n\tdef nib_load(file_name):\n\t    if not os.path.exists(file_name):\n\t        print('Invalid file name, can not find the file!')\n", "    proxy = nib.load(file_name)\n\t    data = proxy.get_fdata()\n\t    proxy.uncache()\n\t    return data\n\tdef convert(niipath, matpath):\n\t    niifiles = os.listdir(niipath)\n\t    os.makedirs(matpath, exist_ok=True)\n\t    for nif in tqdm(niifiles):\n\t        nifile = os.path.join(niipath, nif)\n\t        fname = nif.split('.')[0]\n", "        f = nib_load(nifile)\n\t        slices = f.shape[2]\n\t        for slice in range(slices):\n\t            img = f[..., slice]\n\t            matfile = os.path.join(matpath, fname + '-{:03d}.mat'.format(slice))\n\t            sio.savemat(matfile, {'img':img})\n\tdef main(args):\n\t    os.makedirs(args.dst_root, exist_ok=True)\n\t    convert(args.src_root, args.dst_root)\n\tif __name__ == '__main__':\n", "    parser = argparse.ArgumentParser(description=\"convert h5 to mat\")\n\t    parser.add_argument(\n\t        \"--src_root\", default=\"\", help=\"choose a experiment to do\")\n\t    parser.add_argument(\n\t        \"--dst_root\", default=\"\", help=\"choose a experiment to do\")\n\t    args = parser.parse_args()\n\t    main(args)"]}
{"filename": "preprocess_dicom.py", "chunked_list": ["import pydicom\n\timport os\n\timport json\n\tfrom scipy.io import savemat\n\timport numpy as np\n\tfrom tqdm import tqdm\n\timport glob\n\tfrom collections import defaultdict\n\timport scipy.io as sio\n\timport nibabel as nib\n", "root = os.path.join(os.path.expanduser('~'), r'data/AAA-Download-Datasets/')\n\tdef make_mat():\n\t    sequence = 'T1'\n\t    src_root = os.path.join(root, f'Fed/Data/IXI-NIFTI/{sequence}')\n\t    dst_root = os.path.join(root, f'Fed/Data/IXI-NIFTI/{sequence}_mat')\n\t    if not os.path.exists(dst_root):\n\t        os.makedirs(dst_root, exist_ok=True)\n\t    names = sorted(os.listdir(src_root))\n\t    for name in tqdm(names):\n\t        niipath = os.path.join(src_root, name)\n", "        data = nib.load(niipath)\n\t        print(f'data orientation {nib.aff2axcodes(data.affine)}')\n\t        img = data.get_fdata().astype(np.float32)\n\t        for slice in range(img.shape[1]):\n\t            matpath = os.path.join(dst_root, name.split('.')[0]+f'-{slice:03d}.mat')\n\t            sio.savemat(file_name=matpath, mdict={'img' : img[:,  slice]})\n\tdef ixi_txt():\n\t    dst_sub = 'Fed/Data/IXI-NIFTI'\n\t    path = os.path.join(root, dst_sub)\n\t    for client in ['HH', 'Guys', 'IOP']:\n", "        lines1 = glob.glob(os.path.join(path, f'T1/*{client}*'))\n\t        lines2 = glob.glob(os.path.join(path, f'T2/*{client}*'))\n\t        names1 = [os.path.basename(l1).split('-T')[0] for l1 in lines1]\n\t        names2 = [os.path.basename(l1).split('-T')[0] for l1 in lines2]\n\t        names = [n for n in names1 if n in names2]\n\t        print(f'there are {len(names)}  {client} intersect files')\n\t        split = int(len(names) * 0.7)\n\t        p1 = os.path.join(dst_sub, 'T1')\n\t        p2 = os.path.join(dst_sub, 'T2')\n\t        with open(os.path.join(path, f'{client}_train.txt'), 'w') as f:\n", "            for name in names[:split]:\n\t                print(os.path.join(p1, name+'-T1.nii.gz') + '    ' + os.path.join(p2, name+'-T2.nii.gz'), file=f)\n\t        with open(os.path.join(path, f'{client}_val.txt'), 'w') as f:\n\t            for name in names[split:]:\n\t                print(os.path.join(p1, name+'-T1.nii.gz') + '    ' + os.path.join(p2, name+'-T2.nii.gz'), file=f)\n\tif __name__ == '__main__':\n\t    # make_mat()\n\t    # ixi_txt()\n\t    print('ok')\n"]}
{"filename": "test.py", "chunked_list": ["import os\n\timport time\n\timport datetime\n\timport random\n\timport numpy as np\n\timport argparse\n\timport h5py\n\timport torch\n\timport logging\n\timport scipy.io as sio\n", "from collections import defaultdict\n\tfrom tqdm import tqdm\n\timport time\n\tfrom config import build_config\n\tfrom data import build_different_dataloader, build_server_dataloader\n\tfrom models.vit_models import Swin\n\tfrom models.loss import Criterion\n\tfrom engine import server_evaluate\n\tfrom util.metric import nmse, psnr, ssim, AverageMeter\n\tdef save_reconstructions(reconstructions, out_dir):\n", "    \"\"\"\n\t    Save reconstruction images.\n\t    This function writes to h5 files that are appropriate for submission to the\n\t    leaderboard.\n\t    Args:\n\t        reconstructions (dict[str, np.array]): A dictionary mapping input\n\t            filenames to corresponding reconstructions (of shape num_slices x\n\t            height x width).\n\t        out_dir (pathlib.Path): Path to the output directory where the\n\t            reconstructions should be saved.\n", "    \"\"\"\n\t    os.makedirs(str(out_dir), exist_ok=True)\n\t    print(out_dir, len(list(reconstructions.keys())))\n\t    idx = min(len(list(reconstructions.keys())), 10)\n\t    for fname in list(reconstructions.keys())[-idx:]:\n\t        f_output = torch.stack([v for _, v in reconstructions[fname].items()])\n\t        basename = fname.split('/')[-1]\n\t        with h5py.File(str(out_dir) + '/' + str(basename) + '.hdf5', \"w\") as f:\n\t            print(fname)\n\t            f.create_dataset(\"reconstruction\", data=f_output.cpu())\n", "def create_all_model(args):\n\t    device = torch.device('cpu')\n\t    server_model = Swin(args).to(device)\n\t    return server_model\n\t@torch.no_grad()\n\tdef server_evaluate(model, criterion, data_loaders, device):\n\t    model.eval()\n\t    criterion.eval()\n\t    criterion.to(device)\n\t    nmse_meter = AverageMeter()\n", "    psnr_meter = AverageMeter()\n\t    ssim_meter = AverageMeter()\n\t    output_dic = defaultdict(dict)\n\t    target_dic = defaultdict(dict)\n\t    start_time = time.time()\n\t    loss_all = 0\n\t    count = 0\n\t    for idx, data_loader in enumerate(data_loaders):\n\t        for i, data in enumerate(data_loader):\n\t            count += 1\n", "            image, target, mean, std, fname, slice_num = data\n\t            image = image.unsqueeze(1)\n\t            image = image.to(device)\n\t            target = target.to(device)\n\t            mean = mean.unsqueeze(1).unsqueeze(2)\n\t            std = std.unsqueeze(1).unsqueeze(2)\n\t            mean = mean.to(device)\n\t            std = std.to(device)\n\t            outputs = model(image)\n\t            outputs = outputs.squeeze(1)\n", "            outputs = outputs * std + mean\n\t            target = target * std + mean\n\t            loss = criterion(outputs, target)\n\t            loss_all += loss['loss'].item()\n\t            for i, f in enumerate(fname):\n\t                output_dic[f][slice_num[i].item()] = outputs[i]\n\t                target_dic[f][slice_num[i].item()] = target[i]\n\t        for name in output_dic.keys():\n\t            f_output = torch.stack([v for _, v in output_dic[name].items()])\n\t            f_target = torch.stack([v for _, v in target_dic[name].items()])\n", "            our_nmse = nmse(f_target.cpu().numpy(), f_output.cpu().numpy())\n\t            our_psnr = psnr(f_target.cpu().numpy(), f_output.cpu().numpy())\n\t            our_ssim = ssim(f_target.cpu().numpy(), f_output.cpu().numpy())\n\t            nmse_meter.update(our_nmse, 1)\n\t            psnr_meter.update(our_psnr, 1)\n\t            ssim_meter.update(our_ssim, 1)\n\t    total_time = time.time() - start_time\n\t    total_time = str(datetime.timedelta(seconds=int(total_time)))\n\t    loss_avg = loss_all / count\n\t    return {'total_time': total_time, 'loss': loss_avg, 'PSNR': psnr_meter.avg,\n", "            'SSIM': ssim_meter.avg, 'NMSE': nmse_meter.avg}\n\tdef main_fed(args):\n\t    device = torch.device('cuda:0')\n\t    cfg.DISTRIBUTION_TYPE = 'in-distribution' # 'out-of-distribution'  # 'in-distribution'  #\n\t    outputdir = os.path.dirname(os.path.dirname(cfg.MODEL.WEIGHT_PATH))\n\t    outputdir = os.path.join(outputdir, 'evaluation')\n\t    os.makedirs(outputdir, exist_ok=True)\n\t    dirs = [d for d in os.listdir(outputdir) if os.path.isdir(os.path.join(outputdir, d))]\n\t    experiments_num = max([int(k.split('_')[0]) + 1 for k in dirs]) if os.path.exists(outputdir) and not len(dirs) == 0 else 0\n\t    outputdir = os.path.join(outputdir, f'{experiments_num:02d}_' + time.strftime('%y-%m-%d_%H-%M'))\n", "    if outputdir:\n\t        os.makedirs(outputdir, exist_ok=True)\n\t    server_model = create_all_model(cfg)\n\t    seed = args.SEED\n\t    torch.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    if cfg.DISTRIBUTION_TYPE == 'in-distribution':\n\t        dataloader_val, lens_val = build_different_dataloader(cfg, mode='val')\n\t    elif cfg.DISTRIBUTION_TYPE == 'out-of-distribution':\n", "        dataloader_val, lens_val = build_server_dataloader(cfg, mode='val')\n\t    else:\n\t        raise ValueError(\"cfg.DISTRIBUTION_TYPE should be in ['in-distribution', 'out-of-distribution']\")\n\t    checkpoint = torch.load(cfg.MODEL.WEIGHT_PATH, map_location='cpu')\n\t    server_model.load_state_dict(checkpoint['server_model'])\n\t    server_model.to(device)\n\t    eval_status = server_evaluate(server_model, Criterion(), dataloader_val, device)\n\t    with open(os.path.join(os.path.dirname(outputdir), 'testlog.txt'), 'a') as f:\n\t        print(f'outputdir: {outputdir} \\n', file=f)\n\t        print('{} Evaluate time {} PSNR: {:.3f} SSIM: {:.4f} NMSE: {:.4f} \\n\\n'.format(time.strftime('%Y-%m-%d %H:%M'),\n", "            eval_status['total_time'], eval_status['PSNR'], eval_status['SSIM'], eval_status['NMSE']), file=f)\n\t        print('{} Evaluate time {} PSNR: {:.3f} SSIM: {:.4f} NMSE: {:.4f} \\n\\n'.format(time.strftime('%Y-%m-%d %H:%M'),\n\t            eval_status['total_time'], eval_status['PSNR'], eval_status['SSIM'], eval_status['NMSE']))\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser(description=\"a swin prompter transformer\")\n\t    parser.add_argument(\"--config\", default=\"different_dataset\", help=\"choose a experiment to do\")\n\t    args = parser.parse_args()\n\t    cfg = build_config(args.config)\n\t    main_fed(cfg)\n\t    print('OK!')"]}
{"filename": "config/different_dataset.py", "chunked_list": ["from yacs.config import CfgNode as CN\n\timport os\n\t_C = CN()\n\t_C.SEED = 42\n\t_C.dist_url = 'env://'\n\t_C.world_size = 1\n\t# _C.MU = 10\n\t_C.lam = 1\n\t_C.beta = 100\n\t_C.DATASET = CN()\n", "_C.DATA_ROOT = os.path.join(os.path.expanduser('~'), 'data/AAA-Download-Datasets')\n\t_C.DATASET.ROOT = [\n\t    os.path.join(_C.DATA_ROOT, 'Fed/Data/JiangSu'),\n\t    os.path.join(_C.DATA_ROOT, 'Fed/Data/lianying'),\n\t    os.path.join(_C.DATA_ROOT, 'Fed/Data/FeTS2022'),\n\t    os.path.join(_C.DATA_ROOT, 'Fed/Data/IXI-NIFTI'),\n\t]\n\t_C.DATASET.CLIENTS = ['JiangSu', 'lianying', 'FedTS01', 'FedTS02', 'FedTS03', 'FedTS04', 'FedTS05', 'FedTS06',\n\t                      'FedTS07', 'FedTS08', 'FedTS09', 'BraTS10', 'ixiHH', 'ixiGuys', 'ixiIOP']\n\t_C.DATASET.PATTERN = ['T2', ] * 15\n", "_C.DATASET.SAMPLE_RATE = [1, ] * 15\n\t_C.DATASET.CHALLENGE = 'singlecoil'\n\t_C.DATASET.NUM_TRAIN = [432, 360, 120, 304, 304, 240, 160, 160, 280, 224, 264, 184, 360, 328, 296]\n\t_C.DATASET.NUM_VAL =   [184, 154, 80,  130, 130, 100, 64,  64,  120, 96,  112, 78,  152, 120, 128]\n\t_C.DATASET.NUM_SERVER_VAL = 1712\n\t_C.TRANSFORMS = CN()\n\t_C.TRANSFORMS.MASK_FILE = [\"1D-Cartesian_3X_320.mat\"] * 15\n\t_C.TRANSFORMS.MASK_DIR = _C.DATA_ROOT + '/Fed/Data/masks'\n\t# _C.TRANSFORMS.MASK_SPEC = ['2D-Radial-4X', '2D-Random-6X']\n\t_C.TRANSFORMS.MASK_SPEC = [\"1D-Cartesian_3X\", ] * 15\n", "_C.FL = CN()\n\t_C.FL.CLIENTS_NUM = 15\n\t_C.FL.MODEL_NAME = 'swin_vpt_nullspace'\n\t_C.FL.SHOW_SIZE = True\n\t# model config\n\t_C.MODEL = CN()\n\t_C.MODEL.TRANSFER_TYPE = \"prompt\"  # \"prompt\"  # one of linear, end2end, prompt, adapter, side, partial-1, tinytl-bias\n\t_C.MODEL.WEIGHT_PATH = None\n\t_C.MODEL.SUBTYPE = \"swin_320\"\n\t_C.MODEL.INPUTSIZE = 320\n", "_C.MODEL.FINALSIZE = 320\n\t_C.MODEL.HEAD_NUM_CONV = 4\n\t_C.MODEL.INPUT_DIM = 1\n\t_C.MODEL.OUTPUT_DIM = 1\n\t_C.MODEL.PROMPT = CN()\n\t_C.MODEL.PROMPT.NUM_TOKENS = 20\n\t_C.MODEL.PROMPT.LOCATION = \"prepend\"\n\t_C.MODEL.PROMPT.INITIATION = \"random\"\n\t_C.MODEL.PROMPT.PROJECT = -1\n\t_C.MODEL.PROMPT.DEEP = True  # \"whether do deep prompt or not, only for prepend location\"\n", "_C.MODEL.PROMPT.DROPOUT = 0.0\n\t# the solver config\n\t_C.SOLVER = CN()\n\t_C.SOLVER.DEVICE = 'cuda:1'\n\t_C.SOLVER.DEVICE_IDS = [0, 1, 2, 3]\n\t_C.SOLVER.LR = [0.1, ] * 15\n\t_C.SOLVER.WEIGHT_DECAY = 0\n\t_C.SOLVER.LR_DROP = 20\n\t_C.SOLVER.BATCH_SIZE = 8\n\t_C.SOLVER.NUM_WORKERS = 4\n", "_C.SOLVER.PRINT_FREQ = 10\n\t_C.SOLVER.LR_GAMMA = 0.1\n\t_C.SOLVER.RATIO = 0.8\n\t# the others config\n\t_C.RESUME = ''  # model resume path\n\t_C.PRETRAINED_FASTMRI_CKPT = 'path/to/pretrained/fastmri_ckpt.pth'\n\t_C.OUTPUTDIR = './saved'\n\t#the train configs\n\t_C.TRAIN = CN()\n\t_C.TRAIN.EPOCHS =       50\n", "_C.TRAIN.LOCAL_EPOCHS = 10\n\t_C.meta_client_num = 15\n\t_C.DISTRIBUTION_TYPE = 'in-distribution'  #  'out-of-distribution'  # 'in-distribution'\n"]}
{"filename": "config/__init__.py", "chunked_list": ["from .different_dataset import _C as DDC\n\tconfig_factory = {\n\t    'different_dataset': DDC,\n\t}\n\tdef build_config(factory):\n\t    return config_factory[factory]"]}
{"filename": "data/transforms.py", "chunked_list": ["\"\"\"\n\tCopyright (c) Facebook, Inc. and its affiliates.\n\tThis source code is licensed under the MIT license found in the\n\tLICENSE file in the root directory of this source tree.\n\t\"\"\"\n\timport os\n\timport numpy as np\n\timport torch\n\t# import data.fastmri as fastmri\n\tfrom scipy.io import loadmat,savemat\n", "from .math import ifft2c, fft2c, complex_abs, tensor_to_complex_np\n\tfrom .subsample import create_mask_for_mask_type, MaskFunc\n\timport matplotlib.pyplot as plt\n\tdef imshow(img, title=\"\"):\n\t    \"\"\" Show image as grayscale. \"\"\"\n\t    if img.dtype == np.complex64 or img.dtype == np.complex128:\n\t        print('img is complex! Take absolute value.')\n\t        img = np.abs(img)\n\t    plt.figure()\n\t    # plt.imshow(img, cmap='gray', interpolation='nearest')\n", "    plt.axis('off')\n\t    plt.title(title)\n\t    plt.imsave('{}.png'.format(title), img, cmap='gray')\n\tdef rss(data, dim=0):\n\t    \"\"\"\n\t    Compute the Root Sum of Squares (RSS).\n\t    RSS is computed assuming that dim is the coil dimension.\n\t    Args:\n\t        data (torch.Tensor): The input tensor\n\t        dim (int): The dimensions along which to apply the RSS transform\n", "    Returns:\n\t        torch.Tensor: The RSS value.\n\t    \"\"\"\n\t    return torch.sqrt((data ** 2).sum(dim))\n\tdef to_tensor(data):\n\t    \"\"\"\n\t    Convert numpy array to PyTorch tensor.\n\t    For complex arrays, the real and imaginary parts are stacked along the last\n\t    dimension.\n\t    Args:\n", "        data (np.array): Input numpy array.\n\t    Returns:\n\t        torch.Tensor: PyTorch version of data.\n\t    \"\"\"\n\t    if np.iscomplexobj(data):\n\t        data = np.stack((data.real, data.imag), axis=-1)\n\t    return torch.from_numpy(data)\n\tdef tensor_to_complex_np(data):\n\t    \"\"\"\n\t    Converts a complex torch tensor to numpy array.\n", "    Args:\n\t        data (torch.Tensor): Input data to be converted to numpy.\n\t    Returns:\n\t        np.array: Complex numpy version of data.\n\t    \"\"\"\n\t    data = data.numpy()\n\t    return data[..., 0] + 1j * data[..., 1]\n\tdef apply_mask(data, mask_func, seed=None, padding=None):\n\t    \"\"\"\n\t    Subsample given k-space by multiplying with a mask.\n", "    Args:\n\t        data (torch.Tensor): The input k-space data. This should have at least 3 dimensions, where\n\t            dimensions -3 and -2 are the spatial dimensions, and the final dimension has size\n\t            2 (for complex values).\n\t        mask_func (callable): A function that takes a shape (tuple of ints) and a random\n\t            number seed and returns a mask.\n\t        seed (int or 1-d array_like, optional): Seed for the random number generator.\n\t    Returns:\n\t        (tuple): tuple containing:\n\t            masked data (torch.Tensor): Subsampled k-space data\n", "            mask (torch.Tensor): The generated mask\n\t    \"\"\"\n\t    shape = np.array(data.shape)\n\t    shape[:-3] = 1\n\t    mask = mask_func(shape, seed)\n\t    # if not isinstance(mask_func, np.ndarray):\n\t        # mask = mask_func(shape, seed)\n\t    # else: mask = mask_func\n\t    if padding is not None:\n\t        mask[:, :, : padding[0]] = 0\n", "        mask[:, :, padding[1] :] = 0  # padding value inclusive on right of zeros\n\t    masked_data = data * mask + 0.0  # the + 0.0 removes the sign of the zeros\n\t    return masked_data, mask\n\tdef mask_center(x, mask_from, mask_to):\n\t    mask = torch.zeros_like(x)\n\t    mask[:, :, :, mask_from:mask_to] = x[:, :, :, mask_from:mask_to]\n\t    return mask\n\tdef center_crop(data, shape):\n\t    \"\"\"\n\t    Apply a center crop to the input real image or batch of real images.\n", "    Args:\n\t        data (torch.Tensor): The input tensor to be center cropped. It should\n\t            have at least 2 dimensions and the cropping is applied along the\n\t            last two dimensions.\n\t        shape (int, int): The output shape. The shape should be smaller than\n\t            the corresponding dimensions of data.\n\t    Returns:\n\t        torch.Tensor: The center cropped image.\n\t    \"\"\"\n\t    assert 0 < shape[0] <= data.shape[-2]\n", "    assert 0 < shape[1] <= data.shape[-1]\n\t    w_from = (data.shape[-2] - shape[0]) // 2\n\t    h_from = (data.shape[-1] - shape[1]) // 2\n\t    w_to = w_from + shape[0]\n\t    h_to = h_from + shape[1]\n\t    return data[..., w_from:w_to, h_from:h_to]\n\tdef complex_center_crop(data, shape):\n\t    \"\"\"\n\t    Apply a center crop to the input image or batch of complex images.\n\t    Args:\n", "        data (torch.Tensor): The complex input tensor to be center cropped. It\n\t            should have at least 3 dimensions and the cropping is applied along\n\t            dimensions -3 and -2 and the last dimensions should have a size of\n\t            2.\n\t        shape (int): The output shape. The shape should be smaller than\n\t            the corresponding dimensions of data.\n\t    Returns:\n\t        torch.Tensor: The center cropped image\n\t    \"\"\"\n\t    assert 0 < shape[0] <= data.shape[-3]\n", "    assert 0 < shape[1] <= data.shape[-2]\n\t    w_from = (data.shape[-3] - shape[0]) // 2   #80\n\t    h_from = (data.shape[-2] - shape[1]) // 2   #80\n\t    w_to = w_from + shape[0]  #240\n\t    h_to = h_from + shape[1]  #240\n\t    return data[..., w_from:w_to, h_from:h_to, :]\n\tdef center_crop_to_smallest(x, y):\n\t    \"\"\"\n\t    Apply a center crop on the larger image to the size of the smaller.\n\t    The minimum is taken over dim=-1 and dim=-2. If x is smaller than y at\n", "    dim=-1 and y is smaller than x at dim=-2, then the returned dimension will\n\t    be a mixture of the two.\n\t    Args:\n\t        x (torch.Tensor): The first image.\n\t        y (torch.Tensor): The second image\n\t    Returns:\n\t        tuple: tuple of tensors x and y, each cropped to the minimim size.\n\t    \"\"\"\n\t    smallest_width = min(x.shape[-1], y.shape[-1])\n\t    smallest_height = min(x.shape[-2], y.shape[-2])\n", "    x = center_crop(x, (smallest_height, smallest_width))\n\t    y = center_crop(y, (smallest_height, smallest_width))\n\t    return x, y\n\tdef norm(data, eps=1e-11):\n\t    data = (data - data.min()) / (data.max() - data.min() + eps)\n\t    return data\n\tdef normalize(data, mean, stddev, eps=0.0):\n\t    \"\"\"\n\t    Normalize the given tensor.\n\t    Applies the formula (data - mean) / (stddev + eps).\n", "    Args:\n\t        data (torch.Tensor): Input data to be normalized.\n\t        mean (float): Mean value.\n\t        stddev (float): Standard deviation.\n\t        eps (float, default=0.0): Added to stddev to prevent dividing by zero.\n\t    Returns:\n\t        torch.Tensor: Normalized tensor\n\t    \"\"\"\n\t    return (data - mean) / (stddev + eps)\n\tdef normalize_instance(data, eps=0.0):\n", "    \"\"\"\n\t    Normalize the given tensor  with instance norm/\n\t    Applies the formula (data - mean) / (stddev + eps), where mean and stddev\n\t    are computed from the data itself.\n\t    Args:\n\t        data (torch.Tensor): Input data to be normalized\n\t        eps (float): Added to stddev to prevent dividing by zero\n\t    Returns:\n\t        torch.Tensor: Normalized tensor\n\t    \"\"\"\n", "    mean = data.mean()\n\t    std = data.std()\n\t    return normalize(data, mean, std, eps), mean, std\n\tclass DataTransform(object):\n\t    \"\"\"\n\t    Data Transformer for training U-Net models.\n\t    \"\"\"\n\t    def __init__(self, which_challenge, mask_func=None, client_name='fastMRI', use_seed=True):\n\t        \"\"\"\n\t        Args:\n", "            which_challenge (str): Either \"singlecoil\" or \"multicoil\" denoting\n\t                the dataset.\n\t            mask_func (fastmri.data.subsample.MaskFunc): A function that can\n\t                create a mask of appropriate shape.\n\t            use_seed (bool): If true, this class computes a pseudo random\n\t                number generator seed from the filename. This ensures that the\n\t                same mask is used for all the slices of a given volume every\n\t                time.\n\t        \"\"\"\n\t        if which_challenge not in (\"singlecoil\", \"multicoil\"):\n", "            raise ValueError(f'Challenge should either be \"singlecoil\" or \"multicoil\"')\n\t        self.which_challenge = which_challenge\n\t        self.mask_func = mask_func\n\t        self.client_name = client_name\n\t        self.use_seed = use_seed\n\t    def __call__(self, kspace, mask, target, attrs, fname, slice_num):\n\t        \"\"\"\n\t        Args:\n\t            kspace (numpy.array): Input k-space of shape (num_coils, rows,\n\t                cols, 2) for multi-coil data or (rows, cols, 2) for single coil\n", "                data.\n\t            mask (numpy.array): Mask from the test dataset.\n\t            target (numpy.array): Target image.\n\t            attrs (dict): Acquisition related information stored in the HDF5\n\t                object.\n\t            fname (str): File name.\n\t            slice_num (int): Serial number of the slice.\n\t        Returns:\n\t            (tuple): tuple containing:\n\t                image (torch.Tensor): Zero-filled input image.\n", "                target (torch.Tensor): Target image converted to a torch\n\t                    Tensor.\n\t                mean (float): Mean value used for normalization.\n\t                std (float): Standard deviation value used for normalization.\n\t                fname (str): File name.\n\t                slice_num (int): Serial number of the slice.\n\t        \"\"\"\n\t        kspace = to_tensor(kspace)\n\t        if type(self.mask_func).__name__ == 'RandomMaskFunc' or type(self.mask_func).__name__ == 'EquispacedMaskFunc':\n\t            seed = None if not self.use_seed else tuple(map(ord, fname))\n", "            masked_kspace, mask = apply_mask(kspace, self.mask_func, seed)\n\t        elif type(self.mask_func).__name__ == 'ndarray' and \\\n\t                self.mask_func.shape[0] == kspace.shape[0] and self.mask_func.shape[1] == kspace.shape[1]:\n\t            masked_kspace, mask = apply_mask(kspace, self.mask_func)\n\t        else:\n\t            masked_kspace = kspace\n\t            mask = self.mask_func\n\t        if self.client_name != 'fastMRI':\n\t                        # inverse Fourier transform to get zero filled solution\n\t            image = ifft2c(masked_kspace)\n", "            # crop input to correct size\n\t            if target is not None:\n\t                crop_size = (target.shape[-2], target.shape[-1])\n\t            else:\n\t                crop_size = (attrs[\"recon_size\"][0], attrs[\"recon_size\"][1])\n\t            # check for sFLAIR 203\n\t            if image.shape[-2] < crop_size[1]:\n\t                crop_size = (image.shape[-2], image.shape[-2])\n\t            image = complex_center_crop(image, crop_size)\n\t            # apply mask only when mask's size is less than kspace's size\n", "            if kspace.shape[0] >= mask.shape[0] and kspace.shape[1] >= mask.shape[1]:\n\t                cropped_kspace = fft2c(image)\n\t                cropped_kspace = complex_center_crop(cropped_kspace, (320,320))\n\t                mask_matched = mask.unsqueeze(-1).repeat(1,1,2)\n\t                masked_cropped_kspace = cropped_kspace * mask_matched + 0.0\n\t                image = ifft2c(masked_cropped_kspace)\n\t            # absolute value\n\t            image = complex_abs(image)\n\t            image, mean, std = normalize_instance(image, eps=1e-11)\n\t            image = image.clamp(-6, 6)\n", "            # normalize target\n\t            if target is not None:\n\t                target = complex_abs(ifft2c(cropped_kspace))\n\t                # target = norm(target, eps=1e-11)\n\t                target = normalize(target, mean, std, eps=1e-11)\n\t                target = target.clamp(-6, 6)\n\t            else:\n\t                target = torch.Tensor([0])\n\t        else:\n\t            # inverse Fourier transform to get zero filled solution\n", "            image = ifft2c(masked_kspace)\n\t            # crop input to correct size\n\t            if target is not None:\n\t                crop_size = (target.shape[-2], target.shape[-1])\n\t            else:\n\t                crop_size = (attrs[\"recon_size\"][0], attrs[\"recon_size\"][1])\n\t            # check for sFLAIR 203\n\t            if image.shape[-2] < crop_size[1]:\n\t                crop_size = (image.shape[-2], image.shape[-2])\n\t            image = complex_center_crop(image, crop_size)\n", "            # apply mask only when mask's size is less than kspace's size\n\t            if kspace.shape[0] >= mask.shape[0] and kspace.shape[1] >= mask.shape[1]:\n\t                cropped_kspace = fft2c(image)\n\t                mask_matched = mask.unsqueeze(-1).repeat(1,1,2)\n\t                masked_cropped_kspace = cropped_kspace * mask_matched + 0.0\n\t                image = ifft2c(masked_cropped_kspace)\n\t            # absolute value\n\t            image = complex_abs(image)\n\t            # apply Root-Sum-of-Squares if multicoil data\n\t            if self.which_challenge == \"multicoil\":\n", "                image = rss(image)\n\t            # normalize input\n\t            image, mean, std = normalize_instance(image, eps=1e-11)\n\t            image = image.clamp(-6, 6)\n\t            # normalize target\n\t            if target is not None:\n\t                target = to_tensor(target)\n\t                target = center_crop(target, crop_size)\n\t                target = normalize(target, mean, std, eps=1e-11)\n\t                target = target.clamp(-6, 6)\n", "            else:\n\t                target = torch.Tensor([0])\n\t        fname = fname.split('/')[-1]\n\t        fname = fname.split('.')[0]\n\t        return image, target, mean, std, fname, slice_num\n\tdef build_transforms(args, mode = 'train', client_name='fastMRI'):\n\t    mask_size = 256 if client_name.find('IXI')>=0 else 320\n\t    if client_name == 'JiangSu':\n\t        mask_dir = os.path.join(args.TRANSFORMS.MASK_DIR, args.TRANSFORMS.MASK_SPEC[0]+'_{}.mat'.format(mask_size))\n\t    elif client_name == 'lianying':\n", "        mask_dir = os.path.join(args.TRANSFORMS.MASK_DIR, args.TRANSFORMS.MASK_SPEC[1]+'_{}.mat'.format(mask_size))\n\t    if mode == 'train':\n\t        if args.TRANSFORMS.MASK_SPEC != '':\n\t            mask = loadmat(mask_dir)['mask']\n\t            mask = torch.from_numpy(mask).float()\n\t            # mask = mask.float()  # or mask.to(torch.float32)\n\t        else:\n\t            mask = create_mask_for_mask_type(\n\t                args.TRANSFORMS.MASKTYPE, args.TRANSFORMS.CENTER_FRACTIONS, args.TRANSFORMS.ACCELERATIONS,\n\t            )\n", "        return DataTransform(args.DATASET.CHALLENGE, mask, client_name, use_seed=False)\n\t    elif mode == 'val':\n\t        if args.TRANSFORMS.MASK_SPEC != '':\n\t            mask = loadmat(mask_dir)['mask']\n\t            mask = torch.from_numpy(mask).float()\n\t        else:\n\t            mask = create_mask_for_mask_type(\n\t                args.TRANSFORMS.MASKTYPE, args.TRANSFORMS.CENTER_FRACTIONS, args.TRANSFORMS.ACCELERATIONS,\n\t            )\n\t        return DataTransform(args.DATASET.CHALLENGE, mask, client_name)\n", "    else:\n\t        return DataTransform(args.DATASET.CHALLENGE, client_name)\n"]}
{"filename": "data/__init__.py", "chunked_list": ["import copy\n\timport os\n\timport torch\n\tfrom .fedTS_mix import build_fed_dataset\n\tfrom .lianying_jiangsu_mix import create_datasets\n\tfrom torch.utils.data import DataLoader, DistributedSampler\n\tfrom .ixi_mix import build_ixi_dataset\n\tdef build_server_dataloader(args, mode='val'):\n\t    mask = os.path.join(args.TRANSFORMS.MASK_DIR, args.TRANSFORMS.MASK_FILE[2])\n\t    data_list = os.path.join(args.DATASET.ROOT[2], 'validation.txt')\n", "    data_loader = []\n\t    dataset = build_fed_dataset(data_root=args.DATASET.ROOT[2], list_file=data_list,\n\t                                                             mask_path=mask, mode=mode,\n\t                                                             sample_rate=args.DATASET.SAMPLE_RATE[2],\n\t                                                             crop_size=(320, 320), pattern=args.DATASET.PATTERN[2],\n\t                                                             totalsize=args.DATASET.NUM_SERVER_VAL,\n\t                                                             sample_delta=4\n\t                                                             )\n\t    sampler = torch.utils.data.SequentialSampler(dataset)\n\t    data_loader.append(DataLoader(dataset, batch_size=args.SOLVER.BATCH_SIZE, sampler=sampler,\n", "                              num_workers=args.SOLVER.NUM_WORKERS, pin_memory=False, drop_last=False))\n\t    return data_loader, [len(dataset)]\n\tdef build_different_dataloader(args, mode='train'):\n\t    mask_path = [os.path.join(args.TRANSFORMS.MASK_DIR, f) for f in args.TRANSFORMS.MASK_FILE]\n\t    crop_size = (args.MODEL.INPUTSIZE, args.MODEL.INPUTSIZE)\n\t    jiangsu_dataset = create_datasets(args, data_root=args.DATASET.ROOT[0],  mode=mode,\n\t                                      sample_rate=args.DATASET.SAMPLE_RATE[0], client_name='JiangSu',\n\t                                      pattern=args.DATASET.PATTERN[0], crop_size=crop_size,\n\t                                      totalsize=args.DATASET.NUM_TRAIN[0] if mode=='train' else args.DATASET.NUM_VAL[0],\n\t                                      slice_range=(0, 19))\n", "    lianying_dataset = create_datasets(args, data_root=args.DATASET.ROOT[1], mode=mode,\n\t                                       sample_rate=args.DATASET.SAMPLE_RATE[1], client_name='lianying',\n\t                                       pattern=args.DATASET.PATTERN[1], crop_size=crop_size,\n\t                                       totalsize=args.DATASET.NUM_TRAIN[1] if mode == 'train' else args.DATASET.NUM_VAL[1],\n\t                                       slice_range=(0, 18))\n\t    fedlist = [3,5,16,20,6,13,21,4,18,1]\n\t    feds = {}\n\t    fed_root = args.DATASET.ROOT[2]\n\t    for idx, sub_client in enumerate(fedlist):\n\t        subidx= idx+2\n", "        train_list = os.path.join(fed_root, f'{mode}_{sub_client:02d}.txt')\n\t        feds[f'fed_dataset{sub_client:02d}'] = build_fed_dataset(data_root=fed_root, list_file=train_list, mask_path=mask_path[subidx], mode=mode,\n\t                                                        sample_rate=args.DATASET.SAMPLE_RATE[subidx],\n\t                                                        crop_size=crop_size, pattern =args.DATASET.PATTERN[subidx],\n\t                                                        totalsize=args.DATASET.NUM_TRAIN[subidx] if mode == 'train' else args.DATASET.NUM_VAL[subidx],\n\t                                                        )\n\t    ixi_totalsize = {'HH':   {'train': args.DATASET.NUM_TRAIN[12], 'val': args.DATASET.NUM_VAL[12]},\n\t                     'Guys': {'train': args.DATASET.NUM_TRAIN[13], 'val': args.DATASET.NUM_VAL[13]},\n\t                     'IOP':  {'train': args.DATASET.NUM_TRAIN[14], 'val': args.DATASET.NUM_VAL[14]}\n\t                     }\n", "    ixi_root = args.DATASET.ROOT[3]\n\t    HH_list = os.path.join(ixi_root, f'HH_{mode}.txt')\n\t    ixi_HH_dataset = build_ixi_dataset(data_root=ixi_root, list_file=HH_list, mask_path=mask_path[12], mode=mode, pattern=args.DATASET.PATTERN[12],\n\t                                       type='HH', sample_rate=args.DATASET.SAMPLE_RATE[12],\n\t                                       totalsize=ixi_totalsize['HH'][mode], crop_size=crop_size)\n\t    Guys_list = os.path.join(ixi_root, f'Guys_{mode}.txt')\n\t    ixi_Guys_dataset = build_ixi_dataset(data_root=ixi_root, list_file=Guys_list, mask_path=mask_path[13], mode=mode, pattern=args.DATASET.PATTERN[13],\n\t                                         type='Guys', sample_rate=args.DATASET.SAMPLE_RATE[13],\n\t                                         totalsize=ixi_totalsize['Guys'][mode], crop_size=crop_size)\n\t    IOP_list = os.path.join(ixi_root, f'IOP_{mode}.txt')\n", "    ixi_IOP_dataset = build_ixi_dataset(data_root=ixi_root, list_file=IOP_list, mask_path=mask_path[14], mode=mode, pattern=args.DATASET.PATTERN[14],\n\t                                        type='IOP', sample_rate=args.DATASET.SAMPLE_RATE[14],\n\t                                        totalsize=ixi_totalsize['IOP'][mode], crop_size=crop_size)\n\t    datasets = [jiangsu_dataset, lianying_dataset, *feds.values(), ixi_HH_dataset, ixi_Guys_dataset, ixi_IOP_dataset]\n\t    data_loader = []\n\t    dataset_len = []\n\t    for dataset in datasets:\n\t        dataset_len.append(len(dataset))\n\t        if mode == 'train':\n\t            sampler = torch.utils.data.RandomSampler(dataset)\n", "            data_loader.append(DataLoader(dataset, batch_size=args.SOLVER.BATCH_SIZE, sampler=sampler,\n\t                                         num_workers=args.SOLVER.NUM_WORKERS, pin_memory=False, drop_last=False))\n\t        elif mode == 'val':\n\t            sampler = torch.utils.data.SequentialSampler(dataset)\n\t            data_loader.append(DataLoader(dataset, batch_size=args.SOLVER.BATCH_SIZE,  sampler=sampler,\n\t                                         num_workers=args.SOLVER.NUM_WORKERS, pin_memory=False, drop_last=False))\n\t    return data_loader, dataset_len\n"]}
{"filename": "data/fedTS_mix.py", "chunked_list": ["import os\n\timport random\n\tfrom .transforms import normalize, normalize_instance\n\tfrom torch.utils.data import Dataset\n\tfrom scipy.io import loadmat\n\timport pickle\n\tfrom matplotlib import pyplot as plt\n\tfrom .math import *\n\tclass FedTS(Dataset):\n\t    def __init__(self, data_root, list_file, mask_path, mode='train', sample_rate=1., crop_size=(320, 320), pattern='T2',\n", "                 totalsize=None, sample_delta=4):\n\t        self.mask = loadmat(mask_path)['mask']\n\t        self.crop_size = crop_size\n\t        paths = []\n\t        with open(list_file) as f:\n\t            for line in f:\n\t                line = os.path.join(data_root, line.strip())\n\t                name = line.split('/')[-1]\n\t                for slice in range(44, 124, sample_delta):\n\t                    if not pattern=='T1+T2':\n", "                        if pattern=='T1':\n\t                            path1 = os.path.join(line, name + '_t1' + '-%03d' % (slice) + '.mat')\n\t                            paths.append((path1, slice, name+'_t1'+'-%03d'%(slice)))\n\t                        if pattern=='T2':\n\t                            path2 = os.path.join(line, name + '_t2' + '-%03d' % (slice) + '.mat')\n\t                            paths.append((path2, slice, name+'_t2'+'-%03d'%(slice)))\n\t                    else:\n\t                        path1 = os.path.join(line, name + '_t1' + '-%03d' % (slice) + '.mat')\n\t                        path2 = os.path.join(line, name + '_t2' + '-%03d' % (slice) + '.mat')\n\t                        paths.append((path1, slice, name + '_t1' + '-%03d' % (slice)))\n", "                        paths.append((path2, slice, name + '_t2' + '-%03d' % (slice)))\n\t                if len(paths) > totalsize:\n\t                    break\n\t        if sample_rate < 1:\n\t            if mode == 'train':\n\t                random.shuffle(paths)\n\t            num_examples = round(len(paths) * sample_rate)\n\t            self.examples = paths[0:num_examples]\n\t        else:\n\t            self.examples = paths\n", "        self.examples = self.examples[:totalsize]\n\t    def __getitem__(self, item):\n\t        path, slice, fname = self.examples[item]\n\t        img = loadmat(path)\n\t        img = img['img'].transpose(1, 0)\n\t        kspace = np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(img)))\n\t        kspace = self.pad_toshape(kspace, pad_shape=self.crop_size)\n\t        maskedkspace = kspace * self.mask\n\t        subsample = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(maskedkspace)))\n\t        subsample = abs(subsample)\n", "        target = np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(kspace)))\n\t        target = abs(target)\n\t        subsample = torch.from_numpy(subsample).float()\n\t        target = torch.from_numpy(target).float()\n\t        subsample, mean, std = normalize_instance(subsample, eps=1e-11)\n\t        target = normalize(target, mean, std, eps=1e-11)\n\t        subsample = subsample.clamp(-6, 6)\n\t        target = target.clamp(-6, 6)\n\t        return subsample, target, mean, std, fname, slice\n\t    def __len__(self):\n", "        return len(self.examples)\n\t    def collate(self, batch):\n\t        return [torch.cat(v) for v in zip(*batch)]\n\t    def pad_toshape(self, data, pad_shape):\n\t        assert 0 < data.shape[-2] <= pad_shape[0], 'Error: pad_shape: {}, data.shape: {}'.format(pad_shape, data.shape)  # 556...556\n\t        assert 0 < data.shape[-1] <= pad_shape[1]   # 640...640\n\t        k = np.zeros(shape=pad_shape, dtype=np.complex64)\n\t        h_from = (pad_shape[0] - data.shape[-2]) // 2\n\t        w_from = (pad_shape[1] - data.shape[-1]) // 2\n\t        h_to = h_from + data.shape[-1]\n", "        w_to = w_from + data.shape[-2]\n\t        k[...,h_from:h_to, w_from:w_to] = data\n\t        return k\n\tdef build_fed_dataset(data_root, list_file, mask_path, mode='train',  sample_rate=1.0, crop_size=320, pattern='T2',\n\t                      totalsize=None, sample_delta=4):\n\t    if mode in ['train', 'val']:\n\t        return FedTS(data_root, list_file, mask_path, mode, sample_rate, crop_size=crop_size, pattern=pattern,\n\t                     totalsize=totalsize, sample_delta=sample_delta)\n\t    else:\n\t        raise ValueError('mode in fedts2022 dataset setup process error')\n"]}
{"filename": "data/subsample.py", "chunked_list": ["\"\"\"\n\tCopyright (c) Facebook, Inc. and its affiliates.\n\tThis source code is licensed under the MIT license found in the\n\tLICENSE file in the root directory of this source tree.\n\t\"\"\"\n\timport contextlib\n\timport numpy as np\n\timport torch\n\t@contextlib.contextmanager\n\tdef temp_seed(rng, seed):\n", "    state = rng.get_state()\n\t    rng.seed(seed)\n\t    try:\n\t        yield\n\t    finally:\n\t        rng.set_state(state)\n\tdef create_mask_for_mask_type(mask_type_str, center_fractions, accelerations):\n\t    if mask_type_str == \"random\":\n\t        return RandomMaskFunc(center_fractions, accelerations)\n\t    elif mask_type_str == \"equispaced\":\n", "        return EquispacedMaskFunc(center_fractions, accelerations)\n\t    else:\n\t        raise Exception(f\"{mask_type_str} not supported\")\n\tclass MaskFunc(object):\n\t    \"\"\"\n\t    An object for GRAPPA-style sampling masks.\n\t    This crates a sampling mask that densely samples the center while\n\t    subsampling outer k-space regions based on the undersampling factor.\n\t    \"\"\"\n\t    def __init__(self, center_fractions, accelerations):\n", "        \"\"\"\n\t        Args:\n\t            center_fractions (List[float]): Fraction of low-frequency columns to be\n\t                retained. If multiple values are provided, then one of these\n\t                numbers is chosen uniformly each time. \n\t            accelerations (List[int]): Amount of under-sampling. This should have\n\t                the same length as center_fractions. If multiple values are\n\t                provided, then one of these is chosen uniformly each time.\n\t        \"\"\"\n\t        if len(center_fractions) != len(accelerations):\n", "            raise ValueError(\n\t                \"Number of center fractions should match number of accelerations\"\n\t            )\n\t        self.center_fractions = center_fractions\n\t        self.accelerations = accelerations\n\t        self.rng = np.random\n\t    def choose_acceleration(self):\n\t        \"\"\"Choose acceleration based on class parameters.\"\"\"\n\t        choice = self.rng.randint(0, len(self.accelerations))\n\t        center_fraction = self.center_fractions[choice]\n", "        acceleration = self.accelerations[choice]\n\t        return center_fraction, acceleration\n\tclass RandomMaskFunc(MaskFunc):\n\t    \"\"\"\n\t    RandomMaskFunc creates a sub-sampling mask of a given shape.\n\t    The mask selects a subset of columns from the input k-space data. If the\n\t    k-space data has N columns, the mask picks out:\n\t        1. N_low_freqs = (N * center_fraction) columns in the center\n\t           corresponding to low-frequencies.\n\t        2. The other columns are selected uniformly at random with a\n", "        probability equal to: prob = (N / acceleration - N_low_freqs) /\n\t        (N - N_low_freqs). This ensures that the expected number of columns\n\t        selected is equal to (N / acceleration).\n\t    It is possible to use multiple center_fractions and accelerations, in which\n\t    case one possible (center_fraction, acceleration) is chosen uniformly at\n\t    random each time the RandomMaskFunc object is called.\n\t    For example, if accelerations = [4, 8] and center_fractions = [0.08, 0.04],\n\t    then there is a 50% probability that 4-fold acceleration with 8% center\n\t    fraction is selected and a 50% probability that 8-fold acceleration with 4%\n\t    center fraction is selected.\n", "    \"\"\"\n\t    def __call__(self, shape, seed=None):\n\t        \"\"\"\n\t        Create the mask.\n\t        Args:\n\t            shape (iterable[int]): The shape of the mask to be created. The\n\t                shape should have at least 3 dimensions. Samples are drawn\n\t                along the second last dimension.\n\t            seed (int, optional): Seed for the random number generator. Setting\n\t                the seed ensures the same mask is generated each time for the\n", "                same shape. The random state is reset afterwards.\n\t        Returns:\n\t            torch.Tensor: A mask of the specified shape.\n\t        \"\"\"\n\t        if len(shape) < 3:\n\t            raise ValueError(\"Shape should have 3 or more dimensions\")\n\t        with temp_seed(self.rng, seed):\n\t            num_cols = shape[-2]\n\t            center_fraction, acceleration = self.choose_acceleration()\n\t            # create the mask\n", "            num_low_freqs = int(round(num_cols * center_fraction))\n\t            prob = (num_cols / acceleration - num_low_freqs) / (\n\t                num_cols - num_low_freqs\n\t            )\n\t            mask = self.rng.uniform(size=num_cols) < prob\n\t            pad = (num_cols - num_low_freqs + 1) // 2\n\t            mask[pad : pad + num_low_freqs] = True\n\t            # reshape the mask\n\t            mask_shape = [1 for _ in shape]\n\t            mask_shape[-2] = num_cols\n", "            mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n\t        return mask\n\tclass EquispacedMaskFunc(MaskFunc):\n\t    \"\"\"\n\t    EquispacedMaskFunc creates a sub-sampling mask of a given shape.\n\t    The mask selects a subset of columns from the input k-space data. If the\n\t    k-space data has N columns, the mask picks out:\n\t        1. N_low_freqs = (N * center_fraction) columns in the center\n\t           corresponding tovlow-frequencies.\n\t        2. The other columns are selected with equal spacing at a proportion\n", "           that reaches the desired acceleration rate taking into consideration\n\t           the number of low frequencies. This ensures that the expected number\n\t           of columns selected is equal to (N / acceleration)\n\t    It is possible to use multiple center_fractions and accelerations, in which\n\t    case one possible (center_fraction, acceleration) is chosen uniformly at\n\t    random each time the EquispacedMaskFunc object is called.\n\t    Note that this function may not give equispaced samples (documented in\n\t    https://github.com/facebookresearch/fastMRI/issues/54), which will require\n\t    modifications to standard GRAPPA approaches. Nonetheless, this aspect of\n\t    the function has been preserved to match the public multicoil data. \n", "    \"\"\"\n\t    def __call__(self, shape, seed):\n\t        \"\"\"\n\t        Args:\n\t            shape (iterable[int]): The shape of the mask to be created. The\n\t                shape should have at least 3 dimensions. Samples are drawn\n\t                along the second last dimension.\n\t            seed (int, optional): Seed for the random number generator. Setting\n\t                the seed ensures the same mask is generated each time for the\n\t                same shape. The random state is reset afterwards.\n", "        Returns:\n\t            torch.Tensor: A mask of the specified shape.\n\t        \"\"\"\n\t        if len(shape) < 3:\n\t            raise ValueError(\"Shape should have 3 or more dimensions\")\n\t        with temp_seed(self.rng, seed):\n\t            center_fraction, acceleration = self.choose_acceleration()\n\t            num_cols = shape[-2]\n\t            num_low_freqs = int(round(num_cols * center_fraction))\n\t            # create the mask\n", "            mask = np.zeros(num_cols, dtype=np.float32)\n\t            pad = (num_cols - num_low_freqs + 1) // 2\n\t            mask[pad : pad + num_low_freqs] = True\n\t            # determine acceleration rate by adjusting for the number of low frequencies\n\t            adjusted_accel = (acceleration * (num_low_freqs - num_cols)) / (\n\t                num_low_freqs * acceleration - num_cols\n\t            )\n\t            offset = self.rng.randint(0, round(adjusted_accel))\n\t            accel_samples = np.arange(offset, num_cols - 1, adjusted_accel)\n\t            accel_samples = np.around(accel_samples).astype(np.uint)\n", "            mask[accel_samples] = True\n\t            # reshape the mask\n\t            mask_shape = [1 for _ in shape]\n\t            mask_shape[-2] = num_cols\n\t            mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n\t        return mask\n"]}
{"filename": "data/ixi_mix.py", "chunked_list": ["from os.path import splitext\n\timport os\n\tfrom os import listdir, path\n\tfrom torch.utils.data import Dataset\n\timport random\n\tfrom scipy.io import loadmat, savemat\n\timport scipy.io as sio\n\timport torch\n\timport cv2\n\timport numpy as np\n", "class IXIdataset(Dataset):\n\t    def __init__(self, data_root, list_file, maskdir, mode='train', pattern='T2', sample_rate=1.0, totalsize=None,\n\t                 crop_size=(224,224), ):\n\t        self.list_file = list_file\n\t        self.mode = mode\n\t        self.maskdir = maskdir\n\t        self.examples = []\n\t        self.img_size = crop_size\n\t        file_names = open(list_file).readlines()\n\t        if not pattern == 'T1+T2':\n", "            if pattern == 'T1':\n\t                idx = 0\n\t            elif pattern == 'T2':\n\t                idx = 1\n\t            for file_name in file_names:\n\t                splits = file_name.split()\n\t                for slice_id in [25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 119]:\n\t                    name = os.path.basename(splits[idx]).split('.')[0]\n\t                    path = os.path.dirname(splits[idx]) + '_mat'\n\t                    self.examples.append((os.path.join(data_root, path), name, slice_id))\n", "                    if len(self.examples) > totalsize:\n\t                        break\n\t        else:\n\t            for idx, file_name in enumerate(file_names):\n\t                splits = file_name.split()\n\t                for slice_id in [25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 119]:\n\t                    name = os.path.basename(splits[0]).split('.')[0]\n\t                    path = os.path.dirname(splits[0]) + '_mat'\n\t                    self.examples.append((os.path.join(data_root, path), name, slice_id))\n\t                    name = os.path.basename(splits[1]).split('.')[0]\n", "                    path = os.path.dirname(splits[1]) + '_mat'\n\t                    self.examples.append((os.path.join(data_root, path), name, slice_id))\n\t                    if len(self.examples) > totalsize:\n\t                        break\n\t        self.mask = loadmat(maskdir)['mask']\n\t        self.examples = self.examples[:totalsize]\n\t        if sample_rate < 1:\n\t            if mode == 'train':\n\t                random.shuffle(self.examples)\n\t            num_examples = round(len(self.examples) * sample_rate)\n", "            self.examples = self.examples[0:num_examples]\n\t    def __len__(self):\n\t        return len(self.examples)\n\t    def __getitem__(self, i):\n\t        fpath, fname, slice_num = self.examples[i]\n\t        file_name = '%s-%03d.mat' % (fname, slice_num)\n\t        file_name = os.path.join(fpath, file_name)\n\t        img = loadmat(file_name)\n\t        img = img['img']\n\t        if 'T2' in fname:\n", "            img_height, img_width  = img.shape\n\t            img_matRotate = cv2.getRotationMatrix2D((img_height * 0.5, img_width * 0.5), 90, 1)\n\t            img = cv2.warpAffine(img, img_matRotate, (img_height, img_width))\n\t        kspace = self.fft2(img).astype(np.complex64)\n\t        cropped_kspace = self.pad_toshape(kspace, pad_shape=self.img_size)\n\t        # target image:\n\t        target = self.ifft2(cropped_kspace).astype(np.complex64)\n\t        target = np.absolute(target)\n\t        masked_cropped_kspace = cropped_kspace * self.mask\n\t        subsample = self.ifft2(masked_cropped_kspace).astype(np.complex64)\n", "        subsample = np.absolute(subsample)\n\t        subsample, target = torch.from_numpy(subsample).float(), torch.from_numpy(target).float()\n\t        subsample, mean, std = self.normalize_instance(subsample, eps=1e-11)\n\t        target = (target - mean) / std\n\t        subsample = subsample.clamp(-6, 6)\n\t        target = target.clamp(-6, 6)\n\t        return subsample, target, mean, std, fname, slice_num\n\t    def fft2(self, img):\n\t        return np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(img), norm=None))\n\t    def ifft2(self, kspace):\n", "        return np.fft.fftshift(np.fft.ifft2(np.fft.ifftshift(kspace), norm=None))\n\t    def pad_toshape(self, data, pad_shape):\n\t        assert 0 < data.shape[-2] <= pad_shape[0], 'Error: pad_shape: {}, data.shape: {}'.format(pad_shape, data.shape)  # 556...556\n\t        assert 0 < data.shape[-1] <= pad_shape[1]   # 640...640\n\t        k = np.zeros(shape=pad_shape, dtype=np.complex64)\n\t        h_from = (pad_shape[0] - data.shape[-2]) // 2\n\t        w_from = (pad_shape[1] - data.shape[-1]) // 2\n\t        h_to = h_from + data.shape[-1]\n\t        w_to = w_from + data.shape[-2]\n\t        k[...,h_from:h_to, w_from:w_to] = data\n", "        return k\n\t    def normalize_instance(self, data, eps=0.0):\n\t        mean = data.mean()\n\t        std = data.std()\n\t        return (data - mean) / (std + eps), mean, std\n\tdef build_ixi_dataset(data_root, list_file, mask_path, mode, type=None, pattern='T2', sample_rate=1.0, totalsize=None,\n\t                      crop_size=(320, 320)):\n\t    assert type in ['HH', 'Guys', 'IOP'], \"IXI type should choosen from ['HH','Guys','IOP'] \"\n\t    return IXIdataset(data_root, list_file=list_file, maskdir=mask_path, mode=mode, pattern=pattern, sample_rate=sample_rate,\n\t                      totalsize=totalsize, crop_size=crop_size)\n"]}
{"filename": "data/math.py", "chunked_list": ["\"\"\"\n\tCopyright (c) Facebook, Inc. and its affiliates.\n\tThis source code is licensed under the MIT license found in the\n\tLICENSE file in the root directory of this source tree.\n\t\"\"\"\n\timport torch\n\timport numpy as np\n\tdef complex_mul(x, y):\n\t    \"\"\"\n\t    Complex multiplication.\n", "    This multiplies two complex tensors assuming that they are both stored as\n\t    real arrays with the last dimension being the complex dimension.\n\t    Args:\n\t        x (torch.Tensor): A PyTorch tensor with the last dimension of size 2.\n\t        y (torch.Tensor): A PyTorch tensor with the last dimension of size 2.\n\t    Returns:\n\t        torch.Tensor: A PyTorch tensor with the last dimension of size 2.\n\t    \"\"\"\n\t    assert x.shape[-1] == y.shape[-1] == 2\n\t    re = x[..., 0] * y[..., 0] - x[..., 1] * y[..., 1]\n", "    im = x[..., 0] * y[..., 1] + x[..., 1] * y[..., 0]\n\t    return torch.stack((re, im), dim=-1)\n\tdef complex_conj(x):\n\t    \"\"\"\n\t    Complex conjugate.\n\t    This applies the complex conjugate assuming that the input array has the\n\t    last dimension as the complex dimension.\n\t    Args:\n\t        x (torch.Tensor): A PyTorch tensor with the last dimension of size 2.\n\t        y (torch.Tensor): A PyTorch tensor with the last dimension of size 2.\n", "    Returns:\n\t        torch.Tensor: A PyTorch tensor with the last dimension of size 2.\n\t    \"\"\"\n\t    assert x.shape[-1] == 2\n\t    return torch.stack((x[..., 0], -x[..., 1]), dim=-1)\n\tdef fft2c(data: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"\n\t    Apply centered 2 dimensional Fast Fourier Transform.\n\t    Args:\n\t        data: Complex valued input data containing at least 3 dimensions:\n", "            dimensions -3 & -2 are spatial dimensions and dimension -1 has size\n\t            2. All other dimensions are assumed to be batch dimensions.\n\t    Returns:\n\t        The FFT of the input.\n\t    \"\"\"\n\t    if not data.shape[-1] == 2:\n\t        raise ValueError(\"Tensor does not have separate complex dim.\")\n\t    data = ifftshift(data, dim=[-3, -2])\n\t    data = torch.view_as_real(\n\t        torch.fft.fftn(  # type: ignore\n", "            torch.view_as_complex(data), dim=(-2, -1),\n\t        )\n\t    )\n\t    data = fftshift(data, dim=[-3, -2])\n\t    return data\n\tdef ifft2c(data: torch.Tensor) -> torch.Tensor:\n\t    \"\"\"\n\t    Apply centered 2-dimensional Inverse Fast Fourier Transform.\n\t    Args:\n\t        data: Complex valued input data containing at least 3 dimensions:\n", "            dimensions -3 & -2 are spatial dimensions and dimension -1 has size\n\t            2. All other dimensions are assumed to be batch dimensions.\n\t    Returns:\n\t        The IFFT of the input.\n\t    \"\"\"\n\t    if not data.shape[-1] == 2:\n\t        raise ValueError(\"Tensor does not have separate complex dim.\")\n\t    data = ifftshift(data, dim=[-3, -2])\n\t    data = torch.view_as_real(\n\t        torch.fft.ifftn(  # type: ignore\n", "            torch.view_as_complex(data), dim=(-2, -1),\n\t        )\n\t    )\n\t    data = fftshift(data, dim=[-3, -2])\n\t    return data\n\tdef complex_abs(data):\n\t    \"\"\"\n\t    Compute the absolute value of a complex valued input tensor.\n\t    Args:\n\t        data (torch.Tensor): A complex valued tensor, where the size of the\n", "            final dimension should be 2.\n\t    Returns:\n\t        torch.Tensor: Absolute value of data.\n\t    \"\"\"\n\t    assert data.size(-1) == 2\n\t    return (data ** 2).sum(dim=-1).sqrt()\n\tdef complex_abs_numpy(data):\n\t    assert data.shape[-1] == 2\n\t    return np.sqrt(np.sum(data ** 2, axis=-1))\n\tdef complex_abs_sq(data): # multi coil\n", "    \"\"\"\n\t    Compute the squared absolute value of a complex tensor.\n\t    Args:\n\t        data (torch.Tensor): A complex valued tensor, where the size of the\n\t            final dimension should be 2.\n\t    Returns:\n\t        torch.Tensor: Squared absolute value of data.\n\t    \"\"\"\n\t    assert data.size(-1) == 2\n\t    return (data ** 2).sum(dim=-1)\n", "# Helper functions\n\tdef roll(x, shift, dim):\n\t    \"\"\"\n\t    Similar to np.roll but applies to PyTorch Tensors.\n\t    Args:\n\t        x (torch.Tensor): A PyTorch tensor.\n\t        shift (int): Amount to roll.\n\t        dim (int): Which dimension to roll.\n\t    Returns:\n\t        torch.Tensor: Rolled version of x.\n", "    \"\"\"\n\t    if isinstance(shift, (tuple, list)):\n\t        assert len(shift) == len(dim)\n\t        for s, d in zip(shift, dim):\n\t            x = roll(x, s, d)\n\t        return x\n\t    shift = shift % x.size(dim)\n\t    if shift == 0:\n\t        return x\n\t    left = x.narrow(dim, 0, x.size(dim) - shift)\n", "    right = x.narrow(dim, x.size(dim) - shift, shift)\n\t    return torch.cat((right, left), dim=dim)\n\tdef fftshift(x, dim=None):\n\t    \"\"\"\n\t    Similar to np.fft.fftshift but applies to PyTorch Tensors\n\t    Args:\n\t        x (torch.Tensor): A PyTorch tensor.\n\t        dim (int): Which dimension to fftshift.\n\t    Returns:\n\t        torch.Tensor: fftshifted version of x.\n", "    \"\"\"\n\t    if dim is None:\n\t        dim = tuple(range(x.dim()))\n\t        shift = [dim // 2 for dim in x.shape]\n\t    elif isinstance(dim, int):\n\t        shift = x.shape[dim] // 2\n\t    else:\n\t        shift = [x.shape[i] // 2 for i in dim]\n\t    return roll(x, shift, dim)\n\tdef ifftshift(x, dim=None):\n", "    \"\"\"\n\t    Similar to np.fft.ifftshift but applies to PyTorch Tensors\n\t    Args:\n\t        x (torch.Tensor): A PyTorch tensor.\n\t        dim (int): Which dimension to ifftshift.\n\t    Returns:\n\t        torch.Tensor: ifftshifted version of x.\n\t    \"\"\"\n\t    if dim is None:\n\t        dim = tuple(range(x.dim()))\n", "        shift = [(dim + 1) // 2 for dim in x.shape]\n\t    elif isinstance(dim, int):\n\t        shift = (x.shape[dim] + 1) // 2\n\t    else:\n\t        shift = [(x.shape[i] + 1) // 2 for i in dim]\n\t    return roll(x, shift, dim)\n\tdef tensor_to_complex_np(data):\n\t    \"\"\"\n\t    Converts a complex torch tensor to numpy array.\n\t    Args:\n", "        data (torch.Tensor): Input data to be converted to numpy.\n\t    Returns:\n\t        np.array: Complex numpy version of data\n\t    \"\"\"\n\t    data = data.numpy()\n\t    return data[..., 0] + 1j * data[..., 1]\n"]}
{"filename": "data/dicom_mix.py", "chunked_list": ["import pydicom\n\timport os\n\timport torch\n\timport numpy as np\n\timport random\n\tfrom scipy.io import loadmat\n\tfrom .transforms import center_crop,normalize_instance, normalize\n\tfrom torch.utils.data import Dataset\n\tfrom data.transforms import to_tensor\n\tfrom .math import *\n", "class FastMRIDicom(Dataset):\n\t    def __init__(self, list_file, mask, crop_size=(320, 320), mode='train', sample_rate=1, totalsize=None):\n\t        data_root = os.path.join(os.path.expanduser('~'), 'Apollo2022/AAA-Download-Datasets')\n\t        paths = []\n\t        with open(list_file) as f:\n\t                for idx, line in enumerate(f):\n\t                    path = os.path.join(data_root, line.strip())\n\t                    name = line.strip().split('/')[-1]\n\t                    slices = sorted([s.split('.')[0] for s in os.listdir(path)])\n\t                    for idx,slice in enumerate(slices[:16]):\n", "                        paths.append((path, slice, name))\n\t                    if len(paths) > totalsize:\n\t                        break\n\t        paths = paths[: totalsize]\n\t        self.crop_size = crop_size\n\t        self.mask = loadmat(mask)['mask']\n\t        if sample_rate < 1:\n\t            if mode == 'train':\n\t                random.shuffle(paths)\n\t            num_examples = round(len(paths) * sample_rate)\n", "            self.examples = paths[0:num_examples]\n\t        else:\n\t            self.examples = paths\n\t    def __getitem__(self, item):\n\t        path, slice, fname = self.examples[item]\n\t        img_path = os.path.join(path, str(slice)+'.mat')\n\t        img = loadmat(img_path)['img']\n\t        kspace = np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(img)))\n\t        kspace = center_crop(kspace, self.crop_size)\n\t        maskedkspace = kspace * self.mask\n", "        maskedkspace = to_tensor(maskedkspace)\n\t        subsample = complex_abs(ifft2c(maskedkspace))\n\t        kspace = to_tensor(kspace)\n\t        target = complex_abs(ifft2c(kspace))\n\t        subsample, mean, std = normalize_instance(subsample, eps=1e-11)\n\t        target = normalize(target, mean, std, eps=1e-11)\n\t        subsample = subsample.float()\n\t        target = target.float()\n\t        return subsample, target, mean, std, fname, slice\n\t    def __len__(self):\n", "        return len(self.examples)\n\t    def collate(self, batch):\n\t        return [torch.cat(v) for v in zip(*batch)]\n\tdef build_fastmri_dataset(args, mode='train'):\n\t    mask = os.path.join(args.FASTMRI_MASK_DIR, args.FASTMRI_MASK_FILE)\n\t    if mode == 'train':\n\t        data_list = os.path.join(args.FASTMRIROOT, 'train.txt')\n\t    elif mode == 'val':\n\t        data_list = os.path.join(args.FASTMRIROOT, 'val.txt')\n\t    else:\n", "        raise ValueError('mode error')\n\t    return FastMRIDicom(data_list, mask, mode=mode, sample_rate=args.DATASET.SAMPLE_RATE[0],\n\t                        totalsize=0)\n"]}
{"filename": "data/lianying_jiangsu_mix.py", "chunked_list": ["import csv\n\timport os\n\timport logging\n\timport pickle\n\timport random\n\timport scipy.io as sio\n\tfrom scipy.io import loadmat, savemat\n\tfrom os import listdir, path\n\tfrom os.path import splitext\n\tfrom types import SimpleNamespace\n", "from .transforms import build_transforms\n\timport numpy as np\n\timport torch\n\timport yaml\n\tfrom torch.utils.data import Dataset\n\tclass LianYingdataset(Dataset):\n\t    def __init__(self, data_dir,  transforms,  crop_size, challenge, sample_rate=1, mode='train', pattern='T2',\n\t            totalsize=None, slice_range=(0, 18)):\n\t        self.transform = transforms\n\t        self.data_dir = data_dir\n", "        self.img_size = crop_size\n\t        self.examples = []\n\t        self.recons_key = (\n\t            \"reconstruction_esc\" if challenge == \"singlecoil\" else \"reconstruction_rss\"\n\t        )\n\t        #make an image id's list\n\t        if mode == 'train':\n\t            f = open(path.join(str(data_dir),'lianying_train.txt'),'r')\n\t        elif mode == 'val':\n\t            f = open(path.join(str(data_dir),'lianying_val.txt'),'r')\n", "        elif mode == 'test':\n\t            f = open(path.join(str(data_dir),'lianying_test.txt'),'r')\n\t        else: \n\t            raise ValueError(\"No mode like this, please choose one in ['train', 'val', 'test'].\")\n\t        file_names = f.readlines()\n\t        metadata = {\n\t            'acquisition': pattern,\n\t            'encoding_size': (640, 556, 1),\n\t            'max': 0,\n\t            'norm': 0,\n", "            'padding_left': 0,\n\t            'padding_right': 0,\n\t            'patient_id':'0',\n\t            'recon_size': (320, 320, 1),\n\t        }\n\t        if not pattern == 'T1+T2':\n\t            if pattern == 'T1':\n\t                idx = 0\n\t            elif pattern == 'T2':\n\t                idx = 1\n", "            for file_name in file_names:\n\t                splits = file_name.split()  # 分离空格\n\t                for slice_id in range(slice_range[0], slice_range[1]+1):  # 0:19==20\n\t                    self.examples.append((os.path.join(self.data_dir, splits[idx]), slice_id, metadata))\n\t                    if len(self.examples) > totalsize:\n\t                        break\n\t        else:\n\t            for file_name in file_names:\n\t                splits = file_name.split()\n\t                for slice_id in range(slice_range[0], slice_range[1]+1):  # 0:18==19\n", "                    self.examples.append((os.path.join(self.data_dir, splits[0]), slice_id, metadata))\n\t                    self.examples.append((os.path.join(self.data_dir, splits[1]), slice_id, metadata))\n\t                    if len(self.examples) > totalsize:\n\t                        break\n\t        self.examples = self.examples[:totalsize]\n\t        if mode == 'train':\n\t            logging.info(f'Creating training dataset with {len(self.examples)} examples')\n\t        elif mode == 'val':\n\t            logging.info(f'Creating validation dataset with {len(self.examples)} examples')\n\t        elif mode == 'test':\n", "            logging.info(f'Creating test dataset with {len(self.examples)} examples')\n\t        if sample_rate < 1:\n\t            random.shuffle(self.examples)\n\t            num_examples = round(len(self.examples) * sample_rate)\n\t            self.examples = self.examples[0:num_examples]\n\t    def __len__(self):\n\t        return len(self.examples)\n\t    def __getitem__(self, slice_id):\n\t        fname_nii, slice_idx, metadata =self.examples[slice_id]\n\t        slice_path = self.niipath2matpath(fname_nii, slice_idx)\n", "        image = loadmat(slice_path)['img']\n\t        mask = None\n\t        attrs = metadata\n\t        image = np.rot90(image)\n\t        kspace = self.fft2(image).astype(np.complex64)\n\t        target = image.astype(np.float32)\n\t        if self.transform is None:\n\t            sample = (kspace, mask, target, attrs, fname_nii, slice_idx)\n\t        else:\n\t            sample = self.transform(kspace, mask, target, attrs, fname_nii, slice_idx)\n", "        return sample\n\t    def fft2(self, img):\n\t        return np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(img)))\n\t    def niipath2matpath(self, T1,slice_id):\n\t        filedir,filename = path.split(T1)\n\t        filedir,_ = path.split(filedir)\n\t        mat_dir = path.join(filedir,'mat_320')\n\t        basename, ext = path.splitext(filename)\n\t        file_name = '%s-%03d.mat'%(basename,slice_id)\n\t        T1_file_path = path.join(mat_dir,file_name)\n", "        return T1_file_path\n\tclass JiangSudataset(Dataset):\n\t    def __init__(self,  data_dir, transforms, crop_size, challenge, sample_rate=1, mode='train',  pattern='T2',\n\t                 totalsize=None, slice_range=(0, 19)):\n\t        self.transform = transforms\n\t        self.data_dir = data_dir\n\t        self.img_size = crop_size\n\t        self.examples = []\n\t        self.recons_key = (\n\t            \"reconstruction_esc\" if challenge == \"singlecoil\" else \"reconstruction_rss\"\n", "        )\n\t        #make an image id's list\n\t        if mode == 'train':\n\t            f = open(path.join(str(data_dir), 'jiangsu_train.txt'),'r')\n\t        elif mode == 'val':\n\t            f = open(path.join(str(data_dir), 'jiangsu_val.txt'),'r')\n\t        elif mode == 'test':\n\t            f = open(path.join(str(data_dir), 'jiangsu_test.txt'),'r')\n\t        else: \n\t            raise ValueError(\"No mode like this, please choose one in ['train', 'val', 'test'].\")\n", "        file_names = f.readlines()\n\t        metadata = {\n\t            'acquisition': pattern,\n\t            'encoding_size': (640, 556, 1),\n\t            'max': 0,\n\t            'norm': 0,\n\t            'padding_left': 0,\n\t            'padding_right': 0,\n\t            'patient_id':'0',\n\t            'recon_size': (320, 320, 1),\n", "        }\n\t        if not pattern == 'T1+T2':\n\t            if pattern == 'T1':\n\t                idx = 0\n\t            elif pattern == 'T2':\n\t                idx = 1\n\t            for file_name in file_names:\n\t                splits = file_name.split()\n\t                for slice_id in range(slice_range[0], slice_range[1]+1):  # 0:19==20\n\t                    self.examples.append((os.path.join(self.data_dir, splits[idx]), slice_id, metadata))\n", "                    if len(self.examples) > totalsize:\n\t                        break\n\t        else:\n\t            for file_name in file_names:\n\t                splits = file_name.split()\n\t                for slice_id in range(slice_range[0], slice_range[1]+1):  # 0:19==20\n\t                    self.examples.append((os.path.join(self.data_dir, splits[0]), slice_id, metadata))\n\t                    self.examples.append((os.path.join(self.data_dir, splits[1]), slice_id, metadata))\n\t                    if len(self.examples) > totalsize[mode]:\n\t                        break\n", "        self.examples = self.examples[:totalsize]\n\t        if sample_rate < 1:\n\t            random.shuffle(self.examples)\n\t            num_examples = round(len(self.examples) * sample_rate)\n\t            self.examples = self.examples[0:num_examples]\n\t    def __len__(self):\n\t        return len(self.examples)\n\t    def __getitem__(self, slice_id):\n\t        fname_nii, slice_idx, metadata =self.examples[slice_id]\n\t        slice_path = self.niipath2matpath(fname_nii, slice_idx)\n", "        image = loadmat(slice_path)['img']\n\t        mask = None\n\t        attrs = metadata\n\t        image = np.rot90(image)\n\t        kspace = self.fft2(image).astype(np.complex64)\n\t        target = image.astype(np.float32)\n\t        if self.transform is None:\n\t            sample = (kspace, mask, target, attrs, fname_nii, slice_idx)\n\t        else:\n\t            sample = self.transform(kspace, mask, target, attrs, fname_nii, slice_idx)\n", "        return sample\n\t    def fft2(self, img):\n\t        return np.fft.fftshift(np.fft.fft2(np.fft.ifftshift(img)))\n\t    def niipath2matpath(self, T1,slice_id):\n\t        filedir, filename = path.split(T1)\n\t        filedir,_ = path.split(filedir)\n\t        mat_dir = path.join(filedir, 'mat320')\n\t        basename, ext = path.splitext(filename)\n\t        # base_name = basename[:-1]\n\t        file_name = '%s-%03d.mat'%(basename, slice_id)\n", "        T1_file_path = path.join(mat_dir, file_name)\n\t        return T1_file_path\n\tdef create_datasets(args, data_root=None, mode='train', sample_rate=1, client_name='fastMRI', pattern='pd',\n\t                    crop_size=None, totalsize=None, slice_range=None):\n\t    assert mode in ['train', 'val', 'test'], 'unknown mode'\n\t    transforms = build_transforms(args, mode, client_name=client_name)\n\t    if client_name == 'JiangSu':\n\t        return JiangSudataset(data_root, transforms=transforms, crop_size=crop_size, challenge=args.DATASET.CHALLENGE,\n\t                            sample_rate=sample_rate, mode=mode, pattern=pattern,\n\t                            totalsize=totalsize, slice_range=slice_range)\n", "    elif client_name == 'lianying':\n\t        return LianYingdataset(data_root, transforms=transforms, crop_size=crop_size, challenge=args.DATASET.CHALLENGE,\n\t                               sample_rate=sample_rate, mode=mode, pattern=pattern,\n\t                               totalsize=totalsize, slice_range=slice_range)\n"]}
{"filename": "util/adam_svd.py", "chunked_list": ["import math\n\timport re\n\tfrom collections import defaultdict\n\timport torch\n\tfrom torch.optim.optimizer import Optimizer\n\tclass AdamSVD(Optimizer):\n\t    r\"\"\"Implements Adam algorithm.\n\t    It has been proposed in `Adam: A Method for Stochastic Optimization`_.\n\t    Arguments:\n\t        params (iterable): iterable of parameters to optimize or dicts defining\n", "            parameter groups\n\t        lr (float, optional): learning rate (default: 1e-3)\n\t        betas (Tuple[float, float], optional): coefficients used for computing\n\t            running averages of gradient and its square (default: (0.9, 0.999))\n\t        eps (float, optional): term added to the denominator to improve\n\t            numerical stability (default: 1e-8)\n\t        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n\t        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n\t            algorithm from the paper `On the Convergence of Adam and Beyond`_\n\t            (default: False)\n", "    .. _Adam\\: A Method for Stochastic Optimization:\n\t        https://arxiv.org/abs/1412.6980\n\t    .. _On the Convergence of Adam and Beyond:\n\t        https://openreview.net/forum?id=ryQu7f-RZ\n\t    \"\"\"\n\t    def __init__(self, params, lr=1e-2, betas=(0.9, 0.999), eps=1e-8, svd=True, thres=600.,\n\t                 weight_decay=0, amsgrad=False, ratio=0.8):\n\t        if not 0.0 <= lr:\n\t            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n\t        if not 0.0 <= eps:\n", "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n\t        if not 0.0 <= betas[0] < 1.0:\n\t            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n\t        if not 0.0 <= betas[1] < 1.0:\n\t            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n\t        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, amsgrad=amsgrad, svd=svd, thres=thres)\n\t        super(AdamSVD, self).__init__(params, defaults)\n\t        self.eigens = defaultdict(dict)\n\t        self.transforms = defaultdict(dict)\n\t        self.ratio = ratio\n", "    def __setstate__(self, state):\n\t        super(AdamSVD, self).__setstate__(state)\n\t        for group in self.param_groups:\n\t            group.setdefault('amsgrad', False)\n\t            group.setdefault('svd', False)\n\t    def step(self, closure=None):\n\t        \"\"\"Performs a single optimization step.\n\t        Arguments:\n\t            closure (callable, optional): A closure that reevaluates the model\n\t                and returns the loss.\n", "        \"\"\"\n\t        loss = None\n\t        if closure is not None:\n\t            loss = closure()\n\t        for group in self.param_groups:\n\t            svd = group['svd']\n\t            for p in group['params']:\n\t                if p.grad is None:\n\t                    continue\n\t                grad = p.grad.data\n", "                if grad.is_sparse:\n\t                    raise RuntimeError('AdamSVD does not support sparse gradients, please consider SparseAdam instead')\n\t                update = self.get_update(group, grad, p)\n\t                if svd and len(self.transforms) > 0:\n\t                    # if len(update.shape) == 4:\n\t                    if len(update.shape) == 3:\n\t                        # the transpose of the manuscript\n\t                        update_ = torch.bmm(update, self.transforms[p]).view_as(update)\n\t                    else:\n\t                        update_ = torch.mm(update, self.transforms[p])\n", "                else:\n\t                    update_ = update\n\t                p.data.add_(update_)\n\t        return loss\n\t    def get_transforms(self):\n\t        for group in self.param_groups:\n\t            svd = group['svd']\n\t            if svd is False:\n\t                continue\n\t            for p in group['params']:\n", "                if p.grad is None:\n\t                    continue\n\t                thres = group['thres']\n\t                temp = []\n\t                for s in range(self.eigens[p]['eigen_value'].shape[0]):\n\t                    ind = self.eigens[p]['eigen_value'][s] <= self.eigens[p]['eigen_value'][s][-1] * thres\n\t                    ind = torch.ones_like(ind)\n\t                    ind[: int(ind.shape[0]*(1.0-self.ratio))] = False\n\t                    # GVV^T\n\t                    # get the columns\n", "                    basis = self.eigens[p]['eigen_vector'][s][:, ind]\n\t                    transform = torch.mm(basis, basis.transpose(1, 0))\n\t                    temp.append(transform/torch.norm(transform))\n\t                self.transforms[p] = torch.stack(temp, dim=0)\n\t                self.transforms[p].detach_()\n\t    def get_eigens(self, fea_in):\n\t        for group in self.param_groups:\n\t            svd = group['svd']\n\t            if svd is False:\n\t                continue\n", "            for idx, p in enumerate(group['params']):\n\t                if p.grad is None:\n\t                    continue\n\t                eigen = self.eigens[p]\n\t                eigen_values, eigen_vectors = [], []\n\t                for s in range(fea_in[idx].shape[0]):\n\t                    _, eigen_value, eigen_vector = torch.svd(fea_in[idx][s], some=False)\n\t                    eigen_values.append(eigen_value)\n\t                    eigen_vectors.append(eigen_vector)\n\t                eigen['eigen_value'] = torch.stack(eigen_values, dim=0)\n", "                eigen['eigen_vector'] = torch.stack(eigen_vectors, dim=0)\n\t    def get_update(self, group, grad, p):\n\t        amsgrad = group['amsgrad']\n\t        state = self.state[p]\n\t        # State initialization\n\t        if len(state) == 0:\n\t            state['step'] = 0\n\t            # Exponential moving average of gradient values\n\t            state['exp_avg'] = torch.zeros_like(p.data)\n\t            # Exponential moving average of squared gradient values\n", "            state['exp_avg_sq'] = torch.zeros_like(p.data)\n\t            if amsgrad:\n\t                # Maintains max of all exp. moving avg. of sq. grad. values\n\t                state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n\t        exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n\t        if amsgrad:\n\t            max_exp_avg_sq = state['max_exp_avg_sq']\n\t        beta1, beta2 = group['betas']\n\t        state['step'] += 1\n\t        if group['weight_decay'] != 0:\n", "            grad.add_(group['weight_decay'], p.data)\n\t        # Decay the first and second moment running average coefficient\n\t        exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\t        exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n\t        if amsgrad:\n\t            # Maintains the maximum of all 2nd moment running avg. till now\n\t            torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n\t            # Use the max. for normalizing running avg. of gradient\n\t            denom = max_exp_avg_sq.sqrt().add_(group['eps'])\n\t        else:\n", "            denom = exp_avg_sq.sqrt().add_(group['eps'])\n\t        bias_correction1 = 1 - beta1 ** state['step']\n\t        bias_correction2 = 1 - beta2 ** state['step']\n\t        step_size = group['lr'] * \\\n\t            math.sqrt(bias_correction2) / bias_correction1\n\t        update = - step_size * exp_avg / denom\n\t        return update\n\tdef init_model_optimizer(self):\n\t    fea_params = [p for n, p in self.model.named_parameters(\n\t    ) if not bool(re.match('last', n)) and 'bn' not in n]\n", "    bn_params = [p for n, p in self.model.named_parameters() if 'bn' in n]\n\t    model_optimizer_arg = {'params': [{'params': fea_params, 'svd': True, 'lr': self.svd_lr,\n\t                                        'thres': self.config['svd_thres']},\n\t                                      {'params': bn_params, 'lr': self.config['bn_lr']}],\n\t                           'lr': self.config['model_lr'],\n\t                           'weight_decay': self.config['model_weight_decay']}\n\t    if self.config['model_optimizer'] in ['SGD', 'RMSprop']:\n\t        model_optimizer_arg['momentum'] = self.config['momentum']\n\t    elif self.config['model_optimizer'] in ['Rprop']:\n\t        model_optimizer_arg.pop('weight_decay')\n", "    elif self.config['model_optimizer'] in ['amsgrad']:\n\t        if self.config['model_optimizer'] == 'amsgrad':\n\t            model_optimizer_arg['amsgrad'] = True\n\t        self.config['model_optimizer'] = 'Adam'\n\t    self.model_optimizer = AdamSVD(**model_optimizer_arg)\n\t    self.model_scheduler = torch.optim.lr_scheduler.MultiStepLR(self.model_optimizer,\n\t                                                                milestones=self.config['schedule'],\n\t                                                                gamma=self.config['gamma'])\n"]}
{"filename": "util/__init__.py", "chunked_list": []}
{"filename": "util/misc.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\t\"\"\"\n\tMisc functions, including distributed helpers.\n\tMostly copy-paste from torchvision references.\n\t\"\"\"\n\timport os\n\timport subprocess\n\timport time\n\tfrom collections import defaultdict, deque\n\timport datetime\n", "import pickle\n\tfrom typing import Optional, List\n\timport torch\n\timport torch.distributed as dist\n\tfrom torch import Tensor\n\t# needed due to empty tensor bug in pytorch and torchvision 0.5\n\timport torchvision\n\tif float(torchvision.__version__.split('.')[1]) < 0.7:\n\t    from torchvision.ops import _new_empty_tensor\n\t    from torchvision.ops.misc import _output_size\n", "class SmoothedValue(object):\n\t    \"\"\"Track a series of values and provide access to smoothed values over a\n\t    window or the global series average.\n\t    \"\"\"\n\t    def __init__(self, window_size=20, fmt=None):\n\t        if fmt is None:\n\t            fmt = \"{median:.4f} ({global_avg:.4f})\"\n\t        self.deque = deque(maxlen=window_size)\n\t        self.total = 0.0\n\t        self.count = 0\n", "        self.fmt = fmt\n\t    def update(self, value, n=1):\n\t        self.deque.append(value)\n\t        self.count += n\n\t        self.total += value * n\n\t    def synchronize_between_processes(self):\n\t        \"\"\"\n\t        Warning: does not synchronize the deque!\n\t        \"\"\"\n\t        if not is_dist_avail_and_initialized():\n", "            return\n\t        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n\t        dist.barrier()\n\t        dist.all_reduce(t)\n\t        t = t.tolist()\n\t        self.count = int(t[0])\n\t        self.total = t[1]\n\t    @property\n\t    def median(self):\n\t        d = torch.tensor(list(self.deque))\n", "        return d.median().item()\n\t    @property\n\t    def avg(self):\n\t        d = torch.tensor(list(self.deque), dtype=torch.float32)\n\t        return d.mean().item()\n\t    @property\n\t    def global_avg(self):\n\t        return self.total / self.count\n\t    @property\n\t    def max(self):\n", "        return max(self.deque)\n\t    @property\n\t    def value(self):\n\t        return self.deque[-1]\n\t    def __str__(self):\n\t        return self.fmt.format(\n\t            median=self.median,\n\t            avg=self.avg,\n\t            global_avg=self.global_avg,\n\t            max=self.max,\n", "            value=self.value)\n\tdef all_gather(data):\n\t    \"\"\"\n\t    Run all_gather on arbitrary picklable data (not necessarily tensors)\n\t    Args:\n\t        data: any picklable object\n\t    Returns:\n\t        list[data]: list of data gathered from each rank\n\t    \"\"\"\n\t    world_size = get_world_size()\n", "    if world_size == 1:\n\t        return [data]\n\t    # serialized to a Tensor\n\t    buffer = pickle.dumps(data)\n\t    storage = torch.ByteStorage.from_buffer(buffer)\n\t    tensor = torch.ByteTensor(storage).to(\"cuda\")\n\t    # obtain Tensor size of each rank\n\t    local_size = torch.tensor([tensor.numel()], device=\"cuda\")\n\t    size_list = [torch.tensor([0], device=\"cuda\") for _ in range(world_size)]\n\t    dist.all_gather(size_list, local_size)\n", "    size_list = [int(size.item()) for size in size_list]\n\t    max_size = max(size_list)\n\t    # receiving Tensor from all ranks\n\t    # we pad the tensor because torch all_gather does not support\n\t    # gathering tensors of different shapes\n\t    tensor_list = []\n\t    for _ in size_list:\n\t        tensor_list.append(torch.empty((max_size,), dtype=torch.uint8, device=\"cuda\"))\n\t    if local_size != max_size:\n\t        padding = torch.empty(size=(max_size - local_size,), dtype=torch.uint8, device=\"cuda\")\n", "        tensor = torch.cat((tensor, padding), dim=0)\n\t    dist.all_gather(tensor_list, tensor)\n\t    data_list = []\n\t    for size, tensor in zip(size_list, tensor_list):\n\t        buffer = tensor.cpu().numpy().tobytes()[:size]\n\t        data_list.append(pickle.loads(buffer))\n\t    return data_list\n\tdef reduce_dict(input_dict, average=True):\n\t    \"\"\"\n\t    Args:\n", "        input_dict (dict): all the values will be reduced\n\t        average (bool): whether to do average or sum\n\t    Reduce the values in the dictionary from all processes so that all processes\n\t    have the averaged results. Returns a dict with the same fields as\n\t    input_dict, after reduction.\n\t    \"\"\"\n\t    world_size = get_world_size()\n\t    if world_size < 2:\n\t        return input_dict\n\t    with torch.no_grad():\n", "        names = []\n\t        values = []\n\t        # sort the keys so that they are consistent across processes\n\t        for k in sorted(input_dict.keys()):\n\t            names.append(k)\n\t            values.append(input_dict[k])\n\t        values = torch.stack(values, dim=0)\n\t        dist.all_reduce(values)\n\t        if average:\n\t            values /= world_size\n", "        reduced_dict = {k: v for k, v in zip(names, values)}\n\t    return reduced_dict\n\tclass MetricLogger(object):\n\t    def __init__(self, delimiter=\"\\t\"):\n\t        self.meters = defaultdict(SmoothedValue)\n\t        self.delimiter = delimiter\n\t    def update(self, **kwargs):\n\t        for k, v in kwargs.items():\n\t            if isinstance(v, torch.Tensor):\n\t                v = v.item()\n", "            assert isinstance(v, (float, int))\n\t            self.meters[k].update(v)\n\t    def __getattr__(self, attr):\n\t        if attr in self.meters:\n\t            return self.meters[attr]\n\t        if attr in self.__dict__:\n\t            return self.__dict__[attr]\n\t        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\t            type(self).__name__, attr))\n\t    def __str__(self):\n", "        loss_str = []\n\t        for name, meter in self.meters.items():\n\t            loss_str.append(\n\t                \"{}: {}\".format(name, str(meter))\n\t            )\n\t        return self.delimiter.join(loss_str)\n\t    def synchronize_between_processes(self):\n\t        for meter in self.meters.values():\n\t            meter.synchronize_between_processes()\n\t    def add_meter(self, name, meter):\n", "        self.meters[name] = meter\n\t    def log_every(self, iterable, print_freq, header=None):\n\t        i = 0\n\t        if not header:\n\t            header = ''\n\t        start_time = time.time()\n\t        end = time.time()\n\t        iter_time = SmoothedValue(fmt='{avg:.4f}')\n\t        data_time = SmoothedValue(fmt='{avg:.4f}')\n\t        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n", "        if torch.cuda.is_available():\n\t            log_msg = self.delimiter.join([\n\t                header,\n\t                '[{0' + space_fmt + '}/{1}]',\n\t                'eta: {eta}',\n\t                '{meters}',\n\t                'time: {time}',\n\t                'data: {data}',\n\t                'max mem: {memory:.0f}'\n\t            ])\n", "        else:\n\t            log_msg = self.delimiter.join([\n\t                header,\n\t                '[{0' + space_fmt + '}/{1}]',\n\t                'eta: {eta}',\n\t                '{meters}',\n\t                'time: {time}',\n\t                'data: {data}'\n\t            ])\n\t        MB = 1024.0 * 1024.0\n", "        for obj in iterable:\n\t            data_time.update(time.time() - end)\n\t            yield obj\n\t            iter_time.update(time.time() - end)\n\t            if i % print_freq == 0 or i == len(iterable) - 1:\n\t                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n\t                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n\t                if torch.cuda.is_available():\n\t                    print(log_msg.format(\n\t                        i, len(iterable), eta=eta_string,\n", "                        meters=str(self),\n\t                        time=str(iter_time), data=str(data_time),\n\t                        memory=torch.cuda.max_memory_allocated() / MB))\n\t                else:\n\t                    print(log_msg.format(\n\t                        i, len(iterable), eta=eta_string,\n\t                        meters=str(self),\n\t                        time=str(iter_time), data=str(data_time)))\n\t            i += 1\n\t            end = time.time()\n", "        total_time = time.time() - start_time\n\t        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t        print('{} Total time: {} ({:.4f} s / it)'.format(\n\t            header, total_time_str, total_time / len(iterable)))\n\tdef get_sha():\n\t    cwd = os.path.dirname(os.path.abspath(__file__))\n\t    def _run(command):\n\t        return subprocess.check_output(command, cwd=cwd).decode('ascii').strip()\n\t    sha = 'N/A'\n\t    diff = \"clean\"\n", "    branch = 'N/A'\n\t    try:\n\t        sha = _run(['git', 'rev-parse', 'HEAD'])\n\t        subprocess.check_output(['git', 'diff'], cwd=cwd)\n\t        diff = _run(['git', 'diff-index', 'HEAD'])\n\t        diff = \"has uncommited changes\" if diff else \"clean\"\n\t        branch = _run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'])\n\t    except Exception:\n\t        pass\n\t    message = f\"sha: {sha}, status: {diff}, branch: {branch}\"\n", "    return message\n\tdef collate_fn(batch):\n\t    batch = list(zip(*batch))\n\t    batch[0] = nested_tensor_from_tensor_list(batch[0])\n\t    return tuple(batch)\n\tdef _max_by_axis(the_list):\n\t    # type: (List[List[int]]) -> List[int]\n\t    maxes = the_list[0]\n\t    for sublist in the_list[1:]:\n\t        for index, item in enumerate(sublist):\n", "            maxes[index] = max(maxes[index], item)\n\t    return maxes\n\tclass NestedTensor(object):\n\t    def __init__(self, tensors, mask: Optional[Tensor]):\n\t        self.tensors = tensors\n\t        self.mask = mask\n\t    def to(self, device):\n\t        # type: (Device) -> NestedTensor # noqa\n\t        cast_tensor = self.tensors.to(device)\n\t        mask = self.mask\n", "        if mask is not None:\n\t            assert mask is not None\n\t            cast_mask = mask.to(device)\n\t        else:\n\t            cast_mask = None\n\t        return NestedTensor(cast_tensor, cast_mask)\n\t    def decompose(self):\n\t        return self.tensors, self.mask\n\t    def __repr__(self):\n\t        return str(self.tensors)\n", "def nested_tensor_from_tensor_list(tensor_list: List[Tensor]):\n\t    # TODO make this more general\n\t    if tensor_list[0].ndim == 3:\n\t        if torchvision._is_tracing():\n\t            # nested_tensor_from_tensor_list() does not export well to ONNX\n\t            # call _onnx_nested_tensor_from_tensor_list() instead\n\t            return _onnx_nested_tensor_from_tensor_list(tensor_list)\n\t        # TODO make it support different-sized images\n\t        max_size = _max_by_axis([list(img.shape) for img in tensor_list])\n\t        # min_size = tuple(min(s) for s in zip(*[img.shape for img in tensor_list]))\n", "        batch_shape = [len(tensor_list)] + max_size\n\t        b, c, h, w = batch_shape\n\t        dtype = tensor_list[0].dtype\n\t        device = tensor_list[0].device\n\t        tensor = torch.zeros(batch_shape, dtype=dtype, device=device)\n\t        mask = torch.ones((b, h, w), dtype=torch.bool, device=device)\n\t        for img, pad_img, m in zip(tensor_list, tensor, mask):\n\t            pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n\t            m[: img.shape[1], :img.shape[2]] = False\n\t    else:\n", "        raise ValueError('not supported')\n\t    return NestedTensor(tensor, mask)\n\t# _onnx_nested_tensor_from_tensor_list() is an implementation of\n\t# nested_tensor_from_tensor_list() that is supported by ONNX tracing.\n\t@torch.jit.unused\n\tdef _onnx_nested_tensor_from_tensor_list(tensor_list: List[Tensor]) -> NestedTensor:\n\t    max_size = []\n\t    for i in range(tensor_list[0].dim()):\n\t        max_size_i = torch.max(torch.stack([img.shape[i] for img in tensor_list]).to(torch.float32)).to(torch.int64)\n\t        max_size.append(max_size_i)\n", "    max_size = tuple(max_size)\n\t    # work around for\n\t    # pad_img[: img.shape[0], : img.shape[1], : img.shape[2]].copy_(img)\n\t    # m[: img.shape[1], :img.shape[2]] = False\n\t    # which is not yet supported in onnx\n\t    padded_imgs = []\n\t    padded_masks = []\n\t    for img in tensor_list:\n\t        padding = [(s1 - s2) for s1, s2 in zip(max_size, tuple(img.shape))]\n\t        padded_img = torch.nn.functional.pad(img, (0, padding[2], 0, padding[1], 0, padding[0]))\n", "        padded_imgs.append(padded_img)\n\t        m = torch.zeros_like(img[0], dtype=torch.int, device=img.device)\n\t        padded_mask = torch.nn.functional.pad(m, (0, padding[2], 0, padding[1]), \"constant\", 1)\n\t        padded_masks.append(padded_mask.to(torch.bool))\n\t    tensor = torch.stack(padded_imgs)\n\t    mask = torch.stack(padded_masks)\n\t    return NestedTensor(tensor, mask=mask)\n\tdef setup_for_distributed(is_master):\n\t    \"\"\"\n\t    This function disables printing when not in master process\n", "    \"\"\"\n\t    import builtins as __builtin__\n\t    builtin_print = __builtin__.print\n\t    def print(*args, **kwargs):\n\t        force = kwargs.pop('force', False)\n\t        if is_master or force:\n\t            builtin_print(*args, **kwargs)\n\t    __builtin__.print = print\n\tdef is_dist_avail_and_initialized():\n\t    if not dist.is_available():\n", "        return False\n\t    if not dist.is_initialized():\n\t        return False\n\t    return True\n\tdef get_world_size():\n\t    if not is_dist_avail_and_initialized():\n\t        return 1\n\t    return dist.get_world_size()\n\tdef get_rank():\n\t    if not is_dist_avail_and_initialized():\n", "        return 0\n\t    return dist.get_rank()\n\tdef is_main_process():\n\t    return get_rank() == 0\n\tdef save_on_master(*args, **kwargs):\n\t    if is_main_process():\n\t        torch.save(*args, **kwargs)\n\tdef init_distributed_mode(args):\n\t    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n\t        args.rank = int(os.environ[\"RANK\"])\n", "        args.world_size = int(os.environ['WORLD_SIZE'])\n\t        args.gpu = int(os.environ['LOCAL_RANK'])\n\t    elif 'SLURM_PROCID' in os.environ:\n\t        args.rank = int(os.environ['SLURM_PROCID'])\n\t        args.gpu = args.rank % torch.cuda.device_count()\n\t    else:\n\t        print('Not using distributed mode')\n\t        args.distributed = False\n\t        return\n\t    args.distributed = True\n", "    torch.cuda.set_device(args.gpu)\n\t    args.dist_backend = 'nccl'\n\t    print('| distributed init (rank {}): {}'.format(\n\t        args.rank, args.dist_url), flush=True)\n\t    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n\t                                         world_size=args.world_size, rank=args.rank)\n\t    torch.distributed.barrier()\n\t    setup_for_distributed(args.rank == 0)\n\t@torch.no_grad()\n\tdef accuracy(output, target, topk=(1,)):\n", "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n\t    if target.numel() == 0:\n\t        return [torch.zeros([], device=output.device)]\n\t    maxk = max(topk)\n\t    batch_size = target.size(0)\n\t    _, pred = output.topk(maxk, 1, True, True)\n\t    pred = pred.t()\n\t    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\t    res = []\n\t    for k in topk:\n", "        correct_k = correct[:k].view(-1).float().sum(0)\n\t        res.append(correct_k.mul_(100.0 / batch_size))\n\t    return res\n\tdef interpolate(input, size=None, scale_factor=None, mode=\"nearest\", align_corners=None):\n\t    # type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -> Tensor\n\t    \"\"\"\n\t    Equivalent to nn.functional.interpolate, but with support for empty batch sizes.\n\t    This will eventually be supported natively by PyTorch, and this\n\t    class can go away.\n\t    \"\"\"\n", "    if float(torchvision.__version__[:3]) < 0.7:\n\t        if input.numel() > 0:\n\t            return torch.nn.functional.interpolate(\n\t                input, size, scale_factor, mode, align_corners\n\t            )\n\t        output_shape = _output_size(2, input, size, scale_factor)\n\t        output_shape = list(input.shape[:-2]) + list(output_shape)\n\t        return _new_empty_tensor(input, output_shape)\n\t    else:\n\t        return torchvision.ops.misc.interpolate(input, size, scale_factor, mode, align_corners)\n"]}
{"filename": "util/metric.py", "chunked_list": ["import numpy as np\n\tfrom skimage.metrics import peak_signal_noise_ratio, structural_similarity\n\tdef nmse(gt, pred):\n\t    \"\"\"Compute Normalized Mean Squared Error (NMSE)\"\"\"\n\t    result = 0\n\t    for idx in range(gt.shape[0]):\n\t        result+=np.linalg.norm(gt[idx] - pred[idx]) ** 2 / np.linalg.norm(gt[idx]) ** 2\n\t    result = result/gt.shape[0]\n\t    return result\n\tdef psnr(gt, pred):\n", "    \"\"\"Compute Peak Signal to Noise Ratio metric (PSNR)\"\"\"\n\t    maxval = gt.max()\n\t    result = 0\n\t    for idx in range(gt.shape[0]):\n\t        result += peak_signal_noise_ratio(gt[idx], pred[idx], data_range = maxval)\n\t    result = result / gt.shape[0]\n\t    return result\n\tdef ssim(gt, pred, maxval=None):\n\t    \"\"\"Compute Structural Similarity Index Metric (SSIM)\"\"\"\n\t    maxval = gt.max() if maxval is None else maxval\n", "    ssim = 0\n\t    for slice_num in range(gt.shape[0]):\n\t        ssim = ssim + structural_similarity(\n\t            gt[slice_num], pred[slice_num], data_range=maxval\n\t        )\n\t    ssim = ssim / gt.shape[0]\n\t    return ssim\n\tclass AverageMeter(object):\n\t    \"\"\"Computes and stores the average and current value.\n\t       Code imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n", "    \"\"\"\n\t    def __init__(self):\n\t        self.reset()\n\t    def reset(self):\n\t        self.val = 0\n\t        self.avg = 0\n\t        self.sum = 0\n\t        self.count = 0\n\t    def update(self, val, n=1):\n\t        self.val = val\n", "        self.sum += val * n\n\t        self.count += n\n\t        self.avg = self.sum / self.count"]}
{"filename": "models/loss.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tclass LossWrapper(nn.Module):\n\t    def __init__(self):\n\t        super(LossWrapper, self).__init__()\n\t        self.l1_loss = nn.L1Loss()\n\t        self.l2_loss = nn.MSELoss()\n\t    def forward(self, outputs, targets):\n\t        l1_loss = self.l1_loss(outputs, targets)\n", "        loss = l1_loss + self.l2_loss(outputs, targets)\n\t        return {'l1_loss': l1_loss, 'loss': loss}\n\tdef Criterion():\n\t    return LossWrapper()"]}
{"filename": "models/__init__.py", "chunked_list": []}
{"filename": "models/vit_models.py", "chunked_list": ["#!/usr/bin/env python3\n\t\"\"\"\n\tViT-related models\n\tNote: models return logits instead of prob\n\t\"\"\"\n\timport torch.nn as nn\n\tfrom models.vit_prompt.swin_transformer import SwinTransformer, PromptedSwinTransformer\n\tfrom models.decode_heads import VisionTransformerUpHead\n\tclass Swin(nn.Module):\n\t    \"\"\"Swin-related model.\"\"\"\n", "    def __init__(self, cfg):\n\t        super(Swin, self).__init__()\n\t        if \"prompt\" in cfg.MODEL.TRANSFER_TYPE:\n\t            prompt_cfg = cfg.MODEL.PROMPT\n\t        else:\n\t            prompt_cfg = None\n\t        if cfg.MODEL.TRANSFER_TYPE != \"end2end\" and \"prompt\" not in cfg.MODEL.TRANSFER_TYPE:\n\t            # linear, cls, tiny-tl, parital, adapter\n\t            self.froze_enc = True\n\t        else:\n", "            # prompt, end2end, cls+prompt\n\t            self.froze_enc = False\n\t        self.build_backbone(prompt_cfg, cfg)\n\t        self.cfg = cfg\n\t        self.setup_head(cfg)\n\t    def build_backbone(self, prompt_cfg, cfg):\n\t        transfer_type = cfg.MODEL.TRANSFER_TYPE\n\t        self.enc, self.feat_dim = build_swin_model(cfg.MODEL.SUBTYPE, cfg.MODEL.INPUTSIZE, prompt_cfg)\n\t        # linear, prompt, cls, cls+prompt, partial_1\n\t        if transfer_type == \"prompt\":\n", "            for k, p in self.enc.named_parameters():\n\t                if \"prompt\" not in k:\n\t                    p.requires_grad = False\n\t    def setup_head(self, cfg):\n\t        self.head = VisionTransformerUpHead(img_size=cfg.MODEL.FINALSIZE, embed_dim=self.feat_dim, norm_cfg={'type': 'BN'},\n\t                                num_conv=cfg.MODEL.HEAD_NUM_CONV)\n\t    def forward(self, x, return_feature=False):\n\t        if self.froze_enc and self.enc.training:\n\t            self.enc.eval()\n\t        x = self.enc(x)  # batch_size x self.feat_dim\n", "        if return_feature:\n\t            return x, x\n\t        x = self.head(x)\n\t        return x\n\tdef build_swin_model(model_type, crop_size, prompt_cfg):\n\t    if prompt_cfg is not None:\n\t        return _build_prompted_swin_model(\n\t            model_type, crop_size, prompt_cfg)\n\t    else:\n\t        return _build_swin_model(model_type, crop_size)\n", "def _build_prompted_swin_model(model_type, crop_size, prompt_cfg):\n\t    if model_type == \"swin_320\":\n\t        model = PromptedSwinTransformer(\n\t            prompt_cfg,\n\t            img_size=crop_size,\n\t            in_chans=1,\n\t            patch_size=8,\n\t            embed_dim=256,\n\t            depths=[22],\n\t            num_heads=[8],\n", "            window_size=10,\n\t            drop_path_rate=0.5,\n\t            num_classes=-1,\n\t        )\n\t        embed_dim = 256\n\t    feat_dim = int(embed_dim)\n\t    return model, feat_dim\n\tdef _build_swin_model(model_type, crop_size):\n\t    if model_type == \"swinb_fastmri_320\":\n\t        model = SwinTransformer(\n", "            img_size=crop_size,\n\t            in_chans=1,\n\t            patch_size=8,\n\t            embed_dim=256,\n\t            depths= [22],\n\t            num_heads= [8],\n\t            window_size=10,\n\t            drop_path_rate=0.5,\n\t            num_classes=-1,\n\t        )\n", "        embed_dim = 256\n\t        feat_dim = int(embed_dim)\n\t    return model, feat_dim"]}
{"filename": "models/decode_heads/decode_head.py", "chunked_list": ["from abc import ABCMeta, abstractmethod\n\timport torch.nn.functional as F\n\timport torch\n\timport torch.nn as nn\n\tfrom mmcv.cnn import normal_init\n\tfrom mmcv.runner import auto_fp16\n\timport warnings\n\tdef resize(input,\n\t           size=None,\n\t           scale_factor=None,\n", "           mode='nearest',\n\t           align_corners=None,\n\t           warning=True):\n\t    if warning:\n\t        if size is not None and align_corners:\n\t            input_h, input_w = tuple(int(x) for x in input.shape[2:])\n\t            output_h, output_w = tuple(int(x) for x in size)\n\t            if output_h > input_h or output_w > output_h:\n\t                if ((output_h > 1 and output_w > 1 and input_h > 1\n\t                     and input_w > 1) and (output_h - 1) % (input_h - 1)\n", "                        and (output_w - 1) % (input_w - 1)):\n\t                    warnings.warn(\n\t                        f'When align_corners={align_corners}, '\n\t                        'the output would more aligned if '\n\t                        f'input size {(input_h, input_w)} is `x+1` and '\n\t                        f'out size {(output_h, output_w)} is `nx+1`')\n\t    if isinstance(size, torch.Size):\n\t        size = tuple(int(x) for x in size)\n\t    return F.interpolate(input, size, scale_factor, mode, align_corners)\n\tclass BaseDecodeHead(nn.Module, metaclass=ABCMeta):\n", "    \"\"\"Base class for BaseDecodeHead.\n\t    Args:\n\t        in_channels (int|Sequence[int]): Input channels.\n\t        channels (int): Channels after modules, before conv_seg.\n\t        num_classes (int): Number of classes.\n\t        dropout_ratio (float): Ratio of dropout layer. Default: 0.1.\n\t        conv_cfg (dict|None): Config of conv layers. Default: None.\n\t        norm_cfg (dict|None): Config of norm layers. Default: None.\n\t        act_cfg (dict): Config of activation layers.\n\t            Default: dict(type='ReLU')\n", "        in_index (int|Sequence[int]): Input feature index. Default: -1\n\t        input_transform (str|None): Transformation type of input features.\n\t            Options: 'resize_concat', 'multiple_select', None.\n\t            'resize_concat': Multiple feature maps will be resize to the\n\t                same size as first one and than concat together.\n\t                Usually used in FCN head of HRNet.\n\t            'multiple_select': Multiple feature maps will be bundle into\n\t                a list and passed into decode head.\n\t            None: Only one select feature map is allowed.\n\t            Default: None.\n", "        loss_decode (dict): Config of decode loss.\n\t            Default: dict(type='CrossEntropyLoss').\n\t        ignore_index (int): The label index to be ignored. Default: 255\n\t        sampler (dict|None): The config of segmentation map sampler.\n\t            Default: None.\n\t        align_corners (bool): align_corners argument of F.interpolate.\n\t            Default: False.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 in_channels,\n", "                 channels,\n\t                 *,\n\t                 num_classes,\n\t                 dropout_ratio=0.1,\n\t                 conv_cfg=None,\n\t                 norm_cfg=None,\n\t                 act_cfg=dict(type='ReLU'),\n\t                 in_index=-1,\n\t                 input_transform=None,\n\t                 loss_decode=dict(\n", "                     type='CrossEntropyLoss',\n\t                     use_sigmoid=False,\n\t                     loss_weight=1.0),\n\t                 ignore_index=255,\n\t                 sampler=None,\n\t                 align_corners=False):\n\t        super(BaseDecodeHead, self).__init__()\n\t        self._init_inputs(in_channels, in_index, input_transform)\n\t        self.channels = channels\n\t        self.num_classes = num_classes\n", "        self.dropout_ratio = dropout_ratio\n\t        self.conv_cfg = conv_cfg\n\t        self.norm_cfg = norm_cfg\n\t        self.act_cfg = act_cfg\n\t        self.in_index = in_index\n\t        self.ignore_index = ignore_index\n\t        self.align_corners = align_corners\n\t        self.sampler = None\n\t        self.conv_seg = nn.Conv2d(channels, num_classes, kernel_size=1)\n\t        if dropout_ratio > 0:\n", "            self.dropout = nn.Dropout2d(dropout_ratio)\n\t        else:\n\t            self.dropout = None\n\t        self.fp16_enabled = False\n\t    def extra_repr(self):\n\t        \"\"\"Extra repr.\"\"\"\n\t        s = f'input_transform={self.input_transform}, ' \\\n\t            f'ignore_index={self.ignore_index}, ' \\\n\t            f'align_corners={self.align_corners}'\n\t        return s\n", "    def _init_inputs(self, in_channels, in_index, input_transform):\n\t        \"\"\"Check and initialize input transforms.\n\t        The in_channels, in_index and input_transform must match.\n\t        Specifically, when input_transform is None, only single feature map\n\t        will be selected. So in_channels and in_index must be of type int.\n\t        When input_transform\n\t        Args:\n\t            in_channels (int|Sequence[int]): Input channels.\n\t            in_index (int|Sequence[int]): Input feature index.\n\t            input_transform (str|None): Transformation type of input features.\n", "                Options: 'resize_concat', 'multiple_select', None.\n\t                'resize_concat': Multiple feature maps will be resize to the\n\t                    same size as first one and than concat together.\n\t                    Usually used in FCN head of HRNet.\n\t                'multiple_select': Multiple feature maps will be bundle into\n\t                    a list and passed into decode head.\n\t                None: Only one select feature map is allowed.\n\t        \"\"\"\n\t        if input_transform is not None:\n\t            assert input_transform in ['resize_concat', 'multiple_select']\n", "        self.input_transform = input_transform\n\t        self.in_index = in_index\n\t        if input_transform is not None:\n\t            assert isinstance(in_channels, (list, tuple))\n\t            assert isinstance(in_index, (list, tuple))\n\t            assert len(in_channels) == len(in_index)\n\t            if input_transform == 'resize_concat':\n\t                self.in_channels = sum(in_channels)\n\t            else:\n\t                self.in_channels = in_channels\n", "        else:\n\t            assert isinstance(in_channels, int)\n\t            assert isinstance(in_index, int)\n\t            self.in_channels = in_channels\n\t    def init_weights(self):\n\t        \"\"\"Initialize weights of classification layer.\"\"\"\n\t        normal_init(self.conv_seg, mean=0, std=0.01)\n\t    def _transform_inputs(self, inputs):\n\t        \"\"\"Transform inputs for decoder.\n\t        Args:\n", "            inputs (list[Tensor]): List of multi-level img features.\n\t        Returns:\n\t            Tensor: The transformed inputs\n\t        \"\"\"\n\t        if self.input_transform == 'resize_concat':\n\t            inputs = [inputs[i] for i in self.in_index]\n\t            upsampled_inputs = [\n\t                resize(\n\t                    input=x,\n\t                    size=inputs[0].shape[2:],\n", "                    mode='bilinear',\n\t                    align_corners=self.align_corners) for x in inputs\n\t            ]\n\t            inputs = torch.cat(upsampled_inputs, dim=1)\n\t        elif self.input_transform == 'multiple_select':\n\t            inputs = [inputs[i] for i in self.in_index]\n\t        else:\n\t            inputs = inputs[self.in_index]\n\t        return inputs\n\t    @auto_fp16()\n", "    @abstractmethod\n\t    def forward(self, inputs):\n\t        \"\"\"Placeholder of forward function.\"\"\"\n\t        pass\n\t    def forward_train(self, inputs, img_metas, gt_semantic_seg, train_cfg):\n\t        \"\"\"Forward function for training.\n\t        Args:\n\t            inputs (list[Tensor]): List of multi-level img features.\n\t            img_metas (list[dict]): List of image info dict where each dict\n\t                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n", "                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n\t                For details on the values of these keys see\n\t                `mmseg/datasets/pipelines/formatting.py:Collect`.\n\t            gt_semantic_seg (Tensor): Semantic segmentation masks\n\t                used if the architecture supports semantic segmentation task.\n\t            train_cfg (dict): The training config.\n\t        Returns:\n\t            dict[str, Tensor]: a dictionary of loss components\n\t        \"\"\"\n\t        seg_logits = self.forward(inputs)\n", "        losses = self.losses(seg_logits, gt_semantic_seg)\n\t        return losses\n\t    def forward_test(self, inputs, img_metas, test_cfg):\n\t        \"\"\"Forward function for testing.\n\t        Args:\n\t            inputs (list[Tensor]): List of multi-level img features.\n\t            img_metas (list[dict]): List of image info dict where each dict\n\t                has: 'img_shape', 'scale_factor', 'flip', and may also contain\n\t                'filename', 'ori_shape', 'pad_shape', and 'img_norm_cfg'.\n\t                For details on the values of these keys see\n", "                `mmseg/datasets/pipelines/formatting.py:Collect`.\n\t            test_cfg (dict): The testing config.\n\t        Returns:\n\t            Tensor: Output segmentation map.\n\t        \"\"\"\n\t        return self.forward(inputs)\n\t    def cls_seg(self, feat):\n\t        \"\"\"Classify each pixel.\"\"\"\n\t        if self.dropout is not None:\n\t            feat = self.dropout(feat)\n", "        output = self.conv_seg(feat)\n\t        return output\n"]}
{"filename": "models/decode_heads/__init__.py", "chunked_list": ["from .vit_up_head import VisionTransformerUpHead"]}
{"filename": "models/decode_heads/vit_up_head.py", "chunked_list": ["import torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom functools import partial\n\timport math\n\timport torch\n\t# from .layers import trunc_normal_\n\timport warnings\n\tfrom models.decode_heads.decode_head import BaseDecodeHead\n\tfrom mmcv.cnn import build_norm_layer\n\tdef _no_grad_trunc_normal_(tensor, mean, std, a, b):\n", "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n\t    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n\t    def norm_cdf(x):\n\t        # Computes standard normal cumulative distribution function\n\t        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n\t    if (mean < a - 2 * std) or (mean > b + 2 * std):\n\t        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n\t                      \"The distribution of values may be incorrect.\",\n\t                      stacklevel=2)\n\t    with torch.no_grad():\n", "        # Values are generated by using a truncated uniform distribution and\n\t        # then using the inverse CDF for the normal distribution.\n\t        # Get upper and lower cdf values\n\t        l = norm_cdf((a - mean) / std)\n\t        u = norm_cdf((b - mean) / std)\n\t        # Uniformly fill tensor with values from [l, u], then translate to\n\t        # [2l-1, 2u-1].\n\t        tensor.uniform_(2 * l - 1, 2 * u - 1)\n\t        # Use inverse cdf transform for normal distribution to get truncated\n\t        # standard normal\n", "        tensor.erfinv_()\n\t        # Transform to proper mean, std\n\t        tensor.mul_(std * math.sqrt(2.))\n\t        tensor.add_(mean)\n\t        # Clamp to ensure it's in the proper range\n\t        tensor.clamp_(min=a, max=b)\n\t        return tensor\n\tdef trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n\t    # type: (Tensor, float, float, float, float) -> Tensor\n\t    r\"\"\"Fills the input Tensor with values drawn from a truncated\n", "    normal distribution. The values are effectively drawn from the\n\t    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n\t    with values outside :math:`[a, b]` redrawn until they are within\n\t    the bounds. The method used for generating the random values works\n\t    best when :math:`a \\leq \\text{mean} \\leq b`.\n\t    Args:\n\t        tensor: an n-dimensional `torch.Tensor`\n\t        mean: the mean of the normal distribution\n\t        std: the standard deviation of the normal distribution\n\t        a: the minimum cutoff value\n", "        b: the maximum cutoff value\n\t    Examples:\n\t        >>> w = torch.empty(3, 5)\n\t        >>> nn.init.trunc_normal_(w)\n\t    \"\"\"\n\t    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n\tclass bilinear_VisionTransformerUpHead(BaseDecodeHead):\n\t    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\t    \"\"\"\n\t    def __init__(self, img_size=224, embed_dim=768,\n", "                 norm_layer=partial(nn.LayerNorm, eps=1e-6), norm_cfg=None,\n\t                 num_conv=4, upsampling_method='bilinear', num_upsampe_layer=4, conv3x3_conv1x1=True, **kwargs):\n\t        super(VisionTransformerUpHead, self).__init__(img_size, embed_dim, num_classes=1, **kwargs)\n\t        self.img_size = img_size\n\t        self.norm_cfg = norm_cfg\n\t        self.num_conv = num_conv\n\t        self.norm = norm_layer(embed_dim)\n\t        self.upsampling_method = upsampling_method\n\t        self.num_upsampe_layer = num_upsampe_layer\n\t        self.conv3x3_conv1x1 = conv3x3_conv1x1\n", "        out_channel = self.num_classes\n\t        if self.num_conv == 2:\n\t            if self.conv3x3_conv1x1:\n\t                self.conv_0 = nn.Conv2d(\n\t                    embed_dim, 256, kernel_size=3, stride=1, padding=1)\n\t            else:\n\t                self.conv_0 = nn.Conv2d(embed_dim, 256, 1, 1)\n\t            self.conv_1 = nn.Conv2d(256, out_channel, 1, 1)\n\t            _, self.syncbn_fc_0 = build_norm_layer(self.norm_cfg, 256)\n\t        elif self.num_conv == 3:\n", "            self.conv_0 = nn.Conv2d(\n\t                embed_dim, 256, kernel_size=3, stride=1, padding=1)\n\t            self.conv_1 = nn.Conv2d(\n\t                256, 128, kernel_size=3, stride=1, padding=1)\n\t            self.conv_2 = nn.Conv2d(\n\t                128, 64, kernel_size=3, stride=1, padding=1)\n\t            self.conv_3 = nn.Conv2d(\n\t                64, out_channel, kernel_size=3, stride=1, padding=1)\n\t            _, self.syncbn_fc_0 = build_norm_layer(self.norm_cfg, 256)\n\t            _, self.syncbn_fc_1 = build_norm_layer(self.norm_cfg, 128)\n", "            _, self.syncbn_fc_2 = build_norm_layer(self.norm_cfg, 64)\n\t        elif self.num_conv == 4:\n\t            self.conv_0 = nn.Conv2d(\n\t                embed_dim, 256, kernel_size=3, stride=1, padding=1)\n\t            self.conv_1 = nn.Conv2d(\n\t                256, 128, kernel_size=3, stride=1, padding=1)\n\t            self.conv_2 = nn.Conv2d(\n\t                128, 64, kernel_size=3, stride=1, padding=1)\n\t            self.conv_3 = nn.Conv2d(\n\t                64, 16, kernel_size=3, stride=1, padding=1)\n", "            self.conv_4 = nn.Conv2d(16, out_channel, kernel_size=1, stride=1)\n\t            self.conv_0 = nn.ConvTranspose2d(embed_dim, 256, 2, 2, 1)\n\t            _, self.syncbn_fc_0 = build_norm_layer(self.norm_cfg, 256)\n\t            _, self.syncbn_fc_1 = build_norm_layer(self.norm_cfg, 128)\n\t            _, self.syncbn_fc_2 = build_norm_layer(self.norm_cfg, 64)\n\t            _, self.syncbn_fc_3 = build_norm_layer(self.norm_cfg, 16)\n\t        # Segmentation head\n\t    def init_weights(self):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Linear):\n", "                trunc_normal_(m.weight, std=.02)\n\t                if isinstance(m, nn.Linear) and m.bias is not None:\n\t                    nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.LayerNorm):\n\t                nn.init.constant_(m.bias, 0)\n\t                nn.init.constant_(m.weight, 1.0)\n\t    def forward(self, x):\n\t        # x = self._transform_inputs(x)\n\t        if x.dim() == 3:\n\t            # if x.shape[1] % 48 != 0:\n", "            #     x = x[:, 1:]\n\t            x = self.norm(x)\n\t        if self.upsampling_method == 'bilinear':\n\t            if x.dim() == 3:\n\t                n, hw, c = x.shape\n\t                h = w = int(math.sqrt(hw))\n\t                x = x.transpose(1, 2).reshape(n, c, h, w)\n\t            if self.num_conv == 2:\n\t                if self.num_upsampe_layer == 2:\n\t                    x = self.conv_0(x)\n", "                    x = self.syncbn_fc_0(x)\n\t                    x = F.relu(x, inplace=True)\n\t                    x = F.interpolate(\n\t                        x, size=x.shape[-1] * 4, mode='bilinear', align_corners=self.align_corners)\n\t                    x = self.conv_1(x)\n\t                    x = F.interpolate(\n\t                        x, size=self.img_size, mode='bilinear', align_corners=self.align_corners)\n\t                elif self.num_upsampe_layer == 1:\n\t                    x = self.conv_0(x)\n\t                    x = self.syncbn_fc_0(x)\n", "                    x = F.relu(x, inplace=True)\n\t                    x = self.conv_1(x)\n\t                    x = F.interpolate(\n\t                        x, size=self.img_size, mode='bilinear', align_corners=self.align_corners)\n\t            elif self.num_conv == 3:\n\t                x = self.conv_0(x)\n\t                x = self.syncbn_fc_0(x)\n\t                x = F.relu(x, inplace=True)\n\t                x = F.interpolate(\n\t                    x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n", "                x = self.conv_1(x)\n\t                x = self.syncbn_fc_1(x)\n\t                x = F.relu(x, inplace=True)\n\t                x = F.interpolate(\n\t                    x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t                x = self.conv_2(x)\n\t                x = self.syncbn_fc_2(x)\n\t                x = F.relu(x, inplace=True)\n\t                x = F.interpolate(\n\t                    x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n", "                x = self.conv_3(x)\n\t                x = F.interpolate(\n\t                    x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t            elif self.num_conv == 4:\n\t                if self.num_upsampe_layer == 4:\n\t                    x = self.conv_0(x)\n\t                    x = self.syncbn_fc_0(x)\n\t                    x = F.relu(x, inplace=True)\n\t                    x = F.interpolate(\n\t                        x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n", "                    x = self.conv_1(x)\n\t                    x = self.syncbn_fc_1(x)\n\t                    x = F.relu(x, inplace=True)\n\t                    x = F.interpolate(\n\t                        x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t                    x = self.conv_2(x)\n\t                    x = self.syncbn_fc_2(x)\n\t                    x = F.relu(x, inplace=True)\n\t                    x = F.interpolate(\n\t                        x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n", "                    x = self.conv_3(x)\n\t                    x = self.syncbn_fc_3(x)\n\t                    x = F.relu(x, inplace=True)\n\t                    x = F.interpolate(\n\t                        x, size=x.shape[-1]*2, mode='bilinear', align_corners=self.align_corners)\n\t                    x = self.conv_4(x)\n\t                    x = F.interpolate(\n\t                        x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t        return x\n\tclass VisionTransformerUpHead(BaseDecodeHead):\n", "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n\t    \"\"\"\n\t    def __init__(self, img_size=320, embed_dim=1024,\n\t                 norm_layer=partial(nn.LayerNorm, eps=1e-6), norm_cfg=None,\n\t                 num_conv=4, upsampling_method='bilinear', conv3x3_conv1x1=True, **kwargs):\n\t        super(VisionTransformerUpHead, self).__init__(img_size, embed_dim, num_classes=1, **kwargs)\n\t        self.img_size = img_size\n\t        self.norm_cfg = norm_cfg\n\t        self.num_conv = num_conv\n\t        self.norm = norm_layer(embed_dim)\n", "        self.upsampling_method = upsampling_method\n\t        self.conv3x3_conv1x1 = conv3x3_conv1x1\n\t        self.inter_size = 20\n\t        out_channel = self.num_classes\n\t        if self.num_conv == 4:\n\t            self.conv_0 = nn.Conv2d(embed_dim, 256, 3, 1, 1)\n\t            self.conv_1 = nn.Conv2d(256, 128,  3, 1, 1)\n\t            self.conv_2 = nn.Conv2d(128, 64, 3, 1, 1)\n\t            self.conv_3 = nn.Conv2d(64, 16, 3, 1, 1)\n\t            self.conv_4 = nn.Conv2d(16, 1, 3, 1, 1)\n", "            _, self.syncbn_fc_0 = build_norm_layer(self.norm_cfg, 256)\n\t            _, self.syncbn_fc_1 = build_norm_layer(self.norm_cfg, 128)\n\t            _, self.syncbn_fc_2 = build_norm_layer(self.norm_cfg, 64)\n\t            _, self.syncbn_fc_3 = build_norm_layer(self.norm_cfg, 16)\n\t        elif self.num_conv == 5:\n\t            self.conv_0 = nn.Conv2d(embed_dim, 256, 3, 1, 1)\n\t            self.conv_1 = nn.Conv2d(256, 128, 3, 1, 1)\n\t            self.conv_2 = nn.Conv2d(128, 64, 3, 1, 1)\n\t            self.conv_3 = nn.Conv2d(64,  16, 3, 1, 1)\n\t            self.conv_4 = nn.Conv2d(16, 16, 3, 1, 1)\n", "            self.conv_5 = nn.Conv2d(16, 1, 3, 1, 1)\n\t            _, self.syncbn_fc_0 = build_norm_layer(self.norm_cfg, 256)\n\t            _, self.syncbn_fc_1 = build_norm_layer(self.norm_cfg, 128)\n\t            _, self.syncbn_fc_2 = build_norm_layer(self.norm_cfg, 64)\n\t            _, self.syncbn_fc_3 = build_norm_layer(self.norm_cfg, 16)\n\t            _, self.syncbn_fc_4 = build_norm_layer(self.norm_cfg, 16)\n\t        # Segmentation head\n\t    def init_weights(self):\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Linear):\n", "                trunc_normal_(m.weight, std=.02)\n\t                if isinstance(m, nn.Linear) and m.bias is not None:\n\t                    nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.LayerNorm):\n\t                nn.init.constant_(m.bias, 0)\n\t                nn.init.constant_(m.weight, 1.0)\n\t    def forward(self, x):\n\t        if x.dim() == 3:\n\t            n, hw, c = x.shape\n\t            h = w = int(math.sqrt(hw))\n", "            x = x.transpose(1, 2).reshape(n, c, h, w)\n\t        if self.num_conv == 4:\n\t            x = self.conv_0(x)\n\t            x = self.syncbn_fc_0(x)\n\t            x = F.relu(x, inplace=True)\n\t            x = self.conv_1(x)\n\t            x = self.syncbn_fc_1(x)\n\t            x = F.relu(x, inplace=True)\n\t            x = F.interpolate(\n\t                x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n", "            x = self.conv_2(x)\n\t            x = self.syncbn_fc_2(x)\n\t            x = F.relu(x, inplace=True)\n\t            x = F.interpolate(\n\t                x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t            x = self.conv_3(x)\n\t            x = self.syncbn_fc_3(x)\n\t            x = F.relu(x, inplace=True)\n\t            x = F.interpolate(\n\t                x, size=self.img_size, mode='bilinear', align_corners=self.align_corners)\n", "            out = self.conv_4(x)\n\t        elif self.num_conv == 5:\n\t            x = self.conv_0(x)\n\t            x = self.syncbn_fc_0(x)\n\t            x = F.relu(x, inplace=True)\n\t            # x = F.interpolate(\n\t            #     x, size=self.inter_size, mode='bilinear', align_corners=self.align_corners)\n\t            x = self.conv_1(x)\n\t            x = self.syncbn_fc_1(x)\n\t            x = F.relu(x, inplace=True)\n", "            x = F.interpolate(\n\t                x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t            x = self.conv_2(x)\n\t            x = self.syncbn_fc_2(x)\n\t            x = F.relu(x, inplace=True)\n\t            x = F.interpolate(\n\t                x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t            x = self.conv_3(x)\n\t            x = self.syncbn_fc_3(x)\n\t            x = F.relu(x, inplace=True)\n", "            x = F.interpolate(\n\t                x, size=x.shape[-1]*2, mode='bilinear', align_corners=self.align_corners)\n\t            x = self.conv_4(x)\n\t            x = self.syncbn_fc_4(x)\n\t            x = F.relu(x, inplace=True)\n\t            x = F.interpolate(\n\t                x, size=x.shape[-1] * 2, mode='bilinear', align_corners=self.align_corners)\n\t            out = self.conv_5(x)\n\t        else:\n\t            raise ValueError('num_conv error')\n", "        return out\n"]}
{"filename": "models/vit_prompt/swin_transformer.py", "chunked_list": ["#!/usr/bin/env python3\n\t\"\"\"\n\tswin transformer with prompt\n\t\"\"\"\n\timport math\n\timport torch\n\timport torch.nn as nn\n\timport torchvision as tv\n\tfrom collections import defaultdict\n\tfrom functools import reduce\n", "from operator import mul\n\tfrom torch.nn import Conv2d, Dropout\n\tfrom timm.models.layers import to_2tuple\n\tfrom .swin_backbone import (\n\t    BasicLayer, PatchMerging, SwinTransformer, SwinTransformerBlock,\n\t    window_partition, window_reverse, WindowAttention)\n\tclass PromptedSwinTransformer(SwinTransformer):\n\t    def __init__(\n\t        self, prompt_config, img_size=224, patch_size=4, in_chans=3,\n\t        num_classes=1000, embed_dim=96, depths=[2, 2, 6, 2],\n", "        num_heads=[3, 6, 12, 24], window_size=7, mlp_ratio=4., qkv_bias=True,\n\t        qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n\t        norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n\t        use_checkpoint=False, **kwargs\n\t    ):\n\t        if prompt_config.LOCATION == \"pad\":\n\t            img_size += 2 * prompt_config.NUM_TOKENS\n\t        super(PromptedSwinTransformer, self).__init__(\n\t            img_size, patch_size, in_chans, num_classes, embed_dim, depths,\n\t            num_heads, window_size, mlp_ratio, qkv_bias, qk_scale, drop_rate,\n", "            attn_drop_rate, drop_path_rate, norm_layer, ape, patch_norm,\n\t            use_checkpoint, **kwargs\n\t        )\n\t        self.prompt_config = prompt_config\n\t        img_size = to_2tuple(img_size)\n\t        patch_size = to_2tuple(patch_size)\n\t        if self.prompt_config.LOCATION == \"add\":\n\t            num_tokens = self.embeddings.position_embeddings.shape[1]\n\t        elif self.prompt_config.LOCATION == \"add-1\":\n\t            num_tokens = 1\n", "        else:\n\t            num_tokens = self.prompt_config.NUM_TOKENS\n\t        self.prompt_dropout = Dropout(self.prompt_config.DROPOUT)\n\t        # if project the prompt embeddings\n\t        if self.prompt_config.PROJECT > -1:\n\t            # only for prepend / add\n\t            prompt_dim = self.prompt_config.PROJECT\n\t            self.prompt_proj = nn.Linear(prompt_dim, embed_dim)\n\t            nn.init.kaiming_normal_(self.prompt_proj.weight, a=0, mode='fan_out')\n\t        else:\n", "            self.prompt_proj = nn.Identity()\n\t        # build layers\n\t        # stochastic depth\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\t        self.layers = nn.ModuleList()\n\t        for i_layer in range(self.num_layers):\n\t            layer = BasicLayer(\n\t                dim=int(embed_dim),\n\t                input_resolution=(\n\t                    self.patches_resolution[0],\n", "                    self.patches_resolution[1]\n\t                ),\n\t                depth=depths[i_layer],\n\t                num_heads=num_heads[i_layer],\n\t                window_size=window_size,\n\t                mlp_ratio=self.mlp_ratio,\n\t                qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t                drop=drop_rate, attn_drop=attn_drop_rate,\n\t                drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n\t                norm_layer=norm_layer,\n", "                block_module=PromptedSwinTransformerBlock,\n\t                downsample= None,\n\t                use_checkpoint=use_checkpoint,\n\t                num_prompts=num_tokens,\n\t                prompt_location=self.prompt_config.LOCATION,\n\t                deep_prompt=self.prompt_config.DEEP\n\t            )\n\t            self.layers.append(layer)\n\t        if self.prompt_config.INITIATION == \"random\":\n\t            val = math.sqrt(6. / float(3 * reduce(mul, patch_size, 1) + embed_dim))  # noqa\n", "            if self.prompt_config.LOCATION == \"below\":\n\t                self.patch_embed.proj = Conv2d(\n\t                    in_channels=num_tokens+3,\n\t                    out_channels=embed_dim,\n\t                    kernel_size=patch_size,\n\t                    stride=patch_size\n\t                )\n\t                # add xavier_uniform initialization\n\t                nn.init.uniform_(self.patch_embed.proj.weight, -val, val)\n\t                nn.init.zeros_(self.patch_embed.proj.bias)\n", "                self.prompt_embeddings = nn.Parameter(torch.zeros(\n\t                    1, num_tokens, img_size[0], img_size[1]))\n\t                nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n\t            elif self.prompt_config.LOCATION == \"pad\":\n\t                self.prompt_embeddings_tb = nn.Parameter(torch.zeros(\n\t                    1, 3, 2 * num_tokens, img_size[0]\n\t                ))\n\t                self.prompt_embeddings_lr = nn.Parameter(torch.zeros(\n\t                    1, 3, img_size[0] - 2 * num_tokens, 2 * num_tokens\n\t                ))\n", "                nn.init.uniform_(self.prompt_embeddings_tb.data, 0.0, 1.0)\n\t                nn.init.uniform_(self.prompt_embeddings_lr.data, 0.0, 1.0)\n\t                self.prompt_norm = tv.transforms.Normalize(\n\t                    mean=[0.485, 0.456, 0.406],\n\t                    std=[0.229, 0.224, 0.225],\n\t                )\n\t            else:\n\t                # for \"prepend\"\n\t                # self.prompter = nn.ParameterList()\n\t                # self.prompt_embeddings = nn.Parameter(torch.zeros(1, num_tokens, embed_dim))\n", "                # nn.init.uniform_(self.prompt_embeddings.data, -val, val)\n\t                # self.prompter.append(self.prompt_embeddings)\n\t                # for \"prepend\"\n\t                self.prompter = nn.ParameterDict()\n\t                prompt0 = nn.Parameter(torch.zeros(1, num_tokens, embed_dim))\n\t                # nn.init.uniform_(prompt0.data, -val, val)\n\t                nn.init.uniform_(prompt0.data)\n\t                self.prompter[f'prompt_{0}'] = prompt0\n\t                if self.prompt_config.DEEP:\n\t                    for i in range(1, sum(depths)):\n", "                        temp = nn.Parameter(torch.zeros(1, num_tokens, embed_dim))\n\t                        # nn.init.uniform_(temp.data, -val, val)\n\t                        nn.init.uniform_(temp.data)\n\t                        self.prompter[f'prompt_{i}'] = temp\n\t        else:\n\t            raise ValueError(\"Other initiation scheme is not supported\")\n\t    def incorporate_prompt(self, x):\n\t        # combine prompt embeddings with image-patch embeddings\n\t        B = x.shape[0]\n\t        if self.prompt_config.LOCATION == \"prepend\":\n", "            # after CLS token, all before image patches\n\t            x = self.get_patch_embeddings(x)  # (batch_size, n_patches, hidden_dim)\n\t            prompt_embd = self.prompt_dropout(self.prompter['prompt_0'].expand(B, -1, -1))\n\t            x = torch.cat((prompt_embd, x), dim=1)\n\t            # (batch_size, n_prompt + n_patches, hidden_dim)\n\t        elif self.prompt_config.LOCATION == \"add\":\n\t            # add to the input patches + CLS\n\t            # assert self.prompt_config.NUM_TOKENS == x.shape[1]\n\t            x = self.get_patch_embeddings(x)  # (batch_size, 1 + n_patches, hidden_dim)\n\t            x = x + self.prompt_dropout(\n", "                self.prompt_embeddings.expand(B, -1, -1))\n\t            # (batch_size, n_patches, hidden_dim)\n\t        elif self.prompt_config.LOCATION == \"add-1\":\n\t            x = self.get_patch_embeddings(x)  # (batch_size, 1 + n_patches, hidden_dim)\n\t            L = x.shape[1]\n\t            prompt_emb = self.prompt_dropout(\n\t                self.prompt_embeddings.expand(B, -1, -1))\n\t            x = x + prompt_emb.expand(-1, L, -1)\n\t            # (batch_size, cls_token + n_patches, hidden_dim)\n\t        elif self.prompt_config.LOCATION == \"pad\":\n", "            prompt_emb_lr = self.prompt_norm(\n\t                self.prompt_embeddings_lr).expand(B, -1, -1, -1)\n\t            prompt_emb_tb = self.prompt_norm(\n\t                self.prompt_embeddings_tb).expand(B, -1, -1, -1)\n\t            x = torch.cat((\n\t                prompt_emb_lr[:, :, :, :self.num_tokens],\n\t                x, prompt_emb_lr[:, :, :, self.num_tokens:]\n\t                ), dim=-1)\n\t            x = torch.cat((\n\t                prompt_emb_tb[:, :, :self.num_tokens, :],\n", "                x, prompt_emb_tb[:, :, self.num_tokens:, :]\n\t            ), dim=-2)\n\t            x = self.get_patch_embeddings(x)  # (batch_size, n_patches, hidden_dim)\n\t        elif self.prompt_config.LOCATION == \"below\":\n\t            # (batch, 3, height, width)\n\t            x = torch.cat((\n\t                    x,\n\t                    self.prompt_norm(\n\t                        self.prompt_embeddings).expand(B, -1, -1, -1),\n\t                ), dim=1)\n", "            x = self.get_patch_embeddings(x)\n\t            # (batch_size, n_patches, hidden_dim)\n\t        else:\n\t            raise ValueError(\"Other prompt locations are not supported\")\n\t        return x\n\t    def get_patch_embeddings(self, x):\n\t        x = self.patch_embed(x)\n\t        if self.ape:\n\t            x = x + self.absolute_pos_embed\n\t        x = self.pos_drop(x)\n", "        return x\n\t    def train(self, mode=True):\n\t        # set train status for this class: disable all but the prompt-related modules\n\t        if mode:\n\t            # training:\n\t            # first set all to eval and set the prompt to train later\n\t            for module in self.children():\n\t                module.train(False)\n\t            self.prompt_proj.train()\n\t            self.prompt_dropout.train()\n", "        else:\n\t            # eval:\n\t            for module in self.children():\n\t                module.train(mode)\n\t    def forward_features(self, x):\n\t        x = self.incorporate_prompt(x)\n\t        if self.prompt_config.LOCATION == \"prepend\" and self.prompt_config.DEEP:\n\t            for layer, deep_prompt_embd in zip(self.layers, [list(self.prompter.values())]):\n\t                # deep_prompt_embd = self.prompt_dropout(deep_prompt_embd)\n\t                x = layer(x, deep_prompt_embd)\n", "        else:\n\t            for layer in self.layers:\n\t                x = layer(x)\n\t        x = self.norm(x)  # B L C\n\t        x = x[:, self.prompt_config.NUM_TOKENS:, :]\n\t        return x\n\t    def load_state_dict(self, state_dict, strict):\n\t        if self.prompt_config.LOCATION == \"below\":\n\t            # modify state_dict first   [768, 4, 16, 16]\n\t            conv_weight = state_dict[\"patch_embed.proj.weight\"]\n", "            conv_weight = torch.cat(\n\t                (conv_weight, self.patch_embed.proj.weight[:, 3:, :, :]),\n\t                dim=1\n\t            )\n\t            state_dict[\"patch_embed.proj.weight\"] = conv_weight\n\t        super(PromptedSwinTransformer, self).load_state_dict(state_dict, strict)\n\tclass PromptedPatchMerging(PatchMerging):\n\t    r\"\"\" Patch Merging Layer.\n\t    Args:\n\t        input_resolution (tuple[int]): Resolution of input feature.\n", "        dim (int): Number of input channels.\n\t        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n\t    \"\"\"\n\t    def __init__(\n\t        self, num_prompts, prompt_location, deep_prompt, input_resolution,\n\t        dim, norm_layer=nn.LayerNorm\n\t    ):\n\t        super(PromptedPatchMerging, self).__init__(\n\t            input_resolution, dim, norm_layer)\n\t        self.num_prompts = num_prompts\n", "        self.prompt_location = prompt_location\n\t        if prompt_location == \"prepend\":\n\t            if not deep_prompt:\n\t                self.prompt_upsampling = None\n\t                # self.prompt_upsampling = nn.Linear(dim, 4 * dim, bias=False)\n\t            else:\n\t                self.prompt_upsampling = None\n\t    def upsample_prompt(self, prompt_emb):\n\t        if self.prompt_upsampling is not None:\n\t            prompt_emb = self.prompt_upsampling(prompt_emb)\n", "        else:\n\t            prompt_emb = torch.cat(\n\t                (prompt_emb, prompt_emb, prompt_emb, prompt_emb), dim=-1)\n\t        return prompt_emb\n\t    def forward(self, x):\n\t        \"\"\"\n\t        x: B, H*W, C\n\t        \"\"\"\n\t        H, W = self.input_resolution\n\t        B, L, C = x.shape\n", "        if self.prompt_location == \"prepend\":\n\t            # change input size\n\t            prompt_emb = x[:, :self.num_prompts, :]\n\t            x = x[:, self.num_prompts:, :]\n\t            L = L - self.num_prompts\n\t            prompt_emb = self.upsample_prompt(prompt_emb)\n\t        assert L == H * W, \"input feature has wrong size, should be {}, got {}\".format(H*W, L)\n\t        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\t        x = x.view(B, H, W, C)\n\t        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n", "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n\t        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n\t        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n\t        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n\t        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\t        # add the prompt back:\n\t        if self.prompt_location == \"prepend\":\n\t            x = torch.cat((prompt_emb, x), dim=1)\n\t        x = self.norm(x)\n\t        x = self.reduction(x)\n", "        return x\n\tclass PromptedSwinTransformerBlock(SwinTransformerBlock):\n\t    def __init__(\n\t        self, num_prompts, prompt_location, dim, input_resolution,\n\t        num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True,\n\t        qk_scale=None, drop=0., attn_drop=0., drop_path=0., act_layer=nn.GELU,\n\t        norm_layer=nn.LayerNorm\n\t    ):\n\t        super(PromptedSwinTransformerBlock, self).__init__(\n\t            dim, input_resolution, num_heads, window_size,\n", "            shift_size, mlp_ratio, qkv_bias, qk_scale, drop,\n\t            attn_drop, drop_path, act_layer, norm_layer)\n\t        self.num_prompts = num_prompts\n\t        self.prompt_location = prompt_location\n\t        if self.prompt_location == \"prepend\":\n\t            self.attn = PromptedWindowAttention(\n\t                num_prompts, prompt_location,\n\t                dim, window_size=to_2tuple(self.window_size),\n\t                num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t                attn_drop=attn_drop, proj_drop=drop)\n", "    def forward(self, x):\n\t        H, W = self.input_resolution\n\t        B, L, C = x.shape\n\t        shortcut = x\n\t        x = self.norm1(x)\n\t        if self.prompt_location == \"prepend\":\n\t            # change input size\n\t            prompt_emb = x[:, :self.num_prompts, :]\n\t            x = x[:, self.num_prompts:, :]\n\t            L = L - self.num_prompts\n", "        assert L == H * W, \"input feature has wrong size, should be {}, got {}\".format(H*W, L)\n\t        x = x.view(B, H, W, C)\n\t        # cyclic shift\n\t        if self.shift_size > 0:\n\t            shifted_x = torch.roll(\n\t                x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n\t        else:\n\t            shifted_x = x\n\t        # partition windows --> nW*B, window_size, window_size, C\n\t        x_windows = window_partition(shifted_x, self.window_size)\n", "        # nW*B, window_size*window_size, C\n\t        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)\n\t        # W-MSA/SW-MSA\n\t        # nW*B, window_size*window_size, C\n\t        # add back the prompt for attn for parralel-based prompts\n\t        # nW*B, num_prompts + window_size*window_size, C\n\t        num_windows = int(x_windows.shape[0] / B)\n\t        if self.prompt_location == \"prepend\":\n\t            # expand prompts_embs\n\t            # B, num_prompts, C --> nW*B, num_prompts, C\n", "            prompt_emb = prompt_emb.unsqueeze(0)\n\t            prompt_emb = prompt_emb.expand(num_windows, -1, -1, -1)\n\t            prompt_emb = prompt_emb.reshape((-1, self.num_prompts, C))\n\t            x_windows = torch.cat((prompt_emb, x_windows), dim=1)\n\t        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n\t        # seperate prompt embs --> nW*B, num_prompts, C\n\t        if self.prompt_location == \"prepend\":\n\t            # change input size\n\t            prompt_emb = attn_windows[:, :self.num_prompts, :]\n\t            attn_windows = attn_windows[:, self.num_prompts:, :]\n", "            # change prompt_embs's shape:\n\t            # nW*B, num_prompts, C - B, num_prompts, C\n\t            prompt_emb = prompt_emb.view(-1, B, self.num_prompts, C)\n\t            prompt_emb = prompt_emb.mean(0)\n\t        # merge windows\n\t        attn_windows = attn_windows.view(\n\t            -1, self.window_size, self.window_size, C)\n\t        shifted_x = window_reverse(\n\t            attn_windows, self.window_size, H, W)  # B H W C\n\t        # reverse cyclic shift\n", "        if self.shift_size > 0:\n\t            x = torch.roll(\n\t                shifted_x,\n\t                shifts=(self.shift_size, self.shift_size),\n\t                dims=(1, 2)\n\t            )\n\t        else:\n\t            x = shifted_x\n\t        x = x.view(B, H * W, C)\n\t        # add the prompt back:\n", "        if self.prompt_location == \"prepend\":\n\t            x = torch.cat((prompt_emb, x), dim=1)\n\t        # FFN\n\t        x = shortcut + self.drop_path(x)\n\t        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\t        return x\n\tclass PromptedWindowAttention(WindowAttention):\n\t    def __init__(\n\t        self, num_prompts, prompt_location, dim, window_size, num_heads,\n\t        qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.\n", "    ):\n\t        super(PromptedWindowAttention, self).__init__(\n\t            dim, window_size, num_heads, qkv_bias, qk_scale,\n\t            attn_drop, proj_drop)\n\t        self.num_prompts = num_prompts\n\t        self.prompt_location = prompt_location\n\t    def forward(self, x, mask=None):\n\t        \"\"\"\n\t        Args:\n\t            x: input features with shape of (num_windows*B, N, C)\n", "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n\t        \"\"\"\n\t        B_, N, C = x.shape\n\t        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\t        q = q * self.scale\n\t        attn = (q @ k.transpose(-2, -1))\n\t        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n\t            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n\t        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n", "        # account for prompt nums for relative_position_bias\n\t        # attn: [1920, 6, 649, 649]\n\t        # relative_position_bias: [6, 49, 49])\n\t        if self.prompt_location == \"prepend\":\n\t            # expand relative_position_bias\n\t            _C, _H, _W = relative_position_bias.shape\n\t            relative_position_bias = torch.cat((\n\t                torch.zeros(_C, self.num_prompts, _W, device=attn.device),\n\t                relative_position_bias\n\t                ), dim=1)\n", "            relative_position_bias = torch.cat((\n\t                torch.zeros(_C, _H + self.num_prompts, self.num_prompts, device=attn.device),\n\t                relative_position_bias\n\t                ), dim=-1)\n\t        attn = attn + relative_position_bias.unsqueeze(0)\n\t        if mask is not None:\n\t            # incorporate prompt\n\t            # mask: (nW, 49, 49) --> (nW, 49 + n_prompts, 49 + n_prompts)\n\t            nW = mask.shape[0]\n\t            if self.prompt_location == \"prepend\":\n", "                # expand relative_position_bias\n\t                mask = torch.cat((\n\t                    torch.zeros(nW, self.num_prompts, _W, device=attn.device),\n\t                    mask), dim=1)\n\t                mask = torch.cat((\n\t                    torch.zeros(\n\t                        nW, _H + self.num_prompts, self.num_prompts,\n\t                        device=attn.device),\n\t                    mask), dim=-1)\n\t            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n", "            attn = attn.view(-1, self.num_heads, N, N)\n\t            attn = self.softmax(attn)\n\t        else:\n\t            attn = self.softmax(attn)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x\n"]}
{"filename": "models/vit_prompt/__init__.py", "chunked_list": []}
{"filename": "models/vit_prompt/swin_backbone.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Copyright (c) Meta Platforms, Inc. All Rights Reserved\n\t\"\"\"\n\tborrowed from the official swin implementation, with some modification.\n\tsearch \"prompt\" for details.\n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torch.utils.checkpoint as checkpoint\n\tfrom timm.models.layers import DropPath, to_2tuple, trunc_normal_\n", "class Mlp(nn.Module):\n\t    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.act = act_layer()\n\t        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t    def forward(self, x):\n", "        x = self.fc1(x)\n\t        x = self.act(x)\n\t        x = self.drop(x)\n\t        x = self.fc2(x)\n\t        x = self.drop(x)\n\t        return x\n\tdef window_partition(x, window_size):\n\t    \"\"\"\n\t    Args:\n\t        x: (B, H, W, C)\n", "        window_size (int): window size\n\t    Returns:\n\t        windows: (num_windows*B, window_size, window_size, C)\n\t    \"\"\"\n\t    B, H, W, C = x.shape\n\t    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n\t    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n\t    return windows\n\tdef window_reverse(windows, window_size, H, W):\n\t    \"\"\"\n", "    Args:\n\t        windows: (num_windows*B, window_size, window_size, C)\n\t        window_size (int): Window size\n\t        H (int): Height of image\n\t        W (int): Width of image\n\t    Returns:\n\t        x: (B, H, W, C)\n\t    \"\"\"\n\t    B = int(windows.shape[0] / (H * W / window_size / window_size))\n\t    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n", "    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n\t    return x\n\tclass WindowAttention(nn.Module):\n\t    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n\t    It supports both of shifted and non-shifted window.\n\t    Args:\n\t        dim (int): Number of input channels.\n\t        window_size (tuple[int]): The height and width of the window.\n\t        num_heads (int): Number of attention heads.\n\t        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n", "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n\t        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n\t        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n\t    \"\"\"\n\t    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n\t        super().__init__()\n\t        self.dim = dim\n\t        self.window_size = window_size  # Wh, Ww\n\t        self.num_heads = num_heads\n\t        head_dim = dim // num_heads\n", "        self.scale = qk_scale or head_dim ** -0.5\n\t        # define a parameter table of relative position bias\n\t        self.relative_position_bias_table = nn.Parameter(\n\t            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n\t        # get pair-wise relative position index for each token inside the window\n\t        coords_h = torch.arange(self.window_size[0])\n\t        coords_w = torch.arange(self.window_size[1])\n\t        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n\t        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n\t        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n", "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n\t        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n\t        relative_coords[:, :, 1] += self.window_size[1] - 1\n\t        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n\t        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n\t        self.register_buffer(\"relative_position_index\", relative_position_index)\n\t        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(dim, dim)\n\t        self.proj_drop = nn.Dropout(proj_drop)\n", "        trunc_normal_(self.relative_position_bias_table, std=.02)\n\t        self.softmax = nn.Softmax(dim=-1)\n\t    def forward(self, x, mask=None):\n\t        \"\"\"\n\t        Args:\n\t            x: input features with shape of (num_windows*B, N, C)\n\t            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n\t        \"\"\"\n\t        B_, N, C = x.shape\n\t        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n", "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n\t        q = q * self.scale\n\t        attn = (q @ k.transpose(-2, -1))\n\t        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n\t            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n\t        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n\t        attn = attn + relative_position_bias.unsqueeze(0)\n\t        if mask is not None:\n\t            nW = mask.shape[0]\n\t            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n", "            attn = attn.view(-1, self.num_heads, N, N)\n\t            attn = self.softmax(attn)\n\t        else:\n\t            attn = self.softmax(attn)\n\t        attn = self.attn_drop(attn)\n\t        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n\t        x = self.proj(x)\n\t        x = self.proj_drop(x)\n\t        return x\n\t    def extra_repr(self) -> str:\n", "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n\t    def flops(self, N):\n\t        # calculate flops for 1 window with token length of N\n\t        flops = 0\n\t        # qkv = self.qkv(x)\n\t        flops += N * self.dim * 3 * self.dim\n\t        # attn = (q @ k.transpose(-2, -1))\n\t        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n\t        #  x = (attn @ v)\n\t        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n", "        # x = self.proj(x)\n\t        flops += N * self.dim * self.dim\n\t        return flops\n\tclass SwinTransformerBlock(nn.Module):\n\t    r\"\"\" Swin Transformer Block.\n\t    Args:\n\t        dim (int): Number of input channels.\n\t        input_resolution (tuple[int]): Input resulotion.\n\t        num_heads (int): Number of attention heads.\n\t        window_size (int): Window size.\n", "        shift_size (int): Shift size for SW-MSA.\n\t        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n\t        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n\t        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n\t        drop (float, optional): Dropout rate. Default: 0.0\n\t        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n\t        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n\t        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n\t        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n\t    \"\"\"\n", "    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n\t                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n\t                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n\t        super().__init__()\n\t        self.dim = dim\n\t        self.input_resolution = input_resolution\n\t        self.num_heads = num_heads\n\t        self.window_size = window_size\n\t        self.shift_size = shift_size\n\t        self.mlp_ratio = mlp_ratio\n", "        if min(self.input_resolution) <= self.window_size:\n\t            # if window size is larger than input resolution, we don't partition windows\n\t            self.shift_size = 0\n\t            self.window_size = min(self.input_resolution)\n\t        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n\t        self.norm1 = norm_layer(dim)\n\t        self.attn = WindowAttention(\n\t            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n\t            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n\t        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n", "        self.norm2 = norm_layer(dim)\n\t        mlp_hidden_dim = int(dim * mlp_ratio)\n\t        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n\t        if self.shift_size > 0:\n\t            # calculate attention mask for SW-MSA\n\t            H, W = self.input_resolution\n\t            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n\t            h_slices = (slice(0, -self.window_size),\n\t                        slice(-self.window_size, -self.shift_size),\n\t                        slice(-self.shift_size, None))\n", "            w_slices = (slice(0, -self.window_size),\n\t                        slice(-self.window_size, -self.shift_size),\n\t                        slice(-self.shift_size, None))\n\t            cnt = 0\n\t            for h in h_slices:\n\t                for w in w_slices:\n\t                    img_mask[:, h, w, :] = cnt\n\t                    cnt += 1\n\t            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n\t            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n", "            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n\t            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n\t        else:\n\t            attn_mask = None\n\t        self.register_buffer(\"attn_mask\", attn_mask)\n\t    def forward(self, x):\n\t        H, W = self.input_resolution\n\t        B, L, C = x.shape\n\t        assert L == H * W, \"input feature has wrong size\"\n\t        shortcut = x\n", "        x = self.norm1(x)\n\t        x = x.view(B, H, W, C)\n\t        # cyclic shift\n\t        if self.shift_size > 0:\n\t            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n\t        else:\n\t            shifted_x = x\n\t        # partition windows\n\t        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n\t        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n", "        # W-MSA/SW-MSA\n\t        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n\t        # merge windows\n\t        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n\t        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n\t        # reverse cyclic shift\n\t        if self.shift_size > 0:\n\t            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n\t        else:\n\t            x = shifted_x\n", "        x = x.view(B, H * W, C)\n\t        # FFN\n\t        x = shortcut + self.drop_path(x)\n\t        x = x + self.drop_path(self.mlp(self.norm2(x)))\n\t        return x\n\t    def extra_repr(self) -> str:\n\t        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n\t               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n\t    def flops(self):\n\t        flops = 0\n", "        H, W = self.input_resolution\n\t        # norm1\n\t        flops += self.dim * H * W\n\t        # W-MSA/SW-MSA\n\t        nW = H * W / self.window_size / self.window_size\n\t        flops += nW * self.attn.flops(self.window_size * self.window_size)\n\t        # mlp\n\t        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n\t        # norm2\n\t        flops += self.dim * H * W\n", "        return flops\n\tclass PatchMerging(nn.Module):\n\t    r\"\"\" Patch Merging Layer.\n\t    Args:\n\t        input_resolution (tuple[int]): Resolution of input feature.\n\t        dim (int): Number of input channels.\n\t        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n\t    \"\"\"\n\t    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n\t        super().__init__()\n", "        self.input_resolution = input_resolution\n\t        self.dim = dim\n\t        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n\t        self.norm = norm_layer(4 * dim)\n\t    def forward(self, x):\n\t        \"\"\"\n\t        x: B, H*W, C\n\t        \"\"\"\n\t        H, W = self.input_resolution\n\t        B, L, C = x.shape\n", "        assert L == H * W, \"input feature has wrong size\"\n\t        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n\t        x = x.view(B, H, W, C)\n\t        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n\t        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n\t        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n\t        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n\t        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n\t        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n\t        x = self.norm(x)\n", "        x = self.reduction(x)\n\t        return x\n\t    def extra_repr(self) -> str:\n\t        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n\t    def flops(self):\n\t        H, W = self.input_resolution\n\t        flops = H * W * self.dim\n\t        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n\t        return flops\n\tclass BasicLayer(nn.Module):\n", "    \"\"\" A basic Swin Transformer layer for one stage.\n\t    Args:\n\t        dim (int): Number of input channels.\n\t        input_resolution (tuple[int]): Input resolution.\n\t        depth (int): Number of blocks.\n\t        num_heads (int): Number of attention heads.\n\t        window_size (int): Local window size.\n\t        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n\t        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n\t        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n", "        drop (float, optional): Dropout rate. Default: 0.0\n\t        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n\t        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n\t        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n\t        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n\t        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n\t    \"\"\"\n\t    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n\t                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0.,\n\t                 attn_drop=0., drop_path=0., norm_layer=nn.LayerNorm,\n", "                 downsample=None, use_checkpoint=False,\n\t                 block_module=SwinTransformerBlock,\n\t                 # add two more parameters for prompt\n\t                 num_prompts=None, prompt_location=None, deep_prompt=None,\n\t        ):\n\t        super().__init__()\n\t        self.dim = dim\n\t        self.input_resolution = input_resolution\n\t        self.depth = depth\n\t        self.use_checkpoint = use_checkpoint\n", "        # build blocks\n\t        if num_prompts is not None:\n\t            self.blocks = nn.ModuleList([\n\t                block_module(\n\t                    num_prompts, prompt_location,\n\t                    dim=dim, input_resolution=input_resolution,\n\t                    num_heads=num_heads, window_size=window_size,\n\t                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n\t                    mlp_ratio=mlp_ratio,\n\t                    qkv_bias=qkv_bias, qk_scale=qk_scale,\n", "                    drop=drop, attn_drop=attn_drop,\n\t                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,  # noqa\n\t                    norm_layer=norm_layer)\n\t                for i in range(depth)])\n\t            self.deep_prompt = deep_prompt\n\t            self.num_prompts = num_prompts\n\t            self.prompt_location = prompt_location\n\t            if self.deep_prompt and self.prompt_location != \"prepend\":\n\t                raise ValueError(\"deep prompt mode for swin is only applicable to prepend\")\n\t        else:\n", "            self.blocks = nn.ModuleList([\n\t                block_module(\n\t                    dim=dim, input_resolution=input_resolution,\n\t                    num_heads=num_heads, window_size=window_size,\n\t                    shift_size=0 if (i % 2 == 0) else window_size // 2,\n\t                    mlp_ratio=mlp_ratio,\n\t                    qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t                    drop=drop, attn_drop=attn_drop,\n\t                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,  # noqa\n\t                    norm_layer=norm_layer)\n", "                for i in range(depth)])\n\t            self.deep_prompt = deep_prompt\n\t            self.num_prompts = num_prompts\n\t            self.prompt_location = prompt_location\n\t        # patch merging layer\n\t        if downsample is not None:\n\t            if num_prompts is None:\n\t                self.downsample = downsample(\n\t                    input_resolution, dim=dim, norm_layer=norm_layer\n\t                )\n", "            else:\n\t                self.downsample = downsample(\n\t                    num_prompts, prompt_location, deep_prompt,\n\t                    input_resolution, dim=dim, norm_layer=norm_layer\n\t                )\n\t        else:\n\t            self.downsample = None\n\t    def forward(self, x, deep_prompt_embd=None):\n\t        if self.deep_prompt and deep_prompt_embd is None:\n\t            raise ValueError(\"need deep_prompt embddings\")\n", "        if not self.deep_prompt:\n\t            for blk in self.blocks:\n\t                if self.use_checkpoint:\n\t                    x = checkpoint.checkpoint(blk, x)\n\t                else:\n\t                    x = blk(x)\n\t        else:\n\t            # add the prompt embed before each blk call\n\t            B = x.shape[0]  # batchsize\n\t            num_blocks = len(self.blocks)\n", "            # if deep_prompt_embd.shape[0] != num_blocks:\n\t            # first layer\n\t            for i in range(num_blocks):\n\t                if i == 0:\n\t                    x = self.blocks[i](x)\n\t                else:\n\t                    prompt_emb = deep_prompt_embd[i].expand(B, -1, -1)\n\t                    x = torch.cat(\n\t                        (prompt_emb, x[:, self.num_prompts:, :]),\n\t                        dim=1\n", "                    )\n\t                    x = self.blocks[i](x)\n\t        if self.downsample is not None:\n\t            x = self.downsample(x)\n\t        return x\n\t    def extra_repr(self) -> str:\n\t        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n\t    def flops(self):\n\t        flops = 0\n\t        for blk in self.blocks:\n", "            flops += blk.flops()\n\t        if self.downsample is not None:\n\t            flops += self.downsample.flops()\n\t        return flops\n\tclass PatchEmbed(nn.Module):\n\t    r\"\"\" Image to Patch Embedding\n\t    Args:\n\t        img_size (int): Image size.  Default: 224.\n\t        patch_size (int): Patch token size. Default: 4.\n\t        in_chans (int): Number of input image channels. Default: 3.\n", "        embed_dim (int): Number of linear projection output channels. Default: 96.\n\t        norm_layer (nn.Module, optional): Normalization layer. Default: None\n\t    \"\"\"\n\t    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n\t        super().__init__()\n\t        img_size = to_2tuple(img_size)\n\t        patch_size = to_2tuple(patch_size)\n\t        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n\t        self.img_size = img_size\n\t        self.patch_size = patch_size\n", "        self.patches_resolution = patches_resolution\n\t        self.num_patches = patches_resolution[0] * patches_resolution[1]\n\t        self.in_chans = in_chans\n\t        self.embed_dim = embed_dim\n\t        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\t        if norm_layer is not None:\n\t            self.norm = norm_layer(embed_dim)\n\t        else:\n\t            self.norm = None\n\t    def forward(self, x):\n", "        B, C, H, W = x.shape\n\t        # FIXME look at relaxing size constraints\n\t        assert H == self.img_size[0] and W == self.img_size[1], \\\n\t            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n\t        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C\n\t        if self.norm is not None:\n\t            x = self.norm(x)\n\t        return x\n\t    def flops(self):\n\t        Ho, Wo = self.patches_resolution\n", "        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n\t        if self.norm is not None:\n\t            flops += Ho * Wo * self.embed_dim\n\t        return flops\n\tclass SwinTransformer(nn.Module):\n\t    r\"\"\" Swin Transformer\n\t        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -\n\t          https://arxiv.org/pdf/2103.14030\n\t    Args:\n\t        img_size (int | tuple(int)): Input image size. Default 224\n", "        patch_size (int | tuple(int)): Patch size. Default: 4\n\t        in_chans (int): Number of input image channels. Default: 3\n\t        num_classes (int): Number of classes for classification head. Default: 1000\n\t        embed_dim (int): Patch embedding dimension. Default: 96\n\t        depths (tuple(int)): Depth of each Swin Transformer layer.\n\t        num_heads (tuple(int)): Number of attention heads in different layers.\n\t        window_size (int): Window size. Default: 7\n\t        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n\t        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n\t        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n", "        drop_rate (float): Dropout rate. Default: 0\n\t        attn_drop_rate (float): Attention dropout rate. Default: 0\n\t        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n\t        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n\t        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n\t        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n\t        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n\t    \"\"\"\n\t    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,\n\t                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],\n", "                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n\t                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n\t                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n\t                 use_checkpoint=False, **kwargs):\n\t        super().__init__()\n\t        self.num_classes = num_classes\n\t        self.depths = depths\n\t        self.num_layers = len(depths)\n\t        self.embed_dim = embed_dim\n\t        self.ape = ape\n", "        self.patch_norm = patch_norm\n\t        # self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n\t        self.num_features = int(embed_dim)\n\t        self.mlp_ratio = mlp_ratio\n\t        # split image into non-overlapping patches\n\t        self.patch_embed = PatchEmbed(\n\t            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,\n\t            norm_layer=norm_layer if self.patch_norm else None)\n\t        num_patches = self.patch_embed.num_patches\n\t        patches_resolution = self.patch_embed.patches_resolution\n", "        self.patches_resolution = patches_resolution\n\t        # absolute position embedding\n\t        if self.ape:\n\t            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n\t            trunc_normal_(self.absolute_pos_embed, std=.02)\n\t        self.pos_drop = nn.Dropout(p=drop_rate)\n\t        # stochastic depth\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n\t        # build layers\n\t        self.layers = nn.ModuleList()\n", "        for i_layer in range(self.num_layers):\n\t            layer = BasicLayer(dim=int(embed_dim),\n\t                               input_resolution=(patches_resolution[0],\n\t                                                 patches_resolution[1]),\n\t                               depth=depths[i_layer],\n\t                               num_heads=num_heads[i_layer],\n\t                               window_size=window_size,\n\t                               mlp_ratio=self.mlp_ratio,\n\t                               qkv_bias=qkv_bias, qk_scale=qk_scale,\n\t                               drop=drop_rate, attn_drop=attn_drop_rate,\n", "                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],\n\t                               norm_layer=norm_layer,\n\t                               downsample=None,\n\t                               use_checkpoint=use_checkpoint)\n\t            self.layers.append(layer)\n\t        self.norm = norm_layer(self.num_features)\n\t        self.avgpool = nn.AdaptiveAvgPool1d(1)\n\t        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n", "        if isinstance(m, nn.Linear):\n\t            trunc_normal_(m.weight, std=.02)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    @torch.jit.ignore\n\t    def no_weight_decay(self):\n\t        return {'absolute_pos_embed'}\n", "    @torch.jit.ignore\n\t    def no_weight_decay_keywords(self):\n\t        return {'relative_position_bias_table'}\n\t    def forward_features(self, x):\n\t        x = self.patch_embed(x)\n\t        if self.ape:\n\t            x = x + self.absolute_pos_embed\n\t        x = self.pos_drop(x)\n\t        for layer in self.layers:\n\t            x = layer(x)\n", "        return x\n\t    def forward(self, x):\n\t        x = self.forward_features(x)\n\t        # x = self.head(x)\n\t        return x\n\t    def flops(self):\n\t        flops = 0\n\t        flops += self.patch_embed.flops()\n\t        for i, layer in enumerate(self.layers):\n\t            flops += layer.flops()\n", "        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)\n\t        flops += self.num_features * self.num_classes\n\t        return flops\n"]}
