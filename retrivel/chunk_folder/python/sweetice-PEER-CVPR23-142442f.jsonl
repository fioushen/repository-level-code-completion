{"filename": "bullet_mujoco/peer.py", "chunked_list": ["import copy\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t# Implementation of PEER, based on (TD3)\n\t# TD3 Paper: https://arxiv.org/abs/1802.09477\n\tclass Actor(nn.Module):\n\t    def __init__(self, state_dim, action_dim, max_action):\n\t        super(Actor, self).__init__()\n", "        self.l1 = nn.Linear(state_dim, 256)\n\t        self.l2 = nn.Linear(256, 256)\n\t        self.l3 = nn.Linear(256, action_dim)\n\t        self.max_action = max_action\n\t    def forward(self, state):\n\t        a = F.relu(self.l1(state))\n\t        a = F.relu(self.l2(a))\n\t        return self.max_action * torch.tanh(self.l3(a))\n\tclass Critic(nn.Module):\n\t    def __init__(self, state_dim, action_dim):\n", "        super(Critic, self).__init__()\n\t        # Q1 architecture\n\t        self.l1 = nn.Linear(state_dim + action_dim, 256)\n\t        self.l2 = nn.Linear(256, 256)\n\t        self.l3 = nn.Linear(256, 1)\n\t        # Q2 architecture\n\t        self.l4 = nn.Linear(state_dim + action_dim, 256)\n\t        self.l5 = nn.Linear(256, 256)\n\t        self.l6 = nn.Linear(256, 1)\n\t    def forward(self, state, action, feature=False):\n", "        sa = torch.cat([state, action], 1)\n\t        q1 = F.relu(self.l1(sa))\n\t        f1 = F.relu(self.l2(q1))\n\t        q1 = self.l3(f1)\n\t        q2 = F.relu(self.l4(sa))\n\t        f2 = F.relu(self.l5(q2))\n\t        q2 = self.l6(f2)\n\t        if feature:\n\t            return q1, f1, q2, f2\n\t        else:\n", "            return q1, q2\n\t    def Q1(self, state, action):\n\t        sa = torch.cat([state, action], 1)\n\t        q1 = F.relu(self.l1(sa))\n\t        q1 = F.relu(self.l2(q1))\n\t        q1 = self.l3(q1)\n\t        return q1\n\t    def feature(self, state, action):\n\t        sa = torch.cat([state, action], 1)\n\t        q1 = F.relu(self.l1(sa))\n", "        q1 = F.relu(self.l2(q1))\n\t        return q1\n\tclass PEER(object):\n\t    def __init__(\n\t            self,\n\t            state_dim,\n\t            action_dim,\n\t            max_action,\n\t            discount=0.99,\n\t            tau=0.005,\n", "            policy_noise=0.2,\n\t            noise_clip=0.5,\n\t            policy_freq=2,\n\t            feature_coef = 1e-3\n\t    ):\n\t        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n\t        self.actor_target = copy.deepcopy(self.actor)\n\t        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n\t        self.critic = Critic(state_dim, action_dim).to(device)\n\t        self.critic_target = copy.deepcopy(self.critic)\n", "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n\t        self.max_action = max_action\n\t        self.discount = discount\n\t        self.tau = tau\n\t        self.policy_noise = policy_noise\n\t        self.noise_clip = noise_clip\n\t        self.policy_freq = policy_freq\n\t        self.total_it = 0\n\t        self.beta = feature_coef # beta in paper\n\t    def select_action(self, state):\n", "        with torch.no_grad():\n\t            state = torch.from_numpy(state).reshape(1, -1).to(device)\n\t            return self.actor(state).cpu().numpy().flatten()\n\t    def train(self, replay_buffer, batch_size=100):\n\t        self.total_it += 1\n\t        # Sample replay buffer\n\t        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n\t        with torch.no_grad():\n\t            # Select action according to policy and add clipped noise\n\t            noise = (\n", "                    torch.randn_like(action) * self.policy_noise\n\t            ).clamp(-self.noise_clip, self.noise_clip)\n\t            next_action = (\n\t                    self.actor_target(next_state) + noise\n\t            ).clamp(-self.max_action, self.max_action)\n\t            # Compute the target Q value\n\t            target_Q1, target_feature1, target_Q2, target_feature2 = self.critic_target(next_state, next_action, feature=True)\n\t            target_Q = torch.min(target_Q1, target_Q2)\n\t            target_Q = reward + not_done * self.discount * target_Q\n\t        # Get current Q estimates\n", "        current_Q1, current_feature1, current_Q2, current_feature2 = self.critic(state, action, feature=True)\n\t        # Compute critic loss\n\t        critic_loss1, critic_loss2 = F.mse_loss(current_Q1, target_Q), F.mse_loss(current_Q2, target_Q)\n\t        peer_loss1 = torch.einsum('ij,ij->i', [current_feature1, target_feature1]).mean()\n\t        peer_loss2 = torch.einsum('ij,ij->i', [current_feature2, target_feature2]).mean()\n\t        total_peer_loss = (peer_loss1 + peer_loss2)* self.beta\n\t        critic_loss = critic_loss1 + critic_loss2\n\t        Q_function_loss = critic_loss + total_peer_loss\n\t        # Optimize the critic\n\t        self.critic_optimizer.zero_grad()\n", "        Q_function_loss.backward()\n\t        self.critic_optimizer.step()\n\t        # Delayed policy updates\n\t        if self.total_it % self.policy_freq == 0:\n\t            # Compute actor loss\n\t            actor_loss = - self.critic.Q1(state, self.actor(state)).mean()\n\t            # Optimize the actor\n\t            self.actor_optimizer.zero_grad()\n\t            actor_loss.backward()\n\t            self.actor_optimizer.step()\n", "            # Update the frozen target models\n\t            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n\t                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n\t            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n\t                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"]}
{"filename": "bullet_mujoco/main.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport gym\n\timport argparse\n\timport os\n\timport mujoco\n\timport utils\n\timport time\n\timport random\n\timport pybullet_envs\n", "if __name__ == \"__main__\":\n\t    begin_time = time.asctime(time.localtime(time.time()))\n\t    start = time.time()\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--policy\", default=\"PEER\")\n\t    parser.add_argument(\"--env\", default='HopperBulletEnv-v0')\n\t    parser.add_argument(\"--seed\", default=0, type=int)\n\t    parser.add_argument(\"--start_timesteps\", default=25e3, type=int)  #\n\t    # default 25e3, for test 1e3\n\t    parser.add_argument(\"--eval_freq\", default=5e3, type=int)\n", "    parser.add_argument(\"--max_timesteps\", default=1e6, type=int)\n\t    parser.add_argument(\"--expl_noise\", default=0.1)\n\t    parser.add_argument(\"--batch_size\", default=256, type=int)\n\t    parser.add_argument(\"--discount\", default=0.99)\n\t    parser.add_argument(\"--tau\", default=0.005)\n\t    parser.add_argument(\"--policy_noise\", default=0.2)\n\t    parser.add_argument(\"--noise_clip\", default=0.5)\n\t    parser.add_argument(\"--policy_freq\", default=2, type=int)\n\t    parser.add_argument(\"--save_model\", default=True)\n\t    parser.add_argument(\"--load_model\", default=\"\")\n", "    parser.add_argument(\"--eval_state_value\", default=True)\n\t    parser.add_argument(\"--gpu_idx\", default=0, type=int)\n\t    parser.add_argument(\"--gpu_num\", default=0, type=int)\n\t    parser.add_argument(\"--feature_coef\", default=5e-4, type=float) # PRO coef\n\t    args = parser.parse_args()\n\t    num_thread = args.gpu_num\n\t    gpu_idx = args.gpu_idx\n\t    file_name = f\"{args.policy}_{args.env}_{args.seed}\"\n\t    print(\"---------------------------------------\")\n\t    print(f\"Policy: {args.policy}, Env: {args.env}, Seed: {args.seed}\")\n", "    print(\"---------------------------------------\")\n\t    env = gym.make(args.env)\n\t    # Set seeds\n\t    random.seed(args.seed)\n\t    env.seed(args.seed)\n\t    torch.manual_seed(args.seed)\n\t    torch.cuda.manual_seed_all(args.seed)\n\t    torch.backends.cudnn.deterministic = True\n\t    np.random.seed(args.seed)\n\t    state_dim = env.observation_space.shape[0]\n", "    action_dim = env.action_space.shape[0]\n\t    max_action = float(env.action_space.high[0])\n\t    kwargs = {\n\t        \"state_dim\": state_dim,\n\t        \"action_dim\": action_dim,\n\t        \"max_action\": max_action,\n\t        \"discount\": args.discount,\n\t        \"tau\": args.tau,\n\t    }\n\t    if args.policy == \"METD3\":\n", "        # Target policy smoothing is scaled wrt the action\n\t        import metd3\n\t        kwargs[\"policy_noise\"] = args.policy_noise * max_action\n\t        kwargs[\"noise_clip\"] = args.noise_clip * max_action\n\t        kwargs[\"policy_freq\"] = args.policy_freq\n\t        policy = metd3.METD3(**kwargs)\n\t    elif args.policy == \"PEER\":\n\t        # Target policy smoothing is scaled wrt the action\n\t        import peer\n\t        kwargs[\"policy_noise\"] = args.policy_noise * max_action\n", "        kwargs[\"noise_clip\"] = args.noise_clip * max_action\n\t        kwargs[\"policy_freq\"] = args.policy_freq\n\t        kwargs[\"feature_coef\"] = args.feature_coef\n\t        policy = peer.PEER(**kwargs)\n\t    else:\n\t        raise NotImplementedError(\"No policy named\", args.policy)\n\t    replay_buffer = utils.ReplayBufferMuJoCo(state_dim, action_dim)\n\t    state, done = env.reset(), False\n\t    episode_reward = 0\n\t    episode_timesteps = 0\n", "    episode_num = 0\n\t    for t in range(int(args.max_timesteps)):\n\t        episode_timesteps += 1\n\t        # Select action randomly or according to policy\n\t        if t < args.start_timesteps:\n\t            action = env.action_space.sample()\n\t        else:\n\t            action = (\n\t                    policy.select_action(np.array(state, dtype='float32'))\n\t                    + np.random.normal(0, max_action * args.expl_noise, size=action_dim)\n", "            ).clip(-max_action, max_action)\n\t        # Perform action\n\t        next_state, reward, done, _ = env.step(action)\n\t        done_bool = float(done) if episode_timesteps < env._max_episode_steps else 0\n\t        # Store data in replay buffer\n\t        replay_buffer.add(state, action, next_state, reward, done_bool)\n\t        state = next_state\n\t        episode_reward += reward\n\t        # Train agent after collecting sufficient data\n\t        if t >= args.start_timesteps:\n", "            policy.train(replay_buffer, args.batch_size)\n\t        if done:\n\t            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n\t            print(\n\t                f\"Algo: {args.policy} Env: {args.env} Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_reward:.3f}\")\n\t            # Reset environment\n\t            state, done = env.reset(), False\n\t            episode_reward = 0\n\t            episode_timesteps = 0\n\t            episode_num += 1"]}
{"filename": "bullet_mujoco/utils.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport torch.nn as nn\n\timport gym\n\timport os\n\tfrom collections import deque\n\timport random\n\tfrom torch.utils.data import Dataset, DataLoader\n\timport time\n\tfrom skimage.util.shape import view_as_windows\n", "class eval_mode(object):\n\t    def __init__(self, *models):\n\t        self.models = models\n\t    def __enter__(self):\n\t        self.prev_states = []\n\t        for model in self.models:\n\t            self.prev_states.append(model.training)\n\t            model.train(False)\n\t    def __exit__(self, *args):\n\t        for model, state in zip(self.models, self.prev_states):\n", "            model.train(state)\n\t        return False\n\tdef soft_update_params(net, target_net, tau):\n\t    for param, target_param in zip(net.parameters(), target_net.parameters()):\n\t        target_param.data.copy_(\n\t            tau * param.data + (1 - tau) * target_param.data\n\t        )\n\tdef set_seed_everywhere(seed):\n\t    torch.manual_seed(seed)\n\t    if torch.cuda.is_available():\n", "        torch.cuda.manual_seed_all(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\tdef module_hash(module):\n\t    result = 0\n\t    for tensor in module.state_dict().values():\n\t        result += tensor.sum().item()\n\t    return result\n\tdef make_dir(dir_path):\n\t    try:\n", "        os.mkdir(dir_path)\n\t    except OSError:\n\t        pass\n\t    return dir_path\n\tdef preprocess_obs(obs, bits=5):\n\t    \"\"\"Preprocessing image, see https://arxiv.org/abs/1807.03039.\"\"\"\n\t    bins = 2**bits\n\t    assert obs.dtype == torch.float32\n\t    if bits < 8:\n\t        obs = torch.floor(obs / 2**(8 - bits))\n", "    obs = obs / bins\n\t    obs = obs + torch.rand_like(obs) / bins\n\t    obs = obs - 0.5\n\t    return obs\n\tclass ReplayBuffer(Dataset):\n\t    \"\"\"Buffer to store environment transitions.\"\"\"\n\t    def __init__(self, obs_shape, action_shape, capacity, batch_size, device,image_size=84,transform=None):\n\t        self.capacity = capacity\n\t        self.batch_size = batch_size\n\t        self.device = device\n", "        self.image_size = image_size\n\t        self.transform = transform\n\t        # the proprioceptive obs is stored as float32, pixels obs as uint8\n\t        obs_dtype = np.float32 if len(obs_shape) == 1 else np.uint8\n\t        self.obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n\t        self.next_obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n\t        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n\t        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n\t        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\t        self.idx = 0\n", "        self.last_save = 0\n\t        self.full = False\n\t    def add(self, obs, action, reward, next_obs, done):\n\t        np.copyto(self.obses[self.idx], obs)\n\t        np.copyto(self.actions[self.idx], action)\n\t        np.copyto(self.rewards[self.idx], reward)\n\t        np.copyto(self.next_obses[self.idx], next_obs)\n\t        np.copyto(self.not_dones[self.idx], not done)\n\t        self.idx = (self.idx + 1) % self.capacity\n\t        self.full = self.full or self.idx == 0\n", "    def sample_proprio(self):\n\t        idxs = np.random.randint(\n\t            0, self.capacity if self.full else self.idx, size=self.batch_size\n\t        )\n\t        obses = self.obses[idxs]\n\t        next_obses = self.next_obses[idxs]\n\t        obses = torch.as_tensor(obses, device=self.device).float()\n\t        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n\t        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n\t        next_obses = torch.as_tensor(\n", "            next_obses, device=self.device\n\t        ).float()\n\t        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n\t        return obses, actions, rewards, next_obses, not_dones\n\t    def sample_cpc(self):\n\t        start = time.time()\n\t        idxs = np.random.randint(\n\t            0, self.capacity if self.full else self.idx, size=self.batch_size\n\t        )\n\t        obses = self.obses[idxs]\n", "        next_obses = self.next_obses[idxs]\n\t        pos = obses.copy()\n\t        obses = random_crop(obses, self.image_size)\n\t        next_obses = random_crop(next_obses, self.image_size)\n\t        pos = random_crop(pos, self.image_size)\n\t        obses = torch.as_tensor(obses, device=self.device).float()\n\t        next_obses = torch.as_tensor(\n\t            next_obses, device=self.device\n\t        ).float()\n\t        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n", "        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n\t        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n\t        pos = torch.as_tensor(pos, device=self.device).float()\n\t        cpc_kwargs = dict(obs_anchor=obses, obs_pos=pos,\n\t                          time_anchor=None, time_pos=None)\n\t        return obses, actions, rewards, next_obses, not_dones, cpc_kwargs\n\t    def save(self, save_dir):\n\t        if self.idx == self.last_save:\n\t            return\n\t        path = os.path.join(save_dir, '%d_%d.pt' % (self.last_save, self.idx))\n", "        payload = [\n\t            self.obses[self.last_save:self.idx],\n\t            self.next_obses[self.last_save:self.idx],\n\t            self.actions[self.last_save:self.idx],\n\t            self.rewards[self.last_save:self.idx],\n\t            self.not_dones[self.last_save:self.idx]\n\t        ]\n\t        self.last_save = self.idx\n\t        torch.save(payload, path)\n\t    def load(self, save_dir):\n", "        chunks = os.listdir(save_dir)\n\t        chucks = sorted(chunks, key=lambda x: int(x.split('_')[0]))\n\t        for chunk in chucks:\n\t            start, end = [int(x) for x in chunk.split('.')[0].split('_')]\n\t            path = os.path.join(save_dir, chunk)\n\t            payload = torch.load(path)\n\t            assert self.idx == start\n\t            self.obses[start:end] = payload[0]\n\t            self.next_obses[start:end] = payload[1]\n\t            self.actions[start:end] = payload[2]\n", "            self.rewards[start:end] = payload[3]\n\t            self.not_dones[start:end] = payload[4]\n\t            self.idx = end\n\t    def __getitem__(self, idx):\n\t        idx = np.random.randint(\n\t            0, self.capacity if self.full else self.idx, size=1\n\t        )\n\t        idx = idx[0]\n\t        obs = self.obses[idx]\n\t        action = self.actions[idx]\n", "        reward = self.rewards[idx]\n\t        next_obs = self.next_obses[idx]\n\t        not_done = self.not_dones[idx]\n\t        if self.transform:\n\t            obs = self.transform(obs)\n\t            next_obs = self.transform(next_obs)\n\t        return obs, action, reward, next_obs, not_done\n\t    def __len__(self):\n\t        return self.capacity \n\tclass FrameStack(gym.Wrapper):\n", "    def __init__(self, env, k):\n\t        gym.Wrapper.__init__(self, env)\n\t        self._k = k\n\t        self._frames = deque([], maxlen=k)\n\t        shp = env.observation_space.shape\n\t        self.observation_space = gym.spaces.Box(\n\t            low=0,\n\t            high=1,\n\t            shape=((shp[0] * k,) + shp[1:]),\n\t            dtype=env.observation_space.dtype\n", "        )\n\t        self._max_episode_steps = env._max_episode_steps\n\t    def reset(self):\n\t        obs = self.env.reset()\n\t        for _ in range(self._k):\n\t            self._frames.append(obs)\n\t        return self._get_obs()\n\t    def step(self, action):\n\t        obs, reward, done, info = self.env.step(action)\n\t        self._frames.append(obs)\n", "        return self._get_obs(), reward, done, info\n\t    def _get_obs(self):\n\t        assert len(self._frames) == self._k\n\t        return np.concatenate(list(self._frames), axis=0)\n\tdef random_crop(imgs, output_size):\n\t    \"\"\"\n\t    Vectorized way to do random crop using sliding windows\n\t    and picking out random ones\n\t    args:\n\t        imgs, batch images with shape (B,C,H,W)\n", "    \"\"\"\n\t    # batch size\n\t    n = imgs.shape[0]\n\t    img_size = imgs.shape[-1]\n\t    crop_max = img_size - output_size\n\t    imgs = np.transpose(imgs, (0, 2, 3, 1))\n\t    w1 = np.random.randint(0, crop_max, n)\n\t    h1 = np.random.randint(0, crop_max, n)\n\t    # creates all sliding windows combinations of size (output_size)\n\t    windows = view_as_windows(\n", "        imgs, (1, output_size, output_size, 1))[..., 0,:,:, 0]\n\t    # selects a random window for each batch element\n\t    cropped_imgs = windows[np.arange(n), w1, h1]\n\t    return cropped_imgs\n\tdef center_crop_image(image, output_size):\n\t    h, w = image.shape[1:]\n\t    new_h, new_w = output_size, output_size\n\t    top = (h - new_h)//2\n\t    left = (w - new_w)//2\n\t    image = image[:, top:top + new_h, left:left + new_w]\n", "    return image\n\tclass ReplayBufferMuJoCo(object):\n\t    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n\t        self.max_size = max_size\n\t        self.ptr = 0\n\t        self.size = 0\n\t        self.state = np.zeros((max_size, state_dim), dtype='float32')\n\t        self.action = np.zeros((max_size, action_dim), dtype='float32')\n\t        self.next_state = np.zeros((max_size, state_dim), dtype='float32')\n\t        self.reward = np.zeros((max_size, 1), dtype='float32')\n", "        self.not_done = np.zeros((max_size, 1), dtype='float32')\n\t        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t    def add(self, state, action, next_state, reward, done):\n\t        self.state[self.ptr] = state\n\t        self.action[self.ptr] = action\n\t        self.next_state[self.ptr] = next_state\n\t        self.reward[self.ptr] = reward\n\t        self.not_done[self.ptr] = 1. - done\n\t        self.ptr = (self.ptr + 1) % self.max_size\n\t        self.size = min(self.size + 1, self.max_size)\n", "    def sample(self, batch_size):\n\t        ind = np.random.randint(0, self.size, size=batch_size)\n\t        return (\n\t            torch.from_numpy(self.state[ind]).to(self.device),\n\t            torch.from_numpy(self.action[ind]).to(self.device),\n\t            torch.from_numpy(self.next_state[ind]).to(self.device),\n\t            torch.from_numpy(self.reward[ind]).to(self.device),\n\t            torch.from_numpy(self.not_done[ind]).to(self.device)\n\t        )\n"]}
{"filename": "dmc/PEER-DrQ/peer.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport utils\n\timport os\n\tclass Encoder(nn.Module):\n\t    \"\"\"Convolutional encoder for image-based observations.\"\"\"\n\t    def __init__(self, obs_shape, feature_dim):\n\t        super().__init__()\n", "        assert len(obs_shape) == 3\n\t        self.num_layers = 4\n\t        self.num_filters = 32\n\t        self.output_dim = 35\n\t        self.output_logits = False\n\t        self.feature_dim = feature_dim\n\t        self.convs = nn.ModuleList([\n\t            nn.Conv2d(obs_shape[0], self.num_filters, 3, stride=2),\n\t            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n\t            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1),\n", "            nn.Conv2d(self.num_filters, self.num_filters, 3, stride=1)\n\t        ])\n\t        self.head = nn.Sequential(\n\t            nn.Linear(self.num_filters * 35 * 35, self.feature_dim),\n\t            nn.LayerNorm(self.feature_dim))\n\t        self.outputs = dict()\n\t    def forward_conv(self, obs):\n\t        obs = obs / 255.\n\t        self.outputs['obs'] = obs\n\t        conv = torch.relu(self.convs[0](obs))\n", "        self.outputs['conv1'] = conv\n\t        for i in range(1, self.num_layers):\n\t            conv = torch.relu(self.convs[i](conv))\n\t            self.outputs['conv%s' % (i + 1)] = conv\n\t        h = conv.view(conv.size(0), -1)\n\t        return h\n\t    def forward(self, obs, detach=False):\n\t        h = self.forward_conv(obs)\n\t        if detach:\n\t            h = h.detach()\n", "        out = self.head(h)\n\t        if not self.output_logits:\n\t            out = torch.tanh(out)\n\t        self.outputs['out'] = out\n\t        return out\n\t    def copy_conv_weights_from(self, source):\n\t        \"\"\"Tie convolutional layers\"\"\"\n\t        for i in range(self.num_layers):\n\t            utils.tie_weights(src=source.convs[i], trg=self.convs[i])\n\tclass Actor(nn.Module):\n", "    \"\"\"torch.distributions implementation of an diagonal Gaussian policy.\"\"\"\n\t    def __init__(self, action_shape, hidden_dim, hidden_depth,\n\t                 log_std_bounds, obs_shape, feature_dim=50):\n\t        super().__init__()\n\t        self.encoder = Encoder(obs_shape=obs_shape, feature_dim=feature_dim)\n\t        self.log_std_bounds = log_std_bounds\n\t        self.trunk = utils.mlp(self.encoder.feature_dim, hidden_dim,\n\t                               2 * action_shape[0], hidden_depth)\n\t        self.outputs = dict()\n\t        self.apply(utils.weight_init)\n", "    def forward(self, obs, detach_encoder=False):\n\t        obs = self.encoder(obs, detach=detach_encoder)\n\t        mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n\t        # constrain log_std inside [log_std_min, log_std_max]\n\t        log_std = torch.tanh(log_std)\n\t        log_std_min, log_std_max = self.log_std_bounds\n\t        log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std +\n\t                                                                     1)\n\t        std = log_std.exp()\n\t        self.outputs['mu'] = mu\n", "        self.outputs['std'] = std\n\t        dist = utils.SquashedNormal(mu, std)\n\t        return dist\n\tclass Critic(nn.Module):\n\t    \"\"\"Critic network, employes double Q-learning.\"\"\"\n\t    def __init__(self, action_shape, hidden_dim, hidden_depth, obs_shape, feature_dim=50):\n\t        super().__init__()\n\t        # self.encoder = hydra.utils.instantiate(encoder_cfg)\n\t        self.encoder = Encoder(obs_shape=obs_shape, feature_dim=feature_dim)\n\t        self.repr1 = utils.mlp(self.encoder.feature_dim + action_shape[0],\n", "                            hidden_dim, hidden_dim, hidden_depth-1)\n\t        self.repr2 = utils.mlp(self.encoder.feature_dim + action_shape[0],\n\t                            hidden_dim, hidden_dim, hidden_depth-1)\n\t        self.Q1=torch.nn.Linear(hidden_dim, 1)\n\t        self.Q2=torch.nn.Linear(hidden_dim, 1)\n\t        self.outputs = dict()\n\t        self.apply(utils.weight_init)\n\t    def forward(self, obs, action, detach_encoder=False):\n\t        assert obs.size(0) == action.size(0)\n\t        obs = self.encoder(obs, detach=detach_encoder)\n", "        obs_action = torch.cat([obs, action], dim=-1)\n\t        repr1, repr2 = self.repr1(obs_action), self.repr2(obs_action) # output_dim=32\n\t        q1 = self.Q1(repr1)  # input_dim=1024\n\t        q2 = self.Q2(repr2)\n\t        self.outputs['q1'] = q1\n\t        self.outputs['q2'] = q2\n\t        return repr1, repr2, q1, q2\n\tclass PEER(object):\n\t    \"\"\"Data regularized Q: actor-critic method for learning from pixels.\"\"\"\n\t    def __init__(self, action_shape, action_range, device,\n", "                discount,\n\t                 init_temperature, lr, actor_update_frequency, critic_tau,\n\t                 critic_target_update_frequency, batch_size, env_name, seed,\n\t                 # critic setting\n\t                 obs_shape,\n\t                 feature_dim,\n\t                 hidden_dim=1024,\n\t                 hidden_depth=2,\n\t                 # actor setting\n\t                 log_std_bounds=[-10, 2],\n", "                 # PEER\n\t                 beta = None\n\t                 ):\n\t        assert beta is not None\n\t        self.action_range = action_range\n\t        self.device = \"cuda\"\n\t        self.discount = discount\n\t        self.critic_tau = critic_tau\n\t        self.actor_update_frequency = actor_update_frequency\n\t        self.critic_target_update_frequency = critic_target_update_frequency\n", "        self.batch_size = batch_size\n\t        self.actor = Actor(action_shape, hidden_dim, hidden_depth,\n\t                 log_std_bounds, obs_shape, feature_dim=50).to(self.device)\n\t        # self.critic = hydra.utils.instantiate(critic_cfg).to(self.device)\n\t        self.critic = Critic(action_shape, hidden_dim, hidden_depth, obs_shape, feature_dim).to(self.device)\n\t        self.critic_target = Critic(action_shape, hidden_dim, hidden_depth, obs_shape, feature_dim).to(self.device)\n\t        self.critic_target.load_state_dict(self.critic.state_dict())\n\t        # tie conv layers between actor and critic\n\t        self.actor.encoder.copy_conv_weights_from(self.critic.encoder)\n\t        self.log_alpha = torch.tensor(np.log(init_temperature)).to(device)\n", "        self.log_alpha.requires_grad = True\n\t        # set target entropy to -|A|\n\t        self.target_entropy = -action_shape[0]\n\t        # optimizers\n\t        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)\n\t        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n\t                                                 lr=lr)\n\t        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=lr)\n\t        # PEER loss\n\t        self.beta = beta\n", "        # self.count = 0\n\t        self.train()\n\t        self.critic_target.train()\n\t    def train(self, training=True):\n\t        self.training = training\n\t        self.actor.train(training)\n\t        self.critic.train(training)\n\t    @property\n\t    def alpha(self):\n\t        return self.log_alpha.exp()\n", "    def act(self, obs, sample=False):\n\t        with torch.no_grad():\n\t            obs = torch.FloatTensor(obs).to(self.device)\n\t            obs = obs.unsqueeze(0)\n\t            dist = self.actor(obs)\n\t            action = dist.sample() if sample else dist.mean\n\t            action = action.clamp(*self.action_range)\n\t            assert action.ndim == 2 and action.shape[0] == 1\n\t            return utils.to_np(action[0])\n\t    def update_critic(self, obs, obs_aug, action, reward, next_obs,\n", "                      next_obs_aug, not_done):\n\t        with torch.no_grad():\n\t            dist = self.actor(next_obs)\n\t            next_action = dist.rsample()\n\t            log_prob = dist.log_prob(next_action).sum(-1, keepdim=True)\n\t            target_repr1, target_repr2, target_Q1, target_Q2 = self.critic_target(next_obs, next_action)\n\t            target_V = torch.min(target_Q1,\n\t                                 target_Q2) - self.alpha.detach() * log_prob\n\t            target_Q = reward + (not_done * self.discount * target_V)\n\t            dist_aug = self.actor(next_obs_aug)\n", "            next_action_aug = dist_aug.rsample()\n\t            log_prob_aug = dist_aug.log_prob(next_action_aug).sum(-1,\n\t                                                                  keepdim=True)\n\t            target_repr1_aug, target_repr2_aug, target_Q1, target_Q2 = self.critic_target(next_obs_aug,\n\t                                                      next_action_aug)\n\t            # We should get the target repr\n\t            target_V = torch.min(\n\t                target_Q1, target_Q2) - self.alpha.detach() * log_prob_aug\n\t            target_Q_aug = reward + (not_done * self.discount * target_V)\n\t            target_Q = (target_Q + target_Q_aug) / 2\n", "        # get current Q estimates\n\t        current_repr1, current_repr2, current_Q1, current_Q2 = self.critic(obs, action)\n\t        PEER_loss1 = torch.einsum('ij,ij->i', [current_repr1, target_repr1])\n\t        PEER_loss2 = torch.einsum('ij,ij->i', [current_repr2, target_repr2])\n\t        critic_loss1 = F.mse_loss(current_Q1, target_Q)\n\t        critic_loss2 = F.mse_loss(current_Q2, target_Q)\n\t        critic_loss_original = critic_loss1 + critic_loss2\n\t        PEER_loss1_beta = self.beta * PEER_loss1.mean()\n\t        PEER_loss2_beta = self.beta * PEER_loss2.mean()\n\t        critic_loss_before_aug = critic_loss_original + PEER_loss1_beta + PEER_loss2_beta\n", "        # critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(\n\t        #     current_Q2, target_Q) + self.beta * PEER_loss1.mean() + self.beta * PEER_loss2.mean()\n\t        repr_aug1, repr_aug2, Q1_aug, Q2_aug = self.critic(obs_aug, action)\n\t        PEER_aug1 = torch.einsum('ij,ij->i', [repr_aug1, target_repr1_aug])\n\t        PEER_aug2 = torch.einsum('ij,ij->i', [repr_aug2, target_repr2_aug])\n\t        PEER_aug1_beta = PEER_aug1.mean() * self.beta\n\t        PEER_aug2_beta = PEER_aug2.mean() * self.beta\n\t        aug_loss_without_peer = F.mse_loss(Q1_aug, target_Q) + F.mse_loss(\n\t            Q2_aug, target_Q)\n\t        critic_aug_loss = aug_loss_without_peer + PEER_aug1_beta + PEER_aug2_beta\n", "        total_critic_loss = critic_loss_before_aug + critic_aug_loss\n\t        # Optimize the critic\n\t        self.critic_optimizer.zero_grad()\n\t        total_critic_loss.backward()\n\t        self.critic_optimizer.step()\n\t    def update_actor_and_alpha(self, obs):\n\t        # detach conv filters, so we don't update them with the actor loss\n\t        dist = self.actor(obs, detach_encoder=True)\n\t        action = dist.rsample()\n\t        log_prob = dist.log_prob(action).sum(-1, keepdim=True)\n", "        # detach conv filters, so we don't update them with the actor loss\n\t        _, _, actor_Q1, actor_Q2 = self.critic(obs, action, detach_encoder=True)\n\t        actor_Q = torch.min(actor_Q1, actor_Q2)\n\t        actor_loss = (self.alpha.detach() * log_prob - actor_Q).mean()\n\t        # optimize the actor\n\t        self.actor_optimizer.zero_grad()\n\t        actor_loss.backward()\n\t        self.actor_optimizer.step()\n\t        self.log_alpha_optimizer.zero_grad()\n\t        alpha_loss = (self.alpha *\n", "                      (-log_prob - self.target_entropy).detach()).mean()\n\t        alpha_loss.backward()\n\t        self.log_alpha_optimizer.step()\n\t    def update(self, replay_buffer, step):\n\t        obs, action, reward, next_obs, not_done, obs_aug, next_obs_aug = replay_buffer.sample(\n\t            self.batch_size)\n\t        self.update_critic(obs, obs_aug, action, reward, next_obs,\n\t                           next_obs_aug, not_done)\n\t        if step % self.actor_update_frequency == 0:\n\t            self.update_actor_and_alpha(obs)\n", "        if step % self.critic_target_update_frequency == 0:\n\t            utils.soft_update_params(self.critic, self.critic_target,\n\t                                     self.critic_tau)\n"]}
{"filename": "dmc/PEER-DrQ/main.py", "chunked_list": ["import os\n\timport time\n\timport numpy as np\n\timport dmc2gym\n\tfrom peer import PEER\n\timport torch\n\timport utils\n\tfrom replay_buffer import ReplayBuffer\n\tfrom utils import CFG\n\timport argparse\n", "from tqdm import trange\n\ttorch.backends.cudnn.benchmark = True\n\ttorch.set_num_threads(8)\n\tdef make_env(cfg, seed=None):\n\t    \"\"\"Helper function to create dm_control environment\"\"\"\n\t    if cfg.env == 'ball_in_cup_catch':\n\t        domain_name = 'ball_in_cup'\n\t        task_name = 'catch'\n\t    elif cfg.env == 'point_mass_easy':\n\t        domain_name = 'point_mass'\n", "        task_name = 'easy'\n\t    else:\n\t        domain_name = cfg.env.split('_')[0]\n\t        task_name = '_'.join(cfg.env.split('_')[1:])\n\t    # per dreamer: https://github.com/danijar/dreamer/blob/02f0210f5991c7710826ca7881f19c64a012290c/wrappers.py#L26\n\t    camera_id = 2 if domain_name == 'quadruped' else 0\n\t    seed = cfg.seed if seed is None else seed\n\t    env = dmc2gym.make(domain_name=domain_name,\n\t                       task_name=task_name,\n\t                       seed=seed,\n", "                       visualize_reward=False,\n\t                       from_pixels=True,\n\t                       height=cfg.image_size,\n\t                       width=cfg.image_size,\n\t                       frame_skip=cfg.action_repeat,\n\t                       camera_id=camera_id)\n\t    env = utils.FrameStack(env, k=cfg.frame_stack)\n\t    env.seed(cfg.seed)\n\t    assert env.action_space.low.min() >= -1\n\t    assert env.action_space.high.max() <= 1\n", "    return env\n\tclass Workspace(object):\n\t    def __init__(self, cfg):\n\t        self.work_dir = os.getcwd()\n\t        print(f'workspace: {self.work_dir}')\n\t        self.cfg = cfg\n\t        utils.set_seed_everywhere(cfg.seed)\n\t        self.device = torch.device(cfg.device)\n\t        self.env = make_env(cfg)\n\t        if self.cfg.algo == \"PEER\":\n", "            print(\"Agent: \", self.cfg.algo)\n\t            self.agent = PEER(\n\t                action_shape = self.env.action_space.shape,\n\t                action_range = [\n\t                float(self.env.action_space.low.min()),\n\t                float(self.env.action_space.high.max())\n\t            ],\n\t                device=\"cuda\",\n\t                discount=self.cfg.discount,\n\t                init_temperature = self.cfg.init_temperature,\n", "                lr=self.cfg.lr,\n\t                actor_update_frequency=self.cfg.actor_update_frequency,\n\t                critic_tau=self.cfg.critic_tau,\n\t                critic_target_update_frequency=self.cfg.critic_target_update_frequency,\n\t                batch_size=self.cfg.batch_size,\n\t                env_name=self.cfg.env_name,\n\t                seed=self.cfg.seed,\n\t                obs_shape=self.env.observation_space.shape,\n\t                feature_dim=self.cfg.feature_dim,\n\t                hidden_dim=self.cfg.hidden_dim,\n", "                hidden_depth=self.cfg.hidden_depth,\n\t                # actor setting\n\t                log_std_bounds=[-10, 2],\n\t                beta=args.beta\n\t            )\n\t        else:\n\t            raise  NotImplementedError(\"Unknown Agent\")\n\t        self.replay_buffer = ReplayBuffer(self.env.observation_space.shape,\n\t                                          self.env.action_space.shape,\n\t                                          cfg.replay_buffer_capacity,\n", "                                          self.cfg.image_pad, self.device)\n\t        self.step = 0\n\t    def run(self):\n\t        episode, episode_reward, episode_step, done = 0, 0, 1, True\n\t        start_time = time.time()\n\t        for place_holder in trange(self.cfg.num_train_steps):\n\t            if done:\n\t                print(\n\t                    f\"Algorithm: {self.cfg.algo}, Env: {self.cfg.env_name}, Seed: {self.cfg.seed}, Num Step: {self.step}, Episode Reward: {episode_reward}\")\n\t                # evaluate agent periodically\n", "                obs = self.env.reset()\n\t                done = False\n\t                episode_reward = 0\n\t                episode_step = 0\n\t                episode += 1\n\t            # sample action for data collection\n\t            if self.step < self.cfg.num_seed_steps:\n\t                action = self.env.action_space.sample()\n\t            else:\n\t                with utils.eval_mode(self.agent):\n", "                    action = self.agent.act(obs, sample=True)\n\t            # run training update\n\t            if self.step >= self.cfg.num_seed_steps:\n\t                for _ in range(self.cfg.num_train_iters):\n\t                    self.agent.update(self.replay_buffer, self.step)\n\t            next_obs, reward, done, info = self.env.step(action)\n\t            # allow infinite bootstrap\n\t            done = float(done)\n\t            done_no_max = 0 if episode_step + 1 == self.env._max_episode_steps else done\n\t            episode_reward += reward\n", "            self.replay_buffer.add(obs, action, reward, next_obs, done,\n\t                                   done_no_max)\n\t            obs = next_obs\n\t            episode_step += 1\n\t            self.step += 1\n\tdef main(cfg):\n\t    from main import Workspace as W\n\t    workspace = W(cfg)\n\t    workspace.run()\n\tparser = argparse.ArgumentParser(\"DRQ-PEER\")\n", "parser.add_argument('--domain_name', default=\"ball_in_cup\")\n\tparser.add_argument('--task_name', default=\"catch\")\n\tparser.add_argument('--seed', default=1, type=int)\n\tparser.add_argument('--gpu_idx', default=0, type=int)\n\tparser.add_argument('--algo', default=\"PEER\", type=str)\n\tparser.add_argument('--beta', default=5e-3, type=float)\n\t# parser.add_argument('--debug', action=, type=bool)\n\targs = parser.parse_args()\n\targs.debug = False\n\tcfg=CFG(domain=args.domain_name,\n", "        task=args.task_name,\n\t        seed=args.seed,\n\t        debug=args.debug,\n\t        algo=args.algo,\n\t        beta=args.beta\n\t        )\n\tmain(cfg)"]}
{"filename": "dmc/PEER-DrQ/utils.py", "chunked_list": ["import math\n\timport os\n\timport random\n\tfrom collections import deque\n\timport numpy as np\n\timport scipy.linalg as sp_la\n\timport gym\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n", "from skimage.util.shape import view_as_windows\n\tfrom torch import distributions as pyd\n\timport pickle\n\timport pandas as pd\n\tclass eval_mode(object):\n\t    def __init__(self, *models):\n\t        self.models = models\n\t    def __enter__(self):\n\t        self.prev_states = []\n\t        for model in self.models:\n", "            self.prev_states.append(model.training)\n\t            model.train(False)\n\t    def __exit__(self, *args):\n\t        for model, state in zip(self.models, self.prev_states):\n\t            model.train(state)\n\t        return False\n\tdef soft_update_params(net, target_net, tau):\n\t    for param, target_param in zip(net.parameters(), target_net.parameters()):\n\t        target_param.data.copy_(tau * param.data +\n\t                                (1 - tau) * target_param.data)\n", "def set_seed_everywhere(seed):\n\t    torch.manual_seed(seed)\n\t    if torch.cuda.is_available():\n\t        torch.cuda.manual_seed_all(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\tdef make_dir(*path_parts):\n\t    dir_path = os.path.join(*path_parts)\n\t    try:\n\t        os.mkdir(dir_path)\n", "    except OSError:\n\t        pass\n\t    return dir_path\n\tdef tie_weights(src, trg):\n\t    assert type(src) == type(trg)\n\t    trg.weight = src.weight\n\t    trg.bias = src.bias\n\tdef weight_init(m):\n\t    \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n\t    if isinstance(m, nn.Linear):\n", "        nn.init.orthogonal_(m.weight.data)\n\t        if hasattr(m.bias, 'data'):\n\t            m.bias.data.fill_(0.0)\n\t    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n\t        gain = nn.init.calculate_gain('relu')\n\t        nn.init.orthogonal_(m.weight.data, gain)\n\t        if hasattr(m.bias, 'data'):\n\t            m.bias.data.fill_(0.0)\n\tdef mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n\t    if hidden_depth == 0:\n", "        mods = [nn.Linear(input_dim, output_dim)]\n\t    else:\n\t        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n\t        for i in range(hidden_depth - 1):\n\t            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n\t        mods.append(nn.Linear(hidden_dim, output_dim))\n\t    if output_mod is not None:\n\t        mods.append(output_mod)\n\t    trunk = nn.Sequential(*mods)\n\t    return trunk\n", "def to_np(t):\n\t    if t is None:\n\t        return None\n\t    elif t.nelement() == 0:\n\t        return np.array([])\n\t    else:\n\t        return t.cpu().detach().numpy()\n\tclass FrameStack(gym.Wrapper):\n\t    def __init__(self, env, k):\n\t        gym.Wrapper.__init__(self, env)\n", "        self._k = k\n\t        self._frames = deque([], maxlen=k)\n\t        shp = env.observation_space.shape\n\t        self.observation_space = gym.spaces.Box(\n\t            low=0,\n\t            high=1,\n\t            shape=((shp[0] * k,) + shp[1:]),\n\t            dtype=env.observation_space.dtype)\n\t        self._max_episode_steps = env._max_episode_steps\n\t    def reset(self):\n", "        obs = self.env.reset()\n\t        for _ in range(self._k):\n\t            self._frames.append(obs)\n\t        return self._get_obs()\n\t    def step(self, action):\n\t        obs, reward, done, info = self.env.step(action)\n\t        self._frames.append(obs)\n\t        return self._get_obs(), reward, done, info\n\t    def _get_obs(self):\n\t        assert len(self._frames) == self._k\n", "        return np.concatenate(list(self._frames), axis=0)\n\tclass TanhTransform(pyd.transforms.Transform):\n\t    domain = pyd.constraints.real\n\t    codomain = pyd.constraints.interval(-1.0, 1.0)\n\t    bijective = True\n\t    sign = +1\n\t    def __init__(self, cache_size=1):\n\t        super().__init__(cache_size=cache_size)\n\t    @staticmethod\n\t    def atanh(x):\n", "        return 0.5 * (x.log1p() - (-x).log1p())\n\t    def __eq__(self, other):\n\t        return isinstance(other, TanhTransform)\n\t    def _call(self, x):\n\t        return x.tanh()\n\t    def _inverse(self, y):\n\t        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n\t        # one should use `cache_size=1` instead\n\t        return self.atanh(y)\n\t    def log_abs_det_jacobian(self, x, y):\n", "        # We use a formula that is more numerically stable, see details in the following link\n\t        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n\t        return 2. * (math.log(2.) - x - F.softplus(-2. * x))\n\tclass SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n\t    def __init__(self, loc, scale):\n\t        self.loc = loc\n\t        self.scale = scale\n\t        self.base_dist = pyd.Normal(loc, scale)\n\t        transforms = [TanhTransform()]\n\t        super().__init__(self.base_dist, transforms)\n", "    @property\n\t    def mean(self):\n\t        mu = self.loc\n\t        for tr in self.transforms:\n\t            mu = tr(mu)\n\t        return mu\n\tclass CFG():\n\t    env = \"cartpole_swingup\"\n\t    # IMPORTANT= if action_repeat is used the effective number of env steps needs to be\n\t    # multiplied by action_repeat in the result graphs.\n", "    # This is a common practice for a fair comparison.\n\t    # See the 2nd paragraph in Appendix C of SLAC= https=//arxiv.org/pdf/1907.00953.pdf\n\t    # See Dreamer TF2's implementation= https=//github.com/danijar/dreamer/blob/02f0210f5991c7710826ca7881f19c64a012290c/dreamer.py#L340\n\t    action_repeat = 4\n\t    # train\n\t    num_train_steps = 110000\n\t    num_train_iters = 1\n\t    num_seed_steps = 1000 #\n\t    replay_buffer_capacity = 100000 # √\n\t    seed = 1\n", "    # eval\n\t    eval_frequency = 10000\n\t    num_eval_episodes = 10\n\t    # misc\n\t    log_frequency_step = 10000\n\t    log_save_tb = False\n\t    save_video = False\n\t    device = \"cuda\"\n\t    # observation\n\t    image_size = 84\n", "    image_pad = 4\n\t    frame_stack = 3\n\t    # global params\n\t    lr = 1e-3 # √\n\t    # IMPORTANT= please use a batch size of 512 to reproduce the results in the paper. Hovewer with a smaller batch size it still works well.\n\t    batch_size = 512\n\t    obs_shape = 128\n\t    action_shape = 128\n\t    action_range = 128\n\t    encoder_cfg = None\n", "    critic_cfg = None\n\t    actor_cfg = None\n\t    discount = 0.99 # √\n\t    init_temperature = 0.1 # √\n\t    actor_update_frequency = 2 # √\n\t    critic_tau = 0.01 # √\n\t    critic_target_update_frequency = 2 # √\n\t    hidden_dim = 1024 # √\n\t    hidden_depth = 2 # √\n\t    log_std_bounds = [-10, 2] #\n", "    feature_dim = 50 # √\n\t    def __init__(self, domain=None, task=None, seed=None, debug=False, algo=None, beta=None):\n\t        self.domain = domain\n\t        self.task = task\n\t        self.env_name = f\"{self.domain}-{self.task}\"\n\t        self.seed = seed\n\t        self.algo = algo\n\t        self.beta = beta\n\t        if debug:\n\t            self.num_seed_steps = 2\n", "            self.batch_size = 2\n\t            self.num_eval_episodes = 2\n\t            self.eval_frequency = 2000\n\t            self.logger_interval = 500"]}
{"filename": "dmc/PEER-DrQ/replay_buffer.py", "chunked_list": ["import numpy as np\n\timport kornia\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport utils\n\tclass ReplayBuffer(object):\n\t    \"\"\"Buffer to store environment transitions.\"\"\"\n\t    def __init__(self, obs_shape, action_shape, capacity, image_pad, device):\n\t        self.capacity = capacity\n", "        self.device = device\n\t        self.aug_trans = nn.Sequential(\n\t            nn.ReplicationPad2d(image_pad),\n\t            kornia.augmentation.RandomCrop((obs_shape[-1], obs_shape[-1])))\n\t        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n\t        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n\t        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n\t        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n\t        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\t        self.not_dones_no_max = np.empty((capacity, 1), dtype=np.float32)\n", "        self.idx = 0\n\t        self.full = False\n\t    def __len__(self):\n\t        return self.capacity if self.full else self.idx\n\t    def add(self, obs, action, reward, next_obs, done, done_no_max):\n\t        np.copyto(self.obses[self.idx], obs)\n\t        np.copyto(self.actions[self.idx], action)\n\t        np.copyto(self.rewards[self.idx], reward)\n\t        np.copyto(self.next_obses[self.idx], next_obs)\n\t        np.copyto(self.not_dones[self.idx], not done)\n", "        np.copyto(self.not_dones_no_max[self.idx], not done_no_max)\n\t        self.idx = (self.idx + 1) % self.capacity\n\t        self.full = self.full or self.idx == 0\n\t    def sample(self, batch_size):\n\t        idxs = np.random.randint(0,\n\t                                 self.capacity if self.full else self.idx,\n\t                                 size=batch_size)\n\t        obses = self.obses[idxs]\n\t        next_obses = self.next_obses[idxs]\n\t        obses_aug = obses.copy()\n", "        next_obses_aug = next_obses.copy()\n\t        obses = torch.as_tensor(obses, device=self.device).float()\n\t        next_obses = torch.as_tensor(next_obses, device=self.device).float()\n\t        obses_aug = torch.as_tensor(obses_aug, device=self.device).float()\n\t        next_obses_aug = torch.as_tensor(next_obses_aug,\n\t                                         device=self.device).float()\n\t        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n\t        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n\t        not_dones_no_max = torch.as_tensor(self.not_dones_no_max[idxs],\n\t                                           device=self.device)\n", "        obses = self.aug_trans(obses)\n\t        next_obses = self.aug_trans(next_obses)\n\t        obses_aug = self.aug_trans(obses_aug)\n\t        next_obses_aug = self.aug_trans(next_obses_aug)\n\t        return obses, actions, rewards, next_obses, not_dones_no_max, obses_aug, next_obses_aug\n"]}
{"filename": "dmc/PEER-CURL/peer.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport copy\n\timport math\n\timport os\n\timport utils\n\tfrom encoder import make_encoder\n\t# Implementation of peer, based on (CURL)\n", "# CURL code: https://github.com/MishaLaskin/curl\n\tdef gaussian_logprob(noise, log_std):\n\t    \"\"\"Compute Gaussian log probability.\"\"\"\n\t    residual = (-0.5 * noise.pow(2) - log_std).sum(-1, keepdim=True)\n\t    return residual - 0.5 * np.log(2 * np.pi) * noise.size(-1)\n\tdef squash(mu, pi, log_pi):\n\t    \"\"\"Apply squashing function.\n\t    See appendix C from https://arxiv.org/pdf/1812.05905.pdf.\n\t    \"\"\"\n\t    mu = torch.tanh(mu)\n", "    if pi is not None:\n\t        pi = torch.tanh(pi)\n\t    if log_pi is not None:\n\t        log_pi -= torch.log(F.relu(1 - pi.pow(2)) + 1e-6).sum(-1, keepdim=True)\n\t    return mu, pi, log_pi\n\tdef weight_init(m):\n\t    \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n\t    if isinstance(m, nn.Linear):\n\t        nn.init.orthogonal_(m.weight.data)\n\t        m.bias.data.fill_(0.0)\n", "    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n\t        # delta-orthogonal init from https://arxiv.org/pdf/1806.05393.pdf\n\t        assert m.weight.size(2) == m.weight.size(3)\n\t        m.weight.data.fill_(0.0)\n\t        m.bias.data.fill_(0.0)\n\t        mid = m.weight.size(2) // 2\n\t        gain = nn.init.calculate_gain('relu')\n\t        nn.init.orthogonal_(m.weight.data[:, :, mid, mid], gain)\n\tclass Actor(nn.Module):\n\t    \"\"\"MLP actor network.\"\"\"\n", "    def __init__(\n\t            self, obs_shape, action_shape, hidden_dim, encoder_type,\n\t            encoder_feature_dim, log_std_min, log_std_max, num_layers, num_filters\n\t    ):\n\t        super().__init__()\n\t        self.encoder = make_encoder(\n\t            encoder_type, obs_shape, encoder_feature_dim, num_layers,\n\t            num_filters, output_logits=True\n\t        )\n\t        self.log_std_min = log_std_min\n", "        self.log_std_max = log_std_max\n\t        self.trunk = nn.Sequential(\n\t            nn.Linear(self.encoder.feature_dim, hidden_dim), nn.ReLU(),\n\t            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n\t            nn.Linear(hidden_dim, 2 * action_shape[0])\n\t        )\n\t        self.outputs = dict()\n\t        self.apply(weight_init)\n\t    def forward(\n\t            self, obs, compute_pi=True, compute_log_pi=True, detach_encoder=False\n", "    ):\n\t        obs = self.encoder(obs, detach=detach_encoder)\n\t        mu, log_std = self.trunk(obs).chunk(2, dim=-1)\n\t        # constrain log_std inside [log_std_min, log_std_max]\n\t        log_std = torch.tanh(log_std)\n\t        log_std = self.log_std_min + 0.5 * (\n\t                self.log_std_max - self.log_std_min\n\t        ) * (log_std + 1)\n\t        self.outputs['mu'] = mu\n\t        self.outputs['std'] = log_std.exp()\n", "        if compute_pi:\n\t            std = log_std.exp()\n\t            noise = torch.randn_like(mu)\n\t            pi = mu + noise * std\n\t        else:\n\t            pi = None\n\t            entropy = None\n\t        if compute_log_pi:\n\t            log_pi = gaussian_logprob(noise, log_std)\n\t        else:\n", "            log_pi = None\n\t        mu, pi, log_pi = squash(mu, pi, log_pi)\n\t        return mu, pi, log_pi, log_std\n\tclass QFunction(nn.Module):\n\t    \"\"\"MLP for q-function.\"\"\"\n\t    def __init__(self, obs_dim, action_dim, hidden_dim):\n\t        super().__init__()\n\t        self.trunk = nn.Sequential(\n\t            nn.Linear(obs_dim + action_dim, hidden_dim), nn.ReLU(),\n\t            nn.Linear(hidden_dim, hidden_dim), nn.ReLU(),\n", "        )\n\t        self.fc = nn.Linear(hidden_dim, 1)\n\t    def forward(self, obs, action):\n\t        assert obs.size(0) == action.size(0)\n\t        obs_action = torch.cat([obs, action], dim=1)\n\t        feature = self.trunk(obs_action)\n\t        Q = self.fc(feature)\n\t        return Q, feature\n\tclass Critic(nn.Module):\n\t    \"\"\"Critic network, employes two q-functions.\"\"\"\n", "    def __init__(\n\t            self, obs_shape, action_shape, hidden_dim, encoder_type,\n\t            encoder_feature_dim, num_layers, num_filters\n\t    ):\n\t        super().__init__()\n\t        self.encoder = make_encoder(\n\t            encoder_type, obs_shape, encoder_feature_dim, num_layers,\n\t            num_filters, output_logits=True\n\t        )\n\t        self.Q1 = QFunction(\n", "            self.encoder.feature_dim, action_shape[0], hidden_dim\n\t        )\n\t        self.Q2 = QFunction(\n\t            self.encoder.feature_dim, action_shape[0], hidden_dim\n\t        )\n\t        self.outputs = dict()\n\t        self.apply(weight_init)\n\t    def forward(self, obs, action, detach_encoder=False, feature=False):\n\t        # detach_encoder allows to stop gradient propogation to encoder\n\t        obs = self.encoder(obs, detach=detach_encoder)\n", "        q1, f1 = self.Q1(obs, action)\n\t        q2, f2 = self.Q2(obs, action)\n\t        self.outputs['q1'] = q1\n\t        self.outputs['q2'] = q2\n\t        return q1, f1, q2, f2\n\t    def feature(self, obs, action, detach_encoder=False):\n\t        obs = self.encoder(obs, detach=detach_encoder)\n\t        q1, f1 = self.Q1(obs, action)\n\t        return f1\n\tclass PEER(nn.Module):\n", "    def __init__(self, obs_shape, z_dim, batch_size, critic, critic_target, output_type=\"continuous\"):\n\t        super(PEER, self).__init__()\n\t        self.batch_size = batch_size\n\t        self.encoder = critic.encoder\n\t        self.encoder_target = critic_target.encoder\n\t        self.W = nn.Parameter(torch.rand(z_dim, z_dim))\n\t        self.output_type = output_type\n\t    def encode(self, x, detach=False, ema=False):\n\t        \"\"\"\n\t        Encoder: z_t = e(x_t)\n", "        :param x: x_t, x y coordinates\n\t        :return: z_t, value in r2\n\t        \"\"\"\n\t        if ema:\n\t            with torch.no_grad():\n\t                z_out = self.encoder_target(x)\n\t        else:\n\t            z_out = self.encoder(x)\n\t        if detach:\n\t            z_out = z_out.detach()\n", "        return z_out\n\t    def compute_logits(self, z_a, z_pos):\n\t        Wz = torch.matmul(self.W, z_pos.T)  # (z_dim,B)\n\t        logits = torch.matmul(z_a, Wz)  # (B,B)\n\t        logits = logits - torch.max(logits, 1)[0][:, None]\n\t        return logits\n\tclass PEERAgent(object):\n\t    def __init__(\n\t            self,\n\t            obs_shape,\n", "            action_shape,\n\t            device,\n\t            hidden_dim=256,\n\t            discount=0.99,\n\t            init_temperature=0.01,\n\t            alpha_lr=1e-3,\n\t            alpha_beta=0.9,\n\t            actor_lr=1e-3,\n\t            actor_beta=0.9,\n\t            actor_log_std_min=-10,\n", "            actor_log_std_max=2,\n\t            actor_update_freq=2,\n\t            critic_lr=1e-3,\n\t            critic_beta=0.9,\n\t            critic_tau=0.005,\n\t            critic_target_update_freq=2,\n\t            encoder_type='pixel',\n\t            encoder_feature_dim=50,\n\t            encoder_lr=1e-3,\n\t            encoder_tau=0.005,\n", "            num_layers=4,\n\t            num_filters=32,\n\t            cpc_update_freq=1,\n\t            log_interval=100,\n\t            detach_encoder=False,\n\t            curl_latent_dim=128,\n\t            env_name=None,\n\t            seed=None\n\t    ):\n\t        self.device = device\n", "        self.discount = discount\n\t        self.critic_tau = critic_tau\n\t        self.encoder_tau = encoder_tau\n\t        self.actor_update_freq = actor_update_freq\n\t        self.critic_target_update_freq = critic_target_update_freq\n\t        self.cpc_update_freq = cpc_update_freq\n\t        self.log_interval = log_interval\n\t        self.image_size = obs_shape[-1]\n\t        self.curl_latent_dim = curl_latent_dim\n\t        self.detach_encoder = detach_encoder\n", "        self.encoder_type = encoder_type\n\t        self.feature_coef = 5e-4 # beta in paper\n\t        assert env_name is not None\n\t        assert seed is not None\n\t        self.actor = Actor(\n\t            obs_shape, action_shape, hidden_dim, encoder_type,\n\t            encoder_feature_dim, actor_log_std_min, actor_log_std_max,\n\t            num_layers, num_filters\n\t        ).to(device)\n\t        self.critic = Critic(\n", "            obs_shape, action_shape, hidden_dim, encoder_type,\n\t            encoder_feature_dim, num_layers, num_filters\n\t        ).to(device)\n\t        self.critic_target = Critic(\n\t            obs_shape, action_shape, hidden_dim, encoder_type,\n\t            encoder_feature_dim, num_layers, num_filters\n\t        ).to(device)\n\t        self.critic_target.load_state_dict(self.critic.state_dict())\n\t        self.actor.encoder.copy_conv_weights_from(self.critic.encoder)\n\t        self.log_alpha = torch.tensor(np.log(init_temperature)).to(device)\n", "        self.log_alpha.requires_grad = True\n\t        self.target_entropy = -np.prod(action_shape)\n\t        # optimizers\n\t        self.actor_optimizer = torch.optim.Adam(\n\t            self.actor.parameters(), lr=actor_lr, betas=(actor_beta, 0.999)\n\t        )\n\t        self.critic_optimizer = torch.optim.Adam(\n\t            self.critic.parameters(), lr=critic_lr, betas=(critic_beta, 0.999)\n\t        )\n\t        self.log_alpha_optimizer = torch.optim.Adam(\n", "            [self.log_alpha], lr=alpha_lr, betas=(alpha_beta, 0.999)\n\t        )\n\t        if self.encoder_type == 'pixel':\n\t            self.PEER = PEER(obs_shape, encoder_feature_dim,\n\t                             self.curl_latent_dim, self.critic, self.critic_target, output_type='continuous').to(\n\t                self.device)\n\t            # optimizer for critic encoder for reconstruction loss\n\t            self.encoder_optimizer = torch.optim.Adam(\n\t                self.critic.encoder.parameters(), lr=encoder_lr\n\t            )\n", "            self.cpc_optimizer = torch.optim.Adam(\n\t                self.PEER.parameters(), lr=encoder_lr\n\t            )\n\t        self.cross_entropy_loss = nn.CrossEntropyLoss()\n\t        self.train()\n\t        self.critic_target.train()\n\t    def train(self, training=True):\n\t        self.training = training\n\t        self.actor.train(training)\n\t        self.critic.train(training)\n", "        if self.encoder_type == 'pixel':\n\t            self.PEER.train(training)\n\t    @property\n\t    def alpha(self):\n\t        return self.log_alpha.exp()\n\t    def select_action(self, obs):\n\t        with torch.no_grad():\n\t            obs = torch.FloatTensor(obs).to(self.device)\n\t            obs = obs.unsqueeze(0)\n\t            mu, _, _, _ = self.actor(\n", "                obs, compute_pi=False, compute_log_pi=False\n\t            )\n\t            return mu.cpu().data.numpy().flatten()\n\t    def sample_action(self, obs):\n\t        if obs.shape[-1] != self.image_size:\n\t            obs = utils.center_crop_image(obs, self.image_size)\n\t        with torch.no_grad():\n\t            obs = torch.FloatTensor(obs).to(self.device)\n\t            obs = obs.unsqueeze(0)\n\t            mu, pi, _, _ = self.actor(obs, compute_log_pi=False)\n", "            return pi.cpu().data.numpy().flatten()\n\t    def update_critic(self, obs, action, reward, next_obs, not_done):\n\t        with torch.no_grad():\n\t            _, policy_action, log_pi, _ = self.actor(next_obs)\n\t            target_Q1, target_feature1, target_Q2, target_feature2 = self.critic_target(next_obs, policy_action)\n\t            target_V = torch.min(target_Q1,\n\t                                 target_Q2) - self.alpha.detach() * log_pi\n\t            target_Q = reward + (not_done * self.discount * target_V)\n\t        # get current Q estimates\n\t        current_Q1, current_feature1, current_Q2, current_feature2 = self.critic(\n", "            obs, action, detach_encoder=self.detach_encoder)\n\t        critic_loss = F.mse_loss(current_Q1,\n\t                                 target_Q) + F.mse_loss(current_Q2, target_Q)\n\t        feature_loss1 = torch.einsum('ij,ij->i', [current_feature1, target_feature1]).mean()\n\t        feature_loss2 = torch.einsum('ij,ij->i', [current_feature2, target_feature2]).mean()\n\t        total_feature_loss = (feature_loss1 + feature_loss2) * self.feature_coef\n\t        critic_loss = total_feature_loss + critic_loss\n\t        # Optimize the critic\n\t        self.critic_optimizer.zero_grad()\n\t        critic_loss.backward()\n", "        self.critic_optimizer.step()\n\t    def update_actor_and_alpha(self, obs):\n\t        # detach encoder, so we don't update it with the actor loss\n\t        _, pi, log_pi, log_std = self.actor(obs, detach_encoder=True)\n\t        actor_Q1,_, actor_Q2,_ = self.critic(obs, pi, detach_encoder=True)\n\t        actor_Q = torch.min(actor_Q1, actor_Q2)\n\t        actor_loss = (self.alpha.detach() * log_pi - actor_Q).mean()\n\t        # optimize the actor\n\t        self.actor_optimizer.zero_grad()\n\t        actor_loss.backward()\n", "        self.actor_optimizer.step()\n\t        self.log_alpha_optimizer.zero_grad()\n\t        alpha_loss = (self.alpha *\n\t                      (-log_pi - self.target_entropy).detach()).mean()\n\t        alpha_loss.backward()\n\t        self.log_alpha_optimizer.step()\n\t    def update_cpc(self, obs_anchor, obs_pos):\n\t        z_a = self.PEER.encode(obs_anchor)\n\t        z_pos = self.PEER.encode(obs_pos, ema=True)\n\t        logits = self.PEER.compute_logits(z_a, z_pos)\n", "        labels = torch.arange(logits.shape[0]).long().to(self.device)\n\t        loss = self.cross_entropy_loss(logits, labels)\n\t        self.encoder_optimizer.zero_grad()\n\t        self.cpc_optimizer.zero_grad()\n\t        loss.backward()\n\t        self.encoder_optimizer.step()\n\t        self.cpc_optimizer.step()\n\t    def update(self, replay_buffer, MyLogger, step):\n\t        if self.encoder_type == 'pixel':\n\t            obs, action, reward, next_obs, not_done, cpc_kwargs = replay_buffer.sample_cpc()\n", "        else:\n\t            obs, action, reward, next_obs, not_done = replay_buffer.sample_proprio()\n\t        self.update_critic(obs, action, reward, next_obs, not_done, MyLogger, step)\n\t        if step % self.actor_update_freq == 0:\n\t            self.update_actor_and_alpha(obs, MyLogger, step)\n\t        if step % self.critic_target_update_freq == 0:\n\t            utils.soft_update_params(\n\t                self.critic.Q1, self.critic_target.Q1, self.critic_tau\n\t            )\n\t            utils.soft_update_params(\n", "                self.critic.Q2, self.critic_target.Q2, self.critic_tau\n\t            )\n\t            utils.soft_update_params(\n\t                self.critic.encoder, self.critic_target.encoder,\n\t                self.encoder_tau\n\t            )\n\t        if step % self.cpc_update_freq == 0 and self.encoder_type == 'pixel':\n\t            obs_anchor, obs_pos = cpc_kwargs[\"obs_anchor\"], cpc_kwargs[\"obs_pos\"]\n\t            self.update_cpc(obs_anchor, obs_pos, cpc_kwargs, MyLogger, step)\n\t    def save(self, model_dir, step):\n", "        torch.save(\n\t            self.actor.state_dict(), '%s/actor_%s.pt' % (model_dir, step)\n\t        )\n\t        torch.save(\n\t            self.critic.state_dict(), '%s/critic_%s.pt' % (model_dir, step)\n\t        )\n\t    def save_curl(self, model_dir, step):\n\t        torch.save(\n\t            self.PEER.state_dict(), '%s/curl_%s.pt' % (model_dir, step)\n\t        )\n", "    def load(self, model_dir, step):\n\t        self.actor.load_state_dict(\n\t            torch.load('%s/actor_%s.pt' % (model_dir, step))\n\t        )\n\t        self.critic.load_state_dict(\n\t            torch.load('%s/critic_%s.pt' % (model_dir, step))\n\t        )"]}
{"filename": "dmc/PEER-CURL/main.py", "chunked_list": ["import mujoco\n\timport dm_control\n\tfrom tqdm import trange\n\timport numpy as np\n\timport torch\n\timport argparse\n\timport dmc2gym\n\timport utils\n\tdef parse_args():\n\t    parser = argparse.ArgumentParser()\n", "    # environment\n\t    parser.add_argument('--domain_name', default=\"ball_in_cup\")\n\t    parser.add_argument('--task_name', default=\"catch\")\n\t    parser.add_argument('--pre_transform_image_size', default=100, type=int)\n\t    parser.add_argument('--image_size', default=84, type=int)\n\t    parser.add_argument('--action_repeat', default=1, type=int)\n\t    parser.add_argument('--frame_stack', default=3, type=int)\n\t    # replay buffer\n\t    parser.add_argument('--replay_buffer_capacity', default=100000, type=int)\n\t    # train\n", "    parser.add_argument('--agent', default='curl_sac', type=str)\n\t    parser.add_argument('--init_steps', default=1000, type=int) \n\t    parser.add_argument('--num_train_steps', default=500000, type=int)\n\t    parser.add_argument('--batch_size', default=512, type=int)\n\t    parser.add_argument('--hidden_dim', default=1024, type=int)\n\t    # eval\n\t    parser.add_argument('--eval_freq', default=5000, type=int)\n\t    parser.add_argument('--num_eval_episodes', default=10, type=int)\n\t    # critic\n\t    parser.add_argument('--critic_lr', default=1e-3, type=float)\n", "    parser.add_argument('--critic_beta', default=0.9, type=float)\n\t    parser.add_argument('--critic_tau', default=0.01, type=float) \n\t    parser.add_argument('--critic_target_update_freq', default=2, type=int) \n\t    # actor\n\t    parser.add_argument('--actor_lr', default=1e-3, type=float)\n\t    parser.add_argument('--actor_beta', default=0.9, type=float)\n\t    parser.add_argument('--actor_log_std_min', default=-10, type=float)\n\t    parser.add_argument('--actor_log_std_max', default=2, type=float)\n\t    parser.add_argument('--actor_update_freq', default=2, type=int)\n\t    # encoder\n", "    parser.add_argument('--encoder_type', default='pixel', type=str)\n\t    parser.add_argument('--encoder_feature_dim', default=50, type=int)\n\t    parser.add_argument('--encoder_lr', default=1e-3, type=float)\n\t    parser.add_argument('--encoder_tau', default=0.05, type=float)\n\t    parser.add_argument('--num_layers', default=4, type=int)\n\t    parser.add_argument('--num_filters', default=32, type=int)\n\t    parser.add_argument('--curl_latent_dim', default=128, type=int)\n\t    # sac\n\t    parser.add_argument('--discount', default=0.99, type=float)\n\t    parser.add_argument('--init_temperature', default=0.1, type=float)\n", "    parser.add_argument('--alpha_lr', default=1e-4, type=float)\n\t    parser.add_argument('--alpha_beta', default=0.5, type=float)\n\t    # misc\n\t    parser.add_argument('--seed', default=1, type=int)\n\t    parser.add_argument('--work_dir', default='./curl_exp', type=str)\n\t    parser.add_argument('--save_tb', default=False, action='store_true')\n\t    parser.add_argument('--save_buffer', default=False, action='store_true')\n\t    parser.add_argument('--save_video', default=False, action='store_true')\n\t    parser.add_argument('--save_model', default=False, action='store_true')\n\t    parser.add_argument('--detach_encoder', default=False, action='store_true')\n", "    parser.add_argument('--log_interval', default=100, type=int)\n\t    args = parser.parse_args()\n\t    # setting hyper-parameter\n\t    # action repeat\n\t    if args.domain_name in [\"finger\", \"walker\"]:\n\t        args.action_repeat = 2\n\t    elif args.domain_name == \"cartpole\":\n\t        args.action_repeat = 8\n\t    else:\n\t        args.action_repeat = 4\n", "    # learning rate\n\t    if args.domain_name in [\"cheetah\"]:\n\t        args.actor_lr = 2e-4\n\t        args.critic_lr = 2e-4\n\t        args.encoder_lr = 2e-4\n\t    else:\n\t        args.actor_lr = 1e-3\n\t        args.critic_lr = 1e-3\n\t        args.encoder_lr = 1e-3\n\t    return args\n", "def make_agent(obs_shape, action_shape, args, device):\n\t    import peer\n\t    return peer.PEERAgent(\n\t        obs_shape=obs_shape,\n\t        action_shape=action_shape,\n\t        device=device,\n\t        hidden_dim=args.hidden_dim,\n\t        discount=args.discount,\n\t        init_temperature=args.init_temperature,\n\t        alpha_lr=args.alpha_lr,\n", "        alpha_beta=args.alpha_beta,\n\t        actor_lr=args.actor_lr,\n\t        actor_beta=args.actor_beta,\n\t        actor_log_std_min=args.actor_log_std_min,\n\t        actor_log_std_max=args.actor_log_std_max,\n\t        actor_update_freq=args.actor_update_freq,\n\t        critic_lr=args.critic_lr,\n\t        critic_beta=args.critic_beta,\n\t        critic_tau=args.critic_tau,\n\t        critic_target_update_freq=args.critic_target_update_freq,\n", "        encoder_type=args.encoder_type,\n\t        encoder_feature_dim=args.encoder_feature_dim,\n\t        encoder_lr=args.encoder_lr,\n\t        encoder_tau=args.encoder_tau,\n\t        num_layers=args.num_layers,\n\t        num_filters=args.num_filters,\n\t        log_interval=args.log_interval,\n\t        detach_encoder=args.detach_encoder,\n\t        curl_latent_dim=args.curl_latent_dim,\n\t        env_name=f\"{args.domain_name}-{args.task_name}\",\n", "        seed=args.seed\n\t    )\n\tdef main():\n\t    args = parse_args()\n\t    if args.seed == -1:\n\t        args.__dict__[\"seed\"] = np.random.randint(1, 1000000)\n\t    utils.set_seed_everywhere(args.seed)\n\t    env = dmc2gym.make(\n\t        domain_name=args.domain_name,\n\t        task_name=args.task_name,\n", "        seed=args.seed,\n\t        visualize_reward=False,\n\t        from_pixels=(args.encoder_type == 'pixel'),\n\t        height=args.pre_transform_image_size,\n\t        width=args.pre_transform_image_size,\n\t        frame_skip=args.action_repeat\n\t    )\n\t    env.seed(args.seed)\n\t    # stack several consecutive frames together\n\t    if args.encoder_type == 'pixel':\n", "        env = utils.FrameStack(env, k=args.frame_stack)\n\t    device = torch.device('cuda')\n\t    action_shape = env.action_space.shape\n\t    if args.encoder_type == 'pixel':\n\t        obs_shape = (3 * args.frame_stack, args.image_size, args.image_size)\n\t        pre_aug_obs_shape = (3 * args.frame_stack, args.pre_transform_image_size, args.pre_transform_image_size)\n\t    else:\n\t        obs_shape = env.observation_space.shape\n\t        pre_aug_obs_shape = obs_shape\n\t    replay_buffer = utils.ReplayBuffer(\n", "        obs_shape=pre_aug_obs_shape,\n\t        action_shape=action_shape,\n\t        capacity=args.replay_buffer_capacity,\n\t        batch_size=args.batch_size,\n\t        device=device,\n\t        image_size=args.image_size,\n\t    )\n\t    agent = make_agent(\n\t        obs_shape=obs_shape,\n\t        action_shape=action_shape,\n", "        args=args,\n\t        device=device\n\t    )\n\t    episode, episode_reward, done = 0, 0, True\n\t    episode_step = 0\n\t    for step in trange(args.num_train_steps):\n\t        if done:\n\t            obs, done = env.reset(), False\n\t            print(f\"Domain_name: {args.domain_name}, Task_name: {args.task_name}, Seed: {args.seed}, Num_steps: {step} Episode_length: {episode_step}, Episode_reward: {episode_reward}\")\n\t            # done = False\n", "            episode_reward = 0\n\t            episode_step = 0\n\t            episode += 1\n\t        # sample action for data collection\n\t        if step < args.init_steps:\n\t            action = env.action_space.sample()\n\t        else:\n\t            with utils.eval_mode(agent):\n\t                action = agent.sample_action(obs)\n\t        # run training update\n", "        if step >= args.init_steps:\n\t            num_updates = 1\n\t            for _ in range(num_updates):\n\t                agent.update(replay_buffer, None, step)\n\t        next_obs, reward, done, _ = env.step(action)\n\t        # allow infinit bootstrap\n\t        done_bool = 0 if episode_step + 1 == env._max_episode_steps else float(\n\t            done\n\t        )\n\t        episode_reward += reward\n", "        replay_buffer.add(obs, action, reward, next_obs, done_bool)\n\t        obs = next_obs\n\t        episode_step += 1\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "dmc/PEER-CURL/utils.py", "chunked_list": ["import torch\n\timport numpy as np\n\timport torch.nn as nn\n\timport gym\n\timport os\n\tfrom collections import deque\n\timport random\n\tfrom torch.utils.data import Dataset, DataLoader\n\timport time\n\tfrom skimage.util.shape import view_as_windows\n", "class eval_mode(object):\n\t    def __init__(self, *models):\n\t        self.models = models\n\t    def __enter__(self):\n\t        self.prev_states = []\n\t        for model in self.models:\n\t            self.prev_states.append(model.training)\n\t            model.train(False)\n\t    def __exit__(self, *args):\n\t        for model, state in zip(self.models, self.prev_states):\n", "            model.train(state)\n\t        return False\n\tdef soft_update_params(net, target_net, tau):\n\t    for param, target_param in zip(net.parameters(), target_net.parameters()):\n\t        target_param.data.copy_(\n\t            tau * param.data + (1 - tau) * target_param.data\n\t        )\n\tdef set_seed_everywhere(seed):\n\t    torch.manual_seed(seed)\n\t    if torch.cuda.is_available():\n", "        torch.cuda.manual_seed_all(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\tdef module_hash(module):\n\t    result = 0\n\t    for tensor in module.state_dict().values():\n\t        result += tensor.sum().item()\n\t    return result\n\tdef make_dir(dir_path):\n\t    try:\n", "        os.mkdir(dir_path)\n\t    except OSError:\n\t        pass\n\t    return dir_path\n\tdef preprocess_obs(obs, bits=5):\n\t    \"\"\"Preprocessing image, see https://arxiv.org/abs/1807.03039.\"\"\"\n\t    bins = 2**bits\n\t    assert obs.dtype == torch.float32\n\t    if bits < 8:\n\t        obs = torch.floor(obs / 2**(8 - bits))\n", "    obs = obs / bins\n\t    obs = obs + torch.rand_like(obs) / bins\n\t    obs = obs - 0.5\n\t    return obs\n\tclass ReplayBuffer(Dataset):\n\t    \"\"\"Buffer to store environment transitions.\"\"\"\n\t    def __init__(self, obs_shape, action_shape, capacity, batch_size, device,image_size=84,transform=None):\n\t        self.capacity = capacity\n\t        self.batch_size = batch_size\n\t        self.device = device\n", "        self.image_size = image_size\n\t        self.transform = transform\n\t        # the proprioceptive obs is stored as float32, pixels obs as uint8\n\t        obs_dtype = np.float32 if len(obs_shape) == 1 else np.uint8\n\t        self.obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n\t        self.next_obses = np.empty((capacity, *obs_shape), dtype=obs_dtype)\n\t        self.actions = np.empty((capacity, *action_shape), dtype=np.float32)\n\t        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n\t        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\t        self.idx = 0\n", "        self.last_save = 0\n\t        self.full = False\n\t    def add(self, obs, action, reward, next_obs, done):\n\t        np.copyto(self.obses[self.idx], obs)\n\t        np.copyto(self.actions[self.idx], action)\n\t        np.copyto(self.rewards[self.idx], reward)\n\t        np.copyto(self.next_obses[self.idx], next_obs)\n\t        np.copyto(self.not_dones[self.idx], not done)\n\t        self.idx = (self.idx + 1) % self.capacity\n\t        self.full = self.full or self.idx == 0\n", "    def sample_proprio(self):\n\t        idxs = np.random.randint(\n\t            0, self.capacity if self.full else self.idx, size=self.batch_size\n\t        )\n\t        obses = self.obses[idxs]\n\t        next_obses = self.next_obses[idxs]\n\t        obses = torch.as_tensor(obses, device=self.device).float()\n\t        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n\t        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n\t        next_obses = torch.as_tensor(\n", "            next_obses, device=self.device\n\t        ).float()\n\t        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n\t        return obses, actions, rewards, next_obses, not_dones\n\t    def sample_cpc(self):\n\t        start = time.time()\n\t        idxs = np.random.randint(\n\t            0, self.capacity if self.full else self.idx, size=self.batch_size\n\t        )\n\t        obses = self.obses[idxs]\n", "        next_obses = self.next_obses[idxs]\n\t        pos = obses.copy()\n\t        obses = random_crop(obses, self.image_size)\n\t        next_obses = random_crop(next_obses, self.image_size)\n\t        pos = random_crop(pos, self.image_size)\n\t        obses = torch.as_tensor(obses, device=self.device).float()\n\t        next_obses = torch.as_tensor(\n\t            next_obses, device=self.device\n\t        ).float()\n\t        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n", "        rewards = torch.as_tensor(self.rewards[idxs], device=self.device)\n\t        not_dones = torch.as_tensor(self.not_dones[idxs], device=self.device)\n\t        pos = torch.as_tensor(pos, device=self.device).float()\n\t        cpc_kwargs = dict(obs_anchor=obses, obs_pos=pos,\n\t                          time_anchor=None, time_pos=None)\n\t        return obses, actions, rewards, next_obses, not_dones, cpc_kwargs\n\t    def save(self, save_dir):\n\t        if self.idx == self.last_save:\n\t            return\n\t        path = os.path.join(save_dir, '%d_%d.pt' % (self.last_save, self.idx))\n", "        payload = [\n\t            self.obses[self.last_save:self.idx],\n\t            self.next_obses[self.last_save:self.idx],\n\t            self.actions[self.last_save:self.idx],\n\t            self.rewards[self.last_save:self.idx],\n\t            self.not_dones[self.last_save:self.idx]\n\t        ]\n\t        self.last_save = self.idx\n\t        torch.save(payload, path)\n\t    def load(self, save_dir):\n", "        chunks = os.listdir(save_dir)\n\t        chucks = sorted(chunks, key=lambda x: int(x.split('_')[0]))\n\t        for chunk in chucks:\n\t            start, end = [int(x) for x in chunk.split('.')[0].split('_')]\n\t            path = os.path.join(save_dir, chunk)\n\t            payload = torch.load(path)\n\t            assert self.idx == start\n\t            self.obses[start:end] = payload[0]\n\t            self.next_obses[start:end] = payload[1]\n\t            self.actions[start:end] = payload[2]\n", "            self.rewards[start:end] = payload[3]\n\t            self.not_dones[start:end] = payload[4]\n\t            self.idx = end\n\t    def __getitem__(self, idx):\n\t        idx = np.random.randint(\n\t            0, self.capacity if self.full else self.idx, size=1\n\t        )\n\t        idx = idx[0]\n\t        obs = self.obses[idx]\n\t        action = self.actions[idx]\n", "        reward = self.rewards[idx]\n\t        next_obs = self.next_obses[idx]\n\t        not_done = self.not_dones[idx]\n\t        if self.transform:\n\t            obs = self.transform(obs)\n\t            next_obs = self.transform(next_obs)\n\t        return obs, action, reward, next_obs, not_done\n\t    def __len__(self):\n\t        return self.capacity \n\tclass FrameStack(gym.Wrapper):\n", "    def __init__(self, env, k):\n\t        gym.Wrapper.__init__(self, env)\n\t        self._k = k\n\t        self._frames = deque([], maxlen=k)\n\t        shp = env.observation_space.shape\n\t        self.observation_space = gym.spaces.Box(\n\t            low=0,\n\t            high=1,\n\t            shape=((shp[0] * k,) + shp[1:]),\n\t            dtype=env.observation_space.dtype\n", "        )\n\t        self._max_episode_steps = env._max_episode_steps\n\t    def reset(self):\n\t        obs = self.env.reset()\n\t        for _ in range(self._k):\n\t            self._frames.append(obs)\n\t        return self._get_obs()\n\t    def step(self, action):\n\t        obs, reward, done, info = self.env.step(action)\n\t        self._frames.append(obs)\n", "        return self._get_obs(), reward, done, info\n\t    def _get_obs(self):\n\t        assert len(self._frames) == self._k\n\t        return np.concatenate(list(self._frames), axis=0)\n\tdef random_crop(imgs, output_size):\n\t    \"\"\"\n\t    Vectorized way to do random crop using sliding windows\n\t    and picking out random ones\n\t    args:\n\t        imgs, batch images with shape (B,C,H,W)\n", "    \"\"\"\n\t    # batch size\n\t    n = imgs.shape[0]\n\t    img_size = imgs.shape[-1]\n\t    crop_max = img_size - output_size\n\t    imgs = np.transpose(imgs, (0, 2, 3, 1))\n\t    w1 = np.random.randint(0, crop_max, n)\n\t    h1 = np.random.randint(0, crop_max, n)\n\t    # creates all sliding windows combinations of size (output_size)\n\t    windows = view_as_windows(\n", "        imgs, (1, output_size, output_size, 1))[..., 0,:,:, 0]\n\t    # selects a random window for each batch element\n\t    cropped_imgs = windows[np.arange(n), w1, h1]\n\t    return cropped_imgs\n\tdef center_crop_image(image, output_size):\n\t    h, w = image.shape[1:]\n\t    new_h, new_w = output_size, output_size\n\t    top = (h - new_h)//2\n\t    left = (w - new_w)//2\n\t    image = image[:, top:top + new_h, left:left + new_w]\n", "    return image\n\tclass ReplayBufferMuJoCo(object):\n\t    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n\t        self.max_size = max_size\n\t        self.ptr = 0\n\t        self.size = 0\n\t        self.state = np.zeros((max_size, state_dim), dtype='float32')\n\t        self.action = np.zeros((max_size, action_dim), dtype='float32')\n\t        self.next_state = np.zeros((max_size, state_dim), dtype='float32')\n\t        self.reward = np.zeros((max_size, 1), dtype='float32')\n", "        self.not_done = np.zeros((max_size, 1), dtype='float32')\n\t        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\t    def add(self, state, action, next_state, reward, done):\n\t        self.state[self.ptr] = state\n\t        self.action[self.ptr] = action\n\t        self.next_state[self.ptr] = next_state\n\t        self.reward[self.ptr] = reward\n\t        self.not_done[self.ptr] = 1. - done\n\t        self.ptr = (self.ptr + 1) % self.max_size\n\t        self.size = min(self.size + 1, self.max_size)\n", "    def sample(self, batch_size):\n\t        ind = np.random.randint(0, self.size, size=batch_size)\n\t        return (\n\t            torch.from_numpy(self.state[ind]).to(self.device),\n\t            torch.from_numpy(self.action[ind]).to(self.device),\n\t            torch.from_numpy(self.next_state[ind]).to(self.device),\n\t            torch.from_numpy(self.reward[ind]).to(self.device),\n\t            torch.from_numpy(self.not_done[ind]).to(self.device)\n\t        )\n"]}
{"filename": "dmc/PEER-CURL/encoder.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tdef tie_weights(src, trg):\n\t    assert type(src) == type(trg)\n\t    trg.weight = src.weight\n\t    trg.bias = src.bias\n\t# for 84 x 84 inputs\n\tOUT_DIM = {2: 39, 4: 35, 6: 31}\n\t# for 64 x 64 inputs\n\tOUT_DIM_64 = {2: 29, 4: 25, 6: 21}\n", "class PixelEncoder(nn.Module):\n\t    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n\t    def __init__(self, obs_shape, feature_dim, num_layers=2, num_filters=32,output_logits=False):\n\t        super().__init__()\n\t        assert len(obs_shape) == 3\n\t        self.obs_shape = obs_shape\n\t        self.feature_dim = feature_dim\n\t        self.num_layers = num_layers\n\t        self.convs = nn.ModuleList(\n\t            [nn.Conv2d(obs_shape[0], num_filters, 3, stride=2)]\n", "        )\n\t        for i in range(num_layers - 1):\n\t            self.convs.append(nn.Conv2d(num_filters, num_filters, 3, stride=1))\n\t        out_dim = OUT_DIM_64[num_layers] if obs_shape[-1] == 64 else OUT_DIM[num_layers] \n\t        self.fc = nn.Linear(num_filters * out_dim * out_dim, self.feature_dim)\n\t        self.ln = nn.LayerNorm(self.feature_dim)\n\t        self.outputs = dict()\n\t        self.output_logits = output_logits\n\t    def reparameterize(self, mu, logstd):\n\t        std = torch.exp(logstd)\n", "        eps = torch.randn_like(std)\n\t        return mu + eps * std\n\t    def forward_conv(self, obs):\n\t        obs = obs / 255.\n\t        self.outputs['obs'] = obs\n\t        conv = torch.relu(self.convs[0](obs))\n\t        self.outputs['conv1'] = conv\n\t        for i in range(1, self.num_layers):\n\t            conv = torch.relu(self.convs[i](conv))\n\t            self.outputs['conv%s' % (i + 1)] = conv\n", "        h = conv.reshape(conv.size(0), -1)\n\t        return h\n\t    def forward(self, obs, detach=False):\n\t        h = self.forward_conv(obs)\n\t        if detach:\n\t            h = h.detach()\n\t        h_fc = self.fc(h)\n\t        self.outputs['fc'] = h_fc\n\t        h_norm = self.ln(h_fc)\n\t        self.outputs['ln'] = h_norm\n", "        if self.output_logits:\n\t            out = h_norm\n\t        else:\n\t            out = torch.tanh(h_norm)\n\t            self.outputs['tanh'] = out\n\t        return out\n\t    def copy_conv_weights_from(self, source):\n\t        \"\"\"Tie convolutional layers\"\"\"\n\t        # only tie conv layers\n\t        for i in range(self.num_layers):\n", "            tie_weights(src=source.convs[i], trg=self.convs[i])\n\t    def log(self, L, step, log_freq):\n\t        if step % log_freq != 0:\n\t            return\n\t        for k, v in self.outputs.items():\n\t            L.log_histogram('train_encoder/%s_hist' % k, v, step)\n\t            if len(v.shape) > 2:\n\t                L.log_image('train_encoder/%s_img' % k, v[0], step)\n\t        for i in range(self.num_layers):\n\t            L.log_param('train_encoder/conv%s' % (i + 1), self.convs[i], step)\n", "        L.log_param('train_encoder/fc', self.fc, step)\n\t        L.log_param('train_encoder/ln', self.ln, step)\n\tclass IdentityEncoder(nn.Module):\n\t    def __init__(self, obs_shape, feature_dim, num_layers, num_filters,*args):\n\t        super().__init__()\n\t        assert len(obs_shape) == 1\n\t        self.feature_dim = obs_shape[0]\n\t    def forward(self, obs, detach=False):\n\t        return obs\n\t    def copy_conv_weights_from(self, source):\n", "        pass\n\t    def log(self, L, step, log_freq):\n\t        pass\n\t_AVAILABLE_ENCODERS = {'pixel': PixelEncoder, 'identity': IdentityEncoder}\n\tdef make_encoder(\n\t    encoder_type, obs_shape, feature_dim, num_layers, num_filters, output_logits=False\n\t):\n\t    assert encoder_type in _AVAILABLE_ENCODERS\n\t    return _AVAILABLE_ENCODERS[encoder_type](\n\t        obs_shape, feature_dim, num_layers, num_filters, output_logits\n", "    )\n"]}
{"filename": "atari/PEER-DrQ/peer.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport copy\n\timport math\n\timport utils\n\timport hydra\n\timport kornia\n\timport os\n", "from replay_buffer import PrioritizedReplayBuffer\n\t# from\n\t# https://github.com/mlperf/inference/blob/master/others/edge/object_detection/ssd_mobilenet/pytorch/utils.py#L40\n\tclass Conv2d_tf(nn.Conv2d):\n\t    \"\"\"\n\t    Conv2d with the padding behavior from TF\n\t    \"\"\"\n\t    def __init__(self, *args, **kwargs):\n\t        super(Conv2d_tf, self).__init__(*args, **kwargs)\n\t        self.padding = kwargs.get(\"padding\", \"SAME\")\n", "    def _compute_padding(self, input, dim):\n\t        input_size = input.size(dim + 2)\n\t        filter_size = self.weight.size(dim + 2)\n\t        effective_filter_size = (filter_size - 1) * self.dilation[dim] + 1\n\t        out_size = (input_size + self.stride[dim] - 1) // self.stride[dim]\n\t        total_padding = max(0, (out_size - 1) * self.stride[dim] +\n\t                            effective_filter_size - input_size)\n\t        additional_padding = int(total_padding % 2 != 0)\n\t        return additional_padding, total_padding\n\t    def forward(self, input):\n", "        if self.padding == \"VALID\":\n\t            return F.conv2d(\n\t                input,\n\t                self.weight,\n\t                self.bias,\n\t                self.stride,\n\t                padding=0,\n\t                dilation=self.dilation,\n\t                groups=self.groups,\n\t            )\n", "        rows_odd, padding_rows = self._compute_padding(input, dim=0)\n\t        cols_odd, padding_cols = self._compute_padding(input, dim=1)\n\t        if rows_odd or cols_odd:\n\t            input = F.pad(input, [0, cols_odd, 0, rows_odd])\n\t        return F.conv2d(\n\t            input,\n\t            self.weight,\n\t            self.bias,\n\t            self.stride,\n\t            padding=(padding_rows // 2, padding_cols // 2),\n", "            dilation=self.dilation,\n\t            groups=self.groups,\n\t        )\n\tclass SEEncoder(nn.Module):\n\t    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n\t    def __init__(self, obs_shape):\n\t        super().__init__()\n\t        self.feature_dim = 64 * 4 * 4\n\t        self.conv1 = Conv2d_tf(obs_shape[0], 32, 5, stride=5, padding='SAME')\n\t        self.conv2 = Conv2d_tf(32, 64, 5, stride=5, padding='SAME')\n", "        self.outputs = dict()\n\t    def forward(self, obs):\n\t        obs = obs / 255.\n\t        self.outputs['obs'] = obs\n\t        h = torch.relu(self.conv1(obs))\n\t        self.outputs['conv1'] = h\n\t        h = torch.relu(self.conv2(h))\n\t        self.outputs['conv2'] = h\n\t        out = h.view(h.size(0), -1)\n\t        self.outputs['out'] = out\n", "        assert out.shape[1] == self.feature_dim\n\t        return out\n\tclass Encoder(nn.Module):\n\t    \"\"\"Convolutional encoder of pixels observations.\"\"\"\n\t    def __init__(self, obs_shape):\n\t        super().__init__()\n\t        self.feature_dim = 64 * 11 * 11\n\t        self.conv1 = Conv2d_tf(obs_shape[0], 32, 8, stride=4, padding='valid')\n\t        self.conv2 = Conv2d_tf(32, 64, 4, stride=2, padding='valid')\n\t        self.conv3 = Conv2d_tf(64, 64, 3, stride=1, padding='valid')\n", "        self.outputs = dict()\n\t    def forward(self, obs):\n\t        obs = obs / 255.\n\t        self.outputs['obs'] = obs\n\t        h = torch.relu(self.conv1(obs))\n\t        self.outputs['conv1'] = h\n\t        h = torch.relu(self.conv2(h))\n\t        self.outputs['conv2'] = h\n\t        h = torch.relu(self.conv3(h))\n\t        self.outputs['conv3'] = h\n", "        out = h.view(h.size(0), -1)\n\t        self.outputs['out'] = out\n\t        assert out.shape[1] == self.feature_dim\n\t        return out\n\tclass Intensity(nn.Module):\n\t    def __init__(self, scale=0.1):\n\t        super().__init__()\n\t        self.scale = scale\n\t    def forward(self, x):\n\t        noise = 1.0 + (self.scale * torch.randn(\n", "            (x.size(0), 1, 1, 1), device=x.device).clamp_(-2.0, 2.0))\n\t        return x * noise\n\tclass Critic(nn.Module):\n\t    \"\"\"Critic network, employes double Q-learning.\"\"\"\n\t    def __init__(self, obs_shape, num_actions, hidden_dim, hidden_depth,\n\t                 dueling, aug_type, image_pad, intensity_scale):\n\t        super().__init__()\n\t        AUGMENTATIONS = {\n\t            'intensity':\n\t            Intensity(scale=intensity_scale),\n", "            'reflect_crop':\n\t            nn.Sequential(nn.ReplicationPad2d(image_pad),\n\t                          kornia.augmentation.RandomCrop((84, 84))),\n\t            'crop_intensity':\n\t            nn.Sequential(nn.ReplicationPad2d(image_pad),\n\t                          kornia.augmentation.RandomCrop((84, 84)),\n\t                          Intensity(scale=intensity_scale)),\n\t            'zero_crop':\n\t            nn.Sequential(nn.ZeroPad2d(image_pad),\n\t                          kornia.augmentation.RandomCrop((84, 84))),\n", "            'rotate':\n\t            kornia.augmentation.RandomRotation(degrees=5.0),\n\t            'h_flip':\n\t            kornia.augmentation.RandomHorizontalFlip(p=0.5),\n\t            'v_flip':\n\t            kornia.augmentation.RandomVerticalFlip(p=0.5),\n\t            'none':\n\t            nn.Identity(),\n\t            'all':\n\t            nn.Sequential(nn.ReplicationPad2d(image_pad),\n", "                          kornia.augmentation.RandomCrop((84, 84)),\n\t                          kornia.augmentation.RandomHorizontalFlip(p=0.5),\n\t                          kornia.augmentation.RandomVerticalFlip(p=0.5),\n\t                          kornia.augmentation.RandomRotation(degrees=5.0))\n\t        }\n\t        assert aug_type in AUGMENTATIONS.keys()\n\t        self.aug_trans = AUGMENTATIONS.get(aug_type)\n\t        # self.encoder = hydra.utils.instantiate(encoder_cfg)\n\t        self.encoder = Encoder(obs_shape)\n\t        self.dueling = dueling\n", "        self.num_actions = num_actions\n\t        if dueling:\n\t            self.V = utils.mlp(self.encoder.feature_dim, hidden_dim, 1,\n\t                               hidden_depth)\n\t            self.A = utils.mlp(self.encoder.feature_dim, hidden_dim,\n\t                               num_actions, hidden_depth)\n\t        else:\n\t            self.Q = utils.mlp(self.encoder.feature_dim, hidden_dim,\n\t                               num_actions, hidden_depth)\n\t        self.outputs = dict()\n", "        self.apply(utils.weight_init)\n\t    def forward(self, obs, use_aug=False):\n\t        if use_aug:\n\t            obs = self.aug_trans(obs)\n\t        obs = self.encoder(obs)\n\t        if self.dueling:\n\t            v = self.V(obs)\n\t            a = self.A(obs)\n\t            q = v + a - a.mean(1, keepdim=True)\n\t        else:\n", "            q = self.Q(obs)\n\t        self.outputs['q'] = q\n\t        # return representation\n\t        return obs, q\n\tclass PEER(object):\n\t    \"\"\"Data regularized Q-learning: Deep Q-learning.\"\"\"\n\t    def __init__(self, obs_shape, num_actions, device,\n\t                 # encoder_cfg, critic_cfg,\n\t                 discount, lr, beta_1, beta_2, weight_decay, adam_eps,\n\t                 max_grad_norm, critic_tau, critic_target_update_frequency,\n", "                 batch_size, multistep_return, eval_eps, double_q,\n\t                 prioritized_replay_beta0, prioritized_replay_beta_steps,\n\t                 # New add for logging\n\t                 env_name, seed,\n\t                 # encoder\n\t                 # obs_shape\n\t                 # decoder\n\t                 hidden_dim, hidden_depth,\n\t                 dueling, aug_type, image_pad, intensity_scale\n\t                 ):\n", "        self.device = device\n\t        self.discount = discount\n\t        self.critic_tau = critic_tau\n\t        self.num_actions = num_actions\n\t        self.critic_target_update_frequency = critic_target_update_frequency\n\t        self.batch_size = batch_size\n\t        self.eval_eps = eval_eps\n\t        self.max_grad_norm = max_grad_norm\n\t        self.multistep_return = multistep_return\n\t        self.double_q = double_q\n", "        assert prioritized_replay_beta0 <= 1.0\n\t        self.prioritized_replay_beta0 = prioritized_replay_beta0\n\t        self.prioritized_replay_beta_steps = prioritized_replay_beta_steps\n\t        self.eps = 0\n\t        self.critic = Critic(obs_shape, num_actions, hidden_dim, hidden_depth,\n\t                 dueling, aug_type, image_pad, intensity_scale).to(self.device)\n\t        self.critic_target = Critic(obs_shape, num_actions, hidden_dim, hidden_depth,\n\t                 dueling, aug_type, image_pad, intensity_scale).to(self.device)\n\t        self.critic_target.load_state_dict(self.critic.state_dict())\n\t        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n", "                                                 lr=lr,\n\t                                                 betas=(beta_1, beta_2),\n\t                                                 weight_decay=weight_decay,\n\t                                                 eps=adam_eps)\n\t        # PEER setting\n\t        self.beta = 5e-4\n\t        self.train()\n\t        self.critic_target.train()\n\t    def train(self, training=True):\n\t        self.training = training\n", "        self.critic.train(training)\n\t    def act(self, obs):\n\t        with torch.no_grad():\n\t            obs = torch.FloatTensor(obs).to(self.device)\n\t            obs = obs.unsqueeze(0).contiguous()\n\t            _, q = self.critic(obs)\n\t            action = q.max(dim=1)[1].item()\n\t        return action\n\t    def update_critic(self, obs, action, reward, next_obs, not_done, weights,\n\t                      logger, step):\n", "        with torch.no_grad():\n\t            discount = self.discount**self.multistep_return\n\t            if self.double_q:\n\t                next_repr, next_Q_target, = self.critic_target(next_obs, use_aug=True)\n\t                _, next_Q = self.critic(next_obs, use_aug=True)\n\t                next_action = next_Q.max(dim=1)[1].unsqueeze(1)\n\t                next_Q = next_Q_target.gather(1, next_action)\n\t                target_Q = reward + (not_done * discount * next_Q)\n\t            else:\n\t                next_repr, next_Q = self.critic_target(next_obs, use_aug=True)\n", "                next_Q = next_Q.max(dim=1)[0].unsqueeze(1)\n\t                target_Q = reward + (not_done * discount * next_Q)\n\t        # get current Q estimates\n\t        current_repr, current_Q = self.critic(obs, use_aug=True)\n\t        current_Q = current_Q.gather(1, action)\n\t        td_errors = current_Q - target_Q\n\t        peer_loss = torch.einsum('ij,ij->i', [current_repr, next_repr]).mean() * self.beta\n\t        critic_losses = F.smooth_l1_loss(current_Q, target_Q, reduction='none')  + peer_loss\n\t        if weights is not None:\n\t            critic_losses *= weights\n", "        critic_loss = critic_losses.mean()\n\t        # Optimize the critic\n\t        self.critic_optimizer.zero_grad()\n\t        critic_loss.backward()\n\t        if self.max_grad_norm > 0.0:\n\t            nn.utils.clip_grad_norm_(self.critic.parameters(),\n\t                                     self.max_grad_norm)\n\t        self.critic_optimizer.step()\n\t        return td_errors.squeeze(dim=1).detach().cpu().numpy()\n\t    def update(self, replay_buffer, logger, step):\n", "        prioritized_replay = type(replay_buffer) == PrioritizedReplayBuffer\n\t        if prioritized_replay:\n\t            fraction = min(step / self.prioritized_replay_beta_steps, 1.0)\n\t            beta = self.prioritized_replay_beta0 + fraction * (\n\t                1.0 - self.prioritized_replay_beta0)\n\t            obs, action, reward, next_obs, not_done, weights, idxs = replay_buffer.sample_multistep(\n\t                self.batch_size, beta, self.discount, self.multistep_return)\n\t        else:\n\t            obs, action, reward, next_obs, not_done = replay_buffer.sample_multistep(\n\t                self.batch_size, self.discount, self.multistep_return)\n", "            weights = None\n\t        td_errors = self.update_critic(obs, action, reward, next_obs, not_done,\n\t                                       weights, logger, step)\n\t        if prioritized_replay:\n\t            prios = np.abs(td_errors) + 1e-6\n\t            replay_buffer.update_priorities(idxs, prios)\n\t        if step % self.critic_target_update_frequency == 0:\n\t            utils.soft_update_params(self.critic, self.critic_target,\n\t                                     self.critic_tau)\n"]}
{"filename": "atari/PEER-DrQ/main.py", "chunked_list": ["import os\n\timport time\n\timport numpy as np\n\timport atari\n\timport torch\n\timport utils\n\timport argparse\n\tfrom replay_buffer import ReplayBuffer, PrioritizedReplayBuffer\n\tfrom tqdm import trange\n\tfrom peer import PEER\n", "torch.backends.cudnn.benchmark = True\n\ttorch.set_num_threads(4)\n\tclass Workspace(object):\n\t    def __init__(self, cfg):\n\t        self.work_dir = os.getcwd()\n\t        print(f'workspace: {self.work_dir}')\n\t        self.cfg = cfg\n\t        utils.set_seed_everywhere(cfg.seed)\n\t        self.device = torch.device(cfg.device)\n\t        self.env = atari.make_env(cfg.env, cfg.seed, cfg.terminal_on_life_loss)\n", "        self.eval_env = atari.make_env(cfg.env, cfg.seed + 1,\n\t                                       cfg.terminal_on_life_loss)\n\t        self.agent = PEER(\n\t            obs_shape=self.env.observation_space.shape,\n\t            num_actions=self.env.action_space.n,\n\t            device=cfg.device,\n\t            # encoder_cfg,\n\t            # critic_cfg,\n\t            discount=cfg.discount,\n\t            lr=cfg.lr,\n", "            beta_1=cfg.beta_1,\n\t            beta_2=cfg.beta_2,\n\t            weight_decay=cfg.weight_decay,\n\t            adam_eps=cfg.adam_eps,\n\t            max_grad_norm=cfg.max_grad_norm,\n\t            critic_tau=cfg.critic_tau,\n\t            critic_target_update_frequency=cfg.critic_target_update_frequency,\n\t            batch_size=cfg.batch_size,\n\t            multistep_return=cfg.multistep_return,\n\t            eval_eps=cfg.eval_eps,\n", "            double_q=cfg.double_q,\n\t            prioritized_replay_beta0=cfg.prioritized_replay_beta0,\n\t            prioritized_replay_beta_steps=cfg.prioritized_replay_beta_steps,\n\t            env_name=cfg.env,\n\t            seed=cfg.seed,\n\t            hidden_dim=cfg.hidden_dim, hidden_depth=cfg.hidden_depth,\n\t            dueling=cfg.dueling, aug_type=cfg.aug_type,\n\t            image_pad=cfg.image_pad, intensity_scale=cfg.intensity_scale,\n\t        )\n\t        if cfg.prioritized_replay:\n", "            self.replay_buffer = PrioritizedReplayBuffer(\n\t                self.env.observation_space.shape, cfg.replay_buffer_capacity,\n\t                cfg.prioritized_replay_alpha, self.device)\n\t        else:\n\t            self.replay_buffer = ReplayBuffer(self.env.observation_space.shape,\n\t                                              cfg.replay_buffer_capacity,\n\t                                              self.device)\n\t        self.step = 0\n\t    def run(self):\n\t        episode, episode_reward, episode_step, done = 0, 0, 1, True\n", "        for placeholder in trange(self.cfg.num_train_steps):\n\t        # while self.step < self.cfg.num_train_steps:\n\t            if done:\n\t                print(\n\t                    f\"Algorithm: DrQ PEER,  Env: {self.cfg.env}, Num Step: {self.step}, Episode Return: {episode_reward}\")\n\t                obs = self.env.reset()\n\t                done = False\n\t                episode_reward = 0\n\t                episode_step = 0\n\t                episode += 1\n", "            steps_left = self.cfg.num_exploration_steps + self.cfg.start_training_steps - self.step\n\t            bonus = (1.0 - self.cfg.min_eps\n\t                     ) * steps_left / self.cfg.num_exploration_steps\n\t            bonus = np.clip(bonus, 0., 1. - self.cfg.min_eps)\n\t            self.agent.eps = self.cfg.min_eps + bonus\n\t            # sample action for data collection\n\t            if np.random.rand() < self.agent.eps:\n\t                action = self.env.action_space.sample()\n\t            else:\n\t                with utils.eval_mode(self.agent):\n", "                    action = self.agent.act(obs)\n\t            # run training update\n\t            if self.step >= self.cfg.start_training_steps:\n\t                for _ in range(self.cfg.num_train_iters):\n\t                    self.agent.update(self.replay_buffer, None,\n\t                                      self.step)\n\t                    # print(\"Train \" * 30)\n\t            next_obs, reward, terminal, info = self.env.step(action)\n\t            time_limit = 'TimeLimit.truncated' in info\n\t            done = info['game_over'] or time_limit\n", "            terminal = float(terminal)\n\t            terminal_no_max = 0 if time_limit else terminal\n\t            episode_reward += reward\n\t            self.replay_buffer.add(obs, action, reward, next_obs,\n\t                                   terminal_no_max)\n\t            obs = next_obs\n\t            episode_step += 1\n\t            self.step += 1\n\tdef main(cfg):\n\t    # from train import Workspace as W\n", "    workspace = Workspace(cfg)\n\t    workspace.run()\n\tparser = argparse.ArgumentParser(\"DrQ-Atari-100k\")\n\tparser.add_argument(\"--env\", default=\"Breakout\", type=str)\n\tparser.add_argument('--seed', default=1, type=int)\n\tparser.add_argument('--gpu_idx', default=0, type=int)\n\t# parser.add_argument('--debug', action=\"store_true\", type=bool)\n\targs = parser.parse_args()\n\targs.debug = False\n\tif args.debug == True:\n", "    print(\"-\" * 30)\n\t    print(\"Mode: Debug\")\n\t    print(\"-\"*30)\n\t    print()\n\t    print()\n\t    time.sleep(3)\n\tcfg=utils.CFG(env=args.env,\n\t        seed=args.seed,\n\t        debug=args.debug\n\t        )\n", "main(cfg)\n"]}
{"filename": "atari/PEER-DrQ/segment_tree.py", "chunked_list": ["import operator\n\tclass SegmentTree(object):\n\t    def __init__(self, capacity, operation, neutral_element):\n\t        \"\"\"Build a Segment Tree data structure.\n\t        https://en.wikipedia.org/wiki/Segment_tree\n\t        Can be used as regular array, but with two\n\t        important differences:\n\t            a) setting item's value is slightly slower.\n\t               It is O(lg capacity) instead of O(1).\n\t            b) user has access to an efficient ( O(log segment size) )\n", "               `reduce` operation which reduces `operation` over\n\t               a contiguous subsequence of items in the array.\n\t        Paramters\n\t        ---------\n\t        capacity: int\n\t            Total size of the array - must be a power of two.\n\t        operation: lambda obj, obj -> obj\n\t            and operation for combining elements (eg. sum, max)\n\t            must form a mathematical group together with the set of\n\t            possible values for array elements (i.e. be associative)\n", "        neutral_element: obj\n\t            neutral element for the operation above. eg. float('-inf')\n\t            for max and 0 for sum.\n\t        \"\"\"\n\t        assert capacity > 0 and capacity & (\n\t            capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n\t        self._capacity = capacity\n\t        self._value = [neutral_element for _ in range(2 * capacity)]\n\t        self._operation = operation\n\t    def _reduce_helper(self, start, end, node, node_start, node_end):\n", "        if start == node_start and end == node_end:\n\t            return self._value[node]\n\t        mid = (node_start + node_end) // 2\n\t        if end <= mid:\n\t            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n\t        else:\n\t            if mid + 1 <= start:\n\t                return self._reduce_helper(start, end, 2 * node + 1, mid + 1,\n\t                                           node_end)\n\t            else:\n", "                return self._operation(\n\t                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n\t                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1,\n\t                                        node_end))\n\t    def reduce(self, start=0, end=None):\n\t        \"\"\"Returns result of applying `self.operation`\n\t        to a contiguous subsequence of the array.\n\t            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n\t        Parameters\n\t        ----------\n", "        start: int\n\t            beginning of the subsequence\n\t        end: int\n\t            end of the subsequences\n\t        Returns\n\t        -------\n\t        reduced: obj\n\t            result of reducing self.operation over the specified range of array elements.\n\t        \"\"\"\n\t        if end is None:\n", "            end = self._capacity\n\t        if end < 0:\n\t            end += self._capacity\n\t        end -= 1\n\t        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n\t    def __setitem__(self, idx, val):\n\t        # index of the leaf\n\t        idx += self._capacity\n\t        self._value[idx] = val\n\t        idx //= 2\n", "        while idx >= 1:\n\t            self._value[idx] = self._operation(self._value[2 * idx],\n\t                                               self._value[2 * idx + 1])\n\t            idx //= 2\n\t    def __getitem__(self, idx):\n\t        assert 0 <= idx < self._capacity\n\t        return self._value[self._capacity + idx]\n\tclass SumSegmentTree(SegmentTree):\n\t    def __init__(self, capacity):\n\t        super(SumSegmentTree, self).__init__(capacity=capacity,\n", "                                             operation=operator.add,\n\t                                             neutral_element=0.0)\n\t    def sum(self, start=0, end=None):\n\t        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n\t        return super(SumSegmentTree, self).reduce(start, end)\n\t    def find_prefixsum_idx(self, prefixsum):\n\t        \"\"\"Find the highest index `i` in the array such that\n\t            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\t        if array values are probabilities, this function\n\t        allows to sample indexes according to the discrete\n", "        probability efficiently.\n\t        Parameters\n\t        ----------\n\t        perfixsum: float\n\t            upperbound on the sum of array prefix\n\t        Returns\n\t        -------\n\t        idx: int\n\t            highest index satisfying the prefixsum constraint\n\t        \"\"\"\n", "        assert 0 <= prefixsum <= self.sum() + 1e-5\n\t        idx = 1\n\t        while idx < self._capacity:  # while non-leaf\n\t            if self._value[2 * idx] > prefixsum:\n\t                idx = 2 * idx\n\t            else:\n\t                prefixsum -= self._value[2 * idx]\n\t                idx = 2 * idx + 1\n\t        return idx - self._capacity\n\tclass MinSegmentTree(SegmentTree):\n", "    def __init__(self, capacity):\n\t        super(MinSegmentTree, self).__init__(capacity=capacity,\n\t                                             operation=min,\n\t                                             neutral_element=float('inf'))\n\t    def min(self, start=0, end=None):\n\t        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n\t        return super(MinSegmentTree, self).reduce(start, end)\n"]}
{"filename": "atari/PEER-DrQ/utils.py", "chunked_list": ["import math\n\timport os\n\timport random\n\tfrom collections import deque\n\timport numpy as np\n\timport gym\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch import distributions as pyd\n", "import pickle\n\timport pandas as pd\n\tclass eval_mode(object):\n\t    def __init__(self, *models):\n\t        self.models = models\n\t    def __enter__(self):\n\t        self.prev_states = []\n\t        for model in self.models:\n\t            self.prev_states.append(model.training)\n\t            model.train(False)\n", "    def __exit__(self, *args):\n\t        for model, state in zip(self.models, self.prev_states):\n\t            model.train(state)\n\t        return False\n\tdef soft_update_params(net, target_net, tau):\n\t    for param, target_param in zip(net.parameters(), target_net.parameters()):\n\t        target_param.data.copy_(tau * param.data +\n\t                                (1 - tau) * target_param.data)\n\tdef set_seed_everywhere(seed):\n\t    torch.manual_seed(seed)\n", "    if torch.cuda.is_available():\n\t        torch.cuda.manual_seed_all(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\tdef make_dir(*path_parts):\n\t    dir_path = os.path.join(*path_parts)\n\t    try:\n\t        os.mkdir(dir_path)\n\t    except OSError:\n\t        pass\n", "    return dir_path\n\tdef tie_weights(src, trg):\n\t    assert type(src) == type(trg)\n\t    trg.weight = src.weight\n\t    trg.bias = src.bias\n\tdef weight_init(m):\n\t    \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n\t    if isinstance(m, nn.Linear):\n\t        nn.init.orthogonal_(m.weight.data)\n\t        if hasattr(m.bias, 'data'):\n", "            m.bias.data.fill_(0.0)\n\t    elif isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n\t        gain = nn.init.calculate_gain('relu')\n\t        nn.init.orthogonal_(m.weight.data, gain)\n\t        if hasattr(m.bias, 'data'):\n\t            m.bias.data.fill_(0.0)\n\tdef mlp(input_dim, hidden_dim, output_dim, hidden_depth, output_mod=None):\n\t    if hidden_depth == 0:\n\t        mods = [nn.Linear(input_dim, output_dim)]\n\t    else:\n", "        mods = [nn.Linear(input_dim, hidden_dim), nn.ReLU(inplace=True)]\n\t        for i in range(hidden_depth - 1):\n\t            mods += [nn.Linear(hidden_dim, hidden_dim), nn.ReLU(inplace=True)]\n\t        mods.append(nn.Linear(hidden_dim, output_dim))\n\t    if output_mod is not None:\n\t        mods.append(output_mod)\n\t    trunk = nn.Sequential(*mods)\n\t    return trunk\n\tdef to_np(t):\n\t    if t is None:\n", "        return None\n\t    elif t.nelement() == 0:\n\t        return np.array([])\n\t    else:\n\t        return t.cpu().detach().numpy()\n\tclass FrameStack(gym.Wrapper):\n\t    def __init__(self, env, k):\n\t        gym.Wrapper.__init__(self, env)\n\t        self._k = k\n\t        self._frames = deque([], maxlen=k)\n", "        shp = env.observation_space.shape\n\t        self.observation_space = gym.spaces.Box(\n\t            low=0,\n\t            high=1,\n\t            shape=((shp[0] * k,) + shp[1:]),\n\t            dtype=env.observation_space.dtype)\n\t        self._max_episode_steps = env._max_episode_steps\n\t    def reset(self):\n\t        obs = self.env.reset()\n\t        for _ in range(self._k):\n", "            self._frames.append(obs)\n\t        return self._get_obs()\n\t    def step(self, action):\n\t        obs, reward, done, info = self.env.step(action)\n\t        self._frames.append(obs)\n\t        return self._get_obs(), reward, done, info\n\t    def _get_obs(self):\n\t        assert len(self._frames) == self._k\n\t        return np.concatenate(list(self._frames), axis=0)\n\tclass TanhTransform(pyd.transforms.Transform):\n", "    domain = pyd.constraints.real\n\t    codomain = pyd.constraints.interval(-1.0, 1.0)\n\t    bijective = True\n\t    sign = +1\n\t    def __init__(self, cache_size=1):\n\t        super().__init__(cache_size=cache_size)\n\t    @staticmethod\n\t    def atanh(x):\n\t        return 0.5 * (x.log1p() - (-x).log1p())\n\t    def __eq__(self, other):\n", "        return isinstance(other, TanhTransform)\n\t    def _call(self, x):\n\t        return x.tanh()\n\t    def _inverse(self, y):\n\t        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n\t        # one should use `cache_size=1` instead\n\t        return self.atanh(y)\n\t    def log_abs_det_jacobian(self, x, y):\n\t        # We use a formula that is more numerically stable, see details in the following link\n\t        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n", "        return 2. * (math.log(2.) - x - F.softplus(-2. * x))\n\tclass SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n\t    def __init__(self, loc, scale):\n\t        self.loc = loc\n\t        self.scale = scale\n\t        self.base_dist = pyd.Normal(loc, scale)\n\t        transforms = [TanhTransform()]\n\t        super().__init__(self.base_dist, transforms)\n\t    @property\n\t    def mean(self):\n", "        mu = self.loc\n\t        for tr in self.transforms:\n\t            mu = tr(mu)\n\t        return mu\n\tclass CFG():\n\t    env = \"Breakout\"\n\t    debug=False\n\t    terminal_on_life_loss = True\n\t    # train\n\t    num_train_steps = 100000\n", "    num_train_iters = 1\n\t    num_exploration_steps = 5000\n\t    start_training_steps = 1600 #\n\t    min_eps = 0.1\n\t    prioritized_replay = False\n\t    prioritized_replay_alpha = 0.6\n\t    seed = 1\n\t    discount=0.99\n\t    eval_frequency = 100000\n\t    num_eval_steps = 100000\n", "    log_frequency_step = 10000\n\t    log_save_tb = False\n\t    save_video = False\n\t    save_train_video = False\n\t    device = \"cuda\"\n\t    # observation\n\t    image_pad = 4\n\t    intensity_scale = 0.1\n\t    aug_type = \"all\"\n\t    # global params\n", "    lr = 0.0001\n\t    beta_1 = 0.9\n\t    beta_2 = 0.999\n\t    weight_decay = 0.0\n\t    adam_eps = 0.00015\n\t    max_grad_norm = 10.0\n\t    hidden_depth = 1\n\t    batch_size = 32\n\t    # agent\n\t    obs_shape= 100 # to be specified later\n", "    num_actions= 100\n\t    critic_tau= 1.0\n\t    critic_target_update_frequency= 1\n\t    multistep_return= 10\n\t    eval_eps= 0.05\n\t    double_q = True\n\t    prioritized_replay_beta0= 0.4\n\t    # critic\n\t    hidden_dim= 512\n\t    dueling= True\n", "    def __init__(self, env, seed, debug):\n\t        self.replay_buffer_capacity = self.num_train_steps\n\t        self.prioritized_replay_beta_steps = self.num_train_steps\n\t        self.env = env\n\t        self.seed = int(seed)\n\t        self.debug = debug\n"]}
{"filename": "atari/PEER-DrQ/replay_buffer.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport utils\n\tfrom segment_tree import SumSegmentTree, MinSegmentTree\n\tclass ReplayBuffer(object):\n\t    def __init__(self, obs_shape, capacity, device):\n\t        self.capacity = capacity\n\t        self.device = device\n", "        self.obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n\t        self.next_obses = np.empty((capacity, *obs_shape), dtype=np.uint8)\n\t        self.actions = np.empty((capacity, 1), dtype=np.int64)\n\t        self.rewards = np.empty((capacity, 1), dtype=np.float32)\n\t        self.not_dones = np.empty((capacity, 1), dtype=np.float32)\n\t        self.idx = 0\n\t        self.full = False\n\t    def __len__(self):\n\t        return self.capacity if self.full else self.idx\n\t    def add(self, obs, action, reward, next_obs, done):\n", "        np.copyto(self.obses[self.idx], obs)\n\t        np.copyto(self.actions[self.idx], action)\n\t        np.copyto(self.rewards[self.idx], reward)\n\t        np.copyto(self.next_obses[self.idx], next_obs)\n\t        np.copyto(self.not_dones[self.idx], not done)\n\t        self.idx = (self.idx + 1) % self.capacity\n\t        self.full = self.full or self.idx == 0\n\t    def fetch(self, idxs, discount, n):\n\t        assert idxs.max() + n <= len(self)\n\t        obses = self.obses[idxs]\n", "        next_obses = self.next_obses[idxs + n - 1]\n\t        obses = torch.as_tensor(obses, device=self.device).float()\n\t        next_obses = torch.as_tensor(next_obses, device=self.device).float()\n\t        actions = torch.as_tensor(self.actions[idxs], device=self.device)\n\t        rewards = np.zeros((idxs.shape[0], 1), dtype=np.float32)\n\t        not_dones = np.ones((idxs.shape[0], 1), dtype=np.float32)\n\t        for i in range(n):\n\t            rewards += (discount**n) * not_dones * np.sign(\n\t                self.rewards[idxs + i])\n\t            not_dones = np.minimum(not_dones, self.not_dones[idxs + i])\n", "        rewards = torch.as_tensor(rewards, device=self.device)\n\t        not_dones = torch.as_tensor(not_dones, device=self.device)\n\t        return obses, actions, rewards, next_obses, not_dones\n\t    def sample_idxs(self, batch_size, n):\n\t        last_idx = (self.capacity if self.full else self.idx) - (n - 1)\n\t        idxs = np.random.randint(0, last_idx, size=batch_size)\n\t        return idxs\n\t    def sample_multistep(self, batch_size, discount, n):\n\t        assert n <= self.idx or self.full\n\t        idxs = self.sample_idxs(batch_size, n)\n", "        return self.fetch(idxs, discount, n)\n\tclass PrioritizedReplayBuffer(ReplayBuffer):\n\t    def __init__(self, obs_shape, capacity, alpha, device):\n\t        super().__init__(obs_shape, capacity, device)\n\t        assert alpha >= 0\n\t        self.alpha = alpha\n\t        tree_capacity = 1\n\t        while tree_capacity < capacity:\n\t            tree_capacity *= 2\n\t        self.sum_tree = SumSegmentTree(tree_capacity)\n", "        self.min_tree = MinSegmentTree(tree_capacity)\n\t        self.max_priority = 1.0\n\t    def add(self, obs, action, reward, next_obs, done):\n\t        super().add(obs, action, reward, next_obs, done)\n\t        self.sum_tree[self.idx] = self.max_priority**self.alpha\n\t        self.min_tree[self.idx] = self.max_priority**self.alpha\n\t    def sample_idxs(self, batch_size, n):\n\t        idxs = []\n\t        p_total = self.sum_tree.sum(0, len(self) - n - 1)\n\t        every_range_len = p_total / batch_size\n", "        for i in range(batch_size):\n\t            while True:\n\t                mass = np.random.rand() * every_range_len + i * every_range_len\n\t                idx = self.sum_tree.find_prefixsum_idx(mass)\n\t                if idx + n <= len(self):\n\t                    idxs.append(idx)\n\t                    break\n\t        return np.array(idxs)\n\t    def sample_multistep(self, batch_size, beta, discount, n):\n\t        assert n <= self.idx or self.full\n", "        assert beta > 0\n\t        idxs = self.sample_idxs(batch_size, n)\n\t        weights = []\n\t        p_min = self.min_tree.min() / self.sum_tree.sum()\n\t        max_weight = (p_min * len(self))**(-beta)\n\t        for idx in idxs:\n\t            p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n\t            weight = (p_sample * len(self))**(-beta)\n\t            weights.append(weight / max_weight)\n\t        weights = torch.as_tensor(np.array(weights),\n", "                                  device=self.device).unsqueeze(dim=1)\n\t        sample = self.fetch(idxs, discount, n)\n\t        return tuple(list(sample) + [weights, idxs])\n\t    def update_priorities(self, idxs, prios):\n\t        assert idxs.shape[0] == prios.shape[0]\n\t        for idx, prio in zip(idxs, prios):\n\t            assert prio > 0\n\t            assert 0 <= idx < len(self)\n\t            self.sum_tree[idx] = prio**self.alpha\n\t            self.min_tree[idx] = prio**self.alpha\n", "            self.max_priority = max(self.max_priority, prio)\n"]}
{"filename": "atari/PEER-DrQ/atari.py", "chunked_list": ["# coding=utf-8\n\t# Copyright 2019 The SEED Authors\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\t\"\"\"A class implementing minimal Atari 2600 preprocessing.\n\tAdapted from Dopamine.\n\t\"\"\"\n\tfrom gym.spaces.box import Box\n\timport numpy as np\n\timport gym\n\tfrom collections import deque\n", "import cv2\n\tclass AtariPreprocessing(object):\n\t    \"\"\"A class implementing image preprocessing for Atari 2600 agents.\n\t    Specifically, this provides the following subset from the JAIR paper\n\t    (Bellemare et al., 2013) and Nature DQN paper (Mnih et al., 2015):\n\t    * Frame skipping (defaults to 4).\n\t    * Terminal signal when a life is lost (off by default).\n\t    * Grayscale and max-pooling of the last two frames.\n\t    * Downsample the screen to a square image (defaults to 84x84).\n\t    More generally, this class follows the preprocessing guidelines set down in\n", "    Machado et al. (2018), \"Revisiting the Arcade Learning Environment:\n\t    Evaluation Protocols and Open Problems for General Agents\".\n\t    It also provides random starting no-ops, which are used in the Rainbow, Apex\n\t    and R2D2 papers.\n\t    \"\"\"\n\t    def __init__(self,\n\t                 environment,\n\t                 frame_skip=4,\n\t                 terminal_on_life_loss=False,\n\t                 screen_size=84,\n", "                 max_random_noops=0):\n\t        \"\"\"Constructor for an Atari 2600 preprocessor.\n\t        Args:\n\t          environment: Gym environment whose observations are preprocessed.\n\t          frame_skip: int, the frequency at which the agent experiences the game.\n\t          terminal_on_life_loss: bool, If True, the step() method returns\n\t            is_terminal=True whenever a life is lost. See Mnih et al. 2015.\n\t          screen_size: int, size of a resized Atari 2600 frame.\n\t          max_random_noops: int, maximum number of no-ops to apply at the beginning\n\t            of each episode to reduce determinism. These no-ops are applied at a\n", "            low-level, before frame skipping.\n\t        Raises:\n\t          ValueError: if frame_skip or screen_size are not strictly positive.\n\t        \"\"\"\n\t        if frame_skip <= 0:\n\t            raise ValueError(\n\t                'Frame skip should be strictly positive, got {}'.format(\n\t                    frame_skip))\n\t        if screen_size <= 0:\n\t            raise ValueError(\n", "                'Target screen size should be strictly positive, got {}'.\n\t                format(screen_size))\n\t        self.environment = environment\n\t        self.terminal_on_life_loss = terminal_on_life_loss\n\t        self.frame_skip = frame_skip\n\t        self.screen_size = screen_size\n\t        self.max_random_noops = max_random_noops\n\t        obs_dims = self.environment.observation_space\n\t        # Stores temporary observations used for pooling over two successive\n\t        # frames.\n", "        self.screen_buffer = [\n\t            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8),\n\t            np.empty((obs_dims.shape[0], obs_dims.shape[1]), dtype=np.uint8)\n\t        ]\n\t        self.game_over = False\n\t        self.lives = 0  # Will need to be set by reset().\n\t    @property\n\t    def observation_space(self):\n\t        # Return the observation space adjusted to match the shape of the processed\n\t        # observations.\n", "        return Box(low=0,\n\t                   high=255,\n\t                   shape=(self.screen_size, self.screen_size, 1),\n\t                   dtype=np.uint8)\n\t    @property\n\t    def action_space(self):\n\t        return self.environment.action_space\n\t    @property\n\t    def reward_range(self):\n\t        return self.environment.reward_range\n", "    @property\n\t    def metadata(self):\n\t        return self.environment.metadata\n\t    def close(self):\n\t        return self.environment.close()\n\t    def apply_random_noops(self):\n\t        \"\"\"Steps self.environment with random no-ops.\"\"\"\n\t        if self.max_random_noops <= 0:\n\t            return\n\t        # Other no-ops implementations actually always do at least 1 no-op. We\n", "        # follow them.\n\t        no_ops = self.environment.np_random.randint(1,\n\t                                                    self.max_random_noops + 1)\n\t        for _ in range(no_ops):\n\t            _, _, game_over, _ = self.environment.step(0)\n\t            if game_over:\n\t                self.environment.reset()\n\t    def reset(self):\n\t        \"\"\"Resets the environment.\n\t        Returns:\n", "          observation: numpy array, the initial observation emitted by the\n\t            environment.\n\t        \"\"\"\n\t        self.environment.reset()\n\t        self.apply_random_noops()\n\t        self.lives = self.environment.ale.lives()\n\t        self._fetch_grayscale_observation(self.screen_buffer[0])\n\t        self.screen_buffer[1].fill(0)\n\t        return self._pool_and_resize()\n\t    def render(self, mode):\n", "        \"\"\"Renders the current screen, before preprocessing.\n\t        This calls the Gym API's render() method.\n\t        Args:\n\t          mode: Mode argument for the environment's render() method.\n\t            Valid values (str) are:\n\t              'rgb_array': returns the raw ALE image.\n\t              'human': renders to display via the Gym renderer.\n\t        Returns:\n\t          if mode='rgb_array': numpy array, the most recent screen.\n\t          if mode='human': bool, whether the rendering was successful.\n", "        \"\"\"\n\t        return self.environment.render(mode)\n\t    def step(self, action):\n\t        \"\"\"Applies the given action in the environment.\n\t        Remarks:\n\t          * If a terminal state (from life loss or episode end) is reached, this may\n\t            execute fewer than self.frame_skip steps in the environment.\n\t          * Furthermore, in this case the returned observation may not contain valid\n\t            image data and should be ignored.\n\t        Args:\n", "          action: The action to be executed.\n\t        Returns:\n\t          observation: numpy array, the observation following the action.\n\t          reward: float, the reward following the action.\n\t          is_terminal: bool, whether the environment has reached a terminal state.\n\t            This is true when a life is lost and terminal_on_life_loss, or when the\n\t            episode is over.\n\t          info: Gym API's info data structure.\n\t        \"\"\"\n\t        accumulated_reward = 0.\n", "        for time_step in range(self.frame_skip):\n\t            # We bypass the Gym observation altogether and directly fetch the\n\t            # grayscale image from the ALE. This is a little faster.\n\t            _, reward, game_over, info = self.environment.step(action)\n\t            accumulated_reward += reward\n\t            info['game_over'] = game_over\n\t            if self.terminal_on_life_loss:\n\t                new_lives = self.environment.ale.lives()\n\t                is_terminal = game_over or new_lives < self.lives\n\t                self.lives = new_lives\n", "            else:\n\t                is_terminal = game_over\n\t            if is_terminal:\n\t                break\n\t            # We max-pool over the last two frames, in grayscale.\n\t            elif time_step >= self.frame_skip - 2:\n\t                t = time_step - (self.frame_skip - 2)\n\t                self._fetch_grayscale_observation(self.screen_buffer[t])\n\t        # Pool the last two observations.\n\t        observation = self._pool_and_resize()\n", "        self.game_over = game_over\n\t        return observation, accumulated_reward, is_terminal, info\n\t    def _fetch_grayscale_observation(self, output):\n\t        \"\"\"Returns the current observation in grayscale.\n\t        The returned observation is stored in 'output'.\n\t        Args:\n\t          output: numpy array, screen buffer to hold the returned observation.\n\t        Returns:\n\t          observation: numpy array, the current observation in grayscale.\n\t        \"\"\"\n", "        self.environment.ale.getScreenGrayscale(output)\n\t        return output\n\t    def _pool_and_resize(self):\n\t        \"\"\"Transforms two frames into a Nature DQN observation.\n\t        For efficiency, the transformation is done in-place in self.screen_buffer.\n\t        Returns:\n\t          transformed_screen: numpy array, pooled, resized screen.\n\t        \"\"\"\n\t        # Pool if there are enough screens to do so.\n\t        if self.frame_skip > 1:\n", "            np.maximum(self.screen_buffer[0],\n\t                       self.screen_buffer[1],\n\t                       out=self.screen_buffer[0])\n\t        transformed_image = cv2.resize(self.screen_buffer[0],\n\t                                       (self.screen_size, self.screen_size),\n\t                                       interpolation=cv2.INTER_LINEAR)\n\t        int_image = np.asarray(transformed_image, dtype=np.uint8)\n\t        return np.expand_dims(int_image, axis=2)\n\tclass TimeLimit(gym.Wrapper):\n\t    def __init__(self, env, max_episode_steps=None):\n", "        super().__init__(env)\n\t        self._max_episode_steps = max_episode_steps\n\t        self._elapsed_steps = 0\n\t    def step(self, ac):\n\t        observation, reward, done, info = self.env.step(ac)\n\t        self._elapsed_steps += 1\n\t        if self._elapsed_steps >= self._max_episode_steps:\n\t            done = True\n\t            info['TimeLimit.truncated'] = True\n\t        return observation, reward, done, info\n", "    def reset(self, **kwargs):\n\t        self._elapsed_steps = 0\n\t        return self.env.reset(**kwargs)\n\tclass FrameStack(gym.Wrapper):\n\t    def __init__(self, env, k):\n\t        super().__init__(env)\n\t        self.k = k\n\t        self.frames = deque([], maxlen=k)\n\t        shp = env.observation_space.shape\n\t        self.observation_space = gym.spaces.Box(\n", "            low=0,\n\t            high=255,\n\t            shape=(shp[:-1] + (shp[-1] * k,)),\n\t            dtype=env.observation_space.dtype)\n\t    def reset(self):\n\t        ob = self.env.reset()\n\t        for _ in range(self.k):\n\t            self.frames.append(ob)\n\t        return self._get_ob()\n\t    def step(self, action):\n", "        ob, reward, done, info = self.env.step(action)\n\t        self.frames.append(ob)\n\t        return self._get_ob(), reward, done, info\n\t    def _get_ob(self):\n\t        assert len(self.frames) == self.k\n\t        return np.concatenate(self.frames, axis=-1)\n\tclass ImageToPyTorch(gym.ObservationWrapper):\n\t    def __init__(self, env):\n\t        super().__init__(env)\n\t        old_shape = self.observation_space.shape\n", "        self.observation_space = gym.spaces.Box(\n\t            low=0,\n\t            high=255,\n\t            shape=(old_shape[-1], old_shape[0], old_shape[1]),\n\t            dtype=np.uint8,\n\t        )\n\t    def observation(self, observation):\n\t        return np.transpose(observation, axes=(2, 0, 1))\n\tdef make_env(env, seed, terminal_on_life_loss):\n\t    # Also try with sticky actions, instead of random noops\n", "    env_id = f'{env}NoFrameskip-v4'\n\t    env = gym.make(env_id)\n\t    env.seed(seed)\n\t    # terminal_on_life_loss: see section 4.1 in https://arxiv.org/pdf/1812.06110.pdf\n\t    # use env.env to remove TimeLimit\n\t    env = AtariPreprocessing(env.env,\n\t                             frame_skip=4,\n\t                             max_random_noops=0,\n\t                             terminal_on_life_loss=terminal_on_life_loss)\n\t    env = TimeLimit(env, max_episode_steps=27000)\n", "    env = FrameStack(env, k=4)\n\t    env = ImageToPyTorch(env)\n\t    return env\n"]}
{"filename": "atari/PEER-CURL/model.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# MIT License\n\t#\n\t# Copyright (c) 2017 Kai Arulkumaran\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\t#\n\t# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\t# ==============================================================================\n\tfrom __future__ import division\n", "import math\n\timport torch\n\tfrom torch import nn\n\tfrom torch.nn import functional as F\n\t# Factorised NoisyLinear layer with bias\n\tclass NoisyLinear(nn.Module):\n\t    def __init__(self, in_features, out_features, std_init=0.5):\n\t        super(NoisyLinear, self).__init__()\n\t        self.module_name = 'noisy_linear'\n\t        self.in_features = in_features\n", "        self.out_features = out_features\n\t        self.std_init = std_init\n\t        self.weight_mu = nn.Parameter(torch.empty(out_features, in_features))\n\t        self.weight_sigma = nn.Parameter(torch.empty(out_features, in_features))\n\t        self.register_buffer('weight_epsilon', torch.empty(out_features, in_features))\n\t        self.bias_mu = nn.Parameter(torch.empty(out_features))\n\t        self.bias_sigma = nn.Parameter(torch.empty(out_features))\n\t        self.register_buffer('bias_epsilon', torch.empty(out_features))\n\t        self.reset_parameters()\n\t        self.reset_noise()\n", "    def reset_parameters(self):\n\t        mu_range = 1 / math.sqrt(self.in_features)\n\t        self.weight_mu.data.uniform_(-mu_range, mu_range)\n\t        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n\t        self.bias_mu.data.uniform_(-mu_range, mu_range)\n\t        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n\t    def _scale_noise(self, size):\n\t        x = torch.randn(size)\n\t        return x.sign().mul_(x.abs().sqrt_())\n\t    def reset_noise(self):\n", "        epsilon_in = self._scale_noise(self.in_features)\n\t        epsilon_out = self._scale_noise(self.out_features)\n\t        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n\t        self.bias_epsilon.copy_(epsilon_out)\n\t    def forward(self, input):\n\t        if self.training:\n\t            return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon,\n\t                            self.bias_mu + self.bias_sigma * self.bias_epsilon)\n\t        else:\n\t            return F.linear(input, self.weight_mu, self.bias_mu)\n", "class DQN(nn.Module):\n\t    def __init__(self, args, action_space):\n\t        super(DQN, self).__init__()\n\t        self.atoms = args.atoms\n\t        self.action_space = action_space\n\t        if args.architecture == 'canonical':\n\t            self.convs = nn.Sequential(nn.Conv2d(args.history_length, 32, 8, stride=4, padding=0), nn.ReLU(),\n\t                                       nn.Conv2d(32, 64, 4, stride=2, padding=0), nn.ReLU(),\n\t                                       nn.Conv2d(64, 64, 3, stride=1, padding=0), nn.ReLU())\n\t            self.conv_output_size = 3136\n", "        elif args.architecture == 'data-efficient':\n\t            self.convs = nn.Sequential(nn.Conv2d(args.history_length, 32, 5, stride=5, padding=0), nn.ReLU(),\n\t                                       nn.Conv2d(32, 64, 5, stride=5, padding=0), nn.ReLU())\n\t            self.conv_output_size = 576\n\t        self.fc_h_v = NoisyLinear(self.conv_output_size, args.hidden_size, std_init=args.noisy_std)\n\t        self.fc_h_a = NoisyLinear(self.conv_output_size, args.hidden_size, std_init=args.noisy_std)\n\t        self.fc_z_v = NoisyLinear(args.hidden_size, self.atoms, std_init=args.noisy_std)\n\t        self.fc_z_a = NoisyLinear(args.hidden_size, action_space * self.atoms, std_init=args.noisy_std)\n\t        self.W_h = nn.Parameter(torch.rand(self.conv_output_size, args.hidden_size))\n\t        self.W_c = nn.Parameter(torch.rand(args.hidden_size, 128))\n", "        self.b_h = nn.Parameter(torch.zeros(args.hidden_size))\n\t        self.b_c = nn.Parameter(torch.zeros(128))\n\t        self.W = nn.Parameter(torch.rand(128, 128))\n\t        # self.layernorm1 = nn.LayerNorm(256).to(\"cuda\")\n\t        # self.layernorm2 = nn.LayerNorm(128).to(\"cuda\")\n\t    def forward(self, x, log=False, representation=False):\n\t        x = self.convs(x)\n\t        x = x.view(-1, self.conv_output_size) # X is representation\n\t        v = self.fc_z_v(F.relu(self.fc_h_v(x)))  # Value stream\n\t        a = self.fc_z_a(F.relu(self.fc_h_a(x)))  # Advantage stream\n", "        h = torch.matmul(x, self.W_h) + self.b_h  # Contrastive head\n\t        # h = self.layernorm1(h)\n\t        h = nn.LayerNorm(h.shape[1], device=\"cuda\")(h)\n\t        h = F.relu(h)\n\t        h = torch.matmul(h, self.W_c) + self.b_c  # Contrastive head\n\t        # h = self.layernorm2(h)\n\t        h = nn.LayerNorm(128, device=\"cuda\")(h)\n\t        v, a = v.view(-1, 1, self.atoms), a.view(-1, self.action_space, self.atoms)\n\t        q = v + a - a.mean(1, keepdim=True)  # Combine streams\n\t        if log:  # Use log softmax for numerical stability\n", "            q = F.log_softmax(q, dim=2)  # Log probabilities with action over second dimension\n\t        else:\n\t            q = F.softmax(q, dim=2)  # Probabilities with action over second dimension\n\t        if representation:\n\t            return q, h, x\n\t        else:\n\t            return q, h\n\t    def reset_noise(self):\n\t        for name, module in self.named_children():\n\t            if 'fc' in name:\n", "                module.reset_noise()\n"]}
{"filename": "atari/PEER-CURL/main.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# MIT License\n\t#\n\t# Copyright (c) 2017 Kai Arulkumaran\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\t#\n\t# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\t# ==============================================================================\n\tfrom __future__ import division\n", "import argparse\n\timport bz2\n\timport os\n\timport pickle\n\tfrom datetime import datetime\n\timport atari_py\n\timport numpy as np\n\timport torch\n\tfrom tqdm import trange\n\tfrom agent import Agent\n", "from env import Env\n\tfrom memory import ReplayMemory\n\tfrom utils import set_seed_everywhere\n\t# Note that hyperparameters may originally be reported in ATARI game frames instead of agent steps\n\tparser = argparse.ArgumentParser(description='PEER')\n\tparser.add_argument('--id', type=str, default='default', help='Experiment ID')\n\tparser.add_argument('--seed', type=int, default=0, help='Random seed')\n\tparser.add_argument('--disable-cuda', action='store_true', help='Disable CUDA')\n\tparser.add_argument('--game', type=str, default='ms_pacman', choices=atari_py.list_games(), help='ATARI game')\n\tparser.add_argument('--T-max', type=int, default=int(1e5), metavar='STEPS',\n", "                    help='Number of training steps (4x number of frames)')\n\tparser.add_argument('--max-episode-length', type=int, default=int(108e3), metavar='LENGTH',\n\t                    help='Max episode length in game frames (0 to disable)')\n\tparser.add_argument('--history-length', type=int, default=4, metavar='T', help='Number of consecutive states processed')\n\tparser.add_argument('--architecture', type=str, default='data-efficient', choices=['canonical', 'data-efficient'],\n\t                    metavar='ARCH', help='Network architecture')\n\tparser.add_argument('--hidden-size', type=int, default=256, metavar='SIZE', help='Network hidden size')\n\tparser.add_argument('--noisy-std', type=float, default=0.1, metavar='σ',\n\t                    help='Initial standard deviation of noisy linear layers')\n\tparser.add_argument('--atoms', type=int, default=51, metavar='C', help='Discretised size of value distribution')\n", "parser.add_argument('--V-min', type=float, default=-10, metavar='V', help='Minimum of value distribution support')\n\tparser.add_argument('--V-max', type=float, default=10, metavar='V', help='Maximum of value distribution support')\n\tparser.add_argument('--model', type=str, metavar='PARAMS', help='Pretrained model (state dict)')\n\tparser.add_argument('--memory-capacity', type=int, default=int(1e5), metavar='CAPACITY',\n\t                    help='Experience replay memory capacity')\n\tparser.add_argument('--replay-frequency', type=int, default=1, metavar='k', help='Frequency of sampling from memory')\n\tparser.add_argument('--priority-exponent', type=float, default=0.5, metavar='ω',\n\t                    help='Prioritised experience replay exponent (originally denoted α)')\n\tparser.add_argument('--priority-weight', type=float, default=0.4, metavar='β',\n\t                    help='Initial prioritised experience replay importance sampling weight')\n", "parser.add_argument('--multi-step', type=int, default=20, metavar='n', help='Number of steps for multi-step return')\n\tparser.add_argument('--discount', type=float, default=0.99, metavar='γ', help='Discount factor')\n\tparser.add_argument('--target-update', type=int, default=int(2e3), metavar='τ',\n\t                    help='Number of steps after which to update target network')\n\tparser.add_argument('--reward-clip', type=int, default=1, metavar='VALUE', help='Reward clipping (0 to disable)')\n\tparser.add_argument('--learning-rate', type=float, default=0.0001, metavar='η', help='Learning rate')\n\tparser.add_argument('--adam-eps', type=float, default=1.5e-5, metavar='ε', help='Adam epsilon')\n\tparser.add_argument('--batch-size', type=int, default=32, metavar='SIZE', help='Batch size') # May be we\n\tparser.add_argument('--norm-clip', type=float, default=10, metavar='NORM', help='Max L2 norm for gradient clipping')\n\tparser.add_argument('--learn-start', type=int, default=int(1600), metavar='STEPS',\n", "                    help='Number of steps before starting training')\n\tparser.add_argument('--evaluate', action='store_true', help='Evaluate only')\n\tparser.add_argument('--evaluation-interval', type=int, default=10000, metavar='STEPS',\n\t                    help='Number of training steps between evaluations')\n\tparser.add_argument('--evaluation-episodes', type=int, default=10, metavar='N',\n\t                    help='Number of evaluation episodes to average over')\n\tparser.add_argument('--evaluation-size', type=int, default=500, metavar='N',\n\t                    help='Number of transitions to use for validating Q')\n\tparser.add_argument('--render', action='store_true', help='Display screen (testing only)')\n\tparser.add_argument('--enable-cudnn', default=True, help='Enable cuDNN (faster but nondeterministic)')\n", "parser.add_argument('--checkpoint-interval', default=0,\n\t                    help='How often to checkpoint the model, defaults to 0 (never checkpoint)')\n\tparser.add_argument('--memory', help='Path to save/load the memory from')\n\tparser.add_argument('--disable-bzip-memory', action='store_true',\n\t                    help='Don\\'t zip the memory file. Not recommended (zipping is a bit slower and much, much smaller)')\n\tparser.add_argument(\"--peer_coef\", type=float, default=5e-4)\n\t# Setup\n\targs = parser.parse_args()\n\txid = 'peer-' + args.game + '-' + str(args.seed)\n\targs.id = xid\n", "print(' ' * 26 + 'Options')\n\tfor k, v in vars(args).items():\n\t    print(' ' * 26 + k + ': ' + str(v))\n\tset_seed_everywhere(args.seed)\n\targs.device = torch.device('cuda')\n\t# torch.cuda.manual_seed(np.random.randint(1, 10000))\n\ttorch.backends.cudnn.enabled = True\n\tdef load_memory(memory_path, disable_bzip):\n\t    if disable_bzip:\n\t        with open(memory_path, 'rb') as pickle_file:\n", "            return pickle.load(pickle_file)\n\t    else:\n\t        with bz2.open(memory_path, 'rb') as zipped_pickle_file:\n\t            return pickle.load(zipped_pickle_file)\n\tdef save_memory(memory, memory_path, disable_bzip):\n\t    if disable_bzip:\n\t        with open(memory_path, 'wb') as pickle_file:\n\t            pickle.dump(memory, pickle_file)\n\t    else:\n\t        with bz2.open(memory_path, 'wb') as zipped_pickle_file:\n", "            pickle.dump(memory, zipped_pickle_file)\n\t# Environment\n\tenv = Env(args)\n\tenv.train()\n\taction_space = env.action_space()\n\t# PEER Agent\n\tpeer = Agent(args, env)\n\tmem = ReplayMemory(args, args.memory_capacity)\n\tpriority_weight_increase = (1 - args.priority_weight) / (args.T_max - args.learn_start)\n\t# Construct validation memory\n", "val_mem = ReplayMemory(args, args.evaluation_size)\n\tT, done = 0, True\n\twhile T < args.evaluation_size:\n\t    if done:\n\t        state, done = env.reset(), False\n\t    next_state, _, done = env.step(np.random.randint(0, action_space))\n\t    val_mem.append(state, None, None, done)\n\t    state = next_state\n\t    T += 1\n\telse:\n", "    # Training loop\n\t    peer.train()\n\t    # Initial testing\n\t    T, done = 0, True\n\t    for T in trange(1, args.T_max + 1):\n\t        if done:\n\t            state, done = env.reset(), False\n\t        if T % args.replay_frequency == 0:\n\t            peer.reset_noise()  # Draw a new set of noisy weights\n\t        action = peer.act(state)  # Choose an action greedily (with noisy weights)\n", "        next_state, reward, done = env.step(action)  # Step\n\t        if args.reward_clip > 0:\n\t            reward = max(min(reward, args.reward_clip), -args.reward_clip)  # Clip rewards\n\t        mem.append(state, action, reward, done)  # Append transition to memory\n\t        # Train and test\n\t        if T >= args.learn_start:\n\t            mem.priority_weight = min(mem.priority_weight + priority_weight_increase,\n\t                                      1)  # Anneal importance sampling weight β to 1\n\t            if T % args.replay_frequency == 0:\n\t                # for _ in range(4):\n", "                peer.learn(mem)  # Train with n-step distributional double-Q learning\n\t                peer.update_momentum_net()  # MoCo momentum upate\n\t            if T % args.evaluation_interval == 0:\n\t                peer.eval()  # Set DQN (online network) to evaluation mode\n\t                peer.train()  # Set DQN (online network) back to training mode\n\t                # If memory path provided, save it\n\t                if args.memory is not None:\n\t                    save_memory(mem, args.memory, args.disable_bzip_memory)\n\t            # Update target network\n\t            if T % args.target_update == 0:\n", "                peer.update_target_net()\n\t        state = next_state\n\tenv.close()\n"]}
{"filename": "atari/PEER-CURL/agent.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# MIT License\n\t#\n\t# Copyright (c) 2017 Kai Arulkumaran\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\t#\n\t# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\t# ==============================================================================\n\t# PEER agent\n", "from __future__ import division\n\timport os\n\timport kornia.augmentation as aug\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom torch import optim\n\tfrom torch.nn.utils import clip_grad_norm_\n\tfrom model import DQN\n\trandom_shift = nn.Sequential(aug.RandomCrop((80, 80)), nn.ReplicationPad2d(4), aug.RandomCrop((84, 84)))\n", "aug = random_shift\n\tclass Agent():\n\t    def __init__(self, args, env):\n\t        self.args = args\n\t        self.action_space = env.action_space()\n\t        self.atoms = args.atoms\n\t        self.Vmin = args.V_min\n\t        self.Vmax = args.V_max\n\t        self.support = torch.linspace(args.V_min, args.V_max, self.atoms).to(device=args.device)  # Support (range) of z\n\t        self.delta_z = (args.V_max - args.V_min) / (self.atoms - 1)\n", "        self.batch_size = args.batch_size\n\t        self.n = args.multi_step\n\t        self.discount = args.discount\n\t        self.norm_clip = args.norm_clip\n\t        self.coeff = 0.01 if args.game in ['pong', 'boxing', 'private_eye', 'freeway'] else 1.\n\t        self.peer_coef = args.peer_coef\n\t        self.online_net = DQN(args, self.action_space).to(device=args.device)\n\t        self.momentum_net = DQN(args, self.action_space).to(device=args.device)\n\t        self.online_net.train()\n\t        self.initialize_momentum_net()\n", "        self.momentum_net.train()\n\t        self.target_net = DQN(args, self.action_space).to(device=args.device)\n\t        self.update_target_net()\n\t        self.target_net.train()\n\t        for param in self.target_net.parameters():\n\t            param.requires_grad = False\n\t        for param in self.momentum_net.parameters():\n\t            param.requires_grad = False\n\t        self.optimiser = optim.Adam(self.online_net.parameters(), lr=args.learning_rate, eps=args.adam_eps)\n\t    # Resets noisy weights in all linear layers (of online net only)\n", "    def reset_noise(self):\n\t        self.online_net.reset_noise()\n\t    # Acts based on single state (no batch)\n\t    def act(self, state):\n\t        with torch.no_grad():\n\t            a, _ = self.online_net(state.unsqueeze(0))\n\t            return (a * self.support).sum(2).argmax(1).item()\n\t    # Acts with an ε-greedy policy (used for evaluation only)\n\t    def act_e_greedy(self, state, epsilon=0.001):  # High ε can reduce evaluation scores drastically\n\t        return np.random.randint(0, self.action_space) if np.random.random() < epsilon else self.act(state)\n", "    def learn(self, mem):\n\t        # Sample transitions\n\t        idxs, states, actions, returns, next_states, nonterminals, weights = mem.sample(self.batch_size)\n\t        aug_states_1 = aug(states).to(device=self.args.device)\n\t        aug_states_2 = aug(states).to(device=self.args.device)\n\t        # Calculate current state probabilities (online network noise already sampled)\n\t        log_ps, _, current_representation = self.online_net(states, log=True, representation=True)  # Log probabilities log p(s_t, ·; θonline)\n\t        _, z_anch = self.online_net(aug_states_1, log=True)\n\t        _, z_target = self.momentum_net(aug_states_2, log=True)\n\t        z_proj = torch.matmul(self.online_net.W, z_target.T)\n", "        logits = torch.matmul(z_anch, z_proj)\n\t        logits = (logits - torch.max(logits, 1)[0][:, None])\n\t        logits = logits * 0.1\n\t        labels = torch.arange(logits.shape[0]).long().to(device=self.args.device)\n\t        moco_loss = (nn.CrossEntropyLoss()(logits, labels)).to(device=self.args.device)\n\t        log_ps_a = log_ps[range(self.batch_size), actions]  # log p(s_t, a_t; θonline)\n\t        with torch.no_grad():\n\t            # Calculate nth next state probabilities\n\t            pns, _ = self.online_net(next_states)  # Probabilities p(s_t+n, ·; θonline)\n\t            dns = self.support.expand_as(pns) * pns  # Distribution d_t+n = (z, p(s_t+n, ·; θonline))\n", "            argmax_indices_ns = dns.sum(2).argmax(\n\t                1)  # Perform argmax action selection using online network: argmax_a[(z, p(s_t+n, a; θonline))]\n\t            self.target_net.reset_noise()  # Sample new target net noise\n\t            pns, _, target_representation = self.target_net(next_states, representation=True)  # Probabilities p(s_t+n, ·; θtarget)\n\t            pns_a = pns[range(\n\t                self.batch_size), argmax_indices_ns]  # Double-Q probabilities p(s_t+n, argmax_a[(z, p(s_t+n, a; θonline))]; θtarget)\n\t            # Compute Tz (Bellman operator T applied to z)\n\t            Tz = returns.unsqueeze(1) + nonterminals * (self.discount ** self.n) * self.support.unsqueeze(\n\t                0)  # Tz = R^n + (γ^n)z (accounting for terminal states)\n\t            Tz = Tz.clamp(min=self.Vmin, max=self.Vmax)  # Clamp between supported values\n", "            # Compute L2 projection of Tz onto fixed support z\n\t            b = (Tz - self.Vmin) / self.delta_z  # b = (Tz - Vmin) / Δz\n\t            l, u = b.floor().to(torch.int64), b.ceil().to(torch.int64)\n\t            # Fix disappearing probability mass when l = b = u (b is int)\n\t            l[(u > 0) * (l == u)] -= 1\n\t            u[(l < (self.atoms - 1)) * (l == u)] += 1\n\t            # Distribute probability of Tz\n\t            m = states.new_zeros(self.batch_size, self.atoms)\n\t            offset = torch.linspace(0, ((self.batch_size - 1) * self.atoms), self.batch_size).unsqueeze(1).expand(\n\t                self.batch_size, self.atoms).to(actions)\n", "            m.view(-1).index_add_(0, (l + offset).view(-1),\n\t                                  (pns_a * (u.float() - b)).view(-1))  # m_l = m_l + p(s_t+n, a*)(u - b)\n\t            m.view(-1).index_add_(0, (u + offset).view(-1),\n\t                                  (pns_a * (b - l.float())).view(-1))  # m_u = m_u + p(s_t+n, a*)(b - l)\n\t        peer_loss = torch.einsum('ij,ij->i', [current_representation, target_representation]).mean()\n\t        # feature_loss2 = torch.einsum('ij,ij->i', [current_feature2, target_feature2]).mean()\n\t        loss = -torch.sum(m * log_ps_a, 1)  # Cross-entropy loss (minimises DKL(m||p(s_t, a_t)))\n\t        loss = loss + (moco_loss * self.coeff) + self.peer_coef * peer_loss\n\t        self.online_net.zero_grad()\n\t        curl_loss = (weights * loss).mean()\n", "        curl_loss.mean().backward()  # Backpropagate importance-weighted minibatch loss\n\t        clip_grad_norm_(self.online_net.parameters(), self.norm_clip)  # Clip gradients by L2 norm\n\t        self.optimiser.step()\n\t        mem.update_priorities(idxs, loss.detach().cpu().numpy())  # Update priorities of sampled transitions\n\t    def update_target_net(self):\n\t        self.target_net.load_state_dict(self.online_net.state_dict())\n\t    def initialize_momentum_net(self):\n\t        for param_q, param_k in zip(self.online_net.parameters(), self.momentum_net.parameters()):\n\t            param_k.data.copy_(param_q.data)  # update\n\t            param_k.requires_grad = False  # not update by gradient\n", "    # Code for this function from https://github.com/facebookresearch/moco\n\t    @torch.no_grad()\n\t    def update_momentum_net(self, momentum=0.999):\n\t        for param_q, param_k in zip(self.online_net.parameters(), self.momentum_net.parameters()):\n\t            param_k.data.copy_(momentum * param_k.data + (1. - momentum) * param_q.data)  # update\n\t    # Save model parameters on current device (don't move model between devices)\n\t    def save(self, path, name='model.pth'):\n\t        torch.save(self.online_net.state_dict(), os.path.join(path, name))\n\t    # Evaluates Q-value based on single state (no batch)\n\t    def evaluate_q(self, state):\n", "        with torch.no_grad():\n\t            a, _ = self.online_net(state.unsqueeze(0))\n\t            return (a * self.support).sum(2).max(1)[0].item()\n\t    def train(self):\n\t        self.online_net.train()\n\t    def eval(self):\n\t        self.online_net.eval()\n"]}
{"filename": "atari/PEER-CURL/utils.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport random\n\tdef set_seed_everywhere(seed):\n\t    torch.manual_seed(seed)\n\t    if torch.cuda.is_available():\n\t        torch.cuda.manual_seed_all(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n"]}
{"filename": "atari/PEER-CURL/env.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# MIT License\n\t#\n\t# Copyright (c) 2017 Kai Arulkumaran\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\t#\n\t# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\t# ==============================================================================\n\timport random\n", "from collections import deque\n\timport atari_py\n\timport cv2\n\timport torch\n\tclass Env():\n\t    def __init__(self, args):\n\t        # self.device = \"cpu\"\n\t        self.device = args.device\n\t        self.ale = atari_py.ALEInterface()\n\t        self.ale.setInt('random_seed', args.seed)\n", "        self.ale.setInt('max_num_frames_per_episode', args.max_episode_length)\n\t        self.ale.setFloat('repeat_action_probability', 0)  # Disable sticky actions\n\t        self.ale.setInt('frame_skip', 0)\n\t        self.ale.setBool('color_averaging', False)\n\t        self.ale.loadROM(atari_py.get_game_path(args.game))  # ROM loading must be done after setting options\n\t        actions = self.ale.getMinimalActionSet()\n\t        self.actions = dict([i, e] for i, e in zip(range(len(actions)), actions))\n\t        self.lives = 0  # Life counter (used in DeepMind training)\n\t        self.life_termination = False  # Used to check if resetting only from loss of life\n\t        self.window = args.history_length  # Number of frames to concatenate\n", "        self.state_buffer = deque([], maxlen=args.history_length)\n\t        self.training = True  # Consistent with model training mode\n\t    def _get_state(self):\n\t        state = cv2.resize(self.ale.getScreenGrayscale(), (84, 84), interpolation=cv2.INTER_LINEAR)\n\t        return torch.tensor(state, dtype=torch.float32, device=self.device).div_(255)\n\t    def _reset_buffer(self):\n\t        for _ in range(self.window):\n\t            self.state_buffer.append(torch.zeros(84, 84, device=self.device))\n\t    def reset(self):\n\t        if self.life_termination:\n", "            self.life_termination = False  # Reset flag\n\t            self.ale.act(0)  # Use a no-op after loss of life\n\t        else:\n\t            # Reset internals\n\t            self._reset_buffer()\n\t            self.ale.reset_game()\n\t            # Perform up to 30 random no-ops before starting\n\t            for _ in range(random.randrange(30)):\n\t                self.ale.act(0)  # Assumes raw action 0 is always no-op\n\t                if self.ale.game_over():\n", "                    self.ale.reset_game()\n\t        # Process and return \"initial\" state\n\t        observation = self._get_state()\n\t        self.state_buffer.append(observation)\n\t        self.lives = self.ale.lives()\n\t        return torch.stack(list(self.state_buffer), 0)\n\t    def step(self, action):\n\t        # Repeat action 4 times, max pool over last 2 frames\n\t        frame_buffer = torch.zeros(2, 84, 84, device=self.device)\n\t        reward, done = 0, False\n", "        for t in range(4):\n\t            reward += self.ale.act(self.actions.get(action))\n\t            if t == 2:\n\t                frame_buffer[0] = self._get_state()\n\t            elif t == 3:\n\t                frame_buffer[1] = self._get_state()\n\t            done = self.ale.game_over()\n\t            if done:\n\t                break\n\t        observation = frame_buffer.max(0)[0]\n", "        self.state_buffer.append(observation)\n\t        # Detect loss of life as terminal in training mode\n\t        if self.training:\n\t            lives = self.ale.lives()\n\t            if lives < self.lives and lives > 0:  # Lives > 0 for Q*bert\n\t                self.life_termination = not done  # Only set flag when not truly done\n\t                done = True\n\t            self.lives = lives\n\t        # Return state, reward, done\n\t        return torch.stack(list(self.state_buffer), 0), reward, done\n", "    # Uses loss of life as terminal signal\n\t    def train(self):\n\t        self.training = True\n\t    # Uses standard terminal signal\n\t    def eval(self):\n\t        self.training = False\n\t    def action_space(self):\n\t        return len(self.actions)\n\t    def render(self):\n\t        cv2.imshow('screen', self.ale.getScreenRGB()[:, :, ::-1])\n", "        cv2.waitKey(1)\n\t    def close(self):\n\t        cv2.destroyAllWindows()\n"]}
{"filename": "atari/PEER-CURL/memory.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# MIT License\n\t#\n\t# Copyright (c) 2017 Kai Arulkumaran\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\t#\n\t# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\t# ==============================================================================\n\tfrom __future__ import division\n", "from collections import namedtuple\n\timport numpy as np\n\timport torch\n\tTransition = namedtuple('Transition', ('timestep', 'state', 'action', 'reward', 'nonterminal'))\n\tblank_trans = Transition(0, torch.zeros(84, 84, dtype=torch.uint8), None, 0, False)\n\t# Segment tree data structure where parent node values are sum/max of children node values\n\tclass SegmentTree():\n\t    def __init__(self, size):\n\t        self.index = 0\n\t        self.size = size\n", "        self.full = False  # Used to track actual capacity\n\t        self.sum_tree = np.zeros((2 * size - 1,),\n\t                                 dtype=np.float32)  # Initialise fixed size tree with all (priority) zeros\n\t        self.data = np.array([None] * size)  # Wrap-around cyclic buffer\n\t        self.max = 1  # Initial max value to return (1 = 1^ω)\n\t    # Propagates value up tree given a tree index\n\t    def _propagate(self, index, value):\n\t        parent = (index - 1) // 2\n\t        left, right = 2 * parent + 1, 2 * parent + 2\n\t        self.sum_tree[parent] = self.sum_tree[left] + self.sum_tree[right]\n", "        if parent != 0:\n\t            self._propagate(parent, value)\n\t    # Updates value given a tree index\n\t    def update(self, index, value):\n\t        self.sum_tree[index] = value  # Set new value\n\t        self._propagate(index, value)  # Propagate value\n\t        self.max = max(value, self.max)\n\t    def append(self, data, value):\n\t        self.data[self.index] = data  # Store data in underlying data structure\n\t        self.update(self.index + self.size - 1, value)  # Update tree\n", "        self.index = (self.index + 1) % self.size  # Update index\n\t        self.full = self.full or self.index == 0  # Save when capacity reached\n\t        self.max = max(value, self.max)\n\t    # Searches for the location of a value in sum tree\n\t    def _retrieve(self, index, value):\n\t        left, right = 2 * index + 1, 2 * index + 2\n\t        if left >= len(self.sum_tree):\n\t            return index\n\t        elif value <= self.sum_tree[left]:\n\t            return self._retrieve(left, value)\n", "        else:\n\t            return self._retrieve(right, value - self.sum_tree[left])\n\t    # Searches for a value in sum tree and returns value, data index and tree index\n\t    def find(self, value):\n\t        index = self._retrieve(0, value)  # Search for index of item from root\n\t        data_index = index - self.size + 1\n\t        return (self.sum_tree[index], data_index, index)  # Return value, data index, tree index\n\t    # Returns data given a data index\n\t    def get(self, data_index):\n\t        return self.data[data_index % self.size]\n", "    def total(self):\n\t        return self.sum_tree[0]\n\tclass ReplayMemory():\n\t    def __init__(self, args, capacity):\n\t        self.device = args.device\n\t        self.capacity = capacity\n\t        self.history = args.history_length\n\t        self.discount = args.discount\n\t        self.n = args.multi_step\n\t        self.priority_weight = args.priority_weight  # Initial importance sampling weight β, annealed to 1 over course of training\n", "        self.priority_exponent = args.priority_exponent\n\t        self.t = 0  # Internal episode timestep counter\n\t        self.transitions = SegmentTree(\n\t            capacity)  # Store transitions in a wrap-around cyclic buffer within a sum tree for querying priorities\n\t    # Adds state and action at time t, reward and terminal at time t + 1\n\t    def append(self, state, action, reward, terminal):\n\t        state = state[-1].mul(255).to(dtype=torch.uint8,\n\t                                      device=torch.device('cpu'))  # Only store last frame and discretise to save memory\n\t        self.transitions.append(Transition(self.t, state, action, reward, not terminal),\n\t                                self.transitions.max)  # Store new transition with maximum priority\n", "        self.t = 0 if terminal else self.t + 1  # Start new episodes with t = 0\n\t    # Returns a transition with blank states where appropriate\n\t    def _get_transition(self, idx):\n\t        transition = np.array([None] * (self.history + self.n))\n\t        transition[self.history - 1] = self.transitions.get(idx)\n\t        for t in range(self.history - 2, -1, -1):  # e.g. 2 1 0\n\t            if transition[t + 1].timestep == 0:\n\t                transition[t] = blank_trans  # If future frame has timestep 0\n\t            else:\n\t                transition[t] = self.transitions.get(idx - self.history + 1 + t)\n", "        for t in range(self.history, self.history + self.n):  # e.g. 4 5 6\n\t            if transition[t - 1].nonterminal:\n\t                transition[t] = self.transitions.get(idx - self.history + 1 + t)\n\t            else:\n\t                transition[t] = blank_trans  # If prev (next) frame is terminal\n\t        return transition\n\t    # Returns a valid sample from a segment\n\t    def _get_sample_from_segment(self, segment, i):\n\t        valid = False\n\t        while not valid:\n", "            sample = np.random.uniform(i * segment,\n\t                                       (i + 1) * segment)  # Uniformly sample an element from within a segment\n\t            prob, idx, tree_idx = self.transitions.find(\n\t                sample)  # Retrieve sample from tree with un-normalised probability\n\t            # Resample if transition straddled current index or probablity 0\n\t            if (self.transitions.index - idx) % self.capacity > self.n and (\n\t                    idx - self.transitions.index) % self.capacity >= self.history and prob != 0:\n\t                valid = True  # Note that conditions are valid but extra conservative around buffer index 0\n\t        # Retrieve all required transition data (from t - h to t + n)\n\t        transition = self._get_transition(idx)\n", "        # Create un-discretised state and nth next state\n\t        state = torch.stack([trans.state for trans in transition[:self.history]]).to(device=self.device).to(\n\t            dtype=torch.float32).div_(255)\n\t        next_state = torch.stack([trans.state for trans in transition[self.n:self.n + self.history]]).to(\n\t            device=self.device).to(dtype=torch.float32).div_(255)\n\t        # Discrete action to be used as index\n\t        action = torch.tensor([transition[self.history - 1].action], dtype=torch.int64, device=self.device)\n\t        # Calculate truncated n-step discounted return R^n = Σ_k=0->n-1 (γ^k)R_t+k+1 (note that invalid nth next states have reward 0)\n\t        R = torch.tensor([sum(self.discount ** n * transition[self.history + n - 1].reward for n in range(self.n))],\n\t                         dtype=torch.float32, device=self.device)\n", "        # Mask for non-terminal nth next states\n\t        nonterminal = torch.tensor([transition[self.history + self.n - 1].nonterminal], dtype=torch.float32,\n\t                                   device=self.device)\n\t        return prob, idx, tree_idx, state, action, R, next_state, nonterminal\n\t    def sample(self, batch_size):\n\t        p_total = self.transitions.total()  # Retrieve sum of all priorities (used to create a normalised probability distribution)\n\t        segment = p_total / batch_size  # Batch size number of segments, based on sum over all probabilities\n\t        batch = [self._get_sample_from_segment(segment, i) for i in range(batch_size)]  # Get batch of valid samples\n\t        probs, idxs, tree_idxs, states, actions, returns, next_states, nonterminals = zip(*batch)\n\t        states, next_states, = torch.stack(states), torch.stack(next_states)\n", "        actions, returns, nonterminals = torch.cat(actions), torch.cat(returns), torch.stack(nonterminals)\n\t        probs = np.array(probs, dtype=np.float32) / p_total  # Calculate normalised probabilities\n\t        capacity = self.capacity if self.transitions.full else self.transitions.index\n\t        weights = (capacity * probs) ** -self.priority_weight  # Compute importance-sampling weights w\n\t        weights = torch.tensor(weights / weights.max(), dtype=torch.float32,\n\t                               device=self.device)  # Normalise by max importance-sampling weight from batch\n\t        return tree_idxs, states, actions, returns, next_states, nonterminals, weights\n\t    def update_priorities(self, idxs, priorities):\n\t        priorities = np.power(priorities, self.priority_exponent)\n\t        [self.transitions.update(idx, priority) for idx, priority in zip(idxs, priorities)]\n", "    # Set up internal state for iterator\n\t    def __iter__(self):\n\t        self.current_idx = 0\n\t        return self\n\t    # Return valid states for validation\n\t    def __next__(self):\n\t        if self.current_idx == self.capacity:\n\t            raise StopIteration\n\t        # Create stack of states\n\t        state_stack = [None] * self.history\n", "        state_stack[-1] = self.transitions.data[self.current_idx].state\n\t        prev_timestep = self.transitions.data[self.current_idx].timestep\n\t        for t in reversed(range(self.history - 1)):\n\t            if prev_timestep == 0:\n\t                state_stack[t] = blank_trans.state  # If future frame has timestep 0\n\t            else:\n\t                state_stack[t] = self.transitions.data[self.current_idx + t - self.history + 1].state\n\t                prev_timestep -= 1\n\t        state = torch.stack(state_stack, 0).to(dtype=torch.float32, device=self.device).div_(\n\t            255)  # Agent will turn into batch\n", "        self.current_idx += 1\n\t        return state\n\t    next = __next__  # Alias __next__ for Python 2 compatibility\n"]}
