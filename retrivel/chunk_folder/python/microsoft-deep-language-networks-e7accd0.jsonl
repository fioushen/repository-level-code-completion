{"filename": "scripts/split_bigbench_date_understanding.py", "chunked_list": ["import os\n\timport json\n\timport codecs\n\tfrom utils import download_json_from_url\n\tbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/date_understanding/task.json\"\n\tbbh_file_path = \"data/bbh/date_understanding.json\"\n\toption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]\n\tbbh_sentence, bbh_label = [], []\n\tbbh_dict = {}\n\twith open(bbh_file_path) as fin:\n", "    data = json.load(fin)\n\t    data = data[\"examples\"]\n\t    for i in range(len(data)):\n\t        input, target = data[i][\"input\"], data[i][\"target\"]\n\t        bbh_sentence.append(input)\n\t        bbh_label.append(target)\n\t        q = input.split(\"\\n\")[0]\n\t        assert q not in bbh_dict\n\t        bbh_dict[q] = len(bbh_sentence) - 1\n\tdata = download_json_from_url(bb_raw_json_url)\n", "data = data[\"examples\"]\n\tbb_sentence, bb_label = [], []\n\tbb_len = 0\n\tin_bbh = 0\n\tfor i in range(len(data)):\n\t    bb_len += 1\n\t    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n\t    if input in bbh_dict:\n\t        in_bbh += 1\n\t        continue\n", "    res_input = [input]\n\t    res_target = \"\"\n\t    for j, key in enumerate(target):\n\t        if target[key] == 1:\n\t            res_target = option_list[j]\n\t        res_input.append(\" \".join([option_list[j], key]))\n\t    assert len(res_target) > 0\n\t    res_input = \"\\n\".join(res_input)\n\t    bb_sentence.append(res_input)\n\t    bb_label.append(res_target)\n", "assert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n\tdata = []\n\tfor i in range(len(bb_sentence)):\n\t    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\n\tprint(\"there are %s data points in big bench\" % str(bb_len))\n\tprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\n\tprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\tbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\n\tprint(\"writing data to \", bb_minus_bbh_file_path)\n\tif not os.path.exists(bb_minus_bbh_file_path):\n", "    os.makedirs(bb_minus_bbh_file_path)\n\twith codecs.open(bb_minus_bbh_file_path + \"/date_understanding.json\", 'w', encoding='utf-8') as json_file:\n\t    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/split_bigbench_hyperbaton.py", "chunked_list": ["import os\n\timport json\n\timport codecs\n\tfrom utils import download_json_from_url\n\tbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/hyperbaton/task.json\"\n\tbbh_file_path = \"data/bbh/hyperbaton.json\"\n\toption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]\n\tbbh_sentence, bbh_label = [], []\n\tbbh_dict = {}\n\twith open(bbh_file_path) as fin:\n", "    data = json.load(fin)\n\t    data = data[\"examples\"]\n\t    for i in range(len(data)):\n\t        input, target = data[i][\"input\"], data[i][\"target\"]\n\t        bbh_sentence.append(input)\n\t        bbh_label.append(target)\n\t        assert input not in bbh_dict\n\t        bbh_dict[input] = len(bbh_sentence) - 1\n\tdata = download_json_from_url(bb_raw_json_url)\n\tdata = data[\"examples\"]\n", "bb_sentence, bb_label = [], []\n\tbb_len = 0\n\tin_bbh = 0\n\tfor i in range(len(data)):\n\t    bb_len += 1\n\t    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n\t    candidates = input.split(\"Which sentence has the correct adjective order:\")[-1].strip()\n\t    input = \"Which sentence has the correct adjective order:\"\n\t    candidates = [candidates.split(\"\\\"\")[1].strip(), candidates.split(\"\\\"\")[3].strip()]\n\t    res_input = [input, \"Options:\"]\n", "    res_target = \"\"\n\t    for j, key in enumerate(target):\n\t        if target[key] == 1:\n\t            res_target = option_list[j]\n\t        res_input.append(\" \".join([option_list[j], candidates[j]]))\n\t    assert len(res_target) > 0\n\t    res_input = \"\\n\".join(res_input)\n\t    if res_input in bbh_dict:\n\t        in_bbh += 1\n\t        continue\n", "    bb_sentence.append(res_input)\n\t    bb_label.append(res_target)\n\tassert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n\tdata = []\n\tfor i in range(len(bb_sentence)):\n\t    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\n\tprint(\"there are %s data points in big bench\" % str(bb_len))\n\tprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\n\tprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\tbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\n", "print(\"writing data to \", bb_minus_bbh_file_path)\n\tif not os.path.exists(bb_minus_bbh_file_path):\n\t    os.makedirs(bb_minus_bbh_file_path)\n\twith codecs.open(bb_minus_bbh_file_path + \"/hyperbaton.json\", 'w', encoding='utf-8') as json_file:\n\t    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/split_bigbench_logical_deduction_seven_objects.py", "chunked_list": ["import os\n\timport json\n\timport codecs\n\tfrom utils import download_json_from_url\n\tbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/logical_deduction/seven_objects/task.json\"\n\tbbh_file_path = \"data/bbh/logical_deduction_seven_objects.json\"\n\toption_list = [\"(A)\", \"(B)\", \"(C)\", \"(D)\", \"(E)\", \"(F)\", \"(G)\", \"(H)\", \"(I)\", \"(J)\", \"(K)\", \"(L)\"]\n\tbbh_sentence, bbh_label = [], []\n\tbbh_dict = {}\n\twith open(bbh_file_path) as fin:\n", "    data = json.load(fin)\n\t    data = data[\"examples\"]\n\t    for i in range(len(data)):\n\t        input, target = data[i][\"input\"], data[i][\"target\"]\n\t        bbh_sentence.append(input)\n\t        bbh_label.append(target)\n\t        assert input not in bbh_dict\n\t        bbh_dict[input] = len(bbh_sentence) - 1\n\tdata = download_json_from_url(bb_raw_json_url)\n\tdata = data[\"examples\"]\n", "bb_sentence, bb_label = [], []\n\tbb_len = 0\n\tin_bbh = 0\n\tfor i in range(len(data)):\n\t    bb_len += 1\n\t    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n\t    res_input = [\"The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph. \" + input, \"Options:\"]\n\t    res_target = \"\"\n\t    for j, key in enumerate(target):\n\t        if target[key] == 1:\n", "            res_target = option_list[j]\n\t        res_input.append(\" \".join([option_list[j], key[:-1]]))  # no periods in bbh\n\t    assert len(res_target) > 0\n\t    res_input = \"\\n\".join(res_input)\n\t    if res_input in bbh_dict:\n\t        in_bbh += 1\n\t        continue\n\t    bb_sentence.append(res_input)\n\t    bb_label.append(res_target)\n\tassert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n", "data = []\n\tfor i in range(len(bb_sentence)):\n\t    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\n\tprint(\"there are %s data points in big bench\" % str(bb_len))\n\tprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\n\tprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\tbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\n\tprint(\"writing data to \", bb_minus_bbh_file_path)\n\tif not os.path.exists(bb_minus_bbh_file_path):\n\t    os.makedirs(bb_minus_bbh_file_path)\n", "with codecs.open(bb_minus_bbh_file_path + \"/logical_deduction_seven_objects.json\", 'w', encoding='utf-8') as json_file:\n\t    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/split_bigbench_navigate.py", "chunked_list": ["import os\n\timport json\n\timport codecs\n\tfrom utils import download_json_from_url\n\tbb_raw_json_url = \"https://raw.githubusercontent.com/google/BIG-bench/main/bigbench/benchmark_tasks/navigate/task.json\"\n\tbbh_file_path = \"data/bbh/navigate.json\"\n\toption_list = [\"Yes\", \"No\"]\n\toption_map = {\"True\": \"- Yes\", \"False\": \"- No\"}\n\tbbh_sentence, bbh_label = [], []\n\tbbh_dict = {}\n", "with open(bbh_file_path) as fin:\n\t    data = json.load(fin)\n\t    data = data[\"examples\"]\n\t    for i in range(len(data)):\n\t        input, target = data[i][\"input\"], data[i][\"target\"]\n\t        bbh_sentence.append(input)\n\t        bbh_label.append(target)\n\t        assert input not in bbh_dict\n\t        bbh_dict[input] = len(bbh_sentence) - 1\n\tdata = download_json_from_url(bb_raw_json_url)\n", "data = data[\"examples\"]\n\tbb_sentence, bb_label = [], []\n\tbb_len = 0\n\tin_bbh = 0\n\tfor i in range(len(data)):\n\t    bb_len += 1\n\t    input, target = data[i][\"input\"], data[i][\"target_scores\"]\n\t    res_input = [\"If you follow these instructions, do you return to the starting point? \" + input, \"Options:\"]\n\t    res_target = \"\"\n\t    for j, key in enumerate(target):\n", "        if target[key] == 1:\n\t            res_target = option_list[j]\n\t        res_input.append(option_map[key])\n\t    assert len(res_target) > 0\n\t    res_input = \"\\n\".join(res_input)\n\t    if res_input in bbh_dict:\n\t        in_bbh += 1\n\t        continue\n\t    bb_sentence.append(res_input)\n\t    bb_label.append(res_target)\n", "assert in_bbh == len(bbh_dict), \"%s, %s\" % (str(in_bbh), str(len(bbh_dict)))\n\tdata = []\n\tfor i in range(len(bb_sentence)):\n\t    data.append({\"input\": bb_sentence[i], \"target\": bb_label[i]})\n\tprint(\"there are %s data points in big bench\" % str(bb_len))\n\tprint(\"there are %s data points in big bench hard\" % str(len(bbh_dict)))\n\tprint(\"collected %s data points from big bench - big bench hard\" % str(len(data)))\n\tbb_minus_bbh_file_path = \"data/bb_minus_bbh/\"\n\tprint(\"writing data to \", bb_minus_bbh_file_path)\n\tif not os.path.exists(bb_minus_bbh_file_path):\n", "    os.makedirs(bb_minus_bbh_file_path)\n\twith codecs.open(bb_minus_bbh_file_path + \"/navigate.json\", 'w', encoding='utf-8') as json_file:\n\t    json.dump({\"examples\": data}, json_file, ensure_ascii=False)\n"]}
{"filename": "scripts/__init__.py", "chunked_list": []}
{"filename": "scripts/utils.py", "chunked_list": ["import requests\n\tdef download_json_from_url(url):\n\t    response = requests.get(url)\n\t    response.raise_for_status()\n\t    json_data = response.json()\n\t    return json_data\n"]}
{"filename": "tests/test_dln_score.py", "chunked_list": ["from unittest.mock import patch\n\timport numpy as np\n\timport tiktoken\n\tfrom dln.score import LogProbsScore, OutputClasses\n\tdef test_logprobs_score_with_output_classes(score_requests, top_logprobs):\n\t    encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n\t    logprobs_score = LogProbsScore(encoder)\n\t    with patch(\"dln.score.forward_evaluate\", top_logprobs):\n\t        logprobs = logprobs_score.score_requests(\n\t            score_requests, output_classes=OutputClasses(protos=[\"a|A\", \"b|B\"])\n", "        )\n\t    np.testing.assert_almost_equal(logprobs.targets, [-8.6746863, -0.4428973])\n\t    np.testing.assert_almost_equal(\n\t        logprobs.contexts,\n\t        [\n\t            [9.99829143e-01, 1.70856546e-04],\n\t            [6.42173164e-01, 3.57826836e-01],\n\t        ],\n\t    )\n\tdef test_logprobs_score_without_output_classes(score_requests, raw_logprobs):\n", "    encoder = tiktoken.encoding_for_model(\"text-davinci-003\")\n\t    logprobs_score = LogProbsScore(encoder)\n\t    with patch(\"dln.score.forward_evaluate\", raw_logprobs):\n\t        logprobs = logprobs_score.score_requests(score_requests)\n\t    np.testing.assert_almost_equal(logprobs.targets, [-0.7682657, -0.7632834])\n\t    np.testing.assert_almost_equal(logprobs.contexts, [-2.8217665, -2.73069])\n"]}
{"filename": "tests/test_dln_losses.py", "chunked_list": ["import numpy as np\n\tfrom dln.loss import ZeroOneLoss\n\tdef test_zero_one_loss():\n\t    y = [\"a\", \"b\", \"c\", \"a\", \"b\", \"c\"]\n\t    y_hat = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"]\n\t    zero_one_loss = ZeroOneLoss(lambda x: x)\n\t    losses = zero_one_loss(y, y_hat)\n\t    np.testing.assert_array_equal(losses, [0.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n\tdef test_zero_one_loss_no_postproc():\n\t    y = [\"A\", \"B\", \"C\", \"a\", \"b\", \"c\"]\n", "    y_hat = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"]\n\t    zero_one_loss = ZeroOneLoss()\n\t    losses = zero_one_loss(y, y_hat)\n\t    np.testing.assert_array_equal(losses, [1.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n\tdef test_zero_one_loss_postproc():\n\t    y = [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"]\n\t    y_hat = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\"]\n\t    zero_one_loss = ZeroOneLoss(lambda x: x.lower())\n\t    losses = zero_one_loss(y, y_hat)\n\t    np.testing.assert_array_equal(losses, [0.0, 1.0, 1.0, 1.0, 0.0, 0.0])\n", "    assert y == [\"A\", \"B\", \"C\", \"A\", \"B\", \"C\"]  # no side effect\n\tdef test_zero_one_loss_postproc_property():\n\t    zero_one_loss = ZeroOneLoss(lambda x: x.upper())\n\t    assert zero_one_loss.postproc(\"abc\") == \"ABC\"\n\t    zero_one_loss = ZeroOneLoss()\n\t    assert zero_one_loss.postproc(\"abc\") == \"abc\"\n"]}
{"filename": "tests/test_vi_layers.py", "chunked_list": ["from unittest.mock import patch\n\timport numpy as np\n\tfrom dln.operator import forward_instantiate\n\tfrom dln.score import OutputClasses\n\tfrom dln.vi.layers import PriorLayer, ResidualPriorLayer\n\tdef test_apply_residual_without_template():\n\t    inputs = np.array([\"input1\", \"input2\", \"input3\"])\n\t    outputs = np.array([\"output1\", \"output2\", \"output3\"])\n\t    residual_prior_layer = ResidualPriorLayer(\n\t        forward_template=\"suffix_forward\",\n", "        init=\"A task description\",\n\t    )\n\t    result = residual_prior_layer.apply_residual(outputs, inputs)\n\t    expected_outputs = np.array(\n\t        [\n\t            \"input1\\nYour thoughts were:\\noutput1\",\n\t            \"input2\\nYour thoughts were:\\noutput2\",\n\t            \"input3\\nYour thoughts were:\\noutput3\",\n\t        ]\n\t    )\n", "    np.testing.assert_equal(result, expected_outputs)\n\tdef test_apply_residual_with_template():\n\t    inputs = np.array([\"input1\", \"input2\", \"input3\"])\n\t    outputs = np.array([\"output1\", \"output2\", \"output3\"])\n\t    residual_prior_layer = ResidualPriorLayer(\n\t        forward_template=\"suffix_forward\",\n\t        init=\"A task description\",\n\t    )\n\t    result = residual_prior_layer.apply_residual(outputs, inputs, use_template=True)\n\t    expected_outputs = np.array(\n", "        [\n\t            \"input1\\n\\nA task description\\nYour thoughts were:\\noutput1\",\n\t            \"input2\\n\\nA task description\\nYour thoughts were:\\noutput2\",\n\t            \"input3\\n\\nA task description\\nYour thoughts were:\\noutput3\",\n\t        ]\n\t    )\n\t    np.testing.assert_equal(result, expected_outputs)\n\tdef test_log_p_with_output_classes(top_logprobs):\n\t    forward_instantiate()\n\t    inputs = [\"1 + 1\", \"1 * 1\"]\n", "    outputs = [\"B\", \"A\"]\n\t    output_classes = OutputClasses(protos=[\"a|A\", \"b|B\"])\n\t    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n\t    with patch(\"dln.score.forward_evaluate\", top_logprobs):\n\t        logp = prior_layer.log_p(\n\t            inputs, outputs, output_classes=output_classes\n\t        )\n\t    np.testing.assert_almost_equal(logp.targets, [-8.67468626, -0.44289729])\n\t    np.testing.assert_almost_equal(\n\t        logp.contexts,\n", "        [\n\t            [9.99829143e-01, 1.70856546e-04],\n\t            [6.42173164e-01, 3.57826836e-01],\n\t        ],\n\t    )\n\tdef test_log_p_without_output_classes(raw_logprobs, score_requests):\n\t    forward_instantiate()\n\t    inputs = [s.context for s in score_requests]\n\t    outputs = [\"B\", \"A\"]\n\t    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n", "    with patch(\"dln.score.forward_evaluate\", raw_logprobs):\n\t        logp = prior_layer.log_p(inputs, outputs)\n\t    np.testing.assert_almost_equal(logp.targets, [-1.48348267, -1.47351816])\n\tdef test_forward_with_output_class(top_logprobs):\n\t    forward_instantiate()\n\t    inputs = [\"1 + 1\", \"1 * 1\"]\n\t    output_classes = OutputClasses(protos=[\"A|a\", \"B|b\"])\n\t    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n\t    with patch(\"dln.score.forward_evaluate\", top_logprobs):\n\t        result = prior_layer.forward(inputs, output_classes)\n", "    np.testing.assert_equal(result, [\"A\", \"A\"])\n\tdef test_forward_without_output_class(text_outputs):\n\t    forward_instantiate()\n\t    inputs = [\"1 + 1\", \"1 * 1\"]\n\t    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n\t    with patch(\"dln.vi.layers.forward_evaluate\", text_outputs):\n\t        result = prior_layer.forward(inputs)\n\t    np.testing.assert_equal(result, [\"A\", \"A\"])\n\tdef test_forward_strip_double_newlines():\n\t    forward_instantiate()\n", "    inputs = [\"1 + 1\"]\n\t    prior_layer = PriorLayer(forward_template=\"suffix_forward\", init=\"\")\n\t    text_output = lambda *args, **kwargs: [\"A\\n\\n\"]\n\t    with patch(\"dln.vi.layers.forward_evaluate\", text_output):\n\t        result = prior_layer.forward(inputs, strip_double_newlines=True)\n\t    np.testing.assert_equal(result, [\"A\\n\"])\n"]}
{"filename": "tests/test_dln_dataset.py", "chunked_list": ["from unittest.mock import MagicMock, patch\n\timport pytest\n\tfrom dln.dataset import Dataset, init_dataset\n\tdef test_init_dataset_subj():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\"subj\", 42, \"./data\")\n\t    assert (\n\t        dataset.instruction\n\t        == \"Read the following sentence, then choose whether it is subjective or objective.\"\n\t    )\n", "    assert dataset.dataset_name == \"subj\"\n\t    assert output_classes.protos == [\"subjective\", \"objective\"]\n\t    assert val_examples == -1\n\tdef test_init_dataset_mpqa():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\"mpqa\", 42, \"./data\")\n\t    assert (\n\t        dataset.instruction\n\t        == \"Read the following review, then choose whether it is negative or positive.\"\n\t    )\n", "    assert dataset.dataset_name == \"mpqa\"\n\t    assert output_classes.protos == [\"negative\", \"positive\"]\n\t    assert val_examples == -1\n\tdef test_init_dataset_trec():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\"trec\", 42, \"./data\")\n\t    assert (\n\t        dataset.instruction\n\t        == \"Read the following question, then choose whether it is about a description, entity, expression, human, location or number.\"\n\t    )\n", "    assert dataset.dataset_name == \"trec\"\n\t    assert output_classes.protos == [\n\t        \"description\",\n\t        \"entity\",\n\t        \"expression\",\n\t        \"human\",\n\t        \"location\",\n\t        \"number\",\n\t    ]\n\t    assert val_examples == -1\n", "def test_init_dataset_disaster():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\"disaster\", 42, \"./data\")\n\t    assert (\n\t        dataset.instruction\n\t        == \"Read the following sentence, then choose whether it is relevant to a disaster.\"\n\t    )\n\t    assert dataset.dataset_name == \"disaster\"\n\t    assert output_classes.protos == [\"no\", \"yes\"]\n\t    assert val_examples == -1\n", "def test_init_dataset_airline():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\"airline\", 42, \"./data\")\n\t    assert (\n\t        dataset.instruction\n\t        == \"Read the following sentence, then choose whether it is positive, negative, or neutral.\"\n\t    )\n\t    assert dataset.dataset_name == \"airline\"\n\t    assert output_classes.protos == [\"positive\", \"negative\", \"neutral\"]\n\t    assert val_examples == -1\n", "def test_init_dataset_hyperbaton():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\"hyperbaton\", 42, \"./data\")\n\t    assert dataset.instruction == \"Which sentence has the correct adjective order:\"\n\t    assert dataset.dataset_name == \"hyperbaton\"\n\t    assert output_classes.protos == [\"a|A\", \"b|B\"]\n\t    assert val_examples == 300\n\tdef test_init_dataset_navigate():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\"navigate\", 42, \"./data\")\n", "    assert (\n\t        dataset.instruction\n\t        == \"If you follow these instructions, do you return to the starting point?\"\n\t    )\n\t    assert dataset.dataset_name == \"navigate\"\n\t    assert output_classes.protos == [\"yes|Yes\", \"no|No\"]\n\t    assert val_examples == -1\n\tdef test_init_dataset_date_understanding():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\n", "            \"date_understanding\", 42, \"./data\"\n\t        )\n\t    assert dataset.instruction == \"Infer the date from context.\"\n\t    assert dataset.dataset_name == \"date_understanding\"\n\t    assert output_classes.protos == [\"a|A\", \"b|B\", \"c|C\", \"d|D\", \"e|E\", \"f|F\"]\n\t    assert val_examples == -1\n\tdef test_init_dataset_logical_deduction_seven_objects():\n\t    with patch.object(Dataset, \"load_dataset\", MagicMock):\n\t        dataset, output_classes, val_examples = init_dataset(\n\t            \"logical_deduction_seven_objects\", 42, \"./data\"\n", "        )\n\t    assert (\n\t        dataset.instruction\n\t        == \"The following paragraphs each describe a set of seven objects arranged in a fixed order. The statements are logically consistent within each paragraph.\"\n\t    )\n\t    assert dataset.dataset_name == \"logical_deduction_seven_objects\"\n\t    assert output_classes.protos == [\"a|A\", \"b|B\", \"c|C\", \"d|D\", \"e|E\", \"f|F\", \"g|G\"]\n\t    assert val_examples == -1\n\tdef test_init_dataset_not_found():\n\t    with pytest.raises(AssertionError, match=r\"Dataset test not found\"):\n", "        init_dataset(\"test\", 42, \"./data\")\n"]}
{"filename": "tests/test_vi_model.py", "chunked_list": ["from unittest.mock import patch\n\timport numpy as np\n\timport pytest\n\tfrom dln.loss import ZeroOneLoss\n\tfrom dln.postprocessing import postprocess_prediction\n\tfrom dln.score import LogProbs, OutputClasses\n\tfrom dln.vi.layers import PriorLayer, ResidualPriorLayer\n\tfrom dln.vi.model import VILModel\n\tfrom dln.vi.sampler import PosteriorSampler, PromptSampler\n\t@pytest.fixture\n", "def loss_fn():\n\t    loss_fn = ZeroOneLoss(postproc=postprocess_prediction)\n\t    return loss_fn\n\t@pytest.fixture\n\tdef log_likes():\n\t    return np.array(\n\t        [\n\t            [[0.2, 0.3], [0.4, 0.1]],\n\t            [[0.5, 0.5], [0.3, 0.2]],\n\t        ]\n", "    )\n\t@pytest.fixture\n\tdef class_weights():\n\t    return np.array([[0.6, 0.4], [0.8, 0.2]])\n\t@pytest.fixture\n\tdef q_h():\n\t    return np.array(\n\t        [\n\t            [\"test 1.1\", \"test 1.2\"],\n\t            [\"test 2.1\", \"test 2.2\"],\n", "            [\"test 3.1\", \"test 3.2\"],\n\t            [\"test 4.1\", \"test 4.2\"],\n\t        ]\n\t    )\n\t@pytest.fixture\n\tdef log_p_fn():\n\t    def log_p(\n\t        self,\n\t        inputs,\n\t        targets,\n", "        prompts=None,\n\t        output_classes=None,\n\t        agg=\"max\",\n\t    ):\n\t        np.random.seed(42)\n\t        logprobs = LogProbs(\n\t            np.random.rand(len(inputs)),\n\t            np.random.rand(len(inputs), len(output_classes))\n\t            if output_classes else None\n\t        )\n", "        return logprobs\n\t    return log_p\n\tdef test_memory(loss_fn):\n\t    model = VILModel(loss_fn, task_description=\"Test task description\", use_memory=2)\n\t    assert model.get_from_memory(0).size == 0\n\t    assert model.get_from_memory(1).size == 0\n\t    with pytest.raises(BaseException):\n\t        model.get_from_memory(2)\n\t    model.add_to_memory(\"test1\", \"test2\", 0.5)\n\t    model.add_to_memory(\"test3\", \"test4\", 0.2)\n", "    model.add_to_memory(\"test5\", \"test6\", 0.7)\n\t    np.testing.assert_array_equal(model.get_from_memory(0), [\"test1\", \"test5\"])\n\tdef test_compute_elbo_score(loss_fn, log_likes, class_weights):\n\t    model = VILModel(loss_fn, task_description=\"Test task description\")\n\t    elbo_score = model.compute_elbo_score(log_likes)\n\t    expected_output = [[0.35, 0.4], [0.35, 0.15]]\n\t    assert np.allclose(elbo_score, expected_output)\n\t    elbo_score = model.compute_elbo_score(log_likes, class_weights)\n\t    assert np.allclose(elbo_score, [0.37, 0.33])\n\tdef test_sample_hidden_states(loss_fn, q_h):\n", "    np.random.seed(42)\n\t    inputs = np.array([\"test-1\", \"test-2\", \"test-3\", \"test-4\"])\n\t    y = np.array([\"test_1\", \"test_2\", \"test_3\", \"test_4\"])\n\t    h = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n\t    num_h_samples = 2\n\t    model = VILModel(\n\t        loss_fn, task_description=\"Test task description\", num_h_samples=num_h_samples\n\t    )\n\t    total_h_samples = len(inputs) * num_h_samples\n\t    mock_l2_log_p = LogProbs(\n", "        np.random.rand(total_h_samples),\n\t        np.random.rand(total_h_samples, num_h_samples),\n\t    )\n\t    mock_l1_log_p = LogProbs(np.random.rand(total_h_samples), None)\n\t    with patch.object(\n\t        PosteriorSampler, \"sample_q_h\", return_value=q_h\n\t    ), patch.object(\n\t        PriorLayer, \"log_p\", return_value=mock_l2_log_p\n\t    ), patch.object(\n\t        ResidualPriorLayer, \"log_p\", return_value=mock_l1_log_p\n", "    ):\n\t        hidden_states = model.sample_hidden_states(x=inputs, y=y, h1=h)\n\t    residual_h_tilde_1, h_tilde_1, h_tilde_1_star, weights = hidden_states\n\t    np.testing.assert_equal(\n\t        residual_h_tilde_1,\n\t        [\n\t            [\n\t                \"test-1\\nYour thoughts were:\\ntest 1.1\",\n\t                \"test-1\\nYour thoughts were:\\ntest 1.2\",\n\t            ],\n", "            [\n\t                \"test-2\\nYour thoughts were:\\ntest 2.1\",\n\t                \"test-2\\nYour thoughts were:\\ntest 2.2\",\n\t            ],\n\t            [\n\t                \"test-3\\nYour thoughts were:\\ntest 3.1\",\n\t                \"test-3\\nYour thoughts were:\\ntest 3.2\",\n\t            ],\n\t            [\n\t                \"test-4\\nYour thoughts were:\\ntest 4.1\",\n", "                \"test-4\\nYour thoughts were:\\ntest 4.2\",\n\t            ],\n\t        ],\n\t    )\n\t    np.testing.assert_equal(h_tilde_1, q_h)\n\t    np.testing.assert_equal(\n\t        h_tilde_1_star, [\"test 1.2\", \"test 2.2\", \"test 3.1\", \"test 4.2\"]\n\t    )\n\t    np.testing.assert_almost_equal(\n\t        weights,\n", "        [\n\t            [0.28796663, 0.71203337],\n\t            [0.45481729, 0.54518271],\n\t            [0.63320434, 0.36679566],\n\t            [0.40828206, 0.59171794],\n\t        ],\n\t    )\n\tdef test_inference_one_layer(loss_fn, backward_info, log_p_fn):\n\t    np.random.seed(42)\n\t    inputs, y, y_hat, losses = backward_info\n", "    num_h_samples = 2\n\t    num_p_samples = 2\n\t    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n\t    model = VILModel(\n\t        loss_fn,\n\t        task_description=\"Test task description\",\n\t        output_classes=output_classes,\n\t        num_h_samples=num_h_samples,\n\t        num_p_samples=num_p_samples,\n\t        two_layers=False,\n", "    )\n\t    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n\t    with patch.object(\n\t        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n\t    ), patch.object(\n\t        PriorLayer, \"log_p\", log_p_fn\n\t    ):\n\t        elbo, _, p2 = model.inference_one_layer(inputs, y, y_hat, losses)\n\t    np.testing.assert_almost_equal(elbo, 0.64288586)\n\t    assert p2 == \"prompt 2\"\n", "@pytest.mark.parametrize(\n\t    \"train_p1, train_p2, expec_best_p1_elbo, expec_best_p2_elbo, expec_best_p1, expec_best_p2\",\n\t    [\n\t        (True, False, 0.39742681, 0.0, \"prompt 2\", \"Test task description\"),  # Train p1\n\t        (False, True, 0.0, 0.49596743, \"Test task description\", \"prompt 2\"),  # Train p2\n\t        (True, True, 0.39742681, 0.49596743, \"prompt 2\", \"prompt 2\"),  # Train e2e\n\t    ],\n\t)\n\tdef test_inference_vi(\n\t    loss_fn,\n", "    backward_info,\n\t    q_h,\n\t    log_p_fn,\n\t    train_p1,\n\t    train_p2,\n\t    expec_best_p1_elbo,\n\t    expec_best_p2_elbo,\n\t    expec_best_p1,\n\t    expec_best_p2,\n\t):\n", "    inputs, y, y_hat, losses = backward_info\n\t    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n\t    num_h_samples = 2\n\t    num_p_samples = 2\n\t    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n\t    model = VILModel(\n\t        loss_fn,\n\t        task_description=\"Test task description\",\n\t        output_classes=output_classes,\n\t        num_h_samples=num_h_samples,\n", "        num_p_samples=num_p_samples,\n\t        train_p1=train_p1,\n\t        train_p2=train_p2,\n\t    )\n\t    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n\t    with patch.object(\n\t        PosteriorSampler, \"sample_q_h\", return_value=q_h\n\t    ), patch.object(\n\t        PriorLayer, \"log_p\", log_p_fn\n\t    ), patch.object(\n", "        ResidualPriorLayer, \"log_p\", log_p_fn\n\t    ), patch.object(\n\t        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n\t    ):\n\t        r_h1 = model.encoder_l1.apply_residual(h1, inputs)\n\t        best_p1_elbo, best_p2_elbo, best_p1, best_p2 = model.inference_vi(\n\t            inputs, h1, r_h1, y, y_hat, losses\n\t        )\n\t    np.testing.assert_almost_equal(best_p1_elbo, expec_best_p1_elbo)\n\t    np.testing.assert_almost_equal(best_p2_elbo, expec_best_p2_elbo)\n", "    assert best_p1 == expec_best_p1\n\t    assert best_p2 == expec_best_p2\n\t@pytest.mark.parametrize(\n\t    \"train_p1, train_p2, expec_elbo, expec_best_p1, expec_best_p2, expec_loss_mean, expec_elbo1, expec_elbo2\",\n\t    [\n\t        (True, False, 0.39742681, 'prompt 2', 'Test task description', 0.5, 0.39742681, 0.0),  # Train p1\n\t        (False, True, 0.49596743, 'Test task description', 'prompt 2', 0.5, 0.0, 0.49596743),  # Train p2\n\t        (True, True, 0.89339424, 'prompt 2', 'prompt 2', 0.5, 0.39742681, 0.49596743),  # Train e2e\n\t    ],\n\t)\n", "def test_forward_two_layers(\n\t    loss_fn,\n\t    backward_info,\n\t    q_h,\n\t    log_p_fn,\n\t    train_p1,\n\t    train_p2,\n\t    expec_elbo,\n\t    expec_best_p1,\n\t    expec_best_p2,\n", "    expec_loss_mean,\n\t    expec_elbo1,\n\t    expec_elbo2,\n\t):\n\t    inputs, y, y_hat, _ = backward_info\n\t    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n\t    num_h_samples = 2\n\t    num_p_samples = 2\n\t    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n\t    model = VILModel(\n", "        loss_fn,\n\t        task_description=\"Test task description\",\n\t        output_classes=output_classes,\n\t        num_h_samples=num_h_samples,\n\t        num_p_samples=num_p_samples,\n\t        train_p1=train_p1,\n\t        train_p2=train_p2,\n\t        two_layers=True,\n\t    )\n\t    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n", "    with patch.object(\n\t        PosteriorSampler, \"sample_q_h\", return_value=q_h\n\t    ), patch.object(\n\t        PriorLayer, \"forward\", return_value=y_hat\n\t    ), patch.object(\n\t        PriorLayer, \"log_p\", log_p_fn\n\t    ), patch.object(\n\t        ResidualPriorLayer, \"forward\", return_value=h1\n\t    ), patch.object(\n\t        ResidualPriorLayer, \"log_p\", log_p_fn\n", "    ), patch.object(\n\t        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n\t    ):\n\t        elbo, best_p1, best_p2, loss_mean, elbo1, elbo2 = model.forward(inputs, y)\n\t    assert best_p1 == expec_best_p1\n\t    assert best_p2 == expec_best_p2\n\t    np.testing.assert_almost_equal(elbo, expec_elbo)\n\t    np.testing.assert_almost_equal(elbo1, expec_elbo1)\n\t    np.testing.assert_almost_equal(elbo2, expec_elbo2)\n\t    np.testing.assert_almost_equal(loss_mean, expec_loss_mean)\n", "def test_forward_one_layer(\n\t    loss_fn,\n\t    backward_info,\n\t    q_h,\n\t    log_p_fn,\n\t):\n\t    inputs, y, y_hat, _ = backward_info\n\t    num_h_samples = 2\n\t    num_p_samples = 2\n\t    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n", "    model = VILModel(\n\t        loss_fn,\n\t        task_description=\"Test task description\",\n\t        output_classes=output_classes,\n\t        num_h_samples=num_h_samples,\n\t        num_p_samples=num_p_samples,\n\t        train_p1=False,\n\t        train_p2=False,\n\t        two_layers=False,\n\t    )\n", "    mock_q_p = np.array([\"prompt 1\", \"prompt 2\"])\n\t    with patch.object(\n\t        PosteriorSampler, \"sample_q_h\", return_value=q_h\n\t    ), patch.object(\n\t        PriorLayer, \"forward\", return_value=y_hat\n\t    ), patch.object(\n\t        PriorLayer, \"log_p\", log_p_fn\n\t    ), patch.object(\n\t        PromptSampler, \"sample_q_p\", return_value=mock_q_p\n\t    ):\n", "        elbo, best_p1, best_p2, loss_mean, elbo1, elbo2 = model.forward(inputs, y)\n\t    assert best_p1 == None\n\t    assert best_p2 == \"prompt 2\"\n\t    np.testing.assert_almost_equal(elbo, 0.64288586)\n\t    np.testing.assert_almost_equal(elbo1, 0.0)\n\t    np.testing.assert_almost_equal(elbo2, 0.64288586)\n\t    np.testing.assert_almost_equal(loss_mean, 0.5)\n\t@pytest.mark.parametrize(\n\t    \"train_p1, train_p2, two_layers, expec_l1_calls\",\n\t    [\n", "        (True, False, True, 1),\n\t        (False, True, True, 1),\n\t        (True, True, True, 1),\n\t        (True, False, False, 0),\n\t        (False, True, False, 0),\n\t        (True, True, False, 0),\n\t    ],\n\t)\n\tdef test_forward_inference(\n\t    loss_fn,\n", "    backward_info,\n\t    train_p1,\n\t    train_p2,\n\t    two_layers,\n\t    expec_l1_calls,\n\t):\n\t    inputs, _, y_hat, _ = backward_info\n\t    h1 = np.array([\"test 1\", \"test2\", \"test 3\", \"test4\"])\n\t    num_h_samples = 2\n\t    num_p_samples = 2\n", "    output_classes = OutputClasses(protos=[\"A\", \"B\"])\n\t    model = VILModel(\n\t        loss_fn,\n\t        task_description=\"Test task description\",\n\t        output_classes=output_classes,\n\t        num_h_samples=num_h_samples,\n\t        num_p_samples=num_p_samples,\n\t        train_p1=train_p1,\n\t        train_p2=train_p2,\n\t        two_layers=two_layers,\n", "    )\n\t    # should only require forward pass\n\t    with patch.object(\n\t        PriorLayer, \"forward\", return_value=y_hat\n\t    ) as l2, patch.object(\n\t        ResidualPriorLayer, \"forward\", return_value=h1\n\t    ) as l1:\n\t        result = model.forward(inputs)\n\t    np.testing.assert_equal(result, ['test_', 'test', 'test_', 'test'])\n\t    assert l1.call_count == expec_l1_calls\n", "    assert l2.call_count == 1\n\tdef test_strip_options(loss_fn):\n\t    model = VILModel(loss_fn, task_description=\"Test task description\")\n\t    input_data = np.array(\n\t        [\n\t            \"This is a test\\nOptions:\\n(A)\\n(B)\",\n\t            \"No options here\",\n\t            \"Another testOptions:(A)(B)\",\n\t        ]\n\t    )\n", "    expected_output = np.array([\"This is a test\", \"No options here\", \"Another test\"])\n\t    output_data = model.strip_options(input_data)\n\t    assert np.array_equal(output_data, expected_output)\n\tdef test_strip_answer(loss_fn):\n\t    model = VILModel(loss_fn, task_description=\"Test task description\")\n\t    input_data = np.array(\n\t        [\"This is a test\\nAnswer: A\", \"No answer here\", \"Another testAnswer:\"]\n\t    )\n\t    expected_output = np.array([\"This is a test\", \"No answer here\", \"Another test\"])\n\t    output_data = model.strip_answer(input_data)\n", "    assert np.array_equal(output_data, expected_output)\n\tdef test_strip_prefix(loss_fn):\n\t    model = VILModel(\n\t        loss_fn, \"Test task description\", strip_prefix_for_hidden=\"PREFIX:\"\n\t    )\n\t    input_data = np.array(\n\t        [\"PREFIX: This is a test\", \"No prefix here\", \"PREFIX: Another test\"]\n\t    )\n\t    expected_output = np.array([\"This is a test\", \"No prefix here\", \"Another test\"])\n\t    output_data = model.strip_prefix(input_data)\n", "    assert np.array_equal(output_data, expected_output)\n"]}
{"filename": "tests/test_dln_postprocessing.py", "chunked_list": ["import pytest\n\tfrom dln.postprocessing import postprocess_prediction, remove_extra_spaces\n\t@pytest.mark.parametrize(\n\t    \"input,expected\",\n\t    [\n\t        (\"foo  bar\", \"foo bar\"),\n\t        (\"  foo  bar  \", \" foo bar \"),\n\t        (\"foo\\n\\nbar\", \"foo\\nbar\"),\n\t        (\"\\nfoo\\n\\nbar\\n\", \"\\nfoo\\nbar\\n\"),\n\t    ],\n", ")\n\tdef test_remove_extra_spaces(input, expected):\n\t    assert remove_extra_spaces(input, False) == expected\n\t@pytest.mark.parametrize(\n\t    \"input,expected\",\n\t    [\n\t        (\"foo  bar\", \"foo bar\"),\n\t        (\"  foo  bar  \", \" foo bar \"),\n\t        (\"foo\\n\\nbar\", \"foo bar\"),\n\t        (\"\\nfoo\\n\\nbar\\n\", \" foo bar \"),\n", "    ],\n\t)\n\tdef test_remove_extra_spaces_and_replace_new_lines(input, expected):\n\t    assert remove_extra_spaces(input, True) == expected\n\t@pytest.mark.parametrize(\n\t    \"input,expected\",\n\t    [\n\t        (\"foo@bar\", \"foo\"),\n\t        (\"foo123bar\", \"foo\"),\n\t        (\"  Foo  Bar  \", \"foo\"),\n", "        (\"\", \"\"),\n\t        (\"Foo\", \"foo\"),\n\t        (\"Option (A)\", \"a\"),\n\t        (\"Option (A, B)\", \"a\"),\n\t        (\"Foo Bar\", \"foo\"),\n\t    ],\n\t)\n\tdef test_postprocess_prediction(input, expected):\n\t    assert postprocess_prediction(input) == expected\n"]}
{"filename": "tests/test_vi_sampler.py", "chunked_list": ["import re\n\tfrom unittest.mock import MagicMock\n\timport numpy as np\n\timport pytest\n\tfrom dln.vi.sampler import PosteriorSampler, PromptSampler\n\tdef test_sample_q_p(backward_info):\n\t    inputs, y, y_hat, losses = backward_info\n\t    sampler = PromptSampler()\n\t    mock_eval_fn = MagicMock(return_value=[\"new prompt 1\", \"new prompt 2\"])\n\t    sampler.evaluate_func = mock_eval_fn\n", "    prompt = \"test prompt\"\n\t    num_samples = 2\n\t    held_out_half = False\n\t    prompts = sampler.sample_q_p(\n\t        inputs, y, y_hat, losses, prompt, num_samples, held_out_half\n\t    )\n\t    q_prompt = mock_eval_fn.call_args[0][0][\n\t        0\n\t    ]  # rendered template sent to evaluate_func\n\t    success_block = re.findall(r\"# Student successes(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n", "    assert \"test_1\" in success_block\n\t    assert \"test_3\" in success_block\n\t    assert \"test_2\" not in success_block\n\t    assert \"test_4\" not in success_block\n\t    error_block = re.findall(r\"# Student errors(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n\t    assert \"test_2\" in error_block\n\t    assert \"test_4\" in error_block\n\t    assert \"test_1\" not in error_block\n\t    assert \"test_3\" not in error_block\n\t    np.testing.assert_array_equal(\n", "        prompts, [\"test prompt\", \"new prompt 1\", \"new prompt 2\"]\n\t    )\n\tdef test_sample_q_p_hold_out_half(backward_info):\n\t    inputs, y, y_hat, losses = backward_info\n\t    sampler = PromptSampler()\n\t    mock_eval_fn = MagicMock(return_value=[\"new prompt 1\", \"new prompt 2\"])\n\t    sampler.evaluate_func = mock_eval_fn\n\t    prompt = \"test prompt\"\n\t    num_samples = 2\n\t    held_out_half = True\n", "    prompts = sampler.sample_q_p(\n\t        inputs, y, y_hat, losses, prompt, num_samples, held_out_half\n\t    )\n\t    q_prompt = mock_eval_fn.call_args[0][0][\n\t        0\n\t    ]  # rendered template sent to evaluate_func\n\t    success_block = re.findall(r\"# Student successes(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n\t    error_block = re.findall(r\"# Student errors(.*?)\\n\\n\", q_prompt, re.DOTALL)[0]\n\t    success_examples = [i for i in y if i in success_block]\n\t    error_examples = [i for i in y_hat if i in error_block]\n", "    assert len(success_examples + error_examples) == 2\n\t    assert \"test_2\" not in success_block\n\t    assert \"test_4\" not in success_block\n\t    assert \"test_1\" not in error_block\n\t    assert \"test_3\" not in error_block\n\t    np.testing.assert_array_equal(\n\t        prompts, [\"test prompt\", \"new prompt 1\", \"new prompt 2\"]\n\t    )\n\tdef test_sample_q_h(backward_info):\n\t    inputs, y, _, _ = backward_info\n", "    h = [\"test 1\", \"test2\", \"test 3\", \"test4\"]\n\t    num_samples = 2\n\t    sampler = PosteriorSampler(\"suffix_forward_tbs\")\n\t    mock_eval_fn = MagicMock(\n\t        # h * num_samples\n\t        return_value=[\n\t            \"test 1.1\",\n\t            \"test 1.2\",\n\t            \"test 2.1\",\n\t            \"test 2.2\",\n", "            \"test 3.1\",\n\t            \"test 3.2\",\n\t            \"test 4.1\",\n\t            \"test 4.2\",\n\t        ]\n\t    )\n\t    sampler.evaluate_func = mock_eval_fn\n\t    prompt = \"test prompt\"\n\t    next_prompt = \"test next prompt\"\n\t    h_hat = sampler.sample_q_h(\n", "        inputs,\n\t        y,\n\t        h,\n\t        prompt,\n\t        next_prompt,\n\t        num_samples,\n\t    )\n\t    np.testing.assert_equal(\n\t        h_hat,\n\t        [\n", "            [\"test 1.1\", \"test 1.2\"],\n\t            [\"test 2.1\", \"test 2.2\"],\n\t            [\"test 3.1\", \"test 3.2\"],\n\t            [\"test 4.1\", \"test 4.2\"],\n\t        ],\n\t    )\n"]}
{"filename": "tests/test_dln_templates.py", "chunked_list": ["import pytest\n\tfrom dln.template import DLNTemplate, Templates, load_template\n\tdef test_DLNTemplate_render():\n\t    template = DLNTemplate(template=\"{{ message }}\")\n\t    rendered = template.render(message=\"Foo bar!\")\n\t    assert rendered == \"Foo bar!\"\n\tdef test_DLNTemplate_render_default_message():\n\t    template = DLNTemplate(template=\"{{ message }}\", message=\"Default foo bar\")\n\t    rendered = template.render()\n\t    assert rendered == \"Default foo bar\"\n", "def test_template_get_template():\n\t    suffix_forward = Templates.get(\"suffix_forward\")\n\t    assert suffix_forward.template == \"{{ input }}\\n\\n{{ prompt }}\"\n\tdef test_template_template_not_found():\n\t    with pytest.raises(KeyError):\n\t        Templates.get(\"foo\")\n\tdef test_load_template():\n\t    template = load_template(\"suffix_forward\")\n\t    rendered = template.render(input=\"input test\", prompt=\"prompt test\")\n\t    assert rendered == (\"\"\"input test\\n\\nprompt test\"\"\")\n"]}
{"filename": "tests/test_dln_operator.py", "chunked_list": ["from unittest.mock import AsyncMock, MagicMock\n\timport openai\n\timport pytest\n\tfrom dln.operator import (\n\t    GPT,\n\t    backward_evaluate,\n\t    backward_instantiate,\n\t    forward_evaluate,\n\t    forward_instantiate,\n\t)\n", "@pytest.fixture\n\tdef mock_data():\n\t    chat_completion_data = {\"choices\": [{\"message\": {\"content\": \"Montreal\"}}]}\n\t    completion_data = {\n\t        \"choices\": [\n\t            {\n\t                \"text\": \"Montreal\",\n\t                \"logprobs\": {\"token_logprobs\": [0], \"top_logprobs\": [{}], \"tokens\": {}},\n\t            }\n\t        ]\n", "    }\n\t    return chat_completion_data, completion_data\n\t@pytest.fixture\n\tdef mock_openai_api(monkeypatch, mock_data):\n\t    chat_completion_data, completion_data = mock_data\n\t    mock_api = MagicMock()\n\t    mock_api.ChatCompletion.create.return_value = chat_completion_data\n\t    mock_api.Completion.create.return_value = completion_data\n\t    monkeypatch.setattr(openai, \"ChatCompletion\", mock_api.ChatCompletion)\n\t    monkeypatch.setattr(openai, \"Completion\", mock_api.Completion)\n", "@pytest.fixture\n\tdef mock_openai_api_async(monkeypatch, mock_data):\n\t    chat_completion_data, completion_data = mock_data\n\t    mock_api = MagicMock()\n\t    mock_api.ChatCompletion.acreate = AsyncMock(return_value=chat_completion_data)\n\t    monkeypatch.setattr(openai, \"ChatCompletion\", mock_api.ChatCompletion)\n\tdef test_invalid_model_name():\n\t    with pytest.raises(ValueError):\n\t        GPT(\"invalid-model-name\")\n\tdef test_valid_model_name():\n", "    gpt = GPT(\"text-davinci-003\")\n\t    assert gpt.engine == \"text-davinci-003\"\n\t@pytest.mark.asyncio\n\tasync def test_aget_chat_completion_response(mock_openai_api_async):\n\t    gpt = GPT(\"text-davinci-003\")\n\t    prompt = \"What is the largest city in Quebec?\"\n\t    response = await gpt.aget_chat_completion_response(prompt)\n\t    assert \"Montreal\" in response\n\tdef test_get_chat_completion_response(mock_openai_api):\n\t    gpt = GPT(\"text-davinci-003\")\n", "    prompt = \"What is the largest city in Quebec?\"\n\t    response = gpt.get_chat_completion_response(prompt)\n\t    assert \"Montreal\" in response\n\tdef test_get_completion_response(mock_openai_api):\n\t    gpt = GPT(\"text-davinci-003\")\n\t    prompt = \"What is the largest city in Quebec?\"\n\t    response = gpt.get_completion_response([prompt])\n\t    assert \"Montreal\" in response[0]\n\tdef test_forward_evaluate(mock_openai_api):\n\t    forward_instantiate(\"text-davinci-003\")\n", "    prompt = \"What is the largest city in Quebec?\"\n\t    response = forward_evaluate([prompt])\n\t    assert \"Montreal\" in response[0]\n\tdef test_backward_evaluate(mock_openai_api):\n\t    backward_instantiate(\"text-davinci-003\")\n\t    prompt = \"What is the largest city in Quebec?\"\n\t    response = backward_evaluate([prompt])\n\t    assert \"Montreal\" in response[0]\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import numpy as np\n\timport pytest\n\tfrom dln.score import ScoreRequest\n\t@pytest.fixture\n\tdef backward_info():\n\t    inputs = np.array([\"test-1\", \"test-2\", \"test-3\", \"test-4\"])\n\t    y = np.array([\"test_1\", \"test_2\", \"test_3\", \"test_4\"])\n\t    y_hat = np.array([\"test_1\", \"test2\", \"test_3\", \"test4\"])\n\t    losses = np.array([0.0, 1.0, 0.0, 1.0])\n\t    return inputs, y, y_hat, losses\n", "@pytest.fixture\n\tdef score_requests():\n\t    return [\n\t        ScoreRequest(\n\t            context=\"1 + 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer:\",\n\t            target=\"B\",\n\t            payload=\"B\",\n\t        ),\n\t        ScoreRequest(\n\t            context=\"1 * 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer:\",\n", "            target=\"A\",\n\t            payload=\"A\",\n\t        ),\n\t    ]\n\t@pytest.fixture\n\tdef text_outputs():\n\t    def logprobs_fn(contexts, *args, **kwargs):\n\t        #  return logprobs in the same order it was requested (contexts)\n\t        logprobs = {\n\t            \"1 + 1\": \"A\",\n", "            \"1 * 1\": \"A\",\n\t        }\n\t        return [logprobs[context[:5]] for context in contexts]\n\t    return logprobs_fn\n\t@pytest.fixture\n\tdef top_logprobs():\n\t    def logprobs_fn(contexts, *args, **kwargs):\n\t        #  return logprobs in the same order it was requested (contexts)\n\t        logprobs = {\n\t            \"1 + 1\": {\n", "                \"Option\": -8.876863,\n\t                \"Result\": -17.299635,\n\t                \"choice\": -17.710045,\n\t                \"<\": -17.075796,\n\t                \"=\": -15.760291,\n\t                \"correct\": -13.988989,\n\t                \" A\": -10.678262,\n\t                \"A\": -3.663905,\n\t                \"All\": -16.454699,\n\t                \"B\": -12.343077,\n", "            },\n\t            \"1 * 1\": {\n\t                \"Option\": -8.315238,\n\t                \"=\": -16.698154,\n\t                \"A\": -11.863415,\n\t                \"B\": -12.451943,\n\t                \"Answer\": -7.4255853,\n\t                \"answer\": -14.647212,\n\t                \"Correct\": -8.74908,\n\t                \"Choice\": -13.000805,\n", "                \"Yes\": -14.741361,\n\t                \"b\": -17.22967,\n\t            },\n\t        }\n\t        ordered_log_p = [logprobs[context[:5]] for context in contexts]\n\t        return [\n\t            [0, [ordered_log_p[0]], 2],\n\t            [0, [ordered_log_p[1]], 2],\n\t        ]\n\t    return logprobs_fn\n", "@pytest.fixture\n\tdef raw_logprobs():\n\t    def logprobs_fn(contexts, *args, **kwargs):\n\t        #  return logprobs in the same order it was requested (contexts)\n\t        logprobs = {\n\t            \"1 + 1\": [\n\t                \"1 + 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer:\\nB\",\n\t                [\n\t                    None,\n\t                    -5.550775,\n", "                    -3.194002,\n\t                    -8.062983,\n\t                    -1.9706848,\n\t                    -0.9759903,\n\t                    -11.239477,\n\t                    -2.745899,\n\t                    -0.030587194,\n\t                    -1.4996661,\n\t                    -0.068833716,\n\t                    -0.009404114,\n", "                    -0.0001532674,\n\t                    -6.5041706e-05,\n\t                    -0.056048736,\n\t                    -0.05334273,\n\t                    -8.41094,\n\t                    -6.9211907,\n\t                    -0.001781753,\n\t                    -0.053041545,\n\t                    -1.4834975,\n\t                ],\n", "                [\n\t                    \"1\",\n\t                    \" +\",\n\t                    \" 1\",\n\t                    \" is\",\n\t                    \":\",\n\t                    \"\\\\n\",\n\t                    \"(\",\n\t                    \"A\",\n\t                    \")\",\n", "                    \" 1\",\n\t                    \"\\\\n\",\n\t                    \"(\",\n\t                    \"B\",\n\t                    \")\",\n\t                    \" 2\",\n\t                    \"\\\\n\",\n\t                    \"\\\\n\",\n\t                    \"Answer\",\n\t                    \":\",\n", "                    \"\\\\n\",\n\t                    \"B\",\n\t                ],\n\t            ],\n\t            \"1 * 1\": [\n\t                \"1 * 1 is:\\n(A) 1\\n(B) 2\\n\\nAnswer: A\",\n\t                [\n\t                    None,\n\t                    -6.06174,\n\t                    -4.7931056,\n", "                    -8.253801,\n\t                    -2.3915708,\n\t                    -0.5870681,\n\t                    -10.741921,\n\t                    -3.3388677,\n\t                    -0.011392174,\n\t                    -0.86958236,\n\t                    -0.11698982,\n\t                    -0.48095098,\n\t                    -0.002377014,\n", "                    -8.3404535e-05,\n\t                    -1.417262,\n\t                    -0.027041545,\n\t                    -5.510647,\n\t                    -4.546986,\n\t                    -0.0010610583,\n\t                    -0.053041545,\n\t                    -1.4735329,\n\t                ],\n\t                [\n", "                    \"1\",\n\t                    \" *\",\n\t                    \" 1\",\n\t                    \" is\",\n\t                    \":\",\n\t                    \"\\\\n\",\n\t                    \"(\",\n\t                    \"A\",\n\t                    \")\",\n\t                    \" 1\",\n", "                    \"\\\\n\",\n\t                    \"(\",\n\t                    \"B\",\n\t                    \")\",\n\t                    \" 2\",\n\t                    \"\\\\n\",\n\t                    \"\\\\n\",\n\t                    \"Answer\",\n\t                    \":\",\n\t                    \"\\\\n\",\n", "                    \"A\",\n\t                ],\n\t            ],\n\t        }\n\t        return [logprobs[context[:5]] for context in contexts]\n\t    return logprobs_fn\n"]}
{"filename": "dln/loss.py", "chunked_list": ["from abc import ABC\n\timport numpy as np\n\tclass LLoss(ABC):\n\t    pass\n\tclass ZeroOneLoss(LLoss):\n\t    def __init__(self, postproc=None):\n\t        \"\"\"\n\t        Args:\n\t            postproc: a function that takes and returns a string to be apply before calculating the loss\n\t        Returns:\n", "            ZeroOneLoss as an float32 np.array\n\t        \"\"\"\n\t        self._postproc = postproc\n\t    def __call__(self, input, target):\n\t        \"\"\"\n\t        Args:\n\t            input: a list of strings\n\t            target: a list of strings\n\t        \"\"\"\n\t        if self._postproc:\n", "            input = [self.postproc(i) for i in input]\n\t            target = [self.postproc(t) for t in target]\n\t        losses = (np.array(input) != np.array(target)).astype(\"float32\")\n\t        return losses\n\t    @property\n\t    def postproc(self):\n\t        if self._postproc is None:\n\t            return lambda x: x\n\t        return self._postproc"]}
{"filename": "dln/template.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom typing import List\n\timport os\n\timport glob\n\timport yaml\n\timport logging\n\tfrom jinja2 import Template\n\tfrom packaging import version as pkg_version\n\t@dataclass\n\tclass DLNTemplate:\n", "    template: str\n\t    stop_tokens: List[str] = None\n\t    version: int = \"latest\"\n\t    description: str = None\n\t    message: str = None\n\t    message_alternatives: List[str] = None\n\t    def render(self, **kwargs):\n\t        if kwargs.get(\"message\") is None:\n\t            kwargs[\"message\"] = self.message\n\t        return Template(self.template).render(**kwargs)\n", "class Templates:\n\t    _instance = None\n\t    def __init__(self):\n\t        self._data = {}\n\t        template_directory = os.path.join(os.path.dirname(__file__), 'templates/')\n\t        for filename in glob.glob(f\"{template_directory}/*.yaml\"):\n\t            template_name = os.path.basename(filename).split(\".\")[0]\n\t            template = yaml.safe_load(open(filename, \"r\"))\n\t            self._data[template_name] = []\n\t            for tversion, ttemplate in template.items():\n", "                if \"v\" not in tversion:\n\t                    raise ValueError(\"Version must be in the format v1, v1.2, etc.\")\n\t                ttemplate[\"version\"] = pkg_version.parse(tversion.split(\"v\")[-1])\n\t                if \"stop_tokens\" in ttemplate:\n\t                    # strip the first \\ of \\\\n from the stop tokens\n\t                    for i, stop_token in enumerate(ttemplate[\"stop_tokens\"]):\n\t                        ttemplate[\"stop_tokens\"][i] = ttemplate[\"stop_tokens\"][\n\t                            i\n\t                        ].replace(\"\\\\n\", \"\\n\")\n\t                self._data[template_name].append(DLNTemplate(**ttemplate))\n", "    @staticmethod\n\t    def get(template_name):\n\t        template_name, _, version = template_name.partition(\":\")\n\t        if not version:\n\t            version = \"latest\"\n\t        if Templates._instance is None:\n\t            Templates._instance = Templates()\n\t        templates = Templates._instance._data[template_name]\n\t        if version == \"latest\":\n\t            template = max(templates, key=lambda x: x.version)\n", "        else:\n\t            template = [\n\t                t for t in templates if t.version == pkg_version.parse(version)\n\t            ][0]\n\t        logging.info(f\"Loaded template {template_name} v{template.version}\")\n\t        return template\n\tdef load_template(template_name):\n\t    return Templates.get(template_name)\n"]}
{"filename": "dln/postprocessing.py", "chunked_list": ["import re\n\tdef remove_extra_spaces(input, remove_new_line=False):\n\t    assert isinstance(input, str)\n\t    output = input\n\t    if remove_new_line:\n\t        output = output.replace(\"\\n\", \" \")\n\t    # remove extra spaces\n\t    while True:\n\t        if len(output) == 0 or \"  \" not in output:\n\t            break\n", "        output = output.replace(\"  \", \" \")\n\t    # remove extra new lines\n\t    while True:\n\t        if len(output) == 0 or \"\\n\\n\" not in output:\n\t            break\n\t        output = output.replace(\"\\n\\n\", \"\\n\")\n\t    return output\n\tdef postprocess_prediction(input):\n\t    assert isinstance(input, str)\n\t    output = input\n", "    output = re.sub(r\"\\W+\", \" \", output)  # remove non word\n\t    output = re.sub(r\"\\d+\", \" \", output)  # remove digits\n\t    output = remove_extra_spaces(output)\n\t    output = output.lower()\n\t    output = output.split()\n\t    if len(output) == 0:\n\t        return \"\"\n\t    if len(output) == 1:\n\t        return output[0]\n\t    # More than one word\n", "    # Useful when the model predicts \"Option (A)\" instead of (A).\n\t    if \"option\" == output[0]:\n\t        return output[1]\n\t    # Return the first word\n\t    return output[0]\n"]}
{"filename": "dln/dataset.py", "chunked_list": ["import json\n\timport os\n\tfrom collections import defaultdict\n\tfrom os.path import join as pjoin\n\timport numpy as np\n\timport yaml\n\tfrom dln.score import OutputClasses\n\tclass Dataset:\n\t    def __init__(\n\t        self,\n", "        dataset_path,\n\t        dataset,\n\t        seed,\n\t        use_label_mapping=True,\n\t        append_options=True,\n\t    ):\n\t        self.dataset_name = dataset\n\t        self.data_path = dataset_path\n\t        self.random_seed = seed\n\t        self.dataset_info = self._load_config(\n", "            pjoin(os.path.dirname(os.path.abspath(__file__)), \"dataset_info.yaml\")\n\t        )\n\t        self.label_mapping = self.dataset_info[self.dataset_name].get(\n\t            \"label_mapping\", {}\n\t        )\n\t        self.use_label_mapping = use_label_mapping and self.label_mapping\n\t        self.append_options = append_options\n\t        self.instruction = self.dataset_info[self.dataset_name][\"instruction\"]\n\t        self.rng = np.random.RandomState(self.random_seed)\n\t        # load dataset from file\n", "        self.dataset = dict(\n\t            train_per_class=dict(),\n\t            train=dict(sentence=[], label=[]),\n\t            dev=dict(sentence=[], label=[]),\n\t            test=dict(sentence=[], label=[]),\n\t        )\n\t        self.load_dataset()\n\t        print(\"loaded dataset from %s ...\" % self.data_path)\n\t        print(\n\t            \"we have %s training, %s dev, and %s test data points.\"\n", "            % (self.train_size, self.dev_size, self.test_size)\n\t        )\n\t        self.reset()\n\t    @staticmethod\n\t    def _load_config(config_file):\n\t        assert os.path.exists(config_file), \"Invalid config file\"\n\t        with open(config_file) as reader:\n\t            config = yaml.safe_load(reader)\n\t        return config\n\t    @property\n", "    def train_size(self):\n\t        return len(self.dataset[\"train\"][\"label\"])\n\t    @property\n\t    def dev_size(self):\n\t        return len(self.dataset[\"dev\"][\"label\"])\n\t    @property\n\t    def test_size(self):\n\t        return len(self.dataset[\"test\"][\"label\"])\n\t    def resize(self, split, size):\n\t        indices = np.random.permutation(np.arange(len(self.dataset[split][\"label\"])))[\n", "            :size\n\t        ]\n\t        self.dataset[split][\"label\"] = [\n\t            self.dataset[split][\"label\"][i] for i in indices\n\t        ]\n\t        self.dataset[split][\"sentence\"] = [\n\t            self.dataset[split][\"sentence\"][i] for i in indices\n\t        ]\n\t    def reset(self):\n\t        self.train_pointer, self.dev_pointer, self.test_pointer = 0, 0, 0\n", "    def load_dataset(self):\n\t        data_shuffling_rng = np.random.RandomState(42)\n\t        if \"bbh\" in self.data_path:\n\t            assert self.dataset_name in (\n\t                \"logical_deduction_seven_objects\",\n\t                \"hyperbaton\",\n\t                \"navigate\",\n\t                \"date_understanding\",\n\t            ), self.dataset_name\n\t            train_valid_file_path = os.path.join(\n", "                self.data_path.replace(\"bbh\", \"bb_minus_bbh\"),\n\t                self.dataset_name + \".json\",\n\t            )\n\t            with open(train_valid_file_path) as fin:\n\t                data = json.load(fin)\n\t                data = data[\"examples\"]\n\t                train_size = len(data) // 2\n\t                dev_size = len(data) - train_size\n\t                train_size = min(\n\t                    train_size, 1000\n", "                )  # some dataset has too many data, don't want to have a too large train/valid size\n\t                dev_size = min(dev_size, 1000)\n\t                assert train_size > 0, train_size\n\t                assert dev_size > 0, dev_size\n\t                data_shuffling_rng.shuffle(data)\n\t                for i in range(len(data)):\n\t                    if i < train_size:\n\t                        split = \"train\"\n\t                    elif i < train_size + dev_size:\n\t                        split = \"dev\"\n", "                    else:\n\t                        break\n\t                    input, target = data[i][\"input\"], data[i][\"target\"]\n\t                    if self.dataset_name == \"date_understanding\" and split == \"train\":\n\t                        # for date understanding, we add training data to dev set, as the dev set is too small\n\t                        self.dataset[\"dev\"][\"sentence\"].append(input)\n\t                        self.dataset[\"dev\"][\"label\"].append(target)\n\t                    self.dataset[split][\"sentence\"].append(input)\n\t                    self.dataset[split][\"label\"].append(target)\n\t            test_file_path = os.path.join(self.data_path, self.dataset_name + \".json\")\n", "            with open(test_file_path) as fin:\n\t                data = json.load(fin)\n\t                data = data[\"examples\"]\n\t                for i in range(len(data)):\n\t                    input, target = data[i][\"input\"], data[i][\"target\"]\n\t                    self.dataset[\"test\"][\"sentence\"].append(input)\n\t                    self.dataset[\"test\"][\"label\"].append(target)\n\t        elif \"ordered_prompt\" in self.data_path:\n\t            assert self.dataset_name in (\"mpqa\", \"trec\", \"subj\"), self.dataset_name\n\t            for split in [\"train\", \"dev\", \"test\"]:\n", "                _split = \"dev_subsample\" if split == \"dev\" else split\n\t                file_path = os.path.join(\n\t                    self.data_path, self.dataset_name, _split + \".jsonl\"\n\t                )\n\t                sentence_list, label_list = [], []\n\t                with open(file_path) as fin:\n\t                    for line in fin:\n\t                        datapoint = json.loads(line)\n\t                        sentence, label = datapoint[\"sentence\"], datapoint[\"label\"]\n\t                        if self.append_options:\n", "                            sentence = [sentence, \"Options:\"] + [\n\t                                \"- \" + item\n\t                                for item in list(self.label_mapping.values())\n\t                            ]\n\t                            sentence = \"\\n\".join(sentence)\n\t                        sentence_list.append(sentence)\n\t                        label_list.append(label)\n\t                if split == \"train\":\n\t                    indices = data_shuffling_rng.choice(\n\t                        len(sentence_list), 1000, replace=False\n", "                    )\n\t                    for idx in indices:\n\t                        self.dataset[split][\"sentence\"].append(sentence_list[idx])\n\t                        self.dataset[split][\"label\"].append(label_list[idx])\n\t                elif split == \"dev\":\n\t                    self.dataset[split][\"sentence\"] = sentence_list\n\t                    self.dataset[split][\"label\"] = label_list\n\t                elif split == \"test\":\n\t                    indices = data_shuffling_rng.choice(\n\t                        len(sentence_list), 250, replace=False\n", "                    )\n\t                    for idx in indices:\n\t                        self.dataset[split][\"sentence\"].append(sentence_list[idx])\n\t                        self.dataset[split][\"label\"].append(label_list[idx])\n\t        elif \"leopard\" in self.data_path:\n\t            assert self.dataset_name in (\"disaster\", \"airline\"), self.dataset_name\n\t            file_path = os.path.join(\n\t                self.data_path, self.dataset_name, self.dataset_name + \"_eval.json\"\n\t            )\n\t            sentence_list, label_list = [], []\n", "            with open(file_path) as fin:\n\t                data = json.load(fin)\n\t                for i in range(len(data)):\n\t                    sentence, label = data[i][\"sentence1\"], data[i][\"label\"]\n\t                    if self.append_options:\n\t                        sentence = [sentence, \"Options:\"] + [\n\t                            \"- \" + item for item in list(self.label_mapping.values())\n\t                        ]\n\t                        sentence = \"\\n\".join(sentence)\n\t                    sentence_list.append(sentence)\n", "                    label_list.append(label)\n\t            indices = data_shuffling_rng.choice(len(sentence_list), 1500, replace=False)\n\t            for idx in indices[:1000]:\n\t                self.dataset[\"train\"][\"sentence\"].append(sentence_list[idx])\n\t                self.dataset[\"train\"][\"label\"].append(label_list[idx])\n\t            for idx in indices[1000:1250]:\n\t                self.dataset[\"dev\"][\"sentence\"].append(sentence_list[idx])\n\t                self.dataset[\"dev\"][\"label\"].append(label_list[idx])\n\t            for idx in indices[1250:]:\n\t                self.dataset[\"test\"][\"sentence\"].append(sentence_list[idx])\n", "                self.dataset[\"test\"][\"label\"].append(label_list[idx])\n\t        else:\n\t            raise NotImplementedError\n\t        train_per_class = defaultdict(list)\n\t        for index, label in enumerate(self.dataset[\"train\"][\"label\"]):\n\t            train_per_class[label].append(index)\n\t        self.dataset[\"train_per_class\"] = train_per_class\n\t    def reset_pointer(self, split):\n\t        if split == \"train\":\n\t            self.train_pointer = 0\n", "        elif split == \"dev\":\n\t            self.dev_pointer = 0\n\t        elif split == \"test\":\n\t            self.test_pointer = 0\n\t    def get_batch(self, split, batch_size, random_sample=False, balance=False):\n\t        assert batch_size > 0\n\t        assert split in [\"train\", \"dev\", \"test\"]\n\t        if split == \"train\":\n\t            pointer = self.train_pointer\n\t            data_size = self.train_size\n", "            self.train_pointer += batch_size\n\t            if self.train_pointer >= data_size:\n\t                self.train_pointer = 0\n\t        elif split == \"dev\":\n\t            pointer = self.dev_pointer\n\t            data_size = self.dev_size\n\t            self.dev_pointer += batch_size\n\t            if self.dev_pointer >= data_size:\n\t                self.dev_pointer = 0\n\t        else:\n", "            pointer = self.test_pointer\n\t            data_size = self.test_size\n\t            self.test_pointer += batch_size\n\t            if self.test_pointer >= data_size:\n\t                self.test_pointer = 0\n\t        if random_sample is True:\n\t            if balance is True:\n\t                indices = []\n\t                pick_order = self.rng.choice(\n\t                    list(self.dataset[\"train_per_class\"].keys()),\n", "                    len(self.dataset[\"train_per_class\"].keys()),\n\t                    replace=False,\n\t                )\n\t                i = 0\n\t                while len(indices) < batch_size:\n\t                    indices += self.rng.choice(\n\t                        self.dataset[\"train_per_class\"][\n\t                            pick_order[i % len(pick_order)]\n\t                        ],\n\t                        1,\n", "                    ).tolist()\n\t                    i += 1\n\t            else:\n\t                indices = self.rng.choice(data_size, batch_size, replace=False)\n\t        else:\n\t            start = pointer\n\t            end = min(start + batch_size, data_size)\n\t            indices = np.arange(start, end)\n\t        sentence_list, label_list = [], []\n\t        for idx in indices:\n", "            sentence_list.append(self.dataset[split][\"sentence\"][idx])\n\t            if self.use_label_mapping:\n\t                label_mapping = self.label_mapping\n\t                label_list.append(label_mapping[self.dataset[split][\"label\"][idx]])\n\t            else:\n\t                label_list.append(self.dataset[split][\"label\"][idx])\n\t        return sentence_list, label_list\n\t    def iterate(self, split, batch_size, random_sample=False):\n\t        if split == \"train\":\n\t            self.train_pointer = 0\n", "        elif split == \"dev\":\n\t            self.dev_pointer = 0\n\t        else:\n\t            self.test_pointer = 0\n\t        while True:\n\t            yield self.get_batch(split, batch_size, random_sample)\n\t            if not random_sample:\n\t                if split == \"dev\" and self.dev_pointer == 0:\n\t                    return\n\t                if split == \"test\" and self.test_pointer == 0:\n", "                    return\n\t    def get_size(self, split):\n\t        return len(self.dataset[split][\"label\"])\n\t    def get_data(self, split=\"train\", indices=None):\n\t        \"\"\"get all data from a split\"\"\"\n\t        assert split in self.dataset\n\t        if indices is None:\n\t            res_sentence = self.dataset[split][\"sentence\"]\n\t            res_label = self.dataset[split][\"label\"]\n\t        else:\n", "            assert isinstance(indices, list) and len(indices) > 0\n\t            res_sentence, res_label = [], []\n\t            for idx in indices:\n\t                res_sentence.append(self.dataset[split][\"sentence\"][idx])\n\t                res_sentence.append(self.dataset[split][\"label\"][idx])\n\t        return res_sentence, res_label\n\tdef init_dataset(dataset_id, seed, data_dir):\n\t    ordered_prompt = os.path.join(data_dir, \"ordered_prompt\")\n\t    leopard = os.path.join(data_dir, \"leopard\")\n\t    bbh = os.path.join(data_dir, \"bbh\")\n", "    dataset_location = {\n\t        \"subj\": ordered_prompt,\n\t        \"mpqa\": ordered_prompt,\n\t        \"trec\": ordered_prompt,\n\t        \"disaster\": leopard,\n\t        \"airline\": leopard,\n\t        \"hyperbaton\": bbh,\n\t        \"navigate\": bbh,\n\t        \"date_understanding\": bbh,\n\t        \"logical_deduction_seven_objects\": bbh,\n", "    }\n\t    assert dataset_id in dataset_location, f\"Dataset {dataset_id} not found\"\n\t    dataset = Dataset(dataset_location[dataset_id], dataset_id, seed)\n\t    val_examples = {\"hyperbaton\": 300}.get(dataset_id, -1)\n\t    protos = {\n\t        \"hyperbaton\": [\"a|A\", \"b|B\"],\n\t        \"navigate\": [\"yes|Yes\", \"no|No\"],\n\t        \"date_understanding\": [\"a|A\", \"b|B\", \"c|C\", \"d|D\", \"e|E\", \"f|F\"],\n\t        \"logical_deduction_seven_objects\": [\n\t            \"a|A\",\n", "            \"b|B\",\n\t            \"c|C\",\n\t            \"d|D\",\n\t            \"e|E\",\n\t            \"f|F\",\n\t            \"g|G\",\n\t        ],\n\t    }.get(dataset_id, list(dataset.label_mapping.values()))\n\t    output_classes = OutputClasses(protos=protos)\n\t    return dataset, output_classes, val_examples\n"]}
{"filename": "dln/__init__.py", "chunked_list": []}
{"filename": "dln/operator.py", "chunked_list": ["# global interpreter\n\tfrom typing import List\n\timport asyncio\n\timport numpy as np\n\timport openai\n\timport logging\n\tfrom tenacity import (\n\t    retry,\n\t    stop_after_attempt,\n\t    wait_exponential,\n", "    retry_if_exception_type,\n\t)\n\tforward_interpreter = None\n\tbackward_interpreter = None\n\tclass GPT:\n\t    AVAILABLE_MODELS = [\n\t        \"text-davinci-003\",\n\t        \"text-davinci-002\",\n\t        \"code-davinci-002\",\n\t        \"text-curie-001\",\n", "        \"text-babbage-001\",\n\t        \"text-ada-001\",\n\t        \"gpt-3.5-turbo\",\n\t        \"gpt-4\",\n\t        \"gpt-4-32k\",\n\t    ]\n\t    def __init__(self, model_name=\"text-davinci-003\", **generation_options):\n\t        if model_name not in self.AVAILABLE_MODELS:\n\t            raise ValueError(\n\t                f\"model_name should be one of: {','.join(self.AVAILABLE_MODELS)}\"\n", "            )\n\t        self.generation_options = generation_options\n\t        self.engine = model_name\n\t    @retry(\n\t        reraise=True,\n\t        stop=stop_after_attempt(100),\n\t        wait=wait_exponential(multiplier=1, min=4, max=10),\n\t        retry=(\n\t            retry_if_exception_type(openai.error.Timeout)\n\t            | retry_if_exception_type(openai.error.APIError)\n", "            | retry_if_exception_type(openai.error.APIConnectionError)\n\t            | retry_if_exception_type(openai.error.RateLimitError)\n\t            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n\t        ),\n\t    )\n\t    async def aget_chat_completion_response(self, prompt, **kwargs):\n\t        \"\"\"\n\t        prompting chatgpt via openai api\n\t        now batching only works for completion, not on chat\n\t        \"\"\"\n", "        if openai.api_type == \"azure\":\n\t            try:\n\t                response = await openai.ChatCompletion.acreate(\n\t                    deployment_id=self.engine,\n\t                    messages=[{\"role\": \"user\", \"content\": prompt}],\n\t                    **kwargs,\n\t                )\n\t            except openai.InvalidRequestError as e:\n\t                # Most likely a content filtering error from Azure.\n\t                logging.warn(str(e))\n", "                return str(e)\n\t        else:\n\t            response = await openai.ChatCompletion.acreate(\n\t                model=self.engine,\n\t                messages=[{\"role\": \"user\", \"content\": prompt}],\n\t                **kwargs,\n\t            )\n\t        if \"content\" not in response[\"choices\"][0][\"message\"]:\n\t            return \"\"\n\t        output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n", "        return output\n\t    @retry(\n\t        reraise=True,\n\t        stop=stop_after_attempt(100),\n\t        wait=wait_exponential(multiplier=1, min=4, max=10),\n\t        retry=(\n\t            retry_if_exception_type(openai.error.Timeout)\n\t            | retry_if_exception_type(openai.error.APIError)\n\t            | retry_if_exception_type(openai.error.APIConnectionError)\n\t            | retry_if_exception_type(openai.error.RateLimitError)\n", "            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n\t        ),\n\t    )\n\t    def get_chat_completion_response(self, prompt, **kwargs):\n\t        \"\"\"\n\t        prompting chatgpt via openai api\n\t        now batching only works for completion, not on chat\n\t        \"\"\"\n\t        if openai.api_type == \"azure\":\n\t            try:\n", "                response = openai.ChatCompletion.create(\n\t                    deployment_id=self.engine,\n\t                    messages=[{\"role\": \"user\", \"content\": prompt}],\n\t                    **kwargs,\n\t                )\n\t            except openai.InvalidRequestError as e:\n\t                # Most likely a content filtering error from Azure.\n\t                logging.warn(str(e))\n\t                return str(e)\n\t        else:\n", "            response = openai.ChatCompletion.create(\n\t                model=self.engine,\n\t                messages=[{\"role\": \"user\", \"content\": prompt}],\n\t                **kwargs,\n\t            )\n\t        if \"content\" not in response[\"choices\"][0][\"message\"]:\n\t            return \"\"\n\t        output = response[\"choices\"][0][\"message\"][\"content\"].strip()\n\t        return output\n\t    @retry(\n", "        reraise=True,\n\t        stop=stop_after_attempt(100),\n\t        wait=wait_exponential(multiplier=1, min=4, max=10),\n\t        retry=(\n\t            retry_if_exception_type(openai.error.Timeout)\n\t            | retry_if_exception_type(openai.error.APIError)\n\t            | retry_if_exception_type(openai.error.APIConnectionError)\n\t            | retry_if_exception_type(openai.error.RateLimitError)\n\t            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n\t        ),\n", "    )\n\t    def get_completion_response(\n\t        self,\n\t        prompt_batch,\n\t        return_logprobs=False,\n\t        raw_logprobs=False,\n\t        top_logprobs=False,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"\n", "        prompting gpt-3 via openai api\n\t        now batching only works for completion, not on chat\n\t        \"\"\"\n\t        logging.debug(kwargs)\n\t        try:\n\t            response = openai.Completion.create(\n\t                engine=self.engine,\n\t                prompt=prompt_batch,\n\t                logprobs=top_logprobs or 1,\n\t                **kwargs,\n", "            )\n\t        except openai.InvalidRequestError as e:\n\t            # Most likely a content filtering error from Azure.\n\t            if \"filtering\" in str(e):\n\t                logging.warn(str(e))\n\t                # Process each element in the batch individually.\n\t                response = {\"choices\": []}\n\t                for prompt in prompt_batch:\n\t                    try:\n\t                        response[\"choices\"].append(\n", "                            openai.Completion.create(\n\t                                engine=self.engine,\n\t                                prompt=prompt,\n\t                                logprobs=top_logprobs or 1,\n\t                                **kwargs,\n\t                            )[\"choices\"][0]\n\t                        )\n\t                    except openai.InvalidRequestError as e:\n\t                        response[\"choices\"].append(\n\t                            {\n", "                                \"text\": str(e),\n\t                                \"logprobs\": {\"token_logprobs\": [0], \"top_logprobs\": [{}], \"tokens\": {}},\n\t                            }\n\t                        )\n\t            else:\n\t                raise e\n\t        output = []\n\t        nlls = []\n\t        lengths = []\n\t        for response in response[\"choices\"]:\n", "            output.append(response[\"text\"].strip())\n\t            if raw_logprobs:\n\t                nlls.append(response[\"logprobs\"][\"token_logprobs\"])\n\t                lengths.append(response[\"logprobs\"][\"tokens\"])\n\t            elif top_logprobs:\n\t                nlls.append(response[\"logprobs\"][\"top_logprobs\"])\n\t                lengths.append(response[\"logprobs\"][\"tokens\"])\n\t            else:\n\t                if \"token_logprobs\" in response[\"logprobs\"]:\n\t                    nlls.append(sum(response[\"logprobs\"][\"token_logprobs\"]))\n", "                    lengths.append(len(response[\"logprobs\"][\"token_logprobs\"]))\n\t                else:\n\t                    nlls.append(-np.inf)\n\t                    lengths.append(1)\n\t        if return_logprobs:\n\t            output = list(zip(output, nlls, lengths))\n\t        return output\n\t    async def gather_chat_response(self, inputs, **generation_options):\n\t        outputs = await asyncio.gather(\n\t            *[\n", "                self.aget_chat_completion_response(_input, **generation_options)\n\t                for _input in inputs\n\t            ]\n\t        )\n\t        return outputs\n\t    def _mini_batch(self, inputs, batch_size=20):\n\t        input_length = len(inputs)\n\t        num_batches = input_length // batch_size + (\n\t            1 if input_length % batch_size > 0 else 0\n\t        )\n", "        for i in range(num_batches):\n\t            input_batch = inputs[batch_size * i : batch_size * (i + 1)]\n\t            yield input_batch\n\t    def generate(self, inputs, async_generation=True, batch_size=20, **kwargs):\n\t        if type(inputs) is not list:\n\t            inputs = [inputs]\n\t        kwargs.pop(\"output_space\", None)\n\t        generation_options = self.generation_options.copy()\n\t        generation_options.update(**kwargs)\n\t        if self.engine in (\"gpt-3.5-turbo\", \"gpt-4\", \"gpt-4-32k\"):\n", "            if \"return_logprobs\" in generation_options:\n\t                del generation_options[\"return_logprobs\"]\n\t            if async_generation is True:\n\t                # async call api, devide to mini batches to avoid call rate limit\n\t                outputs = []\n\t                for input_batch in self._mini_batch(inputs, batch_size=10):\n\t                    outputs_batch = asyncio.run(\n\t                        self.gather_chat_response(input_batch, **generation_options)\n\t                    )\n\t                    outputs = outputs + outputs_batch\n", "            else:\n\t                # call api one by one\n\t                outputs = [\n\t                    self.get_chat_completion_response(_input, **generation_options)\n\t                    for _input in inputs\n\t                ]\n\t        else:\n\t            # devide to mini batches (max batch size = 20 according to openai)\n\t            outputs = []\n\t            for input_batch in self._mini_batch(inputs, batch_size=batch_size):\n", "                outputs_batch = self.get_completion_response(\n\t                    input_batch, **generation_options\n\t                )\n\t                outputs = outputs + outputs_batch\n\t        return outputs\n\tdef forward_instantiate(model_name=\"text-davinci-003\", **generation_options):\n\t    global forward_interpreter\n\t    if forward_interpreter is None:\n\t        forward_interpreter = GPT(model_name, **generation_options)\n\t    else:\n", "        print(\"Forward interpreter already instantiated.\")\n\t        pass\n\tdef backward_instantiate(model_name=\"text-davinci-003\", **generation_options):\n\t    global backward_interpreter\n\t    if backward_interpreter is None:\n\t        backward_interpreter = GPT(model_name, **generation_options)\n\t    else:\n\t        print(\"Backward interpreter already instantiated.\")\n\t        pass\n\tdef forward_evaluate(input: List[str], **kwargs):\n", "    return forward_interpreter.generate(input, **kwargs)\n\tdef backward_evaluate(input: List[str], **kwargs):\n\t    return backward_interpreter.generate(input, **kwargs)\n"]}
{"filename": "dln/score.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\tfrom typing import Any, List\n\timport numpy as np\n\tfrom dln.operator import forward_evaluate\n\t@dataclass\n\tclass ScoreRequest:\n\t    context: str\n\t    target: str\n\t    payload: Any = None\n", "@dataclass\n\tclass OutputClasses:\n\t    protos: List[str]\n\t    def __iter__(self):\n\t        return iter(self.protos)\n\t    def __len__(self):\n\t        return len(self.protos)\n\t    def verbalizers(self, i):\n\t        return self.protos[i].split(\"|\")\n\t    def prototype(self, i):\n", "        return self.protos[i].split(\"|\")[0]\n\t@dataclass\n\tclass LogProbs:\n\t    targets: np.ndarray\n\t    contexts: np.ndarray\n\tclass LogProbsScore:\n\t    def __init__(self, encoder=None):\n\t        if encoder is None:\n\t            import tiktoken\n\t            from dln.operator import forward_interpreter\n", "            encoder = tiktoken.encoding_for_model(forward_interpreter.engine)\n\t        self.encoder = encoder\n\t    def score_requests(self, requests, output_classes=None, agg=\"max\") -> LogProbs:\n\t        # create the batched inputs for the model\n\t        if output_classes is not None:\n\t            return self._forward_logprobs_score_api_with_classes(\n\t                [b.context for b in requests],\n\t                [b.target for b in requests],\n\t                output_classes,\n\t                agg=agg,\n", "            )\n\t        return self._forward_logprobs_score_api(\n\t            [b.context for b in requests],\n\t            [b.target for b in requests],\n\t        )\n\t    def _forward_logprobs_score_api_with_classes(\n\t        self, contexts, targets, output_classes, agg=\"max\"\n\t    ) -> LogProbs:\n\t        eval_kwargs = {\n\t            \"temperature\": 0.,\n", "            \"max_tokens\": 1,\n\t            \"echo\": False,\n\t            \"return_logprobs\": True,\n\t            \"raw_logprobs\": False,\n\t            \"top_logprobs\": 100,\n\t        }\n\t        unique_contexts = list(set(contexts))\n\t        context_to_position = {context: i for i, context in enumerate(unique_contexts)}\n\t        to_eval = [f\"{context}\\n\" for context in unique_contexts]\n\t        print(\"# Scoring requests = {}\".format(len(contexts)))\n", "        print(\"# Scoring unique requests = {}\".format(len(unique_contexts)))\n\t        eval_results = forward_evaluate(\n\t            to_eval,\n\t            async_generation=True,\n\t            **eval_kwargs,\n\t        )\n\t        top_logprobs = []\n\t        for context in contexts:\n\t            position = context_to_position[context]\n\t            context_top_logprobs = eval_results[position][1][0]\n", "            top_logprobs.append(dict(context_top_logprobs))\n\t        output_logprobs = []\n\t        output_distribs = []\n\t        for context, target, context_top_logprobs in zip(contexts, targets, top_logprobs):\n\t            position = context_to_position[context]\n\t            # make this fixed\n\t            if context_top_logprobs:\n\t                min_prob = np.exp(np.min(list(context_top_logprobs.values())))\n\t            else:\n\t                min_prob = 1e-6\n", "            output_classes_scores = np.asarray([min_prob for _ in output_classes])\n\t            # accumulate probability mass for each class verbalizer\n\t            # the class verbalizer can be either \" a\" or \"a\" (with or without space)\n\t            for i in range(len(output_classes)):\n\t                verbalizers = output_classes.verbalizers(i)\n\t                verbalizers.extend([f\" {v}\" for v in verbalizers])\n\t                verbalizers = set(verbalizers)\n\t                verbalizers_scores = [0.]\n\t                for verbalizer in verbalizers:\n\t                    if verbalizer in context_top_logprobs:\n", "                        prob_orig = np.exp(context_top_logprobs[verbalizer])\n\t                    else:\n\t                        prob_orig = min_prob\n\t                    verbalizers_scores.append(prob_orig)\n\t                if agg == \"max\":\n\t                    output_classes_scores[i] += np.max(verbalizers_scores)\n\t                else:\n\t                    output_classes_scores[i] += np.sum(verbalizers_scores)\n\t            output_class_index = [i for i, output_class in enumerate(output_classes) if target in output_class.split(\"|\")]\n\t            assert (\n", "                len(output_class_index) == 1\n\t            ), \"The target shouldn't appear in two output classes!\"\n\t            # accuracy here\n\t            output_classes_scores = output_classes_scores / output_classes_scores.sum()\n\t            output_logprobs.append(np.log(output_classes_scores[output_class_index[0]]))\n\t            output_distribs.append(output_classes_scores)\n\t        return LogProbs(np.asarray(output_logprobs), np.asarray(output_distribs))\n\t    def _forward_logprobs_score_api(self, contexts, targets) -> LogProbs:\n\t        logging.info(\"# Scoring requests = {}\".format(len(contexts)))\n\t        eval_kwargs = {\n", "            \"temperature\": 0,\n\t            \"max_tokens\": 0,\n\t            \"echo\": True,\n\t            \"return_logprobs\": True,\n\t            \"raw_logprobs\": True,\n\t        }\n\t        eval_batch = []\n\t        for context, target in zip(contexts, targets):\n\t            to_eval = f\"{context}\\n{target}\"\n\t            eval_batch.append(to_eval)\n", "        # there might be doubles in the eval_batch, so we need to\n\t        # only perform unique evals\n\t        unique_keys = list(set(eval_batch))\n\t        unique_keys_to_positions = {key: i for i, key in enumerate(unique_keys)}\n\t        unique_eval_results = forward_evaluate(\n\t            unique_keys,\n\t            async_generation=True,\n\t            **eval_kwargs,\n\t        )\n\t        # get the results in the same order as the eval_batch\n", "        eval_results = []\n\t        for eval_key in eval_batch:\n\t            eval_results.append(unique_eval_results[unique_keys_to_positions[eval_key]])\n\t        # get the nll results\n\t        log_probs = [eval_result[1] for eval_result in eval_results]\n\t        # get the logprobs results\n\t        output_logprobs = []\n\t        context_logprobs = []\n\t        for context, token_log_probs in zip(contexts, log_probs):\n\t            num_tokens_prompt = len(self.encoder.encode(context))\n", "            target_log_probs = token_log_probs[num_tokens_prompt:]\n\t            context_log_probs = token_log_probs[1:num_tokens_prompt]\n\t            output_logprobs.append(sum(target_log_probs) / (len(target_log_probs) + 1e-5))\n\t            context_logprobs.append(sum(context_log_probs) / (len(context_log_probs) + 1e-5))\n\t        return LogProbs(np.asarray(output_logprobs), np.asarray(context_logprobs))\n"]}
{"filename": "dln/vi/model.py", "chunked_list": ["from collections import Counter\n\timport numpy as np\n\tfrom termcolor import colored\n\tfrom dln.loss import LLoss\n\tfrom dln.score import OutputClasses\n\tfrom dln.vi.layers import PriorLayer, ResidualPriorLayer\n\tfrom dln.vi.sampler import PosteriorSampler, PromptSampler\n\tfrom dln.vi.utils import compute_pairwise_kl, log_message, ResultLogEntry\n\tclass VILModel:\n\t    def __init__(\n", "        self,\n\t        loss_fn: LLoss,\n\t        task_description: str,\n\t        two_layers=True,\n\t        num_h_samples: int = 3,\n\t        num_p_samples: int = 5,\n\t        use_h_argmax: bool = False,\n\t        init_p1: str = None,\n\t        init_p2: str = None,\n\t        q_prompt: str = \"q_action_prompt:latest\",\n", "        q_hidden: str = \"suffix_forward_tbs:latest\",\n\t        p_hidden: str = \"suffix_forward_tbs:latest\",\n\t        p_class: str = \"classify_forward:latest\",\n\t        output_classes: OutputClasses = None,\n\t        strip_options_for_hidden: bool = False,\n\t        strip_answer_for_hidden: bool = False,\n\t        trust_factor: float = 0.0,\n\t        forward_use_classes: bool = False,\n\t        held_out_prompt_ranking: bool = False,\n\t        use_memory: int = 0,\n", "        train_p1: bool = True,\n\t        train_p2: bool = True,\n\t        logp_penalty: float = 0.0,\n\t        p1_max_tokens: int = 256,\n\t        p2_max_tokens: int = 20,\n\t        posterior_temp: float = 1.0,\n\t        strip_prefix_for_hidden: str = None,\n\t    ):\n\t        \"\"\"\n\t        Args:\n", "            loss_fn: loss function to use\n\t            task_description: task description, required\n\t            two_layers: whether to use two layers or one layer\n\t            num_h_samples: number of posterior samples to use for the hidden state\n\t            num_p_samples: number of posterior samples to use for the prompt\n\t            use_h_argmax: whether to use the argmax of the posterior distribution when selecting best prompts, if False, then\n\t                          we compute num_h_samples * num_p_samples scores and select prompts based on the sum of the num_h_samples scores\n\t            init_p1: initialization for the first prompt, if None, uses \"Decompose the problem to make it simpler:\"\n\t            init_p2: initialization for the second prompt, if None, uses task description\n\t            q_prompt: prompt for the posterior over the prompt\n", "            q_hidden: prompt for the posterior over the hidden state\n\t            p_hidden: forward template for the forward pass that generates the hidden state\n\t            p_class: forward template for the classification layer\n\t            output_classes: if specified, we compute log-likelihood over these classes only\n\t            strip_options_for_hidden: whether to strip the options from the input when computing the hidden state, don't use it.\n\t            strip_answer_for_hidden: whether to strip the answer from the input when computing the hidden state, don't use it.\n\t            trust_factor: trust factor for the KL divergence between the current prompt and the new prompt, it acts *only* at the last layer, a sort of step size.\n\t            forward_use_classes: whether to use the classes in the forward pass, if True, then we pick the class with the highest probability.\n\t            held_out_prompt_ranking: when proposing the prompts from the posterior, we only use HALF of the batch, kind of limiting over-fitting, but it decreases batch size\n\t                                     for posterior distribution.\n", "            use_memory: whether to use memory, if 0, we don't use memory, if n, we include n best DEV prompts in the list of candidate prompts to select from, etc...\n\t            train_p1: whether to train the first prompt\n\t            train_p2: whether to train the second prompt\n\t        \"\"\"\n\t        if task_description is None:\n\t            raise ValueError(\n\t                \"task_description must be provided when instantiating the VIModel.\"\n\t            )\n\t        self.encoder_l1 = ResidualPriorLayer(\n\t            forward_template=p_hidden,\n", "            init=init_p1 if init_p1 is not None else task_description,\n\t        )\n\t        self.encoder_l2 = PriorLayer(\n\t            forward_template=p_class,\n\t            init=init_p2 if init_p2 is not None else task_description,\n\t        )\n\t        if not two_layers:\n\t            self.encoder_l1.weight = None\n\t        self.prompt_sampler = PromptSampler(q_prompt)\n\t        self.q_sampler = PosteriorSampler(q_hidden)\n", "        self.trust_factor = trust_factor\n\t        self.strip_answer_for_hidden = strip_answer_for_hidden\n\t        self.strip_options_for_hidden = strip_options_for_hidden\n\t        self.strip_prefix_for_hidden = strip_prefix_for_hidden\n\t        self.output_classes = output_classes\n\t        self.two_layers = two_layers\n\t        self.loss_fn = loss_fn\n\t        self.num_h_samples = num_h_samples\n\t        self.num_p_samples = num_p_samples\n\t        self.use_h_argmax = use_h_argmax\n", "        self.task_description = task_description\n\t        self.forward_use_classes = forward_use_classes\n\t        self.held_out_prompt_ranking = held_out_prompt_ranking\n\t        self.use_memory = use_memory\n\t        self.train_p1 = train_p1\n\t        self.train_p2 = train_p2\n\t        self.logp_penalty = logp_penalty\n\t        self.p1_max_tokens = p1_max_tokens\n\t        self.p2_max_tokens = p2_max_tokens\n\t        self.posterior_temp = posterior_temp\n", "        self.num_p2_steps = 1\n\t        self.num_p1_steps = 1\n\t        if self.forward_use_classes:\n\t            assert (\n\t                self.output_classes is not None\n\t            ), \"Cannot use classes for forward without output classes\"\n\t        self.prompt_memory = []\n\t        self.result_entry = ResultLogEntry()\n\t    def get_from_memory(self, layer_index=0):\n\t        assert layer_index in [0, 1], \"Layer index out of bounds\"\n", "        return np.asarray([p[layer_index] for p in self.prompt_memory])\n\t    def add_to_memory(self, p1, p2, score):\n\t        \"\"\"\n\t        Max memory size is 2. Add (p1, p2, score) to memory and keep memory sorted.\n\t        Keep best two prompts in memory.\n\t        \"\"\"\n\t        if self.use_memory == 0:\n\t            raise ValueError(\"Cannot add to memory if use_memory is 0\")\n\t        self.prompt_memory.append((p1, p2, score))\n\t        self.prompt_memory = sorted(\n", "            self.prompt_memory, key=lambda x: x[2], reverse=True\n\t        )\n\t        if len(self.prompt_memory) > self.use_memory:\n\t            self.prompt_memory = self.prompt_memory[: self.use_memory][::-1]\n\t    def inference_one_layer(\n\t        self,\n\t        x: np.array,\n\t        y: np.array,\n\t        y_hat: np.array,\n\t        losses: np.array,\n", "    ):\n\t        batch_size = y.shape[0]\n\t        p_tilde_2: np.array = self.prompt_sampler.sample_q_p(\n\t            x,\n\t            y,\n\t            y_hat,\n\t            losses,\n\t            prompt=self.encoder_l2.weight,\n\t            num_samples=self.num_p_samples,\n\t            held_out_half=self.held_out_prompt_ranking,\n", "        )\n\t        if self.prompt_memory:\n\t            p_tilde_2 = np.concatenate([p_tilde_2, self.get_from_memory(1)], 0)\n\t        # sum over all samples\n\t        # build array: (num_samples, num_p_samples)\n\t        evals = []\n\t        for i in range(x.shape[0]):\n\t            for k in range(p_tilde_2.shape[0]):\n\t                evals.append((x[i], y[i], p_tilde_2[k]))\n\t        # batch_size, num_p_samples\n", "        ll = self.encoder_l2.log_p(\n\t            inputs=np.array([eval[0] for eval in evals]),\n\t            targets=np.array([eval[1] for eval in evals]),\n\t            prompts=np.array([eval[2] for eval in evals]),\n\t            output_classes=self.output_classes,\n\t            agg=\"sum\" if self.forward_use_classes else \"max\",\n\t        ).targets\n\t        # batch_size, num_p_samples\n\t        ll = ll.reshape(batch_size, p_tilde_2.shape[0])\n\t        p2_elbo = ll.mean(axis=0)\n", "        self.result_entry.log_candidates(p_tilde_2, p2_elbo)\n\t        best_p2 = p_tilde_2[np.argmax(p2_elbo)]\n\t        best_p2_elbo = np.max(p2_elbo)\n\t        log_message(\"--- P2 ---\")\n\t        for i, (p_tilde_2_i, p2_elbo_i) in enumerate(zip(p_tilde_2, p2_elbo)):\n\t            log_message(\"#\", i, \"ELBO\", p2_elbo_i, \",\", p_tilde_2_i)\n\t        log_message(\"----------\")\n\t        log_message(\"Best P2 Index: \", np.argmax(p2_elbo))\n\t        log_message(\"Best P2: \", best_p2)\n\t        return best_p2_elbo, None, best_p2\n", "    def sample_hidden_states(\n\t        self,\n\t        x,\n\t        y,\n\t        h1,\n\t        include_h1=False,\n\t        add_prior_term_to_score=True,\n\t        posterior_temp=1.0,\n\t    ):\n\t        # samples from the approx. posterior of h_1\n", "        # (batch_size, num_h_samples)\n\t        # q(h | x, y, p_1, p_2)\n\t        batch_size = x.shape[0]\n\t        if not self.num_h_samples and not include_h1:\n\t            raise ValueError(\"Must sample at least one h or include h1\")\n\t        if self.num_h_samples:\n\t            h_tilde_1 = self.q_sampler.sample_q_h(\n\t                x=x,\n\t                y=y,\n\t                h=h1,\n", "                prompt=self.encoder_l1.weight,\n\t                next_prompt=self.encoder_l2.weight,\n\t                num_samples=self.num_h_samples,\n\t            )\n\t        # concatenate the original sample\n\t        if include_h1:\n\t            log_message(\n\t                colored(\"Concatenating original sample to h_tilde_1!\", \"yellow\")\n\t            )\n\t            if self.num_h_samples:\n", "                h_tilde_1 = np.concatenate([h1[:, None], h_tilde_1], axis=1)\n\t            else:\n\t                h_tilde_1 = h1[:, None]\n\t        num_h_samples = h_tilde_1.shape[1]\n\t        ## TIGHTEN POSTERIOR APPROXIMATION...\n\t        ## e.g. compute log p(y | ~h, p_2, x) + log p(~h | x, p_1)\n\t        # compute log p(y | ~h, p_2) (residual connection added!)\n\t        x_repeat = x.repeat(num_h_samples, axis=0)\n\t        residual_h_tilde_1 = self.encoder_l1.apply_residual(\n\t            h_tilde_1.flatten(), x_repeat\n", "        ).reshape(batch_size, num_h_samples)\n\t        if num_h_samples > 1:\n\t            log_message(colored(\"Tightening posterior approximation...\", \"yellow\"))\n\t            y_repeat = y.repeat(num_h_samples, axis=0)\n\t            ll = self.encoder_l2.log_p(\n\t                inputs=residual_h_tilde_1.flatten(),\n\t                targets=y_repeat.flatten(),\n\t                output_classes=self.output_classes,\n\t                agg=\"sum\" if self.forward_use_classes else \"max\",\n\t            ).targets\n", "            ll = ll.reshape(batch_size, num_h_samples)\n\t            if add_prior_term_to_score:\n\t                # now compute the prior log-prob of ~h, log p(~h | x, p_1)\n\t                log_message(\n\t                    colored(\n\t                        \"Scoring posterior samples only with log-likelihood + prior\",\n\t                        \"yellow\",\n\t                    )\n\t                )\n\t                pr = self.encoder_l1.log_p(\n", "                    x_repeat, h_tilde_1.flatten()\n\t                ).targets.reshape(\n\t                    batch_size, num_h_samples\n\t                )\n\t                logits = pr + ll\n\t            else:\n\t                log_message(\n\t                    colored(\n\t                        \"Scoring posterior samples only with log-likelihood!\", \"yellow\"\n\t                    )\n", "                )\n\t                # we don't need to compute the prior log-prob of ~h\n\t                logits = ll\n\t        else:\n\t            logits = np.zeros((batch_size, num_h_samples))\n\t        # posterior weights for h_tilde_1, (batch_size, num_h_samples,)\n\t        weights = np.exp(logits / posterior_temp) / np.sum(\n\t            np.exp(logits / posterior_temp), axis=1, keepdims=True\n\t        )\n\t        assert (weights.sum(1).sum(0) - batch_size) < 1e-5\n", "        # get best hidden state\n\t        best_h_tilde_1_index: np.array = np.argmax(weights, axis=1)\n\t        residual_h_tilde_1_star = residual_h_tilde_1[\n\t            np.arange(batch_size), best_h_tilde_1_index\n\t        ]\n\t        h_tilde_1_star = h_tilde_1[np.arange(batch_size), best_h_tilde_1_index]\n\t        num_h_samples = h_tilde_1.shape[1]\n\t        log_message(\"Prior h:\", h1[0])\n\t        log_message(\"Best Posterior h:\", h_tilde_1_star[0])\n\t        log_message(\"Best Posterior index:\", best_h_tilde_1_index[0])\n", "        counter = Counter(best_h_tilde_1_index)\n\t        log_message(\"Best Posterior indices:\", counter)\n\t        if self.use_h_argmax:\n\t            h_tilde_1 = h_tilde_1_star[:, None]\n\t            residual_h_tilde_1 = residual_h_tilde_1_star[:, None]\n\t            weights = np.ones((batch_size, 1))\n\t        # return both samples and weights associated with them\n\t        return (\n\t            residual_h_tilde_1,\n\t            h_tilde_1,\n", "            h_tilde_1_star,\n\t            weights,\n\t        )\n\t    def compute_elbo_score(self, log_likes, class_weights=None):\n\t        \"\"\"\n\t        Args:\n\t            log_likes: (batch_size, num_h_samples, num_p_samples)\n\t        \"\"\"\n\t        if class_weights is None:\n\t            score = log_likes.mean(0)\n", "        else:\n\t            assert log_likes.shape[1] == class_weights.shape[1]\n\t            score = np.sum(log_likes * class_weights[:, :, None], axis=1).mean(0)\n\t        return score\n\t    def inference_vi(\n\t        self,\n\t        x: np.array,\n\t        h1: np.array,\n\t        r_h1: np.array,\n\t        y: np.array,\n", "        y_hat: np.array,\n\t        losses: np.array,\n\t    ):\n\t        batch_size = x.shape[0]\n\t        assert y.shape[0] == batch_size\n\t        # sample hidden states from the proposal distribution\n\t        (\n\t            residual_h_tilde_1,\n\t            h_tilde_1,\n\t            h_tilde_1_star,\n", "            weights,\n\t        ) = self.sample_hidden_states(\n\t            x=x,\n\t            y=y,\n\t            h1=h1,\n\t            include_h1=True,\n\t            add_prior_term_to_score=True,\n\t            posterior_temp=self.posterior_temp,\n\t        )\n\t        num_h_samples = h_tilde_1.shape[1]\n", "        log_message(colored(\"Number of h samples: {}\".format(num_h_samples), \"yellow\"))\n\t        log_message(\n\t            colored(\n\t                \"Norm. entropy of posterior q(h): {}\".format(\n\t                    -(weights * np.log(weights)).sum(-1).mean(0)\n\t                    / (np.log(weights.shape[1]) + 1e-5)\n\t                ),\n\t                \"yellow\",\n\t            )\n\t        )\n", "        # marginalize over posterior samples\n\t        # build array: (num_samples, num_h_samples, num_p_samples)\n\t        eval_batch_size = batch_size\n\t        eval_x = x\n\t        eval_weights = weights\n\t        eval_r_h_tilde_1 = residual_h_tilde_1\n\t        eval_y = y\n\t        eval_h_tilde_1 = h_tilde_1\n\t        if self.train_p2:\n\t            current_prompt = self.encoder_l2.weight\n", "            p2_elbos = []\n\t            for i in range(self.num_p2_steps):\n\t                # sample from the prompt distribution, (num_prompts,)\n\t                p_tilde_2: np.array = self.prompt_sampler.sample_q_p(\n\t                    inputs=r_h1,\n\t                    y=y,\n\t                    y_hat=y_hat,\n\t                    losses=losses,\n\t                    prompt=current_prompt,\n\t                    num_samples=self.num_p_samples,\n", "                    held_out_half=self.held_out_prompt_ranking,\n\t                )\n\t                if self.prompt_memory:\n\t                    p_tilde_2 = np.concatenate([p_tilde_2, self.get_from_memory(1)], 0)\n\t                evals = []\n\t                for i in range(eval_batch_size):\n\t                    for j in range(num_h_samples):\n\t                        for k in range(p_tilde_2.shape[0]):\n\t                            evals.append(\n\t                                (\n", "                                    eval_r_h_tilde_1[i, j],\n\t                                    eval_y[i],\n\t                                    p_tilde_2[k],\n\t                                )\n\t                            )\n\t                # batch_size, num_h_samples, num_p_samples\n\t                log_message(colored(\"Evaluating log likelihoods for p2...\", \"yellow\"))\n\t                ll = self.encoder_l2.log_p(\n\t                    inputs=np.array([eval[0] for eval in evals]),\n\t                    targets=np.array([eval[1] for eval in evals]),\n", "                    prompts=np.array([eval[2] for eval in evals]),\n\t                    output_classes=self.output_classes,\n\t                    agg=\"sum\" if self.forward_use_classes else \"max\",\n\t                ).targets\n\t                ll = ll.reshape(eval_batch_size, num_h_samples, p_tilde_2.shape[0])\n\t                if self.trust_factor > 0.0:\n\t                    evals = []\n\t                    for i in range(batch_size):\n\t                        for k in range(p_tilde_2.shape[0]):\n\t                            evals.append((r_h1[i], y[i], p_tilde_2[k]))\n", "                    lps = self.encoder_l2.log_p(\n\t                        inputs=np.array([eval[0] for eval in evals]),\n\t                        targets=np.array([eval[1] for eval in evals]),\n\t                        prompts=np.array([eval[2] for eval in evals]),\n\t                        output_classes=self.output_classes,\n\t                        agg=\"sum\" if self.forward_use_classes else \"max\",\n\t                    ).contexts\n\t                    lps = lps.reshape(batch_size, p_tilde_2.shape[0], -1)\n\t                    p2_kl = compute_pairwise_kl(lps)\n\t                else:\n", "                    p2_kl = np.zeros(p_tilde_2.shape[0])\n\t                p2_elbo = self.compute_elbo_score(ll[:, :, :], eval_weights)\n\t                p2_reward = p2_elbo - self.trust_factor * p2_kl\n\t                best_p2 = p_tilde_2[np.argmax(p2_reward)]\n\t                best_p2_elbo = np.max(p2_reward)\n\t                best_p2_index = np.argmax(p2_reward)\n\t                current_prompt = best_p2\n\t                p2_elbos.append(best_p2_elbo)\n\t            log_message(\"Optimization of P2... DONE.\", p2_elbos)\n\t        else:\n", "            p_tilde_2 = np.asarray([self.encoder_l2.weight])\n\t            p2_elbo = np.zeros(self.num_p_samples)\n\t            p2_kl = np.zeros(self.num_p_samples)\n\t            best_p2 = self.encoder_l2.weight\n\t            best_p2_elbo = 0.0\n\t            best_p2_index = 0\n\t        if self.train_p1:\n\t            # update w.r.t. p2 is done at this point, proceed with p1,\n\t            # sample proposals for the first layer prompt given the best ~h, h_tilde_1_star\n\t            current_prompt = self.encoder_l1.weight\n", "            p1_elbos = []\n\t            for i in range(self.num_p1_steps):\n\t                p_tilde_1: np.array = self.prompt_sampler.sample_q_p(\n\t                    inputs=x,\n\t                    y=h_tilde_1_star,\n\t                    y_hat=h1,\n\t                    losses=losses,\n\t                    prompt=current_prompt,\n\t                    num_samples=self.num_p_samples,\n\t                    held_out_half=self.held_out_prompt_ranking,\n", "                )\n\t                if self.prompt_memory:\n\t                    p_tilde_1 = np.concatenate([p_tilde_1, self.get_from_memory(0)], 0)\n\t                # marginalize over all posterior samples\n\t                # build array: (num_samples, num_h_samples, num_p_samples)\n\t                evals = []\n\t                eval_h_tilde_1 = np.concatenate([h1[:, None], eval_h_tilde_1], 1)\n\t                for i in range(eval_batch_size):\n\t                    for j in range(num_h_samples + 1):\n\t                        for k in range(p_tilde_1.shape[0]):\n", "                            evals.append(\n\t                                (\n\t                                    eval_x[i],\n\t                                    eval_h_tilde_1[i, j], \n\t                                    p_tilde_1[k],\n\t                                )\n\t                            )\n\t                # (batch_size, num_h_samples, num_p_samples)\n\t                log_message(colored(\"Evaluating log likelihoods for p1...\", \"yellow\"))\n\t                ll = self.encoder_l1.log_p(\n", "                    inputs=np.array([eval[0] for eval in evals]),\n\t                    targets=np.array([eval[1] for eval in evals]),\n\t                    prompts=np.array([eval[2] for eval in evals]),\n\t                ).targets\n\t                ll = ll.reshape(\n\t                    eval_batch_size,\n\t                    num_h_samples + 1,\n\t                    p_tilde_1.shape[0],\n\t                )\n\t                ll_orig = ll[:, 0, :]\n", "                p1_elbo = self.compute_elbo_score(ll[:, 1:, :], eval_weights)\n\t                # Compute an exploration like logp penalty.\n\t                if self.logp_penalty > 0.0:\n\t                    error_terms = np.where(losses > 0.0)[0]\n\t                    if len(error_terms) > 0:\n\t                        ll_errors = ll_orig[error_terms]\n\t                        p1_elbo = p1_elbo - self.logp_penalty * ll_errors.mean(0)\n\t                best_p1 = p_tilde_1[np.argmax(p1_elbo)]\n\t                best_p1_elbo = np.max(p1_elbo)\n\t                best_p1_index = np.argmax(p1_elbo)\n", "                current_prompt = best_p1\n\t                p1_elbos.append(best_p1_elbo)\n\t            log_message(\"Optimization of P1... DONE.\", p1_elbos)\n\t        else:\n\t            p_tilde_1 = np.asarray([self.encoder_l1.weight])\n\t            p1_elbo = np.zeros(self.num_p_samples)\n\t            best_p1 = self.encoder_l1.weight\n\t            best_p1_elbo = 0.0\n\t            best_p1_index = 0\n\t        self.result_entry.log_candidates(p_tilde_2, p2_elbo, p_tilde_1, p1_elbo)\n", "        log_message(\"--- P1 ---\")\n\t        for i, (p_tilde_1_i, p1_elbo_i) in enumerate(zip(p_tilde_1, p1_elbo)):\n\t            log_message(\"#\", i, \"ELBO:\", p1_elbo_i, \",\", p_tilde_1_i)\n\t        log_message(\"--- P2 ---\")\n\t        for i, (p_tilde_2_i, p2_elbo_i, p2_kl_i) in enumerate(\n\t            zip(p_tilde_2, p2_elbo, p2_kl)\n\t        ):\n\t            log_message(\"#\", i, \"ELBO:\", p2_elbo_i, \"XE:\", p2_kl_i, \",\", p_tilde_2_i)\n\t        log_message(\"Best P1 Index: \", best_p1_index)\n\t        log_message(\"Best P2 Index: \", best_p2_index)\n", "        log_message(\"Best P1: \", best_p1, best_p1_elbo)\n\t        log_message(\"Best P2: \", best_p2, best_p2_elbo)\n\t        return best_p1_elbo, best_p2_elbo, best_p1, best_p2\n\t    def strip_options(self, x):\n\t        \"\"\"\n\t        In bbh, there is the lame pre-processing that appends the options\n\t        to the input. This function removes them, this can be useful for\n\t        hidden states, where we don't want the model to output the option directly.\n\t        \"\"\"\n\t        x_ = []\n", "        for x_i in x:\n\t            if \"Options:\" in x_i:\n\t                x_i = x_i[: x_i.index(\"Options:\")].strip()\n\t            x_.append(x_i)\n\t        return np.array(x_)\n\t    def strip_answer(self, x):\n\t        \"\"\"\n\t        Strip 'Answer:' from the hidden state if the model generates it.\n\t        \"\"\"\n\t        x_ = []\n", "        for x_i in x:\n\t            if \"Answer:\" in x_i:\n\t                x_i = x_i[: x_i.index(\"Answer:\")].strip()\n\t            x_.append(x_i)\n\t        return np.array(x_)\n\t    def strip_prefix(self, x):\n\t        \"\"\"\n\t        Strip prefix from the hidden state if the model generates it.\n\t        \"\"\"\n\t        x_ = []\n", "        for x_i in x:\n\t            if self.strip_prefix_for_hidden in x_i:\n\t                x_i = x_i[\n\t                    x_i.index(self.strip_prefix_for_hidden)\n\t                    + len(self.strip_prefix_for_hidden) :\n\t                ].strip()\n\t            x_.append(x_i)\n\t        return np.array(x_)\n\t    def forward(self, x, y=None, temperature=0.0):\n\t        \"\"\"\n", "        Args:\n\t            temperature: temperature to use for the forward pass.\n\t        \"\"\"\n\t        # execute first template\n\t        if self.two_layers:\n\t            if self.strip_options_for_hidden:\n\t                x_stripped = self.strip_options(x)\n\t            else:\n\t                x_stripped = x\n\t            if self.strip_prefix_for_hidden:\n", "                x_stripped = self.strip_prefix(x_stripped)\n\t            h_1_out = self.encoder_l1(\n\t                x_stripped, temperature=temperature, max_tokens=self.p1_max_tokens\n\t            )\n\t            if self.strip_answer_for_hidden:\n\t                h_1_out = self.strip_answer(h_1_out)\n\t            # execute second template\n\t            h_1 = self.encoder_l1.apply_residual(h_1_out, x)\n\t            y_hat = self.encoder_l2(\n\t                h_1,\n", "                output_classes=self.output_classes\n\t                if self.forward_use_classes\n\t                else None,\n\t                temperature=temperature,\n\t            )\n\t        else:\n\t            h_1_out, h_1 = None, None\n\t            y_hat = self.encoder_l2(\n\t                x,\n\t                output_classes=self.output_classes\n", "                if self.forward_use_classes\n\t                else None,\n\t                temperature=temperature,\n\t                max_tokens=self.p2_max_tokens,\n\t            )\n\t        self.result_entry.log_hiddens(hiddens=h_1_out, size=len(x))\n\t        self.result_entry.log_outputs(outputs=y_hat)\n\t        y_hat = np.array([self.loss_fn.postproc(y_hat_i) for y_hat_i in y_hat])\n\t        if y is not None:\n\t            y = np.array([self.loss_fn.postproc(y_i) for y_i in y])\n", "            losses = self.loss_fn(y_hat, y)\n\t            if self.two_layers:\n\t                elbo1, elbo2, p1, p2 = self.inference_vi(\n\t                    x_stripped, h_1_out, h_1, y, y_hat, losses\n\t                )\n\t                elbo = elbo1 + elbo2\n\t                return elbo, p1, p2, np.mean(losses), elbo1, elbo2\n\t            else:\n\t                elbo, p1, p2 = self.inference_one_layer(x, y, y_hat, losses)\n\t                return elbo, p1, p2, np.mean(losses), 0.0, elbo\n", "        else:\n\t            return y_hat\n"]}
{"filename": "dln/vi/layers.py", "chunked_list": ["from typing import List\n\timport numpy as np\n\tfrom dln.operator import forward_evaluate\n\tfrom dln.score import LogProbs, LogProbsScore, OutputClasses, ScoreRequest\n\tfrom dln.template import load_template\n\tfrom dln.vi.utils import log_message\n\tclass PriorLayer:\n\t    def __init__(self, forward_template, init=None):\n\t        self.forward_template = load_template(\n\t            forward_template\n", "        )\n\t        log_message(\"Forward template:\\n\", f\"{repr(self.forward_template.template)}\")\n\t        self.weight = init\n\t    def __call__(self, *args, **kwargs):\n\t        return self.forward(*args, **kwargs)\n\t    def forward(\n\t        self,\n\t        inputs,\n\t        output_classes: OutputClasses = None,\n\t        temperature=0.0,\n", "        strip_double_newlines=True,\n\t        max_tokens=256,\n\t    ) -> np.array:\n\t        \"\"\"Forward pass throught this layer.\n\t        Args:\n\t            output_classes: if not None, compute the constrained forward pass on the output classes, pick the highest probability amongst\n\t                            the prototypes.\n\t            temperature: temperature to use for the forward pass\n\t            strip_double_newlines: if True, strip any \"\\n\\n\" that might have been added\n\t            max_tokens: cap the max length for the forward pass\n", "        \"\"\"\n\t        if output_classes is None:\n\t            tpl_inputs = [\n\t                self.forward_template.render(input=input, prompt=self.weight)\n\t                for input in inputs\n\t            ]\n\t            outputs = forward_evaluate(\n\t                tpl_inputs,\n\t                stop=self.forward_template.stop_tokens,\n\t                temperature=temperature,\n", "                max_tokens=max_tokens,\n\t            )\n\t        else:\n\t            # compute constrained forward pass on the output classes\n\t            targets = [output_classes.prototype(0) for _ in inputs]\n\t            # compute log p of each output class, second return value is the p(class)\n\t            lp = self.log_p(\n\t                inputs, targets, output_classes=output_classes, agg=\"sum\"\n\t            ).contexts\n\t            # best output class index\n", "            best_output_class_index = np.argmax(lp, axis=1)\n\t            # get the best output class token\n\t            outputs = [output_classes.prototype(idx) for idx in best_output_class_index]\n\t        # strip any \"\\n\\n\" that might have been added\n\t        if strip_double_newlines:\n\t            outputs = [o.replace(\"\\n\\n\", \"\\n\") for o in outputs]\n\t        return np.asarray(outputs)\n\t    def log_p_request(self, input: str, target: str, prompt: str) -> ScoreRequest:\n\t        # build up a set of score requests\n\t        context = self.forward_template.render(input=input, prompt=prompt)\n", "        return ScoreRequest(context=context, target=target, payload=target)\n\t    def log_p(\n\t        self,\n\t        inputs: List[str],\n\t        targets: List[str],\n\t        prompts=None,\n\t        output_classes=None,\n\t        agg=\"max\",\n\t    ) -> LogProbs:\n\t        requests = []\n", "        if prompts is None:\n\t            prompts = [self.weight for _ in inputs]\n\t        for input, target, prompt in zip(inputs, targets, prompts):\n\t            requests.append(self.log_p_request(input, target, prompt=prompt))\n\t        # build up a set of score requests\n\t        logprobs = LogProbsScore().score_requests(requests, output_classes, agg=agg)\n\t        return logprobs\n\tclass ResidualPriorLayer(PriorLayer):\n\t    def __init__(self, forward_template, init=None, residual_template=\"classify_residual\"):\n\t        super().__init__(forward_template, init=init)\n", "        self.residual_template = load_template(\n\t            residual_template\n\t        )\n\t        log_message(\"Residual template:\\n\", f\"{repr(self.residual_template.template)}\")\n\t    def forward(self, inputs, **kwargs) -> np.array:\n\t        outputs = super().forward(inputs, **kwargs)\n\t        return outputs\n\t    def apply_residual(\n\t        self, outputs: np.array, inputs: np.array, use_template=False\n\t    ) -> np.array:\n", "        outputs_ = []\n\t        if use_template:\n\t            for output, input in zip(outputs, inputs):\n\t                tpl_input = self.forward_template.render(\n\t                    input=input, prompt=self.weight\n\t                )\n\t                outputs_.append(\n\t                    self.residual_template.render(\n\t                        input=tpl_input, output=output\n\t                    )\n", "                )\n\t        else:\n\t            for output, input in zip(outputs, inputs):\n\t                outputs_.append(\n\t                    self.residual_template.render(\n\t                        input=input, output=output\n\t                    )\n\t                )\n\t        return np.array(outputs_)\n"]}
{"filename": "dln/vi/sampler.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\timport numpy as np\n\tfrom dln.operator import backward_evaluate\n\tfrom dln.template import load_template\n\tfrom dln.vi.utils import log_message\n\t@dataclass\n\tclass Info:\n\t    input: str = None\n\t    output: str = None\n", "    target: str = None\n\t    loss: float = 0.0\n\tclass PromptSampler:\n\t    def __init__(self, p_template=\"q_action_prompt:v3.5\"):\n\t        self.prompt_template = load_template(p_template)\n\t        log_message(\"Prompt template:\\n\", f\"{repr(self.prompt_template.template)}\")\n\t        log_message(\n\t            \"Message alternatives:\\n\", f\"{self.prompt_template.message_alternatives}\"\n\t        )\n\t        self.evaluate_func = backward_evaluate\n", "    def sample_q_p(\n\t        self,\n\t        inputs: np.array,\n\t        y: np.array,\n\t        y_hat: np.array,\n\t        losses: np.array,\n\t        prompt: str,\n\t        num_samples=1,\n\t        held_out_half=False,\n\t    ):\n", "        \"\"\"\n\t        Args:\n\t            inputs: input sequences\n\t            y: target sequences\n\t            y_hat: predicted sequences\n\t            losses: losses for each sequence\n\t            prompt: prompt to use for sampling\n\t            num_samples: number of samples to generate\n\t            held_out_half: if True, only use the first half of the data points for sampling prompts\n\t        \"\"\"\n", "        infos = [\n\t            Info(input=input_i, output=y_hat_i, target=y_i, loss=loss)\n\t            for input_i, y_i, y_hat_i, loss in zip(inputs, y, y_hat, losses)\n\t        ]\n\t        while True:\n\t            try:\n\t                tpls = []\n\t                for i in range(num_samples - 1):\n\t                    if self.prompt_template.message_alternatives is None:\n\t                        message = None\n", "                    else:\n\t                        message = self.prompt_template.message_alternatives[\n\t                            i % len(self.prompt_template.message_alternatives)\n\t                        ]\n\t                    indices = np.random.permutation(np.arange(len(infos)))\n\t                    if held_out_half:\n\t                        infos_ = [infos[i] for i in indices[: len(infos) // 2]]\n\t                    else:\n\t                        infos_ = [infos[i] for i in indices]\n\t                    tpls.append(\n", "                        self.prompt_template.render(\n\t                            backward_infos=infos_,\n\t                            prompt=prompt,\n\t                            message=message,\n\t                        )\n\t                    )\n\t                log_message(\"Prompt Sampler:\", tpls[-1])\n\t                log_message(\"Generating {} ~p proposals...\".format(num_samples))\n\t                prompts = self.evaluate_func(\n\t                    tpls, stop=self.prompt_template.stop_tokens, n=1,\n", "                )\n\t                log_message(\"DONE...\")\n\t                prompts = np.array([prompt] + list(prompts))\n\t                return prompts\n\t            except KeyboardInterrupt:\n\t                break\n\t            except:\n\t                if len(infos) > 1:\n\t                    infos = infos[1:]\n\t                    logging.info(\"DROPPING A DATA POINT...\")\n", "                else:\n\t                    error_message = \"Still exeeding context length after shrinking backward_infos.\"\n\t                    logging.info(\n\t                        error_message\n\t                    )\n\t                    raise ValueError(error_message)\n\tclass PosteriorSampler:\n\t    def __init__(self, q_template):\n\t        self.q_templates = []\n\t        for q_template in q_template.split(\"|\"):\n", "            self.q_templates.append(load_template(q_template))\n\t        for q_template in self.q_templates:\n\t            log_message(\"Q template:\", f\"{repr(q_template.template)}\")\n\t        self.stop_tokens = self.q_templates[0].stop_tokens\n\t        self.evaluate_func = backward_evaluate\n\t    def sample_q_h(\n\t        self,\n\t        x: np.array,\n\t        y: np.array,\n\t        h: np.array,\n", "        prompt: str,\n\t        next_prompt: str,\n\t        num_samples=1,\n\t        strip_double_newlines=True,\n\t    ):\n\t        \"\"\"\n\t        Sample a new hidden state from the posterior distribution.\n\t        Args:\n\t            x: inputs\n\t            y: labels\n", "            y_hat: model predictions for the forward pass\n\t            h: hidden states for the forward pass\n\t            task_description: task description if any\n\t            prompt: prompt for the layer that generated h\n\t            next_prompt: prompt for the layer above h\n\t            forward_template: template for the forward pass that generated h\n\t            num_samples: number of samples to generate\n\t            strip_double_newlines: strip double new lines from the output samples\n\t        Returns\n\t            (batch_size, num_samples) array of hidden states\n", "        \"\"\"\n\t        tpls = []\n\t        for x_i, h_i, y_i in zip(x, h, y):\n\t            for j in range(num_samples):\n\t                # pick a template at random\n\t                q_template = self.q_templates[\n\t                    np.random.choice(np.arange(len(self.q_templates)))\n\t                ]\n\t                if q_template.message_alternatives is not None:\n\t                    message = q_template.message_alternatives[\n", "                        j % len(q_template.message_alternatives)\n\t                    ]\n\t                else:\n\t                    message = None\n\t                tpl = q_template.render(\n\t                    input=x_i,\n\t                    h=h_i,\n\t                    prompt=prompt,\n\t                    next_prompt=next_prompt,\n\t                    y=y_i,\n", "                    message=message,\n\t                )\n\t                tpls.append(tpl)\n\t        # WATCH OUT: we only use max_tokens=128\n\t        max_tokens = 256\n\t        assert len(\n\t            tpls\n\t        ), \"If we are here, it means that either we resample hidden states, or that there are some errors.\"\n\t        # this might happen when all memories are correct\n\t        log_message(\"Q proposals: \" + str(len(tpls)) + \", Q template:\" + \"\\n\" + tpls[0])\n", "        log_message(\n\t            \"Generating {} ~h proposals... max_tokens={}\".format(\n\t                num_samples, max_tokens\n\t            )\n\t        )\n\t        sampled = self.evaluate_func(\n\t            tpls,\n\t            stop=self.stop_tokens,\n\t            n=1,\n\t            max_tokens=max_tokens,\n", "        )\n\t        # strip any \"\\n\\n\" that might have been added\n\t        if strip_double_newlines:\n\t            sampled = [s.replace(\"\\n\\n\", \"\\n\") for s in sampled]\n\t        sampled = np.asarray(sampled).reshape(x.shape[0], num_samples)\n\t        assert sampled.shape[0] == x.shape[0]\n\t        return sampled\n"]}
{"filename": "dln/vi/utils.py", "chunked_list": ["import logging\n\timport os\n\timport copy\n\timport json\n\tfrom typing import Dict\n\timport numpy as np\n\t# Use this function to log messages to the file\n\tdef log_message(*messages):\n\t    print(*messages)\n\t    logging.info(\" \".join(map(str, messages)))\n", "def compute_pairwise_kl(lps):\n\t    \"\"\"We make updates constrained by the KL divergence between the function induced by the current prompt\n\t    and the function induced by the new prompt. We compute the KL divergence\n\t    between the functions induced by the prompts.\n\t    The distribution induced by the current prompt is the first element of the second axis in lps.\n\t    \"\"\"\n\t    # compute pairwise kl, considers reference always as the first prompt\n\t    return (\n\t        (lps[:, :1, :] * (np.log(lps[:, :1, :]) - np.log(lps[:, :, :]))).sum(-1).mean(0)\n\t    )\n", "class ResultLogEntry():\n\t    def __init__(self):\n\t        self.hiddens = None\n\t        self.candidates = [[], []]\n\t        self.metrics = {}\n\t        self.outputs = []\n\t    def log_metric(self, metric: str, value: float):\n\t        self.metrics[metric] = value\n\t    def log_outputs(self, outputs):\n\t        self.outputs = outputs\n", "    def log_hiddens(self, hiddens, size):\n\t        self.hiddens = [[]] * size if hiddens is None else [[h] for h in hiddens]\n\t    def log_candidates(self, p_tilde_2, p2_elbo, p_tilde_1=None, p1_elbo=None):\n\t        \"\"\"\n\t            If one_layer, p_tilde_1 and p1_elbo are None,\n\t            and we only store the two-layer candidates in the 0th list element. 1st element stays [].\n\t            If two_layer, we store the first layer candidates in the 0th list element\n\t            and the second layer candidates in the 1st list element.\n\t        \"\"\"\n\t        self.candidates = [[],[]]\n", "        if p_tilde_1 is not None:\n\t            for i in range(p_tilde_1.shape[0]):\n\t                self.candidates[0].append({\n\t                    \"layer\": p_tilde_1[i],\n\t                    \"score\": p1_elbo[i],\n\t                })\n\t            p2_ind = 1\n\t        else:\n\t            p2_ind = 0\n\t        for i in range(p_tilde_2.shape[0]):\n", "            self.candidates[p2_ind].append({\n\t                \"layer\": p_tilde_2[i],\n\t                \"score\": p2_elbo[i],\n\t            })\n\tclass ResultLogWriter(object):\n\t    def __init__(self, dataset: str, path: str, name: str = None):\n\t        \"\"\"\n\t        Args:\n\t            dataset: The name of the dataset (used as name if save_name is None)\n\t            save_name: Dictionary key to save the results under\n", "        Returns:\n\t            A ResultLogWriter object\n\t        \"\"\"\n\t        self.name = name if name is not None else dataset\n\t        self.path = path\n\t        self.result_dict = {}\n\t        self.result_dict[self.name] = {'training': [], 'examples': []}\n\t    def write_result(self, step, layers, metrics, candidates):\n\t        self.result_dict[self.name]['training'].append({'step': step})\n\t        self.result_dict[self.name]['training'][-1]['layers'] = copy.deepcopy(layers)\n", "        self.result_dict[self.name]['training'][-1]['metrics'] = copy.deepcopy(metrics)\n\t        self.result_dict[self.name]['training'][-1]['candidates'] = copy.deepcopy(candidates)\n\t    def write_examples(self, step, inputs, labels, outputs, hiddens):\n\t        \"\"\"\n\t        Args:\n\t            step: The iteration number\n\t            inputs: A list of input strings\n\t            labels: A list of label strings\n\t            outputs: A list of output strings\n\t            hiddens: A list of hidden strings for two-layer-dlns\n", "        An element of the \"examples\" list in the json file looks like:\n\t        {\n\t            \"input\": \"Do cats sit on mats?\",\n\t            \"label\": \"Yes\",\n\t            \"trace\": [\n\t                {\n\t                    \"step\": 0,\n\t                    \"hiddens\": [\"Cats are picky.\"],\n\t                    \"output\": \"No\"\n\t                },\n", "                {\n\t                    \"step\": 1,\n\t                    \"hiddens\": [\"Cats would sit anywhere.\"],\n\t                    \"output\": \"Yes\"\n\t                }\n\t            ]\n\t        }\n\t        \"\"\"\n\t        for inp, lab, outp, hidden in zip(inputs, labels, outputs, hiddens):\n\t            # Get the element in the list of examples that matches the input\n", "            example = next((ex for ex in self.result_dict[self.name]['examples'] if ex['input'] == inp), None)\n\t            if example is None:\n\t                self.result_dict[self.name]['examples'].append({\n\t                    \"input\": inp,\n\t                    \"label\": lab,\n\t                    \"trace\": [{\"step\": step, \"hiddens\": hidden, \"output\": outp}],\n\t                })\n\t            else:\n\t                example['trace'].append({\"step\": step, \"hiddens\": hidden, \"output\": outp})\n\t    def save_to_json_file(self):\n", "        # self.path is a path to a file\n\t        os.makedirs(os.path.dirname(self.path), exist_ok=True)\n\t        try:\n\t            with open(self.path, 'r') as f:\n\t                print('Loading existing json file %s' % self.path)\n\t                loaded_dict = json.load(f)\n\t        except FileNotFoundError:\n\t            loaded_dict = {}\n\t        if self.name not in loaded_dict:\n\t            # Append or add the json dictionary if the result doesn't exist\n", "            loaded_dict[self.name] = self.result_dict[self.name]\n\t            with open(self.path, 'w') as f:\n\t                json.dump(loaded_dict, f, indent=4)\n\t        else:\n\t            print(f\"Result named {self.name} already exists in {self.path}!\")"]}
{"filename": "projects/demo/demo.py", "chunked_list": ["import argparse\n\timport json\n\timport textwrap\n\timport dash\n\timport dash_bootstrap_components as dbc\n\timport numpy as np\n\timport pandas as pd\n\timport plotly.graph_objects as go\n\tfrom dash import dcc, html\n\tfrom dash.dependencies import Input, Output\n", "from jinja2 import Template\n\tfrom plotly.subplots import make_subplots\n\tforward_template_L1 = Template(\n\t    \"{{ input }}\\n\\n{{ prompt }} Let's think step by step.\"\n\t)  # Loaded template suffix_forward_tbs v1.0\n\tforward_template_L2 = Template(\n\t    \"{{ prompt }}\\n\\n{{ input }}\\n\\nAnswer:\"\n\t)  #  Loaded template classify_forward v3.0\n\tDATASETS = [\n\t    (\"subj\", \"1 Layer - Subj\"),\n", "    (\"hyperbaton\", \"1 Layer - Hyperbaton\"),\n\t    (\"navigate\", \"2 Layers - Navigate\"),\n\t]\n\tdef wrap_text(text, width=100):\n\t    text = text.replace(\"\\n\\n\", \"\\n\")\n\t    return \"\\n\".join(\"\\n\".join(textwrap.wrap(line, width)) for line in text.split(\"\\n\"))\n\tdef load_data(log_file, dataset):\n\t    with open(log_file) as f:\n\t        logs = json.load(f)[dataset]\n\t    flattened_data = []\n", "    flattened_candidates = []\n\t    for item in logs[\"training\"]:\n\t        flat_item = {\"step\": item[\"step\"]}\n\t        flat_item.update(\n\t            {\n\t                metric: value if value is not None else np.nan\n\t                for metric, value in item[\"metrics\"].items()\n\t            }\n\t        )\n\t        flat_item.update(\n", "            {f\"layer_{i}\": wrap_text(l) for i, l in enumerate(item[\"layers\"], 1)}\n\t        )\n\t        flattened_data.append(flat_item)\n\t        candidates_data = {}\n\t        for layer, candidates in enumerate(item[\"candidates\"], 1):\n\t            for idx, candidate in enumerate(candidates):\n\t                candidate_data = candidates_data.setdefault(idx, {\"step\": item[\"step\"]})\n\t                candidate_data[f\"layer_{layer}_candidate\"] = candidate[\"layer\"]\n\t                candidate_data[f\"layer_{layer}_score\"] = candidate[\"score\"]\n\t        flattened_candidates += list(candidates_data.values())\n", "    flattened_examples = []\n\t    for i, example in enumerate(logs[\"examples\"]):\n\t        for item in example[\"trace\"]:\n\t            flat_item = {\n\t                \"id\": i + 1,\n\t                \"input\": wrap_text(example[\"input\"]),\n\t                \"label\": example[\"label\"],\n\t                \"step\": item[\"step\"],\n\t                \"hidden\": wrap_text(item[\"hiddens\"][0]) if item[\"hiddens\"] else \"\",\n\t                \"output\": wrap_text(item[\"output\"]),\n", "            }\n\t            flattened_examples.append(flat_item)\n\t    return (\n\t        pd.DataFrame(flattened_data),\n\t        pd.DataFrame(flattened_candidates).dropna(),\n\t        pd.DataFrame(flattened_examples),\n\t    )\n\tdef load_dataset_names(log_file):\n\t    with open(log_file) as f:\n\t        logs = json.load(f)\n", "    return [(x, x) for x in list(logs.keys())]\n\tdef main(args):\n\t    datasets = load_dataset_names(args.logfile) if args.logfile else DATASETS\n\t    app = dash.Dash()\n\t    app.layout = html.Div(\n\t        [\n\t            html.H2(\n\t                \"Deep Language Networks\",\n\t                style={\n\t                    \"textAlign\": \"center\",\n", "                },\n\t            ),\n\t            dcc.Dropdown(\n\t                id=\"dataset_dropdown\",\n\t                options=[\n\t                    {\"label\": f\"{title}\", \"value\": id_} for id_, title in datasets\n\t                ],\n\t                value=datasets[0][0],\n\t                multi=False,\n\t                style={\n", "                    \"backgroundColor\": \"rgb(229, 236, 246)\",\n\t                    \"margin\": \"10px 0\",\n\t                },\n\t            ),\n\t            dcc.Dropdown(\n\t                id=\"example_dropdown\",\n\t                options=[\n\t                    {\n\t                        \"label\": f\"Example {i}\" if i > 0 else \"Show only prompts\",\n\t                        \"value\": i,\n", "                    }\n\t                    for i in range(0, 20 + 1)\n\t                ],\n\t                value=0,  # df['id'].iloc[0],\n\t                multi=False,\n\t                style={\n\t                    \"backgroundColor\": \"rgb(229, 236, 246)\",\n\t                    \"margin\": \"10px 0\",\n\t                },\n\t            ),\n", "            dcc.Graph(id=\"scatter-plot\"),\n\t            html.Div(\n\t                id=\"table-container\",\n\t                style={\n\t                    \"backgroundColor\": \"rgb(229, 236, 246)\",\n\t                    \"margin\": \"10px 0\",\n\t                    \"padding\": \"10px\",\n\t                },\n\t            ),\n\t        ]\n", "    )\n\t    # Create a callback to update the table-container\n\t    @app.callback(\n\t        Output(\"table-container\", \"children\"),\n\t        [Input(\"scatter-plot\", \"hoverData\"), Input(\"dataset_dropdown\", \"value\")],\n\t    )  # coulbe be either clickData, hoverData\n\t    def update_table(callbackData, dataset_dropdown):\n\t        df, candidates, examples = load_data(\n\t            args.logfile or \"data.json\", dataset_dropdown\n\t        )\n", "        # Merge layers and examples\n\t        df = df.merge(examples, on=\"step\", how=\"left\")\n\t        step = callbackData[\"points\"][0][\"x\"] if callbackData is not None else 0\n\t        filtered_df = candidates[candidates[\"step\"] == step]\n\t        table = dbc.Table.from_dataframe(\n\t            filtered_df, striped=True, bordered=True, hover=True\n\t        )\n\t        return table\n\t    @app.callback(\n\t        Output(\"scatter-plot\", \"figure\"),\n", "        [Input(\"example_dropdown\", \"value\"), Input(\"dataset_dropdown\", \"value\")],\n\t    )\n\t    def update_scatter_plot(example_dropdown, dataset_dropdown):\n\t        df, candidates, examples = load_data(\n\t            args.logfile or \"data.json\", dataset_dropdown\n\t        )\n\t        # Merge layers and examples\n\t        df = df.merge(examples, on=\"step\", how=\"left\")\n\t        EXAMPLE_ID = example_dropdown or 1\n\t        dev_df = df\n", "        dev_df = df[df[\"id\"] == EXAMPLE_ID]\n\t        dev_df = dev_df[dev_df[\"dev_acc\"] >= 0]\n\t        NB_LAYERS = len([c for c in dev_df.columns if c.startswith(\"layer\")])\n\t        if example_dropdown == 0:\n\t            if NB_LAYERS == 1:\n\t                layers_columns = [c for c in dev_df.columns if c.startswith(\"layer\")]\n\t                for column in layers_columns:\n\t                    # Wrap text for display in hover\n\t                    dev_df[column] = dev_df[column].apply(\n\t                        lambda x: x.replace(\"\\n\", \"<br>\")\n", "                    )\n\t                hover_template = \"<b> prompt:</b> %{customdata[0]}\"\n\t            elif NB_LAYERS == 2:\n\t                layers_columns = [c for c in dev_df.columns if c.startswith(\"layer\")]\n\t                for column in layers_columns:\n\t                    # Wrap text for display in hover\n\t                    dev_df[column] = dev_df[column].apply(\n\t                        lambda x: x.replace(\"\\n\", \"<br>\")\n\t                    )\n\t                hover_template = (\n", "                    \"<b>Layer 1 prompt:</b> %{customdata[0]}\"\n\t                    \"<br><b>Layer 2 prompt:</b> %{customdata[1]}\"\n\t                )\n\t        else:\n\t            if NB_LAYERS == 1:\n\t                layers_columns = [\n\t                    c for c in dev_df.columns if c.startswith(\"layer\")\n\t                ] + [\"input\", \"output\", \"label\"]\n\t                for column in layers_columns:\n\t                    # Wrap text for display in hover\n", "                    dev_df[column] = dev_df[column].apply(\n\t                        lambda x: x.replace(\"\\n\", \"<br>\")\n\t                    )\n\t                hover_template = (\n\t                    \"<b>Input:</b> %{customdata[1]}\"\n\t                    \"<br><b>Layer 1 prompt:</b> %{customdata[0]}\"\n\t                    \"<br><b>Output:</b> %{customdata[2]}\"\n\t                    \"<br><b>Label:</b> %{customdata[3]}\"\n\t                )\n\t            elif NB_LAYERS == 2:\n", "                layers_columns = [\n\t                    c for c in dev_df.columns if c.startswith(\"layer\")\n\t                ] + [\"input\", \"hidden\", \"output\", \"label\"]\n\t                for column in layers_columns:\n\t                    # Wrap text for display in hover\n\t                    dev_df[column] = dev_df[column].apply(\n\t                        lambda x: x.replace(\"\\n\", \"<br>\")\n\t                    )\n\t                hover_template = (\n\t                    \"<b>Input:</b> %{customdata[2]}\"\n", "                    \"<br><b>Layer 1 prompt:</b> %{customdata[0]}\"\n\t                    \"<br><b>Hidden:</b> %{customdata[3]}\"\n\t                    \"<br><b>Layer 2 prompt:</b> %{customdata[1]}\"\n\t                    \"<br><b>Output:</b> %{customdata[4]}\"\n\t                    \"<br><b>Label:</b> %{customdata[5]}\"\n\t                )\n\t            else:\n\t                raise NotImplementedError()\n\t        hover_config = {\n\t            \"customdata\": dev_df[layers_columns],\n", "            \"hovertemplate\": hover_template,\n\t        }\n\t        # Create figure with secondary y-axis\n\t        fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\t        # Make Figure taller\n\t        fig.update_layout(\n\t            autosize=False,\n\t            width=1900,\n\t            height=1000,\n\t            margin=dict(l=50, r=50, b=100, t=100, pad=4),\n", "        )\n\t        # Add traces\n\t        # text = [\"Acc\"] * len(df[\"step\"])\n\t        fig.add_trace(\n\t            go.Scatter(\n\t                x=df[\"step\"], y=df[\"run_acc\"], name=\"Running acc\", hoverinfo=\"none\"\n\t            ),  # , text=text, **hover_config\n\t            secondary_y=False,\n\t        )\n\t        # text = [\"Dev Acc\"] * len(df[\"step\"][df[\"dev_acc\"] >= 0])\n", "        fig.add_trace(\n\t            go.Scatter(\n\t                x=dev_df[\"step\"], y=dev_df[\"dev_acc\"], name=\"Dev acc\", **hover_config\n\t            ),\n\t            secondary_y=False,\n\t        )\n\t        # text = [\"ELBO\"] * len(df[\"step\"])\n\t        fig.add_trace(\n\t            go.Scatter(\n\t                x=df[\"step\"],\n", "                y=df[\"run_elbo\"],\n\t                name=\"Running ELBO\",\n\t                hoverinfo=\"none\",\n\t                visible=\"legendonly\",\n\t            ),  # , text=text, **hover_config),\n\t            secondary_y=True,\n\t        )\n\t        # Set x-axes title\n\t        fig.update_xaxes(title_text=\"steps\", nticks=20)\n\t        # Set y-axes titles\n", "        fig.update_yaxes(title_text=\"<b>Accuracy</b>\", secondary_y=False)\n\t        fig.update_yaxes(title_text=\"<b>ELBO</b>\", secondary_y=True)\n\t        # Define hover text font and color.\n\t        fig.update_layout(hovermode=\"x\")\n\t        fig.update_layout(\n\t            hoverlabel=dict(\n\t                bgcolor=\"rgba(255,255,255,0.75)\",\n\t                font_size=16,\n\t                font_family=\"Rockwell\",\n\t            ),\n", "        )\n\t        return fig\n\t    app.run_server(debug=args.debug, host=args.dash_host or \"127.0.0.1\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"logfile\", nargs=\"?\", help=\"Log file to use (JSON).\")\n\t    parser.add_argument(\n\t        \"--dash-host\", help=\"Host for Dash (setting this, implies --dash).\"\n\t    )\n\t    parser.add_argument(\n", "        \"--debug\", action=argparse.BooleanOptionalAction, help=\"Launch in debug mode.\"\n\t    )\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "projects/vi_dln/vi_main.py", "chunked_list": ["import datetime\n\timport json\n\timport logging\n\timport os\n\tfrom collections import Counter\n\timport click\n\timport numpy as np\n\timport tqdm\n\tfrom termcolor import colored\n\tfrom torch.utils.tensorboard import SummaryWriter\n", "from dln.dataset import init_dataset\n\tfrom dln.loss import ZeroOneLoss\n\tfrom dln.operator import backward_instantiate, forward_instantiate\n\tfrom dln.postprocessing import postprocess_prediction\n\tfrom dln.vi.model import VILModel, log_message\n\tfrom dln.vi.utils import ResultLogWriter\n\ttry:\n\t    import wandb\n\t    wandb_installed = True\n\texcept ImportError:\n", "    wandb_installed = False\n\tdef init_prompts(dataset, init_p1, init_p2):\n\t    \"\"\"Initialize the prompts for the two layers of the model.\n\t    If init_p1 or init_p2 is a json file, load the best weights from the json file.\n\t    \"\"\"\n\t    if init_p1 and init_p1.endswith(\".json\"):\n\t        with open(init_p1) as f:\n\t            best_weights = json.load(f)\n\t        init_p1 = best_weights[dataset][\"best_weights\"]\n\t    elif init_p2 and init_p2.endswith(\".json\"):\n", "        with open(init_p2) as f:\n\t            best_weights = json.load(f)\n\t        init_p2 = best_weights[dataset][\"best_weights\"]\n\t    return init_p1, init_p2\n\tdef validate(dataset, model, loss_fn, iteration, val_examples, val_scores, writer, result_writer):\n\t    log_message(colored(\"VALIDATING...\", \"red\"))\n\t    log_message(\"Current L1 weights:\", model.encoder_l1.weight)\n\t    log_message(\"Current L2 weights:\", model.encoder_l2.weight)\n\t    val_key = \"{}-{}\".format(model.encoder_l1.weight, model.encoder_l2.weight)\n\t    if val_key in val_scores:\n", "        log_message(\"Already evaluated this configuration, skipping...\")\n\t        dev_acc = val_scores[val_key]\n\t    else:\n\t        acc = 0.0\n\t        tot = 0.0\n\t        pbar = tqdm.tqdm(\n\t            total=dataset.get_size(\"dev\") if val_examples < 0 else val_examples,\n\t            bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\",\n\t            desc=\"Eval\",\n\t        )\n", "        dataset.reset_pointer(\"dev\")\n\t        num_examples = 0\n\t        for batch in dataset.iterate(\"dev\", batch_size=20):\n\t            x, y = batch\n\t            y_hat = model.forward(np.array(x))\n\t            result_writer.write_examples(iteration, x, y, model.result_entry.outputs, model.result_entry.hiddens)\n\t            acc += len(y) - np.sum(loss_fn(y_hat, y))\n\t            tot += len(y)\n\t            pbar.update(len(y))\n\t            pbar.set_postfix_str(f\"{acc / tot:.1%}\")\n", "            num_examples += len(y)\n\t            if num_examples == val_examples:\n\t                break\n\t        dev_acc = acc / tot\n\t        val_scores[val_key] = dev_acc\n\t    if iteration == 0:\n\t        log_message(colored(\"INIT DEV ACC: {}\".format(dev_acc), \"red\"))\n\t    log_message(colored(\"DEV ACC: {}\".format(dev_acc), \"red\"))\n\t    writer.add_scalar(\"dev/acc\", (dev_acc), iteration)\n\t    return dev_acc\n", "def test(dataset, model, loss_fn, iteration, writer):\n\t    log_message(colored(\"TESTING...\", \"red\"))\n\t    acc = 0.0\n\t    tot = 0.0\n\t    pbar = tqdm.tqdm(\n\t        total=dataset.get_size(\"test\"),\n\t        bar_format=\"{l_bar}{bar:10}{r_bar}{bar:-10b}\",\n\t        desc=\"Eval\",\n\t    )\n\t    dataset.reset_pointer(\"test\")\n", "    for batch in dataset.iterate(\"test\", batch_size=20):\n\t        x, y = batch\n\t        y_hat = model.forward(np.array(x))\n\t        acc += len(y) - np.sum(loss_fn(y_hat, y))\n\t        tot += len(y)\n\t        pbar.update(len(y))\n\t        pbar.set_postfix_str(f\"{acc / tot:.1%}\")\n\t    test_acc = acc / tot\n\t    writer.add_scalar(\"test/acc\", (test_acc), iteration)\n\t    return test_acc\n", "@click.command()\n\t@click.option(\"--seed\", default=42, help=\"Random seed.\")\n\t@click.option(\"--out_dir\", default=\"log/\")\n\t@click.option(\"--data_dir\", default=\"../../data\")\n\t@click.option(\"--val_freq\", default=2)\n\t@click.option(\"--do_first_eval\", is_flag=True)\n\t@click.option(\"--do_zero_shot\", is_flag=True)\n\t@click.option(\"--q_hidden\", default=\"suffix_forward_tbs\")\n\t@click.option(\"--q_prompt\", default=\"q_action_prompt\")\n\t@click.option(\"--p_hidden\", default=\"suffix_forward_tbs\")\n", "@click.option(\"--p_class\", default=\"classify_forward\")\n\t@click.option(\"--balance_batch\", is_flag=True, help=\"Balance batch.\")\n\t@click.option(\"--batch_size\", type=int, default=20)\n\t@click.option(\"--one_layer\", is_flag=True)\n\t@click.option(\"--dataset\", type=str, default=\"subj\")\n\t@click.option(\"--use_h_argmax\", type=bool, default=False)\n\t@click.option(\"--iters\", type=int, default=20)\n\t@click.option(\"--num_p_samples\", type=int, default=5)\n\t@click.option(\"--num_h_samples\", type=int, default=3)\n\t@click.option(\"--tolerance\", type=int, default=-1)\n", "@click.option(\n\t    \"--strip_options_for_hidden\",\n\t    type=bool,\n\t    default=False,\n\t    help=\"Remove options from examples for the hidden layer.\",\n\t)\n\t@click.option(\n\t    \"--strip_prefix_for_hidden\",\n\t    type=bool,\n\t    default=False,\n", "    help=\"Strip the prefix from the examples if it exists in some tasks, e.g. BBH.\",\n\t)\n\t@click.option(\n\t    \"--strip_answer_for_hidden\",\n\t    type=bool,\n\t    default=False,\n\t    help=\"Strip the 'Answer:' from the hidden state, if the model generates it.\",\n\t)\n\t@click.option(\n\t    \"--trust_factor\",\n", "    default=0.0,\n\t    help=\"Trust-region factor for prompt update. Ensures KL divergence between the old and new prompt is small.\",\n\t)\n\t@click.option(\n\t    \"--fwd_temp\",\n\t    default=0.0,\n\t    help=\"Forward temperature\",\n\t)\n\t@click.option(\n\t    \"--bwd_temp\",\n", "    default=0.7,\n\t    help=\"Backward temperature\",\n\t)\n\t@click.option(\n\t    \"--one_batch\", type=float, default=0.0, help=\"Run only one batch, debug mode.\"\n\t)\n\t@click.option(\n\t    \"--use_memory\",\n\t    type=int,\n\t    default=0,\n", "    help=\"Include evaluation of past prompts that have worked well in the selection list.\",\n\t)\n\t@click.option(\n\t    \"--forward_use_classes\",\n\t    type=bool,\n\t    default=False,\n\t    help=\"Uses classes in the forward pass, constrains the output space.\",\n\t)\n\t@click.option(\n\t    \"--init_p1\",\n", "    type=str,\n\t    default=\"Decompose the problem to make it simpler:\",\n\t)\n\t@click.option(\n\t    \"--init_p2\",\n\t    type=str,\n\t    default=None,\n\t)\n\t@click.option(\n\t    \"--held_out_prompt_ranking\",\n", "    type=bool,\n\t    default=False,\n\t    help=\"Evaluate prompts to keep for the next iteration on held-out examples in the current batch.\",\n\t)\n\t@click.option(\n\t    \"--train_p1\",\n\t    type=bool,\n\t    default=True,\n\t    help=\"Train 1 layer, if False, keep it fixed.\",\n\t)\n", "@click.option(\n\t    \"--train_p2\",\n\t    type=bool,\n\t    default=True,\n\t    help=\"Train 2 layer, if False, keep it fixed.\",\n\t)\n\t@click.option(\n\t    \"--logp_penalty\",\n\t    type=float,\n\t    default=0.0,\n", "    help=\"Logp penalty for hiddens that haven't worked. Encourages exploration.\",\n\t)\n\t@click.option(\n\t    \"--decay_logp_penalty\",\n\t    type=bool,\n\t    default=True,\n\t    help=\"Decay logp penalty linearly, reaching zero at the last iteration.\",\n\t)\n\t@click.option(\n\t    \"--posterior_temp\",\n", "    type=float,\n\t    default=1.0,\n\t    help=\"Sharpen (<1.0)/Flatten (>1.0) the posterior distribution over h.\",\n\t)\n\t@click.option(\n\t    \"--model_type\",\n\t    type=str,\n\t    default=\"text-davinci-003\",\n\t)\n\t@click.option(\n", "    \"--fwd_max_tokens\",\n\t    type=int,\n\t    default=256,\n\t    help=\"Forward max tokens.\",\n\t)\n\t@click.option(\n\t    \"--bwd_max_tokens\",\n\t    type=int,\n\t    default=512,\n\t    help=\"Backward max tokens.\",\n", ")\n\t@click.option(\n\t    \"--result_data_path\",\n\t    type=str,\n\t    default=\"../demo/result_data.json\",\n\t    help=\"The path of the file where the result logs json are stored\",\n\t)\n\t@click.option(\n\t    \"--result_exp_name\",\n\t    type=str,\n", "    default=None,\n\t    help=\"(Optional) Name of the experiment run to be saved in the result logs json file.\"\n\t    \"Useful when running multiple experiments with the same dataset name.\",\n\t)\n\t@click.option(\n\t    \"--enable_wandb\",\n\t    is_flag=True,\n\t    help=\"Enable wandb logging. Requires wandb to be installed.\",\n\t)\n\tdef main(\n", "    seed,\n\t    out_dir,\n\t    data_dir,\n\t    val_freq,\n\t    do_first_eval,\n\t    do_zero_shot,\n\t    q_hidden,\n\t    q_prompt,\n\t    p_hidden,\n\t    p_class,\n", "    fwd_temp,\n\t    bwd_temp,\n\t    balance_batch,\n\t    batch_size,\n\t    one_layer,\n\t    dataset,\n\t    use_h_argmax,\n\t    iters,\n\t    num_p_samples,\n\t    num_h_samples,\n", "    strip_options_for_hidden,\n\t    strip_answer_for_hidden,\n\t    strip_prefix_for_hidden,\n\t    trust_factor,\n\t    one_batch,\n\t    use_memory,\n\t    init_p1,\n\t    init_p2,\n\t    tolerance,\n\t    forward_use_classes,\n", "    held_out_prompt_ranking,\n\t    train_p1,\n\t    train_p2,\n\t    logp_penalty,\n\t    decay_logp_penalty,\n\t    posterior_temp,\n\t    model_type,\n\t    fwd_max_tokens,\n\t    bwd_max_tokens,\n\t    result_data_path,\n", "    result_exp_name,\n\t    enable_wandb,\n\t):\n\t    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n\t    out_dir = f\"{out_dir}/{timestamp}\"\n\t    os.makedirs(out_dir, exist_ok=True)\n\t    logging.basicConfig(\n\t        filename=f\"{out_dir}/output.log\",\n\t        level=logging.INFO,\n\t        format=\"%(asctime)s - %(message)s\",\n", "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n\t    )\n\t    log_message(json.dumps(locals()))\n\t    log_message(\"Logging to... {}\".format(out_dir + \"/output.log\"))\n\t    wandb_enabled = False\n\t    if enable_wandb:\n\t        if wandb_installed:\n\t            wandb_enabled = True\n\t            wandb.init(config=locals(), project=\"dln\")\n\t            prompt_table = wandb.Table(columns=[\"epoch\", \"w1\", \"w2\"])\n", "        else:\n\t            log_message(colored(\"Wandb is not installed. Please install it to enable wandb logging.\", \"red\"))\n\t    writer = SummaryWriter(f\"{out_dir}\")\n\t    result_writer = ResultLogWriter(dataset, path=result_data_path, name=result_exp_name)\n\t    init_p1, init_p2 = init_prompts(dataset, init_p1, init_p2)\n\t    if wandb_enabled:\n\t        prompt_table.add_data(0, init_p1, init_p2)\n\t    dataset, output_classes, val_examples = init_dataset(dataset, seed, data_dir)\n\t    forward_instantiate(\n\t        model_type,\n", "        temperature=0.0,\n\t        max_tokens=fwd_max_tokens,\n\t        stop=None,\n\t    )\n\t    backward_instantiate(\n\t        model_type,\n\t        temperature=bwd_temp,\n\t        max_tokens=bwd_max_tokens,\n\t        stop=None,\n\t    )\n", "    loss_fn = ZeroOneLoss(postproc=postprocess_prediction)\n\t    model = VILModel(\n\t        loss_fn,\n\t        task_description=dataset.instruction,\n\t        two_layers=not one_layer,\n\t        num_p_samples=num_p_samples,\n\t        num_h_samples=num_h_samples,\n\t        q_hidden=q_hidden,\n\t        q_prompt=q_prompt,\n\t        p_hidden=p_hidden,\n", "        p_class=p_class,\n\t        init_p1=init_p1,\n\t        init_p2=init_p2,\n\t        use_h_argmax=use_h_argmax,\n\t        output_classes=output_classes,\n\t        strip_options_for_hidden=strip_options_for_hidden,\n\t        strip_answer_for_hidden=strip_answer_for_hidden,\n\t        trust_factor=trust_factor,\n\t        forward_use_classes=forward_use_classes,\n\t        held_out_prompt_ranking=held_out_prompt_ranking,\n", "        use_memory=use_memory,\n\t        train_p1=train_p1,\n\t        train_p2=train_p2,\n\t        logp_penalty=logp_penalty,\n\t        p1_max_tokens=256,\n\t        p2_max_tokens=20,\n\t        posterior_temp=posterior_temp,\n\t        strip_prefix_for_hidden=dataset.prefix if strip_prefix_for_hidden else None,\n\t    )\n\t    running_acc = 0.0\n", "    running_elbo = 0.0\n\t    best_dev = 0.0\n\t    best_ps = [model.encoder_l1.weight, model.encoder_l2.weight]\n\t    train_x, train_y = None, None\n\t    sample_next_batch = False\n\t    val_scores = {}\n\t    patience = 0\n\t    for iteration in range(iters + 1):\n\t        log_message(\"STARTING EPOCH {} - {}\".format(iteration, out_dir))\n\t        if iteration % val_freq == 0 and (\n", "            iteration > 0 or do_first_eval or do_zero_shot\n\t        ):\n\t            dev_acc = validate(\n\t                dataset, model, loss_fn, iteration, val_examples, val_scores, writer, result_writer\n\t            )\n\t            if wandb_enabled:\n\t                wandb.log({\"dev/acc\": dev_acc, \"epoch\": iteration})\n\t            model.result_entry.log_metric('dev_acc', dev_acc)\n\t            if dev_acc > best_dev:\n\t                best_dev = dev_acc\n", "                best_ps = (model.encoder_l1.weight, model.encoder_l2.weight)\n\t                if use_memory:\n\t                    model.add_to_memory(*best_ps, score=best_dev)\n\t                log_message(colored(\"BEST DEV ACC: {}\".format(best_dev), \"red\"))\n\t                patience = 0\n\t            else:\n\t                patience += 1\n\t            if tolerance >= 0 and patience >= tolerance:\n\t                log_message(\"Loading back the best model...\")\n\t                model.encoder_l1.weight = best_ps[0]\n", "                model.encoder_l2.weight = best_ps[1]\n\t                patience = 0\n\t        else:\n\t            model.result_entry.log_metric('dev_acc', None)\n\t        result_writer.write_result(\n\t            step=iteration,\n\t            layers=[model.encoder_l2.weight] if one_layer else [model.encoder_l1.weight, model.encoder_l2.weight],\n\t            metrics=model.result_entry.metrics,\n\t            candidates=model.result_entry.candidates,\n\t        )\n", "        # zero shot or allow last iteration for validation\n\t        if do_zero_shot or iteration == iters:\n\t            break\n\t        if one_batch > 0.0 and train_x is not None and not sample_next_batch:\n\t            # use the same batch, just re-shuffle the examples in the batch\n\t            permutation_indices = np.random.permutation(np.arange(len(train_x)))\n\t            x, y = np.asarray([train_x[i] for i in permutation_indices]), np.asarray(\n\t                [train_y[i] for i in permutation_indices]\n\t            )\n\t            log_message(colored(\"USING SAME BATCH FOR TRAINING!!!\", \"yellow\"))\n", "        else:\n\t            x, y = dataset.get_batch(\n\t                \"train\", batch_size, random_sample=True, balance=balance_batch\n\t            )\n\t            train_x, train_y = x, y\n\t        if decay_logp_penalty:\n\t            model.logp_penalty = logp_penalty * (1.0 - (iteration / iters))\n\t        log_message(colored(\"Training P2? {}\".format(model.train_p2), \"red\"))\n\t        log_message(colored(\"LOGPenalty? {}\".format(model.logp_penalty), \"red\"))\n\t        elbo, p1, p2, loss, elbo1, elbo2 = model.forward(\n", "            np.array(x), np.array(y), temperature=fwd_temp\n\t        )\n\t        # Update prompts\n\t        model.encoder_l1.weight = p1\n\t        model.encoder_l2.weight = p2\n\t        log_message(\"Patience: {}\".format(patience))\n\t        if iteration == 0:\n\t            running_elbo = elbo\n\t            running_acc = 1.0 - loss\n\t        else:\n", "            running_elbo = 0.2 * elbo + 0.8 * running_elbo\n\t            running_acc = 0.2 * (1.0 - loss) + 0.8 * running_acc\n\t        # get another batch if training accuracy is too good!\n\t        sample_next_batch = (1.0 - loss) > one_batch\n\t        log_message(\"--------------------\")\n\t        log_message(colored(\"{} TRAINING EPOCH DONE.\".format(iteration), \"blue\"))\n\t        log_message(colored(\"ELBO: {}\".format(elbo), \"blue\"))\n\t        log_message(colored(\"ACC: {}\".format((1.0 - loss)), \"blue\"))\n\t        log_message(colored(\"RUN ELBO: {}\".format(running_elbo), \"blue\"))\n\t        log_message(colored(\"RUN ACC: {}\".format(running_acc), \"blue\"))\n", "        log_message(colored(\"BATCH Y BALANCE: {}\".format(Counter(y)), \"blue\"))\n\t        log_message(colored(\"BATCH X LEN: {}\".format([len(x_i) for x_i in x]), \"blue\"))\n\t        if wandb_enabled:\n\t            prompt_table.add_data(iteration + 1, str(p1), str(p2))\n\t            wandb.log({\"train/prompts\" : prompt_table})\n\t            wandb.log({\"train/elbo\": elbo, \"train/acc\": (1.0 - loss), \"epoch\": iteration})\n\t        writer.add_scalar(\"elbo\", elbo, iteration)\n\t        writer.add_scalar(\"elbo1\", elbo1, iteration)\n\t        writer.add_scalar(\"elbo2\", elbo2, iteration)\n\t        writer.add_scalar(\"acc\", (1.0 - loss), iteration)\n", "        model.result_entry.log_metric('elbo', elbo)\n\t        model.result_entry.log_metric('acc', (1.0 - loss))\n\t        model.result_entry.log_metric('run_elbo', running_elbo)\n\t        model.result_entry.log_metric('run_acc', running_acc)\n\t    log_message(\"--------------------\")\n\t    log_message(\"Loading best model...\")\n\t    model.encoder_l1.weight = best_ps[0]\n\t    model.encoder_l2.weight = best_ps[1]\n\t    log_message(\"Best L1 weights:\", model.encoder_l1.weight)\n\t    log_message(\"Best L2 weights:\", model.encoder_l2.weight)\n", "    test_acc = test(dataset, model, loss_fn, iteration, writer)\n\t    if wandb_enabled:\n\t        wandb.log({\"test/acc\": test_acc, \"epoch\": iteration})\n\t    log_message(colored(\"DEV ACC: {}\".format(best_dev), \"green\"))\n\t    log_message(colored(\"TEST ACC: {}\".format(test_acc), \"green\"))\n\t    result_writer.save_to_json_file()\n\t    writer.close()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "projects/vi_dln/scripts/one_layer/one_layer.py", "chunked_list": ["import json\n\timport subprocess\n\timport click\n\t@click.command()\n\t@click.option(\n\t    \"--dataset\",\n\t    type=str,\n\t    help=\"Dataset name\",\n\t    required=True,\n\t)\n", "def main(dataset):\n\t    config_file=\"one-layer-dln-hp-search-result.json\"\n\t    with open(config_file) as f:\n\t        config_data = json.load(f)\n\t    config = config_data[dataset]\n\t    q_prompt_tpl = config[\"hyperparam\"][\"q_prompt_tpl\"]\n\t    tolerance = config[\"hyperparam\"][\"tolerance\"]\n\t    use_memory = config[\"hyperparam\"][\"use_memory\"]\n\t    held_out_prompt_ranking = config[\"hyperparam\"][\"held_out_prompt_ranking\"]\n\t    output_dir = f\"log/one_layer/{dataset}\"\n", "    for seed in [13, 42, 25]:\n\t        command = list(map(str, [\n\t            \"python\",\n\t            \"vi_main.py\",\n\t            \"--balance_batch\",\n\t            \"--num_p_samples\", 20,\n\t            \"--bwd_temp\", 0.7,\n\t            \"--iters\", 20,\n\t            \"--p_class\", \"classify_forward:3.0\",\n\t            \"--q_prompt\", q_prompt_tpl,\n", "            \"--out_dir\", output_dir,\n\t            \"--batch_size\", 20,\n\t            \"--seed\", seed,\n\t            \"--dataset\", dataset,\n\t            \"--tolerance\", tolerance,\n\t            \"--use_memory\", use_memory,\n\t            \"--held_out_prompt_ranking\", held_out_prompt_ranking,\n\t            \"--one_layer\",\n\t            \"--do_first_eval\",\n\t        ]))\n", "        print(' '.join(command))\n\t        subprocess.run(command)\n\tif __name__ == \"__main__\":\n\t    main()"]}
