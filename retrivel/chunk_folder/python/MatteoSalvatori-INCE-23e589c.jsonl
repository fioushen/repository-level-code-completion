{"filename": "main.py", "chunked_list": ["from src.common.constants import *\n\tfrom src.common.input_parser import *\n\tfrom src.common.loads_functions import *\n\tfrom src.datasets.dataset_factory import DatasetFactory\n\tfrom src.trainers.standard_problem_to_opt import StandardProblemToOpt\n\tdef main():\n\t    parser = get_parser_general_test_without_optimization()\n\t    args = parser.parse_args()\n\t    data_json_path = args.dataset_config\n\t    brain_trainer_json_path = args.model_config\n", "    dataset_params, brain_params = load_config_json([data_json_path, brain_trainer_json_path])\n\t    dataset_data = DatasetFactory(dataset_params=dataset_params)()\n\t    to_opt = StandardProblemToOpt(dataset_data=dataset_data,\n\t                                  brain_params=brain_params,\n\t                                  problem_type=dataset_params[PROBLEM_TYPE],\n\t                                  problem_size=dataset_params[PROBLEM_SIZE],\n\t                                  target_embedding=dataset_params[USE_TARGET_EMBEDDING])\n\t    to_opt()\n\t    print('============== END SCRIPT ==============')\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "src/common/input_parser.py", "chunked_list": ["import argparse\n\tdef get_parser_general_test_without_optimization():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('-d', '--dataset_config', type=str, required=True, help='Data set configuration file')\n\t    parser.add_argument('-m', '--model_config', type=str, required=True, help='Model parameters file')\n\t    return parser\n\tdef get_parser_general_cross_val_test_without_optimization():\n\t    parser = get_parser_general_test_without_optimization()\n\t    parser.add_argument('-o', '--optimization_config', type=str, required=True, help='Optimization configuration file')\n\t    return parser\n", "def get_parser_general_cross_val_test_with_optimization():\n\t    parser = get_parser_general_cross_val_test_without_optimization()\n\t    parser.add_argument('-r', '--ray_config', type=str, required=True, help='Model parameters file to optimize by Ray')\n\t    return parser\n"]}
{"filename": "src/common/constants.py", "chunked_list": ["# Datasets\n\tDATASET_NAME = 'dataset_name'\n\tPROBLEM_TYPE = 'problem_type'\n\tPROBLEM_SIZE = 'problem_size'\n\tUSE_TARGET_EMBEDDING = 'use_target_embedding'\n\tREGRESSION = 'regression'\n\tCLASSIFICATION = 'classification'\n\tEXPLAINABILITY = 'explainability'\n\t# Discretización de variables continuas\n\tN_QUANTILE = 'n_quantile'\n", "OUTPUT_DISTRIBUTION = 'output_distribution'\n\t# Split train/test\n\tPERCENTAGE_TEST = 'percentage_test'\n\t# Brain struct\n\tBRAIN_PARAMS = 'brain_params'\n\tCOMMON = 'common'\n\tINPUT_EMBEDDING = 'input_embedding'\n\tINPUT_GRAPH_EMBEDDING = 'input_graph_embedding'\n\tINPUT_MLP_EMBEDDING = 'input_mlp_embedding'\n\tGRAPH_EMBEDDING = 'graph_embedding'\n", "TRANSFORMER_EMBEDDING = 'transformer_embedding'\n\tDECODER = 'decoder'\n\tTRAINER_PARAMS = 'trainer_params'\n\tENABLE = 'enable'\n\tINPUT_EMBEDDING_CON = 'input_embedding_con'\n\t# Brain\n\tLATENT_SPACE_SIZE = 'latent_space_size'\n\tCLS_NUM = 'cls_num'\n\tDROPOUT = 'dropout'\n\tGNN_TYPE = 'gnn_type'\n", "GNN_NUM = 'gnn_num'\n\tIN_GNN = 'in_gnn'\n\tIN_MULTI_CHANNEL_GNN = 'in_multichannel_gnn'\n\tIN_MLP_DEEP = 'in_mlp_deep'\n\tIN_CHANNELS = 'in_channels'\n\tIN_CHANNEL_AGG = 'in_channel_agg'\n\tIN_CHANNEL_AGG_CONCAT = 'concat'\n\tIN_CHANNEL_AGG_MLP = 'mlp'\n\tIN_CHANNEL_AGG_SUM = 'sum'\n\tIN_TRANSFORMER = 'in_transformer'\n", "GAT_GNN = 'gat_gnn'\n\tGAT_HEADS = 'gat_heads'\n\tTRANSFORMER_TYPE = 'transformer_type'\n\tTRANSFORMER_NUM = 'transformer_num'\n\tTRANSFORMER_HEADS = 'transformer_heads'\n\tDECODER_DEEP = 'decoder_deep'\n\tDECODER_TYPE = 'decoder_type'\n\tTRAINABLE_PARAMS_NUM = 'trainable_params_num'\n\t# Trainer\n\tEPOCHS = 'epochs'\n", "BATCH_SIZE = 'batch_size'\n\tTRAIN_LEARNING_RATE = 'training_learning_rate'\n\tDEVICE = 'device'\n\tMETRIC = 'metric'\n\tMETRIC_LIST = 'metric_list'\n\tEPOCH = 'epoch'\n\tTRAINING_TITLE = 'TRAINING'\n\tTRAIN_TITLE = 'TRAIN'\n\tTRAIN_TIME = 'train_time'\n\tTEST_TIME = 'test_time'\n", "TEST_TITLE = 'TEST'\n\tMEAN = 'mean'\n\tSTD = 'sdt'\n\tPATH = 'path'\n"]}
{"filename": "src/common/loads_functions.py", "chunked_list": ["from typing import Dict, List\n\timport json\n\tdef load_config_json(jsons: List[str]) -> List[Dict]:\n\t    \"\"\"Load json files\n\t    :param jsons: List[str], list of json paths to load\n\t    :return: List[Dict] with the info contained in the jsons\n\t    \"\"\"\n\t    to_return = []\n\t    for path in jsons:\n\t        with open(path) as f:\n", "            to_return.append(json.load(f))\n\t    return to_return\n"]}
{"filename": "src/models/brain.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom src.models.brain_struct import *\n\tfrom src.models.building_blocks.decoder_mlp import DecoderMLP\n\tfrom src.models.building_blocks.input_graph_embedding import InputGraphEmbedding\n\tfrom src.models.building_blocks.in_encoder import INEncoder\n\tclass Brain(nn.Module):\n\t    def __init__(self,\n\t                 general_info: GeneralInfo,\n\t                 common_info: CommonInfo,\n", "                 input_graph_embedding: InputGraphEmbeddingInfo,\n\t                 encoder_gnn_info: GNNEmbeddingInfo,\n\t                 decoder_info: DecoderInfo,\n\t                 device: str):\n\t        super(Brain, self).__init__()\n\t        assert general_info.cat_num == len(general_info.cat_degrees), \\\n\t            \"cat_num has to be compatible with the len of cat_degrees\"\n\t        self.continuous_num = general_info.con_num\n\t        self.categorical_num = general_info.cat_num\n\t        # ===========================================\n", "        # ENCODER\n\t        # ===========================================\n\t        self.encoder = nn.ModuleList()\n\t        # COLUMNAR EMBEDDING\n\t        columnar_embedding = InputGraphEmbedding(con_features_num=general_info.con_num,\n\t                                                 cat_features_num=general_info.cat_num,\n\t                                                 cat_features_degrees=general_info.cat_degrees,\n\t                                                 latent_space_size=common_info.latent_space_size,\n\t                                                 cls_num=input_graph_embedding.cls_num)\n\t        self.cls_num = input_graph_embedding.cls_num\n", "        self.mlp_emb = False\n\t        print(\"\\tColumnar Embedding - Number of trainable parameters: {}\"\n\t              .format(sum(p.numel() for p in columnar_embedding.parameters() if p.requires_grad)))\n\t        self.encoder.append(columnar_embedding)\n\t        # CONTEXTUAL EMBEDDING - IN Graph ENCODER\n\t        self.in_multichannel = False\n\t        self.encoder.append(INEncoder(general_info=general_info,\n\t                                      common_info=common_info,\n\t                                      input_graph_embedding=input_graph_embedding,\n\t                                      gnn_info=encoder_gnn_info,\n", "                                      device=device))\n\t        print(\"\\tGNN Contextual Encoder - Number of trainable parameters: {}\"\n\t              .format(sum(p.numel() for p in self.encoder[-1].parameters() if p.requires_grad)))\n\t        channels_multiplier = (1 if encoder_gnn_info.in_channel_agg != IN_CHANNEL_AGG_CONCAT else\n\t                               encoder_gnn_info.in_channels)\n\t        decoder_input_size = common_info.latent_space_size * self.cls_num * channels_multiplier\n\t        decoder_sequence_length = 0\n\t        # ===========================================\n\t        # DECODER\n\t        # ===========================================\n", "        self.decoder = DecoderMLP(input_size=decoder_input_size,\n\t                                  latent_space_size=common_info.latent_space_size,\n\t                                  output_size=decoder_info.class_num,\n\t                                  layer_num=decoder_info.decoder_deep,\n\t                                  sequence_length=decoder_sequence_length)\n\t        print(\"\\tDecoder - Number of trainable parameters: {}\"\n\t              .format(sum(p.numel() for p in self.decoder.parameters() if p.requires_grad)))\n\t    def forward(self, features: torch.tensor) -> torch.tensor:\n\t        \"\"\"\n\t        Forward function\n", "        :param features: torch.tensor of shape = [batch_size, con_features_num + cat_features_num]\n\t        :return torch.tensor of shape = [batch_size, num_targets]\n\t        \"\"\"\n\t        # Encoder\n\t        for i, layer in enumerate(self.encoder):\n\t            if i == 0:\n\t                con = features[:, :self.continuous_num] if self.continuous_num > 0 else None\n\t                cat = features[:, self.continuous_num:].to(dtype=torch.long) if self.categorical_num > 0 else None\n\t                features = layer(con, cat)\n\t            else:\n", "                features = layer(features)\n\t        if not self.in_multichannel:\n\t            batch_size = features.shape[0]\n\t            x = (features[:, :self.cls_num, :].reshape(batch_size, -1)\n\t                 if ((not self.mlp_emb) and self.cls_num > 0) else features)\n\t        else:\n\t            x = features\n\t        # Decoder\n\t        return self.decoder(x)  # shape = [batch_size, num_targets]\n"]}
{"filename": "src/models/brain_struct.py", "chunked_list": ["from collections import namedtuple\n\tfrom src.common.constants import *\n\tGeneralInfo = namedtuple('GeneralInfo',\n\t                         'con_num, cat_num, cat_degrees')\n\tCommonInfo = namedtuple('CommonInfo',\n\t                        'latent_space_size, dropout')\n\tInputGraphEmbeddingInfo = namedtuple('InputGraphEmbeddingInfo',\n\t                                     'cls_num')\n\tInputMLPEmbeddingInfo = namedtuple('InputMLPEmbeddingInfo',\n\t                                   'input_embedding_con')\n", "GNNEmbeddingInfo = namedtuple('GNNEmbeddingInfo',\n\t                              'gnn_type, gnn_num, in_mlp_deep, in_channels, in_channel_agg, gat_heads')\n\tTransformerEmbeddingInfo = namedtuple('EncoderTransformerInfo',\n\t                                      'transformer_type, layer_num, heads_num')\n\tDecoderInfo = namedtuple('DecoderInfo',\n\t                         'class_num, decoder_deep')\n\tdef build_brain_structs(dataset_data, brain_params):\n\t    # Sanity check\n\t    graph_and_mlp_emb = (brain_params[INPUT_GRAPH_EMBEDDING][ENABLE] and\n\t                         brain_params[INPUT_MLP_EMBEDDING][ENABLE])\n", "    assert not graph_and_mlp_emb,\\\n\t        \"Input Graph Embedding and Input MLP Embedding cannot be simultaneously enabled\"\n\t    # Wrapper to Python struct\n\t    general_info = GeneralInfo(con_num=dataset_data.con_num,\n\t                               cat_num=dataset_data.cat_num,\n\t                               cat_degrees=dataset_data.cat_degrees)\n\t    common_info = CommonInfo(latent_space_size=brain_params[COMMON][LATENT_SPACE_SIZE],\n\t                             dropout=brain_params[COMMON][DROPOUT])\n\t    input_graph_embedding = None\n\t    if brain_params[INPUT_GRAPH_EMBEDDING][ENABLE]:\n", "        input_graph_embedding = InputGraphEmbeddingInfo(\n\t            cls_num=brain_params[INPUT_GRAPH_EMBEDDING][CLS_NUM])\n\t    input_mlp_embedding = None\n\t    if brain_params[INPUT_MLP_EMBEDDING][ENABLE]:\n\t        input_mlp_embedding = InputMLPEmbeddingInfo(\n\t            input_embedding_con=brain_params[INPUT_MLP_EMBEDDING][INPUT_EMBEDDING_CON])\n\t    encoder_gnn_info = None\n\t    if brain_params[GRAPH_EMBEDDING][ENABLE]:\n\t        encoder_gnn_info = GNNEmbeddingInfo(gnn_type=brain_params[GRAPH_EMBEDDING][GNN_TYPE],\n\t                                            gnn_num=brain_params[GRAPH_EMBEDDING][GNN_NUM],\n", "                                            in_mlp_deep=brain_params[GRAPH_EMBEDDING][IN_MLP_DEEP],\n\t                                            in_channels=brain_params[GRAPH_EMBEDDING][IN_CHANNELS],\n\t                                            in_channel_agg=brain_params[GRAPH_EMBEDDING][IN_CHANNEL_AGG],\n\t                                            gat_heads=brain_params[GRAPH_EMBEDDING][GAT_HEADS])\n\t    encoder_transformer_info = None\n\t    if brain_params[TRANSFORMER_EMBEDDING][ENABLE]:\n\t        encoder_transformer_info = TransformerEmbeddingInfo(\n\t            transformer_type=brain_params[TRANSFORMER_EMBEDDING][TRANSFORMER_TYPE],\n\t            layer_num=brain_params[TRANSFORMER_EMBEDDING][TRANSFORMER_NUM],\n\t            heads_num=brain_params[TRANSFORMER_EMBEDDING][TRANSFORMER_HEADS]\n", "        )\n\t    decoder_info = DecoderInfo(class_num=dataset_data.problem_size,\n\t                               decoder_deep=brain_params[DECODER][DECODER_DEEP])\n\t    return (general_info, common_info, input_graph_embedding, input_mlp_embedding,\n\t            encoder_gnn_info, encoder_transformer_info, decoder_info)\n"]}
{"filename": "src/models/building_blocks/mlp.py", "chunked_list": ["import math\n\timport torch\n\tclass MLP(torch.nn.Module):\n\t    def __init__(self, input_size: int, hidden_size: int, output_size: int, layers: int, layernorm: bool =True):\n\t        \"\"\" Multi-Layer perceptron\n\t        Initial Code: https://colab.research.google.com/drive/1hirUfPgLU35QCSQSZ7T2lZMyFMOaK_OF?usp=sharing\n\t        :param input_size: int,\n\t        :param hidden_size: int\n\t        :param output_size: int\n\t        :param layers: int\n", "        :param layernorm: bool\n\t        \"\"\"\n\t        super().__init__()\n\t        self.layers = torch.nn.ModuleList()\n\t        for i in range(layers):\n\t            self.layers.append(torch.nn.Linear(\n\t                input_size if i == 0 else hidden_size,\n\t                output_size if i == layers - 1 else hidden_size,\n\t            ))\n\t            if i != layers - 1:\n", "                self.layers.append(torch.nn.ReLU())\n\t        if layernorm:\n\t            self.layers.append(torch.nn.LayerNorm(output_size))\n\t        self.reset_parameters()\n\t    def reset_parameters(self):\n\t        for layer in self.layers:\n\t            if isinstance(layer, torch.nn.Linear):\n\t                layer.weight.data.normal_(0, 1 / math.sqrt(layer.in_features))\n\t                layer.bias.data.fill_(0)\n\t    def forward(self, x):\n", "        for layer in self.layers:\n\t            x = layer(x)\n\t        return x\n"]}
{"filename": "src/models/building_blocks/decoder_mlp.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass DecoderMLP(nn.Module):\n\t    def __init__(self,\n\t                 input_size: int,\n\t                 latent_space_size: int,\n\t                 output_size: int,\n\t                 layer_num: int,\n\t                 sequence_length: int):\n\t        super(DecoderMLP, self).__init__()\n", "        self.output_size = output_size\n\t        self.hiddens = nn.ModuleList()\n\t        for i in range(layer_num):\n\t            self.hiddens.append(nn.Linear(latent_space_size if i > 0 else input_size, latent_space_size))\n\t        self.output = (nn.Linear(latent_space_size, output_size) if sequence_length <= 1 else\n\t                       MultiNodeOutputLayer(output_size, latent_space_size, sequence_length))\n\t    def forward(self, x: torch.tensor) -> torch.tensor:\n\t        \"\"\"\n\t        :param x: shape = [batch_size, features_num=input_size]\n\t        :return: shape = [batch_size, output_size]\n", "        \"\"\"\n\t        for h in self.hiddens:\n\t            x = torch.nn.functional.relu(h(x))\n\t        return self.output(x)\n\t    def predict_cls(self, x: torch.tensor) -> torch.tensor:\n\t        \"\"\"\n\t        :param x: shape = [batch_size, features_num=input_size]\n\t        :return: shape = [batch_size, output_size]\n\t        \"\"\"\n\t        for h in self.hiddens:\n", "            x = torch.nn.functional.relu(h(x))\n\t        return x\n\tclass MultiNodeOutputLayer(nn.Module):\n\t    def __init__(self, output_size: int, latent_space_size: int, sequence_length: int):\n\t        super(MultiNodeOutputLayer, self).__init__()\n\t        self.outputs = nn.ModuleList()\n\t        for _ in range(sequence_length):\n\t            self.outputs.append(nn.Linear(latent_space_size, output_size))\n\t    def forward(self, x: torch.tensor) -> torch.tensor:\n\t        out = torch.stack([o(x[:, i, :]) for i, o in enumerate(self.outputs)], dim=0)  # [node_num, batch_size, num_targets]\n", "        return torch.permute(out, (1, 0, 2)).sum(dim=1)\n"]}
{"filename": "src/models/building_blocks/input_graph_embedding.py", "chunked_list": ["from typing import List\n\timport torch\n\timport torch.nn as nn\n\tclass InputGraphEmbedding(nn.Module):\n\t    def __init__(self,\n\t                 con_features_num: int,\n\t                 cat_features_num: int,\n\t                 cat_features_degrees: List[int],\n\t                 latent_space_size: int,\n\t                 cls_num: int):\n", "        super(InputGraphEmbedding, self).__init__()\n\t        self.con_features_num = con_features_num\n\t        self.cat_features_num = cat_features_num\n\t        self.cls_num = cls_num\n\t        # cls embedding as in the Transformer case\n\t        if self.cls_num > 0:\n\t            cls = torch.Tensor(self.cls_num, latent_space_size)\n\t            self.cls = nn.Parameter(nn.init.xavier_uniform_(cls))\n\t        # Continuous Features embedding\n\t        if self.con_features_num > 0:\n", "            self.con_emb = nn.ModuleList()\n\t            for _ in range(self.con_features_num):\n\t                self.con_emb.append(nn.Linear(1, latent_space_size))\n\t        # Categorical Features embedding\n\t        if self.cat_features_num > 0:\n\t            self.cat_emb = nn.ModuleList()\n\t            for size in cat_features_degrees:\n\t                self.cat_emb.append(nn.Embedding(size, latent_space_size))\n\t    def forward(self, x_con: torch.tensor, x_cat: torch.tensor) -> torch.tensor:\n\t        \"\"\"\n", "        :param x_con: shape = [batch_size, con_features_num]\n\t        :param x_cat: shape = [batch_size, cat_features_num]\n\t        :return: shape = [batch_size, features_num=con_features_num+cat_features_num, latent_space_size]\n\t        \"\"\"\n\t        batch_size = x_cat.shape[0] if x_cat is not None else x_con.shape[0]\n\t        final_list = []\n\t        if self.cls_num > 0:\n\t            final_list.append(self.cls.repeat(batch_size, 1, 1))\n\t        if self.con_features_num > 0:\n\t            con_feature_nodes = torch.permute(\n", "                torch.stack([torch.nn.functional.relu(con_emb(x_con[:, i].unsqueeze(dim=1)))\n\t                             for i, con_emb in enumerate(self.con_emb)], dim=0), (1, 0, 2))\n\t            final_list.append(con_feature_nodes)\n\t        if self.cat_features_num > 0:\n\t            cat_feature_nodes = torch.permute(\n\t                torch.stack([cat_emb(x_cat[:, i]) for i, cat_emb in enumerate(self.cat_emb)], dim=0), (1, 0, 2))\n\t            final_list.append(cat_feature_nodes)\n\t        return torch.cat(final_list, dim=1)\n"]}
{"filename": "src/models/building_blocks/in_encoder.py", "chunked_list": ["from typing import Optional\n\timport torch\n\timport torch.nn as nn\n\tfrom src.models.brain_struct import *\n\tfrom src.models.building_blocks.interaction_network import InteractionNetwork\n\tclass INEncoder(nn.Module):\n\t    def __init__(self,\n\t                 general_info: GeneralInfo,\n\t                 common_info: CommonInfo,\n\t                 input_graph_embedding: InputGraphEmbeddingInfo,\n", "                 gnn_info: GNNEmbeddingInfo,\n\t                 device: str):\n\t        super(INEncoder, self).__init__()\n\t        # Sanity checks\n\t        assert_msg = \"IN_CHANNEL_AGG={} is not allowed, only the following values are currently available: [{}, {}, {}]\"\n\t        assert gnn_info.in_channel_agg in [IN_CHANNEL_AGG_CONCAT, IN_CHANNEL_AGG_MLP, IN_CHANNEL_AGG_SUM], \\\n\t            assert_msg.format(gnn_info.in_channel_agg, IN_CHANNEL_AGG_CONCAT, IN_CHANNEL_AGG_MLP, IN_CHANNEL_AGG_SUM)\n\t        # General info\n\t        self.channels = gnn_info.in_channels\n\t        self.channel_agg = gnn_info.in_channel_agg\n", "        nodes_num = general_info.con_num + general_info.cat_num\n\t        if input_graph_embedding.cls_num > 0:\n\t            nodes_num += input_graph_embedding.cls_num\n\t        a, b = zip(*[[i, j] for i in range(nodes_num) for j in range(nodes_num) if i != j])\n\t        self.indexes = torch.tensor([list(a), list(b)]).to(device)\n\t        # Parameters\n\t        self.node_dropouts = nn.ModuleList()\n\t        self.edge_dropouts = nn.ModuleList()\n\t        self.gnns = nn.ModuleList()\n\t        self.node_norms = nn.ModuleList()\n", "        self.edge_norms = nn.ModuleList()\n\t        for i in range(gnn_info.gnn_num):\n\t            self.node_dropouts.append(nn.Dropout(p=common_info.dropout))\n\t            if i > 0:\n\t                self.edge_dropouts.append(nn.Dropout(p=common_info.dropout))\n\t            # channels, mlp_output_size, mlp_hidden_size, mlp_layer_num, with_input_channels, with_edge_features\n\t            self.gnns.append(InteractionNetwork(self.channels,\n\t                                                common_info.latent_space_size,\n\t                                                common_info.latent_space_size,\n\t                                                gnn_info.in_mlp_deep,\n", "                                                with_input_channels=(i > 0),\n\t                                                with_edge_features=(i > 0)))\n\t            self.node_norms.append(nn.LayerNorm([self.channels, nodes_num, common_info.latent_space_size]))\n\t            if i < (gnn_info.gnn_num - 1):\n\t                self.edge_norms.append(nn.LayerNorm([self.channels,\n\t                                                     nodes_num * (nodes_num - 1),\n\t                                                     common_info.latent_space_size]))\n\t        if self.channels > 1 and self.channel_agg == IN_CHANNEL_AGG_MLP:\n\t            self.channel_mlp = nn.Linear(self.channels * common_info.latent_space_size, common_info.latent_space_size)\n\t    def forward_xai(self, nodes: torch.tensor, edges: Optional[torch.tensor] = None) -> torch.tensor:\n", "        \"\"\"\n\t        :param nodes: shape = [batch_size, node_num, features_size]\n\t        :param edges: shape = [batch_size, edge num, features_size]\n\t        :return: shape = [batch_size, node_num, features_size]\n\t        \"\"\"\n\t        batch_size = nodes.shape[0]\n\t        node_num = nodes.shape[1]\n\t        # Encoder GNN\n\t        for i, gnn in enumerate(self.gnns):\n\t            nodes = self.node_dropouts[i](nodes)\n", "            if i > 0:\n\t                self.edge_dropouts[i - 1](edges)\n\t            nodes, edges = gnn(x=nodes, edge_index=self.indexes, edge_attr=edges)\n\t            nodes = self.node_norms[i](nodes.permute(1, 0, 2, 3)).permute(1, 0, 2, 3)\n\t            if i < (len(self.gnns) - 1):\n\t                edges = self.edge_norms[i](edges.permute(1, 0, 2, 3)).permute(1, 0, 2, 3)\n\t        if self.channels > 1:\n\t            if self.channel_agg == IN_CHANNEL_AGG_MLP:\n\t                nodes = nn.functional.relu(self.channel_mlp(nodes.permute(1, 2, 3, 0).reshape(batch_size,\n\t                                                                                              node_num,\n", "                                                                                              -1)))\n\t            elif self.channel_agg == IN_CHANNEL_AGG_CONCAT:\n\t                nodes = nodes.permute(1, 2, 3, 0).reshape(batch_size, node_num, -1)\n\t            else:\n\t                nodes = nodes.sum(dim=0)\n\t        else:\n\t            nodes = nodes.squeeze()\n\t        return nodes, edges\n\t    def forward(self, nodes: torch.tensor, edges: Optional[torch.tensor] = None) -> torch.tensor:\n\t        \"\"\"\n", "        :param nodes: shape = [batch_size, node_num, features_size]\n\t        :param edges: shape = [batch_size, edge num, features_size]\n\t        :return: shape = [batch_size, node_num, features_size]\n\t        \"\"\"\n\t        nodes, _ = self.forward_xai(nodes, edges)\n\t        return nodes\n"]}
{"filename": "src/models/building_blocks/interaction_network.py", "chunked_list": ["from typing import Optional\n\timport torch\n\tfrom torch_geometric import nn as pyg_nn\n\tfrom src.models.building_blocks.mlp import MLP\n\tdef broadcast(src: torch.Tensor, other: torch.Tensor, dim: int):\n\t    if dim < 0:\n\t        dim = other.dim() + dim\n\t    if src.dim() == 1:\n\t        for _ in range(0, dim):\n\t            src = src.unsqueeze(0)\n", "    for _ in range(src.dim(), other.dim()):\n\t        src = src.unsqueeze(-1)\n\t    src = src.expand(other.size())\n\t    return src\n\tdef scatter_sum(src: torch.Tensor, index: torch.Tensor, dim: int = -1,\n\t                out: Optional[torch.Tensor] = None,\n\t                dim_size: Optional[int] = None) -> torch.Tensor:\n\t    index = broadcast(index, src, dim)\n\t    if out is None:\n\t        size = list(src.size())\n", "        if dim_size is not None:\n\t            size[dim] = dim_size\n\t        elif index.numel() == 0:\n\t            size[dim] = 0\n\t        else:\n\t            size[dim] = int(index.max()) + 1\n\t        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n\t        return out.scatter_add_(dim, index, src)\n\t    else:\n\t        return out.scatter_add_(dim, index, src)\n", "class InteractionNetwork(pyg_nn.MessagePassing):\n\t    \"\"\"\n\t    Interaction Network as proposed in this paper:\n\t    https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html\n\t    Initial Code: https://colab.research.google.com/drive/1hirUfPgLU35QCSQSZ7T2lZMyFMOaK_OF?usp=sharing\n\t    \"\"\"\n\t    def __init__(self,\n\t                 channels: int,\n\t                 mlp_output_size: int,\n\t                 mlp_hidden_size: int,\n", "                 mlp_layer_num: int,\n\t                 with_input_channels: bool,\n\t                 with_edge_features: bool):\n\t        super().__init__()\n\t        self.channels = channels\n\t        self.with_input_channels = with_input_channels\n\t        self.with_edge_features = with_edge_features\n\t        self.lin_edge = torch.nn.ModuleList()\n\t        self.lin_node = torch.nn.ModuleList()\n\t        multiplier = 3 if with_edge_features else 2\n", "        for ch in range(self.channels):\n\t            self.lin_edge.append(MLP(mlp_hidden_size * multiplier, mlp_hidden_size, mlp_output_size, mlp_layer_num))\n\t            self.lin_node.append(MLP(mlp_hidden_size * 2, mlp_hidden_size, mlp_output_size, mlp_layer_num))\n\t    def forward(self, x, edge_index, edge_attr):\n\t        if self.with_input_channels:\n\t            assert len(x.shape) == 4\n\t            assert x.shape[0] == self.channels\n\t            if edge_attr is not None:\n\t                assert edge_attr.shape[0] == self.channels\n\t        else:\n", "            x = x.repeat(self.channels, 1, 1, 1)\n\t            if edge_attr is not None:\n\t                edge_attr = edge_attr.repeat(self.channels, 1, 1, 1)\n\t        edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_attr)\n\t        node_out = torch.cat((x, aggr), dim=-1)\n\t        node_out = torch.stack([l(node_out[ch, :, :, :])\n\t                                for ch, l in enumerate(self.lin_node)], dim=0)\n\t        if self.with_edge_features:\n\t            edge_out = edge_attr + edge_out\n\t        node_out = x + node_out\n", "        return node_out, edge_out\n\t    def message(self, x_i, x_j, edge_feature):\n\t        if self.with_edge_features:\n\t            x = torch.cat((x_i, x_j, edge_feature), dim=-1)\n\t        else:\n\t            x = torch.cat((x_i, x_j), dim=-1)\n\t        x = torch.stack([l(x[ch, :, :, :])\n\t                        for ch, l in enumerate(self.lin_edge)], dim=0)\n\t        return x\n\t    def aggregate(self, inputs, index, dim_size=None):\n", "        # out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\")\n\t        out = scatter_sum(inputs, index, dim=self.node_dim, dim_size=dim_size)\n\t        return (inputs, out)\n"]}
{"filename": "src/trainers/standard_problem_to_opt.py", "chunked_list": ["import torch.optim as optim\n\tfrom src.models.brain import Brain\n\tfrom src.models.brain_struct import *\n\tfrom src.trainers.trainer_factory import TrainerFactory\n\tclass StandardProblemToOpt(object):\n\t    def __init__(self, dataset_data, brain_params, problem_type: str, problem_size: int,\n\t                 target_embedding: bool):\n\t        self.dataset_data = dataset_data\n\t        self.brain_params = brain_params\n\t        self.problem_type = problem_type\n", "        self.problem_size = problem_size\n\t        self.target_embedding = target_embedding\n\t        self.weights = None\n\t        if problem_type == CLASSIFICATION:\n\t            self.weights = dataset_data.weights\n\t    def __call__(self, *args, **kwargs):\n\t        # Model\n\t        print('Building brain...')\n\t        (general_info, common_info, input_graph_embedding, _,\n\t         encoder_gnn_info, _, decoder_info) = build_brain_structs(self.dataset_data, self.brain_params)\n", "        # Brain\n\t        brain = Brain(general_info=general_info,\n\t                      common_info=common_info,\n\t                      input_graph_embedding=input_graph_embedding,\n\t                      encoder_gnn_info=encoder_gnn_info,\n\t                      decoder_info=decoder_info,\n\t                      device=self.brain_params[TRAINER_PARAMS][DEVICE]).to(self.brain_params[TRAINER_PARAMS][DEVICE])\n\t        print(brain)\n\t        print(\"Number of trainable parameters: {}\"\n\t              .format(sum(p.numel() for p in brain.parameters() if p.requires_grad)))\n", "        optimizer = optim.Adam(brain.parameters(), self.brain_params[TRAINER_PARAMS][TRAIN_LEARNING_RATE])\n\t        # Trainer\n\t        print('Building trainer...')\n\t        trainer_type = (TrainerFactory.TRAINER_WITH_TARGET_EMBEDDING if self.target_embedding else\n\t                        TrainerFactory.TRAINER_WITHOUT_TARGET_EMBEDDING)\n\t        weights = self.weights if self.weights is None else self.weights.to(self.brain_params[TRAINER_PARAMS][DEVICE])\n\t        checkpoint_path = self.brain_params[TRAINER_PARAMS][PATH]\n\t        trainer = TrainerFactory.get_trainer(trainer_type=trainer_type,\n\t                                             problem_type=self.problem_type,\n\t                                             weights=weights,\n", "                                             checkpoint_path=checkpoint_path)\n\t        # Train-Test\n\t        print('Start training...')\n\t        return trainer.train_model(brain=brain,\n\t                                   optimizer=optimizer,\n\t                                   train_data=[self.dataset_data.x_train, self.dataset_data.y_train],\n\t                                   test_data=[self.dataset_data.x_test, self.dataset_data.y_test],\n\t                                   epochs=self.brain_params[TRAINER_PARAMS][EPOCHS],\n\t                                   batch_size=self.brain_params[TRAINER_PARAMS][BATCH_SIZE],\n\t                                   device=self.brain_params[TRAINER_PARAMS][DEVICE])\n"]}
{"filename": "src/trainers/trainer_te.py", "chunked_list": ["from typing import Optional\n\timport torch\n\tfrom src.trainers.trainer import Trainer\n\tclass TrainerTE(Trainer):\n\t    def __init__(self,\n\t                 problem_type: str,\n\t                 weights: Optional[torch.tensor] = None,\n\t                 checkpoint_path: Optional[str] = None):\n\t        \"\"\" rainer class with Target Embedding.\n\t        NOTE: using Target Embedding means that edge features are used\n", "        :param brain: nn.Module, the brain to train/test\n\t        :param optimizer: the optimizer to use\n\t        :param problem_type: str, the problem to solve (REGRESSION or CLASSIFICATION)\n\t        :param weights: torch.tensor, in the CLASSIFICATION case, the weight of each class. Optional\n\t        \"\"\"\n\t        super(TrainerTE, self).__init__(problem_type, weights, checkpoint_path)\n\t    def get_train_batch(self, data, epoch_shuffle_idx, ini, fin, device):\n\t        if self.problem_type == Trainer.REGRESSION:\n\t            return ([data[0][epoch_shuffle_idx[ini:fin], :].to(device),   # shape: [batch_size, features_num]\n\t                     data[1][epoch_shuffle_idx[ini:fin], :].to(device),   # shape: [batch_size, node_num, node_feature_size]\n", "                     data[1][epoch_shuffle_idx[ini:fin], :].to(device)],  # shape: [batch_size, edge_num, edge_feature_size]\n\t                    data[1][epoch_shuffle_idx[ini:fin], :].squeeze().to(device))    # shape: [batch_size, target_num]\n\t        else:\n\t            return ([data[0][epoch_shuffle_idx[ini:fin], :].to(device),   # shape: [batch_size, features_num]\n\t                     data[1][epoch_shuffle_idx[ini:fin], :].to(device),   # shape: [batch_size, node_num, node_feature_size]\n\t                     data[1][epoch_shuffle_idx[ini:fin], :].to(device)],  # shape: [batch_size, edge_num, edge_feature_size]\n\t                    data[1][epoch_shuffle_idx[ini:fin], :].to(device))    # shape: [batch_size, target_num]\n\t    def get_test_batch(self, data, ini, fin, device):\n\t        if self.problem_type == Trainer.REGRESSION:\n\t            return ([data[0][ini:fin, :].to(device),   # shape: [batch_size, features_num]\n", "                     data[1][ini:fin, :].to(device),   # shape: [batch_size, node_num, node_feature_size]\n\t                     data[1][ini:fin, :].to(device)],  # shape: [batch_size, edge_num, edge_feature_size]\n\t                    data[1][ini:fin, :].squeeze().to(device))    # shape: [batch_size, target_num]\n\t        else:\n\t            return ([data[0][ini:fin, :].to(device),  # shape: [batch_size, features_num]\n\t                     data[1][ini:fin, :].to(device),  # shape: [batch_size, node_num, node_feature_size]\n\t                     data[1][ini:fin, :].to(device)],  # shape: [batch_size, edge_num, edge_feature_size]\n\t                    data[1][ini:fin, :].to(device))  # shape: [batch_size, target_num]\n"]}
{"filename": "src/trainers/trainer_factory.py", "chunked_list": ["from typing import Optional\n\timport torch\n\tfrom src.trainers.trainer import Trainer\n\tfrom src.trainers.trainer_te import TrainerTE\n\tfrom src.trainers.trainer_wo_te import TrainerWOTE\n\tclass TrainerFactory(object):\n\t    TRAINER_WITH_TARGET_EMBEDDING = 'trainer_with_target_embedding'\n\t    TRAINER_WITHOUT_TARGET_EMBEDDING = 'trainer_without_target_embedding'\n\t    @staticmethod\n\t    def get_trainer(trainer_type: str,\n", "                    problem_type: str,\n\t                    weights: Optional[torch.tensor] = None,\n\t                    checkpoint_path: Optional[str] = None) -> Trainer:\n\t        if trainer_type == TrainerFactory.TRAINER_WITH_TARGET_EMBEDDING:\n\t            trainer = TrainerTE(problem_type, weights, checkpoint_path)\n\t        elif trainer_type == TrainerFactory.TRAINER_WITHOUT_TARGET_EMBEDDING:\n\t            trainer = TrainerWOTE(problem_type, weights, checkpoint_path)\n\t        else:\n\t            print('{} trainer is not implemented!'.format(trainer_type))\n\t            trainer = None\n", "        return trainer\n"]}
{"filename": "src/trainers/trainer.py", "chunked_list": ["import random\n\timport time\n\tfrom typing import Callable, List, Optional, Tuple\n\tfrom sklearn.metrics import accuracy_score\n\tfrom tqdm import tqdm\n\timport torch\n\timport torch.nn as nn\n\tfrom src.common.constants import *\n\tclass Trainer(nn.Module):\n\t    def __init__(self,\n", "                 problem_type: str,\n\t                 weights: Optional[torch.tensor] = None,\n\t                 checkpoint_path: str = None,\n\t                 logger_func: Optional[Callable] = print,\n\t                 test_on_train_dataset: bool = False):\n\t        \"\"\" Trainer class\n\t        :param problem_type: str, the problem to solve (REGRESSION or CLASSIFICATION)\n\t        :param weights: torch.tensor, in the CLASSIFICATION case, the weight of each class. Optional\n\t        \"\"\"\n\t        super(Trainer, self).__init__()\n", "        self.problem_type = problem_type\n\t        if problem_type == REGRESSION:\n\t            self.criterion = self.custom_mse\n\t            self.test_criterion = self.custom_mse\n\t            self.str_test = 'MSE'\n\t        elif problem_type == CLASSIFICATION:\n\t            self.criterion = torch.nn.CrossEntropyLoss(weight=weights)\n\t            self.test_criterion = self.custom_accuracy\n\t            self.str_test = 'Accuracy'\n\t        self.logger_func = logger_func\n", "        self.test_on_train_dataset = test_on_train_dataset\n\t        self.checkpoint_path = checkpoint_path\n\t    def train_model(self,\n\t                    brain: nn.Module,\n\t                    optimizer,\n\t                    train_data: List[torch.tensor],\n\t                    test_data: List[torch.tensor],\n\t                    epochs: int,\n\t                    batch_size: int,\n\t                    device: str,\n", "                    extra_text: Optional[str]=None):\n\t        \"\"\" Main flux of train/test\n\t        :param brain: Model to train\n\t        :param optimizer: Optimizer to use\n\t        :param train_data: List of tensors to use for training\n\t        :param test_data: List of tensors to use for testing\n\t        :param epochs: int, number of epochs\n\t        :param batch_size: int, batch size\n\t        :param device: str, device to use\n\t        :param extra_text: str\n", "        :return:\n\t        \"\"\"\n\t        start_train = time.time()\n\t        # Preparamos el array de indices sobre el que nos apoyaremos para hacer shuffle en cada época\n\t        tot = train_data[0].shape[0]\n\t        tot_test = test_data[0].shape[0]\n\t        epoch_shuffle_idx = [i for i in range(tot)]\n\t        # For sobre las epocas\n\t        best_res = float('inf') if self.problem_type == REGRESSION else -float('inf')\n\t        best_res_str = ''\n", "        epoch_metrics = []\n\t        epoch_train_time = []\n\t        epoch_test_time = []\n\t        for epoch in range(epochs):\n\t            new_extra_text = (extra_text + \"EPOCH: {}/{} - \".format(epoch + 1, epochs) if extra_text is not None else\n\t                              \"EPOCH: {}/{} - \".format(epoch + 1, epochs))\n\t            start_train_epoch_time = time.time()\n\t            train_str = self.epoch_train(brain, optimizer, train_data, tot, epoch_shuffle_idx, batch_size, device, new_extra_text)\n\t            epoch_train_time.append(time.time() - start_train_epoch_time)\n\t            if self.test_on_train_dataset:\n", "                _, new_extra_text = self.epoch_test(brain, train_data, tot, batch_size, device, TRAIN_TITLE, new_extra_text)\n\t            start_test_epoch_time = time.time()\n\t            epoch_res, test_str = self.epoch_test(brain, test_data, tot_test, batch_size, device, TEST_TITLE, new_extra_text)\n\t            epoch_test_time.append(time.time() - start_test_epoch_time)\n\t            epoch_metrics.append(epoch_res)\n\t            if (((self.problem_type == REGRESSION) and (epoch_res.item() < best_res)) or\n\t                ((self.problem_type == CLASSIFICATION) and (epoch_res.item() > best_res))):\n\t                best_res = epoch_res.item()\n\t                best_res_str = ' - BEST: {:.4f}'.format(best_res)\n\t                if self.checkpoint_path is not None:\n", "                    torch.save({\n\t                        'epoch': epoch,\n\t                        'model_state_dict': brain.state_dict(),\n\t                        'optimizer_state_dict': optimizer.state_dict(),\n\t                        'loss': best_res\n\t                    }, self.checkpoint_path)\n\t            print(new_extra_text + train_str + test_str + best_res_str)\n\t        if self.problem_type == REGRESSION:\n\t            best_res, best_epoch = torch.tensor(epoch_metrics).min().item(), torch.argmin(torch.tensor(epoch_metrics)).item()\n\t        else:\n", "            best_res, best_epoch = torch.tensor(epoch_metrics).max().item(), torch.argmax(torch.tensor(epoch_metrics)).item()\n\t        self.logger_func('Best result: {} - epoch: {} - time: {:.1f}'.format(best_res, best_epoch + 1, time.time() - start_train))\n\t        return {METRIC: best_res,\n\t                EPOCH: best_epoch + 1,\n\t                TRAIN_TIME: torch.tensor(epoch_train_time).mean().item(),\n\t                TEST_TIME: torch.tensor(epoch_test_time).mean().item()}\n\t    def epoch_train(self,\n\t                    brain: nn.Module,\n\t                    optimizer,\n\t                    data: List[torch.tensor],\n", "                    dataset_size: int,\n\t                    epoch_shuffle_idx: List[int],\n\t                    batch_size: int,\n\t                    device: str,\n\t                    extra_text: Optional[str]=None) -> str:\n\t        \"\"\" Epoch train\n\t        :param brain: Model to train\n\t        :param optimizer: Optimizer to use\n\t        :param data: List of torch.tensor to use for training\n\t        :param dataset_size: int, size of the training dataset\n", "        :param epoch_shuffle_idx: list of indexes (used to shuffle the dataset in each epoch)\n\t        :param batch_size: int\n\t        :param device: str\n\t        :param extra_text: str\n\t        :return: str, the trace of training\n\t        \"\"\"\n\t        start_epoch = time.time()\n\t        # brain -> train mode\n\t        brain.train()\n\t        # Epoch shuffle\n", "        random.shuffle(epoch_shuffle_idx)\n\t        # For over the batches of the current epoch\n\t        ini = 0\n\t        fin = ini + batch_size\n\t        train_loss_list = []\n\t        title = TRAINING_TITLE if extra_text is None else extra_text + TRAINING_TITLE\n\t        progress_bool = True # TODO: a parámetro!\n\t        pbar = None\n\t        if progress_bool:\n\t            pbar = tqdm(total=int(dataset_size / batch_size) + 1, desc=title)\n", "        while True:\n\t            # Batch data preprocessing\n\t            forward_input, targets = self.get_train_batch(data=data,\n\t                                                          epoch_shuffle_idx=epoch_shuffle_idx,\n\t                                                          ini=ini,\n\t                                                          fin=fin,\n\t                                                          device=device)\n\t            # Optimization\n\t            optimizer.zero_grad()\n\t            predictions: torch.tensor = brain.forward(*forward_input)  # Shape: [batch_size, class_num]\n", "            total_loss_function = self.criterion(predictions, targets.squeeze().detach())\n\t            total_loss_function.backward()\n\t            torch.nn.utils.clip_grad_value_(self.parameters(), 1.0)\n\t            optimizer.step()\n\t            # Trace\n\t            train_loss_list.append(total_loss_function.detach().item())\n\t            if progress_bool:\n\t                pbar.update(1)\n\t            ini = fin\n\t            fin = ini + batch_size\n", "            if fin > dataset_size:\n\t                break\n\t            del total_loss_function\n\t            del predictions\n\t            del targets\n\t            del forward_input\n\t            if device == 'cuda':\n\t                torch.cuda.empty_cache()\n\t        trace_txt = 'Train Loss: {:.10f} - Time: {:.4f} - '.format(torch.tensor(train_loss_list).mean(),\n\t                                                                time.time() - start_epoch)\n", "        del train_loss_list\n\t        return trace_txt\n\t    def epoch_test(self,\n\t                   brain: nn.Module,\n\t                   epoch_data_test: List[torch.tensor],\n\t                   dataset_size: int,\n\t                   batch_size: int,\n\t                   device: str,\n\t                   test_type: str,\n\t                   extra_text: Optional[str]=None) -> Tuple[float, str]:\n", "        \"\"\" Epoch test\n\t        :param brain: nn.Module to test\n\t        :param epoch_data_test: List of torch.tensor to use for testing\n\t        :param dataset_size: int, size of the testing dataset\n\t        :param batch_size: int\n\t        :param device: str\n\t        :param test_type: str to use in the logging\n\t        :param extra_text: str\n\t        :return: Tuple[float->metric, str->trace of test results]\n\t        \"\"\"\n", "        start_test = time.time()\n\t        # brain -> eval mode\n\t        brain.eval()\n\t        # For over the batches of the current epoch\n\t        pre_list = []\n\t        true_list = []\n\t        title = test_type if extra_text is None else extra_text + test_type\n\t        progress_bool = False  # TODO: a parámetro!\n\t        pbar =None\n\t        if progress_bool:\n", "            pbar = tqdm(total=int(dataset_size / batch_size) + 1, desc=title)\n\t        with torch.no_grad():\n\t            ini = 0\n\t            fin = ini + batch_size\n\t            while True:\n\t                # Batch data preprocessing\n\t                forward_input, targets = self.get_test_batch(data=epoch_data_test,\n\t                                                             ini=ini,\n\t                                                             fin=fin,\n\t                                                             device=device)\n", "                # Test\n\t                predictions: torch.tensor = brain.forward(*forward_input)\n\t                pre_list = pre_list + predictions.detach().to('cpu').numpy().tolist()\n\t                true_list = true_list + targets.detach().to('cpu').numpy().tolist()\n\t                if progress_bool:\n\t                    pbar.update(1)\n\t                ini = fin\n\t                fin = min(ini + batch_size, dataset_size)\n\t                if ini == dataset_size:\n\t                    break\n", "                del predictions\n\t                del targets\n\t                del forward_input\n\t                if device == 'cuda':\n\t                    torch.cuda.empty_cache()\n\t        test_metric = self.test_criterion(torch.tensor(pre_list), torch.tensor(true_list))\n\t        trace_txt = '{} {}: {:.10f} - Time: {:.4f}'.format(test_type, self.str_test, test_metric,\n\t                                                           time.time() - start_test)\n\t        del pre_list, true_list\n\t        return test_metric, trace_txt\n", "    def get_train_batch(self, data, epoch_shuffle_idx, ini, fin, device):\n\t        raise NotImplementedError\n\t    def get_test_batch(self, data, ini, fin, device):\n\t        raise NotImplementedError\n\t    @staticmethod\n\t    def custom_accuracy(predictions: torch.tensor, targets: torch.tensor) -> torch.tensor:\n\t        \"\"\" Accuracy score\n\t        :param predictions: torch.tensor, shape = [batch_size, class_num]\n\t        :param targets: torch.tensor, shape = [batch_size, class_num]\n\t        :return: float\n", "        \"\"\"\n\t        preds = torch.argmax(predictions, dim=1).cpu()\n\t        trues = targets.squeeze().cpu()\n\t        return torch.tensor(accuracy_score(trues, preds))\n\t    @staticmethod\n\t    def custom_mse(predictions: torch.tensor, targets: torch.tensor) -> torch.tensor:\n\t        loss = torch.nn.MSELoss()\n\t        return loss(predictions.squeeze(), targets.squeeze())          \n"]}
{"filename": "src/trainers/trainer_wo_te.py", "chunked_list": ["from typing import Optional\n\timport torch\n\tfrom src.trainers.trainer import Trainer\n\tclass TrainerWOTE(Trainer):\n\t    def __init__(self,\n\t                 problem_type: str,\n\t                 weights: Optional[torch.tensor] = None,\n\t                 checkpoint_path: Optional[str] = None\n\t                 ):\n\t        \"\"\" Trainer class without Target Embedding.\n", "        NOTE: without Target Embedding, there is no edge features\n\t        :param problem_type: str, the problem to solve (REGRESSION or CLASSIFICATION)\n\t        :param weights: torch.tensor, in the CLASSIFICATION case, the weight of each class. Optional\n\t        \"\"\"\n\t        super(TrainerWOTE, self).__init__(problem_type, weights, checkpoint_path)\n\t    def get_train_batch(self, data, epoch_shuffle_idx, ini, fin, device):\n\t        return ([data[0][epoch_shuffle_idx[ini:fin], :].to(device)],  # shape: [batch_size, features_num]\n\t                data[1][epoch_shuffle_idx[ini:fin], :].to(device))    # shape: [batch_size, target_num]\n\t    def get_test_batch(self, data, ini, fin, device):\n\t        return ([data[0][ini:fin, :].to(device)],  # shape: [batch_size, features_num]\n", "                data[1][ini:fin, :].to(device))    # shape: [batch_size, target_num]\n"]}
{"filename": "src/datasets/experiment_dataset.py", "chunked_list": ["from collections import namedtuple\n\tExperimentData = namedtuple('ExperimentData',\n\t                            'x_train, y_train, x_test, y_test, con_num, cat_num, cat_degrees, problem_type, problem_size, weights, aux')\n\tclass ExperimentDataset(object):\n\t    def get_data_without_target_embedding(self, params) -> ExperimentData:\n\t        raise NotImplementedError\n\t    def get_data_with_target_embedding(self, params) -> ExperimentData:\n\t        raise NotImplementedError\n"]}
{"filename": "src/datasets/dataset_factory.py", "chunked_list": ["from typing import Dict\n\tfrom src.common.constants import *\n\tfrom src.datasets.etl_datasets.california_housing import CaliforniaHousing\n\tclass DatasetFactory(object):\n\t    def __init__(self, dataset_params: Dict):\n\t        self.dataset_params = dataset_params\n\t    def __call__(self, *args, **kwargs):\n\t        if self.dataset_params[DATASET_NAME] == 'California_Housing':\n\t            return CaliforniaHousing().get_data_without_target_embedding(self.dataset_params)\n"]}
{"filename": "src/datasets/etl_utils.py", "chunked_list": ["from typing import Dict, List, Optional, Tuple\n\timport numpy as np\n\timport pandas as pd\n\tfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\n\tdef preprocess_continous_features(cont_feature_names: List[str],\n\t                                  cat_features_name: List[str],\n\t                                  n_quantiles: int,\n\t                                  output_distribution: str,\n\t                                  df_train: pd.DataFrame,\n\t                                  df_test: Optional[pd.DataFrame]=None,\n", "                                  discretize: Optional[bool]=True) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n\t    \"\"\" Continuous features preprocessing\n\t    :param cont_feature_names: List of str, the names of the continuous features\n\t    :param cat_features_name: List of str, the names of the categorical features\n\t    :param n_quantiles: int\n\t    :param output_distribution: str, 'normal' or 'uniform' -> output distribution of  QuantileTransformer\n\t    :param df_train: pd.DataFrame to discretize\n\t    :param df_test: pd.DataFrame to discretize\n\t    :param discretize: bool, if True the continuous fatures are discretized\n\t    :return: Tuple[pd.DataFrame, pd.DataFrame], the new version of df_train and df_test\n", "    \"\"\"\n\t    # Change in the distribution\n\t    if n_quantiles > 0:\n\t        tr = QuantileTransformer(n_quantiles=n_quantiles, random_state=0, output_distribution=output_distribution)\n\t    else:\n\t        tr = StandardScaler()\n\t    tr.fit(df_train[cont_feature_names])\n\t    cont_train = tr.transform(df_train[cont_feature_names])\n\t    if df_test is not None:\n\t        cont_test = tr.transform(df_test[cont_feature_names])\n", "    if len(cat_features_name) > 0:\n\t        new_train = np.concatenate((cont_train, df_train[cat_features_name].values), axis=1).tolist()\n\t        if df_test is not None:\n\t            new_test = np.concatenate((cont_test, df_test[cat_features_name].values), axis=1).tolist()\n\t        features_names = cont_feature_names + cat_features_name\n\t    else:\n\t        new_train = cont_train.tolist()\n\t        if df_test is not None:\n\t            new_test = cont_test.tolist()\n\t        features_names = cont_feature_names\n", "    x_train = pd.DataFrame(new_train, columns=features_names)\n\t    if df_test is not None:\n\t        x_test = pd.DataFrame(new_test, columns=features_names)\n\t    # One value for each quantile\n\t    if discretize:\n\t        for c in cont_feature_names:\n\t            c_min = x_train[c].values.min()\n\t            c_max = x_train[c].values.max()\n\t            cuant = (c_max - c_min) / n_quantiles\n\t            for i in range(n_quantiles):\n", "                for df in ([x_train, x_test] if df_test is not None else [x_train]):\n\t                    df[c][(df[c] >= c_min + i * cuant) & (df[c] <= c_min + (i + 1) * cuant)] = round(c_min + i * cuant, 2)\n\t    if df_test is not None:\n\t        return x_train, x_test\n\t    else:\n\t        return x_train, None\n"]}
{"filename": "src/datasets/etl_datasets/california_housing.py", "chunked_list": ["from sklearn.datasets import fetch_california_housing\n\tfrom sklearn.model_selection import train_test_split\n\timport torch\n\tfrom src.common.constants import *\n\tfrom src.datasets.experiment_dataset import ExperimentData, ExperimentDataset\n\tfrom src.datasets.etl_utils import *\n\tclass CaliforniaHousing(ExperimentDataset):\n\t    def get_data_without_target_embedding(self, params: Dict) -> ExperimentData:\n\t        # Load data\n\t        print('Loading CALIFORNIA HOUSING data...')\n", "        data = fetch_california_housing(as_frame=True)\n\t        df = data['frame']\n\t        target_cols = ['MedHouseVal']\n\t        cont_cols = [c for c in df.columns if c not in target_cols]\n\t        cat_cols = []\n\t        print(df.columns)\n\t        print('Preprocessing continuous features...')\n\t        targets_df = df[target_cols]\n\t        df, _ = preprocess_continous_features(cont_feature_names=cont_cols,\n\t                                              cat_features_name=cat_cols,\n", "                                              n_quantiles=params[N_QUANTILE],\n\t                                              output_distribution=params[OUTPUT_DISTRIBUTION],\n\t                                              df_train=df.drop(target_cols, axis=1),\n\t                                              df_test=None,\n\t                                              discretize=False)\n\t        df = pd.DataFrame(data=np.concatenate([df.values, targets_df.values], axis=1),\n\t                          columns=list(df.columns) + target_cols)\n\t        # Split train-test\n\t        print('Splitting train/test data...')\n\t        data_train, data_test = train_test_split(df, test_size=params[PERCENTAGE_TEST], random_state=0)\n", "        # Features-target split\n\t        X_train, y_train = data_train.drop(target_cols, axis=1), data_train[target_cols]\n\t        X_test, y_test = data_test.drop(target_cols, axis=1), data_test[target_cols]\n\t        # Data-->Torch\n\t        print('Summary data:')\n\t        torch_x_train = torch.tensor(X_train[list(X_train.columns)].values, dtype=torch.float)\n\t        torch_y_train = torch.tensor(y_train[target_cols].values, dtype=torch.float)\n\t        print('\\tTrain Features: {}'.format(torch_x_train.shape))\n\t        print('\\tTrain Targets: {}'.format(torch_y_train.shape))\n\t        torch_x_test = torch.tensor(X_test[list(X_train.columns)].values, dtype=torch.float)\n", "        torch_y_test = torch.tensor(y_test[target_cols].values, dtype=torch.float)\n\t        print('\\tTest Features: {}'.format(torch_x_test.shape))\n\t        print('\\tTest Targets: {}'.format(torch_y_test.shape))\n\t        data = ExperimentData(x_train=torch_x_train,\n\t                              y_train=torch_y_train,\n\t                              x_test=torch_x_test,\n\t                              y_test=torch_y_test,\n\t                              con_num=len(cont_cols),\n\t                              cat_num=len(cat_cols),\n\t                              cat_degrees=[],\n", "                              problem_type=params[PROBLEM_TYPE],\n\t                              problem_size=params[PROBLEM_SIZE],\n\t                              weights=None,\n\t                              aux=None)\n\t        return data\n\t    def get_data_with_target_embedding(self, params: Dict) -> ExperimentData:\n\t        return None, None, None, None\n"]}
