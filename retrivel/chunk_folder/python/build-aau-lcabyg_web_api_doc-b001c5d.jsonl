{"filename": "example.py", "chunked_list": ["import os\n\tfrom lcabyg_web_api_py import *\n\t\"\"\"\n\tThis example uses the libraries lcabyg_web_api_py and sbi_web_api.py connecting, logging, and sending/receiving job into the LCAbyg Web API. \n\tSee raw_api.py in the package / folder sbi_web_api.py for source code.\n\tGet started by creating a user account at https://api.lcabyg.dk/da/\n\tPlace the user name and api key in the fields \"USERNAME\" and \"API_KEY\" located in the function example_send_job().\n\t\"\"\"\n\t# Turn this on if not using environment variables\n\tUSERNAME = 'INSERT YOUR USERNAME'\n", "API_KEY = 'INSERT YOUR API KEY'\n\t# Turn this on if using environment variables\n\t#USERNAME = os.environ.get('INSERT YOUR USERNAME')\n\t#API_KEY = os.environ.get('INSERT YOUR API API_KEY')\n\tif not USERNAME or not API_KEY:\n\t    print('Please provide a USERNAME, API_KEY')\n\t    exit(1)\n\tcli = Client(USERNAME, API_KEY)\n\tfor project_path in ['testdata/lcabyg/empty/', 'testdata/lcabyg/reno/', 'testdata/lcabyg/single/']:\n\t    data = NewJob(project=collect_json([project_path]))\n", "    job, output = cli.submit_job(data)\n\t    print(f'{project_path}: {job.status}')\n\t    print(output)\n\t    print()"]}
{"filename": "example_explanation.py", "chunked_list": ["import os\n\timport json\n\timport sys\n\tfrom uuid import UUID\n\tfrom typing import Optional, Dict, Tuple, Any, List\n\timport requests\n\timport base64\n\timport time\n\tSERVER_URL = 'https://api1.lcabyg.dk'\n\tUSERNAME = 'INSERT YOUR USERNAME'\n", "API_KEY = 'INSERT YOUR API KEY'\n\tTARGET = 'lcabyg5+br23'\n\tOUTPUT_PATH = 'results.json'\n\t# EXAMPLE sending a job containing multiple projects:\n\tPROJECT_PATHS = ['testdata/lcabyg/single/', 'testdata/lcabyg/reno', 'testdata/lcabyg/empty']\n\t\"\"\" ABOUT example_explanation.py:\n\tThis example file gives a detailed run-through of the connecting, logging, and sending/receiving job into the API WITHOUT the packages sbi_web_api_py and lcabyg_web_api_py. \n\tGet started by creating a user account at https://api.lcabyg.dk/da/\n\tPlace the user name and api key in the fields \"USERNAME\" and \"API_KEY\" located in the function example_send_job().\n\tThe function example_send_job() shows examples of each step of interacting with the API. \n", "The function example_debug_job_data provides an example for debugging errors in the job_data sent to the API. We always recommend reading the LCAbyg JSON Guide prior to working with LCAbyg JSON.  \n\tNOTICE: the API_KEY is not the same as the TOKEN.\n\tNOTICE: The functions get(), post(), put(), and delete() should NOT be modified. They are the essential functions for talking to the API.\n\tNOTICE: It is important to indicate when you have finished talking to the API. The queue limit will fill if you have yet to cancel your calls. See the function mark_job_as_finished. \n\tNOTICE: When the job is marked as finished, your job will be removed from the system sooner or later. It can only be done one time per job.\n\t\"\"\"\n\tclass AuthTokenExpired(Exception):\n\t    def __init__(self, res: requests.Response):\n\t        self.response = res\n\tdef login_via_headers(server_url: str, username: str, password: str) -> str:\n", "    \"\"\"\n\t    Use TOKEN for all calls where security is involved.\n\t    Create a user account at https://api.lcabyg.dk/da/\n\t    NOTICE: Login can be done using other methods. Read more in the section \"Login\", in api.md.\n\t    :param server_url:\n\t    :param username:\n\t    :param password:\n\t    :return: TOKEN\n\t    \"\"\"\n\t    res = post(f'{server_url}/v2/login', auth=(username, password))\n", "    UUID(res)\n\t    # In this case, res should be a string and not UUID\n\t    return res\n\tdef ping(server_url: str) -> str:\n\t    \"\"\"\n\t    Check server connection WITHOUT login.\n\t    If connection, the response is 'pong'.\n\t    :param server_url:\n\t    :return: 'pong'\n\t    \"\"\"\n", "    return get(f'{server_url}/v2/ping')\n\tdef ping_secure(server_url: str, auth_token: str) -> str:\n\t    \"\"\"\n\t    Check server connection WITH login.\n\t    If connection, the response is 'pong_secure'.\n\t    :param server_url:\n\t    :param auth_token: Generated from login_via_headers(SERVER_URL, USERNAME, API_KEY)\n\t    :return: 'pong_secure'\n\t    \"\"\"\n\t    return get(f'{server_url}/v2/ping_secure', auth_token=auth_token)\n", "def collect_json(json_paths: List[str]) -> List[Dict[str, Any]]:\n\t    \"\"\"\n\t    The API requires the job_data to contain one json file per project.\n\t    Secrets.json is immutable Gen_DK data.\n\t    :param json_paths: One path per json project\n\t    :return: One combined json file for the json project of the given project path\n\t    \"\"\"\n\t    assert isinstance(json_paths, list)\n\t    collected_projects = list()\n\t    # Project paths is a list of project paths. For each path:\n", "    for start_path in json_paths:\n\t        # Makes sure the base path is the user's home directory\n\t        start_path = os.path.expanduser(start_path)\n\t        # If the path is an existing directory\n\t        if os.path.isdir(start_path):\n\t            # Goes through everything in the directory of the project path\n\t            for root, directories, files in os.walk(start_path):\n\t                for file in files:\n\t                    target_path = os.path.join(root, file)\n\t                    # If the file extension is \".json\", then combine into one json file which Python can understand (Converts JSON to the datatype Dict)\n", "                    if os.path.isfile(target_path) and os.path.splitext(target_path)[1].lower() == '.json':\n\t                        with open(target_path, 'r', encoding='utf-8') as f:\n\t                            collected_project = json.load(f)\n\t                        assert isinstance(collected_project, list)\n\t                        collected_projects.extend(collected_project)\n\t        # If the path is one json file\n\t        else:\n\t            with open(start_path, 'r', encoding='utf-8') as f:\n\t                collected_project = json.load(f)\n\t            assert isinstance(collected_project, list)\n", "            collected_projects.extend(collected_project)\n\t    return collected_projects\n\tdef create_job_data(target: str, collected_projects: list) -> Dict:\n\t    \"\"\"\n\t    As the scope of underlying software architecture is broader than LCAbyg Web API, the system must be able to handle other data types than JSON.\n\t    This makes it necessary to encode and decode the JSON data.\n\t    This function describe the processes for creating the job data. This includes encoding/decoding json TO the API (packing bytes). Read more here: https://en.wikipedia.org/wiki/Base64\n\t    See the function unpack_bytes for decoding data from the API.\n\t    :param target:\n\t    :param collected_projects: JSON files collected and converted to the datatype Dict)\n", "    :return: job_data (json data that the API can understand)\n\t    \"\"\"\n\t    # Datatype: dict converted from json\n\t    input_dict = collect_json(collected_projects)\n\t    # Datatype: string converted from dict\n\t    input_string = json.dumps(input_dict)\n\t    # Datatype: bytes converted from string\n\t    input_bytes = input_string.encode('utf-8')\n\t    # Datatype: string base 64 converted from bytes (encoding) (https://en.wikipedia.org/wiki/Base64)\n\t    input_string_base64 = base64.standard_b64encode(input_bytes).decode('utf-8')\n", "    job_data = {\n\t        'priority': 0,\n\t        'job_target': target,\n\t        'job_target_min_ver': '',\n\t        'job_target_max_ver': '',\n\t        'job_arguments': '',\n\t        'extra_input': '',\n\t        'input_blob': input_string_base64,\n\t        'input_data': collect_json(collected_projects)\n\t    }\n", "    return job_data\n\tdef unpack_bytes(data):\n\t    \"\"\"\n\t    This function unpack_bytes for decoding data from the API.\n\t    See the function create_job_data for packing bytes and creating data for the API.\n\t    :param data:\n\t    :return: decoded data (json data that Python can understand)\n\t    \"\"\"\n\t    decode_data = base64.standard_b64decode(data)\n\t    bytes_to_string = decode_data.decode('utf-8')\n", "    string_to_dict = json.loads(bytes_to_string)\n\t    return string_to_dict\n\tdef get_job_ids(server_url: str, auth_token: str) -> str:\n\t    \"\"\"\n\t    Gives all job ids on the account you are logged in with.\n\t    :param server_url:\n\t    :param auth_token:\n\t    :return: List of job ids (UUIDs converted to strings)\n\t    \"\"\"\n\t    return get(f'{server_url}/v2/jobs', auth_token=auth_token)\n", "def get_job_by_id(server_url: str, job_id: str, auth_token: str) -> str:\n\t    \"\"\"\n\t    :param server_url:\n\t    :param job_id: UUIDs converted to strings. See the function get_job_ids\n\t    :param auth_token:\n\t    :return: List of dictionaries for the jobs in the account. Each dictionary contains information on the specific job.\n\t    \"\"\"\n\t    return get(f'{server_url}/v2/jobs/{job_id}', auth_token=auth_token)\n\tdef post_job(server_url: str, body: dict, auth_token: str) -> str:\n\t    \"\"\"\n", "    :param server_url:\n\t    :param body: The json data you want to send.\n\t    :param auth_token:\n\t    :return: output data. Error_messages are found in 'extra_output'\n\t    \"\"\"\n\t    return post(f'{server_url}/v2/jobs', body, auth_token=auth_token)\n\tdef get_job_input_by_id(server_url: str, job_id: str, auth_token: str):\n\t    \"\"\"\n\t    The return data needs to be decoded for Python to understand the data. See an example of this in the function unpack_bytes.\n\t    :param server_url:\n", "    :param job_id:\n\t    :param auth_token:\n\t    :return: input model.json.\n\t    \"\"\"\n\t    return get(f'{server_url}/v2/jobs/{job_id}/input', auth_token=auth_token)\n\tdef get_job_output_by_id(server_url, job_id: str, auth_token: str):\n\t    \"\"\"\n\t    The return data needs to be decoded for Python to understand the data. See an example of this in the function unpack_bytes.\n\t    A three-layer hierarchy structures the results:\n\t    1. Instance id corresponding to ids in the model\n", "    2. Stage/aggregation approach (for instance, SUM)\n\t    3. Year - please note that year 9999 represents the total sum of all years.\n\t    Read the LCAbyg JSON guide for more info.\n\t    :param server_url:\n\t    :param job_id:\n\t    :param auth_token:\n\t    :return: results and input model.json.\n\t    \"\"\"\n\t    return get(f'{server_url}/v2/jobs/{job_id}/output', auth_token=auth_token)\n\tdef mark_job_as_finished(server_url: str, job_id: str, auth_token: str):\n", "    \"\"\"\n\t    Finally, you should mark the job as finished when you have all the data you need.\n\t    It is important to indicate when you have finished talking to the API. If you have not canceled your calls, the queue limit will fill. See the function mark_job_as_finished.\n\t    NOTICE: It can only be done one time per job.\n\t    :param server_url:\n\t    :param job_id:\n\t    :param auth_token:\n\t    :return: Please be aware that when the job is marked as finished, your job will be removed from the system sooner or later.\n\t    \"\"\"\n\t    return delete(f'{server_url}/v2/jobs/{job_id}', auth_token=auth_token)\n", "def get(url: str,\n\t        headers: Optional[Dict] = None,\n\t        auth_token: Optional[str] = None) -> Any:\n\t    \"\"\"\n\t    NOTICE: The functions get(), post(), put(), and delete() should NOT be modified. They are the essential functions for talking to the API.\n\t    :param url:\n\t    :param headers:\n\t    :param auth_token:\n\t    :return:\n\t    \"\"\"\n", "    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n\t        headers['Authorization'] = f'Bearer {auth_token}'\n\t    res = requests.get(url, headers=headers)\n\t    if res.status_code == 200:\n\t        return res.json()\n\t    elif res.status_code == 440:\n\t        raise AuthTokenExpired(res)\n\t    else:\n", "        print(f'GET ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n\tdef post(url: str,\n\t         data: Optional[Dict] = None,\n\t         headers: Optional[Dict] = None,\n\t         auth_token: Optional[str] = None,\n\t         auth: Optional[Tuple[str, str]] = None) -> Any:\n\t    \"\"\"\n\t    NOTICE: The functions get(), post(), put(), and delete() should NOT be modified. They are the essential functions for talking to the API.\n\t    :param url:\n", "    :param data:\n\t    :param headers:\n\t    :param auth_token:\n\t    :param auth:\n\t    :return:\n\t    \"\"\"\n\t    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n\t        headers['Authorization'] = f'Bearer {auth_token}'\n", "    res = requests.post(url, json=data, headers=headers, auth=auth)\n\t    if res.status_code == 200:\n\t        return res.json()\n\t    elif res.status_code == 440:\n\t        raise AuthTokenExpired(res)\n\t    else:\n\t        print(f'POST ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n\tdef put(url: str,\n\t        data: Optional[Dict] = None,\n", "        headers: Optional[Dict] = None,\n\t        auth_token: Optional[str] = None,\n\t        auth: Optional[Tuple[str, str]] = None) -> Any:\n\t    \"\"\"\n\t    NOTICE: The functions get(), post(), put(), and delete() should NOT be modified. They are the essential functions for talking to the API.\n\t    :param url:\n\t    :param data:\n\t    :param headers:\n\t    :param auth_token:\n\t    :param auth:\n", "    :return:\n\t    \"\"\"\n\t    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n\t        headers['Authorization'] = f'Bearer {auth_token}'\n\t    res = requests.put(url, json=data, headers=headers, auth=auth)\n\t    if res.status_code == 200:\n\t        return res.json()\n\t    elif res.status_code == 440:\n", "        raise AuthTokenExpired(res)\n\t    else:\n\t        print(f'PUT ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n\tdef delete(url: str,\n\t           headers: Optional[Dict] = None,\n\t           auth_token: Optional[str] = None) -> Any:\n\t    \"\"\"\n\t    NOTICE: The functions get(), post(), put(), and delete() should NOT be modified. They are the essential functions for talking to the API.\n\t    :param url:\n", "    :param headers:\n\t    :param auth_token:\n\t    :return:\n\t    \"\"\"\n\t    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n\t        headers['Authorization'] = f'Bearer {auth_token}'\n\t    res = requests.delete(url, headers=headers)\n\t    if res.status_code == 200:\n", "        return res.json()\n\t    elif res.status_code == 440:\n\t        raise AuthTokenExpired(res)\n\t    else:\n\t        print(f'DELETE ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n\tdef example_send_job(server_url, token, target, output_path, project_paths):\n\t    # Notice:  list of paths\n\t    for project_path in project_paths:\n\t        print('Connecting to the server:')\n", "        res_ping = ping(server_url)\n\t        print(f'res_ping = {res_ping}. You are  connected to the server!\\n')\n\t        print('Connecting to the server and logging in:')\n\t        res_ping_secure = ping_secure(server_url, auth_token=token)\n\t        print(f'res_ping = {res_ping_secure}. You are connected to the server and logged in!\\n')\n\t        print('Create job data from list of paths:')\n\t        res_job_data = create_job_data(target, [project_path])\n\t        print('The json data has created!\\n')\n\t        print('\\Post job:')\n\t        res_post_job = post_job(server_url, body=res_job_data, auth_token=token)\n", "        print(f\"Job posted! Status: {res_post_job}\\n\")\n\t        print('Waiting for the job to finish:')\n\t        not_done = True\n\t        while not_done:\n\t            status = res_post_job['status']\n\t            print(f'status = {status}')\n\t            not_done = (status == 'New') or (status == 'Started')\n\t            time.sleep(1)\n\t            res_post_job = get_job_by_id(server_url, job_id=res_post_job['id'], auth_token=token)\n\t            print(f\"The information for job id: {res_post_job['id']} is: {res_post_job}\\n\")\n", "            print()\n\t        print(f'Done: status = {status}')\n\t        print('Get job input (model) by job id:')\n\t        res_get_job_input_by_id = get_job_input_by_id(server_url, job_id=res_post_job['id'], auth_token=token)\n\t        decode_res_get_job_input_by_id = unpack_bytes(res_get_job_input_by_id)\n\t        print(f\"The job input for job id: {res_post_job['id']} is: {decode_res_get_job_input_by_id}\\n\")\n\t        print('Download the results:')\n\t        res_get_job_output_by_id = get_job_output_by_id(server_url, job_id=res_post_job['id'], auth_token=token)\n\t        decode_res_get_job_output_by_id = unpack_bytes(data=res_get_job_output_by_id)\n\t        print(f\"The LCA results for job id {res_post_job['id']} are: {decode_res_get_job_output_by_id}\\n\")\n", "        # This can only be done one time per job:\n\t        print('Mark job as finished:')\n\t        # res_mark_job_as_finished = mark_job_as_finished(SERVER_URL, job_id=example_job_id, auth_token=TOKEN)\n\t        print('Job is marked as finished:\\n')\n\t        print('Saving the results to disk:')\n\t        # data = json.loads(decode_res_get_job_output_by_id)\n\t        with open(output_path, 'w', encoding='utf-8') as f:\n\t            json.dump(decode_res_get_job_output_by_id, f, indent=2, ensure_ascii=False)\n\t        print()\n\tdef example_debug_job_data(server_url, token, project_path):\n", "    \"\"\"\n\t    Running this function will give the Traceback error: \"json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\"\n\t    A more detailed explanation.\n\t    :param server_url:\n\t    :param token:\n\t    :param target:\n\t    :param project_path:\n\t    :return:\n\t    \"\"\"\n\t    # Collection and encoding job data with errors\n", "    res_job_data = create_job_data(TARGET, project_path)\n\t    # Post the job\n\t    res_post_job_error = post_job(server_url, body=res_job_data, auth_token=token)\n\t    print('Waiting for the job to finish:')\n\t    not_done = True\n\t    while not_done:\n\t        status = res_post_job_error['status']\n\t        print(f'status = {status}')\n\t        not_done = (status == 'New') or (status == 'Started')\n\t        time.sleep(1)\n", "        res_post_job_error = get_job_by_id(server_url, job_id=res_post_job_error['id'], auth_token=token)\n\t        print(f\"The information for job id: {res_post_job_error['id']} is: {res_post_job_error}\")\n\t        print()\n\t    print(f'Done: status = {status}')\n\t    if status == 'Failed':\n\t        print(\n\t            f\"\\nLog error causing the job id {res_post_job_error['id']} to fail: \\n{res_post_job_error['extra_output']}\\n\")\n\t    else:\n\t        print('Get job input (model) by job id:')\n\t        res_get_job_input_by_id = get_job_input_by_id(server_url, job_id=res_post_job_error['id'], auth_token=token)\n", "        decode_res_get_job_input_by_id = unpack_bytes(res_get_job_input_by_id)\n\t        print(f\"The job input for job id: {res_post_job_error['id']} is: {decode_res_get_job_input_by_id}\\n\")\n\t        print('Download the results:')\n\t        res_get_job_output_by_id = get_job_output_by_id(server_url, job_id=res_post_job_error['id'], auth_token=token)\n\t        decode_res_get_job_output_by_id = unpack_bytes(data=res_get_job_output_by_id)\n\t        print(f\"The LCA results for job id {res_post_job_error['id']} are: {decode_res_get_job_output_by_id}\\n\")\n\t        # This can only be done one time per job:\n\t        print('Mark job as finished:')\n\t        # res_mark_job_as_finished = mark_job_as_finished(SERVER_URL, job_id=example_job_id, auth_token=TOKEN)\n\t        print('Job is marked as finished:\\n')\n", "def main():\n\t    # Create a user account  and get a API_KEY at https://api.lcabyg.dk/da/\n\t    token = login_via_headers(SERVER_URL, USERNAME, API_KEY)\n\t    example_send_job(SERVER_URL, token, TARGET, OUTPUT_PATH, PROJECT_PATHS)\n\t    # EXAMPLE sending a job containing one projects with one error in Building.json:\n\t    # project_path_with_error = ['testdata/lcabyg/single_with_error']\n\t    # example_debug_job_data(SERVER_URL, token, project_path_with_error)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "sensitivity_analysis.py", "chunked_list": ["import os\n\tfrom serde.json import to_json\n\tfrom sensitivity_analysis.generator import read_json_file, calculate_impacts, find_building_sum\n\tfrom sensitivity_analysis.sensitivity import sensitivity_of_file, how_sensitive_plural\n\tfrom sensitivity_analysis.visualize import bar_plot\n\t# Turn this on if not using environment variables\n\tUSERNAME = 'INSERT YOUR USERNAME'\n\tAPI_KEY = 'INSERT YOUR API KEY'\n\t# Save username and password as environment variables\n\tos.environ['USERNAME'] = USERNAME\n", "os.environ['API_KEY'] = API_KEY\n\tdef main():\n\t    \"\"\"\n\t    This example aims to show how the LCAbyg web API can be used to conduct a sensitivity analysis.\n\t    The analysis is made on an example of a wall, where only the product (byggevare) amounts are perturbed.\n\t    This could be expanded to other parameters, which will require some adaption of the sensitivity code.\n\t    The sensitivity calculations are only made for GWP, which can be expanded to other impact categories.\n\t    Sensitivity tells whether LCIA results (impacts) are sensitive to perturbation in specific parameters.\n\t    This can help in the iterative LCA process to only focus on detailing \" important \" parameters.\n\t    Disclaimer: The code is a simple example, part of a bigger project (WIP). It is not optimized, so\n", "                suggestions for changes will be appreciated.\n\t    :return: Prints the info from sensitivity analyses as they happen and visualized in a bar plot at the end\n\t    \"\"\"\n\t    # This wall is an example of an element with three constructions (interior alkyd paint w. full plastering,\n\t    # wood construction w. mineral wool and exterior bricks)\n\t    data_path = 'sensitivity_analysis/testdata/wall.json'\n\t    # Get the initial data as objects, see constructions.py:\n\t    data_obj = read_json_file(data_path)\n\t    # Calculate the base case without perturbation to get the base impacts, needed to calculate sensitivity\n\t    data_json = to_json(data_obj)  # Serializing the data from objects to json\n", "    initial_calculation = calculate_impacts(data_json)  # json data is posted to LCAbyg web API in calculate_impacts\n\t    initial_building_impact_sum = find_building_sum(initial_calculation)  # find_building_sum locates the impact\n\t    # Conduct sensitivity analysis for all product amounts in the wall\n\t    sens_coeffs, product_names = sensitivity_of_file(data_obj, initial_building_impact_sum, 0.9)\n\t    # Visualize the results based on the sensitivity coefficients for each product\n\t    sensitivity_levels = how_sensitive_plural(sens_coeffs)\n\t    print(f'Bar plot shows the sensitivity coefficient for each product. Coefficients are shown on the x-axis. \\n'\n\t          f'For the colors of the bars, green indicates low sensitivity, yellow medium and red high and very high.')\n\t    bar_plot(product_names, sens_coeffs, sensitivity_levels, 'Sensitivity')\n\tif __name__ == '__main__':\n", "    main()\n"]}
{"filename": "sensitivity_analysis/sensitivity.py", "chunked_list": ["from sensitivity_analysis.nodes_and_edges import Edge, File\n\tfrom sensitivity_analysis.constructions import ConstructionToProduct\n\tfrom sensitivity_analysis.generator import calculate_impacts, find_building_sum, find_product_name\n\tfrom serde.json import to_json\n\tdef calc_sensitivity(base_value: float | int, new_value: float | int, base_impact: float, new_impact: float):\n\t    \"\"\"\n\t    Calculates a normalized sensitivity coefficient following the book \" Life Cycle Assessment - Theory and Practice\"\n\t    by Hauschild, Rosenbaum and Irving Olsen, 2018, page 1083.\n\t    :param base_value: The pase value for the specific parameter\n\t    :param new_value: The perturbed value for the specific parameter\n", "    :param base_impact: The base impact calculated without any perturbation\n\t    :param new_impact: The new impact calculated with the perturbed parameter\n\t    :return: The normalized sensitivity coefficient. According to Hauschild et al., 2018. The results show medium\n\t             sensitivity when coefficient > 0.3, and high sensitivity when coefficient > 0.5\n\t    \"\"\"\n\t    assert base_impact != new_value\n\t    # Partial results calculates for better understanding\n\t    delta_impact = abs(new_impact - base_impact)\n\t    fraction_impact = delta_impact / base_impact\n\t    delta_value = abs(new_value - base_value)\n", "    fraction_value = delta_value / base_value\n\t    sensitivity_coefficient = fraction_impact / fraction_value\n\t    return sensitivity_coefficient\n\tdef sensitivity_of_file(data: File, initial_impact: float, perturbation: float = 0.1):\n\t    \"\"\"\n\t    This function is made specifically to conduct sensitivity, when perturbing the product amounts.\n\t    These amounts are found in the ConstructionToProduct edges. Therefore, this function should be expanded if\n\t    wanting to test other parameters than product amounts.\n\t    :param data: The specific LCAbyg data as objects. File contains Construction nodes and ConstructionToProduct edges.\n\t    :param initial_impact: The base impact calculated before any perturbations are made\n", "    :param perturbation: The fraction to perturb the value with. Normally 10%, so 0.1.\n\t    :return: Several lists with the stored data, from the different analyses\n\t    \"\"\"\n\t    # make a copy of the data to avoid overwriting the data\n\t    data_copy = data.copy()\n\t    # Lists for storing the needed data\n\t    store_sensitivity_coefficients = []\n\t    stor_product_names = []\n\t    # Now this loop is looking specifically for ConstructionToProduct edges\n\t    for instance in data_copy:\n", "        if isinstance(instance, Edge):\n\t            if isinstance(instance.edge[0], ConstructionToProduct):\n\t                # The specific product amount is perturbed and assigned in order to make a new LCIA calculation\n\t                initial_value, instance_id, perturbed_value = perturbation_of_value(instance, perturbation)\n\t                # Serialize data in order to make a calculation\n\t                data_json = to_json(data_copy)\n\t                print(f'Initial impact: {initial_impact}')\n\t                # Data is posted to the LCAbyg web API and impacts are calculated\n\t                print(f'Calculate new impact...')\n\t                results = calculate_impacts(data_json)\n", "                # The impact is found in the result data and stored\n\t                building_sum = find_building_sum(results)\n\t                # Print an overview of the sensitivity analysis\n\t                product_name = find_product_name(results, instance_id)\n\t                stor_product_names.append(product_name)\n\t                print(f'Perturbation of product: {product_name}')\n\t                print(f'Amount changed from {initial_value} to {perturbed_value}')\n\t                print(f'This perturbations results in new impact of {building_sum}')\n\t                # calculate sensitivity coefficient\n\t                sensitivity_coefficient = calc_sensitivity(initial_value, perturbed_value, initial_impact, building_sum)\n", "                store_sensitivity_coefficients.append(sensitivity_coefficient)\n\t                sensitivity_level = how_sensitive(sensitivity_coefficient)\n\t                print(f'Sensitivity coefficient is {sensitivity_coefficient}, which means:')\n\t                print(f'Building impact has a {sensitivity_level} sensitivity to this perturbation\\n')\n\t    return store_sensitivity_coefficients, stor_product_names\n\tdef perturbation_of_value(edge: Edge, perturbation: float):\n\t    \"\"\"\n\t    Perturbs the value and extracts the needed information for communication of the sensitivity analysis\n\t    :param edge: The ConstructionToProduct edge, with the product amount\n\t    :param perturbation: The fraction, which is the percentage, to perturb the amount.\n", "    :return: ID of the product, the unperturbed value and the perturbed value.\n\t    \"\"\"\n\t    initial_value = edge.edge[0].amount\n\t    instance_id = edge.edge[-1]\n\t    varied_value = edge.edge[0].amount * perturbation\n\t    edge.edge[0].amount = varied_value\n\t    return initial_value, instance_id, varied_value\n\tdef how_sensitive(sens_coeff: float):\n\t    \"\"\"\n\t    This function translates the sensitivity coefficient into one of four groups.\n", "    The thresholds are inspired by the book \" Life Cycle Assessment - Theory and Practice\"\n\t    by Hauschild, Rosenbaum and Irving Olsen, 2018, page 1083.\n\t    :param sens_coeff: The sensitivity coefficient\n\t    :return: A string with the level of sensitivity\n\t    \"\"\"\n\t    sens_coeff = abs(sens_coeff)\n\t    sensitivity_level = 'low'\n\t    if sens_coeff >= 0.3 and sens_coeff < 0.5:\n\t        sensitivity_level = 'medium'\n\t    elif sens_coeff >= 0.5 and sens_coeff < 1.0:\n", "        sensitivity_level = 'high'\n\t    elif sens_coeff >= 1.0:\n\t        sensitivity_level = 'very high'\n\t    return sensitivity_level\n\tdef how_sensitive_plural(coefficients: list):\n\t    \"\"\"\n\t    Function to assess the level of sensitivity for a list of sensitivity coefficients\n\t    :param coefficients: list of sensitivity coefficients\n\t    :return: list of strings with the level of sensitivity\n\t    \"\"\"\n", "    store = []\n\t    for coefficient in coefficients:\n\t        store.append(how_sensitive(coefficient))\n\t    return store\n"]}
{"filename": "sensitivity_analysis/general.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom serde import serde, field, ExternalTagging\n\tfrom typing import Optional\n\t# This script contains all the classes that represent general content in the LCAbyg json\n\t@serde(tagging=ExternalTagging)\n\t@dataclass\n\tclass Name:\n\t    \"\"\"\n\t    Represents the names used in all nodes.\n\t    Names in all three languages are not needed for deserialization to work.\n", "    When serializing and deserializing, names in all three languages will be created as default empty str.\n\t    \"\"\"\n\t    danish: Optional[str] = field(default='', skip_if_default=True, rename='Danish')\n\t    english: Optional[str] = field(default='', skip_if_default=True, rename='English')\n\t    german: Optional[str] = field(default='', skip_if_default=True, rename='German')\n\t    def __init__(self, danish_name: str = None, english_name: str = None, german_name: str = None):\n\t        self.danish = danish_name\n\t        self.english = english_name\n\t        self.german = german_name\n\t@serde\n", "@dataclass\n\tclass Comment:\n\t    \"\"\"\n\t    Represents the comments used in all nodes\n\t    \"\"\"\n\t    danish: Optional[str] = field(default='', skip_if_default=True, rename='Danish')\n\t    english: Optional[str] = field(default='', skip_if_default=True, rename='English')\n\t    german: Optional[str] = field(default='', skip_if_default=True, rename='German')\n\t    def __init__(self, danish_name: str = None, english_name: str = None, german_name: str = None):\n\t        self.danish = danish_name\n", "        self.english = english_name\n\t        self.german = german_name\n"]}
{"filename": "sensitivity_analysis/nodes_and_edges.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom serde import serde, field, ExternalTagging\n\tfrom typing import List, Tuple, TypeAlias, Union\n\tfrom sensitivity_analysis.constructions import Construction, ConstructionToProduct\n\t# The class Temp is a workaround to make the serializing and deserializing work correctly\n\t# If anyone is interested in solving this you are welcome to give is a try or contact Lærke Vejsnæs (lhv@build.aau.dk)\n\t@serde(tagging=ExternalTagging)\n\t@dataclass\n\tclass Temp:\n\t    pass\n", "# This script contains the classes representing the overall nodes and edges and which types of content\n\t# they can have in the LCAbyg json\n\t@serde(tagging=ExternalTagging)\n\t@dataclass\n\tclass Node:\n\t    \"\"\"\n\t    All nodes in the LCAbyg json contain a dictionary with the possible node types\n\t    \"\"\"\n\t    node: Union[Construction, Temp] = field(rename='Node')\n\t@serde(tagging=ExternalTagging)\n", "@dataclass\n\tclass Edge:\n\t    \"\"\"\n\t    All edges in the LCAbyg json contain a list of a dictionary and two strings (uuids).\n\t    the possible dictionaries are represented in the different classes.\n\t    \"\"\"\n\t    edge: Tuple[ConstructionToProduct | Temp, str, str] = field(rename='Edge')\n\t# This TypeAlias is needed to handle a json, which is a list of nodes and edges.\n\tFile: TypeAlias = List[Node | Edge]\n"]}
{"filename": "sensitivity_analysis/__init__.py", "chunked_list": []}
{"filename": "sensitivity_analysis/visualize.py", "chunked_list": ["import matplotlib.pyplot as plt\n\tfrom mpl_toolkits.axes_grid1.axes_divider import make_axes_area_auto_adjustable\n\tfrom typing import List\n\tdef bar_plot(names: List[str], values: List[float], sensitivity_levels: List[str], title: str):\n\t    \"\"\"\n\t    Plot to illustrate the sensitivity of the products in the wall. Is a horizontal bar plot.\n\t    The sensitivity coefficients are plotted for each product and the bars are colored to show the\n\t    level of sensitivity\n\t    The higher the sensitivity coefficient, the more sensitive the impact is to perturbation in the product amount.\n\t    :param names: Names of the products as list with str\n", "    :param values: sensitivity coefficients in a list\n\t    :param sensitivity_levels: level of sensitivity as str in a list\n\t    :param title: The title of the plot\n\t    :return: Will show the plot\n\t    \"\"\"\n\t    # Adjusts the size of the plot, which are given in inches, therefore the translation from cm\n\t    cm = 1 / 2.54  # cm to inches\n\t    fig, ax = plt.subplots(figsize=(26.48 * cm, 15 * cm))\n\t    # Gets the colors for the plot\n\t    colors = colors_for_bar_plot_sensitivity_level(sensitivity_levels)\n", "    # The actual plot and the content for the plot\n\t    ax.barh(names, values, color=colors, align='center', height=0.5)\n\t    plt.title(title)\n\t    plt.xlabel('Sensitivity coefficients')\n\t    plt.ylabel('Products')\n\t    # This ensures that the y-axis labels (product names) are visible\n\t    make_axes_area_auto_adjustable(ax)\n\t    # The plot will be shown and can be copied as an image by right-clicking\n\t    plt.show()\n\tdef colors_for_bar_plot_sensitivity_level(sensitivity_levels: List[str]):\n", "    \"\"\"\n\t    The colors are chosen based on the level of sensitivity to make the plot more illustrative.\n\t    sensitivity_coefficient < 0.3 is low sensitivity\n\t    0.3 <= sensitivity_coefficient < 0.5 is medium sensitivity\n\t    0.5 <= sensitivity_coefficient < 1.0 is high sensitivity\n\t    1.0 <= sensitivity_coefficient is very high sensitivity\n\t    :param sensitivity_levels: level of sensitivity as str in a list\n\t    :return: list with matplotlib color names\n\t    \"\"\"\n\t    color_list = []\n", "    for level in sensitivity_levels:\n\t        if level == 'low':\n\t            color_list.append('limegreen')\n\t        elif level == 'medium':\n\t            color_list.append('yellow')\n\t        elif level == 'high':\n\t            color_list.append('red')\n\t        else:\n\t            color_list.append('red')\n\t    return color_list\n"]}
{"filename": "sensitivity_analysis/generator.py", "chunked_list": ["import json\n\tfrom uuid import UUID\n\tfrom sensitivity_analysis.nodes_and_edges import File\n\tfrom serde.json import from_json\n\timport os\n\timport time\n\tfrom lcabyg_web_api_py import *\n\tdef read_json_file(path: str) -> File:\n\t    \"\"\"\n\t    ONLY WORKS WITH THE DATA THAT FIT WITH IMPLEMENTED CLASSES -> Construction and ConstructionToProduct in Node and\n", "    Edge in File\n\t    Read one json file and deserialize to the objects that fit the LCAbyg json structure.\n\t    :param path: str of path to file location\n\t    :return: Will return a list of objects\n\t    \"\"\"\n\t    with open(path, 'r', encoding='utf-8') as f:\n\t        data = f.read()\n\t        return from_json(File, data)\n\tdef calculate_impacts(data: str):\n\t    \"\"\"\n", "    The LCAbyg web API code, which posts the json project to the API.\n\t    :param data: str with the data\n\t    :return: The output from the API, which is the model and results as json\n\t    \"\"\"\n\t    # Next two lines are compatible with pycharm. Otherwise, just replace os.environ.get('USERNAME') with username ect.\n\t    USERNAME = os.environ.get('USERNAME')  # Username is the username, from the API website\n\t    API_KEY = os.environ.get('API_KEY')  # API_KEY is the key retrieved from the API website\n\t    cli = Client(USERNAME, API_KEY)\n\t    cli.api_ping_test()\n\t    # Write the json data to a file to prepare for job submission\n", "    # When running the sensitivity this file will be overwritten for every calculation\n\t    with open('sensitivity_analysis/testdata/wall_example/User/wall_data.json', 'w', encoding='utf-8') as f:\n\t        json_data = json.loads(data)\n\t        json.dump(json_data, f, indent=2, ensure_ascii=False)\n\t    # The job is submitted\n\t    job, output_blob = cli.submit_job(NewJob(project=collect_json(['sensitivity_analysis/testdata/wall_example/'])))\n\t    return output_blob\n\tdef find_building_sum(result_data: dict):\n\t    \"\"\"\n\t    Helper function to get the GWP result/impact for the full building\n", "    :param result_data: The API result output\n\t    :return: a float with the result\n\t    \"\"\"\n\t    building_id = ''\n\t    assert 'model' in result_data\n\t    for instance in result_data['model']:\n\t        assert 'node_type' in instance\n\t        if instance['node_type'] == 'Building':\n\t            building_id = instance['id']\n\t    assert 'results' in result_data\n", "    result = result_data['results'][building_id]['Sum']['9999']['GWP']\n\t    return result\n\tdef find_product_name(result_data: dict, product_id: UUID):\n\t    \"\"\"\n\t    Helper function to find the name for a product, from its id.\n\t    :param result_data: The API result output\n\t    :param product_id: The uuid of the product - the model id\n\t    :return: str with product name\n\t    \"\"\"\n\t    assert 'model' in result_data\n", "    for instance in result_data['model']:\n\t        assert 'node_type' in instance\n\t        if instance['node_type'] == 'ProductInstance':\n\t            if instance['model_id'] == product_id:\n\t                assert 'Danish' in instance['name']\n\t                product_name = instance['name']['Danish']\n\t                return product_name\n"]}
{"filename": "sensitivity_analysis/constructions.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom serde import serde, ExternalTagging\n\tfrom sensitivity_analysis.general import Name, Comment\n\t# This script represents the content of nodes and edges needed for constructions in LCAbyg json\n\t@serde(tagging=ExternalTagging)\n\t@dataclass\n\tclass Construction:\n\t    id: str\n\t    name: Name\n\t    unit: str\n", "    source: str\n\t    comment: Comment\n\t    layer: int\n\t    locked: bool\n\t    def __init__(self, construction_id: str, name: Name, unit: str,\n\t                 source: str, comment: Comment, layer: int, locked: bool):\n\t        self.id = construction_id\n\t        self.name = name\n\t        self.unit = unit\n\t        self.source = source\n", "        self.comment = comment\n\t        self.layer = layer\n\t        self.locked = locked\n\t@serde\n\t@dataclass\n\tclass ConstructionToProduct:\n\t    id: str\n\t    amount: float | int\n\t    unit: str\n\t    lifespan: int\n", "    demolition: bool\n\t    enabled: bool\n\t    delayed_start: int\n\t    def __init__(self, edge_id: str, amount: float, unit: str, lifespan: int,\n\t                 demolition: bool, enabled: bool, delayed_start: int):\n\t        self.id = edge_id\n\t        self.amount = amount\n\t        self.unit = unit\n\t        self.lifespan = lifespan\n\t        self.demolition = demolition\n", "        self.enabled = enabled\n\t        self.delayed_start = delayed_start\n"]}
{"filename": "lcabyg_web_api_py/client.py", "chunked_list": ["import json\n\timport sbi_web_api_py\n\tfrom typing import Tuple, Optional\n\tfrom sbi_web_api_py import raw_api, Job\n\tfrom sbi_web_api_py.type_hinting import JsonList, JsonObject\n\tfrom lcabyg_web_api_py import NewJob\n\tfrom lcabyg_web_api_py.utils import pack_json\n\tclass Client(sbi_web_api_py.Client):\n\t    def api_submit_job(self, project: NewJob):\n\t        return super().api_submit_job(project)\n", "    def api_get_job_input(self, job_id: str) -> JsonList:\n\t        data = super().api_get_job_input(job_id)\n\t        return json.loads(data)\n\t    def api_get_job_output(self, job_id: str) -> Optional[JsonObject]:\n\t        data = super().api_get_job_output(job_id)\n\t        if data is not None and len(data) > 0:\n\t            return json.loads(data)\n\t        else:\n\t            return None\n\t    def submit_job(self, project: NewJob, pool_interval: float = 1, auto_mark_as_finished: bool = True) -> Tuple[Job, Optional[JsonObject]]:\n", "        return super().submit_job(project, pool_interval=pool_interval, auto_mark_as_finished=auto_mark_as_finished)\n"]}
{"filename": "lcabyg_web_api_py/new_job.py", "chunked_list": ["from dataclasses import dataclass, InitVar\n\tfrom sbi_web_api_py.type_hinting import JsonList\n\tfrom lcabyg_web_api_py.utils import pack_json\n\t@dataclass\n\tclass NewJob:\n\t    priority: int = 0\n\t    job_target: str = 'lcabyg5+br23'\n\t    job_target_min_ver: str = ''\n\t    job_target_max_ver: str = ''\n\t    job_arguments: str = ''\n", "    extra_input: str = ''\n\t    input_blob: str = ''\n\t    project: InitVar[JsonList | None] = None\n\t    def __post_init__(self, project: JsonList):\n\t        self.input_blob = pack_json(project)\n\t    def to_dict(self):\n\t        return self.__dict__\n"]}
{"filename": "lcabyg_web_api_py/type_hinting.py", "chunked_list": ["from typing import List, Dict, Any\n\tJsonObject = Dict[str, Any]\n\tJsonList = List[JsonObject]\n"]}
{"filename": "lcabyg_web_api_py/__init__.py", "chunked_list": ["from lcabyg_web_api_py.new_job import NewJob\n\tfrom lcabyg_web_api_py.client import Client\n\tfrom lcabyg_web_api_py.utils import collect_json\n\tfrom sbi_web_api_py import JobStatus"]}
{"filename": "lcabyg_web_api_py/utils.py", "chunked_list": ["import os\n\timport json\n\tfrom typing import List\n\tfrom lcabyg_web_api_py.type_hinting import JsonList\n\tfrom sbi_web_api_py.utils import pack_bytes\n\tdef collect_json(json_paths: List[str]) -> JsonList:\n\t    assert isinstance(json_paths, list)\n\t    out = list()\n\t    for start_path in json_paths:\n\t        start_path = os.path.expanduser(start_path)\n", "        if os.path.isdir(start_path):\n\t            for root, directories, files in os.walk(start_path):\n\t                for file in files:\n\t                    target_path = os.path.join(root, file)\n\t                    if os.path.isfile(target_path) and os.path.splitext(target_path)[1].lower() == '.json':\n\t                        with open(target_path, 'r', encoding='utf-8') as f:\n\t                            data = json.load(f)\n\t                        assert isinstance(data, list)\n\t                        out.extend(data)\n\t        else:\n", "            with open(start_path, 'r', encoding='utf-8') as f:\n\t                data = json.load(f)\n\t            assert isinstance(data, list)\n\t            out.extend(data)\n\t    return out\n\tdef pack_json(data: JsonList):\n\t    text = json.dumps(data, ensure_ascii=False)\n\t    encoded_text = pack_bytes(text.encode('utf-8'))\n\t    return encoded_text\n"]}
{"filename": "sbi_web_api_py/client.py", "chunked_list": ["import json\n\timport logging\n\timport time\n\tfrom typing import List, Tuple\n\tfrom sbi_web_api_py.type_hinting import JsonList, JsonObject, IdList\n\tfrom sbi_web_api_py.new_job import NewJob\n\tfrom sbi_web_api_py.job import Job\n\tfrom sbi_web_api_py.utils import unpack_bytes\n\tfrom sbi_web_api_py import raw_api\n\tclass JobFailed(Exception):\n", "    def __init__(self, job: Job, output_blob: bytes):\n\t        self.job = job\n\t        self.output_blob = output_blob\n\t    def __str__(self):\n\t        return self.job.extra_output\n\tclass Client:\n\t    def __init__(self,\n\t                 username: str,\n\t                 password: str,\n\t                 api_base_url: str = 'https://api1.lcabyg.dk',\n", "                 login_now: bool = True):\n\t        self._username = username\n\t        self._password = password\n\t        self._api_base_url = api_base_url\n\t        self._token = self._login() if login_now else None\n\t        self.api_ping_test()\n\t    def _login(self):\n\t        return raw_api.login_via_body(self._api_base_url, self._username, self._password)\n\t    def _smart_call(self, api_function, *args, **kwargs):\n\t        # Inject the auth token\n", "        if 'auth_token' not in kwargs:\n\t            kwargs['auth_token'] = self._token\n\t        else:\n\t            logging.warning('Auth token for smart_call overridden!')\n\t        # Inject base url\n\t        if 'api_root' not in kwargs:\n\t            kwargs['api_root'] = self._api_base_url\n\t        else:\n\t            logging.warning('Base url for smart_call overridden!')\n\t        try:\n", "            return api_function(*args, **kwargs)\n\t        except raw_api.AuthTokenExpired:\n\t            logging.debug('Token expired. Logging in again automatically.')\n\t            self._token = self._login()\n\t            return api_function(*args, **kwargs)\n\t    def api_ping_test(self):\n\t        assert raw_api.ping(self._api_base_url) == 'pong'\n\t        assert self._smart_call(raw_api.ping_secure) == 'pong secure'\n\t    def api_get_account_info(self) -> JsonObject:\n\t        return self._smart_call(raw_api.get_account)\n", "    def api_submit_job(self, job: NewJob) -> Job:\n\t        res = self._smart_call(raw_api.post_job, payload=job.to_dict())\n\t        return Job.from_json(res)\n\t    def api_get_jobs(self) -> IdList:\n\t        res = self._smart_call(raw_api.get_jobs)\n\t        return res\n\t    def api_get_job(self, job_id: str) -> Job:\n\t        res = self._smart_call(raw_api.get_job_by_id, job_id=job_id)\n\t        return Job.from_json(res)\n\t    def api_get_job_input(self, job_id: str) -> bytes:\n", "        res = self._smart_call(raw_api.get_job_input_by_id, job_id=job_id)\n\t        return unpack_bytes(res)\n\t    def api_get_job_output(self, job_id: str) -> bytes:\n\t        res = self._smart_call(raw_api.get_job_output_by_id, job_id=job_id)\n\t        return unpack_bytes(res)\n\t    def api_mark_job_as_finished(self, job_id: str):\n\t        self._smart_call(raw_api.mark_job_as_finished, job_id=job_id)\n\t    def submit_job(self, job: NewJob, pool_interval: float = 1, auto_mark_as_finished: bool = True) -> Tuple[Job, bytes]:\n\t        job_row = self.api_submit_job(job)\n\t        while not job_row.status.worker_done():\n", "            job_row = self.api_get_job(job_row.id)\n\t            time.sleep(pool_interval)\n\t        output_blob = self.api_get_job_output(job_row.id)\n\t        if job_row.status.failed():\n\t            raise JobFailed(job_row, output_blob)\n\t        if auto_mark_as_finished:\n\t            self.api_mark_job_as_finished(job_row.id)\n\t            job_row = self.api_get_job(job_row.id)\n\t        return job_row, output_blob\n"]}
{"filename": "sbi_web_api_py/new_job.py", "chunked_list": ["from dataclasses import dataclass, InitVar\n\tfrom sbi_web_api_py.utils import pack_bytes\n\t@dataclass\n\tclass NewJob:\n\t    priority: int = 0\n\t    job_target: str = ''\n\t    job_target_min_ver: str = ''\n\t    job_target_max_ver: str = ''\n\t    job_arguments: str = ''\n\t    extra_input: str = ''\n", "    input_blob: str = ''\n\t    input_data: InitVar[bytes | None] = None\n\t    def __post_init__(self, input_data: bytes):\n\t        self.input_blob = pack_bytes(input_data)\n\t    def to_dict(self):\n\t        return self.__dict__\n"]}
{"filename": "sbi_web_api_py/type_hinting.py", "chunked_list": ["from typing import List, Dict, Any\n\tJsonObject = Dict[str, Any]\n\tJsonList = List[JsonObject]\n\tIdList = List[str]\n"]}
{"filename": "sbi_web_api_py/__init__.py", "chunked_list": ["from sbi_web_api_py.client import Client\n\tfrom sbi_web_api_py.new_job import NewJob\n\tfrom sbi_web_api_py.job import Job\n\tfrom sbi_web_api_py.job_status import JobStatus"]}
{"filename": "sbi_web_api_py/raw_api.py", "chunked_list": ["import sys\n\tfrom uuid import UUID\n\timport urllib.parse\n\tfrom typing import Optional, Dict, Tuple, Any\n\timport requests\n\tfrom sbi_web_api_py.type_hinting import JsonObject\n\tclass AuthTokenExpired(Exception):\n\t    def __init__(self, res: requests.Response):\n\t        self.response = res\n\tdef ping(api_root: str) -> str:\n", "    return get(f'{api_root}/v2/ping')\n\tdef ping_secure(api_root: str, **kwargs) -> str:\n\t    return get(f'{api_root}/v2/ping_secure', **kwargs)\n\tdef login_via_query(api_root: str, username: str, password: str) -> str:\n\t    query = urllib.parse.urlencode(dict(username=username, password=password))\n\t    res = post(f'{api_root}/v2/login?{query}')\n\t    UUID(res)\n\t    return res\n\tdef login_via_body(api_root: str, username: str, password: str) -> str:\n\t    res = post(f'{api_root}/v2/login', data=dict(username=username, password=password))\n", "    UUID(res)\n\t    return res\n\tdef login_via_headers(api_root: str, username: str, password: str) -> str:\n\t    res = post(f'{api_root}/v2/login', auth=(username, password))\n\t    UUID(res)\n\t    return res\n\tdef get_account(api_root: str, **kwargs) -> JsonObject:\n\t    return get(f'{api_root}/v2/account', **kwargs)\n\tdef get_jobs(api_root: str, **kwargs):\n\t    return get(f'{api_root}/v2/jobs', **kwargs)\n", "def get_job_by_id(api_root: str, job_id: str, **kwargs):\n\t    return get(f'{api_root}/v2/jobs/{job_id}', **kwargs)\n\tdef post_job(api_root: str, payload: dict, **kwargs):\n\t    return post(f'{api_root}/v2/jobs', payload, **kwargs)\n\tdef get_job_input_by_id(api_root: str, job_id: str, **kwargs):\n\t    return get(f'{api_root}/v2/jobs/{job_id}/input', **kwargs)\n\tdef get_job_output_by_id(api_root: str, job_id: str, **kwargs):\n\t    return get(f'{api_root}/v2/jobs/{job_id}/output', **kwargs)\n\tdef mark_job_as_finished(api_root: str, job_id: str, **kwargs):\n\t    return delete(f'{api_root}/v2/jobs/{job_id}', **kwargs)\n", "def get(url: str,\n\t        headers: Optional[Dict] = None,\n\t        auth_token: Optional[UUID] = None) -> Any:\n\t    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n\t        headers['Authorization'] = f'Bearer {auth_token}'\n\t    res = requests.get(url, headers=headers)\n\t    if res.status_code == 200:\n\t        return res.json()\n", "    elif res.status_code == 440:\n\t        raise AuthTokenExpired(res)\n\t    else:\n\t        print(f'GET ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n\tdef post(url: str,\n\t         data: Optional[Dict] = None,\n\t         headers: Optional[Dict] = None,\n\t         auth_token: Optional[UUID] = None,\n\t         auth: Optional[Tuple[str, str]] = None) -> Any:\n", "    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n\t        headers['Authorization'] = f'Bearer {auth_token}'\n\t    res = requests.post(url, json=data, headers=headers, auth=auth)\n\t    if res.status_code == 200:\n\t        return res.json()\n\t    elif res.status_code == 440:\n\t        raise AuthTokenExpired(res)\n\t    else:\n", "        print(f'POST ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n\tdef put(url: str,\n\t         data: Optional[Dict] = None,\n\t         headers: Optional[Dict] = None,\n\t         auth_token: Optional[UUID] = None,\n\t         auth: Optional[Tuple[str, str]] = None) -> Any:\n\t    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n", "        headers['Authorization'] = f'Bearer {auth_token}'\n\t    res = requests.put(url, json=data, headers=headers, auth=auth)\n\t    if res.status_code == 200:\n\t        return res.json()\n\t    elif res.status_code == 440:\n\t        raise AuthTokenExpired(res)\n\t    else:\n\t        print(f'PUT ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n\tdef delete(url: str,\n", "        headers: Optional[Dict] = None,\n\t        auth_token: Optional[UUID] = None) -> Any:\n\t    if auth_token:\n\t        if not headers:\n\t            headers = dict()\n\t        headers['Authorization'] = f'Bearer {auth_token}'\n\t    res = requests.delete(url, headers=headers)\n\t    if res.status_code == 200:\n\t        return res.json()\n\t    elif res.status_code == 440:\n", "        raise AuthTokenExpired(res)\n\t    else:\n\t        print(f'DELETE ERROR: {res.text}', file=sys.stderr)\n\t        res.raise_for_status()\n"]}
{"filename": "sbi_web_api_py/utils.py", "chunked_list": ["import base64\n\tdef pack_bytes(data: bytes) -> str:\n\t    return base64.standard_b64encode(data).decode('utf-8')\n\tdef unpack_bytes(data: str) -> bytes:\n\t    return base64.standard_b64decode(data.encode('utf-8'))\n"]}
{"filename": "sbi_web_api_py/job.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom datetime import datetime\n\tfrom sbi_web_api_py.type_hinting import JsonObject\n\tfrom sbi_web_api_py.job_status import JobStatus\n\t@dataclass\n\tclass Job:\n\t    id: str\n\t    created: datetime\n\t    account_id: str\n\t    status: JobStatus\n", "    priority: int\n\t    job_target: str\n\t    job_target_min_ver: str\n\t    job_target_max_ver: str\n\t    job_arguments: str\n\t    extra_input: str\n\t    extra_output: str\n\t    input_blob_id: str\n\t    output_blob_id: str\n\t    input_cache_hit: bool\n", "    output_cache_hit: bool\n\t    @staticmethod\n\t    def from_json(data: JsonObject) -> 'Job':\n\t        data['created'] = datetime.strptime(data['created'], '%Y-%m-%dT%H:%M:%S.%fZ')\n\t        data['status'] = JobStatus(data['status'])\n\t        return Job(**data)"]}
{"filename": "sbi_web_api_py/job_status.py", "chunked_list": ["from enum import Enum\n\tclass JobStatus(Enum):\n\t    NEW = 'New'\n\t    STARTED = 'Started'\n\t    READY = 'Ready'\n\t    FINISHED = 'Finished'\n\t    ABANDONED = 'Abandoned'\n\t    FAILED = 'Failed'\n\t    def worker_done(self) -> bool:\n\t        if self == JobStatus.NEW or self == JobStatus.STARTED:\n", "            return False\n\t        else:\n\t            return True\n\t    def failed(self) -> bool:\n\t        if self == JobStatus.FAILED or self == JobStatus.ABANDONED:\n\t            return True\n\t        else:\n\t            return False\n"]}
