{"filename": "pubgpt/vllm_test.py", "chunked_list": ["from vllm import LLM, SamplingParams\n\tprompts = [\n\t    \"Hello, my name is\",\n\t    \"The president of the United States is\",\n\t    \"The capital of France is\",\n\t    \"The future of AI is\",\n\t]\n\tsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n\tllm = LLM(model=\"facebook/opt-125m\")\n\toutputs = llm.generate(prompts, sampling_params)\n", "# Print the outputs.\n\tfor output in outputs:\n\t    prompt = output.prompt\n\t    generated_text = output.outputs[0].text\n\t    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n"]}
{"filename": "pubgpt/app.py", "chunked_list": ["import streamlit as st\n\tfrom online_parser.article_parser import (\n\t    parse_article,\n\t    extract_genes_and_diseases,\n\t    extract_mesh_terms,\n\t    extract_other_terms,\n\t)\n\tfrom pdf_parser.utils import read_pdf, extract_pdf_content, split_pdf_content\n\t## PDF PARSER\n\t# from pdf_parser.openai import create_embeddings_openai, retriever_openai\n", "# from pdf_parser.cohere import create_embeddings_cohere, retriever_cohere\n\tfrom pdf_parser.starcoder import create_embeddings, retriever\n\t## LLM\n\t# from llm.openai import get_associations, summarize\n\t# from llm_parser.cohere import get_associations, summarize\n\t# from llm_parser.starcoder import get_associations, summarize\n\tfrom llm_parser.falcon import get_associations, summarize\n\tst.set_page_config(page_title=\"PubGPT\", initial_sidebar_state=\"auto\")\n\thide_streamlit_style = \"\"\"\n\t            <style>\n", "            footer {visibility: hidden;}          \n\t            </style>\n\t            \"\"\"\n\tst.markdown(hide_streamlit_style, unsafe_allow_html=True)\n\tst.title(\"PubGPT 💉📄\")\n\t@st.cache_data\n\tdef convert_df(df):\n\t    return df.to_csv().encode(\"utf-8\")\n\tdef online_parser():\n\t    st.markdown(\"\"\"## Online parser\"\"\")\n", "    pubmed_id = st.text_input(\"PubMed ID\", \"32819603\")\n\t    parse_paper = st.button(\"Parse paper\")\n\t    if parse_paper:\n\t        paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n\t        st.write(f\"Paper ID: {paper_id}\")\n\t        st.write(f\"Title: {title}\")\n\t        st.write(f\"Abstract: {abstract}\")\n\t        gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n\t        mesh_terms = extract_mesh_terms(pubmed_id=pubmed_id)\n\t        other_terms = extract_other_terms(pubmed_id=pubmed_id)\n", "        first_row = st.columns(2)\n\t        first_row[0].markdown(\"### Genes\")\n\t        first_row[0].dataframe(gene_df)\n\t        st.download_button(\n\t            label=\"Download genes as CSV\",\n\t            data=convert_df(df=gene_df),\n\t            file_name=f\"{pubmed_id}_genes.csv\",\n\t        )\n\t        first_row[1].markdown(\"### Diseases\")\n\t        first_row[1].dataframe(disease_df)\n", "        st.download_button(\n\t            label=\"Download diseases as CSV\",\n\t            data=convert_df(df=disease_df),\n\t            file_name=f\"{pubmed_id}_diseases.csv\",\n\t        )\n\t        second_row = st.columns(2)\n\t        if mesh_terms is not None:\n\t            second_row[0].markdown(\"### MeSH terms\")\n\t            second_row[0].dataframe(mesh_terms)\n\t            st.download_button(\n", "                label=\"Download MeSH terms as CSV\",\n\t                data=convert_df(df=mesh_terms),\n\t                file_name=f\"{pubmed_id}_mesh_terms.csv\",\n\t            )\n\t        if other_terms is not None:\n\t            second_row[1].markdown(\"### Other terms\")\n\t            second_row[1].dataframe(other_terms)\n\t            st.download_button(\n\t                label=\"Download other terms as CSV\",\n\t                data=convert_df(df=other_terms),\n", "                file_name=f\"{pubmed_id}_other_terms.csv\",\n\t            )\n\t    extract_associations = st.button(\"Extract associations between genes and diseases\")\n\t    st.warning(\"May produce incorrect informations\", icon=\"⚠️\")\n\t    if extract_associations:\n\t        paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n\t        gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n\t        result_cohere = get_associations(\n\t            document=document, pubmed_id=pubmed_id, pairs=pairs\n\t        )\n", "        st.write(result_cohere)\n\tdef local_parser():\n\t    st.markdown(\"\"\"## Local parser\"\"\")\n\t    uploaded_file = st.file_uploader(\"Choose a file\")\n\t    if uploaded_file:\n\t        pdf = read_pdf(uploaded_file)\n\t        pdf_content = extract_pdf_content(pdf=pdf)\n\t        splitted_text_from_pdf = split_pdf_content(\n\t            pdf_content=pdf_content, chunk_size=1000, chunk_overlap=200\n\t        )\n", "        embeddings = create_embeddings(splitted_text_from_pdf=splitted_text_from_pdf)\n\t        query = st.text_input(\n\t            \"Insert query here:\",\n\t            \"Es: Is BRCA1 associated with breast cancer?\",\n\t        )\n\t        if st.button(\"Query document\"):\n\t            st.write(retriever(query=query, embeddings=embeddings))\n\tif __name__ == \"__main__\":\n\t    online_parser()\n\t    local_parser()\n"]}
{"filename": "pubgpt/local.py", "chunked_list": ["from pdf_parser.utils import (\n\t    read_pdf,\n\t    extract_pdf_content,\n\t    split_pdf_content,\n\t)\n\t# from pdf_parser.openai import create_embeddings_openai, retriever_openai\n\tfrom pdf_parser.cohere import create_embeddings_cohere, retriever_cohere\n\tif __name__ == \"__main__\":\n\t    pdf_path = \"input/Breast cancer genes: beyond BRCA1 and BRCA2.pdf\"\n\t    query = \"Is BRCA1 associated with breast cancer?\"\n", "    pdf = read_pdf(pdf_path)\n\t    pdf_content = extract_pdf_content(pdf=pdf)\n\t    splitted_text_from_pdf = split_pdf_content(\n\t        pdf_content=pdf_content, chunk_size=1000, chunk_overlap=200\n\t    )\n\t    ##########################################################\n\t    # embeddings = create_embeddings_openai(splitted_text_from_pdf=splitted_text_from_pdf)\n\t    # print(retriever_openai(query=query, embeddings=embeddings))\n\t    embeddings = create_embeddings_cohere(splitted_text_from_pdf=splitted_text_from_pdf)\n\t    print(retriever_cohere(query=query, embeddings=embeddings))\n"]}
{"filename": "pubgpt/__init__.py", "chunked_list": []}
{"filename": "pubgpt/online.py", "chunked_list": ["from online_parser.article_parser import (\n\t    parse_article,\n\t    extract_genes_and_diseases,\n\t    extract_mesh_terms,\n\t    extract_other_terms,\n\t)\n\t# from llm.openai import get_associations, summarize\n\t# from llm_parser.cohere import get_associations, summarize\n\t# from llm_parser.starcoder import get_associations, summarize\n\t# from llm_parser.falcon import get_associations, summarize\n", "from llm_parser.llama2 import get_associations, summarize\n\tif __name__ == \"__main__\":\n\t    pubmed_id = \"32819603\"\n\t    paper_id, title, abstract, document = parse_article(pubmed_id=pubmed_id)\n\t    gene_df, disease_df, pairs = extract_genes_and_diseases(pubmed_id=pubmed_id)\n\t    mesh_terms = extract_mesh_terms(pubmed_id=pubmed_id)\n\t    other_terms = extract_other_terms(pubmed_id=pubmed_id)\n\t    #####################################################\n\t    associations = get_associations(document=document, pubmed_id=pubmed_id, pairs=pairs)\n\t    digest = summarize(document=document, pubmed_id=pubmed_id)\n"]}
{"filename": "pubgpt/utils/__init__.py", "chunked_list": []}
{"filename": "pubgpt/utils/utils.py", "chunked_list": ["import pathlib\n\tdef create_id_folder(pubmed_id: str) -> None:\n\t    pathlib.Path(f\"output/{pubmed_id}\").mkdir(parents=True, exist_ok=True)\n"]}
{"filename": "pubgpt/llm_parser/llama2.py", "chunked_list": ["from typing import List, Tuple\n\tfrom dotenv import load_dotenv\n\timport requests\n\timport os\n\tload_dotenv()\n\tdef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n\t    \"\"\"\n\t    Get associations using Llama-2 LLM.\n\t    Args:\n\t        document (str): Text (abstract or full text)\n", "        pubmed_id (str): PubMed ID\n\t        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\t    Returns:\n\t        str: Response\n\t    \"\"\"\n\t    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n\t    pre_prompt: list = []\n\t    for index, item in enumerate(pairs, 1):\n\t        pre_prompt.append(\n\t            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n", "        )\n\t    pre_prompt = \"\\n\".join(pre_prompt)\n\t    prompt = f\"\"\"\n\t    According to this abstract:\\n\n\t{document.strip()}\\n\n\tCan you tell me if:\\n\n\t{pre_prompt.strip()}\\n\n\tAs result, provide me only CSV with:\n\t- Boolean result (only 'Yes' or 'No')\n\t- The entire part before the sentence \"is associated with\"\n", "- The entire part after the sentence \"is associated with\"\n\tFor instance:\n\t'Yes,X,Y'\n\tAlso, remove the numbers list (like 1)) from the CSV\n\t    \"\"\".strip()\n\t    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n\t    api_url: str = (\n\t        \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b-chat-hf\"\n\t    )\n\t    response = requests.post(\n", "        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n\t    )\n\t    print(response)\n\t    print(response.text)\n\t    result: str = response.json()[0][\"generated_text\"]\n\t    with open(f\"output/{pubmed_id}/starcoder_results.csv\", \"w\") as f:\n\t        f.write(\"result,gene,disease\")\n\t        f.write(result)\n\t    return result\n\tdef summarize(document: str, pubmed_id: str) -> str:\n", "    \"\"\"\n\t    Summarize the paper.\n\t    Args:\n\t        document (str): Text (abstract or full text)\n\t        pubmed_id (str): PubMed ID\n\t    Returns:\n\t        str: Digest\n\t    \"\"\"\n\t    prompt = f\"\"\"\n\tSummarize this text, trying to keep all relevant informations:\n", "{document.strip()}\n\t    \"\"\"\n\t    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n\t    api_url: str = \"https://api-inference.huggingface.co/models/meta-llama/Llama-2-7b\"\n\t    response = requests.post(\n\t        api_url,\n\t        headers=headers,\n\t        json={\"inputs\": f\"{prompt}\", \"max_tokens\": 1024},\n\t        timeout=60,\n\t    )\n", "    result: str = response.json()[0][\"generated_text\"]\n\t    with open(f\"output/{pubmed_id}/llama2_digest.txt\", \"w\") as f:\n\t        f.write(result)\n\t    return result\n"]}
{"filename": "pubgpt/llm_parser/falcon.py", "chunked_list": ["from typing import List, Tuple\n\tfrom dotenv import load_dotenv\n\timport requests\n\timport os\n\tload_dotenv()\n\tdef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n\t    \"\"\"\n\t    Get associations using Falcon LLM.\n\t    Args:\n\t        document (str): Text (abstract or full text)\n", "        pubmed_id (str): PubMed ID\n\t        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\t    Returns:\n\t        str: Response\n\t    \"\"\"\n\t    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n\t    pre_prompt: list = []\n\t    for index, item in enumerate(pairs, 1):\n\t        pre_prompt.append(\n\t            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n", "        )\n\t    pre_prompt = \"\\n\".join(pre_prompt)\n\t    prompt = f\"\"\"\n\t    According to this abstract:\\n\n\t{document.strip()}\\n\n\tCan you tell me if:\\n\n\t{pre_prompt.strip()}\\n\n\tAs result, provide me only CSV with:\n\t- Boolean result (only 'Yes' or 'No')\n\t- The entire part before the sentence \"is associated with\"\n", "- The entire part after the sentence \"is associated with\"\n\tFor instance:\n\t'Yes,X,Y'\n\tAlso, remove the numbers list (like 1)) from the CSV\n\t    \"\"\".strip()\n\t    headers = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n\t    api_url: str = (\n\t        \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n\t    )\n\t    response = requests.post(\n", "        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n\t    )\n\t    result = response.json()[0][\"generated_text\"]\n\t    with open(f\"output/{pubmed_id}/falcon_results.csv\", \"w\") as f:\n\t        f.write(\"result,gene,disease\")\n\t        f.write(result)\n\t    return result\n\tdef summarize(document: str, pubmed_id: str) -> str:\n\t    \"\"\"\n\t    Summarize the paper.\n", "    Args:\n\t        document (str): Text (abstract or full text)\n\t        pubmed_id (str): PubMed ID\n\t    Returns:\n\t        str: Digest\n\t    \"\"\"\n\t    prompt = f\"\"\"\n\tSummarize this text, trying to keep all relevant informations:\n\t{document.strip()}\n\t    \"\"\"\n", "    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n\t    api_url: str = (\n\t        \"https://api-inference.huggingface.co/models/tiiuae/falcon-7b-instruct\"\n\t    )\n\t    response = requests.post(\n\t        api_url,\n\t        headers=headers,\n\t        json={\"inputs\": f\"{prompt}\", \"max_tokens\": 1024},\n\t        timeout=60,\n\t    )\n", "    result: str = response.json()[0][\"generated_text\"]\n\t    with open(f\"output/{pubmed_id}/falcon_digest.txt\", \"w\") as f:\n\t        f.write(result)\n\t    return result\n"]}
{"filename": "pubgpt/llm_parser/openai.py", "chunked_list": ["from typing import List, Tuple\n\tfrom dotenv import load_dotenv\n\timport openai\n\timport os\n\tload_dotenv()\n\tdef get_associations(\n\t    document: str, pubmed_id: str, pairs: List[Tuple[str, str]]\n\t) -> str:\n\t    \"\"\"\n\t    Get associations using OpenAI LLM.\n", "    Args:\n\t        document (str): Text (abstract or full text)\n\t        pubmed_id (str): PubMed ID\n\t        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\t    Returns:\n\t        str: Response\n\t    \"\"\"\n\t    temperature, frequency_penalty, presence_penalty, max_tokens, top_p, engine = (\n\t        0,\n\t        0,\n", "        0,\n\t        500,\n\t        1,\n\t        \"gpt-3.5-turbo\",\n\t    )\n\t    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n\t    pre_prompt: list = []\n\t    for index, item in enumerate(pairs, 1):\n\t        pre_prompt.append(\n\t            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n", "        )\n\t    pre_prompt = \"\\n\".join(pre_prompt)\n\t    prompt = f\"\"\"\n\t    According to this abstract:\\n\n\t{document.strip()}\\n\n\tCan you tell me if:\\n\n\t{pre_prompt.strip()}\\n\n\tAs result, provide me only CSV with:\n\t- Boolean result (only 'Yes' or 'No')\n\t- The entire part before the sentence \"is associated with\"\n", "- The entire part after the sentence \"is associated with\"\n\tFor instance:\n\t'Yes,X,Y'\n\tAlso, remove the numbers list (like 1)) from the CSV\n\t    \"\"\".strip()\n\t    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\t    response = openai.Completion.create(\n\t        engine=engine,\n\t        prompt=prompt,\n\t        temperature=temperature,\n", "        max_tokens=max_tokens,\n\t        top_p=top_p,\n\t        frequency_penalty=frequency_penalty,\n\t        presence_penalty=presence_penalty,\n\t    )[\"choices\"]\n\t    with open(f\"output/{pubmed_id}/openai_results.csv\", \"w\") as f:\n\t        f.write(\"result,gene,disease\")\n\t        f.write(response)\n\t    return response\n"]}
{"filename": "pubgpt/llm_parser/__init__.py", "chunked_list": []}
{"filename": "pubgpt/llm_parser/cohere.py", "chunked_list": ["from typing import List, Tuple\n\tfrom dotenv import load_dotenv\n\timport cohere\n\timport os\n\tload_dotenv()\n\tdef get_associations(\n\t    document: str, pubmed_id: str, pairs: List[Tuple[str, str]]\n\t) -> str:\n\t    \"\"\"\n\t    Get associations using Cohere LLM.\n", "    Args:\n\t        document (str): Text (abstract or full text)\n\t        pubmed_id (str): PubMed ID\n\t        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\t    Returns:\n\t        str: Response\n\t    \"\"\"\n\t    temperature, max_tokens = (0, 500)\n\t    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n\t    pre_prompt: list = []\n", "    for index, item in enumerate(pairs, 1):\n\t        pre_prompt.append(\n\t            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n\t        )\n\t    pre_prompt = \"\\n\".join(pre_prompt)\n\t    prompt = f\"\"\"\n\t    According to this abstract:\\n\n\t{document.strip()}\\n\n\tCan you tell me if:\\n\n\t{pre_prompt.strip()}\\n\n", "As result, provide me only CSV with:\n\t- Boolean result (only 'Yes' or 'No')\n\t- The entire part before the sentence \"is associated with\"\n\t- The entire part after the sentence \"is associated with\"\n\tFor instance:\n\t'Yes,X,Y'\n\tAlso, remove the numbers list (like 1)) from the CSV\n\t    \"\"\".strip()\n\t    co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n\t    response = (\n", "        co.generate(\n\t            model=\"command-xlarge-nightly\",\n\t            prompt=prompt,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t        )\n\t        .generations[0]\n\t        .text\n\t    )\n\t    with open(f\"output/{pubmed_id}/cohere_results.csv\", \"w\") as f:\n", "        f.write(\"result,gene,disease\")\n\t        f.write(response)\n\t    return response\n\tdef summarize(document: str, pubmed_id: str) -> str:\n\t    prompt = f\"\"\"\n\tSummarize this text, trying to keep all relevant informations:\n\t{document.strip()}\n\t    \"\"\"\n\t    co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))\n\t    temperature, max_tokens = (0, 500)\n", "    response = (\n\t        co.generate(\n\t            model=\"command-xlarge-nightly\",\n\t            prompt=prompt,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t        )\n\t        .generations[0]\n\t        .text\n\t    )\n", "    with open(f\"output/{pubmed_id}/cohere_digest.txt\", \"w\") as f:\n\t        f.write(response)\n\t    return response\n"]}
{"filename": "pubgpt/llm_parser/starcoder.py", "chunked_list": ["from typing import List, Tuple\n\tfrom dotenv import load_dotenv\n\timport requests\n\timport os\n\tload_dotenv()\n\tdef get_associations(document: str, pubmed_id: str, pairs: List[Tuple[str, str]]):\n\t    \"\"\"\n\t    Get associations using Starcoder LLM.\n\t    Args:\n\t        document (str): Text (abstract or full text)\n", "        pubmed_id (str): PubMed ID\n\t        pairs (List[Tuple[str, str]]): Gene-disease pairs\n\t    Returns:\n\t        str: Response\n\t    \"\"\"\n\t    gene_id, disease_id, disease_umls = ([] for _ in range(3))\n\t    pre_prompt: list = []\n\t    for index, item in enumerate(pairs, 1):\n\t        pre_prompt.append(\n\t            f\"{index}) {item[0][0].strip()} associated with {item[1][0].strip()}?\"\n", "        )\n\t    pre_prompt = \"\\n\".join(pre_prompt)\n\t    prompt = f\"\"\"\n\t    According to this abstract:\\n\n\t{document.strip()}\\n\n\tCan you tell me if:\\n\n\t{pre_prompt.strip()}\\n\n\tAs result, provide me only CSV with:\n\t- Boolean result (only 'Yes' or 'No')\n\t- The entire part before the sentence \"is associated with\"\n", "- The entire part after the sentence \"is associated with\"\n\tFor instance:\n\t'Yes,X,Y'\n\tAlso, remove the numbers list (like 1)) from the CSV\n\t    \"\"\".strip()\n\t    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n\t    api_url: str = \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n\t    response = requests.post(\n\t        api_url, headers=headers, json={\"inputs\": f\"{prompt}\"}, timeout=60\n\t    )\n", "    result: str = response.json()[0][\"generated_text\"]\n\t    with open(f\"output/{pubmed_id}/starcoder_results.csv\", \"w\") as f:\n\t        f.write(\"result,gene,disease\")\n\t        f.write(result)\n\t    return result\n\tdef summarize(document: str, pubmed_id: str) -> str:\n\t    \"\"\"\n\t    Summarize the paper.\n\t    Args:\n\t        document (str): Text (abstract or full text)\n", "        pubmed_id (str): PubMed ID\n\t    Returns:\n\t        str: Digest\n\t    \"\"\"\n\t    prompt = f\"\"\"\n\tSummarize this text, trying to keep all relevant informations:\n\t{document.strip()}\n\t    \"\"\"\n\t    headers: dict = {\"Authorization\": f\"Bearer {os.getenv('HUGGINGFACE_API_KEY')}\"}\n\t    api_url: str = \"https://api-inference.huggingface.co/models/bigcode/starcoder\"\n", "    response = requests.post(\n\t        api_url,\n\t        headers=headers,\n\t        json={\"inputs\": f\"{prompt}\", \"max_tokens\": 1024},\n\t        timeout=60,\n\t    )\n\t    result: str = response.json()[0][\"generated_text\"]\n\t    with open(f\"output/{pubmed_id}/starcoder_digest.txt\", \"w\") as f:\n\t        f.write(result)\n\t    return result\n"]}
{"filename": "pubgpt/pdf_parser/openai.py", "chunked_list": ["from langchain.embeddings import OpenAIEmbeddings\n\tfrom langchain.vectorstores import FAISS\n\tfrom langchain.chains import RetrievalQA\n\tfrom langchain.chains.question_answering import load_qa_chain\n\tfrom langchain.llms import OpenAI\n\tfrom typing import List, Any\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tdef create_embeddings_openai(splitted_text_from_pdf: List) -> Any:\n\t    \"\"\"\n", "    Create embeddings from chunks for OpenAI.\n\t    Args:\n\t        splitted_text_from_pdf (List): List of chunks\n\t    Returns:\n\t        Any: Embeddings\n\t    \"\"\"\n\t    embeddings = OpenAIEmbeddings()\n\t    documents = FAISS.from_texts(texts=splitted_text_from_pdf, embedding=embeddings)\n\t    return documents\n\tdef create_opeanai_chain(query: str, embeddings: Any) -> None:\n", "    \"\"\"\n\t    Create chain for OpenAI.\n\t    Args:\n\t        query (str): Query\n\t        embeddings (Any): Embeddings\n\t    \"\"\"\n\t    chain = load_qa_chain(llm=OpenAI(), chain_type=\"stuff\")\n\t    docs = embeddings.similarity_search(query)\n\t    chain.run(input_documents=docs, question=query)\n\tdef retriever_openai(query: str, embeddings: Any) -> str:\n", "    \"\"\"\n\t    Create retriever for OpenAI.\n\t    Args:\n\t        query (str): Query\n\t        embeddings (Any): Embeddings\n\t    Returns:\n\t        str: Result of retriever\n\t    \"\"\"\n\t    retriever = embeddings.as_retriever(search_type=\"similarity\")\n\t    result = RetrievalQA.from_chain_type(\n", "        llm=OpenAI,\n\t        chain_type=\"stuff\",\n\t        retriever=retriever,\n\t        return_source_documents=True,\n\t    )\n\t    return result(query)[\"result\"]\n"]}
{"filename": "pubgpt/pdf_parser/__init__.py", "chunked_list": []}
{"filename": "pubgpt/pdf_parser/utils.py", "chunked_list": ["from PyPDF2 import PdfReader\n\tfrom langchain.text_splitter import CharacterTextSplitter\n\tfrom typing import List\n\tdef read_pdf(pdf_path: str) -> None:\n\t    \"\"\"\n\t    Read a local pdf.\n\t    Args:\n\t        pdf_path (str): PDF path\n\t    Returns:\n\t        PdfReader: PdfReader\n", "    \"\"\"\n\t    doc_reader = PdfReader(pdf_path)\n\t    return doc_reader\n\tdef extract_pdf_content(pdf: PdfReader) -> str:\n\t    \"\"\"\n\t    Extract the content of the PDF.\n\t    Args:\n\t        pdf (PdfReader): PDF as PdfReader\n\t    Returns:\n\t        str: Raw content of the PDF\n", "    \"\"\"\n\t    raw_text: str = \"\"\n\t    for index, page in enumerate(pdf.pages):\n\t        text = page.extract_text()\n\t        if text:\n\t            raw_text += text\n\t    return raw_text\n\tdef split_pdf_content(pdf_content: str, chunk_size: int, chunk_overlap: int) -> List:\n\t    \"\"\"\n\t    Split the content of the PDF into chunks.\n", "    Args:\n\t        pdf_content (str): Raw content of the PDF\n\t        chunk_size (int): Dimension of each chunk\n\t        chunk_overlap (int): Dimension of the overlap for each chunk\n\t    Returns:\n\t        List: List of chunks\n\t    \"\"\"\n\t    text_splitter = CharacterTextSplitter(\n\t        separator=\"\\n\",\n\t        chunk_size=chunk_size,\n", "        chunk_overlap=chunk_overlap,\n\t        length_function=len,\n\t    )\n\t    return text_splitter.split_text(pdf_content)\n"]}
{"filename": "pubgpt/pdf_parser/cohere.py", "chunked_list": ["from langchain.embeddings import CohereEmbeddings\n\tfrom langchain.vectorstores import FAISS\n\tfrom langchain.chains import RetrievalQA\n\tfrom langchain.chains.question_answering import load_qa_chain\n\tfrom langchain.llms import Cohere\n\tfrom typing import List, Any\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tdef create_embeddings(splitted_text_from_pdf: List) -> Any:\n\t    \"\"\"\n", "    Create embeddings from chunks for Cohere.\n\t    Args:\n\t        splitted_text_from_pdf (List): List of chunks\n\t    Returns:\n\t        Any: Embeddings\n\t    \"\"\"\n\t    embeddings = CohereEmbeddings()\n\t    documents = FAISS.from_texts(texts=splitted_text_from_pdf, embedding=embeddings)\n\t    return documents\n\tdef create_chain(query: str, embeddings: Any) -> None:\n", "    \"\"\"\n\t    Create chain for Cohere.\n\t    Args:\n\t        query (str): Query\n\t        embeddings (Any): Embeddings\n\t    \"\"\"\n\t    chain = load_qa_chain(llm=Cohere(), chain_type=\"stuff\")\n\t    docs = embeddings.similarity_search(query)\n\t    chain.run(input_documents=docs, question=query)\n\tdef retriver(query: str, embeddings: Any) -> str:\n", "    \"\"\"\n\t    Create retriever for Cohere.\n\t    Args:\n\t        query (str): Query\n\t        embeddings (Any): Embeddings\n\t    Returns:\n\t        str: Result of retriever\n\t    \"\"\"\n\t    retriever = embeddings.as_retriever(search_type=\"similarity\")\n\t    result = RetrievalQA.from_chain_type(\n", "        llm=Cohere(),\n\t        chain_type=\"stuff\",\n\t        retriever=retriever,\n\t        return_source_documents=True,\n\t    )\n\t    return result(query)[\"result\"]\n"]}
{"filename": "pubgpt/pdf_parser/starcoder.py", "chunked_list": ["from langchain.embeddings import HuggingFaceHubEmbeddings\n\tfrom langchain.vectorstores import FAISS\n\tfrom langchain.chains import RetrievalQA\n\tfrom langchain.chains.question_answering import load_qa_chain\n\tfrom langchain.llms import HuggingFaceHub\n\tfrom typing import List, Any\n\tfrom dotenv import load_dotenv\n\timport os\n\tload_dotenv()\n\tdef create_embeddings(splitted_text_from_pdf: List) -> Any:\n", "    \"\"\"\n\t    Create embeddings from chunks for Starcoder.\n\t    Args:\n\t        splitted_text_from_pdf (List): List of chunks\n\t    Returns:\n\t        Any: Embeddings\n\t    \"\"\"\n\t    embeddings = HuggingFaceHubEmbeddings(\n\t        repo_id=\"sentence-transformers/all-mpnet-base-v2\",\n\t        huggingfacehug_api_key=os.getenv(\"HUGGINFACE_API_KEY\"),\n", "    )\n\t    documents = FAISS.from_texts(texts=splitted_text_from_pdf, embedding=embeddings)\n\t    return documents\n\tdef create_chain(query: str, embeddings: Any) -> None:\n\t    \"\"\"\n\t    Create chain for Starcoder.\n\t    Args:\n\t        query (str): Query\n\t        embeddings (Any): Embeddings\n\t    \"\"\"\n", "    chain = load_qa_chain(\n\t        llm=HuggingFaceHub(\n\t            repo_id=\"bigcode/starcoder\",\n\t            huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n\t        ),\n\t        chain_type=\"stuff\",\n\t    )\n\t    docs = embeddings.similarity_search(query)\n\t    chain.run(input_documents=docs, question=query)\n\tdef retriever(query: str, embeddings: Any) -> str:\n", "    \"\"\"\n\t    Create retriever for Cohere.\n\t    Args:\n\t        query (str): Query\n\t        embeddings (Any): Embeddings\n\t    Returns:\n\t        str: Result of retriever\n\t    \"\"\"\n\t    retriever = embeddings.as_retriever(search_type=\"similarity\")\n\t    result = RetrievalQA.from_chain_type(\n", "        llm=HuggingFaceHub(\n\t            repo_id=\"bigcode/starcoder\",\n\t            huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\"),\n\t        ),\n\t        chain_type=\"stuff\",\n\t        retriever=retriever,\n\t        return_source_documents=True,\n\t    )\n\t    return result(query)[\"result\"]\n"]}
{"filename": "pubgpt/online_parser/__init__.py", "chunked_list": []}
{"filename": "pubgpt/online_parser/article_parser.py", "chunked_list": ["from typing import Tuple\n\tfrom Bio import Entrez, Medline\n\timport pandas as pd\n\timport numpy as np\n\timport requests\n\timport time\n\tfrom xml.etree import ElementTree\n\tfrom utils.utils import create_id_folder\n\tdef search_on_pubmed(pubmed_id: str) -> list:\n\t    \"\"\"\n", "    Search on PubMed using a PubMed ID.\n\t    Args:\n\t        pubmed_id (str): PubMed ID\n\t    Returns:\n\t        list: List of records of the article\n\t    \"\"\"\n\t    Entrez.email = \"random@example.com\"\n\t    handle = Entrez.esearch(\n\t        db=\"pubmed\", sort=\"relevance\", retmax=1, retmode=\"text\", term=pubmed_id\n\t    )\n", "    result = Entrez.read(handle)\n\t    ids = result[\"IdList\"]\n\t    handle = Entrez.efetch(\n\t        db=\"pubmed\", sort=\"relevance\", retmode=\"text\", rettype=\"medline\", id=ids\n\t    )\n\t    records = Medline.parse(handle)\n\t    return records\n\tdef parse_article(pubmed_id: str) -> Tuple[str, str, str, str]:\n\t    \"\"\"\n\t    Parse the article.\n", "    Args:\n\t        pubmed_id (str): PubMed ID\n\t    Returns:\n\t        Tuple[str, str, str, str]: PubMed ID, title, abstract, title+abstract\n\t    \"\"\"\n\t    create_id_folder(pubmed_id=pubmed_id)\n\t    for record in search_on_pubmed(pubmed_id=pubmed_id):\n\t        title = record.get(\"TI\", \"\")\n\t        abstract = record.get(\"AB\", \"\")\n\t        pubmed_id = record.get(\"PMID\", \"\")\n", "    document = title + \" \" + abstract\n\t    with open(f\"output/{pubmed_id}/document.txt\", \"w\") as f:\n\t        f.write(document)\n\t    return pubmed_id, title, abstract, document\n\tdef extract_mesh_terms(pubmed_id: str) -> pd.DataFrame:\n\t    \"\"\"\n\t    Extract MeSH terms from article.\n\t    Args:\n\t        pubmed_id (str): PubMed ID\n\t    Returns:\n", "        pd.DataFrame: DataFrame with MeSH terms\n\t    \"\"\"\n\t    create_id_folder(pubmed_id=pubmed_id)\n\t    for record in search_on_pubmed(pubmed_id=pubmed_id):\n\t        mesh_terms = record.get(\"MH\", \"\")\n\t    df = pd.DataFrame(data=zip(mesh_terms), columns=[\"element\"])\n\t    df.to_csv(f\"output/{pubmed_id}/mesh_terms.csv\", encoding=\"utf-8\", index=False)\n\tdef extract_other_terms(pubmed_id: str) -> pd.DataFrame:\n\t    \"\"\"\n\t    Extract other terms from article.\n", "    Args:\n\t        pubmed_id (str): PubMed ID\n\t    Returns:\n\t        pd.DataFrame: DataFrame with other terms\n\t    \"\"\"\n\t    create_id_folder(pubmed_id=pubmed_id)\n\t    for record in search_on_pubmed(pubmed_id=pubmed_id):\n\t        other_terms = record.get(\"OT\", \"\")\n\t    df = pd.DataFrame(data=zip(other_terms), columns=[\"element\"])\n\t    df.to_csv(f\"output/{pubmed_id}/other_terms.csv\", encoding=\"utf-8\", index=False)\n", "def extract_genes_and_diseases(pubmed_id: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n\t    \"\"\"\n\t    Extract genes and disease from article.\n\t    Args:\n\t        pubmed_id (str): PubMed ID\n\t    Returns:\n\t        Tuple[pd.DataFrame, pd.DataFrame]: DataFrame with genes and DataFrame with diseases\n\t    \"\"\"\n\t    create_id_folder(pubmed_id=pubmed_id)\n\t    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=gene,disease\"\n", "    response = requests.get(url)\n\t    time.sleep(0.5)\n\t    doc = ElementTree.fromstring(response.content)\n\t    tree = ElementTree.ElementTree(doc)\n\t    root = tree.getroot()\n\t    doc = root[3]\n\t    passage = doc[1:]\n\t    text_list, element_list, identifier_list, type_list = [], [], [], []\n\t    for i in passage:\n\t        for text in i.iterfind(\"text\"):\n", "            text = text.text\n\t            text_list.append(text)\n\t        for annotation in i.iterfind(\"annotation\"):\n\t            for text in annotation.iterfind(\"text\"):\n\t                element = text.text\n\t                element_list.append(element)\n\t            infos = annotation.findall(\"infon\")\n\t            try:\n\t                identifier = infos[0].text\n\t            except Exception:\n", "                identifier = \"\"\n\t            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n\t            try:\n\t                typex = infos[1].text\n\t            except Exception:\n\t                typex = infos[0].text\n\t            type_list.append(typex)\n\t    text_list = [i.strip() for i in text_list]\n\t    element_list = [i.strip() for i in element_list]\n\t    identifier_list = [i.strip() for i in identifier_list]\n", "    type_list = [i.strip() for i in type_list]\n\t    df = pd.DataFrame(\n\t        data=zip(element_list, identifier_list, type_list),\n\t        columns=[\"element\", \"identifier\", \"type\"],\n\t    )\n\t    df.identifier = df.identifier.replace(\"Disease\", np.nan)\n\t    df = df.drop_duplicates(\"identifier\")\n\t    disease_df = df[df.type == \"Disease\"]\n\t    gene_df = df[df.type == \"Gene\"]\n\t    disease_df = disease_df.drop(\"type\", axis=1)\n", "    gene_df = gene_df.drop(\"type\", axis=1)\n\t    genes_pairs = list(\n\t        gene_df[[\"element\", \"identifier\"]].itertuples(index=False, name=None)\n\t    )\n\t    disease_pairs = list(\n\t        disease_df[[\"element\", \"identifier\"]].itertuples(index=False, name=None)\n\t    )\n\t    pairs = list(set([(i, j) for i in genes_pairs for j in disease_pairs]))\n\t    gene_df.to_csv(f\"output/{pubmed_id}/genes.csv\", encoding=\"utf-8\", index=False)\n\t    disease_df.to_csv(f\"output/{pubmed_id}/diseases.csv\", encoding=\"utf-8\", index=False)\n", "    return gene_df, disease_df, pairs\n\tdef extract_chemicals(pubmed_id: str):\n\t    create_id_folder(pubmed_id=pubmed_id)\n\t    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=chemical\"\n\t    response = requests.get(url)\n\t    time.sleep(0.5)\n\t    doc = ElementTree.fromstring(response.content)\n\t    tree = ElementTree.ElementTree(doc)\n\t    root = tree.getroot()\n\t    doc = root[3]\n", "    passage = doc[1:]\n\t    text_list, element_list, identifier_list, type_list = [], [], [], []\n\t    for i in passage:\n\t        for text in i.iterfind(\"text\"):\n\t            text = text.text\n\t            text_list.append(text)\n\t        for annotation in i.iterfind(\"annotation\"):\n\t            for text in annotation.iterfind(\"text\"):\n\t                element = text.text\n\t                element_list.append(element)\n", "            infos = annotation.findall(\"infon\")\n\t            try:\n\t                identifier = infos[0].text\n\t            except Exception:\n\t                identifier = \"\"\n\t            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n\t            try:\n\t                typex = infos[1].text\n\t            except Exception:\n\t                typex = infos[0].text\n", "            type_list.append(typex)\n\t    text_list = [i.strip() for i in text_list]\n\t    element_list = [i.strip() for i in element_list]\n\t    identifier_list = [i.strip() for i in identifier_list]\n\t    type_list = [i.strip() for i in type_list]\n\t    df = pd.DataFrame(\n\t        data=zip(element_list, identifier_list, type_list),\n\t        columns=[\"element\", \"identifier\", \"type\"],\n\t    )\n\t    df[\"element\"] = df[\"element\"].astype(str)\n", "    df[\"identifier\"] = df[\"identifier\"].astype(str)\n\t    df[\"type\"] = df[\"type\"].astype(str)\n\t    df = df.drop_duplicates(\"identifier\")\n\t    df.to_csv(f\"output/{pubmed_id}/chemicals.csv\", sep=\",\", index=False)\n\t    return df\n\tdef extract_mutations(pubmed_id: str):\n\t    create_id_folder(pubmed_id=pubmed_id)\n\t    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=mutation\"\n\t    response = requests.get(url)\n\t    time.sleep(0.5)\n", "    doc = ElementTree.fromstring(response.content)\n\t    tree = ElementTree.ElementTree(doc)\n\t    root = tree.getroot()\n\t    doc = root[3]\n\t    passage = doc[1:]\n\t    text_list, element_list, identifier_list, type_list = [], [], [], []\n\t    for i in passage:\n\t        for text in i.iterfind(\"text\"):\n\t            text = text.text\n\t            text_list.append(text)\n", "        for annotation in i.iterfind(\"annotation\"):\n\t            for text in annotation.iterfind(\"text\"):\n\t                element = text.text\n\t                element_list.append(element)\n\t            infos = annotation.findall(\"infon\")\n\t            try:\n\t                identifier = infos[0].text\n\t            except Exception:\n\t                identifier = \"\"\n\t            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n", "            try:\n\t                typex = infos[1].text\n\t            except Exception:\n\t                typex = infos[0].text\n\t            type_list.append(typex)\n\t    text_list = [i.strip() for i in text_list]\n\t    element_list = [i.strip() for i in element_list]\n\t    identifier_list = [i.strip() for i in identifier_list]\n\t    type_list = [i.strip() for i in type_list]\n\t    df = pd.DataFrame(\n", "        data=zip(element_list, identifier_list, type_list),\n\t        columns=[\"element\", \"identifier\", \"type\"],\n\t    )\n\t    df[\"element\"] = df[\"element\"].astype(str)\n\t    df[\"identifier\"] = df[\"identifier\"].astype(str)\n\t    df[\"type\"] = df[\"type\"].astype(str)\n\t    df = df.drop_duplicates(\"identifier\")\n\t    df.to_csv(f\"output/{pubmed_id}/mutation.csv\", sep=\",\", index=False)\n\t    return df\n\tdef extract_species(pubmed_id: str):\n", "    create_id_folder(pubmed_id=pubmed_id)\n\t    url = f\"https://www.ncbi.nlm.nih.gov/research/pubtator-api/publications/export/biocxml?pmids={pubmed_id}&concepts=species\"\n\t    response = requests.get(url)\n\t    time.sleep(0.5)\n\t    doc = ElementTree.fromstring(response.content)\n\t    tree = ElementTree.ElementTree(doc)\n\t    root = tree.getroot()\n\t    doc = root[3]\n\t    passage = doc[1:]\n\t    text_list, element_list, identifier_list, type_list = [], [], [], []\n", "    for i in passage:\n\t        for text in i.iterfind(\"text\"):\n\t            text = text.text\n\t            text_list.append(text)\n\t        for annotation in i.iterfind(\"annotation\"):\n\t            for text in annotation.iterfind(\"text\"):\n\t                element = text.text\n\t                element_list.append(element)\n\t            infos = annotation.findall(\"infon\")\n\t            try:\n", "                identifier = infos[0].text\n\t            except Exception:\n\t                identifier = \"\"\n\t            identifier_list.append(identifier.replace(\"MESH:\", \"\"))\n\t            try:\n\t                typex = infos[1].text\n\t            except Exception:\n\t                typex = infos[0].text\n\t            type_list.append(typex)\n\t    text_list = [i.strip() for i in text_list]\n", "    element_list = [i.strip() for i in element_list]\n\t    identifier_list = [i.strip() for i in identifier_list]\n\t    type_list = [i.strip() for i in type_list]\n\t    df = pd.DataFrame(\n\t        data=zip(element_list, identifier_list, type_list),\n\t        columns=[\"element\", \"identifier\", \"type\"],\n\t    )\n\t    df[\"element\"] = df[\"element\"].astype(str)\n\t    df[\"identifier\"] = df[\"identifier\"].astype(str)\n\t    df[\"type\"] = df[\"type\"].astype(str)\n", "    df = df.drop_duplicates(\"identifier\")\n\t    df.to_csv(f\"output/{pubmed_id}/species.csv\", sep=\",\", index=False)\n\t    return df\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"PubGPT tests\"\"\"\n"]}
{"filename": "tests/llm_parser/test_falcon.py", "chunked_list": ["import unittest\n\tfrom pubgpt.llm_parser.falcon import get_associations\n\tclass TestGetAssociations(unittest.TestCase):\n\t    def test_get_associations(self):\n\t        document = \"This is a sample abstract.\"\n\t        pubmed_id = \"123456\"\n\t        pairs = [(\"GeneA\", \"DiseaseX\"), (\"GeneB\", \"DiseaseY\")]\n\t        result = get_associations(document, pubmed_id, pairs)\n\t        self.assertEqual(result, \"Yes,GeneA,DiseaseX\\nYes,GeneB,DiseaseY\")\n\tif __name__ == \"__main__\":\n", "    unittest.main()\n"]}
{"filename": "tests/llm_parser/__init__.py", "chunked_list": ["\"\"\"PubGPT tests\"\"\"\n"]}
{"filename": "tests/online_parser/__init__.py", "chunked_list": []}
{"filename": "tests/online_parser/test_article_parser.py", "chunked_list": []}
