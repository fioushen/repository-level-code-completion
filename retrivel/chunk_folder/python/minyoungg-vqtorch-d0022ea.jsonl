{"filename": "setup.py", "chunked_list": ["import setuptools\n\tsetuptools.setup(\n\t    name=\"vqtorch\",\n\t    packages=setuptools.find_packages(),\n\t    version=\"0.1.0\",\n\t    author=\"Minyoung Huh\",\n\t    author_email=\"minhuh@mit.edu\",\n\t    description=f\"vector-quantization for pytorch\",\n\t    url=\"git@github.com:minyoungg/vqtorch.git\",\n\t    classifiers=[\n", "        \"Programming Language :: Python :: 3\",\n\t        \"License :: OSI Approved :: MIT License\",\n\t        \"Operating System :: OS Independent\",\n\t    ],\n\t    install_requires=[\n\t            \"torch>=1.13.0\",\n\t            \"string-color==1.2.3\",\n\t            \"torchpq==0.3.0.1\",\n\t    ],\n\t    python_requires='>=3.6', # developed on 3.9 / 3.10\n", ")\n"]}
{"filename": "vqtorch/dists.py", "chunked_list": ["import math\n\timport time\n\timport warnings\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tdef check_shape(tensor, codebook):\n\t\tif len(tensor.shape) != 3:\n\t\t\traise RuntimeError(f'expected 3d tensor but found {tensor.size()}')\n", "\tif tensor.size(2) != codebook.size(1):\n\t\t\traise RuntimeError(\n\t\t\t\t\tf'expected tensor and codebook to have the same feature ' + \\\n\t\t\t\t\tf'dimensions but found: {tensor.size()} vs {codebook.size()}'\n\t\t\t\t\t)\n\t\treturn\n\tdef get_dist_fns(dist):\n\t\tif dist in ['euc', 'euclidean']:\n\t\t\tloss_fn = euclidean_distance\n\t\t\tdist_fn = euclidean_cdist_topk\n", "\telif dist in ['cos', 'cosine']:\n\t\t\tloss_fn = cosine_distance\n\t\t\tdist_fn = cosine_cdist_topk\n\t\telse:\n\t\t\traise ValueError(f'unknown distance method: {dist}')\n\t\treturn loss_fn, dist_fn\n\tdef cosine_distance(z, z_q):\n\t\t\"\"\"\n\t\tComputes element wise euclidean of z and z_q\n\t\tNOTE: the euclidean distance is not a true euclidean distance.\n", "\t\"\"\"\n\t\tz = F.normalize(z, p=2, dim=-1)\n\t\tz_q = F.normalize(z_q, p=2, dim=-1)\n\t\treturn euclidean_distance(z, z_q)\n\tdef euclidean_distance(z, z_q):\n\t\t\"\"\"\n\t\tComputes element wise euclidean of z and z_q\n\t\tNOTE: uses spatial averaging and no square root is applied. hence this is\n\t\tnot a true euclidean distance but makes no difference in practice.\n\t\t\"\"\"\n", "\tif z.size() != z_q.size():\n\t\t\traise RuntimeError(\n\t\t\t\t\t\tf'expected z and z_q to have the same shape but got ' + \\\n\t\t\t\t\t\tf'{z.size()} vs {z_q.size()}'\n\t\t\t\t\t\t)\n\t\tz, z_q = z.reshape(z.size(0), -1), z_q.reshape(z_q.size(0), -1)\n\t\treturn ((z_q - z) ** 2).mean(1) #.sqrt()\n\tdef euclidean_cdist_topk(tensor, codebook, compute_chunk_size=1024, topk=1,\n\t\t\t\t\t\t\t half_precision=False):\n\t\t\"\"\"\n", "\tCompute the euclidean distance between tensor and every element in the\n\t\tcodebook.\n\t\tArgs:\n\t\t\ttensor (Tensor): a 3D tensor of shape [batch x HWG x feats].\n\t\t\tcodebook (Tensor): a 2D tensor of shape [num_codes x feats].\n\t\t\tcompute_chunk_size (int): the chunk size to use when computing cdist.\n\t\t\ttopk (int): stores `topk` distance minimizers. If -1, topk is the\n\t\t\t\tsame length as the codebook\n\t\t\thalf_precision (bool): if True, matrix multiplication is computed\n\t\t\t\tusing half-precision to save memory.\n", "\tReturns:\n\t\t\td (Tensor): distance matrix of shape [batch x HWG x topk].\n\t\t\t\teach element is the distance of tensor[i] to every codebook.\n\t\t\tq (Tensor): code matrix of the same dimension as `d`. The index of the\n\t\t\t\tcorresponding topk distances.\n\t\tNOTE: Compute chunk only looks at tensor since optimal codebook size\n\t\tgenerally does not vary too much. In future versions, should consider\n\t\tcomputing chunk size while taking into consideration of codebook and\n\t\tfeature dimension size.\n\t\t\"\"\"\n", "\tcheck_shape(tensor, codebook)\n\t\tb, n, c = tensor.shape\n\t\ttensor_dtype = tensor.dtype\n\t\ttensor = tensor.reshape(-1, tensor.size(-1))\n\t\ttensor = tensor.split(compute_chunk_size)\n\t\tdq = []\n\t\tif topk == -1:\n\t\t\ttopk = codebook.size(0)\n\t\tfor i, tc in enumerate(tensor):\n\t\t\tcb = codebook\n", "\t\tif half_precision:\n\t\t\t\ttc = tc.half()\n\t\t\t\tcb = cb.half()\n\t\t\td = torch.cdist(tc, cb)\n\t\t\tdq.append(torch.topk(d, k=topk, largest=False, dim=-1))\n\t\td, q = torch.cat([_dq[0] for _dq in dq]), torch.cat([_dq[1] for _dq in dq])\n\t\treturn_dict = {'d': d.to(tensor_dtype).reshape(b, n, -1),\n\t\t\t\t\t   'q': q.long().reshape(b, n, -1)}\n\t\treturn return_dict\n\tdef cosine_cdist_topk(tensor, codebook, chunks=4, topk=1, half_precision=False):\n", "\t\"\"\" Computes cosine distance instead. see `euclidean_cdist_topk` \"\"\"\n\t\tcheck_shape(tensor, codebook, mask)\n\t\ttensor   = F.normalize(tensor,   p=2, dim=-1)\n\t\tcodebook = F.normalize(codebook, p=2, dim=-1)\n\t\td, q = euclidean_cdist_topk(tensor, codebook, chunks, topk, half_precision)\n\t\td = 0.5 * (d ** 2)\n\t\treturn d, q.long()\n"]}
{"filename": "vqtorch/__init__.py", "chunked_list": ["from vqtorch.nn import *\n\tfrom vqtorch.utils import *\n\tfrom vqtorch.math_fns import *\n"]}
{"filename": "vqtorch/utils.py", "chunked_list": ["import numpy as np\n\timport torch\n\tfrom vqtorch.nn import _VQBaseLayer\n\tdef is_vq(m):\n\t\t\"\"\" checks if the module is a VQ layer \"\"\"\n\t\treturn issubclass(type(m), _VQBaseLayer)\n\tdef is_vqn(net):\n\t\t\"\"\" checks if the network contains a VQ layer \"\"\"\n\t\treturn np.any([is_vq(layer) for layer in net.modules()])\n\tdef get_vq_layers(model):\n", "\t\"\"\" returns vq layers from a network \"\"\"\n\t\treturn [m for m in model.modules() if is_vq(m)]\n\tclass no_vq():\n\t    \"\"\"\n\t    Function to turn off VQ by setting all the VQ layers to identity function.\n\t    Examples::\n\t        >>> with vqtorch.no_vq():\n\t        ...     out = model(x)\n\t    \"\"\"\n\t    def __init__(self, modules):\n", "        if type(modules) is not list:\n\t            modules = [modules]\n\t        for module in modules:\n\t            if isinstance(module, torch.nn.DataParallel):\n\t                module = module.module\n\t            assert isinstance(module, torch.nn.Module), \\\n\t                f'expected input to be nn.Module or a list of nn.Module ' + \\\n\t                f'but found {type(module)}'\n\t            self.enable_vq(module, enable=False)\n\t        self.modules = modules\n", "        return\n\t    def __enter__(self):\n\t        pass\n\t    def __exit__(self, exception_type, exception_value, traceback):\n\t        for module in self.modules:\n\t            if isinstance(module, torch.nn.DataParallel):\n\t                module = module.module\n\t            self.enable_vq(module, enable=True)\n\t        return\n\t    def enable_vq(self, module, enable):\n", "        for m in module.modules():\n\t            if is_vq(m):\n\t                m.enabled = enable\n\t        return\n"]}
{"filename": "vqtorch/math_fns.py", "chunked_list": ["def entropy(x, dim=-1, eps=1e-8, keepdim=False):\n\t\tassert x.min() >= 0., \\\n\t\t\tf'function takes non-negative values but found x.min(): {x.min()}'\n\t\tis_tensor = True\n\t\tif len(x.shape) == 1:\n\t\t\tis_tensor = False\n\t\t\tx = x.unsqueeze(0)\n\t\tx = x.moveaxis(dim, -1)\n\t\tx_shape = x.shape\n\t\tx = x.view(-1, x.size(-1)) + eps\n", "\tp = x / x.sum(dim=1, keepdim=True)\n\t\th = - (p * p.log()).sum(dim=1, keepdim=True)\n\t\th = h.view(*x_shape[:-1], 1).moveaxis(dim, -1)\n\t\tif not keepdim:\n\t\t\th = h.squeeze(dim)\n\t\tif not is_tensor:\n\t\t\th = h.squeeze(0)\n\t\treturn h\n"]}
{"filename": "vqtorch/norms.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tMAXNORM_CONSTRAINT_VALUE = 10\n\tclass Normalize(nn.Module):\n\t    \"\"\"\n\t    Simple vector normalization module. By default, vectors are normalizes\n\t    along the channel dimesion. Each vector associated to the spatial\n\t    location is normalized. Used along with cosine-distance VQ layer.\n\t    \"\"\"\n", "    def __init__(self, p=2, dim=1, eps=1e-6):\n\t        super().__init__()\n\t        self.p = p\n\t        self.dim = dim\n\t        self.eps = eps\n\t        return\n\t    def forward(self, x):\n\t        return F.normalize(x, p=self.p, dim=self.dim, eps=self.eps)\n\tdef max_norm(w, p=2, dim=-1, max_norm=MAXNORM_CONSTRAINT_VALUE, eps=1e-8):\n\t    norm = w.norm(p=p, dim=dim, keepdim=True)\n", "    desired = torch.clamp(norm.data, max=max_norm)\n\t    # desired = torch.clamp(norm, max=max_norm)\n\t    return w * (desired / (norm + eps))\n\tclass MaxNormConstraint(nn.Module):\n\t    def __init__(self, max_norm=1, p=2, dim=-1, eps=1e-8):\n\t        super().__init__()\n\t        self.p = p\n\t        self.eps = eps\n\t        self.dim = dim\n\t        self.max_norm = max_norm\n", "    def forward(self, x):\n\t        return max_norm(x, self.p, self.dim, max_norm=self.max_norm)\n\t@torch.no_grad()\n\tdef with_codebook_normalization(func):\n\t    def wrapper(*args):\n\t        self = args[0]\n\t        for n, m in self.named_modules():\n\t            if isinstance(m, nn.Embedding):\n\t                if self.codebook_norm == 'l2':\n\t                    m.weight.data = max_norm(m.weight.data, p=2, dim=1, eps=1e-8)\n", "                elif self.codebook_norm == 'l2c':\n\t                    m.weight.data = F.normalize(m.weight.data, p=2, dim=1, eps=1e-8)\n\t        return func(*args)\n\t    return wrapper\n\tdef get_norm(norm, num_channels=None):\n\t    before_grouping = True\n\t    if norm == 'l2':\n\t        norm_layer = Normalize(p=2, dim=-1)\n\t        before_grouping = False\n\t    elif norm == 'l2c':\n", "        norm_layer = MaxNormConstraint(p=2, dim=-1, max_norm=MAXNORM_CONSTRAINT_VALUE)\n\t        before_grouping = False\n\t    elif norm == 'bn':\n\t        norm_layer = nn.BatchNorm2d(num_channels)\n\t    elif norm == 'gn':\n\t        norm_layer = GroupNorm(num_channels)\n\t    elif norm in ['none', None]:\n\t        norm_layer = nn.Identity()\n\t    elif norm == 'in':\n\t        norm_layer = nn.InstanceNorm2d(num_channels)\n", "    else:\n\t        raise ValueError(f'unknown norm {norm}')\n\t    return norm_layer, before_grouping\n\tdef GroupNorm(in_channels):\n\t    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\tdef match_norm(x, y, dim=-1, eps=1e-8):\n\t    \"\"\"\n\t    matches vector norm of x to that of y\n\t    Args:\n\t        x (Tensor): a tensor of any shape\n", "        y (Tensor): a tensor of the same shape as `x`.\n\t        dim (int): dimension to match the norm over\n\t        eps (float): epsilon to mitigate division by zero.\n\t    Returns:\n\t        `x` with the same norm as `y` across `dim`\n\t    \"\"\"\n\t    assert x.shape == y.shape, \\\n\t        f'expected `x` and `y` to have the same dim but found {x.shape} vs {y.shape}'\n\t    # move chosen dim to last dim\n\t    x = x.moveaxis(dim, -1).contiguous()\n", "    y = y.moveaxis(dim, -1).contiguous()\n\t    x_shape = x.shape\n\t    # unravel everything such that [GBHW X C]\n\t    # print(x.shape)\n\t    x = x.view(-1, x.size(-1))\n\t    y = y.view(-1, y.size(-1))\n\t    # compute norm on C\n\t    x_norm = torch.norm(x, p=2, dim=1, keepdim=True)\n\t    y_norm = torch.norm(y, p=2, dim=1, keepdim=True)\n\t    # clamp y_norm for division by 0\n", "    x_norm = torch.clamp(x_norm, min=eps)\n\t    # normalize (x now has same norm as y)\n\t    x = y_norm * (x / x_norm)\n\t    x = x.view(x_shape)\n\t    x = x.moveaxis(-1, dim).contiguous()\n\t    return x\n"]}
{"filename": "vqtorch/nn/gvq.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom stringcolor import cs\n\tfrom vqtorch.norms import with_codebook_normalization\n\tfrom .vq import VectorQuant\n\tclass GroupVectorQuant(VectorQuant):\n\t\t\"\"\"\n\t\tVector quantization layer.\n\t\tArgs:\n\t\t\tgroups (int): Number of groups for vector quantization. The vectors are divided\n", "\t\t\tinto group chunks. When groups=1, it is the same as VectorQuant.\n\t\t\tshare (bool): If True, codebook is shared for each sub-vector.\n\t\t\t*rest*: see VectorQuant()\n\t\t\"\"\"\n\t\tdef __init__(\n\t\t\t\tself,\n\t\t\t\tfeature_size : int,\n\t\t\t\tnum_codes : int,\n\t\t\t\tgroups : int = 1,\n\t\t\t\tshare : bool = True,\n", "\t\t\t**kwargs,\n\t\t\t\t):\n\t\t\tif not share and not feature_size % groups == 0:\n\t\t\t\te_msg = f'feature_size {self.feature_size} must be divisible by groups {groups}.'\n\t\t\t\traise RuntimeError(str(cs(e_msg, 'red')))\n\t\t\tnum_codebooks = 1 if share else groups\n\t\t\tin_dim  = self.group_size = num_codes // num_codebooks\n\t\t\tout_dim = feature_size // groups\n\t\t\tsuper().__init__(feature_size, num_codes, code_vector_size=out_dim, **kwargs)\n\t\t\tself.groups = groups\n", "\t\tself.share = share\n\t\t\tself.codebook = nn.Embedding(in_dim * num_codebooks, out_dim)\n\t\t\treturn\n\t\tdef get_codebook_by_group(self, group):\n\t\t\tcb = self.codebook.weight\n\t\t\toffset = 0 if self.share else group * self.group_size\n\t\t\treturn cb[offset : offset + self.group_size], offset\n\t\t@with_codebook_normalization\n\t\tdef forward(self, z):\n\t\t\t######\n", "\t\t## (1) formatting data by groups and invariant to dim\n\t\t\t######\n\t\t\tz = self.prepare_inputs(z, self.groups)\n\t\t\tif not self.enabled:\n\t\t\t\tz = self.to_original_format(z)\n\t\t\t\treturn z, {}\n\t\t\t######\n\t\t\t## (2) quantize latent vector\n\t\t\t######\n\t\t\tz_q = torch.zeros_like(z)\n", "\t\td = torch.zeros(z_q.shape[:-1]).to(z_q.device)\n\t\t\tq = torch.zeros(z_q.shape[:-1], dtype=torch.long).to(z_q.device)\n\t\t\tfor i in range(self.groups):\n\t\t\t\t# select group\n\t\t\t\t_z = z[..., i:i+1, :]\n\t\t\t\t# quantize\n\t\t\t\tcb, offset = self.get_codebook_by_group(i)\n\t\t\t\t_z_q, _d, _q = self.quantize(cb, _z)\n\t\t\t\t# assign to tensor\n\t\t\t\tz_q[..., i:i+1, :] = _z_q\n", "\t\t\td[..., i:i+1] = _d\n\t\t\t\tq[..., i:i+1] = _q + offset\n\t\t\tto_return = {\n\t\t\t\t'z'   : z,               # each group input z_e\n\t\t\t\t'z_q' : z_q,             # quantized output z_q\n\t\t\t\t'd'   : d,               # distance function for each group\n\t\t\t\t'q'\t  : q,\t\t\t\t # codes using offsetted indices\n\t\t\t\t'loss': self.compute_loss(z, z_q),\n\t\t\t\t'perplexity': None,\n\t\t\t\t}\n", "\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\t\tz_q = self.to_original_format(z_q)\n\t\t\treturn z_q, to_return\n"]}
{"filename": "vqtorch/nn/rvq.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom stringcolor import cs\n\tfrom vqtorch.norms import with_codebook_normalization\n\tfrom .vq import VectorQuant\n\tclass ResidualVectorQuant(VectorQuant):\n\t\t\"\"\"\n\t\tArgs\n\t\t\tgroups (int): Number of residual VQ to apply. When num_residual=1, \n\t\t\t\tlayer acts will be equivalent to VectorQuant.\n", "\t\tshare (bool): If True, codebook is shared for every quantization.\n\t\t\t*rest*: see VectorQuant()\n\t\tNOTE: Don't use L2 normalization on the codebook. ResidualVQ is norm sensitive.\n\t\t\tFor norm invariant, consider using cosine distance variant.\n\t\t\"\"\"\n\t\tdef __init__(\n\t\t\t\tself,\n\t\t\t\tfeature_size : int,\n\t\t\t\tnum_codes : int,\n\t\t\t\tgroups : int = 1,\n", "\t\t\tshare : bool = True,\n\t\t\t\t**kwargs,\n\t\t\t\t):\n\t\t\tif not share and not self.feature_size % groups == 0:\n\t\t\t\te_msg = f'feature_size {self.feature_size} must be divisible by num_residual {groups}.'\n\t\t\t\traise RuntimeError(str(cs(e_msg, 'red')))\n\t\t\tself.groups = groups\n\t\t\tself.share = share\n\t\t\tnum_codebooks = 1 if share else groups\n\t\t\tin_dim  = self.group_size = num_codes // num_codebooks\n", "\t\tout_dim = feature_size\n\t\t\tsuper().__init__(feature_size, num_codes, code_vector_size=out_dim, **kwargs)\n\t\t\tself.groups = groups\n\t\t\tself.share = share\n\t\t\tself.codebook = nn.Embedding(in_dim * num_codebooks, out_dim)\n\t\t\treturn\n\t\tdef get_codebook_by_group(self, group):\n\t\t\tcb = self.codebook.weight\n\t\t\toffset = 0 if self.share else group * self.group_size\n\t\t\treturn cb[offset : offset + self.group_size], offset\n", "\t@with_codebook_normalization\n\t\tdef forward(self, z):\n\t\t\t######\n\t\t\t## (1) formatting data by groups and invariant to dim\n\t\t\t######\n\t\t\tz = self.prepare_inputs(z, groups=1)\n\t\t\tif not self.enabled:\n\t\t\t\tz = self.to_original_format(z)\n\t\t\t\treturn z, {}\n\t\t\t######\n", "\t\t## (2) quantize latent vector\n\t\t\t######\n\t\t\tz_q = torch.zeros_like(z)\n\t\t\tz_res = torch.zeros(*z.shape[:-2], self.groups + 1, z.shape[-1]).to(z.device)\n\t\t\td = torch.zeros(z_q.shape[:-1]).to(z_q.device)\n\t\t\tq = torch.zeros(z_q.shape[:-1], dtype=torch.long).to(z_q.device)\n\t\t\tfor i in range(self.groups):\n\t\t\t\t# select group\n\t\t\t\t_z = (z - z_q) # compute resiudal\n\t\t\t\tz_res[..., i:i+1, :] = _z\n", "\t\t\t# quantize\n\t\t\t\tcb, offset = self.get_codebook_by_group(i)\n\t\t\t\t_z_q, _d, _q = self.quantize(cb, _z)\n\t\t\t\t# update estimate\n\t\t\t\tz_q = z_q + _z_q\n\t\t\t\t# assign to tensor\n\t\t\t\td[..., i:i+1] = _d\n\t\t\t\tq[..., i:i+1] = _q + offset\n\t\t\tz_res[..., -1:, :] = z - z_q\n\t\t\tto_return = {\n", "\t\t\t'z'    : z,               # each group input z_e\n\t\t\t\t'z_q'  : z_q,             # quantized output z_q\n\t\t\t\t'd'    : d,               # distance function for each group\n\t\t\t\t'q'\t  : q,\t\t\t\t  # codes using offsetted indices\n\t\t\t\t'z_res': z_res,\n\t\t\t\t'loss' : self.compute_loss(z, z_q),\n\t\t\t\t'perplexity': None,\n\t\t\t\t}\n\t\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\t\tz_q = self.to_original_format(z_q)\n", "\t\treturn z_q, to_return\n"]}
{"filename": "vqtorch/nn/__init__.py", "chunked_list": ["from .vq_base import _VQBaseLayer\n\tfrom .vq import VectorQuant\n\tfrom .gvq import GroupVectorQuant\n\tfrom .rvq import ResidualVectorQuant\n\tfrom .affine import AffineTransform\n\tfrom . import utils\n"]}
{"filename": "vqtorch/nn/affine.py", "chunked_list": ["import math\n\timport time\n\timport warnings\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass AffineTransform(nn.Module):\n\t\tdef __init__(\n\t\t\t\tself, \n", "\t\t\tfeature_size, \n\t\t\t\tuse_running_statistics=False, \n\t\t\t\tmomentum=0.1, \n\t\t\t\tlr_scale=1,\n\t\t\t\tnum_groups=1,\n\t\t\t\t):\n\t\t\tsuper().__init__()\n\t\t\tself.use_running_statistics = use_running_statistics\n\t\t\tself.num_groups = num_groups\n\t\t\tif use_running_statistics:\n", "\t\t\tself.momentum = momentum\n\t\t\t\tself.register_buffer('running_statistics_initialized', torch.zeros(1))\n\t\t\t\tself.register_buffer('running_ze_mean', torch.zeros(num_groups, feature_size))\n\t\t\t\tself.register_buffer('running_ze_var', torch.ones(num_groups, feature_size))\n\t\t\t\tself.register_buffer('running_c_mean', torch.zeros(num_groups, feature_size))\n\t\t\t\tself.register_buffer('running_c_var', torch.ones(num_groups, feature_size))\n\t\t\telse:\n\t\t\t\tself.scale = nn.parameter.Parameter(torch.zeros(num_groups, feature_size))\n\t\t\t\tself.bias = nn.parameter.Parameter(torch.zeros(num_groups, feature_size))\n\t\t\t\tself.lr_scale = lr_scale\n", "\t\treturn\n\t\t@torch.no_grad()\n\t\tdef update_running_statistics(self, z_e, c):\n\t\t\t# we find it helpful to often to make an under-estimation on the\n\t\t\t# z_e embedding statistics. Empirically we observe a slight\n\t\t\t# over-estimation of the statistics, causing the straight-through\n\t\t\t# estimation to grow indefinitely. While this is not an issue\n\t\t\t# for most model architecture, some model architectures that don't\n\t\t\t# have normalized bottlenecks, can cause it to eventually explode.\n\t        # placing the VQ layer in certain layers of ViT exhibits this behavior\n", "\t\tif self.training and self.use_running_statistics:\n\t\t\t\tunbiased = False\n\t\t\t\tze_mean = z_e.mean([0, 1]).unsqueeze(0)\n\t\t\t\tze_var = z_e.var([0, 1], unbiased=unbiased).unsqueeze(0)\n\t\t\t\tc_mean = c.mean([0]).unsqueeze(0)\n\t\t\t\tc_var = c.var([0], unbiased=unbiased).unsqueeze(0)\n\t\t\t\tif not self.running_statistics_initialized:\n\t\t\t\t\tself.running_ze_mean.data.copy_(ze_mean)\n\t\t\t\t\tself.running_ze_var.data.copy_(ze_var)\n\t\t\t\t\tself.running_c_mean.data.copy_(c_mean)\n", "\t\t\t\tself.running_c_var.data.copy_(c_var)\n\t\t\t\t\tself.running_statistics_initialized.fill_(1)\n\t\t\t\telse:\n\t\t\t\t\tself.running_ze_mean = (self.momentum * ze_mean) + (1 - self.momentum) * self.running_ze_mean\n\t\t\t\t\tself.running_ze_var = (self.momentum * ze_var) + (1 - self.momentum) * self.running_ze_var\n\t\t\t\t\tself.running_c_mean = (self.momentum * c_mean) + (1 - self.momentum) * self.running_c_mean\n\t\t\t\t\tself.running_c_var = (self.momentum * c_var) + (1 - self.momentum) * self.running_c_var\n\t\t\t# wd = 0.9998 # 0.995\n\t\t\t# self.running_ze_mean = wd * self.running_ze_mean\n\t\t\t# self.running_ze_var = wd * self.running_ze_var\n", "\t\treturn\n\t\tdef forward(self, codebook):\n\t\t\tscale, bias = self.get_affine_params()\n\t\t\tn, c = codebook.shape\n\t\t\tcodebook = codebook.view(self.num_groups, -1, codebook.shape[-1])\n\t\t\tcodebook = scale * codebook + bias\n\t\t\treturn codebook.reshape(n, c)\n\t\tdef get_affine_params(self):\n\t\t\tif self.use_running_statistics:\n\t\t\t\tscale = (self.running_ze_var / (self.running_c_var + 1e-8)).sqrt()\n", "\t\t\tbias = - scale * self.running_c_mean + self.running_ze_mean\n\t\t\telse:\n\t\t\t\tscale = (1. + self.lr_scale * self.scale)\n\t\t\t\tbias = self.lr_scale * self.bias\n\t\t\treturn scale.unsqueeze(1), bias.unsqueeze(1)\n"]}
{"filename": "vqtorch/nn/pool.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass _VecPool2d(nn.Module):\n\t    def __init__(self, weighting_fn, kernel_size=3, stride=2, padding=2, dilation=1):\n\t        super().__init__()\n\t        self.k = kernel_size\n\t        self.s = stride\n\t        self.p = padding\n\t        self.d = dilation\n", "        self.weighting_fn = weighting_fn\n\t        self.unfold = nn.Unfold(kernel_size=self.k, dilation=self.d, padding=self.p, stride=self.s)\n\t        return\n\t    def forward(self, x):\n\t        b, c, h, w = x.shape\n\t        out_h = (h - self.k + 2 * self.p) // self.s + 1\n\t        out_w = (w - self.k + 2 * self.p) // self.s + 1\n\t        with torch.no_grad():\n\t            n = x.norm(dim=1, p=2, keepdim=True)\n\t            n = self.unfold(n) # B x (K**2) x N\n", "            n = self.weighting_fn(n, dim=1).unsqueeze(1)\n\t        x = self.unfold(x)  # B x C * (K**2) x N\n\t        x = x.view(b, c, -1, x.size(-1))\n\t        x = n * x\n\t        x = x.sum(2).view(b, c, out_h, out_w)\n\t        return x\n\tclass MaxVecPool2d(_VecPool2d):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(MaxVecPool2d.max_onehot, *args, **kwargs)\n\t    @staticmethod\n", "    def max_onehot(x, dim):\n\t        b, s, n = x.shape\n\t        x = x.argmax(dim=1)\n\t        x = x.view(-1)\n\t        x = F.one_hot(x, s).view(b, n, s).swapaxes(-1, dim)\n\t        return x\n\tclass SoftMaxVecPool2d(_VecPool2d):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(torch.softmax, *args, **kwargs)\n"]}
{"filename": "vqtorch/nn/vq_base.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom vqtorch.norms import get_norm\n\tfrom vqtorch.nn.utils.init import data_dependent_init_forward_hook\n\tclass _VQBaseLayer(nn.Module):\n\t\t\"\"\"\n\t\tBase template code for vector quanitzation. All VQ layers will inherit\n\t\tfrom this class.\n\t\tArgs:\n\t\t\tfeature_size (int):\n", "\t\t\tThe size of the feature. this is the length of each\n\t\t\t\tcode vector. the dimensions must match the input feature size.\n\t\t\tnum_codes (int):\n\t\t\t\tNumber of codes to use in the codebook.\n\t\t\tdim (int): Dimension to quantize. by default quantization happens on\n\t\t\t\tthe channel dimension. For example, given an image tensor\n\t\t\t\t(B x C x H x W) and dim=1, the channels are treated as features \n\t\t\t\tand the resulting codes `q` has the shape (B x H x W). \n\t\t\t\tFor transformers (B x N x C), you should set dim=2 or -1.\n\t\t\tnorm (str): Feature normalization.\n", "\t\tcodebook_norm (str): Codebook normalization.\n\t\tReturns:\n\t\t\tQuantized vector z_q and return dict\n\t\tAttributes:\n\t\t\tcdist_chunk_size (int): chunk size for divide-and-conquer topk cdist.\n\t\t\tenabled (bool): If false, the model is not quantized and acts as an identity layer.\n\t\t\"\"\"\n\t\tcdist_chunk_size = 1024\n\t\tenabled = True\n\t\tdef __init__(\n", "\t\t\tself,\n\t\t\t\tfeature_size : int,\n\t\t\t\tnum_codes :\tint,\n\t\t\t\tdim : int = 1,\n\t\t\t\tnorm :\tstr = 'none',\n\t\t\t\tcb_norm : str = 'none',\n\t\t\t\tkmeans_init : bool = False,\n\t\t\t\tcode_vector_size : int = None,\n\t\t\t\t):\n\t\t\tsuper().__init__()\n", "\t\tself.feature_size = feature_size\n\t\t\tself.code_vector_size = feature_size if code_vector_size is None else code_vector_size\n\t\t\tself.num_codes = num_codes\n\t\t\tself.dim = dim\n\t\t\tself.groups = 1 # for group VQ\n\t\t\tself.topk = 1   # for probabilistic VQ\n\t\t\tself.norm = norm\n\t\t\tself.codebook_norm = cb_norm\n\t\t\tself.norm_layer, self.norm_before_grouping = get_norm(norm, feature_size)\n\t\t\tif kmeans_init:\n", "\t\t\tself.register_buffer('data_initialized', torch.zeros(1))\n\t\t\t\tself.register_forward_hook(data_dependent_init_forward_hook)\n\t\t\treturn\n\t\tdef quantize(self, codebook, z):\n\t\t\t\"\"\"\n\t\t\tQuantizes the latent codes z with the codebook\n\t\t\tArgs:\n\t\t\t\tcodebook (Tensor): B x C\n\t\t\t\tz (Tensor): B x ... x C\n\t\t\t\"\"\"\n", "\t\traise NotImplementedError\n\t\tdef compute_loss(self, z_e, z_q):\n\t\t\t\"\"\" computes error between z and z_q \"\"\"\n\t\t\traise NotImplementedError\n\t\tdef to_canonical_group_format(self, z, groups):\n\t\t\t\"\"\"\n\t\t\tConverts data into canonical group format\n\t\t\tThe quantization dim is sent to the last dimension.\n\t\t\tThe features are then resized such that C -> G x C'\n\t\t\tArgs:\n", "\t\t\tx (Tensor): a tensor in group form [B x C x ... ]\n\t\t\t\tgroups (int): number of groups\n\t\t\tReturns:\n\t\t\t\tx of shape [B x ... x G x C']\n\t\t\t\"\"\"\n\t\t\tz = z.moveaxis(self.dim, -1).contiguous()\n\t\t\tz = z.unflatten(-1, (groups, -1))\n\t\t\treturn z\n\t\tdef to_original_format(self, x):\n\t\t\t\"\"\"\n", "\t\tMerges group and permutes dimension back\n\t\t\tArgs:\n\t\t\t\tx (Tensor): a tensor in group form [B x ... x G x C // G]\n\t\t\tReturns:\n\t\t\t\tmerged `x` of shape [B x ... x C] (assuming dim=1)\n\t\t\t\"\"\"\n\t\t\treturn x.flatten(-2, -1).moveaxis(-1, self.dim)\n\t\tdef prepare_inputs(self, z, groups):\n\t\t\t\"\"\"\n\t\t\tPrepare input with normalization and group format\n", "\t\tArgs:\n\t\t\t\tx (Tensor): a tensor in group form [B x C x ... ]\n\t\t\t\tgroups (int): number of groups\n\t\t\t\"\"\"\n\t\t\tif len(z.shape) <= 1:\n\t\t\t\te_msg = f'expected a tensor of at least 2 dimensions but found {z.size()}'\n\t\t\t\traise ValueError(e_msg)\n\t\t\tif self.norm_before_grouping:\n\t\t\t\tz = self.norm_layer(z)\n\t\t\tz = self.to_canonical_group_format(z, groups)\n", "\t\tif not self.norm_before_grouping:\n\t\t\t\tz = self.norm_layer(z)\n\t\t\treturn z\n\t\t@property\n\t\tdef requires_grad(self):\n\t\t\treturn self.codebook[0].weight.requires_grad\n\t\tdef set_requires_grad(self, requires_grad):\n\t\t\tfor codebook in self.codebook:\n\t\t\t\tcodebook.weight.requires_grad = requires_grad\n\t\t\treturn\n", "\tdef extra_repr(self):\n\t\t\trepr = \"\\n\".join([\n\t\t\t\tf\"num_codes: {self.num_codes}\",\n\t\t\t\tf\"groups: {self.groups}\",\n\t\t\t\tf\"enabled: {self.enabled}\",\n\t\t\t])\n\t\t\treturn repr\n"]}
{"filename": "vqtorch/nn/vq.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom vqtorch.dists import get_dist_fns\n\timport vqtorch\n\tfrom vqtorch.norms import with_codebook_normalization\n\tfrom .vq_base import _VQBaseLayer\n\tfrom .affine import AffineTransform\n\tclass VectorQuant(_VQBaseLayer):\n\t\t\"\"\"\n", "\tVector quantization layer using straight-through estimation.\n\t\tArgs:\n\t\t\tfeature_size (int): feature dimension corresponding to the vectors\n\t\t\tnum_codes (int): number of vectors in the codebook\n\t\t\tbeta (float): commitment loss weighting\n\t\t\tsync_nu (float): sync loss weighting\n\t\t\taffine_lr (float): learning rate for affine transform\n\t\t\taffine_groups (int): number of affine parameter groups\n\t\t\treplace_freq (int): frequency to replace dead codes\n\t\t\tinplace_optimizer (Optimizer): optimizer for inplace codebook updates\n", "\t\t**kwargs: additional arguments for _VQBaseLayer\n\t\tReturns:\n\t\t\tQuantized vector z_q and return dict\n\t\t\"\"\"\n\t\tdef __init__(\n\t\t\t\tself,\n\t\t\t\tfeature_size : int,\n\t\t\t\tnum_codes : int,\n\t\t\t\tbeta : float = 0.95,\n\t\t\t\tsync_nu : float = 0.0,\n", "\t\t\taffine_lr:\tfloat = 0.0,\n\t\t\t\taffine_groups: int = 1,\n\t\t\t\treplace_freq: int = 0,\n\t\t\t\tinplace_optimizer: torch.optim.Optimizer = None,\n\t\t\t\t**kwargs,\n\t\t\t\t):\n\t\t\tsuper().__init__(feature_size, num_codes, **kwargs)\n\t\t\tself.loss_fn, self.dist_fn = get_dist_fns('euclidean')\n\t\t\tif beta < 0.0 or beta > 1.0:\n\t\t\t\traise ValueError(f'beta must be in [0, 1] but got {beta}')\n", "\t\tself.beta = beta\n\t\t\tself.nu = sync_nu\n\t\t\tself.affine_lr = affine_lr\n\t\t\tself.codebook = nn.Embedding(self.num_codes, self.feature_size)\n\t\t\tif inplace_optimizer is not None:\n\t\t\t\tif beta != 1.0:\n\t\t\t\t\traise ValueError('inplace_optimizer can only be used with beta=1.0')\n\t\t\t\tself.inplace_codebook_optimizer = inplace_optimizer(self.codebook.parameters())\t\t\t\n\t\t\tif affine_lr > 0:\n\t\t\t\t# defaults to using learnable affine parameters\n", "\t\t\tself.affine_transform = AffineTransform(\n\t\t\t\t\t\t\t\t\t\t\tself.code_vector_size,\n\t\t\t\t\t\t\t\t\t\t\tuse_running_statistics=False,\n\t\t\t\t\t\t\t\t\t\t\tlr_scale=affine_lr,\n\t\t\t\t\t\t\t\t\t\t\tnum_groups=affine_groups,\n\t\t\t\t\t\t\t\t\t\t\t)\n\t\t\tif replace_freq > 0:\n\t\t\t\tvqtorch.nn.utils.lru_replacement(self, rho=0.01, timeout=replace_freq)\n\t\t\treturn\n\t\tdef straight_through_approximation(self, z, z_q):\n", "\t\t\"\"\" passed gradient from z_q to z \"\"\"\n\t\t\tif self.nu > 0:\n\t\t\t\tz_q = z + (z_q - z).detach() + (self.nu * z_q) + (-self.nu * z_q).detach()\n\t\t\telse:\n\t\t\t\tz_q = z + (z_q - z).detach()\n\t\t\treturn z_q\n\t\tdef compute_loss(self, z_e, z_q):\n\t\t\t\"\"\" computes loss between z and z_q \"\"\"\n\t\t\treturn ((1.0 - self.beta) * self.loss_fn(z_e, z_q.detach()) + \\\n\t\t\t\t\t\t  (self.beta) * self.loss_fn(z_e.detach(), z_q))\n", "\tdef quantize(self, codebook, z):\n\t\t\t\"\"\"\n\t\t\tQuantizes the latent codes z with the codebook\n\t\t\tArgs:\n\t\t\t\tcodebook (Tensor): B x F\n\t\t\t\tz (Tensor): B x ... x F\n\t\t\t\"\"\"\n\t\t\t# reshape to (BHWG x F//G) and compute distance\n\t\t\tz_shape = z.shape[:-1]\n\t\t\tz_flat = z.view(z.size(0), -1, z.size(-1))\n", "\t\tif hasattr(self, 'affine_transform'):\n\t\t\t\tself.affine_transform.update_running_statistics(z_flat, codebook)\n\t\t\t\tcodebook = self.affine_transform(codebook)\n\t\t\twith torch.no_grad():\n\t\t\t\tdist_out = self.dist_fn(\n\t\t\t\t\t\t\t\ttensor=z_flat,\n\t\t\t\t\t\t\t\tcodebook=codebook,\n\t\t\t\t\t\t\t\ttopk=self.topk,\n\t\t\t\t\t\t\t\tcompute_chunk_size=self.cdist_chunk_size,\n\t\t\t\t\t\t\t\thalf_precision=(z.is_cuda),\n", "\t\t\t\t\t\t\t)\n\t\t\t\td = dist_out['d'].view(z_shape)\n\t\t\t\tq = dist_out['q'].view(z_shape).long()\n\t\t\tz_q = F.embedding(q, codebook)\n\t\t\tif self.training and hasattr(self, 'inplace_codebook_optimizer'):\n\t\t\t\t# update codebook inplace \n\t\t\t\t((z_q - z.detach()) ** 2).mean().backward()\n\t\t\t\tself.inplace_codebook_optimizer.step()\n\t\t\t\tself.inplace_codebook_optimizer.zero_grad()\n\t\t\t\t# forward pass again with the update codebook\n", "\t\t\tz_q = F.embedding(q, codebook)\n\t\t\t\t# NOTE to save compute, we assumed Q did not change.\n\t\t\treturn z_q, d, q\n\t\t@torch.no_grad()\n\t\tdef get_codebook(self):\n\t\t\tcb = self.codebook.weight\n\t\t\tif hasattr(self, 'affine_transform'):\n\t\t\t\tcb = self.affine_transform(cb)\n\t\t\treturn cb\n\t\tdef get_codebook_affine_params(self):\n", "\t\tif hasattr(self, 'affine_transform'):\n\t\t\t\treturn self.affine_transform.get_affine_params()\n\t\t\treturn None\n\t\t@with_codebook_normalization\n\t\tdef forward(self, z):\n\t\t\t######\n\t\t\t## (1) formatting data by groups and invariant to dim\n\t\t\t######\n\t\t\tz = self.prepare_inputs(z, self.groups)\n\t\t\tif not self.enabled:\n", "\t\t\tz = self.to_original_format(z)\n\t\t\t\treturn z, {}\n\t\t\t######\n\t\t\t## (2) quantize latent vector\n\t\t\t######\n\t\t\tz_q, d, q = self.quantize(self.codebook.weight, z)\n\t\t\t# e_mean = F.one_hot(q, num_classes=self.num_codes).view(-1, self.num_codes).float().mean(0)\n\t\t\t# perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n\t\t\tperplexity = None\n\t\t\tto_return = {\n", "\t\t\t'z'  : z,               # each group input z_e\n\t\t\t\t'z_q': z_q,             # quantized output z_q\n\t\t\t\t'd'  : d,               # distance function for each group\n\t\t\t\t'q'\t : q,\t\t\t\t# codes\n\t\t\t\t'loss': self.compute_loss(z, z_q).mean(),\n\t\t\t\t'perplexity': perplexity,\n\t\t\t\t}\n\t\t\tz_q = self.straight_through_approximation(z, z_q)\n\t\t\tz_q = self.to_original_format(z_q)\n\t\t\treturn z_q, to_return\n"]}
{"filename": "vqtorch/nn/utils/__init__.py", "chunked_list": ["from .replace import lru_replacement\n"]}
{"filename": "vqtorch/nn/utils/replace.py", "chunked_list": ["import torch\n\tclass ReplaceLRU():\n\t\t\"\"\"\n\t\tAttributes:\n\t\t\trho (float): mutation noise\n\t\t\ttimeout (int): number of batch it has seen\n\t\t\"\"\"\n\t\tVALID_POLICIES = ['input_random', 'input_kmeans', 'self']\n\t\tdef __init__(self, rho=1e-4, timeout=100):\n\t\t\tassert timeout > 1\n", "\t\tassert rho > 0.0\n\t\t\tself.rho = rho\n\t\t\tself.timeout = timeout\n\t\t\tself.policy = 'input_random'\n\t\t\t# self.policy = 'input_kmeans'\n\t\t\t# self.policy = 'self'\n\t\t\tself.tau = 2.0\n\t\t\tassert self.policy in self.VALID_POLICIES\n\t\t\treturn\n\t\t@staticmethod\n", "\tdef apply(module, rho=0., timeout=100):\n\t\t\t\"\"\" register forward hook \"\"\"\n\t\t\tfn = ReplaceLRU(rho, timeout)\n\t\t\tdevice = next(module.parameters()).device\n\t\t\tmodule.register_forward_hook(fn)\n\t\t\tmodule.register_buffer('_counts', timeout * torch.ones(module.num_codes))\n\t\t\tmodule._counts = module._counts.to(device)\n\t\t\treturn fn\n\t\tdef __call__(self, module, inputs, outputs):\n\t\t\t\"\"\"\n", "\t\tThis function is triggered during forward pass\n\t\t\trecall: z_q, misc = vq(x)\n\t\t\tArgs\n\t\t\t\tmodule (nn.VectorQuant)\n\t\t\t\tinputs (tuple): A tuple with 1 element\n\t\t\t\t\tx (Tensor)\n\t\t\t\toutputs (tuple): A tuple with 2 elements\n\t\t\t\t\tz_q (Tensor), misc (dict)\n\t\t\t\"\"\"\n\t\t\tif not module.training:\n", "\t\t\treturn\n\t\t\t# count down all code by 1 and if used, reset timer to timeout value\n\t\t\tmodule._counts -= 1\n\t\t\t# --- computes most recent codebook usage --- #\n\t\t\tunique, counts = torch.unique(outputs[1]['q'], return_counts=True)\n\t\t\tmodule._counts.index_fill_(0, unique, self.timeout)\n\t\t\t# --- find how many needs to be replaced --- #\n\t\t\t# num_active = self.check_and_replace_dead_codes(module, outputs)\n\t\t\tinactive_indices = torch.argwhere(module._counts == 0).squeeze(-1)\n\t\t\tnum_inactive = inactive_indices.size(0)\n", "\t\tif num_inactive > 0:\n\t\t\t\tif self.policy == 'self':\n\t\t\t\t\t# exponential distance allows more recently used codes to be even more preferable\n\t\t\t\t\tp = torch.zeros_like(module._counts)\n\t\t\t\t\tp[unique] = counts.float()\n\t\t\t\t\tp = p / p.sum()\n\t\t\t\t\tp = torch.exp(self.tau * p) - 1 # the negative 1 is to drive p=0 to stay 0\n\t\t\t\t\tselected_indices = torch.multinomial(p, num_inactive, replacement=True)\n\t\t\t\t\tselected_values = module.codebook.weight.data[selected_indices].clone()\n\t\t\t\telif self.policy == 'input_random':\n", "\t\t\t\tz_e = outputs[1]['z'].flatten(0, -2)   # flatten to 2D\n\t\t\t\t\tz_e = z_e[torch.randperm(z_e.size(0))] # shuffle\n\t\t\t\t\tmult = num_inactive // z_e.size(0) + 1\n\t\t\t\t\tif mult > 1: # if theres not enough\n\t\t\t\t\t\tz_e = torch.cat(mult * [z_e])\n\t\t\t\t\tselected_values = z_e[:num_inactive]\n\t\t\t\telif self.policy == 'input_kmeans':\n\t\t\t\t\t# can be extremely slow\n\t\t\t\t\tfrom torchpq.clustering import KMeans\n\t\t\t\t\tz_e = outputs[1]['z'].flatten(0, -2)   # flatten to 2D\n", "\t\t\t\tz_e = z_e[torch.randperm(z_e.size(0))] # shuffle\n\t\t\t\t\tkmeans = KMeans(n_clusters=num_inactive, distance='euclidean', init_mode=\"kmeans++\")\n\t\t\t\t\tkmeans.fit(z_e.data.T.contiguous())\n\t\t\t\t\tselected_values = kmeans.centroids.T\n\t\t\t\tif self.rho > 0:\n\t\t\t\t\tnorm = selected_values.norm(p=2, dim=-1, keepdim=True)\n\t\t\t\t\tnoise = torch.randn_like(selected_values)\n\t\t\t\t\tselected_values = selected_values + self.rho * norm * noise\n\t\t\t\t# --- update dead codes with new codes --- #\n\t\t\t\tmodule.codebook.weight.data[inactive_indices] = selected_values\n", "\t\t\tmodule._counts[inactive_indices] += self.timeout\n\t\t\treturn outputs\n\tdef lru_replacement(vq_module, rho=1e-4, timeout=100):\n\t\t\"\"\"\n\t\tExample::\n\t\t\t>>> vq = VectorQuant(...)\n\t\t\t>>> vq = lru_replacement(vq)\n\t\t\"\"\"\n\t\tReplaceLRU.apply(vq_module, rho, timeout)\n\t\treturn vq_module\n"]}
{"filename": "vqtorch/nn/utils/init.py", "chunked_list": ["import torch\n\tfrom stringcolor import cs\n\timport warnings\n\timport vqtorch\n\t@torch.no_grad()\n\tdef data_dependent_init_forward_hook(self, inputs, outputs, use_kmeans=True, verbose=False):\n\t\t\"\"\" initializes codebook from data \"\"\"\n\t\tif (not self.training) or (self.data_initialized.item() == 1):\n\t\t\treturn\n\t\tif verbose:\n", "\t\tprint(cs('initializing codebook with k-means++', 'y'))\n\t\tdef sample_centroids(z_e, num_codes):\n\t\t\t\"\"\" replaces the data of the codebook with z_e randomly. \"\"\"\n\t\t\tz_e = z_e.reshape(-1, z_e.size(-1))\n\t\t\tif num_codes >= z_e.size(0):\n\t\t\t\te_msg = f'\\ncodebook size > warmup samples: {num_codes} vs {z_e.size(0)}. ' + \\\n\t\t\t\t\t\t 'recommended to decrease the codebook size or increase batch size.'\n\t\t\t\twarnings.warn(str(cs(e_msg, 'yellow')))\n\t\t\t\t# repeat until it fits and add noise\n\t\t\t\trepeat = num_codes // z_e.shape[0]\n", "\t\t\tnew_codes = z_e.data.tile([repeat, 1])[:num_codes]\n\t\t\t\tnew_codes += 1e-3 * torch.randn_like(new_codes.data)\n\t\t\telse:\n\t\t\t\t# you have more warmup samples than codebook. subsample data\n\t\t\t\tif use_kmeans:\n\t\t\t\t\tfrom torchpq.clustering import KMeans\n\t\t\t\t\tkmeans = KMeans(n_clusters=num_codes, distance='euclidean', init_mode=\"kmeans++\")\n\t\t\t\t\tkmeans.fit(z_e.data.T.contiguous())\n\t\t\t\t\tnew_codes = kmeans.centroids.T\n\t\t\t\telse:\n", "\t\t\t\tindices = torch.randint(low=0, high=num_codes, size=(num_codes,))\n\t\t\t\t\tindices = indices.to(z_e.device)\n\t\t\t\t\tnew_codes = torch.index_select(z_e, 0, indices).to(z_e.device).data\n\t\t\treturn new_codes\n\t\t_, misc = outputs\n\t\tz_e, z_q = misc['z'], misc['z_q']\n\t\tif type(self) is vqtorch.nn.VectorQuant:\n\t\t\tnum_codes = self.codebook.weight.shape[0]\n\t\t\tnew_codebook = sample_centroids(z_e, num_codes)\n\t\t\tself.codebook.weight.data = new_codebook\n", "\telif type(self) is vqtorch.nn.GroupVectorQuant:\n\t\t\tif self.share:\n\t\t\t\tprint(self.codebook.weight.shape)\n\t\t\t\tnew_codebook = sample_centroids(z_e, self.group_size)\n\t\t\t\tself.codebook.weight.data = new_codebook\n\t\t\telse:\n\t\t\t\tfor i in range(self.groups):\n\t\t\t\t\toffset = i * self.group_size\n\t\t\t\t\tnew_codebook = sample_centroids(z_e[..., i, :], self.group_size)\n\t\t\t\t\tself.codebook.weight.data[offset:offset+self.group_size] = new_codebook\n", "\telif type(self) is vqtorch.nn.ResidualVectorQuant:\n\t\t\tz_e = misc['z_res']\n\t\t\tif self.share:\n\t\t\t\tnew_codebook = sample_centroids(z_e, self.group_size)\n\t\t\t\tself.codebook.weight.data = new_codebook\n\t\t\telse:\n\t\t\t\tfor i in range(self.groups):\n\t\t\t\t\toffset = i * self.group_size\n\t\t\t\t\tnew_codebook = sample_centroids(z_e[..., i, :], self.group_size)\n\t\t\t\t\tself.codebook.weight.data[offset:offset+self.group_size] = new_codebook\n", "\tself.data_initialized.fill_(1)\n\t\treturn\n"]}
{"filename": "examples/experimental_inplace_update.py", "chunked_list": ["# mnist VQ experiment with various settings.\n\timport torch \n\timport torch.nn as nn \n\timport torch.nn.functional as F\n\tfrom torchvision import datasets\n\tfrom vqtorch.nn import VectorQuant\n\tfrom tqdm.auto import trange\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tlr = 3e-4\n", "train_iter = 300\n\tnum_codes = 256\n\tseed = 1234\n\tclass SimpleVQClassifier(nn.Module):\n\t    def __init__(self, **vq_kwargs):    \n\t        super().__init__()\n\t        self.layers = nn.ModuleList([\n\t                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n\t                nn.MaxPool2d(kernel_size=2, stride=2),\n\t                nn.GELU(),\n", "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n\t                nn.MaxPool2d(kernel_size=2, stride=2),\n\t                VectorQuant(32, **vq_kwargs),\n\t                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n\t                nn.AdaptiveMaxPool2d((1, 1)),\n\t                nn.Flatten(),\n\t                nn.Linear(64, 10),\n\t                ])\n\t        return\n\t    def forward(self, x):\n", "        for layer in self.layers:\n\t            if isinstance(layer, VectorQuant):\n\t                x, vq_dict = layer(x)\n\t            else:\n\t                x = layer(x)\n\t        return x, vq_dict\n\tdef train(model, train_loader, train_iterations=1000, alpha=10, ignore_commitment_loss=False):\n\t    def iterate_dataset(data_loader):\n\t        data_iter = iter(data_loader)\n\t        while True:\n", "            try:\n\t                x, y = next(data_iter)\n\t            except StopIteration:\n\t                data_iter = iter(data_loader)\n\t                x, y = next(data_iter)\n\t            yield x.cuda(), y.cuda()\n\t    criterion = nn.CrossEntropyLoss()\n\t    for _ in (pbar := trange(train_iterations)):\n\t        opt.zero_grad()\n\t        x, y = next(iterate_dataset(train_loader))\n", "        out, vq_out = model(x)\n\t        sce_loss = criterion(out, y)\n\t        cmt_loss = vq_out['loss']\n\t        if ignore_commitment_loss:\n\t            sce_loss.backward()\n\t        else:\n\t            (sce_loss + alpha * cmt_loss).backward()\n\t        acc = (out.argmax(dim=1) == y).float().mean()\n\t        opt.step()\n\t        pbar.set_description(f'sce loss: {sce_loss.item():.3f} | ' + \\\n", "                             f'cmt loss: {cmt_loss.item():.3f} | ' + \\\n\t                             f'acc: {acc.item() * 100:.1f} | ' + \\\n\t                             f'active %: {vq_out[\"q\"].unique().numel() / num_codes * 100:.3f}')\n\t    return\n\ttransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ttrain_dataset = DataLoader(datasets.MNIST(root='~/data/mnist', train=True, download=True, transform=transform), batch_size=256, shuffle=True)\n\tprint('baseline + kmeans init')\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n", "train(model, train_dataset, train_iterations=train_iter, alpha=50)\n\tprint('+ inplace alt update')\n\tinplace_optimizer = lambda *args, **kwargs: torch.optim.SGD(*args, **kwargs, lr=50.0, momentum=0.9)\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True, beta=1.0, inplace_optimizer=inplace_optimizer).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n\ttrain(model, train_dataset, train_iterations=train_iter, ignore_commitment_loss=True)\n"]}
{"filename": "examples/classification.py", "chunked_list": ["# mnist VQ experiment with various settings.\n\timport torch \n\timport torch.nn as nn \n\timport torch.nn.functional as F\n\tfrom torchvision import datasets\n\tfrom vqtorch.nn import VectorQuant\n\tfrom tqdm.auto import trange\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tlr = 3e-4\n", "train_iter = 1000\n\tnum_codes = 256\n\tseed = 1234\n\tclass SimpleVQClassifier(nn.Module):\n\t    def __init__(self, **vq_kwargs):    \n\t        super().__init__()\n\t        self.layers = nn.ModuleList([\n\t                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n\t                nn.MaxPool2d(kernel_size=2, stride=2),\n\t                nn.GELU(),\n", "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n\t                nn.MaxPool2d(kernel_size=2, stride=2),\n\t                VectorQuant(32, **vq_kwargs),\n\t                nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n\t                nn.AdaptiveMaxPool2d((1, 1)),\n\t                nn.Flatten(),\n\t                nn.Linear(64, 10),\n\t                ])\n\t        return\n\t    def forward(self, x):\n", "        for layer in self.layers:\n\t            if isinstance(layer, VectorQuant):\n\t                x, vq_dict = layer(x)\n\t            else:\n\t                x = layer(x)\n\t        return x, vq_dict\n\tdef train(model, train_loader, train_iterations=1000, alpha=10):\n\t    def iterate_dataset(data_loader):\n\t        data_iter = iter(data_loader)\n\t        while True:\n", "            try:\n\t                x, y = next(data_iter)\n\t            except StopIteration:\n\t                data_iter = iter(data_loader)\n\t                x, y = next(data_iter)\n\t            yield x.cuda(), y.cuda()\n\t    criterion = nn.CrossEntropyLoss()\n\t    for _ in (pbar := trange(train_iterations)):\n\t        opt.zero_grad()\n\t        x, y = next(iterate_dataset(train_loader))\n", "        out, vq_out = model(x)\n\t        sce_loss = criterion(out, y)\n\t        cmt_loss = vq_out['loss']\n\t        acc = (out.argmax(dim=1) == y).float().mean()\n\t        (sce_loss + alpha * cmt_loss).backward()\n\t        opt.step()\n\t        pbar.set_description(f'sce loss: {sce_loss.item():.3f} | ' + \\\n\t                             f'cmt loss: {cmt_loss.item():.3f} | ' + \\\n\t                             f'acc: {acc.item() * 100:.1f} | ' + \\\n\t                             f'active %: {vq_out[\"q\"].unique().numel() / num_codes * 100:.3f}')\n", "    return\n\ttransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\ttrain_dataset = DataLoader(datasets.MNIST(root='~/data/mnist', train=True, download=True, transform=transform), batch_size=256, shuffle=True)\n\tprint('baseline')\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQClassifier(num_codes=num_codes).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n\ttrain(model, train_dataset, train_iterations=train_iter)\n\tprint('+ kmeans init')\n\ttorch.random.manual_seed(seed)\n", "model = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n\ttrain(model, train_dataset, train_iterations=train_iter)\n\tprint('+ synchronized update')\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True, sync_nu=1.0).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n\ttrain(model, train_dataset, train_iterations=train_iter)\n\tprint('+ affine parameterization')\n\ttorch.random.manual_seed(seed)\n", "model = SimpleVQClassifier(num_codes=num_codes, kmeans_init=True, sync_nu=1.0, affine_lr=10).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n\ttrain(model, train_dataset, train_iterations=train_iter)"]}
{"filename": "examples/autoencoder.py", "chunked_list": ["# mnist VQ experiment with various settings.\n\timport torch \n\timport torch.nn as nn \n\timport torch.nn.functional as F\n\tfrom torchvision import datasets\n\tfrom vqtorch.nn import VectorQuant\n\tfrom tqdm.auto import trange\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tlr = 3e-4\n", "train_iter = 1000\n\tnum_codes = 256\n\tseed = 1234\n\tclass SimpleVQAutoEncoder(nn.Module):\n\t    def __init__(self, **vq_kwargs):    \n\t        super().__init__()\n\t        self.layers = nn.ModuleList([\n\t                nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n\t                nn.MaxPool2d(kernel_size=2, stride=2),\n\t                nn.GELU(),\n", "                nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n\t                nn.MaxPool2d(kernel_size=2, stride=2),\n\t                VectorQuant(32, **vq_kwargs),\n\t                nn.Upsample(scale_factor=2, mode='nearest'),\n\t                nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n\t                nn.GELU(),\n\t                nn.Upsample(scale_factor=2, mode='nearest'),\n\t                nn.Conv2d(16, 1, kernel_size=3, stride=1, padding=1),\n\t                ])\n\t        return\n", "    def forward(self, x):\n\t        for layer in self.layers:\n\t            if isinstance(layer, VectorQuant):\n\t                x, vq_dict = layer(x)\n\t            else:\n\t                x = layer(x)\n\t        return x.clamp(-1, 1), vq_dict\n\tdef train(model, train_loader, train_iterations=1000, alpha=10):\n\t    def iterate_dataset(data_loader):\n\t        data_iter = iter(data_loader)\n", "        while True:\n\t            try:\n\t                x, y = next(data_iter)\n\t            except StopIteration:\n\t                data_iter = iter(data_loader)\n\t                x, y = next(data_iter)\n\t            yield x.cuda(), y.cuda()\n\t    for _ in (pbar := trange(train_iterations)):\n\t        opt.zero_grad()\n\t        x, _ = next(iterate_dataset(train_loader))\n", "        out, vq_out = model(x)\n\t        rec_loss = (out - x).abs().mean()\n\t        cmt_loss = vq_out['loss']\n\t        (rec_loss + alpha * cmt_loss).backward()\n\t        opt.step()\n\t        pbar.set_description(f'rec loss: {rec_loss.item():.3f} | ' + \\\n\t                             f'cmt loss: {cmt_loss.item():.3f} | ' + \\\n\t                             f'active %: {vq_out[\"q\"].unique().numel() / num_codes * 100:.3f}')\n\t    return\n\ttransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n", "train_dataset = DataLoader(datasets.MNIST(root='~/data/mnist', train=True, download=True, transform=transform), batch_size=256, shuffle=True)\n\tprint('baseline')\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQAutoEncoder(num_codes=num_codes).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n\ttrain(model, train_dataset, train_iterations=train_iter)\n\tprint('+ kmeans init')\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQAutoEncoder(num_codes=num_codes, kmeans_init=True).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n", "train(model, train_dataset, train_iterations=train_iter)\n\tprint('+ synchronized update')\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQAutoEncoder(num_codes=num_codes, kmeans_init=True, sync_nu=2.0).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n\ttrain(model, train_dataset, train_iterations=train_iter)\n\tprint('+ affine parameterization')\n\ttorch.random.manual_seed(seed)\n\tmodel = SimpleVQAutoEncoder(num_codes=num_codes, kmeans_init=True, sync_nu=2.0, affine_lr=2.0).cuda()\n\topt = torch.optim.AdamW(model.parameters(), lr=lr)\n", "train(model, train_dataset, train_iterations=train_iter)"]}
{"filename": "examples/test.py", "chunked_list": ["import torch\n\tfrom vqtorch.nn import VectorQuant, GroupVectorQuant, ResidualVectorQuant\n\tprint('Testing VectorQuant')\n\t# create VQ layer\n\tvq_layer = VectorQuant(\n\t                feature_size=32,     # feature dimension corresponding to the vectors\n\t                num_codes=1024,      # number of codebook vectors\n\t                beta=0.98,           # (default: 0.9) commitment trade-off\n\t                kmeans_init=True,    # (default: False) whether to use kmeans++ init\n\t                norm=None,           # (default: None) normalization for input vector\n", "                cb_norm=None,        # (default: None) normalization for codebook vectors\n\t                affine_lr=10.0,      # (default: 0.0) lr scale for affine parameters\n\t                sync_nu=0.2,         # (default: 0.0) codebook syncronization contribution\n\t                replace_freq=20,     # (default: None) frequency to replace dead codes\n\t                dim=-1,              # (default: -1) dimension to be quantized\n\t                ).cuda()\n\t# when using `kmeans_init`, we can warmup the codebook\n\twith torch.no_grad():\n\t    z_e = torch.randn(128, 8, 8, 32).cuda()\n\t    vq_layer(z_e)\n", "# standard forward pass\n\tz_e = torch.randn(128, 8, 8, 32).cuda()\n\tz_q, vq_dict = vq_layer(z_e) # equivalent to above\n\tassert z_e.shape == z_q.shape\n\terr = ((z_e - z_q) ** 2).mean().item()\n\tprint(f'>>> quantization error: {err:.3f}')\n\tprint('Testing GroupVectorQuant')\n\t# create VQ layer\n\tvq_layer = GroupVectorQuant(\n\t                feature_size=32,     \n", "                num_codes=1024,      \n\t                beta=0.98,    \n\t                kmeans_init=True,    \n\t                norm=None,         \n\t                cb_norm=None,        \n\t                affine_lr=10.0,   \n\t                sync_nu=0.2,         \n\t                replace_freq=20,     \n\t                dim=-1,             \n\t                groups=4,            # (default: 1) number of groups to divide the feature dimension\n", "                share=False,         # (default: True) when True, same codebook is used for each group\n\t                ).cuda()\n\t# when using `kmeans_init`, we can warmup the codebook\n\twith torch.no_grad():\n\t    z_e = torch.randn(128, 8, 8, 32).cuda()\n\t    vq_layer(z_e)\n\t# standard forward pass\n\tz_e = torch.randn(128, 8, 8, 32).cuda()\n\tz_q, vq_dict = vq_layer(z_e) # equivalent to above\n\tassert z_e.shape == z_q.shape\n", "err = ((z_e - z_q) ** 2).mean().item()\n\tprint(f'>>> quantization error: {err:.3f}')\n\tprint('Testing ResidualVectorQuant')\n\t# create VQ layer\n\tvq_layer = ResidualVectorQuant(\n\t                feature_size=32,     \n\t                num_codes=1024,      \n\t                beta=0.98,    \n\t                kmeans_init=True,    \n\t                norm=None,         \n", "                cb_norm=None,        \n\t                affine_lr=10.0,   \n\t                sync_nu=0.2,         \n\t                replace_freq=20,     \n\t                dim=-1,             \n\t                groups=4,            # (default: 1) number of groups to divide the feature dimension\n\t                share=True,          # (default: True) when True, same codebook is used for each group\n\t                ).cuda()\n\t# when using `kmeans_init`, we can warmup the codebook\n\twith torch.no_grad():\n", "    z_e = torch.randn(128, 8, 8, 32).cuda()\n\t    vq_layer(z_e)\n\t# standard forward pass\n\tz_e = torch.randn(128, 8, 8, 32).cuda()\n\tz_q, vq_dict = vq_layer(z_e) # equivalent to above\n\tassert z_e.shape == z_q.shape\n\terr = ((z_e - z_q) ** 2).mean().item()\n\tprint(f'>>> quantization error: {err:.3f}')\n"]}
{"filename": "examples/experimental_group_affine.py", "chunked_list": ["import torch\n\tfrom vqtorch.nn import VectorQuant, GroupVectorQuant, ResidualVectorQuant\n\tprint('Testing VectorQuant')\n\t# create VQ layer\n\tvq_layer = VectorQuant(\n\t                feature_size=32,     # feature dimension corresponding to the vectors\n\t                num_codes=1024,      # number of codebook vectors\n\t                beta=0.98,           # (default: 0.9) commitment trade-off\n\t                kmeans_init=True,    # (default: False) whether to use kmeans++ init\n\t                norm=None,           # (default: None) normalization for input vector\n", "                cb_norm=None,        # (default: None) normalization for codebook vectors\n\t                affine_lr=10.0,      # (default: 0.0) lr scale for affine parameters\n\t                affine_groups=8,     # *** NEW *** (default: 1) number of affine parameter groups\n\t                sync_nu=0.2,         # (default: 0.0) codebook syncronization contribution\n\t                replace_freq=20,     # (default: None) frequency to replace dead codes\n\t                dim=-1,              # (default: -1) dimension to be quantized\n\t                ).cuda()\n\t# when using `kmeans_init`, we can warmup the codebook\n\twith torch.no_grad():\n\t    z_e = torch.randn(128, 8, 8, 32).cuda()\n", "    vq_layer(z_e)\n\t# standard forward pass\n\tz_e = torch.randn(128, 8, 8, 32).cuda()\n\tz_q, vq_dict = vq_layer(z_e) # equivalent to above\n\tassert z_e.shape == z_q.shape\n\terr = ((z_e - z_q) ** 2).mean().item()\n\tprint(f'>>> quantization error: {err:.3f}')"]}
