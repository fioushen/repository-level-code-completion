{"filename": "examples/alpaca-lora-7B/model.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\tfrom typing import Union\n\tfrom get_models import ALPACA_ID, LLAMA_ID, TOKENIZER_ID\n\tfrom peft import PeftModel\n\tfrom simple_ai.api.grpc.completion.server import LanguageModel\n\tfrom transformers import GenerationConfig, LLaMAForCausalLM, LLaMATokenizer\n\t@dataclass(unsafe_hash=True)\n\tclass AlpacaModel(LanguageModel):\n\t    try:\n", "        tokenizer = LLaMATokenizer.from_pretrained(TOKENIZER_ID)\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not load tokenizer: {ex}\")\n\t        tokenizer = None\n\t    try:\n\t        model = LLaMAForCausalLM.from_pretrained(\n\t            LLAMA_ID,\n\t            load_in_8bit=True,\n\t            device_map=\"auto\",\n\t        )\n", "    except Exception as ex:\n\t        logging.exception(f\"Could not load pretrained LlaMa model: {ex}\")\n\t        model = None\n\t    try:\n\t        model = PeftModel.from_pretrained(model, ALPACA_ID)\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not load pretrained Peft model: {ex}\")\n\t        model = None\n\t    def complete(\n\t        self,\n", "        prompt: str = \"<|endoftext|>\",\n\t        suffix: str = \"\",\n\t        max_tokens: int = 512,\n\t        temperature: float = 1.0,\n\t        top_p: float = 1.0,\n\t        n: int = 1,\n\t        stream: bool = False,\n\t        logprobs: int = 0,\n\t        echo: bool = False,\n\t        stop: Union[str, list] = \"\",\n", "        presence_penalty: float = 0.0,\n\t        frequence_penalty: float = 0.0,\n\t        best_of: int = 0,\n\t        logit_bias: dict = {},\n\t    ) -> str:\n\t        generation_config = GenerationConfig(\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            num_beams=4,\n\t        )\n", "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n\t        input_ids = inputs[\"input_ids\"]\n\t        input_ids = input_ids.cuda()\n\t        output = self.model.generate(\n\t            input_ids=input_ids,\n\t            generation_config=generation_config,\n\t            return_dict_in_generate=True,\n\t            output_scores=True,\n\t            max_new_tokens=max_tokens,\n\t        )\n", "        results = []\n\t        for sequence in output.sequences:\n\t            results.append(self.tokenizer.decode(sequence).split(\"### Response:\")[1].strip())\n\t        return results[0]\n"]}
{"filename": "examples/alpaca-lora-7B/get_models.py", "chunked_list": ["import logging\n\tTOKENIZER_ID = \"decapoda-research/llama-7b-hf\"\n\tLLAMA_ID = \"decapoda-research/llama-7b-hf\"\n\tALPACA_ID = \"tloen/alpaca-lora-7b\"\n\tif __name__ == \"__main__\":\n\t    from huggingface_hub import snapshot_download\n\t    for repo_id in (TOKENIZER_ID, LLAMA_ID, ALPACA_ID):\n\t        try:\n\t            snapshot_download(repo_id)\n\t        except Exception as ex:\n", "            logging.exception(f\"Could not retrieve {repo_id}: {ex}\")\n"]}
{"filename": "examples/alpaca-lora-7B/server.py", "chunked_list": ["import logging\n\tfrom model import AlpacaModel as Model\n\tfrom simple_ai.api.grpc.completion.server import LanguageModelServicer, serve\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    logging.basicConfig(level=logging.INFO)\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"-a\", \"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    logging.info(f\"Starting gRPC server on {args.address}\")\n", "    model_servicer = LanguageModelServicer(model=Model())\n\t    serve(address=args.address, model_servicer=model_servicer)\n"]}
{"filename": "examples/sentence-transformers/model.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\tfrom get_models import MODEL_ID\n\tfrom sentence_transformers import SentenceTransformer\n\t@dataclass(unsafe_hash=True)\n\tclass SentenceTransformerModel:\n\t    model = SentenceTransformer(MODEL_ID)\n\t    def embed(\n\t        self,\n\t        inputs: list = [],\n", "    ) -> list:\n\t        logging.info(f\"Processing inputs : {inputs}\")\n\t        embeddings = self.model.encode(inputs)\n\t        logging.info(\n\t            f\"Successfully computed embeddings (shape : {embeddings.shape}) for inputs : {inputs}\"\n\t        )\n\t        return embeddings.tolist()\n"]}
{"filename": "examples/sentence-transformers/get_models.py", "chunked_list": ["import logging\n\tfrom sentence_transformers import SentenceTransformer\n\tMODEL_ID = \"all-MiniLM-L6-v2\"\n\tif __name__ == \"__main__\":\n\t    try:\n\t        model = SentenceTransformer(MODEL_ID)\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not retrieve {MODEL_ID}: {ex}\")\n"]}
{"filename": "examples/sentence-transformers/server.py", "chunked_list": ["import logging\n\tfrom model import SentenceTransformerModel as Model\n\tfrom simple_ai.api.grpc.embedding.server import LanguageModelServicer, serve\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    logging.basicConfig(level=logging.INFO)\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"-a\", \"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    logging.info(f\"Starting gRPC server on {args.address}\")\n", "    model_servicer = LanguageModelServicer(model=Model())\n\t    serve(address=args.address, model_servicer=model_servicer)\n"]}
{"filename": "examples/stablelm-open-assistant/model.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\tfrom threading import Thread\n\timport torch\n\tfrom get_models import MODEL_ID\n\tfrom simple_ai.api.grpc.chat.server import LanguageModel\n\tfrom transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n\tdef preprocess_role(role: str):\n\t    if role == \"user\":\n\t        return \"prompter\"\n", "    if role == \"system\":\n\t        return \"assistant\"\n\t    return role\n\tdef format_chat_log(chat: list[dict[str, str]] = dict()) -> str:\n\t    raw_chat_text = \"\"\n\t    for item in chat:\n\t        raw_chat_text += (\n\t            f\"<|{preprocess_role(item.get('role'))}|>{item.get('content')}<|endoftext|>\"\n\t        )\n\t    return raw_chat_text + \"<|assistant|>\"\n", "@dataclass(unsafe_hash=True)\n\tclass OpenAssistantModel(LanguageModel):\n\t    gpu_id: int = 0\n\t    device = torch.device(\"cuda\", gpu_id)\n\t    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\").half()\n\t    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\t    def chat(\n\t        self,\n\t        chatlog: list[list[str]] = None,\n\t        max_tokens: int = 512,\n", "        temperature: float = 0.9,\n\t        top_p: int = 0.5,\n\t        role: str = \"system\",\n\t        *args,\n\t        **kwargs,\n\t    ) -> str:\n\t        logging.info(f\"Preprocessing chatlog:\\n{chatlog}\")\n\t        prompt = format_chat_log(chatlog)\n\t        logging.info(f\"Input prompt:\\n{prompt}\")\n\t        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n", "        outputs = self.model.generate(\n\t            **inputs,\n\t            max_new_tokens=max_tokens,\n\t            do_sample=True,\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            pad_token_id=self.tokenizer.eos_token_id,\n\t        )\n\t        output = self.tokenizer.batch_decode(outputs)[0]\n\t        logging.info(f\"Model output:\\n{output}\")\n", "        # Remove the context from the output\n\t        output = output[len(prompt) :]\n\t        # Stop at \"<|endoftext|>\"\n\t        if \"<|endoftext|>\" in output:\n\t            output = output.split(\"<|endoftext|>\")[0]\n\t        return [{\"role\": role, \"content\": output}]\n\t    def stream(\n\t        self,\n\t        chatlog: list[list[str]] = None,\n\t        max_tokens: int = 512,\n", "        temperature: float = 0.9,\n\t        top_p: int = 0.5,\n\t        role: str = \"system\",\n\t        *args,\n\t        **kwargs,\n\t    ):\n\t        yield [{\"role\": role}]\n\t        logging.info(f\"Preprocessing chatlog:\\n{chatlog}\")\n\t        prompt = format_chat_log(chatlog)\n\t        logging.info(f\"Input prompt:\\n{prompt}\")\n", "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n\t        # Generate stream, yield delta\n\t        streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n\t        generation_kwargs = dict(\n\t            **inputs,\n\t            streamer=streamer,\n\t            max_new_tokens=max_tokens,\n\t            do_sample=True,\n\t            temperature=temperature,\n\t            top_p=top_p,\n", "            pad_token_id=self.tokenizer.eos_token_id,\n\t        )\n\t        thread = Thread(target=self.model.generate, kwargs=generation_kwargs)\n\t        thread.start()\n\t        logging.info(\"Output:\")\n\t        for delta in streamer:\n\t            if delta:\n\t                if \"<|endoftext|>\" in delta:\n\t                    logging.info(delta)\n\t                    yield [{\"content\": delta.split(\"<|endoftext|>\")[0]}]\n", "                    break\n\t                logging.info(delta)\n\t                yield [{\"content\": delta}]\n"]}
{"filename": "examples/stablelm-open-assistant/get_models.py", "chunked_list": ["import logging\n\tMODEL_ID = \"OpenAssistant/stablelm-7b-sft-v7-epoch-3\"\n\tif __name__ == \"__main__\":\n\t    from huggingface_hub import snapshot_download\n\t    from transformers.utils import move_cache\n\t    repo_id = MODEL_ID\n\t    try:\n\t        snapshot_download(repo_id)\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not retrieve {repo_id}: {ex}\")\n", "    try:\n\t        move_cache()\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not migrate cache: {ex}\")\n"]}
{"filename": "examples/stablelm-open-assistant/server.py", "chunked_list": ["import logging\n\tfrom concurrent import futures\n\timport grpc\n\tfrom model import OpenAssistantModel as Model\n\tfrom simple_ai.api.grpc.chat.server import (\n\t    LanguageModelServicer as ChatServicer,\n\t    llm_chat_pb2_grpc,\n\t)\n\tdef serve(\n\t    address=\"[::]:50051\",\n", "    chat_servicer=None,\n\t    max_workers=10,\n\t):\n\t    server = grpc.server(futures.ThreadPoolExecutor(max_workers=max_workers))\n\t    llm_chat_pb2_grpc.add_LanguageModelServicer_to_server(chat_servicer, server)\n\t    server.add_insecure_port(address=address)\n\t    server.start()\n\t    server.wait_for_termination()\n\tif __name__ == \"__main__\":\n\t    import argparse\n", "    logging.basicConfig(level=logging.INFO)\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"-a\", \"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    logging.info(f\"Starting gRPC server on {args.address}\")\n\t    chat_servicer = ChatServicer(model=Model())\n\t    serve(\n\t        address=args.address,\n\t        chat_servicer=chat_servicer,\n\t    )\n"]}
{"filename": "examples/MPT-7B-Chat/model.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\tfrom threading import Event, Thread\n\tfrom typing import Optional, Union\n\timport torch\n\tfrom get_models import MODEL_ID\n\tfrom simple_ai.api.grpc.chat.server import LanguageModel\n\tfrom transformers import (\n\t    AutoConfig,\n\t    AutoModelForCausalLM,\n", "    AutoTokenizer,\n\t    StoppingCriteria,\n\t    StoppingCriteriaList,\n\t    TextIteratorStreamer,\n\t)\n\tdef sanitize_user_input(\n\t    text: str,\n\t    blacklist: Union[tuple, str] = (\n\t        \"<|im_start|>\",\n\t        \"<|im_end|>\",\n", "    ),\n\t) -> str:\n\t    \"\"\"To avoid injections of special tokens, we remove \"forbidden\" inputs from strings.\n\t    Args:\n\t        text (str): Input string\n\t    Returns:\n\t        str: Sanitized input string\n\t    \"\"\"\n\t    for forbidden in blacklist:\n\t        text = text.replace(forbidden, \"\")\n", "    return text\n\tdef format_chat_log(chat: list[dict[str, str]] = dict()) -> str:\n\t    \"\"\"MosaicML's MPT-7B-Chat uses [ChatML](https://github.com/openai/openai-python/blob/main/chatml.md) format.\"\"\"\n\t    raw_chat_text = \"\"\n\t    for item in chat:\n\t        raw_chat_text += f\"<|im_start|>{sanitize_user_input(item.get('role'))}\\n{sanitize_user_input(item.get('content'))}<|im_end|>\\n\"\n\t    return f\"{raw_chat_text}<|im_start|>assistant\\n\"\n\t@dataclass\n\tclass StopOnTokens(StoppingCriteria):\n\t    stop_token_ids: Optional[Union[list, tuple]]\n", "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n\t        for stop_id in self.stop_token_ids:\n\t            if input_ids[0][-1] == stop_id:\n\t                return True\n\t        return False\n\t# Default sequence length is 2048, can be increased thanks to ALiBi\n\t# Note: ALiBi is only implemented with torch and triton attention.\n\tMAX_SEQUENCE_LENGTH = 4096\n\t@dataclass(unsafe_hash=True)\n\tclass ChatModel(LanguageModel):\n", "    gpu_id: int = 0\n\t    device = torch.device(\"cuda\", gpu_id)\n\t    tokenizer = AutoTokenizer.from_pretrained(\n\t        MODEL_ID, model_max_length=MAX_SEQUENCE_LENGTH, truncation_side=\"left\"\n\t    )\n\t    config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n\t    # Attention: either \"torch\" (default), \"flash\" or \"triton\"\n\t    config.attn_config[\"attn_impl\"] = \"torch\"\n\t    config.update({\"max_seq_len\": MAX_SEQUENCE_LENGTH})\n\t    model = AutoModelForCausalLM.from_pretrained(\n", "        MODEL_ID,\n\t        config=config,\n\t        torch_dtype=torch.bfloat16,\n\t        trust_remote_code=True,\n\t    ).to(device)\n\t    def chat(\n\t        self,\n\t        chatlog: list[list[str]] = None,\n\t        max_tokens: int = MAX_SEQUENCE_LENGTH // 2,\n\t        temperature: float = 0.9,\n", "        top_p: int = 0.5,\n\t        role: str = \"assistant\",\n\t        end_of_text: Union[str, list, tuple] = (\"<|im_end|>\", \"<|endoftext|>\"),\n\t        *args,\n\t        **kwargs,\n\t    ) -> str:\n\t        try:\n\t            if isinstance(end_of_text, str):\n\t                end_of_text = (end_of_text,)\n\t            logging.info(f\"Preprocessing chatlog:\\n{chatlog}\")\n", "            prompt = format_chat_log(chatlog)\n\t            logging.info(f\"Input prompt:\\n{prompt}\")\n\t            inputs = self.tokenizer(\n\t                prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQUENCE_LENGTH // 2\n\t            ).to(self.model.device)\n\t            # Use Torch's Flash attention\n\t            with torch.backends.cuda.sdp_kernel(\n\t                enable_flash=True, enable_math=False, enable_mem_efficient=False\n\t            ):\n\t                outputs = self.model.generate(\n", "                    **inputs,\n\t                    max_new_tokens=max_tokens,\n\t                    do_sample=True,\n\t                    temperature=temperature,\n\t                    top_p=top_p,\n\t                    pad_token_id=self.tokenizer.eos_token_id,\n\t                )\n\t                output = self.tokenizer.batch_decode(outputs)[0]\n\t                logging.info(f\"Model output:\\n{output}\")\n\t                # Remove the context from the output\n", "                output = output[len(prompt) :]\n\t                # Stop if end of text\n\t                for item in end_of_text:\n\t                    if item in output:\n\t                        output = output.split(item)[0]\n\t                    break\n\t                # Avoid issues with GPU vRAM\n\t                del inputs\n\t                torch.cuda.empty_cache()\n\t                return [{\"role\": role, \"content\": output}]\n", "        except Exception as ex:\n\t            logging.exception(ex)\n\t        return\n\t    def stream(\n\t        self,\n\t        chatlog: list[list[str]] = None,\n\t        max_tokens: int = MAX_SEQUENCE_LENGTH // 2,\n\t        temperature: float = 0.9,\n\t        top_p: int = 0.5,\n\t        role: str = \"assistant\",\n", "        end_of_text: Union[str, list, tuple] = (\"<|im_end|>\", \"<|endoftext|>\"),\n\t        *args,\n\t        **kwargs,\n\t    ):\n\t        try:\n\t            if isinstance(end_of_text, str):\n\t                end_of_text = (end_of_text,)\n\t            stop = StopOnTokens(stop_token_ids=self.tokenizer.convert_tokens_to_ids(end_of_text))\n\t            # Yield role\n\t            yield [{\"role\": role}]\n", "            logging.info(f\"Preprocessing chatlog:\\n{chatlog}\")\n\t            prompt = format_chat_log(chatlog)\n\t            logging.info(f\"Input prompt:\\n{prompt}\")\n\t            inputs = self.tokenizer(\n\t                prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQUENCE_LENGTH // 2\n\t            ).to(self.model.device)\n\t            logging.info(f\"Input has {len(inputs[0])} tokens.\")\n\t            # Use Torch's Flash attention\n\t            with torch.backends.cuda.sdp_kernel(\n\t                enable_flash=True, enable_math=False, enable_mem_efficient=False\n", "            ):\n\t                # Generate stream, yield delta\n\t                streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n\t                generation_kwargs = dict(\n\t                    **inputs,\n\t                    streamer=streamer,\n\t                    max_new_tokens=max_tokens,\n\t                    do_sample=True,\n\t                    temperature=temperature,\n\t                    top_p=top_p,\n", "                    pad_token_id=self.tokenizer.eos_token_id,\n\t                    stopping_criteria=StoppingCriteriaList([stop]),\n\t                )\n\t                stream_complete = Event()\n\t                def generate_and_signal_complete(generation_kwargs=generation_kwargs):\n\t                    self.model.generate(**generation_kwargs)\n\t                    stream_complete.set()\n\t                thread = Thread(target=generate_and_signal_complete)\n\t                thread.start()\n\t                logging.info(\"Output:\")\n", "                ended = False\n\t                for delta in streamer:\n\t                    if delta:\n\t                        for item in end_of_text:\n\t                            if item in delta:\n\t                                logging.info(delta)\n\t                                yield [{\"content\": delta.split(item)[0]}]\n\t                                ended = True\n\t                                break\n\t                        if ended:\n", "                            break\n\t                        logging.info(delta)\n\t                        yield [{\"content\": delta}]\n\t                thread.join(timeout=60)\n\t                # Avoid issues with GPU vRAM\n\t                del streamer\n\t                del inputs\n\t                torch.cuda.empty_cache()\n\t        except Exception as ex:\n\t            logging.exception(ex)\n", "        return\n"]}
{"filename": "examples/MPT-7B-Chat/get_models.py", "chunked_list": ["import logging\n\tMODEL_ID = \"mosaicml/mpt-7b-chat\"\n\tif __name__ == \"__main__\":\n\t    from huggingface_hub import snapshot_download\n\t    from transformers.utils import move_cache\n\t    for repo_id in (MODEL_ID,):\n\t        try:\n\t            snapshot_download(repo_id)\n\t        except Exception as ex:\n\t            logging.exception(f\"Could not retrieve {repo_id}: {ex}\")\n", "    try:\n\t        move_cache()\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not migrate cache: {ex}\")\n"]}
{"filename": "examples/MPT-7B-Chat/server.py", "chunked_list": ["import logging\n\tfrom concurrent import futures\n\timport grpc\n\tfrom model import ChatModel as Model\n\tfrom simple_ai.api.grpc.chat.server import (\n\t    LanguageModelServicer as ChatServicer,\n\t    llm_chat_pb2_grpc,\n\t)\n\tdef serve(\n\t    address=\"[::]:50051\",\n", "    chat_servicer=None,\n\t    max_workers=10,\n\t):\n\t    server = grpc.server(futures.ThreadPoolExecutor(max_workers=max_workers))\n\t    llm_chat_pb2_grpc.add_LanguageModelServicer_to_server(chat_servicer, server)\n\t    server.add_insecure_port(address=address)\n\t    server.start()\n\t    server.wait_for_termination()\n\tif __name__ == \"__main__\":\n\t    import argparse\n", "    logging.basicConfig(level=logging.INFO)\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"-a\", \"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    logging.info(f\"Starting gRPC server on {args.address}\")\n\t    chat_servicer = ChatServicer(model=Model())\n\t    serve(\n\t        address=args.address,\n\t        chat_servicer=chat_servicer,\n\t    )\n"]}
{"filename": "examples/GPT-NeoXT-Chat-Base-20B/model.py", "chunked_list": ["from dataclasses import dataclass\n\timport torch\n\tfrom get_models import MODEL_ID\n\tfrom simple_ai.api.grpc.completion.server import LanguageModel\n\tfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\t@dataclass(unsafe_hash=True)\n\tclass OpenChatModel(LanguageModel):\n\t    gpu_id: int = 0\n\t    device = torch.device(\"cuda\", gpu_id)\n\t    model = AutoModelForCausalLM.from_pretrained(MODEL_ID).half()\n", "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\t    def __post_init__(self):\n\t        self.model.to(self.device)\n\t    def complete(\n\t        self,\n\t        prompt: str = \"<|endoftext|>\",\n\t        max_tokens: int = 512,\n\t        temperature: float = 0.6,\n\t        *args,\n\t        **kwargs,\n", "    ) -> str:\n\t        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n\t        outputs = self.model.generate(\n\t            **inputs,\n\t            max_new_tokens=max_tokens,\n\t            do_sample=True,\n\t            temperature=temperature,\n\t            top_k=40,\n\t            pad_token_id=self.tokenizer.eos_token_id,\n\t        )\n", "        output = self.tokenizer.batch_decode(outputs)[0]\n\t        # remove the context from the output\n\t        output = output[len(prompt) :]\n\t        return output\n"]}
{"filename": "examples/GPT-NeoXT-Chat-Base-20B/get_models.py", "chunked_list": ["import logging\n\tMODEL_ID = \"togethercomputer/GPT-NeoXT-Chat-Base-20B\"\n\tif __name__ == \"__main__\":\n\t    from huggingface_hub import snapshot_download\n\t    repo_id = MODEL_ID\n\t    try:\n\t        snapshot_download(repo_id)\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not retrieve {repo_id}: {ex}\")\n"]}
{"filename": "examples/GPT-NeoXT-Chat-Base-20B/server.py", "chunked_list": ["import logging\n\tfrom model import OpenChatModel as Model\n\tfrom simple_ai.api.grpc.completion.server import LanguageModelServicer, serve\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    logging.basicConfig(level=logging.INFO)\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"-a\", \"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    logging.info(f\"Starting gRPC server on {args.address}\")\n", "    model_servicer = LanguageModelServicer(model=Model())\n\t    serve(address=args.address, model_servicer=model_servicer)\n"]}
{"filename": "examples/MPT-7B-Storywriter-65kplus/model.py", "chunked_list": ["import logging\n\tfrom dataclasses import dataclass\n\tfrom threading import Event, Thread\n\tfrom typing import Optional, Union\n\timport torch\n\tfrom get_models import MODEL_ID\n\tfrom simple_ai.api.grpc.chat.server import LanguageModel\n\tfrom transformers import (\n\t    AutoConfig,\n\t    AutoModelForCausalLM,\n", "    AutoTokenizer,\n\t    StoppingCriteria,\n\t    StoppingCriteriaList,\n\t    TextIteratorStreamer,\n\t)\n\t@dataclass\n\tclass StopOnTokens(StoppingCriteria):\n\t    stop_token_ids: Optional[Union[list, tuple]]\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n\t        for stop_id in self.stop_token_ids:\n", "            if input_ids[0][-1] == stop_id:\n\t                return True\n\t        return False\n\t# Model:\n\t# - Has been trained with a sequence length of 2048\n\t# - Fine tuned with a sequence length of 65536 (thanks to ALiBi)\n\t# - Can use a higher sequence length at inferernce (thanks to ALiBi)\n\t# Note: ALiBi is only implemented with torch and triton attention.\n\tMAX_SEQUENCE_LENGTH = 65536\n\t@dataclass(unsafe_hash=True)\n", "class CompletionModel(LanguageModel):\n\t    gpu_id: int = 0\n\t    device = torch.device(\"cuda\", gpu_id)\n\t    tokenizer = AutoTokenizer.from_pretrained(\n\t        MODEL_ID, model_max_length=MAX_SEQUENCE_LENGTH, truncation_side=\"left\"\n\t    )\n\t    config = AutoConfig.from_pretrained(MODEL_ID, trust_remote_code=True)\n\t    # Attention: either \"torch\" (default), \"flash\" or \"triton\"\n\t    config.attn_config[\"attn_impl\"] = \"torch\"\n\t    config.update({\"max_seq_len\": MAX_SEQUENCE_LENGTH})\n", "    model = AutoModelForCausalLM.from_pretrained(\n\t        MODEL_ID,\n\t        config=config,\n\t        torch_dtype=torch.bfloat16,\n\t        trust_remote_code=True,\n\t    ).to(device)\n\t    def complete(\n\t        self,\n\t        prompt: str = \"<|endoftext|>\",\n\t        max_tokens: int = 512,\n", "        temperature: float = 0.6,\n\t        end_of_text: Union[str, list, tuple] = (\"<|endoftext|>\",),\n\t        *args,\n\t        **kwargs,\n\t    ) -> str:\n\t        try:\n\t            if isinstance(end_of_text, str):\n\t                end_of_text = (end_of_text,)\n\t            logging.info(f\"Input prompt:\\n{prompt}\")\n\t            inputs = self.tokenizer(\n", "                prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQUENCE_LENGTH // 2\n\t            ).to(self.model.device)\n\t            # Use Torch's Flash attention\n\t            with torch.backends.cuda.sdp_kernel(\n\t                enable_flash=True, enable_math=False, enable_mem_efficient=False\n\t            ):\n\t                outputs = self.model.generate(\n\t                    **inputs,\n\t                    max_new_tokens=max_tokens,\n\t                    do_sample=True,\n", "                    temperature=temperature,\n\t                    pad_token_id=self.tokenizer.eos_token_id,\n\t                )\n\t                output = self.tokenizer.batch_decode(outputs)[0]\n\t                logging.info(f\"Model output:\\n{output}\")\n\t                # Remove the context from the output\n\t                output = output[len(prompt) :]\n\t                # Stop if end of text\n\t                for item in end_of_text:\n\t                    if item in output:\n", "                        output = output.split(item)[0]\n\t                    break\n\t                # Avoid issues with GPU vRAM\n\t                del inputs\n\t                torch.cuda.empty_cache()\n\t                return output\n\t        except Exception as ex:\n\t            logging.exception(ex)\n\t        return \"\"\n\t    def stream_complete(\n", "        self,\n\t        prompt: str = None,\n\t        max_tokens: int = 512,\n\t        temperature: float = 0.6,\n\t        end_of_text: Union[str, list, tuple] = (\"<|endoftext|>\",),\n\t        *args,\n\t        **kwargs,\n\t    ) -> str:\n\t        try:\n\t            if isinstance(end_of_text, str):\n", "                end_of_text = (end_of_text,)\n\t            stop = StopOnTokens(stop_token_ids=self.tokenizer.convert_tokens_to_ids(end_of_text))\n\t            if isinstance(end_of_text, str):\n\t                end_of_text = (end_of_text,)\n\t            logging.info(f\"Input prompt:\\n{prompt}\")\n\t            inputs = self.tokenizer(\n\t                prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQUENCE_LENGTH // 2\n\t            ).to(self.model.device)\n\t            # Use Torch's Flash attention\n\t            with torch.backends.cuda.sdp_kernel(\n", "                enable_flash=True, enable_math=False, enable_mem_efficient=False\n\t            ):\n\t                # Generate stream, yield delta\n\t                streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)\n\t                generation_kwargs = dict(\n\t                    **inputs,\n\t                    streamer=streamer,\n\t                    max_new_tokens=max_tokens,\n\t                    do_sample=True,\n\t                    temperature=temperature,\n", "                    pad_token_id=self.tokenizer.eos_token_id,\n\t                    stopping_criteria=StoppingCriteriaList([stop]),\n\t                )\n\t                stream_complete = Event()\n\t                def generate_and_signal_complete(generation_kwargs=generation_kwargs):\n\t                    self.model.generate(**generation_kwargs)\n\t                    stream_complete.set()\n\t                thread = Thread(target=generate_and_signal_complete)\n\t                thread.start()\n\t                logging.info(\"[Completions/Streaming] Output:\")\n", "                ended = False\n\t                for delta in streamer:\n\t                    if delta:\n\t                        for item in end_of_text:\n\t                            if item in delta:\n\t                                logging.info(delta)\n\t                                yield delta.split(item)[0]\n\t                                ended = True\n\t                                break\n\t                        if ended:\n", "                            break\n\t                        logging.info(delta)\n\t                        yield delta\n\t                thread.join(timeout=60)\n\t                # Avoid issues with GPU vRAM\n\t                del streamer\n\t                del inputs\n\t                torch.cuda.empty_cache()\n\t        except Exception as ex:\n\t            logging.exception(ex)\n", "        return\n"]}
{"filename": "examples/MPT-7B-Storywriter-65kplus/get_models.py", "chunked_list": ["import logging\n\tMODEL_ID = \"mosaicml/mpt-7b-storywriter\"\n\tif __name__ == \"__main__\":\n\t    from huggingface_hub import snapshot_download\n\t    from transformers.utils import move_cache\n\t    for repo_id in (MODEL_ID,):\n\t        try:\n\t            snapshot_download(repo_id)\n\t        except Exception as ex:\n\t            logging.exception(f\"Could not retrieve {repo_id}: {ex}\")\n", "    try:\n\t        move_cache()\n\t    except Exception as ex:\n\t        logging.exception(f\"Could not migrate cache: {ex}\")\n"]}
{"filename": "examples/MPT-7B-Storywriter-65kplus/server.py", "chunked_list": ["import logging\n\tfrom model import CompletionModel as Model\n\tfrom simple_ai.api.grpc.completion.server import (\n\t    LanguageModelServicer as CompletionServicer,\n\t    serve,\n\t)\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    logging.basicConfig(level=logging.INFO)\n\t    parser = argparse.ArgumentParser()\n", "    parser.add_argument(\"-a\", \"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    logging.info(f\"Starting gRPC server on {args.address}\")\n\t    completion_servicer = CompletionServicer(model=Model())\n\t    serve(\n\t        address=args.address,\n\t        model_servicer=completion_servicer,\n\t    )\n"]}
{"filename": "src/tests/__init__.py", "chunked_list": []}
{"filename": "src/simple_ai/api_models.py", "chunked_list": ["from enum import Enum\n\tfrom typing import List, Optional, Union\n\tfrom pydantic import BaseModel\n\tclass ExtendedEnum(Enum):\n\t    @classmethod\n\t    def list(cls):\n\t        return list(map(lambda c: c.value, cls))\n\tclass ModelInterfaceTypes(str, ExtendedEnum):\n\t    gRPC = \"gRPC\"\n\tclass ModelTaskTypes(str, ExtendedEnum):\n", "    complete = \"complete\"\n\t    chat = \"chat\"\n\t    embed = \"embed\"\n\t    @classmethod\n\t    def list(cls):\n\t        return list(map(lambda c: c.value, cls))\n\tclass ModelMetadata(BaseModel):\n\t    owned_by: Optional[str]\n\t    permission: Optional[List]\n\t    description: Optional[str] = \"\"\n", "class ModelInterface(BaseModel):\n\t    type: ModelInterfaceTypes = \"gRPC\"\n\t    url: str\n\tclass ModelConfig(BaseModel):\n\t    metadata: ModelMetadata\n\t    network: ModelInterface\n\tclass EmbeddingInput(BaseModel):\n\t    model: str\n\t    input: Union[str, list]\n\t    user: str = \"\"\n", "class CompletionInput(BaseModel):\n\t    model: str\n\t    prompt: Union[str, List[str]] = \"<|endoftext|>\"\n\t    suffix: str = \"\"\n\t    max_tokens: int = 7\n\t    temperature: float = 1.0\n\t    top_p: float = 1.0\n\t    n: int = 1\n\t    stream: bool = False\n\t    logprobs: int = 0\n", "    echo: bool = False\n\t    stop: Optional[Union[str, list]] = \"\"\n\t    presence_penalty: float = 0.0\n\t    frequence_penalty: float = 0.0\n\t    best_of: int = 0\n\t    logit_bias: dict = {}\n\t    user: str = \"\"\n\tclass ChatCompletionInput(BaseModel):\n\t    model: str\n\t    messages: list[dict]\n", "    temperature: float = 1.0\n\t    top_p: float = 1.0\n\t    n: int = 1\n\t    stream: bool = False\n\t    stop: Optional[Union[str, list]] = \"\"\n\t    max_tokens: int = 7\n\t    presence_penalty: float = 0.0\n\t    frequence_penalty: float = 0.0\n\t    logit_bias: Optional[dict] = {}\n\t    user: str = \"\"\n", "class InstructionInput(BaseModel):\n\t    model: str\n\t    instruction: str\n\t    input: str = \"\"\n\t    top_p: float = 1.0\n\t    n: int = 1\n\t    temperature: float = 1.0\n\t    max_tokens: int = 256\n"]}
{"filename": "src/simple_ai/__main__.py", "chunked_list": ["import argparse\n\timport shutil\n\tfrom pathlib import Path\n\timport uvicorn\n\tdef serve_app(host=\"127.0.0.1\", port=8080, **kwargs):\n\t    from . import server\n\t    uvicorn.run(app=server.app, host=host, port=port)\n\tdef init_app(path=\"./\", **kwargs):\n\t    shutil.copy(\n\t        src=Path(Path(__file__).parent.absolute(), \"models.toml.template\"),\n", "        dst=Path(path, \"models.toml\"),\n\t    )\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    subparsers = parser.add_subparsers()\n\t    # Init config args\n\t    init_parser = subparsers.add_parser(\"init\")\n\t    init_parser.add_argument(\"--path\", default=\"./\", type=str)\n\t    init_parser.set_defaults(func=init_app)\n\t    # Serving args\n", "    serving_parser = subparsers.add_parser(\"serve\")\n\t    serving_parser.add_argument(\"--host\", default=\"127.0.0.1\", type=str)\n\t    serving_parser.add_argument(\"--port\", default=8080, type=int)\n\t    serving_parser.set_defaults(func=serve_app)\n\t    # Parse, call the appropriate function\n\t    args = parser.parse_args()\n\t    args.func(**args.__dict__)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/simple_ai/models.py", "chunked_list": ["import os\n\timport pathlib\n\timport sys\n\tfrom dataclasses import dataclass\n\tfrom typing import Union\n\tif sys.version_info >= (3, 11):\n\t    import tomllib\n\telse:\n\t    import tomli as tomllib\n\tfrom .api.grpc.chat import client as chat_client\n", "from .api.grpc.completion import client as lm_client\n\tfrom .api.grpc.embedding import client as embed_client\n\tfrom .api_models import ModelConfig, ModelInterfaceTypes, ModelTaskTypes\n\tpath = pathlib.Path(os.environ.get(\"SIMPLEAI_CONFIG_PATH\", \"models.toml\"))\n\twith path.open(mode=\"rb\") as fp:\n\t    MODELS_ZOO = tomllib.load(fp)\n\t@dataclass(unsafe_hash=True)\n\tclass RpcCompletionLanguageModel:\n\t    name: str\n\t    url: str\n", "    def complete(\n\t        self,\n\t        prompt: str = \"<|endoftext|>\",\n\t        suffix: str = \"\",\n\t        max_tokens: int = 7,\n\t        temperature: float = 1.0,\n\t        top_p: float = 1.0,\n\t        n: int = 1,\n\t        stream: bool = False,\n\t        logprobs: int = 0,\n", "        echo: bool = False,\n\t        stop: Union[str, list] = \"\",\n\t        presence_penalty: float = 0.0,\n\t        frequence_penalty: float = 0.0,\n\t        best_of: int = 0,\n\t        logit_bias: dict = {},\n\t    ) -> str:\n\t        return lm_client.run(\n\t            url=self.url,\n\t            prompt=prompt,\n", "            suffix=suffix,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            n=n,\n\t            stream=stream,\n\t            logprobs=logprobs,\n\t            echo=echo,\n\t            stop=stop,\n\t            presence_penalty=presence_penalty,\n", "            frequence_penalty=frequence_penalty,\n\t            best_of=best_of,\n\t            logit_bias=logit_bias,\n\t        )\n\t    def stream_complete(\n\t        self,\n\t        prompt: str = \"<|endoftext|>\",\n\t        suffix: str = \"\",\n\t        max_tokens: int = 7,\n\t        temperature: float = 1.0,\n", "        top_p: float = 1.0,\n\t        n: int = 1,\n\t        stream: bool = False,\n\t        logprobs: int = 0,\n\t        echo: bool = False,\n\t        stop: Union[str, list] = \"\",\n\t        presence_penalty: float = 0.0,\n\t        frequence_penalty: float = 0.0,\n\t        best_of: int = 0,\n\t        logit_bias: dict = {},\n", "    ) -> str:\n\t        yield from lm_client.run_stream(\n\t            url=self.url,\n\t            prompt=prompt,\n\t            suffix=suffix,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            n=n,\n\t            stream=stream,\n", "            logprobs=logprobs,\n\t            echo=echo,\n\t            stop=stop,\n\t            presence_penalty=presence_penalty,\n\t            frequence_penalty=frequence_penalty,\n\t            best_of=best_of,\n\t            logit_bias=logit_bias,\n\t        )\n\t@dataclass(unsafe_hash=True)\n\tclass RpcEmbeddingLanguageModel:\n", "    name: str\n\t    url: str\n\t    def embed(\n\t        self,\n\t        inputs: Union[str, list] = \"\",\n\t    ) -> str:\n\t        return embed_client.run(url=self.url, inputs=inputs)\n\t@dataclass(unsafe_hash=True)\n\tclass RpcChatLanguageModel:\n\t    name: str\n", "    url: str\n\t    def chat(\n\t        self,\n\t        messages: list[list[str]] = [],\n\t        max_tokens: int = 64,\n\t        temperature: float = 1.0,\n\t        top_p: float = 1.0,\n\t        n: int = 1,\n\t        stream: bool = False,\n\t        stop: Union[str, list] = \"\",\n", "        presence_penalty: float = 0.0,\n\t        frequence_penalty: float = 0.0,\n\t        logit_bias: dict = {},\n\t    ) -> str:\n\t        return chat_client.run(\n\t            url=self.url,\n\t            messages=messages,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n", "            n=n,\n\t            stream=stream,\n\t            stop=stop,\n\t            presence_penalty=presence_penalty,\n\t            frequence_penalty=frequence_penalty,\n\t            logit_bias=logit_bias,\n\t        )\n\t    def stream(\n\t        self,\n\t        messages: list[list[str]] = [],\n", "        max_tokens: int = 64,\n\t        temperature: float = 1.0,\n\t        top_p: float = 1.0,\n\t        n: int = 1,\n\t        stream: bool = False,\n\t        stop: Union[str, list] = \"\",\n\t        presence_penalty: float = 0.0,\n\t        frequence_penalty: float = 0.0,\n\t        logit_bias: dict = {},\n\t    ) -> str:\n", "        yield from chat_client.run_stream(\n\t            url=self.url,\n\t            messages=messages,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            n=n,\n\t            stream=stream,\n\t            stop=stop,\n\t            presence_penalty=presence_penalty,\n", "            frequence_penalty=frequence_penalty,\n\t            logit_bias=logit_bias,\n\t        )\n\tdef select_model_type(model_interface: str, task: str):\n\t    if model_interface == \"gRPC\":\n\t        if task == \"embed\":\n\t            return RpcEmbeddingLanguageModel\n\t        if task == \"chat\":\n\t            return RpcChatLanguageModel\n\t        if task == \"complete\":\n", "            return RpcCompletionLanguageModel\n\t        raise ValueError(f\"`task` value must be in {ModelTaskTypes.list()}, got `{task}` instead`.\")\n\t    return ValueError(\n\t        f\"`model_interface` value must be in {ModelInterfaceTypes.list()} `gRPC`, got\"\n\t        f\" `{model_interface}` instead.\"\n\t    )\n\tdef get_model(model_id: str, task: ModelTaskTypes, metadata: dict = MODELS_ZOO):\n\t    if model_id in metadata.keys():\n\t        model_config = ModelConfig(**metadata.get(model_id)).network\n\t        return select_model_type(model_config.type, task)(name=model_id, url=model_config.url)\n", "    else:\n\t        raise ValueError(f\"Cannot find model named `{model_id}` in configuration.\")\n\tdef list_models(metadata: dict = MODELS_ZOO) -> list:\n\t    return dict(\n\t        data=[\n\t            {\"id\": key, **meta.get(\"metadata\"), \"object\": \"model\"} for key, meta in metadata.items()\n\t        ],\n\t        object=\"list\",\n\t    )\n\tdef get_model_infos(model_id, metadata: dict = MODELS_ZOO) -> list:\n", "    if model_id in metadata.keys():\n\t        return {\"id\": model_id, **metadata.get(model_id).get(\"metadata\")}\n\t    return {}\n"]}
{"filename": "src/simple_ai/__init__.py", "chunked_list": []}
{"filename": "src/simple_ai/utils.py", "chunked_list": ["import json\n\timport uuid\n\tfrom datetime import datetime as dt\n\tfrom .dummy import dummy_usage\n\tdef format_autocompletion_response(model_name, predictions, usage=dummy_usage) -> dict:\n\t    response_id = uuid.uuid4()\n\t    current_timestamp = int(dt.now().timestamp())\n\t    return {\n\t        \"id\": response_id,\n\t        \"object\": \"text_completion\",\n", "        \"created\": current_timestamp,\n\t        \"model\": model_name,\n\t        \"choices\": [\n\t            {\"text\": text, \"index\": idx, \"logprobs\": None, \"finish_reason\": \"\"}\n\t            for idx, text in enumerate(predictions)\n\t        ],\n\t        \"usage\": usage,\n\t    }\n\tdef format_autocompletion_stream_response(\n\t    current_timestamp, response_id, model_name, predictions\n", ") -> dict:\n\t    data = {\n\t        \"id\": response_id,\n\t        \"object\": \"text_completion\",\n\t        \"created\": current_timestamp,\n\t        \"model\": model_name,\n\t        \"choices\": [\n\t            {\"text\": text, \"index\": idx, \"logprobs\": None, \"finish_reason\": None}\n\t            for idx, text in enumerate(predictions)\n\t        ],\n", "    }\n\t    data = f\"DATA: {data}\\n\\n\"\n\t    return data\n\tdef format_edits_response(model_name, predictions, usage=dummy_usage) -> dict:\n\t    response_id = uuid.uuid4()\n\t    current_timestamp = int(dt.now().timestamp())\n\t    return {\n\t        \"id\": response_id,\n\t        \"object\": \"edit\",\n\t        \"created\": current_timestamp,\n", "        \"model\": model_name,\n\t        \"choices\": [\n\t            {\n\t                \"text\": text,\n\t                \"index\": idx,\n\t            }\n\t            for idx, text in enumerate(predictions)\n\t        ],\n\t        \"usage\": usage,\n\t    }\n", "def format_chat_response(model_name: str, predictions, usage=dummy_usage) -> dict:\n\t    response_id = uuid.uuid4()\n\t    current_timestamp = int(dt.now().timestamp())\n\t    return {\n\t        \"id\": response_id,\n\t        \"model\": model_name,\n\t        \"object\": \"chat.completion\",\n\t        \"created\": current_timestamp,\n\t        \"choices\": [\n\t            {\n", "                \"index\": idx,\n\t                \"message\": message,\n\t                \"finish_reason\": \"stop\",\n\t            }\n\t            for idx, message in enumerate(predictions)\n\t        ],\n\t        \"usage\": usage,\n\t    }\n\tdef format_chat_delta_response_helper(\n\t    current_timestamp, response_id, model_name: str, predictions, finish_reason=None\n", ") -> dict:\n\t    data = {\n\t        \"id\": response_id,\n\t        \"model\": model_name,\n\t        \"object\": \"chat.completion.chunk\",\n\t        \"created\": current_timestamp,\n\t        \"choices\": [\n\t            {\n\t                \"index\": idx,\n\t                \"delta\": message,\n", "                \"finish_reason\": finish_reason,\n\t            }\n\t            for idx, message in enumerate(predictions)\n\t        ],\n\t    }\n\t    return f\"data: {json.dumps(data)}\\n\\n\"\n\tdef format_chat_delta_response(\n\t    current_timestamp, response_id, model_name: str, predictions\n\t) -> dict:\n\t    data = format_chat_delta_response_helper(\n", "        current_timestamp, response_id, model_name, predictions, finish_reason=None\n\t    )\n\t    return data\n\tdef format_embeddings_results(model_name: str, embeddings: list, usage: dict = dummy_usage) -> dict:\n\t    return {\n\t        \"object\": \"list\",\n\t        \"data\": [\n\t            {\"object\": \"embedding\", \"embedding\": embedding, \"index\": idx}\n\t            for idx, embedding in enumerate(embeddings)\n\t        ],\n", "        \"model\": model_name,\n\t        \"usage\": usage,\n\t    }\n\tdef add_instructions(instructions: str, text: str) -> str:\n\t    prompt = f\"### Instruction:\\n{instructions}.\\n\\n\"\n\t    if text:\n\t        prompt += f\"### Input:\\n{text}.\\n\\n\"\n\t    prompt += \"### Response:\"\n\t    return prompt\n\tdef format_chat_log(chat: list[dict[str, str]] = dict()) -> str:\n", "    raw_chat_text = \"\"\n\t    for item in chat:\n\t        raw_chat_text += f\"{item.get('role')}: {item.get('content')}\\n\\n\"\n\t    return raw_chat_text + \"assistant: \"\n"]}
{"filename": "src/simple_ai/server.py", "chunked_list": ["from datetime import datetime as dt\n\tfrom functools import partial\n\tfrom itertools import chain\n\tfrom typing import Annotated\n\tfrom uuid import uuid4\n\timport fastapi\n\tfrom fastapi import Body, FastAPI, Response\n\tfrom fastapi.responses import StreamingResponse\n\tfrom .api_models import ChatCompletionInput, CompletionInput, EmbeddingInput, InstructionInput\n\tfrom .dummy import dummy_chat, dummy_complete, dummy_edit, dummy_embedding\n", "from .models import get_model, get_model_infos, list_models\n\tfrom .utils import (\n\t    add_instructions,\n\t    format_autocompletion_response,\n\t    format_autocompletion_stream_response,\n\t    format_chat_delta_response,\n\t    format_chat_delta_response_helper,\n\t    format_chat_response,\n\t    format_edits_response,\n\t    format_embeddings_results,\n", ")\n\tapp = FastAPI(\n\t    title=\"SimpleAI\",\n\t    description=\"A self-hosted alternative API to the not so Open one\",\n\t    version=\"0.2\",\n\t    terms_of_service=\"https://github.com/lhenault\",\n\t    contact={\n\t        \"name\": \"Lhenault\",\n\t        \"url\": \"https://github.com/lhenault\",\n\t    },\n", ")\n\t# Models\n\t@app.get(\"/models\")\n\tasync def show_models():\n\t    return list_models()\n\t@app.get(\"/models/{model_id}\")\n\tasync def show_model(model_id: str):\n\t    return get_model_infos(model_id)\n\t# Completions\n\t@app.post(\"/completions\")\n", "async def complete(\n\t    body: Annotated[CompletionInput, Body(example=dummy_complete)],\n\t    background_tasks: fastapi.background.BackgroundTasks,\n\t):\n\t    assert body.logprobs <= 5\n\t    prompt = body.prompt\n\t    if isinstance(prompt, list):\n\t        assert len(body.prompt) == 1, \"unsupported, at most 1 prompt allowed\"\n\t        prompt = body.prompt[0]\n\t    llm = get_model(model_id=body.model, task=\"complete\")\n", "    if not body.stream:\n\t        predictions = llm.complete(\n\t            prompt=prompt,\n\t            suffix=body.suffix,\n\t            max_tokens=body.max_tokens,\n\t            temperature=body.temperature,\n\t            top_p=body.top_p,\n\t            n=body.n,\n\t            stream=body.stream,\n\t            logprobs=body.logprobs,\n", "            echo=body.echo,\n\t            stop=body.stop,\n\t            presence_penalty=body.presence_penalty,\n\t            frequence_penalty=body.frequence_penalty,\n\t            best_of=body.best_of,\n\t            logit_bias=body.logit_bias,\n\t        )\n\t        output = format_autocompletion_response(model_name=llm.name, predictions=predictions)\n\t        return output\n\t    predictions_stream = llm.stream_complete(\n", "        prompt=prompt,\n\t        suffix=body.suffix,\n\t        max_tokens=body.max_tokens,\n\t        temperature=body.temperature,\n\t        top_p=body.top_p,\n\t        n=body.n,\n\t        stream=body.stream,\n\t        logprobs=body.logprobs,\n\t        echo=body.echo,\n\t        stop=body.stop,\n", "        presence_penalty=body.presence_penalty,\n\t        frequence_penalty=body.frequence_penalty,\n\t        best_of=body.best_of,\n\t        logit_bias=body.logit_bias,\n\t    )\n\t    background_tasks.add_task(lambda f: f.close(), predictions_stream)\n\t    uuid = uuid4().hex\n\t    current_timestamp = int(dt.now().timestamp())\n\t    postprocessed = map(\n\t        partial(format_autocompletion_stream_response, current_timestamp, uuid, body.model),\n", "        predictions_stream,\n\t    )\n\t    with_finaliser = chain(postprocessed, (\"data: [DONE]\\n\",))\n\t    return StreamingResponse(with_finaliser, media_type=\"text/event-stream\")\n\t# Chat / completions\n\t@app.post(\"/chat/completions\")\n\tasync def chat_complete(\n\t    body: Annotated[ChatCompletionInput, Body(example=dummy_chat)],\n\t    response: Response,\n\t    background_tasks: fastapi.background.BackgroundTasks,\n", "):\n\t    llm = get_model(model_id=body.model, task=\"chat\")\n\t    messages = [[message.get(\"role\", \"\"), message.get(\"content\", \"\")] for message in body.messages]\n\t    if not body.stream:\n\t        predictions = llm.chat(\n\t            messages=messages,\n\t            temperature=body.temperature,\n\t            top_p=body.top_p,\n\t            n=body.n,\n\t            stream=body.stream,\n", "            max_tokens=body.max_tokens,\n\t            stop=body.stop,\n\t            presence_penalty=body.presence_penalty,\n\t            frequence_penalty=body.frequence_penalty,\n\t            logit_bias=body.logit_bias,\n\t        )\n\t        output = format_chat_response(model_name=llm.name, predictions=predictions)\n\t        return output\n\t    predictions_stream = llm.stream(\n\t        messages=messages,\n", "        temperature=body.temperature,\n\t        top_p=body.top_p,\n\t        n=body.n,\n\t        stream=body.stream,\n\t        max_tokens=body.max_tokens,\n\t        stop=body.stop,\n\t        presence_penalty=body.presence_penalty,\n\t        frequence_penalty=body.frequence_penalty,\n\t        logit_bias=body.logit_bias,\n\t    )\n", "    background_tasks.add_task(lambda f: f.close(), predictions_stream)\n\t    uuid = uuid4().hex\n\t    current_timestamp = int(dt.now().timestamp())\n\t    postprocessed = map(\n\t        partial(format_chat_delta_response, current_timestamp, uuid, body.model), predictions_stream\n\t    )\n\t    stop_message = format_chat_delta_response_helper(\n\t        current_timestamp, uuid, body.model, predictions=(\"\",), finish_reason=\"stop\"\n\t    )\n\t    with_finaliser = chain(postprocessed, stop_message, (\"data: [DONE]\\n\",))\n", "    return StreamingResponse(with_finaliser, media_type=\"text/event-stream\")\n\t# Edits\n\t@app.post(\"/edits\")\n\tasync def edit(body: Annotated[InstructionInput, Body(example=dummy_edit)]):\n\t    llm = get_model(model_id=body.model, task=\"complete\")\n\t    input_text = add_instructions(instructions=body.instruction, text=body.input)\n\t    predictions = llm.complete(\n\t        prompt=input_text,\n\t        temperature=body.temperature,\n\t        top_p=body.top_p,\n", "        n=body.n,\n\t        max_tokens=body.max_tokens,\n\t    )\n\t    output = format_edits_response(model_name=llm.name, predictions=predictions)\n\t    return output\n\t# Embeddings\n\t@app.post(\"/embeddings\")\n\tasync def embed(body: Annotated[EmbeddingInput, Body(example=dummy_embedding)]):\n\t    llm = get_model(model_id=body.model, task=\"embed\")\n\t    if isinstance(body.input, str):\n", "        body.input = [body.input]\n\t    results = llm.embed(inputs=body.input)\n\t    output = format_embeddings_results(model_name=llm.name, embeddings=results)\n\t    return output\n"]}
{"filename": "src/simple_ai/dummy.py", "chunked_list": ["dummy_models = [\n\t    {\n\t        \"data\": [\n\t            {\n\t                \"id\": \"model-id-0\",\n\t                \"object\": \"model\",\n\t                \"owned_by\": \"organization-owner\",\n\t                \"permission\": [],\n\t            },\n\t            {\n", "                \"id\": \"model-id-1\",\n\t                \"object\": \"model\",\n\t                \"owned_by\": \"organization-owner\",\n\t                \"permission\": [],\n\t            },\n\t            {\"id\": \"model-id-2\", \"object\": \"model\", \"owned_by\": \"openai\", \"permission\": []},\n\t        ],\n\t        \"object\": \"list\",\n\t    }\n\t]\n", "dummy_model = {\n\t    \"id\": \"model-id-0\",\n\t    \"object\": \"model\",\n\t    \"owned_by\": \"organization-owner\",\n\t    \"permission\": [],\n\t}\n\tdummy_complete = {\n\t    \"id\": \"cmpl-uqkvlQyYK7bGYrRHQ0eXlWi7\",\n\t    \"object\": \"text_completion\",\n\t    \"created\": 1589478378,\n", "    \"model\": \"text-davinci-003\",\n\t    \"choices\": [\n\t        {\n\t            \"text\": \"\\n\\nThis is indeed a test\",\n\t            \"index\": 0,\n\t            \"logprobs\": None,\n\t            \"finish_reason\": \"length\",\n\t        }\n\t    ],\n\t    \"usage\": {\"prompt_tokens\": 5, \"completion_tokens\": 7, \"total_tokens\": 12},\n", "}\n\tdummy_chat = {\n\t    \"id\": \"chatcmpl-123\",\n\t    \"object\": \"chat.completion\",\n\t    \"created\": 1677652288,\n\t    \"choices\": [\n\t        {\n\t            \"index\": 0,\n\t            \"message\": {\n\t                \"role\": \"assistant\",\n", "                \"content\": \"\\n\\nHello there, how may I assist you today?\",\n\t            },\n\t            \"finish_reason\": \"stop\",\n\t        }\n\t    ],\n\t    \"usage\": {\"prompt_tokens\": 9, \"completion_tokens\": 12, \"total_tokens\": 21},\n\t}\n\tdummy_edit = {\n\t    \"object\": \"edit\",\n\t    \"created\": 1589478378,\n", "    \"choices\": [\n\t        {\n\t            \"text\": \"What day of the week is it?\",\n\t            \"index\": 0,\n\t        }\n\t    ],\n\t    \"usage\": {\"prompt_tokens\": 25, \"completion_tokens\": 32, \"total_tokens\": 57},\n\t}\n\tdummy_embedding = {\n\t    \"object\": \"list\",\n", "    \"data\": [\n\t        {\n\t            \"object\": \"embedding\",\n\t            \"embedding\": [\n\t                0.0023064255,\n\t                -0.009327292,\n\t                -0.0028842222,\n\t            ],\n\t            \"index\": 0,\n\t        }\n", "    ],\n\t    \"model\": \"text-embedding-ada-002\",\n\t    \"usage\": {\"prompt_tokens\": 8, \"total_tokens\": 8},\n\t}\n\tdummy_usage = {\"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0}\n"]}
{"filename": "src/simple_ai/api/grpc/chat/llm_chat_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: llm_chat.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf.internal import builder as _builder\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n\t    b'\\n\\x0ellm_chat.proto\\x12\\x11languagemodelchat\"\\xe3\\x01\\n\\x0c\\x43hatLogInput\\x12)\\n\\x08messages\\x18\\x01'\n\t    b\" \\x03(\\x0b\\x32\\x17.languagemodelchat.Chat\\x12\\x12\\n\\nmax_tokens\\x18\\x02\"\n\t    b\" \\x01(\\x05\\x12\\x13\\n\\x0btemperature\\x18\\x03 \\x01(\\x02\\x12\\r\\n\\x05top_p\\x18\\x04\"\n\t    b\" \\x01(\\x02\\x12\\t\\n\\x01n\\x18\\x05 \\x01(\\x05\\x12\\x0e\\n\\x06stream\\x18\\x06\"\n\t    b\" \\x01(\\x08\\x12\\x0c\\n\\x04stop\\x18\\x07 \\x01(\\t\\x12\\x18\\n\\x10presence_penalty\\x18\\x08\"\n\t    b\" \\x01(\\x02\\x12\\x19\\n\\x11\\x66requence_penalty\\x18\\t \\x01(\\x02\\x12\\x12\\n\\nlogit_bias\\x18\\n\"\n\t    b' \\x01(\\t\":\\n\\rChatLogOutput\\x12)\\n\\x08messages\\x18\\x01'\n\t    b' \\x03(\\x0b\\x32\\x17.languagemodelchat.Chat\"D\\n\\x04\\x43hat\\x12\\x11\\n\\x04role\\x18\\x01'\n\t    b\" \\x01(\\tH\\x00\\x88\\x01\\x01\\x12\\x14\\n\\x07\\x63ontent\\x18\\x02\"\n", "    b\" \\x01(\\tH\\x01\\x88\\x01\\x01\\x42\\x07\\n\\x05_roleB\\n\\n\\x08_content2\\xad\\x01\\n\\rLanguageModel\\x12K\\n\\x04\\x43hat\\x12\\x1f.languagemodelchat.ChatLogInput\\x1a\"\n\t    b' .languagemodelchat.ChatLogOutput\"\\x00\\x12O\\n\\x06Stream\\x12\\x1f.languagemodelchat.ChatLogInput\\x1a'\n\t    b' .languagemodelchat.ChatLogOutput\"\\x00\\x30\\x01\\x42\\x33\\n\\x15io.grpc.examples.chatB\\x11LanguageModelChatP\\x01\\xa2\\x02\\x04\\x63hatb\\x06proto3'\n\t)\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"llm_chat_pb2\", globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t    DESCRIPTOR._options = None\n\t    DESCRIPTOR._serialized_options = (\n\t        b\"\\n\\025io.grpc.examples.chatB\\021LanguageModelChatP\\001\\242\\002\\004chat\"\n", "    )\n\t    _CHATLOGINPUT._serialized_start = 38\n\t    _CHATLOGINPUT._serialized_end = 265\n\t    _CHATLOGOUTPUT._serialized_start = 267\n\t    _CHATLOGOUTPUT._serialized_end = 325\n\t    _CHAT._serialized_start = 327\n\t    _CHAT._serialized_end = 395\n\t    _LANGUAGEMODEL._serialized_start = 398\n\t    _LANGUAGEMODEL._serialized_end = 571\n\t# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "src/simple_ai/api/grpc/chat/model.py", "chunked_list": ["from dataclasses import dataclass\n\t@dataclass(unsafe_hash=True)\n\tclass LanguageModel:\n\t    def chat(self, chatlog: list[list[str]] = [], *args, **kwargs) -> list:\n\t        return [\n\t            {\"role\": message.get(\"role\"), \"content\": message.get(\"content\")[::-1]}\n\t            for message in chatlog\n\t        ]\n\t    def stream(self, chatlog: list[list[str]] = [], *args, **kwargs) -> list:\n\t        raise NotImplementedError()\n"]}
{"filename": "src/simple_ai/api/grpc/chat/client.py", "chunked_list": ["\"\"\"The Python implementation of the gRPC route guide client.\"\"\"\n\tfrom __future__ import print_function\n\tfrom typing import List, Union\n\timport grpc\n\tfrom google.protobuf.json_format import MessageToDict\n\tfrom . import llm_chat_pb2\n\tfrom . import llm_chat_pb2_grpc\n\tdef get_chatlog(stub, chatlog):\n\t    response = stub.Chat(chatlog)\n\t    results = []\n", "    for message in response.messages:\n\t        results.append(MessageToDict(message))\n\t    return results\n\tdef run(\n\t    url: str = \"localhost:50051\",\n\t    messages: List[List[str]] = [],\n\t    max_tokens: int = 512,\n\t    temperature: float = 1.0,\n\t    top_p: float = 1.0,\n\t    n: int = 1,\n", "    stream: bool = False,\n\t    stop: Union[str, list] = \"\",\n\t    presence_penalty: float = 0.0,\n\t    frequence_penalty: float = 0.0,\n\t    logit_bias: dict = {},\n\t):\n\t    with grpc.insecure_channel(url) as channel:\n\t        stub = llm_chat_pb2_grpc.LanguageModelStub(channel)\n\t        grpc_chatlog = llm_chat_pb2.ChatLogInput(\n\t            max_tokens=max_tokens,\n", "            temperature=temperature,\n\t            top_p=top_p,\n\t            n=n,\n\t            stream=stream,\n\t            stop=str(stop),\n\t            presence_penalty=presence_penalty,\n\t            frequence_penalty=frequence_penalty,\n\t            logit_bias=str(logit_bias),\n\t        )\n\t        for role, content in messages:\n", "            grpc_chat = llm_chat_pb2.Chat(role=role, content=content)\n\t            grpc_chatlog.messages.append(grpc_chat)\n\t        return get_chatlog(stub, grpc_chatlog)\n\tdef stream_chatlog(stub, chatlog):\n\t    responses = stub.Stream(chatlog)\n\t    try:\n\t        yield from map(lambda x: [MessageToDict(x_i) for x_i in x.messages], responses)\n\t    finally:\n\t        responses.cancel()\n\tdef run_stream(\n", "    url: str = \"localhost:50051\",\n\t    messages: List[List[str]] = [],\n\t    max_tokens: int = 512,\n\t    temperature: float = 1.0,\n\t    top_p: float = 1.0,\n\t    n: int = 1,\n\t    stream: bool = False,\n\t    stop: Union[str, list] = \"\",\n\t    presence_penalty: float = 0.0,\n\t    frequence_penalty: float = 0.0,\n", "    logit_bias: dict = {},\n\t):\n\t    with grpc.insecure_channel(url) as channel:\n\t        stub = llm_chat_pb2_grpc.LanguageModelStub(channel)\n\t        grpc_chatlog = llm_chat_pb2.ChatLogInput(\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            n=n,\n\t            stream=stream,\n", "            stop=str(stop),\n\t            presence_penalty=presence_penalty,\n\t            frequence_penalty=frequence_penalty,\n\t            logit_bias=str(logit_bias),\n\t        )\n\t        for role, content in messages:\n\t            grpc_chat = llm_chat_pb2.Chat(role=role, content=content)\n\t            grpc_chatlog.messages.append(grpc_chat)\n\t        yield from stream_chatlog(stub, grpc_chatlog)\n\tif __name__ == \"__main__\":\n", "    import argparse\n\t    import logging\n\t    logging.basicConfig()\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    res = run(messages=[[\"user\", \"hello\"] for _ in range(5)])\n\t    print(res)\n"]}
{"filename": "src/simple_ai/api/grpc/chat/llm_chat_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport grpc\n\tfrom . import llm_chat_pb2 as llm__chat__pb2\n\tclass LanguageModelStub(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n\t            channel: A grpc.Channel.\n", "        \"\"\"\n\t        self.Chat = channel.unary_unary(\n\t            \"/languagemodelchat.LanguageModel/Chat\",\n\t            request_serializer=llm__chat__pb2.ChatLogInput.SerializeToString,\n\t            response_deserializer=llm__chat__pb2.ChatLogOutput.FromString,\n\t        )\n\t        self.Stream = channel.unary_stream(\n\t            \"/languagemodelchat.LanguageModel/Stream\",\n\t            request_serializer=llm__chat__pb2.ChatLogInput.SerializeToString,\n\t            response_deserializer=llm__chat__pb2.ChatLogOutput.FromString,\n", "        )\n\tclass LanguageModelServicer(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    def Chat(self, request, context):\n\t        \"\"\"Simple RPC\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\t    def Stream(self, request, context):\n\t        \"\"\"Server-to-client streaming RPC.\"\"\"\n", "        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\tdef add_LanguageModelServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n\t        \"Chat\": grpc.unary_unary_rpc_method_handler(\n\t            servicer.Chat,\n\t            request_deserializer=llm__chat__pb2.ChatLogInput.FromString,\n\t            response_serializer=llm__chat__pb2.ChatLogOutput.SerializeToString,\n\t        ),\n", "        \"Stream\": grpc.unary_stream_rpc_method_handler(\n\t            servicer.Stream,\n\t            request_deserializer=llm__chat__pb2.ChatLogInput.FromString,\n\t            response_serializer=llm__chat__pb2.ChatLogOutput.SerializeToString,\n\t        ),\n\t    }\n\t    generic_handler = grpc.method_handlers_generic_handler(\n\t        \"languagemodelchat.LanguageModel\", rpc_method_handlers\n\t    )\n\t    server.add_generic_rpc_handlers((generic_handler,))\n", "# This class is part of an EXPERIMENTAL API.\n\tclass LanguageModel(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    @staticmethod\n\t    def Chat(\n\t        request,\n\t        target,\n\t        options=(),\n\t        channel_credentials=None,\n\t        call_credentials=None,\n", "        insecure=False,\n\t        compression=None,\n\t        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n\t    ):\n\t        return grpc.experimental.unary_unary(\n\t            request,\n\t            target,\n\t            \"/languagemodelchat.LanguageModel/Chat\",\n", "            llm__chat__pb2.ChatLogInput.SerializeToString,\n\t            llm__chat__pb2.ChatLogOutput.FromString,\n\t            options,\n\t            channel_credentials,\n\t            insecure,\n\t            call_credentials,\n\t            compression,\n\t            wait_for_ready,\n\t            timeout,\n\t            metadata,\n", "        )\n\t    @staticmethod\n\t    def Stream(\n\t        request,\n\t        target,\n\t        options=(),\n\t        channel_credentials=None,\n\t        call_credentials=None,\n\t        insecure=False,\n\t        compression=None,\n", "        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n\t    ):\n\t        return grpc.experimental.unary_stream(\n\t            request,\n\t            target,\n\t            \"/languagemodelchat.LanguageModel/Stream\",\n\t            llm__chat__pb2.ChatLogInput.SerializeToString,\n\t            llm__chat__pb2.ChatLogOutput.FromString,\n", "            options,\n\t            channel_credentials,\n\t            insecure,\n\t            call_credentials,\n\t            compression,\n\t            wait_for_ready,\n\t            timeout,\n\t            metadata,\n\t        )\n"]}
{"filename": "src/simple_ai/api/grpc/chat/server.py", "chunked_list": ["\"\"\"The Python implementation of the gRPC server.\"\"\"\n\tfrom concurrent import futures\n\timport grpc\n\tfrom google.protobuf.json_format import MessageToDict\n\tfrom . import llm_chat_pb2\n\tfrom . import llm_chat_pb2_grpc\n\tfrom .model import LanguageModel\n\tclass LanguageModelServicer(llm_chat_pb2_grpc.LanguageModelServicer):\n\t    \"\"\"Provides methods that implement functionality of route guide server.\"\"\"\n\t    def __init__(self, model=LanguageModel()) -> None:\n", "        super().__init__()\n\t        self.model = model\n\t    def Chat(self, request, context):\n\t        output = self.model.chat(\n\t            chatlog=[MessageToDict(message=message) for message in request.messages],\n\t            max_tokens=request.max_tokens,\n\t            temperature=request.temperature,\n\t            top_p=request.top_p,\n\t            n=request.n,\n\t            stream=request.stream,\n", "            stop=request.stop,\n\t            presence_penalty=request.presence_penalty,\n\t            frequence_penalty=request.frequence_penalty,\n\t            logit_bias=request.logit_bias,\n\t        )\n\t        grpc_chatlog = llm_chat_pb2.ChatLogOutput()\n\t        for chat in output:\n\t            grpc_chat = llm_chat_pb2.Chat(role=chat.get(\"role\"), content=chat.get(\"content\"))\n\t            grpc_chatlog.messages.append(grpc_chat)\n\t        return grpc_chatlog\n", "    def Stream(self, request, context):\n\t        output = self.model.stream(\n\t            chatlog=[MessageToDict(message=message) for message in request.messages],\n\t            max_tokens=request.max_tokens,\n\t            temperature=request.temperature,\n\t            top_p=request.top_p,\n\t            n=request.n,\n\t            stream=request.stream,\n\t            stop=request.stop,\n\t            presence_penalty=request.presence_penalty,\n", "            frequence_penalty=request.frequence_penalty,\n\t            logit_bias=request.logit_bias,\n\t        )\n\t        for chat in output:\n\t            grpc_chatlog = llm_chat_pb2.ChatLogOutput()\n\t            for message in chat:\n\t                grpc_chat = llm_chat_pb2.Chat(\n\t                    role=message.get(\"role\"), content=message.get(\"content\")\n\t                )\n\t                grpc_chatlog.messages.append(grpc_chat)\n", "            yield grpc_chatlog\n\tdef serve(address=\"[::]:50051\", model_servicer=LanguageModelServicer(), max_workers=10):\n\t    server = grpc.server(futures.ThreadPoolExecutor(max_workers=max_workers))\n\t    llm_chat_pb2_grpc.add_LanguageModelServicer_to_server(model_servicer, server)\n\t    server.add_insecure_port(address=address)\n\t    server.start()\n\t    server.wait_for_termination()\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    import logging\n", "    logging.basicConfig()\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    serve(address=args.address)\n"]}
{"filename": "src/simple_ai/api/grpc/embedding/model.py", "chunked_list": ["from typing import List\n\tfrom dataclasses import dataclass\n\t@dataclass(unsafe_hash=True)\n\tclass LanguageModel:\n\t    def embed(\n\t        self,\n\t        inputs: list = [],\n\t    ) -> List[list]:\n\t        # TODO : implement method for your LLM\n\t        return [[]]\n"]}
{"filename": "src/simple_ai/api/grpc/embedding/llm_embed_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: llm_embed.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf.internal import builder as _builder\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n\t    b'\\n\\x0fllm_embed.proto\\x12\\x17languagemodelembeddings\"\\x1b\\n\\tSentences\\x12\\x0e\\n\\x06inputs\\x18\\x01'\n\t    b' \\x03(\\t\"\\x1c\\n\\tEmbedding\\x12\\x0f\\n\\x07\\x66\\x65\\x61ture\\x18\\x01'\n\t    b' \\x03(\\x02\"I\\n\\x10ListOfEmbeddings\\x12\\x35\\n\\tembedding\\x18\\x01'\n\t    b' \\x03(\\x0b\\x32\".languagemodelembeddings.Embedding2i\\n\\rLanguageModel\\x12X\\n\\x05\\x45mbed\\x12\".languagemodelembeddings.Sentences\\x1a).languagemodelembeddings.ListOfEmbeddings\"\\x00\\x42:\\n\\x16io.grpc.examples.embedB\\x16LanguageModelEmbeddingP\\x01\\xa2\\x02\\x05\\x65mbedb\\x06proto3'\n\t)\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"llm_embed_pb2\", globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t    DESCRIPTOR._options = None\n", "    DESCRIPTOR._serialized_options = (\n\t        b\"\\n\\026io.grpc.examples.embedB\\026LanguageModelEmbeddingP\\001\\242\\002\\005embed\"\n\t    )\n\t    _SENTENCES._serialized_start = 44\n\t    _SENTENCES._serialized_end = 71\n\t    _EMBEDDING._serialized_start = 73\n\t    _EMBEDDING._serialized_end = 101\n\t    _LISTOFEMBEDDINGS._serialized_start = 103\n\t    _LISTOFEMBEDDINGS._serialized_end = 176\n\t    _LANGUAGEMODEL._serialized_start = 178\n", "    _LANGUAGEMODEL._serialized_end = 283\n\t# @@protoc_insertion_point(module_scope)\n"]}
{"filename": "src/simple_ai/api/grpc/embedding/client.py", "chunked_list": ["\"\"\"The Python implementation of the gRPC route guide client.\"\"\"\n\tfrom __future__ import print_function\n\tfrom typing import List\n\timport grpc\n\tfrom google.protobuf.json_format import MessageToDict\n\tfrom . import llm_embed_pb2\n\tfrom . import llm_embed_pb2_grpc\n\tdef get_embeddings(stub, sentences):\n\t    response = stub.Embed(sentences)\n\t    results = []\n", "    for message in response.embedding:\n\t        results.append(MessageToDict(message).get(\"feature\"))\n\t    return results\n\tdef run(\n\t    url: str = \"localhost:50051\",\n\t    inputs: List[str] = \"\",\n\t):\n\t    with grpc.insecure_channel(url) as channel:\n\t        stub = llm_embed_pb2_grpc.LanguageModelStub(channel)\n\t        sentences = llm_embed_pb2.Sentences(\n", "            inputs=inputs,\n\t        )\n\t        return get_embeddings(stub, sentences)\n"]}
{"filename": "src/simple_ai/api/grpc/embedding/llm_embed_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport grpc\n\tfrom . import llm_embed_pb2 as llm__embed__pb2\n\tclass LanguageModelStub(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n\t            channel: A grpc.Channel.\n", "        \"\"\"\n\t        self.Embed = channel.unary_unary(\n\t            \"/languagemodelembeddings.LanguageModel/Embed\",\n\t            request_serializer=llm__embed__pb2.Sentences.SerializeToString,\n\t            response_deserializer=llm__embed__pb2.ListOfEmbeddings.FromString,\n\t        )\n\tclass LanguageModelServicer(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    def Embed(self, request, context):\n\t        \"\"\"Simple RPC\"\"\"\n", "        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\tdef add_LanguageModelServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n\t        \"Embed\": grpc.unary_unary_rpc_method_handler(\n\t            servicer.Embed,\n\t            request_deserializer=llm__embed__pb2.Sentences.FromString,\n\t            response_serializer=llm__embed__pb2.ListOfEmbeddings.SerializeToString,\n\t        ),\n", "    }\n\t    generic_handler = grpc.method_handlers_generic_handler(\n\t        \"languagemodelembeddings.LanguageModel\", rpc_method_handlers\n\t    )\n\t    server.add_generic_rpc_handlers((generic_handler,))\n\t# This class is part of an EXPERIMENTAL API.\n\tclass LanguageModel(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    @staticmethod\n\t    def Embed(\n", "        request,\n\t        target,\n\t        options=(),\n\t        channel_credentials=None,\n\t        call_credentials=None,\n\t        insecure=False,\n\t        compression=None,\n\t        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n", "    ):\n\t        return grpc.experimental.unary_unary(\n\t            request,\n\t            target,\n\t            \"/languagemodelembeddings.LanguageModel/Embed\",\n\t            llm__embed__pb2.Sentences.SerializeToString,\n\t            llm__embed__pb2.ListOfEmbeddings.FromString,\n\t            options,\n\t            channel_credentials,\n\t            insecure,\n", "            call_credentials,\n\t            compression,\n\t            wait_for_ready,\n\t            timeout,\n\t            metadata,\n\t        )\n"]}
{"filename": "src/simple_ai/api/grpc/embedding/server.py", "chunked_list": ["\"\"\"The Python implementation of the gRPC server.\"\"\"\n\tfrom concurrent import futures\n\timport logging\n\timport grpc\n\tfrom . import llm_embed_pb2\n\tfrom . import llm_embed_pb2_grpc\n\tfrom .model import LanguageModel\n\tclass LanguageModelServicer(llm_embed_pb2_grpc.LanguageModelServicer):\n\t    \"\"\"Provides methods that implement functionality of route guide server.\"\"\"\n\t    def __init__(self, model=LanguageModel()) -> None:\n", "        super().__init__()\n\t        self.model = model\n\t    def Embed(self, request, context):\n\t        embeddings = self.model.embed(\n\t            inputs=request.inputs,\n\t        )\n\t        grpc_embeddings = llm_embed_pb2.ListOfEmbeddings()\n\t        for embedding in embeddings:\n\t            grpc_embedding = llm_embed_pb2.Embedding()\n\t            for feature in embedding:\n", "                grpc_embedding.feature.append(feature)\n\t            grpc_embeddings.embedding.append(grpc_embedding)\n\t        return grpc_embeddings\n\tdef serve(address=\"[::]:50051\", model_servicer=LanguageModelServicer(), max_workers=10):\n\t    server = grpc.server(futures.ThreadPoolExecutor(max_workers=max_workers))\n\t    llm_embed_pb2_grpc.add_LanguageModelServicer_to_server(model_servicer, server)\n\t    server.add_insecure_port(address=address)\n\t    server.start()\n\t    server.wait_for_termination()\n\tif __name__ == \"__main__\":\n", "    import argparse\n\t    logging.basicConfig()\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n\t    serve(address=args.address)\n"]}
{"filename": "src/simple_ai/api/grpc/completion/model.py", "chunked_list": ["from typing import Union\n\tfrom dataclasses import dataclass\n\t@dataclass(unsafe_hash=True)\n\tclass LanguageModel:\n\t    def complete(\n\t        self,\n\t        prompt: str = \"<|endoftext|>\",\n\t        suffix: str = \"\",\n\t        max_tokens: int = 7,\n\t        temperature: float = 1.0,\n", "        top_p: float = 1.0,\n\t        n: int = 1,\n\t        stream: bool = False,\n\t        logprobs: int = 0,\n\t        echo: bool = False,\n\t        stop: Union[str, list] = \"\",\n\t        presence_penalty: float = 0.0,\n\t        frequence_penalty: float = 0.0,\n\t        best_of: int = 0,\n\t        logit_bias: dict = {},\n", "    ) -> str:\n\t        # TODO : implement method for your LLM\n\t        return \"QWERTYUIOP\\nASDFGHJKL\\nZXCVBNM\"\n\t    def stream_complete(\n\t        self,\n\t        prompt: str = \"<|endoftext|>\",\n\t        suffix: str = \"\",\n\t        max_tokens: int = 7,\n\t        temperature: float = 1.0,\n\t        top_p: float = 1.0,\n", "        n: int = 1,\n\t        stream: bool = False,\n\t        logprobs: int = 0,\n\t        echo: bool = False,\n\t        stop: Union[str, list] = \"\",\n\t        presence_penalty: float = 0.0,\n\t        frequence_penalty: float = 0.0,\n\t        best_of: int = 0,\n\t        logit_bias: dict = {},\n\t    ) -> str:\n", "        # TODO : implement method for your LLM\n\t        return \"QWERTYUIOP\\nASDFGHJKL\\nZXCVBNM\".split(\"\\n\")\n"]}
{"filename": "src/simple_ai/api/grpc/completion/client.py", "chunked_list": ["\"\"\"The Python implementation of the gRPC route guide client.\"\"\"\n\tfrom __future__ import print_function\n\tfrom typing import Union\n\timport grpc\n\tfrom google.protobuf.json_format import MessageToDict\n\tfrom . import llm_pb2\n\tfrom . import llm_pb2_grpc\n\tdef get_one_completion(stub, message):\n\t    response = stub.Complete(message)\n\t    return response.reply\n", "def get_completion(stub, message):\n\t    return get_one_completion(stub=stub, message=message)\n\tdef run(\n\t    url: str = \"localhost:50051\",\n\t    prompt: str = \"<|endoftext|>\",\n\t    suffix: str = \"\",\n\t    max_tokens: int = 512,\n\t    temperature: float = 1.0,\n\t    top_p: float = 1.0,\n\t    n: int = 1,\n", "    stream: bool = False,\n\t    logprobs: int = 0,\n\t    echo: bool = False,\n\t    stop: Union[str, list] = \"\",\n\t    presence_penalty: float = 0.0,\n\t    frequence_penalty: float = 0.0,\n\t    best_of: int = 0,\n\t    logit_bias: dict = {},\n\t):\n\t    with grpc.insecure_channel(url) as channel:\n", "        stub = llm_pb2_grpc.LanguageModelStub(channel)\n\t        message = llm_pb2.Message(\n\t            prompt=prompt,\n\t            suffix=suffix,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n\t            n=n,\n\t            stream=stream,\n\t            logprobs=logprobs,\n", "            echo=echo,\n\t            stop=str(stop),\n\t            presence_penalty=presence_penalty,\n\t            frequence_penalty=frequence_penalty,\n\t            best_of=best_of,\n\t            logit_bias=str(logit_bias),\n\t        )\n\t        return [get_completion(stub, message)]\n\tdef stream_completions(stub, message):\n\t    responses = stub.StreamComplete(message)\n", "    try:\n\t        ## TODO x.reply should be a list of strings. wrapping in list here for now\n\t        yield from map(lambda x: [x.reply], responses)\n\t    finally:\n\t        responses.cancel()\n\tdef run_stream(\n\t    url: str = \"localhost:50051\",\n\t    prompt: str = \"<|endoftext|>\",\n\t    suffix: str = \"\",\n\t    max_tokens: int = 512,\n", "    temperature: float = 1.0,\n\t    top_p: float = 1.0,\n\t    n: int = 1,\n\t    stream: bool = False,\n\t    logprobs: int = 0,\n\t    echo: bool = False,\n\t    stop: Union[str, list] = \"\",\n\t    presence_penalty: float = 0.0,\n\t    frequence_penalty: float = 0.0,\n\t    best_of: int = 0,\n", "    logit_bias: dict = {},\n\t):\n\t    with grpc.insecure_channel(url) as channel:\n\t        stub = llm_pb2_grpc.LanguageModelStub(channel)\n\t        message = llm_pb2.Message(\n\t            prompt=prompt,\n\t            suffix=suffix,\n\t            max_tokens=max_tokens,\n\t            temperature=temperature,\n\t            top_p=top_p,\n", "            n=n,\n\t            stream=stream,\n\t            logprobs=logprobs,\n\t            echo=echo,\n\t            stop=str(stop),\n\t            presence_penalty=presence_penalty,\n\t            frequence_penalty=frequence_penalty,\n\t            best_of=best_of,\n\t            logit_bias=str(logit_bias),\n\t        )\n", "        yield from stream_completions(stub, message)\n"]}
{"filename": "src/simple_ai/api/grpc/completion/llm_pb2_grpc.py", "chunked_list": ["# Generated by the gRPC Python protocol compiler plugin. DO NOT EDIT!\n\t\"\"\"Client and server classes corresponding to protobuf-defined services.\"\"\"\n\timport grpc\n\tfrom . import llm_pb2 as llm__pb2\n\tclass LanguageModelStub(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    def __init__(self, channel):\n\t        \"\"\"Constructor.\n\t        Args:\n\t            channel: A grpc.Channel.\n", "        \"\"\"\n\t        self.Complete = channel.unary_unary(\n\t            \"/languagemodel.LanguageModel/Complete\",\n\t            request_serializer=llm__pb2.Message.SerializeToString,\n\t            response_deserializer=llm__pb2.Completions.FromString,\n\t        )\n\t        self.StreamComplete = channel.unary_stream(\n\t            \"/languagemodel.LanguageModel/StreamComplete\",\n\t            request_serializer=llm__pb2.Message.SerializeToString,\n\t            response_deserializer=llm__pb2.Completions.FromString,\n", "        )\n\tclass LanguageModelServicer(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    def Complete(self, request, context):\n\t        \"\"\"Simple RPC.\"\"\"\n\t        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\t    def StreamComplete(self, request, context):\n\t        \"\"\"Server-to-client streaming RPC.\"\"\"\n", "        context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n\t        context.set_details(\"Method not implemented!\")\n\t        raise NotImplementedError(\"Method not implemented!\")\n\tdef add_LanguageModelServicer_to_server(servicer, server):\n\t    rpc_method_handlers = {\n\t        \"Complete\": grpc.unary_unary_rpc_method_handler(\n\t            servicer.Complete,\n\t            request_deserializer=llm__pb2.Message.FromString,\n\t            response_serializer=llm__pb2.Completions.SerializeToString,\n\t        ),\n", "        \"StreamComplete\": grpc.unary_stream_rpc_method_handler(\n\t            servicer.StreamComplete,\n\t            request_deserializer=llm__pb2.Message.FromString,\n\t            response_serializer=llm__pb2.Completions.SerializeToString,\n\t        ),\n\t    }\n\t    generic_handler = grpc.method_handlers_generic_handler(\n\t        \"languagemodel.LanguageModel\", rpc_method_handlers\n\t    )\n\t    server.add_generic_rpc_handlers((generic_handler,))\n", "# This class is part of an EXPERIMENTAL API.\n\tclass LanguageModel(object):\n\t    \"\"\"Interface exported by the server.\"\"\"\n\t    @staticmethod\n\t    def Complete(\n\t        request,\n\t        target,\n\t        options=(),\n\t        channel_credentials=None,\n\t        call_credentials=None,\n", "        insecure=False,\n\t        compression=None,\n\t        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n\t    ):\n\t        return grpc.experimental.unary_unary(\n\t            request,\n\t            target,\n\t            \"/languagemodel.LanguageModel/Complete\",\n", "            llm__pb2.Message.SerializeToString,\n\t            llm__pb2.Completions.FromString,\n\t            options,\n\t            channel_credentials,\n\t            insecure,\n\t            call_credentials,\n\t            compression,\n\t            wait_for_ready,\n\t            timeout,\n\t            metadata,\n", "        )\n\t    @staticmethod\n\t    def StreamComplete(\n\t        request,\n\t        target,\n\t        options=(),\n\t        channel_credentials=None,\n\t        call_credentials=None,\n\t        insecure=False,\n\t        compression=None,\n", "        wait_for_ready=None,\n\t        timeout=None,\n\t        metadata=None,\n\t    ):\n\t        return grpc.experimental.unary_stream(\n\t            request,\n\t            target,\n\t            \"/languagemodel.LanguageModel/StreamComplete\",\n\t            llm__pb2.Message.SerializeToString,\n\t            llm__pb2.Completions.FromString,\n", "            options,\n\t            channel_credentials,\n\t            insecure,\n\t            call_credentials,\n\t            compression,\n\t            wait_for_ready,\n\t            timeout,\n\t            metadata,\n\t        )\n"]}
{"filename": "src/simple_ai/api/grpc/completion/server.py", "chunked_list": ["\"\"\"The Python implementation of the gRPC server.\"\"\"\n\tfrom concurrent import futures\n\timport logging\n\timport grpc\n\tfrom . import llm_pb2\n\tfrom . import llm_pb2_grpc\n\tfrom .model import LanguageModel\n\tclass LanguageModelServicer(llm_pb2_grpc.LanguageModelServicer):\n\t    \"\"\"Provides methods that implement functionality of route guide server.\"\"\"\n\t    def __init__(self, model=LanguageModel()) -> None:\n", "        super().__init__()\n\t        self.model = model\n\t    def Complete(self, request, context):\n\t        predicted = self.model.complete(\n\t            prompt=request.prompt,\n\t            suffix=request.suffix,\n\t            max_tokens=request.max_tokens,\n\t            temperature=request.temperature,\n\t            top_p=request.top_p,\n\t            n=request.n,\n", "            stream=request.stream,\n\t            logprobs=request.logprobs,\n\t            echo=request.echo,\n\t            stop=request.stop,\n\t            presence_penalty=request.presence_penalty,\n\t            frequence_penalty=request.frequence_penalty,\n\t            best_of=request.best_of,\n\t            logit_bias=request.logit_bias,\n\t        )\n\t        return llm_pb2.Completions(reply=predicted)\n", "    def StreamComplete(self, request, context):\n\t        predicted_stream = self.model.stream_complete(\n\t            prompt=request.prompt,\n\t            suffix=request.suffix,\n\t            max_tokens=request.max_tokens,\n\t            temperature=request.temperature,\n\t            top_p=request.top_p,\n\t            n=request.n,\n\t            stream=request.stream,\n\t            logprobs=request.logprobs,\n", "            echo=request.echo,\n\t            stop=request.stop,\n\t            presence_penalty=request.presence_penalty,\n\t            frequence_penalty=request.frequence_penalty,\n\t            best_of=request.best_of,\n\t            logit_bias=request.logit_bias,\n\t        )\n\t        yield from map(lambda predicted: llm_pb2.Completions(reply=predicted), predicted_stream)\n\tdef serve(address=\"[::]:50051\", model_servicer=LanguageModelServicer(), max_workers=10):\n\t    server = grpc.server(futures.ThreadPoolExecutor(max_workers=max_workers))\n", "    llm_pb2_grpc.add_LanguageModelServicer_to_server(model_servicer, server)\n\t    server.add_insecure_port(address=address)\n\t    server.start()\n\t    server.wait_for_termination()\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    logging.basicConfig()\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"address\", type=str, default=\"[::]:50051\")\n\t    args = parser.parse_args()\n", "    serve(address=args.address)\n"]}
{"filename": "src/simple_ai/api/grpc/completion/llm_pb2.py", "chunked_list": ["# -*- coding: utf-8 -*-\n\t# Generated by the protocol buffer compiler.  DO NOT EDIT!\n\t# source: llm.proto\n\t\"\"\"Generated protocol buffer code.\"\"\"\n\tfrom google.protobuf.internal import builder as _builder\n\tfrom google.protobuf import descriptor as _descriptor\n\tfrom google.protobuf import descriptor_pool as _descriptor_pool\n\tfrom google.protobuf import symbol_database as _symbol_database\n\t# @@protoc_insertion_point(imports)\n\t_sym_db = _symbol_database.Default()\n", "DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(\n\t    b'\\n\\tllm.proto\\x12\\rlanguagemodel\"\\x84\\x02\\n\\x07Message\\x12\\x0e\\n\\x06prompt\\x18\\x01'\n\t    b\" \\x01(\\t\\x12\\x0e\\n\\x06suffix\\x18\\x02 \\x01(\\t\\x12\\x12\\n\\nmax_tokens\\x18\\x03\"\n\t    b\" \\x01(\\x05\\x12\\x13\\n\\x0btemperature\\x18\\x04 \\x01(\\x02\\x12\\r\\n\\x05top_p\\x18\\x05\"\n\t    b\" \\x01(\\x02\\x12\\t\\n\\x01n\\x18\\x06 \\x01(\\x05\\x12\\x0e\\n\\x06stream\\x18\\x07\"\n\t    b\" \\x01(\\x08\\x12\\x10\\n\\x08logprobs\\x18\\x08 \\x01(\\x05\\x12\\x0c\\n\\x04\\x65\\x63ho\\x18\\t\"\n\t    b\" \\x01(\\x08\\x12\\x0c\\n\\x04stop\\x18\\n \\x01(\\t\\x12\\x18\\n\\x10presence_penalty\\x18\\x0b\"\n\t    b\" \\x01(\\x02\\x12\\x19\\n\\x11\\x66requence_penalty\\x18\\x0c\"\n\t    b\" \\x01(\\x02\\x12\\x0f\\n\\x07\\x62\\x65st_of\\x18\\r \\x01(\\x05\\x12\\x12\\n\\nlogit_bias\\x18\\x0e\"\n\t    b' \\x01(\\t\"\\x1c\\n\\x0b\\x43ompletions\\x12\\r\\n\\x05reply\\x18\\x01'\n", "    b' \\x01(\\t2\\x9b\\x01\\n\\rLanguageModel\\x12@\\n\\x08\\x43omplete\\x12\\x16.languagemodel.Message\\x1a\\x1a.languagemodel.Completions\"\\x00\\x12H\\n\\x0eStreamComplete\\x12\\x16.languagemodel.Message\\x1a\\x1a.languagemodel.Completions\"\\x00\\x30\\x01\\x42+\\n\\x13io.grpc.examples.lmB\\rLanguageModelP\\x01\\xa2\\x02\\x02LMb\\x06proto3'\n\t)\n\t_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, globals())\n\t_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, \"llm_pb2\", globals())\n\tif _descriptor._USE_C_DESCRIPTORS == False:\n\t    DESCRIPTOR._options = None\n\t    DESCRIPTOR._serialized_options = b\"\\n\\023io.grpc.examples.lmB\\rLanguageModelP\\001\\242\\002\\002LM\"\n\t    _MESSAGE._serialized_start = 29\n\t    _MESSAGE._serialized_end = 289\n\t    _COMPLETIONS._serialized_start = 291\n", "    _COMPLETIONS._serialized_end = 319\n\t    _LANGUAGEMODEL._serialized_start = 322\n\t    _LANGUAGEMODEL._serialized_end = 477\n\t# @@protoc_insertion_point(module_scope)\n"]}
