{"filename": "runner.py", "chunked_list": ["import os\n\timport time\n\timport datetime\n\timport yaml\n\timport git\n\timport torch\n\timport torch.backends.cudnn as cudnn\n\tfrom timm.utils import AverageMeter\n\tfrom utils import load_checkpoint, load_pretrained, save_checkpoint, save_image_torch, get_grad_norm\n\tfrom utils.config import parse_options, copy_cfg, ordered_dict_to_dict\n", "from utils.scheduler import build_scheduler\n\tfrom utils.optimizer import build_optimizer\n\tfrom utils.metrics import get_psnr_torch, get_ssim_torch\n\tfrom utils.loss import build_loss\n\tfrom utils.logger import create_logger\n\tfrom models import build_model\n\tfrom datasets import build_train_loader, build_valid_loader, build_test_loader\n\tfrom forwards import build_forwards, build_profile\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tdef main(config):\n", "    writer = SummaryWriter(os.path.join(config['output'], 'tensorboard'))\n\t    train_dataloader = build_train_loader(config['data'])\n\t    if not config['testset_as_validset']:\n\t        valid_dataloader =  build_valid_loader(config['data'], 1)\n\t    else:\n\t        valid_dataloader = build_test_loader(config['data'], 2)\n\t    logger.info(f\"Creating model:{config['name']}/{config['model']['type']}\")\n\t    model = build_model(config['model'])\n\t    model.cuda()\n\t    logger.info(str(model))\n", "    profile_forward = build_profile(config)\n\t    profile_model(config, profile_forward, model, train_dataloader, logger)\n\t    optimizer = build_optimizer(config['train'], model)\n\t    lr_scheduler = build_scheduler(config['train'], optimizer, len(train_dataloader))\n\t    loss_list = build_loss(config['loss'])\n\t    logger.info(str(loss_list))\n\t    logger.info('Building forwards:')\n\t    logger.info(f'Train forward: {config[\"train\"][\"forward_type\"]}')\n\t    logger.info(f'Test forward: {config[\"test\"][\"forward_type\"]}')\n\t    train_forward, test_forward = build_forwards(config)\n", "    max_psnr = 0.0\n\t    max_ssim = 0.0\n\t    total_epochs = config['train']['early_stop'] if config['train']['early_stop'] is not None else config['train']['epochs']\n\t    if config.get('throughput_mode', False):\n\t        throughput(config, train_forward, model, valid_dataloader, logger)\n\t        return\n\t    # set auto resume\n\t    if config['train']['auto_resume']:\n\t        auto_resume_path = os.path.join(config['output'], 'checkpoints', 'checkpoint.pth')\n\t        if os.path.exists(auto_resume_path):\n", "            config['train']['resume'] = auto_resume_path\n\t            logger.info(f'Auto resume: setting resume path to {auto_resume_path}')\n\t    if config['train'].get('resume'):\n\t        max_psnr = load_checkpoint(config, model, optimizer, lr_scheduler, logger)\n\t        validate(config, test_forward, model, loss_list, valid_dataloader, config['train'].get('start_epoch', 0), writer)\n\t        if config.get('eval_mode', False):\n\t            return\n\t    if config['train'].get('pretrained') and (not config['train'].get('resume')):\n\t        load_pretrained(config, model, logger)\n\t        validate(config, test_forward, model, loss_list, valid_dataloader, config['train'].get('start_epoch', 0), writer)\n", "        if config.get('eval_mode', False):\n\t            return\n\t    logger.info(\"Start training\")\n\t    start_time = time.time()\n\t    start = time.time()\n\t    lr_scheduler.step(config['train'].get('start_epoch', 0))\n\t    for epoch in range(config['train'].get('start_epoch', 0)+1, total_epochs+1):\n\t        train_one_epoch(config, train_forward, model, loss_list, train_dataloader, optimizer, None, epoch, lr_scheduler, writer)\n\t        if epoch % config['valid_per_epoch'] == 0 or (total_epochs - epoch) < 50:\n\t            psnr, ssim, loss = validate(config, test_forward, model, loss_list, valid_dataloader, epoch, writer)\n", "            max_psnr = max(max_psnr, psnr)\n\t            max_ssim = max(max_ssim, ssim)\n\t            writer.add_scalar('eval/max_psnr', max_psnr, epoch)\n\t            writer.add_scalar('eval/max_ssim', max_ssim, epoch)\n\t        else:\n\t            psnr = 0\n\t        save_checkpoint(config, epoch, model, max_psnr, optimizer, lr_scheduler, logger, is_best=(max_psnr==psnr))\n\t        logger.info(f'Train: [{epoch}/{config[\"train\"][\"epochs\"]}] Max Valid PSNR: {max_psnr:.4f}, Max Valid SSIM: {max_ssim:.4f}')\n\t        logger.info(f\"Train: [{epoch}/{config['train']['epochs']}] Total Time {datetime.timedelta(seconds=int(time.time()-start))}\")\n\t        start = time.time()\n", "    total_time = time.time() - start_time\n\t    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t    logger.info('Training time {}'.format(total_time_str))\n\t@torch.no_grad()\n\tdef profile_model(config, profile_forward, model, data_loader, logger):\n\t    if profile_forward is not None:\n\t        data_iter = iter(data_loader)\n\t        data = next(data_iter)\n\t        del data_iter\n\t        profile_forward(config, model, data, logger)\n", "    n_parameters = sum(p.numel() for p in model.parameters())\n\t    logger.info(f\"Total Params: {n_parameters:,}\")\n\tdef train_one_epoch(config, train_forward, model, loss_list, data_loader, optimizer, scaler, epoch, lr_scheduler, writer):\n\t    torch.cuda.reset_peak_memory_stats()\n\t    model.train()\n\t    optimizer.zero_grad()\n\t    num_steps = len(data_loader)\n\t    batch_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    loss_meter = AverageMeter()\n", "    norm_meter = AverageMeter()\n\t    losses_count = len(loss_list)\n\t    losses_meter = [AverageMeter() for _ in range(losses_count)]\n\t    start = time.time()\n\t    end = time.time()\n\t    for idx, data in enumerate(data_loader):\n\t        data_time.update(time.time() - end)\n\t        outputs, targets = train_forward(config, model, data)\n\t        losses = loss_list(outputs, targets)\n\t        loss = sum(losses)\n", "        optimizer.zero_grad()\n\t        loss.backward()\n\t        if config['train'].get('clip_grad'):\n\t            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config['train']['clip_grad'])\n\t        else:\n\t            grad_norm = get_grad_norm(model.parameters())\n\t        optimizer.step()\n\t        if not config['train']['lr_scheduler']['t_in_epochs']:\n\t            lr_scheduler.step_update((epoch-1)*num_steps+idx)\n\t        batch_size = list(targets.values())[0].size(0)\n", "        for _loss_meter, _loss in zip(losses_meter, losses):\n\t            _loss_meter.update(_loss.item(), batch_size)   \n\t        loss_meter.update(loss.item(), batch_size)\n\t        norm_meter.update(grad_norm)\n\t        batch_time.update(time.time() - end)\n\t        end = time.time()\n\t        if idx % config['print_per_iter'] == 0 or idx == num_steps:\n\t            lr = optimizer.param_groups[0]['lr']\n\t            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n\t            etas = batch_time.avg * (num_steps - idx)\n", "            logger.info(\n\t                f'Train: [{epoch}/{config[\"train\"][\"epochs\"]}][{idx}/{num_steps}]\\t'\n\t                f'ETA {datetime.timedelta(seconds=int(etas))} LR {lr:.6f}\\t'\n\t                f'Time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n\t                f'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n\t                f'Loss {loss_meter.val:.8f} ({loss_meter.avg:.8f})\\t'\n\t                f'GradNorm {norm_meter.val:.4f} ({norm_meter.avg:.4f})\\t'\n\t                f'Mem {memory_used:.0f}MB')\n\t    if config['train']['lr_scheduler']['t_in_epochs']:\n\t        lr_scheduler.step(epoch)\n", "    logger.info(f\"Train: [{epoch}/{config['train']['epochs']}] Time {datetime.timedelta(seconds=int(time.time()-start))}\")\n\t    tensor_board_dict = {'train/loss_total':loss_meter.avg}\n\t    for index, (_loss, _loss_meter) in enumerate(zip(losses, losses_meter)):\n\t        tensor_board_dict[f'train/loss_{index}'] = _loss_meter.avg\n\t    for log_key, log_value in tensor_board_dict.items():\n\t        writer.add_scalar(log_key, log_value, epoch)\n\t@torch.no_grad()\n\tdef validate(config, test_forward, model, loss_list, data_loader, epoch, writer):\n\t    torch.cuda.reset_max_memory_allocated()\n\t    model.eval()\n", "    logger.info(f\"Valid: [{epoch}/{config['train']['epochs']}]\\t\")\n\t    batch_time = AverageMeter()\n\t    data_time = AverageMeter()\n\t    loss_meter = AverageMeter()\n\t    psnr_meter = AverageMeter()\n\t    ssim_meter = AverageMeter()\n\t    losses_count = len(loss_list)\n\t    losses_meter = [AverageMeter() for _ in range(losses_count)]\n\t    start = time.time()\n\t    end = time.time()\n", "    for idx, data in enumerate(data_loader):\n\t        data_time.update(time.time() - end)\n\t        outputs, targets, img_files, lbl_files = test_forward(config, model, data)\n\t        if config['testset_as_validset']:\n\t            psnr, ssim = test_metric_cuda(config, epoch, outputs[config['test']['which_stage']], targets[config['test']['which_gt']], img_files, lbl_files)\n\t        else:\n\t            psnr, ssim = validate_metric(config, epoch, outputs[config['test']['which_stage']], targets[config['test']['which_gt']], img_files, lbl_files)\n\t        losses = loss_list(outputs, targets)\n\t        loss = sum(losses)\n\t        batch_size = targets[config['test']['which_gt']].size(0)\n", "        for _loss_meter, _loss in zip(losses_meter, losses):\n\t            _loss_meter.update(_loss.item(), batch_size)   \n\t        loss_meter.update(loss.item(), batch_size)\n\t        psnr_meter.update(psnr.item(), batch_size)\n\t        ssim_meter.update(ssim.item(), batch_size)\n\t        # measure elapsed time\n\t        batch_time.update(time.time() - end)\n\t        end = time.time()\n\t        if config['testset_as_validset'] or idx % config['print_per_iter'] == 0 or idx == len(data_loader):\n\t            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n", "            logger.info(\n\t                f'Valid: [{epoch}/{config[\"train\"][\"epochs\"]}][{idx}/{len(data_loader)}]\\t'\n\t                f'Time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n\t                f'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n\t                f'Loss {loss_meter.val:.8f} ({loss_meter.avg:.8f})\\t'\n\t                f'PSNR {psnr_meter.val:.4f} ({psnr_meter.avg:.4f})\\t'\n\t                f'SSIM {ssim_meter.val:.4f} ({ssim_meter.avg:.4f})\\t'\n\t                f'Mem {memory_used:.0f}MB\\t{os.path.basename(img_files[0])}')\n\t    logger.info(f'Valid: [{epoch}/{config[\"train\"][\"epochs\"]}] PSNR {psnr_meter.avg:.4f}\\tSSIM {ssim_meter.avg:.4f}')\n\t    logger.info(f'Valid: [{epoch}/{config[\"train\"][\"epochs\"]}] Time {datetime.timedelta(seconds=int(time.time()-start))}')\n", "    tensor_board_dict = {'eval/loss_total':loss_meter.avg}\n\t    for index, (loss, loss_meter) in enumerate(zip(losses, losses_meter)):\n\t        tensor_board_dict[f'eval/loss_{index}'] = loss_meter.avg\n\t    tensor_board_dict['eval/psnr'] = psnr_meter.avg\n\t    tensor_board_dict['eval/ssim'] = ssim_meter.avg\n\t    for log_key, log_value in tensor_board_dict.items():\n\t        writer.add_scalar(log_key, log_value, epoch)\n\t    return psnr_meter.avg, ssim_meter.avg, loss_meter.avg\n\t@torch.no_grad()\n\tdef validate_metric(config, epoch, outputs, targets, image_paths, target_params=None):\n", "    outputs = torch.clamp(outputs, 0, 1) * 255\n\t    targets = targets * 255\n\t    if config['test']['round']:\n\t        outputs = outputs.round()\n\t        targets = targets.round()\n\t    psnrs = get_psnr_torch(outputs, targets)\n\t    ssims = get_ssim_torch(outputs, targets)\n\t    if config['test']['save_image'] and epoch % config['save_per_epoch'] == 0:\n\t        images = torch.cat((outputs, targets), dim=3)\n\t        result_path = os.path.join(config['output'], 'results', f'valid_{epoch:04d}')\n", "        os.makedirs(result_path, exist_ok=True)\n\t        for image, image_path, psnr in zip(images, image_paths, psnrs):\n\t            save_path = os.path.join(result_path, f'{os.path.basename(image_path)[:-4]}_{psnr:.2f}.jpg')\n\t            save_image_torch(image, save_path)\n\t    return psnrs.mean(), ssims.mean()\n\t@torch.no_grad()\n\tdef test_metric_cuda(config, epoch, outputs, targets, image_paths, target_params=None):\n\t    outputs = torch.clamp(outputs, 0, 1) * 255\n\t    targets = torch.clamp(targets, 0, 1) * 255\n\t    if config['test']['round']:\n", "        outputs = outputs.round()\n\t        targets = targets.round()\n\t    psnr = get_psnr_torch(outputs, targets)\n\t    ssim = get_ssim_torch(outputs, targets)\n\t    if config['test']['save_image']:\n\t        result_path = os.path.join(config['output'], 'results', f'test_{epoch:04d}')\n\t        os.makedirs(result_path, exist_ok=True)\n\t        save_path = os.path.join(result_path, f'{os.path.basename(image_paths[0])[:-4]}_{psnr.item():.2f}.png')\n\t        save_image_torch(outputs[0], save_path)\n\t    return psnr, ssim\n", "@torch.no_grad()\n\tdef throughput(config, forward, model, data_loader, logger):\n\t    model.eval()\n\t    for idx, data in enumerate(data_loader):\n\t        for i in range(30):\n\t            forward(config, model, data)\n\t        logger.info(f\"throughput averaged with 100 times\")\n\t        torch.cuda.synchronize()\n\t        tic = time.time()\n\t        for i in range(100):\n", "            pred, label = forward(config, model, data)\n\t        batch_size = list(pred.values())[0].size(0)\n\t        torch.cuda.synchronize()\n\t        toc = time.time()\n\t        logger.info(f\"batch_size {batch_size} throughput {(toc - tic) * 1000 / (100 * batch_size)}ms\")\n\t        return\n\tif __name__ == '__main__':\n\t    args, config = parse_options()\n\t    phase = 'train' if not args.test else 'test'\n\t    cudnn.benchmark = True\n", "    os.makedirs(config['output'], exist_ok=True)\n\t    start_time = time.strftime(\"%y%m%d-%H%M\", time.localtime())\n\t    logger = create_logger(output_dir=config['output'], name=f\"{config['tag']}\", action=f\"{phase}-{start_time}\")\n\t    path = os.path.join(config['output'], f\"{phase}-{start_time}.yaml\")\n\t    try:\n\t        repo = git.Repo(search_parent_directories=True)\n\t        sha = repo.head.object.hexsha\n\t        if repo.is_dirty():\n\t            logger.warning(f'Current work on commit: {sha}, however the repo is dirty (not committed)!')\n\t        else:\n", "            logger.info(f'Current work on commit: {sha}.')\n\t    except git.exc.InvalidGitRepositoryError as e:\n\t        logger.warn(f'No git repo base.')\n\t    copy_cfg(config, path)\n\t    logger.info(f\"Full config saved to {path}\")\n\t    # print config\n\t    logger.info(\"Config:\\n\" + yaml.dump(ordered_dict_to_dict(config), default_flow_style=False, sort_keys=False))\n\t    current_cuda_device = torch.cuda.get_device_properties(torch.cuda.current_device())\n\t    logger.info(f\"Current CUDA Device: {current_cuda_device.name}, Total Mem: {int(current_cuda_device.total_memory / 1024 / 1024)}MB\")\n\t    main(config)\n"]}
{"filename": "utils/registry.py", "chunked_list": ["# Modified from: https://github.com/facebookresearch/fvcore/blob/master/fvcore/common/registry.py  # noqa: E501\n\tclass Registry():\n\t    \"\"\"\n\t    The registry that provides name -> object mapping, to support third-party\n\t    users' custom modules.\n\t    To create a registry (e.g. a backbone registry):\n\t    .. code-block:: python\n\t        BACKBONE_REGISTRY = Registry('BACKBONE')\n\t    To register an object:\n\t    .. code-block:: python\n", "        @BACKBONE_REGISTRY.register()\n\t        class MyBackbone():\n\t            ...\n\t    Or:\n\t    .. code-block:: python\n\t        BACKBONE_REGISTRY.register(MyBackbone)\n\t    \"\"\"\n\t    def __init__(self, name):\n\t        \"\"\"\n\t        Args:\n", "            name (str): the name of this registry\n\t        \"\"\"\n\t        self._name = name\n\t        self._obj_map = {}\n\t    def _do_register(self, name, obj, suffix=None):\n\t        if isinstance(suffix, str):\n\t            name = name + '_' + suffix\n\t        assert (name not in self._obj_map), (f\"An object named '{name}' was already registered \"\n\t                                             f\"in '{self._name}' registry!\")\n\t        self._obj_map[name] = obj\n", "    def register(self, obj=None, suffix=None):\n\t        \"\"\"\n\t        Register the given object under the the name `obj.__name__`.\n\t        Can be used as either a decorator or not.\n\t        See docstring of this class for usage.\n\t        \"\"\"\n\t        if obj is None:\n\t            # used as a decorator\n\t            def deco(func_or_class):\n\t                name = func_or_class.__name__\n", "                self._do_register(name, func_or_class, suffix)\n\t                return func_or_class\n\t            return deco\n\t        # used as a function call\n\t        name = obj.__name__\n\t        self._do_register(name, obj, suffix)\n\t    def get(self, name, suffix='basicsr'):\n\t        ret = self._obj_map.get(name)\n\t        if ret is None:\n\t            ret = self._obj_map.get(name + '_' + suffix)\n", "            print(f'Name {name} is not found, use name: {name}_{suffix}!')\n\t        if ret is None:\n\t            raise KeyError(f\"No object named '{name}' found in '{self._name}' registry!\")\n\t        return ret\n\t    def __contains__(self, name):\n\t        return name in self._obj_map\n\t    def __iter__(self):\n\t        return iter(self._obj_map.items())\n\t    def keys(self):\n\t        return self._obj_map.keys()\n", "DATASET_REGISTRY = Registry('dataset')\n\tMODEL_REGISTRY = Registry('model')\n\tFORWARD_REGISTRY = Registry('forward')\n"]}
{"filename": "utils/loss.py", "chunked_list": ["import torch.nn as nn\n\tfrom torch.nn import MSELoss, L1Loss\n\tclass Losses(nn.Module):\n\t    def __init__(self, classes, names, weights, positions, gt_positions):\n\t        super().__init__()\n\t        self.module_list = nn.ModuleList()\n\t        self.names = names\n\t        self.weights = weights\n\t        self.positions = positions\n\t        self.gt_positions = gt_positions\n", "        for class_name in classes:\n\t            module_class = eval(class_name)\n\t            self.module_list.append(module_class())\n\t    def __len__(self):\n\t        return len(self.names)\n\t    def forward(self, outputs, targets):\n\t        losses = []\n\t        for i in range(len(self.names)):\n\t            loss = self.module_list[i](outputs[self.positions[i]], targets[self.gt_positions[i]]) * self.weights[i]\n\t            losses.append(loss)\n", "        return losses\n\tdef build_loss(config):\n\t    loss_names = config['types']\n\t    loss_classes = config['classes']\n\t    loss_weights = config['weights']\n\t    loss_positions = config['which_stage']\n\t    loss_gt_positions = config['which_gt']\n\t    assert len(loss_names) == len(loss_weights) == \\\n\t           len(loss_classes) == len(loss_positions) == \\\n\t           len(loss_gt_positions)\n", "    criterion = Losses(classes=loss_classes, names=loss_names,\n\t                          weights=loss_weights, positions=loss_positions,\n\t                          gt_positions=loss_gt_positions)\n\t    return criterion\n"]}
{"filename": "utils/config.py", "chunked_list": ["import argparse\n\timport os\n\timport random\n\timport torch\n\timport yaml\n\tfrom collections import OrderedDict\n\tfrom os import path as osp\n\timport numpy as np\n\tfrom copy import deepcopy\n\tdef set_random_seed(seed):\n", "    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\tdef ordered_yaml():\n\t    \"\"\"Support OrderedDict for yaml.\n\t    Returns:\n\t        tuple: yaml Loader and Dumper.\n\t    \"\"\"\n\t    try:\n", "        from yaml import CDumper as Dumper\n\t        from yaml import CLoader as Loader\n\t    except ImportError:\n\t        from yaml import Dumper, Loader\n\t    _mapping_tag = yaml.resolver.BaseResolver.DEFAULT_MAPPING_TAG\n\t    def dict_representer(dumper, data):\n\t        return dumper.represent_dict(data.items())\n\t    def dict_constructor(loader, node):\n\t        return OrderedDict(loader.construct_pairs(node))\n\t    Dumper.add_representer(OrderedDict, dict_representer)\n", "    Loader.add_constructor(_mapping_tag, dict_constructor)\n\t    return Loader, Dumper\n\tdef yaml_load(f):\n\t    \"\"\"Load yaml file or string.\n\t    Args:\n\t        f (str): File path or a python string.\n\t    Returns:\n\t        dict: Loaded dict.\n\t    \"\"\"\n\t    if os.path.isfile(f):\n", "        with open(f, 'r') as f:\n\t            return yaml.load(f, Loader=ordered_yaml()[0])\n\t    else:\n\t        return yaml.load(f, Loader=ordered_yaml()[0])\n\tdef dict2str(opt, indent_level=1):\n\t    \"\"\"dict to string for printing options.\n\t    Args:\n\t        opt (dict): Option dict.\n\t        indent_level (int): Indent level. Default: 1.\n\t    Return:\n", "        (str): Option string for printing.\n\t    \"\"\"\n\t    msg = '\\n'\n\t    for k, v in opt.items():\n\t        if isinstance(v, dict):\n\t            msg += ' ' * (indent_level * 2) + k + ':['\n\t            msg += dict2str(v, indent_level + 1)\n\t            msg += ' ' * (indent_level * 2) + ']\\n'\n\t        else:\n\t            msg += ' ' * (indent_level * 2) + k + ': ' + str(v) + '\\n'\n", "    return msg\n\tdef _postprocess_yml_value(value):\n\t    # None\n\t    if value == '~' or value.lower() == 'none':\n\t        return None\n\t    # bool\n\t    if value.lower() == 'true':\n\t        return True\n\t    elif value.lower() == 'false':\n\t        return False\n", "    # !!float number\n\t    if value.startswith('!!float'):\n\t        return float(value.replace('!!float', ''))\n\t    # number\n\t    if value.isdigit():\n\t        return int(value)\n\t    elif value.replace('.', '', 1).isdigit() and value.count('.') < 2:\n\t        return float(value)\n\t    # list\n\t    if value.startswith('['):\n", "        return eval(value)\n\t    # str\n\t    return value\n\tdef merge_from_base(cfg, cfg_path):\n\t    def _merge_a_into_b(cfg_a, cfg_b):\n\t        for k, v_ in cfg_a.items():\n\t            v = deepcopy(v_)\n\t            if isinstance(v, dict) and k in cfg_b:\n\t                _merge_a_into_b(v, cfg_b[k])\n\t            else:\n", "                cfg_b[k] = v\n\t    if 'base' in cfg:\n\t        if isinstance(cfg['base'], str):\n\t            cfg['base'] = [ cfg['base'] ]\n\t        for base_cfg_path in cfg['base']:\n\t            full_base_cfg_path = os.path.join(os.path.dirname(cfg_path), base_cfg_path)\n\t            base_cfg = yaml_load(full_base_cfg_path)\n\t            base_cfg = merge_from_base(base_cfg, full_base_cfg_path)\n\t            _merge_a_into_b(cfg, base_cfg)\n\t        return base_cfg\n", "    return cfg\n\tdef set_default_config(cfg):\n\t    if 'train' not in cfg:\n\t        cfg['train'] = {}\n\t    ##### default #####\n\t    if 'output' not in cfg:\n\t        cfg['output'] = 'runs'\n\t    if 'tag' not in cfg:\n\t        cfg['tag'] = 'debug'\n\t    ##### data #####\n", "    if 'persistent_workers' not in cfg['data']:\n\t        cfg['data']['persistent_workers'] = False\n\t    if 'train' in cfg['data'] and 'repeat' not in cfg['data']['train']:\n\t        cfg['data']['train']['repeat'] = 1\n\t    # augmentation \n\t    if 'transpose' not in cfg['data']['process']:\n\t        cfg['data']['process']['transpose'] = False\n\t    if 'h_flip' not in cfg['data']['process']:\n\t        cfg['data']['process']['h_flip'] = True\n\t    if 'v_flip' not in cfg['data']['process']:\n", "        cfg['data']['process']['v_flip'] = True\n\t    if 'rotation' not in cfg['data']['process']:\n\t        cfg['data']['process']['rotation'] = False\n\t    ##### train #####\n\t    if 'auto_resume' not in cfg['train']:\n\t        cfg['train']['auto_resume'] = False\n\t    ##### test #####\n\t    if 'test' in cfg:\n\t        if 'round' not in cfg['test']:\n\t            cfg['test']['round'] = False\n", "        if 'save_image' not in cfg['test']:\n\t            cfg['test']['save_image'] = False\n\t    cfg['output'] = os.path.join(cfg.get('output', 'runs'), cfg['name'], cfg.get('tag', ''))\n\tdef parse_options():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('-cfg', type=str, required=True, help='Path to option YAML file.')\n\t    parser.add_argument('--auto-resume', action='store_true', default=False, help='Auto resume from latest checkpoint')\n\t    parser.add_argument('--resume', type=str, default=None, help='Path to resume.')\n\t    parser.add_argument('--pretrain', type=str, default=None, help='Path to the pretrained checkpoint path.')\n\t    parser.add_argument('--test', action='store_true', default=False, help='Test mode')\n", "    parser.add_argument('--save-image', action='store_true', default=False, help='Save image during test or validation')\n\t    parser.add_argument(\n\t        '--force-yml', nargs='+', default=None, help='Force to update yml files. Examples: train:ema_decay=0.999')\n\t    args = parser.parse_args()\n\t    # parse yml to dict\n\t    cfg = yaml_load(args.cfg)\n\t    cfg = merge_from_base(cfg, args.cfg)\n\t    set_default_config(cfg)\n\t    # random seed\n\t    seed = cfg.get('manual_seed')\n", "    if seed is None:\n\t        seed = random.randint(1, 10000)\n\t        cfg['manual_seed'] = seed\n\t    set_random_seed(seed)\n\t    if args.test:\n\t        assert not args.auto_resume and \\\n\t            (args.resume is not None or cfg['train'].get('resume') is not None) or \\\n\t            (args.pretrain is not None or cfg['train'].get('pretrained') is not None)\n\t        cfg['testset_as_validset'] = True\n\t        cfg['eval_mode'] = True\n", "    if args.auto_resume:\n\t        assert args.resume is None\n\t        cfg['train']['auto_resume'] = True\n\t    if args.resume:\n\t        assert args.pretrain is None\n\t        cfg['train']['resume'] = args.resume\n\t    if args.pretrain:\n\t        cfg['train']['pretrained'] = args.pretrain\n\t    if args.save_image:\n\t        cfg['test']['save_image'] = True\n", "    # force to update yml options\n\t    if args.force_yml is not None:\n\t        for entry in args.force_yml:\n\t            # now do not support creating new keys\n\t            keys, value = entry.split('=')\n\t            keys, value = keys.strip(), value.strip()\n\t            value = _postprocess_yml_value(value)\n\t            eval_str = 'cfg'\n\t            for key in keys.split(':'):\n\t                eval_str += f'[\"{key}\"]'\n", "            eval_str += '=value'\n\t            # using exec function\n\t            exec(eval_str)\n\t    return args, cfg\n\tdef copy_cfg(cfg, filename):\n\t    # copy the yml file to the experiment root\n\t    import sys\n\t    import time\n\t    from shutil import copyfile\n\t    cmd = ' '.join(sys.argv)\n", "    with open(filename, 'w') as f:\n\t        lines = [f'# GENERATE TIME: {time.asctime()}\\n# CMD:\\n# {cmd}\\n\\n']\n\t        lines.append(yaml.dump(ordered_dict_to_dict(cfg), default_flow_style=False, sort_keys=False))\n\t        f.writelines(lines)\n\tdef ordered_dict_to_dict(cfg):\n\t    cfg_dict = {}\n\t    for k, v in cfg.items():\n\t        if isinstance(v, OrderedDict):\n\t            cfg_dict[k] = ordered_dict_to_dict(deepcopy(v))\n\t        else:\n", "            cfg_dict[k] = v\n\t    return cfg_dict\n"]}
{"filename": "utils/scheduler.py", "chunked_list": ["from timm.scheduler.cosine_lr import CosineLRScheduler\n\tfrom timm.scheduler.step_lr import StepLRScheduler\n\tdef build_scheduler(config, optimizer, n_iter_per_epoch=1):\n\t    if config['lr_scheduler']['t_in_epochs']:\n\t        n_iter_per_epoch = 1\n\t    num_steps = int(config['epochs'] * n_iter_per_epoch)\n\t    warmup_steps = int(config['warmup_epochs'] * n_iter_per_epoch)\n\t    lr_scheduler = None\n\t    if config['lr_scheduler']['type'] == 'cosine':\n\t        lr_scheduler = CosineLRScheduler(\n", "            optimizer,\n\t            t_initial=num_steps,\n\t            cycle_mul=1.,\n\t            lr_min=config['min_lr'],\n\t            warmup_lr_init=config.get('warmup_lr', 0.0),\n\t            warmup_t=warmup_steps,\n\t            cycle_limit=1,\n\t            t_in_epochs=config['lr_scheduler']['t_in_epochs'],\n\t        )\n\t    elif config['lr_scheduler']['type'] == 'step':\n", "        decay_steps = int(config['lr_scheduler']['decay_epochs'] * n_iter_per_epoch)\n\t        lr_scheduler = StepLRScheduler(\n\t            optimizer,\n\t            decay_t=decay_steps,\n\t            decay_rate=config['lr_scheduler']['decay_rate'],\n\t            warmup_lr_init=config.get('warmup_lr', 0.0),\n\t            warmup_t=warmup_steps,\n\t            t_in_epochs=config['lr_scheduler']['t_in_epochs'],\n\t        )\n\t    else:\n", "        raise NotImplementedError()\n\t    return lr_scheduler\n"]}
{"filename": "utils/metrics.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tclass PSNR(nn.Module):\n\t    def __init__(self):\n\t        super().__init__()\n\t    def forward(self, x, gt):\n\t        mse = torch.mean((x - gt) ** 2, dim=[1, 2, 3])\n\t        return 20 * torch.log10(255.0 / torch.sqrt(mse))\n\tdef get_ssim_torch(x, gt):\n\t    return ssim(x, gt, size_average=False)\n", "def get_psnr_torch(x, gt, data_range=255.0):\n\t    mse = torch.mean((x - gt) ** 2, dim=[1, 2, 3])\n\t    return 20 * torch.log10(data_range / torch.sqrt(mse))\n\t# Copyright 2020 by Gongfan Fang, Zhejiang University.\n\t# All rights reserved.\n\timport warnings\n\timport torch\n\timport torch.nn.functional as F\n\tdef _fspecial_gauss_1d(size, sigma):\n\t    r\"\"\"Create 1-D gauss kernel\n", "    Args:\n\t        size (int): the size of gauss kernel\n\t        sigma (float): sigma of normal distribution\n\t    Returns:\n\t        torch.Tensor: 1D kernel (1 x 1 x size)\n\t    \"\"\"\n\t    coords = torch.arange(size, dtype=torch.float)\n\t    coords -= size // 2\n\t    g = torch.exp(-(coords ** 2) / (2 * sigma ** 2))\n\t    g /= g.sum()\n", "    return g.unsqueeze(0).unsqueeze(0)\n\tdef gaussian_filter(input, win):\n\t    r\"\"\" Blur input with 1-D kernel\n\t    Args:\n\t        input (torch.Tensor): a batch of tensors to be blurred\n\t        window (torch.Tensor): 1-D gauss kernel\n\t    Returns:\n\t        torch.Tensor: blurred tensors\n\t    \"\"\"\n\t    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n", "    if len(input.shape) == 4:\n\t        conv = F.conv2d\n\t    elif len(input.shape) == 5:\n\t        conv = F.conv3d\n\t    else:\n\t        raise NotImplementedError(input.shape)\n\t    C = input.shape[1]\n\t    out = input\n\t    for i, s in enumerate(input.shape[2:]):\n\t        if s >= win.shape[-1]:\n", "            out = conv(out, weight=win.transpose(2 + i, -1), stride=1, padding=0, groups=C)\n\t        else:\n\t            warnings.warn(\n\t                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n\t            )\n\t    return out\n\tdef _ssim(X, Y, data_range, win, size_average=True, K=(0.01, 0.03)):\n\t    r\"\"\" Calculate ssim index for X and Y\n\t    Args:\n\t        X (torch.Tensor): images\n", "        Y (torch.Tensor): images\n\t        win (torch.Tensor): 1-D gauss kernel\n\t        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n\t        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n\t    Returns:\n\t        torch.Tensor: ssim results.\n\t    \"\"\"\n\t    K1, K2 = K\n\t    # batch, channel, [depth,] height, width = X.shape\n\t    compensation = 1.0\n", "    C1 = (K1 * data_range) ** 2\n\t    C2 = (K2 * data_range) ** 2\n\t    win = win.to(X.device, dtype=X.dtype)\n\t    mu1 = gaussian_filter(X, win)\n\t    mu2 = gaussian_filter(Y, win)\n\t    mu1_sq = mu1.pow(2)\n\t    mu2_sq = mu2.pow(2)\n\t    mu1_mu2 = mu1 * mu2\n\t    sigma1_sq = compensation * (gaussian_filter(X * X, win) - mu1_sq)\n\t    sigma2_sq = compensation * (gaussian_filter(Y * Y, win) - mu2_sq)\n", "    sigma12 = compensation * (gaussian_filter(X * Y, win) - mu1_mu2)\n\t    cs_map = (2 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2)  # set alpha=beta=gamma=1\n\t    ssim_map = ((2 * mu1_mu2 + C1) / (mu1_sq + mu2_sq + C1)) * cs_map\n\t    ssim_per_channel = torch.flatten(ssim_map, 2).mean(-1)\n\t    cs = torch.flatten(cs_map, 2).mean(-1)\n\t    return ssim_per_channel, cs\n\tdef ssim(\n\t    X,\n\t    Y,\n\t    data_range=255,\n", "    size_average=True,\n\t    win_size=11,\n\t    win_sigma=1.5,\n\t    win=None,\n\t    K=(0.01, 0.03),\n\t    nonnegative_ssim=False,\n\t):\n\t    r\"\"\" interface of ssim\n\t    Args:\n\t        X (torch.Tensor): a batch of images, (N,C,H,W)\n", "        Y (torch.Tensor): a batch of images, (N,C,H,W)\n\t        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n\t        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n\t        win_size: (int, optional): the size of gauss kernel\n\t        win_sigma: (float, optional): sigma of normal distribution\n\t        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n\t        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n\t        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n\t    Returns:\n\t        torch.Tensor: ssim results\n", "    \"\"\"\n\t    if not X.shape == Y.shape:\n\t        raise ValueError(\"Input images should have the same dimensions.\")\n\t    for d in range(len(X.shape) - 1, 1, -1):\n\t        X = X.squeeze(dim=d)\n\t        Y = Y.squeeze(dim=d)\n\t    if len(X.shape) not in (4, 5):\n\t        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n\t    if not X.type() == Y.type():\n\t        raise ValueError(\"Input images should have the same dtype.\")\n", "    if win is not None:  # set win_size\n\t        win_size = win.shape[-1]\n\t    if not (win_size % 2 == 1):\n\t        raise ValueError(\"Window size should be odd.\")\n\t    if win is None:\n\t        win = _fspecial_gauss_1d(win_size, win_sigma)\n\t        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n\t    ssim_per_channel, cs = _ssim(X, Y, data_range=data_range, win=win, size_average=False, K=K)\n\t    if nonnegative_ssim:\n\t        ssim_per_channel = torch.relu(ssim_per_channel)\n", "    if size_average:\n\t        return ssim_per_channel.mean()\n\t    else:\n\t        return ssim_per_channel.mean(1)\n\tdef ms_ssim(\n\t    X, Y, data_range=255, size_average=True, win_size=11, win_sigma=1.5, win=None, weights=None, K=(0.01, 0.03)\n\t):\n\t    r\"\"\" interface of ms-ssim\n\t    Args:\n\t        X (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n", "        Y (torch.Tensor): a batch of images, (N,C,[T,]H,W)\n\t        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n\t        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n\t        win_size: (int, optional): the size of gauss kernel\n\t        win_sigma: (float, optional): sigma of normal distribution\n\t        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n\t        weights (list, optional): weights for different levels\n\t        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n\t    Returns:\n\t        torch.Tensor: ms-ssim results\n", "    \"\"\"\n\t    if not X.shape == Y.shape:\n\t        raise ValueError(\"Input images should have the same dimensions.\")\n\t    for d in range(len(X.shape) - 1, 1, -1):\n\t        X = X.squeeze(dim=d)\n\t        Y = Y.squeeze(dim=d)\n\t    if not X.type() == Y.type():\n\t        raise ValueError(\"Input images should have the same dtype.\")\n\t    if len(X.shape) == 4:\n\t        avg_pool = F.avg_pool2d\n", "    elif len(X.shape) == 5:\n\t        avg_pool = F.avg_pool3d\n\t    else:\n\t        raise ValueError(f\"Input images should be 4-d or 5-d tensors, but got {X.shape}\")\n\t    if win is not None:  # set win_size\n\t        win_size = win.shape[-1]\n\t    if not (win_size % 2 == 1):\n\t        raise ValueError(\"Window size should be odd.\")\n\t    smaller_side = min(X.shape[-2:])\n\t    assert smaller_side > (win_size - 1) * (\n", "        2 ** 4\n\t    ), \"Image size should be larger than %d due to the 4 downsamplings in ms-ssim\" % ((win_size - 1) * (2 ** 4))\n\t    if weights is None:\n\t        weights = [0.0448, 0.2856, 0.3001, 0.2363, 0.1333]\n\t    weights = torch.tensor(weights, device=X.device, dtype=X.dtype)\n\t    if win is None:\n\t        win = _fspecial_gauss_1d(win_size, win_sigma)\n\t        win = win.repeat([X.shape[1]] + [1] * (len(X.shape) - 1))\n\t    levels = weights.shape[0]\n\t    mcs = []\n", "    for i in range(levels):\n\t        ssim_per_channel, cs = _ssim(X, Y, win=win, data_range=data_range, size_average=False, K=K)\n\t        if i < levels - 1:\n\t            mcs.append(torch.relu(cs))\n\t            padding = [s % 2 for s in X.shape[2:]]\n\t            X = avg_pool(X, kernel_size=2, padding=padding)\n\t            Y = avg_pool(Y, kernel_size=2, padding=padding)\n\t    ssim_per_channel = torch.relu(ssim_per_channel)  # (batch, channel)\n\t    mcs_and_ssim = torch.stack(mcs + [ssim_per_channel], dim=0)  # (level, batch, channel)\n\t    ms_ssim_val = torch.prod(mcs_and_ssim ** weights.view(-1, 1, 1), dim=0)\n", "    if size_average:\n\t        return ms_ssim_val.mean()\n\t    else:\n\t        return ms_ssim_val.mean(1)\n\tclass SSIM(torch.nn.Module):\n\t    def __init__(\n\t        self,\n\t        data_range=255,\n\t        size_average=True,\n\t        win_size=11,\n", "        win_sigma=1.5,\n\t        channel=3,\n\t        spatial_dims=2,\n\t        K=(0.01, 0.03),\n\t        nonnegative_ssim=False,\n\t    ):\n\t        r\"\"\" class for ssim\n\t        Args:\n\t            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n\t            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n", "            win_size: (int, optional): the size of gauss kernel\n\t            win_sigma: (float, optional): sigma of normal distribution\n\t            channel (int, optional): input channels (default: 3)\n\t            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n\t            nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n\t        \"\"\"\n\t        super(SSIM, self).__init__()\n\t        self.win_size = win_size\n\t        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n\t        self.size_average = size_average\n", "        self.data_range = data_range\n\t        self.K = K\n\t        self.nonnegative_ssim = nonnegative_ssim\n\t    def forward(self, X, Y):\n\t        return ssim(\n\t            X,\n\t            Y,\n\t            data_range=self.data_range,\n\t            size_average=self.size_average,\n\t            win=self.win,\n", "            K=self.K,\n\t            nonnegative_ssim=self.nonnegative_ssim,\n\t        )\n\tclass MS_SSIM(torch.nn.Module):\n\t    def __init__(\n\t        self,\n\t        data_range=255,\n\t        size_average=True,\n\t        win_size=11,\n\t        win_sigma=1.5,\n", "        channel=3,\n\t        spatial_dims=2,\n\t        weights=None,\n\t        K=(0.01, 0.03),\n\t    ):\n\t        r\"\"\" class for ms-ssim\n\t        Args:\n\t            data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n\t            size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n\t            win_size: (int, optional): the size of gauss kernel\n", "            win_sigma: (float, optional): sigma of normal distribution\n\t            channel (int, optional): input channels (default: 3)\n\t            weights (list, optional): weights for different levels\n\t            K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n\t        \"\"\"\n\t        super(MS_SSIM, self).__init__()\n\t        self.win_size = win_size\n\t        self.win = _fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n\t        self.size_average = size_average\n\t        self.data_range = data_range\n", "        self.weights = weights\n\t        self.K = K\n\t    def forward(self, X, Y):\n\t        return ms_ssim(\n\t            X,\n\t            Y,\n\t            data_range=self.data_range,\n\t            size_average=self.size_average,\n\t            win=self.win,\n\t            weights=self.weights,\n", "            K=self.K,\n\t        )"]}
{"filename": "utils/optimizer.py", "chunked_list": ["# --------------------------------------------------------\n\t# Swin Transformer\n\t# Copyright (c) 2021 Microsoft\n\t# Licensed under The MIT License [see LICENSE for details]\n\t# Written by Ze Liu\n\t# --------------------------------------------------------\n\tfrom torch import optim as optim\n\t# from pytorch_lamb import Lamb\n\tdef build_optimizer(config, model):\n\t    \"\"\"\n", "    Build optimizer, set weight decay of normalization to 0 by default.\n\t    \"\"\"\n\t    skip = {}\n\t    skip_keywords = {}\n\t    if hasattr(model, 'no_weight_decay'):\n\t        skip = model.no_weight_decay()\n\t    if hasattr(model, 'no_weight_decay_keywords'):\n\t        skip_keywords = model.no_weight_decay_keywords()\n\t    parameters = set_weight_decay(model, skip, skip_keywords)\n\t    opt_lower = config['optimizer']['type'].lower()\n", "    optimizer = None\n\t    if opt_lower == 'sgd':\n\t        optimizer = optim.SGD(parameters, momentum=config['optimizer']['momentum'], nesterov=True,\n\t                              lr=config['base_lr'], weight_decay=config['weight_decay'])\n\t    elif opt_lower == 'adamw':\n\t        optimizer = optim.AdamW(parameters, eps=config['optimizer']['eps'], betas=config['optimizer']['betas'],\n\t                                lr=config['base_lr'], weight_decay=config['weight_decay'])\n\t    return optimizer\n\tdef set_weight_decay(model, skip_list=(), skip_keywords=()):\n\t    has_decay = []\n", "    no_decay = []\n\t    for name, param in model.named_parameters():\n\t        if not param.requires_grad:\n\t            continue  # frozen weights\n\t        if len(param.shape) == 1 or name.endswith(\".bias\") or (name in skip_list) or \\\n\t                check_keywords_in_name(name, skip_keywords):\n\t            no_decay.append(param)\n\t            # print(f\"{name} has no weight decay\")\n\t        else:\n\t            has_decay.append(param)\n", "    return [{'params': has_decay},\n\t            {'params': no_decay, 'weight_decay': 0.}]\n\tdef check_keywords_in_name(name, keywords=()):\n\t    isin = False\n\t    for keyword in keywords:\n\t        if keyword in name:\n\t            isin = True\n\t    return isin\n"]}
{"filename": "utils/logger.py", "chunked_list": ["# --------------------------------------------------------\n\t# Swin Transformer\n\t# Copyright (c) 2021 Microsoft\n\t# Licensed under The MIT License [see LICENSE for details]\n\t# Written by Ze Liu\n\t# --------------------------------------------------------\n\timport os\n\timport sys\n\timport logging\n\timport functools\n", "from termcolor import colored\n\tlogger = None\n\t@functools.lru_cache()\n\tdef create_logger(output_dir, dist_rank=None, name='', action='train'):\n\t    global logger\n\t    if logger is not None:\n\t        return logger\n\t    # create logger\n\t    logger = logging.getLogger()\n\t    logger.setLevel(logging.INFO)\n", "    logger.propagate = False\n\t    # create formatter\n\t    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n\t    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n\t                colored('(%(filename)s %(lineno)d)', 'yellow') + ': %(levelname)s %(message)s'\n\t    if dist_rank is not None:\n\t        # create console handlers for master process\n\t        if dist_rank == 0:\n\t            console_handler = logging.StreamHandler(sys.stdout)\n\t            console_handler.setLevel(logging.DEBUG)\n", "            console_handler.setFormatter(\n\t                logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n\t            logger.addHandler(console_handler)\n\t        file_handler = logging.FileHandler(os.path.join(output_dir, f'{action}-rank-{dist_rank}.log'), mode='a')\n\t    else:\n\t        console_handler = logging.StreamHandler(sys.stdout)\n\t        console_handler.setLevel(logging.DEBUG)\n\t        console_handler.setFormatter(\n\t            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n\t        logger.addHandler(console_handler)\n", "        file_handler = logging.FileHandler(os.path.join(output_dir, f'{action}.log'), mode='a')\n\t    file_handler.setLevel(logging.DEBUG)\n\t    file_handler.setFormatter(logging.Formatter(fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n\t    logger.addHandler(file_handler)\n\t    return logger\n"]}
{"filename": "utils/miscs.py", "chunked_list": ["import torch\n\timport os\n\timport shutil\n\timport cv2\n\timport numpy as np\n\tdef load_checkpoint(config, model, optimizer, lr_scheduler, logger, epoch=None):\n\t    resume_ckpt_path = config['train']['resume']\n\t    logger.info(f\"==============> Resuming form {resume_ckpt_path}....................\")\n\t    if resume_ckpt_path.startswith('https'):\n\t        checkpoint = torch.hub.load_state_dict_from_url(\n", "            resume_ckpt_path, map_location='cpu', check_hash=True)\n\t    else:\n\t        checkpoint = torch.load(resume_ckpt_path, map_location='cpu')\n\t    msg = model.load_state_dict(checkpoint['model'], strict=False)\n\t    logger.info(msg)\n\t    max_psnr = 0.0\n\t    if not config.get('eval_mode', False) and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint:\n\t        optimizer.load_state_dict(checkpoint['optimizer'])\n\t        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n\t        if 'max_psnr' in checkpoint:\n", "            max_psnr = checkpoint['max_psnr']\n\t    if epoch is None and 'epoch' in checkpoint:\n\t        config['train']['start_epoch'] = checkpoint['epoch']\n\t        logger.info(f\"=> loaded successfully '{resume_ckpt_path}' (epoch {checkpoint['epoch']})\")\n\t    del checkpoint\n\t    torch.cuda.empty_cache()\n\t    return max_psnr\n\tdef load_pretrained(config, model, logger):\n\t    logger.info(f\"==============> Loading weight {config['train']['pretrained']}....................\")\n\t    checkpoint = torch.load(config['train']['pretrained'], map_location='cpu')\n", "    state_dict = checkpoint['model']\n\t    msg = model.load_state_dict(state_dict, strict=False)\n\t    logger.warning(msg)\n\t    logger.info(f\"=> loaded successfully '{config['train']['pretrained']}'\")\n\t    del checkpoint\n\t    torch.cuda.empty_cache()\n\tdef save_checkpoint(config, epoch, model, max_psnr, optimizer, lr_scheduler, logger, is_best=False):\n\t    save_state = {'model': model.state_dict(),\n\t                  'optimizer': optimizer.state_dict(),\n\t                  'lr_scheduler': lr_scheduler.state_dict(),\n", "                  'max_psnr': max_psnr,\n\t                  'epoch': epoch,\n\t                  'config': config}\n\t    os.makedirs(os.path.join(config['output'], 'checkpoints'), exist_ok=True)\n\t    save_path = os.path.join(config['output'], 'checkpoints', 'checkpoint.pth')\n\t    logger.info(f\"{save_path} saving......\")\n\t    torch.save(save_state, save_path)\n\t    logger.info(f\"{save_path} saved\")\n\t    if epoch % config['save_per_epoch'] == 0 or (config['train']['epochs'] - epoch) < 50:\n\t        shutil.copy(save_path, os.path.join(config['output'], 'checkpoints', f'epoch_{epoch:04d}.pth'))\n", "        logger.info(f\"{save_path} copied to epoch_{epoch:04d}.pth\")\n\t    if is_best:\n\t        shutil.copy(save_path, os.path.join(config['output'], 'checkpoints', 'model_best.pth'))\n\t        logger.info(f\"{save_path} copied to model_best.pth\")\n\tdef get_grad_norm(parameters, norm_type=2):\n\t    if isinstance(parameters, torch.Tensor):\n\t        parameters = [parameters]\n\t    parameters = list(filter(lambda p: p.grad is not None, parameters))\n\t    norm_type = float(norm_type)\n\t    total_norm = 0\n", "    for p in parameters:\n\t        param_norm = p.grad.data.norm(norm_type)\n\t        total_norm += param_norm.item() ** norm_type\n\t    total_norm = total_norm ** (1. / norm_type)\n\t    return total_norm\n\tdef save_image_torch(img, file_path, range_255_float=True, params=None, auto_mkdir=True):\n\t    \"\"\"Write image to file.\n\t    Args:\n\t        img (ndarray): Image array to be written.\n\t        file_path (str): Image file path.\n", "        params (None or list): Same as opencv's :func:`imwrite` interface.\n\t        auto_mkdir (bool): If the parent folder of `file_path` does not exist,\n\t            whether to create it automatically.\n\t    Returns:\n\t        bool: Successful or not.\n\t    \"\"\"\n\t    if auto_mkdir:\n\t        dir_name = os.path.abspath(os.path.dirname(file_path))\n\t        os.makedirs(dir_name, exist_ok=True)\n\t    assert len(img.size()) == 3\n", "    img = img.clone().cpu().detach().numpy().transpose(1, 2, 0)\n\t    if range_255_float:\n\t        # Unlike MATLAB, numpy.unit8() WILL NOT round by default.\n\t        img = img.clip(0, 255).round()\n\t        img = img.astype(np.uint8)\n\t    else:\n\t        img = img.clip(0, 1)\n\t    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n\t    ok = cv2.imwrite(file_path, img, params)\n\t    if not ok:\n", "        raise IOError('Failed in writing images.')"]}
{"filename": "utils/__init__.py", "chunked_list": ["from .miscs import *"]}
{"filename": "scripts/inference.py", "chunked_list": ["from copy import deepcopy\n\timport os\n\timport time\n\timport datetime\n\timport yaml\n\timport git\n\timport torch\n\timport torch.backends.cudnn as cudnn\n\tfrom timm.utils import AverageMeter\n\tfrom utils import load_checkpoint, load_pretrained, save_image_torch\n", "from utils.config import parse_options, copy_cfg, ordered_dict_to_dict\n\tfrom utils.metrics import get_psnr_torch, get_ssim_torch\n\tfrom utils.logger import create_logger\n\tfrom models import build_model\n\tfrom datasets import build_test_loader\n\tfrom forwards import build_forward\n\tdef main(config):\n\t    data_loader = build_test_loader(config['data'], 2)\n\t    logger.info(f\"Creating model:{config['name']}/{config['model']['type']}\")\n\t    model = build_model(config['model'])\n", "    model.cuda()\n\t    logger.info(str(model))\n\t    logger.info('Building forwards:')\n\t    logger.info(f'Inference forward: {config[\"inference\"][\"forward_type\"]}')\n\t    forward = build_forward(config[\"inference\"][\"forward_type\"])\n\t    if config['train'].get('resume'):\n\t        load_checkpoint(config, model, None, None, logger)\n\t    if config['train'].get('pretrained') and (not config['train'].get('resume')):\n\t        load_pretrained(config, model, logger)\n\t    logger.info(\"Start Inference\")\n", "    start_time = time.time()\n\t    inference(config, forward, model, data_loader, logger)\n\t    total_time = time.time() - start_time\n\t    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t    logger.info('Total time {}'.format(total_time_str))\n\t@torch.no_grad()\n\tdef inference(config, forward, model, data_loader, logger):\n\t    torch.cuda.reset_max_memory_allocated()\n\t    model.eval()\n\t    batch_time = AverageMeter()\n", "    data_time = AverageMeter()\n\t    start = time.time()\n\t    end = time.time()\n\t    for idx, data in enumerate(data_loader):\n\t        data_time.update(time.time() - end)\n\t        outputs, img_files = forward(config, model, data)\n\t        output = outputs[config['inference']['which_stage']]\n\t        output = torch.clamp(output, 0, 1) * 255\n\t        result_path = os.path.join(config['output'], 'results', f'inference')\n\t        os.makedirs(result_path, exist_ok=True)\n", "        for i, result in enumerate(output):\n\t            save_path = os.path.join(result_path, f'{os.path.basename(img_files[i])[:-4]}.png')\n\t            save_image_torch(result, save_path)\n\t        # measure elapsed time\n\t        batch_time.update(time.time() - end)\n\t        end = time.time()\n\t        if config['testset_as_validset'] or idx % config['print_per_iter'] == 0 or idx == len(data_loader):\n\t            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n\t            logger.info(\n\t                f'Infer: [{idx}/{len(data_loader)}]\\t'\n", "                f'Time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n\t                f'Data {data_time.val:.4f} ({data_time.avg:.4f})\\t'\n\t                f'Mem {memory_used:.0f}MB\\t{os.path.basename(img_files[0])}')\n\t    logger.info(f'Infer: Time {datetime.timedelta(seconds=int(time.time()-start))}')\n\t@torch.no_grad()\n\tdef test_metric_cuda(config, epoch, outputs, targets, image_paths, target_params=None):\n\t    outputs = torch.clamp(outputs, 0, 1) * 255\n\t    targets = torch.clamp(targets, 0, 1) * 255\n\t    if config['test']['round']:\n\t        outputs = outputs.round()\n", "        targets = targets.round()\n\t    psnr = get_psnr_torch(outputs, targets)\n\t    ssim = get_ssim_torch(outputs, targets)\n\t    if config['test']['save_image']:\n\t        result_path = os.path.join(config['output'], 'results', f'test_{epoch:04d}')\n\t        os.makedirs(result_path, exist_ok=True)\n\t        save_path = os.path.join(result_path, f'{os.path.basename(image_paths[0])[:-4]}_{psnr.item():.2f}.png')\n\t        save_image_torch(outputs[0], save_path)\n\t    return psnr, ssim\n\tif __name__ == '__main__':\n", "    args, config = parse_options()\n\t    phase = 'infer'\n\t    if 'inference' not in config:\n\t        config['inference'] = deepcopy(config['test'])\n\t    config['testset_as_validset'] = True\n\t    config['eval_mode'] = True\n\t    assert not args.auto_resume and \\\n\t            (args.resume is not None or config['train'].get('resume') is not None) or \\\n\t            (args.pretrain is not None or config['train'].get('pretrained') is not None)\n\t    cudnn.benchmark = True\n", "    os.makedirs(config['output'], exist_ok=True)\n\t    start_time = time.strftime(\"%y%m%d-%H%M\", time.localtime())\n\t    logger = create_logger(output_dir=config['output'], name=f\"{config['tag']}\", action=f\"{phase}-{start_time}\")\n\t    path = os.path.join(config['output'], f\"{phase}-{start_time}.yaml\")\n\t    try:\n\t        repo = git.Repo(search_parent_directories=True)\n\t        sha = repo.head.object.hexsha\n\t        if repo.is_dirty():\n\t            logger.warning(f'Current work on commit: {sha}, however the repo is dirty (not committed)!')\n\t        else:\n", "            logger.info(f'Current work on commit: {sha}.')\n\t    except git.exc.InvalidGitRepositoryError as e:\n\t        logger.warn(f'No git repo base.')\n\t    copy_cfg(config, path)\n\t    logger.info(f\"Full config saved to {path}\")\n\t    # print config\n\t    logger.info(\"Config:\\n\" + yaml.dump(ordered_dict_to_dict(config), default_flow_style=False, sort_keys=False))\n\t    current_cuda_device = torch.cuda.get_device_properties(torch.cuda.current_device())\n\t    logger.info(f\"Current CUDA Device: {current_cuda_device.name}, Total Mem: {int(current_cuda_device.total_memory / 1024 / 1024)}MB\")\n\t    main(config)\n"]}
{"filename": "scripts/generate_video_from_images.py", "chunked_list": ["import argparse\n\timport cv2\n\timport os\n\timport glob\n\tfrom tqdm import tqdm\n\tIMAGES_PATH = ''\n\tSAVE_PATH = ''\n\tFILE_NAME = None\n\tFPS = 24\n\tdef parse_options():\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--images-path', type=str, required=True, help='Path to the image sequences.')\n\t    parser.add_argument('--save-path', type=str, required=True, help='Path to the save path.')\n\t    parser.add_argument('--file-name', type=str, required=True, help='File name.')\n\t    parser.add_argument('--fps', type=int, default=24, help='FPS of the target video.')\n\t    args = parser.parse_args()\n\t    global IMAGES_PATH, SAVE_PATH, FPS, FILE_NAME\n\t    IMAGES_PATH = args.images_path\n\t    SAVE_PATH = args.save_path if args.save_path != \"\" else \"runs/CVPR_DEMO/video/videos\"\n\t    FPS = args.fps\n", "    FILE_NAME = args.file_name\n\tdef main(images_path, save_path, fps, file_name=None, img_postfix='png', vid_postfix='mp4', scale_factor=1.0):\n\t    vid_to_fourcc = {\n\t        'avi': 'DIVX',\n\t        'mp4': 'mp4v'\n\t    }\n\t    images = list(sorted(glob.glob(f'{images_path}/*.{img_postfix}')))\n\t    H, W, _ = cv2.imread(images[0], cv2.IMREAD_COLOR).shape\n\t    frame_size = (int(W * scale_factor), int(H * scale_factor))\n\t    os.makedirs(save_path, exist_ok=True)\n", "    file_name = os.path.basename(images_path) if not file_name else file_name\n\t    out = cv2.VideoWriter(f'{save_path}/{file_name}.{vid_postfix}', cv2.VideoWriter_fourcc(*vid_to_fourcc[vid_postfix]), fps, frame_size)\n\t    for filename in tqdm(images):\n\t        img = cv2.imread(filename, cv2.IMREAD_COLOR)\n\t        if scale_factor != 1.0:\n\t            img = cv2.resize(img, frame_size, cv2.INTER_NEAREST)\n\t        out.write(img)\n\t    out.release()\n\tif __name__ == \"__main__\":\n\t    parse_options()\n", "    main(IMAGES_PATH, SAVE_PATH, FPS, file_name=FILE_NAME)"]}
{"filename": "scripts/preprocess/preprocess_sid.py", "chunked_list": ["import numpy as np\n\timport rawpy\n\timport os\n\tfrom glob import glob\n\tfrom tqdm import tqdm\n\timport time\n\timport argparse\n\tfrom multiprocessing import Pool\n\tfrom utils import save_image\n\timport cv2\n", "def parse_args():\n\t    parser = argparse.ArgumentParser('Preprocess dataset for fast training', add_help=False)\n\t    parser.add_argument('--data-path', type=str, default='./dataset/sid', help='path to dataset')\n\t    parser.add_argument('--camera', type=str, default='Sony', choices=['Sony', 'Fuji'], help='Determine the CFA pattern')\n\t    parser.add_argument('--split', type=str, default='long', choices=['long', 'short'], help='Preprocess long/short split of SID dataset')\n\t    args, unparsed = parser.parse_known_args()\n\t    return args\n\tmeta = {\n\t    'Sony': {\n\t        'white_level': 16383,\n", "        'black_level': 512,\n\t        'raw_ext': 'ARW',\n\t        'post_shape': (2848, 4256)\n\t    },\n\t    'Fuji': {\n\t        'white_level': 16383,\n\t        'black_level': 1024,\n\t        'raw_ext': 'RAF',\n\t        'post_shape': (4032, 6030)\n\t    }\n", "}\n\tdef pack_raw(im, camera):\n\t    if camera == 'Sony':\n\t        im = np.expand_dims(im, axis=0)\n\t        C, H, W = im.shape\n\t        out = np.concatenate((im[:, 0:H:2, 0:W:2],\n\t                              im[:, 0:H:2, 1:W:2],\n\t                              im[:, 1:H:2, 1:W:2],\n\t                              im[:, 1:H:2, 0:W:2]), axis=0)\n\t    elif camera == 'Fuji':\n", "        img_shape = im.shape\n\t        # orig 4032, 6032\n\t        # crop 4032, 6030\n\t        # pack 1344, 2010\n\t        H = (img_shape[0] // 6) * 6\n\t        W = (img_shape[1] // 6) * 6\n\t        out = np.zeros((9, H // 3, W // 3), dtype=np.uint16)\n\t        # 0 R\n\t        out[0, 0::2, 0::2] = im[0:H:6, 0:W:6]\n\t        out[0, 0::2, 1::2] = im[0:H:6, 4:W:6]\n", "        out[0, 1::2, 0::2] = im[3:H:6, 1:W:6]\n\t        out[0, 1::2, 1::2] = im[3:H:6, 3:W:6]\n\t        # 1 G\n\t        out[1, 0::2, 0::2] = im[0:H:6, 2:W:6]\n\t        out[1, 0::2, 1::2] = im[0:H:6, 5:W:6]\n\t        out[1, 1::2, 0::2] = im[3:H:6, 2:W:6]\n\t        out[1, 1::2, 1::2] = im[3:H:6, 5:W:6]\n\t        # 1 B\n\t        out[2, 0::2, 0::2] = im[0:H:6, 1:W:6]\n\t        out[2, 0::2, 1::2] = im[0:H:6, 3:W:6]\n", "        out[2, 1::2, 0::2] = im[3:H:6, 0:W:6]\n\t        out[2, 1::2, 1::2] = im[3:H:6, 4:W:6]\n\t        # 4 R\n\t        out[3, 0::2, 0::2] = im[1:H:6, 2:W:6]\n\t        out[3, 0::2, 1::2] = im[2:H:6, 5:W:6]\n\t        out[3, 1::2, 0::2] = im[5:H:6, 2:W:6]\n\t        out[3, 1::2, 1::2] = im[4:H:6, 5:W:6]\n\t        # 5 B\n\t        out[4, 0::2, 0::2] = im[2:H:6, 2:W:6]\n\t        out[4, 0::2, 1::2] = im[1:H:6, 5:W:6]\n", "        out[4, 1::2, 0::2] = im[4:H:6, 2:W:6]\n\t        out[4, 1::2, 1::2] = im[5:H:6, 5:W:6]\n\t        out[5, :, :] = im[1:H:3, 0:W:3]\n\t        out[6, :, :] = im[1:H:3, 1:W:3]\n\t        out[7, :, :] = im[2:H:3, 0:W:3]\n\t        out[8, :, :] = im[2:H:3, 1:W:3]\n\t    return out\n\tdef preprocess(image_path):\n\t    # print(image_path)\n\t    image_name = os.path.basename(image_path)\n", "    # read raw image\n\t    raw = rawpy.imread(image_path)\n\t    image_visible = raw.raw_image_visible.astype(np.uint16)\n\t    # pack raw image\n\t    pack_image = pack_raw(image_visible, args.camera)\n\t    # save packed image\n\t    save_pack_name = os.path.join(pack_path, image_name)\n\t    np.save(save_pack_name, pack_image, allow_pickle=True)\n\t    # post process raw image using rawpy\n\t    if args.split == 'long':\n", "        post_image = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n\t        # post_image_norm = np.float32(post_image) / 255\n\t        post_image = post_image.transpose(2, 0, 1)\n\t        if post_image.shape[1:] != meta[args.camera]['post_shape']:\n\t            H, W = meta[args.camera]['post_shape']\n\t            post_image = post_image[:, :H, :W]\n\t        save_post_name = os.path.join(post_path, image_name)\n\t        # save_post_jpg_name = os.path.join(post_jpg_path, image_name+'.jpg')\n\t        # save post processed image\n\t        np.save(save_post_name, post_image, allow_pickle=True)\n", "        # save_image(post_image_norm, save_post_jpg_name)\n\t    if args.split == 'long':\n\t        post_image = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n\t        post_image = cv2.cvtColor(post_image, cv2.COLOR_RGB2BGR)\n\t        post_image = (post_image / 65535.0 * 255.0).round().astype(np.uint8)\n\t        if post_image.shape[:2] != meta[args.camera]['post_shape']:\n\t            print(111)\n\t            H, W = meta[args.camera]['post_shape']\n\t            post_image = post_image[:H, :W, :]\n\t        save_post_png_name = os.path.join(post_png_path, image_name + '.png')\n", "        cv2.imwrite(save_post_png_name, post_image)\n\tif __name__ == '__main__':\n\t    args = parse_args()\n\t    print(args)\n\t    data_path = os.path.join(args.data_path, args.camera)\n\t    split_path = os.path.join(data_path, args.split)\n\t    pack_path = os.path.join(data_path, f'{args.split}_pack')\n\t    post_path = os.path.join(data_path, f'{args.split}_post_int')\n\t    post_jpg_path = os.path.join(data_path, f'{args.split}_post_jpg')\n\t    post_png_path = os.path.join(data_path, f'{args.split}_post_png_16')\n", "    os.makedirs(pack_path, exist_ok=True)\n\t    if args.split == 'long':\n\t        os.makedirs(post_path, exist_ok=True)\n\t        os.makedirs(post_jpg_path, exist_ok=True)\n\t        os.makedirs(post_png_path, exist_ok=True)\n\t    image_list = glob(os.path.join(split_path, f'*.{meta[args.camera][\"raw_ext\"]}'))\n\t    # image_list = image_list[:10]\n\t    print('number of images:', len(image_list))\n\t    with Pool(8) as pool:\n\t        with tqdm(total=len(image_list)) as t:\n", "            for i, x in enumerate(pool.imap(preprocess, image_list)):\n\t                t.update()\n"]}
{"filename": "models/sid_model.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tfrom utils.registry import MODEL_REGISTRY\n\t@MODEL_REGISTRY.register()\n\tclass SIDUNet(nn.Module):\n\t    def __init__(self, block_size=2, channels=32) -> None:\n\t        super().__init__()\n\t        inchannels = block_size * block_size\n\t        outchannels = block_size * block_size * 3\n\t        self.conv1_1 = nn.Conv2d(inchannels, channels, kernel_size=3, stride=1, padding=1)\n", "        self.conv1_2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n\t        self.pool1 = nn.MaxPool2d(kernel_size=2)\n\t        self.conv2_1 = nn.Conv2d(channels, channels * 2, kernel_size=3, stride=1, padding=1)\n\t        self.conv2_2 = nn.Conv2d(channels * 2, channels * 2, kernel_size=3, stride=1, padding=1)\n\t        self.pool2 = nn.MaxPool2d(kernel_size=2)\n\t        self.conv3_1 = nn.Conv2d(channels * 2, channels * 4, kernel_size=3, stride=1, padding=1)\n\t        self.conv3_2 = nn.Conv2d(channels * 4, channels * 4, kernel_size=3, stride=1, padding=1)\n\t        self.pool3 = nn.MaxPool2d(kernel_size=2)\n\t        self.conv4_1 = nn.Conv2d(channels * 4, channels * 8, kernel_size=3, stride=1, padding=1)\n\t        self.conv4_2 = nn.Conv2d(channels * 8, channels * 8, kernel_size=3, stride=1, padding=1)\n", "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n\t        self.conv5_1 = nn.Conv2d(channels * 8, channels * 16, kernel_size=3, stride=1, padding=1)\n\t        self.conv5_2 = nn.Conv2d(channels * 16, channels * 16, kernel_size=3, stride=1, padding=1)\n\t        self.upv6 = nn.ConvTranspose2d(channels * 16, channels * 8, 2, stride=2)\n\t        self.conv6_1 = nn.Conv2d(channels * 16, channels * 8, kernel_size=3, stride=1, padding=1)\n\t        self.conv6_2 = nn.Conv2d(channels * 8, channels * 8, kernel_size=3, stride=1, padding=1)\n\t        self.upv7 = nn.ConvTranspose2d(channels * 8, channels * 4, 2, stride=2)\n\t        self.conv7_1 = nn.Conv2d(channels * 8, channels * 4, kernel_size=3, stride=1, padding=1)\n\t        self.conv7_2 = nn.Conv2d(channels * 4, channels * 4, kernel_size=3, stride=1, padding=1)\n\t        self.upv8 = nn.ConvTranspose2d(channels * 4, channels * 2, 2, stride=2)\n", "        self.conv8_1 = nn.Conv2d(channels * 4, channels * 2, kernel_size=3, stride=1, padding=1)\n\t        self.conv8_2 = nn.Conv2d(channels * 2, channels * 2, kernel_size=3, stride=1, padding=1)\n\t        self.upv9 = nn.ConvTranspose2d(channels * 2, channels, 2, stride=2)\n\t        self.conv9_1 = nn.Conv2d(channels * 2, channels, kernel_size=3, stride=1, padding=1)\n\t        self.conv9_2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n\t        self.conv10_1 = nn.Conv2d(channels, outchannels, kernel_size=1, stride=1)\n\t        self.pixel_shuffle = nn.PixelShuffle(block_size)\n\t    def forward(self, x):\n\t        conv1 = self.lrelu(self.conv1_1(x))\n\t        conv1 = self.lrelu(self.conv1_2(conv1))\n", "        pool1 = self.pool1(conv1)\n\t        conv2 = self.lrelu(self.conv2_1(pool1))\n\t        conv2 = self.lrelu(self.conv2_2(conv2))\n\t        pool2 = self.pool1(conv2)\n\t        conv3 = self.lrelu(self.conv3_1(pool2))\n\t        conv3 = self.lrelu(self.conv3_2(conv3))\n\t        pool3 = self.pool1(conv3)\n\t        conv4 = self.lrelu(self.conv4_1(pool3))\n\t        conv4 = self.lrelu(self.conv4_2(conv4))\n\t        pool4 = self.pool1(conv4)\n", "        conv5 = self.lrelu(self.conv5_1(pool4))\n\t        conv5 = self.lrelu(self.conv5_2(conv5))\n\t        up6 = self.upv6(conv5)\n\t        up6 = torch.cat([up6, conv4], 1)\n\t        conv6 = self.lrelu(self.conv6_1(up6))\n\t        conv6 = self.lrelu(self.conv6_2(conv6))\n\t        up7 = self.upv7(conv6)\n\t        up7 = torch.cat([up7, conv3], 1)\n\t        conv7 = self.lrelu(self.conv7_1(up7))\n\t        conv7 = self.lrelu(self.conv7_2(conv7))\n", "        up8 = self.upv8(conv7)\n\t        up8 = torch.cat([up8, conv2], 1)\n\t        conv8 = self.lrelu(self.conv8_1(up8))\n\t        conv8 = self.lrelu(self.conv8_2(conv8))\n\t        up9 = self.upv9(conv8)\n\t        up9 = torch.cat([up9, conv1], 1)\n\t        conv9 = self.lrelu(self.conv9_1(up9))\n\t        conv9 = self.lrelu(self.conv9_2(conv9))\n\t        conv10 = self.conv10_1(conv9)\n\t        out = self.pixel_shuffle(conv10)\n", "        return out\n\t    def lrelu(self, x):\n\t        outt = torch.max(0.2 * x, x)\n\t        return outt\n"]}
{"filename": "models/__init__.py", "chunked_list": ["import importlib\n\tfrom copy import deepcopy\n\tfrom os import path as osp\n\tfrom glob import glob\n\tfrom utils.registry import MODEL_REGISTRY\n\t__all__ = ['build_model']\n\t# automatically scan and import model modules for registry\n\t# scan all the files under the 'models' folder and collect files ending with '_model.py'\n\tmodel_folder = osp.dirname(osp.abspath(__file__))\n\tmodel_filenames = [osp.splitext(osp.basename(v))[0] for v in glob(osp.join(model_folder, '*_model.py'))]\n", "# import all the model modules\n\t_model_modules = [importlib.import_module(f'models.{file_name}') for file_name in model_filenames]\n\tdef build_model(model_cfg):\n\t    model_cfg = deepcopy(model_cfg)\n\t    model_type = model_cfg.pop('type')\n\t    model = MODEL_REGISTRY.get(model_type)(**model_cfg)\n\t    return model"]}
{"filename": "models/utils.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tfrom torch.nn import functional as F\n\tclass LayerNorm(nn.Module):\n\t    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first.\n\t    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with\n\t    shape (batch_size, height, width, channels) while channels_first corresponds to inputs\n\t    with shape (batch_size, channels, height, width).\n\t    \"\"\"\n\t    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n", "        super().__init__()\n\t        self.weight = nn.Parameter(torch.ones(normalized_shape))\n\t        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n\t        self.eps = eps\n\t        self.data_format = data_format\n\t        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n\t            raise NotImplementedError\n\t        self.normalized_shape = (normalized_shape, )\n\t    def forward(self, x):\n\t        if self.data_format == \"channels_last\":\n", "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n\t        elif self.data_format == \"channels_first\":\n\t            u = x.mean(1, keepdim=True)\n\t            s = (x - u).pow(2).mean(1, keepdim=True)\n\t            x = (x - u) / torch.sqrt(s + self.eps)\n\t            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n\t            return x\n"]}
{"filename": "models/dnf_model.py", "chunked_list": ["from torch import nn\n\tfrom .dnf_utils.base import DNFBase\n\tfrom .dnf_utils.cid import CID\n\tfrom .dnf_utils.mcc import MCC\n\tfrom .dnf_utils.fuse import PDConvFuse, GFM\n\tfrom .dnf_utils.resudual_switch import ResidualSwitchBlock\n\tfrom utils.registry import MODEL_REGISTRY\n\t@MODEL_REGISTRY.register()\n\tclass SingleStageNet(DNFBase):\n\t    def __init__(self, f_number, *, \n", "                 block_size=1, \n\t                 layers=4, \n\t                 denoising_block='CID', \n\t                 color_correction_block='MCC') -> None:\n\t        super().__init__(f_number, block_size=block_size, layers=layers, \n\t                         denoising_block=denoising_block, color_correction_block=color_correction_block)\n\t    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n\t        return x, None, encoder_features\n\t@MODEL_REGISTRY.register()\n\tclass MultiStageNet(DNFBase):\n", "    def __init__(self, f_number, *, \n\t                 block_size=1, \n\t                 layers=4, \n\t                 denoising_block='CID', \n\t                 color_correction_block='MCC') -> None:\n\t        super().__init__(f_number, block_size=block_size, layers=layers, \n\t                         denoising_block=denoising_block, color_correction_block=color_correction_block)\n\t        aux_outchannel = 3 if block_size == 1 else block_size * block_size\n\t        self.aux_denoising_blocks = nn.ModuleList([\n\t            ResidualSwitchBlock(\n", "                self.denoising_block_class(\n\t                    f_number=f_number * (2**idx),\n\t                    padding_mode=self.padding_mode\n\t                )\n\t            )\n\t            for idx in range(layers)\n\t        ])\n\t        self.aux_upsamples = nn.ModuleList([\n\t            self.upsample_class(\n\t                f_number * (2**idx), \n", "                padding_mode=self.padding_mode\n\t            )\n\t            for idx in range(1, layers)\n\t        ])\n\t        self.denoising_decoder_fuses = nn.ModuleList([\n\t            self.decoder_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers - 1)\n\t        ])\n\t        self.aux_conv_fuse_0 = nn.Conv2d(f_number, f_number, 3, 1, 1, bias=True, padding_mode=self.padding_mode)\n\t        self.aux_conv_fuse_1 = nn.Conv2d(f_number, aux_outchannel, 1, 1, 0, bias=True)\n\t        inchannel = 3 if block_size == 1 else block_size * block_size\n", "        self.aux_feature_conv_0 = nn.Conv2d(inchannel, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n\t        self.aux_feature_conv_1 = nn.Conv2d(f_number, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n\t        head = [2 ** layer for layer in range(layers)]\n\t        self.aux_color_correction_blocks = nn.ModuleList([\n\t            self.color_correction_block_class(\n\t                f_number=f_number * (2 ** idx),\n\t                num_heads=head[idx],\n\t                padding_mode=self.padding_mode,\n\t            )\n\t            for idx in range(layers)\n", "        ])\n\t        self.aux_downsamples = nn.ModuleList([\n\t            self.downsample_class(\n\t                f_number * (2**idx), \n\t                padding_mode=self.padding_mode\n\t            )\n\t            for idx in range(layers - 1)\n\t        ])\n\t    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n\t        denoise_decoder_features = []\n", "        for denoise, up, fuse, encoder_feature in reversed(list(zip(\n\t            self.aux_denoising_blocks[1:], \n\t            self.aux_upsamples, \n\t            self.denoising_decoder_fuses,\n\t            encoder_features    \n\t        ))):\n\t            x = denoise(x, 1)\n\t            denoise_decoder_features.append(x)\n\t            x = up(x)\n\t            x = fuse(x, encoder_feature)\n", "        x = self.aux_denoising_blocks[0](x, 1)\n\t        denoise_decoder_features.append(x)\n\t        x = x + f_short_cut\n\t        x = self.act(self.aux_conv_fuse_0(x))\n\t        x = self.aux_conv_fuse_1(x)\n\t        res1 = x\n\t        encoder_features = []\n\t        x = self.act(self.aux_feature_conv_0(res1))\n\t        x = self.aux_feature_conv_1(x)\n\t        for color_correction, down in zip(self.aux_color_correction_blocks[:-1], self.aux_downsamples):\n", "            x = color_correction(x)\n\t            encoder_features.append(x)\n\t            x = down(x)\n\t        x = self.aux_color_correction_blocks[-1](x)\n\t        return x, res1, encoder_features\n\t@MODEL_REGISTRY.register()\n\tclass DNF(DNFBase):\n\t    def __init__(self, f_number, *,\n\t                block_size=1,\n\t                layers=4,\n", "                denoising_block='CID',\n\t                color_correction_block='MCC',\n\t                feedback_fuse='GFM'\n\t                ) -> None:\n\t        super(DNF, self).__init__(f_number=f_number, block_size=block_size, layers=layers,\n\t                                  denoising_block=denoising_block, color_correction_block=color_correction_block)\n\t        def get_class(class_or_class_str):\n\t            return eval(class_or_class_str) if isinstance(class_or_class_str, str) else class_or_class_str\n\t        self.feedback_fuse_class = get_class(feedback_fuse)\n\t        self.feedback_fuses = nn.ModuleList([\n", "            self.feedback_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers)\n\t        ])\n\t        aux_outchannel = 3 if block_size == 1 else block_size * block_size\n\t        self.aux_denoising_blocks = nn.ModuleList([\n\t            ResidualSwitchBlock(\n\t                self.denoising_block_class(\n\t                    f_number=f_number * (2**idx),\n\t                    padding_mode=self.padding_mode\n\t                )   \n\t            )\n", "            for idx in range(layers)\n\t        ])\n\t        self.aux_upsamples = nn.ModuleList([\n\t            self.upsample_class(\n\t                f_number * (2**idx), \n\t                padding_mode=self.padding_mode\n\t            )\n\t            for idx in range(1, layers)\n\t        ])\n\t        self.denoising_decoder_fuses = nn.ModuleList([\n", "            self.decoder_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers - 1)\n\t        ])\n\t        self.aux_conv_fuse_0 = nn.Conv2d(f_number, f_number, 3, 1, 1, bias=True, padding_mode=self.padding_mode)\n\t        self.aux_conv_fuse_1 = nn.Conv2d(f_number, aux_outchannel, 1, 1, 0, bias=True)\n\t    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n\t        ## denoising decoder\n\t        denoise_decoder_features = []\n\t        for denoise, up, fuse, encoder_feature in reversed(list(zip(\n\t            self.aux_denoising_blocks[1:], \n\t            self.aux_upsamples, \n", "            self.denoising_decoder_fuses,\n\t            encoder_features    \n\t        ))):\n\t            x = denoise(x, 1)\n\t            denoise_decoder_features.append(x)\n\t            x = up(x)\n\t            x = fuse(x, encoder_feature)\n\t        x = self.aux_denoising_blocks[0](x, 1)\n\t        denoise_decoder_features.append(x)\n\t        x = x + f_short_cut\n", "        x = self.act(self.aux_conv_fuse_0(x))\n\t        x = self.aux_conv_fuse_1(x)\n\t        res1 = x\n\t        ## feedback, local residual switch on\n\t        encoder_features = []\n\t        denoise_decoder_features.reverse()\n\t        x = f_short_cut\n\t        for fuse, denoise, down, decoder_feedback_feature in zip(\n\t            self.feedback_fuses[:-1], \n\t            self.denoising_blocks[:-1], \n", "            self.downsamples,\n\t            denoise_decoder_features[:-1]\n\t        ):\n\t            x = fuse(x, decoder_feedback_feature)\n\t            x = denoise(x, 1)  # residual switch on\n\t            encoder_features.append(x)\n\t            x = down(x)\n\t        x = self.feedback_fuses[-1](x, denoise_decoder_features[-1])\n\t        x = self.denoising_blocks[-1](x, 1)  # residual switch on\n\t        return x, res1, encoder_features\n"]}
{"filename": "models/dnf_utils/base.py", "chunked_list": ["from torch import nn\n\tfrom torch.nn import functional as F\n\tfrom .fuse import PDConvFuse\n\tfrom .cid import CID\n\tfrom .mcc import MCC\n\tfrom .sample import SimpleDownsample, SimpleUpsample\n\tfrom .resudual_switch import ResidualSwitchBlock\n\tfrom abc import abstractmethod\n\tclass DNFBase(nn.Module):\n\t    def __init__(self, f_number, *,\n", "                block_size=1,\n\t                layers=4,\n\t                denoising_block='CID',\n\t                color_correction_block='MCC'\n\t                ) -> None:\n\t        super().__init__()\n\t        def get_class(class_or_class_str):\n\t            return eval(class_or_class_str) if isinstance(class_or_class_str, str) else class_or_class_str\n\t        self.denoising_block_class = get_class(denoising_block)\n\t        self.color_correction_block_class = get_class(color_correction_block)\n", "        self.downsample_class = SimpleDownsample\n\t        self.upsample_class = SimpleUpsample\n\t        self.decoder_fuse_class = PDConvFuse\n\t        self.padding_mode = 'reflect'\n\t        self.act = nn.GELU()\n\t        self.layers = layers\n\t        head = [2 ** layer for layer in range(layers)]\n\t        self.block_size = block_size\n\t        inchannel = 3 if block_size == 1 else block_size * block_size\n\t        outchannel = 3 * block_size * block_size\n", "        self.feature_conv_0 = nn.Conv2d(inchannel, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n\t        self.feature_conv_1 = nn.Conv2d(f_number, f_number, 5, 1, 2, bias=True, padding_mode=self.padding_mode)\n\t        self.downsamples = nn.ModuleList([\n\t            self.downsample_class(\n\t                f_number * (2**idx), \n\t                padding_mode=self.padding_mode\n\t            )\n\t            for idx in range(layers - 1)\n\t        ])\n\t        self.upsamples = nn.ModuleList([\n", "            self.upsample_class(\n\t                f_number * (2**idx), \n\t                padding_mode=self.padding_mode\n\t            )\n\t            for idx in range(1, layers)\n\t        ])\n\t        self.denoising_blocks = nn.ModuleList([\n\t            ResidualSwitchBlock(\n\t                self.denoising_block_class(\n\t                    f_number=f_number * (2**idx),\n", "                    padding_mode=self.padding_mode\n\t                )\n\t            )\n\t            for idx in range(layers)\n\t        ])\n\t        self.color_correction_blocks = nn.ModuleList([\n\t            self.color_correction_block_class(\n\t                f_number=f_number * (2 ** idx),\n\t                num_heads=head[idx],\n\t                padding_mode=self.padding_mode,\n", "            )\n\t            for idx in range(layers)\n\t        ])\n\t        self.color_decoder_fuses = nn.ModuleList([\n\t            self.decoder_fuse_class(in_channels=f_number * (2 ** idx)) for idx in range(layers - 1)\n\t        ])\n\t        self.conv_fuse_0 = nn.Conv2d(f_number, f_number, 3, 1, 1, bias=True, padding_mode=self.padding_mode)\n\t        self.conv_fuse_1 = nn.Conv2d(f_number, outchannel, 1, 1, 0, bias=True)\n\t        if block_size > 1:\n\t            self.pixel_shuffle = nn.PixelShuffle(block_size)\n", "        else:\n\t            self.pixel_shuffle = nn.Identity()\n\t    @abstractmethod\n\t    def _pass_features_to_color_decoder(self, x, f_short_cut, encoder_features):\n\t        pass\n\t    def _check_and_padding(self, x):\n\t        # Calculate the required size based on the input size and required factor\n\t        _, _, h, w = x.size()\n\t        stride = (2 ** (self.layers - 1))\n\t        # Calculate the number of pixels needed to reach the required size\n", "        dh = -h % stride\n\t        dw = -w % stride\n\t        # Calculate the amount of padding needed for each side\n\t        top_pad = dh // 2\n\t        bottom_pad = dh - top_pad\n\t        left_pad = dw // 2\n\t        right_pad = dw - left_pad\n\t        self.crop_indices = (left_pad, w+left_pad, top_pad, h+top_pad)\n\t        # Pad the tensor with reflect mode\n\t        padded_tensor = F.pad(\n", "            x, (left_pad, right_pad, top_pad, bottom_pad), mode=\"reflect\"\n\t        )\n\t        return padded_tensor\n\t    def _check_and_crop(self, x, res1):\n\t        left, right, top, bottom = self.crop_indices\n\t        x = x[:, :, top*self.block_size:bottom*self.block_size, left*self.block_size:right*self.block_size]\n\t        res1 = res1[:, :, top:bottom, left:right] if res1 is not None else None\n\t        return x, res1\n\t    def forward(self, x):\n\t        x = self._check_and_padding(x)\n", "        x = self.act(self.feature_conv_0(x))\n\t        x = self.feature_conv_1(x)\n\t        f_short_cut = x\n\t        ## encoder, local residual switch off\n\t        encoder_features = []\n\t        for denoise, down in zip(self.denoising_blocks[:-1], self.downsamples):\n\t            x = denoise(x, 0)  # residual switch off\n\t            encoder_features.append(x)\n\t            x = down(x)\n\t        x = self.denoising_blocks[-1](x, 0)  # residual switch off\n", "        x, res1, refined_encoder_features = self._pass_features_to_color_decoder(x, f_short_cut, encoder_features) \n\t        ## color correction\n\t        for color_correction, up, fuse, encoder_feature in reversed(list(zip(\n\t            self.color_correction_blocks[1:], \n\t            self.upsamples, \n\t            self.color_decoder_fuses,\n\t            refined_encoder_features\n\t        ))):\n\t            x = color_correction(x)\n\t            x = up(x)\n", "            x = fuse(x, encoder_feature)\n\t        x = self.color_correction_blocks[0](x)\n\t        x = self.act(self.conv_fuse_0(x))\n\t        x = self.conv_fuse_1(x)\n\t        x = self.pixel_shuffle(x)\n\t        rgb, raw = self._check_and_crop(x, res1)\n\t        return rgb, raw\n"]}
{"filename": "models/dnf_utils/mcc.py", "chunked_list": ["from einops import rearrange\n\timport torch\n\tfrom torch import nn\n\tfrom ..utils import LayerNorm\n\tclass MCC(nn.Module):\n\t    def __init__(self, f_number, num_heads, padding_mode, bias=False) -> None:\n\t        super().__init__()\n\t        self.norm = LayerNorm(f_number, eps=1e-6, data_format='channels_first')\n\t        self.num_heads = num_heads\n\t        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n", "        self.pwconv = nn.Conv2d(f_number, f_number * 3, kernel_size=1, bias=bias)\n\t        self.dwconv = nn.Conv2d(f_number * 3, f_number * 3, 3, 1, 1, bias=bias, padding_mode=padding_mode, groups=f_number * 3)\n\t        self.project_out = nn.Conv2d(f_number, f_number, kernel_size=1, bias=bias)\n\t        self.feedforward = nn.Sequential(\n\t            nn.Conv2d(f_number, f_number, 1, 1, 0, bias=bias),\n\t            nn.GELU(),\n\t            nn.Conv2d(f_number, f_number, 3, 1, 1, bias=bias, groups=f_number, padding_mode=padding_mode),\n\t            nn.GELU()\n\t        )\n\t    def forward(self, x):\n", "        attn = self.norm(x)\n\t        _, _, h, w = attn.shape\n\t        qkv = self.dwconv(self.pwconv(attn))\n\t        q, k, v = qkv.chunk(3, dim=1)\n\t        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\t        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\t        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\t        q = torch.nn.functional.normalize(q, dim=-1)\n\t        k = torch.nn.functional.normalize(k, dim=-1)\n\t        attn = (q @ k.transpose(-2, -1)) * self.temperature\n", "        attn = attn.softmax(dim=-1)\n\t        out = (attn @ v)\n\t        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n\t        out = self.project_out(out)\n\t        out = self.feedforward(out + x)\n\t        return out\n"]}
{"filename": "models/dnf_utils/__init__.py", "chunked_list": []}
{"filename": "models/dnf_utils/cid.py", "chunked_list": ["from torch import nn\n\t# CI\n\tclass DConv7(nn.Module):\n\t    def __init__(self, f_number, padding_mode='reflect') -> None:\n\t        super().__init__()\n\t        self.dconv = nn.Conv2d(f_number, f_number, kernel_size=7, padding=3, groups=f_number, padding_mode=padding_mode)\n\t    def forward(self, x):\n\t        return self.dconv(x)\n\t# Post-CI\n\tclass MLP(nn.Module):\n", "    def __init__(self, f_number, excitation_factor=2) -> None:\n\t        super().__init__()\n\t        self.act = nn.GELU()\n\t        self.pwconv1 = nn.Conv2d(f_number, excitation_factor * f_number, kernel_size=1)\n\t        self.pwconv2 = nn.Conv2d(f_number * excitation_factor, f_number, kernel_size=1)\n\t    def forward(self, x):\n\t        x = self.pwconv1(x)\n\t        x = self.act(x)\n\t        x = self.pwconv2(x)\n\t        return x\n", "class CID(nn.Module):\n\t    def __init__(self, f_number, padding_mode) -> None:\n\t        super().__init__()\n\t        self.channel_independent = DConv7(f_number, padding_mode)\n\t        self.channel_dependent = MLP(f_number, excitation_factor=2)\n\t    def forward(self, x):\n\t        return self.channel_dependent(self.channel_independent(x))\n"]}
{"filename": "models/dnf_utils/sample.py", "chunked_list": ["from torch import nn\n\tclass SimpleDownsample(nn.Module):\n\t    def __init__(self, dim, *, padding_mode='reflect'):\n\t        super().__init__()\n\t        self.body = nn.Conv2d(dim, dim*2, kernel_size=2, stride=2, padding=0, bias=False, padding_mode=padding_mode)\n\t    def forward(self, x):\n\t        return self.body(x)\n\tclass SimpleUpsample(nn.Module):\n\t    def __init__(self, dim, *, padding_mode='reflect'):\n\t        super().__init__()\n", "        self.body = nn.ConvTranspose2d(dim, dim//2, kernel_size=2, stride=2, padding=0, bias=False)\n\t    def forward(self, x):\n\t        return self.body(x)\n"]}
{"filename": "models/dnf_utils/fuse.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tfrom torch.nn import functional as F\n\tclass PDConvFuse(nn.Module):\n\t    def __init__(self, in_channels=None, f_number=None, feature_num=2, bias=True, **kwargs) -> None:\n\t        super().__init__()\n\t        if in_channels is None:\n\t            assert f_number is not None\n\t            in_channels = f_number\n\t        self.feature_num = feature_num\n", "        self.act = nn.GELU()\n\t        self.pwconv = nn.Conv2d(feature_num * in_channels, in_channels, 1, 1, 0, bias=bias)\n\t        self.dwconv = nn.Conv2d(in_channels, in_channels, 3, 1, 1, bias=bias, groups=in_channels, padding_mode='reflect')\n\t    def forward(self, *inp_feats):\n\t        assert len(inp_feats) == self.feature_num\n\t        return self.dwconv(self.act(self.pwconv(torch.cat(inp_feats, dim=1))))\n\tclass GFM(nn.Module):\n\t    def __init__(self, in_channels, feature_num=2, bias=True, padding_mode='reflect', **kwargs) -> None:\n\t        super().__init__()\n\t        self.feature_num = feature_num\n", "        hidden_features = in_channels * feature_num\n\t        self.pwconv = nn.Conv2d(hidden_features, hidden_features * 2, 1, 1, 0, bias=bias)\n\t        self.dwconv = nn.Conv2d(hidden_features * 2, hidden_features * 2, 3, 1, 1, bias=bias, padding_mode=padding_mode, groups=hidden_features * 2)\n\t        self.project_out = nn.Conv2d(hidden_features, in_channels, kernel_size=1, bias=bias)\n\t        self.mlp = nn.Conv2d(in_channels, in_channels, 1, 1, 0, bias=True)\n\t    def forward(self, *inp_feats):\n\t        assert len(inp_feats) == self.feature_num\n\t        shortcut = inp_feats[0]\n\t        x = torch.cat(inp_feats, dim=1)\n\t        x = self.pwconv(x)\n", "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n\t        x = F.gelu(x1) * x2\n\t        x = self.project_out(x)\n\t        return self.mlp(x + shortcut)\n"]}
{"filename": "models/dnf_utils/resudual_switch.py", "chunked_list": ["from torch import nn\n\tclass ResidualSwitchBlock(nn.Module):\n\t    def __init__(self, block) -> None:\n\t        super().__init__()\n\t        self.block = block\n\t    def forward(self, x, residual_switch):\n\t        return self.block(x) + residual_switch * x"]}
{"filename": "forwards/single_stage_forward.py", "chunked_list": ["from utils.registry import FORWARD_REGISTRY\n\tfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\t@FORWARD_REGISTRY.register()\n\tdef ss_train_forward(config, model, data):\n\t    raw = data['noisy_raw'].cuda(non_blocking=True)\n\t    rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n\t    rgb_out = model(raw)\n\t    ###### | output          | label\n\t    return {'rgb': rgb_out}, {'rgb': rgb_gt}\n\t@FORWARD_REGISTRY.register()\n", "def ss_test_forward(config, model, data):\n\t    if not config['test'].get('cpu', False):\n\t        raw = data['noisy_raw'].cuda(non_blocking=True)\n\t        rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n\t    else:\n\t        raw = data['noisy_raw']\n\t        rgb_gt = data['clean_rgb']\n\t    img_files = data['img_file']\n\t    lbl_files = data['lbl_file']\n\t    rgb_out = model(raw)\n", "    return {'rgb': rgb_out}, {'rgb': rgb_gt}, img_files, lbl_files\n\t@FORWARD_REGISTRY.register()  # without label, for inference only\n\tdef ss_inference(config, model, data):\n\t    raw = data['noisy_raw'].cuda(non_blocking=True)\n\t    img_files = data['img_file']\n\t    rgb_out = model(raw)\n\t    ###### | output          | img names\n\t    return {'rgb': rgb_out}, img_files\n"]}
{"filename": "forwards/__init__.py", "chunked_list": ["import importlib\n\tfrom copy import deepcopy\n\tfrom os import path as osp\n\tfrom glob import glob\n\tfrom utils.registry import FORWARD_REGISTRY\n\t__all__ = ['build_forwards', 'build_profile']\n\t# automatically scan and import forward modules for registry\n\t# scan all the files under the 'forwards' folder and collect files ending with '_forward.py'\n\tforward_folder = osp.dirname(osp.abspath(__file__))\n\tforward_filenames = [osp.splitext(osp.basename(v))[0] for v in glob(osp.join(forward_folder, '*_forward.py'))]\n", "# import all the forward modules\n\t_forward_modules = [importlib.import_module(f'forwards.{file_name}') for file_name in forward_filenames]\n\tdef build_forwards(cfg):\n\t    cfg = deepcopy(cfg)\n\t    train_fwd_type = cfg['train']['forward_type']\n\t    test_fwd_type = cfg['test']['forward_type']\n\t    train_forward = FORWARD_REGISTRY.get(train_fwd_type)\n\t    test_forward = FORWARD_REGISTRY.get(test_fwd_type)\n\t    return train_forward, test_forward\n\tdef build_forward(forward_type):\n", "    return FORWARD_REGISTRY.get(forward_type)\n\tdef build_profile(cfg):\n\t    cfg = deepcopy(cfg)\n\t    profile = cfg.get('profile')\n\t    if profile is None:\n\t        return profile\n\t    return FORWARD_REGISTRY.get(profile)\n"]}
{"filename": "forwards/dnf_forward.py", "chunked_list": ["from utils.registry import FORWARD_REGISTRY\n\tfrom fvcore.nn import FlopCountAnalysis, flop_count_table\n\t@FORWARD_REGISTRY.register(suffix='DNF')\n\tdef train_forward(config, model, data):\n\t    raw = data['noisy_raw'].cuda(non_blocking=True)\n\t    raw_gt = data['clean_raw'].cuda(non_blocking=True)\n\t    rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n\t    rgb_out, raw_out = model(raw)\n\t    ###### | output                          | label\n\t    return {'rgb': rgb_out, 'raw': raw_out}, {'rgb': rgb_gt, 'raw': raw_gt}\n", "@FORWARD_REGISTRY.register(suffix='DNF')\n\tdef test_forward(config, model, data):\n\t    if not config['test'].get('cpu', False):\n\t        raw = data['noisy_raw'].cuda(non_blocking=True)\n\t        raw_gt = data['clean_raw'].cuda(non_blocking=True)\n\t        rgb_gt = data['clean_rgb'].cuda(non_blocking=True)\n\t    else:\n\t        raw = data['noisy_raw']\n\t        raw_gt = data['clean_raw']\n\t        rgb_gt = data['clean_rgb']\n", "    img_files = data['img_file']\n\t    lbl_files = data['lbl_file']\n\t    rgb_out, raw_out = model(raw)\n\t    ###### | output                          | label                         | img and label names\n\t    return {'rgb': rgb_out, 'raw': raw_out}, {'rgb': rgb_gt, 'raw': raw_gt}, img_files, lbl_files\n\t@FORWARD_REGISTRY.register(suffix='DNF')  # without label, for inference only\n\tdef inference(config, model, data):\n\t    raw = data['noisy_raw'].cuda(non_blocking=True)\n\t    img_files = data['img_file']\n\t    rgb_out, raw_out = model(raw)\n", "    ###### | output                          | img names\n\t    return {'rgb': rgb_out, 'raw': raw_out}, img_files\n\t@FORWARD_REGISTRY.register()\n\tdef DNF_profile(config, model, data, logger):\n\t    x = data['noisy_raw'].cuda()\n\t    flops = FlopCountAnalysis(model, x)\n\t    logger.info('Detaild FLOPs:\\n' + flop_count_table(flops))\n\t    flops_total = flops.total()\n\t    logger.info(f\"Total FLOPs: {flops_total:,}\")\n"]}
{"filename": "datasets/MCR_dict_dataset.py", "chunked_list": ["#!/usr/bin/env python\n\timport os\n\timport numpy as np\n\timport rawpy\n\timport torch\n\tfrom torch.utils import data\n\timport time\n\tfrom utils.registry import DATASET_REGISTRY\n\tfrom timm.utils import AverageMeter\n\timport imageio\n", "import tqdm\n\t@DATASET_REGISTRY.register()\n\tclass MCRDictSet(data.Dataset):\n\t    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, repeat=1,\n\t                 raw_ext='ARW', max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n\t                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n\t        \"\"\"\n\t        :param data_path: dataset directory\n\t        :param image_list_file: contains image file names under data_path\n\t        :param patch_size: if None, full images are returned, otherwise patches are returned\n", "        :param split: train or valid\n\t        :param upper: max number of image used for debug\n\t        \"\"\"\n\t        assert os.path.exists(data_path), \"data_path: {} not found.\".format(data_path)\n\t        self.data_path = data_path\n\t        image_list_file = os.path.join(data_path, image_list_file)\n\t        assert os.path.exists(image_list_file), \"image_list_file: {} not found.\".format(image_list_file)\n\t        self.image_list_file = image_list_file\n\t        self.patch_size = patch_size\n\t        self.split = split\n", "        self.load_npy = load_npy\n\t        self.raw_ext = raw_ext\n\t        self.max_clip = max_clip\n\t        self.min_clip = min_clip\n\t        self.transpose = transpose\n\t        self.h_flip = h_flip\n\t        self.v_flip = v_flip\n\t        self.rotation = rotation\n\t        self.ratio = ratio\n\t        self.only_00 = only_00\n", "        self.repeat = repeat\n\t        self.raw_short_read_time = AverageMeter()\n\t        self.raw_short_pack_time = AverageMeter()\n\t        self.raw_short_post_time = AverageMeter()\n\t        self.raw_long_read_time = AverageMeter()\n\t        self.raw_long_pack_time = AverageMeter()\n\t        self.raw_long_post_time = AverageMeter()\n\t        self.npy_long_read_time = AverageMeter()\n\t        self.data_aug_time = AverageMeter()\n\t        self.data_norm_time = AverageMeter()\n", "        self.count = 0\n\t        self.block_size = 2\n\t        self.black_level = 0\n\t        self.white_level = 255\n\t        self.raw_input_path = []\n\t        self.raw_gt_path = []\n\t        self.rgb_gt_path = []\n\t        self.rgb_gt_dict = {}\n\t        self.raw_input_list = []\n\t        self.raw_gt_dict = {}\n", "        with open(self.image_list_file, 'r') as f:\n\t            for i, img_pair in enumerate(f):\n\t                raw_input_path, raw_gt_path, rgb_gt_path = img_pair.strip().split(' ')\n\t                self.raw_input_path.append(os.path.join(self.data_path, raw_input_path))\n\t                self.raw_gt_path.append(os.path.join(self.data_path, raw_gt_path))\n\t                self.rgb_gt_path.append(os.path.join(self.data_path, rgb_gt_path))\n\t                raw_input = imageio.imread(os.path.join(self.data_path, raw_input_path))\n\t                self.raw_input_list.append(raw_input)\n\t                raw_gt = imageio.imread(os.path.join(self.data_path, raw_gt_path))\n\t                raw_gt_name = os.path.basename(raw_gt_path)\n", "                if raw_gt_name not in self.raw_gt_dict:\n\t                    self.raw_gt_dict[raw_gt_name] = raw_gt\n\t                rgb_gt = imageio.imread(os.path.join(self.data_path, rgb_gt_path)).transpose(2, 0, 1)\n\t                rgb_gt_name = os.path.basename(rgb_gt_path)\n\t                if rgb_gt_name not in self.rgb_gt_dict:\n\t                    self.rgb_gt_dict[rgb_gt_name] = rgb_gt\n\t                if max_samples and i == max_samples - 1:  # for debug purpose\n\t                    break\n\t        print(\"processing: {} images for {}\".format(len(self.raw_input_path), self.split))\n\t    def __len__(self):\n", "        return len(self.raw_input_path) * self.repeat\n\t    def print_time(self):\n\t        print('self.raw_short_read_time:', self.raw_short_read_time.avg)\n\t        print('self.raw_short_pack_time:', self.raw_short_pack_time.avg)\n\t        print('self.raw_short_post_time:', self.raw_short_post_time.avg)\n\t        print('self.raw_long_read_time:', self.raw_long_read_time.avg)\n\t        print('self.raw_long_pack_time:', self.raw_long_pack_time.avg)\n\t        print('self.raw_long_post_time:', self.raw_long_post_time.avg)\n\t        print('self.npy_long_read_time:', self.npy_long_read_time.avg)\n\t        print('self.data_aug_time:', self.data_aug_time.avg)\n", "        print('self.data_norm_time:', self.data_norm_time.avg)\n\t    def __getitem__(self, index):\n\t        self.count += 1\n\t        idx = index // self.repeat\n\t        if self.count % 100 == 0 and False:\n\t            self.print_time()\n\t        info = self.raw_input_path[idx]\n\t        img_file = info\n\t        start = time.time()\n\t        noisy_raw = self.raw_input_list[idx]\n", "        if self.patch_size is None:\n\t            # pack raw with patch size is implemented in clip patch for reduce computation\n\t            noisy_raw = self._pack_raw(noisy_raw)\n\t        self.raw_short_read_time.update(time.time() - start)\n\t        lbl_file = self.rgb_gt_path[idx]\n\t        start = time.time()\n\t        clean_rgb = self.rgb_gt_dict[os.path.basename(self.rgb_gt_path[idx])]\n\t        self.raw_long_post_time.update(time.time() - start)\n\t        start = time.time()\n\t        clean_raw = self.raw_gt_dict[os.path.basename(self.raw_gt_path[idx])]\n", "        if self.patch_size is None:\n\t            clean_raw = self._pack_raw(clean_raw)\n\t        self.raw_long_read_time.update(time.time() - start)\n\t        if self.patch_size:\n\t            start = time.time()\n\t            patch_size = self.patch_size\n\t            H, W = clean_rgb.shape[1:3]\n\t            if self.split == 'train':\n\t                if (H - patch_size) // self.block_size > 0:\n\t                    yy = torch.randint(0, (H - patch_size) // self.block_size, (1,))\n", "                else:\n\t                    yy = 0\n\t                if (W - patch_size) // self.block_size > 0:\n\t                    xx = torch.randint(0, (W - patch_size) // self.block_size, (1,))\n\t                else:\n\t                    xx = 0\n\t                # yy, xx = torch.randint(0, (H - patch_size) // self.block_size, (1,)),  torch.randint(0, (W - patch_size) // self.block_size, (1,))\n\t            else:\n\t                yy, xx = (H - patch_size) // self.block_size // 2, (W - patch_size) // self.block_size // 2\n\t            input_patch = self._pack_raw(noisy_raw, yy, xx)\n", "            clean_raw_patch = self._pack_raw(clean_raw, yy, xx)\n\t            gt_patch = clean_rgb[:, yy*self.block_size:yy*self.block_size + patch_size, xx*self.block_size:xx*self.block_size + patch_size]\n\t            if self.h_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random horizontal flip\n\t                input_patch = np.flip(input_patch, axis=2)\n\t                gt_patch = np.flip(gt_patch, axis=2)\n\t                clean_raw_patch = np.flip(clean_raw_patch, axis=2)\n\t            if self.v_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random vertical flip\n\t                input_patch = np.flip(input_patch, axis=1)\n\t                gt_patch = np.flip(gt_patch, axis=1)\n\t                clean_raw_patch = np.flip(clean_raw_patch, axis=1)\n", "            if self.transpose and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random transpose\n\t                input_patch = np.transpose(input_patch, (0, 2, 1))\n\t                gt_patch = np.transpose(gt_patch, (0, 2, 1))\n\t                clean_raw_patch = np.transpose(clean_raw_patch, (0, 2, 1))\n\t            if self.rotation and self.split == 'train':\n\t                raise NotImplementedError('rotation')\n\t            noisy_raw = input_patch.copy()\n\t            clean_rgb = gt_patch.copy()\n\t            clean_raw = clean_raw_patch.copy()\n\t            self.data_aug_time.update(time.time() - start)\n", "        start = time.time()\n\t        noisy_raw = (np.float32(noisy_raw) - self.black_level) / np.float32(self.white_level - self.black_level)  # subtract the black level\n\t        clean_raw = (np.float32(clean_raw) - self.black_level) / np.float32(self.white_level - self.black_level)\n\t        clean_rgb = np.float32(clean_rgb) / np.float32(255)\n\t        self.data_norm_time.update(time.time() - start)\n\t        img_num = int(self.raw_input_path[idx][-23:-20])\n\t        img_expo = int(self.raw_input_path[idx][-8:-4],16)\n\t        if img_num < 500:\n\t            gt_expo = 12287\n\t        else:\n", "            gt_expo = 1023\n\t        ratio = gt_expo / img_expo\n\t        if self.ratio:\n\t            noisy_raw = noisy_raw * ratio\n\t        if self.max_clip is not None:\n\t            noisy_raw = np.minimum(noisy_raw, self.max_clip)\n\t        if self.min_clip is not None:\n\t            noisy_raw = np.maximum(noisy_raw, self.min_clip)\n\t        clean_rgb = clean_rgb.clip(0.0, 1.0)\n\t        noisy_raw = torch.from_numpy(noisy_raw).float()\n", "        clean_rgb = torch.from_numpy(clean_rgb).float()\n\t        clean_raw = torch.from_numpy(clean_raw).float()\n\t        return {\n\t            'noisy_raw': noisy_raw,\n\t            'clean_raw': clean_raw,\n\t            'clean_rgb': clean_rgb,\n\t            'img_file': img_file,\n\t            'lbl_file': lbl_file,\n\t            'img_exposure': img_expo,\n\t            'lbl_exposure': gt_expo,\n", "            'ratio': ratio\n\t        }\n\t    def _pack_raw(self, raw, hh=None, ww=None):\n\t        if self.patch_size is None:\n\t            assert hh is None and ww is None\n\t        # pack Bayer image to 4 channels (RGBG)\n\t        # im = raw.raw_image_visible.astype(np.uint16)\n\t        H, W = raw.shape\n\t        im = np.expand_dims(raw, axis=0)\n\t        if self.patch_size is None:\n", "            out = np.concatenate((im[:, 0:H:2, 0:W:2],\n\t                                  im[:, 0:H:2, 1:W:2],\n\t                                  im[:, 1:H:2, 1:W:2],\n\t                                  im[:, 1:H:2, 0:W:2]), axis=0)\n\t        else:\n\t            h1 = hh * 2\n\t            h2 = hh * 2 + self.patch_size\n\t            w1 = ww * 2\n\t            w2 = ww * 2 + self.patch_size\n\t            out = np.concatenate((im[:, h1:h2:2, w1:w2:2],\n", "                                  im[:, h1:h2:2, w1+1:w2:2],\n\t                                  im[:, h1+1:h2:2, w1+1:w2:2],\n\t                                  im[:, h1+1:h2:2, w1:w2:2]), axis=0)\n\t        return out"]}
{"filename": "datasets/single_bayer_dict_dataset.py", "chunked_list": ["#!/usr/bin/env python\n\timport os\n\timport numpy as np\n\timport rawpy\n\timport torch\n\tfrom torch.utils import data\n\tfrom glob import glob\n\timport time\n\tfrom utils.registry import DATASET_REGISTRY\n\tfrom timm.utils import AverageMeter\n", "class BaseDictSet(data.Dataset):\n\t    def __init__(self, data_path, load_npy=True,\n\t                 max_clip=1.0, min_clip=None, ratio=1, **kwargs):\n\t        \"\"\"\n\t        :param data_path: dataset directory\n\t        :param image_list_file: contains image file names under data_path\n\t        :param patch_size: if None, full images are returned, otherwise patches are returned\n\t        :param split: train or valid\n\t        :param upper: max number of image used for debug\n\t        \"\"\"\n", "        assert os.path.exists(data_path), \"data_path: {} not found.\".format(data_path)\n\t        self.data_path = data_path\n\t        self.load_npy = load_npy\n\t        self.max_clip = max_clip\n\t        self.min_clip = min_clip\n\t        self.ratio = ratio\n\t        self.raw_short_read_time = AverageMeter()\n\t        self.raw_short_pack_time = AverageMeter()\n\t        self.data_norm_time = AverageMeter()\n\t        self.count = 0\n", "        self.img_info = []\n\t        img_list = sorted(glob(f'{self.data_path}/*'))\n\t        for i, img_file in enumerate(img_list):\n\t            img_file = os.path.basename(img_file)\n\t            self.img_info.append({\n\t                'img': img_file,\n\t                'ratio': np.float32(ratio)\n\t            })\n\t        print(\"processing: {} images\".format(len(self.img_info)))\n\t    def __len__(self):\n", "        return len(self.img_info)\n\t    def print_time(self):\n\t        print('self.raw_short_read_time:', self.raw_short_read_time.avg)\n\t        print('self.raw_short_pack_time:', self.raw_short_pack_time.avg)\n\t        print('self.data_norm_time:', self.data_norm_time.avg)\n\t    def __getitem__(self, index):\n\t        self.count += 1\n\t        if self.count % 100 == 0 and False:\n\t            self.print_time()\n\t        info = self.img_info[index]\n", "        img_file = info['img']\n\t        if not self.load_npy:\n\t            start = time.time()\n\t            raw = rawpy.imread(os.path.join(self.data_path, img_file))\n\t            self.raw_short_read_time.update(time.time() - start)\n\t            start = time.time()\n\t            noisy_raw = self._pack_raw(raw)\n\t            self.raw_short_pack_time.update(time.time() - start)\n\t        else:\n\t            start = time.time()\n", "            noisy_raw = np.load(os.path.join(self.data_path, img_file), allow_pickle=True)\n\t            self.raw_short_read_time.update(time.time() - start)\n\t        start = time.time()\n\t        noisy_raw = (np.float32(noisy_raw) - self.black_level) / np.float32(self.white_level - self.black_level)  # subtract the black level\n\t        self.data_norm_time.update(time.time() - start)\n\t        if self.ratio:\n\t            noisy_raw = noisy_raw * info['ratio']\n\t        if self.max_clip is not None:\n\t            noisy_raw = np.minimum(noisy_raw, self.max_clip)\n\t        if self.min_clip is not None:\n", "            noisy_raw = np.maximum(noisy_raw, self.min_clip)\n\t        noisy_raw = torch.from_numpy(noisy_raw).float()\n\t        return {\n\t            'noisy_raw': noisy_raw,\n\t            'img_file': img_file,\n\t            'ratio': info['ratio']\n\t        }\n\t@DATASET_REGISTRY.register()\n\tclass SingleBayerDictSet(BaseDictSet):\n\t    def __init__(self, data_path, load_npy=True, max_clip=1, min_clip=None, ratio=1, black_level=512, white_level=16383, **kwargs):\n", "        super().__init__(data_path, load_npy, max_clip, min_clip, ratio, **kwargs)\n\t        self.block_size = 2\n\t        self.black_level = black_level\n\t        self.white_level = white_level\n\t    def _pack_raw(self, raw):\n\t        # pack Bayer image to 4 channels (RGBG)\n\t        im = raw.raw_image_visible.astype(np.uint16)\n\t        H, W = im.shape\n\t        im = np.expand_dims(im, axis=0)\n\t        out = np.concatenate((im[:, 0:H:2, 0:W:2],\n", "                              im[:, 0:H:2, 1:W:2],\n\t                              im[:, 1:H:2, 1:W:2],\n\t                              im[:, 1:H:2, 0:W:2]), axis=0)\n\t        return out\n"]}
{"filename": "datasets/SID_dict_dataset.py", "chunked_list": ["#!/usr/bin/env python\n\timport os\n\timport numpy as np\n\timport rawpy\n\timport torch\n\tfrom torch.utils import data\n\timport time\n\tfrom utils.registry import DATASET_REGISTRY\n\tfrom timm.utils import AverageMeter\n\tclass BaseDictSet(data.Dataset):\n", "    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, repeat=1,\n\t                 raw_ext='ARW', max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n\t                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n\t        \"\"\"\n\t        :param data_path: dataset directory\n\t        :param image_list_file: contains image file names under data_path\n\t        :param patch_size: if None, full images are returned, otherwise patches are returned\n\t        :param split: train or valid\n\t        :param upper: max number of image used for debug\n\t        \"\"\"\n", "        assert os.path.exists(data_path), \"data_path: {} not found.\".format(data_path)\n\t        self.data_path = data_path\n\t        image_list_file = os.path.join(data_path, image_list_file)\n\t        assert os.path.exists(image_list_file), \"image_list_file: {} not found.\".format(image_list_file)\n\t        self.image_list_file = image_list_file\n\t        self.patch_size = patch_size\n\t        self.split = split\n\t        self.load_npy = load_npy\n\t        self.raw_ext = raw_ext\n\t        self.max_clip = max_clip\n", "        self.min_clip = min_clip\n\t        self.transpose = transpose\n\t        self.h_flip = h_flip\n\t        self.v_flip = v_flip\n\t        self.rotation = rotation\n\t        self.ratio = ratio\n\t        self.only_00 = only_00\n\t        self.repeat = repeat\n\t        self.raw_short_read_time = AverageMeter()\n\t        self.raw_short_pack_time = AverageMeter()\n", "        self.raw_short_post_time = AverageMeter()\n\t        self.raw_long_read_time = AverageMeter()\n\t        self.raw_long_pack_time = AverageMeter()\n\t        self.raw_long_post_time = AverageMeter()\n\t        self.npy_long_read_time = AverageMeter()\n\t        self.data_aug_time = AverageMeter()\n\t        self.data_norm_time = AverageMeter()\n\t        self.count = 0\n\t        self.img_info = []\n\t        with open(self.image_list_file, 'r') as f:\n", "            for i, img_pair in enumerate(f):\n\t                img_pair = img_pair.strip()  # ./Sony/short/10003_00_0.04s.ARW ./Sony/long/10003_00_10s.ARW ISO200 F9\n\t                img_file, lbl_file, iso, focus = img_pair.split(' ')\n\t                if self.split == 'test' and self.only_00:\n\t                    if os.path.split(img_file)[-1][5:8] != '_00':\n\t                        continue\n\t                img_exposure = float(os.path.split(img_file)[-1][9:-5]) # 0.04\n\t                lbl_exposure = float(os.path.split(lbl_file)[-1][9:-5]) # 10\n\t                ratio = min(lbl_exposure/img_exposure, 300)\n\t                self.img_info.append({\n", "                    'img': img_file,\n\t                    'lbl': lbl_file,\n\t                    'img_exposure': img_exposure,\n\t                    'lbl_exposure': lbl_exposure,\n\t                    'ratio': np.float32(ratio),\n\t                    'iso': float(iso[3::]),\n\t                    'focus': focus,\n\t                })\n\t                if max_samples and i == max_samples - 1:  # for debug purpose\n\t                    break\n", "        print(\"processing: {} images for {}\".format(len(self.img_info), self.split))\n\t    def __len__(self):\n\t        return len(self.img_info) * self.repeat\n\t    def print_time(self):\n\t        print('self.raw_short_read_time:', self.raw_short_read_time.avg)\n\t        print('self.raw_short_pack_time:', self.raw_short_pack_time.avg)\n\t        print('self.raw_short_post_time:', self.raw_short_post_time.avg)\n\t        print('self.raw_long_read_time:', self.raw_long_read_time.avg)\n\t        print('self.raw_long_pack_time:', self.raw_long_pack_time.avg)\n\t        print('self.raw_long_post_time:', self.raw_long_post_time.avg)\n", "        print('self.npy_long_read_time:', self.npy_long_read_time.avg)\n\t        print('self.data_aug_time:', self.data_aug_time.avg)\n\t        print('self.data_norm_time:', self.data_norm_time.avg)\n\t    def __getitem__(self, index):\n\t        self.count += 1\n\t        if self.count % 100 == 0 and False:\n\t            self.print_time()\n\t        info = self.img_info[index // self.repeat]\n\t        img_file = info['img']\n\t        if not self.load_npy:\n", "            start = time.time()\n\t            raw = rawpy.imread(os.path.join(self.data_path, img_file))\n\t            self.raw_short_read_time.update(time.time() - start)\n\t            start = time.time()\n\t            if self.patch_size is None:\n\t                # pack raw with patch size is implemented in clip patch for reducing computation\n\t                noisy_raw = self._pack_raw(raw)\n\t            self.raw_short_pack_time.update(time.time() - start)\n\t        else:\n\t            start = time.time()\n", "            noisy_raw = np.load(os.path.join(self.data_path, img_file.replace('short', 'short_pack')+'.npy'), allow_pickle=True)\n\t            self.raw_short_read_time.update(time.time() - start)\n\t        lbl_file = info['lbl']\n\t        if self.load_npy:\n\t            start = time.time()\n\t            clean_rgb = np.load(os.path.join(self.data_path, lbl_file.replace('long', 'long_post_int')+'.npy'), allow_pickle=True)\n\t            self.npy_long_read_time.update(time.time() - start)\n\t        else:\n\t            start = time.time()\n\t            lbl_raw = rawpy.imread(os.path.join(self.data_path, lbl_file))\n", "            clean_rgb = lbl_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n\t            clean_rgb = clean_rgb.transpose(2, 0, 1)\n\t            # clean_rgb = clean_rgb / 65535\n\t            self.raw_long_post_time.update(time.time() - start)\n\t        start = time.time()\n\t        if not self.load_npy:\n\t            lbl_raw = rawpy.imread(os.path.join(self.data_path, lbl_file))\n\t            if self.patch_size is None:\n\t                clean_raw = self._pack_raw(lbl_raw)\n\t        else:\n", "            clean_raw = np.load(os.path.join(self.data_path, lbl_file.replace('long', 'long_pack')+'.npy'), allow_pickle=True)\n\t        self.raw_long_read_time.update(time.time() - start)\n\t        if self.patch_size:\n\t            start = time.time()\n\t            patch_size = self.patch_size\n\t            # crop\n\t            H, W = clean_rgb.shape[1:3]\n\t            if self.split == 'train':\n\t                yy, xx = torch.randint(0, (H - patch_size) // self.block_size, (1,)),  torch.randint(0, (W - patch_size) // self.block_size, (1,))\n\t            else:\n", "                yy, xx = (H - patch_size) // self.block_size // 2, (W - patch_size) // self.block_size // 2\n\t            if not self.load_npy:\n\t                input_patch = self._pack_raw(raw, yy, xx)\n\t                clean_raw_patch = self._pack_raw(lbl_raw, yy, xx)\n\t            else:\n\t                input_patch = noisy_raw[:, yy:yy+patch_size//self.block_size, xx:xx+patch_size//self.block_size]\n\t                clean_raw_patch = clean_raw[:, yy:yy+patch_size//self.block_size, xx:xx+patch_size//self.block_size]\n\t            gt_patch = clean_rgb[:, yy*self.block_size:yy*self.block_size + patch_size, xx*self.block_size:xx*self.block_size + patch_size]\n\t            if self.h_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random horizontal flip\n\t                input_patch = np.flip(input_patch, axis=2)\n", "                gt_patch = np.flip(gt_patch, axis=2)\n\t                clean_raw_patch = np.flip(clean_raw_patch, axis=2)\n\t            if self.v_flip and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random vertical flip\n\t                input_patch = np.flip(input_patch, axis=1)\n\t                gt_patch = np.flip(gt_patch, axis=1)\n\t                clean_raw_patch = np.flip(clean_raw_patch, axis=1)\n\t            if self.transpose and torch.randint(0, 2, (1,)) == 1 and self.split == 'train':  # random transpose\n\t                input_patch = np.transpose(input_patch, (0, 2, 1))\n\t                gt_patch = np.transpose(gt_patch, (0, 2, 1))\n\t                clean_raw_patch = np.transpose(clean_raw_patch, (0, 2, 1))\n", "            if self.rotation and self.split == 'train':\n\t                raise NotImplementedError('rotation')\n\t            noisy_raw = input_patch.copy()\n\t            clean_rgb = gt_patch.copy()\n\t            clean_raw = clean_raw_patch.copy()\n\t            self.data_aug_time.update(time.time() - start)\n\t        start = time.time()\n\t        noisy_raw = (np.float32(noisy_raw) - self.black_level) / np.float32(self.white_level - self.black_level)  # subtract the black level\n\t        clean_raw = (np.float32(clean_raw) - self.black_level) / np.float32(self.white_level - self.black_level)\n\t        clean_rgb = np.float32(clean_rgb) / np.float32(65535)\n", "        self.data_norm_time.update(time.time() - start)\n\t        if self.ratio:\n\t            noisy_raw = noisy_raw * info['ratio']\n\t        if self.max_clip is not None:\n\t            noisy_raw = np.minimum(noisy_raw, self.max_clip)\n\t        if self.min_clip is not None:\n\t            noisy_raw = np.maximum(noisy_raw, self.min_clip)\n\t        clean_rgb = clean_rgb.clip(0.0, 1.0)\n\t        noisy_raw = torch.from_numpy(noisy_raw).float()\n\t        clean_rgb = torch.from_numpy(clean_rgb).float()\n", "        clean_raw = torch.from_numpy(clean_raw).float()\n\t        return {\n\t            'noisy_raw': noisy_raw,\n\t            'clean_raw': clean_raw,\n\t            'clean_rgb': clean_rgb,\n\t            'img_file': img_file,\n\t            'lbl_file': lbl_file,\n\t            'img_exposure': info['img_exposure'],\n\t            'lbl_exposure': info['lbl_exposure'],\n\t            'ratio': info['ratio']\n", "        }\n\t@DATASET_REGISTRY.register()\n\tclass SonyDictSet(BaseDictSet):\n\t    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, raw_ext='ARW', repeat=1,\n\t                 max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n\t                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n\t        super(SonyDictSet, self).__init__(data_path, image_list_file, split=split, patch_size=patch_size,\n\t                                   load_npy=load_npy, raw_ext=raw_ext, max_samples=max_samples, max_clip=max_clip, repeat=repeat,\n\t                                   min_clip=min_clip, only_00=only_00, transpose=transpose, h_flip=h_flip, v_flip=v_flip, rotation=rotation, ratio=ratio)\n\t        self.block_size = 2\n", "        self.black_level = 512\n\t        self.white_level = 16383\n\t    def _pack_raw(self, raw, hh=None, ww=None):\n\t        if self.patch_size is None:\n\t            assert hh is None and ww is None\n\t        # pack Bayer image to 4 channels (RGBG)\n\t        im = raw.raw_image_visible.astype(np.uint16)\n\t        H, W = im.shape\n\t        im = np.expand_dims(im, axis=0)\n\t        if self.patch_size is None:\n", "            out = np.concatenate((im[:, 0:H:2, 0:W:2],\n\t                                  im[:, 0:H:2, 1:W:2],\n\t                                  im[:, 1:H:2, 1:W:2],\n\t                                  im[:, 1:H:2, 0:W:2]), axis=0)\n\t        else:\n\t            h1 = hh * 2\n\t            h2 = hh * 2 + self.patch_size\n\t            w1 = ww * 2\n\t            w2 = ww * 2 + self.patch_size\n\t            out = np.concatenate((im[:, h1:h2:2, w1:w2:2],\n", "                                  im[:, h1:h2:2, w1+1:w2:2],\n\t                                  im[:, h1+1:h2:2, w1+1:w2:2],\n\t                                  im[:, h1+1:h2:2, w1:w2:2]), axis=0)\n\t        return out\n\t@DATASET_REGISTRY.register()\n\tclass FujiDictSet(BaseDictSet):\n\t    def __init__(self, data_path, image_list_file, patch_size=None, split='train', load_npy=True, raw_ext='RAF', repeat=1,\n\t                 max_samples=None, max_clip=1.0, min_clip=None, only_00=False,\n\t                 transpose=True, h_flip=True, v_flip=True, rotation=False, ratio=True, **kwargs):\n\t        super(FujiDictSet, self).__init__(data_path, image_list_file, split=split, patch_size=patch_size,\n", "                                   load_npy=load_npy, raw_ext=raw_ext, max_samples=max_samples, max_clip=max_clip, repeat=repeat,\n\t                                   min_clip=min_clip, only_00=only_00, transpose=transpose, h_flip=h_flip, v_flip=v_flip, rotation=rotation, ratio=ratio)\n\t        self.block_size = 3\n\t        self.black_level = 1024\n\t        self.white_level = 16383\n\t    def _pack_raw(self, raw, hh=None, ww=None):\n\t        if self.patch_size is None:\n\t            assert hh is None and ww is None\n\t        # pack XTrans image to 9 channels ()\n\t        im = raw.raw_image_visible.astype(np.uint16)\n", "        H, W = im.shape\n\t        if self.patch_size is None:\n\t            h1 = 0\n\t            h2 = H // 6 * 6\n\t            w1 = 0\n\t            w2 = W // 6 * 6\n\t            out = np.zeros((9, h2 // 3, w2 // 3), dtype=np.uint16)\n\t        else:\n\t            h1 = hh * 3\n\t            h2 = hh * 3 + self.patch_size\n", "            w1 = ww * 3\n\t            w2 = ww * 3 + self.patch_size\n\t            out = np.zeros((9, self.patch_size // 3, self.patch_size // 3), dtype=np.uint16)\n\t        # 0 R\n\t        out[0, 0::2, 0::2] = im[h1:h2:6, w1:w2:6]\n\t        out[0, 0::2, 1::2] = im[h1:h2:6, w1+4:w2:6]\n\t        out[0, 1::2, 0::2] = im[h1+3:h2:6, w1+1:w2:6]\n\t        out[0, 1::2, 1::2] = im[h1+3:h2:6, w1+3:w2:6]\n\t        # 1 G\n\t        out[1, 0::2, 0::2] = im[h1:h2:6, w1+2:w2:6]\n", "        out[1, 0::2, 1::2] = im[h1:h2:6, w1+5:w2:6]\n\t        out[1, 1::2, 0::2] = im[h1+3:h2:6, w1+2:w2:6]\n\t        out[1, 1::2, 1::2] = im[h1+3:h2:6, w1+5:w2:6]\n\t        # 1 B\n\t        out[2, 0::2, 0::2] = im[h1:h2:6, w1+1:w2:6]\n\t        out[2, 0::2, 1::2] = im[h1:h2:6, w1+3:w2:6]\n\t        out[2, 1::2, 0::2] = im[h1+3:h2:6, w1:w2:6]\n\t        out[2, 1::2, 1::2] = im[h1+3:h2:6, w1+4:w2:6]\n\t        # 4 R\n\t        out[3, 0::2, 0::2] = im[h1+1:h2:6, w1+2:w2:6]\n", "        out[3, 0::2, 1::2] = im[h1+2:h2:6, w1+5:w2:6]\n\t        out[3, 1::2, 0::2] = im[h1+5:h2:6, w1+2:w2:6]\n\t        out[3, 1::2, 1::2] = im[h1+4:h2:6, w1+5:w2:6]\n\t        # 5 B\n\t        out[4, 0::2, 0::2] = im[h1+2:h2:6, w1+2:w2:6]\n\t        out[4, 0::2, 1::2] = im[h1+1:h2:6, w1+5:w2:6]\n\t        out[4, 1::2, 0::2] = im[h1+4:h2:6, w1+2:w2:6]\n\t        out[4, 1::2, 1::2] = im[h1+5:h2:6, w1+5:w2:6]\n\t        out[5, :, :] = im[h1+1:h2:3, w1:w2:3]\n\t        out[6, :, :] = im[h1+1:h2:3, w1+1:w2:3]\n", "        out[7, :, :] = im[h1+2:h2:3, w1:w2:3]\n\t        out[8, :, :] = im[h1+2:h2:3, w1+1:w2:3]\n\t        return out\n"]}
{"filename": "datasets/__init__.py", "chunked_list": ["import importlib\n\tfrom copy import deepcopy\n\tfrom os import path as osp\n\tfrom glob import glob\n\timport torch\n\tfrom utils.registry import DATASET_REGISTRY\n\t__all__ = ['build_train_loader', 'build_valid_loader', 'build_test_loader']\n\t# automatically scan and import dataset modules for registry\n\t# scan all the files under the 'datasets' folder and collect files ending with '_dataset.py'\n\tdataset_folder = osp.dirname(osp.abspath(__file__))\n", "dataset_filenames = [osp.splitext(osp.basename(v))[0] for v in glob(osp.join(dataset_folder, '*_dataset.py'))]\n\t# import all the dataset modules\n\t_dataset_modules = [importlib.import_module(f'datasets.{file_name}') for file_name in dataset_filenames]\n\tdef build_dataset(dataset_cfg, split: str):\n\t    assert split.upper() in ['TRAIN', 'VALID', 'TEST']\n\t    dataset_cfg = deepcopy(dataset_cfg)\n\t    dataset_type = dataset_cfg.pop('type')\n\t    process_cfg = dataset_cfg.pop('process')\n\t    split_cfg = dataset_cfg.pop(split)\n\t    dataset = DATASET_REGISTRY.get(dataset_type)(\n", "        **dataset_cfg,\n\t        **process_cfg,\n\t        **split_cfg,\n\t        split=split\n\t    )\n\t    return dataset\n\tdef build_train_loader(dataset_cfg):\n\t    train_dataset = build_dataset(dataset_cfg, 'train')\n\t    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=dataset_cfg['train']['batch_size'],\n\t                                                   shuffle=True, num_workers=dataset_cfg['num_workers'],\n", "                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n\t    return train_dataloader\n\tdef build_valid_loader(dataset_cfg, num_workers=None):\n\t    valid_dataset = build_dataset(dataset_cfg, 'valid')\n\t    if num_workers is None:\n\t        num_workers = dataset_cfg['num_workers']\n\t    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=dataset_cfg['valid']['batch_size'],\n\t                                                   shuffle=False, num_workers=num_workers,\n\t                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n\t    return valid_dataloader\n", "def build_test_loader(dataset_cfg, num_workers=None):\n\t    test_dataset = build_dataset(dataset_cfg, 'test')\n\t    if num_workers is None:\n\t        num_workers = dataset_cfg['num_workers']\n\t    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=dataset_cfg['test']['batch_size'],\n\t                                                   shuffle=False, num_workers=num_workers,\n\t                                                   pin_memory=dataset_cfg['pin_memory'], persistent_workers=dataset_cfg['persistent_workers'])\n\t    return test_dataloader"]}
