{"filename": "subchain/fabric_api.py", "chunked_list": ["import subprocess\n\timport json\n\timport sys\n\timport time\n\tsys.path.append('../')\n\tfrom common.ipfs import ipfsGetFile\n\t# def queryLocal(lock, taskID, deviceID, currentEpoch, flagSet, localFileName):\n\t#     \"\"\"\n\t#     Query and download the paras file of local model trained by the device.\n\t#     \"\"\"\n", "#     localQuery = subprocess.Popen(args=['../commonComponent/interRun.sh query '+deviceID], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t#     outs, errs = localQuery.communicate(timeout=15)\n\t#     if localQuery.poll() == 0:\n\t#         localDetail = json.loads(outs.strip())\n\t#         if localDetail['epoch'] == currentEpoch and localDetail['taskID'] == taskID:\n\t#             print(\"The query result of the \" + deviceID + \" is \", outs.strip())\n\t#             while 1:\n\t#                 # localFileName = './clientS/paras/' + taskID + '-' + deviceID + '-epoch-' + str(currentEpoch) + '.pkl'\n\t#                 outs, stt = ipfsGetFile(localDetail['paras'], localFileName)\n\t#                 if stt == 0:\n", "#                     break\n\t#                 # else:\n\t#                 #     print(outs.strip())\n\t#             lock.acquire()\n\t#             t1 = flagSet\n\t#             t1.add(deviceID)\n\t#             flagSet = t1\n\t#             lock.release()\n\t        # else:\n\t        #     print('*** This device %s has not updated its model! ***'%(deviceID))\n", "    # else:\n\t    #     print(\"Failed to query this device!\", errs)\n\t# def simpleQuery(key):\n\t#     \"\"\"\n\t#     Use the only key to query info from fabric network.\n\t#     \"\"\"\n\t#     infoQuery = subprocess.Popen(args=[\"./hyperledger_invoke.sh query \" + key], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t#     outs, errs = infoQuery.communicate(timeout=15)\n\t#     if infoQuery.poll() == 0:\n\t#         return outs.strip(), infoQuery.poll()\n", "#     else:\n\t#         print(\"*** Failed to query the info of \" + str(key) + \"! ***\" + errs.strip())\n\t#         return errs.strip(), infoQuery.poll()\n\tdef query_release():\n\t    \"\"\"\n\t    Use the only key to query info from fabric network.\n\t    \"\"\"\n\t    infoQuery = subprocess.Popen(args=[\"./hyperledger_invoke.sh query_release\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t    outs, errs = infoQuery.communicate(timeout=15)\n\t    if infoQuery.poll() == 0:\n", "        return outs.strip(), infoQuery.poll()\n\t    else:\n\t        print(\"*** Failed to query the info of query_release! ***\" + errs.strip())\n\t        time.sleep(2)\n\t        return errs.strip(), infoQuery.poll()\n\tdef query_task(taskID):\n\t    \"\"\"\n\t    Use the only key to query info from fabric network.\n\t    \"\"\"\n\t    infoQuery = subprocess.Popen(args=[f\"./hyperledger_invoke.sh query_task {taskID}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n", "    outs, errs = infoQuery.communicate(timeout=15)\n\t    if infoQuery.poll() == 0:\n\t        return outs.strip(), infoQuery.poll()\n\t    else:\n\t        print(\"*** Failed to query the info of \" + taskID + \"! ***\" + errs.strip())\n\t        time.sleep(2)\n\t        return errs.strip(), infoQuery.poll()\n\tdef query_local(lock, taskID, deviceID, currentEpoch, flagSet, localFileName):\n\t    \"\"\"\n\t    Query and download the paras file of local model trained by the device.\n", "    \"\"\"\n\t    localQuery = subprocess.Popen(args=[f\"./hyperledger_invoke.sh query_local {deviceID} {taskID}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t    outs, errs = localQuery.communicate(timeout=15)\n\t    if localQuery.poll() == 0:\n\t        localDetail = json.loads(outs.strip())\n\t        if localDetail['CurrentEpoch'] == currentEpoch and localDetail['ClientID'] == f\"{taskID}_{deviceID}\":\n\t            print(f\"The query result of the {deviceID} is {outs.strip()}\")\n\t            while 1:\n\t                # localFileName = './clientS/paras/' + taskID + '-' + deviceID + '-epoch-' + str(currentEpoch) + '.pkl'\n\t                outs, stt = ipfsGetFile(localDetail['LocalParamHash'], localFileName)\n", "                if stt == 0:\n\t                    break\n\t                # else:\n\t                #     print(outs.strip())\n\t            lock.acquire()\n\t            t1 = flagSet\n\t            t1.add(deviceID)\n\t            flagSet = t1\n\t            lock.release()\n\t        else:\n", "            print(f\"*** This device {deviceID} has not updated its model! ***\")\n\t    else:\n\t        print(\"Failed to query this device!\", errs)"]}
{"filename": "subchain/federated_local.py", "chunked_list": ["import os\n\timport shutil\n\timport sys\n\timport json\n\timport time\n\timport torch\n\timport copy\n\timport subprocess\n\tsys.path.append('./ml')\n\tsys.path.append('../')\n", "from common.ipfs import ipfsAddFile\n\tfrom common.ipfs import ipfsGetFile\n\tfrom ml.utils.settings import BaseSettings\n\tfrom ml.model_build import model_build\n\tfrom ml.model_build import model_evaluate\n\tfrom ml.models.FedAvg import FedAvg\n\tfrom ml.models.train_model import LocalUpdate\n\timport fabric_api\n\tif __name__ == '__main__':\n\t    if os.path.exists('./cache/local'):\n", "        shutil.rmtree('./cache/local')\n\t    os.mkdir('./cache/local')\n\t    if os.path.exists('./cache/local/agg'):\n\t        shutil.rmtree('./cache/local/agg')\n\t    os.mkdir('./cache/local/agg')\n\t    if os.path.exists('./cache/local/paras'):\n\t        shutil.rmtree('./cache/local/paras')\n\t    os.mkdir('./cache/local/paras')\n\t    # build network\n\t    net_glob, settings, dataset_train, dataset_test, dict_users = model_build(BaseSettings())\n", "    net_glob.train()\n\t    # with open('../commonComponent/dict_users.pkl', 'rb') as f:\n\t    #     dict_users = pickle.load(f)\n\t    checkTaskID = ''\n\t    iteration = 0\n\t    while 1:\n\t        taskRelInfo = {}\n\t        # taskRelease info template {\"taskID\":\"task1994\",\"epoch\":10,\"status\":\"start\",\"usersFrac\":0.1}\n\t        while 1:\n\t            taskRelQue, taskRelQueStt = fabric_api.query_release()\n", "            if taskRelQueStt == 0:\n\t                taskRelInfo = json.loads(taskRelQue)\n\t                print('\\n*************************************************************************************')\n\t                print('Latest task release status is %s!'%taskRelQue.strip())\n\t                break\n\t        taskID = taskRelInfo['TaskID']\n\t        totalEpochs = taskRelInfo['Epochs']\n\t        print(f\"Current task is {taskID}\")\n\t        taskInfo = {}\n\t        while 1:\n", "            taskInQue, taskInQueStt = fabric_api.query_task(taskID)\n\t            if taskInQueStt == 0:\n\t                taskInfo = json.loads(taskInQue)\n\t                print('Latest task info is %s!'%taskInQue.strip())\n\t                print('*************************************************************************************\\n')\n\t                break\n\t        if taskInfo['TaskStatus'] == 'done' or checkTaskID == taskID:\n\t            print('*** %s has been completed! ***\\n'%taskID)\n\t            time.sleep(5)\n\t        else:\n", "            print('\\n******************************* Iteration #%d starting ********************************'%iteration+'\\n')\n\t            print('Iteration %d starting!'%iteration)\n\t            print('\\n*************************************************************************************\\n')\n\t            currentEpoch = int(taskInfo['TaskEpochs']) + 1\n\t            loss_train = []\n\t            while currentEpoch <= totalEpochs:\n\t                while 1:\n\t                    taskInQueEpoch, taskInQueEpochStt = fabric_api.query_task(taskID)\n\t                    if taskInQueEpochStt == 0:\n\t                        taskInfoEpoch = json.loads(taskInQueEpoch)\n", "                        if int(taskInfoEpoch['TaskEpochs']) == (currentEpoch-1):\n\t                            print('\\n****************************** Latest status of %s ******************************'%taskID)\n\t                            print('(In loop) Latest task info is \\n %s!'%taskInQueEpoch)\n\t                            print('*************************************************************************************\\n')\n\t                            break\n\t                        else:\n\t                            print('\\n*************************** %s has not been updated ***************************'%taskID)\n\t                            print('(In loop) Latest task info is \\n %s!'%taskInQueEpoch)\n\t                            print('*************************************************************************************\\n')\n\t                            time.sleep(10)\n", "                # download aggregated model in current epoch from ipfs\n\t                aggBaseModelFile = f\"./cache/local/agg/aggModel-iter-{str(iteration)}-epoch-{str(currentEpoch-1)}.pkl\"\n\t                while 1:\n\t                    aggBasMod, aggBasModStt = ipfsGetFile(taskInfoEpoch['ParamHash'], aggBaseModelFile)\n\t                    if aggBasModStt == 0:\n\t                        print('\\nThe paras file of aggregated model for epoch %d training has been downloaded!\\n'%(int(taskInfoEpoch['TaskEpochs'])+1))\n\t                        break\n\t                    else:\n\t                        print('\\nFailed to download the paras file of aggregated model for epoch %d training!\\n'%(int(taskInfoEpoch['TaskEpochs'])+1))\n\t                w_glob = net_glob.state_dict()\n", "                net_glob.load_state_dict(torch.load(aggBaseModelFile))\n\t                selectedDevices = [0, 1, 2, 3, 4]\n\t                loss_locals = []\n\t                for idx_user in selectedDevices:\n\t                    local = LocalUpdate(settings, dataset_train, dict_users[idx_user])\n\t                    w_local, loss_local = local.train(net=copy.deepcopy(net_glob).to(settings.device), user=idx_user)\n\t                    loss_locals.append(copy.deepcopy(loss_local))\n\t                    localParamFile = f\"./cache/local/paras/{taskID}-{selectedDevices[idx_user]}-epoch-{str(currentEpoch)}.pkl\"\n\t                    torch.save(w_local, localParamFile)\n\t                    while 1:\n", "                        localParamHash, localAddStt = ipfsAddFile(localParamFile)\n\t                        if localAddStt == 0:\n\t                            print('%s has been added to the IPFS network!'%localParamFile)\n\t                            print('And the hash value of this file is %s'%localParamHash)\n\t                            break\n\t                        else:\n\t                            print('Failed to add %s to the IPFS network!'%localParamFile)\n\t                    while 1:\n\t                        localRelease = subprocess.Popen(args=[f\"./hyperledger_invoke.sh local {selectedDevices[idx_user]} {taskID} {localParamHash} {str(currentEpoch)}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t                        localOuts, localErrs = localRelease.communicate(timeout=10)\n", "                        if localRelease.poll() == 0:\n\t                            print('*** Local model train in epoch ' + str(currentEpoch) + ' of ' + str(selectedDevices[idx_user]) + ' has been uploaded! ***\\n')\n\t                            break\n\t                        else:\n\t                            print(localErrs.strip())\n\t                            print('*** Failed to release Local model train in epoch ' + str(currentEpoch) + ' of ' + str(selectedDevices[idx_user]) + '! ***\\n')\n\t                            time.sleep(2)\n\t                loss_avg = sum(loss_locals) / len(loss_locals)\n\t                print('Epoch {:3d}, Average loss {:.3f}'.format(currentEpoch, loss_avg))\n\t                loss_train.append(loss_avg)\n", "                currentEpoch += 1\n\t            checkTaskID = taskID\n\t            iteration += 1\n"]}
{"filename": "subchain/client.py", "chunked_list": ["import socket\n\timport sys\n\timport pathlib\n\timport struct\n\timport json\n\tBUFFER_SIZE = 1024\n\tdef require_tx_from_server(ip_addr, tx_name):\n\t    try:\n\t        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t        s.connect((ip_addr, 65432))\n", "    except socket.error as msg:\n\t        print(msg)\n\t        sys.exit(1)\n\t    print(s.recv(1024).decode())\n\t    data = 'requireTx'.encode()\n\t    s.send(data) # initiate request\n\t    response = s.recv(1024) # server agrees\n\t    print(\"Got response from server\", response)\n\t    data = tx_name.encode()\n\t    s.send(data) # send tx name\n", "    rev_file(s, pathlib.Path('./cache/client/txs') / f\"{tx_name}.json\")\n\t    s.close()\n\tdef require_tips_from_server(ip_addr):\n\t    try:\n\t        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t        s.connect((ip_addr, 65432))\n\t    except socket.error as msg:\n\t        print(msg)\n\t        sys.exit(1)\n\t    print(s.recv(1024).decode())\n", "    data = 'requireTips'.encode()\n\t    s.send(data) # initiate request\n\t    rev_file(s, pathlib.Path('./cache/client/pools/tip_pool.json'))\n\t    s.close()\n\tdef upload_tx_to_server(ip_addr, tx_info):\n\t    try:\n\t        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t        s.connect((ip_addr, 65432))\n\t    except socket.error as msg:\n\t        print(msg)\n", "        sys.exit(1)\n\t    print(s.recv(1024).decode())\n\t    data = 'uploadTx'.encode()\n\t    s.send(data)\n\t    response = s.recv(BUFFER_SIZE)\n\t    data = json.dumps(tx_info).encode()\n\t    s.send(data)\n\t    s.close()\n\tdef rev_file(conn, tx_file_path):\n\t    header_size = struct.calcsize('64si')\n", "    header = conn.recv(header_size)\n\t    _, tx_size = struct.unpack('64si', header)\n\t    print(f\"Size of the block is {tx_size}\")\n\t    conn.send('header_resp'.encode())\n\t    with open(tx_file_path, 'wb') as f:\n\t        bytes_received = 0\n\t        while not bytes_received >= tx_size:\n\t            buff = conn.recv(BUFFER_SIZE)\n\t            bytes_received += len(buff)\n\t            f.write(buff)\n"]}
{"filename": "subchain/__init__.py", "chunked_list": []}
{"filename": "subchain/shard_run.py", "chunked_list": ["import os\n\timport shutil\n\timport sys\n\timport pathlib\n\timport torch\n\timport time\n\timport uuid\n\timport json\n\timport random\n\timport copy\n", "import subprocess\n\timport threading\n\timport client\n\timport fabric_api\n\tsys.path.append('./ml')\n\tsys.path.append('../')\n\t# sys.path.append('../../commonComponent')\n\tfrom ml.utils.settings import BaseSettings\n\tfrom ml.model_build import model_build\n\tfrom ml.model_build import model_evaluate\n", "from ml.models.FedAvg import FedAvg\n\tfrom common.ipfs import ipfsAddFile\n\tfrom common.ipfs import ipfsGetFile\n\tCACHE_DIR = \"./cache/\"\n\tCLIENT_DATA_DIR = pathlib.Path(CACHE_DIR) / \"client\"\n\tTX_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"txs\"\n\tTIPS_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"pools\"\n\tPARAMS_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"params\"\n\tLOCAL_DATA_DIR = pathlib.Path(CLIENT_DATA_DIR) / \"local\"\n\tdef main():\n", "    if os.path.exists(CACHE_DIR) == False:\n\t        os.mkdir(CACHE_DIR)\n\t    if os.path.exists(CLIENT_DATA_DIR):\n\t        shutil.rmtree(CLIENT_DATA_DIR)\n\t    os.mkdir(CLIENT_DATA_DIR)\n\t    if os.path.exists(TX_DATA_DIR):\n\t        shutil.rmtree(TX_DATA_DIR)\n\t    os.mkdir(TX_DATA_DIR)\n\t    if os.path.exists(TIPS_DATA_DIR):\n\t        shutil.rmtree(TIPS_DATA_DIR)\n", "    os.mkdir(TIPS_DATA_DIR)\n\t    if os.path.exists(PARAMS_DATA_DIR):\n\t        shutil.rmtree(PARAMS_DATA_DIR)\n\t    os.mkdir(PARAMS_DATA_DIR)\n\t    if os.path.exists(LOCAL_DATA_DIR):\n\t        shutil.rmtree(LOCAL_DATA_DIR)\n\t    os.mkdir(LOCAL_DATA_DIR)\n\t    ## setup\n\t    alpha = 3\n\t    # client.require_tx_from_server(\"localhost\", \"genesis\")\n", "    # client.require_tips_from_server(\"localhost\")\n\t    net, settings, _, test_dataset, data_user_mapping = model_build(BaseSettings())\n\t    net_weight = net.state_dict()\n\t    net_accuracy, _ = model_evaluate(net, net_weight, test_dataset, settings)\n\t    genesisFile = './cache/client/genesis.pkl'\n\t    torch.save(net_weight, genesisFile)\n\t    while 1:\n\t        genesisHash, statusCode = ipfsAddFile(genesisFile)\n\t        if statusCode == 0:\n\t            print('\\nThe base mode parasfile ' + genesisFile + ' has been uploaded!')\n", "            print('And the fileHash is ' + genesisHash + '\\n')\n\t            break\n\t        else:\n\t            print('Error: ' + genesisHash)\n\t            print('\\nFailed to upload the aggregated parasfile ' + genesisFile + ' !\\n')\n\t    genesisTxInfo = {\"approved_tips\": [], \"model_accuracy\": float(net_accuracy), \"param_hash\": genesisHash, \"shard_id\": 0, \"timestamp\": time.time()}\n\t    client.upload_tx_to_server(\"localhost\", genesisTxInfo)\n\t    time.sleep(1)\n\t    iteration = 0\n\t    while 1:\n", "        print(f\"********************* Iteration {iteration} ***************************\")\n\t        taskID = str(uuid.uuid4())[:8]\n\t        apv_tx_cands = []\n\t        client.require_tips_from_server(\"localhost\") \n\t        # implement promise later\n\t        time.sleep(2)\n\t        with open(\"./cache/client/pools/tip_pool.json\", 'r') as f:\n\t            tips_dict = json.load(f)\n\t        if len(tips_dict) <= alpha:\n\t            apv_tx_cands = list(tips_dict.keys())\n", "        else:\n\t            apv_tx_cands = random.sample(tips_dict.keys(), alpha)\n\t        print(f\"The candidates tips are {apv_tx_cands}\")\n\t        apv_tx_cands_dict = {}\n\t        for apv_tx in apv_tx_cands:\n\t            apv_tx_file = f\"./cache/client/txs/{apv_tx}.json\"\n\t            client.require_tx_from_server(\"localhost\", apv_tx)\n\t            with open(apv_tx_file) as f:\n\t                tx_info = json.load(f)\n\t            print(tx_info)\n", "            apv_tx_file = f\"./cache/client/params/iter-{iteration}-{apv_tx}.pkl\"\n\t            while 1:\n\t                status, code = ipfsGetFile(tx_info['param_hash'], apv_tx_file)\n\t                print('The filehash of this approved trans is ' + tx_info['param_hash'] + ', and the file is ' + apv_tx_file + '!')\n\t                if code == 0:\n\t                    print(status.strip())\n\t                    print('The apv parasfile ' + apv_tx_file + ' has been downloaded!\\n')\n\t                    break\n\t                else:\n\t                    print(status)\n", "                    print('\\nFailed to download the apv parasfile ' + apv_tx_file + ' !\\n')\n\t            apv_tx_cands_dict[apv_tx] = float(tx_info['model_accuracy'])\n\t        apv_trans_final = []\n\t        if len(apv_tx_cands_dict) == alpha:\n\t            sort_dict = sorted(apv_tx_cands_dict.items(),key=lambda x:x[1],reverse=True)\n\t            for i in range(alpha - 1):\n\t                apv_trans_final.append(sort_dict[i][0])\n\t        else:\n\t            apv_trans_final = apv_tx_cands\n\t        print(f\"***************************************************\")\n", "        print(f\"The candidates tips are {apv_tx_cands}\")\n\t        print(f\"***************************************************\")\n\t        # aggregating approver parameters\n\t        w_apv_agg = []\n\t        for apv_tx in apv_trans_final:\n\t            apv_param_file = f\"./cache/client/params/iter-{iteration}-{apv_tx}.pkl\"\n\t            net.load_state_dict(torch.load(apv_param_file))\n\t            w_tmp_iter = net.state_dict()\n\t            w_apv_agg.append(copy.deepcopy(w_tmp_iter))\n\t        if len(w_apv_agg) == 1:\n", "            w_glob = w_apv_agg[0]\n\t        else:\n\t            w_glob = FedAvg(w_apv_agg)\n\t        iteration_base_param_file = f\"./cache/client/params/base-iter-{iteration}.pkl\"\n\t        torch.save(w_glob, iteration_base_param_file)\n\t        base_model_acc, base_model_loss = model_evaluate(net, w_glob, test_dataset, settings)\n\t        print(base_model_acc)\n\t        while 1:\n\t            basefileHash, baseSttCode = ipfsAddFile(iteration_base_param_file)\n\t            if baseSttCode == 0:\n", "                print('\\nThe base mode parasfile ' + iteration_base_param_file + ' has been uploaded!')\n\t                print('And the fileHash is ' + basefileHash + '\\n')\n\t                break\n\t            else:\n\t                print('Error: ' + basefileHash)\n\t                print('\\nFailed to uploaded the aggregated parasfile ' + iteration_base_param_file + ' !\\n')\n\t        taskEpochs = settings.epochs\n\t        taskInitStatus = \"start\"\n\t        ## initiate task release\n\t        while 1:\n", "            taskRelease = subprocess.Popen([f\"./hyperledger_invoke.sh release {taskID} {taskEpochs}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t            trOuts, trErrs = taskRelease.communicate(timeout=10)\n\t            if taskRelease.poll() == 0:\n\t                print('*** ' + taskID + ' has been released! ***')\n\t                print('*** And the detail of this task is ' + trOuts.strip() + '! ***\\n')\n\t                break\n\t            else:\n\t                print(trErrs)\n\t                print('*** Failed to release ' + taskID + ' ! ***\\n')\n\t                time.sleep(2)\n", "        ## initiate task with base parameter hash\n\t        while 1:\n\t            spcAggModelPublish = subprocess.Popen(args=[f\"./hyperledger_invoke.sh aggregated {taskID} 0 training {basefileHash}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t            aggPubOuts, aggPubErrs = spcAggModelPublish.communicate(timeout=10)\n\t            if spcAggModelPublish.poll() == 0:\n\t                print('*** The init aggModel of ' + taskID + ' has been published! ***')\n\t                print('*** And the detail of the init aggModel is ' + aggPubOuts.strip() + ' ! ***\\n')\n\t                break\n\t            else:\n\t                print(aggPubErrs)\n", "                print('*** Failed to publish the init aggModel of ' + taskID + ' ! ***\\n')\n\t        # ## wait the local train\n\t        time.sleep(10)\n\t        selectedDevices = [0, 1, 2, 3, 4]\n\t        currentEpoch = 1\n\t        aggModelAcc = 50.0\n\t        while (currentEpoch <= settings.epochs):\n\t            flagList = set(copy.deepcopy(selectedDevices))\n\t            w_locals = []\n\t            while (len(flagList) != 0):\n", "                flagSet = set()\n\t                ts = []\n\t                lock = threading.Lock()\n\t                for deviceID in flagList:\n\t                    localFileName = f\"./cache/client/local/{taskID}-{deviceID}-epoch-{str(currentEpoch)}.pkl\"\n\t                    t = threading.Thread(target=fabric_api.query_local,args=(lock,taskID,deviceID,currentEpoch,flagSet,localFileName,))\n\t                    t.start()\n\t                    ts.append(t)\n\t                for t in ts:\n\t                    t.join()\n", "                time.sleep(2)\n\t                flagList = flagList - flagSet\n\t            for deviceID in selectedDevices:\n\t                localFileName = f\"./cache/client/local/{taskID}-{deviceID}-epoch-{str(currentEpoch)}.pkl\"\n\t                ## check the acc of the models trained by selected device & drop the low quality model\n\t                canddts_dev_pas = torch.load(localFileName,map_location=torch.device('cpu'))\n\t                acc_canddts_dev, loss_canddts_dev = model_evaluate(net, canddts_dev_pas, test_dataset, settings)\n\t                acc_canddts_dev = acc_canddts_dev.cpu().numpy().tolist()\n\t                print(\"Test acc of the model trained by \"+str(deviceID)+\" is \" + str(acc_canddts_dev))\n\t                if (acc_canddts_dev - aggModelAcc) < -10:\n", "                    print(str(deviceID)+\" is a malicious device!\")\n\t                else:\n\t                    w_locals.append(copy.deepcopy(canddts_dev_pas))\n\t            w_glob = FedAvg(w_locals)\n\t            aggEchoParasFile = './cache/client/params/aggModel-iter-'+str(iteration)+'-epoch-'+str(currentEpoch)+'.pkl'\n\t            torch.save(w_glob, aggEchoParasFile)\n\t            # evalute the acc of datatest\n\t            aggModelAcc, aggModelLoss = model_evaluate(net, w_glob, test_dataset, settings)\n\t            aggModelAcc = aggModelAcc.cpu().numpy().tolist()\n\t            print(\"\\n************************************\")\n", "            print(\"Acc of the agg model of Round \"+str(currentEpoch)+\" in iteration \"+str(iteration)+\" is \"+str(aggModelAcc))\n\t            print(\"************************************\")\n\t            while 1:\n\t                aggEchoFileHash, sttCodeAdd = ipfsAddFile(aggEchoParasFile)\n\t                if sttCodeAdd == 0:\n\t                    print('\\n*************************')\n\t                    print('The aggregated parasfile ' + aggEchoParasFile + ' has been uploaded!')\n\t                    print('And the fileHash is ' + aggEchoFileHash + '!')\n\t                    print('*************************\\n')\n\t                    break\n", "                else:\n\t                    print('Error: ' + aggEchoFileHash)\n\t                    print('\\nFailed to uploaded the aggregated parasfile ' + aggEchoParasFile + ' !\\n')\n\t            taskStatus = 'training'\n\t            if currentEpoch == settings.epochs:\n\t                taskStatus = 'done'\n\t            while 1:\n\t                epochAggModelPublish = subprocess.Popen(args=[f\"./hyperledger_invoke.sh aggregated {taskID} {str(currentEpoch)} {taskStatus} {aggEchoFileHash}\"], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t                aggPubOuts, aggPubErrs = epochAggModelPublish.communicate(timeout=10)\n\t                if epochAggModelPublish.poll() == 0:\n", "                    print('\\n******************')\n\t                    print('The info of task ' + taskID + ' is ' + aggPubOuts.strip())\n\t                    print('The model aggregated in epoch ' + str(currentEpoch) + ' for ' + taskID + ' has been published!')\n\t                    print('******************\\n')\n\t                    break\n\t                else:\n\t                    print(aggPubErrs)\n\t                    print('*** Failed to publish the Model aggregated in epoch ' + str(currentEpoch) + ' for ' + taskID + ' ! ***\\n')\n\t            currentEpoch += 1\n\t        new_tx = {\"approved_tips\": apv_trans_final, \"model_accuracy\": aggModelAcc, \"param_hash\": aggEchoFileHash, \"shard_id\": 1, \"timestamp\": time.time()}\n", "        # upload the trans to DAG network\n\t        client.upload_tx_to_server(\"localhost\", new_tx)\n\t        print('\\n******************************* Transaction upload *******************************')\n\t        print('The details of this trans are', new_tx)\n\t        print('The trans generated in the iteration #%d had been uploaded!'%iteration)\n\t        print('*************************************************************************************\\n')\n\t        iteration += 1\n\t        time.sleep(2)\n\t    # new_tx_01 = {\"approved_tips\": [], \"model_accuracy\": 34.0, \"param_hash\": \"jyjtyjftyj\", \"shard_id\": 1, \"timestamp\": 1683119166.5689557}\n\t    # new_tx_02 = {\"approved_tips\": [], \"model_accuracy\": 2.0, \"param_hash\": \"asefasef\", \"shard_id\": 0, \"timestamp\": 2345234525.5689557}\n", "    # new_tx_01 = MainchainTransaction(**new_tx_01)\n\t    # new_tx_02 = MainchainTransaction(**new_tx_02)\n\t    # client.upload_tx_to_server(\"localhost\", new_tx_01)\n\t    # client.upload_tx_to_server(\"localhost\", new_tx_02)\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "subchain/ml/__init__.py", "chunked_list": []}
{"filename": "subchain/ml/model_build.py", "chunked_list": ["import torch\n\timport torchvision\n\timport copy\n\tfrom torchvision import datasets\n\tfrom models.Nets import CnnMnist\n\tfrom models.test_model import test\n\tfrom utils.sampling import mnist_noniid\n\tfrom utils.sampling import mnist_iid\n\tfrom utils.settings import BaseSettings\n\tfrom models.train_model import LocalUpdate\n", "from models.FedAvg import FedAvg\n\tfrom utils.datasets import get_dataset\n\timport os\n\timport shutil\n\tdef model_build(settings):\n\t    settings.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\t    dataset_train, dataset_test = get_dataset('mnist')\n\t    # sample dataset by individual user\n\t    if settings.iid:\n\t        data_user_mapping = mnist_iid(dataset_train, settings.num_users)\n", "    else:\n\t        data_user_mapping = mnist_noniid(dataset_train, settings)\n\t    net = CnnMnist(settings=settings).to(device=settings.device)\n\t    return net, settings, dataset_train, dataset_test, data_user_mapping\n\tdef model_evaluate(net: torch.nn.Module, params, test_set, settings):\n\t    net.load_state_dict(params)\n\t    acc_test, loss_test = test(net, test_set, settings=settings)\n\t    return acc_test, loss_test\n\t# if __name__ == '__main__':\n\t#     settings = BaseSettings()\n", "#     if os.path.exists('../data/local'):\n\t#         shutil.rmtree('../data/local')\n\t#     os.mkdir('../data/local')\n\t#     net, settings, train_dataset, test_dataset, data_user_mapping = model_build(settings)\n\t#     # sum(p.numel() for p in net.parameters() if p.requires_grad)\n\t#     users = [0, 1, 2, 3, 4]\n\t#     local_acc = []\n\t#     local_losses = []\n\t#     w_glob = net.state_dict()\n\t#     weightAggFile = lambda epoch : f\"../data/local/aggWeight-epoch-{epoch}.pkl\"\n", "#     torch.save(w_glob, weightAggFile(0))\n\t#     for epoch in range(settings.epochs):\n\t#         w_locals = []\n\t#         for user in users:\n\t#             net.load_state_dict(torch.load(weightAggFile(epoch)))\n\t#             local = LocalUpdate(settings=settings, dataset=train_dataset, idxs=data_user_mapping[user])\n\t#             w, loss = local.train(net=copy.deepcopy(net).to(settings.device), user=user)\n\t#             acc, loss = model_evaluate(net, w, test_dataset, settings)\n\t#             local_acc.append(acc)\n\t#             local_losses.append(loss)\n", "#             w_locals.append(w)\n\t#         w_glob = FedAvg(w_locals)\n\t#         torch.save(w_glob, weightAggFile(epoch + 1))\n\t#         loss_avg = sum(local_losses) / len(local_losses)\n\t#         acc_avg = sum(local_acc) / len(local_acc)\n\t#         print('Epoch {:3d}, Average loss {:.3f}'.format(epoch, loss_avg))\n\t#         print('Epoch {:3d}, Average acc {:.3f}'.format(epoch, acc_avg))\n\t#     print(acc, loss)"]}
{"filename": "subchain/ml/utils/sampling.py", "chunked_list": ["import numpy as np\n\tdef cifar_iid(dataset, num_users):\n\t    \"\"\"\n\t    Sample I.I.D. client data from CIFAR10 dataset\n\t    :param dataset:\n\t    :param num_users:\n\t    :return: dict of image index\n\t    \"\"\"\n\t    num_items = int(len(dataset)/num_users)\n\t    data_user_mapping, all_idxs = {}, [i for i in range(len(dataset))]\n", "    for i in range(num_users):\n\t        data_user_mapping[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n\t        all_idxs = list(set(all_idxs) - data_user_mapping[i])\n\t    return data_user_mapping\n\tdef mnist_iid(dataset, num_users):\n\t    \"\"\"\n\t    Sample I.I.D. client data from MNIST dataset\n\t    :param dataset:\n\t    :param num_users:\n\t    :return: dict of image index\n", "    \"\"\"\n\t    num_items = int(len(dataset)/num_users)\n\t    data_user_mapping, all_idxs = {}, [i for i in range(len(dataset))]\n\t    for i in range(num_users):\n\t        data_user_mapping[i] = set(np.random.choice(all_idxs, num_items, replace=False))\n\t        all_idxs = list(set(all_idxs) - data_user_mapping[i])\n\t    return data_user_mapping\n\tdef mnist_noniid(dataset, settings):\n\t    \"\"\"\n\t    Sample non-I.I.D client data from MNIST dataset\n", "    :param dataset:\n\t    :param num_users:\n\t    :return:\n\t    \"\"\"\n\t    dataset_length = len(dataset)\n\t    dataset_class_length = int(dataset_length / 10)\n\t    data_user_mapping = {i: np.array([], dtype='int64') for i in range(settings.num_users)}\n\t    idxs = np.arange(dataset_length)\n\t    labels = np.array(dataset.targets)\n\t    # sort labels\n", "    idxs_labels = np.vstack((idxs, labels))\n\t    idxs_labels = idxs_labels[:,idxs_labels[1,:].argsort()]\n\t    idxs = idxs_labels[0,:]\n\t    for user in range(settings.num_users):\n\t        preffered_class = np.random.choice(np.arange(10), 1, replace=False).item()\n\t        start = preffered_class * dataset_class_length\n\t        pref_indexes = idxs_labels[0, np.arange(start, start + dataset_class_length)]\n\t        pref_indexes_chosen = np.random.choice(\n\t            pref_indexes,\n\t            int(dataset_class_length * settings.non_iid_frac),\n", "            replace=False\n\t        )\n\t        data_user_mapping[user] = np.concatenate((data_user_mapping[user], pref_indexes_chosen), axis=0)\n\t        import copy\n\t        idxs_labels_del = copy.deepcopy(idxs_labels)\n\t        idxs_labels_del = np.delete(idxs_labels_del, slice(start, start + dataset_class_length), axis=1)\n\t        non_preffered_class = np.random.choice(idxs_labels_del[0,:], int(dataset_class_length * (1 - settings.non_iid_frac)))\n\t        data_user_mapping[user] = np.concatenate((data_user_mapping[user], non_preffered_class), axis=0)\n\t        np.random.shuffle(data_user_mapping[user])\n\t    return data_user_mapping"]}
{"filename": "subchain/ml/utils/settings.py", "chunked_list": ["class BaseSettings:\n\t    epochs = 5\n\t    num_users = 5\n\t    client_frac = 0.5\n\t    non_iid_frac = 0.8\n\t    epochs_local = 3\n\t    batch_size_local = 10\n\t    batch_size = 32\n\t    learning_rate = 1e-3\n\t    momentum = 0.9\n", "    weight_decay = 5e-4\n\t    split = 'user'\n\t    device = -1\n\t    iid = False\n\t    num_channels = 3\n\t    num_classes = 10\n\t    kernel_size = 5"]}
{"filename": "subchain/ml/utils/__init__.py", "chunked_list": []}
{"filename": "subchain/ml/utils/datasets.py", "chunked_list": ["import torch\n\tfrom torchvision import datasets, transforms\n\tdef get_dataset(type):\n\t    if type == 'cifar':\n\t        train_transform = transforms.Compose(\n\t            [\n\t                transforms.RandomCrop(32, padding=4),\n\t                transforms.RandomHorizontalFlip(),\n\t                transforms.ToTensor(),\n\t                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n", "            ]\n\t        )\n\t        test_transform = transforms.Compose(\n\t            [\n\t                transforms.ToTensor(),\n\t                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n\t            ]\n\t        )\n\t        dataset_train = datasets.CIFAR10('../data/cifar10', train=True, download=True, transform=train_transform)\n\t        dataset_test = datasets.CIFAR10('../data/cifar10', train=False, download=True, transform=test_transform)\n", "        return dataset_train, dataset_test\n\t    else:\n\t        transform = transforms.Compose(\n\t            [\n\t                transforms.ToTensor(), \n\t                transforms.Normalize((0.1307,), (0.3081,)),\n\t            ]\n\t        )\n\t        dataset_train = datasets.MNIST('../data/mnist', train=True, download=True, transform=transform)\n\t        dataset_test = datasets.MNIST('../data/mnist', train=False, download=True, transform=transform)\n", "        return dataset_train, dataset_test"]}
{"filename": "subchain/ml/models/Nets.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tclass CnnNet(nn.Module):\n\t    def __init__(self, settings):\n\t        super().__init__()\n\t        self.conv1 = nn.Conv2d(settings.num_channels, 6, settings.kernel_size)\n\t        self.pool = nn.MaxPool2d(2, 2)\n\t        self.conv2 = nn.Conv2d(6, 16, 5)\n\t        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n", "        self.fc2 = nn.Linear(120, 84)\n\t        self.fc3 = nn.Linear(84, settings.num_classes)\n\t    def forward(self, x):\n\t        x = self.pool(F.relu(self.conv1(x)))\n\t        x = self.pool(F.relu(self.conv2(x)))\n\t        x = torch.flatten(x, 1) # flatten all dimensions except batch\n\t        x = F.relu(self.fc1(x))\n\t        x = F.relu(self.fc2(x))\n\t        x = self.fc3(x)\n\t        return x\n", "class CnnMnist(nn.Module):\n\t    def __init__(self, settings):\n\t        super(CnnMnist, self).__init__()\n\t        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n\t        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n\t        self.conv2_drop = nn.Dropout2d()\n\t        self.fc1 = nn.Linear(320, 50)\n\t        self.fc2 = nn.Linear(50, 10)\n\t    def forward(self, x):\n\t        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n", "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n\t        x = x.view(-1, x.shape[1]*x.shape[2]*x.shape[3])\n\t        x = F.relu(self.fc1(x))\n\t        x = F.dropout(x, training=self.training)\n\t        x = self.fc2(x)\n\t        return x"]}
{"filename": "subchain/ml/models/FedAvg.py", "chunked_list": ["import copy\n\timport torch\n\tdef FedAvg(w):\n\t    w_avg = copy.deepcopy(w[0])\n\t    for k in w_avg.keys():\n\t        for i in range(1, len(w)):\n\t            w_avg[k] += w[i][k]\n\t        w_avg[k] = torch.div(w_avg[k], len(w))\n\t    return w_avg"]}
{"filename": "subchain/ml/models/__init__.py", "chunked_list": []}
{"filename": "subchain/ml/models/test_model.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import DataLoader\n\tdef test(net: nn.Module, test_dataset, settings):\n\t    net.eval()\n\t    test_loss = 0\n\t    correct = 0\n\t    loader = DataLoader(dataset=test_dataset, batch_size=settings.batch_size)\n\t    for _, (test_dataset, target) in enumerate(loader):\n", "        test_dataset, target = test_dataset.cuda(), target.cuda()\n\t        logits = net(test_dataset)\n\t        test_loss += F.cross_entropy(logits, target=target, reduction='sum').item()\n\t        y_pred = logits.data.max(1, keepdim=True)[1]\n\t        correct += y_pred.eq(target.data.view_as(y_pred)).long().cpu().sum()\n\t    test_loss /= len(loader.dataset)\n\t    accuracy = 100.00 * correct / len(loader.dataset)\n\t    print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n\t            test_loss, correct, len(loader.dataset), accuracy))\n\t    return accuracy, test_loss"]}
{"filename": "subchain/ml/models/train_model.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t# Python version: 3.6\n\timport torch\n\tfrom torch import nn\n\tfrom torch.utils.data import DataLoader, Dataset\n\tclass DatasetSplit(Dataset):\n\t    def __init__(self, dataset, idxs):\n\t        self.dataset = dataset\n\t        self.idxs = list(idxs)\n", "    def __len__(self):\n\t        return len(self.idxs)\n\t    def __getitem__(self, item):\n\t        image, label = self.dataset[self.idxs[item]]\n\t        return image, label\n\tclass LocalUpdate(object):\n\t    def __init__(self, settings, dataset=None, idxs=None):\n\t        self.settings = settings\n\t        self.loss_func = nn.CrossEntropyLoss()\n\t        self.ldr_train = DataLoader(DatasetSplit(dataset, idxs), batch_size=self.settings.batch_size, shuffle=True)\n", "    def train(self, net, user):\n\t        net.train()\n\t        # train and update\n\t        optimizer = torch.optim.SGD(\n\t            net.parameters(),\n\t            lr=self.settings.learning_rate,\n\t            momentum=self.settings.momentum,\n\t            weight_decay=self.settings.weight_decay\n\t        )\n\t        epoch_loss = []\n", "        for iter in range(self.settings.epochs_local):\n\t            batch_loss = []\n\t            for batch_idx, (images, labels) in enumerate(self.ldr_train):\n\t                images, labels = images.to(self.settings.device), labels.to(self.settings.device)\n\t                net.zero_grad()\n\t                log_probs = net(images)\n\t                loss = self.loss_func(log_probs, labels)\n\t                loss.backward()\n\t                optimizer.step()\n\t                if batch_idx % 10 == 0:\n", "                    print('Update Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, device {}, user {}'.format(\n\t                        iter, batch_idx * len(images), len(self.ldr_train.dataset),\n\t                               100. * batch_idx / len(self.ldr_train), loss.item(), self.settings.device, user))\n\t                batch_loss.append(loss.item())\n\t            epoch_loss.append(sum(batch_loss)/len(batch_loss))\n\t        return net.state_dict(), sum(epoch_loss) / len(epoch_loss)\n"]}
{"filename": "common/ipfs.py", "chunked_list": ["import subprocess\n\tdef ipfsGetFile(hashValue, fileName):\n\t    \"\"\"\n\t    Use hashValue to download the file from IPFS network.\n\t    \"\"\"\n\t    ipfsGet = subprocess.Popen(args=['ipfs get ' + hashValue + ' -o ' + fileName], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t    outs, errs = ipfsGet.communicate(timeout=10)\n\t    if ipfsGet.poll() == 0:\n\t        return outs.strip(), ipfsGet.poll()\n\t    else:\n", "        return errs.strip(), ipfsGet.poll()\n\tdef ipfsAddFile(fileName):\n\t    \"\"\"\n\t    Upload the file to IPFS network and return the exclusive fileHash value.\n\t    \"\"\"\n\t    ipfsAdd = subprocess.Popen(args=['ipfs add ' + fileName + ' | tr \\' \\' \\'\\\\n\\' | grep Qm'], shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, encoding='utf-8')\n\t    outs, errs = ipfsAdd.communicate(timeout=10)\n\t    if ipfsAdd.poll() == 0:\n\t        return outs.strip(), ipfsAdd.poll()\n\t    else:\n", "        return errs.strip(), ipfsAdd.poll()"]}
{"filename": "mainchain/server_run.py", "chunked_list": ["import os\n\timport shutil\n\timport pathlib\n\timport dag_model.transaction as transaction\n\tfrom dag_model.dag import DAG\n\tfrom  dag_socket import server\n\tCACHE_DIR = \"./cache/\"\n\tSERVER_DATA_DIR = pathlib.Path(CACHE_DIR) / \"server\"\n\tTX_DATA_DIR = pathlib.Path(SERVER_DATA_DIR) / \"txs\"\n\tDAG_DATA_DIR = pathlib.Path(SERVER_DATA_DIR) / \"pools\"\n", "def main():\n\t    if os.path.exists(CACHE_DIR) == False:\n\t        os.mkdir(CACHE_DIR)\n\t    if os.path.exists(SERVER_DATA_DIR):\n\t        shutil.rmtree(SERVER_DATA_DIR)\n\t    os.mkdir(SERVER_DATA_DIR)\n\t    if os.path.exists(TX_DATA_DIR):\n\t        shutil.rmtree(TX_DATA_DIR)\n\t    os.mkdir(TX_DATA_DIR)\n\t    if os.path.exists(DAG_DATA_DIR):\n", "        shutil.rmtree(DAG_DATA_DIR)\n\t    os.mkdir(DAG_DATA_DIR)\n\t    server_dag = DAG()\n\t    # genesis_info = \"QmaBYCmzPQ2emuXpVykLDHra7t8tPiU8reFMkbHpN1rRoo\"\n\t    # genesis_block = transaction.MainchainTransaction(genesis_info)\n\t    # genesis_block.tx_name = 'genesis'\n\t    # transaction.tx_save(genesis_block)\n\t    # server_dag.tx_publish(genesis_block)\n\t    # server_dag.remove_genesis()\n\t    while True:\n", "        server.create_server_socket(server_dag)\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "mainchain/__init__.py", "chunked_list": []}
{"filename": "mainchain/dag_socket/__init__.py", "chunked_list": []}
{"filename": "mainchain/dag_socket/server.py", "chunked_list": ["import socket\n\timport threading\n\timport sys\n\timport struct\n\timport os\n\timport json\n\timport pathlib\n\tfrom dag_model.dag import DAG\n\timport dag_model.transaction as transaction\n\tBUFFER_SIZE = 1024\n", "def create_server_socket(dag_obj, num_shards = 5):\n\t    try:\n\t        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\t        s.bind((\"\", 65432))\n\t        s.listen(num_shards)\n\t        print(\"DAG server created, awaiting connections...\")\n\t    except socket.error as msg:\n\t        print(msg)\n\t        sys.exit(1)\n", "    while True:\n\t        conn, addr = s.accept()\n\t        t = threading.Thread(target=handle_conn, args=(conn, addr, dag_obj))\n\t        t.start()\n\tdef file_send(conn: socket, file_addr):\n\t    try:\n\t        file_header = struct.pack('64si', os.path.basename(file_addr).encode(),\n\t                                          os.stat(file_addr).st_size)\n\t        conn.send(file_header) # send header\n\t        conn.recv(BUFFER_SIZE) # header response\n", "        with open(file_addr, 'rb') as f:\n\t            while True:\n\t                data = f.read(BUFFER_SIZE)\n\t                if not data:\n\t                    break\n\t                conn.send(data)\n\t    except Exception as e:\n\t        print(e)\n\tdef handle_conn(conn: socket, addr, dag_obj: DAG):\n\t    print(f\"Accept new connection from {addr}\")\n", "    conn.send((f\"Connection accepted on server.\").encode())\n\t    while 1:\n\t        msg = conn.recv(1024).decode()\n\t        if msg == 'requireTx':\n\t            conn.send('ok'.encode())\n\t            recv_data_file = conn.recv(BUFFER_SIZE).decode() # which tx is needed\n\t            tx_file_addr = f\"./cache/server/txs/{recv_data_file}.json\"\n\t            file_send(conn, tx_file_addr)\n\t        elif msg == 'requireTips':\n\t            tips_file_addr = \"./cache/server/pools/tip_pool.json\"\n", "            file_send(conn, tips_file_addr)\n\t        elif msg == 'uploadTx':\n\t            conn.send('ok'.encode())\n\t            recv_data = conn.recv(BUFFER_SIZE).decode()\n\t            json_tx_data = json.loads(recv_data)\n\t            new_tx = transaction.MainchainTransaction(**json_tx_data)\n\t            transaction.tx_save(new_tx)\n\t            dag_obj.tx_publish(new_tx)\n\t            print(f\"The new block {new_tx.tx_name} has been published!\")\n\t    conn.close()"]}
{"filename": "mainchain/dag_model/dag.py", "chunked_list": ["import json\n\timport pathlib\n\tfrom dag_model.transaction import MainchainTransaction\n\tclass DAG:\n\t    def __init__(self, data_address = './cache/server/pools/',\n\t                       alpha = 3,\n\t                       freshness = 60,\n\t                       genesis_name = \"genesis\") -> None:\n\t        self.data_address = data_address\n\t        self.alpha = alpha\n", "        self.freshness = freshness\n\t        self.active_nodes_pool = dict()\n\t        self.tip_nodes_pool = dict()\n\t        self.genesis_name = genesis_name\n\t    def tx_publish(self, tx: MainchainTransaction) -> None:\n\t        if len(self.active_nodes_pool) == 0:\n\t            self.genesis_name = tx.tx_name\n\t        self.active_nodes_pool[tx.tx_name] = tx.timestamp\n\t        self.tip_nodes_pool[tx.tx_name] = tx.timestamp\n\t        if (len(tx.approved_tips) + self.alpha) < len(self.tip_nodes_pool):\n", "            for tip in tx.approved_tips:\n\t                self.tip_nodes_pool.pop(tip, None)\n\t        if self.genesis_name in tx.approved_tips:\n\t            self.tip_nodes_pool.pop(self.genesis_name, None)\n\t        with open(pathlib.Path(self.data_address) / 'active_pool.json', 'w') as f:\n\t            json.dump(self.active_nodes_pool, f)\n\t        with open(pathlib.Path(self.data_address) / 'tip_pool.json', 'w') as f:\n\t            json.dump(self.tip_nodes_pool, f)\n\t    def remove_genesis(self):\n\t        self.tip_nodes_pool.pop('genesis', None)\n", "        with open(pathlib.Path(self.data_address) / 'tip_pool.json', 'w') as f:\n\t            json.dump(self.tip_nodes_pool, f)\n"]}
{"filename": "mainchain/dag_model/transaction.py", "chunked_list": ["import time\n\timport json\n\timport hashlib\n\timport pathlib\n\t\"\"\"\n\tMainchain transaction as described in Fig. 5\n\tAlso referred as Mainchain Regular Block\n\ttx_hash          - hash of the block (transation)\n\tshard_id         - id of the node (shard) invoking the transaction\n\tapproved_tips    - approve tips set (list of rank)\n", "timestamp        - timestamp of the block\n\tmodel_accuracy   - accuracy of the aggregated model\n\tparam_hash       - hash of the parameters file\n\t\"\"\"\n\tclass MainchainTransaction:\n\t    def __init__(self,\n\t                 param_hash,\n\t                 timestamp = time.time(),\n\t                 shard_id = 0,\n\t                 approved_tips = [],\n", "                 model_accuracy = 0.0,\n\t                ) -> None:\n\t        self.param_hash = param_hash\n\t        self.timestamp = timestamp\n\t        self.shard_id = shard_id\n\t        self.approved_tips = approved_tips\n\t        self.model_accuracy = model_accuracy\n\t        self.tx_hash = self.hash()\n\t        self.tx_name = f\"shard_{self.shard_id}_{str(self.timestamp)}\"\n\t    def hash(self):\n", "        header = {\n\t            'shard_id': self.shard_id,\n\t            'approved_tips': self.approved_tips,\n\t            'timestamp': self.timestamp,\n\t        }\n\t        data_to_hash = json.dumps(header,\n\t                                  default=lambda obj: obj.__dict__,\n\t                                  sort_keys=True)\n\t        data_encoded = data_to_hash.encode()\n\t        return hashlib.sha256(data_encoded).hexdigest()\n", "    def json_output(self):\n\t        return {\n\t            'param_hash': self.param_hash,\n\t            'timestamp': self.timestamp,\n\t            'shard_id': self.shard_id,\n\t            'approved_tips': self.approved_tips,\n\t            'model_accuracy': self.model_accuracy,\n\t        }\n\t# tx fs storage functions\n\tdef tx_read(tx_name: str) -> MainchainTransaction:\n", "    with open(pathlib.Path('./cache/server/txs/') / f\"{tx_name}.json\", 'r') as f:\n\t        object_params = json.load(f)\n\t    return MainchainTransaction(**object_params)\n\tdef tx_save(tx: MainchainTransaction) -> None:\n\t    tx_file_path = pathlib.Path('./cache/server/txs/') / f\"{tx.tx_name}.json\"\n\t    try:\n\t        with open(tx_file_path, 'w') as f:\n\t            f.write(\n\t                json.dumps(\n\t                    tx.json_output(),\n", "                    default=lambda obj: obj.__dict__,\n\t                    sort_keys=True,\n\t                )\n\t            )\n\t    except Exception as e:\n\t        print(e)\n"]}
