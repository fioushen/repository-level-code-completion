{"filename": "tests/test_connection_options.py", "chunked_list": ["\"\"\"\n\tUnit tests for the backend connection arguments.\n\t\"\"\"\n\timport sys\n\timport pytest\n\tfrom tests.test_databases import DATABASE_URLS, async_adapter\n\tfrom databasez.backends.aiopg import AiopgBackend\n\tfrom databasez.backends.asyncmy import AsyncMyBackend\n\tfrom databasez.backends.mysql import MySQLBackend\n\tfrom databasez.backends.postgres import PostgresBackend\n", "from databasez.core import DatabaseURL\n\tdef test_postgres_pool_size():\n\t    backend = PostgresBackend(\"postgres://localhost/database?min_size=1&max_size=20\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"min_size\": 1, \"max_size\": 20}\n\t@async_adapter\n\tasync def test_postgres_pool_size_connect():\n\t    for url in DATABASE_URLS:\n\t        if DatabaseURL(url).dialect != \"postgresql\":\n\t            continue\n", "        backend = PostgresBackend(url + \"?min_size=1&max_size=20\")\n\t        await backend.connect()\n\t        await backend.disconnect()\n\tdef test_postgres_explicit_pool_size():\n\t    backend = PostgresBackend(\"postgres://localhost/database\", min_size=1, max_size=20)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"min_size\": 1, \"max_size\": 20}\n\tdef test_postgres_ssl():\n\t    backend = PostgresBackend(\"postgres://localhost/database?ssl=true\")\n\t    kwargs = backend._get_connection_kwargs()\n", "    assert kwargs == {\"ssl\": True}\n\tdef test_postgres_explicit_ssl():\n\t    backend = PostgresBackend(\"postgres://localhost/database\", ssl=True)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n\tdef test_postgres_no_extra_options():\n\t    backend = PostgresBackend(\"postgres://localhost/database\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {}\n\tdef test_postgres_password_as_callable():\n", "    def gen_password():\n\t        return \"Foo\"\n\t    backend = PostgresBackend(\"postgres://:password@localhost/database\", password=gen_password)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"password\": gen_password}\n\t    assert kwargs[\"password\"]() == \"Foo\"\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mysql_pool_size():\n\t    backend = MySQLBackend(\"mysql://localhost/database?min_size=1&max_size=20\")\n\t    kwargs = backend._get_connection_kwargs()\n", "    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mysql_unix_socket():\n\t    backend = MySQLBackend(\n\t        \"mysql+aiomysql://username:password@/testsuite?unix_socket=/tmp/mysqld/mysqld.sock\"\n\t    )\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"unix_socket\": \"/tmp/mysqld/mysqld.sock\"}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mysql_explicit_pool_size():\n", "    backend = MySQLBackend(\"mysql://localhost/database\", min_size=1, max_size=20)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mysql_ssl():\n\t    backend = MySQLBackend(\"mysql://localhost/database?ssl=true\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mysql_explicit_ssl():\n", "    backend = MySQLBackend(\"mysql://localhost/database\", ssl=True)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mysql_pool_recycle():\n\t    backend = MySQLBackend(\"mysql://localhost/database?pool_recycle=20\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"pool_recycle\": 20}\n\t@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n\tdef test_asyncmy_pool_size():\n", "    backend = AsyncMyBackend(\"mysql+asyncmy://localhost/database?min_size=1&max_size=20\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\t@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n\tdef test_asyncmy_unix_socket():\n\t    backend = AsyncMyBackend(\n\t        \"mysql+asyncmy://username:password@/testsuite?unix_socket=/tmp/mysqld/mysqld.sock\"\n\t    )\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"unix_socket\": \"/tmp/mysqld/mysqld.sock\"}\n", "@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n\tdef test_asyncmy_explicit_pool_size():\n\t    backend = AsyncMyBackend(\"mysql://localhost/database\", min_size=1, max_size=20)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\t@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n\tdef test_asyncmy_ssl():\n\t    backend = AsyncMyBackend(\"mysql+asyncmy://localhost/database?ssl=true\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n", "@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n\tdef test_asyncmy_explicit_ssl():\n\t    backend = AsyncMyBackend(\"mysql+asyncmy://localhost/database\", ssl=True)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n\t@pytest.mark.skipif(sys.version_info < (3, 7), reason=\"requires python3.7 or higher\")\n\tdef test_asyncmy_pool_recycle():\n\t    backend = AsyncMyBackend(\"mysql+asyncmy://localhost/database?pool_recycle=20\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"pool_recycle\": 20}\n", "def test_aiopg_pool_size():\n\t    backend = AiopgBackend(\"postgresql+aiopg://localhost/database?min_size=1&max_size=20\")\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\tdef test_aiopg_explicit_pool_size():\n\t    backend = AiopgBackend(\"postgresql+aiopg://localhost/database\", min_size=1, max_size=20)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\tdef test_aiopg_ssl():\n\t    backend = AiopgBackend(\"postgresql+aiopg://localhost/database?ssl=true\")\n", "    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n\tdef test_aiopg_explicit_ssl():\n\t    backend = AiopgBackend(\"postgresql+aiopg://localhost/database\", ssl=True)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mssql_pool_size():\n\t    backend = MySQLBackend(\"mssql+pyodbc://localhost/database?min_size=1&max_size=20\")\n\t    kwargs = backend._get_connection_kwargs()\n", "    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mssql_explicit_pool_size():\n\t    backend = MySQLBackend(\"mssql+pyodbc://localhost/database\", min_size=1, max_size=20)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"minsize\": 1, \"maxsize\": 20}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mssql_ssl():\n\t    backend = MySQLBackend(\"mssql+pyodbc://localhost/database?ssl=true\")\n\t    kwargs = backend._get_connection_kwargs()\n", "    assert kwargs == {\"ssl\": True}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mssql_explicit_ssl():\n\t    backend = MySQLBackend(\"mssql+pyodbc://localhost/database\", ssl=True)\n\t    kwargs = backend._get_connection_kwargs()\n\t    assert kwargs == {\"ssl\": True}\n\t@pytest.mark.skipif(sys.version_info >= (3, 10), reason=\"requires python3.9 or lower\")\n\tdef test_mssql_pool_recycle():\n\t    backend = MySQLBackend(\"mssql+pyodbc://localhost/database?pool_recycle=20\")\n\t    kwargs = backend._get_connection_kwargs()\n", "    assert kwargs == {\"pool_recycle\": 20}\n"]}
{"filename": "tests/test_importer.py", "chunked_list": ["import pytest\n\tfrom databasez.importer import ImportFromStringError, import_from_string\n\tdef test_invalid_format():\n\t    with pytest.raises(ImportFromStringError) as exc_info:\n\t        import_from_string(\"example:\")\n\t    expected = 'Import string \"example:\" must be in format \"<module>:<attribute>\".'\n\t    assert exc_info.match(expected)\n\tdef test_invalid_module():\n\t    with pytest.raises(ImportFromStringError) as exc_info:\n\t        import_from_string(\"module_does_not_exist:myattr\")\n", "    expected = 'Could not import module \"module_does_not_exist\".'\n\t    assert exc_info.match(expected)\n\tdef test_invalid_attr():\n\t    with pytest.raises(ImportFromStringError) as exc_info:\n\t        import_from_string(\"tempfile:attr_does_not_exist\")\n\t    expected = 'Attribute \"attr_does_not_exist\" not found in module \"tempfile\".'\n\t    assert exc_info.match(expected)\n\tdef test_internal_import_error():\n\t    with pytest.raises(ImportError):\n\t        import_from_string(\"tests.importer.raise_import_error:myattr\")\n", "def test_valid_import():\n\t    instance = import_from_string(\"tempfile:TemporaryFile\")\n\t    from tempfile import TemporaryFile\n\t    assert instance == TemporaryFile\n"]}
{"filename": "tests/test_integration.py", "chunked_list": ["import pytest\n\timport sqlalchemy\n\tfrom esmerald import Gateway\n\tfrom esmerald import JSONResponse as EsmeraldJSONResponse\n\tfrom esmerald import Request, route\n\tfrom esmerald.applications import Esmerald\n\tfrom esmerald.testclient import EsmeraldTestClient\n\tfrom starlette.applications import Starlette\n\tfrom starlette.responses import JSONResponse\n\tfrom starlette.testclient import TestClient\n", "from tests.test_databases import DATABASE_URLS\n\tfrom databasez import Database, DatabaseURL\n\tmetadata = sqlalchemy.MetaData()\n\tnotes = sqlalchemy.Table(\n\t    \"notes\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"text\", sqlalchemy.String(length=100)),\n\t    sqlalchemy.Column(\"completed\", sqlalchemy.Boolean),\n\t)\n", "@pytest.fixture(autouse=True, scope=\"module\")\n\tdef create_test_database():\n\t    # Create test databases\n\t    for url in DATABASE_URLS:\n\t        database_url = DatabaseURL(url)\n\t        if database_url.scheme in [\"mysql\", \"mysql+aiomysql\", \"mysql+asyncmy\"]:\n\t            url = str(database_url.replace(driver=\"pymysql\"))\n\t        elif database_url.scheme in [\n\t            \"postgresql+aiopg\",\n\t            \"sqlite+aiosqlite\",\n", "            \"postgresql+asyncpg\",\n\t        ]:\n\t            url = str(database_url.replace(driver=None))\n\t        elif database_url.scheme in [\n\t            \"mssql\",\n\t            \"mssql+pyodbc\",\n\t            \"mssql+aioodbc\",\n\t            \"mssql+pymssql\",\n\t        ]:\n\t            url = str(database_url.replace(driver=\"pyodbc\"))\n", "        engine = sqlalchemy.create_engine(url)\n\t        metadata.create_all(engine)\n\t    # Run the test suite\n\t    yield\n\t    for url in DATABASE_URLS:\n\t        database_url = DatabaseURL(url)\n\t        if database_url.scheme in [\"mysql\", \"mysql+aiomysql\", \"mysql+asyncmy\"]:\n\t            url = str(database_url.replace(driver=\"pymysql\"))\n\t        elif database_url.scheme in [\n\t            \"postgresql+aiopg\",\n", "            \"sqlite+aiosqlite\",\n\t            \"postgresql+asyncpg\",\n\t        ]:\n\t            url = str(database_url.replace(driver=None))\n\t        elif database_url.scheme in [\n\t            \"mssql\",\n\t            \"mssql+pyodbc\",\n\t            \"mssql+aioodbc\",\n\t            \"mssql+pymssql\",\n\t        ]:\n", "            url = str(database_url.replace(driver=\"pyodbc\"))\n\t        engine = sqlalchemy.create_engine(url)\n\t        metadata.drop_all(engine)\n\tdef get_app(database_url):\n\t    database = Database(database_url, force_rollback=True)\n\t    app = Starlette()\n\t    @app.on_event(\"startup\")\n\t    async def startup():\n\t        await database.connect()\n\t    @app.on_event(\"shutdown\")\n", "    async def shutdown():\n\t        await database.disconnect()\n\t    @app.route(\"/notes\", methods=[\"GET\"])\n\t    async def list_notes(request):\n\t        query = notes.select()\n\t        results = await database.fetch_all(query)\n\t        content = [\n\t            {\"text\": result[\"text\"], \"completed\": result[\"completed\"]} for result in results\n\t        ]\n\t        return JSONResponse(content)\n", "    @app.route(\"/notes\", methods=[\"POST\"])\n\t    async def add_note(request):\n\t        data = await request.json()\n\t        query = notes.insert().values(text=data[\"text\"], completed=data[\"completed\"])\n\t        await database.execute(query)\n\t        return JSONResponse({\"text\": data[\"text\"], \"completed\": data[\"completed\"]})\n\t    return app\n\tdef get_esmerald_app(database_url):\n\t    database = Database(database_url, force_rollback=True)\n\t    @route(\"/notes\", methods=[\"GET\"])\n", "    async def list_notes(request: Request) -> EsmeraldJSONResponse:\n\t        query = notes.select()\n\t        results = await database.fetch_all(query)\n\t        content = [\n\t            {\"text\": result[\"text\"], \"completed\": result[\"completed\"]} for result in results\n\t        ]\n\t        return EsmeraldJSONResponse(content)\n\t    @route(\"/notes\", methods=[\"POST\"])\n\t    async def add_notes(request: Request) -> EsmeraldJSONResponse:\n\t        data = await request.json()\n", "        query = notes.insert().values(text=data[\"text\"], completed=data[\"completed\"])\n\t        await database.execute(query)\n\t        return EsmeraldJSONResponse({\"text\": data[\"text\"], \"completed\": data[\"completed\"]})\n\t    app = Esmerald(routes=[Gateway(handler=list_notes), Gateway(handler=add_notes)])\n\t    @app.on_event(\"startup\")\n\t    async def startup():\n\t        await database.connect()\n\t    @app.on_event(\"shutdown\")\n\t    async def shutdown():\n\t        await database.disconnect()\n", "    return app\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\tdef test_integration(database_url):\n\t    app = get_app(database_url)\n\t    with TestClient(app) as client:\n\t        response = client.post(\"/notes\", json={\"text\": \"example\", \"completed\": True})\n\t        assert response.status_code == 200\n\t        assert response.json() == {\"text\": \"example\", \"completed\": True}\n\t        response = client.get(\"/notes\")\n\t        assert response.status_code == 200\n", "        assert response.json() == [{\"text\": \"example\", \"completed\": True}]\n\t    with TestClient(app) as client:\n\t        # Ensure sessions are isolated\n\t        response = client.get(\"/notes\")\n\t        assert response.status_code == 200\n\t        assert response.json() == []\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\tdef test_integration_esmerald(database_url):\n\t    app = get_esmerald_app(database_url)\n\t    with EsmeraldTestClient(app) as client:\n", "        response = client.post(\"/notes\", json={\"text\": \"example\", \"completed\": True})\n\t        assert response.status_code == 200\n\t        assert response.json() == {\"text\": \"example\", \"completed\": True}\n\t        response = client.get(\"/notes\")\n\t        assert response.status_code == 200\n\t        assert response.json() == [{\"text\": \"example\", \"completed\": True}]\n\t    with EsmeraldTestClient(app) as client:\n\t        # Ensure sessions are isolated\n\t        response = client.get(\"/notes\")\n\t        assert response.status_code == 200\n", "        assert response.json() == []\n"]}
{"filename": "tests/test_databases.py", "chunked_list": ["import asyncio\n\timport datetime\n\timport decimal\n\timport functools\n\timport gc\n\timport os\n\tfrom typing import MutableMapping\n\tfrom unittest.mock import MagicMock, patch\n\tfrom urllib.parse import parse_qsl, urlsplit\n\timport pytest\n", "import sqlalchemy\n\tfrom databasez import Database, DatabaseURL\n\tassert \"TEST_DATABASE_URLS\" in os.environ, \"TEST_DATABASE_URLS is not set.\"\n\tDATABASE_URLS = [url.strip() for url in os.environ[\"TEST_DATABASE_URLS\"].split(\",\")]\n\tDATABASE_CONFIG_URLS = []\n\tfor value in DATABASE_URLS:\n\t    spliter = urlsplit(value)\n\t    DATABASE_CONFIG_URLS.append(\n\t        {\n\t            \"connection\": {\n", "                \"credentials\": {\n\t                    \"scheme\": spliter.scheme.split(\"+\")[0],\n\t                    \"host\": spliter.hostname,\n\t                    \"port\": spliter.port,\n\t                    \"user\": spliter.username,\n\t                    \"password\": spliter.password,\n\t                    \"database\": spliter.path[1:],\n\t                    \"options\": dict(parse_qsl(spliter.query)),\n\t                }\n\t            }\n", "        }\n\t    )\n\tclass AsyncMock(MagicMock):\n\t    async def __call__(self, *args, **kwargs):\n\t        return super(AsyncMock, self).__call__(*args, **kwargs)\n\tclass MyEpochType(sqlalchemy.types.TypeDecorator):\n\t    impl = sqlalchemy.Integer\n\t    epoch = datetime.date(1970, 1, 1)\n\t    def process_bind_param(self, value, dialect):\n\t        return (value - self.epoch).days\n", "    def process_result_value(self, value, dialect):\n\t        return self.epoch + datetime.timedelta(days=value)\n\tmetadata = sqlalchemy.MetaData()\n\tnotes = sqlalchemy.Table(\n\t    \"notes\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"text\", sqlalchemy.String(length=100)),\n\t    sqlalchemy.Column(\"completed\", sqlalchemy.Boolean),\n\t)\n", "# Used to test DateTime\n\tarticles = sqlalchemy.Table(\n\t    \"articles\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"title\", sqlalchemy.String(length=100)),\n\t    sqlalchemy.Column(\"published\", sqlalchemy.DateTime),\n\t)\n\t# Used to test JSON\n\tsession = sqlalchemy.Table(\n", "    \"session\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"data\", sqlalchemy.JSON),\n\t)\n\t# Used to test custom column types\n\tcustom_date = sqlalchemy.Table(\n\t    \"custom_date\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n", "    sqlalchemy.Column(\"title\", sqlalchemy.String(length=100)),\n\t    sqlalchemy.Column(\"published\", MyEpochType),\n\t)\n\t# Used to test Numeric\n\tprices = sqlalchemy.Table(\n\t    \"prices\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"price\", sqlalchemy.Numeric(precision=30, scale=20)),\n\t)\n", "@pytest.fixture(autouse=True, scope=\"function\")\n\tdef create_test_database():\n\t    # Create test databases with tables creation\n\t    for url in DATABASE_URLS:\n\t        database_url = DatabaseURL(url)\n\t        if database_url.scheme in [\"mysql\", \"mysql+aiomysql\", \"mysql+asyncmy\"]:\n\t            url = str(database_url.replace(driver=\"pymysql\"))\n\t        elif database_url.scheme in [\n\t            \"postgresql+aiopg\",\n\t            \"sqlite+aiosqlite\",\n", "            \"postgresql+asyncpg\",\n\t            \"mssql+pyodbc\",\n\t            \"mssql+aioodbc\",\n\t        ]:\n\t            url = str(database_url.replace(driver=None))\n\t        engine = sqlalchemy.create_engine(url)\n\t        metadata.create_all(engine)\n\t    # Run the test suite\n\t    yield\n\t    # Drop test databases\n", "    for url in DATABASE_URLS:\n\t        database_url = DatabaseURL(url)\n\t        if database_url.scheme in [\"mysql\", \"mysql+aiomysql\", \"mysql+asyncmy\"]:\n\t            url = str(database_url.replace(driver=\"pymysql\"))\n\t        elif database_url.scheme in [\n\t            \"postgresql+aiopg\",\n\t            \"sqlite+aiosqlite\",\n\t            \"postgresql+asyncpg\",\n\t            \"mssql+pyodbc\",\n\t            \"mssql+aioodbc\",\n", "        ]:\n\t            url = str(database_url.replace(driver=None))\n\t        engine = sqlalchemy.create_engine(url)\n\t        metadata.drop_all(engine)\n\tdef async_adapter(wrapped_func):\n\t    \"\"\"\n\t    Decorator used to run async test cases.\n\t    \"\"\"\n\t    @functools.wraps(wrapped_func)\n\t    def run_sync(*args, **kwargs):\n", "        loop = asyncio.new_event_loop()\n\t        task = wrapped_func(*args, **kwargs)\n\t        return loop.run_until_complete(task)\n\t    return run_sync\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_queries(database_url):\n\t    \"\"\"\n\t    Test that the basic `execute()`, `execute_many()`, `fetch_all()``, and\n\t    `fetch_one()` interfaces are all supported (using SQLAlchemy core).\n", "    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            # execute()\n\t            query = notes.insert()\n", "            values = {\"text\": \"example1\", \"completed\": True}\n\t            await database.execute(query, values)\n\t            # execute_many()\n\t            query = notes.insert()\n\t            values = [\n\t                {\"text\": \"example2\", \"completed\": False},\n\t                {\"text\": \"example3\", \"completed\": True},\n\t            ]\n\t            await database.execute_many(query, values)\n\t            # fetch_all()\n", "            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 3\n\t            assert results[0][\"text\"] == \"example1\"\n\t            assert results[0][\"completed\"] is True\n\t            assert results[1][\"text\"] == \"example2\"\n\t            assert results[1][\"completed\"] is False\n\t            assert results[2][\"text\"] == \"example3\"\n\t            assert results[2][\"completed\"] is True\n\t            # fetch_one()\n", "            query = notes.select()\n\t            result = await database.fetch_one(query=query)\n\t            assert result[\"text\"] == \"example1\"\n\t            assert result[\"completed\"] is True\n\t            # fetch_val()\n\t            query = sqlalchemy.sql.select(*[notes.c.text])\n\t            result = await database.fetch_val(query=query)\n\t            assert result == \"example1\"\n\t            # fetch_val() with no rows\n\t            query = sqlalchemy.sql.select(*[notes.c.text]).where(notes.c.text == \"impossible\")\n", "            result = await database.fetch_val(query=query)\n\t            assert result is None\n\t            # fetch_val() with a different column\n\t            query = sqlalchemy.sql.select(*[notes.c.id, notes.c.text])\n\t            result = await database.fetch_val(query=query, column=1)\n\t            assert result == \"example1\"\n\t            # row access (needed to maintain test coverage for Record.__getitem__ in postgres backend)\n\t            query = sqlalchemy.sql.select(*[notes.c.text])\n\t            result = await database.fetch_one(query=query)\n\t            assert result[\"text\"] == \"example1\"\n", "            assert result[0] == \"example1\"\n\t            # iterate()\n\t            query = notes.select()\n\t            iterate_results = []\n\t            async for result in database.iterate(query=query):\n\t                iterate_results.append(result)\n\t            assert len(iterate_results) == 3\n\t            assert iterate_results[0][\"text\"] == \"example1\"\n\t            assert iterate_results[0][\"completed\"] is True\n\t            assert iterate_results[1][\"text\"] == \"example2\"\n", "            assert iterate_results[1][\"completed\"] is False\n\t            assert iterate_results[2][\"text\"] == \"example3\"\n\t            assert iterate_results[2][\"completed\"] is True\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_queries_raw(database_url):\n\t    \"\"\"\n\t    Test that the basic `execute()`, `execute_many()`, `fetch_all()``, and\n\t    `fetch_one()` interfaces are all supported (raw queries).\n\t    \"\"\"\n", "    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            # execute()\n\t            query = \"INSERT INTO notes(text, completed) VALUES (:text, :completed)\"\n\t            values = {\"text\": \"example1\", \"completed\": True}\n", "            await database.execute(query, values)\n\t            # execute_many()\n\t            query = \"INSERT INTO notes(text, completed) VALUES (:text, :completed)\"\n\t            values = [\n\t                {\"text\": \"example2\", \"completed\": False},\n\t                {\"text\": \"example3\", \"completed\": True},\n\t            ]\n\t            await database.execute_many(query, values)\n\t            # fetch_all()\n\t            query = \"SELECT * FROM notes WHERE completed = :completed\"\n", "            results = await database.fetch_all(query=query, values={\"completed\": True})\n\t            assert len(results) == 2\n\t            assert results[0][\"text\"] == \"example1\"\n\t            assert results[0][\"completed\"] == True\n\t            assert results[1][\"text\"] == \"example3\"\n\t            assert results[1][\"completed\"] == True\n\t            # fetch_one()\n\t            query = \"SELECT * FROM notes WHERE completed = :completed\"\n\t            result = await database.fetch_one(query=query, values={\"completed\": False})\n\t            assert result[\"text\"] == \"example2\"\n", "            assert result[\"completed\"] == False\n\t            # fetch_val()\n\t            query = \"SELECT completed FROM notes WHERE text = :text\"\n\t            result = await database.fetch_val(query=query, values={\"text\": \"example1\"})\n\t            assert result == True\n\t            query = \"SELECT * FROM notes WHERE text = :text\"\n\t            result = await database.fetch_val(\n\t                query=query, values={\"text\": \"example1\"}, column=\"completed\"\n\t            )\n\t            assert result == True\n", "            # iterate()\n\t            query = \"SELECT * FROM notes\"\n\t            iterate_results = []\n\t            async for result in database.iterate(query=query):\n\t                iterate_results.append(result)\n\t            assert len(iterate_results) == 3\n\t            assert iterate_results[0][\"text\"] == \"example1\"\n\t            assert iterate_results[0][\"completed\"] == True\n\t            assert iterate_results[1][\"text\"] == \"example2\"\n\t            assert iterate_results[1][\"completed\"] == False\n", "            assert iterate_results[2][\"text\"] == \"example3\"\n\t            assert iterate_results[2][\"completed\"] == True\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_ddl_queries(database_url):\n\t    \"\"\"\n\t    Test that the built-in DDL elements such as `DropTable()`,\n\t    `CreateTable()` are supported (using SQLAlchemy core).\n\t    \"\"\"\n\t    database_url = database_url[0]\n", "    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            # DropTable()\n\t            query = sqlalchemy.schema.DropTable(notes)\n\t            await database.execute(query)\n\t            # CreateTable()\n", "            query = sqlalchemy.schema.CreateTable(notes)\n\t            await database.execute(query)\n\t@pytest.mark.parametrize(\"exception\", [Exception, asyncio.CancelledError])\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_queries_after_error(database_url, exception):\n\t    \"\"\"\n\t    Test that the basic `execute()` works after a previous error.\n\t    \"\"\"\n\t    database_url = database_url[0]\n", "    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        with patch.object(\n\t            database.connection()._connection,\n\t            \"acquire\",\n\t            new=AsyncMock(side_effect=exception),\n\t        ):\n", "            with pytest.raises(exception):\n\t                query = notes.select()\n\t                await database.fetch_all(query)\n\t        query = notes.select()\n\t        await database.fetch_all(query)\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_results_support_mapping_interface(database_url):\n\t    \"\"\"\n\t    Casting results to a dict should work, since the interface defines them\n", "    as supporting the mapping interface.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            # execute()\n", "            query = notes.insert()\n\t            values = {\"text\": \"example1\", \"completed\": True}\n\t            await database.execute(query, values)\n\t            # fetch_all()\n\t            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            results_as_dicts = [dict(item) for item in results]\n\t            assert len(results[0]) == 3\n\t            assert len(results_as_dicts[0]) == 3\n\t            assert isinstance(results_as_dicts[0][\"id\"], int)\n", "            assert results_as_dicts[0][\"text\"] == \"example1\"\n\t            assert results_as_dicts[0][\"completed\"] is True\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_results_support_column_reference(database_url):\n\t    \"\"\"\n\t    Casting results to a dict should work, since the interface defines them\n\t    as supporting the mapping interface.\n\t    \"\"\"\n\t    database_url = database_url[0]\n", "    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            now = datetime.datetime.now().replace(microsecond=0)\n\t            today = datetime.date.today()\n\t            # execute()\n\t            query = articles.insert()\n", "            values = {\"title\": \"Hello, world Article\", \"published\": now}\n\t            await database.execute(query, values)\n\t            query = custom_date.insert()\n\t            values = {\"title\": \"Hello, world Custom\", \"published\": today}\n\t            await database.execute(query, values)\n\t            # fetch_all()\n\t            query = sqlalchemy.select(*[articles, custom_date])\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 1\n\t            assert results[0][articles.c.title] == \"Hello, world Article\"\n", "            assert results[0][articles.c.published] == now\n\t            assert results[0][custom_date.c.title] == \"Hello, world Custom\"\n\t            assert results[0][custom_date.c.published] == today\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_result_values_allow_duplicate_names(database_url):\n\t    \"\"\"\n\t    The values of a result should respect when two columns are selected\n\t    with the same name.\n\t    \"\"\"\n", "    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            query = \"SELECT 1 AS id, 2 AS id\"\n\t            row = await database.fetch_one(query=query)\n\t            assert list(row._mapping.keys()) == [\"id\", \"id\"]\n", "            assert list(row._mapping.values()) == [1, 2]\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_fetch_one_returning_no_results(database_url):\n\t    \"\"\"\n\t    fetch_one should return `None` when no results match.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n", "    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            # fetch_all()\n\t            query = notes.select()\n\t            result = await database.fetch_one(query=query)\n\t            assert result is None\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n", "async def test_execute_return_val(database_url):\n\t    \"\"\"\n\t    Test using return value from `execute()` to get an inserted primary key.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n", "        async with database.transaction(force_rollback=True):\n\t            query = notes.insert()\n\t            values = {\"text\": \"example1\", \"completed\": True}\n\t            pk = await database.execute(query, values)\n\t            assert isinstance(pk, int)\n\t            # Apparently for `aiopg` it's OID that will always 0 in this case\n\t            # As it's only one action within this cursor life cycle\n\t            # It's recommended to use the `RETURNING` clause\n\t            # For obtaining the record id\n\t            if database.url.scheme == \"postgresql+aiopg\":\n", "                assert pk == 0\n\t            else:\n\t                query = notes.select().where(notes.c.id == pk)\n\t                result = await database.fetch_one(query)\n\t                assert result[\"text\"] == \"example1\"\n\t                assert result[\"completed\"] is True\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_rollback_isolation(database_url):\n\t    \"\"\"\n", "    Ensure that `database.transaction(force_rollback=True)` provides strict isolation.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        # Perform some INSERT operations on the database.\n\t        async with database.transaction(force_rollback=True):\n", "            query = notes.insert().values(text=\"example1\", completed=True)\n\t            await database.execute(query)\n\t        # Ensure INSERT operations have been rolled back.\n\t        query = notes.select()\n\t        results = await database.fetch_all(query=query)\n\t        assert len(results) == 0\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_rollback_isolation_with_contextmanager(database_url):\n\t    \"\"\"\n", "    Ensure that `database.force_rollback()` provides strict isolation.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    database = Database(**data)\n\t    with database.force_rollback():\n\t        async with database:\n", "            # Perform some INSERT operations on the database.\n\t            query = notes.insert().values(text=\"example1\", completed=True)\n\t            await database.execute(query)\n\t        async with database:\n\t            # Ensure INSERT operations have been rolled back.\n\t            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 0\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n", "async def test_transaction_commit(database_url):\n\t    \"\"\"\n\t    Ensure that transaction commit is supported.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n", "        async with database.transaction(force_rollback=True):\n\t            async with database.transaction():\n\t                query = notes.insert().values(text=\"example1\", completed=True)\n\t                await database.execute(query)\n\t            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 1\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_transaction_commit_serializable(database_url):\n", "    \"\"\"\n\t    Ensure that serializable transaction commit via extra parameters is supported.\n\t    \"\"\"\n\t    database_url = DatabaseURL(database_url)\n\t    if database_url.scheme not in [\"postgresql\", \"postgresql+asyncpg\"]:\n\t        pytest.skip(\"Test (currently) only supports asyncpg\")\n\t    if database_url.scheme == \"postgresql+asyncpg\":\n\t        database_url = database_url.replace(driver=None)\n\t    def insert_independently():\n\t        engine = sqlalchemy.create_engine(str(database_url))\n", "        conn = engine.connect()\n\t        query = notes.insert().values(text=\"example1\", completed=True)\n\t        conn.execute(query)\n\t        conn.close()\n\t    def delete_independently():\n\t        engine = sqlalchemy.create_engine(str(database_url))\n\t        conn = engine.connect()\n\t        query = notes.delete()\n\t        conn.execute(query)\n\t        conn.close()\n", "    async with Database(database_url) as database:\n\t        async with database.transaction(force_rollback=True, isolation=\"serializable\"):\n\t            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 0\n\t            insert_independently()\n\t            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 0\n\t            delete_independently()\n", "@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_transaction_rollback(database_url):\n\t    \"\"\"\n\t    Ensure that transaction rollback is supported.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n", "        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            try:\n\t                async with database.transaction():\n\t                    query = notes.insert().values(text=\"example1\", completed=True)\n\t                    await database.execute(query)\n\t                    raise RuntimeError()\n\t            except RuntimeError:\n\t                pass\n", "            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 0\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_transaction_commit_low_level(database_url):\n\t    \"\"\"\n\t    Ensure that an explicit `await transaction.commit()` is supported.\n\t    \"\"\"\n\t    database_url = database_url[0]\n", "    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            transaction = await database.transaction()\n\t            try:\n\t                query = notes.insert().values(text=\"example1\", completed=True)\n\t                await database.execute(query)\n", "            except Exception:\n\t                await transaction.rollback()\n\t            else:\n\t                await transaction.commit()\n\t            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 1\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_transaction_rollback_low_level(database_url):\n", "    \"\"\"\n\t    Ensure that an explicit `await transaction.rollback()` is supported.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n", "            transaction = await database.transaction()\n\t            try:\n\t                query = notes.insert().values(text=\"example1\", completed=True)\n\t                await database.execute(query)\n\t                raise RuntimeError()\n\t            except Exception:\n\t                await transaction.rollback()\n\t            else:  # pragma: no cover\n\t                await transaction.commit()\n\t            query = notes.select()\n", "            results = await database.fetch_all(query=query)\n\t            assert len(results) == 0\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_transaction_decorator(database_url):\n\t    \"\"\"\n\t    Ensure that @database.transaction() is supported.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n", "        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    database = Database(force_rollback=True, **data)\n\t    @database.transaction()\n\t    async def insert_data(raise_exception):\n\t        query = notes.insert().values(text=\"example\", completed=True)\n\t        await database.execute(query)\n\t        if raise_exception:\n\t            raise RuntimeError()\n", "    async with database:\n\t        with pytest.raises(RuntimeError):\n\t            await insert_data(raise_exception=True)\n\t        query = notes.select()\n\t        results = await database.fetch_all(query=query)\n\t        assert len(results) == 0\n\t        await insert_data(raise_exception=False)\n\t        query = notes.select()\n\t        results = await database.fetch_all(query=query)\n\t        assert len(results) == 1\n", "@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_datetime_field(database_url):\n\t    \"\"\"\n\t    Test DataTime columns, to ensure records are coerced to/from proper Python types.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n", "        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            now = datetime.datetime.now().replace(microsecond=0)\n\t            # execute()\n\t            query = articles.insert()\n\t            values = {\"title\": \"Hello, world\", \"published\": now}\n\t            await database.execute(query, values)\n\t            # fetch_all()\n\t            query = articles.select()\n", "            results = await database.fetch_all(query=query)\n\t            assert len(results) == 1\n\t            assert results[0][\"title\"] == \"Hello, world\"\n\t            assert results[0][\"published\"] == now\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_decimal_field(database_url):\n\t    \"\"\"\n\t    Test Decimal (NUMERIC) columns, to ensure records are coerced to/from proper Python types.\n\t    \"\"\"\n", "    async with Database(database_url) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            price = decimal.Decimal(\"0.700000000000001\")\n\t            # execute()\n\t            query = prices.insert()\n\t            values = {\"price\": price}\n\t            await database.execute(query, values)\n\t            # fetch_all()\n\t            query = prices.select()\n\t            results = await database.fetch_all(query=query)\n", "            assert len(results) == 1\n\t            if database_url.startswith(\"sqlite\"):\n\t                # aiosqlite does not support native decimals --> a roud-off error is expected\n\t                assert results[0][\"price\"] == pytest.approx(price)\n\t            else:\n\t                assert results[0][\"price\"] == price\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_json_field(database_url):\n\t    \"\"\"\n", "    Test JSON columns, to ensure correct cross-database support.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            # execute()\n", "            data = {\"text\": \"hello\", \"boolean\": True, \"int\": 1}\n\t            values = {\"data\": data}\n\t            query = session.insert()\n\t            await database.execute(query, values)\n\t            # fetch_all()\n\t            query = session.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 1\n\t            assert results[0][\"data\"] == {\"text\": \"hello\", \"boolean\": True, \"int\": 1}\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n", "@async_adapter\n\tasync def test_custom_field(database_url):\n\t    \"\"\"\n\t    Test custom column types.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n", "    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            today = datetime.date.today()\n\t            # execute()\n\t            query = custom_date.insert()\n\t            values = {\"title\": \"Hello, world\", \"published\": today}\n\t            await database.execute(query, values)\n\t            # fetch_all()\n\t            query = custom_date.select()\n\t            results = await database.fetch_all(query=query)\n", "            assert len(results) == 1\n\t            assert results[0][\"title\"] == \"Hello, world\"\n\t            assert results[0][\"published\"] == today\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_connections_isolation(database_url):\n\t    \"\"\"\n\t    Ensure that changes are visible between different connections.\n\t    To check this we have to not create a transaction, so that\n\t    each query ends up on a different connection from the pool.\n", "    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        try:\n\t            query = notes.insert().values(text=\"example1\", completed=True)\n\t            await database.execute(query)\n", "            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 1\n\t        finally:\n\t            query = notes.delete()\n\t            await database.execute(query)\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_commit_on_root_transaction(database_url):\n\t    \"\"\"\n", "    Because our tests are generally wrapped in rollback-islation, they\n\t    don't have coverage for commiting the root transaction.\n\t    Deal with this here, and delete the records rather than rolling back.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n", "        try:\n\t            async with database.transaction():\n\t                query = notes.insert().values(text=\"example1\", completed=True)\n\t                await database.execute(query)\n\t            query = notes.select()\n\t            results = await database.fetch_all(query=query)\n\t            assert len(results) == 1\n\t        finally:\n\t            query = notes.delete()\n\t            await database.execute(query)\n", "@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_connect_and_disconnect(database_url):\n\t    \"\"\"\n\t    Test explicit connect() and disconnect().\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n", "        data = {\"config\": database_url}\n\t    database = Database(**data)\n\t    assert not database.is_connected\n\t    await database.connect()\n\t    assert database.is_connected\n\t    await database.disconnect()\n\t    assert not database.is_connected\n\t    # connect and disconnect idempotence\n\t    await database.connect()\n\t    await database.connect()\n", "    assert database.is_connected\n\t    await database.disconnect()\n\t    await database.disconnect()\n\t    assert not database.is_connected\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_connection_context(database_url):\n\t    \"\"\"\n\t    Test connection contexts are task-local.\n\t    \"\"\"\n", "    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.connection() as connection_1:\n\t            async with database.connection() as connection_2:\n\t                assert connection_1 is connection_2\n\t    async with Database(**data) as database:\n", "        connection_1 = None\n\t        connection_2 = None\n\t        test_complete = asyncio.Event()\n\t        async def get_connection_1():\n\t            nonlocal connection_1\n\t            async with database.connection() as connection:\n\t                connection_1 = connection\n\t                await test_complete.wait()\n\t        async def get_connection_2():\n\t            nonlocal connection_2\n", "            async with database.connection() as connection:\n\t                connection_2 = connection\n\t                await test_complete.wait()\n\t        loop = asyncio.get_event_loop()\n\t        task_1 = loop.create_task(get_connection_1())\n\t        task_2 = loop.create_task(get_connection_2())\n\t        while connection_1 is None or connection_2 is None:\n\t            await asyncio.sleep(0.000001)\n\t        assert connection_1 is not connection_2\n\t        test_complete.set()\n", "        await task_1\n\t        await task_2\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_connection_context_with_raw_connection(database_url):\n\t    \"\"\"\n\t    Test connection contexts with respect to the raw connection.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n", "        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.connection() as connection_1:\n\t            async with database.connection() as connection_2:\n\t                assert connection_1 is connection_2\n\t                assert connection_1.raw_connection is connection_2.raw_connection\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n", "async def test_queries_with_expose_backend_connection(database_url):\n\t    \"\"\"\n\t    Replication of `execute()`, `execute_many()`, `fetch_all()``, and\n\t    `fetch_one()` using the raw driver interface.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n", "    async with Database(**data) as database:\n\t        async with database.connection() as connection:\n\t            async with connection.transaction(force_rollback=True):\n\t                # Get the raw connection\n\t                raw_connection = connection.raw_connection\n\t                # Insert query\n\t                if database.url.scheme in [\n\t                    \"mysql\",\n\t                    \"mysql+asyncmy\",\n\t                    \"mysql+aiomysql\",\n", "                    \"postgresql+aiopg\",\n\t                ]:\n\t                    insert_query = \"INSERT INTO notes (text, completed) VALUES (%s, %s)\"\n\t                elif database.url.scheme in [\n\t                    \"mssql\",\n\t                    \"mssql+pyodbc\",\n\t                    \"mssql+aioodbc\",\n\t                ]:\n\t                    insert_query = \"INSERT INTO notes (text, completed) VALUES (?, ?)\"\n\t                else:\n", "                    insert_query = \"INSERT INTO notes (text, completed) VALUES ($1, $2)\"\n\t                # execute()\n\t                values = (\"example1\", True)\n\t                if database.url.scheme in [\n\t                    \"mysql\",\n\t                    \"mysql+aiomysql\",\n\t                    \"postgresql+aiopg\",\n\t                    \"mssql\",\n\t                    \"mssql+pyodbc\",\n\t                    \"mssql+aioodbc\",\n", "                ]:\n\t                    cursor = await raw_connection.cursor()\n\t                    await cursor.execute(insert_query, values)\n\t                elif database.url.scheme == \"mysql+asyncmy\":\n\t                    async with raw_connection.cursor() as cursor:\n\t                        await cursor.execute(insert_query, values)\n\t                elif database.url.scheme in [\"postgresql\", \"postgresql+asyncpg\"]:\n\t                    await raw_connection.execute(insert_query, *values)\n\t                elif database.url.scheme in [\"sqlite\", \"sqlite+aiosqlite\"]:\n\t                    await raw_connection.execute(insert_query, values)\n", "                # execute_many()\n\t                values = [(\"example2\", False), (\"example3\", True)]\n\t                if database.url.scheme in [\"mysql\", \"mysql+aiomysql\"]:\n\t                    cursor = await raw_connection.cursor()\n\t                    await cursor.executemany(insert_query, values)\n\t                elif database.url.scheme == \"mysql+asyncmy\":\n\t                    async with raw_connection.cursor() as cursor:\n\t                        await cursor.executemany(insert_query, values)\n\t                elif database.url.scheme == \"postgresql+aiopg\":\n\t                    cursor = await raw_connection.cursor()\n", "                    # No async support for `executemany`\n\t                    for value in values:\n\t                        await cursor.execute(insert_query, value)\n\t                elif database.url.scheme in [\"mssql\", \"mssql+aioodbc\", \"mssql+pyodbc\"]:\n\t                    cursor = await raw_connection.cursor()\n\t                    for value in values:\n\t                        await cursor.execute(insert_query, value)\n\t                else:\n\t                    await raw_connection.executemany(insert_query, values)\n\t                # Select query\n", "                select_query = \"SELECT notes.id, notes.text, notes.completed FROM notes\"\n\t                # fetch_all()\n\t                if database.url.scheme in [\n\t                    \"mysql\",\n\t                    \"mysql+aiomysql\",\n\t                    \"postgresql+aiopg\",\n\t                    \"mssql\",\n\t                    \"mssql+pyodbc\",\n\t                    \"mssql+aioodbc\",\n\t                ]:\n", "                    cursor = await raw_connection.cursor()\n\t                    await cursor.execute(select_query)\n\t                    results = await cursor.fetchall()\n\t                elif database.url.scheme == \"mysql+asyncmy\":\n\t                    async with raw_connection.cursor() as cursor:\n\t                        await cursor.execute(select_query)\n\t                        results = await cursor.fetchall()\n\t                elif database.url.scheme in [\"postgresql\", \"postgresql+asyncpg\"]:\n\t                    results = await raw_connection.fetch(select_query)\n\t                elif database.url.scheme in [\"sqlite\", \"sqlite+aiosqlite\"]:\n", "                    results = await raw_connection.execute_fetchall(select_query)\n\t                assert len(results) == 3\n\t                # Raw output for the raw request\n\t                assert results[0][1] == \"example1\"\n\t                assert results[0][2] == True\n\t                assert results[1][1] == \"example2\"\n\t                assert results[1][2] == False\n\t                assert results[2][1] == \"example3\"\n\t                assert results[2][2] == True\n\t                # fetch_one()\n", "                if database.url.scheme in [\"postgresql\", \"postgresql+asyncpg\"]:\n\t                    result = await raw_connection.fetchrow(select_query)\n\t                elif database.url.scheme == \"mysql+asyncmy\":\n\t                    async with raw_connection.cursor() as cursor:\n\t                        await cursor.execute(select_query)\n\t                        result = await cursor.fetchone()\n\t                elif database.url.scheme in [\"mssql\", \"mssql+pyodbc\", \"mssql+aioodbc\"]:\n\t                    cursor = await raw_connection.cursor()\n\t                    try:\n\t                        await cursor.execute(select_query)\n", "                        result = await cursor.fetchone()\n\t                    finally:\n\t                        await cursor.close()\n\t                else:\n\t                    cursor = await raw_connection.cursor()\n\t                    await cursor.execute(select_query)\n\t                    result = await cursor.fetchone()\n\t                # Raw output for the raw request\n\t                assert result[1] == \"example1\"\n\t                assert result[2] == True\n", "@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_database_url_interface(database_url):\n\t    \"\"\"\n\t    Test that Database instances expose a `.url` attribute.\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n", "        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        assert isinstance(database.url, DatabaseURL)\n\t        if isinstance(database_url, str):\n\t            assert database.url == database_url\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_concurrent_access_on_single_connection(database_url):\n\t    database_url = DatabaseURL(database_url)\n\t    if database_url.dialect != \"postgresql\":\n", "        pytest.skip(\"Test requires `pg_sleep()`\")\n\t    async with Database(database_url, force_rollback=True) as database:\n\t        async def db_lookup():\n\t            await database.fetch_one(\"SELECT pg_sleep(1)\")\n\t        await asyncio.gather(db_lookup(), db_lookup())\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\tdef test_global_connection_is_initialized_lazily(database_url):\n\t    \"\"\"\n\t    Ensure that global connection is initialized at latest possible time\n\t    so it's _query_lock will belong to same event loop that async_adapter has\n", "    initialized.\n\t    See https://github.com/dymmond/databasez/issues/157 for more context.\n\t    \"\"\"\n\t    database_url = DatabaseURL(database_url)\n\t    if database_url.dialect != \"postgresql\":\n\t        pytest.skip(\"Test requires `pg_sleep()`\")\n\t    database = Database(database_url, force_rollback=True)\n\t    @async_adapter\n\t    async def run_database_queries():\n\t        async with database:\n", "            async def db_lookup():\n\t                await database.fetch_one(\"SELECT pg_sleep(1)\")\n\t            await asyncio.gather(db_lookup(), db_lookup())\n\t    run_database_queries()\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_iterate_outside_transaction_with_values(database_url):\n\t    \"\"\"\n\t    Ensure `iterate()` works even without a transaction on all drivers.\n\t    The asyncpg driver relies on server-side cursors without hold\n", "    for iteration, which requires a transaction to be created.\n\t    This is mentionned in both their documentation and their test suite.\n\t    \"\"\"\n\t    database_url = DatabaseURL(database_url)\n\t    if database_url.dialect == \"mysql\":\n\t        pytest.skip(\"MySQL does not support `FROM (VALUES ...)` (F641)\")\n\t    async with Database(database_url) as database:\n\t        if database_url.dialect == \"mssql\":\n\t            query = \"SELECT * FROM (VALUES (1), (2), (3), (4), (5)) as X(t)\"\n\t        else:\n", "            query = \"SELECT * FROM (VALUES (1), (2), (3), (4), (5)) as t\"\n\t        iterate_results = []\n\t        async for result in database.iterate(query=query):\n\t            iterate_results.append(result)\n\t        assert len(iterate_results) == 5\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_iterate_outside_transaction_with_temp_table(database_url):\n\t    \"\"\"\n\t    Same as test_iterate_outside_transaction_with_values but uses a\n", "    temporary table instead of a list of values.\n\t    \"\"\"\n\t    database_url = DatabaseURL(database_url)\n\t    if database_url.dialect == \"sqlite\":\n\t        pytest.skip(\"SQLite interface does not work with temporary tables.\")\n\t    async with Database(database_url) as database:\n\t        if database_url.dialect == \"mssql\":\n\t            query = \"CREATE TABLE ##no_transac(num INTEGER)\"\n\t            await database.execute(query)\n\t            query = \"INSERT INTO ##no_transac VALUES (1), (2), (3), (4), (5)\"\n", "            await database.execute(query)\n\t            query = \"SELECT * FROM ##no_transac\"\n\t        else:\n\t            query = \"CREATE TEMPORARY TABLE no_transac(num INTEGER)\"\n\t            await database.execute(query)\n\t            query = \"INSERT INTO no_transac(num) VALUES (1), (2), (3), (4), (5)\"\n\t            await database.execute(query)\n\t            query = \"SELECT * FROM no_transac\"\n\t        iterate_results = []\n\t        async for result in database.iterate(query=query):\n", "            iterate_results.append(result)\n\t        assert len(iterate_results) == 5\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@pytest.mark.parametrize(\"select_query\", [notes.select(), \"SELECT * FROM notes\"])\n\t@async_adapter\n\tasync def test_column_names(database_url, select_query):\n\t    \"\"\"\n\t    Test that column names are exposed correctly through `._mapping.keys()` on each row.\n\t    \"\"\"\n\t    database_url = database_url[0]\n", "    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        async with database.transaction(force_rollback=True):\n\t            # insert values\n\t            query = notes.insert()\n\t            values = {\"text\": \"example1\", \"completed\": True}\n\t            await database.execute(query, values)\n", "            # fetch results\n\t            results = await database.fetch_all(query=select_query)\n\t            assert len(results) == 1\n\t            assert sorted(results[0]._mapping.keys()) == [\"completed\", \"id\", \"text\"]\n\t            assert results[0][\"text\"] == \"example1\"\n\t            assert results[0][\"completed\"] == True\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_postcompile_queries(database_url):\n\t    \"\"\"\n", "    Since SQLAlchemy 1.4, IN operators needs to do render_postcompile\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        query = notes.insert()\n\t        values = {\"text\": \"example1\", \"completed\": True}\n", "        await database.execute(query, values)\n\t        query = notes.select().where(notes.c.id.in_([2, 3]))\n\t        results = await database.fetch_all(query=query)\n\t        assert len(results) == 0\n\t@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_result_named_access(database_url):\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n", "    else:\n\t        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        query = notes.insert()\n\t        values = {\"text\": \"example1\", \"completed\": True}\n\t        await database.execute(query, values)\n\t        query = notes.select().where(notes.c.text == \"example1\")\n\t        result = await database.fetch_one(query=query)\n\t        assert result.text == \"example1\"\n\t        assert result.completed is True\n", "@pytest.mark.parametrize(\"database_url\", [DATABASE_URLS, DATABASE_CONFIG_URLS])\n\t@async_adapter\n\tasync def test_mapping_property_interface(database_url):\n\t    \"\"\"\n\t    Test that all connections implement interface with `_mapping` property\n\t    \"\"\"\n\t    database_url = database_url[0]\n\t    if isinstance(database_url, str):\n\t        data = {\"url\": database_url}\n\t    else:\n", "        data = {\"config\": database_url}\n\t    async with Database(**data) as database:\n\t        query = notes.insert()\n\t        values = {\"text\": \"example1\", \"completed\": True}\n\t        await database.execute(query, values)\n\t        query = notes.select()\n\t        single_result = await database.fetch_one(query=query)\n\t        assert single_result._mapping[\"text\"] == \"example1\"\n\t        assert single_result._mapping[\"completed\"] is True\n\t        list_result = await database.fetch_all(query=query)\n", "        assert list_result[0]._mapping[\"text\"] == \"example1\"\n\t        assert list_result[0]._mapping[\"completed\"] is True\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_transaction_context_child_task_inheritance(database_url):\n\t    \"\"\"\n\t    Ensure that transactions are inherited by child tasks.\n\t    \"\"\"\n\t    async with Database(database_url) as database:\n\t        async def check_transaction(transaction, active_transaction):\n", "            # Should have inherited the same transaction backend from the parent task\n\t            assert transaction._transaction is active_transaction\n\t        async with database.transaction() as transaction:\n\t            await asyncio.create_task(check_transaction(transaction, transaction._transaction))\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_transaction_context_child_task_inheritance_example(database_url):\n\t    \"\"\"\n\t    Ensure that child tasks may influence inherited transactions.\n\t    \"\"\"\n", "    # This is an practical example of the above test.\n\t    db = Database(database_url)\n\t    if db.url.dialect == \"mssql\":\n\t        return\n\t    async with Database(database_url) as database:\n\t        async with database.transaction():\n\t            # Create a note\n\t            await database.execute(notes.insert().values(id=1, text=\"setup\", completed=True))\n\t            # Change the note from the same task\n\t            await database.execute(notes.update().where(notes.c.id == 1).values(text=\"prior\"))\n", "            # Confirm the change\n\t            result = await database.fetch_one(notes.select().where(notes.c.id == 1))\n\t            assert result.text == \"prior\"\n\t            async def run_update_from_child_task(connection):\n\t                # Change the note from a child task\n\t                await connection.execute(notes.update().where(notes.c.id == 1).values(text=\"test\"))\n\t            await asyncio.create_task(run_update_from_child_task(database.connection()))\n\t            # Confirm the child's change\n\t            result = await database.fetch_one(notes.select().where(notes.c.id == 1))\n\t            assert result.text == \"test\"\n", "@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_transaction_context_sibling_task_isolation(database_url):\n\t    \"\"\"\n\t    Ensure that transactions are isolated between sibling tasks.\n\t    \"\"\"\n\t    start = asyncio.Event()\n\t    end = asyncio.Event()\n\t    async with Database(database_url) as database:\n\t        async def check_transaction(transaction):\n", "            await start.wait()\n\t            # Parent task is now in a transaction, we should not\n\t            # see its transaction backend since this task was\n\t            # _started_ in a context where no transaction was active.\n\t            assert transaction._transaction is None\n\t            end.set()\n\t        transaction = database.transaction()\n\t        assert transaction._transaction is None\n\t        task = asyncio.create_task(check_transaction(transaction))\n\t        async with transaction:\n", "            start.set()\n\t            assert transaction._transaction is not None\n\t            await end.wait()\n\t        # Cleanup for \"Task not awaited\" warning\n\t        await task\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_transaction_context_sibling_task_isolation_example(database_url):\n\t    \"\"\"\n\t    Ensure that transactions are running in sibling tasks are isolated from eachother.\n", "    \"\"\"\n\t    # This is an practical example of the above test.\n\t    db = Database(database_url)\n\t    if db.url.dialect == \"mssql\":\n\t        return\n\t    setup = asyncio.Event()\n\t    done = asyncio.Event()\n\t    async def tx1(connection):\n\t        async with connection.transaction():\n\t            await db.execute(notes.insert(), values={\"id\": 1, \"text\": \"tx1\", \"completed\": False})\n", "            setup.set()\n\t            await done.wait()\n\t    async def tx2(connection):\n\t        async with connection.transaction():\n\t            await setup.wait()\n\t            result = await db.fetch_all(notes.select())\n\t            assert result == [], result\n\t            done.set()\n\t    async with Database(database_url) as db:\n\t        await asyncio.gather(tx1(db), tx2(db))\n", "@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_connection_cleanup_contextmanager(database_url):\n\t    \"\"\"\n\t    Ensure that task connections are not persisted unecessarily.\n\t    \"\"\"\n\t    ready = asyncio.Event()\n\t    done = asyncio.Event()\n\t    async def check_child_connection(database: Database):\n\t        async with database.connection():\n", "            ready.set()\n\t            await done.wait()\n\t    async with Database(database_url) as database:\n\t        # Should have a connection in this task\n\t        # .connect is lazy, it doesn't create a Connection, but .connection does\n\t        connection = database.connection()\n\t        assert isinstance(database._connection_map, MutableMapping)\n\t        assert database._connection_map.get(asyncio.current_task()) is connection\n\t        # Create a child task and see if it registers a connection\n\t        task = asyncio.create_task(check_child_connection(database))\n", "        await ready.wait()\n\t        assert database._connection_map.get(task) is not None\n\t        assert database._connection_map.get(task) is not connection\n\t        # Let the child task finish, and see if it cleaned up\n\t        done.set()\n\t        await task\n\t        # This is normal exit logic cleanup, the WeakKeyDictionary\n\t        # shouldn't have cleaned up yet since the task is still referenced\n\t        assert task not in database._connection_map\n\t    # Context manager closes, all open connections are removed\n", "    assert isinstance(database._connection_map, MutableMapping)\n\t    assert len(database._connection_map) == 0\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_connection_cleanup_garbagecollector(database_url):\n\t    \"\"\"\n\t    Ensure that connections for tasks are not persisted unecessarily, even\n\t    if exit handlers are not called.\n\t    \"\"\"\n\t    database = Database(database_url)\n", "    await database.connect()\n\t    created = asyncio.Event()\n\t    async def check_child_connection(database: Database):\n\t        # neither .disconnect nor .__aexit__ are called before deleting this task\n\t        database.connection()\n\t        created.set()\n\t    task = asyncio.create_task(check_child_connection(database))\n\t    await created.wait()\n\t    assert task in database._connection_map\n\t    await task\n", "    del task\n\t    gc.collect()\n\t    # Should not have a connection for the task anymore\n\t    assert len(database._connection_map) == 0\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_transaction_context_cleanup_contextmanager(database_url):\n\t    \"\"\"\n\t    Ensure that contextvar transactions are not persisted unecessarily.\n\t    \"\"\"\n", "    from databasez.core import ACTIVE_TRANSACTIONS\n\t    assert ACTIVE_TRANSACTIONS.get() is None\n\t    async with Database(database_url) as database:\n\t        async with database.transaction() as transaction:\n\t            open_transactions = ACTIVE_TRANSACTIONS.get()\n\t            assert isinstance(open_transactions, MutableMapping)\n\t            assert open_transactions.get(transaction) is transaction._transaction\n\t        # Context manager closes, open_transactions is cleaned up\n\t        open_transactions = ACTIVE_TRANSACTIONS.get()\n\t        assert isinstance(open_transactions, MutableMapping)\n", "        assert open_transactions.get(transaction, None) is None\n\t@pytest.mark.parametrize(\"database_url\", DATABASE_URLS)\n\t@async_adapter\n\tasync def test_transaction_context_cleanup_garbagecollector(database_url):\n\t    \"\"\"\n\t    Ensure that contextvar transactions are not persisted unecessarily, even\n\t    if exit handlers are not called.\n\t    This test should be an XFAIL, but cannot be due to the way that is hangs\n\t    during teardown.\n\t    \"\"\"\n", "    from databasez.core import ACTIVE_TRANSACTIONS\n\t    assert ACTIVE_TRANSACTIONS.get() is None\n\t    async with Database(database_url) as database:\n\t        transaction = database.transaction()\n\t        await transaction.start()\n\t        # Should be tracking the transaction\n\t        open_transactions = ACTIVE_TRANSACTIONS.get()\n\t        assert isinstance(open_transactions, MutableMapping)\n\t        assert open_transactions.get(transaction) is transaction._transaction\n\t        # neither .commit, .rollback, nor .__aexit__ are called\n", "        del transaction\n\t        gc.collect()\n\t        # A strong reference to the transaction is kept alive by the connection's\n\t        # ._transaction_stack, so it is still be tracked at this point.\n\t        assert len(open_transactions) == 1\n\t        # If that were magically cleared, the transaction would be cleaned up,\n\t        # but as it stands this always causes a hang during teardown at\n\t        # `Database(...).disconnect()` if the transaction is not closed.\n\t        transaction = database.connection()._transaction_stack[-1]\n\t        await transaction.rollback()\n", "        del transaction\n\t        # Now with the transaction rolled-back, it should be cleaned up.\n\t        assert len(open_transactions) == 0\n"]}
{"filename": "tests/test_database_url.py", "chunked_list": ["from urllib.parse import quote\n\timport pytest\n\tfrom databasez import DatabaseURL\n\tdef test_database_url_repr():\n\t    u = DatabaseURL(\"postgresql://localhost/name\")\n\t    assert repr(u) == \"DatabaseURL('postgresql://localhost/name')\"\n\t    u = DatabaseURL(\"postgresql://username@localhost/name\")\n\t    assert repr(u) == \"DatabaseURL('postgresql://username@localhost/name')\"\n\t    u = DatabaseURL(\"postgresql://username:password@localhost/name\")\n\t    assert repr(u) == \"DatabaseURL('postgresql://username:********@localhost/name')\"\n", "    u = DatabaseURL(f\"postgresql://username:{quote('[password')}@localhost/name\")\n\t    assert repr(u) == \"DatabaseURL('postgresql://username:********@localhost/name')\"\n\tdef test_database_url_properties():\n\t    u = DatabaseURL(\"postgresql+asyncpg://username:password@localhost:123/mydatabase\")\n\t    assert u.dialect == \"postgresql\"\n\t    assert u.driver == \"asyncpg\"\n\t    assert u.username == \"username\"\n\t    assert u.password == \"password\"\n\t    assert u.hostname == \"localhost\"\n\t    assert u.port == 123\n", "    assert u.database == \"mydatabase\"\n\t    u = DatabaseURL(\n\t        \"postgresql://username:password@/mydatabase?host=/var/run/postgresql/.s.PGSQL.5432\"\n\t    )\n\t    assert u.dialect == \"postgresql\"\n\t    assert u.username == \"username\"\n\t    assert u.password == \"password\"\n\t    assert u.hostname == \"/var/run/postgresql/.s.PGSQL.5432\"\n\t    assert u.database == \"mydatabase\"\n\t    u = DatabaseURL(\n", "        \"postgresql://username:password@/mydatabase?unix_sock=/var/run/postgresql/.s.PGSQL.5432\"\n\t    )\n\t    assert u.hostname == \"/var/run/postgresql/.s.PGSQL.5432\"\n\tdef test_database_url_escape():\n\t    u = DatabaseURL(f\"postgresql://username:{quote('[password')}@localhost/mydatabase\")\n\t    assert u.username == \"username\"\n\t    assert u.password == \"[password\"\n\t    assert u.userinfo == f\"username:{quote('[password')}\".encode(\"utf-8\")\n\t    u2 = DatabaseURL(u)\n\t    assert u2.password == \"[password\"\n", "    u3 = DatabaseURL(str(u))\n\t    assert u3.password == \"[password\"\n\tdef test_database_url_constructor():\n\t    with pytest.raises(TypeError):\n\t        DatabaseURL((\"postgresql\", \"username\", \"password\", \"localhost\", \"mydatabase\"))\n\t    u = DatabaseURL(\"postgresql+asyncpg://username:password@localhost:123/mydatabase\")\n\t    assert DatabaseURL(u) == u\n\tdef test_database_url_options():\n\t    u = DatabaseURL(\"postgresql://localhost/mydatabase?pool_size=20&ssl=true\")\n\t    assert u.options == {\"pool_size\": \"20\", \"ssl\": \"true\"}\n", "    u = DatabaseURL(\"mysql+asyncmy://username/testsuite?unix_socket=/tmp/mysqld/mysqld.sock\")\n\t    assert u.options == {\"unix_socket\": \"/tmp/mysqld/mysqld.sock\"}\n\tdef test_replace_database_url_components():\n\t    u = DatabaseURL(\"postgresql://localhost/mydatabase\")\n\t    assert u.database == \"mydatabase\"\n\t    new = u.replace(database=\"test_\" + u.database)\n\t    assert new.database == \"test_mydatabase\"\n\t    assert str(new) == \"postgresql://localhost/test_mydatabase\"\n\t    assert u.driver == \"\"\n\t    new = u.replace(driver=\"asyncpg\")\n", "    assert new.driver == \"asyncpg\"\n\t    assert str(new) == \"postgresql+asyncpg://localhost/mydatabase\"\n\t    assert u.port is None\n\t    new = u.replace(port=123)\n\t    assert new.port == 123\n\t    assert str(new) == \"postgresql://localhost:123/mydatabase\"\n\t    assert u.username is None\n\t    assert u.userinfo is None\n\t    u = DatabaseURL(\"sqlite:///mydatabase\")\n\t    assert u.database == \"mydatabase\"\n", "    new = u.replace(database=\"test_\" + u.database)\n\t    assert new.database == \"test_mydatabase\"\n\t    assert str(new) == \"sqlite:///test_mydatabase\"\n\t    u = DatabaseURL(\"sqlite:////absolute/path\")\n\t    assert u.database == \"/absolute/path\"\n\t    new = u.replace(database=u.database + \"_test\")\n\t    assert new.database == \"/absolute/path_test\"\n\t    assert str(new) == \"sqlite:////absolute/path_test\"\n"]}
{"filename": "tests/importer/__init__.py", "chunked_list": []}
{"filename": "tests/importer/raise_import_error.py", "chunked_list": ["# Used by test_importer.py\n\tmyattr = 123\n\timport does_not_exist as does_not_exist  # noqa: E402\n"]}
{"filename": "docs_src/quickstart/quickstart.py", "chunked_list": ["# Create a database instance, and connect to it.\n\tfrom databasez import Database\n\tdatabase = Database(\"sqlite+aiosqlite:///example.db\")\n\tawait database.connect()\n\t# Create a table.\n\tquery = \"\"\"CREATE TABLE HighScores (id INTEGER PRIMARY KEY, name VARCHAR(100), score INTEGER)\"\"\"\n\tawait database.execute(query=query)\n\t# Insert some data.\n\tquery = \"INSERT INTO HighScores(name, score) VALUES (:name, :score)\"\n\tvalues = [\n", "    {\"name\": \"Daisy\", \"score\": 92},\n\t    {\"name\": \"Neil\", \"score\": 87},\n\t    {\"name\": \"Carol\", \"score\": 43},\n\t]\n\tawait database.execute_many(query=query, values=values)\n\t# Run a database query.\n\tquery = \"SELECT * FROM HighScores\"\n\trows = await database.fetch_all(query=query)\n\tprint(\"High Scores:\", rows)\n"]}
{"filename": "docs_src/connections/mssql.py", "chunked_list": ["from databasez import Database\n\tCONFIG = {\n\t    \"connection\": {\n\t        \"credentials\": {\n\t            \"scheme\": \"mssql+aioodbc\",\n\t            \"host\": \"localhost\",\n\t            \"port\": 1433,\n\t            \"user\": \"sa\",\n\t            \"password\": \"Mssql123mssql\",\n\t            \"database\": \"master\",\n", "            \"options\": {\"driver\": \"ODBC Driver 17 for SQL Server\"},\n\t        }\n\t    }\n\t}\n\tdatabase = Database(config=CONFIG)\n"]}
{"filename": "docs_src/connections/as_dict.py", "chunked_list": ["from databasez import Database\n\tCONFIG = {\n\t    \"connection\": {\n\t        \"credentials\": {\n\t            \"scheme\": \"postgres+asyncpg\",\n\t            \"host\": \"localhost\",\n\t            \"port\": 5432,\n\t            \"user\": \"postgres\",\n\t            \"password\": \"password\",\n\t            \"database\": \"my_db\",\n", "        }\n\t    }\n\t}\n\tdatabase = Database(config=CONFIG)\n"]}
{"filename": "docs_src/testclient/tests.py", "chunked_list": ["import datetime\n\timport decimal\n\timport ipaddress\n\timport uuid\n\tfrom enum import Enum\n\timport pytest\n\timport saffier\n\tfrom databasez.testclient import DatabaseTestClient\n\tfrom saffier.db.models import fields\n\tfrom tests.settings import DATABASE_URL\n", "database = DatabaseTestClient(DATABASE_URL, drop_database=True)\n\tmodels = saffier.Registry(database=database)\n\tpytestmark = pytest.mark.anyio\n\tdef time():\n\t    return datetime.datetime.now().time()\n\tclass StatusEnum(Enum):\n\t    DRAFT = \"Draft\"\n\t    RELEASED = \"Released\"\n\tclass Product(saffier.Model):\n\t    id = fields.IntegerField(primary_key=True)\n", "    uuid = fields.UUIDField(null=True)\n\t    created = fields.DateTimeField(default=datetime.datetime.now)\n\t    created_day = fields.DateField(default=datetime.date.today)\n\t    created_time = fields.TimeField(default=time)\n\t    created_date = fields.DateField(auto_now_add=True)\n\t    created_datetime = fields.DateTimeField(auto_now_add=True)\n\t    updated_datetime = fields.DateTimeField(auto_now=True)\n\t    updated_date = fields.DateField(auto_now=True)\n\t    data = fields.JSONField(default={})\n\t    description = fields.CharField(blank=True, max_length=255)\n", "    huge_number = fields.BigIntegerField(default=0)\n\t    price = fields.DecimalField(max_digits=5, decimal_places=2, null=True)\n\t    status = fields.ChoiceField(StatusEnum, default=StatusEnum.DRAFT)\n\t    value = fields.FloatField(null=True)\n\t    class Meta:\n\t        registry = models\n\tclass User(saffier.Model):\n\t    id = fields.UUIDField(primary_key=True, default=uuid.uuid4)\n\t    name = fields.CharField(null=True, max_length=16)\n\t    email = fields.EmailField(null=True, max_length=256)\n", "    ipaddress = fields.IPAddressField(null=True)\n\t    url = fields.URLField(null=True, max_length=2048)\n\t    password = fields.PasswordField(null=True, max_length=255)\n\t    class Meta:\n\t        registry = models\n\tclass Customer(saffier.Model):\n\t    name = fields.CharField(null=True, max_length=16)\n\t    class Meta:\n\t        registry = models\n\t@pytest.fixture(autouse=True, scope=\"module\")\n", "async def create_test_database():\n\t    await models.create_all()\n\t    yield\n\t    await models.drop_all()\n\t@pytest.fixture(autouse=True)\n\tasync def rollback_transactions():\n\t    with database.force_rollback():\n\t        async with database:\n\t            yield\n\tasync def test_model_crud():\n", "    product = await Product.query.create()\n\t    product = await Product.query.get(pk=product.pk)\n\t    assert product.created.year == datetime.datetime.now().year\n\t    assert product.created_day == datetime.date.today()\n\t    assert product.created_date == datetime.date.today()\n\t    assert product.created_datetime.date() == datetime.datetime.now().date()\n\t    assert product.updated_date == datetime.date.today()\n\t    assert product.updated_datetime.date() == datetime.datetime.now().date()\n\t    assert product.data == {}\n\t    assert product.description == \"\"\n", "    assert product.huge_number == 0\n\t    assert product.price is None\n\t    assert product.status == StatusEnum.DRAFT\n\t    assert product.value is None\n\t    assert product.uuid is None\n\t    await product.update(\n\t        data={\"foo\": 123},\n\t        value=123.456,\n\t        status=StatusEnum.RELEASED,\n\t        price=decimal.Decimal(\"999.99\"),\n", "        uuid=uuid.UUID(\"f4e87646-bafa-431e-a0cb-e84f2fcf6b55\"),\n\t    )\n\t    product = await Product.query.get()\n\t    assert product.value == 123.456\n\t    assert product.data == {\"foo\": 123}\n\t    assert product.status == StatusEnum.RELEASED\n\t    assert product.price == decimal.Decimal(\"999.99\")\n\t    assert product.uuid == uuid.UUID(\"f4e87646-bafa-431e-a0cb-e84f2fcf6b55\")\n\t    last_updated_datetime = product.updated_datetime\n\t    last_updated_date = product.updated_date\n", "    user = await User.query.create()\n\t    assert isinstance(user.pk, uuid.UUID)\n\t    user = await User.query.get()\n\t    assert user.email is None\n\t    assert user.ipaddress is None\n\t    assert user.url is None\n\t    await user.update(\n\t        ipaddress=\"192.168.1.1\",\n\t        name=\"Test\",\n\t        email=\"test@saffier.com\",\n", "        url=\"https://saffier.com\",\n\t        password=\"12345\",\n\t    )\n\t    user = await User.query.get()\n\t    assert isinstance(user.ipaddress, (ipaddress.IPv4Address, ipaddress.IPv6Address))\n\t    assert user.password == \"12345\"\n\t    assert user.url == \"https://saffier.com\"\n\t    await product.update(data={\"foo\": 1234})\n\t    assert product.updated_datetime != last_updated_datetime\n\t    assert product.updated_date == last_updated_date\n"]}
{"filename": "docs_src/queries/declarations2.py", "chunked_list": ["import sqlalchemy\n\tmetadata = sqlalchemy.MetaData()\n\tuser = sqlalchemy.Table(\n\t    \"users\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"name\", sqlalchemy.String(length=150)),\n\t    sqlalchemy.Column(\"address\", sqlalchemy.String(length=500)),\n\t    sqlalchemy.Column(\"config\", sqlalchemy.JSON(none_as_null=True)),\n\t)\n"]}
{"filename": "docs_src/queries/create_tables.py", "chunked_list": ["import sqlalchemy\n\tfrom databasez import Database\n\tdatabase = Database(\"postgresql+asyncpg://localhost/example\")\n\t# Establish the connection pool\n\tawait database.connect()\n\tmetadata = sqlalchemy.MetaData()\n\tdialect = sqlalchemy.dialects.postgresql.dialect()\n\t# Define your table(s)\n\tusers = sqlalchemy.Table(\n\t    \"users\",\n", "    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"name\", sqlalchemy.String(length=150)),\n\t    sqlalchemy.Column(\"address\", sqlalchemy.String(length=500)),\n\t)\n\t# Create tables\n\tfor table in metadata.tables.values():\n\t    # Set `if_not_exists=False` if you want the query to throw an\n\t    # exception when the table already exists\n\t    schema = sqlalchemy.schema.CreateTable(table, if_not_exists=True)\n", "    query = str(schema.compile(dialect=dialect))\n\t    await database.execute(query=query)\n\t# Close all connections in the connection pool\n\tawait database.disconnect()\n"]}
{"filename": "docs_src/queries/raw_queries.py", "chunked_list": ["from databasez import Database\n\tdatabase = Database(\"postgresql+asyncpg://localhost/example\")\n\t# Establish the connection pool\n\tawait database.connect()\n\t# Execute\n\tquery = \"INSERT INTO users(name, address) VALUES (:name, :address)\"\n\tvalues = {\"text\": \"databasez\", \"address\": \"London, United Kingdom\"}\n\tawait database.execute(query=query, values=values)\n\t# Execute many\n\tquery = \"INSERT INTO users(name, address) VALUES (:name, :address)\"\n", "values = [\n\t    {\"name\": \"databasez\", \"address\": \"London, United Kingdom\"},\n\t    {\"name\": \"another name\", \"address\": \"The Hague, Netherlands\"},\n\t]\n\tawait database.execute_many(query=query, values=values)\n\t# Fetch multiple rows\n\tquery = \"SELECT * FROM users WHERE address = :address\"\n\trows = await database.fetch_all(query=query, values={\"address\": \"London, United Kingdom\"})\n\t# Fetch single row\n\tquery = \"SELECT * FROM users WHERE id = :id\"\n", "result = await database.fetch_one(query=query, values={\"id\": 1})\n"]}
{"filename": "docs_src/queries/declarations.py", "chunked_list": ["import sqlalchemy\n\tmetadata = sqlalchemy.MetaData()\n\tuser = sqlalchemy.Table(\n\t    \"users\",\n\t    metadata,\n\t    sqlalchemy.Column(\"id\", sqlalchemy.Integer, primary_key=True),\n\t    sqlalchemy.Column(\"name\", sqlalchemy.String(length=150)),\n\t    sqlalchemy.Column(\"address\", sqlalchemy.String(length=500)),\n\t)\n"]}
{"filename": "docs_src/queries/queries.py", "chunked_list": ["from databasez import Database\n\tdatabase = Database(\"postgresql+asyncpg://localhost/example\")\n\t# Establish the connection pool\n\tawait database.connect()\n\t# Execute\n\tquery = users.insert()\n\tvalues = {\"name\": \"databasez\", \"address\": \"London, United Kingdom\"}\n\tawait database.execute(query=query, values=values)\n\t# Execute many\n\tquery = users.insert()\n", "values = [\n\t    {\"name\": \"databasez\", \"address\": \"London, United Kingdom\"},\n\t    {\"name\": \"another name\", \"address\": \"The Hague, Netherlands\"},\n\t]\n\tawait database.execute_many(query=query, values=values)\n\t# Fetch multiple rows\n\tquery = users.select()\n\trows = await database.fetch_all(query=query)\n\t# Fetch single row\n\tquery = users.select()\n", "row = await database.fetch_one(query=query)\n\t# Fetch single value, defaults to `column=0`.\n\tquery = users.select()\n\tvalue = await database.fetch_val(query=query)\n\t# Fetch multiple rows without loading them all into memory at once\n\tquery = users.select()\n\tasync for row in database.iterate(query=query):\n\t    ...\n\t# Close all connections in the connection pool\n\tawait database.disconnect()\n"]}
{"filename": "databasez/types.py", "chunked_list": ["from typing import Any, Dict\n\tDictAny = Dict[str, Any]\n"]}
{"filename": "databasez/importer.py", "chunked_list": ["import importlib\n\timport typing\n\tclass ImportFromStringError(Exception):\n\t    pass\n\tdef import_from_string(import_str: str) -> typing.Any:\n\t    module_str, _, attrs_str = import_str.partition(\":\")\n\t    if not module_str or not attrs_str:\n\t        message = 'Import string \"{import_str}\" must be in format \"<module>:<attribute>\".'\n\t        raise ImportFromStringError(message.format(import_str=import_str))\n\t    try:\n", "        module = importlib.import_module(module_str)\n\t    except ImportError as exc:\n\t        if exc.name != module_str:\n\t            raise exc from None\n\t        message = 'Could not import module \"{module_str}\".'\n\t        raise ImportFromStringError(message.format(module_str=module_str)) from exc\n\t    instance = module\n\t    try:\n\t        for attr_str in attrs_str.split(\".\"):\n\t            instance = getattr(instance, attr_str)\n", "    except AttributeError as e:\n\t        message = 'Attribute \"{attrs_str}\" not found in module \"{module_str}\".'\n\t        raise ImportFromStringError(\n\t            message.format(attrs_str=attrs_str, module_str=module_str)\n\t        ) from e\n\t    return instance\n"]}
{"filename": "databasez/__init__.py", "chunked_list": ["from databasez.core import Database, DatabaseURL\n\t__version__ = \"0.5.0\"\n\t__all__ = [\"Database\", \"DatabaseURL\"]\n"]}
{"filename": "databasez/core.py", "chunked_list": ["import asyncio\n\timport contextlib\n\timport functools\n\timport logging\n\timport typing\n\timport weakref\n\tfrom contextvars import ContextVar\n\tfrom types import TracebackType\n\tfrom urllib.parse import SplitResult, parse_qsl, unquote, urlencode, urlsplit\n\tfrom sqlalchemy import text\n", "from sqlalchemy.sql import ClauseElement\n\tfrom databasez.importer import import_from_string\n\tfrom databasez.interfaces import DatabaseBackend, Record, TransactionBackend\n\tif typing.TYPE_CHECKING:\n\t    from databasez.types import DictAny\n\ttry:  # pragma: no cover\n\t    import click\n\t    # Extra log info for optional coloured terminal outputs.\n\t    LOG_EXTRA = {\"color_message\": \"Query: \" + click.style(\"%s\", bold=True) + \" Args: %s\"}\n\t    CONNECT_EXTRA = {\"color_message\": \"Connected to database \" + click.style(\"%s\", bold=True)}\n", "    DISCONNECT_EXTRA = {\n\t        \"color_message\": \"Disconnected from database \" + click.style(\"%s\", bold=True)\n\t    }\n\texcept ImportError:  # pragma: no cover\n\t    LOG_EXTRA = {}\n\t    CONNECT_EXTRA = {}\n\t    DISCONNECT_EXTRA = {}\n\tlogger = logging.getLogger(\"databasez\")\n\tACTIVE_TRANSACTIONS: ContextVar[\n\t    typing.Optional[\"weakref.WeakKeyDictionary['Transaction', 'TransactionBackend']\"]\n", "] = ContextVar(\"databasez:active_transactions\", default=None)\n\tclass Database:\n\t    \"\"\"\n\t    An abstraction on the top of the EncodeORM databases.Database object.\n\t    This object allows to pass also a configuration dictionary in the format of\n\t    DATABASEZ_CONFIG = {\n\t        \"connection\": {\n\t            \"credentials\": {\n\t                \"scheme\": 'sqlite', \"postgres\"...\n\t                \"host\": ...,\n", "                \"port\": ...,\n\t                \"user\": ...,\n\t                \"password\": ...,\n\t                \"database\": ...,\n\t                \"options\": {\n\t                    \"driver\": ...\n\t                    \"ssl\": ...\n\t                }\n\t            }\n\t        }\n", "    }\n\t    \"\"\"\n\t    SUPPORTED_BACKENDS = {\n\t        \"postgresql\": \"databasez.backends.postgres:PostgresBackend\",\n\t        \"postgresql+aiopg\": \"databasez.backends.aiopg:AiopgBackend\",\n\t        \"postgres\": \"databasez.backends.postgres:PostgresBackend\",\n\t        \"mysql\": \"databasez.backends.mysql:MySQLBackend\",\n\t        \"mysql+asyncmy\": \"databasez.backends.asyncmy:AsyncMyBackend\",\n\t        \"mssql\": \"databasez.backends.mssql:MSSQLBackend\",\n\t        \"mssql+pyodbc\": \"databasez.backends.mssql:MSSQLBackend\",\n", "        \"mssql+aioodbc\": \"databasez.backends.mssql:MSSQLBackend\",\n\t        \"sqlite\": \"databasez.backends.sqlite:SQLiteBackend\",\n\t    }\n\t    DIRECT_URL_SCHEME = {\"sqlite\"}\n\t    MANDATORY_FIELDS = [\"host\", \"port\", \"user\", \"database\"]\n\t    _connection_map: \"weakref.WeakKeyDictionary[asyncio.Task, 'Connection']\"\n\t    def __init__(\n\t        self,\n\t        url: typing.Optional[typing.Union[str, \"DatabaseURL\"]] = None,\n\t        *,\n", "        force_rollback: bool = False,\n\t        config: typing.Optional[\"DictAny\"] = None,\n\t        **options: typing.Any,\n\t    ):\n\t        assert config is None or url is None, \"Use either 'url' or 'config', not both.\"\n\t        _url: typing.Optional[typing.Union[str, \"DatabaseURL\"]] = None\n\t        if not config:\n\t            _url = url\n\t        else:\n\t            _url = self._build_url(config)\n", "        self.url = DatabaseURL(_url)  # type: ignore\n\t        self.options = options\n\t        self.is_connected = False\n\t        self._connection_map = weakref.WeakKeyDictionary()\n\t        self._force_rollback = force_rollback\n\t        backend_str = self._get_backend()\n\t        backend_cls = import_from_string(backend_str)\n\t        assert issubclass(backend_cls, DatabaseBackend)\n\t        self._backend = backend_cls(self.url, **self.options)\n\t        # When `force_rollback=True` is used, we use a single global\n", "        # connection, within a transaction that always rolls back.\n\t        self._global_connection: typing.Optional[Connection] = None\n\t        self._global_transaction: typing.Optional[Transaction] = None\n\t    @property\n\t    def allowed_url_schemes(self) -> typing.Set[str]:\n\t        schemes = {\n\t            value\n\t            for value in self.SUPPORTED_BACKENDS.keys()\n\t            if value not in self.DIRECT_URL_SCHEME\n\t        }\n", "        return schemes\n\t    def _build_url(self, config: \"DictAny\") -> str:\n\t        assert \"connection\" in config, \"connection not found in the database configuration\"\n\t        connection = config[\"connection\"]\n\t        assert \"credentials\" in connection, \"credetials not found in connection\"\n\t        credentials = connection[\"credentials\"]\n\t        assert (\n\t            \"scheme\" in credentials\n\t        ), \"scheme is missing from credentials. Use postgres or mysql instead\"\n\t        scheme = credentials[\"scheme\"]\n", "        if not scheme or scheme is None:\n\t            raise ValueError(\"scheme cannot be None\")\n\t        database = credentials[\"database\"]\n\t        scheme = scheme.lower()\n\t        if scheme.lower() in self.DIRECT_URL_SCHEME:\n\t            return self._build_url_for_direct_url_scheme(scheme, database)\n\t        for value in self.MANDATORY_FIELDS:\n\t            if not value or value is None:\n\t                raise ValueError(f\"{value} is required in the credentials\")\n\t        user = credentials[\"user\"]\n", "        password = credentials.get(\"password\", None)\n\t        host = credentials[\"host\"]\n\t        port = credentials[\"port\"]\n\t        if \"password\" not in credentials:\n\t            connection_string = f\"{scheme}://{user}@{host}:{port}/{database}\"\n\t        else:\n\t            connection_string = f\"{scheme}://{user}:{password}@{host}:{port}/{database}\"\n\t        options = credentials.get(\"options\", None)\n\t        if options is None or not options:\n\t            return connection_string\n", "        return f\"{connection_string}?{urlencode(options)}\"\n\t    def _build_url_for_direct_url_scheme(self, scheme: str, database: str) -> str:\n\t        \"\"\"Builds the URL for direct url schemes that do not support user, password and other\n\t        parameters. Example: SQLite.\n\t        \"\"\"\n\t        return f\"{scheme}:///{database}\"\n\t    @property\n\t    def _current_task(self) -> asyncio.Task:\n\t        task = asyncio.current_task()\n\t        if not task:\n", "            raise RuntimeError(\"No currently active asyncio.Task found\")\n\t        return task\n\t    @property\n\t    def _connection(self) -> typing.Optional[\"Connection\"]:\n\t        return self._connection_map.get(self._current_task)\n\t    @_connection.setter\n\t    def _connection(\n\t        self, connection: typing.Optional[\"Connection\"]\n\t    ) -> typing.Optional[\"Connection\"]:\n\t        task = self._current_task\n", "        if connection is None:\n\t            self._connection_map.pop(task, None)\n\t        else:\n\t            self._connection_map[task] = connection\n\t        return self._connection\n\t    async def connect(self) -> None:\n\t        \"\"\"\n\t        Establish the connection pool.\n\t        \"\"\"\n\t        if self.is_connected:\n", "            logger.debug(\"Already connected, skipping connection\")\n\t            return None\n\t        await self._backend.connect()\n\t        logger.info(\"Connected to database %s\", self.url.obscure_password, extra=CONNECT_EXTRA)\n\t        self.is_connected = True\n\t        if self._force_rollback:\n\t            assert self._global_connection is None\n\t            assert self._global_transaction is None\n\t            self._global_connection = Connection(self, self._backend)\n\t            self._global_transaction = self._global_connection.transaction(force_rollback=True)\n", "            await self._global_transaction.__aenter__()\n\t    async def disconnect(self) -> None:\n\t        \"\"\"\n\t        Close all connections in the connection pool.\n\t        \"\"\"\n\t        if not self.is_connected:\n\t            logger.debug(\"Already disconnected, skipping disconnection\")\n\t            return None\n\t        if self._force_rollback:\n\t            assert self._global_connection is not None\n", "            assert self._global_transaction is not None\n\t            await self._global_transaction.__aexit__()\n\t            self._global_transaction = None\n\t            self._global_connection = None\n\t        else:\n\t            self._connection = None\n\t        await self._backend.disconnect()\n\t        logger.info(\n\t            \"Disconnected from database %s\",\n\t            self.url.obscure_password,\n", "            extra=DISCONNECT_EXTRA,\n\t        )\n\t        self.is_connected = False\n\t    async def __aenter__(self) -> \"Database\":\n\t        await self.connect()\n\t        return self\n\t    async def __aexit__(\n\t        self,\n\t        exc_type: typing.Optional[typing.Type[BaseException]] = None,\n\t        exc_value: typing.Optional[BaseException] = None,\n", "        traceback: typing.Optional[TracebackType] = None,\n\t    ) -> None:\n\t        await self.disconnect()\n\t    async def fetch_all(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.List[Record]:\n\t        async with self.connection() as connection:\n\t            return await connection.fetch_all(query, values)\n", "    async def fetch_one(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.Optional[Record]:\n\t        async with self.connection() as connection:\n\t            return await connection.fetch_one(query, values)\n\t    async def fetch_val(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n", "        values: typing.Optional[dict] = None,\n\t        column: typing.Any = 0,\n\t    ) -> typing.Any:\n\t        async with self.connection() as connection:\n\t            return await connection.fetch_val(query, values, column=column)\n\t    async def execute(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.Any:\n", "        async with self.connection() as connection:\n\t            return await connection.execute(query, values)\n\t    async def execute_many(self, query: typing.Union[ClauseElement, str], values: list) -> None:\n\t        async with self.connection() as connection:\n\t            return await connection.execute_many(query, values)\n\t    async def iterate(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.AsyncGenerator[typing.Mapping, None]:\n", "        async with self.connection() as connection:\n\t            async for record in connection.iterate(query, values):\n\t                yield record\n\t    def connection(self) -> \"Connection\":\n\t        if self._global_connection is not None:\n\t            return self._global_connection\n\t        if not self._connection:\n\t            self._connection = Connection(self, self._backend)\n\t        return self._connection\n\t    def transaction(self, *, force_rollback: bool = False, **kwargs: typing.Any) -> \"Transaction\":\n", "        return Transaction(self.connection, force_rollback=force_rollback, **kwargs)\n\t    @contextlib.contextmanager\n\t    def force_rollback(self) -> typing.Iterator[None]:\n\t        initial = self._force_rollback\n\t        self._force_rollback = True\n\t        try:\n\t            yield\n\t        finally:\n\t            self._force_rollback = initial\n\t    def _get_backend(self) -> str:\n", "        return self.SUPPORTED_BACKENDS.get(\n\t            self.url.scheme, self.SUPPORTED_BACKENDS[self.url.dialect]\n\t        )\n\tclass Connection:\n\t    def __init__(self, database: Database, backend: DatabaseBackend) -> None:\n\t        self._database = database\n\t        self._backend = backend\n\t        self._connection_lock = asyncio.Lock()\n\t        self._connection = self._backend.connection()\n\t        self._connection_counter = 0\n", "        self._transaction_lock = asyncio.Lock()\n\t        self._transaction_stack: typing.List[Transaction] = []\n\t        self._query_lock = asyncio.Lock()\n\t    async def __aenter__(self) -> \"Connection\":\n\t        async with self._connection_lock:\n\t            self._connection_counter += 1\n\t            try:\n\t                if self._connection_counter == 1:\n\t                    await self._connection.acquire()\n\t            except BaseException as e:\n", "                self._connection_counter -= 1\n\t                raise e\n\t        return self\n\t    async def __aexit__(\n\t        self,\n\t        exc_type: typing.Optional[typing.Type[BaseException]] = None,\n\t        exc_value: typing.Optional[BaseException] = None,\n\t        traceback: typing.Optional[TracebackType] = None,\n\t    ) -> None:\n\t        async with self._connection_lock:\n", "            assert self._connection is not None\n\t            self._connection_counter -= 1\n\t            if self._connection_counter == 0:\n\t                await self._connection.release()\n\t                self._database._connection = None\n\t    async def fetch_all(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.List[Record]:\n", "        built_query = self._build_query(query, values)\n\t        async with self._query_lock:\n\t            return await self._connection.fetch_all(built_query)\n\t    async def fetch_one(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.Optional[Record]:\n\t        built_query = self._build_query(query, values)\n\t        async with self._query_lock:\n", "            return await self._connection.fetch_one(built_query)\n\t    async def fetch_val(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t        column: typing.Any = 0,\n\t    ) -> typing.Any:\n\t        built_query = self._build_query(query, values)\n\t        async with self._query_lock:\n\t            return await self._connection.fetch_val(built_query, column)\n", "    async def execute(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.Any:\n\t        built_query = self._build_query(query, values)\n\t        async with self._query_lock:\n\t            return await self._connection.execute(built_query)\n\t    async def execute_many(self, query: typing.Union[ClauseElement, str], values: list) -> None:\n\t        queries = [self._build_query(query, values_set) for values_set in values]\n", "        async with self._query_lock:\n\t            await self._connection.execute_many(queries)\n\t    async def iterate(\n\t        self,\n\t        query: typing.Union[ClauseElement, str],\n\t        values: typing.Optional[dict] = None,\n\t    ) -> typing.AsyncGenerator[typing.Any, None]:\n\t        built_query = self._build_query(query, values)\n\t        async with self.transaction():\n\t            async with self._query_lock:\n", "                async for record in self._connection.iterate(built_query):\n\t                    yield record\n\t    def transaction(self, *, force_rollback: bool = False, **kwargs: typing.Any) -> \"Transaction\":\n\t        def connection_callable() -> Connection:\n\t            return self\n\t        return Transaction(connection_callable, force_rollback, **kwargs)\n\t    @property\n\t    def raw_connection(self) -> typing.Any:\n\t        return self._connection.raw_connection\n\t    @staticmethod\n", "    def _build_query(\n\t        query: typing.Union[ClauseElement, str], values: typing.Optional[dict] = None\n\t    ) -> ClauseElement:\n\t        if isinstance(query, str):\n\t            query = text(query)\n\t            return query.bindparams(**values) if values is not None else query\n\t        elif values:\n\t            return query.values(**values)  # type: ignore\n\t        return query\n\t_CallableType = typing.TypeVar(\"_CallableType\", bound=typing.Callable)\n", "class Transaction:\n\t    def __init__(\n\t        self,\n\t        connection_callable: typing.Callable[[], Connection],\n\t        force_rollback: bool,\n\t        **kwargs: typing.Any,\n\t    ) -> None:\n\t        self._connection_callable = connection_callable\n\t        self._force_rollback = force_rollback\n\t        self._extra_options = kwargs\n", "    @property\n\t    def _connection(self) -> \"Connection\":\n\t        # Returns the same connection if called multiple times\n\t        return self._connection_callable()\n\t    @property\n\t    def _transaction(self) -> typing.Optional[\"TransactionBackend\"]:\n\t        transactions = ACTIVE_TRANSACTIONS.get()\n\t        if transactions is None:\n\t            return None\n\t        return transactions.get(self, None)\n", "    @_transaction.setter\n\t    def _transaction(\n\t        self, transaction: typing.Optional[\"TransactionBackend\"]\n\t    ) -> typing.Optional[\"TransactionBackend\"]:\n\t        transactions = ACTIVE_TRANSACTIONS.get()\n\t        if transactions is None:\n\t            transactions = weakref.WeakKeyDictionary()\n\t        else:\n\t            transactions = transactions.copy()\n\t        if transaction is None:\n", "            transactions.pop(self, None)\n\t        else:\n\t            transactions[self] = transaction\n\t        ACTIVE_TRANSACTIONS.set(transactions)\n\t        return transactions.get(self, None)\n\t    async def __aenter__(self) -> \"Transaction\":\n\t        \"\"\"\n\t        Called when entering `async with database.transaction()`\n\t        \"\"\"\n\t        await self.start()\n", "        return self\n\t    async def __aexit__(\n\t        self,\n\t        exc_type: typing.Optional[typing.Type[BaseException]] = None,\n\t        exc_value: typing.Optional[BaseException] = None,\n\t        traceback: typing.Optional[TracebackType] = None,\n\t    ) -> None:\n\t        \"\"\"\n\t        Called when exiting `async with database.transaction()`\n\t        \"\"\"\n", "        if exc_type is not None or self._force_rollback:\n\t            await self.rollback()\n\t        else:\n\t            await self.commit()\n\t    def __await__(self) -> typing.Generator[None, None, \"Transaction\"]:\n\t        \"\"\"\n\t        Called if using the low-level `transaction = await database.transaction()`\n\t        \"\"\"\n\t        return self.start().__await__()\n\t    def __call__(self, func: _CallableType) -> _CallableType:\n", "        \"\"\"\n\t        Called if using `@database.transaction()` as a decorator.\n\t        \"\"\"\n\t        @functools.wraps(func)\n\t        async def wrapper(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\t            async with self:\n\t                return await func(*args, **kwargs)\n\t        return wrapper  # type: ignore\n\t    async def start(self) -> \"Transaction\":\n\t        self._transaction = self._connection._connection.transaction()\n", "        async with self._connection._transaction_lock:\n\t            is_root = not self._connection._transaction_stack\n\t            await self._connection.__aenter__()\n\t            await self._transaction.start(is_root=is_root, extra_options=self._extra_options)\n\t            self._connection._transaction_stack.append(self)\n\t        return self\n\t    async def commit(self) -> None:\n\t        async with self._connection._transaction_lock:\n\t            assert self._connection._transaction_stack[-1] is self\n\t            self._connection._transaction_stack.pop()\n", "            assert self._transaction is not None\n\t            await self._transaction.commit()\n\t            await self._connection.__aexit__()\n\t            self._transaction = None\n\t    async def rollback(self) -> None:\n\t        async with self._connection._transaction_lock:\n\t            assert self._connection._transaction_stack[-1] is self\n\t            self._connection._transaction_stack.pop()\n\t            assert self._transaction is not None\n\t            await self._transaction.rollback()\n", "            await self._connection.__aexit__()\n\t            self._transaction = None\n\tclass _EmptyNetloc(str):\n\t    def __bool__(self) -> bool:\n\t        return True\n\tclass DatabaseURL:\n\t    def __init__(self, url: typing.Union[str, \"DatabaseURL\"]):\n\t        if isinstance(url, DatabaseURL):\n\t            self._url: str = url._url\n\t        elif isinstance(url, str):\n", "            self._url = url\n\t        else:\n\t            raise TypeError(\n\t                f\"Invalid type for DatabaseURL. Expected str or DatabaseURL, got {type(url)}\"\n\t            )\n\t    @property\n\t    def components(self) -> SplitResult:\n\t        if not hasattr(self, \"_components\"):\n\t            self._components = urlsplit(self._url)\n\t        return self._components\n", "    @property\n\t    def scheme(self) -> str:\n\t        return self.components.scheme\n\t    @property\n\t    def dialect(self) -> str:\n\t        return self.components.scheme.split(\"+\")[0]\n\t    @property\n\t    def driver(self) -> str:\n\t        if \"+\" not in self.components.scheme:\n\t            return \"\"\n", "        return self.components.scheme.split(\"+\", 1)[1]\n\t    @property\n\t    def userinfo(self) -> typing.Optional[bytes]:\n\t        if self.components.username:\n\t            info = self.components.username\n\t            if self.components.password:\n\t                info += \":\" + self.components.password\n\t            return info.encode(\"utf-8\")\n\t        return None\n\t    @property\n", "    def username(self) -> typing.Optional[str]:\n\t        if self.components.username is None:\n\t            return None\n\t        return unquote(self.components.username)\n\t    @property\n\t    def password(self) -> typing.Optional[str]:\n\t        if self.components.password is None:\n\t            return None\n\t        return unquote(self.components.password)\n\t    @property\n", "    def hostname(self) -> typing.Optional[str]:\n\t        return (\n\t            self.components.hostname or self.options.get(\"host\") or self.options.get(\"unix_sock\")\n\t        )\n\t    @property\n\t    def port(self) -> typing.Optional[int]:\n\t        return self.components.port\n\t    @property\n\t    def netloc(self) -> typing.Optional[str]:\n\t        return self.components.netloc\n", "    @property\n\t    def database(self) -> str:\n\t        path = self.components.path\n\t        if path.startswith(\"/\"):\n\t            path = path[1:]\n\t        return unquote(path)\n\t    @property\n\t    def options(self) -> dict:\n\t        if not hasattr(self, \"_options\"):\n\t            self._options = dict(parse_qsl(self.components.query))\n", "        return self._options\n\t    def replace(self, **kwargs: typing.Any) -> \"DatabaseURL\":\n\t        if (\n\t            \"username\" in kwargs\n\t            or \"password\" in kwargs\n\t            or \"hostname\" in kwargs\n\t            or \"port\" in kwargs\n\t        ):\n\t            hostname = kwargs.pop(\"hostname\", self.hostname)\n\t            port = kwargs.pop(\"port\", self.port)\n", "            username = kwargs.pop(\"username\", self.components.username)\n\t            password = kwargs.pop(\"password\", self.components.password)\n\t            netloc = hostname\n\t            if port is not None:\n\t                netloc += f\":{port}\"\n\t            if username is not None:\n\t                userpass = username\n\t                if password is not None:\n\t                    userpass += f\":{password}\"\n\t                netloc = f\"{userpass}@{netloc}\"\n", "            kwargs[\"netloc\"] = netloc\n\t        if \"database\" in kwargs:\n\t            kwargs[\"path\"] = \"/\" + kwargs.pop(\"database\")\n\t        if \"dialect\" in kwargs or \"driver\" in kwargs:\n\t            dialect = kwargs.pop(\"dialect\", self.dialect)\n\t            driver = kwargs.pop(\"driver\", self.driver)\n\t            kwargs[\"scheme\"] = f\"{dialect}+{driver}\" if driver else dialect\n\t        if not kwargs.get(\"netloc\", self.netloc):\n\t            # Using an empty string that evaluates as True means we end up\n\t            # with URLs like `sqlite:///database` instead of `sqlite:/database`\n", "            kwargs[\"netloc\"] = _EmptyNetloc()\n\t        components = self.components._replace(**kwargs)\n\t        return self.__class__(components.geturl())\n\t    @property\n\t    def obscure_password(self) -> str:\n\t        if self.password:\n\t            return self.replace(password=\"********\")._url\n\t        return self._url\n\t    def __str__(self) -> str:\n\t        return self._url\n", "    def __repr__(self) -> str:\n\t        return f\"{self.__class__.__name__}({repr(self.obscure_password)})\"\n\t    def __eq__(self, other: typing.Any) -> bool:\n\t        return str(self) == str(other)\n"]}
{"filename": "databasez/testclient.py", "chunked_list": ["import asyncio\n\timport os\n\timport typing\n\tfrom typing import Any\n\timport nest_asyncio\n\timport sqlalchemy as sa\n\tfrom sqlalchemy.exc import OperationalError, ProgrammingError\n\tfrom sqlalchemy.ext.asyncio import create_async_engine\n\tfrom sqlalchemy_utils.functions.database import _set_url_database, _sqlite_file_exists, make_url\n\tfrom sqlalchemy_utils.functions.orm import quote\n", "from databasez import Database, DatabaseURL\n\tnest_asyncio.apply()\n\tasync def _get_scalar_result(engine: typing.Any, sql: typing.Any) -> Any:\n\t    try:\n\t        async with engine.connect() as conn:\n\t            return await conn.scalar(sql)\n\t    except Exception:\n\t        return False\n\tclass DatabaseTestClient(Database):\n\t    \"\"\"\n", "    Client used only or unit testing.\n\t    This client simply creates a \"test_\" from the database provided in the\n\t    connection.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        url: typing.Union[str, \"DatabaseURL\"],\n\t        *,\n\t        force_rollback: bool = False,\n\t        use_existing: bool = False,\n", "        drop_database: bool = False,\n\t        **options: typing.Any,\n\t    ):\n\t        url = DatabaseURL(url) if isinstance(url, str) else url\n\t        test_database_url = url.replace(database=\"test_\" + url.database)\n\t        self.test_db_url = test_database_url._url\n\t        self.use_existing = use_existing\n\t        asyncio.get_event_loop().run_until_complete(self.setup())\n\t        super().__init__(test_database_url, force_rollback=force_rollback, **options)\n\t        self.drop = drop_database\n", "    async def setup(self) -> None:\n\t        \"\"\"\n\t        Makes sure the database is created if does not exist or use existing\n\t        if needed.\n\t        \"\"\"\n\t        if not self.use_existing:\n\t            if await self.is_database_exist():\n\t                await self.drop_database(self.test_db_url)\n\t            await self.create_database(self.test_db_url)\n\t        else:\n", "            if not self.is_database_exist():\n\t                await self.create_database(self.test_db_url)\n\t    async def is_database_exist(self) -> Any:\n\t        \"\"\"\n\t        Checks if a database exists.\n\t        \"\"\"\n\t        return await self.database_exists(self.test_db_url)\n\t    async def database_exists(self, url: str) -> Any:\n\t        url = make_url(url)\n\t        database = url.database\n", "        dialect_name = url.get_dialect().name\n\t        engine = None\n\t        try:\n\t            if dialect_name == \"postgresql\":\n\t                text = \"SELECT 1 FROM pg_database WHERE datname='%s'\" % database\n\t                for db in (database, \"postgres\", \"template1\", \"template0\", None):\n\t                    url = _set_url_database(url, database=db)\n\t                    engine = create_async_engine(url)\n\t                    try:\n\t                        return bool(await _get_scalar_result(engine, sa.text(text)))\n", "                    except (ProgrammingError, OperationalError):\n\t                        pass\n\t                return False\n\t            elif dialect_name == \"mysql\":\n\t                url = _set_url_database(url, database=None)\n\t                engine = create_async_engine(url)\n\t                text = (\n\t                    \"SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA \"\n\t                    \"WHERE SCHEMA_NAME = '%s'\" % database\n\t                )\n", "                return bool(await _get_scalar_result(engine, sa.text(text)))\n\t            elif dialect_name == \"sqlite\":\n\t                url = _set_url_database(url, database=None)\n\t                engine = create_async_engine(url)\n\t                if database:\n\t                    return database == \":memory:\" or _sqlite_file_exists(database)\n\t                else:\n\t                    # The default SQLAlchemy database is in memory, and :memory: is\n\t                    # not required, thus we should support that use case.\n\t                    return True\n", "            else:\n\t                text = \"SELECT 1\"\n\t                try:\n\t                    engine = create_async_engine(url)\n\t                    return bool(await _get_scalar_result(engine, sa.text(text)))\n\t                except (ProgrammingError, OperationalError):\n\t                    return False\n\t        finally:\n\t            if engine:\n\t                await engine.dispose()\n", "    async def create_database(\n\t        self, url: str, encoding: str = \"utf8\", template: typing.Any = None\n\t    ) -> Any:\n\t        url = make_url(url)\n\t        database = url.database\n\t        dialect_name = url.get_dialect().name\n\t        dialect_driver = url.get_dialect().driver\n\t        if dialect_name == \"postgresql\":\n\t            url = _set_url_database(url, database=\"postgres\")\n\t        elif dialect_name == \"mssql\":\n", "            url = _set_url_database(url, database=\"master\")\n\t        elif dialect_name == \"cockroachdb\":\n\t            url = _set_url_database(url, database=\"defaultdb\")\n\t        elif not dialect_name == \"sqlite\":\n\t            url = _set_url_database(url, database=None)\n\t        if (dialect_name == \"mssql\" and dialect_driver in {\"pymssql\", \"pyodbc\"}) or (\n\t            dialect_name == \"postgresql\"\n\t            and dialect_driver in {\"asyncpg\", \"pg8000\", \"psycopg2\", \"psycopg2cffi\"}\n\t        ):\n\t            engine = create_async_engine(url, isolation_level=\"AUTOCOMMIT\")\n", "        else:\n\t            engine = create_async_engine(url)\n\t        if dialect_name == \"postgresql\":\n\t            if not template:\n\t                template = \"template1\"\n\t            async with engine.begin() as conn:\n\t                text = \"CREATE DATABASE {} ENCODING '{}' TEMPLATE {}\".format(\n\t                    quote(conn, database), encoding, quote(conn, template)\n\t                )\n\t                await conn.execute(sa.text(text))\n", "        elif dialect_name == \"mysql\":\n\t            async with engine.begin() as conn:\n\t                text = \"CREATE DATABASE {} CHARACTER SET = '{}'\".format(\n\t                    quote(conn, database), encoding\n\t                )\n\t                await conn.execute(sa.text(text))\n\t        elif dialect_name == \"sqlite\" and database != \":memory:\":\n\t            if database:\n\t                async with engine.begin() as conn:\n\t                    await conn.execute(sa.text(\"CREATE TABLE DB(id int)\"))\n", "                    await conn.execute(sa.text(\"DROP TABLE DB\"))\n\t        else:\n\t            async with engine.begin() as conn:\n\t                text = f\"CREATE DATABASE {quote(conn, database)}\"\n\t                await conn.execute(sa.text(text))\n\t        await engine.dispose()\n\t    async def drop_database(self, url: str) -> Any:\n\t        url = make_url(url)\n\t        database = url.database\n\t        dialect_name = url.get_dialect().name\n", "        dialect_driver = url.get_dialect().driver\n\t        if dialect_name == \"postgresql\":\n\t            url = _set_url_database(url, database=\"postgres\")\n\t        elif dialect_name == \"mssql\":\n\t            url = _set_url_database(url, database=\"master\")\n\t        elif dialect_name == \"cockroachdb\":\n\t            url = _set_url_database(url, database=\"defaultdb\")\n\t        elif not dialect_name == \"sqlite\":\n\t            url = _set_url_database(url, database=None)\n\t        if dialect_name == \"mssql\" and dialect_driver in {\"pymssql\", \"pyodbc\"}:\n", "            engine = create_async_engine(url, connect_args={\"autocommit\": True})\n\t        elif dialect_name == \"postgresql\" and dialect_driver in {\n\t            \"asyncpg\",\n\t            \"pg8000\",\n\t            \"psycopg2\",\n\t            \"psycopg2cffi\",\n\t        }:\n\t            engine = create_async_engine(url, isolation_level=\"AUTOCOMMIT\")\n\t        else:\n\t            engine = create_async_engine(url)\n", "        if dialect_name == \"sqlite\" and database != \":memory:\":\n\t            if database:\n\t                os.remove(database)\n\t        elif dialect_name == \"postgresql\":\n\t            async with engine.begin() as conn:\n\t                # Disconnect all users from the database we are dropping.\n\t                version = conn.dialect.server_version_info\n\t                pid_column = \"pid\" if (version >= (9, 2)) else \"procpid\"\n\t                text = \"\"\"\n\t                SELECT pg_terminate_backend(pg_stat_activity.{pid_column})\n", "                FROM pg_stat_activity\n\t                WHERE pg_stat_activity.datname = '{database}'\n\t                AND {pid_column} <> pg_backend_pid();\n\t                \"\"\".format(\n\t                    pid_column=pid_column, database=database\n\t                )\n\t                await conn.execute(sa.text(text))\n\t                # Drop the database.\n\t                text = f\"DROP DATABASE {quote(conn, database)}\"\n\t                await conn.execute(sa.text(text))\n", "        else:\n\t            async with engine.begin() as conn:\n\t                text = f\"DROP DATABASE {quote(conn, database)}\"\n\t                await conn.execute(sa.text(text))\n\t        await engine.dispose()\n\t    async def disconnect(self) -> None:\n\t        if self.drop:\n\t            await self.drop_database(self.test_db_url)\n\t        await super().disconnect()\n"]}
{"filename": "databasez/interfaces.py", "chunked_list": ["import typing\n\tfrom collections.abc import Sequence\n\tfrom sqlalchemy.sql import ClauseElement\n\tclass DatabaseBackend:\n\t    async def connect(self) -> None:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def disconnect(self) -> None:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    def connection(self) -> \"ConnectionBackend\":\n\t        raise NotImplementedError()  # pragma: no cover\n", "class ConnectionBackend:\n\t    async def acquire(self) -> None:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def release(self) -> None:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def fetch_all(self, query: ClauseElement) -> typing.List[\"Record\"]:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def fetch_one(self, query: ClauseElement) -> typing.Optional[\"Record\"]:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def fetch_val(self, query: ClauseElement, column: typing.Any = 0) -> typing.Any:\n", "        row = await self.fetch_one(query)\n\t        return None if row is None else row[column]\n\t    async def execute(self, query: ClauseElement) -> typing.Any:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def iterate(self, query: ClauseElement) -> typing.AsyncGenerator[typing.Mapping, None]:\n\t        raise NotImplementedError()  # pragma: no cover\n\t        # mypy needs async iterators to contain a `yield`\n\t        # https://github.com/python/mypy/issues/5385#issuecomment-407281656\n", "        yield True  # pragma: no cover\n\t    def transaction(self) -> \"TransactionBackend\":\n\t        raise NotImplementedError()  # pragma: no cover\n\t    @property\n\t    def raw_connection(self) -> typing.Any:\n\t        raise NotImplementedError()  # pragma: no cover\n\tclass TransactionBackend:\n\t    async def start(\n\t        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n\t    ) -> None:\n", "        raise NotImplementedError()  # pragma: no cover\n\t    async def commit(self) -> None:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    async def rollback(self) -> None:\n\t        raise NotImplementedError()  # pragma: no cover\n\tclass Record(Sequence):\n\t    @property\n\t    def _mapping(self) -> typing.Mapping:\n\t        raise NotImplementedError()  # pragma: no cover\n\t    def __getitem__(self, key: typing.Any) -> typing.Any:\n", "        raise NotImplementedError()  # pragma: no cover\n"]}
{"filename": "databasez/backends/sqlite.py", "chunked_list": ["import logging\n\timport typing\n\timport uuid\n\timport aiosqlite\n\tfrom sqlalchemy.dialects.sqlite import pysqlite\n\tfrom sqlalchemy.engine.cursor import CursorResultMetaData\n\tfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\n\tfrom sqlalchemy.sql import ClauseElement\n\tfrom sqlalchemy.sql.ddl import DDLElement\n\tfrom databasez.backends.common.records import Record, Row, create_column_maps\n", "from databasez.core import LOG_EXTRA, DatabaseURL\n\tfrom databasez.interfaces import ConnectionBackend, DatabaseBackend, TransactionBackend\n\tlogger = logging.getLogger(\"databasez\")\n\tclass SQLiteBackend(DatabaseBackend):\n\t    def __init__(\n\t        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n\t    ) -> None:\n\t        self._database_url = DatabaseURL(database_url)\n\t        self._options = options\n\t        self._dialect = pysqlite.dialect(paramstyle=\"qmark\")\n", "        # aiosqlite does not support decimals\n\t        self._dialect.supports_native_decimal = False\n\t        self._pool = SQLitePool(self._database_url, **self._options)\n\t    async def connect(self) -> None:\n\t        ...\n\t    async def disconnect(self) -> None:\n\t        ...\n\t    def connection(self) -> \"SQLiteConnection\":\n\t        return SQLiteConnection(self._pool, self._dialect)\n\tclass SQLitePool:\n", "    def __init__(self, url: DatabaseURL, **options: typing.Any) -> None:\n\t        self._url = url\n\t        self._options = options\n\t    async def acquire(self) -> aiosqlite.Connection:\n\t        connection = aiosqlite.connect(\n\t            database=self._url.database, isolation_level=None, **self._options\n\t        )\n\t        await connection.__aenter__()\n\t        return connection\n\t    async def release(self, connection: aiosqlite.Connection) -> None:\n", "        await connection.__aexit__(None, None, None)\n\tclass CompilationContext:\n\t    def __init__(self, context: ExecutionContext):\n\t        self.context = context\n\tclass SQLiteConnection(ConnectionBackend):\n\t    def __init__(self, pool: SQLitePool, dialect: Dialect):\n\t        self._pool = pool\n\t        self._dialect = dialect\n\t        self._connection: typing.Optional[aiosqlite.Connection] = None\n\t    async def acquire(self) -> None:\n", "        assert self._connection is None, \"Connection is already acquired\"\n\t        self._connection = await self._pool.acquire()\n\t    async def release(self) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        connection, self._connection = self._connection, None\n\t        await self._pool.release(connection)\n\t    async def fetch_all(self, query: ClauseElement) -> typing.List[Record]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n", "        dialect = self._dialect\n\t        async with self._connection.execute(query_str, args) as cursor:\n\t            rows = await cursor.fetchall()\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            rows = [\n\t                Row(\n\t                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n", "                )\n\t                for row in rows\n\t            ]\n\t            return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\t    async def fetch_one(self, query: ClauseElement) -> typing.Optional[Record]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        async with self._connection.execute(query_str, args) as cursor:\n", "            row = await cursor.fetchone()\n\t            if row is None:\n\t                return None\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            row = Row(\n\t                metadata,\n\t                metadata._effective_processors,\n\t                metadata._key_to_index,\n\t                row,\n\t            )\n", "            return Record(row, result_columns, dialect, column_maps)\n\t    async def execute(self, query: ClauseElement) -> typing.Any:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        async with self._connection.cursor() as cursor:\n\t            await cursor.execute(query_str, args)\n\t            if cursor.lastrowid == 0:\n\t                return cursor.rowcount\n\t            return cursor.lastrowid\n\t    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n", "        assert self._connection is not None, \"Connection is not acquired\"\n\t        for single_query in queries:\n\t            await self.execute(single_query)\n\t    async def iterate(self, query: ClauseElement) -> typing.AsyncGenerator[typing.Any, None]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        async with self._connection.execute(query_str, args) as cursor:\n\t            metadata = CursorResultMetaData(context, cursor.description)\n", "            async for row in cursor:\n\t                record = Row(\n\t                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n\t                )\n\t                yield Record(record, result_columns, dialect, column_maps)\n\t    def transaction(self) -> TransactionBackend:\n\t        return SQLiteTransaction(self)\n", "    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n\t        compiled = query.compile(\n\t            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n\t        )\n\t        execution_context = self._dialect.execution_ctx_cls()\n\t        execution_context.dialect = self._dialect\n\t        args = []\n\t        result_map = None\n\t        if not isinstance(query, DDLElement):\n\t            compiled_params = sorted(compiled.params.items())\n", "            params = compiled.construct_params()\n\t            for key in compiled.positiontup:\n\t                raw_val = params[key]\n\t                if key in compiled._bind_processors:\n\t                    val = compiled._bind_processors[key](raw_val)\n\t                else:\n\t                    val = raw_val\n\t                args.append(val)\n\t            execution_context.result_column_struct = (\n\t                compiled._result_columns,\n", "                compiled._ordered_columns,\n\t                compiled._textual_ordered_columns,\n\t                compiled._ad_hoc_textual,\n\t                compiled._loose_column_name_matching,\n\t            )\n\t            mapping = {key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)}\n\t            compiled_query = compiled.string % mapping\n\t            result_map = compiled._result_columns\n\t        else:\n\t            compiled_query = compiled.string\n", "        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n\t        logger.debug(\"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA)\n\t        return compiled.string, args, result_map, CompilationContext(execution_context)\n\t    @property\n\t    def raw_connection(self) -> aiosqlite.core.Connection:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        return self._connection\n\tclass SQLiteTransaction(TransactionBackend):\n\t    def __init__(self, connection: SQLiteConnection):\n\t        self._connection = connection\n", "        self._is_root = False\n\t        self._savepoint_name = \"\"\n\t    async def start(\n\t        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n\t    ) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        self._is_root = is_root\n\t        if self._is_root:\n\t            async with self._connection._connection.execute(\"BEGIN\") as cursor:\n\t                await cursor.close()\n", "        else:\n\t            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n\t            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n\t            async with self._connection._connection.execute(\n\t                f\"SAVEPOINT {self._savepoint_name}\"\n\t            ) as cursor:\n\t                await cursor.close()\n\t    async def commit(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        if self._is_root:\n", "            async with self._connection._connection.execute(\"COMMIT\") as cursor:\n\t                await cursor.close()\n\t        else:\n\t            async with self._connection._connection.execute(\n\t                f\"RELEASE SAVEPOINT {self._savepoint_name}\"\n\t            ) as cursor:\n\t                await cursor.close()\n\t    async def rollback(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        if self._is_root:\n", "            async with self._connection._connection.execute(\"ROLLBACK\") as cursor:\n\t                await cursor.close()\n\t        else:\n\t            async with self._connection._connection.execute(\n\t                f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\"\n\t            ) as cursor:\n\t                await cursor.close()\n"]}
{"filename": "databasez/backends/postgres.py", "chunked_list": ["import logging\n\timport typing\n\timport asyncpg\n\tfrom sqlalchemy.engine.interfaces import Dialect\n\tfrom sqlalchemy.sql import ClauseElement\n\tfrom sqlalchemy.sql.ddl import DDLElement\n\tfrom databasez.backends.common.records import Record, create_column_maps\n\tfrom databasez.backends.dialects.psycopg import dialect as psycopg_dialect\n\tfrom databasez.core import LOG_EXTRA, DatabaseURL\n\tfrom databasez.interfaces import ConnectionBackend, DatabaseBackend\n", "from databasez.interfaces import Record as RecordInterface\n\tfrom databasez.interfaces import TransactionBackend\n\tlogger = logging.getLogger(\"databasez\")\n\tclass PostgresBackend(DatabaseBackend):\n\t    def __init__(\n\t        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n\t    ) -> None:\n\t        self._database_url = DatabaseURL(database_url)\n\t        self._options = options\n\t        self._dialect = self._get_dialect()\n", "        self._pool = None\n\t    def _get_dialect(self) -> Dialect:\n\t        dialect = psycopg_dialect(paramstyle=\"pyformat\")\n\t        dialect.implicit_returning = True\n\t        dialect.supports_native_enum = True\n\t        dialect.supports_smallserial = True  # 9.2+\n\t        dialect._backslash_escapes = False\n\t        dialect.supports_sane_multi_rowcount = True  # psycopg 2.0.9+\n\t        dialect._has_native_hstore = True\n\t        dialect.supports_native_decimal = True\n", "        return dialect\n\t    def _get_connection_kwargs(self) -> dict:\n\t        url_options = self._database_url.options\n\t        kwargs: typing.Dict[str, typing.Any] = {}\n\t        min_size = url_options.get(\"min_size\")\n\t        max_size = url_options.get(\"max_size\")\n\t        ssl = url_options.get(\"ssl\")\n\t        if min_size is not None:\n\t            kwargs[\"min_size\"] = int(min_size)\n\t        if max_size is not None:\n", "            kwargs[\"max_size\"] = int(max_size)\n\t        if ssl is not None:\n\t            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n\t        kwargs.update(self._options)\n\t        return kwargs\n\t    async def connect(self) -> None:\n\t        assert self._pool is None, \"DatabaseBackend is already running\"\n\t        kwargs = {\n\t            \"host\": self._database_url.hostname,\n\t            \"port\": self._database_url.port,\n", "            \"user\": self._database_url.username,\n\t            \"password\": self._database_url.password,\n\t            \"database\": self._database_url.database,\n\t        }\n\t        kwargs.update(self._get_connection_kwargs())\n\t        self._pool = await asyncpg.create_pool(**kwargs)\n\t    async def disconnect(self) -> None:\n\t        assert self._pool is not None, \"DatabaseBackend is not running\"\n\t        await self._pool.close()\n\t        self._pool = None\n", "    def connection(self) -> \"PostgresConnection\":\n\t        return PostgresConnection(self, self._dialect)\n\tclass PostgresConnection(ConnectionBackend):\n\t    def __init__(self, database: PostgresBackend, dialect: Dialect):\n\t        self._database = database\n\t        self._dialect = dialect\n\t        self._connection: typing.Optional[asyncpg.connection.Connection] = None\n\t    async def acquire(self) -> None:\n\t        assert self._connection is None, \"Connection is already acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n", "        self._connection = await self._database._pool.acquire()\n\t    async def release(self) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        connection, self._connection = self._connection, None\n\t        self._connection = await self._database._pool.release(connection)\n\t    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns = self._compile(query)\n\t        rows = await self._connection.fetch(query_str, *args)\n", "        dialect = self._dialect\n\t        column_maps = create_column_maps(result_columns)\n\t        return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\t    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns = self._compile(query)\n\t        row = await self._connection.fetchrow(query_str, *args)\n\t        if row is None:\n\t            return None\n\t        return Record(\n", "            row,\n\t            result_columns,\n\t            self._dialect,\n\t            create_column_maps(result_columns),\n\t        )\n\t    async def fetch_val(self, query: ClauseElement, column: typing.Any = 0) -> typing.Any:\n\t        # we are not calling self._connection.fetchval here because\n\t        # it does not convert all the types, e.g. JSON stays string\n\t        # instead of an object\n\t        # see also:\n", "        # https://github.com/dymmond/databasez/pull/131\n\t        # https://github.com/dymmond/databasez/pull/132\n\t        # https://github.com/dymmond/databasez/pull/246\n\t        row = await self.fetch_one(query)\n\t        if row is None:\n\t            return None\n\t        return row[column]\n\t    async def execute(self, query: ClauseElement) -> typing.Any:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, _ = self._compile(query)\n", "        return await self._connection.fetchval(query_str, *args)\n\t    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        # asyncpg uses prepared statements under the hood, so we just\n\t        # loop through multiple executes here, which should all end up\n\t        # using the same prepared statement.\n\t        for single_query in queries:\n\t            single_query, args, _ = self._compile(single_query)\n\t            await self._connection.execute(single_query, *args)\n\t    async def iterate(self, query: ClauseElement) -> typing.AsyncGenerator[typing.Any, None]:\n", "        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        async for row in self._connection.cursor(query_str, *args):\n\t            yield Record(row, result_columns, self._dialect, column_maps)\n\t    def transaction(self) -> TransactionBackend:\n\t        return PostgresTransaction(connection=self)\n\t    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n\t        compiled = query.compile(\n\t            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n", "        )\n\t        if not isinstance(query, DDLElement):\n\t            compiled_params = sorted(compiled.params.items())\n\t            mapping = {key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)}\n\t            compiled_query = compiled.string % mapping\n\t            processors = compiled._bind_processors\n\t            args = [\n\t                processors[key](val) if key in processors else val for key, val in compiled_params\n\t            ]\n\t            result_map = compiled._result_columns\n", "        else:\n\t            compiled_query = compiled.string\n\t            args = []\n\t            result_map = None\n\t        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n\t        logger.debug(\"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA)\n\t        return compiled_query, args, result_map\n\t    @property\n\t    def raw_connection(self) -> asyncpg.connection.Connection:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n", "        return self._connection\n\tclass PostgresTransaction(TransactionBackend):\n\t    def __init__(self, connection: PostgresConnection):\n\t        self._connection = connection\n\t        self._transaction: typing.Optional[asyncpg.transaction.Transaction] = None\n\t    async def start(\n\t        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n\t    ) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        self._transaction = self._connection._connection.transaction(**extra_options)\n", "        await self._transaction.start()\n\t    async def commit(self) -> None:\n\t        assert self._transaction is not None\n\t        await self._transaction.commit()\n\t    async def rollback(self) -> None:\n\t        assert self._transaction is not None\n\t        await self._transaction.rollback()\n"]}
{"filename": "databasez/backends/mysql.py", "chunked_list": ["import getpass\n\timport logging\n\timport typing\n\timport uuid\n\timport aiomysql\n\tfrom sqlalchemy.dialects.mysql import pymysql\n\tfrom sqlalchemy.engine.cursor import CursorResultMetaData\n\tfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\n\tfrom sqlalchemy.sql import ClauseElement\n\tfrom sqlalchemy.sql.ddl import DDLElement\n", "from databasez.backends.common.records import Record, Row, create_column_maps\n\tfrom databasez.core import LOG_EXTRA, DatabaseURL\n\tfrom databasez.interfaces import ConnectionBackend, DatabaseBackend\n\tfrom databasez.interfaces import Record as RecordInterface\n\tfrom databasez.interfaces import TransactionBackend\n\tlogger = logging.getLogger(\"databasez\")\n\tclass MySQLBackend(DatabaseBackend):\n\t    def __init__(\n\t        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n\t    ) -> None:\n", "        self._database_url = DatabaseURL(database_url)\n\t        self._options = options\n\t        self._dialect = pymysql.dialect(paramstyle=\"pyformat\")\n\t        self._dialect.supports_native_decimal = True\n\t        self._pool = None\n\t    def _get_connection_kwargs(self) -> dict:\n\t        url_options = self._database_url.options\n\t        kwargs = {}\n\t        min_size = url_options.get(\"min_size\")\n\t        max_size = url_options.get(\"max_size\")\n", "        pool_recycle = url_options.get(\"pool_recycle\")\n\t        unix_socket = url_options.get(\"unix_socket\")\n\t        ssl = url_options.get(\"ssl\")\n\t        if min_size is not None:\n\t            kwargs[\"minsize\"] = int(min_size)\n\t        if max_size is not None:\n\t            kwargs[\"maxsize\"] = int(max_size)\n\t        if unix_socket is not None:\n\t            kwargs[\"unix_socket\"] = unix_socket\n\t        if pool_recycle is not None:\n", "            kwargs[\"pool_recycle\"] = int(pool_recycle)\n\t        if ssl is not None:\n\t            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n\t        for key, value in self._options.items():\n\t            # Coerce 'min_size' and 'max_size' for consistency.\n\t            if key == \"min_size\":\n\t                key = \"minsize\"\n\t            elif key == \"max_size\":\n\t                key = \"maxsize\"\n\t            kwargs[key] = value\n", "        return kwargs\n\t    async def connect(self) -> None:\n\t        assert self._pool is None, \"DatabaseBackend is already running\"\n\t        kwargs = self._get_connection_kwargs()\n\t        self._pool = await aiomysql.create_pool(\n\t            host=self._database_url.hostname,\n\t            port=self._database_url.port or 3306,\n\t            user=self._database_url.username or getpass.getuser(),\n\t            password=self._database_url.password,\n\t            db=self._database_url.database,\n", "            autocommit=True,\n\t            **kwargs,\n\t        )\n\t    async def disconnect(self) -> None:\n\t        assert self._pool is not None, \"DatabaseBackend is not running\"\n\t        self._pool.close()\n\t        pool, self._pool = self._pool, None\n\t        await pool.wait_closed()\n\t    def connection(self) -> \"MySQLConnection\":\n\t        return MySQLConnection(self, self._dialect)\n", "class CompilationContext:\n\t    def __init__(self, context: ExecutionContext):\n\t        self.context = context\n\tclass MySQLConnection(ConnectionBackend):\n\t    def __init__(self, database: MySQLBackend, dialect: Dialect):\n\t        self._database = database\n\t        self._dialect = dialect\n\t        self._connection: typing.Optional[aiomysql.Connection] = None\n\t    async def acquire(self) -> None:\n\t        assert self._connection is None, \"Connection is already acquired\"\n", "        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        self._connection = await self._database._pool.acquire()\n\t    async def release(self) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        await self._database._pool.release(self._connection)\n\t        self._connection = None\n\t    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n", "        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            await cursor.execute(query_str, args)\n\t            rows = await cursor.fetchall()\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            rows = [\n\t                Row(\n\t                    metadata,\n", "                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n\t                )\n\t                for row in rows\n\t            ]\n\t            return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\t        finally:\n\t            await cursor.close()\n\t    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n", "        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            await cursor.execute(query_str, args)\n\t            row = await cursor.fetchone()\n\t            if row is None:\n\t                return None\n", "            metadata = CursorResultMetaData(context, cursor.description)\n\t            row = Row(\n\t                metadata,\n\t                metadata._effective_processors,\n\t                metadata._key_to_index,\n\t                row,\n\t            )\n\t            return Record(row, result_columns, dialect, column_maps)\n\t        finally:\n\t            await cursor.close()\n", "    async def execute(self, query: ClauseElement) -> typing.Any:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, _, _ = self._compile(query)\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            await cursor.execute(query_str, args)\n\t            if cursor.lastrowid == 0:\n\t                return cursor.rowcount\n\t            return cursor.lastrowid\n\t        finally:\n", "            await cursor.close()\n\t    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            for single_query in queries:\n\t                single_query, args, _, _ = self._compile(single_query)\n\t                await cursor.execute(single_query, args)\n\t        finally:\n\t            await cursor.close()\n", "    async def iterate(self, query: ClauseElement) -> typing.AsyncGenerator[typing.Any, None]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            await cursor.execute(query_str, args)\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            async for row in cursor:\n", "                record = Row(\n\t                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n\t                )\n\t                yield Record(record, result_columns, dialect, column_maps)\n\t        finally:\n\t            await cursor.close()\n\t    def transaction(self) -> TransactionBackend:\n", "        return MySQLTransaction(self)\n\t    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n\t        compiled = query.compile(\n\t            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n\t        )\n\t        execution_context = self._dialect.execution_ctx_cls()\n\t        execution_context.dialect = self._dialect\n\t        if not isinstance(query, DDLElement):\n\t            compiled_params = sorted(compiled.params.items())\n\t            args = compiled.construct_params()\n", "            for key, val in args.items():\n\t                if key in compiled._bind_processors:\n\t                    args[key] = compiled._bind_processors[key](val)\n\t            execution_context.result_column_struct = (\n\t                compiled._result_columns,\n\t                compiled._ordered_columns,\n\t                compiled._textual_ordered_columns,\n\t                compiled._ad_hoc_textual,\n\t                compiled._loose_column_name_matching,\n\t            )\n", "            mapping = {key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)}\n\t            compiled_query = compiled.string % mapping\n\t            result_map = compiled._result_columns\n\t        else:\n\t            args = {}\n\t            result_map = None\n\t            compiled_query = compiled.string\n\t        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n\t        logger.debug(\"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA)\n\t        return compiled.string, args, result_map, CompilationContext(execution_context)\n", "    @property\n\t    def raw_connection(self) -> aiomysql.connection.Connection:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        return self._connection\n\tclass MySQLTransaction(TransactionBackend):\n\t    def __init__(self, connection: MySQLConnection):\n\t        self._connection = connection\n\t        self._is_root = False\n\t        self._savepoint_name = \"\"\n\t    async def start(\n", "        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n\t    ) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        self._is_root = is_root\n\t        if self._is_root:\n\t            await self._connection._connection.begin()\n\t        else:\n\t            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n\t            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n\t            cursor = await self._connection._connection.cursor()\n", "            try:\n\t                await cursor.execute(f\"SAVEPOINT {self._savepoint_name}\")\n\t            finally:\n\t                await cursor.close()\n\t    async def commit(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        if self._is_root:\n\t            await self._connection._connection.commit()\n\t        else:\n\t            cursor = await self._connection._connection.cursor()\n", "            try:\n\t                await cursor.execute(f\"RELEASE SAVEPOINT {self._savepoint_name}\")\n\t            finally:\n\t                await cursor.close()\n\t    async def rollback(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        if self._is_root:\n\t            await self._connection._connection.rollback()\n\t        else:\n\t            cursor = await self._connection._connection.cursor()\n", "            try:\n\t                await cursor.execute(f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\")\n\t            finally:\n\t                await cursor.close()\n"]}
{"filename": "databasez/backends/asyncmy.py", "chunked_list": ["import getpass\n\timport logging\n\timport typing\n\timport uuid\n\timport asyncmy\n\tfrom sqlalchemy.dialects.mysql import pymysql\n\tfrom sqlalchemy.engine.cursor import CursorResultMetaData\n\tfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\n\tfrom sqlalchemy.sql import ClauseElement\n\tfrom sqlalchemy.sql.ddl import DDLElement\n", "from databasez.backends.common.records import Record, Row, create_column_maps\n\tfrom databasez.core import LOG_EXTRA, DatabaseURL\n\tfrom databasez.interfaces import ConnectionBackend, DatabaseBackend\n\tfrom databasez.interfaces import Record as RecordInterface\n\tfrom databasez.interfaces import TransactionBackend\n\tlogger = logging.getLogger(\"databasez\")\n\tclass AsyncMyBackend(DatabaseBackend):\n\t    def __init__(\n\t        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n\t    ) -> None:\n", "        self._database_url = DatabaseURL(database_url)\n\t        self._options = options\n\t        self._dialect = pymysql.dialect(paramstyle=\"pyformat\")\n\t        self._dialect.supports_native_decimal = True\n\t        self._pool = None\n\t    def _get_connection_kwargs(self) -> dict:\n\t        url_options = self._database_url.options\n\t        kwargs = {}\n\t        min_size = url_options.get(\"min_size\")\n\t        max_size = url_options.get(\"max_size\")\n", "        pool_recycle = url_options.get(\"pool_recycle\")\n\t        unix_socket = url_options.get(\"unix_socket\")\n\t        ssl = url_options.get(\"ssl\")\n\t        if min_size is not None:\n\t            kwargs[\"minsize\"] = int(min_size)\n\t        if max_size is not None:\n\t            kwargs[\"maxsize\"] = int(max_size)\n\t        if unix_socket is not None:\n\t            kwargs[\"unix_socket\"] = unix_socket\n\t        if pool_recycle is not None:\n", "            kwargs[\"pool_recycle\"] = int(pool_recycle)\n\t        if ssl is not None:\n\t            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n\t        for key, value in self._options.items():\n\t            # Coerce 'min_size' and 'max_size' for consistency.\n\t            if key == \"min_size\":\n\t                key = \"minsize\"\n\t            elif key == \"max_size\":\n\t                key = \"maxsize\"\n\t            kwargs[key] = value\n", "        return kwargs\n\t    async def connect(self) -> None:\n\t        assert self._pool is None, \"DatabaseBackend is already running\"\n\t        kwargs = self._get_connection_kwargs()\n\t        self._pool = await asyncmy.create_pool(\n\t            host=self._database_url.hostname,\n\t            port=self._database_url.port or 3306,\n\t            user=self._database_url.username or getpass.getuser(),\n\t            password=self._database_url.password,\n\t            db=self._database_url.database,\n", "            autocommit=True,\n\t            **kwargs,\n\t        )\n\t    async def disconnect(self) -> None:\n\t        assert self._pool is not None, \"DatabaseBackend is not running\"\n\t        self._pool.close()\n\t        pool, self._pool = self._pool, None\n\t        await pool.wait_closed()\n\t    def connection(self) -> \"AsyncMyConnection\":\n\t        return AsyncMyConnection(self, self._dialect)\n", "class CompilationContext:\n\t    def __init__(self, context: ExecutionContext):\n\t        self.context = context\n\tclass AsyncMyConnection(ConnectionBackend):\n\t    def __init__(self, database: AsyncMyBackend, dialect: Dialect):\n\t        self._database = database\n\t        self._dialect = dialect\n\t        self._connection: typing.Optional[asyncmy.Connection] = None\n\t    async def acquire(self) -> None:\n\t        assert self._connection is None, \"Connection is already acquired\"\n", "        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        self._connection = await self._database._pool.acquire()\n\t    async def release(self) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        await self._database._pool.release(self._connection)\n\t        self._connection = None\n\t    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n", "        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        async with self._connection.cursor() as cursor:\n\t            try:\n\t                await cursor.execute(query_str, args)\n\t                rows = await cursor.fetchall()\n\t                metadata = CursorResultMetaData(context, cursor.description)\n\t                rows = [\n\t                    Row(\n\t                        metadata,\n", "                        metadata._effective_processors,\n\t                        metadata._key_to_index,\n\t                        row,\n\t                    )\n\t                    for row in rows\n\t                ]\n\t                return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\t            finally:\n\t                await cursor.close()\n\t    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n", "        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        async with self._connection.cursor() as cursor:\n\t            try:\n\t                await cursor.execute(query_str, args)\n\t                row = await cursor.fetchone()\n\t                if row is None:\n\t                    return None\n", "                metadata = CursorResultMetaData(context, cursor.description)\n\t                row = Row(\n\t                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n\t                )\n\t                return Record(row, result_columns, dialect, column_maps)\n\t            finally:\n\t                await cursor.close()\n", "    async def execute(self, query: ClauseElement) -> typing.Any:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, _, _ = self._compile(query)\n\t        async with self._connection.cursor() as cursor:\n\t            try:\n\t                await cursor.execute(query_str, args)\n\t                if cursor.lastrowid == 0:\n\t                    return cursor.rowcount\n\t                return cursor.lastrowid\n\t            finally:\n", "                await cursor.close()\n\t    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        async with self._connection.cursor() as cursor:\n\t            try:\n\t                for single_query in queries:\n\t                    single_query, args, _, _ = self._compile(single_query)\n\t                    await cursor.execute(single_query, args)\n\t            finally:\n\t                await cursor.close()\n", "    async def iterate(self, query: ClauseElement) -> typing.AsyncGenerator[typing.Any, None]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        async with self._connection.cursor() as cursor:\n\t            try:\n\t                await cursor.execute(query_str, args)\n\t                metadata = CursorResultMetaData(context, cursor.description)\n\t                async for row in cursor:\n", "                    record = Row(\n\t                        metadata,\n\t                        metadata._effective_processors,\n\t                        metadata._key_to_index,\n\t                        row,\n\t                    )\n\t                    yield Record(record, result_columns, dialect, column_maps)\n\t            finally:\n\t                await cursor.close()\n\t    def transaction(self) -> TransactionBackend:\n", "        return AsyncMyTransaction(self)\n\t    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n\t        compiled = query.compile(\n\t            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n\t        )\n\t        execution_context = self._dialect.execution_ctx_cls()\n\t        execution_context.dialect = self._dialect\n\t        if not isinstance(query, DDLElement):\n\t            compiled_params = sorted(compiled.params.items())\n\t            args = compiled.construct_params()\n", "            for key, val in args.items():\n\t                if key in compiled._bind_processors:\n\t                    args[key] = compiled._bind_processors[key](val)\n\t            execution_context.result_column_struct = (\n\t                compiled._result_columns,\n\t                compiled._ordered_columns,\n\t                compiled._textual_ordered_columns,\n\t                compiled._ad_hoc_textual,\n\t                compiled._loose_column_name_matching,\n\t            )\n", "            mapping = {key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)}\n\t            compiled_query = compiled.string % mapping\n\t            result_map = compiled._result_columns\n\t        else:\n\t            args = {}\n\t            result_map = None\n\t            compiled_query = compiled.string\n\t        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n\t        logger.debug(\"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA)\n\t        return compiled.string, args, result_map, CompilationContext(execution_context)\n", "    @property\n\t    def raw_connection(self) -> asyncmy.connection.Connection:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        return self._connection\n\tclass AsyncMyTransaction(TransactionBackend):\n\t    def __init__(self, connection: AsyncMyConnection):\n\t        self._connection = connection\n\t        self._is_root = False\n\t        self._savepoint_name = \"\"\n\t    async def start(\n", "        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n\t    ) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        self._is_root = is_root\n\t        if self._is_root:\n\t            await self._connection._connection.begin()\n\t        else:\n\t            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n\t            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n\t            async with self._connection._connection.cursor() as cursor:\n", "                try:\n\t                    await cursor.execute(f\"SAVEPOINT {self._savepoint_name}\")\n\t                finally:\n\t                    await cursor.close()\n\t    async def commit(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        if self._is_root:\n\t            await self._connection._connection.commit()\n\t        else:\n\t            async with self._connection._connection.cursor() as cursor:\n", "                try:\n\t                    await cursor.execute(f\"RELEASE SAVEPOINT {self._savepoint_name}\")\n\t                finally:\n\t                    await cursor.close()\n\t    async def rollback(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        if self._is_root:\n\t            await self._connection._connection.rollback()\n\t        else:\n\t            async with self._connection._connection.cursor() as cursor:\n", "                try:\n\t                    await cursor.execute(f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\")\n\t                finally:\n\t                    await cursor.close()\n"]}
{"filename": "databasez/backends/mssql.py", "chunked_list": ["import getpass\n\timport logging\n\timport typing\n\timport uuid\n\timport aioodbc\n\timport pyodbc as ext_pyodbc\n\tfrom sqlalchemy.dialects.mssql import pyodbc\n\tfrom sqlalchemy.engine.cursor import CursorResultMetaData\n\tfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\n\tfrom sqlalchemy.sql import ClauseElement\n", "from sqlalchemy.sql.ddl import DDLElement\n\tfrom databasez.backends.common.records import Record, Row, create_column_maps\n\tfrom databasez.core import LOG_EXTRA, DatabaseURL\n\tfrom databasez.interfaces import ConnectionBackend, DatabaseBackend\n\tfrom databasez.interfaces import Record as RecordInterface\n\tfrom databasez.interfaces import TransactionBackend\n\tlogger = logging.getLogger(\"databasez\")\n\tclass MSSQLBackend(DatabaseBackend):\n\t    def __init__(\n\t        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n", "    ) -> None:\n\t        self._database_url = DatabaseURL(database_url)\n\t        self._options = options\n\t        self._dialect = pyodbc.dialect(paramstyle=\"pyformat\")\n\t        self._dialect.supports_native_decimal = True\n\t        self._pool: aioodbc.Pool = None\n\t    def _get_connection_kwargs(self) -> dict:\n\t        url_options = self._database_url.options\n\t        kwargs = {}\n\t        min_size = url_options.get(\"min_size\")\n", "        max_size = url_options.get(\"max_size\")\n\t        pool_recycle = url_options.get(\"pool_recycle\")\n\t        ssl = url_options.get(\"ssl\")\n\t        driver = url_options.get(\"driver\")\n\t        timeout = url_options.get(\"connection_timeout\", 30)\n\t        trusted_connection = url_options.get(\"trusted_connection\", \"no\")\n\t        assert driver is not None, \"The driver must be specified\"\n\t        if min_size is not None:\n\t            kwargs[\"minsize\"] = int(min_size)\n\t        if max_size is not None:\n", "            kwargs[\"maxsize\"] = int(max_size)\n\t        if pool_recycle is not None:\n\t            kwargs[\"pool_recycle\"] = int(pool_recycle)\n\t        if ssl is not None:\n\t            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n\t        kwargs.update(\n\t            {\n\t                \"ignore_no_transaction_on_rollback\": True,\n\t                \"trusted_connection\": trusted_connection.lower(),\n\t                \"timeout\": timeout,\n", "                \"autocommit\": True,\n\t                \"driver\": driver,\n\t            }\n\t        )\n\t        for key, value in self._options.items():\n\t            # Coerce 'min_size' and 'max_size' for consistency.\n\t            if key == \"min_size\":\n\t                key = \"minsize\"\n\t            elif key == \"max_size\":\n\t                key = \"maxsize\"\n", "            kwargs[key] = value\n\t        return kwargs\n\t    async def connect(self) -> None:\n\t        assert self._pool is None, \"DatabaseBackend is already running\"\n\t        kwargs = self._get_connection_kwargs()\n\t        driver = kwargs[\"driver\"]\n\t        database = self._database_url.database\n\t        hostname = self._database_url.hostname\n\t        port = self._database_url.port or 1433\n\t        user = self._database_url.username or getpass.getuser()\n", "        password = self._database_url.password\n\t        timeout = kwargs.pop(\"timeout\")\n\t        if port:\n\t            dsn = f\"Driver={driver};Database={database};Server={hostname},{port};UID={user};PWD={password};Connection+Timeout={timeout}\"\n\t        else:\n\t            dsn = f\"Driver={driver};Database={database};Server={hostname},{port};UID={user};PWD={password};Connection+Timeout={timeout}\"\n\t        self._pool = await aioodbc.create_pool(\n\t            dsn=dsn,\n\t            **kwargs,\n\t        )\n", "    async def disconnect(self) -> None:\n\t        assert self._pool is not None, \"DatabaseBackend is not running\"\n\t        self._pool.close()\n\t        pool, self._pool = self._pool, None\n\t        await pool.wait_closed()\n\t    def connection(self) -> \"MSSQLConnection\":\n\t        return MSSQLConnection(self, self._dialect)\n\tclass CompilationContext:\n\t    def __init__(self, context: ExecutionContext):\n\t        self.context = context\n", "class MSSQLConnection(ConnectionBackend):\n\t    def __init__(self, database: MSSQLBackend, dialect: Dialect) -> None:\n\t        self._database = database\n\t        self._dialect = dialect\n\t        self._connection: typing.Optional[aioodbc.Connection] = None\n\t    async def acquire(self) -> None:\n\t        assert self._connection is None, \"Connection is already acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        self._connection = await self._database._pool.acquire()\n\t    async def release(self) -> None:\n", "        assert self._connection is not None, \"Connection is not acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        await self._database._pool.release(self._connection)\n\t        self._connection = None\n\t    async def fetch_all(self, query: ClauseElement) -> typing.List[\"RecordInterface\"]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n", "        try:\n\t            await cursor.execute(query_str, args)\n\t            rows = await cursor.fetchall()\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            rows = [\n\t                Row(\n\t                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n", "                )\n\t                for row in rows\n\t            ]\n\t            return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\t        finally:\n\t            await cursor.close()\n\t    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n", "        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            await cursor.execute(query_str, args)\n\t            row = await cursor.fetchone()\n\t            if row is None:\n\t                return None\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            row = Row(\n\t                metadata,\n", "                metadata._effective_processors,\n\t                metadata._key_to_index,\n\t                row,\n\t            )\n\t            return Record(row, result_columns, dialect, column_maps)\n\t        finally:\n\t            await cursor.close()\n\t    async def execute(self, query: ClauseElement) -> typing.Any:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, _, _ = self._compile(query)\n", "        cursor = await self._connection.cursor()\n\t        try:\n\t            values = await cursor.execute(query_str, args)\n\t            try:\n\t                values = await values.fetchone()\n\t                return values[0]\n\t            except Exception:\n\t                ...\n\t        finally:\n\t            await cursor.close()\n", "    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            for single_query in queries:\n\t                single_query, args, _, _ = self._compile(single_query)\n\t                await cursor.execute(single_query, args)\n\t        finally:\n\t            await cursor.close()\n\t    async def iterate(self, query: ClauseElement) -> typing.AsyncGenerator[typing.Any, None]:\n", "        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            await cursor.execute(query_str, args)\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            async for row in cursor:\n\t                record = Row(\n", "                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n\t                )\n\t                yield Record(record, result_columns, dialect, column_maps)\n\t        finally:\n\t            await cursor.close()\n\t    def transaction(self) -> TransactionBackend:\n\t        return MSSQLTransaction(self)\n", "    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n\t        compiled = query.compile(\n\t            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n\t        )\n\t        execution_context = self._dialect.execution_ctx_cls()\n\t        execution_context.dialect = self._dialect\n\t        if not isinstance(query, DDLElement):\n\t            compiled_params = compiled.params.items()\n\t            mapping = {key: \"?\" for _, (key, _) in enumerate(compiled_params, start=1)}\n\t            compiled_query = compiled.string % mapping\n", "            processors = compiled._bind_processors\n\t            args = [\n\t                processors[key](val) if key in processors else val for key, val in compiled_params\n\t            ]\n\t            execution_context.result_column_struct = (\n\t                compiled._result_columns,\n\t                compiled._ordered_columns,\n\t                compiled._textual_ordered_columns,\n\t                compiled._ad_hoc_textual,\n\t                compiled._loose_column_name_matching,\n", "            )\n\t            result_map = compiled._result_columns\n\t        else:\n\t            compiled_query = compiled.string\n\t            args = []\n\t            result_map = None\n\t        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n\t        logger.debug(\"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA)\n\t        return compiled_query, args, result_map, CompilationContext(execution_context)\n\t    @property\n", "    def raw_connection(self) -> aioodbc.connection.Connection:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        return self._connection\n\tclass MSSQLTransaction(TransactionBackend):\n\t    def __init__(self, connection: MSSQLConnection):\n\t        self._connection = connection\n\t        self._is_root = False\n\t        self._savepoint_name = \"\"\n\t    async def start(\n\t        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n", "    ) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        self._is_root = is_root\n\t        cursor = await self._connection._connection.cursor()\n\t        if self._is_root:\n\t            await cursor.execute(\"BEGIN TRANSACTION\")\n\t        else:\n\t            id = str(uuid.uuid4()).replace(\"-\", \"_\")[:12]\n\t            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n\t            try:\n", "                await cursor.execute(f\"SAVE TRANSACTION {self._savepoint_name}\")\n\t            finally:\n\t                cursor.close()\n\t    async def commit(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        cursor = await self._connection._connection.cursor()\n\t        if self._is_root:\n\t            await cursor.execute(\"COMMIT TRANSACTION\")\n\t        else:\n\t            try:\n", "                await cursor.execute(f\"COMMIT TRANSACTION {self._savepoint_name}\")\n\t            finally:\n\t                cursor.close()\n\t    async def rollback(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        cursor = await self._connection._connection.cursor()\n\t        if self._is_root:\n\t            try:\n\t                await cursor.execute(\"ROLLBACK TRANSACTION\")\n\t            except ext_pyodbc.ProgrammingError:\n", "                logger.error(\"There is no transaction to rollback\")\n\t        else:\n\t            try:\n\t                await cursor.execute(f\"ROLLBACK TRANSACTION {self._savepoint_name}\")\n\t            finally:\n\t                cursor.close()\n"]}
{"filename": "databasez/backends/__init__.py", "chunked_list": []}
{"filename": "databasez/backends/aiopg.py", "chunked_list": ["import getpass\n\timport json\n\timport logging\n\timport typing\n\timport uuid\n\timport aiopg\n\tfrom sqlalchemy.engine.cursor import CursorResultMetaData\n\tfrom sqlalchemy.engine.interfaces import Dialect, ExecutionContext\n\tfrom sqlalchemy.sql import ClauseElement\n\tfrom sqlalchemy.sql.ddl import DDLElement\n", "from databasez.backends.common.records import Record, Row, create_column_maps\n\tfrom databasez.backends.compilers.psycopg import PGCompiler_psycopg\n\tfrom databasez.backends.dialects.psycopg import PGDialect_psycopg\n\tfrom databasez.core import LOG_EXTRA, DatabaseURL\n\tfrom databasez.interfaces import ConnectionBackend, DatabaseBackend\n\tfrom databasez.interfaces import Record as RecordInterface\n\tfrom databasez.interfaces import TransactionBackend\n\tlogger = logging.getLogger(\"databasez\")\n\tclass AiopgBackend(DatabaseBackend):\n\t    def __init__(\n", "        self, database_url: typing.Union[DatabaseURL, str], **options: typing.Any\n\t    ) -> None:\n\t        self._database_url = DatabaseURL(database_url)\n\t        self._options = options\n\t        self._dialect = self._get_dialect()\n\t        self._pool: typing.Union[aiopg.Pool, None] = None\n\t    def _get_dialect(self) -> Dialect:\n\t        dialect = PGDialect_psycopg(json_serializer=json.dumps, json_deserializer=lambda x: x)\n\t        dialect.statement_compiler = PGCompiler_psycopg\n\t        dialect.implicit_returning = True\n", "        dialect.supports_native_enum = True\n\t        dialect.supports_smallserial = True  # 9.2+\n\t        dialect._backslash_escapes = False\n\t        dialect.supports_sane_multi_rowcount = True  # psycopg 2.0.9+\n\t        dialect._has_native_hstore = True\n\t        dialect.supports_native_decimal = True\n\t        return dialect\n\t    def _get_connection_kwargs(self) -> dict:\n\t        url_options = self._database_url.options\n\t        kwargs = {}\n", "        min_size = url_options.get(\"min_size\")\n\t        max_size = url_options.get(\"max_size\")\n\t        ssl = url_options.get(\"ssl\")\n\t        if min_size is not None:\n\t            kwargs[\"minsize\"] = int(min_size)\n\t        if max_size is not None:\n\t            kwargs[\"maxsize\"] = int(max_size)\n\t        if ssl is not None:\n\t            kwargs[\"ssl\"] = {\"true\": True, \"false\": False}[ssl.lower()]\n\t        for key, value in self._options.items():\n", "            # Coerce 'min_size' and 'max_size' for consistency.\n\t            if key == \"min_size\":\n\t                key = \"minsize\"\n\t            elif key == \"max_size\":\n\t                key = \"maxsize\"\n\t            kwargs[key] = value\n\t        return kwargs\n\t    async def connect(self) -> None:\n\t        assert self._pool is None, \"DatabaseBackend is already running\"\n\t        kwargs = self._get_connection_kwargs()\n", "        self._pool = await aiopg.create_pool(\n\t            host=self._database_url.hostname,\n\t            port=self._database_url.port,\n\t            user=self._database_url.username or getpass.getuser(),\n\t            password=self._database_url.password,\n\t            database=self._database_url.database,\n\t            **kwargs,\n\t        )\n\t    async def disconnect(self) -> None:\n\t        assert self._pool is not None, \"DatabaseBackend is not running\"\n", "        self._pool.close()\n\t        await self._pool.wait_closed()\n\t        self._pool = None\n\t    def connection(self) -> \"AiopgConnection\":\n\t        return AiopgConnection(self, self._dialect)\n\tclass CompilationContext:\n\t    def __init__(self, context: ExecutionContext):\n\t        self.context = context\n\tclass AiopgConnection(ConnectionBackend):\n\t    def __init__(self, database: AiopgBackend, dialect: Dialect):\n", "        self._database = database\n\t        self._dialect = dialect\n\t        self._connection: typing.Optional[aiopg.Connection] = None\n\t    async def acquire(self) -> None:\n\t        assert self._connection is None, \"Connection is already acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n\t        self._connection = await self._database._pool.acquire()\n\t    async def release(self) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        assert self._database._pool is not None, \"DatabaseBackend is not running\"\n", "        connection, self._connection = self._connection, None\n\t        await self._database._pool.release(connection)\n\t    async def fetch_all(self, query: ClauseElement) -> typing.List[RecordInterface]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            await cursor.execute(query_str, args)\n", "            rows = await cursor.fetchall()\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            rows = [\n\t                Row(\n\t                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n\t                )\n\t                for row in rows\n", "            ]\n\t            return [Record(row, result_columns, dialect, column_maps) for row in rows]\n\t        finally:\n\t            cursor.close()\n\t    async def fetch_one(self, query: ClauseElement) -> typing.Optional[RecordInterface]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n", "        try:\n\t            await cursor.execute(query_str, args)\n\t            row = await cursor.fetchone()\n\t            if row is None:\n\t                return None\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            row = Row(\n\t                metadata,\n\t                metadata._effective_processors,\n\t                metadata._key_to_index,\n", "                row,\n\t            )\n\t            return Record(row, result_columns, dialect, column_maps)\n\t        finally:\n\t            cursor.close()\n\t    async def execute(self, query: ClauseElement) -> typing.Any:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, _, _ = self._compile(query)\n\t        cursor = await self._connection.cursor()\n\t        try:\n", "            await cursor.execute(query_str, args)\n\t            return cursor.lastrowid\n\t        finally:\n\t            cursor.close()\n\t    async def execute_many(self, queries: typing.List[ClauseElement]) -> None:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        cursor = await self._connection.cursor()\n\t        try:\n\t            for single_query in queries:\n\t                single_query, args, _, _ = self._compile(single_query)\n", "                await cursor.execute(single_query, args)\n\t        finally:\n\t            cursor.close()\n\t    async def iterate(self, query: ClauseElement) -> typing.AsyncGenerator[typing.Any, None]:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        query_str, args, result_columns, context = self._compile(query)\n\t        column_maps = create_column_maps(result_columns)\n\t        dialect = self._dialect\n\t        cursor = await self._connection.cursor()\n\t        try:\n", "            await cursor.execute(query_str, args)\n\t            metadata = CursorResultMetaData(context, cursor.description)\n\t            async for row in cursor:\n\t                record = Row(\n\t                    metadata,\n\t                    metadata._effective_processors,\n\t                    metadata._key_to_index,\n\t                    row,\n\t                )\n\t                yield Record(record, result_columns, dialect, column_maps)\n", "        finally:\n\t            cursor.close()\n\t    def transaction(self) -> TransactionBackend:\n\t        return AiopgTransaction(self)\n\t    def _compile(self, query: ClauseElement) -> typing.Tuple[str, list, tuple]:\n\t        compiled = query.compile(\n\t            dialect=self._dialect, compile_kwargs={\"render_postcompile\": True}\n\t        )\n\t        execution_context = self._dialect.execution_ctx_cls()\n\t        execution_context.dialect = self._dialect\n", "        if not isinstance(query, DDLElement):\n\t            compiled_params = sorted(compiled.params.items())\n\t            args = compiled.construct_params()\n\t            for key, val in args.items():\n\t                if key in compiled._bind_processors:\n\t                    args[key] = compiled._bind_processors[key](val)\n\t            execution_context.result_column_struct = (\n\t                compiled._result_columns,\n\t                compiled._ordered_columns,\n\t                compiled._textual_ordered_columns,\n", "                compiled._ad_hoc_textual,\n\t                compiled._loose_column_name_matching,\n\t            )\n\t            mapping = {key: \"$\" + str(i) for i, (key, _) in enumerate(compiled_params, start=1)}\n\t            compiled_query = compiled.string % mapping\n\t            result_map = compiled._result_columns\n\t        else:\n\t            args = {}\n\t            result_map = None\n\t            compiled_query = compiled.string\n", "        query_message = compiled_query.replace(\" \\n\", \" \").replace(\"\\n\", \" \")\n\t        logger.debug(\"Query: %s Args: %s\", query_message, repr(tuple(args)), extra=LOG_EXTRA)\n\t        return compiled.string, args, result_map, CompilationContext(execution_context)\n\t    @property\n\t    def raw_connection(self) -> aiopg.connection.Connection:\n\t        assert self._connection is not None, \"Connection is not acquired\"\n\t        return self._connection\n\tclass AiopgTransaction(TransactionBackend):\n\t    def __init__(self, connection: AiopgConnection):\n\t        self._connection = connection\n", "        self._is_root = False\n\t        self._savepoint_name = \"\"\n\t    async def start(\n\t        self, is_root: bool, extra_options: typing.Dict[typing.Any, typing.Any]\n\t    ) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        self._is_root = is_root\n\t        cursor = await self._connection._connection.cursor()\n\t        if self._is_root:\n\t            await cursor.execute(\"BEGIN\")\n", "        else:\n\t            id = str(uuid.uuid4()).replace(\"-\", \"_\")\n\t            self._savepoint_name = f\"STARLETTE_SAVEPOINT_{id}\"\n\t            try:\n\t                await cursor.execute(f\"SAVEPOINT {self._savepoint_name}\")\n\t            finally:\n\t                cursor.close()\n\t    async def commit(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        cursor = await self._connection._connection.cursor()\n", "        if self._is_root:\n\t            await cursor.execute(\"COMMIT\")\n\t        else:\n\t            try:\n\t                await cursor.execute(f\"RELEASE SAVEPOINT {self._savepoint_name}\")\n\t            finally:\n\t                cursor.close()\n\t    async def rollback(self) -> None:\n\t        assert self._connection._connection is not None, \"Connection is not acquired\"\n\t        cursor = await self._connection._connection.cursor()\n", "        if self._is_root:\n\t            await cursor.execute(\"ROLLBACK\")\n\t        else:\n\t            try:\n\t                await cursor.execute(f\"ROLLBACK TO SAVEPOINT {self._savepoint_name}\")\n\t            finally:\n\t                cursor.close()\n"]}
{"filename": "databasez/backends/compilers/psycopg.py", "chunked_list": ["from sqlalchemy.dialects.postgresql.psycopg import PGCompiler_psycopg\n\tclass APGCompiler_psycopg2(PGCompiler_psycopg):\n\t    def construct_params(self, *args, **kwargs):\n\t        pd = super().construct_params(*args, **kwargs)\n\t        for column in self.prefetch:\n\t            pd[column.key] = self._exec_default(column.default)\n\t        return pd\n\t    def _exec_default(self, default):\n\t        if default.is_callable:\n\t            return default.arg(self.dialect)\n", "        else:\n\t            return default.arg\n"]}
{"filename": "databasez/backends/compilers/__init__.py", "chunked_list": []}
{"filename": "databasez/backends/common/records.py", "chunked_list": ["import json\n\timport typing\n\tfrom datetime import date, datetime\n\tfrom sqlalchemy.engine.interfaces import Dialect\n\tfrom sqlalchemy.engine.row import Row as SQLRow\n\tfrom sqlalchemy.sql.compiler import _CompileLabel\n\tfrom sqlalchemy.sql.schema import Column\n\tfrom sqlalchemy.types import TypeEngine\n\tfrom databasez.interfaces import Record as RecordInterface\n\tDIALECT_EXCLUDE = {\"postgresql\"}\n", "class Record(RecordInterface):\n\t    __slots__ = (\n\t        \"_row\",\n\t        \"_result_columns\",\n\t        \"_dialect\",\n\t        \"_column_map\",\n\t        \"_column_map_int\",\n\t        \"_column_map_full\",\n\t    )\n\t    def __init__(\n", "        self,\n\t        row: typing.Any,\n\t        result_columns: tuple,\n\t        dialect: Dialect,\n\t        column_maps: typing.Tuple[\n\t            typing.Mapping[typing.Any, typing.Tuple[int, TypeEngine]],\n\t            typing.Mapping[int, typing.Tuple[int, TypeEngine]],\n\t            typing.Mapping[str, typing.Tuple[int, TypeEngine]],\n\t        ],\n\t    ) -> None:\n", "        self._row = row\n\t        self._result_columns = result_columns\n\t        self._dialect = dialect\n\t        self._column_map, self._column_map_int, self._column_map_full = column_maps\n\t    @property\n\t    def _mapping(self) -> typing.Mapping:\n\t        return self._row\n\t    def keys(self) -> typing.KeysView:\n\t        return self._mapping.keys()\n\t    def values(self) -> typing.ValuesView:\n", "        return self._mapping.values()\n\t    def __getitem__(self, key: typing.Any) -> typing.Any:\n\t        if len(self._column_map) == 0:\n\t            return self._row[key]\n\t        elif isinstance(key, Column):\n\t            idx, datatype = self._column_map_full[str(key)]\n\t        elif isinstance(key, int):\n\t            idx, datatype = self._column_map_int[key]\n\t        else:\n\t            idx, datatype = self._column_map[key]\n", "        raw = self._row[idx]\n\t        processor = datatype._cached_result_processor(self._dialect, None)\n\t        if self._dialect.name not in DIALECT_EXCLUDE:\n\t            if isinstance(raw, dict):\n\t                raw = json.dumps(raw)\n\t        if processor is not None and (not isinstance(raw, (datetime, date))):\n\t            return processor(raw)\n\t        return raw\n\t    def __iter__(self) -> typing.Iterator:\n\t        return iter(self._row.keys())\n", "    def __len__(self) -> int:\n\t        return len(self._row)\n\t    def __getattr__(self, name: str) -> typing.Any:\n\t        try:\n\t            return self.__getitem__(name)\n\t        except KeyError as e:\n\t            raise AttributeError(e.args[0]) from e\n\t    def __str__(self) -> str:\n\t        return f\"Record{str(self._row)}\"\n\t    def __repr__(self) -> str:\n", "        return str(self)\n\tclass Row(SQLRow):\n\t    def __getitem__(self, key: typing.Any) -> typing.Any:\n\t        \"\"\"\n\t        An instance of a Row in SQLAlchemy allows the access\n\t        to the Row._fields as tuple and the Row._mapping for\n\t        the values.\n\t        return RowMapping(\n\t            self._parent,\n\t            None,\n", "            self._keymap,\n\t            RowMapping._default_key_style,\n\t            self._data,\n\t            return RowMapping(self._parent, None, self._key_to_index, self._data)\n\t        )\n\t        \"\"\"\n\t        if isinstance(key, int):\n\t            field = self._fields[key]\n\t            return self._mapping[field]\n\t        return self._mapping[key]\n", "    def keys(self):\n\t        return self._mapping.keys()\n\t    def values(self):\n\t        return self._mapping.values()\n\t    def __getattr__(self, name: str) -> typing.Any:\n\t        try:\n\t            return self.__getitem__(name)\n\t        except KeyError as e:\n\t            raise AttributeError(e.args[0]) from e\n\tdef create_column_maps(\n", "    result_columns: typing.Any,\n\t) -> typing.Tuple[\n\t    typing.Mapping[typing.Any, typing.Tuple[int, TypeEngine]],\n\t    typing.Mapping[int, typing.Tuple[int, TypeEngine]],\n\t    typing.Mapping[str, typing.Tuple[int, TypeEngine]],\n\t]:\n\t    \"\"\"\n\t    Generate column -> datatype mappings from the column definitions.\n\t    These mappings are used throughout PostgresConnection methods\n\t    to initialize Record-s. The underlying DB driver does not do type\n", "    conversion for us so we have wrap the returned asyncpg.Record-s.\n\t    :return: Three mappings from different ways to address a column to \\\n\t                corresponding column indexes and datatypes: \\\n\t                1. by column identifier; \\\n\t                2. by column index; \\\n\t                3. by column name in Column sqlalchemy objects.\n\t    \"\"\"\n\t    column_map, column_map_int, column_map_full = {}, {}, {}\n\t    for idx, (column_name, _, column, datatype) in enumerate(result_columns):\n\t        column_map[column_name] = (idx, datatype)\n", "        column_map_int[idx] = (idx, datatype)\n\t        if not column:\n\t            continue\n\t        # Added in SQLA 2.0 and _CompileLabels do not have _annotations\n\t        # When this happens, the mapping is on the second position\n\t        if isinstance(column[0], _CompileLabel):\n\t            column_map_full[str(column[2])] = (idx, datatype)\n\t        else:\n\t            column_map_full[str(column[0])] = (idx, datatype)\n\t    return column_map, column_map_int, column_map_full\n"]}
{"filename": "databasez/backends/common/__init__.py", "chunked_list": []}
{"filename": "databasez/backends/dialects/psycopg.py", "chunked_list": ["\"\"\"\n\tAll the unique changes for the databases package\n\twith the custom Numeric as the deprecated pypostgresql\n\tfor backwards compatibility and to make sure the\n\tpackage can go to SQLAlchemy 2.0+.\n\t\"\"\"\n\timport typing\n\tfrom sqlalchemy import types, util\n\tfrom sqlalchemy.dialects.postgresql.base import PGDialect, PGExecutionContext\n\tfrom sqlalchemy.engine import processors\n", "from sqlalchemy.types import Float, Numeric\n\tclass PGExecutionContext_psycopg(PGExecutionContext):\n\t    ...\n\tclass PGNumeric(Numeric):\n\t    def bind_processor(self, dialect: typing.Any) -> typing.Union[str, None]:  # pragma: no cover\n\t        return processors.to_str\n\t    def result_processor(\n\t        self, dialect: typing.Any, coltype: typing.Any\n\t    ) -> typing.Union[float, None]:  # pragma: no cover\n\t        if self.asdecimal:\n", "            return None\n\t        else:\n\t            return processors.to_float\n\tclass PGDialect_psycopg(PGDialect):\n\t    colspecs = util.update_copy(\n\t        PGDialect.colspecs,\n\t        {\n\t            types.Numeric: PGNumeric,\n\t            types.Float: Float,\n\t        },\n", "    )\n\t    execution_ctx_cls = PGExecutionContext_psycopg\n\tdialect = PGDialect_psycopg\n"]}
{"filename": "databasez/backends/dialects/__init__.py", "chunked_list": []}
