{"filename": "babyagi.py", "chunked_list": ["#!/usr/bin/env python3\n\tfrom dotenv import load_dotenv\n\t# Load default environment variables (.env)\n\tload_dotenv()\n\timport os\n\timport time\n\timport logging\n\tfrom collections import deque\n\tfrom typing import Dict, List\n\timport importlib\n", "import openai\n\timport chromadb\n\timport tiktoken as tiktoken\n\tfrom chromadb.utils.embedding_functions import OpenAIEmbeddingFunction\n\tfrom chromadb.api.types import Documents, EmbeddingFunction, Embeddings\n\timport re\n\t# default opt out of chromadb telemetry.\n\tfrom chromadb.config import Settings\n\tclient = chromadb.Client(Settings(anonymized_telemetry=False))\n\t# Engine configuration\n", "# Model: GPT, LLAMA, HUMAN, etc.\n\tLLM_MODEL = os.getenv(\"LLM_MODEL\", os.getenv(\"OPENAI_API_MODEL\", \"gpt-3.5-turbo\")).lower()\n\t# API Keys\n\tOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n\tif not (LLM_MODEL.startswith(\"llama\") or LLM_MODEL.startswith(\"human\")):\n\t    assert OPENAI_API_KEY, \"\\033[91m\\033[1m\" + \"OPENAI_API_KEY environment variable is missing from .env\" + \"\\033[0m\\033[0m\"\n\t# Table config\n\tRESULTS_STORE_NAME = os.getenv(\"RESULTS_STORE_NAME\", os.getenv(\"TABLE_NAME\", \"\"))\n\tassert RESULTS_STORE_NAME, \"\\033[91m\\033[1m\" + \"RESULTS_STORE_NAME environment variable is missing from .env\" + \"\\033[0m\\033[0m\"\n\t# Run configuration\n", "INSTANCE_NAME = os.getenv(\"INSTANCE_NAME\", os.getenv(\"BABY_NAME\", \"BabyAGI\"))\n\tCOOPERATIVE_MODE = \"none\"\n\tJOIN_EXISTING_OBJECTIVE = False\n\t# Goal configuration\n\tOBJECTIVE = os.getenv(\"OBJECTIVE\", \"\")\n\tINITIAL_TASK = os.getenv(\"INITIAL_TASK\", os.getenv(\"FIRST_TASK\", \"\"))\n\t# Model configuration\n\tOPENAI_TEMPERATURE = float(os.getenv(\"OPENAI_TEMPERATURE\", 0.0))\n\t# Extensions support begin\n\tdef can_import(module_name):\n", "    try:\n\t        importlib.import_module(module_name)\n\t        return True\n\t    except ImportError:\n\t        return False\n\tDOTENV_EXTENSIONS = os.getenv(\"DOTENV_EXTENSIONS\", \"\").split(\" \")\n\t# Command line arguments extension\n\t# Can override any of the above environment variables\n\tENABLE_COMMAND_LINE_ARGS = (\n\t        os.getenv(\"ENABLE_COMMAND_LINE_ARGS\", \"false\").lower() == \"true\"\n", ")\n\tif ENABLE_COMMAND_LINE_ARGS:\n\t    if can_import(\"extensions.argparseext\"):\n\t        from extensions.argparseext import parse_arguments\n\t        OBJECTIVE, INITIAL_TASK, LLM_MODEL, DOTENV_EXTENSIONS, INSTANCE_NAME, COOPERATIVE_MODE, JOIN_EXISTING_OBJECTIVE = parse_arguments()\n\t# Human mode extension\n\t# Gives human input to babyagi\n\tif LLM_MODEL.startswith(\"human\"):\n\t    if can_import(\"extensions.human_mode\"):\n\t        from extensions.human_mode import user_input_await\n", "# Load additional environment variables for enabled extensions\n\t# TODO: This might override the following command line arguments as well:\n\t#    OBJECTIVE, INITIAL_TASK, LLM_MODEL, INSTANCE_NAME, COOPERATIVE_MODE, JOIN_EXISTING_OBJECTIVE\n\tif DOTENV_EXTENSIONS:\n\t    if can_import(\"extensions.dotenvext\"):\n\t        from extensions.dotenvext import load_dotenv_extensions\n\t        load_dotenv_extensions(DOTENV_EXTENSIONS)\n\t# TODO: There's still work to be done here to enable people to get\n\t# defaults from dotenv extensions, but also provide command line\n\t# arguments to override them\n", "# Extensions support end\n\tprint(\"\\033[95m\\033[1m\" + \"\\n*****CONFIGURATION*****\\n\" + \"\\033[0m\\033[0m\")\n\tprint(f\"Name  : {INSTANCE_NAME}\")\n\tprint(f\"Mode  : {'alone' if COOPERATIVE_MODE in ['n', 'none'] else 'local' if COOPERATIVE_MODE in ['l', 'local'] else 'distributed' if COOPERATIVE_MODE in ['d', 'distributed'] else 'undefined'}\")\n\tprint(f\"LLM   : {LLM_MODEL}\")\n\t# Check if we know what we are doing\n\tassert OBJECTIVE, \"\\033[91m\\033[1m\" + \"OBJECTIVE environment variable is missing from .env\" + \"\\033[0m\\033[0m\"\n\tassert INITIAL_TASK, \"\\033[91m\\033[1m\" + \"INITIAL_TASK environment variable is missing from .env\" + \"\\033[0m\\033[0m\"\n\tLLAMA_MODEL_PATH = os.getenv(\"LLAMA_MODEL_PATH\", \"models/llama-13B/ggml-model.bin\")\n\tif LLM_MODEL.startswith(\"llama\"):\n", "    if can_import(\"llama_cpp\"):\n\t        from llama_cpp import Llama\n\t        print(f\"LLAMA : {LLAMA_MODEL_PATH}\" + \"\\n\")\n\t        assert os.path.exists(LLAMA_MODEL_PATH), \"\\033[91m\\033[1m\" + f\"Model can't be found.\" + \"\\033[0m\\033[0m\"\n\t        CTX_MAX = 1024\n\t        LLAMA_THREADS_NUM = int(os.getenv(\"LLAMA_THREADS_NUM\", 8))\n\t        print('Initialize model for evaluation')\n\t        llm = Llama(\n\t            model_path=LLAMA_MODEL_PATH,\n\t            n_ctx=CTX_MAX,\n", "            n_threads=LLAMA_THREADS_NUM,\n\t            n_batch=512,\n\t            use_mlock=False,\n\t        )\n\t        print('\\nInitialize model for embedding')\n\t        llm_embed = Llama(\n\t            model_path=LLAMA_MODEL_PATH,\n\t            n_ctx=CTX_MAX,\n\t            n_threads=LLAMA_THREADS_NUM,\n\t            n_batch=512,\n", "            embedding=True,\n\t            use_mlock=False,\n\t        )\n\t        print(\n\t            \"\\033[91m\\033[1m\"\n\t            + \"\\n*****USING LLAMA.CPP. POTENTIALLY SLOW.*****\"\n\t            + \"\\033[0m\\033[0m\"\n\t        )\n\t    else:\n\t        print(\n", "            \"\\033[91m\\033[1m\"\n\t            + \"\\nLlama LLM requires package llama-cpp. Falling back to GPT-3.5-turbo.\"\n\t            + \"\\033[0m\\033[0m\"\n\t        )\n\t        LLM_MODEL = \"gpt-3.5-turbo\"\n\tif LLM_MODEL.startswith(\"gpt-4\"):\n\t    print(\n\t        \"\\033[91m\\033[1m\"\n\t        + \"\\n*****USING GPT-4. POTENTIALLY EXPENSIVE. MONITOR YOUR COSTS*****\"\n\t        + \"\\033[0m\\033[0m\"\n", "    )\n\tif LLM_MODEL.startswith(\"human\"):\n\t    print(\n\t        \"\\033[91m\\033[1m\"\n\t        + \"\\n*****USING HUMAN INPUT*****\"\n\t        + \"\\033[0m\\033[0m\"\n\t    )\n\tprint(\"\\033[94m\\033[1m\" + \"\\n*****OBJECTIVE*****\\n\" + \"\\033[0m\\033[0m\")\n\tprint(f\"{OBJECTIVE}\")\n\tif not JOIN_EXISTING_OBJECTIVE:\n", "    print(\"\\033[93m\\033[1m\" + \"\\nInitial task:\" + \"\\033[0m\\033[0m\" + f\" {INITIAL_TASK}\")\n\telse:\n\t    print(\"\\033[93m\\033[1m\" + f\"\\nJoining to help the objective\" + \"\\033[0m\\033[0m\")\n\t# Configure OpenAI\n\topenai.api_key = OPENAI_API_KEY\n\t# Llama embedding function\n\tclass LlamaEmbeddingFunction(EmbeddingFunction):\n\t    def __init__(self):\n\t        return\n\t    def __call__(self, texts: Documents) -> Embeddings:\n", "        embeddings = []\n\t        for t in texts:\n\t            e = llm_embed.embed(t)\n\t            embeddings.append(e)\n\t        return embeddings\n\t# Results storage using local ChromaDB\n\tclass DefaultResultsStorage:\n\t    def __init__(self):\n\t        logging.getLogger('chromadb').setLevel(logging.ERROR)\n\t        # Create Chroma collection\n", "        chroma_persist_dir = \"chroma\"\n\t        chroma_client = chromadb.Client(\n\t            settings=chromadb.config.Settings(\n\t                chroma_db_impl=\"duckdb+parquet\",\n\t                persist_directory=chroma_persist_dir,\n\t            )\n\t        )\n\t        metric = \"cosine\"\n\t        if LLM_MODEL.startswith(\"llama\"):\n\t            embedding_function = LlamaEmbeddingFunction()\n", "        else:\n\t            embedding_function = OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY)\n\t        self.collection = chroma_client.get_or_create_collection(\n\t            name=RESULTS_STORE_NAME,\n\t            metadata={\"hnsw:space\": metric},\n\t            embedding_function=embedding_function,\n\t        )\n\t    def add(self, task: Dict, result: str, result_id: str):\n\t        # Break the function if LLM_MODEL starts with \"human\" (case-insensitive)\n\t        if LLM_MODEL.startswith(\"human\"):\n", "            return\n\t        # Continue with the rest of the function\n\t        embeddings = llm_embed.embed(result) if LLM_MODEL.startswith(\"llama\") else None\n\t        if (\n\t                len(self.collection.get(ids=[result_id], include=[])[\"ids\"]) > 0\n\t        ):  # Check if the result already exists\n\t            self.collection.update(\n\t                ids=result_id,\n\t                embeddings=embeddings,\n\t                documents=result,\n", "                metadatas={\"task\": task[\"task_name\"], \"result\": result},\n\t            )\n\t        else:\n\t            self.collection.add(\n\t                ids=result_id,\n\t                embeddings=embeddings,\n\t                documents=result,\n\t                metadatas={\"task\": task[\"task_name\"], \"result\": result},\n\t            )\n\t    def query(self, query: str, top_results_num: int) -> List[dict]:\n", "        count: int = self.collection.count()\n\t        if count == 0:\n\t            return []\n\t        results = self.collection.query(\n\t            query_texts=query,\n\t            n_results=min(top_results_num, count),\n\t            include=[\"metadatas\"]\n\t        )\n\t        return [item[\"task\"] for item in results[\"metadatas\"][0]]\n\t# Initialize results storage\n", "def try_weaviate():\n\t    WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\", \"\")\n\t    WEAVIATE_USE_EMBEDDED = os.getenv(\"WEAVIATE_USE_EMBEDDED\", \"False\").lower() == \"true\"\n\t    if (WEAVIATE_URL or WEAVIATE_USE_EMBEDDED) and can_import(\"extensions.weaviate_storage\"):\n\t        WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\", \"\")\n\t        from extensions.weaviate_storage import WeaviateResultsStorage\n\t        print(\"\\nUsing results storage: \" + \"\\033[93m\\033[1m\" + \"Weaviate\" + \"\\033[0m\\033[0m\")\n\t        return WeaviateResultsStorage(OPENAI_API_KEY, WEAVIATE_URL, WEAVIATE_API_KEY, WEAVIATE_USE_EMBEDDED, LLM_MODEL, LLAMA_MODEL_PATH, RESULTS_STORE_NAME, OBJECTIVE)\n\t    return None\n\tdef try_pinecone():\n", "    PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n\t    if PINECONE_API_KEY and can_import(\"extensions.pinecone_storage\"):\n\t        PINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"\")\n\t        assert (\n\t            PINECONE_ENVIRONMENT\n\t        ), \"\\033[91m\\033[1m\" + \"PINECONE_ENVIRONMENT environment variable is missing from .env\" + \"\\033[0m\\033[0m\"\n\t        from extensions.pinecone_storage import PineconeResultsStorage\n\t        print(\"\\nUsing results storage: \" + \"\\033[93m\\033[1m\" + \"Pinecone\" + \"\\033[0m\\033[0m\")\n\t        return PineconeResultsStorage(OPENAI_API_KEY, PINECONE_API_KEY, PINECONE_ENVIRONMENT, LLM_MODEL, LLAMA_MODEL_PATH, RESULTS_STORE_NAME, OBJECTIVE)\n\t    return None\n", "def use_chroma():\n\t    print(\"\\nUsing results storage: \" + \"\\033[93m\\033[1m\" + \"Chroma (Default)\" + \"\\033[0m\\033[0m\")\n\t    return DefaultResultsStorage()\n\tresults_storage = try_weaviate() or try_pinecone() or use_chroma()\n\t# Task storage supporting only a single instance of BabyAGI\n\tclass SingleTaskListStorage:\n\t    def __init__(self):\n\t        self.tasks = deque([])\n\t        self.task_id_counter = 0\n\t    def append(self, task: Dict):\n", "        self.tasks.append(task)\n\t    def replace(self, tasks: List[Dict]):\n\t        self.tasks = deque(tasks)\n\t    def popleft(self):\n\t        return self.tasks.popleft()\n\t    def is_empty(self):\n\t        return False if self.tasks else True\n\t    def next_task_id(self):\n\t        self.task_id_counter += 1\n\t        return self.task_id_counter\n", "    def get_task_names(self):\n\t        return [t[\"task_name\"] for t in self.tasks]\n\t# Initialize tasks storage\n\ttasks_storage = SingleTaskListStorage()\n\tif COOPERATIVE_MODE in ['l', 'local']:\n\t    if can_import(\"extensions.ray_tasks\"):\n\t        import sys\n\t        from pathlib import Path\n\t        sys.path.append(str(Path(__file__).resolve().parent))\n\t        from extensions.ray_tasks import CooperativeTaskListStorage\n", "        tasks_storage = CooperativeTaskListStorage(OBJECTIVE)\n\t        print(\"\\nReplacing tasks storage: \" + \"\\033[93m\\033[1m\" + \"Ray\" + \"\\033[0m\\033[0m\")\n\telif COOPERATIVE_MODE in ['d', 'distributed']:\n\t    pass\n\tdef limit_tokens_from_string(string: str, model: str, limit: int) -> str:\n\t    \"\"\"Limits the string to a number of tokens (estimated).\"\"\"\n\t    try:\n\t        encoding = tiktoken.encoding_for_model(model)\n\t    except:\n\t        encoding = tiktoken.encoding_for_model('gpt2')  # Fallback for others.\n", "    encoded = encoding.encode(string)\n\t    return encoding.decode(encoded[:limit])\n\tdef openai_call(\n\t    prompt: str,\n\t    model: str = LLM_MODEL,\n\t    temperature: float = OPENAI_TEMPERATURE,\n\t    max_tokens: int = 100,\n\t):\n\t    while True:\n\t        try:\n", "            if model.lower().startswith(\"llama\"):\n\t                result = llm(prompt[:CTX_MAX],\n\t                             stop=[\"### Human\"],\n\t                             echo=False,\n\t                             temperature=0.2,\n\t                             top_k=40,\n\t                             top_p=0.95,\n\t                             repeat_penalty=1.05,\n\t                             max_tokens=200)\n\t                # print('\\n*****RESULT JSON DUMP*****\\n')\n", "                # print(json.dumps(result))\n\t                # print('\\n')\n\t                return result['choices'][0]['text'].strip()\n\t            elif model.lower().startswith(\"human\"):\n\t                return user_input_await(prompt)\n\t            elif not model.lower().startswith(\"gpt-\"):\n\t                # Use completion API\n\t                response = openai.Completion.create(\n\t                    engine=model,\n\t                    prompt=prompt,\n", "                    temperature=temperature,\n\t                    max_tokens=max_tokens,\n\t                    top_p=1,\n\t                    frequency_penalty=0,\n\t                    presence_penalty=0,\n\t                )\n\t                return response.choices[0].text.strip()\n\t            else:\n\t                # Use 4000 instead of the real limit (4097) to give a bit of wiggle room for the encoding of roles.\n\t                # TODO: different limits for different models.\n", "                trimmed_prompt = limit_tokens_from_string(prompt, model, 4000 - max_tokens)\n\t                # Use chat completion API\n\t                messages = [{\"role\": \"system\", \"content\": trimmed_prompt}]\n\t                response = openai.ChatCompletion.create(\n\t                    model=model,\n\t                    messages=messages,\n\t                    temperature=temperature,\n\t                    max_tokens=max_tokens,\n\t                    n=1,\n\t                    stop=None,\n", "                )\n\t                return response.choices[0].message.content.strip()\n\t        except openai.error.RateLimitError:\n\t            print(\n\t                \"   *** The OpenAI API rate limit has been exceeded. Waiting 10 seconds and trying again. ***\"\n\t            )\n\t            time.sleep(10)  # Wait 10 seconds and try again\n\t        except openai.error.Timeout:\n\t            print(\n\t                \"   *** OpenAI API timeout occurred. Waiting 10 seconds and trying again. ***\"\n", "            )\n\t            time.sleep(10)  # Wait 10 seconds and try again\n\t        except openai.error.APIError:\n\t            print(\n\t                \"   *** OpenAI API error occurred. Waiting 10 seconds and trying again. ***\"\n\t            )\n\t            time.sleep(10)  # Wait 10 seconds and try again\n\t        except openai.error.APIConnectionError:\n\t            print(\n\t                \"   *** OpenAI API connection error occurred. Check your network settings, proxy configuration, SSL certificates, or firewall rules. Waiting 10 seconds and trying again. ***\"\n", "            )\n\t            time.sleep(10)  # Wait 10 seconds and try again\n\t        except openai.error.InvalidRequestError:\n\t            print(\n\t                \"   *** OpenAI API invalid request. Check the documentation for the specific API method you are calling and make sure you are sending valid and complete parameters. Waiting 10 seconds and trying again. ***\"\n\t            )\n\t            time.sleep(10)  # Wait 10 seconds and try again\n\t        except openai.error.ServiceUnavailableError:\n\t            print(\n\t                \"   *** OpenAI API service unavailable. Waiting 10 seconds and trying again. ***\"\n", "            )\n\t            time.sleep(10)  # Wait 10 seconds and try again\n\t        else:\n\t            break\n\tdef task_creation_agent(\n\t        objective: str, result: Dict, task_description: str, task_list: List[str]\n\t):\n\t    prompt = f\"\"\"\n\tYou are to use the result from an execution agent to create new tasks with the following objective: {objective}.\n\tThe last completed task has the result: \\n{result[\"data\"]}\n", "This result was based on this task description: {task_description}.\\n\"\"\"\n\t    if task_list:\n\t        prompt += f\"These are incomplete tasks: {', '.join(task_list)}\\n\"\n\t    prompt += \"Based on the result, return a list of tasks to be completed in order to meet the objective. \"\n\t    if task_list:\n\t        prompt += \"These new tasks must not overlap with incomplete tasks. \"\n\t    prompt += \"\"\"\n\tReturn one task per line in your response. The result must be a numbered list in the format:\n\t#. First task\n\t#. Second task\n", "The number of each entry must be followed by a period. If your list is empty, write \"There are no tasks to add at this time.\"\n\tUnless your list is empty, do not include any headers before your numbered list or follow your numbered list with any other output.\"\"\"\n\t    print(f'\\n*****TASK CREATION AGENT PROMPT****\\n{prompt}\\n')\n\t    response = openai_call(prompt, max_tokens=2000)\n\t    print(f'\\n****TASK CREATION AGENT RESPONSE****\\n{response}\\n')\n\t    new_tasks = response.split('\\n')\n\t    new_tasks_list = []\n\t    for task_string in new_tasks:\n\t        task_parts = task_string.strip().split(\".\", 1)\n\t        if len(task_parts) == 2:\n", "            task_id = ''.join(s for s in task_parts[0] if s.isnumeric())\n\t            task_name = re.sub(r'[^\\w\\s_]+', '', task_parts[1]).strip()\n\t            if task_name.strip() and task_id.isnumeric():\n\t                new_tasks_list.append(task_name)\n\t            # print('New task created: ' + task_name)\n\t    out = [{\"task_name\": task_name} for task_name in new_tasks_list]\n\t    return out\n\tdef prioritization_agent():\n\t    task_names = tasks_storage.get_task_names()\n\t    bullet_string = '\\n'\n", "    prompt = f\"\"\"\n\tYou are tasked with prioritizing the following tasks: {bullet_string + bullet_string.join(task_names)}\n\tConsider the ultimate objective of your team: {OBJECTIVE}.\n\tTasks should be sorted from highest to lowest priority, where higher-priority tasks are those that act as pre-requisites or are more essential for meeting the objective.\n\tDo not remove any tasks. Return the ranked tasks as a numbered list in the format:\n\t#. First task\n\t#. Second task\n\tThe entries must be consecutively numbered, starting with 1. The number of each entry must be followed by a period.\n\tDo not include any headers before your ranked list or follow your list with any other output.\"\"\"\n\t    print(f'\\n****TASK PRIORITIZATION AGENT PROMPT****\\n{prompt}\\n')\n", "    response = openai_call(prompt, max_tokens=2000)\n\t    print(f'\\n****TASK PRIORITIZATION AGENT RESPONSE****\\n{response}\\n')\n\t    if not response:\n\t        print('Received empty response from priotritization agent. Keeping task list unchanged.')\n\t        return\n\t    new_tasks = response.split(\"\\n\") if \"\\n\" in response else [response]\n\t    new_tasks_list = []\n\t    for task_string in new_tasks:\n\t        task_parts = task_string.strip().split(\".\", 1)\n\t        if len(task_parts) == 2:\n", "            task_id = ''.join(s for s in task_parts[0] if s.isnumeric())\n\t            task_name = re.sub(r'[^\\w\\s_]+', '', task_parts[1]).strip()\n\t            if task_name.strip():\n\t                new_tasks_list.append({\"task_id\": task_id, \"task_name\": task_name})\n\t    return new_tasks_list\n\t# Execute a task based on the objective and five previous tasks\n\tdef execution_agent(objective: str, task: str) -> str:\n\t    \"\"\"\n\t    Executes a task based on the given objective and previous context.\n\t    Args:\n", "        objective (str): The objective or goal for the AI to perform the task.\n\t        task (str): The task to be executed by the AI.\n\t    Returns:\n\t        str: The response generated by the AI for the given task.\n\t    \"\"\"\n\t    context = context_agent(query=objective, top_results_num=5)\n\t    # print(\"\\n****RELEVANT CONTEXT****\\n\")\n\t    # print(context)\n\t    # print('')\n\t    prompt = f'Perform one task based on the following objective: {objective}.\\n'\n", "    if context:\n\t        prompt += 'Take into account these previously completed tasks:' + '\\n'.join(context)\n\t    prompt += f'\\nYour task: {task}\\nResponse:'\n\t    return openai_call(prompt, max_tokens=2000)\n\t# Get the top n completed tasks for the objective\n\tdef context_agent(query: str, top_results_num: int):\n\t    \"\"\"\n\t    Retrieves context for a given query from an index of tasks.\n\t    Args:\n\t        query (str): The query or objective for retrieving context.\n", "        top_results_num (int): The number of top results to retrieve.\n\t    Returns:\n\t        list: A list of tasks as context for the given query, sorted by relevance.\n\t    \"\"\"\n\t    results = results_storage.query(query=query, top_results_num=top_results_num)\n\t    # print(\"****RESULTS****\")\n\t    # print(results)\n\t    return results\n\t# Add the initial task if starting new objective\n\tif not JOIN_EXISTING_OBJECTIVE:\n", "    initial_task = {\n\t        \"task_id\": tasks_storage.next_task_id(),\n\t        \"task_name\": INITIAL_TASK\n\t    }\n\t    tasks_storage.append(initial_task)\n\tdef main():\n\t    loop = True\n\t    while loop:\n\t        # As long as there are tasks in the storage...\n\t        if not tasks_storage.is_empty():\n", "            # Print the task list\n\t            print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\\033[0m\")\n\t            for t in tasks_storage.get_task_names():\n\t                print(\" â€¢ \" + str(t))\n\t            # Step 1: Pull the first incomplete task\n\t            task = tasks_storage.popleft()\n\t            print(\"\\033[92m\\033[1m\" + \"\\n*****NEXT TASK*****\\n\" + \"\\033[0m\\033[0m\")\n\t            print(str(task[\"task_name\"]))\n\t            # Send to execution function to complete the task based on the context\n\t            result = execution_agent(OBJECTIVE, str(task[\"task_name\"]))\n", "            print(\"\\033[93m\\033[1m\" + \"\\n*****TASK RESULT*****\\n\" + \"\\033[0m\\033[0m\")\n\t            print(result)\n\t            # Step 2: Enrich result and store in the results storage\n\t            # This is where you should enrich the result if needed\n\t            enriched_result = {\n\t                \"data\": result\n\t            }\n\t            # extract the actual result from the dictionary\n\t            # since we don't do enrichment currently\n\t            # vector = enriched_result[\"data\"]\n", "            result_id = f\"result_{task['task_id']}\"\n\t            results_storage.add(task, result, result_id)\n\t            # Step 3: Create new tasks and re-prioritize task list\n\t            # only the main instance in cooperative mode does that\n\t            new_tasks = task_creation_agent(\n\t                OBJECTIVE,\n\t                enriched_result,\n\t                task[\"task_name\"],\n\t                tasks_storage.get_task_names(),\n\t            )\n", "            print('Adding new tasks to task_storage')\n\t            for new_task in new_tasks:\n\t                new_task.update({\"task_id\": tasks_storage.next_task_id()})\n\t                print(str(new_task))\n\t                tasks_storage.append(new_task)\n\t            if not JOIN_EXISTING_OBJECTIVE:\n\t                prioritized_tasks = prioritization_agent()\n\t                if prioritized_tasks:\n\t                    tasks_storage.replace(prioritized_tasks)\n\t            # Sleep a bit before checking the task list again\n", "            time.sleep(5)\n\t        else:\n\t            print('Done.')\n\t            loop = False\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/monitor.py", "chunked_list": ["#!/usr/bin/env python3\n\timport sys\n\timport time\n\timport curses\n\tfrom pathlib import Path\n\tsys.path.append(str(Path(__file__).resolve().parent.parent))\n\tfrom extensions.ray_objectives import CooperativeObjectivesListStorage\n\tfrom extensions.ray_tasks import CooperativeTaskListStorage\n\tdef print_buffer(stdscr, lines):\n\t    stdscr.clear()\n", "    y = 0\n\t    x = 0\n\t    for line in lines:\n\t        stdscr.addstr(y, x, line)\n\t        y += 1\n\t    stdscr.refresh()\n\tdef main(stdscr):\n\t    objectives = CooperativeObjectivesListStorage()\n\t    while True:\n\t        objectives_list = objectives.get_objective_names()\n", "        buffer = []\n\t        if not objectives_list:\n\t            buffer.append(\"No objectives\")\n\t        for objective in objectives_list:\n\t            buffer.append(\"-----------------\")\n\t            buffer.append(f\"Objective: {objective}\")\n\t            buffer.append(\"-----------------\")\n\t            tasks = CooperativeTaskListStorage(objective)\n\t            tasks_list = tasks.get_task_names()\n\t            buffer.append(f\"Tasks:\")\n", "            for t in tasks_list:\n\t                buffer.append(f\" * {t}\")\n\t            buffer.append(\"-----------------\")\n\t        print_buffer(stdscr, buffer)\n\t        time.sleep(30)\n\tcurses.wrapper(main)\n"]}
{"filename": "tools/__init__.py", "chunked_list": []}
{"filename": "tools/results.py", "chunked_list": ["#!/usr/bin/env python3\n\timport os\n\timport argparse\n\timport openai\n\timport pinecone\n\tfrom dotenv import load_dotenv\n\tload_dotenv()\n\tOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n\tassert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\n\tPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n", "assert PINECONE_API_KEY, \"PINECONE_API_KEY environment variable is missing from .env\"\n\tPINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n\tassert PINECONE_ENVIRONMENT, \"PINECONE_ENVIRONMENT environment variable is missing from .env\"\n\t# Table config\n\tPINECONE_TABLE_NAME = os.getenv(\"TABLE_NAME\", \"\")\n\tassert PINECONE_TABLE_NAME, \"TABLE_NAME environment variable is missing from .env\"\n\t# Function to query records from the Pinecone index\n\tdef query_records(index, query, top_k=1000):\n\t    results = index.query(query, top_k=top_k, include_metadata=True)\n\t    return [f\"{task.metadata['task']}:\\n{task.metadata['result']}\\n------------------\" for task in results.matches]\n", "# Get embedding for the text\n\tdef get_ada_embedding(text):\n\t    text = text.replace(\"\\n\", \" \")\n\t    return openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n\tdef main():\n\t    # Parse command-line arguments\n\t    parser = argparse.ArgumentParser(description=\"Query Pinecone index using a string.\")\n\t    parser.add_argument('objective', nargs='*', metavar='<objective>', help='''\n\t    main objective description. Doesn\\'t need to be quoted.\n\t    if not specified, get objective from environment.\n", "    ''', default=[os.getenv(\"OBJECTIVE\", \"\")])\n\t    args = parser.parse_args()\n\t    # Configure OpenAI\n\t    openai.api_key = OPENAI_API_KEY\n\t    # Initialize Pinecone\n\t    pinecone.init(api_key=PINECONE_API_KEY)\n\t    # Connect to the objective index\n\t    index = pinecone.Index(PINECONE_TABLE_NAME)\n\t    # Query records from the index\n\t    query = get_ada_embedding(' '.join(args.objective).strip())\n", "    retrieved_tasks = query_records(index, query)\n\t    for r in retrieved_tasks:\n\t        print(r)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tools/results_browser.py", "chunked_list": ["#!/usr/bin/env python3\n\timport os\n\timport curses\n\timport argparse\n\timport openai\n\timport pinecone\n\tfrom dotenv import load_dotenv\n\timport textwrap\n\tload_dotenv()\n\tOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n", "assert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\n\tPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\", \"\")\n\tassert PINECONE_API_KEY, \"PINECONE_API_KEY environment variable is missing from .env\"\n\tPINECONE_ENVIRONMENT = os.getenv(\"PINECONE_ENVIRONMENT\", \"us-east1-gcp\")\n\tassert PINECONE_ENVIRONMENT, \"PINECONE_ENVIRONMENT environment variable is missing from .env\"\n\t# Table config\n\tPINECONE_TABLE_NAME = os.getenv(\"TABLE_NAME\", \"\")\n\tassert PINECONE_TABLE_NAME, \"TABLE_NAME environment variable is missing from .env\"\n\t# Function to query records from the Pinecone index\n\tdef query_records(index, query, top_k=1000):\n", "    results = index.query(query, top_k=top_k, include_metadata=True)\n\t    return [{\"name\": f\"{task.metadata['task']}\", \"result\": f\"{task.metadata['result']}\"} for task in results.matches]\n\t# Get embedding for the text\n\tdef get_ada_embedding(text):\n\t    return openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n\tdef draw_tasks(stdscr, tasks, scroll_pos, selected):\n\t    y = 0\n\t    h, w = stdscr.getmaxyx()\n\t    for idx, task in enumerate(tasks[scroll_pos:], start=scroll_pos):\n\t        if y >= h:\n", "            break\n\t        task_name = f'{task[\"name\"]}'\n\t        truncated_str = task_name[:w-1]\n\t        if idx == selected:\n\t            stdscr.addstr(y, 0, truncated_str, curses.A_REVERSE)\n\t        else:\n\t            stdscr.addstr(y, 0, truncated_str)\n\t        y += 1\n\tdef draw_result(stdscr, task):\n\t    task_name = f'Task: {task[\"name\"]}'\n", "    task_result = f'Result: {task[\"result\"]}'\n\t    _, w = stdscr.getmaxyx()\n\t    task_name_wrapped = textwrap.wrap(task_name, width=w)\n\t    for i, line in enumerate(task_name_wrapped):\n\t        stdscr.addstr(i, 0, line)\n\t    y, _ = stdscr.getyx()\n\t    stdscr.addstr(y+1, 0, '------------------')\n\t    stdscr.addstr(y+2, 0, task_result)\n\tdef draw_summary(stdscr, objective, tasks, start, num):\n\t    stdscr.box()\n", "    summary_text = f'{len(tasks)} tasks ({start}-{num}) | {objective}'\n\t    stdscr.addstr(1, 1, summary_text[:stdscr.getmaxyx()[1] - 2])\n\tdef main(stdscr):\n\t    # Configure OpenAI\n\t    openai.api_key = OPENAI_API_KEY\n\t    # Initialize Pinecone\n\t    pinecone.init(api_key=PINECONE_API_KEY)\n\t    # Connect to the objective index\n\t    index = pinecone.Index(PINECONE_TABLE_NAME)\n\t    curses.curs_set(0)\n", "    stdscr.timeout(1000)\n\t    h, w = stdscr.getmaxyx()\n\t    left_w = w // 2\n\t    visible_lines = h - 3\n\t    scroll_pos = 0\n\t    selected = 0\n\t    # Parse command-line arguments\n\t    parser = argparse.ArgumentParser(description=\"Query Pinecone index using a string.\")\n\t    parser.add_argument('objective', nargs='*', metavar='<objective>', help='''\n\t    main objective description. Doesn\\'t need to be quoted.\n", "    if not specified, get objective from environment.\n\t    ''', default=[os.getenv(\"OBJECTIVE\", \"\")])\n\t    args = parser.parse_args()\n\t    # Query records from the index\n\t    objective = ' '.join(args.objective).strip().replace(\"\\n\", \" \")\n\t    retrieved_tasks = query_records(index, get_ada_embedding(objective))\n\t    while True:\n\t        stdscr.clear()\n\t        draw_tasks(stdscr.subwin(h-3, left_w, 0, 0), retrieved_tasks, scroll_pos, selected)\n\t        draw_result(stdscr.subwin(h, w - left_w, 0, left_w), retrieved_tasks[selected])\n", "        draw_summary(stdscr.subwin(3, left_w, h - 3, 0), objective, retrieved_tasks, scroll_pos+1, scroll_pos+h-3)\n\t        stdscr.refresh()\n\t        key = stdscr.getch()\n\t        if key == ord('q') or key == 27:\n\t            break\n\t        elif key == curses.KEY_UP and selected > 0:\n\t            selected -= 1\n\t            if selected < scroll_pos:\n\t                scroll_pos -= 1\n\t        elif key == curses.KEY_DOWN and selected < len(retrieved_tasks) - 1:\n", "            selected += 1\n\t            if selected - scroll_pos >= visible_lines:\n\t                scroll_pos += 1\n\tcurses.wrapper(main)"]}
{"filename": "babycoder/embeddings.py", "chunked_list": ["import os\n\timport csv \n\timport shutil\n\timport openai\n\timport pandas as pd\n\timport numpy as np\n\tfrom transformers import GPT2TokenizerFast\n\tfrom dotenv import load_dotenv\n\timport time\n\t# Heavily derived from OpenAi's cookbook example\n", "load_dotenv()\n\t# the dir is the ./playground directory\n\tREPOSITORY_PATH = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"playground\")\n\tclass Embeddings:\n\t    def __init__(self, workspace_path: str):\n\t        self.workspace_path = workspace_path\n\t        openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"\")\n\t        self.DOC_EMBEDDINGS_MODEL = f\"text-embedding-ada-002\"\n\t        self.QUERY_EMBEDDINGS_MODEL = f\"text-embedding-ada-002\"\n\t        self.SEPARATOR = \"\\n* \"\n", "        self.tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n\t        self.separator_len = len(self.tokenizer.tokenize(self.SEPARATOR))\n\t    def compute_repository_embeddings(self):\n\t        try:\n\t            playground_data_path = os.path.join(self.workspace_path, 'playground_data')\n\t            # Delete the contents of the playground_data directory but not the directory itself\n\t            # This is to ensure that we don't have any old data lying around\n\t            for filename in os.listdir(playground_data_path):\n\t                file_path = os.path.join(playground_data_path, filename)\n\t                try:\n", "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n\t                        os.unlink(file_path)\n\t                    elif os.path.isdir(file_path):\n\t                        shutil.rmtree(file_path)\n\t                except Exception as e:\n\t                    print(f\"Failed to delete {file_path}. Reason: {str(e)}\")\n\t        except Exception as e:\n\t            print(f\"Error: {str(e)}\")\n\t        # extract and save info to csv\n\t        info = self.extract_info(REPOSITORY_PATH)\n", "        self.save_info_to_csv(info)\n\t        df = pd.read_csv(os.path.join(self.workspace_path, 'playground_data\\\\repository_info.csv'))\n\t        df = df.set_index([\"filePath\", \"lineCoverage\"])\n\t        self.df = df\n\t        context_embeddings = self.compute_doc_embeddings(df)\n\t        self.save_doc_embeddings_to_csv(context_embeddings, df, os.path.join(self.workspace_path, 'playground_data\\\\doc_embeddings.csv'))\n\t        try:\n\t            self.document_embeddings = self.load_embeddings(os.path.join(self.workspace_path, 'playground_data\\\\doc_embeddings.csv'))\n\t        except:\n\t            pass\n", "    # Extract information from files in the repository in chunks\n\t    # Return a list of [filePath, lineCoverage, chunkContent]\n\t    def extract_info(self, REPOSITORY_PATH):\n\t        # Initialize an empty list to store the information\n\t        info = []\n\t        LINES_PER_CHUNK = 60\n\t        # Iterate through the files in the repository\n\t        for root, dirs, files in os.walk(REPOSITORY_PATH):\n\t            for file in files:\n\t                file_path = os.path.join(root, file)\n", "                # Read the contents of the file\n\t                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n\t                    try:\n\t                        contents = f.read()\n\t                    except:\n\t                        continue\n\t                # Split the contents into lines\n\t                lines = contents.split(\"\\n\")\n\t                # Ignore empty lines\n\t                lines = [line for line in lines if line.strip()]\n", "                # Split the lines into chunks of LINES_PER_CHUNK lines\n\t                chunks = [\n\t                        lines[i:i+LINES_PER_CHUNK]\n\t                        for i in range(0, len(lines), LINES_PER_CHUNK)\n\t                    ]\n\t                # Iterate through the chunks\n\t                for i, chunk in enumerate(chunks):\n\t                    # Join the lines in the chunk back into a single string\n\t                    chunk = \"\\n\".join(chunk)\n\t                    # Get the first and last line numbers\n", "                    first_line = i * LINES_PER_CHUNK + 1\n\t                    last_line = first_line + len(chunk.split(\"\\n\")) - 1\n\t                    line_coverage = (first_line, last_line)\n\t                    # Add the file path, line coverage, and content to the list\n\t                    info.append((os.path.join(root, file), line_coverage, chunk))\n\t        # Return the list of information\n\t        return info\n\t    def save_info_to_csv(self, info):\n\t        # Open a CSV file for writing\n\t        os.makedirs(os.path.join(self.workspace_path, \"playground_data\"), exist_ok=True)\n", "        with open(os.path.join(self.workspace_path, 'playground_data\\\\repository_info.csv'), \"w\", newline=\"\") as csvfile:\n\t            # Create a CSV writer\n\t            writer = csv.writer(csvfile)\n\t            # Write the header row\n\t            writer.writerow([\"filePath\", \"lineCoverage\", \"content\"])\n\t            # Iterate through the info\n\t            for file_path, line_coverage, content in info:\n\t                # Write a row for each chunk of data\n\t                writer.writerow([file_path, line_coverage, content])\n\t    def get_relevant_code_chunks(self, task_description: str, task_context: str):\n", "        query = task_description + \"\\n\" + task_context\n\t        most_relevant_document_sections = self.order_document_sections_by_query_similarity(query, self.document_embeddings)\n\t        selected_chunks = []\n\t        for _, section_index in most_relevant_document_sections:\n\t            try:\n\t                document_section = self.df.loc[section_index]\n\t                selected_chunks.append(self.SEPARATOR + document_section['content'].replace(\"\\n\", \" \"))\n\t                if len(selected_chunks) >= 2:\n\t                    break\n\t            except:\n", "                pass\n\t        return selected_chunks\n\t    def get_embedding(self, text: str, model: str) -> list[float]:\n\t        result = openai.Embedding.create(\n\t        model=model,\n\t        input=text\n\t        )\n\t        return result[\"data\"][0][\"embedding\"]\n\t    def get_doc_embedding(self, text: str) -> list[float]:\n\t        return self.get_embedding(text, self.DOC_EMBEDDINGS_MODEL)\n", "    def get_query_embedding(self, text: str) -> list[float]:\n\t        return self.get_embedding(text, self.QUERY_EMBEDDINGS_MODEL)\n\t    def compute_doc_embeddings(self, df: pd.DataFrame) -> dict[tuple[str, str], list[float]]:\n\t        \"\"\"\n\t        Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n\t        Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n\t        \"\"\"\n\t        embeddings = {}\n\t        for idx, r in df.iterrows():\n\t            # Wait one second before making the next call to the OpenAI Embeddings API\n", "            # print(\"Waiting one second before embedding next row\\n\")\n\t            time.sleep(1)\n\t            embeddings[idx] = self.get_doc_embedding(r.content.replace(\"\\n\", \" \"))\n\t        return embeddings\n\t    def save_doc_embeddings_to_csv(self, doc_embeddings: dict, df: pd.DataFrame, csv_filepath: str):\n\t        # Get the dimensionality of the embedding vectors from the first element in the doc_embeddings dictionary\n\t        if len(doc_embeddings) == 0:\n\t            return\n\t        EMBEDDING_DIM = len(list(doc_embeddings.values())[0])\n\t        # Create a new dataframe with the filePath, lineCoverage, and embedding vector columns\n", "        embeddings_df = pd.DataFrame(columns=[\"filePath\", \"lineCoverage\"] + [f\"{i}\" for i in range(EMBEDDING_DIM)])\n\t        # Iterate over the rows in the original dataframe\n\t        for idx, _ in df.iterrows():\n\t            # Get the embedding vector for the current row\n\t            embedding = doc_embeddings[idx]\n\t            # Create a new row in the embeddings dataframe with the filePath, lineCoverage, and embedding vector values\n\t            row = [idx[0], idx[1]] + embedding\n\t            embeddings_df.loc[len(embeddings_df)] = row\n\t        # Save the embeddings dataframe to a CSV file\n\t        embeddings_df.to_csv(csv_filepath, index=False)\n", "    def vector_similarity(self, x: list[float], y: list[float]) -> float:\n\t        return np.dot(np.array(x), np.array(y))\n\t    def order_document_sections_by_query_similarity(self, query: str, contexts: dict[(str, str), np.array]) -> list[(float, (str, str))]:\n\t        \"\"\"\n\t        Find the query embedding for the supplied query, and compare it against all of the pre-calculated document embeddings\n\t        to find the most relevant sections. \n\t        Return the list of document sections, sorted by relevance in descending order.\n\t        \"\"\"\n\t        query_embedding = self.get_query_embedding(query)\n\t        document_similarities = sorted([\n", "            (self.vector_similarity(query_embedding, doc_embedding), doc_index) for doc_index, doc_embedding in contexts.items()\n\t        ], reverse=True)\n\t        return document_similarities\n\t    def load_embeddings(self, fname: str) -> dict[tuple[str, str], list[float]]:       \n\t        df = pd.read_csv(fname, header=0)\n\t        max_dim = max([int(c) for c in df.columns if c != \"filePath\" and c != \"lineCoverage\"])\n\t        return {\n\t            (r.filePath, r.lineCoverage): [r[str(i)] for i in range(max_dim + 1)] for _, r in df.iterrows()\n\t        }"]}
{"filename": "babycoder/babycoder.py", "chunked_list": ["import os\n\timport openai\n\timport time\n\timport sys\n\tfrom typing import List, Dict, Union\n\tfrom dotenv import load_dotenv\n\timport json\n\timport subprocess\n\timport platform\n\tfrom embeddings import Embeddings\n", "# Set Variables\n\tload_dotenv()\n\tcurrent_directory = os.getcwd()\n\tos_version = platform.release()\n\topenai_calls_retried = 0\n\tmax_openai_calls_retries = 3\n\t# Set API Keys\n\tOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n\tassert OPENAI_API_KEY, \"OPENAI_API_KEY environment variable is missing from .env\"\n\topenai.api_key = OPENAI_API_KEY\n", "OPENAI_API_MODEL = os.getenv(\"OPENAI_API_MODEL\", \"gpt-3.5-turbo\")\n\tassert OPENAI_API_MODEL, \"OPENAI_API_MODEL environment variable is missing from .env\"\n\tif \"gpt-4\" in OPENAI_API_MODEL.lower():\n\t    print(\n\t        f\"\\033[91m\\033[1m\"\n\t        + \"\\n*****USING GPT-4. POTENTIALLY EXPENSIVE. MONITOR YOUR COSTS*****\"\n\t        + \"\\033[0m\\033[0m\"\n\t    )\n\tif len(sys.argv) > 1:\n\t    OBJECTIVE = sys.argv[1]\n", "elif os.path.exists(os.path.join(current_directory, \"objective.txt\")):\n\t    with open(os.path.join(current_directory, \"objective.txt\")) as f:\n\t        OBJECTIVE = f.read()\n\tassert OBJECTIVE, \"OBJECTIVE missing\"\n\t## Start of Helper/Utility functions ##\n\tdef print_colored_text(text, color):\n\t    color_mapping = {\n\t        'blue': '\\033[34m',\n\t        'red': '\\033[31m',\n\t        'yellow': '\\033[33m',\n", "        'green': '\\033[32m',\n\t    }\n\t    color_code = color_mapping.get(color.lower(), '')\n\t    reset_code = '\\033[0m'\n\t    print(color_code + text + reset_code)\n\tdef print_char_by_char(text, delay=0.00001, chars_at_once=3):\n\t    for i in range(0, len(text), chars_at_once):\n\t        chunk = text[i:i + chars_at_once]\n\t        print(chunk, end='', flush=True) \n\t        time.sleep(delay) \n", "    print()\n\tdef openai_call(\n\t    prompt: str,\n\t    model: str = OPENAI_API_MODEL,\n\t    temperature: float = 0.5,\n\t    max_tokens: int = 100,\n\t):\n\t    global openai_calls_retried\n\t    if not model.startswith(\"gpt-\"):\n\t        # Use completion API\n", "        response = openai.Completion.create(\n\t            engine=model,\n\t            prompt=prompt,\n\t            temperature=temperature,\n\t            max_tokens=max_tokens,\n\t            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n\t        )\n\t        return response.choices[0].text.strip()\n", "    else:\n\t        # Use chat completion API\n\t        messages=[{\"role\": \"user\", \"content\": prompt}]\n\t        try:\n\t            response = openai.ChatCompletion.create(\n\t                model=model,\n\t                messages=messages,\n\t                temperature=temperature,\n\t                max_tokens=max_tokens,\n\t                n=1,\n", "                stop=None,\n\t            )\n\t            openai_calls_retried = 0\n\t            return response.choices[0].message.content.strip()\n\t        except Exception as e:\n\t            # try again\n\t            if openai_calls_retried < max_openai_calls_retries:\n\t                openai_calls_retried += 1\n\t                print(f\"Error calling OpenAI. Retrying {openai_calls_retried} of {max_openai_calls_retries}...\")\n\t                return openai_call(prompt, model, temperature, max_tokens)\n", "def execute_command_json(json_string):\n\t    try:\n\t        command_data = json.loads(json_string)\n\t        full_command = command_data.get('command')\n\t        process = subprocess.Popen(full_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True, cwd='playground')\n\t        stdout, stderr = process.communicate(timeout=60)\n\t        return_code = process.returncode\n\t        if return_code == 0:\n\t            return stdout\n\t        else:\n", "            return stderr\n\t    except json.JSONDecodeError as e:\n\t        return f\"Error: Unable to decode JSON string: {str(e)}\"\n\t    except subprocess.TimeoutExpired:\n\t        process.terminate()\n\t        return \"Error: Timeout reached (60 seconds)\"\n\t    except Exception as e:\n\t        return f\"Error: {str(e)}\"\n\tdef execute_command_string(command_string):\n\t    try:\n", "        result = subprocess.run(command_string, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, shell=True, cwd='playground')\n\t        output = result.stdout or result.stderr or \"No output\"\n\t        return output\n\t    except Exception as e:\n\t        return f\"Error: {str(e)}\"\n\tdef save_code_to_file(code: str, file_path: str):\n\t    full_path = os.path.join(current_directory, \"playground\", file_path)\n\t    try:\n\t        mode = 'a' if os.path.exists(full_path) else 'w'\n\t        with open(full_path, mode, encoding='utf-8') as f:\n", "            f.write(code + '\\n\\n')\n\t    except:\n\t        pass\n\tdef refactor_code(modified_code: List[Dict[str, Union[int, str]]], file_path: str):\n\t    full_path = os.path.join(current_directory, \"playground\", file_path)\n\t    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n\t        lines = f.readlines()\n\t    for modification in modified_code:\n\t        start_line = modification[\"start_line\"]\n\t        end_line = modification[\"end_line\"]\n", "        modified_chunk = modification[\"modified_code\"].splitlines()\n\t        # Remove original lines within the range\n\t        del lines[start_line - 1:end_line]\n\t        # Insert the new modified_chunk lines\n\t        for i, line in enumerate(modified_chunk):\n\t            lines.insert(start_line - 1 + i, line + \"\\n\")\n\t    with open(full_path, \"w\", encoding=\"utf-8\") as f:\n\t        f.writelines(lines)\n\tdef split_code_into_chunks(file_path: str, chunk_size: int = 50) -> List[Dict[str, Union[int, str]]]:\n\t    full_path = os.path.join(current_directory, \"playground\", file_path)\n", "    with open(full_path, \"r\", encoding=\"utf-8\") as f:\n\t        lines = f.readlines()\n\t    chunks = []\n\t    for i in range(0, len(lines), chunk_size):\n\t        start_line = i + 1\n\t        end_line = min(i + chunk_size, len(lines))\n\t        chunk = {\"start_line\": start_line, \"end_line\": end_line, \"code\": \"\".join(lines[i:end_line])}\n\t        chunks.append(chunk)\n\t    return chunks\n\t## End of Helper/Utility functions ##\n", "## TASKS AGENTS ##\n\tdef code_tasks_initializer_agent(objective: str):\n\t    prompt = f\"\"\"You are an AGI agent responsible for creating a detailed JSON checklist of tasks that will guide other AGI agents to complete a given programming objective. Your task is to analyze the provided objective and generate a well-structured checklist with a clear starting point and end point, as well as tasks broken down to be very specific, clear, and executable by other agents without the context of other tasks.\n\t    The current agents work as follows:\n\t    - code_writer_agent: Writes code snippets or functions and saves them to the appropriate files. This agent can also append code to existing files if required.\n\t    - code_refactor_agent: Responsible for modifying and refactoring existing code to meet the requirements of the task.\n\t    - command_executor_agent: Executes terminal commands for tasks such as creating directories, installing dependencies, etc.\n\t    Keep in mind that the agents cannot open files in text editors, and tasks should be designed to work within these agent capabilities.\n\t    Here is the programming objective you need to create a checklist for: {objective}.\n\t    To generate the checklist, follow these steps:\n", "    1. Analyze the objective to identify the high-level requirements and goals of the project. This will help you understand the scope and create a comprehensive checklist.\n\t    2. Break down the objective into smaller, highly specific tasks that can be worked on independently by other agents. Ensure that the tasks are designed to be executed by the available agents (code_writer_agent, code_refactor and command_executor_agent) without requiring opening files in text editors.\n\t    3. Assign a unique ID to each task for easy tracking and organization. This will help the agents to identify and refer to specific tasks in the checklist.\n\t    4. Organize the tasks in a logical order, with a clear starting point and end point. The starting point should represent the initial setup or groundwork necessary for the project, while the end point should signify the completion of the objective and any finalization steps.\n\t    5. Provide the current context for each task, which should be sufficient for the agents to understand and execute the task without referring to other tasks in the checklist. This will help agents avoid task duplication.\n\t    6. Pay close attention to the objective and make sure the tasks implement all necessary pieces needed to make the program work.\n\t    7. Compile the tasks into a well-structured JSON format, ensuring that it is easy to read and parse by other AGI agents. The JSON should include fields such as task ID, description and file_path.\n\t    IMPORTANT: BE VERY CAREFUL WITH IMPORTS AND MANAGING MULTIPLE FILES. REMEMBER EACH AGENT WILL ONLY SEE A SINGLE TASK. ASK YOURSELF WHAT INFORMATION YOU NEED TO INCLUDE IN THE CONTEXT OF EACH TASK TO MAKE SURE THE AGENT CAN EXECUTE THE TASK WITHOUT SEEING THE OTHER TASKS OR WHAT WAS ACCOMPLISHED IN OTHER TASKS.\n\t    Pay attention to the way files are passed in the tasks, always use full paths. For example 'project/main.py'.\n\t    Make sure tasks are not duplicated.\n", "    Do not take long and complex routes, minimize tasks and steps as much as possible.\n\t    Here is a sample JSON output for a checklist:\n\t            {{\n\t                \"tasks\": [\n\t                    {{\n\t                    \"id\": 1,\n\t                    \"description\": \"Run a command to create the project directory named 'project'\",\n\t                    \"file_path\": \"./project\",\n\t                    }},\n\t                    {{\n", "                    \"id\": 2,\n\t                    \"description\": \"Run a command to Install the following dependencies: 'numpy', 'pandas', 'scikit-learn', 'matplotlib'\",\n\t                    \"file_path\": \"null\",\n\t                    }},\n\t                    {{\n\t                    \"id\": 3,\n\t                    \"description\": \"Write code to create a function named 'parser' that takes an input named 'input' of type str, [perform a specific task on it], and returns a specific output\",\n\t                    \"file_path\": \"./project/main.py\",\n\t                    }},\n\t                    ...\n", "                    {{\n\t                    \"id\": N,\n\t                    \"description\": \"...\",\n\t                    }}\n\t                ],\n\t            }}\n\t    The tasks will be executed by either of the three agents: command_executor, code_writer or code_refactor. They can't interact with programs. They can either run terminal commands or write code snippets. Their output is controlled by other functions to run the commands or save their output to code files. Make sure the tasks are compatible with the current agents. ALL tasks MUST start either with the following phrases: 'Run a command to...', 'Write code to...', 'Edit existing code to...' depending on the agent that will execute the task. RETURN JSON ONLY:\"\"\"\n\t    return openai_call(prompt, temperature=0.8, max_tokens=2000)\n\tdef code_tasks_refactor_agent(objective: str, task_list_json):\n\t    prompt = f\"\"\"You are an AGI tasks_refactor_agent responsible for adapting a task list generated by another agent to ensure the tasks are compatible with the current AGI agents. Your goal is to analyze the task list and make necessary modifications so that the tasks can be executed by the agents listed below\n", "    YOU SHOULD OUTPUT THE MODIFIED TASK LIST IN THE SAME JSON FORMAT AS THE INITIAL TASK LIST. DO NOT CHANGE THE FORMAT OF THE JSON OUTPUT. DO NOT WRITE ANYTHING OTHER THAN THE MODIFIED TASK LIST IN THE JSON FORMAT.\n\t    The current agents work as follows:\n\t    - code_writer_agent: Writes code snippets or functions and saves them to the appropriate files. This agent can also append code to existing files if required.\n\t    - code_refactor_agent: Responsible for editing current existing code/files.\n\t    - command_executor_agent: Executes terminal commands for tasks such as creating directories, installing dependencies, etc.\n\t    Here is the overall objective you need to refactor the tasks for: {objective}.\n\t    Here is the JSON task list you need to refactor for compatibility with the current agents: {task_list_json}.\n\t    To refactor the task list, follow these steps:\n\t    1. Modify the task descriptions to make them compatible with the current agents, ensuring that the tasks are self-contained, clear, and executable by the agents without additional context. You don't need to mention the agents in the task descriptions, but the tasks should be compatible with the current agents.\n\t    2. If necessary, add new tasks or remove irrelevant tasks to make the task list more suitable for the current agents.\n", "    3. Keep the JSON structure of the task list intact, maintaining the \"id\", \"description\" and \"file_path\" fields for each task.\n\t    4. Pay close attention to the objective and make sure the tasks implement all necessary pieces needed to make the program work.\n\t    Always specify file paths to files. Make sure tasks are not duplicated. Never write code to create files. If needed, use commands to create files and folders.\n\t    Return the updated JSON task list with the following format:\n\t            {{\n\t                \"tasks\": [\n\t                    {{\n\t                    \"id\": 1,\n\t                    \"description\": \"Run a commmand to create a folder named 'project' in the current directory\",\n\t                    \"file_path\": \"./project\",\n", "                    }},\n\t                    {{\n\t                    \"id\": 2,\n\t                    \"description\": \"Write code to print 'Hello World!' with Python\",\n\t                    \"file_path\": \"./project/main.py\",\n\t                    }},\n\t                    {{\n\t                    \"id\": 3,\n\t                    \"description\": \"Write code to create a function named 'parser' that takes an input named 'input' of type str, [perform a specific task on it], and returns a specific output\",\n\t                    \"file_path\": \"./project/main.py\",\n", "                    }}\n\t                    {{\n\t                    \"id\": 3,\n\t                    \"description\": \"Run a command calling the script in ./project/main.py\",\n\t                    \"file_path\": \"./project/main.py\",\n\t                    }}\n\t                    ...\n\t                ],\n\t            }}\n\t    IMPORTANT: All tasks should start either with the following phrases: 'Run a command to...', 'Write a code to...', 'Edit the code to...' depending on the agent that will execute the task:\n", "    ALWAYS ENSURE ALL TASKS HAVE RELEVANT CONTEXT ABOUT THE CODE TO BE WRITTEN, INCLUDE DETAILS ON HOW TO CALL FUNCTIONS, CLASSES, IMPORTS, ETC. AGENTS HAVE NO VIEW OF OTHER TASKS, SO THEY NEED TO BE SELF-CONTAINED. RETURN THE JSON:\"\"\"\n\t    return openai_call(prompt, temperature=0, max_tokens=2000)\n\tdef code_tasks_details_agent(objective: str, task_list_json):\n\t    prompt = f\"\"\"You are an AGI agent responsible for improving a list of tasks in JSON format and adding ALL the necessary details to each task. These tasks will be executed individually by agents that have no idea about other tasks or what code exists in the codebase. It is FUNDAMENTAL that each task has enough details so that an individual isolated agent can execute. The metadata of the task is the only information the agents will have.\n\t    Each task should contain the details necessary to execute it. For example, if it creates a function, it needs to contain the details about the arguments to be used in that function and this needs to be consistent across all tasks.\n\t    Look at all tasks at once, and update the task description adding details to it for each task so that it can be executed by an agent without seeing the other tasks and to ensure consistency across all tasks. DETAILS ARE CRUCIAL. For example, if one task creates a class, it should have all the details about the class, including the arguments to be used in the constructor. If another task creates a function that uses the class, it should have the details about the class and the arguments to be used in the constructor.\n\t    RETURN JSON OUTPUTS ONLY.\n\t    Here is the overall objective you need to refactor the tasks for: {objective}.\n\t    Here is the task list you need to improve: {task_list_json}\n\t    RETURN THE SAME TASK LIST but with the description improved to contain the details you is adding for each task in the list. DO NOT MAKE OTHER MODIFICATIONS TO THE LIST. Your input should go in the 'description' field of each task.\n", "    RETURN JSON ONLY:\"\"\"\n\t    return openai_call(prompt, temperature=0.7, max_tokens=2000)\n\tdef code_tasks_context_agent(objective: str, task_list_json):\n\t    prompt = f\"\"\"You are an AGI agent responsible for improving a list of tasks in JSON format and adding ALL the necessary context to it. These tasks will be executed individually by agents that have no idea about other tasks or what code exists in the codebase. It is FUNDAMENTAL that each task has enough context so that an individual isolated agent can execute. The metadata of the task is the only information the agents will have.\n\t    Look at all tasks at once, and add the necessary context to each task so that it can be executed by an agent without seeing the other tasks. Remember, one agent can only see one task and has no idea about what happened in other tasks. CONTEXT IS CRUCIAL. For example, if one task creates one folder and the other tasks creates a file in that folder. The second tasks should contain the name of the folder that already exists and the information that it already exists.\n\t    This is even more important for tasks that require importing functions, classes, etc. If a task needs to call a function or initialize a Class, it needs to have the detailed arguments, etc.\n\t    Note that you should identify when imports need to happen and specify this in the context. Also, you should identify when functions/classes/etc already exist and specify this very clearly because the agents sometimes duplicate things not knowing.\n\t    Always use imports with the file name. For example, 'from my_script import MyScript'. \n\t    RETURN JSON OUTPUTS ONLY.\n\t    Here is the overall objective you need to refactor the tasks for: {objective}.\n", "    Here is the task list you need to improve: {task_list_json}\n\t    RETURN THE SAME TASK LIST but with a new field called 'isolated_context' for each task in the list. This field should be a string with the context you are adding. DO NOT MAKE OTHER MODIFICATIONS TO THE LIST.\n\t    RETURN JSON ONLY:\"\"\"\n\t    return openai_call(prompt, temperature=0.7, max_tokens=2000)\n\tdef task_assigner_recommendation_agent(objective: str, task: str):\n\t    prompt = f\"\"\"You are an AGI agent responsible for providing recommendations on which agent should be used to handle a specific task. Analyze the provided major objective of the project and a single task from the JSON checklist generated by the previous agent, and suggest the most appropriate agent to work on the task.\n\t    The overall objective is: {objective}\n\t    The current task is: {task}\n\t    The available agents are:\n\t    1. code_writer_agent: Responsible for writing code based on the task description.\n", "    2. code_refactor_agent: Responsible for editing existing code.\n\t    3. command_executor_agent: Responsible for executing commands and handling file operations, such as creating, moving, or deleting files.\n\t    When analyzing the task, consider the following tips:\n\t    - Pay attention to keywords in the task description that indicate the type of action required, such as \"write\", \"edit\", \"run\", \"create\", \"move\", or \"delete\".\n\t    - Keep the overall objective in mind, as it can help you understand the context of the task and guide your choice of agent.\n\t    - If the task involves writing new code or adding new functionality, consider using the code_writer_agent.\n\t    - If the task involves modifying or optimizing existing code, consider using the code_refactor_agent.\n\t    - If the task involves file operations, command execution, or running a script, consider using the command_executor_agent.\n\t    Based on the task and overall objective, suggest the most appropriate agent to work on the task.\"\"\"\n\t    return openai_call(prompt, temperature=0.5, max_tokens=2000)\n", "def task_assigner_agent(objective: str, task: str, recommendation: str):\n\t    prompt = f\"\"\"You are an AGI agent responsible for choosing the best agent to work on a given task. Your goal is to analyze the provided major objective of the project and a single task from the JSON checklist generated by the previous agent, and choose the best agent to work on the task.\n\t    The overall objective is: {objective}\n\t    The current task is: {task}\n\t    Use this recommendation to guide you: {recommendation}\n\t    The available agents are:\n\t    1. code_writer_agent: Responsible for writing code based on the task description.\n\t    2. code_refactor_agent: Responsible for editing existing code.\n\t    2. command_executor_agent: Responsible for executing commands and handling file operations, such as creating, moving, or deleting files.\n\t    Please consider the task description and the overall objective when choosing the most appropriate agent. Keep in mind that creating a file and writing code are different tasks. If the task involves creating a file, like \"calculator.py\" but does not mention writing any code inside it, the command_executor_agent should be used for this purpose. The code_writer_agent should only be used when the task requires writing or adding code to a file. The code_refactor_agent should only be used when the task requires modifying existing code.\n", "    TLDR: To create files, use command_executor_agent, to write text/code to files, use code_writer_agent, to modify existing code, use code_refactor_agent.\n\t    Choose the most appropriate agent to work on the task and return a JSON output with the following format: {{\"agent\": \"agent_name\"}}. ONLY return JSON output:\"\"\"\n\t    return openai_call(prompt, temperature=0, max_tokens=2000)\n\tdef command_executor_agent(task: str, file_path: str):\n\t    prompt = f\"\"\"You are an AGI agent responsible for executing a given command on the {os_version} OS. Your goal is to analyze the provided major objective of the project and a single task from the JSON checklist generated by the previous agent, and execute the command on the {os_version} OS. \n\t    The current task is: {task}\n\t    File or folder name referenced in the task (relative file path): {file_path} \n\t    Based on the task, write the appropriate command to execute on the {os_version} OS. Make sure the command is relevant to the task and objective. For example, if the task is to create a new folder, the command should be 'mkdir new_folder_name'. Return the command as a JSON output with the following format: {{\"command\": \"command_to_execute\"}}. ONLY return JSON output:\"\"\"\n\t    return openai_call(prompt, temperature=0, max_tokens=2000)\n\tdef code_writer_agent(task: str, isolated_context: str, context_code_chunks):\n", "    prompt = f\"\"\"You are an AGI agent responsible for writing code to accomplish a given task. Your goal is to analyze the provided major objective of the project and a single task from the JSON checklist generated by the previous agent, and write the necessary code to complete the task.\n\t    The current task is: {task}\n\t    To help you make the code useful in this codebase, use this context as reference of the other pieces of the codebase that are relevant to your task. PAY ATTENTION TO THIS: {isolated_context}\n\t    The following code chunks were found to be relevant to the task. You can use them as reference to write the code if they are useful. PAY CLOSE ATTENTION TO THIS: \n\t    {context_code_chunks}\n\t    Note: Always use 'encoding='utf-8'' when opening files with open().\n\t    Based on the task and objective, write the appropriate code to achieve the task. Make sure the code is relevant to the task and objective, and follows best practices. Return the code as a plain text output and NOTHING ELSE. Use identation and line breaks in the in the code. Make sure to only write the code and nothing else as your output will be saved directly to the file by other agent. IMPORTANT\" If the task is asking you to write code to write files, this is a mistake! Interpret it and either do nothing or return  the plain code, not a code to write file, not a code to write code, etc.\"\"\"\n\t    return openai_call(prompt, temperature=0, max_tokens=2000)\n\tdef code_refactor_agent(task_description: str, existing_code_snippet: str, context_chunks, isolated_context: str):\n\t    prompt = f\"\"\"You are an AGI agent responsible for refactoring code to accomplish a given task. Your goal is to analyze the provided major objective of the project, the task descriptionm and refactor the code accordingly.\n", "    The current task description is: {task_description}\n\t    To help you make the code useful in this codebase, use this context as reference of the other pieces of the codebase that are relevant to your task: {isolated_context}\n\t    Here are some context chunks that might be relevant to the task:\n\t    {context_chunks}\n\t    Existing code you should refactor: \n\t    {existing_code_snippet}\n\t    Based on the task description, objective, refactor the existing code to achieve the task. Make sure the refactored code is relevant to the task and objective, follows best practices, etc.\n\t    Return a plain text code snippet with your refactored code. IMPORTANT: JUST RETURN CODE, YOUR OUTPUT WILL BE ADDED DIRECTLY TO THE FILE BY OTHER AGENT. BE MINDFUL OF THIS:\"\"\"\n\t    return openai_call(prompt, temperature=0, max_tokens=2000)\n\tdef file_management_agent(objective: str, task: str, current_directory_files: str, file_path: str):\n", "    prompt = f\"\"\"You are an AGI agent responsible for managing files in a software project. Your goal is to analyze the provided major objective of the project and a single task from the JSON checklist generated by the previous agent, and determine the appropriate file path and name for the generated code.\n\t    The overall objective is: {objective}\n\t    The current task is: {task}\n\t    Specified file path (relative path from the current dir): {file_path}\n\t    Make the file path adapted for the current directory files. The current directory files are: {current_directory_files}. Assume this file_path will be interpreted from the root path of the directory.\n\t    Do not use '.' or './' in the file path.\n\t    BE VERY SPECIFIC WITH THE FILES, AVOID FILE DUPLICATION, AVOID SPECIFYING THE SAME FILE NAME UNDER DIFFERENT FOLDERS, ETC.\n\t    Based on the task, determine the file path and name for the generated code. Return the file path and name as a JSON output with the following format: {{\"file_path\": \"file_path_and_name\"}}. ONLY return JSON output:\"\"\"\n\t    return openai_call(prompt, temperature=0, max_tokens=2000)\n\tdef code_relevance_agent(objective: str, task_description: str, code_chunk: str):\n", "    prompt = f\"\"\"You are an AGI agent responsible for evaluating the relevance of a code chunk in relation to a given task. Your goal is to analyze the provided major objective of the project, the task description, and the code chunk, and assign a relevance score from 0 to 10, where 0 is completely irrelevant and 10 is highly relevant.\n\t    The overall objective is: {objective}\n\t    The current task description is: {task_description}\n\t    The code chunk is as follows (line numbers included):\n\t    {code_chunk}\n\t    Based on the task description, objective, and code chunk, assign a relevance score between 0 and 10 (inclusive) for the code chunk. DO NOT OUTPUT ANYTHING OTHER THAN THE RELEVANCE SCORE AS A NUMBER.\"\"\"\n\t    relevance_score = openai_call(prompt, temperature=0.5, max_tokens=50)\n\t    return json.dumps({\"relevance_score\": relevance_score.strip()})\n\tdef task_human_input_agent(task: str, human_feedback: str):\n\t    prompt = f\"\"\"You are an AGI agent responsible for getting human input to improve the quality of tasks in a software project. Your goal is to analyze the provided task and adapt it based on the human's suggestions. The tasks should  start with either 'Run a command to...', 'Write code to...', or 'Edit existing code to...' depending on the agent that will execute the task.\n", "    For context, this task will be executed by other AGI agents with the following characteristics:\n\t    - code_writer_agent: Writes code snippets or functions and saves them to the appropriate files. This agent can also append code to existing files if required.\n\t    - code_refactor_agent: Responsible for modifying and refactoring existing code to meet the requirements of the task.\n\t    - command_executor_agent: Executes terminal commands for tasks such as creating directories, installing dependencies, etc.\n\t    The current task is:\n\t    {task}\n\t    The human feedback is:\n\t    {human_feedback}\n\t    If the human feedback is empty, return the task as is. If the human feedback is saying to ignore the task, return the following string: <IGNORE_TASK>\n\t    Note that your output will replace the existing task, so make sure that your output is a valid task that starts with one of the required phrases ('Run a command to...', 'Write code to...', 'Edit existing code to...').\n", "    Please adjust the task based on the human feedback while ensuring it starts with one of the required phrases ('Run a command to...', 'Write code to...', 'Edit existing code to...'). Return the improved task as a plain text output and nothing else. Write only the new task.\"\"\"\n\t    return openai_call(prompt, temperature=0.3, max_tokens=200)\n\t## END OF AGENTS ##\n\tprint_colored_text(f\"****Objective****\", color='green')\n\tprint_char_by_char(OBJECTIVE, 0.00001, 10)\n\t# Create the tasks\n\tprint_colored_text(\"*****Working on tasks*****\", \"red\")\n\tprint_colored_text(\" - Creating initial tasks\", \"yellow\")\n\ttask_agent_output = code_tasks_initializer_agent(OBJECTIVE)\n\tprint_colored_text(\" - Reviewing and refactoring tasks to fit agents\", \"yellow\")\n", "task_agent_output = code_tasks_refactor_agent(OBJECTIVE, task_agent_output)\n\tprint_colored_text(\" - Adding relevant technical details to the tasks\", \"yellow\")\n\ttask_agent_output = code_tasks_details_agent(OBJECTIVE, task_agent_output)\n\tprint_colored_text(\" - Adding necessary context to the tasks\", \"yellow\")\n\ttask_agent_output = code_tasks_context_agent(OBJECTIVE, task_agent_output)\n\tprint()\n\tprint_colored_text(\"*****TASKS*****\", \"green\")\n\tprint_char_by_char(task_agent_output, 0.00000001, 10)\n\t# Task list\n\ttask_json = json.loads(task_agent_output)\n", "embeddings = Embeddings(current_directory)\n\tfor task in task_json[\"tasks\"]:\n\t    task_description = task[\"description\"]\n\t    task_isolated_context = task[\"isolated_context\"]\n\t    print_colored_text(\"*****TASK*****\", \"yellow\")\n\t    print_char_by_char(task_description)\n\t    print_colored_text(\"*****TASK CONTEXT*****\", \"yellow\")\n\t    print_char_by_char(task_isolated_context)\n\t    # HUMAN FEEDBACK\n\t    # Uncomment below to enable human feedback before each task. This can be used to improve the quality of the tasks,\n", "    # skip tasks, etc. I believe it may be very relevant in future versions that may have more complex tasks and could\n\t    # allow a ton of automation when working on large projects.\n\t    #\n\t    # Get user input as a feedback to the task_description\n\t    # print_colored_text(\"*****TASK FEEDBACK*****\", \"yellow\")\n\t    # user_input = input(\"\\n>:\")\n\t    # task_description = task_human_input_agent(task_description, user_input)\n\t    # if task_description == \"<IGNORE_TASK>\":\n\t    #     continue\n\t    # print_colored_text(\"*****IMPROVED TASK*****\", \"green\")\n", "    # print_char_by_char(task_description)\n\t    # Assign the task to an agent\n\t    task_assigner_recommendation = task_assigner_recommendation_agent(OBJECTIVE, task_description)\n\t    task_agent_output = task_assigner_agent(OBJECTIVE, task_description, task_assigner_recommendation)\n\t    print_colored_text(\"*****ASSIGN*****\", \"yellow\")\n\t    print_char_by_char(task_agent_output)\n\t    chosen_agent = json.loads(task_agent_output)[\"agent\"]\n\t    if chosen_agent == \"command_executor_agent\":\n\t        command_executor_output = command_executor_agent(task_description, task[\"file_path\"])\n\t        print_colored_text(\"*****COMMAND*****\", \"green\")\n", "        print_char_by_char(command_executor_output)\n\t        command_execution_output = execute_command_json(command_executor_output)\n\t    else:\n\t        # CODE AGENTS\n\t        if chosen_agent == \"code_writer_agent\":\n\t            # Compute embeddings for the codebase\n\t            # This will recompute embeddings for all files in the 'playground' directory\n\t            print_colored_text(\"*****RETRIEVING RELEVANT CODE CONTEXT*****\", \"yellow\")\n\t            embeddings.compute_repository_embeddings()\n\t            relevant_chunks = embeddings.get_relevant_code_chunks(task_description, task_isolated_context)\n", "            current_directory_files = execute_command_string(\"ls\")\n\t            file_management_output = file_management_agent(OBJECTIVE, task_description, current_directory_files, task[\"file_path\"])\n\t            print_colored_text(\"*****FILE MANAGEMENT*****\", \"yellow\")\n\t            print_char_by_char(file_management_output)\n\t            file_path = json.loads(file_management_output)[\"file_path\"]\n\t            code_writer_output = code_writer_agent(task_description, task_isolated_context, relevant_chunks)\n\t            print_colored_text(\"*****CODE*****\", \"green\")\n\t            print_char_by_char(code_writer_output)\n\t            # Save the generated code to the file the agent selected\n\t            save_code_to_file(code_writer_output, file_path)\n", "        elif chosen_agent == \"code_refactor_agent\":\n\t            # The code refactor agent works with multiple agents:\n\t            # For each task, the file_management_agent is used to select the file to edit.Then, the \n\t            # code_relevance_agent is used to select the relevant code chunks from that filewith the \n\t            # goal of finding the code chunk that is most relevant to the task description. This is \n\t            # the code chunk that will be edited. Finally, the code_refactor_agent is used to edit \n\t            # the code chunk.\n\t            current_directory_files = execute_command_string(\"ls\")\n\t            file_management_output = file_management_agent(OBJECTIVE, task_description, current_directory_files, task[\"file_path\"])\n\t            file_path = json.loads(file_management_output)[\"file_path\"]\n", "            print_colored_text(\"*****FILE MANAGEMENT*****\", \"yellow\")\n\t            print_char_by_char(file_management_output)\n\t            # Split the code into chunks and get the relevance scores for each chunk\n\t            code_chunks = split_code_into_chunks(file_path, 80)\n\t            print_colored_text(\"*****ANALYZING EXISTING CODE*****\", \"yellow\")\n\t            relevance_scores = []\n\t            for chunk in code_chunks:\n\t                score = code_relevance_agent(OBJECTIVE, task_description, chunk[\"code\"])\n\t                relevance_scores.append(score)\n\t            # Select the most relevant chunk\n", "            selected_chunk = sorted(zip(relevance_scores, code_chunks), key=lambda x: x[0], reverse=True)[0][1]\n\t            # Refactor the code\n\t            modified_code_output = code_refactor_agent(task_description, selected_chunk, context_chunks=[selected_chunk], isolated_context=task_isolated_context)\n\t            # Extract the start_line and end_line of the selected chunk. This will be used to replace the code in the original file\n\t            start_line = selected_chunk[\"start_line\"]\n\t            end_line = selected_chunk[\"end_line\"]\n\t            # Count the number of lines in the modified_code_output\n\t            modified_code_lines = modified_code_output.count(\"\\n\") + 1\n\t            # Create a dictionary with the necessary information for the refactor_code function\n\t            modified_code_info = {\n", "                \"start_line\": start_line,\n\t                \"end_line\": start_line + modified_code_lines - 1,\n\t                \"modified_code\": modified_code_output\n\t            }\n\t            print_colored_text(\"*****REFACTORED CODE*****\", \"green\")\n\t            print_char_by_char(modified_code_output)\n\t            # Save the refactored code to the file\n\t            refactor_code([modified_code_info], file_path)\n"]}
{"filename": "classic/babyagi.py", "chunked_list": ["import openai\n\timport pinecone\n\timport time\n\tfrom collections import deque\n\tfrom typing import Dict, List\n\t#Set API Keys\n\tOPENAI_API_KEY = \"\"\n\tPINECONE_API_KEY = \"\"\n\tPINECONE_ENVIRONMENT = \"us-east1-gcp\" #Pinecone Environment (eg. \"us-east1-gcp\")\n\t#Set Variables\n", "YOUR_TABLE_NAME = \"test-table\"\n\tOBJECTIVE = \"Solve world hunger.\"\n\tYOUR_FIRST_TASK = \"Develop a task list.\"\n\t#Print OBJECTIVE\n\tprint(\"\\033[96m\\033[1m\"+\"\\n*****OBJECTIVE*****\\n\"+\"\\033[0m\\033[0m\")\n\tprint(OBJECTIVE)\n\t# Configure OpenAI and Pinecone\n\topenai.api_key = OPENAI_API_KEY\n\tpinecone.init(api_key=PINECONE_API_KEY, environment=PINECONE_ENVIRONMENT)\n\t# Create Pinecone index\n", "table_name = YOUR_TABLE_NAME\n\tdimension = 1536\n\tmetric = \"cosine\"\n\tpod_type = \"p1\"\n\tif table_name not in pinecone.list_indexes():\n\t    pinecone.create_index(table_name, dimension=dimension, metric=metric, pod_type=pod_type)\n\t# Connect to the index\n\tindex = pinecone.Index(table_name)\n\t# Task list\n\ttask_list = deque([])\n", "def add_task(task: Dict):\n\t    task_list.append(task)\n\tdef get_ada_embedding(text):\n\t    text = text.replace(\"\\n\", \" \")\n\t    return openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n\tdef task_creation_agent(objective: str, result: Dict, task_description: str, task_list: List[str]):\n\t    prompt = f\"You are an task creation AI that uses the result of an execution agent to create new tasks with the following objective: {objective}, The last completed task has the result: {result}. This result was based on this task description: {task_description}. These are incomplete tasks: {', '.join(task_list)}. Based on the result, create new tasks to be completed by the AI system that do not overlap with incomplete tasks. Return the tasks as an array.\"\n\t    response = openai.Completion.create(engine=\"text-davinci-003\",prompt=prompt,temperature=0.5,max_tokens=100,top_p=1,frequency_penalty=0,presence_penalty=0)\n\t    new_tasks = response.choices[0].text.strip().split('\\n')\n\t    return [{\"task_name\": task_name} for task_name in new_tasks]\n", "def prioritization_agent(this_task_id:int):\n\t    global task_list\n\t    task_names = [t[\"task_name\"] for t in task_list]\n\t    next_task_id = int(this_task_id)+1\n\t    prompt = f\"\"\"You are an task prioritization AI tasked with cleaning the formatting of and reprioritizing the following tasks: {task_names}. Consider the ultimate objective of your team:{OBJECTIVE}. Do not remove any tasks. Return the result as a numbered list, like:\n\t    #. First task\n\t    #. Second task\n\t    Start the task list with number {next_task_id}.\"\"\"\n\t    response = openai.Completion.create(engine=\"text-davinci-003\",prompt=prompt,temperature=0.5,max_tokens=1000,top_p=1,frequency_penalty=0,presence_penalty=0)\n\t    new_tasks = response.choices[0].text.strip().split('\\n')\n", "    task_list = deque()\n\t    for task_string in new_tasks:\n\t        task_parts = task_string.strip().split(\".\", 1)\n\t        if len(task_parts) == 2:\n\t            task_id = task_parts[0].strip()\n\t            task_name = task_parts[1].strip()\n\t            task_list.append({\"task_id\": task_id, \"task_name\": task_name})\n\tdef execution_agent(objective:str,task: str) -> str:\n\t    #context = context_agent(index=\"quickstart\", query=\"my_search_query\", n=5)\n\t    context=context_agent(index=YOUR_TABLE_NAME, query=objective, n=5)\n", "    #print(\"\\n*******RELEVANT CONTEXT******\\n\")\n\t    #print(context)\n\t    response = openai.Completion.create(\n\t        engine=\"text-davinci-003\",\n\t        prompt=f\"You are an AI who performs one task based on the following objective: {objective}. Your task: {task}\\nResponse:\",\n\t        temperature=0.7,\n\t        max_tokens=2000,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0\n", "    )\n\t    return response.choices[0].text.strip()\n\tdef context_agent(query: str, index: str, n: int):\n\t    query_embedding = get_ada_embedding(query)\n\t    index = pinecone.Index(index_name=index)\n\t    results = index.query(query_embedding, top_k=n,\n\t    include_metadata=True)\n\t    #print(\"***** RESULTS *****\")\n\t    #print(results)\n\t    sorted_results = sorted(results.matches, key=lambda x: x.score, reverse=True)    \n", "    return [(str(item.metadata['task'])) for item in sorted_results]\n\t# Add the first task\n\tfirst_task = {\n\t    \"task_id\": 1,\n\t    \"task_name\": YOUR_FIRST_TASK\n\t}\n\tadd_task(first_task)\n\t# Main loop\n\ttask_id_counter = 1\n\twhile True:\n", "    if task_list:\n\t        # Print the task list\n\t        print(\"\\033[95m\\033[1m\"+\"\\n*****TASK LIST*****\\n\"+\"\\033[0m\\033[0m\")\n\t        for t in task_list:\n\t            print(str(t['task_id'])+\": \"+t['task_name'])\n\t        # Step 1: Pull the first task\n\t        task = task_list.popleft()\n\t        print(\"\\033[92m\\033[1m\"+\"\\n*****NEXT TASK*****\\n\"+\"\\033[0m\\033[0m\")\n\t        print(str(task['task_id'])+\": \"+task['task_name'])\n\t        # Send to execution function to complete the task based on the context\n", "        result = execution_agent(OBJECTIVE,task[\"task_name\"])\n\t        this_task_id = int(task[\"task_id\"])\n\t        print(\"\\033[93m\\033[1m\"+\"\\n*****TASK RESULT*****\\n\"+\"\\033[0m\\033[0m\")\n\t        print(result)\n\t        # Step 2: Enrich result and store in Pinecone\n\t        enriched_result = {'data': result}  # This is where you should enrich the result if needed\n\t        result_id = f\"result_{task['task_id']}\"\n\t        vector = enriched_result['data']  # extract the actual result from the dictionary\n\t        index.upsert([(result_id, get_ada_embedding(vector),{\"task\":task['task_name'],\"result\":result})])\n\t    # Step 3: Create new tasks and reprioritize task list\n", "    new_tasks = task_creation_agent(OBJECTIVE,enriched_result, task[\"task_name\"], [t[\"task_name\"] for t in task_list])\n\t    for new_task in new_tasks:\n\t        task_id_counter += 1\n\t        new_task.update({\"task_id\": task_id_counter})\n\t        add_task(new_task)\n\t    prioritization_agent(this_task_id)\n\ttime.sleep(1)  # Sleep before checking the task list again\n"]}
{"filename": "classic/BabyDeerAGI.py", "chunked_list": ["###### This is a modified version of OG BabyAGI, called BabyDeerAGI (modifications will follow the pattern \"Baby<animal>AGI\").######\n\t######IMPORTANT NOTE: I'm sharing this as a framework to build on top of (with lots of room for improvement), to facilitate discussion around how to improve these. This is NOT for people who are looking for a complete solution that's ready to use. ######\n\timport openai\n\timport time\n\tfrom datetime import datetime\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tfrom collections import deque\n\tfrom typing import Dict, List\n\timport re\n", "import ast\n\timport json\n\tfrom serpapi import GoogleSearch\n\tfrom concurrent.futures import ThreadPoolExecutor\n\timport time\n\t### SET THESE 4 VARIABLES ##############################\n\t# Add your API keys here\n\tOPENAI_API_KEY = \"\"\n\tSERPAPI_API_KEY = \"\" #[optional] web-search becomes available automatically when serpapi api key is provided\n\t# Set variables\n", "OBJECTIVE = \"Research recent AI news and write a poem about your findings in the style of shakespeare.\"\n\t#turn on user input (change to \"True\" to turn on user input tool)\n\tuser_input=False\n\t### UP TO HERE ##############################\n\t# Configure OpenAI and SerpAPI client\n\topenai.api_key = OPENAI_API_KEY\n\tif SERPAPI_API_KEY:\n\t  serpapi_client = GoogleSearch({\"api_key\": SERPAPI_API_KEY})\n\t  websearch_var = \"[web-search] \"\n\telse:\n", "  websearch_var = \"\"\n\tif user_input == True:\n\t  user_input_var = \"[user-input]\"\n\telse:\n\t  user_input_var = \"\"\n\t# Initialize task list\n\ttask_list = []\n\t# Initialize session_summary\n\tsession_summary = \"OBJECTIVE: \"+OBJECTIVE+\"\\n\\n\"\n\t### Task list functions ##############################\n", "def get_task_by_id(task_id: int):\n\t    for task in task_list:\n\t        if task[\"id\"] == task_id:\n\t            return task\n\t    return None\n\t# Print task list and session summary\n\tdef print_tasklist():\n\t  p_tasklist=\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\"\n\t  for t in task_list:\n\t      dependent_task = \"\"\n", "      if t['dependent_task_ids']:\n\t          dependent_task = f\"\\033[31m<dependencies: {', '.join([f'#{dep_id}' for dep_id in t['dependent_task_ids']])}>\\033[0m\"\n\t      status_color = \"\\033[32m\" if t['status'] == \"complete\" else \"\\033[31m\"\n\t      p_tasklist+= f\"\\033[1m{t['id']}\\033[0m: {t['task']} {status_color}[{t['status']}]\\033[0m \\033[93m[{t['tool']}] {dependent_task}\\033[0m\\n\"\n\t  print(p_tasklist)\n\t### Tool functions ##############################\n\tdef text_completion_tool(prompt: str):\n\t    messages = [\n\t        {\"role\": \"user\", \"content\": prompt}\n\t    ]\n", "    response = openai.ChatCompletion.create(\n\t        model=\"gpt-3.5-turbo\",\n\t        messages=messages,\n\t        temperature=0.2,\n\t        max_tokens=1500,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0\n\t    )\n\t    return response.choices[0].message['content'].strip()\n", "def user_input_tool(prompt: str):\n\t    val = input(f\"\\n{prompt}\\nYour response: \")\n\t    return str(val)\n\tdef web_search_tool(query: str , dependent_tasks_output : str):\n\t    if dependent_tasks_output != \"\":\n\t      dependent_task = f\"Use the dependent task output below as reference to help craft the correct search query for the provided task above. Dependent task output:{dependent_tasks_output}.\"\n\t    else:\n\t      dependent_task = \".\"\n\t    query = text_completion_tool(\"You are an AI assistant tasked with generating a Google search query based on the following task: \"+query+\". If the task looks like a search query, return the identical search query as your response. \" + dependent_task + \"\\nSearch Query:\")\n\t    print(\"\\033[90m\\033[3m\"+\"Search query: \" +str(query)+\"\\033[0m\")\n", "    search_params = {\n\t        \"engine\": \"google\",\n\t        \"q\": query,\n\t        \"api_key\": SERPAPI_API_KEY,\n\t        \"num\":3 #edit this up or down for more results, though higher often results in OpenAI rate limits\n\t    }\n\t    search_results = GoogleSearch(search_params)\n\t    search_results = search_results.get_dict()\n\t    try:\n\t      search_results = search_results[\"organic_results\"]\n", "    except:\n\t      search_results = {}\n\t    search_results = simplify_search_results(search_results)\n\t    print(\"\\033[90m\\033[3m\" + \"Completed search. Now scraping results.\\n\" + \"\\033[0m\")\n\t    results = \"\";\n\t    # Loop through the search results\n\t    for result in search_results:\n\t        # Extract the URL from the result\n\t        url = result.get('link')\n\t        # Call the web_scrape_tool function with the URL\n", "        print(\"\\033[90m\\033[3m\" + \"Scraping: \"+url+\"\" + \"...\\033[0m\")\n\t        content = web_scrape_tool(url, task)\n\t        print(\"\\033[90m\\033[3m\" +str(content[0:100])[0:100]+\"...\\n\" + \"\\033[0m\")\n\t        results += str(content)+\". \"\n\t    results = text_completion_tool(f\"You are an expert analyst. Rewrite the following information as one report without removing any facts.\\n###INFORMATION:{results}.\\n###REPORT:\")\n\t    return results\n\tdef simplify_search_results(search_results):\n\t    simplified_results = []\n\t    for result in search_results:\n\t        simplified_result = {\n", "            \"position\": result.get(\"position\"),\n\t            \"title\": result.get(\"title\"),\n\t            \"link\": result.get(\"link\"),\n\t            \"snippet\": result.get(\"snippet\")\n\t        }\n\t        simplified_results.append(simplified_result)\n\t    return simplified_results\n\tdef web_scrape_tool(url: str, task:str):\n\t    content = fetch_url_content(url)\n\t    if content is None:\n", "        return None\n\t    text = extract_text(content)\n\t    print(\"\\033[90m\\033[3m\"+\"Scrape completed. Length:\" +str(len(text))+\".Now extracting relevant info...\"+\"...\\033[0m\")\n\t    info = extract_relevant_info(OBJECTIVE, text[0:5000], task)\n\t    links = extract_links(content)\n\t    #result = f\"{info} URLs: {', '.join(links)}\"\n\t    result = info\n\t    return result\n\theaders = {\n\t    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n", "}\n\tdef fetch_url_content(url: str):\n\t    try:\n\t        response = requests.get(url, headers=headers, timeout=10)\n\t        response.raise_for_status()\n\t        return response.content\n\t    except requests.exceptions.RequestException as e:\n\t        print(f\"Error while fetching the URL: {e}\")\n\t        return \"\"\n\tdef extract_links(content: str):\n", "    soup = BeautifulSoup(content, \"html.parser\")\n\t    links = [link.get('href') for link in soup.findAll('a', attrs={'href': re.compile(\"^https?://\")})]\n\t    return links\n\tdef extract_text(content: str):\n\t    soup = BeautifulSoup(content, \"html.parser\")\n\t    text = soup.get_text(strip=True)\n\t    return text\n\tdef extract_relevant_info(objective, large_string, task):\n\t    chunk_size = 3000\n\t    overlap = 500\n", "    notes = \"\"\n\t    for i in range(0, len(large_string), chunk_size - overlap):\n\t        chunk = large_string[i:i + chunk_size]\n\t        messages = [\n\t            {\"role\": \"system\", \"content\": f\"You are an AI assistant.\"},\n\t            {\"role\": \"user\", \"content\": f\"You are an expert AI research assistant tasked with creating or updating the current notes. If the current note is empty, start a current-notes section by exracting relevant data to the task and objective from the chunk of text to analyze. If there is a current note, add new relevant info frol the chunk of text to analyze. Make sure the new or combined notes is comprehensive and well written. Here's the current chunk of text to analyze: {chunk}. ### Here is the current task: {task}.### For context, here is the objective: {OBJECTIVE}.### Here is the data we've extraced so far that you need to update: {notes}.### new-or-updated-note:\"}\n\t        ]\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n\t            messages=messages,\n", "            max_tokens=800,\n\t            n=1,\n\t            stop=\"###\",\n\t            temperature=0.7,\n\t        )\n\t        notes += response.choices[0].message['content'].strip()+\". \";\n\t    return notes\n\t### Agent functions ##############################\n\tdef execute_task(task, task_list, OBJECTIVE):\n\t    global session_summary\n", "    global task_id_counter\n\t    # Check if dependent_task_ids is not empty\n\t    if task[\"dependent_task_ids\"]:\n\t      all_dependent_tasks_complete = True\n\t      for dep_id in task[\"dependent_task_ids\"]:\n\t          dependent_task = get_task_by_id(dep_id)\n\t          if not dependent_task or dependent_task[\"status\"] != \"complete\":\n\t              all_dependent_tasks_complete = False\n\t              break\n\t    # Execute task\n", "    p_nexttask=\"\\033[92m\\033[1m\"+\"\\n*****NEXT TASK ID:\"+str(task['id'])+\"*****\\n\"+\"\\033[0m\\033[0m\"\n\t    p_nexttask += str(task['id'])+\": \"+str(task['task'])+\" [\"+str(task['tool']+\"]\")\n\t    print(p_nexttask)\n\t    task_prompt = f\"Complete your assigned task based on the objective and only based on information provided in the dependent task output, if provided. \\n###\\nYour objective: {OBJECTIVE}. \\n###\\nYour task: {task['task']}\"\n\t    if task[\"dependent_task_ids\"]:\n\t      dependent_tasks_output = \"\"\n\t      for dep_id in task[\"dependent_task_ids\"]:\n\t          dependent_task_output = get_task_by_id(dep_id)[\"output\"]\n\t          dependent_task_output = dependent_task_output[0:2000]\n\t          dependent_tasks_output += f\" {dependent_task_output}\"\n", "      task_prompt += f\" \\n###\\ndependent tasks output: {dependent_tasks_output}  \\n###\\nYour task: {task['task']}\\n###\\nRESPONSE:\"\n\t    else:\n\t      dependent_tasks_output=\".\"\n\t    # Use tool to complete the task\n\t    if task[\"tool\"] == \"text-completion\":\n\t        task_output = text_completion_tool(task_prompt)\n\t    elif task[\"tool\"] == \"web-search\":\n\t        task_output = web_search_tool(str(task['task']),str(dependent_tasks_output))\n\t    elif task[\"tool\"] == \"web-scrape\":\n\t        task_output = web_scrape_tool(str(task['task']))\n", "    elif task[\"tool\"] == \"user-input\":\n\t        task_output = user_input_tool(str(task['task']))\n\t    # Find task index in the task_list\n\t    task_index = next((i for i, t in enumerate(task_list) if t[\"id\"] == task[\"id\"]), None)\n\t    # Mark task as complete and save output\n\t    task_list[task_index][\"status\"] = \"complete\"\n\t    task_list[task_index][\"output\"] = task_output\n\t    # Print task output\n\t    print(\"\\033[93m\\033[1m\"+\"\\nTask Output (ID:\"+str(task['id'])+\"):\"+\"\\033[0m\\033[0m\")\n\t    print(task_output)\n", "    # Add task output to session_summary\n\t    session_summary += f\"\\n\\nTask {task['id']} - {task['task']}:\\n{task_output}\"\n\tdef task_ready_to_run(task, task_list):\n\t    return all([get_task_by_id(dep_id)[\"status\"] == \"complete\" for dep_id in task[\"dependent_task_ids\"]])\n\ttask_list = []\n\tdef task_creation_agent(objective: str) -> List[Dict]:\n\t    global task_list\n\t    minified_task_list = [{k: v for k, v in task.items() if k != \"result\"} for task in task_list]\n\t    prompt = (\n\t        f\"You are an expert task creation AI tasked with creating a  list of tasks as a JSON array, considering the ultimate objective of your team: {OBJECTIVE}. \"\n", "        f\"Create new tasks based on the objective. Limit tasks types to those that can be completed with the available tools listed below. Task description should be detailed.\"\n\t        f\"Current tool options are [text-completion] {websearch_var} {user_input_var}.\" # web-search is added automatically if SERPAPI exists\n\t        f\"For tasks using [web-search], provide the search query, and only the search query to use (eg. not 'research waterproof shoes, but 'waterproof shoes'). Result will be a summary of relevant information from the first few articles.\"\n\t        f\"When requiring multiple searches, use the [web-search] multiple times. This tool will use the dependent task result to generate the search query if necessary.\"\n\t        f\"Use [user-input] sparingly and only if you need to ask a question to the user who set up the objective. The task description should be the question you want to ask the user.')\"\n\t        f\"dependent_task_ids should always be an empty array, or an array of numbers representing the task ID it should pull results from.\"\n\t        f\"Make sure all task IDs are in chronological order.\\n\"\n\t        f\"EXAMPLE OBJECTIVE=Look up AI news from today (May 27, 2023) and write a poem.\"\n\t        \"TASK LIST=[{\\\"id\\\":1,\\\"task\\\":\\\"AI news today\\\",\\\"tool\\\":\\\"web-search\\\",\\\"dependent_task_ids\\\":[],\\\"status\\\":\\\"incomplete\\\",\\\"result\\\":null,\\\"result_summary\\\":null},{\\\"id\\\":2,\\\"task\\\":\\\"Extract key points from AI news articles\\\",\\\"tool\\\":\\\"text-completion\\\",\\\"dependent_task_ids\\\":[1],\\\"status\\\":\\\"incomplete\\\",\\\"result\\\":null,\\\"result_summary\\\":null},{\\\"id\\\":3,\\\"task\\\":\\\"Generate a list of AI-related words and phrases\\\",\\\"tool\\\":\\\"text-completion\\\",\\\"dependent_task_ids\\\":[2],\\\"status\\\":\\\"incomplete\\\",\\\"result\\\":null,\\\"result_summary\\\":null},{\\\"id\\\":4,\\\"task\\\":\\\"Write a poem using AI-related words and phrases\\\",\\\"tool\\\":\\\"text-completion\\\",\\\"dependent_task_ids\\\":[3],\\\"status\\\":\\\"incomplete\\\",\\\"result\\\":null,\\\"result_summary\\\":null},{\\\"id\\\":5,\\\"task\\\":\\\"Final summary report\\\",\\\"tool\\\":\\\"text-completion\\\",\\\"dependent_task_ids\\\":[1,2,3,4],\\\"status\\\":\\\"incomplete\\\",\\\"result\\\":null,\\\"result_summary\\\":null}]\"\n\t        f\"OBJECTIVE={OBJECTIVE}\"\n", "        f\"TASK LIST=\"\n\t    )\n\t    print(\"\\033[90m\\033[3m\" + \"\\nInitializing...\\n\" + \"\\033[0m\")\n\t    response = openai.ChatCompletion.create(\n\t        model=\"gpt-3.5-turbo\",\n\t        messages=[\n\t            {\n\t                \"role\": \"system\",\n\t                \"content\": \"You are a task creation AI.\"\n\t            },\n", "            {\n\t                \"role\": \"user\",\n\t                \"content\": prompt\n\t            }\n\t        ],\n\t        temperature=0,\n\t        max_tokens=1500,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0\n", "    )\n\t    # Extract the content of the assistant's response and parse it as JSON\n\t    result = response[\"choices\"][0][\"message\"][\"content\"]\n\t    try:\n\t        task_list = json.loads(result)\n\t    except Exception as error:\n\t        print(error)\n\t    return task_list\n\t##### START MAIN LOOP########\n\t#Print OBJECTIVE\n", "print(\"\\033[96m\\033[1m\"+\"\\n*****OBJECTIVE*****\\n\"+\"\\033[0m\\033[0m\")\n\tprint(OBJECTIVE)\n\t# Initialize task_id_counter\n\ttask_id_counter = 1\n\t# Run the task_creation_agent to create initial tasks\n\ttask_list = task_creation_agent(OBJECTIVE)\n\tprint_tasklist()\n\t# Create a ThreadPoolExecutor\n\twith ThreadPoolExecutor() as executor:\n\t    while True:\n", "        tasks_submitted = False\n\t        for task in task_list:\n\t            if task[\"status\"] == \"incomplete\" and task_ready_to_run(task, task_list):\n\t                future = executor.submit(execute_task, task, task_list, OBJECTIVE)\n\t                task[\"status\"] = \"running\"\n\t                tasks_submitted = True\n\t        if not tasks_submitted and all(task[\"status\"] == \"complete\" for task in task_list):\n\t            break\n\t        time.sleep(5)\n\t# Print session summary\n", "print(\"\\033[96m\\033[1m\"+\"\\n*****SAVING FILE...*****\\n\"+\"\\033[0m\\033[0m\")\n\tfile = open(f'output/output_{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")}.txt', 'w')\n\tfile.write(session_summary)\n\tfile.close()\n\tprint(\"...file saved.\")\n\tprint(\"END\")\n"]}
{"filename": "classic/BabyBeeAGI.py", "chunked_list": ["###### This is a modified version of OG BabyAGI, called BabyBeeAGI (future modifications will follow the pattern \"Baby<animal>AGI\"). This version requires GPT-4, it's very slow, and often errors out.######\n\t######IMPORTANT NOTE: I'm sharing this as a framework to build on top of (with lots of errors for improvement), to facilitate discussion around how to improve these. This is NOT for people who are looking for a complete solution that's ready to use. ######\n\timport openai\n\timport pinecone\n\timport time\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tfrom collections import deque\n\tfrom typing import Dict, List\n\timport re\n", "import ast\n\timport json\n\tfrom serpapi import GoogleSearch\n\t### SET THESE 4 VARIABLES ##############################\n\t# Add your API keys here\n\tOPENAI_API_KEY = \"\"\n\tSERPAPI_API_KEY = \"\" #If you include SERPAPI KEY, this will enable web-search. If you don't, it will automatically remove web-search capability.\n\t# Set variables\n\tOBJECTIVE = \"You are an AI. Make the world a better place.\"\n\tYOUR_FIRST_TASK = \"Develop a task list.\"\n", "### UP TO HERE ##############################\n\t# Configure OpenAI and SerpAPI client\n\topenai.api_key = OPENAI_API_KEY\n\tif SERPAPI_API_KEY:\n\t  serpapi_client = GoogleSearch({\"api_key\": SERPAPI_API_KEY})\n\t  websearch_var = \"[web-search] \"\n\telse:\n\t  websearch_var = \"\"\n\t# Initialize task list\n\ttask_list = []\n", "# Initialize session_summary\n\tsession_summary = \"\"\n\t### Task list functions ##############################\n\tdef add_task(task: Dict):\n\t    task_list.append(task)\n\tdef get_task_by_id(task_id: int):\n\t    for task in task_list:\n\t        if task[\"id\"] == task_id:\n\t            return task\n\t    return None\n", "def get_completed_tasks():\n\t    return [task for task in task_list if task[\"status\"] == \"complete\"]\n\t### Tool functions ##############################\n\tdef text_completion_tool(prompt: str):\n\t    response = openai.Completion.create(\n\t        engine=\"text-davinci-003\",\n\t        prompt=prompt,\n\t        temperature=0.5,\n\t        max_tokens=1500,\n\t        top_p=1,\n", "        frequency_penalty=0,\n\t        presence_penalty=0\n\t    )\n\t    return response.choices[0].text.strip()\n\tdef web_search_tool(query: str):\n\t    search_params = {\n\t        \"engine\": \"google\",\n\t        \"q\": query,\n\t        \"api_key\": SERPAPI_API_KEY,\n\t        \"num\":3\n", "    }\n\t    search_results = GoogleSearch(search_params)\n\t    results = search_results.get_dict()\n\t    return str(results[\"organic_results\"])\n\tdef web_scrape_tool(url: str):\n\t    response = requests.get(url)\n\t    print(response)\n\t    soup = BeautifulSoup(response.content, \"html.parser\")\n\t    result = soup.get_text(strip=True)+\"URLs: \"\n\t    for link in soup.findAll('a', attrs={'href': re.compile(\"^https://\")}):\n", "      result+= link.get('href')+\", \"\n\t    return result\n\t### Agent functions ##############################\n\tdef execute_task(task, task_list, OBJECTIVE):\n\t    global task_id_counter\n\t    # Check if dependent_task_id is complete\n\t    if task[\"dependent_task_id\"]:\n\t        dependent_task = get_task_by_id(task[\"dependent_task_id\"])\n\t        if not dependent_task or dependent_task[\"status\"] != \"complete\":\n\t            return\n", "    # Execute task\n\t    print(\"\\033[92m\\033[1m\"+\"\\n*****NEXT TASK*****\\n\"+\"\\033[0m\\033[0m\")\n\t    print(str(task['id'])+\": \"+str(task['task'])+\" [\"+str(task['tool']+\"]\"))\n\t    task_prompt = f\"Complete your assigned task based on the objective: {OBJECTIVE}. Your task: {task['task']}\"\n\t    if task[\"dependent_task_id\"]:\n\t        dependent_task_result = dependent_task[\"result\"]\n\t        task_prompt += f\"\\nThe previous task ({dependent_task['id']}. {dependent_task['task']}) result: {dependent_task_result}\"\n\t    task_prompt += \"\\nResponse:\"\n\t    ##print(\"###task_prompt: \"+task_prompt)\n\t    if task[\"tool\"] == \"text-completion\":\n", "        result = text_completion_tool(task_prompt)\n\t    elif task[\"tool\"] == \"web-search\":\n\t        result = web_search_tool(task_prompt)\n\t    elif task[\"tool\"] == \"web-scrape\":\n\t        result = web_scrape_tool(str(task['task']))\n\t    else:\n\t        result = \"Unknown tool\"\n\t    print(\"\\033[93m\\033[1m\"+\"\\n*****TASK RESULT*****\\n\"+\"\\033[0m\\033[0m\")\n\t    print_result = result[0:2000]\n\t    if result != result[0:2000]:\n", "      print(print_result+\"...\")\n\t    else:\n\t      print(result)\n\t    # Update task status and result\n\t    task[\"status\"] = \"complete\"\n\t    task[\"result\"] = result\n\t    task[\"result_summary\"] = summarizer_agent(result)\n\t    # Update session_summary\n\t    session_summary = overview_agent(task[\"id\"])\n\t    # Increment task_id_counter\n", "    task_id_counter += 1\n\t    # Update task_manager_agent of tasks\n\t    task_manager_agent(\n\t        OBJECTIVE,\n\t        result,\n\t        task[\"task\"],\n\t        [t[\"task\"] for t in task_list if t[\"status\"] == \"incomplete\"],\n\t        task[\"id\"]  \n\t    )\n\tdef task_manager_agent(objective: str, result: str, task_description: str, incomplete_tasks: List[str], current_task_id : int) -> List[Dict]:\n", "    global task_list\n\t    original_task_list = task_list.copy()\n\t    minified_task_list = [{k: v for k, v in task.items() if k != \"result\"} for task in task_list]\n\t    result = result[0:4000] #come up with better solution later.\n\t    prompt = (\n\t        f\"You are a task management AI tasked with cleaning the formatting of and reprioritizing the following tasks: {minified_task_list}. \"\n\t        f\"Consider the ultimate objective of your team: {OBJECTIVE}. \"\n\t        f\"Do not remove any tasks. Return the result as a JSON-formatted list of dictionaries.\\n\"\n\t        f\"Create new tasks based on the result of last task if necessary for the objective. Limit tasks types to those that can be completed with the available tools listed below. Task description should be detailed.\"\n\t        f\"The maximum task list length is 7. Do not add an 8th task.\"\n", "        f\"The last completed task has the following result: {result}. \"\n\t        f\"Current tool option is [text-completion] {websearch_var} and [web-scrape] only.\"# web-search is added automatically if SERPAPI exists\n\t        f\"For tasks using [web-scrape], provide only the URL to scrape as the task description. Do not provide placeholder URLs, but use ones provided by a search step or the initial objective.\"\n\t        #f\"If the objective is research related, use at least one [web-search] with the query as the task description, and after, add up to three URLs from the search result as a task with [web-scrape], then use [text-completion] to write a comprehensive summary of each site thas has been scraped.'\"\n\t        f\"For tasks using [web-search], provide the search query, and only the search query to use (eg. not 'research waterproof shoes, but 'waterproof shoes')\"\n\t        f\"dependent_task_id should always be null or a number.\"\n\t        f\"Do not reorder completed tasks. Only reorder and dedupe incomplete tasks.\\n\"\n\t        f\"Make sure all task IDs are in chronological order.\\n\"\n\t        f\"Do not provide example URLs for [web-scrape].\\n\"\n\t        f\"Do not include the result from the last task in the JSON, that will be added after..\\n\"\n", "        f\"The last step is always to provide a final summary report of all tasks.\\n\"\n\t        f\"An example of the desired output format is: \"\n\t        \"[{\\\"id\\\": 1, \\\"task\\\": \\\"https://untapped.vc\\\", \\\"tool\\\": \\\"web-scrape\\\", \\\"dependent_task_id\\\": null, \\\"status\\\": \\\"incomplete\\\", \\\"result\\\": null, \\\"result_summary\\\": null}, {\\\"id\\\": 2, \\\"task\\\": \\\"Analyze the contents of...\\\", \\\"tool\\\": \\\"text-completion\\\", \\\"dependent_task_id\\\": 1, \\\"status\\\": \\\"incomplete\\\", \\\"result\\\": null, \\\"result_summary\\\": null}, {\\\"id\\\": 3, \\\"task\\\": \\\"Untapped Capital\\\", \\\"tool\\\": \\\"web-search\\\", \\\"dependent_task_id\\\": null, \\\"status\\\": \\\"incomplete\\\", \\\"result\\\": null, \\\"result_summary\\\": null}].\"\n\t    )\n\t    print(\"\\033[90m\\033[3m\" + \"\\nRunning task manager agent...\\n\" + \"\\033[0m\")\n\t    response = openai.ChatCompletion.create(\n\t        model=\"gpt-4\",\n\t        messages=[\n\t            {\n\t                \"role\": \"system\",\n", "                \"content\": \"You are a task manager AI.\"\n\t            },\n\t            {\n\t                \"role\": \"user\",\n\t                \"content\": prompt\n\t            }\n\t        ],\n\t        temperature=0.2,\n\t        max_tokens=1500,\n\t        top_p=1,\n", "        frequency_penalty=0,\n\t        presence_penalty=0\n\t    )\n\t    # Extract the content of the assistant's response and parse it as JSON\n\t    result = response[\"choices\"][0][\"message\"][\"content\"]\n\t    print(\"\\033[90m\\033[3m\" + \"\\nDone!\\n\" + \"\\033[0m\")\n\t    try:\n\t      task_list = json.loads(result)\n\t    except Exception as error:\n\t      print(error)\n", "    # Add the 'result' field back in\n\t    for updated_task, original_task in zip(task_list, original_task_list):\n\t        if \"result\" in original_task:\n\t            updated_task[\"result\"] = original_task[\"result\"]\n\t    task_list[current_task_id][\"result\"]=result\n\t    #print(task_list)\n\t    return task_list\n\tdef summarizer_agent(text: str) -> str:\n\t    text = text[0:4000]\n\t    prompt = f\"Please summarize the following text:\\n{text}\\nSummary:\"\n", "    response = openai.Completion.create(\n\t        engine=\"text-davinci-003\",\n\t        prompt=prompt,\n\t        temperature=0.5,\n\t        max_tokens=100,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0\n\t    )\n\t    return response.choices[0].text.strip()\n", "def overview_agent(last_task_id: int) -> str:\n\t    global session_summary\n\t    completed_tasks = get_completed_tasks()\n\t    completed_tasks_text = \"\\n\".join(\n\t        [f\"{task['id']}. {task['task']} - {task['result_summary']}\" for task in completed_tasks]\n\t    )\n\t    prompt = f\"Here is the current session summary:\\n{session_summary}\\nThe last completed task is task {last_task_id}. Please update the session summary with the information of the last task:\\n{completed_tasks_text}\\nUpdated session summary, which should describe all tasks in chronological order:\"\n\t    response = openai.Completion.create(\n\t        engine=\"text-davinci-003\",\n\t        prompt=prompt,\n", "        temperature=0.5,\n\t        max_tokens=200,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0\n\t    )\n\t    session_summary = response.choices[0].text.strip()\n\t    return session_summary\n\t### Main Loop ##############################\n\t# Add the first task\n", "first_task = {\n\t    \"id\": 1,\n\t    \"task\": YOUR_FIRST_TASK,\n\t    \"tool\": \"text-completion\",\n\t    \"dependent_task_id\": None,\n\t    \"status\": \"incomplete\",\n\t    \"result\": \"\",\n\t    \"result_summary\": \"\"\n\t}\n\tadd_task(first_task)\n", "task_id_counter = 0\n\t#Print OBJECTIVE\n\tprint(\"\\033[96m\\033[1m\"+\"\\n*****OBJECTIVE*****\\n\"+\"\\033[0m\\033[0m\")\n\tprint(OBJECTIVE)\n\t# Continue the loop while there are incomplete tasks\n\twhile any(task[\"status\"] == \"incomplete\" for task in task_list):\n\t    # Filter out incomplete tasks\n\t    incomplete_tasks = [task for task in task_list if task[\"status\"] == \"incomplete\"]\n\t    if incomplete_tasks:\n\t        # Sort tasks by ID\n", "        incomplete_tasks.sort(key=lambda x: x[\"id\"])\n\t        # Pull the first task\n\t        task = incomplete_tasks[0]\n\t        # Execute task & call task manager from function\n\t        execute_task(task, task_list, OBJECTIVE)\n\t        # Print task list and session summary\n\t        print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\")\n\t        for t in task_list:\n\t            dependent_task = \"\"\n\t            if t['dependent_task_id'] is not None:\n", "                dependent_task = f\"\\033[31m<dependency: #{t['dependent_task_id']}>\\033[0m\"\n\t            status_color = \"\\033[32m\" if t['status'] == \"complete\" else \"\\033[31m\"\n\t            print(f\"\\033[1m{t['id']}\\033[0m: {t['task']} {status_color}[{t['status']}]\\033[0m \\033[93m[{t['tool']}] {dependent_task}\\033[0m\")\n\t        print(\"\\033[93m\\033[1m\" + \"\\n*****SESSION SUMMARY*****\\n\" + \"\\033[0m\\033[0m\")\n\t        print(session_summary)\n\t    time.sleep(1)  # Sleep before checking the task list again\n\t### Objective complete ##############################\n\t# Print the full task list if there are no incomplete tasks\n\tif all(task[\"status\"] != \"incomplete\" for task in task_list):\n\t    print(\"\\033[92m\\033[1m\" + \"\\n*****ALL TASKS COMPLETED*****\\n\" + \"\\033[0m\\033[0m\")\n", "    for task in task_list:\n\t        print(f\"ID: {task['id']}, Task: {task['task']}, Result: {task['result']}\")\n"]}
{"filename": "classic/BabyCatAGI.py", "chunked_list": ["###### This is a modified version of OG BabyAGI, called BabyCatAGI (future modifications will follow the pattern \"Baby<animal>AGI\"). This version requires GPT-4, it's very slow, and often errors out.######\n\t######IMPORTANT NOTE: I'm sharing this as a framework to build on top of (with lots of errors for improvement), to facilitate discussion around how to improve these. This is NOT for people who are looking for a complete solution that's ready to use. ######\n\timport openai\n\timport time\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tfrom collections import deque\n\tfrom typing import Dict, List\n\timport re\n\timport ast\n", "import json\n\tfrom serpapi import GoogleSearch\n\t### SET THESE 4 VARIABLES ##############################\n\t# Add your API keys here\n\tOPENAI_API_KEY = \"\"\n\tSERPAPI_API_KEY = \"\" #If you include SERPAPI KEY, this will enable web-search. If you don't, it will autoatically remove web-search capability.\n\t# Set variables\n\tOBJECTIVE = \"Research experts at scaling NextJS and their Twitter accounts.\"\n\tYOUR_FIRST_TASK = \"Develop a task list.\" #you can provide additional instructions here regarding the task list.\n\t### UP TO HERE ##############################\n", "# Configure OpenAI and SerpAPI client\n\topenai.api_key = OPENAI_API_KEY\n\tif SERPAPI_API_KEY:\n\t  serpapi_client = GoogleSearch({\"api_key\": SERPAPI_API_KEY})\n\t  websearch_var = \"[web-search] \"\n\telse:\n\t  websearch_var = \"\"\n\t# Initialize task list\n\ttask_list = []\n\t# Initialize session_summary\n", "session_summary = \"\"\n\t### Task list functions ##############################\n\tdef add_task(task: Dict):\n\t    task_list.append(task)\n\tdef get_task_by_id(task_id: int):\n\t    for task in task_list:\n\t        if task[\"id\"] == task_id:\n\t            return task\n\t    return None\n\tdef get_completed_tasks():\n", "    return [task for task in task_list if task[\"status\"] == \"complete\"]\n\t# Print task list and session summary\n\tdef print_tasklist():\n\t  print(\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\")\n\t  for t in task_list:\n\t      dependent_task = \"\"\n\t      if t['dependent_task_ids']:\n\t          dependent_task = f\"\\033[31m<dependencies: {', '.join([f'#{dep_id}' for dep_id in t['dependent_task_ids']])}>\\033[0m\"\n\t      status_color = \"\\033[32m\" if t['status'] == \"complete\" else \"\\033[31m\"\n\t      print(f\"\\033[1m{t['id']}\\033[0m: {t['task']} {status_color}[{t['status']}]\\033[0m \\033[93m[{t['tool']}] {dependent_task}\\033[0m\")\n", "### Tool functions ##############################\n\tdef text_completion_tool(prompt: str):\n\t    messages = [\n\t        {\"role\": \"user\", \"content\": prompt}\n\t    ]\n\t    response = openai.ChatCompletion.create(\n\t        model=\"gpt-3.5-turbo\",\n\t        messages=messages,\n\t        temperature=0.2,\n\t        max_tokens=1500,\n", "        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0\n\t    )\n\t    return response.choices[0].message['content'].strip()\n\tdef web_search_tool(query: str):\n\t    search_params = {\n\t        \"engine\": \"google\",\n\t        \"q\": query,\n\t        \"api_key\": SERPAPI_API_KEY,\n", "        \"num\":5 #edit this up or down for more results, though higher often results in OpenAI rate limits\n\t    }\n\t    search_results = GoogleSearch(search_params)\n\t    search_results = search_results.get_dict()\n\t    try:\n\t      search_results = search_results[\"organic_results\"]\n\t    except:\n\t      search_results = {}\n\t    search_results = simplify_search_results(search_results)\n\t    print(\"\\033[90m\\033[3m\" + \"Completed search. Now scraping results.\\n\" + \"\\033[0m\")\n", "    results = \"\";\n\t    # Loop through the search results\n\t    for result in search_results:\n\t        # Extract the URL from the result\n\t        url = result.get('link')\n\t        # Call the web_scrape_tool function with the URL\n\t        print(\"\\033[90m\\033[3m\" + \"Scraping: \"+url+\"\" + \"...\\033[0m\")\n\t        content = web_scrape_tool(url, task)\n\t        print(\"\\033[90m\\033[3m\" +str(content[0:100])[0:100]+\"...\\n\" + \"\\033[0m\")\n\t        results += str(content)+\". \"\n", "    return results\n\tdef simplify_search_results(search_results):\n\t    simplified_results = []\n\t    for result in search_results:\n\t        simplified_result = {\n\t            \"position\": result.get(\"position\"),\n\t            \"title\": result.get(\"title\"),\n\t            \"link\": result.get(\"link\"),\n\t            \"snippet\": result.get(\"snippet\")\n\t        }\n", "        simplified_results.append(simplified_result)\n\t    return simplified_results\n\tdef web_scrape_tool(url: str, task:str):\n\t    content = fetch_url_content(url)\n\t    if content is None:\n\t        return None\n\t    text = extract_text(content)\n\t    print(\"\\033[90m\\033[3m\"+\"Scrape completed. Length:\" +str(len(text))+\".Now extracting relevant info...\"+\"...\\033[0m\")\n\t    info = extract_relevant_info(OBJECTIVE, text[0:5000], task)\n\t    links = extract_links(content)\n", "    #result = f\"{info} URLs: {', '.join(links)}\"\n\t    result = info\n\t    return result\n\theaders = {\n\t    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n\t}\n\tdef fetch_url_content(url: str):\n\t    try:\n\t        response = requests.get(url, headers=headers, timeout=10)\n\t        response.raise_for_status()\n", "        return response.content\n\t    except requests.exceptions.RequestException as e:\n\t        print(f\"Error while fetching the URL: {e}\")\n\t        return \"\"\n\tdef extract_links(content: str):\n\t    soup = BeautifulSoup(content, \"html.parser\")\n\t    links = [link.get('href') for link in soup.findAll('a', attrs={'href': re.compile(\"^https?://\")})]\n\t    return links\n\tdef extract_text(content: str):\n\t    soup = BeautifulSoup(content, \"html.parser\")\n", "    text = soup.get_text(strip=True)\n\t    return text\n\tdef extract_relevant_info(objective, large_string, task):\n\t    chunk_size = 3000\n\t    overlap = 500\n\t    notes = \"\"\n\t    for i in range(0, len(large_string), chunk_size - overlap):\n\t        chunk = large_string[i:i + chunk_size]\n\t        messages = [\n\t            {\"role\": \"system\", \"content\": f\"Objective: {objective}\\nCurrent Task:{task}\"},\n", "            {\"role\": \"user\", \"content\": f\"Analyze the following text and extract information relevant to our objective and current task, and only information relevant to our objective and current task. If there is no relevant information do not say that there is no relevant informaiton related to our objective. ### Then, update or start our notes provided here (keep blank if currently blank): {notes}.### Text to analyze: {chunk}.### Updated Notes:\"}\n\t        ]\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n\t            messages=messages,\n\t            max_tokens=800,\n\t            n=1,\n\t            stop=\"###\",\n\t            temperature=0.7,\n\t        )\n", "        notes += response.choices[0].message['content'].strip()+\". \";\n\t    return notes\n\t### Agent functions ##############################\n\tdef execute_task(task, task_list, OBJECTIVE):\n\t    global task_id_counter\n\t    # Check if dependent_task_ids is not empty\n\t    if task[\"dependent_task_ids\"]:\n\t      all_dependent_tasks_complete = True\n\t      for dep_id in task[\"dependent_task_ids\"]:\n\t          dependent_task = get_task_by_id(dep_id)\n", "          if not dependent_task or dependent_task[\"status\"] != \"complete\":\n\t              all_dependent_tasks_complete = False\n\t              break\n\t    # Execute task\n\t    print(\"\\033[92m\\033[1m\"+\"\\n*****NEXT TASK*****\\n\"+\"\\033[0m\\033[0m\")\n\t    print(str(task['id'])+\": \"+str(task['task'])+\" [\"+str(task['tool']+\"]\"))\n\t    task_prompt = f\"Complete your assigned task based on the objective and only based on information provided in the dependent task output, if provided. Your objective: {OBJECTIVE}. Your task: {task['task']}\"\n\t    if task[\"dependent_task_ids\"]:\n\t      dependent_tasks_output = \"\"\n\t      for dep_id in task[\"dependent_task_ids\"]:\n", "          dependent_task_output = get_task_by_id(dep_id)[\"output\"]\n\t          dependent_task_output = dependent_task_output[0:2000]\n\t          dependent_tasks_output += f\" {dependent_task_output}\"\n\t      task_prompt += f\" Your dependent tasks output: {dependent_tasks_output}\\n OUTPUT:\"\n\t    # Use tool to complete the task\n\t    if task[\"tool\"] == \"text-completion\":\n\t        task_output = text_completion_tool(task_prompt)\n\t    elif task[\"tool\"] == \"web-search\":\n\t        task_output = web_search_tool(str(task['task']))\n\t    elif task[\"tool\"] == \"web-scrape\":\n", "        task_output = web_scrape_tool(str(task['task']))\n\t    # Find task index in the task_list\n\t    task_index = next((i for i, t in enumerate(task_list) if t[\"id\"] == task[\"id\"]), None)\n\t    # Mark task as complete and save output\n\t    task_list[task_index][\"status\"] = \"complete\"\n\t    task_list[task_index][\"output\"] = task_output\n\t    # Print task output\n\t    print(\"\\033[93m\\033[1m\"+\"\\nTask Output:\"+\"\\033[0m\\033[0m\")\n\t    print(task_output)\n\t    # Add task output to session_summary\n", "    global session_summary\n\t    session_summary += f\"\\n\\nTask {task['id']} - {task['task']}:\\n{task_output}\"\n\ttask_list = []\n\tdef task_creation_agent(objective: str) -> List[Dict]:\n\t    global task_list\n\t    minified_task_list = [{k: v for k, v in task.items() if k != \"result\"} for task in task_list]\n\t    prompt = (\n\t        f\"You are a task creation AI tasked with creating a list of tasks as a JSON array, considering the ultimate objective of your team: {OBJECTIVE}. \"\n\t        f\"Create new tasks based on the objective. Limit tasks types to those that can be completed with the available tools listed below. Task description should be detailed.\"\n\t        f\"Current tool option is [text-completion] {websearch_var} and only.\" # web-search is added automatically if SERPAPI exists\n", "        f\"For tasks using [web-search], provide the search query, and only the search query to use (eg. not 'research waterproof shoes, but 'waterproof shoes')\"\n\t        f\"dependent_task_ids should always be an empty array, or an array of numbers representing the task ID it should pull results from.\"\n\t        f\"Make sure all task IDs are in chronological order.\\n\"\n\t        f\"The last step is always to provide a final summary report including tasks executed and summary of knowledge acquired.\\n\"\n\t        f\"Do not create any summarizing steps outside of the last step..\\n\"\n\t        f\"An example of the desired output format is: \"\n\t        \"[{\\\"id\\\": 1, \\\"task\\\": \\\"https://untapped.vc\\\", \\\"tool\\\": \\\"web-scrape\\\", \\\"dependent_task_ids\\\": [], \\\"status\\\": \\\"incomplete\\\", \\\"result\\\": null, \\\"result_summary\\\": null}, {\\\"id\\\": 2, \\\"task\\\": \\\"Consider additional insights that can be reasoned from the results of...\\\", \\\"tool\\\": \\\"text-completion\\\", \\\"dependent_task_ids\\\": [1], \\\"status\\\": \\\"incomplete\\\", \\\"result\\\": null, \\\"result_summary\\\": null}, {\\\"id\\\": 3, \\\"task\\\": \\\"Untapped Capital\\\", \\\"tool\\\": \\\"web-search\\\", \\\"dependent_task_ids\\\": [], \\\"status\\\": \\\"incomplete\\\", \\\"result\\\": null, \\\"result_summary\\\": null}].\\n\"\n\t        f\"JSON TASK LIST=\"\n\t    )\n\t    print(\"\\033[90m\\033[3m\" + \"\\nInitializing...\\n\" + \"\\033[0m\")\n", "    print(\"\\033[90m\\033[3m\" + \"Analyzing objective...\\n\" + \"\\033[0m\")\n\t    print(\"\\033[90m\\033[3m\" + \"Running task creation agent...\\n\" + \"\\033[0m\")\n\t    response = openai.ChatCompletion.create(\n\t        model=\"gpt-4\",\n\t        messages=[\n\t            {\n\t                \"role\": \"system\",\n\t                \"content\": \"You are a task creation AI.\"\n\t            },\n\t            {\n", "                \"role\": \"user\",\n\t                \"content\": prompt\n\t            }\n\t        ],\n\t        temperature=0,\n\t        max_tokens=1500,\n\t        top_p=1,\n\t        frequency_penalty=0,\n\t        presence_penalty=0\n\t    )\n", "    # Extract the content of the assistant's response and parse it as JSON\n\t    result = response[\"choices\"][0][\"message\"][\"content\"]\n\t    print(\"\\033[90m\\033[3m\" + \"\\nDone!\\n\" + \"\\033[0m\")\n\t    try:\n\t        task_list = json.loads(result)\n\t    except Exception as error:\n\t        print(error)\n\t    return task_list\n\t##### START MAIN LOOP########\n\t#Print OBJECTIVE\n", "print(\"\\033[96m\\033[1m\"+\"\\n*****OBJECTIVE*****\\n\"+\"\\033[0m\\033[0m\")\n\tprint(OBJECTIVE)\n\t# Initialize task_id_counter\n\ttask_id_counter = 1\n\t# Run the task_creation_agent to create initial tasks\n\ttask_list = task_creation_agent(OBJECTIVE)\n\tprint_tasklist()\n\t# Execute tasks in order\n\twhile len(task_list) > 0:\n\t    for task in task_list:\n", "        if task[\"status\"] == \"incomplete\":\n\t            execute_task(task, task_list, OBJECTIVE)\n\t            print_tasklist()\n\t            break\n\t# Print session summary\n\tprint(\"\\033[96m\\033[1m\"+\"\\n*****SESSION SUMMARY*****\\n\"+\"\\033[0m\\033[0m\")\n\tprint(session_summary)\n"]}
{"filename": "classic/BabyElfAGI/main.py", "chunked_list": ["import os\n\tfrom dotenv import load_dotenv\n\timport importlib.util\n\timport json\n\timport openai\n\timport concurrent.futures\n\timport time\n\tfrom datetime import datetime\n\tfrom skills.skill import Skill\n\tfrom skills.skill_registry import SkillRegistry\n", "from tasks.task_registry import TaskRegistry\n\tload_dotenv()  # Load environment variables from .env file\n\t# Retrieve all API keys\n\tapi_keys = {\n\t    'openai': os.environ['OPENAI_API_KEY'],\n\t    'serpapi': os.environ['SERPAPI_API_KEY']\n\t    # Add more keys here as needed\n\t}\n\t# Set OBJECTIVE\n\tOBJECTIVE = \"Create an example objective and tasklist for 'write a poem', which only uses text_completion in the tasks. Do this by usign code_reader to read example1.json, then writing the JSON objective tasklist pair using text_completion, and saving it using objective_saver.\"\n", "LOAD_SKILLS = ['text_completion','code_reader','objective_saver']\n\tREFLECTION = False\n\t##### START MAIN LOOP########\n\t# Print OBJECTIVE\n\tprint(\"\\033[96m\\033[1m\"+\"\\n*****OBJECTIVE*****\\n\"+\"\\033[0m\\033[0m\")\n\tprint(OBJECTIVE)\n\tif __name__ == \"__main__\":\n\t    session_summary = \"\"\n\t    # Initialize the SkillRegistry and TaskRegistry\n\t    skill_registry = SkillRegistry(api_keys=api_keys, skill_names=LOAD_SKILLS)\n", "    skill_descriptions = \",\".join(f\"[{skill.name}: {skill.description}]\" for skill in skill_registry.skills.values())\n\t    task_registry = TaskRegistry()\n\t    # Create the initial task list based on an objective\n\t    task_registry.create_tasklist(OBJECTIVE, skill_descriptions)\n\t    # Initialize task outputs\n\t    task_outputs = {i: {\"completed\": False, \"output\": None} for i, _ in enumerate(task_registry.get_tasks())}\n\t    # Create a thread pool for parallel execution\n\t    with concurrent.futures.ThreadPoolExecutor() as executor:\n\t        # Loop until all tasks are completed\n\t        while not all(task[\"completed\"] for task in task_outputs.values()):\n", "            # Get the tasks that are ready to be executed (i.e., all their dependencies have been completed)\n\t            tasks = task_registry.get_tasks()\n\t            # Print the updated task list\n\t            task_registry.print_tasklist(tasks) \n\t            # Update task_outputs to include new tasks\n\t            for task in tasks:\n\t                if task[\"id\"] not in task_outputs:\n\t                    task_outputs[task[\"id\"]] = {\"completed\": False, \"output\": None}\n\t            ready_tasks = [(task[\"id\"], task) for task in tasks\n\t               if all((dep in task_outputs and task_outputs[dep][\"completed\"]) \n", "               for dep in task.get('dependent_task_ids', [])) \n\t               and not task_outputs[task[\"id\"]][\"completed\"]]\n\t            session_summary += str(task)+\"\\n\"\n\t            futures = [executor.submit(task_registry.execute_task, task_id, task, skill_registry, task_outputs, OBJECTIVE) \n\t                       for task_id, task in ready_tasks if not task_outputs[task_id][\"completed\"]]\n\t            # Wait for the tasks to complete\n\t            for future in futures:\n\t                i, output = future.result()\n\t                task_outputs[i][\"output\"] = output\n\t                task_outputs[i][\"completed\"] = True\n", "                # Update the task in the TaskRegistry\n\t                task_registry.update_tasks({\"id\": i, \"status\": \"completed\", \"result\": output})\n\t                completed_task = task_registry.get_task(i)\n\t                print(f\"\\033[92mTask #{i}: {completed_task.get('task')} \\033[0m\\033[92m[COMPLETED]\\033[0m\\033[92m[{completed_task.get('skill')}]\\033[0m\")\n\t                # Reflect on the output\n\t                if output:\n\t                    session_summary += str(output)+\"\\n\"\n\t                    if REFLECTION == True:\n\t                      new_tasks, insert_after_ids, tasks_to_update = task_registry.reflect_on_output(output, skill_descriptions)\n\t                      # Insert new tasks\n", "                      for new_task, after_id in zip(new_tasks, insert_after_ids):\n\t                          task_registry.add_task(new_task, after_id)\n\t                      # Update existing tasks\n\t                      for task_to_update in tasks_to_update:\n\t                        task_registry.update_tasks(task_to_update)\n\t            #print(task_outputs.values())\n\t            if all(task[\"status\"] == \"completed\" for task in task_registry.tasks):\n\t              print(\"All tasks completed!\")\n\t              break\n\t            # Short delay to prevent busy looping\n", "            time.sleep(0.1)\n\t        # Print session summary\n\t        print(\"\\033[96m\\033[1m\"+\"\\n*****SAVING FILE...*****\\n\"+\"\\033[0m\\033[0m\")\n\t        file = open(f'output/output_{datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")}.txt', 'w')\n\t        file.write(session_summary)\n\t        file.close()\n\t        print(\"...file saved.\")\n\t        print(\"END\")\n\t        executor.shutdown()"]}
{"filename": "classic/BabyElfAGI/skills/skill.py", "chunked_list": ["class Skill:\n\t    name = 'base skill'\n\t    description = 'This is the base skill.'\n\t    api_keys_required = []\n\t    def __init__(self, api_keys):\n\t        self.api_keys = api_keys\n\t        missing_keys = self.check_required_keys(api_keys)\n\t        if missing_keys:\n\t            print(f\"Missing API keys for {self.name}: {missing_keys}\")\n\t            self.valid = False\n", "        else:\n\t            self.valid = True\n\t        for key in self.api_keys_required:\n\t            if isinstance(key, list):\n\t                for subkey in key:\n\t                    if subkey in api_keys:\n\t                        setattr(self, f\"{subkey}_api_key\", api_keys.get(subkey))\n\t            elif key in api_keys:\n\t                setattr(self, f\"{key}_api_key\", api_keys.get(key))\n\t    def check_required_keys(self, api_keys):\n", "        missing_keys = []\n\t        for key in self.api_keys_required:\n\t            if isinstance(key, list):  # If the key is actually a list of alternatives\n\t                if not any(k in api_keys for k in key):  # If none of the alternatives are present\n\t                    missing_keys.append(key)  # Add the list of alternatives to the missing keys\n\t            elif key not in api_keys:  # If the key is a single key and it's not present\n\t                missing_keys.append(key)  # Add the key to the missing keys\n\t        return missing_keys\n\t    def execute(self, params, dependent_task_outputs, objective):\n\t        raise NotImplementedError('Execute method must be implemented in subclass.')"]}
{"filename": "classic/BabyElfAGI/skills/code_reader.py", "chunked_list": ["from skills.skill import Skill\n\timport openai\n\timport os\n\tclass CodeReader(Skill):\n\t    name = 'code_reader'\n\t    description = \"A skill that finds a file's location in it's own program's directory and returns its contents.\"\n\t    api_keys_required = ['openai']\n\t    def __init__(self, api_keys):\n\t        super().__init__(api_keys)\n\t    def execute(self, params, dependent_task_outputs, objective):\n", "        if not self.valid:\n\t            return\n\t        dir_structure = self.get_directory_structure(self.get_top_parent_path(os.path.realpath(__file__)))\n\t        print(f\"Directory structure: {dir_structure}\")\n\t        example_dir_structure = {'.': {'main.py': None}, 'skills': {'__init__.py': None, 'web_scrape.py': None, 'skill.py': None, 'test_skill.py': None, 'text_completion.py': None, 'web_search.py': None, 'skill_registry.py': None, 'directory_structure.py': None, 'code_reader.py': None}, 'tasks': {'task_registry.py': None}, 'output': {}}\n\t        example_params = \"Analyze main.py\"\n\t        example_response = \"main.py\"\n\t        task_prompt = f\"Find a specific file in a directory and return only the file path, based on the task description below. Always return a directory.###The directory structure is as follows: \\n{example_dir_structure}\\nYour task: {example_params}\\n###\\nRESPONSE:{example_response} ###The directory structure is as follows: \\n{dir_structure}\\nYour task: {params}\\n###\\nRESPONSE:\"\n\t        messages = [\n\t            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n", "            {\"role\": \"user\", \"content\": task_prompt}\n\t        ]\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n\t            messages=messages,\n\t            temperature=0.2,\n\t            max_tokens=1500,\n\t            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n", "        )\n\t        file_path = response.choices[0].message['content'].strip()\n\t        print(f\"AI suggested file path: {file_path}\")\n\t        try:\n\t            with open(file_path, 'r') as file:\n\t                file_content = file.read()\n\t                print(f\"File content:\\n{file_content}\")\n\t                return file_content\n\t        except FileNotFoundError:\n\t            print(\"File not found. Please check the AI's suggested file path.\")\n", "            return None\n\t    def get_directory_structure(self, start_path):\n\t        dir_structure = {}\n\t        ignore_dirs = ['.','__init__.py', '__pycache__', 'pydevd', 'poetry','venv']  # add any other directories to ignore here\n\t        for root, dirs, files in os.walk(start_path):\n\t            dirs[:] = [d for d in dirs if not any(d.startswith(i) for i in ignore_dirs)]  # exclude specified directories\n\t            files = [f for f in files if not f[0] == '.' and f.endswith('.py')]  # exclude hidden files and non-Python files\n\t            current_dict = dir_structure\n\t            path_parts = os.path.relpath(root, start_path).split(os.sep)\n\t            for part in path_parts:\n", "                if part:  # skip empty parts\n\t                    if part not in current_dict:\n\t                        current_dict[part] = {}\n\t                    current_dict = current_dict[part]\n\t            for f in files:\n\t                current_dict[f] = None\n\t        return dir_structure\n\t    def get_top_parent_path(self, current_path):\n\t        relative_path = \"\"\n\t        while True:\n", "            new_path = os.path.dirname(current_path)\n\t            if new_path == '/home/runner/BabyElfAGI/skills':  # reached the top\n\t                return '/home/runner/BabyElfAGI'\n\t            current_path = new_path\n\t            relative_path = os.path.join(\"..\", relative_path)\n\t        return relative_path\n"]}
{"filename": "classic/BabyElfAGI/skills/text_completion.py", "chunked_list": ["from skills.skill import Skill\n\timport openai\n\tclass TextCompletion(Skill):\n\t    name = 'text_completion'\n\t    description = \"A tool that uses OpenAI's text completion API to generate, summarize, and/or analyze text and code.\"\n\t    api_keys_required = ['openai']\n\t    def __init__(self, api_keys):\n\t        super().__init__(api_keys)\n\t    def execute(self, params, dependent_task_outputs, objective):\n\t        if not self.valid:\n", "            return\n\t        task_prompt = f\"Complete your assigned task based on the objective and only based on information provided in the dependent task output, if provided. \\n###\\nYour objective: {objective}. \\n###\\nYour task: {params} \\n###\\nDependent tasks output: {dependent_task_outputs}  \\n###\\nYour task: {params}\\n###\\nRESPONSE:\"\n\t        messages = [\n\t            {\"role\": \"user\", \"content\": task_prompt}\n\t        ]\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n\t            messages=messages,\n\t            temperature=0.4,\n\t            max_tokens=2000,\n", "            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n\t        )\n\t        return \"\\n\\n\"+response.choices[0].message['content'].strip()\n"]}
{"filename": "classic/BabyElfAGI/skills/objective_saver.py", "chunked_list": ["from skills.skill import Skill\n\timport os\n\timport openai\n\tclass ObjectiveSaver(Skill):\n\t    name = 'objective_saver'\n\t    description = \"A skill that saves a new example_objective based on the concepts from skill_saver.py\"\n\t    api_keys_required = []\n\t    def __init__(self, api_keys):\n\t        super().__init__(api_keys)\n\t    def execute(self, params, dependent_task_outputs, objective):\n", "        if not self.valid:\n\t            return\n\t        #print(dependent_task_outputs[2])\n\t        code =  dependent_task_outputs[2]\n\t        task_prompt = f\"Come up with a file name (eg. 'research_shoes.json') for the following objective:{code}\\n###\\nFILE_NAME:\"\n\t        messages = [\n\t            {\"role\": \"user\", \"content\": task_prompt}\n\t        ]\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n", "            messages=messages,\n\t            temperature=0.4,\n\t            max_tokens=3000,\n\t            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n\t        ) \n\t        file_name =  response.choices[0].message['content'].strip()\n\t        file_path = os.path.join('tasks/example_objectives',file_name)\n\t        try:\n", "            with open(file_path, 'w') as file:\n\t                file.write(\"[\"+code+\"]\")\n\t                print(f\"Code saved successfully: {file_name}\")\n\t        except:\n\t            print(\"Error saving code.\")\n\t        return None"]}
{"filename": "classic/BabyElfAGI/skills/skill_registry.py", "chunked_list": ["import os\n\timport importlib.util\n\timport inspect\n\tfrom .skill import Skill\n\tclass SkillRegistry:\n\t    def __init__(self, api_keys, skill_names=None):\n\t        self.skills = {}\n\t        skill_files = [f for f in os.listdir('skills') if f.endswith('.py') and f != 'skill.py']\n\t        for skill_file in skill_files:\n\t            module_name = skill_file[:-3]\n", "            if skill_names and module_name not in skill_names:\n\t                continue\n\t            module = importlib.import_module(f'skills.{module_name}')\n\t            for attr_name in dir(module):\n\t                attr_value = getattr(module, attr_name)\n\t                if inspect.isclass(attr_value) and issubclass(attr_value, Skill) and attr_value is not Skill:\n\t                    skill = attr_value(api_keys)\n\t                    if skill.valid:\n\t                        self.skills[skill.name] = skill\n\t        # Print the names and descriptions of all loaded skills\n", "        skill_info = \"\\n\".join([f\"{skill_name}: {skill.description}\" for skill_name, skill in self.skills.items()])\n\t        # print(skill_info)\n\t    def load_all_skills(self):\n\t        skills_dir = os.path.dirname(__file__)\n\t        for filename in os.listdir(skills_dir):\n\t            if filename.endswith(\".py\") and filename not in [\"__init__.py\", \"skill.py\", \"skill_registry.py\"]:\n\t                skill_name = filename[:-3]  # Remove .py extension\n\t                self.load_skill(skill_name)\n\t    def load_specific_skills(self, skill_names):\n\t        for skill_name in skill_names:\n", "            self.load_skill(skill_name)\n\t    def load_skill(self, skill_name):\n\t        skills_dir = os.path.dirname(__file__)\n\t        filename = f\"{skill_name}.py\"\n\t        if os.path.isfile(os.path.join(skills_dir, filename)):\n\t            spec = importlib.util.spec_from_file_location(skill_name, os.path.join(skills_dir, filename))\n\t            module = importlib.util.module_from_spec(spec)\n\t            spec.loader.exec_module(module)\n\t            for item_name in dir(module):\n\t                item = getattr(module, item_name)\n", "                if isinstance(item, type) and issubclass(item, Skill) and item is not Skill:\n\t                    skill_instance = item(self.api_keys)\n\t                    self.skills[skill_instance.name] = skill_instance\n\t    def get_skill(self, skill_name):\n\t        skill = self.skills.get(skill_name)\n\t        if skill is None:\n\t            raise Exception(\n\t                f\"Skill '{skill_name}' not found. Please make sure the skill is loaded and all required API keys are set.\")\n\t        return skill\n\t    def get_all_skills(self):\n", "        return self.skills"]}
{"filename": "classic/BabyElfAGI/skills/skill_saver.py", "chunked_list": ["from skills.skill import Skill\n\timport os\n\timport openai\n\tclass SkillSaver(Skill):\n\t    name = 'skill_saver'\n\t    description = \"A skill that saves code written in a previous step into a file within the skills folder. Not for writing code.\"\n\t    api_keys_required = []\n\t    def __init__(self, api_keys):\n\t        super().__init__(api_keys)\n\t    def execute(self, params, dependent_task_outputs, objective):\n", "        if not self.valid:\n\t            return\n\t        task_prompt = f\"Extract the code and only the code from the dependent task output here: {dependent_task_outputs}  \\n###\\nCODE:\"\n\t        messages = [\n\t            {\"role\": \"user\", \"content\": task_prompt}\n\t        ]\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n\t            messages=messages,\n\t            temperature=0.4,\n", "            max_tokens=3000,\n\t            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n\t        ) \n\t        code =  response.choices[0].message['content'].strip()\n\t        task_prompt = f\"Come up with a file name (eg. 'get_weather.py') for the following skill:{code}\\n###\\nFILE_NAME:\"\n\t        messages = [\n\t            {\"role\": \"user\", \"content\": task_prompt}\n\t        ]\n", "        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo\",\n\t            messages=messages,\n\t            temperature=0.4,\n\t            max_tokens=3000,\n\t            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n\t        ) \n\t        file_name =  response.choices[0].message['content'].strip()\n", "        file_path = os.path.join('skills',file_name)\n\t        try:\n\t            with open(file_path, 'w') as file:\n\t                file.write(code)\n\t                print(f\"Code saved successfully: {file_name}\")\n\t        except:\n\t            print(\"Error saving code.\")\n\t        return None"]}
{"filename": "classic/BabyElfAGI/skills/directory_structure.py", "chunked_list": ["from skills.skill import Skill\n\timport os\n\tclass DirectoryStructure(Skill):\n\t    name = 'directory_structure'\n\t    description = \"A tool that outputs the file and folder structure of its top parent folder.\"\n\t    def __init__(self, api_keys=None):\n\t        super().__init__(api_keys)\n\t    def execute(self, params, dependent_task_outputs, objective):\n\t        # Get the current script path\n\t        current_script_path = os.path.realpath(__file__)\n", "        # Get the top parent directory of current script\n\t        top_parent_path = self.get_top_parent_path(current_script_path)\n\t        # Get the directory structure from the top parent directory\n\t        dir_structure = self.get_directory_structure(top_parent_path)\n\t        return dir_structure\n\t    def get_directory_structure(self, start_path):\n\t        dir_structure = {}\n\t        ignore_dirs = ['.','__init__.py', '__pycache__', 'pydevd', 'poetry','venv']  # add any other directories to ignore here\n\t        for root, dirs, files in os.walk(start_path):\n\t            dirs[:] = [d for d in dirs if not any(d.startswith(i) for i in ignore_dirs)]  # exclude specified directories\n", "            files = [f for f in files if not f[0] == '.' and f.endswith('.py')]  # exclude hidden files and non-Python files\n\t            current_dict = dir_structure\n\t            path_parts = os.path.relpath(root, start_path).split(os.sep)\n\t            for part in path_parts:\n\t                if part:  # skip empty parts\n\t                    if part not in current_dict:\n\t                        current_dict[part] = {}\n\t                    current_dict = current_dict[part]\n\t            for f in files:\n\t                current_dict[f] = None\n", "        #print(\"#############################\")\n\t        #print(str(current_dict)[0:100])\n\t        return dir_structure\n\t    def get_top_parent_path(self, current_path):\n\t        relative_path = \"\"\n\t        while True:\n\t            new_path = os.path.dirname(current_path)\n\t            print(new_path)\n\t            if new_path == '/home/runner/BabyElfAGI/skills':  # reached the top\n\t            #if new_path == current_path:  # reached the top\n", "                #return relative_path\n\t                return '/home/runner/BabyElfAGI'\n\t            current_path = new_path\n\t            relative_path = os.path.join(\"..\", relative_path)\n\t            print(relative_path)"]}
{"filename": "classic/BabyElfAGI/skills/web_search.py", "chunked_list": ["from skills.skill import Skill\n\tfrom serpapi import GoogleSearch\n\timport openai\n\tfrom bs4 import BeautifulSoup\n\timport requests\n\timport re\n\theaders = {\n\t    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n\t}\n\tclass WebSearch(Skill):\n", "    name = 'web_search'\n\t    description = 'A tool that performs web searches.'\n\t    api_keys_required = [['openai'],['serpapi']]\n\t    def __init__(self, api_keys):\n\t        super().__init__(api_keys)\n\t    def execute(self, params, dependent_task_outputs, objective):\n\t        # Your function goes here\n\t        # Modify the query based on the dependent task output\n\t        if dependent_task_outputs != \"\":\n\t            dependent_task = f\"Use the dependent task output below as reference to help craft the correct search query for the provided task above. Dependent task output:{dependent_task_outputs}.\"\n", "        else:\n\t            dependent_task = \".\"\n\t        query = self.text_completion_tool(\"You are an AI assistant tasked with generating a Google search query based on the following task: \"+params+\". If the task looks like a search query, return the identical search query as your response. \" + dependent_task + \"\\nSearch Query:\")\n\t        print(\"\\033[90m\\033[3m\"+\"Search query: \" +str(query)+\"\\033[0m\")\n\t        # Set the search parameters\n\t        search_params = {\n\t            \"engine\": \"google\",\n\t            \"q\": query,\n\t            \"api_key\": self.serpapi_api_key,\n\t            \"num\": 3\n", "        }\n\t        # Perform the web search\n\t        search_results = GoogleSearch(search_params).get_dict()\n\t        # Simplify the search results\n\t        search_results = self.simplify_search_results(search_results.get('organic_results', []))\n\t        print(\"\\033[90m\\033[3mCompleted search. Now scraping results.\\n\\033[0m\")\n\t        # Store the results from web scraping\n\t        results = \"\"\n\t        for result in search_results:\n\t            url = result.get('link')\n", "            print(\"\\033[90m\\033[3m\" + \"Scraping: \"+url+\"\" + \"...\\033[0m\")\n\t            content = self.web_scrape_tool({\"url\": url, \"task\": params,\"objective\":objective})\n\t            results += str(content) + \". \"\n\t        print(\"\\033[90m\\033[3m\"+str(results[0:100])[0:100]+\"...\\033[0m\")\n\t        # Process the results and generate a report\n\t        results = self.text_completion_tool(f\"You are an expert analyst combining the results of multiple web scrapes. Rewrite the following information as one cohesive report without removing any facts. Ignore any reports of not having info, unless all reports say so - in which case explain that the search did not work and suggest other web search queries to try. \\n###INFORMATION:{results}.\\n###REPORT:\")\n\t        return results\n\t    def simplify_search_results(self, search_results):\n\t        simplified_results = []\n\t        for result in search_results:\n", "            simplified_result = {\n\t                \"position\": result.get(\"position\"),\n\t                \"title\": result.get(\"title\"),\n\t                \"link\": result.get(\"link\"),\n\t                \"snippet\": result.get(\"snippet\")\n\t            }\n\t            simplified_results.append(simplified_result)\n\t        return simplified_results\n\t    def text_completion_tool(self, prompt: str):\n\t        messages = [\n", "            {\"role\": \"user\", \"content\": prompt}\n\t        ]\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo-16k-0613\",\n\t            messages=messages,\n\t            temperature=0.2,\n\t            max_tokens=1500,\n\t            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n", "        )\n\t        return response.choices[0].message['content'].strip()\n\t    def web_scrape_tool(self, params):\n\t        content = self.fetch_url_content(params['url'])\n\t        if content is None:\n\t            return None\n\t        text = self.extract_text(content)\n\t        print(\"\\033[90m\\033[3m\"+\"Scrape completed. Length:\" +str(len(text))+\".Now extracting relevant info...\"+\"...\\033[0m\")\n\t        info = self.extract_relevant_info(params['objective'], text[0:11000], params['task'])\n\t        links = self.extract_links(content)\n", "        #result = f\"{info} URLs: {', '.join(links)}\"\n\t        result = info\n\t        return result\n\t    def fetch_url_content(self,url: str):\n\t        try:\n\t            response = requests.get(url, headers=headers, timeout=10)\n\t            response.raise_for_status()\n\t            return response.content\n\t        except requests.exceptions.RequestException as e:\n\t            print(f\"Error while fetching the URL: {e}\")\n", "            return \"\"\n\t    def extract_links(self,content: str):\n\t        soup = BeautifulSoup(content, \"html.parser\")\n\t        links = [link.get('href') for link in soup.findAll('a', attrs={'href': re.compile(\"^https?://\")})]\n\t        return links\n\t    def extract_text(self,content: str):\n\t        soup = BeautifulSoup(content, \"html.parser\")\n\t        text = soup.get_text(strip=True)\n\t        return text\n\t    def extract_relevant_info(self, objective, large_string, task):\n", "        chunk_size = 12000\n\t        overlap = 500\n\t        notes = \"\"\n\t        if len(large_string) == 0:\n\t          print(\"error scraping\")\n\t          return \"Error scraping.\"\n\t        for i in range(0, len(large_string), chunk_size - overlap):\n\t            print(\"\\033[90m\\033[3m\"+\"Reading chunk...\"+\"\\033[0m\")  \n\t            chunk = large_string[i:i + chunk_size]\n\t            messages = [\n", "                {\"role\": \"system\", \"content\": f\"You are an AI assistant.\"},\n\t                {\"role\": \"user\", \"content\": f\"You are an expert AI research assistant tasked with creating or updating the current notes. If the current note is empty, start a current-notes section by exracting relevant data to the task and objective from the chunk of text to analyze. If there is a current note, add new relevant info frol the chunk of text to analyze. Make sure the new or combined notes is comprehensive and well written. Here's the current chunk of text to analyze: {chunk}. ### Here is the current task: {task}.### For context, here is the objective: {objective}.### Here is the data we've extraced so far that you need to update: {notes}.### new-or-updated-note:\"}\n\t            ]\n\t            response = openai.ChatCompletion.create(\n\t                model=\"gpt-3.5-turbo-16k-0613\",\n\t                messages=messages,\n\t                max_tokens=800,\n\t                n=1,\n\t                stop=\"###\",\n\t                temperature=0.7,\n", "            )\n\t            notes += response.choices[0].message['content'].strip()+\". \";\n\t        return notes"]}
{"filename": "classic/BabyElfAGI/tasks/task_registry.py", "chunked_list": ["import openai\n\timport json\n\timport threading\n\timport os\n\timport numpy as np\n\tclass TaskRegistry:\n\t    def __init__(self):\n\t        self.tasks = []\n\t        # Initialize the lock\n\t        self.lock = threading.Lock()\n", "        objectives_file_path = \"tasks/example_objectives\"\n\t        self.example_loader = ExampleObjectivesLoader(objectives_file_path)\n\t    def load_example_objectives(self, user_objective):\n\t        return self.example_loader.load_example_objectives(user_objective)\n\t    def create_tasklist(self, objective, skill_descriptions):\n\t        #load most relevant object and tasklist from objectives_examples.json\n\t        example_objective, example_tasklist = self.load_example_objectives(objective)\n\t        prompt = (\n\t            f\"You are an expert task list creation AI tasked with creating a  list of tasks as a JSON array, considering the ultimate objective of your team: {objective}. \"\n\t            f\"Create a very short task list based on the objective, the final output of the last task will be provided back to the user. Limit tasks types to those that can be completed with the available skills listed below. Task description should be detailed.###\"\n", "            f\"AVAILABLE SKILLS: {skill_descriptions}.###\"\n\t            f\"RULES:\"\n\t            f\"Do not use skills that are not listed.\"\n\t            f\"Always include one skill.\"\n\t            f\"dependent_task_ids should always be an empty array, or an array of numbers representing the task ID it should pull results from.\"\n\t            f\"Make sure all task IDs are in chronological order.###\\n\"\n\t            f\"EXAMPLE OBJECTIVE={json.dumps(example_objective)}\"\n\t            f\"TASK LIST={json.dumps(example_tasklist)}\"\n\t            f\"OBJECTIVE={objective}\"\n\t            f\"TASK LIST=\"\n", "        )\n\t        print(\"\\033[90m\\033[3m\" + \"\\nInitializing...\\n\" + \"\\033[0m\")\n\t        response = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo-0613\",\n\t            messages=[\n\t                {\n\t                    \"role\": \"system\",\n\t                    \"content\": \"You are a task creation AI.\"\n\t                },\n\t                {\n", "                    \"role\": \"user\",\n\t                    \"content\": prompt\n\t                }\n\t            ],\n\t            temperature=0,\n\t            max_tokens=1500,\n\t            top_p=1,\n\t            frequency_penalty=0,\n\t            presence_penalty=0\n\t        )\n", "        # Extract the content of the assistant's response and parse it as JSON\n\t        result = response[\"choices\"][0][\"message\"][\"content\"]\n\t        try:\n\t            task_list = json.loads(result)\n\t            self.tasks = task_list\n\t        except Exception as error:\n\t            print(error)\n\t    def execute_task(self, i, task, skill_registry, task_outputs, objective):\n\t        p_nexttask=\"\\033[92m\\033[1m\"+\"\\n*****NEXT TASK ID:\"+str(task['id'])+\"*****\\n\"+\"\\033[0m\\033[0m\"\n\t        p_nexttask += f\"\\033[ EExecuting task {task.get('id')}: {task.get('task')}) [{task.get('skill')}]\\033[)\"\n", "        print(p_nexttask)\n\t        # Retrieve the skill from the registry\n\t        skill = skill_registry.get_skill(task['skill'])\n\t        # Get the outputs of the dependent tasks\n\t        dependent_task_outputs = {dep: task_outputs[dep][\"output\"] for dep in task['dependent_task_ids']} if 'dependent_task_ids' in task else {}\n\t        # Execute the skill\n\t        # print(\"execute:\"+str([task['task'], dependent_task_outputs, objective]))\n\t        task_output = skill.execute(task['task'], dependent_task_outputs, objective)\n\t        print(\"\\033[93m\\033[1m\"+\"\\nTask Output (ID:\"+str(task['id'])+\"):\"+\"\\033[0m\\033[0m\")\n\t        print(\"TASK: \"+str(task[\"task\"]))\n", "        print(\"OUTPUT: \"+str(task_output))\n\t        return i, task_output\n\t    def reorder_tasks(self):\n\t        self.tasks = sorted(self.tasks, key=lambda task: task['id'])\n\t    def add_task(self, task, after_task_id):\n\t        # Get the task ids\n\t        task_ids = [t[\"id\"] for t in self.tasks]\n\t        # Get the index of the task id to add the new task after\n\t        insert_index = task_ids.index(after_task_id) + 1 if after_task_id in task_ids else len(task_ids)\n\t        # Insert the new task\n", "        self.tasks.insert(insert_index, task)\n\t        self.reorder_tasks()\n\t    def update_tasks(self, task_update):\n\t        for task in self.tasks:\n\t            if task['id'] == task_update['id']:\n\t                # This merges the original task dictionary with the update, overwriting only the fields present in the update.\n\t                task.update(task_update)\n\t                self.reorder_tasks()\n\t    def reflect_on_output(self, task_output, skill_descriptions):\n\t        with self.lock:\n", "            example = [\n\t                [\n\t                    {\"id\": 3, \"task\": \"New task 1 description\", \"skill\": \"text_completion_skill\",\n\t                     \"dependent_task_ids\": [], \"status\": \"complete\"},\n\t                    {\"id\": 4, \"task\": \"New task 2 description\", \"skill\": \"text_completion_skill\",\n\t                     \"dependent_task_ids\": [], \"status\": \"incomplete\"}\n\t                ],\n\t                [2, 3],\n\t                {\"id\": 5, \"task\": \"Complete the objective and provide a final report\",\n\t                 \"skill\": \"text_completion_skill\", \"dependent_task_ids\": [1, 2, 3, 4], \"status\": \"incomplete\"}\n", "            ]\n\t            prompt = (\n\t                f\"You are an expert task manager, review the task output to decide at least one new task to add.\"\n\t                f\"As you add a new task, see if there are any tasks that need to be updated (such as updating dependencies).\"\n\t                f\"Use the current task list as reference.\"\n\t                f\"Do not add duplicate tasks to those in the current task list.\"\n\t                f\"Only provide JSON as your response without further comments.\"\n\t                f\"Every new and updated task must include all variables, even they are empty array.\"\n\t                f\"Dependent IDs must be smaller than the ID of the task.\"\n\t                f\"New tasks IDs should be no larger than the last task ID.\"\n", "                f\"Always select at least one skill.\"\n\t                f\"Task IDs should be unique and in chronological order.\"                f\"Do not change the status of complete tasks.\"\n\t                f\"Only add skills from the AVAILABLE SKILLS, using the exact same spelling.\"\n\t                f\"Provide your array as a JSON array with double quotes. The first object is new tasks to add as a JSON array, the second array lists the ID numbers where the new tasks should be added after (number of ID numbers matches array), and the third object provides the tasks that need to be updated.\"\n\t                f\"Make sure to keep dependent_task_ids key, even if an empty array.\"\n\t                f\"AVAILABLE SKILLS: {skill_descriptions}.###\"\n\t                f\"\\n###Here is the last task output: {task_output}\"\n\t                f\"\\n###Here is the current task list: {self.tasks}\"\n\t                f\"\\n###EXAMPLE OUTPUT FORMAT = {json.dumps(example)}\"\n\t                f\"\\n###OUTPUT = \"\n", "            )\n\t            print(\"\\033[90m\\033[3m\" + \"\\nReflecting on task output to generate new tasks if necessary...\\n\" + \"\\033[0m\")\n\t            response = openai.ChatCompletion.create(\n\t                model=\"gpt-3.5-turbo-16k-0613\",\n\t                messages=[\n\t                    {\n\t                        \"role\": \"system\",\n\t                        \"content\": \"You are a task creation AI.\"\n\t                    },\n\t                    {\n", "                        \"role\": \"user\",\n\t                        \"content\": prompt\n\t                    }\n\t                ],\n\t                temperature=0.7,\n\t                max_tokens=1500,\n\t                top_p=1,\n\t                frequency_penalty=0,\n\t                presence_penalty=0\n\t            )\n", "            # Extract the content of the assistant's response and parse it as JSON\n\t            result = response[\"choices\"][0][\"message\"][\"content\"]\n\t            print(\"\\n#\" + str(result))\n\t            # Check if the returned result has the expected structure\n\t            if isinstance(result, str):\n\t                try:\n\t                    task_list = json.loads(result)\n\t                    # print(\"RESULT:\")\n\t                    print(task_list)\n\t                    # return [],[],[]\n", "                    return task_list[0], task_list[1], task_list[2]\n\t                except Exception as error:\n\t                    print(error)\n\t            else:\n\t                raise ValueError(\"Invalid task list structure in the output\")\n\t    def get_tasks(self):\n\t        \"\"\"\n\t        Returns the current list of tasks.\n\t        Returns:\n\t        list: the list of tasks.\n", "        \"\"\"\n\t        return self.tasks\n\t    def get_task(self, task_id):\n\t        \"\"\"\n\t        Returns a task given its task_id.\n\t        Parameters:\n\t        task_id : int\n\t            The unique ID of the task.\n\t        Returns:\n\t        dict\n", "            The task that matches the task_id.\n\t        \"\"\"\n\t        matching_tasks = [task for task in self.tasks if task[\"id\"] == task_id]\n\t        if matching_tasks:\n\t            return matching_tasks[0]\n\t        else:\n\t            print(f\"No task found with id {task_id}\")\n\t            return None\n\t    def print_tasklist(self, task_list):\n\t        p_tasklist=\"\\033[95m\\033[1m\" + \"\\n*****TASK LIST*****\\n\" + \"\\033[0m\"\n", "        for t in task_list:\n\t            dependent_task_ids = t.get('dependent_task_ids', [])\n\t            dependent_task = \"\"\n\t            if dependent_task_ids:\n\t                dependent_task = f\"\\033[31m<dependencies: {', '.join([f'#{dep_id}' for dep_id in dependent_task_ids])}>\\033[0m\"\n\t            status_color = \"\\033[32m\" if t.get('status') == \"completed\" else \"\\033[31m\"\n\t            p_tasklist+= f\"\\033[1m{t.get('id')}\\033[0m: {t.get('task')} {status_color}[{t.get('status')}]\\033[0m \\033[93m[{t.get('skill')}] {dependent_task}\\033[0m\\n\"\n\t        print(p_tasklist)\n\tclass ExampleObjectivesLoader:\n\t    def __init__(self, objectives_folder_path):\n", "        self.objectives_folder_path = objectives_folder_path\n\t        self.objectives_examples = []  # Initialize as an empty list\n\t    def load_objectives_examples(self):\n\t        self.objectives_examples = []\n\t        for filename in os.listdir(self.objectives_folder_path):\n\t            file_path = os.path.join(self.objectives_folder_path, filename)\n\t            with open(file_path, 'r') as file:\n\t                objectives = json.load(file)\n\t                self.objectives_examples.extend(objectives)\n\t    def find_most_relevant_objective(self, user_input):\n", "        user_input_embedding = self.get_embedding(user_input, model='text-embedding-ada-002')\n\t        most_relevant_objective = max(\n\t            self.objectives_examples,\n\t            key=lambda pair: self.cosine_similarity(pair['objective'], user_input_embedding)\n\t        )\n\t        return most_relevant_objective['objective'], most_relevant_objective['examples']\n\t    def get_embedding(self, text, model='text-embedding-ada-002'):\n\t        response = openai.Embedding.create(input=[text], model=model)\n\t        embedding = response['data'][0]['embedding']\n\t        return embedding\n", "    def cosine_similarity(self, objective, embedding):\n\t        max_similarity = float('-inf')\n\t        objective_embedding = self.get_embedding(objective, model='text-embedding-ada-002')\n\t        similarity = self.calculate_similarity(objective_embedding, embedding)\n\t        max_similarity = max(max_similarity, similarity)\n\t        return max_similarity\n\t    def calculate_similarity(self, embedding1, embedding2):\n\t        embedding1 = np.array(embedding1, dtype=np.float32)\n\t        embedding2 = np.array(embedding2, dtype=np.float32)\n\t        similarity = np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n", "        return similarity\n\t    def load_example_objectives(self, user_objective):\n\t        self.load_objectives_examples()\n\t        most_relevant_objective, most_relevant_tasklist = self.find_most_relevant_objective(user_objective)\n\t        example_objective = most_relevant_objective\n\t        example_tasklist = most_relevant_tasklist\n\t        return example_objective, example_tasklist\n"]}
{"filename": "extensions/ray_objectives.py", "chunked_list": ["import logging\n\timport ray\n\tfrom collections import deque\n\tACTOR_NAME=\"BabyAGI Objectives\"\n\ttry:\n\t    ray.init(address=\"auto\", namespace=\"babyagi\", logging_level=logging.FATAL, ignore_reinit_error=True)\n\texcept:\n\t    ray.init(namespace=\"babyagi\", logging_level=logging.FATAL, ignore_reinit_error=True)\n\t@ray.remote\n\tclass CooperativeObjectivesListStorageActor:\n", "    def __init__(self):\n\t        self.objectives = deque([])\n\t    def append(self, objective: str):\n\t        if not objective in self.objectives:\n\t            self.objectives.append(objective)\n\t    def is_empty(self):\n\t        return False if self.objectives else True\n\t    def get_objective_names(self):\n\t        return [t for t in self.objectives]\n\tclass CooperativeObjectivesListStorage:\n", "    def __init__(self):\n\t        try:\n\t            self.actor = ray.get_actor(name=ACTOR_NAME, namespace=\"babyagi\")\n\t        except ValueError:\n\t            self.actor = CooperativeObjectivesListStorageActor.options(name=ACTOR_NAME, namespace=\"babyagi\", lifetime=\"detached\").remote()\n\t    def append(self, objective: str):\n\t        self.actor.append.remote(objective)\n\t    def is_empty(self):\n\t        return ray.get(self.actor.is_empty.remote())\n\t    def get_objective_names(self):\n", "        return ray.get(self.actor.get_objective_names.remote())\n"]}
{"filename": "extensions/pinecone_storage.py", "chunked_list": ["from typing import Dict, List\n\timport importlib\n\timport openai\n\timport pinecone\n\timport re\n\tdef can_import(module_name):\n\t    try:\n\t        importlib.import_module(module_name)\n\t        return True\n\t    except ImportError:\n", "        return False\n\tassert (\n\t    can_import(\"pinecone\")\n\t), \"\\033[91m\\033[1m\"+\"Pinecone storage requires package pinecone-client.\\nInstall:  pip install -r extensions/requirements.txt\"\n\tclass PineconeResultsStorage:\n\t    def __init__(self, openai_api_key: str, pinecone_api_key: str, pinecone_environment: str, llm_model: str, llama_model_path: str, results_store_name: str, objective: str):\n\t        openai.api_key = openai_api_key\n\t        pinecone.init(api_key=pinecone_api_key, environment=pinecone_environment)\n\t        # Pinecone namespaces are only compatible with ascii characters (used in query and upsert)\n\t        self.namespace = re.sub(re.compile('[^\\x00-\\x7F]+'), '', objective)\n", "        self.llm_model = llm_model\n\t        self.llama_model_path = llama_model_path\n\t        results_store_name = results_store_name\n\t        dimension = 1536 if not self.llm_model.startswith(\"llama\") else 5120\n\t        metric = \"cosine\"\n\t        pod_type = \"p1\"\n\t        if results_store_name not in pinecone.list_indexes():\n\t            pinecone.create_index(\n\t                results_store_name, dimension=dimension, metric=metric, pod_type=pod_type\n\t            )\n", "        self.index = pinecone.Index(results_store_name)\n\t        index_stats_response = self.index.describe_index_stats()\n\t        assert dimension == index_stats_response['dimension'], \"Dimension of the index does not match the dimension of the LLM embedding\"\n\t    def add(self, task: Dict, result: str, result_id: int):\n\t        vector = self.get_embedding(\n\t            result\n\t        )\n\t        self.index.upsert(\n\t            [(result_id, vector, {\"task\": task[\"task_name\"], \"result\": result})], namespace=self.namespace\n\t        )\n", "    def query(self, query: str, top_results_num: int) -> List[dict]:\n\t        query_embedding = self.get_embedding(query)\n\t        results = self.index.query(query_embedding, top_k=top_results_num, include_metadata=True, namespace=self.namespace)\n\t        sorted_results = sorted(results.matches, key=lambda x: x.score, reverse=True)\n\t        return [(str(item.metadata[\"task\"])) for item in sorted_results]\n\t    # Get embedding for the text\n\t    def get_embedding(self, text: str) -> list:\n\t        text = text.replace(\"\\n\", \" \")\n\t        if self.llm_model.startswith(\"llama\"):\n\t            from llama_cpp import Llama\n", "            llm_embed = Llama(\n\t                model_path=self.llama_model_path,\n\t                n_ctx=2048, n_threads=4,\n\t                embedding=True, use_mlock=True,\n\t            )\n\t            return llm_embed.embed(text)\n\t        return openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")[\"data\"][0][\"embedding\"]\n"]}
{"filename": "extensions/__init__.py", "chunked_list": []}
{"filename": "extensions/argparseext.py", "chunked_list": ["import os\n\timport sys\n\timport importlib\n\timport argparse\n\tdef can_import(module_name):\n\t    try:\n\t        importlib.import_module(module_name)\n\t        return True\n\t    except ImportError:\n\t        return False\n", "# Extract the env filenames in the -e flag only\n\t# Ignore any other arguments\n\tdef parse_dotenv_extensions(argv):\n\t    env_argv = []\n\t    if '-e' in argv:\n\t        tmp_argv = argv[argv.index('-e') + 1:]\n\t        parsed_args = []\n\t        for arg in tmp_argv:\n\t            if arg.startswith('-'):\n\t                break\n", "            parsed_args.append(arg)\n\t        env_argv = ['-e'] + parsed_args\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('-e', '--env', nargs='+', help='''\n\t    filenames for additional env variables to load\n\t    ''', default=os.getenv(\"DOTENV_EXTENSIONS\", \"\").split(' '))\n\t    return parser.parse_args(env_argv).env\n\tdef parse_arguments():\n\t    dotenv_extensions = parse_dotenv_extensions(sys.argv)\n\t    # Check if we need to load any additional env files\n", "    # This allows us to override the default .env file\n\t    # and update the default values for any command line arguments\n\t    if dotenv_extensions:\n\t        from extensions.dotenvext import load_dotenv_extensions\n\t        load_dotenv_extensions(parse_dotenv_extensions(sys.argv))\n\t    # Now parse the full command line arguments\n\t    parser = argparse.ArgumentParser(\n\t        add_help=False,\n\t    )\n\t    parser.add_argument('objective', nargs='*', metavar='<objective>', help='''\n", "    main objective description. Doesn\\'t need to be quoted.\n\t    if not specified, get objective from environment.\n\t    ''', default=[os.getenv(\"OBJECTIVE\", \"\")])\n\t    parser.add_argument('-n', '--name', required=False, help='''\n\t    instance name.\n\t    if not specified, get the instance name from environment.\n\t    ''', default=os.getenv(\"INSTANCE_NAME\", os.getenv(\"BABY_NAME\", \"BabyAGI\")))\n\t    parser.add_argument('-m', '--mode', choices=['n', 'none', 'l', 'local', 'd', 'distributed'], help='''\n\t    cooperative mode type\n\t    ''', default='none')\n", "    group = parser.add_mutually_exclusive_group()\n\t    group.add_argument('-t', '--task', metavar='<initial task>', help='''\n\t    initial task description. must be quoted.\n\t    if not specified, get initial_task from environment.\n\t    ''', default=os.getenv(\"INITIAL_TASK\", os.getenv(\"FIRST_TASK\", \"\")))\n\t    group.add_argument('-j', '--join', action='store_true', help='''\n\t    join an existing objective.\n\t    install cooperative requirements.\n\t    ''')\n\t    group2 = parser.add_mutually_exclusive_group()\n", "    group2.add_argument('-4', '--gpt-4', dest='llm_model', action='store_const', const=\"gpt-4\", help='''\n\t    use GPT-4 instead of the default model.\n\t    ''')\n\t    group2.add_argument('-l', '--llama', dest='llm_model', action='store_const', const=\"llama\", help='''\n\t    use LLaMa instead of the default model. Requires llama.cpp.\n\t    ''')\n\t    # This will parse -e again, which we want, because we need\n\t    # to load those in the main file later as well\n\t    parser.add_argument('-e', '--env', nargs='+', help='''\n\t    filenames for additional env variables to load\n", "    ''', default=os.getenv(\"DOTENV_EXTENSIONS\", \"\").split(' '))\n\t    parser.add_argument('-h', '-?', '--help', action='help', help='''\n\t    show this help message and exit\n\t    ''')\n\t    args = parser.parse_args()\n\t    llm_model = args.llm_model if args.llm_model else os.getenv(\"LLM_MODEL\", os.getenv(\"OPENAI_API_MODEL\", \"gpt-3.5-turbo\")).lower()\n\t    dotenv_extensions = args.env\n\t    instance_name = args.name\n\t    if not instance_name:\n\t        print(\"\\033[91m\\033[1m\" + \"BabyAGI instance name missing\\n\" + \"\\033[0m\\033[0m\")\n", "        parser.print_help()\n\t        parser.exit()\n\t    module_name = \"ray\"\n\t    cooperative_mode = args.mode\n\t    if cooperative_mode in ['l', 'local'] and not can_import(module_name):\n\t        print(\"\\033[91m\\033[1m\"+f\"Local cooperative mode requires package {module_name}\\nInstall:  pip install -r extensions/requirements.txt\\n\" + \"\\033[0m\\033[0m\")\n\t        parser.print_help()\n\t        parser.exit()\n\t    elif cooperative_mode in ['d', 'distributed']:\n\t        print(\"\\033[91m\\033[1m\" + \"Distributed cooperative mode is not implemented yet\\n\" + \"\\033[0m\\033[0m\")\n", "        parser.print_help()\n\t        parser.exit()\n\t    join_existing_objective = args.join\n\t    if join_existing_objective and cooperative_mode in ['n', 'none']:\n\t        print(\"\\033[91m\\033[1m\"+f\"Joining existing objective requires local or distributed cooperative mode\\n\" + \"\\033[0m\\033[0m\")\n\t        parser.print_help()\n\t        parser.exit()\n\t    objective = ' '.join(args.objective).strip()\n\t    if not objective:\n\t        print(\"\\033[91m\\033[1m\" + \"No objective specified or found in environment.\\n\" + \"\\033[0m\\033[0m\")\n", "        parser.print_help()\n\t        parser.exit()\n\t    initial_task = args.task\n\t    if not initial_task and not join_existing_objective:\n\t        print(\"\\033[91m\\033[1m\" + \"No initial task specified or found in environment.\\n\" + \"\\033[0m\\033[0m\")\n\t        parser.print_help()\n\t        parser.exit()\n\t    return objective, initial_task, llm_model, dotenv_extensions, instance_name, cooperative_mode, join_existing_objective"]}
{"filename": "extensions/human_mode.py", "chunked_list": ["import sys\n\tdef user_input_await(prompt: str) -> str:\n\t    print(\"\\033[94m\\033[1m\" + \"\\n> COPY FOLLOWING TEXT TO CHATBOT\\n\" + \"\\033[0m\\033[0m\")\n\t    print(prompt)\n\t    print(\"\\033[91m\\033[1m\" + \"\\n AFTER PASTING, PRESS: (ENTER), (CTRL+Z), (ENTER) TO FINISH\\n\" + \"\\033[0m\\033[0m\")\n\t    print(\"\\033[96m\\033[1m\" + \"\\n> PASTE YOUR RESPONSE:\\n\" + \"\\033[0m\\033[0m\")\n\t    input_text = sys.stdin.read()\n\t    return input_text.strip()"]}
{"filename": "extensions/weaviate_storage.py", "chunked_list": ["import importlib\n\timport logging\n\timport re\n\tfrom typing import Dict, List\n\timport openai\n\timport weaviate\n\tfrom weaviate.embedded import EmbeddedOptions\n\tdef can_import(module_name):\n\t    try:\n\t        importlib.import_module(module_name)\n", "        return True\n\t    except ImportError:\n\t        return False\n\tassert can_import(\"weaviate\"), (\n\t    \"\\033[91m\\033[1m\"\n\t    + \"Weaviate storage requires package weaviate-client.\\nInstall:  pip install -r extensions/requirements.txt\"\n\t)\n\tdef create_client(\n\t    weaviate_url: str, weaviate_api_key: str, weaviate_use_embedded: bool\n\t):\n", "    if weaviate_use_embedded:\n\t        client = weaviate.Client(embedded_options=EmbeddedOptions())\n\t    else:\n\t        auth_config = (\n\t            weaviate.auth.AuthApiKey(api_key=weaviate_api_key)\n\t            if weaviate_api_key\n\t            else None\n\t        )\n\t        client = weaviate.Client(weaviate_url, auth_client_secret=auth_config)\n\t    return client\n", "class WeaviateResultsStorage:\n\t    schema = {\n\t        \"properties\": [\n\t            {\"name\": \"result_id\", \"dataType\": [\"string\"]},\n\t            {\"name\": \"task\", \"dataType\": [\"string\"]},\n\t            {\"name\": \"result\", \"dataType\": [\"text\"]},\n\t        ]\n\t    }\n\t    def __init__(\n\t        self,\n", "        openai_api_key: str,\n\t        weaviate_url: str,\n\t        weaviate_api_key: str,\n\t        weaviate_use_embedded: bool,\n\t        llm_model: str,\n\t        llama_model_path: str,\n\t        results_store_name: str,\n\t        objective: str,\n\t    ):\n\t        openai.api_key = openai_api_key\n", "        self.client = create_client(\n\t            weaviate_url, weaviate_api_key, weaviate_use_embedded\n\t        )\n\t        self.index_name = None\n\t        self.create_schema(results_store_name)\n\t        self.llm_model = llm_model\n\t        self.llama_model_path = llama_model_path\n\t    def create_schema(self, results_store_name: str):\n\t        valid_class_name = re.compile(r\"^[A-Z][a-zA-Z0-9_]*$\")\n\t        if not re.match(valid_class_name, results_store_name):\n", "            raise ValueError(\n\t                f\"Invalid index name: {results_store_name}. \"\n\t                \"Index names must start with a capital letter and \"\n\t                \"contain only alphanumeric characters and underscores.\"\n\t            )\n\t        self.schema[\"class\"] = results_store_name\n\t        if self.client.schema.contains(self.schema):\n\t            logging.info(\n\t                f\"Index named {results_store_name} already exists. Reusing it.\"\n\t            )\n", "        else:\n\t            logging.info(f\"Creating index named {results_store_name}\")\n\t            self.client.schema.create_class(self.schema)\n\t        self.index_name = results_store_name\n\t    def add(self, task: Dict, result: Dict, result_id: int, vector: List):\n\t        enriched_result = {\"data\": result}\n\t        vector = self.get_embedding(enriched_result[\"data\"])\n\t        with self.client.batch as batch:\n\t            data_object = {\n\t                \"result_id\": result_id,\n", "                \"task\": task[\"task_name\"],\n\t                \"result\": result,\n\t            }\n\t            batch.add_data_object(\n\t                data_object=data_object, class_name=self.index_name, vector=vector\n\t            )\n\t    def query(self, query: str, top_results_num: int) -> List[dict]:\n\t        query_embedding = self.get_embedding(query)\n\t        results = (\n\t            self.client.query.get(self.index_name, [\"task\"])\n", "            .with_hybrid(query=query, alpha=0.5, vector=query_embedding)\n\t            .with_limit(top_results_num)\n\t            .do()\n\t        )\n\t        return self._extract_tasks(results)\n\t    def _extract_tasks(self, data):\n\t        task_data = data.get(\"data\", {}).get(\"Get\", {}).get(self.index_name, [])\n\t        return [item[\"task\"] for item in task_data]\n\t    # Get embedding for the text\n\t    def get_embedding(self, text: str) -> list:\n", "        text = text.replace(\"\\n\", \" \")\n\t        if self.llm_model.startswith(\"llama\"):\n\t            from llama_cpp import Llama\n\t            llm_embed = Llama(\n\t                model_path=self.llama_model_path,\n\t                n_ctx=2048,\n\t                n_threads=4,\n\t                embedding=True,\n\t                use_mlock=True,\n\t            )\n", "            return llm_embed.embed(text)\n\t        return openai.Embedding.create(input=[text], model=\"text-embedding-ada-002\")[\n\t            \"data\"\n\t        ][0][\"embedding\"]\n"]}
{"filename": "extensions/ray_tasks.py", "chunked_list": ["import sys\n\timport logging\n\timport ray\n\tfrom collections import deque\n\tfrom typing import Dict, List\n\tfrom pathlib import Path\n\tsys.path.append(str(Path(__file__).resolve().parent.parent))\n\tfrom extensions.ray_objectives import CooperativeObjectivesListStorage\n\ttry:\n\t    ray.init(address=\"auto\", namespace=\"babyagi\", logging_level=logging.FATAL, ignore_reinit_error=True)\n", "except:\n\t    ray.init(namespace=\"babyagi\", logging_level=logging.FATAL, ignore_reinit_error=True)\n\t@ray.remote\n\tclass CooperativeTaskListStorageActor:\n\t    def __init__(self):\n\t        self.tasks = deque([])\n\t        self.task_id_counter = 0\n\t    def append(self, task: Dict):\n\t        self.tasks.append(task)\n\t    def replace(self, tasks: List[Dict]):\n", "        self.tasks = deque(tasks)\n\t    def popleft(self):\n\t        return self.tasks.popleft()\n\t    def is_empty(self):\n\t        return False if self.tasks else True\n\t    def next_task_id(self):\n\t        self.task_id_counter += 1\n\t        return self.task_id_counter\n\t    def get_task_names(self):\n\t        return [t[\"task_name\"] for t in self.tasks]\n", "class CooperativeTaskListStorage:\n\t    def __init__(self, name: str):\n\t        self.name = name\n\t        try:\n\t            self.actor = ray.get_actor(name=self.name, namespace=\"babyagi\")\n\t        except ValueError:\n\t            self.actor = CooperativeTaskListStorageActor.options(name=self.name, namespace=\"babyagi\", lifetime=\"detached\").remote()\n\t        objectives = CooperativeObjectivesListStorage()\n\t        objectives.append(self.name)\n\t    def append(self, task: Dict):\n", "        self.actor.append.remote(task)\n\t    def replace(self, tasks: List[Dict]):\n\t        self.actor.replace.remote(tasks)\n\t    def popleft(self):\n\t        return ray.get(self.actor.popleft.remote())\n\t    def is_empty(self):\n\t        return ray.get(self.actor.is_empty.remote())\n\t    def next_task_id(self):\n\t        return ray.get(self.actor.next_task_id.remote())\n\t    def get_task_names(self):\n", "        return ray.get(self.actor.get_task_names.remote())\n"]}
{"filename": "extensions/dotenvext.py", "chunked_list": ["from dotenv import load_dotenv\n\tdef load_dotenv_extensions(dotenv_files):\n\t    for dotenv_file in dotenv_files:\n\t        load_dotenv(dotenv_file)\n"]}
