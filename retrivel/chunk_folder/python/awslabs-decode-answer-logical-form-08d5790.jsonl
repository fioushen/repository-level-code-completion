{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python3\n\tfrom setuptools import setup\n\twith open(\"README.md\") as f:\n\t    readme = f.read()\n\tsetup(\n\t    name=\"DecAF\",\n\t    version=\"1.0.0\",\n\t    description=\"Joint Decoding Answer and Logical Form for Knowledge Base Question Answering through Retrieval\",\n\t    classifiers=[\n\t        \"Intended Audience :: Science/Research\",\n", "        \"License :: OSI Approved :: Apache Software License\",\n\t        \"Programming Language :: Python :: 3.8\",\n\t        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n\t    ],\n\t    long_description=readme,\n\t    long_description_content_type=\"text/markdown\",\n\t    setup_requires=[\n\t        \"setuptools>=18.0\",\n\t    ],\n\t)"]}
{"filename": "DecAF/Datasets/QA/evaluate.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t#\n\t# Licensed to the Apache Software Foundation (ASF) under one\n\t# or more contributor license agreements.  See the NOTICE file\n\t# distributed with this work for additional information\n\t# regarding copyright ownership.  The ASF licenses this file\n\t# to you under the Apache License, Version 2.0 (the\n\t# \"License\"); you may not use this file except in compliance\n\t# with the License.  You may obtain a copy of the License at\n\t#\n", "#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing,\n\t# software distributed under the License is distributed on an\n\t# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\t# KIND, either express or implied.  See the License for the\n\t# specific language governing permissions and limitations\n\t# under the License.\n\timport os\n\timport json\n", "import argparse\n\tfrom DecAF.Knowledge.linearize import (\n\t    load_nameid_dict,\n\t    get_raw_name,\n\t)\n\tfrom DecAF.Datasets.QA.utils import (\n\t    Prefix_to_id_all, \n\t    answer_ensemble, \n\t    id_to_name,\n\t    to_webqsp_format,\n", "    to_grailqa_format,\n\t    to_cwq_format,\n\t    compute_hits1_dict,\n\t)\n\timport logging\n\tlogging.basicConfig(level=logging.INFO, format='%(message)s')\n\tDATA_DIR = os.environ['DATA_DIR']\n\tSAVE_DIR = os.environ['SAVE_DIR']\n\tparser = argparse.ArgumentParser(description='dataset evaluation')\n\tparser.add_argument('--dataset', type=str, help=\"WebQSP, GrailQA, CWQ, FreebaseQA\")\n", "parser.add_argument('--result_path', type=str, help=\"result directory\")\n\targs = parser.parse_args()\n\tresult_path = args.result_path\n\tif not os.path.exists(result_path):\n\t    exit(f\"{result_path} not found\")\n\t# the evaluation for FreebaseQA is different from the other three datasets\n\t# since it does not provide the labeled logical forms\n\t# we only evaluate the predicted answers under the QA setting\n\tif args.dataset == \"FreebaseQA\":\n\t    with open(result_path, \"r\") as rf:\n", "        results = json.load(rf)\n\t    gold_answers, pred_answers = {}, {}\n\t    for key in results:\n\t        gold_answers[key] = results[key][\"gold answers\"]\n\t        gold_answers[key] = [get_raw_name(each) for each in gold_answers[key]]\n\t        pred_answers[key] = results[key][\"predicted answers\"]\n\t        pred_answers[key] = [get_raw_name(each) for each in pred_answers[key]]\n\t    logging.info(\"Hits@1: {}\".format(compute_hits1_dict(pred_answers, gold_answers)))\n\t    exit()\n\tname_dir = DATA_DIR + \"/knowledge_source/Freebase/id2name_parts_disamb\"\n", "name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\tname_dir = DATA_DIR + \"/knowledge_source/Freebase/id2name_parts\"\n\tname2id_dict_orig, id2name_dict_orig = load_nameid_dict(name_dir, lower=False)\n\tlogging.info(\"parse generation results to answers\")\n\tsave_file_path_all = Prefix_to_id_all(result_path, name2id_dict)\n\tlogging.info(\"Combine answers from SP and QA\")\n\tsave_file_path_id_final = answer_ensemble(save_file_path_all)\n\tif args.dataset == \"WebQSP\":\n\t    logging.info(\"Evaluation Results on WebQSP:\")\n\t    save_file_path_id_offical = to_webqsp_format(save_file_path_id_final)\n", "    eval_code = os.path.join(DATA_DIR, \"tasks/QA/WebQSP/raw/eval/eval.py\")\n\t    gold_file = os.path.join(DATA_DIR, \"tasks/QA/WebQSP/raw/data/WebQSP.test.json\")\n\t    os.system(f\"python2 {eval_code} {gold_file} {save_file_path_id_offical}\")\n\telif args.dataset == \"GrailQA\":\n\t    logging.info(\"Evaluation Results on GrailQA:\")\n\t    save_file_path_id_offical = to_grailqa_format(save_file_path_id_final)\n\t    gold_file = os.path.join(DATA_DIR, f\"tasks/QA/GrailQA/raw/grailqa_v1.0_dev.json\")\n\t    os.system(f\"python GrailQA/evaluate.py {gold_file} {save_file_path_id_offical} --fb_roles SP_tools/ontology/fb_roles --fb_types SP_tools/ontology/fb_types --reverse_properties SP_tools/ontology/reverse_properties\")\n\telif args.dataset == \"CWQ\":\n\t    logging.info(\"Evaluation Results on CWQ:\")\n", "    save_file_path_name_final = id_to_name(save_file_path_id_final, id2name_dict_orig)\n\t    save_file_path_name_official = to_cwq_format(save_file_path_name_final)\n\t    gold_file = os.path.join(DATA_DIR, f\"tasks/QA/CWQ/raw/ComplexWebQuestions_test.json\")\n\t    os.system(f\"python CWQ/eval_script.py {gold_file} {save_file_path_name_official}\")\n"]}
{"filename": "DecAF/Datasets/QA/utils.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t#\n\t# Licensed to the Apache Software Foundation (ASF) under one\n\t# or more contributor license agreements.  See the NOTICE file\n\t# distributed with this work for additional information\n\t# regarding copyright ownership.  The ASF licenses this file\n\t# to you under the Apache License, Version 2.0 (the\n\t# \"License\"); you may not use this file except in compliance\n\t# with the License.  You may obtain a copy of the License at\n\t#\n", "#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing,\n\t# software distributed under the License is distributed on an\n\t# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\t# KIND, either express or implied.  See the License for the\n\t# specific language governing permissions and limitations\n\t# under the License.\n\timport random\n\timport json\n", "import itertools\n\tfrom collections import defaultdict\n\tfrom DecAF.Knowledge.linearize import get_raw_name\n\tfrom collections import OrderedDict\n\tfrom DecAF.Datasets.QA.SP_tools.executor.sparql_executor import get_label, execute_query\n\tfrom DecAF.Datasets.QA.SP_tools.executor.logic_form_util import lisp_to_sparql\n\timport re\n\tfrom tqdm import tqdm\n\timport time as time\n\timport signal\n", "#Close session\n\tdef handler(signum, frame):\n\t    print(\"SPARQL Executation Time out!\")\n\t    raise Exception('Action took too much time')\n\tdef denorm_final(expr, entity_label_map):\n\t    expr = expr.replace('^^http', 'http')\n\t    expr = expr.replace('http', '^^http')\n\t    expr = expr.replace('( ', '(').replace(' )', ')')\n\t    entities = re.findall(r'\\[ (.*?) \\]', expr)\n\t    expr_list = [expr]\n", "    for e in entities:\n\t        # delete the beginning and end space of e\n\t        orig_e = f\"[ {e} ]\"\n\t        new_expr_list = []\n\t        name_e = e\n\t        if name_e in entity_label_map:\n\t            for expr_i in expr_list:\n\t                for id_map in entity_label_map[name_e]:\n\t                    new_expr_list.append(expr_i.replace(orig_e, id_map))\n\t        expr_list = new_expr_list\n", "    expr_list = expr_list[:10]\n\t    return expr_list\n\tdef revise_only_name(expr, entity_label_map):\n\t    expr = expr.replace('(', ' ( ')\n\t    expr = expr.replace(')', ' ) ')\n\t    toks = expr.split(' ')\n\t    toks = [x for x in toks if len(x)]\n\t    norm_toks = []\n\t    for t in toks:\n\t        # normalize entity\n", "        if t.startswith('m.') or t.startswith('g.'):\n\t            if t in entity_label_map:\n\t                t = entity_label_map[t]\n\t            else:\n\t                name = get_label(t)\n\t                if name is not None:\n\t                    entity_label_map[t] = name\n\t                    t = name\n\t            t = \"[ \" + t + \" ]\"\n\t        # normalize type\n", "        norm_toks.append(t)\n\t    return ' '.join(norm_toks)\n\tdef is_valid(answer):\n\t    if answer is not None and answer != '':\n\t        return True\n\t    else:\n\t        return False\n\tdef parse_answer(raw_ans_list, original_name=False):\n\t    answers = [ans['text'] if ans['text'] is not None else ans['freebaseId'] for ans in raw_ans_list]\n\t    if not answers:\n", "        answers = [\"None\"]\n\t    if original_name:\n\t        answers = [get_raw_name(ans) for ans in answers]\n\t    return answers\n\tdef parse_answer_id(raw_ans_list):\n\t    answers = [ans['freebaseId'] for ans in raw_ans_list]\n\t    return answers\n\tdef compute_f1(pred_set, gold_set):\n\t    pred_set = set(pred_set)\n\t    gold_set = set(gold_set)\n", "    precision = len(pred_set & gold_set) / (len(pred_set) + 1e-8)\n\t    recall = len(pred_set & gold_set) / (len(gold_set) + 1e-8)\n\t    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n\t    return f1\n\tdef compute_hits1_dict(pred_dict, gold_dict):\n\t    score = 0\n\t    for key in gold_dict:\n\t        if key in pred_dict:\n\t            if pred_dict[key][0] in gold_dict[key]:\n\t                score += 1\n", "    return score / len(gold_dict)\n\t# permute the list to perform augmentation\n\t# [A, B, C] -> [\"A | B | C\", \"B | A | C\", \"C | A | B\", ...]\n\tdef aug_list_old(org_list, augmentation_num=10):\n\t    all_orders = []\n\t    for i, answer_order in enumerate(itertools.permutations(org_list)):\n\t        all_orders.append(answer_order)\n\t        if i == augmentation_num - 1:\n\t            break\n\t    return [\" | \".join(answers_i) for answers_i in all_orders]\n", "def aug_list(org_list):\n\t    augmentation_num = min( max(1, len(org_list) * (len(org_list) - 1)) , 10)\n\t    all_orders = []\n\t    for _ in range(augmentation_num):\n\t        all_orders.append(random.sample(org_list, len(org_list)))\n\t    return [\" | \".join(answers_i) for answers_i in all_orders]\n\t# transform a freebase id to a name\n\tdef id2name(m_id, id2name_dict):\n\t    if m_id in id2name_dict:\n\t        return id2name_dict[m_id]\n", "    else:\n\t        try:\n\t            name = get_label(m_id)\n\t            if name is not None:\n\t                return name\n\t            else:\n\t                return m_id\n\t        except:\n\t            return m_id\n\tdef oracle_rerank(raw_answer_dict, answer_true):\n", "    '''\n\t    choose the answer with the highest F1 score with the true answer\n\t    '''\n\t    new_answer_dict = {\n\t        \"SP\": [each[\"answer\"] for each in raw_answer_dict[\"SP\"]],\n\t        \"QA\": [each[\"answer\"] for each in raw_answer_dict[\"QA\"]]\n\t    }\n\t    if len(new_answer_dict[\"SP\"]) == 0:\n\t        if len(new_answer_dict[\"QA\"]) > 0:\n\t            return new_answer_dict[\"QA\"][0], None\n", "        else:\n\t            return [\"None\"], None\n\t    if len(new_answer_dict[\"QA\"]) == 0:\n\t        return new_answer_dict[\"SP\"][0], raw_answer_dict[\"SP\"][0][\"logical_form\"]\n\t    # compute the F1 score for top answer\n\t    top_answer_sp = new_answer_dict[\"SP\"][0]\n\t    top_f1_sp = compute_f1(set(top_answer_sp), set(answer_true))\n\t    top_answer_qa = new_answer_dict[\"QA\"][0]\n\t    top_f1_qa = compute_f1(set(top_answer_qa), set(answer_true))\n\t    if top_f1_sp >= top_f1_qa:\n", "        return top_answer_sp, raw_answer_dict[\"SP\"][0][\"logical_form\"]\n\t    else:\n\t        return top_answer_qa, None\n\tdef score_rerank(raw_answer_dict, sp_weight=1.0):\n\t    '''\n\t    SP: [[A, B, C], [B, C]], QA: [[A, B], [B, C]]\n\t    '''\n\t    new_answer_dict = {\n\t        \"SP\": [each[\"answer\"] for each in raw_answer_dict[\"SP\"]],\n\t        \"QA\": [each[\"answer\"] for each in raw_answer_dict[\"QA\"]]\n", "    }\n\t    answer2logic = {}\n\t    for each in raw_answer_dict[\"SP\"]:\n\t        if tuple(each[\"answer\"]) not in answer2logic:\n\t            answer2logic[tuple(each[\"answer\"])] = each[\"logical_form\"]\n\t    if len(new_answer_dict[\"QA\"]) == 0:\n\t        new_answer_dict[\"QA\"] = [[\"None\"]]\n\t    if len(new_answer_dict[\"SP\"]) == 0:\n\t        if len(new_answer_dict[\"QA\"]) > 0:\n\t            return new_answer_dict[\"QA\"][0], None\n", "        else:\n\t            return [\"None\"], None\n\t    score_dict = {}\n\t    for i in range(len(new_answer_dict[\"SP\"])):\n\t        score_dict[\"SP{}\".format(i)] = 1/(1+i) * sp_weight\n\t        # score_dict[\"SP{}\".format(i)] = (len(new_answer_dict[\"QA\"]) - i) * sp_weight\n\t    for i in range(len(new_answer_dict[\"QA\"])):\n\t        score_dict[\"QA{}\".format(i)] = 1/(1+i) * (1-sp_weight)\n\t        # score_dict[\"QA{}\".format(i)] = (len(new_answer_dict[\"QA\"]) - i) * (1-sp_weight)\n\t    answer2score_dict = {}\n", "    for i, sp_i in enumerate(new_answer_dict[\"SP\"]):\n\t        if tuple(sp_i) in answer2score_dict:\n\t            continue\n\t        hits = False\n\t        for j, qa_j in enumerate(new_answer_dict[\"QA\"]):\n\t            if set(sp_i) == set(qa_j):\n\t                answer2score_dict[tuple(sp_i)] = score_dict[\"SP{}\".format(i)]\n\t                answer2score_dict[tuple(sp_i)] += score_dict[\"QA{}\".format(j)]\n\t                hits = True\n\t                break\n", "        if not hits:\n\t            answer2score_dict[tuple(sp_i)] = score_dict[\"SP{}\".format(i)]\n\t    for j, qa_j in enumerate(new_answer_dict[\"QA\"]):\n\t        if tuple(qa_j) not in answer2score_dict:\n\t            answer2score_dict[tuple(qa_j)] = score_dict[\"QA{}\".format(j)]    \n\t    # sort the dict based on value\n\t    sorted_score_dict = sorted(answer2score_dict.items(), key=lambda x: x[1], reverse=True)\n\t    top_answer = sorted_score_dict[0][0]\n\t    if top_answer in answer2logic:\n\t        return list(top_answer), answer2logic[top_answer]\n", "    else:\n\t        return list(top_answer), None\n\tdef Prefix_to_id_all(in_file_path, name2id_dict, SP_early_break=True):\n\t    with open(in_file_path, \"r\") as f:\n\t        data = json.load(f)\n\t    print(len(data))\n\t    print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    new_data = {}\n\t    for key in data:\n\t        task = key.split(\":\")[-1]\n", "        q_key = \":\".join(key.split(\":\")[:-1])\n\t        if q_key not in new_data:\n\t            new_data[q_key] = {\n\t                \"question\": data[key][\"question\"],\n\t                \"gold answers\": data[key][\"gold answers\"],\n\t                \"predicted answers\": {\n\t                    task: data[key][\"predicted answers\"]\n\t                }\n\t            }\n\t        else:\n", "            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n\t    print(len(new_data))\n\t    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\t    result_dict = {}\n\t    for i, key in tqdm(enumerate(new_data)):\n\t        data_i = new_data[key]\n\t        answers = data_i['predicted answers']\n\t        new_answer_total = {\"SP\": [], \"QA\": []}\n\t        SP_answers = answers['SP']\n\t        find_answer = False\n", "        for sp_i, answer in enumerate(SP_answers):\n\t            for result in denorm_final(answer, name2id_dict):\n\t                new_answer = execute_vanilla_s_expr(result)\n\t                if len(new_answer) > 0:\n\t                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n\t                    find_answer = True\n\t                    break\n\t            if find_answer and SP_early_break:\n\t                break\n\t        QA_answers = answers['QA']\n", "        for qa_i, answer in enumerate(QA_answers):\n\t            if \" | \" in answer:\n\t                # multiple answers\n\t                total_answers = []\n\t                answer_split = answer.split(\" | \")\n\t                for each in answer_split:\n\t                    if each in name2id_dict:\n\t                        total_answers += name2id_dict[each]\n\t                if len(total_answers) > 0:\n\t                    # deduplication but keep original order\n", "                    total_answers = list(OrderedDict.fromkeys(total_answers))\n\t                    new_answer = total_answers\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t            else:\n\t                if answer in name2id_dict:\n\t                    new_answer = name2id_dict[answer]\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t        result_dict[key] = {\n\t                \"question\": new_data[key][\"question\"],\n\t                \"gold answers\": new_data[key][\"gold answers\"],\n", "                \"predicted answers\": new_answer_total,\n\t            }\n\t    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n\t    print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(result_dict, wf, indent=2)\n\t    return save_file\n\tdef SP_to_id_all(in_file_path, name2id_dict):\n\t    with open(in_file_path, \"r\") as f:\n\t        data = json.load(f)\n", "    print(len(data))\n\t    print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    new_data = {}\n\t    for key in data:\n\t        task = \"SP\"\n\t        q_key = key\n\t        if q_key not in new_data:\n\t            new_data[q_key] = {\n\t                \"question\": data[key][\"question\"],\n\t                \"gold answers\": data[key][\"gold answers\"],\n", "                \"predicted answers\": {\n\t                    task: data[key][\"predicted answers\"]\n\t                }\n\t            }\n\t        else:\n\t            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n\t    print(len(new_data))\n\t    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\t    result_dict = {}\n\t    for i, key in tqdm(enumerate(new_data)):\n", "        data_i = new_data[key]\n\t        answers = data_i['predicted answers']\n\t        new_answer_total = {\"SP\": [], \"QA\": []}\n\t        SP_answers = answers['SP']\n\t        find_answer = False\n\t        for sp_i, answer in enumerate(SP_answers):\n\t            for result in denorm_final(answer, name2id_dict):\n\t                new_answer = execute_vanilla_s_expr(result)\n\t                if len(new_answer) > 0:\n\t                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n", "                    find_answer = True\n\t                    break\n\t            if find_answer:\n\t                break\n\t        result_dict[key] = {\n\t                \"question\": new_data[key][\"question\"],\n\t                \"gold answers\": new_data[key][\"gold answers\"],\n\t                \"predicted answers\": new_answer_total,\n\t            }\n\t    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n", "    print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(result_dict, wf, indent=2)\n\t    return save_file\n\tdef QA_to_id_all(in_file_path, name2id_dict):\n\t    with open(in_file_path, \"r\") as f:\n\t        data = json.load(f)\n\t    print(len(data))\n\t    print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    new_data = {}\n", "    for key in data:\n\t        task = \"QA\"\n\t        q_key = key\n\t        if q_key not in new_data:\n\t            new_data[q_key] = {\n\t                \"question\": data[key][\"question\"],\n\t                \"gold answers\": data[key][\"gold answers\"],\n\t                \"predicted answers\": {\n\t                    task: data[key][\"predicted answers\"]\n\t                }\n", "            }\n\t        else:\n\t            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n\t    print(len(new_data))\n\t    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\t    result_dict = {}\n\t    for i, key in tqdm(enumerate(new_data)):\n\t        data_i = new_data[key]\n\t        answers = data_i['predicted answers']\n\t        new_answer_total = {\"SP\": [], \"QA\": []}\n", "        QA_answers = answers['QA']\n\t        for qa_i, answer in enumerate(QA_answers):\n\t            if \" | \" in answer:\n\t                # multiple answers\n\t                total_answers = []\n\t                answer_split = answer.split(\" | \")\n\t                for each in answer_split:\n\t                    if each in name2id_dict:\n\t                        total_answers += name2id_dict[each]\n\t                if len(total_answers) > 0:\n", "                    # deduplication but keep original order\n\t                    total_answers = list(OrderedDict.fromkeys(total_answers))\n\t                    new_answer = total_answers\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t            else:\n\t                if answer in name2id_dict:\n\t                    new_answer = name2id_dict[answer]\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t        result_dict[key] = {\n\t                \"question\": new_data[key][\"question\"],\n", "                \"gold answers\": new_data[key][\"gold answers\"],\n\t                \"predicted answers\": new_answer_total,\n\t            }\n\t    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n\t    print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(result_dict, wf, indent=2)\n\t    return save_file\n\tdef QA_to_name_all(in_file_path):\n\t    with open(in_file_path, \"r\") as f:\n", "        data = json.load(f)\n\t    print(len(data))\n\t    print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    new_data = {}\n\t    for key in data:\n\t        task = \"QA\"\n\t        q_key = key\n\t        if q_key not in new_data:\n\t            new_data[q_key] = {\n\t                \"question\": data[key][\"question\"],\n", "                \"gold answers\": data[key][\"gold answers\"],\n\t                \"predicted answers\": {\n\t                    task: data[key][\"predicted answers\"]\n\t                }\n\t            }\n\t        else:\n\t            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n\t    print(len(new_data))\n\t    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\t    result_dict = {}\n", "    for i, key in tqdm(enumerate(new_data)):\n\t        data_i = new_data[key]\n\t        answers = data_i['predicted answers']\n\t        new_answer_total = {\"SP\": [], \"QA\": []}\n\t        QA_answers = answers['QA']\n\t        for qa_i, answer in enumerate(QA_answers):\n\t            if \" | \" in answer:\n\t                # multiple answers\n\t                total_answers = []\n\t                answer_split = answer.split(\" | \")\n", "                for each in answer_split:\n\t                    total_answers += [get_raw_name(each)]\n\t                if len(total_answers) > 0:\n\t                    # deduplication but keep original order\n\t                    total_answers = list(OrderedDict.fromkeys(total_answers))\n\t                    new_answer = total_answers\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t            else:\n\t                new_answer = [get_raw_name(answer)]\n\t                new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n", "        result_dict[key] = {\n\t                \"question\": new_data[key][\"question\"],\n\t                \"gold answers\": new_data[key][\"gold answers\"],\n\t                \"predicted answers\": new_answer_total,\n\t            }\n\t    save_file = in_file_path.replace(\".json\", \"_name_all.json\")\n\t    print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(result_dict, wf, indent=2)\n\t    return save_file\n", "def Combine_QA_SP(in_file_path_qa, in_file_path_sp):\n\t    with open(in_file_path_qa, \"r\") as f:\n\t        data_qa = json.load(f)\n\t    print(len(data_qa))\n\t    print(list(data_qa.keys())[0], data_qa[list(data_qa.keys())[0]])\n\t    with open(in_file_path_sp, \"r\") as f:\n\t        data_sp = json.load(f)\n\t    print(len(data_sp))\n\t    print(list(data_sp.keys())[0], data_sp[list(data_sp.keys())[0]])\n\t    assert len(data_sp) == len(data_qa)\n", "    new_data = {}\n\t    for key in data_qa:\n\t        new_data[key + \":QA\"] = data_qa[key]\n\t    for key in data_sp:\n\t        new_data[key + \":SP\"] = data_sp[key]\n\t    print(len(new_data))\n\t    save_file = in_file_path_qa.replace(\".json\", \"_combine.json\")\n\t    print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(new_data, wf, indent=2)\n", "    return save_file\n\tdef Prefix_to_name_all(in_file_path, name2id_dict, id2name_dict):\n\t    with open(in_file_path, \"r\") as f:\n\t        data = json.load(f)\n\t    print(len(data))\n\t    print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    new_data = {}\n\t    for key in data:\n\t        task = key.split(\":\")[-1]\n\t        q_key = \":\".join(key.split(\":\")[:-1])\n", "        if q_key not in new_data:\n\t            new_data[q_key] = {\n\t                \"question\": data[key][\"question\"],\n\t                \"gold answers\": data[key][\"gold answers\"],\n\t                \"predicted answers\": {\n\t                    task: data[key][\"predicted answers\"]\n\t                }\n\t            }\n\t        else:\n\t            new_data[q_key][\"predicted answers\"][task] = data[key][\"predicted answers\"]\n", "    print(len(new_data))\n\t    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\t    result_dict = {}\n\t    for i, key in tqdm(enumerate(new_data)):\n\t        data_i = new_data[key]\n\t        answers = data_i['predicted answers']\n\t        new_answer_total = {\"SP\": [], \"QA\": []}\n\t        SP_answers = answers['SP']\n\t        find_answer = False\n\t        for sp_i, answer in enumerate(SP_answers):\n", "            for result in denorm_final(answer, name2id_dict):\n\t                new_answer = execute_vanilla_s_expr(result)\n\t                if len(new_answer) > 0:\n\t                    new_answer = [id2name(answer_i, id2name_dict) for answer_i in new_answer]\n\t                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n\t                    find_answer = True\n\t                    break\n\t            if find_answer:\n\t                break\n\t        QA_answers = answers['QA']\n", "        for qa_i, answer in enumerate(QA_answers):\n\t            if \" | \" in answer:\n\t                # multiple answers\n\t                total_answers = []\n\t                answer_split = answer.split(\" | \")\n\t                for each in answer_split:\n\t                    # if each in name2id_dict:\n\t                    total_answers += [get_raw_name(each)]\n\t                if len(total_answers) > 0:\n\t                    # deduplication but keep original order\n", "                    total_answers = list(OrderedDict.fromkeys(total_answers))\n\t                    new_answer = total_answers\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t            else:\n\t                # if answer in name2id_dict:\n\t                new_answer = [get_raw_name(answer)]\n\t                new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t        result_dict[key] = {\n\t                \"question\": new_data[key][\"question\"],\n\t                \"gold answers\": new_data[key][\"gold answers\"],\n", "                \"predicted answers\": new_answer_total,\n\t            }\n\t    save_file = in_file_path.replace(\".json\", \"_name_all.json\")\n\t    print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(result_dict, wf, indent=2)\n\t    return save_file\n\tdef Concat_to_id_all(in_file_path, name2id_dict):\n\t    with open(in_file_path, \"r\") as f:\n\t        data = json.load(f)\n", "    print(len(data))\n\t    print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    new_data = {}\n\t    for key in data:\n\t        sp_answers, qa_answers = [], []\n\t        for each in data[key][\"predicted answers\"]:\n\t            sp_answers.append(each.split(\"expression: \")[-1].split(\" answer: \")[0])\n\t            qa_answers.append(each.split(\"answer: \")[-1])\n\t        new_data[key] = {\n\t            \"question\": data[key][\"question\"],\n", "            \"gold answers\": data[key][\"gold answers\"],\n\t            \"predicted answers\": {\n\t                \"SP\": sp_answers,\n\t                \"QA\": qa_answers\n\t            }\n\t        }\n\t    print(len(new_data))\n\t    print(list(new_data.keys())[0], new_data[list(new_data.keys())[0]])\n\t    result_dict = {}\n\t    for i, key in tqdm(enumerate(new_data)):\n", "        data_i = new_data[key]\n\t        answers = data_i['predicted answers']\n\t        new_answer_total = {\"SP\": [], \"QA\": []}\n\t        SP_answers = answers['SP']\n\t        for sp_i, answer in enumerate(SP_answers):\n\t            for result in denorm_final(answer, name2id_dict):\n\t                new_answer = execute_vanilla_s_expr(result)\n\t                if len(new_answer) > 0:\n\t                    new_answer_total[\"SP\"].append({\"answer\": new_answer, \"logical_form\": result, \"rank\": sp_i})\n\t                    break\n", "        QA_answers = answers['QA']\n\t        for qa_i, answer in enumerate(QA_answers):\n\t            if \" | \" in answer:\n\t                # multiple answers\n\t                total_answers = []\n\t                answer_split = answer.split(\" | \")\n\t                for each in answer_split:\n\t                    if each in name2id_dict:\n\t                        total_answers += name2id_dict[each]\n\t                if len(total_answers) > 0:\n", "                    # deduplication but keep original order\n\t                    total_answers = list(OrderedDict.fromkeys(total_answers))\n\t                    new_answer = total_answers\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t            else:\n\t                if answer in name2id_dict:\n\t                    new_answer = name2id_dict[answer]\n\t                    new_answer_total[\"QA\"].append({\"answer\": new_answer, \"rank\": qa_i})\n\t        result_dict[key] = {\n\t                \"question\": new_data[key][\"question\"],\n", "                \"gold answers\": new_data[key][\"gold answers\"],\n\t                \"predicted answers\": new_answer_total,\n\t            }\n\t    save_file = in_file_path.replace(\".json\", \"_id_all.json\")\n\t    print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(result_dict, wf, indent=2)\n\t    return save_file\n\tdef sort_SP_with_QA(SP_answers, QA_answers):\n\t    # assign scores to QA answers based on rank\n", "    QA_answers_score_dict = defaultdict(float)\n\t    for qa_i, answer in enumerate(QA_answers):\n\t        QA_answers_score_dict[answer] = 1/(1+qa_i)\n\t    # sort SP answers based on QA answers\n\t    SP_answers_sorted = []\n\t    for sp_i, answer in enumerate(SP_answers):\n\t        SP_answers_sorted.append({\"answer\": answer, \"score\": QA_answers_score_dict[answer]})\n\t    SP_answers_sorted = sorted(SP_answers_sorted, key=lambda x: x[\"score\"], reverse=True)\n\t    SP_answers_sorted_answers = [each[\"answer\"] for each in SP_answers_sorted]\n\t    return SP_answers_sorted_answers\n", "def answer_ensemble(in_file_path, mode=\"SP_first\", sp_weight=1.0, gold_answers=None):\n\t    '''\n\t    combine the SP and QA answer to the final answers\n\t    '''\n\t    def get_top_k(answer_list, k=1):\n\t        top_k = min(k, len(answer_list))\n\t        output_answer = []\n\t        for i in range(top_k):\n\t            output_answer += answer_list[i][\"answer\"]\n\t        output_answer = list(OrderedDict.fromkeys(output_answer))\n", "        return output_answer\n\t    assert \"_all.json\" in in_file_path\n\t    with open(in_file_path, \"r\") as f:\n\t        data = json.load(f)\n\t    # print(len(data))\n\t    # print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    num_sp = 0\n\t    for key in data:\n\t        pred_answers = data[key][\"predicted answers\"]\n\t        if gold_answers is not None:\n", "            answer_true = gold_answers[key]\n\t        if mode == \"SP_first\":\n\t            data[key][\"predicted answers\"] = []\n\t            QA_answers = []\n\t            if len(pred_answers[\"QA\"]) > 0:\n\t                # data[key][\"predicted answers\"] = pred_answers[\"QA\"][0][\"answer\"]\n\t                data[key][\"predicted answers\"] = get_top_k(pred_answers[\"QA\"], k=1)\n\t                QA_answers = pred_answers[\"QA\"][0][\"answer\"]\n\t                # QA_answers = [pred_answers[\"QA\"][i][\"answer\"][0] for i in range(len(pred_answers[\"QA\"]))]\n\t            if len(pred_answers[\"SP\"]) > 0:\n", "                # data[key][\"predicted answers\"] = pred_answers[\"SP\"][0][\"answer\"]\n\t                data[key][\"predicted answers\"] = sort_SP_with_QA(pred_answers[\"SP\"][0][\"answer\"], QA_answers)\n\t                data[key][\"logical_form\"] = pred_answers[\"SP\"][0][\"logical_form\"]\n\t                num_sp += 1\n\t        elif mode == \"SP_only\":\n\t            data[key][\"predicted answers\"] = []\n\t            if len(pred_answers[\"SP\"]) > 0:\n\t                data[key][\"predicted answers\"] = pred_answers[\"SP\"][0][\"answer\"]\n\t                data[key][\"logical_form\"] = pred_answers[\"SP\"][0][\"logical_form\"]\n\t                num_sp += 1\n", "        elif mode == \"QA_only\":\n\t            data[key][\"predicted answers\"] = []\n\t            if len(pred_answers[\"QA\"]) > 0:\n\t                # data[key][\"predicted answers\"] = pred_answers[\"QA\"][0][\"answer\"]\n\t                # combine top-k QA answers\n\t                data[key][\"predicted answers\"] = get_top_k(pred_answers[\"QA\"], k=1)\n\t        elif mode == \"Score\":\n\t            data[key][\"predicted answers\"], logical_form = score_rerank(pred_answers, sp_weight)\n\t            if len(pred_answers[\"QA\"]) > 0:\n\t                data[key][\"predicted answers\"] = sort_SP_with_QA(data[key][\"predicted answers\"], pred_answers[\"QA\"][0][\"answer\"])\n", "            if logical_form is not None:\n\t                num_sp += 1\n\t                data[key][\"logical_form\"] = logical_form\n\t        elif mode == \"oracle\":\n\t            data[key][\"predicted answers\"], logical_form = oracle_rerank(pred_answers, answer_true)\n\t            if len(pred_answers[\"QA\"]) > 0:\n\t                data[key][\"predicted answers\"] = sort_SP_with_QA(data[key][\"predicted answers\"], pred_answers[\"QA\"][0][\"answer\"])\n\t            if logical_form is not None:\n\t                num_sp += 1\n\t                data[key][\"logical_form\"] = logical_form\n", "    # print(\"num_sp:\", num_sp)\n\t    save_file = in_file_path.replace(\"_all.json\", \"_final.json\")\n\t    # print(save_file)\n\t    with open(save_file, \"w\") as wf:\n\t        json.dump(data, wf, indent=2)\n\t    return save_file\n\tdef Joint_to_id(in_file_path, name2id_dict):\n\t    with open(in_file_path, \"r\") as f:\n\t        data = json.load(f)\n\t    print(len(data))\n", "    print(list(data.keys())[0], data[list(data.keys())[0]])\n\t    result_dict = {}\n\t    num_qa = 0\n\t    for i, key in tqdm(enumerate(data)):\n\t        data_i = data[key]\n\t        answers = data_i['predicted answers']\n\t        new_answer = []\n\t        find_answer = False\n\t        logical_form = \"None\"\n\t        for answer in answers:\n", "            if \"expression:\" in answer:\n\t                answer = answer.split(\"expression: \")[-1]\n\t                for result in denorm_final(answer, name2id_dict):\n\t                    new_answer = execute_vanilla_s_expr(result)\n\t                    if len(new_answer) > 0:\n\t                        find_answer = True\n\t                        logical_form = result\n\t                        break\n\t                if find_answer:\n\t                    break\n", "            elif \"answer:\" in answer:\n\t                answer = answer.split(\"answer: \")[-1]\n\t                num_qa += 1\n\t                if \" | \" in answer:\n\t                    # multiple answers\n\t                    total_answers = []\n\t                    answer_split = answer.split(\" | \")\n\t                    for each in answer_split:\n\t                        if each in name2id_dict:\n\t                            total_answers += name2id_dict[each]\n", "                    if len(total_answers) > 0:\n\t                        # deduplication but keep original order\n\t                        total_answers = list(OrderedDict.fromkeys(total_answers))\n\t                        new_answer = total_answers\n\t                        break\n\t                else:\n\t                    if answer in name2id_dict:\n\t                        new_answer = name2id_dict[answer]\n\t                        break\n\t        result_dict[key] = {\n", "                \"question\": data[key][\"question\"],\n\t                \"gold answers\": data[key][\"gold answers\"],\n\t                \"predicted answers\": [new_answer],\n\t                \"logical_form\": logical_form,\n\t            }\n\t    print(num_qa)\n\t    save_file_id = in_file_path.replace(\".json\", \"_id.json\")\n\t    print(save_file_id)\n\t    with open(save_file_id, \"w\") as wf:\n\t        json.dump(result_dict, wf, indent=2)\n", "    return save_file_id\n\tdef execute_vanilla_s_expr(s_expr):\n\t    signal.signal(signal.SIGALRM, handler)\n\t    signal.alarm(10) #Set the parameter to the amount of seconds you want to wait\n\t    try:\n\t        # print('parse', query_expr)\n\t        sparql_query = lisp_to_sparql(s_expr)\n\t        # print('sparql', sparql_query)\n\t        # set maximum time as 100 seconds\n\t        denotation = execute_query(sparql_query)\n", "    except:\n\t        denotation = []\n\t    signal.alarm(10) #Resets the alarm to 10 new seconds\n\t    signal.alarm(0) #Disables the alarm\n\t    return denotation\n\tdef denormalize(expr, entity_label_map):\n\t    expr = expr.replace('^^http', 'http')\n\t    expr = expr.replace('http', '^^http')\n\t    expr = expr.replace('( ', '(').replace(' )', ')')\n\t    entities = re.findall(r'\\[ (.*?) \\]', expr)\n", "    expr_list = [expr]\n\t    for e in entities:\n\t        # delete the beginning and end space of e\n\t        orig_e = f\"[ {e} ]\"\n\t        new_expr_list = []\n\t        name_e = e\n\t        if name_e in entity_label_map:\n\t            for expr_i in expr_list:\n\t                for id_map in entity_label_map[name_e]:\n\t                    new_expr_list.append(expr_i.replace(orig_e, id_map))\n", "        expr_list = new_expr_list\n\t    expr_list = expr_list[:10]\n\t    return expr_list\n\tdef SP_to_id(in_file, name2id_dict):\n\t    with open(in_file, 'r') as rf:\n\t        data = json.load(rf)\n\t    print(len(data))\n\t    print(\"Data Example:\")\n\t    print(data[list(data.keys())[0]])\n\t    new_data = {}\n", "    wrong_data = []\n\t    num_no_answers = 0\n\t    for key in tqdm(data):\n\t        logical_form = \"None\"\n\t        answers = []\n\t        for predict_exp in data[key][\"predicted answers\"]:\n\t            predict_exp = denormalize(predict_exp, name2id_dict)\n\t            if isinstance(predict_exp, list):\n\t                for predict_exp_i in predict_exp:\n\t                    predict_answers = execute_vanilla_s_expr(predict_exp_i)\n", "                    if len(predict_answers) != 0:\n\t                        logical_form = predict_exp_i\n\t                        answers = predict_answers\n\t                        break\n\t            else:\n\t                predict_answers = execute_vanilla_s_expr(predict_exp)\n\t                if len(predict_answers) != 0:\n\t                    logical_form = predict_exp\n\t                    answers = predict_answers\n\t            if len(answers) != 0:\n", "                break\n\t        if len(answers) == 0:\n\t            num_no_answers += 1\n\t            data[key].update({\"id\": key})\n\t            wrong_data.append(data[key])\n\t        new_data[key] = {\n\t            \"question\": data[key][\"question\"],\n\t            \"gold answers\": data[key][\"gold answers\"],\n\t            \"logical_form\": logical_form,\n\t            \"predicted answers\": [answers]\n", "        }\n\t    print(\"num_no_answers: \", num_no_answers)\n\t    print(\"Output Example:\")\n\t    print(new_data[list(new_data.keys())[0]])\n\t    out_file = in_file.replace(\".json\", \"_id.json\")\n\t    with open(out_file, 'w') as wf:\n\t        json.dump(new_data, wf, indent=2)\n\t    wrong_file = in_file.replace(\".json\", \"_wrong.json\")\n\t    with open(wrong_file, \"w\") as wf:\n\t        json.dump(wrong_data, wf, indent=2)\n", "    return out_file\n\tdef id_to_name(in_file_path, id2name_dict):\n\t    with open(in_file_path, \"r\") as rf:\n\t        data = json.load(rf)\n\t    # print(\"Data example: \")\n\t    # print(data[list(data.keys())[0]])\n\t    new_data = {}\n\t    for key in tqdm(data):\n\t        new_answer_list = []\n\t        for answer in data[key][\"predicted answers\"]:\n", "            new_answer_list.append(id2name(answer, id2name_dict))\n\t        new_data[key] = {\n\t            \"question\": data[key][\"question\"],\n\t            \"gold answers\": data[key][\"gold answers\"],\n\t            \"predicted answers\": new_answer_list\n\t        }\n\t    # print(\"Output example: \")\n\t    # print(new_data[list(new_data.keys())[0]])\n\t    out_save_path = in_file_path.replace(\".json\", \"_name.json\")\n\t    # print(\"Output File:\", os.path.basename(out_save_path))\n", "    with open(out_save_path, \"w\") as wf:\n\t        json.dump(new_data, wf, indent=2)\n\t    return out_save_path\n\t######## Transform results to official evaluation format ########\n\tdef to_webqsp_format(in_file_path):\n\t    with open(in_file_path, \"r\") as rf:\n\t        data = json.load(rf)\n\t    new_data = []\n\t    for key in data:\n\t        new_data.append({\"QuestionId\": key, \"Answers\": data[key][\"predicted answers\"]})\n", "    # print(\"Output example: \")\n\t    # print(new_data[0])\n\t    out_save_path = in_file_path.replace(\".json\", \"_eval.json\")\n\t    # print(\"Output File:\", os.path.basename(out_save_path))\n\t    with open(out_save_path, \"w\") as wf:\n\t        json.dump(new_data, wf, indent=2)\n\t    return out_save_path\n\tdef to_cwq_format(in_file_path):\n\t    with open(in_file_path, \"r\") as rf:\n\t        data = json.load(rf)\n", "    new_data = []\n\t    for key in data:\n\t        answer = data[key][\"predicted answers\"]\n\t        if len(answer) > 0:\n\t            # randomly pick one as answer\n\t            answer = answer[0]\n\t            # answer = random.sample(answer, 1)[0]\n\t        else:\n\t            answer = \"None\"\n\t        new_data.append({\"ID\": key, \"answer\": answer.lower().strip()})\n", "    # print(\"Output example: \")\n\t    # print(new_data[0])\n\t    out_save_path = in_file_path.replace(\".json\", \"_eval.json\")\n\t    # print(\"Output File:\", os.path.basename(out_save_path))\n\t    with open(out_save_path, \"w\") as wf:\n\t        json.dump(new_data, wf, indent=2)\n\t    return out_save_path\n\tdef to_grailqa_format(in_file_path):\n\t    with open(in_file_path, \"r\") as rf:\n\t        data = json.load(rf)\n", "    new_data = []\n\t    for key in data:\n\t        new_data_i = {\n\t            \"qid\": key,\n\t            \"logical_form\": data[key][\"logical_form\"] if \"logical_form\" in data[key] else \"None\",\n\t            \"answer\": data[key][\"predicted answers\"]\n\t        }\n\t        new_data.append(new_data_i)\n\t    # print(\"Output example: \")\n\t    # print(new_data[0])\n", "    out_save_path = in_file_path.replace(\".json\", \"_eval.json\")\n\t    # print(\"Output File:\", os.path.basename(out_save_path))\n\t    lines = [json.dumps(x) for x in new_data]\n\t    with open(out_save_path, 'w') as wf:\n\t        wf.writelines([x+'\\n' for x in lines])\n\t    return out_save_path"]}
{"filename": "DecAF/Datasets/QA/disambiguate.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t#\n\t# Licensed to the Apache Software Foundation (ASF) under one\n\t# or more contributor license agreements.  See the NOTICE file\n\t# distributed with this work for additional information\n\t# regarding copyright ownership.  The ASF licenses this file\n\t# to you under the Apache License, Version 2.0 (the\n\t# \"License\"); you may not use this file except in compliance\n\t# with the License.  You may obtain a copy of the License at\n\t#\n", "#   http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing,\n\t# software distributed under the License is distributed on an\n\t# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n\t# KIND, either express or implied.  See the License for the\n\t# specific language governing permissions and limitations\n\t# under the License.\n\timport os\n\timport json\n", "from tqdm import tqdm\n\timport argparse\n\tfrom DecAF.Preprocess.linearize import load_nameid_dict\n\timport logging\n\tlogging.basicConfig(level=logging.INFO)\n\tDATA_DIR = os.environ['DATA_DIR']\n\tparser = argparse.ArgumentParser(description='disambiguate dataset answer entities')\n\tparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ')\n\targs = parser.parse_args()\n\tif __name__ == \"__main__\":\n", "    # load id2name mapping\n\t    logging.info(\"Loading id2name mapping\")\n\t    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n\t    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\t    # load original QA dataset\n\t    for split in [\"dev\", \"test\", \"train\"]:\n\t        if f\"{split}.json\" not in os.listdir(args.data_dir):\n\t            continue\n\t        org_file_path = os.path.join(args.data_dir, f\"{split}.json\")\n\t        with open(org_file_path, \"r\") as f:\n", "            org_data = json.load(f)\n\t        new_data = []\n\t        for item in tqdm(org_data):\n\t            new_answers = []\n\t            for answer in item[\"Answers\"]:\n\t                if answer[\"freebaseId\"] in id2name_dict:\n\t                    new_answers.append({\n\t                        \"freebaseId\": answer[\"freebaseId\"],\n\t                        \"text\": id2name_dict[answer[\"freebaseId\"]],\n\t                    })\n", "                else:\n\t                    new_answers.append(answer)\n\t            new_data.append({\n\t                \"QuestionId\": item[\"QuestionId\"],\n\t                \"Question\": item[\"Question\"],\n\t                \"Answers\": new_answers,\n\t            })\n\t        save_file_path = os.path.join(args.data_dir, f\"{split}.json\")\n\t        os.makedirs(os.path.dirname(save_file_path), exist_ok=True)\n\t        with open(save_file_path, \"w\") as f:\n", "            json.dump(new_data, f, indent=2)"]}
{"filename": "DecAF/Datasets/QA/CWQ/parse_sparql.py", "chunked_list": ["# Copyright (c) 2021, salesforce.com, inc. and its affiliates.\n\t# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport pickle\n\timport json\n\timport os\n\timport random\n", "from re import L\n\timport shutil\n\tfrom collections import Counter\n\tfrom DecAF.Datasets.QA.SP_tools.components.utils import *\n\tfrom DecAF.Datasets.QA.SP_tools.components.expr_parser import parse_s_expr, extract_entities, tokenize_s_expr\n\tfrom DecAF.Datasets.QA.SP_tools.executor.sparql_executor import execute_query\n\tfrom DecAF.Datasets.QA.SP_tools.executor.sparql_executor import execute_query\n\tfrom DecAF.Datasets.QA.SP_tools.executor.logic_form_util import lisp_to_sparql\n\timport argparse\n\tDATA_DIR = os.environ['DATA_DIR']\n", "parser = argparse.ArgumentParser(description='generate s-expression')\n\tparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ/raw')\n\targs = parser.parse_args()\n\tclass ParseError(Exception):\n\t    pass\n\tclass Parser:\n\t    def __init__(self):\n\t        pass\n\t    def parse_query(self, query, topic_mid):\n\t        # print('input', query)\n", "        # print(topic_mid)\n\t        lines = query.split('\\n')\n\t        lines = [x for x in lines if x]\n\t        assert lines[0] != '#MANUAL SPARQL'\n\t        prefix_stmts = []        \n\t        line_num = 0\n\t        while True:\n\t            l = lines[line_num]\n\t            if l.startswith('PREFIX'):\n\t                prefix_stmts.append(l)\n", "            else:\n\t                break\n\t            line_num = line_num + 1\n\t        next_line = lines[line_num]\n\t        assert next_line.startswith('SELECT DISTINCT ?x')\n\t        line_num = line_num + 1\n\t        next_line = lines[line_num]\n\t        assert next_line == 'WHERE {'\n\t        assert lines[-1] in ['}', 'LIMIT 1']\n\t        # print(lines)\n", "        lines = lines[line_num :]\n\t        assert all(['FILTER (str' not in x for x in lines])\n\t        # normalize body lines\n\t        body_lines, spec_condition = self.normalize_body_lines(lines)\n\t        # assert all([x.startswith('?') or x.startswith('ns') or x.startswith('FILTER') for x in body_lines])\n\t        # we only parse query following this format\n\t        # print(body_lines)\n\t        if body_lines[0].startswith('FILTER'):\n\t            predefined_filter0 = body_lines[0]\n\t            predefined_filter1 = body_lines[1]\n", "            # assert predefined_filter0 == f'FILTER (?x != ?c)'\n\t            # assert predefined_filter1 == \"FILTER (!isLiteral(?x) OR lang(?x) = '' OR langMatches(lang(?x), 'en'))\"\n\t            # if predefined_filter0 != f'FILTER (?x != ns:{topic_mid})':\n\t            #     print('QUERY', query)\n\t            #     print('First Filter')\n\t            # if predefined_filter1 != \"FILTER (!isLiteral(?x) OR lang(?x) = '' OR langMatches(lang(?x), 'en'))\":\n\t            #     print('QUERY', query)\n\t            #     print('Second Filter')\n\t            # if any([not (x.startswith('?') or x.startswith('ns:')) for x in body_lines]):\n\t            #     print('Unprincipled Filter')\n", "            #     print('QUERY', query)\n\t            body_lines = body_lines[2:]\n\t        # print(body_lines)\n\t        assert all([(x.startswith('?') or x.startswith('ns:')) for x in body_lines])\n\t        var_dep_list = self.parse_naive_body(body_lines, '?x')\n\t        s_expr = self.dep_graph_to_s_expr(var_dep_list, '?x', spec_condition)\n\t        return s_expr\n\t    def normalize_body_lines(self, lines):\n\t        if lines[-1] == 'LIMIT 1':\n\t            # spec_condition = argmax\n", "            # who did jackie robinson first play for?\n\t            # WHERE {\n\t            # ns:m.0443c ns:sports.pro_athlete.teams ?y .\n\t            # ?y ns:sports.sports_team_roster.team ?x .\n\t            # ?y ns:sports.sports_team_roster.from ?sk0 .\n\t            # }\n\t            # ORDER BY DESC(xsd:datetime(?sk0))\n\t            # LIMIT 1\n\t            order_line = lines[-2]\n\t            direction = 'argmax' if 'DESC(' in order_line else 'argmin'\n", "            # assert '?sk0' in\n\t            # print(line)\n\t            assert ('?sk0' in order_line)\n\t            _tmp_body_lines = lines[1:-3]\n\t            body_lines = []\n\t            hit = False\n\t            for l in _tmp_body_lines:\n\t                if '?sk0' in l:\n\t                    self.parse_assert(l.endswith('?sk0 .') and not hit)\n\t                    hit = True\n", "                    arg_var, arg_r = l.split(' ')[0], l.split(' ')[1]\n\t                    arg_r = arg_r[3:] #rm ns:\n\t                else:\n\t                    body_lines.append(l)\n\t            return body_lines, (direction, arg_var, arg_r)\n\t        # check if xxx\n\t        elif lines[-4].startswith('FILTER(NOT EXISTS {?'):\n\t            # WHERE {\n\t            # ns:m.04f_xd8 ns:government.government_office_or_title.office_holders ?y .\n\t            # ?y ns:government.government_position_held.office_holder ?x .\n", "            # FILTER(NOT EXISTS {?y ns:government.government_position_held.from ?sk0} || \n\t            # EXISTS {?y ns:government.government_position_held.from ?sk1 . \n\t            # FILTER(xsd:datetime(?sk1) <= \"2009-12-31\"^^xsd:dateTime) })\n\t            # FILTER(NOT EXISTS {?y ns:government.government_position_held.to ?sk2} || \n\t            # EXISTS {?y ns:government.government_position_held.to ?sk3 . \n\t            # FILTER(xsd:datetime(?sk3) >= \"2009-01-01\"^^xsd:dateTime) })\n\t            # }\n\t            body_lines = lines[1:-7]\n\t            range_lines = lines[-7:-1]\n\t            range_prompt = range_lines[0]\n", "            range_prompt = range_prompt[range_prompt.index('{') + 1:range_prompt.index('}')]\n\t            range_var = range_prompt.split(' ')[0]\n\t            range_relation = range_prompt.split(' ')[1]\n\t            range_relation = '.'.join(range_relation.split('.')[:2]) + '.time_macro'\n\t            range_relation = range_relation[3:] # rm ns:\n\t            range_start = range_lines[2].split(' ')[2]\n\t            range_start = range_start[1:]\n\t            range_start = range_start[:range_start.index('\"')]\n\t            range_end = range_lines[5].split(' ')[2]\n\t            range_end = range_end[1:]\n", "            range_end = range_end[:range_end.index('\"')]\n\t            assert range_start[:4] == range_end[:4]\n\t            range_year = range_start[:4] + '^^http://www.w3.org/2001/XMLSchema#date' #to fit parsable\n\t            return body_lines, ('range', range_var, range_relation, range_year)\n\t        else:\n\t            body_lines = lines[1:-1]\n\t            return body_lines, None        \n\t    def dep_graph_to_s_expr(self, var_dep_list, ret_var, spec_condition=None):\n\t        self.parse_assert(var_dep_list[0][0] == ret_var)\n\t        var_dep_list.reverse()\n", "        parsed_dict = {}\n\t        spec_var = spec_condition[1] if spec_condition is not None else None\n\t        for var_name, dep_relations in var_dep_list:\n\t            # expr = ''\n\t            dep_relations[0]\n\t            clause = self.triplet_to_clause(var_name,  dep_relations[0], parsed_dict)\n\t            for tri in dep_relations[1:]:\n\t                n_clause = self.triplet_to_clause(var_name, tri, parsed_dict)\n\t                clause = 'AND ({}) ({})'.format(n_clause, clause)\n\t            if var_name == spec_var:\n", "                if  spec_condition[0] == 'argmax' or spec_condition[0] == 'argmin':\n\t                    relation = spec_condition[2]\n\t                    clause = '{} ({}) {}'.format(spec_condition[0].upper(), clause, relation)\n\t                elif spec_condition[0] == 'range':\n\t                    relation, time_point = spec_condition[2], spec_condition[3]\n\t                    n_clause = 'JOIN {} {}'.format(relation, time_point)\n\t                    clause = 'AND ({}) ({})'.format(n_clause, clause)\n\t            parsed_dict[var_name] = clause\n\t        return '(' + parsed_dict[ret_var] + ')'\n\t    def triplet_to_clause(self, tgt_var, triplet, parsed_dict):\n", "        if triplet[0] == tgt_var:\n\t            this = triplet[0]\n\t            other = triplet[-1]\n\t            if other in parsed_dict:\n\t                other = '(' + parsed_dict[other] + ')'\n\t            return 'JOIN {} {}'.format(triplet[1], other)\n\t        elif triplet[-1] == tgt_var:\n\t            this = triplet[-1]\n\t            other = triplet[0]\n\t            if other in parsed_dict:\n", "                other = '(' + parsed_dict[other] + ')'\n\t            return 'JOIN (R {}) {}'.format(triplet[1], other)\n\t        else:\n\t            raise ParseError()\n\t    def parse_assert(self, eval):\n\t        if not eval:\n\t            raise ParseError()\n\t    def parse_naive_body(self, body_lines, ret_var):\n\t        # ret_variable\n\t        # body_lines\n", "        body_lines = [x.strip() for x in body_lines]\n\t        assert all([x[-1] == '.' for x in body_lines])\n\t        triplets = [x.split(' ') for x in body_lines]\n\t        triplets = [x[:-1] for x in triplets]\n\t        # print(\"triplets: \", triplets)\n\t        # remove ns \n\t        triplets = [[x[3:] if x.startswith('ns:') else x for x in tri] for tri in triplets]\n\t        # dependancy graph\n\t        triplets_pool = triplets\n\t        # while True:\n", "        var_dep_list = []\n\t        successors = []\n\t        dep_triplets, triplets_pool = self.resolve_dependancy(triplets_pool, ret_var, successors)\n\t        var_dep_list.append((ret_var, dep_triplets))        \n\t        # vars_pool = []\n\t        # go over un resolved vars\n\t        # for tri in triplets_pool:\n\t        #     if tri[0].startswith('?') and tri[0] not in vars_pool and tri[0] != ret_var:\n\t        #         vars_pool.append(tri[0])\n\t        #     if tri[-1].startswith('?') and tri[-1] not in vars_pool and tri[-1] != ret_var:\n", "        #         vars_pool.append(tri[-1])\n\t        # for tgt_var in vars_pool:\n\t        #     dep_triplets, triplets_pool = self.resolve_dependancy(triplets_pool, tgt_var)\n\t        #     self.parse_assert(len(dep_triplets) > 0)\n\t        #     var_dep_list.append((tgt_var, dep_triplets))\n\t        while len(successors):\n\t            tgt_var = successors[0]\n\t            successors = successors[1:]\n\t            dep_triplets, triplets_pool = self.resolve_dependancy(triplets_pool, tgt_var, successors)\n\t            self.parse_assert(len(dep_triplets) > 0)\n", "            var_dep_list.append((tgt_var, dep_triplets))\n\t        self.parse_assert(len(triplets_pool) == 0)\n\t        return var_dep_list\n\t    def resolve_dependancy(self, triplets, target_var, successors):\n\t        dep = []\n\t        left = []\n\t        for tri in triplets:\n\t            if tri[0] == target_var:\n\t                dep.append(tri)\n\t                if tri[-1].startswith('?') and tri[-1] not in successors:\n", "                    successors.append(tri[-1])\n\t            elif tri[-1] == target_var:\n\t                dep.append(tri)\n\t                if tri[0].startswith('?') and tri[0] not in successors:\n\t                    successors.append(tri[0])\n\t            else:\n\t                left.append(tri)\n\t        return dep, left\n\tclass SparqlParse:\n\t    def __init__(self):\n", "        select_stmt = None\n\t        prefix_stmts = None\n\t        where_stmts = None\n\t        query_stmts = None\n\tdef get_mid_from_sparql(sparql):\n\t    for each in sparql.split(' '):\n\t        if each.startswith('ns:m.'):\n\t            return each[3:]\n\tdef convert_parse_instance(parse):\n\t    sparql = parse['sparql']\n", "    # print(parse.keys())\n\t    # print(parse['PotentialTopicEntityMention'])\n\t    # print(parse['TopicEntityMid'], parse['TopicEntityName'])\n\t    try:\n\t        s_expr = parser.parse_query(sparql, get_mid_from_sparql(sparql))\n\t        # print('---GOOD------')\n\t        # print(sparql)\n\t        # print(s_expr)\n\t    except AssertionError:\n\t        s_expr = 'null'\n", "    # print(parse[''])\n\t    parse['SExpr'] = s_expr\n\t    return parse, s_expr != 'null'\n\t# def s_expr_to_sparql(sparql_query):\n\t#     print(sparql_query)\n\tdef webq_s_expr_to_sparql_query(s_expr):\n\t    ast = parse_s_expr(s_expr)\n\tdef execute_webq_s_expr(s_expr):\n\t    try:\n\t        sparql_query = lisp_to_sparql(s_expr)\n", "        denotation = execute_query(sparql_query)\n\t    except:\n\t        denotation = []\n\t    return denotation\n\tdef augment_with_s_expr(split):\n\t    dataset = load_json(args.input_dir + f'/ComplexWebQuestions_{split}.json')\n\t    total_num = 0\n\t    hit_num = 0\n\t    new_dataset = []\n\t    for data in dataset:\n", "        total_num += 1\n\t        instance, flag_success = convert_parse_instance(data)\n\t        if flag_success:\n\t            hit_num += 1\n\t        new_dataset.append(instance)\n\t    print(hit_num, total_num, hit_num/total_num, len(new_dataset))\n\t    dump_json(new_dataset, args.input_dir + f'/ComplexWebQuestions_{split}.expr.json', indent=2)\n\tdef find_macro_template_from_query(query, topic_mid):\n\t    # print('QUERY', query)\n\t    lines = query.split('\\n')\n", "    lines = [x for x in lines if x]\n\t    assert lines[0] != '#MANUAL SPARQL'\n\t    prefix_stmts = []        \n\t    line_num = 0\n\t    while True:\n\t        l = lines[line_num]\n\t        if l.startswith('PREFIX'):\n\t            prefix_stmts.append(l)\n\t        else:\n\t            break\n", "        line_num = line_num + 1\n\t    next_line = lines[line_num]\n\t    assert next_line.startswith('SELECT DISTINCT ?x')\n\t    line_num = line_num + 1\n\t    next_line = lines[line_num]\n\t    assert next_line == 'WHERE {'\n\t    assert lines[-1] in ['}', 'LIMIT 1']\n\t    lines = lines[line_num :]\n\t    assert all(['FILTER (str' not in x for x in lines])\n\t    # normalize body lines\n", "    # return_val = check_time_macro_from_body_lines(lines)\n\t    # if return_val:\n\t    # relation_prefix, suffix_pair = c\n\t    return check_time_macro_from_body_lines(lines)\n\tdef check_time_macro_from_body_lines(lines):\n\t    # check if xxx\n\t    if lines[-4].startswith('FILTER(NOT EXISTS {?'):\n\t        # WHERE {\n\t        # ns:m.04f_xd8 ns:government.government_office_or_title.office_holders ?y .\n\t        # ?y ns:government.government_position_held.office_holder ?x .\n", "        # FILTER(NOT EXISTS {?y ns:government.government_position_held.from ?sk0} || \n\t        # EXISTS {?y ns:government.government_position_held.from ?sk1 . \n\t        # FILTER(xsd:datetime(?sk1) <= \"2009-12-31\"^^xsd:dateTime) })\n\t        # FILTER(NOT EXISTS {?y ns:government.government_position_held.to ?sk2} || \n\t        # EXISTS {?y ns:government.government_position_held.to ?sk3 . \n\t        # FILTER(xsd:datetime(?sk3) >= \"2009-01-01\"^^xsd:dateTime) })\n\t        # }\n\t        body_lines = lines[1:-7]\n\t        range_lines = lines[-7:-1]\n\t        range_prompt_start = range_lines[0]\n", "        range_prompt_start = range_prompt_start[range_prompt_start.index('{') + 1:range_prompt_start.index('}')]\n\t        range_relation_start = range_prompt_start.split(' ')[1]\n\t        # range_relation = '.'.join(range_relation.split('.')[:2]) + '.time_macro'\n\t        # range_relation = range_relation[3:] # rm ns:\n\t        range_prompt_end = range_lines[3]\n\t        range_prompt_end = range_prompt_end[range_prompt_end.index('{') + 1:range_prompt_end.index('}')]\n\t        range_relation_end = range_prompt_end.split(' ')[1]\n\t        assert range_relation_start.split('.')[:2] == range_relation_end.split('.')[:2]\n\t        start_suffix = range_relation_start.split('.')[-1]\n\t        end_suffix = range_relation_end.split('.')[-1]\n", "        prefix = '.'.join(range_relation_start.split('.')[:2])[3:]\n\t        return prefix, start_suffix, end_suffix\n\t    else:\n\t        return None\n\tdef extract_macro_template_from_instance(parse):\n\t    sparql = parse['Sparql']\n\t    # print(parse.keys())\n\t    # print(parse['PotentialTopicEntityMention'])\n\t    # print(parse['TopicEntityMid'], parse['TopicEntityName'])\n\t    try:\n", "        return find_macro_template_from_query(sparql, parse['TopicEntityMid'])\n\t    except AssertionError:\n\t        return None\n\ttrain_templates = [('american_football.football_historical_coach_position', 'from', 'to'), ('architecture.ownership', 'start_date', 'end_date'),\n\t ('award.award_honor', 'year', 'year'), ('business.employment_tenure', 'from', 'to'), ('business.sponsorship', 'from', 'to'),\n\t ('celebrities.romantic_relationship', 'start_date', 'end_date'), ('chemistry.chemical_element', 'discovery_date', 'discovery_date'),\n\t ('film.film', 'initial_release_date', 'initial_release_date'),('government.government_position_held', 'from', 'to'),\n\t ('law.invention', 'date_of_invention', 'date_of_invention'), ('law.judicial_tenure', 'from_date', 'to_date'),\n\t ('organization.organization_relationship', 'to', 'from'), ('people.marriage', 'from', 'to'),\n\t ('people.place_lived', 'end_date', 'start_date'), ('sports.sports_team_coach_tenure', 'from', 'to'),\n", " ('sports.sports_team_roster', 'from', 'to'), ('sports.team_venue_relationship', 'from', 'to'),\n\t ('time.event', 'start_date', 'end_date'), ('tv.regular_tv_appearance', 'from', 'to'), ('tv.tv_network_duration', 'from', 'to')]\n\ttrain_constraints = [('m.0kpys4', 'US State'), ('m.044801x', 'Professional Sports Team'), ('m.01xljyt', 'American Football team'),\n\t('m.01m9', 'City/Town/Village'), ('m.01xpjyz', 'Airport'), ('m.025dnr9', 'American Football Conference'), ('m.01xs05k', 'River'),\n\t('m.01xryvm', 'Book'), ('m.01mh', 'Continent'), ('m.01y2hnl', 'College/University'),('m.01xljv1', 'Super bowl'), ('m.01xxv5b', 'Island Group'),\n\t('m.02_3pws', 'Mexican state'), ('m.025dnqw', 'American Football Division'), ('m.01y2hn6', 'School'), ('m.01n7', 'Location'),\n\t('m.03jz7ls', 'Written Work'), ('m.08scbsj', 'Subatomic particle'), ('m.03w5clp', 'Production company'), ('m.0kpym_', 'US County'),\n\t('m.01xljtp', 'Hospital'), ('m.04fnrhx', 'Monarch'), ('m.01xs039', 'Mountain range'), ('m.01mp', 'Country'), ('m.02knxyp', 'Religious Text'),\n\t('m.0256985', 'Baseball Team'), ('m.05czz29', 'Brand'), ('m.01nt', 'Region'), ('m.02ht342', 'Automobile Make'), ('m.02_3phk', 'Dutch province')]\n\tif __name__ == '__main__':\n", "    parser = Parser()\n\t    augment_with_s_expr('train')\n\t    augment_with_s_expr('dev')\n\t    augment_with_s_expr('test')"]}
{"filename": "DecAF/Datasets/QA/CWQ/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\tfrom tqdm import tqdm\n\timport argparse\n\tfrom collections import defaultdict\n\tfrom DecAF.Datasets.QA.utils import is_valid\n\tDATA_DIR = os.environ['DATA_DIR']\n\tparser = argparse.ArgumentParser(description='process CWQ dataset')\n", "parser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ/raw')\n\tparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ')\n\targs = parser.parse_args()\n\tif __name__ == \"__main__\":\n\t    # process the test data so that each answer contains the \"aliases\" key\n\t    # this is just for the requirement of the evaluation script\n\t    infname = os.path.join(args.input_dir, \"ComplexWebQuestions_test.json\")\n\t    data = json.load(open(infname, \"r\"))\n\t    for question in data:\n\t        for answer in question[\"answers\"]:\n", "            if \"aliases\" not in answer:\n\t                answer[\"aliases\"] = []\n\t    json.dump(data, open(infname, \"w\"), indent=2)\n\t    # process the train/dev/test data\n\t    questions_list = defaultdict(list)\n\t    for split in ['train', 'dev', 'test']:\n\t        infname = os.path.join(args.input_dir, f\"ComplexWebQuestions_{split}.json\")\n\t        data = json.load(open(infname))\n\t        for question in data:\n\t            q_obj = {\n", "                \"QuestionId\": question[\"ID\"],\n\t                \"Question\": question[\"question\"],\n\t                \"Answers\": [\n\t                    {\"freebaseId\": answer[\"answer_id\"],\n\t                    \"text\": answer[\"answer\"]}\n\t                    for answer in question[\"answers\"]\n\t                ]\n\t            }\n\t            questions_list[split].append(q_obj)\n\t    # write down the processed dataset\n", "    os.makedirs(args.output_dir, exist_ok=True)\n\t    for key in ['train', 'dev', 'test']:\n\t        print(f\"{key} has {len(questions_list[key])} questions\")\n\t        outfname = os.path.join(args.output_dir, f\"{key}.json\")\n\t        with open(outfname, 'w') as wf:\n\t            json.dump(questions_list[key], wf, indent=2)"]}
{"filename": "DecAF/Datasets/QA/CWQ/preprocess_SP.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\tfrom collections import Counter\n\timport argparse\n\tfrom DecAF.Preprocess.linearize import load_nameid_dict\n\tfrom DecAF.Datasets.QA.utils import execute_vanilla_s_expr, revise_only_name\n\tfrom collections import defaultdict\n\tDATA_DIR = os.environ['DATA_DIR']\n", "parser = argparse.ArgumentParser(description='process dataset')\n\tparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/CWQ')\n\targs = parser.parse_args()\n\tif __name__ == \"__main__\":\n\t    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n\t    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\t    # Only keep the training data with correct excution results\n\t    if not os.path.exists(os.path.join(args.data_dir, \"raw/ComplexWebQuestions_train_filtered.expr.json\")):\n\t        print(\"filtering the training data\")\n\t        data_file = os.path.join(args.data_dir, \"raw/ComplexWebQuestions_train.expr.json\")\n", "        with open(data_file, \"r\") as rf:\n\t            data = json.load(rf)\n\t        new_data = []\n\t        for data_i in data:\n\t            answers = [answer_i[\"answer_id\"] for answer_i in data_i[\"answers\"]]\n\t            parsing_results = execute_vanilla_s_expr(data_i[\"SExpr\"])\n\t            if set(answers) == set(parsing_results):\n\t                new_data.append(data_i)\n\t        print(\"remaining rate: \", len(new_data) / len(data))\n\t        save_file = os.path.join(args.data_dir, \"raw/ComplexWebQuestions_train_filtered.expr.json\")\n", "        with open(save_file, \"w\") as wf:\n\t            json.dump(new_data, wf, indent=2)\n\t    # preprocess the s-expression\n\t    num_s_expr = []\n\t    data_dict = defaultdict(list)\n\t    none_answer_dict = defaultdict(int)\n\t    ID2LFs = defaultdict(list)\n\t    for split in ['train', 'dev', 'test']:\n\t        if split == \"train\":\n\t            infname = os.path.join(args.data_dir, f\"raw/ComplexWebQuestions_{split}_filtered.expr.json\")\n", "        else:\n\t            infname = os.path.join(args.data_dir, f\"raw/ComplexWebQuestions_{split}.expr.json\")\n\t        data = json.load(open(infname))\n\t        for question in data:\n\t            s_express_list = [question[\"SExpr\"]] if question[\"SExpr\"] != \"null\" else []\n\t            num_s_expr.append(len(s_express_list))\n\t            if len(s_express_list) == 0:\n\t                none_answer_dict[split] += 1\n\t                if split == \"train\":\n\t                    continue\n", "                LFs = {}\n\t            else:\n\t                LFs = {\n\t                    \"LF_original\": s_express_list,\n\t                    \"LF_processed\": [revise_only_name(s_expr, id2name_dict) for s_expr in s_express_list],\n\t                } \n\t            ID2LFs[question[\"ID\"]] = LFs\n\t    print(Counter(num_s_expr))\n\t    for split in ['train', 'dev', 'test']:\n\t        infname = os.path.join(args.data_dir, f\"{split}.json\")\n", "        data = json.load(open(infname))\n\t        for question in data:\n\t            question_id = question[\"QuestionId\"]\n\t            question.update(ID2LFs[question_id])\n\t            data_dict[split].append(question)\n\t    # write down the processed dataset\n\t    os.makedirs(args.data_dir, exist_ok=True)\n\t    for split in ['train', 'dev', 'test']:\n\t        print(len(data_dict[split]))\n\t        outfname = os.path.join(args.data_dir, f\"{split}.json\")\n", "        with open(outfname, 'w') as f:\n\t            json.dump(data_dict[split], f, indent=2)\n"]}
{"filename": "DecAF/Datasets/QA/GrailQA/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\timport argparse\n\tfrom collections import defaultdict\n\tDATA_DIR = os.environ['DATA_DIR']\n\tparser = argparse.ArgumentParser(description='process dataset')\n\tparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/GrailQA_v1.0/raw')\n\tparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/GrailQA_v1.0')\n", "args = parser.parse_args()\n\tif __name__ == \"__main__\":\n\t    data_dict = defaultdict(list)\n\t    for split in ['train', 'dev']:\n\t        infname = os.path.join(args.input_dir, f\"grailqa_v1.0_{split}.json\")\n\t        data = json.load(open(infname))\n\t        for question in data:\n\t            q_obj = {\n\t                \"QuestionId\": question[\"qid\"],\n\t                \"Question\": question[\"question\"],\n", "                \"Answers\": [\n\t                    {\"freebaseId\": answer[\"answer_argument\"],\n\t                    \"text\": answer[\"entity_name\"] if \"entity_name\" in answer else answer[\"answer_argument\"]} for answer in question[\"answer\"]\n\t                ]\n\t            }\n\t            data_dict[split].append(q_obj)\n\t    infname = os.path.join(args.input_dir, f\"grailqa_v1.0_test_public.json\")\n\t    data = json.load(open(infname))\n\t    for question in data:\n\t        q_obj = {\n", "            \"QuestionId\": question[\"qid\"],\n\t            \"Question\": question[\"question\"],\n\t            \"Answers\": [\n\t                {\"freebaseId\": \"None\",\n\t                    \"text\": \"None\" } \n\t            ]\n\t        }\n\t        data_dict[\"test\"].append(q_obj)\n\t    os.makedirs(args.output_dir, exist_ok=True)\n\t    for split in ['train', 'dev', 'test']:\n", "        print(len(data_dict[split]))\n\t        outfname = os.path.join(args.output_dir, f\"{split}.json\")\n\t        with open(outfname, 'w') as wf:\n\t            json.dump(data_dict[split], wf, indent=2)"]}
{"filename": "DecAF/Datasets/QA/GrailQA/preprocess_SP.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\timport argparse\n\tfrom DecAF.Preprocess.linearize import load_nameid_dict\n\tfrom DecAF.Datasets.QA.utils import revise_only_name\n\tfrom collections import defaultdict\n\timport logging\n\tlogging.basicConfig(level=logging.INFO)\n", "DATA_DIR = os.environ['DATA_DIR']\n\tparser = argparse.ArgumentParser(description='process dataset')\n\tparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/GrailQA')\n\targs = parser.parse_args()\n\tif __name__ == \"__main__\":\n\t    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n\t    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\t    data_dict = defaultdict(list)\n\t    ID2LFs = defaultdict(list)\n\t    for split in ['train', 'dev']:\n", "        infname = os.path.join(args.data_dir, f\"raw/grailqa_v1.0_{split}.json\")\n\t        data = json.load(open(infname))\n\t        for question in data:\n\t            assert isinstance(question[\"s_expression\"], str)\n\t            LFs = {\n\t                    \"LF_original\": [question[\"s_expression\"]],\n\t                    \"LF_processed\": [revise_only_name(question[\"s_expression\"], id2name_dict)],\n\t                } \n\t            ID2LFs[question[\"qid\"]] = LFs\n\t    for split in ['train', 'dev', 'test']:\n", "        infname = os.path.join(args.data_dir, f\"{split}.json\")\n\t        data = json.load(open(infname))\n\t        for question in data:\n\t            question_id = question[\"QuestionId\"]\n\t            question.update(ID2LFs[question_id])\n\t            data_dict[split].append(question)\n\t    # write down the processed dataset\n\t    os.makedirs(args.data_dir, exist_ok=True)\n\t    for split in ['train', 'dev', 'test']:\n\t        print(len(data_dict[split]))\n", "        outfname = os.path.join(args.data_dir, f\"{split}.json\")\n\t        with open(outfname, 'w') as wf:\n\t            json.dump(data_dict[split], wf, indent=2)\n"]}
{"filename": "DecAF/Datasets/QA/FreebaseQA/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\tfrom collections import defaultdict, Counter\n\timport argparse\n\tDATA_DIR = os.environ['DATA_DIR']\n\t# read data\n\tdef get_answers(question):\n\t    \"\"\"extract unique answers from question parses.\"\"\"\n", "    answers = set()\n\t    for parse in question[\"Parses\"]:\n\t        for answer in parse[\"Answers\"]:\n\t            assert len(answer[\"AnswersName\"]) == 1\n\t            answers.add((answer[\"AnswersMid\"],\n\t                answer[\"AnswersName\"][0]))\n\t    return answers\n\tdef get_entities(question):\n\t    \"\"\"extract oracle entities from question parses.\"\"\"\n\t    entities = set()\n", "    for parse in question[\"Parses\"]:\n\t        if parse[\"TopicEntityMid\"] is not None:\n\t            entities.add((parse[\"TopicEntityMid\"], parse[\"TopicEntityName\"]))\n\t    return entities\n\tparser = argparse.ArgumentParser(description='process dataset')\n\tparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/FreebaseQA/raw')\n\tparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/FreebaseQA')\n\targs = parser.parse_args()\n\tif __name__ == \"__main__\":\n\t    questions_list = defaultdict(list)\n", "    for split in ['train', 'dev', 'test']:\n\t        num_answer_list = []\n\t        num_without_answers = 0\n\t        if split == 'test':\n\t            infname = os.path.join(args.input_dir, f\"FreebaseQA-eval.json\")\n\t        else:\n\t            infname = os.path.join(args.input_dir, f\"FreebaseQA-{split}.json\")\n\t        data = json.load(open(infname))\n\t        for question in data[\"Questions\"]:\n\t            q_obj = {\n", "                \"QuestionId\": question[\"Question-ID\"],\n\t                \"Question\": question[\"RawQuestion\"],\n\t                \"Answers\": [\n\t                    {\"freebaseId\": answer[0],\n\t                    \"text\": answer[1]}\n\t                    for answer in get_answers(question)\n\t                ]\n\t            }\n\t            num_answer_list.append(len(q_obj[\"Answers\"]))\n\t            # for answer in get_answers(question):\n", "            #     if not answer[0].startswith(\"m.\"):\n\t            #         print(answer)\n\t            if len(get_answers(question)) == 0:\n\t                num_without_answers += 1\n\t            questions_list[f\"{split}\"].append(q_obj)\n\t        print(\"num_without_answers: \", num_without_answers)\n\t        print(\"answer num distribution: \", Counter(num_answer_list))\n\t    for key in questions_list:\n\t        print(key, len(questions_list[key]))\n\t    # write down the processed dataset\n", "    os.makedirs(args.output_dir, exist_ok=True)\n\t    for key in [\"train\", \"dev\", \"test\"]:\n\t        outfname = os.path.join(args.output_dir, f\"{key}.json\")\n\t        with open(outfname, 'w') as wf:\n\t            json.dump(questions_list[key], wf, indent=2)\n"]}
{"filename": "DecAF/Datasets/QA/WebQSP/preprocess_QA.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\tfrom collections import defaultdict\n\timport argparse\n\tDATA_DIR = os.environ['DATA_DIR']\n\tdef get_answers(question):\n\t    \"\"\"extract unique answers from question parses.\"\"\"\n\t    answers = set()\n", "    for parse in question[\"Parses\"]:\n\t        for answer in parse[\"Answers\"]:\n\t            answers.add((answer[\"AnswerArgument\"],\n\t                answer[\"EntityName\"]))\n\t    return answers\n\tdef get_entities(question):\n\t    \"\"\"extract oracle entities from question parses.\"\"\"\n\t    entities = set()\n\t    for parse in question[\"Parses\"]:\n\t        if parse[\"TopicEntityMid\"] is not None:\n", "            entities.add((parse[\"TopicEntityMid\"], parse[\"TopicEntityName\"]))\n\t    return entities\n\tparser = argparse.ArgumentParser(description='process WebQSP dataset')\n\tparser.add_argument('--input_dir', type=str, default=f'{DATA_DIR}/tasks/QA/WebQSP/raw/data')\n\tparser.add_argument('--output_dir', type=str, default=f'{DATA_DIR}/tasks/QA/WebQSP')\n\targs = parser.parse_args()\n\tif __name__ == \"__main__\":\n\t    questions_list = defaultdict(list)\n\t    for split in ['train', 'test']:\n\t        num_without_answers = 0\n", "        infname = os.path.join(args.input_dir, f\"WebQSP.{split}.json\")\n\t        data = json.load(open(infname))\n\t        for question in data[\"Questions\"]:\n\t            q_obj = {\n\t                \"QuestionId\": question[\"QuestionId\"],\n\t                \"Question\": question[\"ProcessedQuestion\"],\n\t                \"Answers\": [\n\t                    {\"freebaseId\": answer[0],\n\t                    \"text\": answer[1]}\n\t                    for answer in get_answers(question)\n", "                ]\n\t            }\n\t            if len(get_answers(question)) == 0:\n\t                num_without_answers += 1\n\t            questions_list[split].append(q_obj)\n\t        print(num_without_answers)\n\t    # write down the processed dataset\n\t    os.makedirs(args.output_dir, exist_ok=True)\n\t    for key in [\"train\", \"test\"]:\n\t        print(f\"{key} has {len(questions_list[key])} questions\")\n", "        outfname = os.path.join(args.output_dir, f\"{key}.json\")\n\t        with open(outfname, 'w') as wf:\n\t            json.dump(questions_list[key], wf, indent=2)\n"]}
{"filename": "DecAF/Datasets/QA/WebQSP/preprocess_SP.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\tfrom collections import Counter\n\timport argparse\n\tfrom DecAF.Preprocess.linearize import load_nameid_dict\n\tfrom DecAF.Datasets.QA.utils import revise_only_name\n\tfrom collections import defaultdict\n\tDATA_DIR = os.environ['DATA_DIR']\n", "parser = argparse.ArgumentParser(description='process dataset')\n\tparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/tasks/QA/WebQSP')\n\targs = parser.parse_args()\n\tif __name__ == \"__main__\":\n\t    name_dir = f\"{DATA_DIR}/knowledge_source/Freebase/id2name_parts_disamb\"\n\t    name2id_dict, id2name_dict = load_nameid_dict(name_dir, lower=False)\n\t    # preprocess the s-expression\n\t    num_s_expr = []\n\t    data_dict = defaultdict(list)\n\t    none_answer_dict = defaultdict(int)\n", "    ID2LFs = defaultdict(list)\n\t    for split in ['train', 'test']:\n\t        infname = os.path.join(args.data_dir, f\"raw/data/WebQSP.{split}.expr.json\")\n\t        data = json.load(open(infname))\n\t        for question in data:\n\t            s_express_list = [parse[\"SExpr\"] for parse in question[\"Parses\"] if parse[\"SExpr\"] != \"null\"]\n\t            num_s_expr.append(len(s_express_list))\n\t            if len(s_express_list) == 0:\n\t                none_answer_dict[split] += 1\n\t                if split == \"train\":\n", "                    continue\n\t                LFs = {}\n\t            else:\n\t                LFs = {\n\t                    \"LF_original\": s_express_list,\n\t                    \"LF_processed\": [revise_only_name(s_expr, id2name_dict) for s_expr in s_express_list],\n\t                } \n\t            ID2LFs[question[\"QuestionId\"]] = LFs\n\t    print(Counter(num_s_expr))\n\t    for split in ['train', 'test']:\n", "        infname = os.path.join(args.data_dir, f\"{split}.json\")\n\t        data = json.load(open(infname))\n\t        for question in data:\n\t            question_id = question[\"QuestionId\"]\n\t            question.update(ID2LFs[question_id])\n\t            data_dict[split].append(question)\n\t    # write down the processed dataset\n\t    os.makedirs(args.data_dir, exist_ok=True)\n\t    for split in ['train', 'test']:\n\t        print(len(data_dict[split]))\n", "        outfname = os.path.join(args.data_dir, f\"{split}.json\")\n\t        with open(outfname, 'w') as f:\n\t            json.dump(data_dict[split], f, indent=2)\n"]}
{"filename": "DecAF/Retrieval/utils.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n\t# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\t'''\n\tevaluation functions for retrieval\n\t'''\n\timport unicodedata\n", "from collections import defaultdict\n\timport numpy as np\n\timport regex as re\n\tfrom tqdm import tqdm\n\tfrom DecAF.Datasets.QA.utils import parse_answer\n\tdef has_answer(answers, text, tokenizer, match_type) -> bool:\n\t    \"\"\"Check if a document contains an answer string.\n\t    If `match_type` is string, token matching is done between the text and answer.\n\t    If `match_type` is regex, we search the whole text with the regex.\n\t    \"\"\"\n", "    text = _normalize(text)\n\t    if match_type == \"string\":\n\t        # Answer is a list of possible strings\n\t        if tokenizer is None:\n\t            text = text.lower()\n\t            for single_answer in answers:\n\t                norm_answer = _normalize(single_answer).lower()\n\t                if norm_answer in text:\n\t                    return True\n\t        else:\n", "            text = tokenizer.tokenize(text).words(uncased=True)\n\t            for single_answer in answers:\n\t                single_answer = _normalize(single_answer)\n\t                single_answer = tokenizer.tokenize(single_answer)\n\t                single_answer = single_answer.words(uncased=True)\n\t                for i in range(0, len(text) - len(single_answer) + 1):\n\t                    if single_answer == text[i : i + len(single_answer)]:\n\t                        return True\n\t    elif match_type == \"regex\":\n\t        # Answer is a regex\n", "        for single_answer in answers:\n\t            single_answer = _normalize(single_answer)\n\t            if regex_match(text, single_answer):\n\t                return True\n\t    return False\n\tdef _normalize(text):\n\t    return unicodedata.normalize(\"NFD\", text)\n\tdef regex_match(text, pattern):\n\t    \"\"\"Test if a regex pattern is contained within a text.\"\"\"\n\t    try:\n", "        pattern = re.compile(pattern, flags=re.IGNORECASE + re.UNICODE + re.MULTILINE)\n\t    except BaseException:\n\t        return False\n\t    return pattern.search(text) is not None\n\tdef eval_top_k_one(data_i, top_k=100, tokenizer=None):\n\t    recall = 0\n\t    answers = parse_answer(data_i['Answers'], original_name=True)\n\t    for answer in answers:\n\t        for ctx in data_i['ctxs'][:top_k]:\n\t            context = ctx['title'] + \" \" + ctx['text']\n", "            if has_answer([answer], context, tokenizer, \"string\"):\n\t                recall += 1\n\t                break\n\t    return recall / (len(answers) + 1e-8)\n\tdef recall_ctx(ctx, answers, tokenizer=None):\n\t    context = ctx['title'] + \" \" + ctx['text']\n\t    recall = 0\n\t    for answer in answers:\n\t        if has_answer([answer], context, tokenizer, \"string\"):\n\t            recall += 1\n", "    return recall / (len(answers) + 1e-8)\n\tdef eval_top_k(output_data, top_k_list=[1, 20, 50, 100, 200, 500], tokenizer=None):\n\t    print(\"Evaluation\")\n\t    hits_dict = defaultdict(int)\n\t    recall_dict = defaultdict(float)\n\t    num_tokens_dict = defaultdict(list)\n\t    top_k_list = [k for k in top_k_list if k <= len(output_data[0]['ctxs'])]\n\t    for data_i in tqdm(output_data):\n\t        for k in top_k_list:\n\t            recall = eval_top_k_one(data_i, top_k=k, tokenizer=tokenizer)\n", "            if recall > 0:\n\t                hits_dict[k] += 1\n\t            recall_dict[k] += recall\n\t            num_tokens_dict[k].append(sum([len(ctx[\"text\"].split(\" \"))+len(ctx[\"title\"].split(\" \")) for ctx in data_i['ctxs'][:k]]))\n\t    for k in top_k_list:\n\t        print(\"Top {}\".format(k), \n\t              \"Hits: \", round(hits_dict[k] * 100 / len(output_data), 1), \n\t              \"Recall: \", round(recall_dict[k] * 100 / len(output_data), 1))\n"]}
{"filename": "DecAF/Retrieval/Pyserini/utils.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\tINDEX_DIR = os.environ[\"DATA_DIR\"] + \"/knowledge_source\"\n\tINDEX_MAP_DICT = {\n\t    \"Freebase\": f\"{INDEX_DIR}/Freebase/processed/index/pyserini_bm25\",\n\t}"]}
{"filename": "DecAF/Retrieval/Pyserini/search.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\tfrom pyserini.search.lucene import LuceneSearcher\n\timport json\n\tfrom tqdm import tqdm\n\timport os\n\timport argparse\n\tfrom DecAF.Retrieval.utils import eval_top_k\n\tfrom DecAF.Retrieval.Pyserini.utils import INDEX_MAP_DICT\n\timport multiprocessing.pool\n", "from functools import partial\n\tdef print_results(hits):\n\t    for i in range(len(hits)):\n\t        print(f'{i+1:2} {hits[i].docid:4} {hits[i].score:.5f} {hits[i].raw}')\n\tclass Bm25Searcher:\n\t    def __init__(self, index_dir, args):\n\t        self.index_dir = index_dir\n\t        self.args = args\n\t        try:\n\t            self.searcher = LuceneSearcher(index_dir)\n", "        except:\n\t            print(\"index dir not found\")\n\t            self.searcher = LuceneSearcher.from_prebuilt_index(index_dir)\n\t        self.searcher.set_bm25(args.k1, args.b)\n\t        if len(args.ignore_string) > 0:\n\t            self.ignore_list = args.ignore_string.split(',')\n\t            print(f'ignore list: {self.ignore_list}')\n\t        else:\n\t            self.ignore_list = []\n\t    def perform_search(self, data_i, top_k):\n", "        query = data_i[\"Question\"]\n\t        for string in self.ignore_list:\n\t            query = query.replace(string, ' ')\n\t        query = query.strip()\n\t        results = self.searcher.search(query, k=top_k)\n\t        ctxs = []\n\t        for result in results:\n\t            doc_dict = json.loads(result.raw)\n\t            ctx_text = doc_dict[\"contents\"]\n\t            ctx = {\"title\": doc_dict[\"title\"], \"text\": ctx_text, \"score\": result.score}\n", "            ctxs.append(ctx)\n\t        output_i = data_i.copy()\n\t        output_i[\"ctxs\"] = ctxs\n\t        return output_i\n\tdef search_all(process_idx, num_process, searcher, args):\n\t    with open(args.query_data_path, 'r') as rf:\n\t        data = json.load(rf)\n\t    output_data = []\n\t    for i, data_i in tqdm(enumerate(data)):\n\t        if i % num_process != process_idx:\n", "            continue\n\t        if i > args.num_queries and args.num_queries != -1:\n\t            break\n\t        output_i = searcher.perform_search(data_i, args.top_k)\n\t        output_data.append(output_i)\n\t    return output_data\n\t# argparse for root_dir, index_dir, query_data_path, output_dir\n\tparser = argparse.ArgumentParser(description='Search using pySerini')\n\tparser.add_argument(\"--index_name\", type=str, default='Wikidata',\n\t                    help=\"directory to store the search index\")\n", "parser.add_argument(\"--query_data_path\", type=str, default='/home/ubuntu/data/KBQA/WebQSP/data/WebQSP_processed.test.json',\n\t                    help=\"directory to store the queries\")\n\tparser.add_argument(\"--output_dir\", type=str, default='/home/ubuntu/data/KBQA/GeneralKB/Retrieval/pyserini/search_results',\n\t                    help=\"directory to store the retrieved output\")\n\tparser.add_argument(\"--num_process\", type=int, default=10,\n\t                    help=\"number of processes to use for multi-threading\")\n\tparser.add_argument(\"--top_k\", type=int, default=150,\n\t                    help=\"number of passages to be retrieved for each query\")\n\tparser.add_argument(\"--ignore_string\", type=str, default=\"\",\n\t                    help=\"string to ignore in the query, split by comma\")\n", "parser.add_argument(\"--b\", type=float, default=0.4,\n\t                    help=\"parameter of BM25\")\n\tparser.add_argument(\"--k1\", type=float, default=0.9,\n\t                    help=\"parameter of BM25\")\n\tparser.add_argument(\"--num_queries\", type=int, default=1000,\n\t                    help=\"number of queries to test\")\n\tparser.add_argument(\"--save\", action=\"store_true\",\n\t                    help=\"whether to save the output\")\n\tparser.add_argument(\"--eval\", action=\"store_true\",\n\t                    help=\"whether to evaluate the output\")\n", "args = parser.parse_args()\n\tif __name__ == '__main__':\n\t    if args.index_name in INDEX_MAP_DICT:\n\t        index_dir = INDEX_MAP_DICT[args.index_name]\n\t    else:\n\t        exit(\"no such index\")\n\t    print(\"index dir: \", index_dir)\n\t    searcher = Bm25Searcher(index_dir, args)\n\t    num_process = args.num_process\n\t    pool = multiprocessing.pool.ThreadPool(processes=num_process)\n", "    sampleData = [x for x in range(num_process)]\n\t    search_all_part = partial(search_all, \n\t                                searcher = searcher,\n\t                                num_process = num_process,\n\t                                args = args)\n\t    results = pool.map(search_all_part, sampleData)\n\t    pool.close()\n\t    output_data = []\n\t    for result in results:\n\t        output_data += result\n", "    # sort the output data by question id\n\t    output_data = sorted(output_data, key=lambda item: item['QuestionId'])\n\t    tokenizer = None\n\t    if args.eval:\n\t        eval_top_k(output_data, top_k_list=[5, 10, 20, 100], tokenizer=tokenizer)\n\t    # save output data\n\t    # create output dir recursively if not exist\n\t    if args.save:\n\t        os.makedirs(args.output_dir, exist_ok=True)\n\t        output_name = args.query_data_path.split('/')[-1]\n", "        output_name_split = output_name.split('.')\n\t        output_name = '.'.join(output_name_split[-2:])\n\t        output_path = os.path.join(args.output_dir, output_name)\n\t        print(\"saving output data to {}\".format(output_path))\n\t        with open(output_path, \"w\") as wf:\n\t            json.dump(output_data, wf, indent=2)\n"]}
{"filename": "DecAF/Reading/process_fid.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport json\n\timport argparse\n\tfrom tqdm import tqdm\n\tfrom DecAF.Datasets.QA.utils import parse_answer\n\tparser = argparse.ArgumentParser(description='Process the retrieved data to fit the format of FiD model')\n\tparser.add_argument(\"--retrieval_data_path\", type=str, \n\t                    default='/home/ubuntu/data/KBQA/WebQSP/test.json')\n", "parser.add_argument(\"--mode\", type=str, \n\t                    default='SPQA', help=\"SPQA or QA or SP\")\n\targs = parser.parse_args()\n\tfor split in [\"dev\", \"train\", \"test\"]:\n\t    file_path = os.path.join(args.retrieval_data_path, f\"{split}.json\")\n\t    if not os.path.exists(file_path):\n\t        continue\n\t    with open(file_path, \"r\") as rf:\n\t        data = json.load(rf)\n\t    new_data_qa = []\n", "    new_data_sp = []\n\t    for data_i in tqdm(data):\n\t        if args.mode != \"QA\":\n\t            if \"LF_processed\" in data_i:\n\t                new_data_i = {\n\t                    \"id\": str(data_i[\"QuestionId\"]) + \":SP\",\n\t                    \"question\": \"Semantic Parsing: \" + data_i[\"Question\"],\n\t                    \"answers\": data_i[\"LF_processed\"],\n\t                    \"ctxs\": data_i[\"ctxs\"],\n\t                }\n", "                new_data_sp.append(new_data_i)\n\t            elif split != \"train\":\n\t                new_data_i = {\n\t                    \"id\": str(data_i[\"QuestionId\"]) + \":SP\",\n\t                    \"question\": \"Semantic Parsing: \" + data_i[\"Question\"],\n\t                    \"answers\": [\"none\"],\n\t                    \"ctxs\": data_i[\"ctxs\"],\n\t                }\n\t                new_data_sp.append(new_data_i)\n\t        if args.mode != \"SP\":\n", "            new_data_i = {\n\t                \"id\": str(data_i[\"QuestionId\"]) + \":QA\",\n\t                \"question\": \"Question Answering: \" + data_i[\"Question\"],\n\t                \"answers\": parse_answer(data_i[\"Answers\"]),\n\t                \"ctxs\": data_i[\"ctxs\"],\n\t            }\n\t            new_data_qa.append(new_data_i)\n\t    new_data = new_data_qa + new_data_sp\n\t    print(len(new_data))\n\t    output_file = file_path.replace(\".json\", f\"_fid_{args.mode}.json\")\n", "    print(output_file)\n\t    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\t    with open(output_file, \"w\") as wf:\n\t        json.dump(new_data, wf, indent=2)"]}
{"filename": "DecAF/Reading/train_reader.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n\t# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport time\n\timport sys\n\timport torch\n\timport transformers\n", "import numpy as np\n\tfrom pathlib import Path\n\tfrom torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n\tfrom src.options import Options\n\timport src.slurm\n\timport src.util\n\timport src.evaluation\n\timport src.data\n\timport src.model\n\tdef train(model, optimizer, scheduler, step, train_dataset, eval_dataset, opt, collator, best_dev_em, checkpoint_path):\n", "    if opt.is_main:\n\t        try:\n\t            tb_logger = torch.utils.tensorboard.SummaryWriter(Path(opt.checkpoint_dir)/opt.name)\n\t        except:\n\t            tb_logger = None\n\t            logger.warning('Tensorboard is not available.')\n\t    torch.manual_seed(opt.global_rank + opt.seed) #different seed for different sampling depending on global_rank\n\t    train_sampler = RandomSampler(train_dataset)\n\t    train_dataloader = DataLoader(\n\t        train_dataset,\n", "        sampler=train_sampler,\n\t        batch_size=opt.per_gpu_batch_size,\n\t        drop_last=True,\n\t        num_workers=10,\n\t        collate_fn=collator\n\t    )\n\t    loss, curr_loss = 0.0, 0.0\n\t    epoch = 1\n\t    model.train()\n\t    while step < opt.total_steps:\n", "        epoch += 1\n\t        for i, batch in enumerate(train_dataloader):\n\t            step += 1\n\t            (idx, labels, _, context_ids, context_mask) = batch\n\t            train_loss = model(\n\t                input_ids=context_ids.cuda(),\n\t                attention_mask=context_mask.cuda(),\n\t                labels=labels.cuda()\n\t            )[0]\n\t            train_loss = train_loss / opt.accumulation_steps # add this to average the loss\n", "            train_loss.backward()\n\t            if step % opt.accumulation_steps == 0:\n\t                torch.nn.utils.clip_grad_norm_(model.parameters(), opt.clip)\n\t                optimizer.step()\n\t                scheduler.step()\n\t                model.zero_grad()\n\t            train_loss = src.util.average_main(train_loss, opt)\n\t            curr_loss += train_loss.item()\n\t            if step % opt.eval_freq == 0:\n\t                dev_em = evaluate(model, eval_dataset, tokenizer, collator, opt)\n", "                model.train()\n\t                if opt.is_main:\n\t                    if dev_em > best_dev_em:\n\t                        best_dev_em = dev_em\n\t                        src.util.save(model, optimizer, scheduler, step, best_dev_em,\n\t                                  opt, checkpoint_path, 'best_dev')\n\t                    log = f\"{step} / {opt.total_steps} |\"\n\t                    log += f\"train: {curr_loss/opt.eval_freq:.3f} |\"\n\t                    log += f\"evaluation: {100*dev_em:.2f}EM |\"\n\t                    log += f\"lr: {scheduler.get_last_lr()[0]:.5f}\"\n", "                    logger.info(log)    \n\t                    if tb_logger is not None:\n\t                        tb_logger.add_scalar(\"Evaluation\", dev_em, step)\n\t                        tb_logger.add_scalar(\"Training\", curr_loss / (opt.eval_freq), step)\n\t                    curr_loss = 0.\n\t            if opt.is_main and step % opt.save_freq == 0:\n\t                src.util.save(model, optimizer, scheduler, step, best_dev_em,\n\t                          opt, checkpoint_path, f\"step-{step}\")\n\t            if step > opt.total_steps:\n\t                break\n", "def evaluate(model, dataset, tokenizer, collator, opt):\n\t    sampler = SequentialSampler(dataset)\n\t    dataloader = DataLoader(dataset,\n\t        sampler=sampler,\n\t        batch_size=opt.per_gpu_batch_size,\n\t        drop_last=False,\n\t        num_workers=10,\n\t        collate_fn=collator\n\t    )\n\t    model.eval()\n", "    total = 0\n\t    exactmatch = []\n\t    model = model.module if hasattr(model, \"module\") else model\n\t    with torch.no_grad():\n\t        for i, batch in enumerate(dataloader):\n\t            (idx, _, _, context_ids, context_mask) = batch\n\t            outputs = model.generate(\n\t                input_ids=context_ids.cuda(),\n\t                attention_mask=context_mask.cuda(),\n\t                max_length=opt.answer_maxlength,\n", "            )\n\t            for k, o in enumerate(outputs):\n\t                ans = tokenizer.decode(o, skip_special_tokens=True)\n\t                gold = dataset.get_example(idx[k])['answers']\n\t                score = src.evaluation.ems(ans, gold)\n\t                total += 1\n\t                exactmatch.append(score)\n\t    exactmatch, total = src.util.weighted_average(np.mean(exactmatch), total, opt)\n\t    return exactmatch\n\tif __name__ == \"__main__\":\n", "    options = Options()\n\t    options.add_reader_options()\n\t    options.add_optim_options()\n\t    options.parser.add_argument('--total_batch_size', type=int, default=16)\n\t    opt = options.parse()\n\t    #opt = options.get_options(use_reader=True, use_optim=True)\n\t    torch.manual_seed(opt.seed)\n\t    src.slurm.init_distributed_mode(opt)\n\t    src.slurm.init_signal_handler()\n\t    # set training steps according to total GPU numbers\n", "    step_scale_size = (opt.total_batch_size // opt.world_size // opt.per_gpu_batch_size)\n\t    opt.total_steps *= step_scale_size\n\t    opt.accumulation_steps *= step_scale_size\n\t    opt.save_freq *= step_scale_size\n\t    opt.eval_freq *= step_scale_size\n\t    checkpoint_path = Path(opt.checkpoint_dir)/opt.name\n\t    # checkpoint_exists = checkpoint_path.exists()\n\t    checkpoint_exists = False\n\t    if opt.is_distributed:\n\t        torch.distributed.barrier()\n", "    checkpoint_path.mkdir(parents=True, exist_ok=True)\n\t    #if not checkpoint_exists and opt.is_main:\n\t    #    options.print_options(opt)\n\t    #checkpoint_path, checkpoint_exists = util.get_checkpoint_path(opt)\n\t    logger = src.util.init_logger(\n\t        opt.is_main,\n\t        opt.is_distributed,\n\t        checkpoint_path / 'run.log'\n\t    )\n\t    model_name = 't5-' + opt.model_size\n", "    model_class = src.model.FiDT5\n\t    #load data\n\t    tokenizer = transformers.T5Tokenizer.from_pretrained(model_name)\n\t    collator = src.data.Collator(opt.text_maxlength, tokenizer, answer_maxlength=opt.answer_maxlength)\n\t    # use golbal rank and world size to split the eval set on multiple gpus\n\t    train_examples = src.data.load_data(\n\t        opt.train_data, \n\t        global_rank=opt.global_rank, \n\t        world_size=opt.world_size,\n\t    )\n", "    train_dataset = src.data.Dataset(train_examples, opt.n_context)\n\t    # use golbal rank and world size to split the eval set on multiple gpus\n\t    eval_examples = src.data.load_data(\n\t        opt.eval_data,\n\t        global_rank=opt.global_rank,\n\t        world_size=opt.world_size,\n\t    )\n\t    eval_dataset = src.data.Dataset(eval_examples, opt.n_context)\n\t    if not checkpoint_exists and opt.model_path == \"none\":\n\t        t5 = transformers.T5ForConditionalGeneration.from_pretrained(model_name)\n", "        model = src.model.FiDT5(t5.config)\n\t        model.load_t5(t5.state_dict())\n\t        model = model.to(opt.local_rank)\n\t        optimizer, scheduler = src.util.set_optim(opt, model)\n\t        step, best_dev_em = 0, 0.0\n\t    elif opt.model_path == \"none\":\n\t        load_path = checkpoint_path / 'checkpoint' / 'latest'\n\t        model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n\t            src.util.load(model_class, load_path, opt, reset_params=False)\n\t        logger.info(f\"Model loaded from {load_path}\")\n", "    else:\n\t        model, optimizer, scheduler, opt_checkpoint, step, best_dev_em = \\\n\t            src.util.load(model_class, opt.model_path, opt, reset_params=True)\n\t        logger.info(f\"Model loaded from {opt.model_path}\")\n\t    model.set_checkpoint(opt.use_checkpoint)\n\t    if opt.is_distributed:\n\t        model = torch.nn.parallel.DistributedDataParallel(\n\t            model,\n\t            device_ids=[opt.local_rank],\n\t            output_device=opt.local_rank,\n", "            find_unused_parameters=False,\n\t        )\n\t    logger.info(\"Start training\")\n\t    train(\n\t        model,\n\t        optimizer,\n\t        scheduler,\n\t        step,\n\t        train_dataset,\n\t        eval_dataset,\n", "        opt,\n\t        collator,\n\t        best_dev_em,\n\t        checkpoint_path\n\t    )\n"]}
{"filename": "DecAF/Reading/test_reader.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n\t# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport torch\n\timport transformers\n\timport numpy as np\n\tfrom pathlib import Path\n", "from torch.utils.data import DataLoader, SequentialSampler\n\timport src.slurm\n\timport src.util\n\tfrom src.options import Options\n\timport src.data\n\timport src.evaluation\n\timport src.model\n\tfrom tqdm import tqdm\n\timport json\n\tclass FiDT5(src.model.FiDT5):\n", "    def generate(self, input_ids, attention_mask, max_length, \n\t                    num_beams=1, num_return_sequences=1, use_cache=False):\n\t        self.encoder.n_passages = input_ids.size(1)\n\t        return transformers.T5ForConditionalGeneration.generate(\n\t            self,\n\t            input_ids=input_ids.view(input_ids.size(0), -1),\n\t            attention_mask=attention_mask.view(attention_mask.size(0), -1),\n\t            max_length=max_length,\n\t            num_beams=num_beams,\n\t            num_return_sequences=num_return_sequences,\n", "            use_cache=use_cache,\n\t        )\n\tdef write_output(glob_path, output_path):\n\t    files = list(glob_path.glob('*.json'))\n\t    files.sort()\n\t    with open(output_path, 'w') as outfile:\n\t        output_data = {}\n\t        for path in files:\n\t            with open(path, 'r') as f:\n\t                data = json.load(f)\n", "                output_data.update(data)\n\t            path.unlink()\n\t        json.dump(output_data, outfile, indent=4)\n\t    glob_path.rmdir()\n\tdef evaluate(model, dataset, dataloader, tokenizer, opt):\n\t    loss, curr_loss = 0.0, 0.0\n\t    model.eval()\n\t    if hasattr(model, \"module\"):\n\t        model = model.module\n\t    if opt.write_crossattention_scores:\n", "        model.overwrite_forward_crossattention()\n\t        model.reset_score_storage() \n\t    total = 0\n\t    exactmatch = []\n\t    if opt.write_results:\n\t        write_path = Path(opt.checkpoint_dir) / opt.name / 'test_results'\n\t        fw = open(write_path / ('%d.json'%opt.global_rank), 'w')\n\t        output_results = {}\n\t    with torch.no_grad():\n\t        for i, batch in tqdm(enumerate(dataloader)):\n", "            (idx, _, _, context_ids, context_mask) = batch\n\t            if opt.write_crossattention_scores:\n\t                model.reset_score_storage()\n\t            outputs = model.generate(\n\t                input_ids=context_ids.cuda(),\n\t                attention_mask=context_mask.cuda(),\n\t                max_length=opt.answer_maxlength,\n\t                num_beams=opt.num_beams,\n\t                num_return_sequences=opt.num_beams,\n\t                use_cache=opt.use_cache_eval,\n", "            )\n\t            outputs = outputs.reshape(len(idx), -1, outputs.shape[-1])\n\t            if opt.write_crossattention_scores:\n\t                crossattention_scores = model.get_crossattention_scores(context_mask.cuda())\n\t            for k, o in enumerate(outputs):\n\t                ans_list = []\n\t                for o_i in o:\n\t                    ans = tokenizer.decode(o_i, skip_special_tokens=True)\n\t                    ans_list.append(ans)\n\t                example = dataset.data[idx[k]]\n", "                if 'answers' in example:\n\t                    score = src.evaluation.ems(ans_list[0], example['answers'])\n\t                    exactmatch.append(score)\n\t                if opt.write_results:\n\t                    # fw.write(str(example['id']) + \"\\t\" + ans + '\\n')\n\t                    output_results[example[\"id\"]] = {\n\t                        \"question\": example[\"question\"],\n\t                        \"gold answers\": example['answers'],\n\t                        \"predicted answers\": ans_list\n\t                    }\n", "                if opt.write_crossattention_scores:\n\t                    for j in range(context_ids.size(1)):\n\t                        example['ctxs'][j]['score'] = crossattention_scores[k, j].item()\n\t                total += 1\n\t            if (i + 1) % opt.eval_print_freq == 0:\n\t                log = f'Process rank:{opt.global_rank}, {i+1} / {len(dataloader)}'\n\t                if len(exactmatch) == 0:\n\t                    log += '| no answer to compute scores'\n\t                else:\n\t                    log += f' | average = {np.mean(exactmatch):.3f}'\n", "                logger.warning(log)\n\t    if opt.write_results:\n\t        json.dump(output_results, fw, indent=4)\n\t        fw.close()\n\t    logger.warning(f'Process rank:{opt.global_rank}, total {total} | average = {np.mean(exactmatch):.3f}')\n\t    if opt.is_distributed:\n\t        torch.distributed.barrier()\n\t    score, total = src.util.weighted_average(np.mean(exactmatch), total, opt)\n\t    return score, total\n\tif __name__ == \"__main__\":\n", "    options = Options()\n\t    options.add_reader_options()\n\t    options.add_eval_options()\n\t    options.parser.add_argument('--num_beams', type=int, default=1)\n\t    options.parser.add_argument('--use_cache_eval', action='store_true')\n\t    opt = options.parse()\n\t    src.slurm.init_distributed_mode(opt)\n\t    src.slurm.init_signal_handler()\n\t    opt.train_batch_size = opt.per_gpu_batch_size * max(1, opt.world_size)\n\t    dir_path = Path(opt.checkpoint_dir)/opt.name\n", "    directory_exists = dir_path.exists()\n\t    if opt.is_distributed:\n\t        torch.distributed.barrier()\n\t    dir_path.mkdir(parents=True, exist_ok=True)\n\t    if opt.write_results:\n\t        (dir_path / 'test_results').mkdir(parents=True, exist_ok=True)\n\t    logger = src.util.init_logger(opt.is_main, opt.is_distributed, Path(opt.checkpoint_dir) / opt.name / 'run.log')\n\t    if not directory_exists and opt.is_main:\n\t        options.print_options(opt)\n\t    tokenizer = transformers.T5Tokenizer.from_pretrained('t5-base', return_dict=False)\n", "    collator_function = src.data.Collator(opt.text_maxlength, tokenizer)\n\t    eval_examples = src.data.load_data(\n\t        opt.eval_data, \n\t        global_rank=opt.global_rank, #use the global rank and world size attibutes to split the eval set on multiple gpus\n\t        world_size=opt.world_size\n\t    )\n\t    eval_dataset = src.data.Dataset(\n\t        eval_examples, \n\t        opt.n_context, \n\t    )\n", "    eval_sampler = SequentialSampler(eval_dataset) \n\t    eval_dataloader = DataLoader(\n\t        eval_dataset, \n\t        sampler=eval_sampler, \n\t        batch_size=opt.per_gpu_batch_size,\n\t        num_workers=20, \n\t        collate_fn=collator_function\n\t    )\n\t    model_class = FiDT5\n\t    model = model_class.from_pretrained(opt.model_path)\n", "    model = model.to(opt.device)\n\t    logger.info(\"Start eval\")\n\t    exactmatch, total = evaluate(model, eval_dataset, eval_dataloader, tokenizer, opt)\n\t    logger.info(f'EM {100*exactmatch:.2f}, Total number of example {total}')\n\t    if opt.write_results and opt.is_main:\n\t        glob_path = Path(opt.checkpoint_dir) / opt.name / 'test_results'\n\t        eval_data_name = opt.eval_data.split('/')[-1].replace('.json', '')\n\t        write_path = Path(opt.checkpoint_dir) / opt.name / f'final_output_{eval_data_name}.json'\n\t        write_output(glob_path, write_path) \n\t    if opt.write_crossattention_scores:\n", "        src.util.save_distributed_dataset(eval_dataset.data, opt)\n"]}
{"filename": "DecAF/Knowledge/process_freebase.py", "chunked_list": ["# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.  \n\t# SPDX-License-Identifier: CC-BY-NC-4.0\n\timport os\n\timport jsonlines\n\timport csv\n\tfrom tqdm import tqdm\n\tfrom collections import defaultdict\n\tfrom multiprocessing import Pool\n\tfrom functools import partial\n\timport argparse\n", "from DecAF.Knowledge.linearize import Relation, convert_relation_to_text\n\tDATA_DIR = os.environ['DATA_DIR']\n\tparser = argparse.ArgumentParser(description='process Freebase')\n\tparser.add_argument('--data_dir', type=str, default=f'{DATA_DIR}/knowledge_source/Freebase')\n\targs = parser.parse_args()\n\t# disambuity for entity name\n\t# if the entity name has appeared in previous ids, we add indicator like \"v1\" \"v2\" to the name\n\tname_dir = os.path.join(args.data_dir, \"id2name_parts\")\n\toutput_dir = os.path.join(args.data_dir, \"id2name_parts_disamb\")\n\tif not os.path.exists(output_dir):\n", "    os.makedirs(output_dir)\n\t    name_num_dict = defaultdict(int)\n\t    file_list = os.listdir(name_dir)\n\t    file_list.sort()\n\t    for file_name in tqdm(file_list):\n\t        print(file_name)\n\t        id_name_list = []\n\t        with open(os.path.join(name_dir, file_name), 'r') as rf:\n\t            data_input = csv.reader(rf, delimiter=\"\\t\")\n\t            for row in data_input:\n", "                if row[2] not in name_num_dict:\n\t                    id_name_list.append(row)\n\t                    name_num_dict[row[2]] += 1\n\t                else:\n\t                    new_row = row[:2] + [row[2] + \" v\" + str(name_num_dict[row[2]])]\n\t                    id_name_list.append(new_row)\n\t                    name_num_dict[row[2]] += 1\n\t        # save the list of rows to a new tsv file\n\t        with open(os.path.join(output_dir, file_name), 'w') as wf:    \n\t            data_output = csv.writer(wf, delimiter=\"\\t\")\n", "            for row in id_name_list:\n\t                data_output.writerow(row)\n\t# load Freebase entity name\n\tname_dir = os.path.join(args.data_dir, \"id2name_parts_disamb\")\n\tid2name_dict = {}\n\tfor file_name in tqdm(os.listdir(name_dir)):\n\t    with open(os.path.join(name_dir, file_name), 'r') as rf:\n\t        data_input = csv.reader(rf, delimiter=\"\\t\")\n\t        for row in data_input:\n\t            id2name_dict[row[0]] = row[2]\n", "print(\"number of entities with names: \", len(id2name_dict))\n\t# load topic and type information from Freebase\n\ttopic_dir = os.path.join(args.data_dir, \"topic_entities_parts\")\n\tid2topic_dict = defaultdict(set)\n\tfile_list = os.listdir(topic_dir)\n\t# sort file list\n\tfile_list.sort()\n\tfor file_name in tqdm(file_list):\n\t    with open(os.path.join(topic_dir, file_name), 'r') as rf:\n\t        for row in rf:\n", "            row = row.strip().split(\"\\t\")\n\t            if len(row) != 3:\n\t                continue\n\t            if \"web\" in row[1] or \"type.object.name\" in row[1]:\n\t                continue\n\t            topic = row[1].split(\".\")[-1].replace('.', ' ').replace('_', ' ')\n\t            topic += \" : \" + row[2].replace('.', ' ').replace('_', ' ')\n\t            id2topic_dict[row[0]].add(topic)\n\tprint(\"number of entities with topics: \", len(id2topic_dict))\n\t# transform each entity centric 1-hop subgraph (only out relations) as one passage\n", "def transform_triples_group_woid(process_idx, num_process, triple_dir, save_dir):\n\t    # do not keep CVT node id\n\t    for file_name in tqdm(os.listdir(triple_dir)):\n\t        file_id = int(file_name.split(\"-\")[-1])\n\t        if file_id % num_process != process_idx:\n\t            continue\n\t        file_path = os.path.join(triple_dir, file_name)\n\t        grouped_entity_triples = defaultdict(list)\n\t        with open(file_path, \"r\") as rf:\n\t            for row in rf:\n", "                if len(row.strip().split(\"\\t\")) != 3:\n\t                    continue\n\t                triple = Relation(row)\n\t                if triple.should_ignore(id2name_dict):\n\t                    continue\n\t                if triple.obj.startswith(\"m\") and triple.obj not in id2name_dict:\n\t                    continue\n\t                subj = triple.subj\n\t                if triple.subj not in id2name_dict:\n\t                    triple.subj = \"\"\n", "                grouped_entity_triples[subj].append(convert_relation_to_text(triple, id2name_dict))\n\t        with jsonlines.open(os.path.join(save_dir, file_name) + \".jsonl\", \"w\") as wf:\n\t            data_id = 0\n\t            for key in grouped_entity_triples:\n\t                if key in id2name_dict:\n\t                    name_key = id2name_dict[key]\n\t                    title = name_key + \" \" + key\n\t                else:\n\t                    name_key = \"\"\n\t                    title = key\n", "                contents = \" \".join(grouped_entity_triples[key])\n\t                # add topic information\n\t                if key in id2topic_dict:\n\t                    topics = id2topic_dict[key]\n\t                    topics = [name_key + \" \" + topic + \" .\" for topic in topics]\n\t                    topics_contents = \" \".join(topics)\n\t                    contents += \" \" + topics_contents\n\t                contents = contents.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n\t                title = title.replace(\"\\t\", \" \").replace(\"\\n\", \" \")\n\t                # split contents by 100 words each chunk\n", "                contents_split = contents.split(\" \")\n\t                contents_chunks = [\" \".join(contents_split[i:i+100]) for i in range(0, len(contents_split), 100)]\n\t                for contents_chunk in contents_chunks:\n\t                    data_output_i = {\"id\": \"freebase-file{}doc{}\".format(file_id, data_id),\n\t                                    \"contents\": contents_chunk,\n\t                                    \"title\": title}\n\t                    wf.write(data_output_i)\n\t                    data_id += 1\n\t# Multi-process data processing\n\tdef transform_triples_multip(triple_dir, save_dir):\n", "    num_process = 10\n\t    pool = Pool(num_process)\n\t    sampleData = [x for x in range(num_process)]\n\t    transform_triples_part = partial(transform_triples_group_woid, \n\t                                     triple_dir=triple_dir, \n\t                                     num_process=num_process, \n\t                                     save_dir=save_dir)\n\t    pool.map(transform_triples_part, sampleData)\n\t    pool.close()\n\t    pool.join()\n", "triple_dir = os.path.join(args.data_dir, \"triple_edges_parts\")\n\tsave_dir = os.path.join(args.data_dir, \"processed/document\")\n\tos.makedirs(save_dir, exist_ok=True)\n\ttransform_triples_multip(triple_dir, save_dir)"]}
{"filename": "DecAF/Knowledge/linearize.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates.\n\t# Modifications Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\timport os\n\timport csv\n\tfrom tqdm import tqdm\n\tfrom collections import defaultdict\n", "class Relation:\n\t    def __init__(self, line):\n\t        if line is None:\n\t            self.subj = self.rel = self.obj = None\n\t            return\n\t        e1, rel, e2 = line.strip().split(\"\\t\")\n\t        self.subj = e1\n\t        self.rel = rel\n\t        self.obj = e2\n\t    def __hash__(self):\n", "        return hash((self.subj, self.rel, self.obj))\n\t    def _filter_relation(self):\n\t        relation = self.rel\n\t        if relation == \"type.object.name\":\n\t            return True\n\t        return False\n\t    def should_ignore(self, id2name_dict):\n\t        if self._filter_relation():\n\t            return True\n\t        return False\n", "    def __repr__(self):\n\t        return f\"Subj: {self.subj}; Rel: {self.rel}; Obj: {self.obj}\"\n\tdef convert_relation_to_text(relation, entity_names):\n\t    if isinstance(relation, Relation):\n\t        subj, rel, obj = relation.subj, relation.rel, relation.obj\n\t    else:\n\t        subj, rel, obj = relation\n\t    # subject\n\t    if subj in entity_names:\n\t        subj_surface = entity_names[subj]\n", "    else:\n\t        subj_surface = subj\n\t    # object\n\t    if obj in entity_names:\n\t        obj_surface = entity_names[obj]\n\t    else:\n\t        obj_surface = obj\n\t    # relation\n\t    # e.g. film.film.other_crew\n\t    # replace '.' and '_' with ' '\n", "    rel_surface = rel.replace('.', ' ')\n\t    rel_surface = rel_surface.replace('_', ' ')\n\t    return ' '.join([subj_surface, rel_surface, obj_surface, '.'])\n\t# replace \"{name} v2\" to \"{name}\"\n\tdef get_raw_name(name_wversion):\n\t    dict_name = name_wversion.split(\" \")\n\t    if dict_name[-1].startswith(\"v\") and dict_name[-1][1:].isnumeric():\n\t        dict_name = \" \".join(dict_name[:-1])\n\t    else:\n\t        dict_name = \" \".join(dict_name)\n", "    return dict_name\n\tdef load_nameid_dict(file_dir, lower):\n\t    print(\"Loading name2id and id2name dict...\")\n\t    name2id_dict = defaultdict(list)\n\t    id2name_dict = {}\n\t    for file in tqdm(os.listdir(file_dir)):\n\t        with open(os.path.join(file_dir, file), 'r') as rf:\n\t            data_input = csv.reader(rf, delimiter=\"\\t\")\n\t            for row in data_input:\n\t                if lower:\n", "                    procesed_name = row[2].lower()\n\t                else:\n\t                    procesed_name = row[2]\n\t                name2id_dict[procesed_name].append(row[0])\n\t                id2name_dict[row[0]] = procesed_name\n\t    return name2id_dict, id2name_dict"]}
