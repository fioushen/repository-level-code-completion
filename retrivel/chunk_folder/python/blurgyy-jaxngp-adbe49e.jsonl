{"filename": "deps/jax-tcnn/setup.py", "chunked_list": ["#!/usr/bin/env python\n\timport os\n\timport subprocess\n\tfrom setuptools import Extension, find_packages, setup\n\tfrom setuptools.command.build_ext import build_ext\n\tHERE = os.path.dirname(os.path.realpath(__file__))\n\tclass CMakeBuildExt(build_ext):\n\t    def build_extensions(self):\n\t        # First: configure CMake build\n\t        import platform\n", "        import sys\n\t        import sysconfig\n\t        import pybind11\n\t        # Work out the relevant Python paths to pass to CMake, adapted from the\n\t        # PyTorch build system\n\t        if platform.system() == \"Windows\":\n\t            cmake_python_library = \"{}/libs/python{}.lib\".format(\n\t                sysconfig.get_config_var(\"prefix\"),\n\t                sysconfig.get_config_var(\"VERSION\"),\n\t            )\n", "            if not os.path.exists(cmake_python_library):\n\t                cmake_python_library = \"{}/libs/python{}.lib\".format(\n\t                    sys.base_prefix,\n\t                    sysconfig.get_config_var(\"VERSION\"),\n\t                )\n\t        else:\n\t            cmake_python_library = \"{}/{}\".format(\n\t                sysconfig.get_config_var(\"LIBDIR\"),\n\t                sysconfig.get_config_var(\"INSTSONAME\"),\n\t            )\n", "        cmake_python_include_dir = sysconfig.get_path(\"include\")\n\t        install_dir = os.path.abspath(\n\t            os.path.dirname(self.get_ext_fullpath(\"dummy\"))\n\t        )\n\t        os.makedirs(install_dir, exist_ok=True)\n\t        cmake_args = [\n\t            \"-DCMAKE_INSTALL_PREFIX={}\".format(install_dir),\n\t            \"-DPython_EXECUTABLE={}\".format(sys.executable),\n\t            \"-DPython_LIBRARIES={}\".format(cmake_python_library),\n\t            \"-DPython_INCLUDE_DIRS={}\".format(cmake_python_include_dir),\n", "            \"-DCMAKE_BUILD_TYPE={}\".format(\n\t                \"Debug\" if self.debug else \"Release\"\n\t            ),\n\t            \"-DCMAKE_PREFIX_PATH={}\".format(pybind11.get_cmake_dir()),\n\t            \"-G Ninja\",\n\t        ] + os.environ[\"cmakeFlags\"].split()\n\t        os.makedirs(self.build_temp, exist_ok=True)\n\t        subprocess.check_call(\n\t            [\"cmake\", HERE] + cmake_args, cwd=self.build_temp\n\t        )\n", "        # Build all the extensions\n\t        super().build_extensions()\n\t        # Finally run install\n\t        subprocess.check_call(\n\t            [\"cmake\", \"--build\", \".\", \"--target\", \"install\"],\n\t            cwd=self.build_temp,\n\t        )\n\t    def build_extension(self, ext):\n\t        target_name = ext.name.split(\".\")[-1]\n\t        subprocess.check_call(\n", "            [\"cmake\", \"--build\", \".\", \"--target\", target_name],\n\t            cwd=self.build_temp,\n\t        )\n\textensions = [\n\t    Extension(\n\t        \"jax_tcnn.tcnnutils\",  # Python dotted name, whose final component should be a buildable target defined in CMakeLists.txt\n\t        [  # source paths, relative to this setup.py file\n\t            \"lib/ffi.cc\",\n\t            \"lib/impl/hashgrid.cu\",\n\t        ],\n", "    ),\n\t]\n\tsetup(\n\t    name=\"jax-tcnn\",\n\t    author=\"blurgyy\",\n\t    package_dir={\"\": \"src\"},\n\t    packages=find_packages(\"src\"),\n\t    include_package_data=True,\n\t    install_requires=[\"jax\", \"jaxlib\", \"chex\"],\n\t    ext_modules=extensions,\n", "    cmdclass={\"build_ext\": CMakeBuildExt},\n\t)\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/__init__.py", "chunked_list": ["from .hashgrid_tcnn import HashGridMetadata, hashgrid_encode\n\t__all__ = [\n\t    \"HashGridMetadata\",\n\t    \"hashgrid_encode\"\n\t]\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/impl.py", "chunked_list": ["import dataclasses\n\timport functools\n\tfrom typing import Tuple\n\timport chex\n\timport jax\n\tfrom jax.interpreters import mlir, xla\n\tfrom jax.lib import xla_client\n\tfrom . import abstract, lowering\n\tfrom .. import tcnnutils\n\t# register GPU XLA custom calls\n", "for name, value in tcnnutils.get_hashgrid_registrations().items():\n\t    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\t# primitives\n\thashgrid_encode_p = jax.core.Primitive(\"hashgridüèÅ\")\n\thashgrid_encode_p.multiple_results = True\n\thashgrid_encode_p.def_impl(functools.partial(xla.apply_primitive, hashgrid_encode_p))\n\thashgrid_encode_p.def_abstract_eval(abstract.hashgrid_encode_abstract)\n\thashgrid_encode_backward_p = jax.core.Primitive(\"hashgridüèÅbackward\")\n\thashgrid_encode_backward_p.multiple_results = True\n\thashgrid_encode_backward_p.def_impl(functools.partial(xla.apply_primitive, hashgrid_encode_backward_p))\n", "hashgrid_encode_backward_p.def_abstract_eval(abstract.hashgrid_encode_backward_abstract)\n\t# lowering rules\n\tmlir.register_lowering(\n\t    prim=hashgrid_encode_p,\n\t    rule=lowering.hashgrid_encode_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n\tmlir.register_lowering(\n\t    prim=hashgrid_encode_backward_p,\n\t    rule=lowering.hashgrid_encode_backward_lowering_rule,\n", "    platform=\"gpu\",\n\t)\n\t@jax.tree_util.register_pytree_node_class\n\t@dataclasses.dataclass(frozen=True, kw_only=True)\n\tclass HashGridMetadata:\n\t    # number of levels, \"n_levels\" in tcnn\n\t    L: int\n\t    # number of features that each level should output, \"n_features_per_level\" in tcnn\n\t    F: int\n\t    # coarsest resolution, \"base_resolution\" in tcnn\n", "    N_min: int\n\t    # scale factor between consecutive levels\n\t    per_level_scale: float\n\t    def tree_flatten(self):\n\t        children = ()\n\t        aux = (self.L, self.F, self.N_min, self.per_level_scale)\n\t        return children, aux\n\t    @classmethod\n\t    def tree_unflatten(cls, aux, children):\n\t        L, F, N_min, per_level_scale = aux\n", "        return cls(\n\t            L=L,\n\t            F=F,\n\t            N_min=N_min,\n\t            per_level_scale=per_level_scale,\n\t        )\n\t@functools.partial(jax.custom_vjp, nondiff_argnums=[0])\n\tdef __hashgrid_encode(\n\t    desc: HashGridMetadata,\n\t    offset_table_data: jax.Array,\n", "    coords_rm: jax.Array,\n\t    params: jax.Array,\n\t):\n\t    encoded_coords_rm, dy_dcoords_rm = hashgrid_encode_p.bind(\n\t        offset_table_data,\n\t        coords_rm,\n\t        params,\n\t        L=desc.L,\n\t        F=desc.F,\n\t        N_min=desc.N_min,\n", "        per_level_scale=desc.per_level_scale,\n\t    )\n\t    return encoded_coords_rm, dy_dcoords_rm\n\tdef __hashgrid_encode_fwd(\n\t    desc: HashGridMetadata,\n\t    offset_table_data: jax.Array,\n\t    coords_rm: jax.Array,\n\t    params: jax.Array,\n\t):\n\t    primal_outputs = __hashgrid_encode(\n", "        desc=desc,\n\t        offset_table_data=offset_table_data,\n\t        coords_rm=coords_rm,\n\t        params=params,\n\t    )\n\t    encoded_coords_rm, dy_dcoords_rm = primal_outputs\n\t    aux = {\n\t        \"in.offset_table_data\": offset_table_data,\n\t        \"in.coords_rm\": coords_rm,\n\t        \"in.params\": params,\n", "        \"out.dy_dcoords_rm\": dy_dcoords_rm,\n\t    }\n\t    return primal_outputs, aux\n\tdef __hashgrid_encode_bwd(desc: HashGridMetadata, aux, grads):\n\t    dL_dy_rm, _ = grads\n\t    dL_dparams, dL_dcoords_rm = hashgrid_encode_backward_p.bind(\n\t        aux[\"in.offset_table_data\"],\n\t        aux[\"in.coords_rm\"],\n\t        aux[\"in.params\"],\n\t        dL_dy_rm,\n", "        aux[\"out.dy_dcoords_rm\"],\n\t        L=desc.L,\n\t        F=desc.F,\n\t        N_min=desc.N_min,\n\t        per_level_scale=desc.per_level_scale,\n\t    )\n\t    return None, dL_dcoords_rm, dL_dparams\n\t__hashgrid_encode.defvjp(\n\t    fwd=__hashgrid_encode_fwd,\n\t    bwd=__hashgrid_encode_bwd,\n", ")\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/lowering.py", "chunked_list": ["from jax.interpreters import mlir\n\tfrom jax.interpreters.mlir import ir\n\tfrom .. import tcnnutils\n\ttry:\n\t    from jaxlib.mhlo_helpers import custom_call\n\texcept ModuleNotFoundError:\n\t    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n\t    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n\t    from jaxlib.hlo_helpers import custom_call\n\t# helper function for mapping given shapes to their default mlir layouts\n", "def default_layouts(*shapes):\n\t    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\tdef hashgrid_encode_lowering_rule(\n\t    ctx: mlir.LoweringRule,\n\t    # arrays\n\t    offset_table_data: ir.Value,\n\t    coords_rm: ir.Value,\n\t    params: ir.Value,\n\t    # static args\n\t    L: int,\n", "    F: int,\n\t    N_min: int,\n\t    per_level_scale: float,\n\t):\n\t    dim, n_coords = ir.RankedTensorType(coords_rm.type).shape\n\t    n_params, _ = ir.RankedTensorType(params.type).shape\n\t    opaque = tcnnutils.make_hashgrid_descriptor(\n\t        n_coords,\n\t        L,\n\t        F,\n", "        N_min,\n\t        per_level_scale,\n\t    )\n\t    shapes = {\n\t        \"in.offset_table_data\": (L + 1,),\n\t        \"in.coords_rm\": (dim, n_coords),\n\t        \"in.params\": (n_params, F),\n\t        \"out.encoded_coords_rm\": (L * F, n_coords),\n\t        \"out.dy_dcoords_rm\": (dim * L * F, n_coords),\n\t    }\n", "    return custom_call(\n\t        call_target_name=\"hashgrid_encode\",\n\t        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"out.encoded_coords_rm\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.dy_dcoords_rm\"], ir.F32Type.get()),\n\t        ],\n\t        operands=[\n\t            offset_table_data,\n\t            coords_rm,\n\t            params,\n", "        ],\n\t        backend_config=opaque,\n\t        operand_layouts=default_layouts(\n\t            shapes[\"in.offset_table_data\"],\n\t            shapes[\"in.coords_rm\"],\n\t            shapes[\"in.params\"],\n\t        ),\n\t        result_layouts=default_layouts(\n\t            shapes[\"out.encoded_coords_rm\"],\n\t            shapes[\"out.dy_dcoords_rm\"],\n", "        ),\n\t    )\n\tdef hashgrid_encode_backward_lowering_rule(\n\t    ctx: mlir.LoweringRule,\n\t    offset_table_data: ir.Value,\n\t    coords_rm: ir.Value,\n\t    params: ir.Value,  # only for determining shape of dL_dparams\n\t    dL_dy_rm: ir.Value,\n\t    dy_dcoords_rm: ir.Value,\n\t    # static args\n", "    L: int,\n\t    F: int,\n\t    N_min: int,\n\t    per_level_scale: float,\n\t):\n\t    dim, n_coords = ir.RankedTensorType(coords_rm.type).shape\n\t    n_params, _ = ir.RankedTensorType(params.type).shape\n\t    opaque = tcnnutils.make_hashgrid_descriptor(\n\t        n_coords,\n\t        L,\n", "        F,\n\t        N_min,\n\t        per_level_scale,\n\t    )\n\t    shapes = {\n\t        \"in.offset_table_data\": (L + 1,),\n\t        \"in.coords_rm\": (dim, n_coords),\n\t        # \"in.params\": (n_params, F),\n\t        \"in.dL_dy_rm\": (L * F, n_coords),\n\t        \"in.dy_dcoords_rm\": (dim * L * F, n_coords),\n", "        \"out.dL_dparams\": (n_params, F),\n\t        \"out.dL_dcoords_rm\": (dim, n_coords),\n\t    }\n\t    return custom_call(\n\t        call_target_name=\"hashgrid_encode_backward\",\n\t        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"out.dL_dparams\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.dL_dcoords_rm\"], ir.F32Type.get()),\n\t        ],\n\t        operands=[\n", "            offset_table_data,\n\t            coords_rm,\n\t            dL_dy_rm,\n\t            dy_dcoords_rm\n\t        ],\n\t        backend_config=opaque,\n\t        result_layouts=default_layouts(\n\t            shapes[\"out.dL_dparams\"],\n\t            shapes[\"out.dL_dcoords_rm\"],\n\t        ),\n", "        operand_layouts=default_layouts(\n\t            shapes[\"in.offset_table_data\"],\n\t            shapes[\"in.coords_rm\"],\n\t            shapes[\"in.dL_dy_rm\"],\n\t            shapes[\"in.dy_dcoords_rm\"],\n\t        ),\n\t    )\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/abstract.py", "chunked_list": ["import chex\n\timport jax\n\timport jax.numpy as jnp\n\tdef hashgrid_encode_abstract(\n\t    # arrays\n\t    offset_table_data: jax.Array,\n\t    coords_rm: jax.Array,\n\t    params: jax.Array,\n\t    # static args\n\t    L: int,\n", "    F: int,\n\t    N_min: int,\n\t    per_level_scale: float,\n\t):\n\t    dim, n_coords = coords_rm.shape\n\t    if dim != 3:\n\t        raise NotImplementedError(\n\t            \"hashgrid encoding is only implemented for 3D coordinates, expected input coordinates to have shape ({}, n_coords), but got shape {}\".format(\n\t                dim, coords_rm.shape\n\t            )\n", "        )\n\t    n_params, _ = params.shape\n\t    chex.assert_shape(offset_table_data, (L + 1,))\n\t    chex.assert_shape(coords_rm, (dim, n_coords))\n\t    chex.assert_shape(params, (n_params, F))\n\t    chex.assert_scalar(L)\n\t    chex.assert_scalar(F)\n\t    chex.assert_scalar(N_min)\n\t    chex.assert_scalar(per_level_scale)\n\t    chex.assert_type([L, F, N_min], int)\n", "    chex.assert_type(per_level_scale, float)\n\t    offset_dtype = jax.dtypes.canonicalize_dtype(offset_table_data.dtype)\n\t    if offset_dtype != jnp.uint32:\n\t        raise RuntimeError(\n\t            \"hashgrid encoding expects `offset_table_data` (a prefix sum of the hash table sizes of each level) to be of type uint32, got {}\".format(offset_dtype)\n\t        )\n\t    coord_dtype = jax.dtypes.canonicalize_dtype(coords_rm.dtype)\n\t    if coord_dtype != jnp.float32:\n\t        raise NotImplementedError(\n\t            \"hashgrid encoding is only implemented for input coordinates of type float32, got {}\".format(\n", "                coord_dtype\n\t            )\n\t        )\n\t    param_dtype = jax.dtypes.canonicalize_dtype(params.dtype)\n\t    if param_dtype != jnp.float32:\n\t        raise NotImplementedError(\n\t            \"hashgrid encoding is only implemented for parameters of type float32, got {}\".format(\n\t                param_dtype\n\t            )\n\t        )\n", "    out_shapes = {\n\t        \"encoded_coords_rm\": (L * F, n_coords),\n\t        \"dy_dcoords_rm\": (dim * L * F, n_coords),\n\t    }\n\t    return (\n\t        jax.ShapedArray(shape=out_shapes[\"encoded_coords_rm\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"dy_dcoords_rm\"], dtype=jnp.float32),\n\t    )\n\tdef hashgrid_encode_backward_abstract(\n\t    offset_table_data: jax.ShapedArray,\n", "    coords_rm: jax.ShapedArray,\n\t    params: jax.ShapedArray,  # only for determining shape of dL_dparams\n\t    dL_dy_rm: jax.ShapedArray,\n\t    dy_dcoords_rm: jax.ShapedArray,\n\t    # static args\n\t    L: int,\n\t    F: int,\n\t    N_min: int,\n\t    per_level_scale: float,\n\t):\n", "    dim, n_coords = coords_rm.shape\n\t    if dim != 3:\n\t        raise NotImplementedError(\n\t            \"hashgrid encoding is only implemented for 3D coordinates, expected input coordinates to have shape ({}, n_coords), but got shape {}\".format(\n\t                dim, coords_rm.shape\n\t            )\n\t        )\n\t    n_params, _ = params.shape\n\t    chex.assert_shape(offset_table_data, (L + 1,))\n\t    chex.assert_shape(coords_rm, (dim, n_coords))\n", "    chex.assert_shape(params, (n_params, F))\n\t    chex.assert_shape(dL_dy_rm, (L*F, n_coords))\n\t    chex.assert_shape(dy_dcoords_rm, (dim*L*F, n_coords))\n\t    chex.assert_scalar(L)\n\t    chex.assert_scalar(F)\n\t    chex.assert_scalar(N_min)\n\t    chex.assert_scalar(per_level_scale)\n\t    chex.assert_type([L, F, N_min], int)\n\t    chex.assert_type(per_level_scale, float)\n\t    offset_dtype = jax.dtypes.canonicalize_dtype(offset_table_data.dtype)\n", "    if offset_dtype != jnp.uint32:\n\t        raise RuntimeError(\n\t            \"hashgrid encoding expects `offset_table_data` (a prefix sum of the hash table sizes of each level) to be of type uint32, got {}\".format(offset_dtype)\n\t        )\n\t    coord_dtype = jax.dtypes.canonicalize_dtype(coords_rm.dtype)\n\t    if coord_dtype != jnp.float32:\n\t        raise NotImplementedError(\n\t            \"hashgrid encoding is only implemented for input coordinates of type float32, got {}\".format(\n\t                coord_dtype\n\t            )\n", "        )\n\t    param_dtype = jax.dtypes.canonicalize_dtype(params.dtype)\n\t    if param_dtype != jnp.float32:\n\t        raise NotImplementedError(\n\t            \"hashgrid encoding is only implemented for parameters of type float32, got {}\".format(\n\t                param_dtype\n\t            )\n\t        )\n\t    out_shapes = {\n\t        \"dL_dparams\": (n_params, F),\n", "        \"dL_dcoords_rm\": (dim, n_coords),\n\t    }\n\t    return (\n\t        jax.ShapedArray(shape=out_shapes[\"dL_dparams\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"dL_dcoords_rm\"], dtype=jnp.float32),\n\t    )\n"]}
{"filename": "deps/jax-tcnn/src/jaxtcnn/hashgrid_tcnn/__init__.py", "chunked_list": ["import jax\n\tfrom .impl import HashGridMetadata, __hashgrid_encode\n\tdef hashgrid_encode(\n\t    desc: HashGridMetadata,\n\t    offset_table_data: jax.Array,\n\t    coords_rm: jax.Array,\n\t    params: jax.Array,\n\t):\n\t    encoded_coords_rm, dy_dcoords_rm = __hashgrid_encode(\n\t        desc,\n", "        offset_table_data,\n\t        coords_rm,\n\t        params,\n\t    )\n\t    return encoded_coords_rm\n"]}
{"filename": "deps/volume-rendering-jax/setup.py", "chunked_list": ["#!/usr/bin/env python\n\timport os\n\timport subprocess\n\tfrom setuptools import Extension, find_packages, setup\n\tfrom setuptools.command.build_ext import build_ext\n\tHERE = os.path.dirname(os.path.realpath(__file__))\n\tclass CMakeBuildExt(build_ext):\n\t    def build_extensions(self):\n\t        # First: configure CMake build\n\t        import platform\n", "        import sys\n\t        import sysconfig\n\t        import pybind11\n\t        # Work out the relevant Python paths to pass to CMake, adapted from the\n\t        # PyTorch build system\n\t        if platform.system() == \"Windows\":\n\t            cmake_python_library = \"{}/libs/python{}.lib\".format(\n\t                sysconfig.get_config_var(\"prefix\"),\n\t                sysconfig.get_config_var(\"VERSION\"),\n\t            )\n", "            if not os.path.exists(cmake_python_library):\n\t                cmake_python_library = \"{}/libs/python{}.lib\".format(\n\t                    sys.base_prefix,\n\t                    sysconfig.get_config_var(\"VERSION\"),\n\t                )\n\t        else:\n\t            cmake_python_library = \"{}/{}\".format(\n\t                sysconfig.get_config_var(\"LIBDIR\"),\n\t                sysconfig.get_config_var(\"INSTSONAME\"),\n\t            )\n", "        cmake_python_include_dir = sysconfig.get_path(\"include\")\n\t        install_dir = os.path.abspath(\n\t            os.path.dirname(self.get_ext_fullpath(\"dummy\"))\n\t        )\n\t        os.makedirs(install_dir, exist_ok=True)\n\t        cmake_args = [\n\t            \"-DCMAKE_INSTALL_PREFIX={}\".format(install_dir),\n\t            \"-DPython_EXECUTABLE={}\".format(sys.executable),\n\t            \"-DPython_LIBRARIES={}\".format(cmake_python_library),\n\t            \"-DPython_INCLUDE_DIRS={}\".format(cmake_python_include_dir),\n", "            \"-DCMAKE_BUILD_TYPE={}\".format(\n\t                \"Debug\" if self.debug else \"Release\"\n\t            ),\n\t            \"-DCMAKE_PREFIX_PATH={}\".format(pybind11.get_cmake_dir()),\n\t            \"-G Ninja\",\n\t        ]\n\t        os.makedirs(self.build_temp, exist_ok=True)\n\t        subprocess.check_call(\n\t            [\"cmake\", HERE] + cmake_args, cwd=self.build_temp\n\t        )\n", "        # Build all the extensions\n\t        super().build_extensions()\n\t        # Finally run install\n\t        subprocess.check_call(\n\t            [\"cmake\", \"--build\", \".\", \"--target\", \"install\"],\n\t            cwd=self.build_temp,\n\t        )\n\t    def build_extension(self, ext):\n\t        target_name = ext.name.split(\".\")[-1]\n\t        subprocess.check_call(\n", "            [\"cmake\", \"--build\", \".\", \"--target\", target_name],\n\t            cwd=self.build_temp,\n\t        )\n\textensions = [\n\t    Extension(\n\t        \"volrendjax.volrendutils_cuda\",  # Python dotted name, whose final component should be a buildable target defined in CMakeLists.txt\n\t        [  # source paths, relative to this setup.py file\n\t            \"lib/ffi.cc\",\n\t            \"lib/impl/packbits.cu\",\n\t            \"lib/impl/marching.cu\",\n", "            \"lib/impl/integrating.cu\",\n\t        ],\n\t    ),\n\t]\n\tsetup(\n\t    name=\"volume-rendering-jax\",\n\t    author=\"blurgyy\",\n\t    package_dir={\"\": \"src\"},\n\t    packages=find_packages(\"src\"),\n\t    include_package_data=True,\n", "    install_requires=[\"jax\", \"jaxlib\", \"chex\"],\n\t    ext_modules=extensions,\n\t    cmdclass={\"build_ext\": CMakeBuildExt},\n\t)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/__init__.py", "chunked_list": ["from .packbits import packbits\n\tfrom .marching import march_rays, march_rays_inference\n\tfrom .morton3d import morton3d, morton3d_invert\n\tfrom .integrating import integrate_rays, integrate_rays_inference\n\t__all__ = [\n\t    \"integrate_rays\",\n\t    \"integrate_rays_inference\",\n\t    \"march_rays\",\n\t    \"march_rays_inference\",\n\t    \"morton3d\",\n", "    \"morton3d_invert\",\n\t    \"packbits\",\n\t]\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/impl.py", "chunked_list": ["import functools\n\timport jax\n\tfrom jax.interpreters import mlir, xla\n\tfrom jax.lib import xla_client\n\tfrom . import abstract, lowering\n\tfrom .. import volrendutils_cuda\n\t# register GPU XLA custom calls\n\tfor name, value in volrendutils_cuda.get_morton3d_registrations().items():\n\t    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\tmorton3d_p = jax.core.Primitive(\"morton3d‚ö°\")\n", "morton3d_p.multiple_results = False\n\tmorton3d_p.def_impl(functools.partial(xla.apply_primitive, morton3d_p))\n\tmorton3d_p.def_abstract_eval(abstract.morton3d_abstract)\n\tmorton3d_invert_p = jax.core.Primitive(\"morton3d‚ö°invert\")\n\tmorton3d_invert_p.multiple_results = False\n\tmorton3d_invert_p.def_impl(functools.partial(xla.apply_primitive, morton3d_invert_p))\n\tmorton3d_invert_p.def_abstract_eval(abstract.morton3d_invert_abstract)\n\t# register mlir lowering rules\n\tmlir.register_lowering(\n\t    prim=morton3d_p,\n", "    rule=lowering.morton3d_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n\tmlir.register_lowering(\n\t    prim=morton3d_invert_p,\n\t    rule=lowering.morton3d_invert_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/lowering.py", "chunked_list": ["from jax.interpreters import mlir\n\tfrom jax.interpreters.mlir import ir\n\tfrom .. import volrendutils_cuda\n\ttry:\n\t    from jaxlib.mhlo_helpers import custom_call\n\texcept ModuleNotFoundError:\n\t    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n\t    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n\t    from jaxlib.hlo_helpers import custom_call\n\t# helper function for mapping given shapes to their default mlir layouts\n", "def default_layouts(*shapes):\n\t    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\tdef morton3d_lowering_rule(\n\t    ctx: mlir.LoweringRule,\n\t    # input array\n\t    xyzs: ir.Value,\n\t):\n\t    length, _ = ir.RankedTensorType(xyzs.type).shape\n\t    opaque = volrendutils_cuda.make_morton3d_descriptor(length)\n\t    shapes = {\n", "        \"in.xyzs\": (length, 3),\n\t        \"out.idcs\": (length,),\n\t    }\n\t    return [custom_call(\n\t        call_target_name=\"morton3d\",\n\t        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"out.idcs\"], ir.IntegerType.get_unsigned(32)),\n\t        ],\n\t        operands=[\n\t            xyzs,\n", "        ],\n\t        backend_config=opaque,\n\t        operand_layouts=default_layouts(\n\t            shapes[\"in.xyzs\"],\n\t        ),\n\t        result_layouts=default_layouts(\n\t            shapes[\"out.idcs\"],\n\t        ),\n\t    )]\n\tdef morton3d_invert_lowering_rule(\n", "    ctx: mlir.LoweringRule,\n\t    # input array\n\t    idcs: ir.Value,\n\t):\n\t    length, = ir.RankedTensorType(idcs.type).shape\n\t    opaque = volrendutils_cuda.make_morton3d_descriptor(length)\n\t    shapes = {\n\t        \"in.idcs\": (length,),\n\t        \"out.xyzs\": (length, 3),\n\t    }\n", "    return [custom_call(\n\t        call_target_name=\"morton3d_invert\",\n\t        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"out.xyzs\"], ir.IntegerType.get_unsigned(32)),\n\t        ],\n\t        operands=[\n\t            idcs,\n\t        ],\n\t        backend_config=opaque,\n\t        operand_layouts=default_layouts(\n", "            shapes[\"in.idcs\"],\n\t        ),\n\t        result_layouts=default_layouts(\n\t            shapes[\"out.xyzs\"],\n\t        ),\n\t    )]\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/abstract.py", "chunked_list": ["import jax\n\timport jax.numpy as jnp\n\t# jit rules\n\tdef morton3d_abstract(\n\t    # input array\n\t    xyzs: jax.ShapedArray,\n\t):\n\t    length, _ = xyzs.shape\n\t    dtype = jax.dtypes.canonicalize_dtype(xyzs.dtype)\n\t    if dtype != jnp.uint32:\n", "        raise NotImplementedError(\n\t            \"morton3d is only implemented for input coordinates of type `jnp.uint32`, got {}\".format(\n\t                dtype,\n\t            )\n\t        )\n\t    out_shapes = {\n\t        \"idcs\": (length,),\n\t    }\n\t    return jax.ShapedArray(shape=out_shapes[\"idcs\"], dtype=jnp.uint32)\n\tdef morton3d_invert_abstract(\n", "    # input array\n\t    idcs: jax.ShapedArray,\n\t):\n\t    length, = idcs.shape\n\t    dtype = jax.dtypes.canonicalize_dtype(idcs.dtype)\n\t    if dtype != jnp.uint32:\n\t        raise NotImplementedError(\n\t            \"morton3d_invert is only implemented for input indices of type `jnp.uint32`, got {}\".format(\n\t                dtype,\n\t            )\n", "        )\n\t    out_shapes = {\n\t        \"xyzs\": (length, 3),\n\t    }\n\t    return jax.ShapedArray(shape=out_shapes[\"xyzs\"], dtype=jnp.uint32)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/morton3d/__init__.py", "chunked_list": ["import jax\n\tfrom . import impl\n\tdef morton3d(xyzs: jax.Array):\n\t    return impl.morton3d_p.bind(xyzs)\n\tdef morton3d_invert(idcs: jax.Array):\n\t    return impl.morton3d_invert_p.bind(idcs)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/impl.py", "chunked_list": ["import functools\n\tfrom typing import Tuple\n\timport jax\n\tfrom jax.interpreters import mlir, xla\n\tfrom jax.lib import xla_client\n\tfrom . import abstract, lowering\n\tfrom .. import volrendutils_cuda\n\t# register GPU XLA custom calls\n\tfor name, value in volrendutils_cuda.get_integrating_registrations().items():\n\t    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n", "# primitives\n\tintegrate_rays_p = jax.core.Primitive(\"integrate_raysüé®\")\n\tintegrate_rays_p.multiple_results = True\n\tintegrate_rays_p.def_impl(functools.partial(xla.apply_primitive, integrate_rays_p))\n\tintegrate_rays_p.def_abstract_eval(abstract.integrate_rays_abstract)\n\tintegrate_rays_bwd_p = jax.core.Primitive(\"integrate_raysüé®backward\")\n\tintegrate_rays_bwd_p.multiple_results = True\n\tintegrate_rays_bwd_p.def_impl(functools.partial(xla.apply_primitive, integrate_rays_bwd_p))\n\tintegrate_rays_bwd_p.def_abstract_eval(abstract.integrate_rays_backward_abstract)\n\tintegrate_rays_inference_p = jax.core.Primitive(\"integrate_raysüé®inference\")\n", "integrate_rays_inference_p.multiple_results = True\n\tintegrate_rays_inference_p.def_impl(functools.partial(xla.apply_primitive, integrate_rays_inference_p))\n\tintegrate_rays_inference_p.def_abstract_eval(abstract.integrate_rays_inference_abstract)\n\t# register mlir lowering rules\n\tmlir.register_lowering(\n\t    prim=integrate_rays_p,\n\t    rule=lowering.integrate_rays_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n\tmlir.register_lowering(\n", "    prim=integrate_rays_bwd_p,\n\t    rule=lowering.integrate_rays_backward_lowring_rule,\n\t    platform=\"gpu\",\n\t)\n\tmlir.register_lowering(\n\t    prim=integrate_rays_inference_p,\n\t    rule=lowering.integrate_rays_inference_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n\t@jax.custom_vjp\n", "def __integrate_rays(\n\t    near_distance: float,\n\t    rays_sample_startidx: jax.Array,\n\t    rays_n_samples: jax.Array,\n\t    bgs: jax.Array,\n\t    dss: jax.Array,\n\t    z_vals: jax.Array,\n\t    drgbs: jax.Array,\n\t) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array]:\n\t    bgs = jax.numpy.broadcast_to(bgs, (rays_sample_startidx.shape[0], 3))\n", "    measured_batch_size, final_rgbds, final_opacities = integrate_rays_p.bind(\n\t        rays_sample_startidx,\n\t        rays_n_samples,\n\t        bgs,\n\t        dss,\n\t        z_vals,\n\t        drgbs,\n\t    )\n\t    return measured_batch_size, final_rgbds, final_opacities\n\tdef __fwd_integrate_rays(\n", "    near_distance: float,\n\t    rays_sample_startidx: jax.Array,\n\t    rays_n_samples: jax.Array,\n\t    bgs: jax.Array,\n\t    dss: jax.Array,\n\t    z_vals: jax.Array,\n\t    drgbs: jax.Array,\n\t):\n\t    bgs = jax.numpy.broadcast_to(bgs, (rays_sample_startidx.shape[0], 3))\n\t    primal_outputs = __integrate_rays(\n", "        near_distance=near_distance,\n\t        rays_sample_startidx=rays_sample_startidx,\n\t        rays_n_samples=rays_n_samples,\n\t        bgs=bgs,\n\t        dss=dss,\n\t        z_vals=z_vals,\n\t        drgbs=drgbs,\n\t    )\n\t    measured_batch_size, final_rgbds, final_opacities = primal_outputs\n\t    aux = {\n", "        \"in.near_distance\": near_distance,\n\t        \"in.rays_sample_startidx\": rays_sample_startidx,\n\t        \"in.rays_n_samples\": rays_n_samples,\n\t        \"in.bgs\": bgs,\n\t        \"in.dss\": dss,\n\t        \"in.z_vals\": z_vals,\n\t        \"in.drgbs\": drgbs,\n\t        \"out.measured_batch_size\": measured_batch_size,\n\t        \"out.final_rgbds\": final_rgbds,\n\t        \"out.final_opacities\": final_opacities,\n", "    }\n\t    return primal_outputs, aux\n\tdef __bwd_integrate_rays(aux, grads):\n\t    _, dL_dfinal_rgbds, dL_dfinal_opacities = grads  # dL_dfinal_rgbds should be zeros everywhere\n\t    dL_dbgs, dL_dz_vals, dL_ddrgbs = integrate_rays_bwd_p.bind(\n\t        aux[\"in.rays_sample_startidx\"],\n\t        aux[\"in.rays_n_samples\"],\n\t        aux[\"in.bgs\"],\n\t        aux[\"in.dss\"],\n\t        aux[\"in.z_vals\"],\n", "        aux[\"in.drgbs\"],\n\t        aux[\"out.final_rgbds\"],\n\t        aux[\"out.final_opacities\"],\n\t        dL_dfinal_rgbds,\n\t        near_distance=aux[\"in.near_distance\"],\n\t    )\n\t    return (\n\t        # The first primal input is `near_distance`, a static argument, return no gradient for it.\n\t        None,\n\t        # The next 2 primal inputs are integer-valued arrays (`rays_sample_startidx`,\n", "        # `rays_n_samples`), return no gradient for them.\n\t        # REF:\n\t        #   <https://jax.readthedocs.io/en/latest/jep/4008-custom-vjp-update.html#what-to-update>:\n\t        #   Wherever we used to use nondiff_argnums for array values, we should just pass those as\n\t        #   regular arguments.  In the bwd rule, we need to produce values for them, but we can just\n\t        #   produce `None` values to indicate there‚Äôs no corresponding gradient value.\n\t        None, None,\n\t        # 4-th primal input is `dss`, no gradient\n\t        None,\n\t        # gradients for background colors, z_vals and model predictions (densites and rgbs)\n", "        dL_dbgs, dL_dz_vals, dL_ddrgbs\n\t    )\n\t__integrate_rays.defvjp(\n\t    fwd=__fwd_integrate_rays,\n\t    bwd=__bwd_integrate_rays,\n\t)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/lowering.py", "chunked_list": ["from jax.interpreters import mlir\n\tfrom jax.interpreters.mlir import ir\n\tfrom .. import volrendutils_cuda\n\ttry:\n\t    from jaxlib.mhlo_helpers import custom_call\n\texcept ModuleNotFoundError:\n\t    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n\t    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n\t    from jaxlib.hlo_helpers import custom_call\n\t# helper function for mapping given shapes to their default mlir layouts\n", "def default_layouts(*shapes):\n\t    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\tdef integrate_rays_lowering_rule(\n\t    ctx: mlir.LoweringRuleContext,\n\t    rays_sample_startidx: ir.Value,\n\t    rays_n_samples: ir.Value,\n\t    bgs: ir.Value,\n\t    dss: ir.Value,\n\t    z_vals: ir.Value,\n\t    drgbs: ir.Value,\n", "):\n\t    n_rays, = ir.RankedTensorType(rays_sample_startidx.type).shape\n\t    total_samples, = ir.RankedTensorType(z_vals.type).shape\n\t    opaque = volrendutils_cuda.make_integrating_descriptor(n_rays, total_samples)\n\t    shapes = {\n\t        \"in.rays_sample_startidx\": (n_rays,),\n\t        \"in.rays_n_samples\": (n_rays,),\n\t        \"in.bgs\": (n_rays, 3),\n\t        \"in.dss\": (total_samples,),\n\t        \"in.z_vals\": (total_samples,),\n", "        \"in.drgbs\": (total_samples, 4),\n\t        \"helper.measured_batch_size\": (1,),\n\t        \"out.final_rgbds\": (n_rays, 4),\n\t        \"out.final_opacities\": (n_rays,),\n\t    }\n\t    return custom_call(\n\t        call_target_name=\"integrate_rays\",\n\t        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"helper.measured_batch_size\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.final_rgbds\"], ir.F32Type.get()),\n", "            ir.RankedTensorType.get(shapes[\"out.final_opacities\"], ir.F32Type.get()),\n\t        ],\n\t        operands=[\n\t            rays_sample_startidx,\n\t            rays_n_samples,\n\t            bgs,\n\t            dss,\n\t            z_vals,\n\t            drgbs,\n\t        ],\n", "        backend_config=opaque,\n\t        operand_layouts=default_layouts(\n\t            shapes[\"in.rays_sample_startidx\"],\n\t            shapes[\"in.rays_n_samples\"],\n\t            shapes[\"in.bgs\"],\n\t            shapes[\"in.dss\"],\n\t            shapes[\"in.z_vals\"],\n\t            shapes[\"in.drgbs\"],\n\t        ),\n\t        result_layouts=default_layouts(\n", "            shapes[\"helper.measured_batch_size\"],\n\t            shapes[\"out.final_rgbds\"],\n\t            shapes[\"out.final_opacities\"],\n\t        ),\n\t    )\n\tdef integrate_rays_backward_lowring_rule(\n\t    ctx: mlir.LoweringRuleContext,\n\t    rays_sample_startidx: ir.Value,\n\t    rays_n_samples: ir.Value,\n\t    # original inputs\n", "    bgs: ir.Value,\n\t    dss: ir.Value,\n\t    z_vals: ir.Value,\n\t    drgbs: ir.Value,\n\t    # original outputs\n\t    final_rgbds: ir.Value,\n\t    final_opacities: ir.Value,\n\t    # gradient inputs\n\t    dL_dfinal_rgbds: ir.Value,\n\t    # static argument\n", "    near_distance: float,\n\t):\n\t    n_rays, = ir.RankedTensorType(rays_sample_startidx.type).shape\n\t    total_samples, = ir.RankedTensorType(z_vals.type).shape\n\t    opaque = volrendutils_cuda.make_integrating_backward_descriptor(n_rays, total_samples, near_distance)\n\t    shapes = {\n\t        \"in.rays_sample_startidx\": (n_rays,),\n\t        \"in.rays_n_samples\": (n_rays,),\n\t        \"in.bgs\": (n_rays, 3),\n\t        \"in.dss\": (total_samples,),\n", "        \"in.z_vals\": (total_samples,),\n\t        \"in.drgbs\": (total_samples, 4),\n\t        \"in.final_rgbds\": (n_rays, 4),\n\t        \"in.final_opacities\": (n_rays,),\n\t        \"in.dL_dfinal_rgbds\": (n_rays, 4),\n\t        \"out.dL_dbgs\": (n_rays, 3),\n\t        \"out.dL_dz_vals\": (total_samples,),\n\t        \"out.dL_ddrgbs\": (total_samples, 4),\n\t    }\n\t    return custom_call(\n", "        call_target_name=\"integrate_rays_backward\",\n\t        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"out.dL_dbgs\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.dL_dz_vals\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.dL_ddrgbs\"], ir.F32Type.get()),\n\t        ],\n\t        operands=[\n\t            rays_sample_startidx,\n\t            rays_n_samples,\n\t            bgs,\n", "            dss,\n\t            z_vals,\n\t            drgbs,\n\t            final_rgbds,\n\t            final_opacities,\n\t            dL_dfinal_rgbds,\n\t        ],\n\t        backend_config=opaque,\n\t        operand_layouts=default_layouts(\n\t            shapes[\"in.rays_sample_startidx\"],\n", "            shapes[\"in.rays_n_samples\"],\n\t            shapes[\"in.bgs\"],\n\t            shapes[\"in.dss\"],\n\t            shapes[\"in.z_vals\"],\n\t            shapes[\"in.drgbs\"],\n\t            shapes[\"in.final_rgbds\"],\n\t            shapes[\"in.final_opacities\"],\n\t            shapes[\"in.dL_dfinal_rgbds\"],\n\t        ),\n\t        result_layouts=default_layouts(\n", "            shapes[\"out.dL_dbgs\"],\n\t            shapes[\"out.dL_dz_vals\"],\n\t            shapes[\"out.dL_ddrgbs\"],\n\t        ),\n\t    )\n\tdef integrate_rays_inference_lowering_rule(\n\t    ctx: mlir.LoweringRuleContext,\n\t    rays_bg: ir.Value,\n\t    rays_rgbd: ir.Value,\n\t    rays_T: ir.Value,\n", "    n_samples: ir.Value,\n\t    indices: ir.Value,\n\t    dss: ir.Value,\n\t    z_vals: ir.Value,\n\t    drgbs: ir.Value,\n\t):\n\t    (n_total_rays, _) = ir.RankedTensorType(rays_rgbd.type).shape\n\t    (n_rays, march_steps_cap) = ir.RankedTensorType(dss.type).shape\n\t    opaque = volrendutils_cuda.make_integrating_inference_descriptor(n_total_rays, n_rays, march_steps_cap)\n\t    shapes = {\n", "        \"in.rays_bg\": (n_total_rays, 3),\n\t        \"in.rays_rgbd\": (n_total_rays, 4),\n\t        \"in.rays_T\": (n_total_rays,),\n\t        \"in.n_samples\": (n_rays,),\n\t        \"in.indices\": (n_rays,),\n\t        \"in.dss\": (n_rays, march_steps_cap),\n\t        \"in.z_vals\": (n_rays, march_steps_cap),\n\t        \"in.drgbs\": (n_rays, march_steps_cap, 4),\n\t        \"out.terminate_cnt\": (1,),\n\t        \"out.terminated\": (n_rays,),\n", "        \"out.rays_rgbd\": (n_rays, 4),\n\t        \"out.rays_T\": (n_rays,),\n\t    }\n\t    return custom_call(\n\t        call_target_name=\"integrate_rays_inference\",\n\t        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"out.terminate_cnt\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.terminated\"], ir.IntegerType.get_signless(1)),\n\t            ir.RankedTensorType.get(shapes[\"out.rays_rgbd\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.rays_T\"], ir.F32Type.get()),\n", "        ],\n\t        operands=[\n\t            rays_bg,\n\t            rays_rgbd,\n\t            rays_T,\n\t            n_samples,\n\t            indices,\n\t            dss,\n\t            z_vals,\n\t            drgbs,\n", "        ],\n\t        backend_config=opaque,\n\t        operand_layouts=default_layouts(\n\t            shapes[\"in.rays_bg\"],\n\t            shapes[\"in.rays_rgbd\"],\n\t            shapes[\"in.rays_T\"],\n\t            shapes[\"in.n_samples\"],\n\t            shapes[\"in.indices\"],\n\t            shapes[\"in.dss\"],\n\t            shapes[\"in.z_vals\"],\n", "            shapes[\"in.drgbs\"],\n\t        ),\n\t        result_layouts=default_layouts(\n\t            shapes[\"out.terminate_cnt\"],\n\t            shapes[\"out.terminated\"],\n\t            shapes[\"out.rays_rgbd\"],\n\t            shapes[\"out.rays_T\"],\n\t        ),\n\t    )\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/abstract.py", "chunked_list": ["import chex\n\timport jax\n\timport jax.numpy as jnp\n\t# jit rules\n\tdef integrate_rays_abstract(\n\t    rays_sample_startidx: jax.Array,\n\t    rays_n_samples: jax.Array,\n\t    bgs: jax.Array,\n\t    dss: jax.Array,\n\t    z_vals: jax.Array,\n", "    drgbs: jax.Array,\n\t):\n\t    (n_rays,), (total_samples,) = rays_sample_startidx.shape, dss.shape\n\t    chex.assert_shape([rays_sample_startidx, rays_n_samples], (n_rays,))\n\t    chex.assert_shape(bgs, (n_rays, 3))\n\t    chex.assert_shape(z_vals, (total_samples,))\n\t    chex.assert_shape(drgbs, (total_samples, 4))\n\t    dtype = jax.dtypes.canonicalize_dtype(drgbs.dtype)\n\t    if dtype != jnp.float32:\n\t        raise NotImplementedError(\n", "            \"integrate_rays is only implemented for input prediction (density, color) of `jnp.float32` type, got {}\".format(\n\t                dtype,\n\t            )\n\t        )\n\t    shapes = {\n\t        \"helper.measured_batch_size\": (1,),\n\t        \"out.final_rgbds\": (n_rays, 4),\n\t        \"out.final_opacities\": (n_rays,),\n\t    }\n\t    return (\n", "        jax.ShapedArray(shape=shapes[\"helper.measured_batch_size\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=shapes[\"out.final_rgbds\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=shapes[\"out.final_opacities\"], dtype=jnp.float32),\n\t    )\n\tdef integrate_rays_backward_abstract(\n\t    rays_sample_startidx: jax.Array,\n\t    rays_n_samples: jax.Array,\n\t    # original inputs\n\t    bgs: jax.Array,\n\t    dss: jax.Array,\n", "    z_vals: jax.Array,\n\t    drgbs: jax.Array,\n\t    # original outputs\n\t    final_rgbds: jax.Array,\n\t    final_opacities: jax.Array,\n\t    # gradient inputs\n\t    dL_dfinal_rgbds: jax.Array,\n\t    # static argument\n\t    near_distance: float,\n\t):\n", "    (n_rays,), (total_samples,) = rays_sample_startidx.shape, dss.shape\n\t    chex.assert_shape([rays_sample_startidx, rays_n_samples, final_opacities], (n_rays,))\n\t    chex.assert_shape(bgs, (n_rays, 3))\n\t    chex.assert_shape(z_vals, (total_samples,))\n\t    chex.assert_shape(drgbs, (total_samples, 4))\n\t    chex.assert_shape([final_rgbds, dL_dfinal_rgbds], (n_rays, 4))\n\t    chex.assert_scalar_non_negative(near_distance)\n\t    dtype = jax.dtypes.canonicalize_dtype(drgbs.dtype)\n\t    if dtype != jnp.float32:\n\t        raise NotImplementedError(\n", "            \"integrate_rays is only implemented for input color of `jnp.float32` type, got {}\".format(\n\t                dtype,\n\t            )\n\t        )\n\t    out_shapes = {\n\t        \"dL_dbgs\": (n_rays, 3),\n\t        \"dL_dz_vals\": (total_samples,),\n\t        \"dL_ddrgbs\": (total_samples, 4),\n\t    }\n\t    return (\n", "        jax.ShapedArray(shape=out_shapes[\"dL_dbgs\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"dL_dz_vals\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"dL_ddrgbs\"], dtype=jnp.float32),\n\t    )\n\tdef integrate_rays_inference_abstract(\n\t    rays_bg: jax.ShapedArray,\n\t    rays_rgbd: jax.ShapedArray,\n\t    rays_T: jax.ShapedArray,\n\t    n_samples: jax.ShapedArray,\n\t    indices: jax.ShapedArray,\n", "    dss: jax.ShapedArray,\n\t    z_vals: jax.ShapedArray,\n\t    drgbs: jax.ShapedArray,\n\t):\n\t    (n_total_rays, _), (n_rays, march_steps_cap) = rays_rgbd.shape, dss.shape\n\t    chex.assert_shape(rays_bg, (n_total_rays, 3))\n\t    chex.assert_shape(rays_rgbd, (n_total_rays, 4))\n\t    chex.assert_shape(rays_T, (n_total_rays,))\n\t    chex.assert_shape([n_samples, indices], (n_rays,))\n\t    chex.assert_shape([dss, z_vals], (n_rays, march_steps_cap))\n", "    chex.assert_shape(drgbs, (n_rays, march_steps_cap, 4))\n\t    out_shapes = {\n\t        \"terminate_cnt\": (1,),\n\t        \"terminated\": (n_rays,),\n\t        \"rays_rgbd\": (n_rays, 4),\n\t        \"rays_T\": (n_rays,),\n\t    }\n\t    return (\n\t        jax.ShapedArray(shape=out_shapes[\"terminate_cnt\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=out_shapes[\"terminated\"], dtype=jnp.bool_),\n", "        jax.ShapedArray(shape=out_shapes[\"rays_rgbd\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"rays_T\"], dtype=jnp.float32),\n\t    )\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/integrating/__init__.py", "chunked_list": ["from typing import Tuple\n\timport jax\n\tfrom . import impl\n\t# this function is a wrapper on top of `__integrate_rays` which has custom vjp (wrapping the\n\t# `__integrate_rays` function because the @jax.custom_vjp decorator makes the decorated function's\n\t# docstring invisible to LSPs).\n\tdef integrate_rays(\n\t    near_distance: float,\n\t    rays_sample_startidx: jax.Array,\n\t    rays_n_samples: jax.Array,\n", "    bgs: jax.Array,\n\t    dss: jax.Array,\n\t    z_vals: jax.Array,\n\t    drgbs: jax.Array,\n\t) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array]:\n\t    \"\"\"\n\t    Inputs:\n\t        near_distance `float`: camera's near distance, samples behind the camera's near plane with\n\t                               non-negligible introduce a penalty on their densities\\n\"\n\t        rays_sample_startidx `[n_rays]`: i-th element is the index of the first sample in z_vals,\n", "                                         densities, and rgbs of the i-th ray\n\t        rays_n_samples `[n_rays]`: i-th element is the number of samples for the i-th ray\n\t        bgs `[n_rays, 3]`: background colors of each ray\n\t        dss [total_samples]: it means `ds`s, the notation `ds` comes from the article \"Local and\n\t                             global illumination in the volume rendering integral\" written by Nelson\n\t                             Max and Min Chen, 2005.  The product of `ds[i]` and `densities[i]`\n\t                             represents the probability of the ray terminates anywhere between\n\t                             `z_vals[i]` and `z_vals[i]+ds[i]`.\n\t                             Note that `ds[i]` is _not_ the same as `z_vals[i+1]-z_vals[i]` (though\n\t                             they may equal), because: (1) if empty spaces are skipped during ray\n", "                             marching, `z_vals[i+1]-z_vals[i]` may be very large, in which case it's\n\t                             no longer appropriate to assume the density is constant along this\n\t                             large segment; (2) `z_vals[i+1]` is not defined for the last sample.\n\t        z_vals [total_samples]: z_vals[i] is the distance of the i-th sample from the camera\n\t        drgbs [total_samples, 4]: density (1) and rgb (3) values along a ray\n\t    Returns:\n\t        measured_batch_size `uint`: total number of samples that got composited into output\n\t        final_rgbds `[n_rays, 4]`: integrated ray colors and estimated depths according to input\n\t                                   densities and rgbs.\n\t        final_opacities `[n_rays]`: accumulated opacities along each ray\n", "    \"\"\"\n\t    measured_batch_size, final_rgbds, final_opacities = impl.__integrate_rays(\n\t        near_distance=near_distance,\n\t        rays_sample_startidx=rays_sample_startidx,\n\t        rays_n_samples=rays_n_samples,\n\t        bgs=bgs,\n\t        dss=dss,\n\t        z_vals=z_vals,\n\t        drgbs=drgbs,\n\t    )\n", "    return measured_batch_size[0], final_rgbds, final_opacities\n\tdef integrate_rays_inference(\n\t    rays_bg: jax.Array,\n\t    rays_rgbd: jax.Array,\n\t    rays_T: jax.Array,\n\t    n_samples: jax.Array,\n\t    indices: jax.Array,\n\t    dss: jax.Array,\n\t    z_vals: jax.Array,\n\t    drgbs: jax.Array,\n", "):\n\t    \"\"\"\n\t    Inputs:\n\t        rays_bg `float` `[n_total_rays, 3]`: normalized background color of each ray in question\n\t        rays_rgbd `float` `[n_total_rays, 4]`: target array to write rendered colors and estimated\n\t                                               depths to\n\t        rays_T `float` `[n_total_rays]`: accumulated transmittance of each ray\n\t        n_samples `uint32` `[n_rays]`: output of ray marching, specifies how many samples are\n\t                                        generated for this ray at this iteration\n\t        indices `uint32` `[n_rays]`: values are in range [0, n_total_rays), specifies the location\n", "                                     in `rays_bg`, `rays_rgbd`, `rays_T`, and `rays_depth`\n\t                                     corresponding to this ray\n\t        dss `float` `[n_rays, march_steps_cap]`: each sample's `ds`\n\t        z_vals `float` `[n_rays, march_steps_cap]`: each sample's distance to its ray origin\n\t        drgbs `float` `[n_rays, march_steps_cap, 4]`: predicted density (1) and RGB (3) values from a NeRF model\n\t    Returns:\n\t        terminate_cnt `uint32`: number of rays that terminated this iteration\n\t        terminated `bool` `[n_rays]`: a binary mask, the i-th location being True means the i-th ray\n\t                                       has terminated\n\t        rays_rgbd `float` `[n_total_rays, 3]`: the input `rays_rgbd` with ray colors and estimated\n", "                                               depths updated\n\t        rays_T `float` `[n_total_rays]`: the input `rays_T` with transmittance values updated\n\t    \"\"\"\n\t    terminate_cnt, terminated, rays_rgbd_out, rays_T_out = impl.integrate_rays_inference_p.bind(\n\t        rays_bg,\n\t        rays_rgbd,\n\t        rays_T,\n\t        n_samples,\n\t        indices,\n\t        dss,\n", "        z_vals,\n\t        drgbs,\n\t    )\n\t    rays_rgbd = rays_rgbd.at[indices].set(rays_rgbd_out)\n\t    rays_T = rays_T.at[indices].set(rays_T_out)\n\t    return terminate_cnt[0], terminated, rays_rgbd, rays_T\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/impl.py", "chunked_list": ["import functools\n\timport jax\n\tfrom jax.interpreters import mlir, xla\n\tfrom jax.lib import xla_client\n\tfrom . import abstract, lowering\n\tfrom .. import volrendutils_cuda\n\t# register GPU XLA custom calls\n\tfor name, value in volrendutils_cuda.get_packbits_registrations().items():\n\t    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\tpackbits_p = jax.core.Primitive(\"packbitsüé±\")\n", "packbits_p.multiple_results = True\n\tpackbits_p.def_impl(functools.partial(xla.apply_primitive, packbits_p))\n\tpackbits_p.def_abstract_eval(abstract.pack_density_into_bits_abstract)\n\tmlir.register_lowering(\n\t    prim=packbits_p,\n\t    rule=lowering.packbits_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/lowering.py", "chunked_list": ["from jax.interpreters import mlir\n\tfrom jax.interpreters.mlir import ir\n\tfrom .. import volrendutils_cuda\n\ttry:\n\t    from jaxlib.mhlo_helpers import custom_call\n\texcept ModuleNotFoundError:\n\t    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n\t    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n\t    from jaxlib.hlo_helpers import custom_call\n\t# helper function for mapping given shapes to their default mlir layouts\n", "def default_layouts(*shapes):\n\t    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\tdef packbits_lowering_rule(\n\t    ctx: mlir.LoweringRule,\n\t    # input array\n\t    density_threshold: ir.Value,\n\t    density_grid: ir.Value,\n\t):\n\t    n_bits = ir.RankedTensorType(density_grid.type).shape[0]\n\t    n_bytes = n_bits // 8\n", "    opaque = volrendutils_cuda.make_packbits_descriptor(n_bytes)\n\t    shapes = {\n\t        \"in.density_threshold\": (n_bits,),\n\t        \"in.density_grid\": (n_bits,),\n\t        \"out.occupied_mask\": (n_bits,),\n\t        \"out.occupancy_bitfield\": (n_bytes,),\n\t    }\n\t    return custom_call(\n\t        call_target_name=\"pack_density_into_bits\",\n\t        out_types = [\n", "            ir.RankedTensorType.get(shapes[\"out.occupied_mask\"], ir.IntegerType.get_signless(1)),\n\t            ir.RankedTensorType.get(shapes[\"out.occupancy_bitfield\"], ir.IntegerType.get_unsigned(8)),\n\t        ],\n\t        operands=[\n\t            density_threshold,\n\t            density_grid,\n\t        ],\n\t        backend_config=opaque,\n\t        operand_layouts=default_layouts(\n\t            shapes[\"in.density_threshold\"],\n", "            shapes[\"in.density_grid\"],\n\t        ),\n\t        result_layouts=default_layouts(\n\t            shapes[\"out.occupied_mask\"],\n\t            shapes[\"out.occupancy_bitfield\"],\n\t        ),\n\t    )\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/abstract.py", "chunked_list": ["import chex\n\timport jax\n\timport jax.numpy as jnp\n\t# jit rules\n\tdef pack_density_into_bits_abstract(\n\t    # input array\n\t    density_threshold: jax.ShapedArray,\n\t    density_grid: jax.ShapedArray,\n\t):\n\t    chex.assert_rank([density_threshold, density_grid], 1)\n", "    chex.assert_shape(density_threshold, density_grid.shape)\n\t    n_bits = density_grid.shape[0]\n\t    if n_bits % 8 != 0:\n\t        raise ValueError(\n\t            \"pack_density_into_bits expects size of density grid to be divisible by 8, got {}\".format(\n\t                n_bits,\n\t            )\n\t        )\n\t    n_bytes = n_bits // 8\n\t    dtype = jax.dtypes.canonicalize_dtype(density_grid.dtype)\n", "    if dtype != jnp.float32:\n\t        raise NotImplementedError(\n\t            \"pack_density_into_bits is only implemented for densities of `jnp.float32` type, got {}\".format(\n\t                dtype,\n\t            )\n\t        )\n\t    out_shapes = {\n\t        \"occupied_mask\": (n_bits,),\n\t        \"occupancy_bitfield\": (n_bytes,),\n\t    }\n", "    return (\n\t        jax.ShapedArray(out_shapes[\"occupied_mask\"], jnp.bool_),\n\t        jax.ShapedArray(out_shapes[\"occupancy_bitfield\"], jnp.uint8),\n\t    )\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/packbits/__init__.py", "chunked_list": ["from typing import Tuple\n\timport jax\n\timport jax.numpy as jnp\n\tfrom . import impl\n\tdef packbits(\n\t    density_threshold: float,\n\t    density_grid: jax.Array,\n\t) -> Tuple[jax.Array, jax.Array]:\n\t    \"\"\"\n\t    Pack the given `density_grid` into a compact representation of type uint8, where each bit is\n", "    high if its corresponding density grid cell's density is larger than `density_threshold`, low\n\t    otherwise.\n\t    Inputs:\n\t        density_threshold `broadcastable to [N]`\n\t        density_grid `[N]`\n\t    Returns:\n\t        occ_mask `[N] bool`: boolean mask that indicates whether this grid is occupied\n\t        occ_bitfield `[N//8]`\n\t    \"\"\"\n\t    return impl.packbits_p.bind(\n", "        jnp.broadcast_to(density_threshold, density_grid.shape),\n\t        density_grid,\n\t    )\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/impl.py", "chunked_list": ["import functools\n\timport jax\n\tfrom jax.interpreters import mlir, xla\n\tfrom jax.lib import xla_client\n\tfrom . import abstract, lowering\n\tfrom .. import volrendutils_cuda\n\t# register GPU XLA custom calls\n\tfor name, value in volrendutils_cuda.get_marching_registrations().items():\n\t    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\tmarch_rays_p = jax.core.Primitive(\"march_raysüóß\")\n", "march_rays_p.multiple_results = True\n\tmarch_rays_p.def_impl(functools.partial(xla.apply_primitive, march_rays_p))\n\tmarch_rays_p.def_abstract_eval(abstract.march_rays_abstract)\n\tmarch_rays_inference_p = jax.core.Primitive(\"march_rays_inferenceüóß\")\n\tmarch_rays_inference_p.multiple_results = True\n\tmarch_rays_inference_p.def_impl(functools.partial(xla.apply_primitive, march_rays_inference_p))\n\tmarch_rays_inference_p.def_abstract_eval(abstract.march_rays_inference_abstract)\n\t# register mlir lowering rules\n\tmlir.register_lowering(\n\t    prim=march_rays_p,\n", "    rule=lowering.march_rays_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n\tmlir.register_lowering(\n\t    prim=march_rays_inference_p,\n\t    rule=lowering.march_rays_inference_lowering_rule,\n\t    platform=\"gpu\",\n\t)\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/lowering.py", "chunked_list": ["from jax.interpreters import mlir\n\tfrom jax.interpreters.mlir import ir\n\tfrom .. import volrendutils_cuda\n\ttry:\n\t    from jaxlib.mhlo_helpers import custom_call\n\texcept ModuleNotFoundError:\n\t    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n\t    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n\t    from jaxlib.hlo_helpers import custom_call\n\t# helper function for mapping given shapes to their default mlir layouts\n", "def default_layouts(*shapes):\n\t    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\tdef march_rays_lowering_rule(\n\t    ctx: mlir.LoweringRule,\n\t    # arrays\n\t    rays_o: ir.Value,\n\t    rays_d: ir.Value,\n\t    t_starts: ir.Value,\n\t    t_ends: ir.Value,\n\t    noises: ir.Value,\n", "    occupancy_bitfield: ir.Value,\n\t    # static args\n\t    total_samples: int,  # int\n\t    diagonal_n_steps: int,  # int\n\t    K: int,  # int\n\t    G: int,  # int\n\t    bound: float,  # float\n\t    stepsize_portion: float,  # float\n\t):\n\t    n_rays, _ = ir.RankedTensorType(rays_o.type).shape\n", "    opaque = volrendutils_cuda.make_marching_descriptor(\n\t        n_rays,\n\t        total_samples,\n\t        diagonal_n_steps,\n\t        K,\n\t        G,\n\t        bound,\n\t        stepsize_portion,\n\t    )\n\t    shapes = {\n", "        \"in.rays_o\": (n_rays, 3),\n\t        \"in.rays_d\": (n_rays, 3),\n\t        \"in.t_starts\": (n_rays,),\n\t        \"in.t_ends\": (n_rays,),\n\t        \"in.noises\": (n_rays,),\n\t        \"in.occupancy_bitfield\": (K*G*G*G//8,),\n\t        \"helper.next_sample_write_location\": (1,),\n\t        \"helper.number_of_exceeded_samples\": (1,),\n\t        \"helper.ray_is_valid\": (n_rays,),\n\t        \"out.rays_n_samples\": (n_rays,),\n", "        \"out.rays_sample_startidx\": (n_rays,),\n\t        \"out.idcs\": (total_samples,),\n\t        \"out.xyzs\": (total_samples, 3),\n\t        \"out.dirs\": (total_samples, 3),\n\t        \"out.dss\": (total_samples,),\n\t        \"out.z_vals\": (total_samples,),\n\t    }\n\t    return custom_call(\n\t        call_target_name=\"march_rays\",\n\t        out_types=[\n", "            ir.RankedTensorType.get(shapes[\"helper.next_sample_write_location\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"helper.number_of_exceeded_samples\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"helper.ray_is_valid\"], ir.IntegerType.get_signless(1)),\n\t            ir.RankedTensorType.get(shapes[\"out.rays_n_samples\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.rays_sample_startidx\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.idcs\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.xyzs\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.dirs\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.dss\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.z_vals\"], ir.F32Type.get()),\n", "        ],\n\t        operands=[\n\t            rays_o,\n\t            rays_d,\n\t            t_starts,\n\t            t_ends,\n\t            noises,\n\t            occupancy_bitfield,\n\t        ],\n\t        backend_config=opaque,\n", "        operand_layouts=default_layouts(\n\t            shapes[\"in.rays_o\"],\n\t            shapes[\"in.rays_d\"],\n\t            shapes[\"in.t_starts\"],\n\t            shapes[\"in.t_ends\"],\n\t            shapes[\"in.noises\"],\n\t            shapes[\"in.occupancy_bitfield\"],\n\t        ),\n\t        result_layouts=default_layouts(\n\t            shapes[\"helper.next_sample_write_location\"],\n", "            shapes[\"helper.number_of_exceeded_samples\"],\n\t            shapes[\"helper.ray_is_valid\"],\n\t            shapes[\"out.rays_n_samples\"],\n\t            shapes[\"out.rays_sample_startidx\"],\n\t            shapes[\"out.idcs\"],\n\t            shapes[\"out.xyzs\"],\n\t            shapes[\"out.dirs\"],\n\t            shapes[\"out.dss\"],\n\t            shapes[\"out.z_vals\"],\n\t        ),\n", "    )\n\tdef march_rays_inference_lowering_rule(\n\t    ctx: mlir.LoweringRule,\n\t    # arrays\n\t    rays_o: ir.BlockArgument,\n\t    rays_d: ir.BlockArgument,\n\t    t_starts: ir.BlockArgument,\n\t    t_ends: ir.BlockArgument,\n\t    occupancy_bitfield: ir.BlockArgument,\n\t    next_ray_index_in: ir.BlockArgument,\n", "    terminated: ir.BlockArgument,\n\t    indices_in: ir.BlockArgument,\n\t    # static args\n\t    diagonal_n_steps: int,\n\t    K: int,\n\t    G: int,\n\t    march_steps_cap: int,\n\t    bound: float,\n\t    stepsize_portion: float,\n\t):\n", "    (n_total_rays, _), (n_rays,) = ir.RankedTensorType(rays_o.type).shape, ir.RankedTensorType(terminated.type).shape\n\t    opaque = volrendutils_cuda.make_marching_inference_descriptor(\n\t        n_total_rays,\n\t        n_rays,\n\t        diagonal_n_steps,\n\t        K,\n\t        G,\n\t        march_steps_cap,\n\t        bound,\n\t        stepsize_portion,\n", "    )\n\t    shapes = {\n\t        \"in.rays_o\": (n_total_rays, 3),\n\t        \"in.rays_d\": (n_total_rays, 3),\n\t        \"in.t_starts\": (n_total_rays,),\n\t        \"in.t_ends\": (n_total_rays,),\n\t        \"in.occupancy_bitfield\": (K*G*G*G//8,),\n\t        \"in.next_ray_index_in\": (1,),\n\t        \"in.terminated\": (n_rays,),\n\t        \"in.indices_in\": (n_rays,),\n", "        \"out.next_ray_index\": (1,),\n\t        \"out.indices_out\": (n_rays,),\n\t        \"out.n_samples\": (n_rays,),\n\t        \"out.t_starts\": (n_rays,),\n\t        \"out.xyzs\": (n_rays, march_steps_cap, 3),\n\t        \"out.dss\": (n_rays, march_steps_cap),\n\t        \"out.z_vals\": (n_rays, march_steps_cap),\n\t    }\n\t    return custom_call(\n\t        call_target_name=\"march_rays_inference\",\n", "        out_types=[\n\t            ir.RankedTensorType.get(shapes[\"out.next_ray_index\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.indices_out\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.n_samples\"], ir.IntegerType.get_unsigned(32)),\n\t            ir.RankedTensorType.get(shapes[\"out.t_starts\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.xyzs\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.dss\"], ir.F32Type.get()),\n\t            ir.RankedTensorType.get(shapes[\"out.z_vals\"], ir.F32Type.get()),\n\t        ],\n\t        operands=[\n", "            rays_o,\n\t            rays_d,\n\t            t_starts,\n\t            t_ends,\n\t            occupancy_bitfield,\n\t            next_ray_index_in,\n\t            terminated,\n\t            indices_in,\n\t        ],\n\t        backend_config=opaque,\n", "        operand_layouts=default_layouts(\n\t            shapes[\"in.rays_o\"],\n\t            shapes[\"in.rays_d\"],\n\t            shapes[\"in.t_starts\"],\n\t            shapes[\"in.t_ends\"],\n\t            shapes[\"in.occupancy_bitfield\"],\n\t            shapes[\"in.next_ray_index_in\"],\n\t            shapes[\"in.terminated\"],\n\t            shapes[\"in.indices_in\"],\n\t        ),\n", "        result_layouts=default_layouts(\n\t            shapes[\"out.next_ray_index\"],\n\t            shapes[\"out.indices_out\"],\n\t            shapes[\"out.n_samples\"],\n\t            shapes[\"out.t_starts\"],\n\t            shapes[\"out.xyzs\"],\n\t            shapes[\"out.dss\"],\n\t            shapes[\"out.z_vals\"],\n\t        ),\n\t    )\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/abstract.py", "chunked_list": ["import chex\n\timport jax\n\timport jax.numpy as jnp\n\t# jit rules\n\tdef march_rays_abstract(\n\t    # arrays\n\t    rays_o: jax.ShapedArray,\n\t    rays_d: jax.ShapedArray,\n\t    t_starts: jax.ShapedArray,\n\t    t_ends: jax.ShapedArray,\n", "    noises: jax.ShapedArray,\n\t    occupancy_bitfield: jax.ShapedArray,\n\t    # static args\n\t    total_samples: int,\n\t    diagonal_n_steps: int,\n\t    K: int,\n\t    G: int,\n\t    bound: float,\n\t    stepsize_portion: float,\n\t):\n", "    n_rays, _ = rays_o.shape\n\t    chex.assert_shape([rays_o, rays_d], (n_rays, 3))\n\t    chex.assert_shape([t_starts, t_ends, noises], (n_rays,))\n\t    chex.assert_shape(occupancy_bitfield, (K*G*G*G//8,))\n\t    chex.assert_type(occupancy_bitfield, jnp.uint8)\n\t    chex.assert_scalar_positive(total_samples)\n\t    chex.assert_scalar_positive(diagonal_n_steps)\n\t    chex.assert_scalar_positive(K)\n\t    chex.assert_scalar_positive(G)\n\t    chex.assert_scalar_positive(bound)\n", "    chex.assert_scalar_non_negative(stepsize_portion)\n\t    dtype = jax.dtypes.canonicalize_dtype(rays_o.dtype)\n\t    if dtype != jnp.float32:\n\t        raise NotImplementedError(\n\t            \"march_rays is only implemented for input coordinates of `jnp.float32` type, got {}\".format(\n\t                dtype,\n\t            )\n\t        )\n\t    shapes = {\n\t        \"helper.next_sample_write_location\": (1,),\n", "        \"helper.number_of_exceeded_samples\": (1,),\n\t        \"helper.ray_is_valid\": (n_rays,),\n\t        \"out.rays_n_samples\": (n_rays,),\n\t        \"out.rays_sample_startidx\": (n_rays,),\n\t        \"out.idcs\": (total_samples,),\n\t        \"out.xyzs\": (total_samples, 3),\n\t        \"out.dirs\": (total_samples, 3),\n\t        \"out.dss\": (total_samples,),\n\t        \"out.z_vals\": (total_samples,),\n\t    }\n", "    return (\n\t        jax.ShapedArray(shape=shapes[\"helper.next_sample_write_location\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=shapes[\"helper.number_of_exceeded_samples\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=shapes[\"helper.ray_is_valid\"], dtype=jnp.bool_),\n\t        jax.ShapedArray(shape=shapes[\"out.rays_n_samples\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=shapes[\"out.rays_sample_startidx\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=shapes[\"out.idcs\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=shapes[\"out.xyzs\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=shapes[\"out.dirs\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=shapes[\"out.dss\"], dtype=jnp.float32),\n", "        jax.ShapedArray(shape=shapes[\"out.z_vals\"], dtype=jnp.float32),\n\t    )\n\tdef march_rays_inference_abstract(\n\t    # arrays\n\t    rays_o: jax.ShapedArray,\n\t    rays_d: jax.ShapedArray,\n\t    t_starts: jax.ShapedArray,\n\t    t_ends: jax.ShapedArray,\n\t    occupancy_bitfield: jax.ShapedArray,\n\t    next_ray_index: jax.ShapedArray,\n", "    terminated: jax.ShapedArray,\n\t    indices_in: jax.ShapedArray,\n\t    # static args\n\t    diagonal_n_steps: int,\n\t    K: int,\n\t    G: int,\n\t    march_steps_cap: int,\n\t    bound: float,\n\t    stepsize_portion: float,\n\t):\n", "    (n_total_rays, _), (n_rays,) = rays_o.shape, terminated.shape\n\t    chex.assert_shape([rays_o, rays_d], (n_total_rays, 3))\n\t    chex.assert_shape([t_starts, t_ends], (n_total_rays,))\n\t    chex.assert_shape(occupancy_bitfield, (K*G*G*G//8,))\n\t    chex.assert_type(occupancy_bitfield, jnp.uint8)\n\t    chex.assert_shape(next_ray_index, (1,))\n\t    chex.assert_shape([terminated, indices_in], (n_rays,))\n\t    out_shapes = {\n\t        \"next_ray_index\": (1,),\n\t        \"indices_out\": (n_rays,),\n", "        \"n_samples\": (n_rays,),\n\t        \"t_starts\": (n_rays,),\n\t        \"xyzs\": (n_rays, march_steps_cap, 3),\n\t        \"dss\": (n_rays, march_steps_cap),\n\t        \"z_vals\": (n_rays, march_steps_cap),\n\t    }\n\t    return (\n\t        jax.ShapedArray(shape=out_shapes[\"next_ray_index\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=out_shapes[\"indices_out\"], dtype=jnp.uint32),\n\t        jax.ShapedArray(shape=out_shapes[\"n_samples\"], dtype=jnp.uint32),\n", "        jax.ShapedArray(shape=out_shapes[\"t_starts\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"xyzs\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"dss\"], dtype=jnp.float32),\n\t        jax.ShapedArray(shape=out_shapes[\"z_vals\"], dtype=jnp.float32),\n\t    )\n"]}
{"filename": "deps/volume-rendering-jax/src/volrendjax/marching/__init__.py", "chunked_list": ["from typing import Tuple\n\timport jax\n\timport jax.numpy as jnp\n\tfrom . import impl\n\tdef march_rays(\n\t    # static\n\t    total_samples: int,\n\t    diagonal_n_steps: int,\n\t    K: int,\n\t    G: int,\n", "    bound: float,\n\t    stepsize_portion: float,\n\t    # inputs\n\t    rays_o: jax.Array,\n\t    rays_d: jax.Array,\n\t    t_starts: jax.Array,\n\t    t_ends: jax.Array,\n\t    noises: jax.Array,\n\t    occupancy_bitfield: jax.Array,\n\t) -> Tuple[jax.Array, jax.Array, jax.Array, jax.Array, jax.Array]:\n", "    \"\"\"\n\t    Given a pack of rays (`rays_o`, `rays_d`), their intersection time with the scene bounding box\n\t    (`t_starts`, `t_ends`), and an occupancy grid (`occupancy_bitfield`), generate samples along\n\t    each ray.\n\t    Inputs:\n\t        total_samples `int`: ,\n\t        diagonal_n_steps `int`: the length of a minimal ray marching step is calculated internally\n\t                                as:\n\t                                    Œîùë° := ‚àö3 / diagonal_n_steps;\n\t                                the NGP paper uses diagonal_n_steps=1024 (as described in appendix\n", "                                E.1).\n\t        K `int`: total number of cascades of `occupancy_bitfield`\n\t        G `int`: occupancy grid resolution, the paper uses 128 for every cascade\n\t        bound `float`: the half length of the longest axis of the scene‚Äôs bounding box,\n\t                       e.g. the `bound` of the bounding box [-1, 1]^3 is 1\n\t        stepsize_portion: next step size is calculated as t * stepsize_portion, the paper uses 1/256\n\t        rays_o `[n_rays, 3]`: ray origins\n\t        rays_d `[n_rays, 3]`: **unit** vectors representing ray directions\n\t        t_starts `[n_rays]`: time of the ray entering the scene bounding box\n\t        t_ends `[n_rays]`: time of the ray leaving the scene bounding box\n", "        noises `broadcastable to [n_rays]`: noises to perturb the starting point of ray marching\n\t        occupancy_bitfield `[K*(G**3)//8]`: the occupancy grid represented as a bit array, grid\n\t                                            cells are laid out in Morton (z-curve) order, as\n\t                                            described in appendix E.2 of the NGP paper\n\t    Returns:\n\t        measured_batch_size_before_compaction `int`: total number of generated samples of all rays\n\t        ray_is_valid `bool` `[n_rays]`: a mask, where a true value denotes the ray's gradients\n\t                                        should flow, even if there are no samples generated for it\n\t        idcs `[total_samples]`: indices indicating which ray the i-th sample comes from.\n\t        rays_n_samples `[n_rays]`: number of samples of each ray, its sum is `total_samples`\n", "                                   referenced below\n\t        rays_sample_startidx `[n_rays]`: indices of each ray's first sample\n\t        xyzs `[total_samples, 3]`: spatial coordinates of the generated samples, invalid array\n\t                                   locations are masked out with zeros\n\t        dirs `[total_samples, 3]`: spatial coordinates of the generated samples, invalid array\n\t                                   locations are masked out with zeros.\n\t        dss `[total_samples]`: `ds`s of each sample, for a more detailed explanation of this\n\t                               notation, see documentation of function `volrendjax.integrate_rays`,\n\t                               invalid array locations are masked out with zeros.\n\t        z_vals `[total_samples]`: samples' distances to their origins, invalid array\n", "                                  locations are masked out with zeros.\n\t    \"\"\"\n\t    n_rays, _ = rays_o.shape\n\t    noises = jnp.broadcast_to(noises, (n_rays,))\n\t    next_sample_write_location, number_of_exceeded_samples, ray_is_valid, rays_n_samples, rays_sample_startidx, idcs, xyzs, dirs, dss, z_vals = impl.march_rays_p.bind(\n\t        # arrays\n\t        rays_o,\n\t        rays_d,\n\t        t_starts,\n\t        t_ends,\n", "        noises,\n\t        occupancy_bitfield,\n\t        # static args\n\t        total_samples=total_samples,\n\t        diagonal_n_steps=diagonal_n_steps,\n\t        K=K,\n\t        G=G,\n\t        bound=bound,\n\t        stepsize_portion=stepsize_portion,\n\t    )\n", "    measured_batch_size_before_compaction = next_sample_write_location[0] - number_of_exceeded_samples[0]\n\t    return measured_batch_size_before_compaction, ray_is_valid, rays_n_samples, rays_sample_startidx, idcs, xyzs, dirs, dss, z_vals\n\tdef march_rays_inference(\n\t    # static\n\t    diagonal_n_steps: int,\n\t    K: int,\n\t    G: int,\n\t    march_steps_cap: int,\n\t    bound: float,\n\t    stepsize_portion: float,\n", "    # inputs\n\t    rays_o: jax.Array,\n\t    rays_d: jax.Array,\n\t    t_starts: jax.Array,\n\t    t_ends: jax.Array,\n\t    occupancy_bitfield: jax.Array,\n\t    next_ray_index_in: jax.Array,\n\t    terminated: jax.Array,\n\t    indices: jax.Array,\n\t):\n", "    \"\"\"\n\t    Inputs:\n\t        diagonal_n_steps, K, G, bound, stepsize_portion: see explanations in function `march_rays`\n\t        march_steps_cap `int`: maximum steps to march for each ray in this iteration\n\t        rays_o `float` `[n_total_rays, 3]`: ray origins\n\t        rays_d `float` `[n_total_rays, 3]`: ray directions\n\t        t_starts `float` `n_total_rays`: distance of each ray's starting point to its origin\n\t        t_ends `float` `n_total_rays`: distance of each ray's ending point to its origin\n\t        occupancy_bitfield `uint8` `[K*(G**3)//8]`: the occupancy grid represented as a bit array\n\t        next_ray_index_in `uint32`: helper variable to keep record of the latest ray that got rendered\n", "        terminated `bool` `[n_rays]`: output of `integrate_rays_inference`, a binary mask indicating\n\t                                      each ray's termination status\n\t        indices `[n_rays]`: each ray's location in the global arrays\n\t    Returns:\n\t        next_ray_index `uint32` `[1]`: for use in next iteration\n\t        indices `uint32` `[n_rays]`: for use in the integrate_rays_inference immediately after\n\t        n_samples `uint32` `[n_rays]`: number of generated samples of each ray in question\n\t        t_starts `float` `[n_rays]`: advanced values of `t` for use in next iteration\n\t        xyzs `float` `[n_rays, march_steps_cap, 3]`: each sample's XYZ coordinate\n\t        dss `float` `[n_rays, march_steps_cap]`: `ds` of each sample\n", "        z_vals `float` `[n_rays, march_steps_cap]`: distance of each sample to their ray origins\n\t    \"\"\"\n\t    next_ray_index, indices, n_samples, t_starts_out, xyzs, dss, z_vals = impl.march_rays_inference_p.bind(\n\t        rays_o,\n\t        rays_d,\n\t        t_starts,\n\t        t_ends,\n\t        occupancy_bitfield,\n\t        next_ray_index_in,\n\t        terminated,\n", "        indices,\n\t        diagonal_n_steps=diagonal_n_steps,\n\t        K=K,\n\t        G=G,\n\t        march_steps_cap=march_steps_cap,\n\t        bound=bound,\n\t        stepsize_portion=stepsize_portion,\n\t    )\n\t    t_starts = t_starts.at[indices].set(t_starts_out)\n\t    return next_ray_index, indices, n_samples, t_starts, xyzs, dss, z_vals\n"]}
{"filename": "deps/spherical-harmonics-encoding-jax/setup.py", "chunked_list": ["#!/usr/bin/env python\n\timport os\n\timport subprocess\n\tfrom setuptools import Extension, find_packages, setup\n\tfrom setuptools.command.build_ext import build_ext\n\tHERE = os.path.dirname(os.path.realpath(__file__))\n\tclass CMakeBuildExt(build_ext):\n\t    def build_extensions(self):\n\t        # First: configure CMake build\n\t        import platform\n", "        import sys\n\t        import sysconfig\n\t        import pybind11\n\t        # Work out the relevant Python paths to pass to CMake, adapted from the\n\t        # PyTorch build system\n\t        if platform.system() == \"Windows\":\n\t            cmake_python_library = \"{}/libs/python{}.lib\".format(\n\t                sysconfig.get_config_var(\"prefix\"),\n\t                sysconfig.get_config_var(\"VERSION\"),\n\t            )\n", "            if not os.path.exists(cmake_python_library):\n\t                cmake_python_library = \"{}/libs/python{}.lib\".format(\n\t                    sys.base_prefix,\n\t                    sysconfig.get_config_var(\"VERSION\"),\n\t                )\n\t        else:\n\t            cmake_python_library = \"{}/{}\".format(\n\t                sysconfig.get_config_var(\"LIBDIR\"),\n\t                sysconfig.get_config_var(\"INSTSONAME\"),\n\t            )\n", "        cmake_python_include_dir = sysconfig.get_path(\"include\")\n\t        install_dir = os.path.abspath(\n\t            os.path.dirname(self.get_ext_fullpath(\"dummy\"))\n\t        )\n\t        os.makedirs(install_dir, exist_ok=True)\n\t        cmake_args = [\n\t            \"-DCMAKE_INSTALL_PREFIX={}\".format(install_dir),\n\t            \"-DPython_EXECUTABLE={}\".format(sys.executable),\n\t            \"-DPython_LIBRARIES={}\".format(cmake_python_library),\n\t            \"-DPython_INCLUDE_DIRS={}\".format(cmake_python_include_dir),\n", "            \"-DCMAKE_BUILD_TYPE={}\".format(\n\t                \"Debug\" if self.debug else \"Release\"\n\t            ),\n\t            \"-DCMAKE_PREFIX_PATH={}\".format(pybind11.get_cmake_dir()),\n\t            \"-G Ninja\",\n\t        ]\n\t        os.makedirs(self.build_temp, exist_ok=True)\n\t        subprocess.check_call(\n\t            [\"cmake\", HERE] + cmake_args, cwd=self.build_temp\n\t        )\n", "        # Build all the extensions\n\t        super().build_extensions()\n\t        # Finally run install\n\t        subprocess.check_call(\n\t            [\"cmake\", \"--build\", \".\", \"--target\", \"install\"],\n\t            cwd=self.build_temp,\n\t        )\n\t    def build_extension(self, ext):\n\t        target_name = ext.name.split(\".\")[-1]\n\t        subprocess.check_call(\n", "            [\"cmake\", \"--build\", \".\", \"--target\", target_name],\n\t            cwd=self.build_temp,\n\t        )\n\textensions = [\n\t    Extension(\n\t        \"shjax.cudaops\",  # Python dotted name, whose final component should be a buildable target defined in CMakeLists.txt\n\t        [  # source paths, relative to this setup.py file\n\t            \"lib/ffi.cc\",\n\t            \"lib/impl/spherical_harmonics_encoding.cu\",\n\t        ],\n", "    )\n\t]\n\tsetup(\n\t    name=\"spherical-harmonics-encoding-jax\",\n\t    author=\"blurgyy\",\n\t    package_dir={\"\": \"src\"},\n\t    packages=find_packages(\"src\"),\n\t    include_package_data=True,\n\t    install_requires=[\"jax\", \"jaxlib\", \"chex\"],\n\t    ext_modules=extensions,\n", "    cmdclass={\"build_ext\": CMakeBuildExt},\n\t)\n"]}
{"filename": "deps/spherical-harmonics-encoding-jax/src/shjax/__init__.py", "chunked_list": ["import functools\n\timport chex\n\timport jax\n\tfrom jax.abstract_arrays import ShapedArray\n\tfrom jax.interpreters import batching, mlir, xla\n\tfrom jax.interpreters.mlir import ir\n\tfrom jax.lib import xla_client\n\timport jax.numpy as jnp\n\tfrom . import cudaops\n\ttry:\n", "    from jaxlib.mhlo_helpers import custom_call\n\texcept ModuleNotFoundError:\n\t    # A more recent jaxlib would have `hlo_helpers` instead of `mhlo_helpers`\n\t    # <https://github.com/google/jax/commit/b8ae8e3fa10f9abe998459fac1513915acee776d#diff-50658d597212b4ce070b8bd8c1fc522deeee1845ba387a0a5b507c446e8ea12a>\n\t    from jaxlib.hlo_helpers import custom_call\n\t# register GPU XLA custom calls\n\tfor name, value in cudaops.get_registrations().items():\n\t    xla_client.register_custom_call_target(name, value, platform=\"gpu\")\n\t# jit rules, infer returned shape according to input\n\tdef _spherical_harmonics_encoding_abstract(coord: jax.Array, hint: jax.Array):\n", "    \"\"\"\n\t    Inputs:\n\t        coord [..., 3] float: input coordinates\n\t        hint [degree]: an array with shape [L], this is used to hint the function with the desired\n\t                  spherical harmonics degrees\n\t    \"\"\"\n\t    (*n, _), dtype = coord.shape, coord.dtype\n\t    n = functools.reduce(lambda x, y: x * y, n)\n\t    degree, = hint.shape\n\t    dtype = jax.dtypes.canonicalize_dtype(coord.dtype)\n", "    return ShapedArray(shape=(n, degree * degree), dtype=dtype)\n\t# register the primitive\n\tsh_enc_p = jax.core.Primitive(\"spherical_harmonics_encodingüåê\")\n\tsh_enc_p.multiple_results = False\n\tsh_enc_p.def_impl(functools.partial(xla.apply_primitive, sh_enc_p))\n\tsh_enc_p.def_abstract_eval(_spherical_harmonics_encoding_abstract)\n\t# helper function for mapping given shapes to their default mlir layouts\n\tdef default_layouts(*shapes):\n\t    return [range(len(shape) - 1, -1, -1) for shape in shapes]\n\t# mlir lowering rule\n", "def _spherical_harmonics_encoding_lowering_cuda(\n\t        ctx: mlir.LoweringRuleContext,\n\t        coord: ir.Value,\n\t        hint: ir.Value,\n\t    ):\n\t    coord_type = ir.RankedTensorType(coord.type)\n\t    coord_shape = coord_type.shape\n\t    n, _ = coord_shape\n\t    degree, = ir.RankedTensorType(hint.type).shape\n\t    result_shape = (n, degree * degree)\n", "    opaque = cudaops.make_spherical_harmonics_encoding_descriptor(n, degree)\n\t    # Documentation says directly return the `custom_call` would suffice, but directly returning\n\t    # here results in error \"Output of translation rule must be iterable: ...\", so let's make it\n\t    # iterable.\n\t    # NOTE:\n\t    #   A newer jaxlib (current 0.3.22) may require this to be a single custom_call(...), instead of\n\t    #   an iterable, as documentation suggests.\n\t    # REF:\n\t    #   documentation: <https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html#lowering-to-mlir-custom-call>\n\t    #   tutorial: <https://github.com/dfm/extending-jax/blob/1cb4c39c524bccb5e3068c5a7f57a425ab0426a2/src/kepler_jax/kepler_jax.py#L113>\n", "    return [custom_call(\n\t        \"spherical_harmonics_encoding_cuda_f32\",  # the name of the registered XLA custom call at the top of this script\n\t        out_types=[\n\t            ir.RankedTensorType.get(result_shape, coord_type.element_type),\n\t        ],\n\t        operands=[coord],\n\t        backend_config=opaque,\n\t        operand_layouts=default_layouts(coord_shape),\n\t        result_layouts=default_layouts(result_shape),\n\t    )]\n", "mlir.register_lowering(\n\t    prim=sh_enc_p,\n\t    rule=_spherical_harmonics_encoding_lowering_cuda,\n\t    platform=\"gpu\",\n\t)\n\t# vmap support. REF: <https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html#batching>\n\tdef spherical_harmonics_encoding_batch(args, axes):\n\t    \"\"\"\n\t    The primitive is already able to handle arbitrary shape (except for the last axis, which must\n\t    have a dimension of 3), directly binding to the primitive impl should suffice.\n", "    Inputs:\n\t        args: Passed to def_impl, contains two tensors: `coord` and `hint`, where only `coord` is\n\t              batched.  args is (coord, hint)\n\t        axes: The axes that are being batched, one value for each arg, value is an integer if the\n\t              arg is batched, value is None if the arg is not batched.  In this case,\n\t              coord.shape[axes[0]] = B, and axes[1] = None.\n\t    \"\"\"\n\t    coord, hint = args\n\t    assert coord.shape[-1] == 3, \"spatial coordinates must be the last dimension\"\n\t    enc = sh_enc_p.bind(coord, hint)\n", "    # or:\n\t    # enc = spherical_harmonics_encoding(\n\t    #     coord=coord,\n\t    #     degree=hint.shape[0],\n\t    # )\n\t    # return the result, and the result axis that was batched\n\t    return enc, axes[0]\n\tbatching.primitive_batchers[sh_enc_p] = spherical_harmonics_encoding_batch\n\t# the only exposed function\n\tdef spherical_harmonics_encoding(coord: jax.Array, degree: int) -> jax.Array:\n", "    \"\"\"\n\t    Spherical harmonics encoding with GPU acceleration, expects unit vectors as input.\n\t    Inputs:\n\t        coord [..., 3] float: input 3D coordinates\n\t        degree int: highest degree used in spherical harmonics\n\t    Returns:\n\t        outputs [..., degree**2] float: encoded coordinates\n\t    \"\"\"\n\t    chex.assert_rank(coord, 2)\n\t    chex.assert_axis_dimension(coord, -1, 3)\n", "    chex.assert_scalar_non_negative(degree)\n\t    return sh_enc_p.bind(coord, jnp.empty((degree,)))\n"]}
{"filename": "utils/__main__.py", "chunked_list": ["#!/usr/bin/env python3\n\tfrom dataclasses import dataclass\n\tfrom functools import partial, reduce\n\timport os\n\tfrom pathlib import Path\n\tfrom typing import Annotated, List\n\tfrom typing_extensions import assert_never\n\tfrom PIL import Image\n\timport numpy as np\n\timport tyro\n", "from utils.common import setup_logging\n\tfrom utils.data import (\n\t    add_border,\n\t    blend_rgba_image_array,\n\t    create_scene_from_single_camera_image_collection,\n\t    create_scene_from_video,\n\t    psnr,\n\t    side_by_side,\n\t)\n\tfrom utils.types import RGBColor, SceneCreationOptions\n", "@dataclass(frozen=True, kw_only=True)\n\tclass Concatenate:\n\t    image_paths: tyro.conf.Positional[List[Path]]\n\t    # output image save path, the path will be overwritten with a warning\n\t    out: Path\n\t    # if specified, concatenate vertically instead of horizontally\n\t    vertical: bool=False\n\t    # gap between adjacent images, in pixels\n\t    gap: int=0\n\t    # border in pixels\n", "    border: int=0\n\t    bg: RGBColor=(1.0, 1.0, 1.0)\n\t@dataclass(frozen=True, kw_only=True)\n\tclass Metrics:\n\t    gt: Path\n\t    image_paths: tyro.conf.Positional[List[Path]]\n\t    psnr: bool=True\n\t    bg: RGBColor=(1.0, 1.0, 1.0)\n\t@dataclass(frozen=True, kw_only=True)\n\tclass CreateScene:\n", "    # path to a video or a directory of image collection\n\t    src: tyro.conf.Positional[Path]\n\t    # where to write the images and transforms_{train,val,test}.json\n\t    root_dir: Path\n\t    # how many frames to extract per second, only required when src is a video\n\t    fps: int | None=None\n\t    scene_opts: tyro.conf.OmitArgPrefixes[SceneCreationOptions]\n\tCmdCat = Annotated[\n\t    Concatenate,\n\t    tyro.conf.subcommand(\n", "        name=\"cat\",\n\t        prefix_name=False,\n\t        description=\"concatenate images horizontally or vertically\",\n\t    ),\n\t]\n\tCmdMetrics = Annotated[\n\t    Metrics,\n\t    tyro.conf.subcommand(\n\t        name=\"metrics\",\n\t        prefix_name=False,\n", "        description=\"compute metrics between images\",\n\t    ),\n\t]\n\tCmdCreateScene = Annotated[\n\t    CreateScene,\n\t    tyro.conf.subcommand(\n\t        name=\"create-scene\",\n\t        prefix_name=False,\n\t        description=\"create a instant-ngp-compatible scene from a video or a directory of images\",\n\t    ),\n", "]\n\tArgs = CmdCat | CmdCreateScene | CmdMetrics\n\tdef main(args: Args):\n\t    logger = setup_logging(\"utils\", level=\"DEBUG\")\n\t    if isinstance(args, Concatenate):\n\t        if args.out.is_dir():\n\t            logger.error(\"output path '{}' is a directory\".format(args.out))\n\t            exit(1)\n\t        if args.out.exists():\n\t            logger.warn(\"output path '{}' exists and will be overwritten\".format(args.out))\n", "            if not os.access(args.out, os.W_OK):\n\t                logger.error(\"output path '{}' is readonly\".format(args.out))\n\t                exit(2)\n\t        if args.out.suffix.lower() not in map(lambda x: \".\" + x, [\"jpg\", \"jpeg\", \"png\", \"tif\", \"tiff\", \"bmp\", \"webp\"]):\n\t            logger.warn(\"the file extension '{}' might not be supported\".format(args.out.suffix))\n\t        images = list(map(\n\t            lambda img: blend_rgba_image_array(img, bg=args.bg) if img.shape[-1] == 4 else img,\n\t            map(np.asarray, map(Image.open, args.image_paths)),\n\t        ))\n\t        height, width = images[0].shape[:2]\n", "        oimg = reduce(\n\t            partial(\n\t                side_by_side,\n\t                height=(None if args.vertical else height),\n\t                width=(width if args.vertical else None),\n\t                vertical=args.vertical,\n\t                gap=args.gap,\n\t            ),\n\t            images,\n\t        )\n", "        oimg = add_border(oimg, border_pixels=args.border)\n\t        logger.info(\"saving image ...\")\n\t        Image.fromarray(np.asarray(oimg)).save(args.out)\n\t        logger.info(\"image ({}x{}) saved to '{}'\".format(oimg.shape[1], oimg.shape[0], args.out))\n\t    elif isinstance(args, Metrics):\n\t        gt_image = np.asarray(Image.open(args.gt))\n\t        if gt_image.shape[-1] == 4:\n\t            gt_image = blend_rgba_image_array(gt_image, bg=args.bg)\n\t        images = list(map(\n\t            lambda img: blend_rgba_image_array(img, bg=args.bg) if img.shape[-1] == 4 else img,\n", "            map(np.asarray, map(Image.open, args.image_paths)),\n\t        ))\n\t        for impath, img in zip(args.image_paths, images):\n\t            if args.psnr:\n\t                logger.info(\"psnr={} ({})\".format(psnr(gt_image, img), impath))\n\t    elif isinstance(args, CreateScene):\n\t        if args.src.is_dir():\n\t            create_scene_from_single_camera_image_collection(\n\t                raw_images_dir=args.src,\n\t                scene_root_dir=args.root_dir,\n", "                opts=args.scene_opts,\n\t            )\n\t        else:\n\t            assert args.fps is not None, \"must specify extracted frames per second via --fps for video source\"\n\t            create_scene_from_video(\n\t                video_path=args.src,\n\t                scene_root_dir=args.root_dir,\n\t                fps=args.fps,\n\t                opts=args.scene_opts,\n\t            )\n", "    else:\n\t        assert_never(\"tyro already ensures subcommand passed here are valid, this line should never be executed\")\n\tif __name__ == \"__main__\":\n\t    args = tyro.cli(Args)\n\t    main(args)\n"]}
{"filename": "utils/types.py", "chunked_list": ["from concurrent.futures import ThreadPoolExecutor\n\timport dataclasses\n\timport functools\n\timport json\n\timport math\n\tfrom pathlib import Path\n\tfrom typing import Callable, List, Literal, Tuple, Type\n\tfrom typing_extensions import assert_never\n\tfrom PIL import Image\n\timport chex\n", "from flax import struct\n\tfrom flax.struct import dataclass\n\tfrom flax.training.train_state import TrainState\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\timport numpy as np\n\timport pydantic\n\tfrom tqdm import tqdm\n\tfrom volrendjax import morton3d_invert, packbits\n", "from ._constants import tqdm_format\n\tCameraModelType = Literal[\n\t    \"SIMPLE_PINHOLE\",\n\t    \"PINHOLE\",\n\t    \"SIMPLE_RADIAL\",\n\t    \"RADIAL\",\n\t    \"OPENCV\",\n\t    \"OPENCV_FISHEYE\",\n\t]\n\tPositionalEncodingType = Literal[\"identity\", \"frequency\", \"hashgrid\", \"tcnn-hashgrid\"]\n", "DirectionalEncodingType = Literal[\"identity\", \"sh\", \"shcuda\"]\n\tEncodingType = Literal[PositionalEncodingType, DirectionalEncodingType]\n\tActivationType = Literal[\n\t    \"exponential\",\n\t    \"relu\",\n\t    \"sigmoid\",\n\t    \"thresholded_exponential\",\n\t    \"truncated_exponential\",\n\t    \"truncated_thresholded_exponential\",\n\t]\n", "ColmapMatcherType = Literal[\"Exhaustive\", \"Sequential\"]\n\tLogLevel = Literal[\"DEBUG\", \"INFO\", \"WARN\", \"WARNING\", \"ERROR\", \"CRITICAL\"]\n\tTransformsProvider = Literal[\"loaded\", \"orbit\"]\n\tDensityAndRGB = Tuple[jax.Array, jax.Array]\n\tRGBColor = Tuple[float, float, float]\n\tRGBColorU8 = Tuple[int, int, int]\n\tFourFloats = Tuple[float, float, float, float]\n\tMatrix4x4 = Tuple[FourFloats, FourFloats, FourFloats, FourFloats]\n\tdef empty_impl(clz):\n\t    if \"__dataclass_fields__\" not in clz.__dict__:\n", "        raise TypeError(\"class `{}` is not a dataclass\".format(clz.__name__))\n\t    fields = clz.__dict__[\"__dataclass_fields__\"]\n\t    def empty_fn(cls, /, **kwargs):\n\t        \"\"\"\n\t        Create an empty instance of the given class, with untransformed fields set to given values.\n\t        \"\"\"\n\t        for field_name, annotation in fields.items():\n\t            if field_name not in kwargs:\n\t                kwargs[field_name] = getattr(annotation.type, \"empty\", lambda: None)()\n\t        return cls(**kwargs)\n", "    setattr(clz, \"empty\", classmethod(empty_fn))\n\t    return clz\n\tdef replace_impl(clz):\n\t    if \"__dataclass_fields__\" not in clz.__dict__:\n\t        raise TypeError(\"class `{}` is not a dataclass\".format(clz.__name__))\n\t    fields = clz.__dict__[\"__dataclass_fields__\"]\n\t    def replace_fn(self, /, **kwargs) -> Type[clz]:\n\t        for k in kwargs.keys():\n\t            if k not in fields:\n\t                raise RuntimeError(\"class `{}` does not have a field with name '{}'\".format(clz.__name__, k))\n", "        ret = dataclasses.replace(self, **kwargs)\n\t        return ret\n\t    setattr(clz, \"replace\", replace_fn)\n\t    return clz\n\t@empty_impl\n\t@dataclass\n\tclass OccupancyDensityGrid:\n\t    # float32, full-precision density values\n\t    density: jax.Array\n\t    # bool, a non-compact representation of the occupancy bitfield\n", "    occ_mask: jax.Array\n\t    # uint8, each bit is an occupancy value of a grid cell\n\t    occupancy: jax.Array\n\t    # uint32, indices of the grids that are alive (trainable)\n\t    alive_indices: jax.Array\n\t    # list of `int`s, upper bound of each cascade\n\t    alive_indices_offset: List[int]=struct.field(pytree_node=False)\n\t    @classmethod\n\t    def create(cls, cascades: int, grid_resolution: int=128):\n\t        \"\"\"\n", "        Inputs:\n\t            cascades: number of cascades, paper: ùêæ = 1 for all synthetic NeRF scenes (single grid)\n\t                      and ùêæ ‚àà [1, 5] for larger real-world scenes (up to 5 grids, depending on scene\n\t                      size)\n\t            grid_resolution: resolution of the occupancy grid, the NGP paper uses 128.\n\t        Example usage:\n\t            ogrid = OccupancyDensityGrid.create(cascades=5, grid_resolution=128)\n\t        \"\"\"\n\t        G3 = grid_resolution**3\n\t        n_grids = cascades * G3\n", "        occupancy = 255 * jnp.ones(\n\t            shape=(n_grids // 8,),  # each bit is an occupancy value\n\t            dtype=jnp.uint8,\n\t        )\n\t        density = jnp.zeros(\n\t            shape=(n_grids,),\n\t            dtype=jnp.float32,\n\t        )\n\t        occ_mask = jnp.zeros(\n\t            shape=(n_grids,),\n", "            dtype=jnp.bool_,\n\t        )\n\t        return cls(\n\t            density=density,\n\t            occ_mask=occ_mask,\n\t            occupancy=occupancy,\n\t            alive_indices=jnp.arange(n_grids, dtype=jnp.uint32),\n\t            alive_indices_offset=np.cumsum([0] + [G3] * cascades).tolist(),\n\t        )\n\t    def mean_density_up_to_cascade(self, cas: int) -> float | jax.Array:\n", "        return self.density[self.alive_indices[:self.alive_indices_offset[cas]]].mean()\n\t@dataclass\n\tclass Camera:\n\t    # resolutions\n\t    width: int=struct.field(pytree_node=False)\n\t    height: int=struct.field(pytree_node=False)\n\t    # focal length\n\t    fx: float=struct.field(pytree_node=False)\n\t    fy: float=struct.field(pytree_node=False)\n\t    # principal point\n", "    cx: float=struct.field(pytree_node=False)\n\t    cy: float=struct.field(pytree_node=False)\n\t    near: float=struct.field(pytree_node=False)\n\t    # distortion parameters\n\t    k1: float=struct.field(default=0.0, pytree_node=False)\n\t    k2: float=struct.field(default=0.0, pytree_node=False)\n\t    k3: float=struct.field(default=0.0, pytree_node=False)\n\t    k4: float=struct.field(default=0.0, pytree_node=False)\n\t    p1: float=struct.field(default=0.0, pytree_node=False)\n\t    p2: float=struct.field(default=0.0, pytree_node=False)\n", "    model: CameraModelType=struct.field(default=\"OPENCV\", pytree_node=False)\n\t    @property\n\t    def has_distortion(self) -> bool:\n\t        return (\n\t            True\n\t            or self.k1 != 0.0\n\t            or self.k2 != 0.0\n\t            or self.k3 != 0.0\n\t            or self.k4 != 0.0\n\t            or self.p1 != 0.0\n", "            or self.p2 != 0.0\n\t        )\n\t    @property\n\t    def _type(self) -> Literal[\"PERSPECTIVE\", \"FISHEYE\"]:\n\t        if \"fisheye\" in self.model.lower():\n\t            return \"FISHEYE\"\n\t        else:\n\t            return \"PERSPECTIVE\"\n\t    @property\n\t    def n_pixels(self) -> int:\n", "        return self.height * self.width\n\t    @property\n\t    def K_numpy(self) -> np.ndarray:\n\t        return np.asarray([\n\t            [self.fx,      0., self.cx],\n\t            [     0., self.fy, self.cy],\n\t            [     0.,      0.,      1.]\n\t        ])\n\t    @property\n\t    def K(self) -> jax.Array:\n", "        return jnp.asarray(self.K_numpy)\n\t    @classmethod\n\t    def from_colmap_txt(cls, txt_path: str | Path) -> \"Camera\":\n\t        \"\"\"Initialize a camera from a colmap's TXT format model.\n\t        References of camera parameters from colmap (search for `InitializeParamsInfo`):\n\t            <https://github.com/colmap/colmap/blob/fac2fa6217a1f5498830769d64861b54c67009dc/src/colmap/base/camera_models.h#L778>\n\t        Example usage:\n\t            cam = PinholeCamera.from_colmap_txt(\"path/to/txt\")\n\t        \"\"\"\n\t        with open(txt_path, \"r\") as f:\n", "            lines = f.readlines()\n\t        cam_line = lines[-1].strip()\n\t        cam_desc = cam_line.split()\n\t        _front = 0\n\t        def next_descs(cnt: int) -> Tuple[str, ...]:\n\t            nonlocal _front\n\t            rear = _front + cnt\n\t            ret = cam_desc[_front:rear]\n\t            _front = rear\n\t            return ret[0] if cnt == 1 else ret\n", "        assert int(next_descs(1)) == 1, \"creating scenes with multiple cameras is not supported\"\n\t        camera_model: CameraModelType = next_descs(1)\n\t        width, height = next_descs(2)\n\t        k1, k2, k3, k4, p1, p2 = [0.] * 6\n\t        if camera_model == \"SIMPLE_PINHOLE\":\n\t            f, cx, cy = next_descs(3)\n\t            fx = fy = f\n\t        elif camera_model == \"SIMPLE_RADIAL\":\n\t            f, cx, cy, k1 = next_descs(4)\n\t            fx = fy = f\n", "        elif camera_model == \"RADIAL\":\n\t            f, cx, cy, k1, k2 = next_descs(5)\n\t            fx = fy = f\n\t        else:\n\t            fx, fy, cx, cy = next_descs(4)\n\t            if camera_model == \"PINHOLE\":\n\t                pass\n\t            elif camera_model == \"OPENCV\":\n\t                k1, k2, p1, p2 = next_descs(4)\n\t            elif camera_model == \"OPENCV_FISHEYE\":\n", "                k1, k2, k3, k4 = next_descs(4)\n\t            else:\n\t                assert_never(camera_model)\n\t        return cls(\n\t            width=int(width),\n\t            height=int(height),\n\t            fx=float(fx),\n\t            fy=float(fy),\n\t            cx=float(cx),\n\t            cy=float(cy),\n", "            near=0.,\n\t            k1=float(k1),\n\t            k2=float(k2),\n\t            k3=float(k3),\n\t            k4=float(k4),\n\t            p1=float(p1),\n\t            p2=float(p2),\n\t        )\n\t    def scale_resolution(self, scale: int | float) -> \"Camera\":\n\t        return self.replace(\n", "            width=int(self.width * scale),\n\t            height=int(self.height * scale),\n\t            fx=self.fx * scale,\n\t            fy=self.fy * scale,\n\t            cx=self.cx * scale,\n\t            cy=self.cy * scale,\n\t        )\n\t    def distort(self, x: jax.Array, y: jax.Array) -> jax.Array:\n\t        \"\"\"Computes distorted coords.\n\t        REF:\n", "            * <https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html>\n\t            * <https://en.wikipedia.org/wiki/Distortion_%28optics%29>\n\t            * <https://docs.opencv.org/3.4/db/d58/group__calib3d__fisheye.html>\n\t        Inputs:\n\t            x, y `float`: normalized undistorted coordinates\n\t        Returns:\n\t            x, y `float`: distorted coordinates\n\t        \"\"\"\n\t        k1, k2, k3, k4, = self.k1, self.k2, self.k3, self.k4\n\t        xx, yy = jnp.square(x), jnp.square(y)\n", "        rr = xx + yy\n\t        if self._type == \"FISHEYE\":\n\t            r = jnp.sqrt(rr)\n\t            theta = jnp.arctan(r)\n\t            thth = theta * theta\n\t            thetad = theta * (1. + thth * (k1 + thth * (k2 + thth * (k3 + thth * k4))))\n\t            dist = thetad / r - 1.\n\t            dx, dy = (\n\t                jnp.where(r < 1e-15, 0., x * dist),\n\t                jnp.where(r < 1e-15, 0., y * dist),\n", "            )\n\t        else:\n\t            radial = rr * (k1 + rr * (k2 + rr * (k3 + rr * k4)))\n\t            # radial distort\n\t            dx, dy = x * radial, y * radial\n\t            p1, p2 = self.p1, self.p2\n\t            # tangential distort\n\t            xy = x * y\n\t            dx += 2 * p1 * xy + p2 * (rr + 2 * xx)\n\t            dx += 2 * p2 * xy + p1 * (rr + 2 * xx)\n", "        return x + dx, y + dy\n\t    def undistort(self, x: jax.Array, y: jax.Array, eps: float=1e-3, max_iterations: int=10) -> jax.Array:\n\t        \"\"\"Computes undistorted coords.\n\t        REF:\n\t            * <https://github.com/google-research/multinerf/blob/b02228160d3179300c7d499dca28cb9ca3677f32/internal/camera_utils.py#L477-L509>\n\t            * <https://github.com/nerfstudio-project/nerfstudio/blob/004d8ca9d24b294b1877d4d5599879c4ce812bc7/nerfstudio/cameras/camera_utils.py#L411-L448>\n\t        Inputs:\n\t            x, y `float`: normalized (x_normalized = (x + cx) / fx) distorted coordinates\n\t            eps `float`: epsilon for the convergence\n\t            max_iterations `int`: maximum number of iterations to perform\n", "        Returns:\n\t            x, y `float`: undistorted coordinates\n\t        \"\"\"\n\t        # the original distorted coordinates\n\t        xd, yd = x.copy(), y.copy()\n\t        @jax.jit\n\t        def compute_residual_and_jacobian(x, y) -> Tuple[Tuple[jax.Array, jax.Array], Tuple[jax.Array, jax.Array, jax.Array, jax.Array]]:\n\t            \"\"\"Auxiliary function of radial_and_tangential_undistort() that computes residuals and\n\t            jacobians.\n\t            REF:\n", "                * <https://github.com/google-research/multinerf/blob/b02228160d3179300c7d499dca28cb9ca3677f32/internal/camera_utils.py#L427-L474>\n\t                * <https://github.com/nerfstudio-project/nerfstudio/blob/004d8ca9d24b294b1877d4d5599879c4ce812bc7/nerfstudio/cameras/camera_utils.py#L345-L407>\n\t            Inputs:\n\t                x: The updated x coordinates.\n\t                y: The updated y coordinates.\n\t            Returns:\n\t                The residuals (fx, fy) and jacobians (fx_x, fx_y, fy_x, fy_y).\n\t            \"\"\"\n\t            k1, k2, k3, k4 = self.k1, self.k2, self.k3, self.k4\n\t            p1, p2 = self.p1, self.p2\n", "            # let r(x, y) = x^2 + y^2;\n\t            #     d(x, y) = 1 + k1 * r(x, y) + k2 * r(x, y) ^2 + k3 * r(x, y)^3 +\n\t            #                   k4 * r(x, y)^4;\n\t            r = x * x + y * y\n\t            d = 1.0 + r * (k1 + r * (k2 + r * (k3 + r * k4)))\n\t            # The perfect projection is:\n\t            # xd = x * d(x, y) + 2 * p1 * x * y + p2 * (r(x, y) + 2 * x^2);\n\t            # yd = y * d(x, y) + 2 * p2 * x * y + p1 * (r(x, y) + 2 * y^2);\n\t            #\n\t            # Let's define\n", "            #\n\t            # fx(x, y) = x * d(x, y) + 2 * p1 * x * y + p2 * (r(x, y) + 2 * x^2) - xd;\n\t            # fy(x, y) = y * d(x, y) + 2 * p2 * x * y + p1 * (r(x, y) + 2 * y^2) - yd;\n\t            #\n\t            # We are looking for a solution that satisfies\n\t            # fx(x, y) = fy(x, y) = 0;\n\t            fx = d * x + 2 * p1 * x * y + p2 * (r + 2 * x * x) - xd\n\t            fy = d * y + 2 * p2 * x * y + p1 * (r + 2 * y * y) - yd\n\t            # Compute derivative of d over [x, y]\n\t            d_r = k1 + r * (2.0 * k2 + r * (3.0 * k3 + r * 4.0 * k4))\n", "            d_x = 2.0 * x * d_r\n\t            d_y = 2.0 * y * d_r\n\t            # Compute derivative of fx over x and y.\n\t            fx_x = d + d_x * x + 2.0 * p1 * y + 6.0 * p2 * x\n\t            fx_y = d_y * x + 2.0 * p1 * x + 2.0 * p2 * y\n\t            # Compute derivative of fy over x and y.\n\t            fy_x = d_x * y + 2.0 * p2 * y + 2.0 * p1 * x\n\t            fy_y = d + d_y * y + 2.0 * p2 * x + 6.0 * p1 * y\n\t            return (fx, fy), (fx_x, fx_y, fy_x, fy_y)\n\t        for _ in range(max_iterations):\n", "            (fx, fy), (fx_x, fx_y, fy_x, fy_y) = compute_residual_and_jacobian(x, y)\n\t            denominator = fy_x * fx_y - fx_x * fy_y\n\t            x_numerator = fx * fy_y - fy * fx_y\n\t            y_numerator = fy * fx_x - fx * fy_x\n\t            step_x = jnp.where(jnp.abs(denominator) > eps, x_numerator / denominator, jnp.zeros_like(denominator))\n\t            step_y = jnp.where(jnp.abs(denominator) > eps, y_numerator / denominator, jnp.zeros_like(denominator))\n\t            x = x + step_x\n\t            y = y + step_y\n\t        return x, y\n\t    def make_ray_directions_from_pixel_coordinates(\n", "        self,\n\t        x: jax.Array,\n\t        y: jax.Array,\n\t        use_pixel_center: bool,\n\t    ) -> jax.Array:\n\t        \"\"\"Given distorted unnormalized pixel coordinates, generate a ray direction for each of them\n\t        Inputs:\n\t            x, y `uint32` `[N]`: unnormalized pixel coordinates\n\t        Returns:\n\t            dirs `float` `[N, 3]`: directions that have taken distortion into account\n", "        \"\"\"\n\t        chex.assert_type([x, y], jnp.uint32)\n\t        chex.assert_rank([x, y], 1)\n\t        chex.assert_equal_shape([x, y])\n\t        pixel_offset = 0.5 if use_pixel_center else 0.0\n\t        x, y, z = (  # in CV coordinates, axes are flipped later\n\t            ((x + pixel_offset) - self.cx) / self.fx,\n\t            ((y + pixel_offset) - self.cy) / self.fy,\n\t            jnp.ones_like(x),\n\t        )\n", "        if self.has_distortion:\n\t            x, y = self.undistort(x, y)\n\t        if self._type == \"FISHEYE\":\n\t            theta = jnp.sqrt(jnp.square(x) + jnp.square(y))\n\t            theta = jnp.clip(theta, 0., jnp.pi)\n\t            co, si = jnp.cos(theta), jnp.sin(theta)\n\t            x, y, z = (\n\t                x * si / theta,\n\t                y * si / theta,\n\t                z * co,\n", "            )\n\t        dirs = jnp.stack([x, y, z], axis=-1)\n\t        # flip axis from CV coordinates to CG coordinates\n\t        dirs = dirs @ jnp.diag(jnp.asarray([1., -1., -1.]))\n\t        return dirs / jnp.linalg.norm(dirs, axis=-1, keepdims=True)\n\t@empty_impl\n\t@dataclass\n\tclass RayMarchingOptions:\n\t    # for calculating the length of a minimal ray marching step, the NGP paper uses 1024 (appendix\n\t    # E.1)\n", "    diagonal_n_steps: int\n\t    # whether to fluctuate the first sample along the ray with a tiny perturbation\n\t    perturb: bool\n\t    # resolution for the auxiliary density/occupancy grid, the NGP paper uses 128 (appendix E.2)\n\t    density_grid_res: int\n\t@empty_impl\n\t@dataclass\n\tclass RenderingOptions:\n\t    # background color for transparent parts of the image, has no effect if `random_bg` is True\n\t    bg: RGBColor\n", "    # ignore `bg` specification and use random color for transparent parts of the image\n\t    random_bg: bool\n\t@empty_impl\n\t@replace_impl\n\t@pydantic.dataclasses.dataclass(frozen=True)\n\tclass CameraOverrideOptions:\n\t    width: int | None=None\n\t    height: int | None=None\n\t    focal: float | None=None\n\t    near: float | None=None\n", "    k1: float | None=None\n\t    k2: float | None=None\n\t    k3: float | None=None\n\t    k4: float | None=None\n\t    p1: float | None=None\n\t    p2: float | None=None\n\t    model: CameraModelType | None=None\n\t    distortion: bool=True\n\t    @property\n\t    def fx(self) -> float:\n", "        return self.focal\n\t    @property\n\t    def fy(self) -> float:\n\t        return self.focal\n\t    @property\n\t    def cx(self) -> float:\n\t        return self.width / 2.\n\t    @property\n\t    def cy(self) -> float:\n\t        return self.height / 2.\n", "    def __post_init__(self):\n\t        if self.width is None and self.height is None:\n\t            return\n\t        if int(self.width is not None) + int(self.height is not None) == 1:\n\t            side = self.width if self.width is not None else self.height\n\t            self.__init__(\n\t                width=side,\n\t                height=side,\n\t                focal=self.focal,\n\t                near=self.near,\n", "                k1=self.k1,\n\t                k2=self.k2,\n\t                k3=self.k3,\n\t                k4=self.k4,\n\t                p1=self.p1,\n\t                p2=self.p2,\n\t                model=self.model,\n\t                distortion=self.distortion,\n\t            )\n\t        assert self.width > 0 and self.height > 0\n", "        if self.focal is None:\n\t            self.__init__(\n\t                width=self.width,\n\t                height=self.height,\n\t                focal=min(self.width, self.height),\n\t                near=self.near,\n\t                k1=self.k1,\n\t                k2=self.k2,\n\t                k3=self.k3,\n\t                k4=self.k4,\n", "                p1=self.p1,\n\t                p2=self.p2,\n\t                model=self.model,\n\t                distortion=self.distortion,\n\t            )\n\t    def update_camera(self, camera: Camera | None=None) -> Camera:\n\t        def try_override(name: str) -> int | float | CameraModelType:\n\t            return getattr(self, name) or getattr(camera, name)\n\t        def try_override_if_has_distortion_else_zero(name: str) -> float:\n\t            return try_override(name) if self.distortion else 0.\n", "        width, height, fx, fy, cx, cy, near, model = map(\n\t            try_override,\n\t            [\"width\", \"height\", \"fx\", \"fy\", \"cx\", \"cy\", \"near\", \"model\"],\n\t        )\n\t        k1, k2, k3, k4, p1, p2 = map(\n\t            try_override_if_has_distortion_else_zero,\n\t            [\"k1\", \"k2\", \"k3\", \"k4\", \"p1\", \"p2\"],\n\t        )\n\t        return camera.replace(\n\t            width=width,\n", "            height=height,\n\t            fx=fx,\n\t            fy=fy,\n\t            cx=cx,\n\t            cy=cy,\n\t            near=near,\n\t            k1=k1,\n\t            k2=k2,\n\t            k3=k3,\n\t            k4=k4,\n", "            p1=p1,\n\t            p2=p2,\n\t            model=model,\n\t        )\n\t    @property\n\t    def enabled(self) -> bool:\n\t        return any(map(\n\t            lambda name: getattr(self, name) is not None,\n\t            [\"width\", \"height\", \"focal\", \"near\", \"model\"]\n\t            + [\"k1\", \"k2\", \"k3\", \"k4\", \"p1\", \"p2\"],\n", "        ))\n\t@empty_impl\n\t@replace_impl\n\t@pydantic.dataclasses.dataclass(frozen=True)\n\tclass SceneOptions:\n\t    # images with sharpness lower than this value will be discarded\n\t    sharpness_threshold: float\n\t    # scale input images in case they are too large, camera intrinsics are also scaled to match the\n\t    # updated image resolution.\n\t    resolution_scale: float\n", "    camera_near: float\n\t    # maximum GPU memory to consume in MB, the pixels are reloaded into GPU memory before each epoch\n\t    # if the scene has more than this number of pixels, otherwise all pixels are loaded once\n\t    max_mem_mbytes: int\n\t    # overrides `aabb_scale` in transforms.json\n\t    bound: float | None=None\n\t    # overrides `up` in transforms.json\n\t    up: Tuple[float, float, float] | None=None\n\t    def __post_init__(self):\n\t        assert 0 <= self.resolution_scale <= 1, (\n", "            \"resolution_scale must be in range [0, 1], got {}\".format(self.resolution_scale)\n\t        )\n\t    @property\n\t    def up_unitvec(self) -> Tuple[float, float, float] | None:\n\t        if self.up is None:\n\t            return None\n\t        up = np.asarray(self.up)\n\t        up = up / np.linalg.norm(up)\n\t        return tuple(up.tolist())\n\t@dataclass\n", "class RigidTransformation:\n\t    # [3, 3] rotation matrix\n\t    rotation: jax.Array\n\t    # [3] translation vector\n\t    translation: jax.Array\n\t    def __post_init__(self):\n\t        chex.assert_shape([self.rotation, self.translation], [(3, 3), (3,)])\n\t@replace_impl\n\t@pydantic.dataclasses.dataclass(frozen=True)\n\tclass TransformJsonFrame:\n", "    file_path: Path | None\n\t    transform_matrix: Matrix4x4\n\t    # unused, kept for compatibility with the original nerf_synthetic dataset\n\t    rotation: float=0.0\n\t    # unused, kept for compatibility with instant-ngp\n\t    sharpness: float=1e5\n\t    @property\n\t    def transform_matrix_numpy(self) -> np.ndarray:\n\t        return np.asarray(self.transform_matrix)\n\t    @property\n", "    def transform_matrix_jax_array(self) -> jax.Array:\n\t        return jnp.asarray(self.transform_matrix)\n\t    def rotate_world_up(self, up: Tuple[float, float, float] | np.ndarray) -> \"TransformJsonFrame\":\n\t        def rotmat(a, b):\n\t            \"copied from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n\t            a, b = a / np.linalg.norm(a), b / np.linalg.norm(b)\n\t            v = np.cross(a, b)\n\t            c = np.dot(a, b)\n\t            # handle exception for the opposite direction input\n\t            if c < -1 + 1e-10:\n", "                return rotmat(a + np.random.uniform(-1e-2, 1e-2, 3), b)\n\t            s = np.linalg.norm(v)\n\t            kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n\t            return np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2 + 1e-10))\n\t        up = np.asarray(up)\n\t        R = rotmat(up, [0, 0, 1])\n\t        R = np.pad(R, [0, 1])\n\t        R[-1, -1] = 1\n\t        new_transform_matrix = R @ self.transform_matrix_numpy\n\t        return self.replace(\n", "            transform_matrix=new_transform_matrix.tolist(),\n\t        )\n\t    def scale_camera_positions(self, scale: float) -> \"TransformJsonFrame\":\n\t        new_transform_matrix = self.transform_matrix_numpy\n\t        new_transform_matrix[:3, 3] *= scale * 2\n\t        return self.replace(\n\t            transform_matrix=new_transform_matrix.tolist(),\n\t        )\n\t@replace_impl\n\t@pydantic.dataclasses.dataclass(frozen=True)\n", "class TransformJsonBase:\n\t    frames: Tuple[TransformJsonFrame, ...]\n\t    # scene's bound, the name `aabb_scale` is for compatibility with instant-ngp (note that\n\t    # instant-ngp requires this value to be a power of 2, other than that a value that can work with\n\t    # instant-ngp will work with this code base as well).\n\t    aabb_scale: float=dataclasses.field(default=1., kw_only=True)\n\t    # scale camera's translation vectors by this factor while loading (default value taken from\n\t    # NVLabs/instant-ngp/include/neural-graphics-primitives/nerf_loader.h), since current\n\t    # implementation (this codebase) represents the scene inside a 2^3 cube centered at origin, to\n\t    # achieve the same scene scale as that of NVLabs/instant-ngp while using the same\n", "    # transform*.json files, the camera translation vectors will be scaled by 2 time this value.\n\t    # I.e. if the transform*.json specifies `\"scale\": 0.3`, loaded cameras' translation vectors will\n\t    # be scaled by `0.6`.  See `utils.types.TransformJsonFrame.scale_camera_positions` for details.\n\t    # NOTE: this value does not affect scene's bounding box\n\t    scale: float=dataclasses.field(default=1/3, kw_only=True)\n\t    bg: bool=dataclasses.field(default=False, kw_only=True)\n\t    up: Tuple[float, float, float]=dataclasses.field(default=(0, 0, 1), kw_only=True)\n\t    n_extra_learnable_dims: int=dataclasses.field(default=0, kw_only=True)\n\t    def rotate_world_up(self) -> \"TransformJsonBase\":\n\t        return self.replace(\n", "            frames=tuple(map(lambda f: f.rotate_world_up(self.up), self.frames)),\n\t            up=(0., 0., 1.),  # so that this operation is idempotent\n\t        )\n\t    def scale_camera_positions(self) -> \"TransformJsonBase\":\n\t        return self.replace(\n\t            frames=tuple(map(lambda f: f.scale_camera_positions(self.scale), self.frames)),\n\t        )\n\t    def merge(self, rhs: \"TransformJsonBase\") -> \"TransformJsonBase\":\n\t        if rhs is None:\n\t            return self\n", "        # sanity checks, transforms to be merged should have same camera intrinsics\n\t        assert isinstance(rhs, type(self))\n\t        assert all(\n\t            getattr(rhs, attr_name) == getattr(self, attr_name)\n\t            for attr_name in (\n\t                field\n\t                for field in self.__dataclass_fields__\n\t                if field != \"frames\"\n\t            )\n\t        )\n", "        return self.replace(frames=self.frames + rhs.frames)\n\t    def make_absolute(self, parent_dir: Path | str) -> \"TransformJsonBase\":\n\t        parent_dir = Path(parent_dir)\n\t        return self.replace(\n\t            frames=tuple(map(\n\t                lambda f: f.replace(\n\t                    file_path=(\n\t                        f.file_path\n\t                        if f.file_path.is_absolute()\n\t                        else parent_dir.joinpath(f.file_path).absolute().as_posix()\n", "                    ),\n\t                ),\n\t                self.frames,\n\t            )),\n\t        )\n\t    def as_json(self, /, indent: int=2) -> str:\n\t        d = dataclasses.asdict(self)\n\t        d = jax.tree_util.tree_map(lambda x: x.as_posix() if isinstance(x, Path) else x, d)\n\t        return json.dumps(d, indent=indent)\n\t    @classmethod\n", "    def from_json(cls, jsonstr: str) -> \"TransformJsonBase\":\n\t        return cls(**json.loads(jsonstr))\n\t    def save(self, path: str | Path) -> None:\n\t        path = Path(path)\n\t        path.write_text(self.as_json())\n\t    @classmethod\n\t    def load(cls, path: str | Path):\n\t        path = Path(path)\n\t        return cls.from_json(path.read_text())\n\t@replace_impl\n", "@pydantic.dataclasses.dataclass(frozen=True)\n\tclass TransformJsonNeRFSynthetic(TransformJsonBase):\n\t    camera_angle_x: float\n\t@replace_impl\n\t@pydantic.dataclasses.dataclass(frozen=True)\n\tclass TransformJsonNGP(TransformJsonBase):\n\t    fl_x: float\n\t    fl_y: float\n\t    cx: float\n\t    cy: float\n", "    w: int\n\t    h: int\n\t    k1: float=0.\n\t    k2: float=0.\n\t    k3: float=0.\n\t    k4: float=0.\n\t    p1: float=0.\n\t    p2: float=0.\n\t    camera_model: CameraModelType=\"OPENCV\"\n\t@replace_impl\n", "@pydantic.dataclasses.dataclass(frozen=True)\n\tclass SceneCreationOptions:\n\t    # given that the cameras' average distance to the origin is (4.0 * `camera_scale`), what would\n\t    # the scene's bound be?\n\t    bound: float\n\t    # `Sequntial` for continuous frames, `Exhaustive` for all possible pairs\n\t    matcher: ColmapMatcherType\n\t    camera_model: CameraModelType=\"OPENCV\"\n\t    # upon loading the created scene during training/inference, scale the camera positions with this\n\t    # factor\n", "    camera_scale: float=dataclasses.field(default=1/3)\n\t    # whether to enable background model\n\t    bg: bool=dataclasses.field(default=False)\n\t    # dimension of NeRF-W-style per-image appearance embeddings, set to 0 to disable\n\t    n_extra_learnable_dims: int=dataclasses.field(default=16)\n\t    # whether undistort the images and write them to disk, so that the camera in transforms.json is\n\t    # a pinhole camera\n\t    undistort: bool=dataclasses.field(default=False)\n\t@dataclass\n\tclass ImageMetadata:\n", "    H: int\n\t    W: int\n\t    xys: jax.Array  # int,[H*W, 2]: original integer coordinates in range [0, W] for x and [0, H] for y\n\t    uvs: jax.Array  # float,[H*W, 2]: normalized coordinates in range [0, 1]\n\t    rgbs: jax.Array  # float,[H*W, 3]: normalized rgb values in range [0, 1]\n\t@dataclass\n\tclass OrbitTrajectoryOptions:\n\t    # cameras' distance to the orbiting axis\n\t    radius: float=1.8\n\t    # lowest height of generated trajectory\n", "    low: float=0.0\n\t    # highest height of generated trajectory\n\t    high: float=1.3\n\t    # how many frames should be rendered per orbit\n\t    n_frames_per_orbit: int=144\n\t    n_orbit: int=2\n\t    # all orbiting cameras will look at this point\n\t    centroid: Tuple[float, float, float]=(0., 0., 0.2)\n\t    @property\n\t    def n_frames(self) -> int:\n", "        return self.n_frames_per_orbit * self.n_orbit\n\t# scene's metadata (computed from SceneOptions and TransformJson)\n\t@dataclass\n\tclass SceneMeta:\n\t    # half width of axis-aligned bounding-box, i.e. aabb's width is `bound*2`\n\t    bound: float\n\t    # whether the scene should be modeled with a background that is not part of the scene geometry\n\t    bg: bool\n\t    # the camera model used to render this scene\n\t    camera: Camera\n", "    n_extra_learnable_dims: int\n\t    frames: Tuple[TransformJsonFrame, ...]=struct.field(pytree_node=False)\n\t    @property\n\t    def cascades(self) -> int:\n\t        return max(1, int(1 + math.ceil(math.log2(self.bound))))\n\t    @property\n\t    def n_pixels(self) -> float:\n\t        return self.camera.n_pixels * len(self.frames)\n\t    @property\n\t    def sharpness_range(self) -> float:\n", "        return functools.reduce(\n\t            lambda prev, frame: (min(prev[0], frame.sharpness), max(prev[1], frame.sharpness)),\n\t            self.frames,\n\t            (1e9, -1e9),\n\t        )\n\t    # this is the same thing as `dt_gamma` in ashawkey/torch-ngp\n\t    @property\n\t    def stepsize_portion(self) -> float:\n\t        if self.bound >= 64:\n\t            return 1/128\n", "        elif self.bound >= 16:\n\t            return 6e-3\n\t        elif self.bound >= 4:\n\t            return 5e-3\n\t        elif self.bound > 1:\n\t            return 1/256\n\t        else:\n\t            return 0\n\t    def make_frames_with_orbiting_trajectory(\n\t        self,\n", "        opts: OrbitTrajectoryOptions,\n\t    ) -> \"SceneMeta\":\n\t        assert isinstance(opts, OrbitTrajectoryOptions)\n\t        thetas = np.linspace(0, opts.n_orbit * 2 * np.pi, opts.n_frames + 1)[:-1]\n\t        xs = np.cos(thetas) * opts.radius\n\t        ys = np.sin(thetas) * opts.radius\n\t        elevation_range = opts.high - opts.low\n\t        mid_elevation = opts.low + .5 * elevation_range\n\t        zs = mid_elevation + .5 * elevation_range * np.sin(np.linspace(0, 2 * np.pi, opts.n_frames + 1)[:-1])\n\t        xyzs = np.stack([xs, ys, zs]).T\n", "        view_dirs = (jnp.asarray(opts.centroid) - xyzs) / np.linalg.norm(xyzs, axis=-1, keepdims=True)\n\t        right_dirs = np.stack([-np.sin(thetas), np.cos(thetas), np.zeros_like(thetas)]).T\n\t        up_dirs = -np.cross(view_dirs, right_dirs)\n\t        up_dirs = up_dirs / np.linalg.norm(up_dirs, axis=-1, keepdims=True)\n\t        rot_cws = np.concatenate([right_dirs[..., None], up_dirs[..., None], -view_dirs[..., None]], axis=-1)\n\t        frames = tuple(map(\n\t            lambda rot_cw, t_cw: TransformJsonFrame(\n\t                file_path=None,\n\t                transform_matrix=np.concatenate(\n\t                    [np.concatenate([rot_cw, t_cw.reshape(3, 1)], axis=-1), np.ones((1, 4))],\n", "                    axis=0,\n\t                ).tolist(),\n\t            ),\n\t            rot_cws,\n\t            xyzs,\n\t        ))\n\t        return self.replace(frames=frames)\n\t@dataclass\n\tclass SceneData:\n\t    @dataclass\n", "    class ViewData:\n\t        width: int\n\t        height: int\n\t        transform: RigidTransformation\n\t        file: Path=struct.field(pytree_node=False)\n\t        @property\n\t        def image_pil(self) -> Image.Image:\n\t            image = Image.open(self.file)\n\t            image = image.resize((self.width, self.height), resample=Image.LANCZOS)\n\t            return image\n", "        @property\n\t        def image_rgba_u8(self) -> jax.Array:\n\t            image = np.asarray(self.image_pil)\n\t            if image.shape[-1] == 1:\n\t                image = np.concatenate([image] * 3 + [255 * np.ones_like(image[..., :1])], axis=-1)\n\t            elif image.shape[-1] == 3:\n\t                image = np.concatenate([image, 255 * np.ones_like(image[..., :1])], axis=-1)\n\t            chex.assert_axis_dimension(image, -1, 4)\n\t            return image\n\t        # float, [H*W, 4]: rgba values of type uint8\n", "        @property\n\t        def rgba_u8(self) -> jax.Array:\n\t            return self.image_rgba_u8.reshape(-1, 4)\n\t    meta: SceneMeta\n\t    # maximum GPU memory to consume in MB, the pixels are reloaded into GPU memory before each epoch\n\t    # if the scene has more than this number of pixels, otherwise all pixels are loaded once\n\t    max_mem_mbytes: int\n\t    # uint32, [n_pixels]\n\t    _view_indices: jax.Array | None=None\n\t    # uint32, [n_pixels]\n", "    _pixel_indices: jax.Array | None=None\n\t    # uint8, [n_pixels, 4]\n\t    rgbas_u8: jax.Array | None=None\n\t    def _free(self):\n\t        backend = jax.lib.xla_bridge.get_backend()\n\t        if self._view_indices is not None: backend.buffer_from_pyval(self._view_indices).delete()\n\t        if self._pixel_indices is not None: backend.buffer_from_pyval(self._pixel_indices).delete()\n\t        if self.rgbas_u8 is not None: backend.buffer_from_pyval(self.rgbas_u8).delete()\n\t        backend.buffer_from_pyval(self.transforms).delete()\n\t        jax.lib.xla_bridge.get_backend().defragment()\n", "    @functools.cached_property\n\t    def all_views(self) -> Tuple[ViewData, ...]:\n\t        return tuple(map(\n\t            lambda frame: self.ViewData(\n\t                width=self.meta.camera.width,\n\t                height=self.meta.camera.height,\n\t                transform=RigidTransformation(\n\t                    rotation=frame.transform_matrix_numpy[:3, :3],\n\t                    translation=frame.transform_matrix_numpy[:3, 3],\n\t                ),\n", "                file=frame.file_path,\n\t            ),\n\t            self.meta.frames,\n\t        ))\n\t    # float, [n_views, 9+3]\n\t    @functools.cached_property\n\t    def transforms(self) -> jax.Array:\n\t        return jnp.stack(list(ThreadPoolExecutor().map(\n\t            lambda view: jnp.concatenate([\n\t                view.transform.rotation.ravel(),\n", "                view.transform.translation,\n\t            ]),\n\t            self.all_views,\n\t        )))\n\t    def _should_load_all_pixels(self, /, max_mem_mbytes: int) -> bool:\n\t        n_bytes = max_mem_mbytes * 1024 * 1024\n\t        required_total_mem_bytes = 4 * self.meta.n_pixels\n\t        return n_bytes >= required_total_mem_bytes\n\t    @property\n\t    def load_all_pixels(self) -> bool:\n", "        return self._should_load_all_pixels(max_mem_mbytes=self.max_mem_mbytes)\n\t    @property\n\t    def n_views(self) -> int:\n\t        return len(self.all_views)\n\t    def _calculate_num_pixels(self, /, max_mem_mbytes: int) -> int:\n\t        if self._should_load_all_pixels(max_mem_mbytes=max_mem_mbytes):\n\t            return self.meta.n_pixels\n\t        else:\n\t            # Dividing by 3 because also need to keep track of view indices and pixel indices, each\n\t            # consuming the same amount of memory as rgba_u8.\n", "            n_bytes = max_mem_mbytes * 1024 * 1024\n\t            return int(n_bytes * .33 / 4)\n\t    @property\n\t    def n_pixels(self) -> int:\n\t        return self._calculate_num_pixels(max_mem_mbytes=self.max_mem_mbytes)\n\t    def get_view_indices(self, perm: jax.Array) -> jax.Array:\n\t        if self._view_indices is not None:\n\t            return self._view_indices[perm]\n\t        else:\n\t            return jnp.floor_divide(perm, self.meta.camera.n_pixels)\n", "    def get_pixel_indices(self, perm: jax.Array) -> jax.Array:\n\t        if self._pixel_indices is not None:\n\t            return self._pixel_indices[perm]\n\t        else:\n\t            return jnp.mod(perm, self.meta.camera.n_pixels)\n\t    def resample_pixels(self, /, KEY: jran.KeyArray, new_max_mem_mbytes: int | None=None) -> \"SceneData\":\n\t        load_all_pixels = self._should_load_all_pixels(new_max_mem_mbytes)\n\t        if (\n\t            (self.load_all_pixels and self.rgbas_u8 is not None)\n\t            and (new_max_mem_mbytes is None or load_all_pixels)\n", "        ):\n\t            return dataclasses.replace(self, max_mem_mbytes=new_max_mem_mbytes)\n\t        if new_max_mem_mbytes is None:\n\t            new_max_mem_mbytes = self.max_mem_mbytes\n\t        # free up GPU memory\n\t        self._free()\n\t        n_pixels = self._calculate_num_pixels(max_mem_mbytes=new_max_mem_mbytes)\n\t        make_progress_bar = functools.partial(\n\t            tqdm,\n\t            total=self.n_views,\n", "            desc=\"| {} {} ({:.2f}% of {}) training pixels\".format(\n\t                \"loading\"\n\t                if self._view_indices is None\n\t                else \"resampling\",\n\t                n_pixels,\n\t                n_pixels / self.meta.n_pixels * 100,\n\t                self.meta.n_pixels,\n\t            ),\n\t            bar_format=tqdm_format,\n\t        )\n", "        if load_all_pixels:\n\t            rgbas_u8 = jnp.concatenate(list(make_progress_bar(ThreadPoolExecutor().map(\n\t                lambda view: view.rgba_u8,\n\t                self.all_views,\n\t            ))))\n\t            return dataclasses.replace(\n\t                self,\n\t                max_mem_mbytes=new_max_mem_mbytes,\n\t                _view_indices=None,\n\t                _pixel_indices=None,\n", "                rgbas_u8=rgbas_u8,\n\t            )\n\t        else:\n\t            KEY, key_n, key_view_perm, key_pixel_idcs = jran.split(KEY, 4)\n\t            ns = jran.uniform(key_n, shape=(self.n_views - 1,), minval=7, maxval=13)\n\t            ns = ns / ns.sum()\n\t            ns = (ns * n_pixels / self.n_views * (self.n_views - 1)).astype(jnp.uint32)\n\t            ns = jnp.concatenate([ns, (n_pixels - ns.sum()) * jnp.ones_like(ns[:1])])\n\t            assert ns.sum() == n_pixels\n\t            sections, ns = jnp.cumsum(ns), ns.tolist()\n", "            pixel_idcs = jran.choice(\n\t                key=key_pixel_idcs,\n\t                a=self.meta.camera.n_pixels,\n\t                shape=(n_pixels,),\n\t                replace=True,\n\t            )\n\t            pixel_idcs_per_view = jnp.split(pixel_idcs, sections)\n\t            view_perm = jran.permutation(key=key_view_perm, x=self.n_views)\n\t            view_idcs = jnp.concatenate(list(ThreadPoolExecutor().map(\n\t                lambda vi, pidcs: vi * jnp.ones(pidcs.shape[0], dtype=jnp.uint32),\n", "                view_perm,\n\t                pixel_idcs_per_view,\n\t            )))\n\t            rgbas_u8 = jnp.concatenate(list(make_progress_bar(ThreadPoolExecutor().map(\n\t                lambda vi, pidcs: self.all_views[vi].rgba_u8[pidcs],\n\t                view_perm,\n\t                pixel_idcs_per_view,\n\t            ))))\n\t            return dataclasses.replace(\n\t                self,\n", "                max_mem_mbytes=new_max_mem_mbytes,\n\t                _view_indices=view_idcs,\n\t                _pixel_indices=pixel_idcs,\n\t                rgbas_u8=rgbas_u8,\n\t            )\n\t@dataclass\n\tclass RenderedImage:\n\t    bg: jax.Array\n\t    rgb: jax.Array\n\t    depth: jax.Array\n", "@empty_impl\n\tclass NeRFState(TrainState):\n\t    # WARN:\n\t    #   do not annotate fields with jax.Array as members with flax.truct.field(pytree_node=False),\n\t    #   otherwise weird issues happen, e.g. jax tracer leak, array-to-boolean conversion exception\n\t    #   while calling a jitted function with no helpful traceback.\n\t    ogrid: OccupancyDensityGrid\n\t    raymarch: RayMarchingOptions=struct.field(pytree_node=False)\n\t    render: RenderingOptions=struct.field(pytree_node=False)\n\t    scene_options: SceneOptions=struct.field(pytree_node=False)\n", "    scene_meta: SceneMeta=struct.field(pytree_node=False)\n\t    nerf_fn: Callable=struct.field(pytree_node=False)\n\t    bg_fn: Callable=struct.field(pytree_node=False)\n\t    @classmethod\n\t    def create(cls, *args, **kwargs):\n\t        return super().create(apply_fn=None, *args, **kwargs)\n\t    def __post_init__(self):\n\t        assert self.apply_fn is None\n\t    def update_ogrid_density(\n\t        self,\n", "        KEY: jran.KeyArray,\n\t        cas: int,\n\t        update_all: bool,\n\t        max_inference: int,\n\t    ) -> \"NeRFState\":\n\t        G3 = self.raymarch.density_grid_res**3\n\t        cas_slice = slice(cas * G3, (cas + 1) * G3)\n\t        cas_alive_indices = self.ogrid.alive_indices[self.ogrid.alive_indices_offset[cas]:self.ogrid.alive_indices_offset[cas+1]]\n\t        aligned_indices = cas_alive_indices % G3  # values are in range [0, G3)\n\t        n_grids = aligned_indices.shape[0]\n", "        decay = .95\n\t        cas_occ_mask = self.ogrid.occ_mask[cas_slice]\n\t        cas_density_grid = self.ogrid.density[cas_slice].at[aligned_indices].set(self.ogrid.density[cas_slice][aligned_indices] * decay)\n\t        if update_all:\n\t            # During the first 256 training steps, we sample M = K * 128^{3} cells uniformly without\n\t            # repetition.\n\t            cas_updated_indices = aligned_indices\n\t        else:\n\t            M = max(1, n_grids // 2)\n\t            # The first M/2 cells are sampled uniformly among all cells.\n", "            KEY, key_firsthalf, key_secondhalf = jran.split(KEY, 3)\n\t            indices_firsthalf = jran.choice(\n\t                key=key_firsthalf,\n\t                a=aligned_indices,\n\t                shape=(max(1, M//2),),\n\t                replace=True,  # allow duplicated choices\n\t            )\n\t            # Rejection sampling is used for the remaining samples to restrict selection to cells\n\t            # that are currently occupied.\n\t            # NOTE: Below is just uniformly sampling the occupied cells, not rejection sampling.\n", "            cas_alive_occ_mask = cas_occ_mask[aligned_indices]\n\t            indices_secondhalf = jran.choice(\n\t                key=key_secondhalf,\n\t                a=aligned_indices,\n\t                shape=(max(1, M//2),),\n\t                replace=True,  # allow duplicated choices\n\t                p=cas_alive_occ_mask.astype(jnp.float32),  # only care about occupied grids\n\t            )\n\t            cas_updated_indices = jnp.concatenate([indices_firsthalf, indices_secondhalf])\n\t        coordinates = morton3d_invert(cas_updated_indices).astype(jnp.float32)\n", "        coordinates = coordinates / (self.raymarch.density_grid_res - 1) * 2 - 1  # in [-1, 1]\n\t        mip_bound = min(self.scene_meta.bound, 2**cas)\n\t        half_cell_width = mip_bound / self.raymarch.density_grid_res\n\t        coordinates *= mip_bound - half_cell_width  # in [-mip_bound+half_cell_width, mip_bound-half_cell_width]\n\t        # random point inside grid cells\n\t        KEY, key = jran.split(KEY, 2)\n\t        coordinates += jran.uniform(\n\t            key,\n\t            coordinates.shape,\n\t            coordinates.dtype,\n", "            minval=-half_cell_width,\n\t            maxval=half_cell_width,\n\t        )\n\t        new_densities = map(\n\t            lambda coords_part: jax.jit(self.nerf_fn)(\n\t                {\"params\": self.locked_params[\"nerf\"]},\n\t                coords_part,\n\t                None,\n\t                None,\n\t            )[0].ravel(),\n", "            jnp.array_split(jax.lax.stop_gradient(coordinates), max(1, n_grids // (max_inference))),\n\t        )\n\t        new_densities = jnp.concatenate(list(new_densities))\n\t        cas_density_grid = cas_density_grid.at[cas_updated_indices].set(\n\t            jnp.maximum(cas_density_grid[cas_updated_indices], new_densities)\n\t        )\n\t        new_ogrid = self.ogrid.replace(\n\t            density=self.ogrid.density.at[cas_slice].set(cas_density_grid),\n\t        )\n\t        return self.replace(ogrid=new_ogrid)\n", "    @jax.jit\n\t    def threshold_ogrid(self) -> \"NeRFState\":\n\t        mean_density = self.ogrid.mean_density_up_to_cascade(1)\n\t        density_threshold = jnp.minimum(self.density_threshold_from_min_step_size, mean_density)\n\t        occupied_mask, occupancy_bitfield = packbits(\n\t            density_threshold=density_threshold,\n\t            density_grid=self.ogrid.density,\n\t        )\n\t        new_ogrid = self.ogrid.replace(\n\t            occ_mask=occupied_mask,\n", "            occupancy=occupancy_bitfield,\n\t        )\n\t        return self.replace(ogrid=new_ogrid)\n\t    def mark_untrained_density_grid(self) -> \"NeRFState\":\n\t        G = self.raymarch.density_grid_res\n\t        G3 = G*G*G\n\t        n_grids = self.scene_meta.cascades * G3\n\t        all_indices = jnp.arange(n_grids, dtype=jnp.uint32)\n\t        level, pos_idcs = all_indices // G3, all_indices % G3\n\t        mip_bound = jnp.minimum(2 ** level, self.scene_meta.bound).astype(jnp.float32)\n", "        cell_width = 2 * mip_bound / G\n\t        grid_xyzs = morton3d_invert(pos_idcs).astype(jnp.float32)  # [G3, 3]\n\t        grid_xyzs /= G  # in range [0, 1)\n\t        grid_xyzs -= 0.5  # in range [-0.5, 0.5)\n\t        grid_xyzs *= 2 * mip_bound[:, None]  # in range [-mip_bound, mip_bound)\n\t        vertex_offsets = cell_width[:, None, None] * jnp.asarray([\n\t            [0, 0, 0],\n\t            [0, 0, 1],\n\t            [0, 1, 0],\n\t            [0, 1, 1],\n", "            [1, 0, 0],\n\t            [1, 0, 1],\n\t            [1, 1, 0],\n\t            [1, 1, 1],\n\t        ], dtype=jnp.float32)\n\t        all_grid_vertices = grid_xyzs[:, None, :] + vertex_offsets\n\t        @jax.jit\n\t        def mark_untrained_density_grid_single_frame(\n\t            alive_marker: jax.Array,\n\t            transform_cw: jax.Array,\n", "            grid_vertices: jax.Array,\n\t        ):\n\t            rot_cw, t_cw = transform_cw[:3, :3], transform_cw[:3, 3]\n\t            # p_world, p_cam, T: [3, 1]\n\t            # rot_cw: [3, 3]\n\t            # p_world = rot_cw @ p_cam + t_cw\n\t            p_aligned = grid_vertices - t_cw\n\t            p_cam = (p_aligned[..., None, :] * rot_cw.T).sum(-1)\n\t            # camera looks along the -z axis\n\t            in_front_of_camera = p_cam[..., -1] < 0\n", "            u, v = jnp.split(p_cam[..., :2] / (-p_cam[..., -1:]), [1], axis=-1)\n\t            if self.scene_meta.camera.has_distortion:\n\t                # distort\n\t                u, v = self.scene_meta.camera.distort(u, v)\n\t                # Pixel coordinates outside the image plane may produce the same `u, v` as those inside\n\t                # the image plane, check if the produced `u, v` match the ray we started with.\n\t                # REF: <https://github.com/NVlabs/instant-ngp/blob/99aed93bbe8c8e074a90ec6c56c616e4fe217a42/src/testbed_nerf.cu#L481-L483>\n\t                re_u, re_v = self.scene_meta.camera.undistort(u, v)\n\t                redir = jnp.concatenate([re_u, re_v, -jnp.ones_like(re_u)], axis=-1)\n\t                redir = redir / jnp.linalg.norm(redir, axis=-1, keepdims=True)\n", "                ogdir = p_cam / jnp.linalg.norm(p_cam, axis=-1, keepdims=True)\n\t                same_ray = (redir * ogdir).sum(axis=-1) > 1. - 1e-3\n\t            else:\n\t                same_ray = True\n\t            uv = jnp.concatenate([\n\t                u * self.scene_meta.camera.fx + self.scene_meta.camera.cx,\n\t                v * self.scene_meta.camera.fy + self.scene_meta.camera.cy,\n\t            ], axis=-1)\n\t            uv = uv / jnp.asarray([self.scene_meta.camera.width, self.scene_meta.camera.height], dtype=jnp.float32)\n\t            within_frame_range = (uv >= 0.) & (uv < 1.)\n", "            within_frame_range = (\n\t                within_frame_range  # shape is [n_grids, 8, 2]\n\t                    .all(axis=-1)  # u and v must both be within frame\n\t            )\n\t            visible_by_camera = (in_front_of_camera & within_frame_range & same_ray).any(axis=-1)  # grid should be trained if any of its 8 vertices is visible\n\t            return alive_marker | visible_by_camera\n\t        # cam_t = np.asarray(list(map(\n\t        #     lambda frame: frame.transform_matrix_numpy[:3, 3],\n\t        #     self.scene_meta.frames,\n\t        # )))\n", "        # np.savetxt(\"cams.xyz\", cam_t)\n\t        alive_marker = jnp.zeros(n_grids, dtype=jnp.bool_)\n\t        for frame in (pbar := tqdm(self.scene_meta.frames, desc=\"| marking trainable grids\".format(n_grids), bar_format=tqdm_format)):\n\t            new_alive_marker_parts = map(\n\t                lambda alive_marker_part, grid_vertices_part: mark_untrained_density_grid_single_frame(\n\t                    alive_marker=alive_marker_part,\n\t                    transform_cw=frame.transform_matrix_jax_array,\n\t                    grid_vertices=grid_vertices_part,\n\t                ),\n\t                jnp.array_split(alive_marker, self.scene_meta.cascades),  # alive_marker_part\n", "                jnp.array_split(all_grid_vertices, self.scene_meta.cascades),  # grid_vertices_part\n\t            )\n\t            alive_marker = jnp.concatenate(list(new_alive_marker_parts), axis=0)\n\t            n_alive_grids = alive_marker.sum()\n\t            ratio_trainable = n_alive_grids / n_grids\n\t            pbar.set_description_str(\"| marked {}/{} ({:.2f}%) grids as trainable\".format(n_alive_grids, n_grids, ratio_trainable * 100))\n\t            if n_alive_grids == n_grids:\n\t                pbar.close()\n\t                break\n\t        marked_density = jnp.where(alive_marker, self.ogrid.density, -1.)\n", "        marked_occ_mask, marked_occupancy = packbits(\n\t            density_threshold=min(self.density_threshold_from_min_step_size, self.ogrid.mean_density_up_to_cascade(1)) if self.step > 0 else -.5,\n\t            density_grid=marked_density\n\t        )\n\t        # rgb = jnp.stack([~marked_occ_mask, jnp.zeros_like(marked_occ_mask, dtype=jnp.float32), marked_occ_mask]).T\n\t        # xyzrgb = np.asarray(jnp.concatenate([grid_xyzs, rgb], axis=-1))\n\t        # np.savetxt(\"blue_for_trainable.txt\", xyzrgb)\n\t        # np.savetxt(\"trainable.txt\", xyzrgb[np.where(marked_occ_mask)])\n\t        # np.savetxt(\"untrainable.txt\", xyzrgb[np.where(~marked_occ_mask)])\n\t        return self.replace(\n", "            ogrid=self.ogrid.replace(\n\t                density=marked_density,\n\t                occ_mask=marked_occ_mask,\n\t                occupancy=marked_occupancy,\n\t                alive_indices=all_indices[alive_marker],\n\t                alive_indices_offset=np.cumsum([0] + list(map(\n\t                    lambda cas_alive_marker: int(cas_alive_marker.sum()),\n\t                    jnp.split(alive_marker, self.scene_meta.cascades),\n\t                ))).tolist(),\n\t            ),\n", "        )\n\t    def epoch(self, iters: int) -> int:\n\t        return self.step // iters\n\t    @property\n\t    def density_threshold_from_min_step_size(self) -> float:\n\t        return .01 * self.raymarch.diagonal_n_steps / (2 * min(self.scene_meta.bound, 1) * 3**.5)\n\t    @property\n\t    def use_background_model(self) -> bool:\n\t        return self.scene_meta.bg and self.params.get(\"bg\") is not None\n\t    @property\n", "    def locked_params(self):\n\t        return jax.lax.stop_gradient(self.params)\n\t    @property\n\t    def update_ogrid_interval(self) -> int:\n\t        return min(16, self.step // 16 + 1)\n\t    @property\n\t    def should_call_update_ogrid(self) -> bool:\n\t        return (\n\t            self.step > 0\n\t            and self.step % self.update_ogrid_interval == 0\n", "        )\n\t    @property\n\t    def should_update_all_ogrid_cells(self) -> bool:\n\t        return self.step < 256\n\t    @property\n\t    def should_write_batch_metrics(self) -> bool:\n\t        return self.step % 16 == 0\n"]}
{"filename": "utils/_constants.py", "chunked_list": ["from colorama import Fore, Style\n\t_tqdm_format = \"SBRIGHT{desc}RESET: HI{percentage:3.0f}%RESET {n_fmt}/{total_fmt} [{elapsed}<HI{remaining}RESET, {rate_fmt}]\"\n\ttqdm_format = _tqdm_format \\\n\t    .replace(\"HI\", Fore.CYAN) \\\n\t    .replace(\"SBRIGHT\", Style.BRIGHT) \\\n\t    .replace(\"RESET\", Style.RESET_ALL)\n"]}
{"filename": "utils/data.py", "chunked_list": ["import collections\n\tfrom concurrent.futures import ThreadPoolExecutor\n\timport functools\n\timport json\n\tfrom pathlib import Path\n\tfrom typing import List, Literal, Sequence, Tuple\n\timport warnings\n\tfrom PIL import Image, ImageFilter, UnidentifiedImageError\n\timport chex\n\timport ffmpeg\n", "import imageio\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\tfrom matplotlib import pyplot as plt\n\tfrom natsort import natsorted\n\timport numpy as np\n\tfrom . import sfm\n\tfrom .common import jit_jaxfn_with, mkValueError, tqdm\n\tfrom .types import (\n", "    ImageMetadata,\n\t    Camera,\n\t    RGBColor,\n\t    RGBColorU8,\n\t    SceneCreationOptions,\n\t    SceneData,\n\t    SceneMeta,\n\t    SceneOptions,\n\t    TransformJsonFrame,\n\t    TransformJsonNGP,\n", "    TransformJsonNeRFSynthetic,\n\t)\n\tdef to_cpu(array: jax.Array) -> jax.Array:\n\t    return jax.device_put(array, device=jax.devices(\"cpu\")[0])\n\t@jax.jit\n\tdef f32_to_u8(img: jax.Array) -> jax.Array:\n\t    return jnp.clip(jnp.round(img * 255), 0, 255).astype(jnp.uint8)\n\tdef mono_to_rgb(img: jax.Array, cm: Literal[\"inferno\", \"jet\", \"turbo\"]=\"inferno\") -> jax.Array:\n\t    return plt.get_cmap(cm)(img)\n\tdef sharpness_of(path: str | Path) -> float | None:\n", "    try:\n\t        image = Image.open(path)\n\t    except (IsADirectoryError, UnidentifiedImageError) as e:\n\t        warnings.warn(\n\t            \"failed loading '{}': {}\".format(path, str(e)))\n\t        return None\n\t    laplacian = image.convert(\"L\").filter(ImageFilter.FIND_EDGES)\n\t    return float(np.asarray(laplacian).var())\n\tdef video_to_images(\n\t    video_in: Path,\n", "    images_dir: Path,\n\t    fmt: str=\"%04d.png\",\n\t    fps: int=3,\n\t):\n\t    video_in, images_dir = Path(video_in), Path(images_dir)\n\t    images_dir.mkdir(parents=True, exist_ok=True)\n\t    (ffmpeg.input(video_in)\n\t        .output(\n\t            images_dir.joinpath(fmt).as_posix(),\n\t            r=fps,\n", "            pix_fmt=\"rgb24\",  # colmap only supports 8-bit color depth\n\t        )\n\t        .run(\n\t            capture_stdout=False,\n\t            capture_stderr=False,\n\t        )\n\t    )\n\tdef qvec2rotmat(qvec):\n\t    \"copied from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n\t    return np.asarray([\n", "        [\n\t            1 - 2 * qvec[2]**2 - 2 * qvec[3]**2,\n\t            2 * qvec[1] * qvec[2] - 2 * qvec[0] * qvec[3],\n\t            2 * qvec[3] * qvec[1] + 2 * qvec[0] * qvec[2]\n\t        ], [\n\t            2 * qvec[1] * qvec[2] + 2 * qvec[0] * qvec[3],\n\t            1 - 2 * qvec[1]**2 - 2 * qvec[3]**2,\n\t            2 * qvec[2] * qvec[3] - 2 * qvec[0] * qvec[1]\n\t        ], [\n\t            2 * qvec[3] * qvec[1] - 2 * qvec[0] * qvec[2],\n", "            2 * qvec[2] * qvec[3] + 2 * qvec[0] * qvec[1],\n\t            1 - 2 * qvec[1]**2 - 2 * qvec[2]**2\n\t        ]\n\t    ])\n\tdef rotmat(a, b):\n\t    \"copied from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n\t    a, b = a / np.linalg.norm(a), b / np.linalg.norm(b)\n\t    v = np.cross(a, b)\n\t    c = np.dot(a, b)\n\t    # handle exception for the opposite direction input\n", "    if c < -1 + 1e-10:\n\t        return rotmat(a + np.random.uniform(-1e-2, 1e-2, 3), b)\n\t    s = np.linalg.norm(v)\n\t    kmat = np.array([[0, -v[2], v[1]], [v[2], 0, -v[0]], [-v[1], v[0], 0]])\n\t    return np.eye(3) + kmat + kmat.dot(kmat) * ((1 - c) / (s ** 2 + 1e-10))\n\tdef closest_point_2_lines(oa, da, ob, db):\n\t    \"\"\"\n\t    (copied from NVLabs/instant-ngp/scripts/colmap2nerf.py)\n\t    returns point closest to both rays of form o+t*d, and a weight factor that goes to 0 if the lines are parallel\n\t    \"\"\"\n", "    da = da / np.linalg.norm(da)\n\t    db = db / np.linalg.norm(db)\n\t    c = np.cross(da, db)\n\t    denom = np.linalg.norm(c)**2\n\t    t = ob - oa\n\t    ta = np.linalg.det([t, db, c]) / (denom + 1e-10)\n\t    tb = np.linalg.det([t, da, c]) / (denom + 1e-10)\n\t    if ta > 0:\n\t        ta = 0\n\t    if tb > 0:\n", "        tb = 0\n\t    return (oa+ta*da+ob+tb*db) * 0.5, denom\n\tdef write_sharpness_json(raw_images_dir: str | Path):\n\t    raw_images_dir = Path(raw_images_dir)\n\t    out_filename = \"sharpnesses.jaxngp.json\"\n\t    image_candidates = tuple(filter(lambda x: x.name != out_filename, raw_images_dir.iterdir()))\n\t    sharpnesses = ThreadPoolExecutor().map(sharpness_of, image_candidates)\n\t    path_sharpness_tuples = zip(map(lambda p: p.absolute().as_posix(), raw_images_dir.iterdir()), sharpnesses)\n\t    path_sharpness_tuples = filter(lambda tup: tup[1] is not None, path_sharpness_tuples)\n\t    path_sharpness_tuples = sorted(tqdm(path_sharpness_tuples, desc=\"| estimating sharpness of image collection\"), key=lambda tup: tup[1], reverse=True)\n", "    with open(raw_images_dir.joinpath(out_filename), \"w\") as f:\n\t        json.dump(path_sharpness_tuples, f)\n\tdef write_transforms_json(\n\t    scene_root_dir: Path,\n\t    images_dir: Path,\n\t    text_model_dir: Path,\n\t    opts: SceneCreationOptions,\n\t):\n\t    \"adapted from NVLabs/instant-ngp/scripts/colmap2nerf.py\"\n\t    scene_root_dir, images_dir, text_model_dir = (\n", "        Path(scene_root_dir),\n\t        Path(images_dir),\n\t        Path(text_model_dir),\n\t    )\n\t    rel_prefix = images_dir.relative_to(scene_root_dir)\n\t    camera = Camera.from_colmap_txt(text_model_dir.joinpath(\"cameras.txt\"))\n\t    images_txt = text_model_dir.joinpath(\"images.txt\")\n\t    images_lines = list(filter(lambda line: line[0] != \"#\", open(images_txt).readlines()))[::2]\n\t    up = np.zeros(3)\n\t    bottom_row = np.asarray((0, 0, 0, 1.0)).reshape(1, 4)\n", "    frames: List[TransformJsonFrame] = []\n\t    for line in images_lines:\n\t        # IMAGE_ID, QW, QX, QY, QZ, TX, TY, TZ, CAMERA_ID, NAME\n\t        _, qw, qx, qy, qz, tx, ty, tz, _, name = line.strip().split()\n\t        R = qvec2rotmat(tuple(map(float, (qw, qx, qy, qz))))\n\t        T = np.asarray(tuple(map(float, (tx, ty, tz)))).reshape(3, 1)\n\t        m = np.concatenate([R, T], axis=-1)\n\t        m = np.concatenate([m, bottom_row], axis=0)\n\t        c2w = np.linalg.inv(m)\n\t        c2w[0:3,2] *= -1 # flip the y and z axis\n", "        c2w[0:3,1] *= -1\n\t        c2w = c2w[[1,0,2,3],:]\n\t        c2w[2,:] *= -1 # flip whole world upside down\n\t        up += c2w[0:3,1]\n\t        frames.append(TransformJsonFrame(\n\t            file_path=rel_prefix.joinpath(name).as_posix(),\n\t            transform_matrix=c2w.tolist(),\n\t        ))\n\t    # estimate sharpness\n\t    sharpnesses = ThreadPoolExecutor().map(lambda f: sharpness_of(scene_root_dir.joinpath(f.file_path)), frames)\n", "    for i, sharpness in enumerate(tqdm(sharpnesses, total=len(frames), desc=\"| estimating sharpness of image collection\")):\n\t        frames[i] = frames[i].replace(sharpness=sharpness)\n\t    # reorient the scene to be easier to work with\n\t    up = up / np.linalg.norm(up)\n\t    print(\"up vector:\", up, \"->\", [0, 0, 1])\n\t    R = rotmat(up,[0,0,1]) # rotate up vector to [0,0,1]\n\t    R = np.pad(R,[0,1])\n\t    R[-1, -1] = 1\n\t    for i, f in enumerate(frames):\n\t        frames[i] = f.replace(transform_matrix=np.matmul(R, f.transform_matrix_numpy).tolist())\n", "    # find a central point they are all looking at\n\t    totw = 0.0\n\t    totp = np.array([0.0, 0.0, 0.0])\n\t    for f in frames:\n\t        mf = f.transform_matrix_numpy[0:3,:]\n\t        for g in frames:\n\t            mg = g.transform_matrix_numpy[0:3,:]\n\t            p, w = closest_point_2_lines(mf[:,3], mf[:,2], mg[:,3], mg[:,2])\n\t            if w > 1e-5:\n\t                totp += p*w\n", "                totw += w\n\t    if totw > 0.0:\n\t        totp /= totw\n\t    # the cameras are looking at totp\n\t    print(\"the cameras are looking at:\", totp, \"->\", [0, 0, 0])\n\t    for i, f in enumerate(frames):\n\t        new_m = f.transform_matrix_numpy\n\t        new_m[0:3,3] -= totp\n\t        frames[i] = f.replace(transform_matrix=new_m.tolist())\n\t    avglen = 0.\n", "    for f in frames:\n\t        avglen += np.linalg.norm(f.transform_matrix_numpy[0:3,3])\n\t    avglen /= len(frames)\n\t    print(\"average camera distance from origin:\", avglen, \"->\", 4.0)\n\t    for i, f in enumerate(frames):\n\t        # scale to \"nerf sized\"\n\t        new_m = f.transform_matrix_numpy\n\t        new_m[0:3, 3] *= 4.0 / avglen\n\t        frames[i] = f.replace(transform_matrix=new_m.tolist())\n\t    print(\"scene bound (i.e. half width of scene's aabb):\", opts.bound)\n", "    all_transform_json = TransformJsonNGP(\n\t        frames=frames,\n\t        fl_x=camera.fx,\n\t        fl_y=camera.fy,\n\t        cx=camera.cx,\n\t        cy=camera.cy,\n\t        w=camera.width,\n\t        h=camera.height,\n\t        k1=camera.k1,\n\t        k2=camera.k2,\n", "        k3=camera.k3,\n\t        k4=camera.k4,\n\t        p1=camera.p1,\n\t        p2=camera.p2,\n\t        aabb_scale=opts.bound,\n\t    )\n\t    all_transform_json: TransformJsonNGP = all_transform_json.replace(\n\t        scale=opts.camera_scale,\n\t        bg=opts.bg,\n\t        up=[0, 0, 1],\n", "        n_extra_learnable_dims=opts.n_extra_learnable_dims,\n\t    )\n\t    all_transform_json.save(scene_root_dir.joinpath(\"transforms.json\"))\n\t    return all_transform_json\n\tdef create_scene_from_single_camera_image_collection(\n\t    raw_images_dir: Path,\n\t    scene_root_dir: Path,\n\t    opts: SceneCreationOptions,\n\t):\n\t    raw_images_dir, scene_root_dir = Path(raw_images_dir), Path(scene_root_dir)\n", "    scene_root_dir.mkdir(parents=True, exist_ok=True)\n\t    artifacts_dir = scene_root_dir.joinpath(\"artifacts\")\n\t    artifacts_dir.mkdir(parents=True, exist_ok=True)\n\t    db_path = artifacts_dir.joinpath(\"colmap.db\")\n\t    sparse_reconstructions_dir = artifacts_dir.joinpath(\"sparse\")\n\t    undistorted_images_dir = scene_root_dir.joinpath(\"images-undistorted\")\n\t    out_model_dir = (\n\t        undistorted_images_dir.joinpath(\"sparse\")\n\t        if opts.undistort\n\t        else sparse_reconstructions_dir.joinpath(\"0\")\n", "    )\n\t    out_images_dir = (\n\t        undistorted_images_dir.joinpath(\"images\")\n\t        if opts.undistort\n\t        else raw_images_dir\n\t    )\n\t    text_model_dir = artifacts_dir.joinpath(\"text\")\n\t    sfm.extract_features(images_dir=raw_images_dir, db_path=db_path, camera_model=opts.camera_model)\n\t    sfm.match_features(matcher=opts.matcher, db_path=db_path)\n\t    maps = sfm.sparse_reconstruction(\n", "        images_dir=raw_images_dir,\n\t        sparse_reconstructions_dir=sparse_reconstructions_dir,\n\t        db_path=db_path,\n\t        matcher=opts.matcher,\n\t    )\n\t    if len(maps) == 0:\n\t        raise RuntimeError(\"mapping with colmap failed\")\n\t    elif len(maps) > 1:\n\t        warnings.warn(\n\t            \"colmap reconstructed more than 1 maps, we will only use the first map\")\n", "    sparse_recon_dir = sparse_reconstructions_dir.joinpath(\"0\")\n\t    sfm.colmap_bundle_adjustment(sparse_reconstruction_dir=sparse_recon_dir, max_num_iterations=200)\n\t    if opts.undistort:\n\t        sfm.undistort(\n\t            images_dir=raw_images_dir,\n\t            sparse_reconstruction_dir=sparse_recon_dir,\n\t            undistorted_images_dir=undistorted_images_dir,\n\t        )\n\t    sfm.export_text_format_model(\n\t        sparse_reconstruction_dir=out_model_dir,\n", "        text_model_dir=text_model_dir,\n\t    )\n\t    write_transforms_json(\n\t        scene_root_dir=scene_root_dir,\n\t        images_dir=out_images_dir,\n\t        text_model_dir=text_model_dir,\n\t        opts=opts,\n\t    )\n\tdef create_scene_from_video(\n\t    video_path: Path,\n", "    scene_root_dir: Path,\n\t    fps: int,\n\t    opts: SceneCreationOptions,\n\t):\n\t    video_path, scene_root_dir = Path(video_path), Path(scene_root_dir)\n\t    raw_images_dir = scene_root_dir.joinpath(\"images-raw\")\n\t    video_to_images(\n\t        video_in=video_path,\n\t        images_dir=raw_images_dir,\n\t        fps=fps,\n", "    )\n\t    create_scene_from_single_camera_image_collection(\n\t        raw_images_dir=raw_images_dir,\n\t        scene_root_dir=scene_root_dir,\n\t        opts=opts,\n\t    )\n\tdef to_unit_cube_2d(xys: jax.Array, W: int, H: int):\n\t    \"Normalizes coordinate (x, y) into range [0, 1], where 0<=x<W, 0<=y<H\"\n\t    uvs = xys / jnp.asarray([[W-1, H-1]])\n\t    return uvs\n", "@jit_jaxfn_with(static_argnames=[\"height\", \"width\", \"vertical\", \"gap\", \"gap_color\"])\n\tdef side_by_side(\n\t    lhs: jax.Array,\n\t    rhs: jax.Array,\n\t    height: int=None,\n\t    width: int=None,\n\t    vertical: bool=False,\n\t    gap: int=5,\n\t    gap_color: RGBColorU8=(0xab, 0xcd, 0xef),\n\t) -> jax.Array:\n", "    chex.assert_not_both_none(height, width)\n\t    chex.assert_scalar_non_negative(vertical)\n\t    chex.assert_type([lhs, rhs], jnp.uint8)\n\t    if len(lhs.shape) == 2 or lhs.shape[-1] == 1:\n\t        lhs = jnp.tile(lhs[..., None], (1, 1, 3))\n\t    if len(rhs.shape) == 2 or rhs.shape[-1] == 1:\n\t        rhs = jnp.tile(rhs[..., None], (1, 1, 3))\n\t    if rhs.shape[-1] == 3:\n\t        rhs = jnp.concatenate([rhs, 255 * jnp.ones_like(rhs[..., -1:], dtype=jnp.uint8)], axis=-1)\n\t    if lhs.shape[-1] == 3:\n", "        lhs = jnp.concatenate([lhs, 255 * jnp.ones_like(lhs[..., -1:], dtype=jnp.uint8)], axis=-1)\n\t    if vertical:\n\t        chex.assert_axis_dimension(lhs, 1, width)\n\t        chex.assert_axis_dimension(rhs, 1, width)\n\t    else:\n\t        chex.assert_axis_dimension(lhs, 0, height)\n\t        chex.assert_axis_dimension(rhs, 0, height)\n\t    concat_axis = 0 if vertical else 1\n\t    if gap > 0:\n\t        gap_color = jnp.asarray(gap_color + (0xff,), dtype=jnp.uint8)\n", "        gap = jnp.broadcast_to(gap_color, (gap, width, 4) if vertical else (height, gap, 4))\n\t        return jnp.concatenate([lhs, gap, rhs], axis=concat_axis)\n\t    else:\n\t        return jnp.concatenate([lhs, rhs], axis=concat_axis)\n\t@jit_jaxfn_with(static_argnames=[\"border_pixels\", \"color\"])\n\tdef add_border(\n\t    img: jax.Array,\n\t    border_pixels: int=5,\n\t    color: RGBColorU8=(0xfe, 0xdc, 0xba)\n\t) -> jax.Array:\n", "    chex.assert_rank(img, 3)\n\t    chex.assert_axis_dimension(img, -1, 4)\n\t    chex.assert_scalar_non_negative(border_pixels)\n\t    chex.assert_type(img, jnp.uint8)\n\t    color = jnp.asarray(color + (0xff,), dtype=jnp.uint8)\n\t    height, width = img.shape[:2]\n\t    leftright = jnp.broadcast_to(color, (height, border_pixels, 4))\n\t    img = jnp.concatenate([leftright, img, leftright], axis=1)\n\t    topbottom = jnp.broadcast_to(color, (border_pixels, width+2*border_pixels, 4))\n\t    img = jnp.concatenate([topbottom, img, topbottom], axis=0)\n", "    return img\n\t@jax.jit\n\tdef linear_to_db(val: float, maxval: float):\n\t    return 20 * jnp.log10(jnp.sqrt(maxval / val))\n\t@jax.jit\n\tdef psnr(lhs: jax.Array, rhs: jax.Array):\n\t    chex.assert_type([lhs, rhs], jnp.uint8)\n\t    mse = ((lhs.astype(float) - rhs.astype(float)) ** 2).mean()\n\t    return jnp.clip(20 * jnp.log10(255 / jnp.sqrt(mse + 1e-15)), 0, 100)\n\tdef write_video(dest: Path, images: Sequence, *, fps: int=24, loop: int=3):\n", "    images = list(images) * loop\n\t    assert len(images) > 0, \"cannot write empty video\"\n\t    video_writer = imageio.get_writer(dest, mode=\"I\", fps=fps)\n\t    try:\n\t        for im in tqdm(images, desc=\"| writing video to {}\".format(dest.as_posix())):\n\t            video_writer.append_data(np.asarray(im))\n\t    except (BrokenPipeError, IOError) as e:  # sometimes ffmpeg encounters io error for no apparent reason\n\t        warnings.warn(\n\t            \"failed writing video: {}\".format(str(e)), RuntimeWarning)\n\t        warnings.warn(\n", "            \"skipping saving video '{}'\".format(dest.as_posix()), RuntimeWarning)\n\t@jax.jit\n\tdef set_pixels(imgarr: jax.Array, xys: jax.Array, selected: jax.Array, preds: jax.Array) -> jax.Array:\n\t    chex.assert_type(imgarr, jnp.uint8)\n\t    H, W = imgarr.shape[:2]\n\t    if len(imgarr.shape) == 3:\n\t        interm = imgarr.reshape(H*W, -1)\n\t    else:\n\t        interm = imgarr.ravel()\n\t    idcs = xys[selected, 1] * W + xys[selected, 0]\n", "    interm = interm.at[idcs].set(f32_to_u8(preds))\n\t    if len(imgarr.shape) == 3:\n\t        return interm.reshape(H, W, -1)\n\t    else:\n\t        return interm.reshape(H, W)\n\tdef blend_rgba_image_array(imgarr, bg: jax.Array):\n\t    \"\"\"\n\t    Blend the given background color according to the given alpha channel from `imgarr`.\n\t    WARN: this function SHOULD NOT be used for blending background colors into volume-rendered\n\t          pixels because the colors of volume-rendered pixels already have the alpha channel\n", "          factored-in.  To blend background for volume-rendered pixels, directly add the scaled\n\t          background color.\n\t          E.g.: `final_color = ray_accumulated_color + (1 - ray_opacity) * bg`\n\t    \"\"\"\n\t    if isinstance(imgarr, Image.Image):\n\t        imgarr = np.asarray(imgarr)\n\t    chex.assert_shape(imgarr, [..., 4])\n\t    chex.assert_type(imgarr, bg.dtype)\n\t    rgbs, alpha = imgarr[..., :-1], imgarr[..., -1:]\n\t    bg = jnp.broadcast_to(bg, rgbs.shape)\n", "    if imgarr.dtype == jnp.uint8:\n\t        rgbs, alpha = rgbs.astype(float) / 255, alpha.astype(float) / 255\n\t        rgbs = rgbs * alpha + bg * (1 - alpha)\n\t        rgbs = f32_to_u8(rgbs)\n\t    else:\n\t        rgbs = rgbs * alpha + bg * (1 - alpha)\n\t    return rgbs\n\tdef get_xyrgbas(imgarr: jax.Array) -> Tuple[jax.Array, jax.Array]:\n\t    assert imgarr.dtype == jnp.uint8\n\t    H, W, C = imgarr.shape\n", "    x, y = jnp.meshgrid(jnp.arange(W), jnp.arange(H))\n\t    x, y = x.reshape(-1, 1), y.reshape(-1, 1)\n\t    xys = jnp.concatenate([x, y], axis=-1)\n\t    flattened = imgarr.reshape(H*W, C) / 255\n\t    if C == 3:\n\t        # images without an alpha channel is equivalent to themselves with an all-opaque alpha\n\t        # channel\n\t        rgbas = jnp.concatenate([flattened, jnp.ones_like(flattened[:, :1])], axis=-1)\n\t        return xys, rgbas\n\t    elif C == 4:\n", "        rgbas = flattened\n\t        return xys, rgbas\n\t    else:\n\t        raise mkValueError(\n\t            desc=\"number of image channels\",\n\t            value=C,\n\t            type=Literal[3, 4],\n\t        )\n\t_ImageSourceType = jax.Array | np.ndarray | Image.Image | Path | str\n\tdef make_image_metadata(\n", "    image: _ImageSourceType,\n\t    bg: RGBColor,\n\t) -> ImageMetadata:\n\t    if isinstance(image, jax.Array):\n\t        pass\n\t    elif isinstance(image, Image.Image):\n\t        image = jnp.asarray(image)\n\t    elif isinstance(image, (Path, str)):\n\t        image = jnp.asarray(Image.open(image))\n\t    elif isinstance(image, np.ndarray):\n", "        image = jnp.asarray(image)\n\t    else:\n\t        raise mkValueError(\n\t            desc=\"image source type\",\n\t            value=image,\n\t            type=_ImageSourceType,\n\t        )\n\t    raise NotImplementedError(\n\t        \"function get_xyrgbs has been renamed to get_xyrgbas and this part has not been updated \"\n\t        \"accordingly\"\n", "    )\n\t    xys, rgbs = get_xyrgbs(image, bg=bg)\n\t    H, W = image.shape[:2]\n\t    uvs = to_unit_cube_2d(xys, W, H)\n\t    return ImageMetadata(\n\t        H=H,\n\t        W=W,\n\t        xys=jnp.asarray(xys),\n\t        uvs=jnp.asarray(uvs),\n\t        rgbs=jnp.asarray(rgbs),\n", "    )\n\tdef merge_transforms(transforms: Sequence[TransformJsonNGP | TransformJsonNeRFSynthetic]) -> TransformJsonNGP | TransformJsonNeRFSynthetic:\n\t    return functools.reduce(\n\t        lambda lhs, rhs: lhs.merge(rhs) if lhs is not None else rhs,\n\t        transforms,\n\t    )\n\tdef load_transform_json_recursive(src: Path | str) -> TransformJsonNGP | TransformJsonNeRFSynthetic | None:\n\t    \"\"\"\n\t    returns a single transforms object with the `file_path` in its `frames` attribute converted to\n\t    absolute paths\n", "    \"\"\"\n\t    src = Path(src)\n\t    if src.is_dir():\n\t        all_transforms = tuple(filter(\n\t            lambda xform: xform is not None,\n\t            map(load_transform_json_recursive, src.iterdir()),\n\t        ))\n\t        if len(all_transforms) == 0:\n\t            return None\n\t        # merge transforms found from descendants if any\n", "        transforms = merge_transforms(all_transforms)\n\t    elif src.suffix == \".json\":  # skip other files for speed\n\t        try:\n\t            transforms = json.load(open(src))\n\t        except:\n\t            # unreadable, or not a json\n\t            return None\n\t        if isinstance(transforms, dict):\n\t            try:\n\t                transforms = (\n", "                    TransformJsonNeRFSynthetic(**transforms)\n\t                    if transforms.get(\"camera_angle_x\") is not None\n\t                    else TransformJsonNGP(**transforms)\n\t                )\n\t                transforms = transforms.make_absolute(src.parent).scale_camera_positions()\n\t            except TypeError:\n\t                # not a valid transform.json\n\t                return None\n\t        else:\n\t            return None\n", "    else:\n\t        return None\n\t    return transforms\n\tdef try_image_extensions(\n\t    file_path: Path,\n\t    extensions: List[str]=[\"png\", \"jpg\", \"jpeg\"],\n\t) -> Path | None:\n\t    if \"\" not in extensions:\n\t        extensions = [\"\"] + list(extensions)\n\t    for ext in extensions:\n", "        if len(ext) > 0 and ext[0] != \".\":\n\t            ext = \".\" + ext\n\t        p = Path(file_path.as_posix() + ext)\n\t        if p.exists():\n\t            return p\n\t        p = Path(file_path.with_suffix(ext))\n\t        if p.exists():\n\t            return p\n\t    warnings.warn(\n\t        \"could not find a file at '{}' with any extension of {}\".format(file_path, extensions),\n", "        RuntimeWarning,\n\t    )\n\t    return None\n\tdef load_scene(\n\t    srcs: Sequence[Path | str],\n\t    scene_options: SceneOptions,\n\t    sort_frames: bool=False,\n\t) -> SceneData:\n\t    \"\"\"\n\t    Inputs:\n", "        srcs: sequence of paths to recursively load transforms.json\n\t        scene_options: see :class:`SceneOptions`\n\t        sort_frames: whether to sort the frames by their filenames, (uses natural sort if enabled)\n\t    \"\"\"\n\t    assert isinstance(srcs, collections.abc.Sequence) and not isinstance(srcs, str), (\n\t        \"load_scene accepts a sequence of paths as srcs to load, did you mean '{}'?\".format([srcs])\n\t    )\n\t    srcs = list(map(Path, srcs))\n\t    transforms = merge_transforms(map(load_transform_json_recursive, srcs))\n\t    if scene_options.up_unitvec is not None:\n", "        transforms = transforms.replace(up=scene_options.up_unitvec).rotate_world_up()\n\t    if transforms is None:\n\t        raise FileNotFoundError(\"could not load transforms from any of {}\".format(srcs))\n\t    loaded_frames, discarded_frames = functools.reduce(\n\t        lambda prev, frame: (\n\t            prev[0] + ((frame,) if (\n\t                frame.file_path is not None\n\t                and frame.sharpness >= scene_options.sharpness_threshold\n\t            ) else ()),\n\t            prev[1] + (() if (\n", "                frame.file_path is not None\n\t                and frame.sharpness >= scene_options.sharpness_threshold\n\t            ) else (frame,)),\n\t        ),\n\t        map(\n\t            lambda f: f.replace(file_path=try_image_extensions(f.file_path)),\n\t            transforms.frames,\n\t        ),\n\t        (tuple(), tuple()),\n\t    )\n", "    if len(loaded_frames) == 0:\n\t        raise RuntimeError(\"loaded 0 frame from '{}' (discarded {} frame(s))\".format(srcs, len(discarded_frames)))\n\t    transforms = transforms.replace(frames=loaded_frames)\n\t    if sort_frames:\n\t        transforms = transforms.replace(\n\t            frames=natsorted(transforms.frames, key=lambda f: f.file_path),\n\t        )\n\t    # shared camera model\n\t    if isinstance(transforms, TransformJsonNeRFSynthetic):\n\t        _img = Image.open(try_image_extensions(transforms.frames[0].file_path))\n", "        fovx = transforms.camera_angle_x\n\t        focal = float(.5 * _img.width / np.tan(fovx / 2))\n\t        camera = Camera(\n\t            width=_img.width,\n\t            height=_img.height,\n\t            fx=focal,\n\t            fy=focal,\n\t            cx=_img.width / 2,\n\t            cy=_img.height / 2,\n\t            near=scene_options.camera_near,\n", "        )\n\t    elif isinstance(transforms, TransformJsonNGP):\n\t        camera = Camera(\n\t            width=transforms.w,\n\t            height=transforms.h,\n\t            fx=transforms.fl_x,\n\t            fy=transforms.fl_y,\n\t            cx=transforms.cx,\n\t            cy=transforms.cy,\n\t            near=scene_options.camera_near,\n", "            k1=transforms.k1,\n\t            k2=transforms.k2,\n\t            k3=transforms.k3,\n\t            k4=transforms.k4,\n\t            p1=transforms.p1,\n\t            p2=transforms.p2,\n\t            model=transforms.camera_model,\n\t        )\n\t    else:\n\t        raise TypeError(\"unexpected type for transforms: {}, expected one of {}\".format(\n", "            type(transforms),\n\t            [TransformJsonNeRFSynthetic, TransformJsonNGP],\n\t        ))\n\t    scene_meta = SceneMeta(\n\t        bound=scene_options.bound\n\t            if scene_options.bound is not None\n\t            else transforms.aabb_scale,\n\t        bg=transforms.bg,\n\t        camera=camera.scale_resolution(scene_options.resolution_scale),\n\t        n_extra_learnable_dims=transforms.n_extra_learnable_dims,\n", "        frames=transforms.frames,\n\t    )\n\t    return SceneData(meta=scene_meta, max_mem_mbytes=scene_options.max_mem_mbytes)\n\t@jit_jaxfn_with(static_argnames=[\"size\", \"loop\", \"shuffle\"])\n\tdef make_permutation(\n\t    key: jran.KeyArray,\n\t    size: int,\n\t    loop: int=1,\n\t    shuffle: bool=True,\n\t) -> jax.Array:\n", "    if shuffle:\n\t        perm = jran.permutation(key, size * loop)\n\t    else:\n\t        perm = jnp.arange(size * loop)\n\t    return perm % size\n\tdef main():\n\t    scene, views = load_scene(\n\t        rootdir=\"data/nerf/nerf_synthetic/lego\",\n\t        split=\"train\",\n\t    )\n", "    print(scene.all_xys.shape)\n\t    print(scene.all_rgbas.shape)\n\t    print(scene.all_transforms.shape)\n\t    print(len(views))\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "utils/sfm.py", "chunked_list": ["from pathlib import Path\n\tfrom typing import Dict\n\tfrom typing_extensions import get_args\n\timport pycolmap\n\tfrom .common import mkValueError\n\tfrom .types import CameraModelType, ColmapMatcherType\n\tdef extract_features(\n\t    images_dir: Path,\n\t    db_path: Path,\n\t    camera_model: CameraModelType,\n", "):\n\t    images_dir, db_path = Path(images_dir), Path(db_path)\n\t    pycolmap.extract_features(\n\t        database_path=db_path,\n\t        image_path=images_dir,\n\t        # REF:\n\t        #   <https://github.com/colmap/colmap/blob/43de802cfb3ed2bd155150e7e5e3e8c8dd5aaa3e/src/exe/feature.h#L44-L52>\n\t        #   <https://github.com/colmap/pycolmap/blob/bdcdf47e0d40240c6f53dc463d7ceaa2cef923fd/pipeline/extract_features.cc#L63>\n\t        camera_mode=\"SINGLE\",\n\t        # REF:\n", "        #   <https://github.com/colmap/pycolmap/blob/bdcdf47e0d40240c6f53dc463d7ceaa2cef923fd/pipeline/extract_features.cc#L64>\n\t        camera_model=camera_model,\n\t        reader_options=pycolmap.ImageReaderOptions(\n\t            # camera_model=\"OPENCV\",  # NOTE: this is obsolete, see camera_model above\n\t            # single_camera=True,  # NOTE: this is obsolete, see camera_mode above\n\t        ),\n\t        sift_options=pycolmap.SiftExtractionOptions(\n\t            estimate_affine_shape=True,\n\t            domain_size_pooling=True,\n\t        ),\n", "    )\n\tdef match_features(\n\t    matcher: ColmapMatcherType,\n\t    db_path: Path,\n\t):\n\t    db_path = Path(db_path)\n\t    if matcher not in get_args(ColmapMatcherType):\n\t        raise mkValueError(\n\t            desc=\"colmap matcher\",\n\t            value=matcher,\n", "            type=ColmapMatcherType,\n\t        )\n\t    match_fn = getattr(pycolmap, \"match_{}\".format(matcher.lower()))\n\t    return match_fn(\n\t        database_path=db_path,\n\t        sift_options=pycolmap.SiftMatchingOptions(\n\t            guided_matching=True,\n\t        ),\n\t    )\n\tdef sparse_reconstruction(\n", "    images_dir: Path,\n\t    sparse_reconstructions_dir: Path,\n\t    db_path: Path,\n\t    matcher: ColmapMatcherType,\n\t) -> Dict[int, pycolmap.Reconstruction]:\n\t    images_dir, sparse_reconstructions_dir = Path(images_dir), Path(sparse_reconstructions_dir)\n\t    mapping_options = pycolmap.IncrementalMapperOptions(\n\t        # principal point estimation is an ill-posed problem in general (`False` is already the\n\t        # default, setting to False here explicitly works as a reminder to self)\n\t        ba_refine_principal_point=False,\n", "        # <colmap/colmap>:src/colmap/util/option_manager.cc:ModifyForExtremeQuality\n\t        ba_local_max_num_iterations=40,\n\t        ba_local_max_refinements=3,\n\t        ba_global_max_num_iterations=100,\n\t        # below 3 options are for individual/video data, for internet photos, they should be left\n\t        # default\n\t        # <colmap/colmap>:src/colmap/util/option_manager.cc:ModifyForVideoData,ModifyForIndividualData\n\t        min_focal_length_ratio=0.1,\n\t        max_focal_length_ratio=10,\n\t        max_extra_param=1e15,\n", "    )\n\t    if matcher == \"Sequential\":\n\t        # <colmap/colmap>:src/colmap/util/option_manager.cc:ModifyForVideoData\n\t        mapping_options.ba_global_images_ratio = 1.4\n\t        mapping_options.ba_global_points_ratio = 1.4\n\t    maps = pycolmap.incremental_mapping(\n\t        database_path=db_path,\n\t        image_path=images_dir,\n\t        output_path=sparse_reconstructions_dir,\n\t        options=mapping_options,\n", "    )\n\t    return maps\n\tdef colmap_bundle_adjustment(\n\t    sparse_reconstruction_dir: Path,\n\t    max_num_iterations: int,\n\t) -> pycolmap.Reconstruction:\n\t    sparse_reconstruction_dir = Path(sparse_reconstruction_dir)\n\t    ba_options = {\n\t        \"refine_principal_point\": True,\n\t        \"solver_options\": {\n", "            \"max_num_iterations\": max_num_iterations,\n\t        },\n\t    }\n\t    recon = pycolmap.bundle_adjustment(\n\t        input_path=sparse_reconstruction_dir,\n\t        output_path=sparse_reconstruction_dir,\n\t        options=ba_options,\n\t    )\n\t    return recon\n\tdef undistort(\n", "    images_dir: Path,\n\t    sparse_reconstruction_dir: Path,\n\t    undistorted_images_dir: Path,\n\t):\n\t    images_dir, sparse_reconstruction_dir, undistorted_images_dir = (\n\t        Path(images_dir),\n\t        Path(sparse_reconstruction_dir),\n\t        Path(undistorted_images_dir),\n\t    )\n\t    pycolmap.undistort_images(\n", "        output_path=undistorted_images_dir,\n\t        input_path=sparse_reconstruction_dir,\n\t        image_path=images_dir,\n\t    )\n\tdef export_text_format_model(\n\t    sparse_reconstruction_dir: Path,\n\t    text_model_dir: Path,\n\t):\n\t    sparse_reconstruction_dir, text_model_dir = (\n\t        Path(sparse_reconstruction_dir),\n", "        Path(text_model_dir),\n\t    )\n\t    text_model_dir.mkdir(parents=True, exist_ok=True)\n\t    reconstruction = pycolmap.Reconstruction(sparse_reconstruction_dir)\n\t    reconstruction.write_text(text_model_dir.as_posix())\n"]}
{"filename": "utils/common.py", "chunked_list": ["from concurrent.futures import Executor, Future, ThreadPoolExecutor\n\timport functools\n\timport logging\n\timport logging\n\timport os\n\tfrom pathlib import Path\n\timport random\n\timport shutil\n\tfrom typing import Any, Dict, Hashable, Iterable, Sequence, get_args\n\timport colorama\n", "from colorama import Back, Fore, Style\n\tfrom flax.metrics import tensorboard\n\timport git\n\timport jax\n\tfrom jax._src.lib import xla_client as xc\n\timport jax.random as jran\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom tqdm import tqdm as tqdm_original\n\tfrom ._constants import tqdm_format\n", "from .types import LogLevel\n\tclass Logger(logging.Logger):\n\t    _tb: tensorboard.SummaryWriter | None=None\n\t    _executor: Executor | None=None\n\t    _last_job: Future=None\n\t    def __init__(self, name: str, level: int | LogLevel) -> None:\n\t        super().__init__(name, level)\n\t    def setup_tensorboard(self, tb: tensorboard.SummaryWriter, executor: Executor) -> None:\n\t        self._tb = tb\n\t        self._executor = executor\n", "    def wait_last_job(self):\n\t        if self._last_job is not None and not self._last_job.done():\n\t            return self._last_job.result()\n\t    def write_scalar(self, tag: str, value: Any, step: int) -> None:\n\t        if self._tb is not None:\n\t            self.wait_last_job()\n\t            # NOTE: writing scalars is fast(ish) enough to not need a thread pool\n\t            self._executor.submit(self._tb.scalar, tag, value, step)\n\t    def write_image(self, tag: str, image: Any, step: int, max_outputs: int) -> None:\n\t        if self._tb is not None:\n", "            self.wait_last_job()\n\t            self._last_job = self._executor.submit(self._tb.image, tag, image, step, max_outputs)\n\t    def write_hparams(self, hparams: Dict[str, Any]) -> None:\n\t        if self._tb is not None:\n\t            self.wait_last_job()\n\t            self._last_job = self._executor.submit(self._tb.hparams, hparams)\n\t    def write_metrics_to_tensorboard(\n\t        self,\n\t        metrics: Dict[str, jax.Array | float],\n\t        step: jax.Array | int,\n", "    ) -> None:\n\t        def linear_to_db(val: float, maxval: float):\n\t            return 20 * np.log10(np.sqrt(maxval / val))\n\t        self.write_scalar(\n\t            \"batch/‚Üìloss (rgb)\",\n\t            metrics[\"loss\"][\"rgb\"],\n\t            step,\n\t        )\n\t        self.write_scalar(\n\t            \"batch/‚Üëestimated PSNR (db)\",\n", "            linear_to_db(metrics[\"loss\"][\"rgb\"], maxval=1.),\n\t            step,\n\t        )\n\t        self.write_scalar(\n\t            \"batch/‚Üìloss (total variation)\",\n\t            metrics[\"loss\"][\"total_variation\"],\n\t            step,\n\t        )\n\t        self.write_scalar(\n\t            \"batch/effective batch size (not compacted)\",\n", "            metrics[\"measured_batch_size_before_compaction\"],\n\t            step,\n\t        )\n\t        self.write_scalar(\n\t            \"batch/‚Üëeffective batch size (compacted)\",\n\t            metrics[\"measured_batch_size\"],\n\t            step,\n\t        )\n\t        self.write_scalar(\n\t            \"rendering/‚Üìeffective samples per ray\",\n", "            metrics[\"measured_batch_size\"] / metrics[\"n_valid_rays\"],\n\t            step,\n\t        )\n\t        self.write_scalar(\n\t            \"rendering/‚Üìmarched samples per ray\",\n\t            metrics[\"measured_batch_size_before_compaction\"] / metrics[\"n_valid_rays\"],\n\t            step,\n\t        )\n\t        self.write_scalar(\n\t            \"rendering/‚Üënumber of marched rays\",\n", "            metrics[\"n_valid_rays\"],\n\t            step,\n\t        )\n\ttqdm = functools.partial(tqdm_original, bar_format=tqdm_format)\n\tdef backup_current_codebase(\n\t    exp_dir: Path | str,\n\t    /,\n\t    name_prefix: str,\n\t    note: str | None,\n\t) -> Path:\n", "    \"\"\"Backup current codebase to a directory named 'src' under the specified `exp_dir` directory,\n\t    creating it if it does not exist.\n\t    \"\"\"\n\t    repo = git.Repo(\".\", search_parent_directories=True)\n\t    try:\n\t        __initial_commit = repo.commit(\"aa9f8c73c2a2d164e9e117b99a6543893eeed23f\")\n\t    except ValueError:\n\t        raise ValueError(\"This is not the jaxngp repository\")\n\t    os.chdir(repo.git.working_dir)\n\t    exp_dir = Path(exp_dir)\n", "    save_root_dir = exp_dir.joinpath(\"runs\")\n\t    save_root_dir.mkdir(parents=False, exist_ok=True)\n\t    epoch = 0\n\t    def save_dir_name(note: str | None) -> str:\n\t        ret = \"{}{:04d}\".format(name_prefix, epoch)\n\t        if note is not None:\n\t            ret += \"#\" + note\n\t        return ret\n\t    save_dir = save_root_dir.joinpath(save_dir_name(note=note))\n\t    while len(tuple(save_root_dir.glob(save_dir_name(note=None) + \"*\"))) > 0:\n", "        epoch += 1\n\t        save_dir = save_root_dir.joinpath(save_dir_name(note=note))\n\t    latest_run_lnk = exp_dir.joinpath(\"{}latest-run\".format(name_prefix))\n\t    if latest_run_lnk.exists():\n\t        if (\n\t            latest_run_lnk.is_symlink()\n\t            and latest_run_lnk.readlink().parent.absolute() == save_dir.parent.absolute()\n\t        ):\n\t            latest_run_lnk.unlink()\n\t        else:\n", "            raise RuntimeError(\n\t                \"the path '{}' exists but is not a symlink to a previous run\".format(latest_run_lnk)\n\t            )\n\t    elif latest_run_lnk.is_symlink():  # the link does not exist, but it is a symlink, that makes it a broken symlink\n\t        latest_run_lnk.unlink()\n\t    save_dir.mkdir(parents=False, exist_ok=False)\n\t    latest_run_lnk.symlink_to(save_dir.absolute())\n\t    shutil.copyfile(\"flake.nix\", save_dir.joinpath(\"flake.nix\"))\n\t    shutil.copyfile(\"flake.lock\", save_dir.joinpath(\"flake.lock\"))\n\t    shutil.copyfile(\"pyproject.toml\", save_dir.joinpath(\"pyproject.toml\"))\n", "    shutil.copyfile(\"README.md\", save_dir.joinpath(\"README.md\"))\n\t    def ignored_files(dir, files):\n\t        return [\n\t            \"result\",\n\t            \".clangd\",\n\t            \".clang-format\",\n\t        ]\n\t    shutil.copytree(\"app\", save_dir.joinpath(\"app\"), dirs_exist_ok=False, ignore=ignored_files)\n\t    shutil.copytree(\"deps\", save_dir.joinpath(\"deps\"), dirs_exist_ok=False, ignore=ignored_files)\n\t    shutil.copytree(\"models\", save_dir.joinpath(\"models\"), dirs_exist_ok=False, ignore=ignored_files)\n", "    shutil.copytree(\"utils\", save_dir.joinpath(\"utils\"), dirs_exist_ok=False, ignore=ignored_files)\n\t    with open(save_dir.joinpath(\"commit-sha\"), \"w\") as f:\n\t        f.write(repo.head.object.hexsha)\n\t    with open(save_dir.joinpath(\"working-directory\"), \"w\") as f:\n\t        f.write(os.getcwd())\n\t    return save_dir\n\tdef compose(*fns):\n\t    def _inner(x):\n\t        for fn in fns:\n\t            x = fn(x)\n", "        return x\n\t    return _inner\n\tdef mkValueError(desc, value, type):\n\t    variants = get_args(type)\n\t    assert value not in variants\n\t    return ValueError(\"Unexpected {}: '{}', expected one of [{}]\".format(desc, value, \"|\".join(variants)))\n\t# NOTE:\n\t#   Jitting a vmapped function seems to give the desired performance boost, while vmapping a jitted\n\t#   function might not work at all.  Except for the experiments I conducted myself, some related\n\t#   issues:\n", "# REF:\n\t#   * <https://github.com/google/jax/issues/6312>\n\t#   * <https://github.com/google/jax/issues/7449>\n\tdef vmap_jaxfn_with(\n\t        # kwargs copied from `jax.vmap` source\n\t        in_axes: int | Sequence[Any]=0,\n\t        out_axes: Any = 0,\n\t        axis_name: Hashable | None = None,\n\t        axis_size: int | None = None,\n\t        spmd_axis_name: Hashable | None = None,\n", "    ):\n\t    return functools.partial(\n\t        jax.vmap,\n\t        in_axes=in_axes,\n\t        out_axes=out_axes,\n\t        axis_name=axis_name,\n\t        axis_size=axis_size,\n\t        spmd_axis_name=spmd_axis_name,\n\t    )\n\tdef jit_jaxfn_with(\n", "        # kwargs copied from `jax.jit` source\n\t        static_argnums: int | Iterable[int] | None = None,\n\t        static_argnames: str | Iterable[str] | None = None,\n\t        device: xc.Device | None = None,\n\t        backend: str | None = None,\n\t        donate_argnums: int | Iterable[int] = (),\n\t        inline: bool = False,\n\t        keep_unused: bool = False,\n\t        abstracted_axes: Any | None = None,\n\t    ):\n", "    return functools.partial(\n\t        jax.jit,\n\t        static_argnums=static_argnums,\n\t        static_argnames=static_argnames,\n\t        device=device,\n\t        backend=backend,\n\t        donate_argnums=donate_argnums,\n\t        inline=inline,\n\t        keep_unused=keep_unused,\n\t        abstracted_axes=abstracted_axes,\n", "    )\n\tdef setup_logging(\n\t    name: str,\n\t    /,\n\t    file: str | Path | None=None,\n\t    with_tensorboard: bool=False,\n\t    level: LogLevel=\"INFO\",\n\t    file_level: LogLevel=\"DEBUG\",\n\t) -> Logger:\n\t    colorama.just_fix_windows_console()\n", "    class _formatter(logging.Formatter):\n\t        def __init__(self, datefmt, rich_color: bool):\n\t            fore = {\n\t                \"blue\": Fore.BLUE if rich_color else \"[\",\n\t                \"green\": Fore.GREEN if rich_color else \"[\",\n\t                \"yellow\": Fore.YELLOW if rich_color else \"[\",\n\t                \"red\": Fore.RED if rich_color else \"[\",\n\t                \"black\": Fore.BLACK if rich_color else \"[\",\n\t            }\n\t            back = {\n", "                \"red\": Back.RED if rich_color else \"[\",\n\t                \"yellow\": Back.YELLOW if rich_color else \"[\",\n\t            }\n\t            style = {\n\t                \"bright\": Style.BRIGHT if rich_color else \"[\",\n\t                \"reset_all\": Style.RESET_ALL if rich_color else \"]\",\n\t            }\n\t            pathfmt = \"%(module)s::%(funcName)s\"\n\t            fmt = \"| %(asctime)s.%(msecs)03dZ LVL {bold}{pathfmt}{reset}: %(message)s\".format(\n\t                bold=style[\"bright\"],\n", "                pathfmt=pathfmt,\n\t                reset=style[\"reset_all\"],\n\t            )\n\t            formats = {\n\t                logging.DEBUG: fmt.replace(\"LVL\", fore[\"blue\"] + \"DEBUG\" + style[\"reset_all\"]),\n\t                logging.INFO: fmt.replace(\"LVL\", \" \" + fore[\"green\"] + \"INFO\" + style[\"reset_all\"]),\n\t                logging.WARN: fmt.replace(\"LVL\", \" \" + back[\"yellow\"] + fore[\"black\"] + \"WARN\" + style[\"reset_all\"]),\n\t                logging.ERROR: fmt.replace(\"LVL\", back[\"red\"] + fore[\"black\"] + \"ERROR\" + style[\"reset_all\"]),\n\t                logging.CRITICAL: fmt.replace(\"LVL\", \" \" + back[\"red\"] + fore[\"black\"] + style[\"bright\"] + \"CRIT\" + style[\"reset_all\"]),\n\t            }\n", "            self.formatters = {\n\t                level: logging.Formatter(fmt=format, datefmt=datefmt)\n\t                for level, format in formats.items()\n\t            }\n\t        def format(self, record):\n\t            return self.formatters.get(record.levelno).format(record)\n\t    datefmt = \"%Y-%m-%dT%T\"\n\t    logger = Logger(name=name, level=level)\n\t    logger.propagate = False\n\t    # console handler\n", "    ch = logging.StreamHandler()\n\t    ch.setLevel(level)\n\t    ch.setFormatter(_formatter(datefmt=datefmt, rich_color=True))\n\t    logger.addHandler(ch)\n\t    logger.setLevel(level)\n\t    # file handler\n\t    if file is not None:\n\t        fh = logging.FileHandler(filename=file)\n\t        fh.setLevel(file_level)\n\t        fh.setFormatter(_formatter(datefmt=datefmt, rich_color=False))\n", "        logger.addHandler(fh)\n\t        def loglevel2int(log_level: LogLevel) -> int:\n\t            return getattr(logging, log_level)\n\t        logger.setLevel(min(loglevel2int(level), loglevel2int(file_level)))\n\t    if with_tensorboard:\n\t        tb = tensorboard.SummaryWriter(log_dir=file.parent, auto_flush=True)\n\t        executor = ThreadPoolExecutor(\n\t            max_workers=1,\n\t            thread_name_prefix=\"logger({})-\".format(name),\n\t        )\n", "        logger.setup_tensorboard(tb=tb, executor=executor)\n\t    # logger complains about `warn` being deprecated with another warning\n\t    logger.warn = logger.warning\n\t    return logger\n\tdef set_deterministic(seed: int) -> jran.KeyArray:\n\t    random.seed(seed)\n\t    np.random.seed(seed)\n\t    tf.random.set_seed(seed)\n\t    return jran.PRNGKey(seed)\n"]}
{"filename": "utils/args.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom pathlib import Path\n\tfrom typing import Tuple\n\timport tyro\n\tfrom utils.types import (\n\t    CameraOverrideOptions,\n\t    LogLevel,\n\t    OrbitTrajectoryOptions,\n\t    RayMarchingOptions,\n\t    RenderingOptions,\n", "    SceneOptions,\n\t    TransformsProvider,\n\t)\n\t@dataclass(frozen=True, kw_only=True)\n\tclass CommonArgs:\n\t    # log level\n\t    logging: LogLevel = \"INFO\"\n\t    # random seed\n\t    seed: int = 1_000_000_007\n\t    # display model information after model init\n", "    summary: bool=False\n\t@dataclass(frozen=True, kw_only=True)\n\tclass TrainingArgs:\n\t    # learning rate\n\t    lr: float\n\t    # scalar multiplied to total variation loss, set this to a positive value to enable calculation\n\t    # of TV loss\n\t    tv_scale: float\n\t    # batch size\n\t    bs: int\n", "    # number of epochs to train\n\t    epochs: int\n\t    # batches per epoch\n\t    iters: int\n\t    # loop within training data for this number of iterations, this helps reduce the effective\n\t    # dataloader overhead.\n\t    data_loop: int\n\t    # will validate every `validate_every` epochs, set this to a large value to disable validation\n\t    validate_every: int\n\t    # number of latest checkpoints to keep\n", "    keep: int=1\n\t    # how many epochs should a new checkpoint to be kept (in addition to keeping the last `keep`\n\t    # checkpoints)\n\t    keep_every: int | None=8\n\t    @property\n\t    def keep_every_n_steps(self) -> int | None:\n\t        if self.keep_every is None:\n\t            return None\n\t        else:\n\t            return self.keep_every * self.iters\n", "@dataclass(frozen=True, kw_only=True)\n\tclass ImageFitArgs:\n\t    common: tyro.conf.OmitArgPrefixes[CommonArgs]=CommonArgs()\n\t    train: tyro.conf.OmitArgPrefixes[TrainingArgs]=TrainingArgs(\n\t        # paper:\n\t        #   We observed fastest convergence with a learning rate of 10^{-4} for signed distance\n\t        #   functions and 10^{-2} otherwise\n\t        #\n\t        # We use a smaller learning rate since our batch size is much smaller the paper (see below).\n\t        lr=1e-3,\n", "        tv_scale=0.,\n\t        # paper:\n\t        #   ...as well a a batch size of 2^{14} for neural radiance caching and 2^{18} otherwise.\n\t        #\n\t        # In our case, setting the batch size to a larger number hinders data loading performance,\n\t        # and thus causes the GPU not being fully occupied.  On the other hand, setting the batch\n\t        # size to a smaller one utilizes the GPU fully, but the iterations per second capped at\n\t        # some rate which results in lower throughput. setting bs to 2^{10} achieves a satisfying\n\t        # tradeoff here.\n\t        bs=2**10,\n", "        epochs=32,\n\t        iters=2**30,\n\t        data_loop=1,\n\t        validate_every=1,\n\t    )\n\t@dataclass(frozen=True, kw_only=True)\n\tclass NeRFArgsBase:\n\t    raymarch: RayMarchingOptions\n\t    render: RenderingOptions\n\t    scene: SceneOptions\n", "    common: tyro.conf.OmitArgPrefixes[CommonArgs]=CommonArgs()\n\t@dataclass(frozen=True, kw_only=True)\n\tclass _SharedNeRFTrainingArgs(NeRFArgsBase):\n\t    # each experiment run will save its own code, config, training logs, and checkpoints into a\n\t    # separate subdirectory of this path\n\t    exp_dir: Path\n\t    # directories or transform.json files containing data for training\n\t    frames_train: tyro.conf.Positional[Tuple[Path, ...]]\n\t    # an optional description of this run\n\t    note: str | None=None\n", "    # directories or transform.json files containing data for validation\n\t    frames_val: Tuple[Path, ...]=()\n\t    # if specified, continue training from this checkpoint\n\t    ckpt: Path | None=None\n\t    # training hyper parameters\n\t    train: tyro.conf.OmitArgPrefixes[TrainingArgs]=TrainingArgs(\n\t        # This is a relatively large learning rate, should be used jointly with\n\t        # `threasholded_exponential` as density activation, and random color as supervision for\n\t        # transparent pixels.\n\t        lr=1e-2,\n", "        tv_scale=0.,\n\t        bs=1024 * (1<<10),\n\t        epochs=50,\n\t        iters=2**10,\n\t        data_loop=1,\n\t        validate_every=10,\n\t    )\n\t    # raymarching/rendering options during training\n\t    raymarch: RayMarchingOptions=RayMarchingOptions(\n\t        diagonal_n_steps=1<<10,\n", "        perturb=True,\n\t        density_grid_res=128,\n\t    )\n\t    render: RenderingOptions=RenderingOptions(\n\t        bg=(1.0, 1.0, 1.0),  # white, but ignored by default due to random_bg=True\n\t        random_bg=True,\n\t    )\n\t    scene: SceneOptions=SceneOptions(\n\t        sharpness_threshold=-1.,\n\t        resolution_scale=1.0,\n", "        camera_near=0.3,\n\t        max_mem_mbytes=2500,  # ~300 1920x1080 8bit RGBA images\n\t    )\n\t    # raymarching/rendering options for validating during training\n\t    raymarch_eval: RayMarchingOptions=RayMarchingOptions(\n\t        diagonal_n_steps=1<<10,\n\t        perturb=False,\n\t        density_grid_res=128,\n\t    )\n\t    render_eval: RenderingOptions=RenderingOptions(\n", "        bg=(0.0, 0.0, 0.0),  # black\n\t        random_bg=False,\n\t    )\n\t@dataclass(frozen=True, kw_only=True)\n\tclass NeRFTrainingArgs(_SharedNeRFTrainingArgs): ...\n\t@dataclass(frozen=True, kw_only=True)\n\tclass NeRFTestingArgs(NeRFArgsBase):\n\t    # testing logs and results are saved to this directory, overwriting its content if any\n\t    logs_dir: Path\n\t    frames: tyro.conf.Positional[Tuple[Path, ...]]\n", "    camera_override: CameraOverrideOptions=CameraOverrideOptions()\n\t    # use checkpoint from this path (can be a directory) for testing\n\t    ckpt: Path\n\t    # if specified, render with a generated orbiting trajectory instead of the loaded frame\n\t    # transformations\n\t    trajectory: TransformsProvider=\"loaded\"\n\t    orbit: OrbitTrajectoryOptions\n\t    # naturally sort frames according to their file names before testing\n\t    sort_frames: bool=False\n\t    # if specified value contains \"video\", a video will be saved; if specified value contains\n", "    # \"image\", rendered images will be saved.  Value can contain both \"video\" and \"image\", e.g.,\n\t    # `--save-as \"video-image\"` will save both video and images.\n\t    save_as: str=\"image and video\"\n\t    # specifies frames per second for saved video\n\t    fps: int=24\n\t    # loop rendered images this many times in saved video\n\t    loop: int=3\n\t    # raymarching/rendering options during testing\n\t    raymarch: RayMarchingOptions=RayMarchingOptions(\n\t        diagonal_n_steps=1<<10,\n", "        perturb=False,\n\t        density_grid_res=128,\n\t    )\n\t    render: RenderingOptions=RenderingOptions(\n\t        bg=(0.0, 0.0, 0.0),  # black\n\t        random_bg=False,\n\t    )\n\t    scene: SceneOptions=SceneOptions(\n\t        sharpness_threshold=-1.,\n\t        resolution_scale=1.0,\n", "        camera_near=0.3,\n\t        max_mem_mbytes=0,  # this is testing, no images is loaded to GPU\n\t    )\n\t    @property\n\t    def report_metrics(self) -> bool:\n\t        return self.camera_override.enabled or self.trajectory != \"loaded\"\n\t@dataclass(frozen=True, kw_only=True)\n\tclass NeRFGUIArgs(_SharedNeRFTrainingArgs):\n\t    @dataclass(frozen=True, kw_only=True)\n\t    class ViewportOptions:\n", "        W: int=1024\n\t        H: int=768\n\t        resolution_scale: float=0.3\n\t        control_window_width: int=300\n\t        #max number of loss steps shown on gui\n\t        max_show_loss_step: int=200\n\t    viewport: ViewportOptions=ViewportOptions()\n\t    train: TrainingArgs=TrainingArgs(\n\t        lr=1e-2,\n\t        tv_scale=0.,\n", "        bs=1<<18,\n\t        iters=5,  # render a frame every 5 steps\n\t        epochs=50,  # ignored\n\t        data_loop=1,  # ignored\n\t        validate_every=10,  # ignored\n\t    )\n"]}
{"filename": "models/nerfs.py", "chunked_list": ["import functools\n\tfrom typing import Callable, List, Tuple\n\timport flax.linen as nn\n\timport jax\n\tfrom jax.nn.initializers import Initializer\n\timport jax.numpy as jnp\n\tfrom models.encoders import (\n\t    Encoder,\n\t    FrequencyEncoder,\n\t    HashGridEncoder,\n", "    SphericalHarmonicsEncoder,\n\t    SphericalHarmonicsEncoderCuda,\n\t    TCNNHashGridEncoder,\n\t)\n\tfrom utils.common import mkValueError\n\tfrom utils.types import (\n\t    ActivationType,\n\t    DirectionalEncodingType,\n\t    PositionalEncodingType,\n\t    empty_impl,\n", ")\n\t@empty_impl\n\tclass NeRF(nn.Module):\n\t    bound: float\n\t    position_encoder: Encoder\n\t    direction_encoder: Encoder\n\t    density_mlp: nn.Module\n\t    rgb_mlp: nn.Module\n\t    density_activation: Callable\n\t    rgb_activation: Callable\n", "    @nn.compact\n\t    def __call__(\n\t        self,\n\t        xyz: jax.Array,\n\t        dir: jax.Array | None,\n\t        appearance_embeddings: jax.Array | None,\n\t    ) -> jax.Array | Tuple[jax.Array, jax.Array]:\n\t        \"\"\"\n\t        Inputs:\n\t            xyz `[..., 3]`: coordinates in $\\R^3$\n", "            dir `[..., 3]`: **unit** vectors, representing viewing directions.  If `None`, only\n\t                            return densities\n\t            appearance_embeddings `[..., n_extra_learnable_dims]` or `[n_extra_learnable_dims]`:\n\t                per-image latent code to model illumination, if it's a 1D vector of length\n\t                `n_extra_learnable_dims`, all sampled points will use this embedding.\n\t        Returns:\n\t            density `[..., 1]`: density (ray terminating probability) of each query points\n\t            rgb `[..., 3]`: predicted color for each query point\n\t        \"\"\"\n\t        original_aux_shapes = xyz.shape[:-1]\n", "        n_samples = functools.reduce(int.__mul__, original_aux_shapes)\n\t        xyz = xyz.reshape(n_samples, 3)\n\t        # [n_samples, D_pos], `float32`\n\t        pos_enc, tv = self.position_encoder(xyz, self.bound)\n\t        x = self.density_mlp(pos_enc)\n\t        # [n_samples, 1], [n_samples, density_MLP_out-1]\n\t        density, _ = jnp.split(x, [1], axis=-1)\n\t        if dir is None:\n\t            return density.reshape(*original_aux_shapes, 1), tv\n\t        dir = dir.reshape(n_samples, 3)\n", "        # [n_samples, D_dir]\n\t        dir_enc = self.direction_encoder(dir)\n\t        # [n_samples, 3]\n\t        rgb = self.rgb_mlp(jnp.concatenate([\n\t            x,\n\t            dir_enc,\n\t            jnp.broadcast_to(appearance_embeddings, (n_samples, appearance_embeddings.shape[-1])),\n\t        ], axis=-1))\n\t        density, rgb = self.density_activation(density), self.rgb_activation(rgb)\n\t        return jnp.concatenate([density, rgb], axis=-1).reshape(*original_aux_shapes, 4), tv\n", "class CoordinateBasedMLP(nn.Module):\n\t    \"Coordinate-based MLP\"\n\t    # hidden layer widths\n\t    Ds: List[int]\n\t    out_dim: int\n\t    skip_in_layers: List[int]\n\t    # as described in the paper\n\t    kernel_init: Initializer=nn.initializers.glorot_uniform()\n\t    @nn.compact\n\t    def __call__(self, x: jax.Array) -> jax.Array:\n", "        in_x = x\n\t        for i, d in enumerate(self.Ds):\n\t            if i in self.skip_in_layers:\n\t                x = jnp.concatenate([in_x, x], axis=-1)\n\t            x = nn.Dense(\n\t                d,\n\t                use_bias=False,\n\t                kernel_init=self.kernel_init,\n\t            )(x)\n\t            x = nn.relu(x)\n", "        x = nn.Dense(\n\t            self.out_dim,\n\t            use_bias=False,\n\t            kernel_init=self.kernel_init,\n\t        )(x)\n\t        return x\n\tclass BackgroundModel(nn.Module): ...\n\t@empty_impl\n\tclass SkySphereBg(BackgroundModel):\n\t    \"\"\"\n", "    A sphere that centers at the origin and encloses a bounded scene and provides all the background\n\t    color, this is an over-simplified model.\n\t    When a ray intersects with the sphere from inside, it calculates the intersection point's\n\t    coordinate and predicts a color based on the intersection point and the viewing direction.\n\t    \"\"\"\n\t    # radius\n\t    r: float\n\t    # encoder for position\n\t    position_encoder: Encoder\n\t    # encoder for viewing direction\n", "    direction_encoder: Encoder\n\t    # color predictor\n\t    rgb_mlp: CoordinateBasedMLP\n\t    activation: Callable\n\t    @nn.compact\n\t    def __call__(\n\t        self,\n\t        rays_o: jax.Array,\n\t        rays_d: jax.Array,\n\t        appearance_embeddings: jax.Array,\n", "    ) -> jax.Array:\n\t        # the distance of a point (o+td) on the ray to the origin is given by:\n\t        #\n\t        #   dist(t) = (dx^2 + dy^2 + dz^2)t^2 + 2(dx*ox + dy*oy + dz*oz)t + ox^2 + oy^2 + oz^2\n\t        #\n\t        # the minimal distance is achieved when\n\t        #\n\t        #   2(dx^2 + dy^2 + dz^2)t + 2(dx*ox + dy*oy + dz*oz) = 0,\n\t        #       ==> t = -(dx*ox + dy*oy + dz*oz) / (dx^2 + dy^2 + dz^2)\n\t        a = (rays_d * rays_d).sum(axis=-1)\n", "        b = 2 * (rays_o * rays_d).sum(axis=-1)\n\t        c = (rays_o * rays_o).sum(axis=-1) - self.r ** 2\n\t        # if min_dist < self.r, there are at most two intersections, given by:\n\t        #\n\t        #   dist(t) = r^2\n\t        #\n\t        # want the farther intersection point\n\t        t = jnp.maximum(\n\t            (-b + jnp.sqrt(b ** 2 - 4 * a * c)) / (2 * a),\n\t            (-b - jnp.sqrt(b ** 2 - 4 * a * c)) / (2 * a),\n", "        )\n\t        t = t.reshape(-1, 1)\n\t        finite_mask = jnp.isfinite(t)\n\t        pos = rays_o + t * rays_d\n\t        pos_dirs = jnp.where(\n\t            finite_mask,\n\t            pos / (jnp.linalg.norm(pos) + 1e-15),\n\t            0.,\n\t        )\n\t        n_rays = functools.reduce(int.__mul__, rays_d.shape[:-1])\n", "        # we then encode the positions/directions, and predict a view-dependent color for each ray\n\t        pos_enc = self.position_encoder(pos_dirs)\n\t        dir_enc = self.direction_encoder(rays_d)\n\t        appearance_embeddings = jnp.broadcast_to(\n\t            appearance_embeddings,\n\t            shape=(n_rays, appearance_embeddings.shape[-1]),\n\t        )\n\t        colors = self.rgb_mlp(jnp.concatenate([pos_enc, dir_enc, appearance_embeddings], axis=-1))\n\t        colors = self.activation(colors)\n\t        return jnp.where(\n", "            finite_mask,\n\t            colors,\n\t            0.,\n\t        )\n\tdef make_activation(act: ActivationType):\n\t    if act == \"sigmoid\":\n\t        return nn.sigmoid\n\t    elif act == \"exponential\":\n\t        return jnp.exp\n\t    elif act == \"truncated_exponential\":\n", "        @jax.custom_vjp\n\t        def trunc_exp(x):\n\t            \"Exponential function, except its gradient calculation uses a truncated input value\"\n\t            return jnp.exp(x)\n\t        def __fwd_trunc_exp(x):\n\t            y = trunc_exp(x)\n\t            aux = x  # aux contains additional information that is useful in the backward pass\n\t            return y, aux\n\t        def __bwd_trunc_exp(aux, grad_y):\n\t            # REF: <https://github.com/NVlabs/instant-ngp/blob/d0d35d215c7c63c382a128676f905ecb676fa2b8/src/testbed_nerf.cu#L303>\n", "            grad_x = jnp.exp(jnp.clip(aux, -15, 15)) * grad_y\n\t            return (grad_x, )\n\t        trunc_exp.defvjp(\n\t            fwd=__fwd_trunc_exp,\n\t            bwd=__bwd_trunc_exp,\n\t        )\n\t        return trunc_exp\n\t    elif act == \"thresholded_exponential\":\n\t        def thresh_exp(x, thresh):\n\t            \"\"\"\n", "            Exponential function translated along -y direction by 1e-2, and thresholded to have\n\t            non-negative values.\n\t            \"\"\"\n\t            # paper:\n\t            #   the occupancy grids ... is updated every 16 steps ... corresponds to thresholding\n\t            #   the opacity of a minimal ray marching step by 1 ‚àí exp(‚àí0.01) ‚âà 0.01\n\t            return nn.relu(jnp.exp(x) - thresh)\n\t        return functools.partial(thresh_exp, thresh=1e-2)\n\t    elif act == \"truncated_thresholded_exponential\":\n\t        @jax.custom_vjp\n", "        def trunc_thresh_exp(x, thresh):\n\t            \"\"\"\n\t            Exponential, but value is translated along -y axis by value `thresh`, negative values\n\t            are removed, and gradient is truncated.\n\t            \"\"\"\n\t            return nn.relu(jnp.exp(x) - thresh)\n\t        def __fwd_trunc_threash_exp(x, thresh):\n\t            y = trunc_thresh_exp(x, thresh=thresh)\n\t            aux = x, thresh  # aux contains additional information that is useful in the backward pass\n\t            return y, aux\n", "        def __bwd_trunc_threash_exp(aux, grad_y):\n\t            x, thresh = aux\n\t            grad_x = jnp.exp(jnp.clip(x, -15, 15)) * grad_y\n\t            # clip gradient for values that has been thresholded by relu during forward pass\n\t            grad_x = jnp.signbit(jnp.log(thresh) - x) * grad_x\n\t            # first tuple element is gradient for input, second tuple element is gradient for the\n\t            # `threshold` value.\n\t            return (grad_x, 0)\n\t        trunc_thresh_exp.defvjp(\n\t            fwd=__fwd_trunc_threash_exp,\n", "            bwd=__bwd_trunc_threash_exp,\n\t        )\n\t        return functools.partial(trunc_thresh_exp, thresh=1e-2)\n\t    elif act == \"relu\":\n\t        return nn.relu\n\t    else:\n\t        raise mkValueError(\n\t            desc=\"activation\",\n\t            value=act,\n\t            type=ActivationType,\n", "        )\n\tdef make_nerf(\n\t    bound: float,\n\t    # encodings\n\t    pos_enc: PositionalEncodingType,\n\t    dir_enc: DirectionalEncodingType,\n\t    # total variation\n\t    tv_scale: float,\n\t    # encoding levels\n\t    pos_levels: int,\n", "    dir_levels: int,\n\t    # layer widths\n\t    density_Ds: List[int],\n\t    rgb_Ds: List[int],\n\t    # output dimensions\n\t    density_out_dim: int,\n\t    rgb_out_dim: int,\n\t    # skip connections\n\t    density_skip_in_layers: List[int],\n\t    rgb_skip_in_layers: List[int],\n", "    # activations\n\t    density_act: ActivationType,\n\t    rgb_act: ActivationType,\n\t) -> NeRF:\n\t    if pos_enc == \"identity\":\n\t        position_encoder = lambda x: x\n\t    elif pos_enc == \"frequency\":\n\t        raise NotImplementedError(\"Frequency encoding for NeRF is not tuned\")\n\t        position_encoder = FrequencyEncoder(L=10)\n\t    elif \"hashgrid\" in pos_enc:\n", "        HGEncoder = TCNNHashGridEncoder if \"tcnn\" in pos_enc else HashGridEncoder\n\t        position_encoder = HGEncoder(\n\t            L=pos_levels,\n\t            T=2**19,\n\t            F=2,\n\t            N_min=2**4,\n\t            N_max=int(2**11 * bound),\n\t            tv_scale=tv_scale,\n\t            param_dtype=jnp.float32,\n\t        )\n", "    else:\n\t        raise mkValueError(\n\t            desc=\"positional encoding\",\n\t            value=pos_enc,\n\t            type=PositionalEncodingType,\n\t        )\n\t    if dir_enc == \"identity\":\n\t        direction_encoder = lambda x: x\n\t    elif dir_enc == \"sh\":\n\t        direction_encoder = SphericalHarmonicsEncoder(L=dir_levels)\n", "    elif dir_enc == \"shcuda\":\n\t        direction_encoder = SphericalHarmonicsEncoderCuda(L=dir_levels)\n\t    else:\n\t        raise mkValueError(\n\t            desc=\"directional encoding\",\n\t            value=dir_enc,\n\t            type=DirectionalEncodingType,\n\t        )\n\t    density_mlp = CoordinateBasedMLP(\n\t        Ds=density_Ds,\n", "        out_dim=density_out_dim,\n\t        skip_in_layers=density_skip_in_layers\n\t    )\n\t    rgb_mlp = CoordinateBasedMLP(\n\t        Ds=rgb_Ds,\n\t        out_dim=rgb_out_dim,\n\t        skip_in_layers=rgb_skip_in_layers\n\t    )\n\t    density_activation = make_activation(density_act)\n\t    rgb_activation = make_activation(rgb_act)\n", "    model = NeRF(\n\t        bound=bound,\n\t        position_encoder=position_encoder,\n\t        direction_encoder=direction_encoder,\n\t        density_mlp=density_mlp,\n\t        rgb_mlp=rgb_mlp,\n\t        density_activation=density_activation,\n\t        rgb_activation=rgb_activation,\n\t    )\n\t    return model\n", "def make_skysphere_background_model(\n\t    radius: float,\n\t    pos_levels: int,\n\t    dir_levels: int,\n\t    Ds: List[int],\n\t    skip_in_layers: List[int],\n\t    act: ActivationType,\n\t) -> SkySphereBg:\n\t    position_encoder = SphericalHarmonicsEncoder(L=pos_levels)\n\t    direction_encoder = SphericalHarmonicsEncoder(L=dir_levels)\n", "    rgb_mlp = CoordinateBasedMLP(\n\t        Ds=Ds,\n\t        out_dim=3,\n\t        skip_in_layers=skip_in_layers,\n\t    )\n\t    activation = make_activation(act)\n\t    return SkySphereBg(\n\t        r=radius,\n\t        position_encoder=position_encoder,\n\t        direction_encoder=direction_encoder,\n", "        rgb_mlp=rgb_mlp,\n\t        activation=activation,\n\t    )\n\tdef make_skysphere_background_model_ngp(bound: float) -> SkySphereBg:\n\t    return make_skysphere_background_model(\n\t        radius=bound*4,\n\t        pos_levels=2,\n\t        dir_levels=4,\n\t        Ds=[32, 32],\n\t        skip_in_layers=[],\n", "        act=\"sigmoid\",\n\t    )\n\tdef make_nerf_ngp(\n\t    bound: float,\n\t    inference: bool,\n\t    tv_scale: float=0.,\n\t) -> NeRF:\n\t    return make_nerf(\n\t        bound=bound,\n\t        pos_enc=(\n", "            # TCNN's hash grid encoding runs faster at inference-time, but during training the JAX\n\t            # implementation is slightly faster.  It's possible to use one at training-time and\n\t            # another at inference-time, because the two implementations optimizes in an identical\n\t            # way (up to numerical error).\n\t            \"tcnn-hashgrid\"\n\t            if inference\n\t            else \"hashgrid\"\n\t        ),\n\t        dir_enc=\"sh\",\n\t        tv_scale=tv_scale,\n", "        pos_levels=16,\n\t        dir_levels=4,\n\t        density_Ds=[64],\n\t        density_out_dim=16,\n\t        density_skip_in_layers=[],\n\t        density_act=\"truncated_exponential\",\n\t        rgb_Ds=[64, 64],\n\t        rgb_out_dim=3,\n\t        rgb_skip_in_layers=[],\n\t        rgb_act=\"sigmoid\",\n", "    )\n\tdef make_debug_nerf(bound: float) -> NeRF:\n\t    return NeRF(\n\t        bound=bound,\n\t        position_encoder=lambda x: x,\n\t        direction_encoder=lambda x: x,\n\t        density_mlp=CoordinateBasedMLP(\n\t            Ds=[64],\n\t            out_dim=16,\n\t            skip_in_layers=[],\n", "        ),\n\t        rgb_mlp=CoordinateBasedMLP(\n\t            Ds=[64, 64],\n\t            out_dim=3,\n\t            skip_in_layers=[],\n\t        ),\n\t        density_activation=lambda x: x,\n\t        rgb_activation=lambda x: x,\n\t    )\n\tdef make_test_cube(width: int, bound: float, density: float=32) -> NeRF:\n", "    @jax.jit\n\t    @jax.vmap\n\t    def cube_density_fn(x: jax.Array) -> jax.Array:\n\t        # x is pre-normalized unit cube, we map it back to specified aabb.\n\t        x = (x + bound) / (2 * bound)\n\t        mask = (abs(x).max(axis=-1, keepdims=True) <= width/2).astype(float)\n\t        # concatenate input xyz for use in later rgb querying\n\t        return jnp.concatenate([density * mask, x], axis=-1)\n\t    @jax.jit\n\t    @jax.vmap\n", "    def cube_rgb_fn(density_xyz_dir: jax.Array) -> jax.Array:\n\t        # xyz(3) + dir(3), only take xyz to infer color\n\t        x = density_xyz_dir[:3]\n\t        x = jnp.clip(x, -width/2, width/2)\n\t        return x / width + .5\n\t    return NeRF(\n\t        bound=bound,\n\t        position_encoder=lambda x: x,\n\t        direction_encoder=lambda x: x,\n\t        density_mlp=cube_density_fn,\n", "        rgb_mlp=cube_rgb_fn,\n\t        density_activation=lambda x: x,\n\t        rgb_activation=lambda x: x,\n\t    )\n\tdef main():\n\t    import jax.numpy as jnp\n\t    import jax.random as jran\n\t    KEY = jran.PRNGKey(0)\n\t    KEY, key = jran.split(KEY, 2)\n\t    m = make_nerf_ngp()\n", "    xyz = jnp.ones((100, 3))\n\t    dir = jnp.ones((100, 2))\n\t    params = m.init(key, xyz, dir)\n\t    print(m.tabulate(key, xyz, dir))\n\t    m = make_test_cube(\n\t        width=2,\n\t        bound=1,\n\t        density=32,\n\t    )\n\t    # params = m.init(key, xyz, dir)\n", "    # print(m.tabulate(key, xyz, dir))\n\t    density, rgb = m.apply(\n\t        {},\n\t        jnp.asarray([[0, 0, 0], [1, 1, 1], [1.1, 0, 0], [0.6, 0.9, -0.5], [0.99, 0.99, 0.99]]),\n\t        jnp.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 1, 0], [1, 0, 0]]),\n\t    )\n\t    print(density)\n\t    print(rgb)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "models/imagefit.py", "chunked_list": ["from typing import Literal\n\timport chex\n\timport flax.linen as nn\n\tfrom flax.linen.dtypes import Dtype\n\timport jax\n\tfrom models.encoders import FrequencyEncoder, HashGridEncoder\n\tclass ImageFitter(nn.Module):\n\t    encoding: Literal[\"hashgrid\", \"frequency\"]\n\t    encoding_dtype: Dtype\n\t    @nn.compact\n", "    def __call__(self, uv: jax.Array) -> jax.Array:\n\t        \"\"\"\n\t        Inputs:\n\t            uv [..., 2]: coordinates in $\\\\R^2$ (normalized in range [0, 1]).\n\t        Returns:\n\t            rgb [..., 3]: predicted color for each input uv coordinate (normalized in range [0, 1]).\n\t        \"\"\"\n\t        chex.assert_axis_dimension(uv, -1, 2)\n\t        if self.encoding == \"hashgrid\":\n\t            # [..., L*F]\n", "            x = HashGridEncoder(\n\t                dim=2,\n\t                L=16,\n\t                # ~1Mi entries per level\n\t                T=2**20,\n\t                F=2,\n\t                N_min=16,\n\t                N_max=2**19,  # 524288\n\t                param_dtype=self.encoding_dtype,\n\t            )(uv)\n", "        elif self.encoding == \"frequency\":\n\t            # [..., dim*L]\n\t            x = FrequencyEncoder(dim=2, L=10)(uv)\n\t        else:\n\t            raise ValueError(\"Unexpected encoding type '{}'\".format(self.encoding))\n\t        DenseLayer = lambda dim, name: nn.Dense(\n\t                features=dim,\n\t                name=name,\n\t                # the paper uses glorot initialization, in practice glorot initialization converges\n\t                # to a better result than kaiming initialization, though the gap is small.\n", "                # TODO:\n\t                #   experiment with initializers (or not)\n\t                kernel_init=nn.initializers.lecun_normal(),\n\t                bias_init=nn.initializers.zeros,\n\t                param_dtype=x.dtype\n\t            )\n\t        # feed to the MLP\n\t        x = DenseLayer(128, name=\"linear1\")(x)\n\t        x = nn.relu(x)\n\t        x = DenseLayer(128, name=\"linear2\")(x)\n", "        x = nn.relu(x)\n\t        if self.encoding == \"frequency\":\n\t            x = nn.relu(DenseLayer(256, name=\"linear3\")(x))\n\t            x = nn.relu(DenseLayer(512, name=\"linear4\")(x))\n\t            x = nn.relu(DenseLayer(512, name=\"linear5\")(x))\n\t            x = nn.relu(DenseLayer(512, name=\"linear6\")(x))\n\t            x = nn.relu(DenseLayer(512, name=\"linear7\")(x))\n\t        x = nn.Dense(3, name=\"color_predictor\", param_dtype=x.dtype)(x)\n\t        rgb = nn.sigmoid(x)\n\t        return rgb\n"]}
{"filename": "models/encoders.py", "chunked_list": ["import math\n\timport chex\n\timport flax.linen as nn\n\tfrom flax.linen.dtypes import Dtype\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\tfrom jaxtcnn import HashGridMetadata, hashgrid_encode\n\timport shjax\n\tfrom utils.common import jit_jaxfn_with, vmap_jaxfn_with\n", "from utils.types import empty_impl\n\tcell_vert_offsets = {\n\t    2: jnp.asarray([\n\t        [0., 0.],\n\t        [0., 1.],\n\t        [1., 0.],\n\t        [1., 1.],\n\t    ]),\n\t    3: jnp.asarray([\n\t        [0., 0., 0.],\n", "        [0., 0., 1.],\n\t        [0., 1., 0.],\n\t        [0., 1., 1.],\n\t        [1., 0., 0.],\n\t        [1., 0., 1.],\n\t        [1., 1., 0.],\n\t        [1., 1., 1.],\n\t    ]),\n\t}\n\tadjacent_offsets = {\n", "    2: jnp.asarray([\n\t        [0., 1.],\n\t        [1., 0.],\n\t        [0., -1.],\n\t        [-1., 0.],\n\t    ]),\n\t    3: jnp.asarray([\n\t        [0., 0., 1.],\n\t        [0., 1., 0.],\n\t        [1., 0., 0.],\n", "        [0., 0., -1.],\n\t        [0., -1., 0.],\n\t        [-1., 0., 0.],\n\t    ]),\n\t}\n\tclass Encoder(nn.Module): ...\n\t# TODO: enforce types used in arrays\n\t@empty_impl\n\tclass HashGridEncoder(Encoder):\n\t    # Let's use the same notations as in the paper\n", "    # Number of levels (16).\n\t    L: int\n\t    # Maximum entries per level (hash table size) (2**14 to 2**24).\n\t    T: int\n\t    # Number of feature dimensions per entry (2).\n\t    F: int\n\t    # Coarsest resolution (16).\n\t    N_min: int\n\t    # Finest resolution (512 to 524288).\n\t    N_max: int\n", "    tv_scale: float\n\t    param_dtype: Dtype = jnp.float32\n\t    @property\n\t    def b(self) -> float:\n\t        # Equation(3)\n\t        # Essentially, it is $(n_max / n_min) ** (1/(L - 1))$\n\t        return math.exp((math.log(self.N_max) - math.log(self.N_min)) / (self.L - 1))\n\t    @nn.compact\n\t    def __call__(self, pos: jax.Array, bound: float) -> jax.Array:\n\t        dim = pos.shape[-1]\n", "        # CAVEAT: hashgrid encoder is defined only in the unit cube [0, 1)^3\n\t        pos = (pos + bound) / (2 * bound)\n\t        scales, resolutions, first_hash_level, offsets = [], [], 0, [0]\n\t        for i in range(self.L):\n\t            scale = self.N_min * (self.b**i) - 1\n\t            scales.append(scale)\n\t            res = math.ceil(scale) + 1\n\t            resolutions.append(res)\n\t            n_entries = res ** dim\n\t            if n_entries <= self.T:\n", "                first_hash_level += 1\n\t            else:\n\t                n_entries = self.T\n\t            offsets.append(offsets[-1] + n_entries)\n\t        latents = self.param(\n\t            \"latent codes stored on grid vertices\",\n\t            # paper:\n\t            #   We initialize the hash table entries using the uniform distribution U(‚àí10^{‚àí4}, 10^{‚àí4})\n\t            #   to provide a small amount of randomness while encouraging initial predictions close\n\t            #   to zero.\n", "            lambda key, shape, dtype: jran.uniform(key, shape, dtype, -1e-4, 1e-4),\n\t            (offsets[-1], self.F),\n\t            self.param_dtype,\n\t        )\n\t        @jax.vmap\n\t        @jax.vmap\n\t        def make_vert_pos(pos_scaled: jax.Array):\n\t            # [dim]\n\t            pos_floored = jnp.floor(pos_scaled)\n\t            # [2**dim, dim]\n", "            vert_pos = pos_floored[None, :] + cell_vert_offsets[dim]\n\t            return vert_pos.astype(jnp.uint32)\n\t        @jax.vmap\n\t        @jax.vmap\n\t        def make_adjacent_pos(pos_scaled: jax.Array):\n\t            # [dim]\n\t            pos_floored = jnp.floor(pos_scaled)\n\t            # [dim * 2, dim]\n\t            adjacent_pos = pos_floored[None, :] + adjacent_offsets[dim]\n\t            return adjacent_pos.astype(jnp.uint32)\n", "        @vmap_jaxfn_with(in_axes=(0, 0))\n\t        @vmap_jaxfn_with(in_axes=(None, 0))\n\t        def make_tiled_indices(res, vert_pos):\n\t            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n\t            Inputs:\n\t                res `uint32` `[L]`: each hierarchy's resolution\n\t                vert_pos `uint32` `[L, n_points, B, dim]`: integer positions of grid cell's\n\t                                                           vertices, of each level\n\t            Returns:\n\t                indices `uint32` `[L, n_points, B]`: grid cell indices of the vertices\n", "            \"\"\"\n\t            # [dim]\n\t            if dim == 2:\n\t                strides = jnp.stack([jnp.ones_like(res), res]).T\n\t            elif dim == 3:\n\t                strides = jnp.stack([jnp.ones_like(res), res, res ** 2]).T\n\t            else:\n\t                raise NotImplementedError(\"{} is only implemented for 2D and 3D data\".format(__class__.__name__))\n\t            # [2**dim]\n\t            indices = jnp.sum(strides[None, :] * vert_pos, axis=-1)\n", "            return indices\n\t        @jax.vmap\n\t        @jax.vmap\n\t        def make_hash_indices(vert_pos):\n\t            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n\t            Inputs:\n\t                vert_pos `uint32` `[L, n_points, B, dim]`: integer positions of grid cell's\n\t                                                           vertices, of each level\n\t            Returns:\n\t                indices `uint32` `[L, n_points, B]`: grid cell indices of the vertices\n", "            \"\"\"\n\t            # use primes as reported in the paper\n\t            primes = jnp.asarray([1, 2_654_435_761, 805_459_861], dtype=jnp.uint32)\n\t            # [2**dim]\n\t            if dim == 2:\n\t                indices = vert_pos[:, 0] ^ (vert_pos[:, 1] * primes[1])\n\t            elif dim == 3:\n\t                indices = vert_pos[:, 0] ^ (vert_pos[:, 1] * primes[1]) ^ (vert_pos[:, 2] * primes[2])\n\t            else:\n\t                raise NotImplementedError(\"{} is only implemented for 2D and 3D data\".format(__class__.__name__))\n", "            return indices\n\t        def make_indices(vert_pos, resolutions, first_hash_level):\n\t            if first_hash_level > 0:\n\t                resolutions = jnp.asarray(resolutions, dtype=jnp.uint32)\n\t                indices = make_tiled_indices(resolutions[:first_hash_level], vert_pos[:first_hash_level, ...])\n\t            else:\n\t                indices = jnp.empty(0, dtype=jnp.uint32)\n\t            if first_hash_level < self.L:\n\t                indices = jnp.concatenate([indices, make_hash_indices(vert_pos[first_hash_level:, ...])], axis=0)\n\t            indices = jnp.mod(indices, self.T)\n", "            indices += jnp.asarray(offsets[:-1], dtype=jnp.uint32)[:, None, None]\n\t            return indices\n\t        @jax.vmap\n\t        @jax.vmap\n\t        def lerp_weights(pos_scaled: jax.Array):\n\t            \"\"\"(first 2 axes `[L, n_points]` are vmapped away)\n\t            Inputs:\n\t                pos_scaled `float` `[L, n_points, dim]`: coordinates of query points, scaled to the\n\t                                                         hierarchy in question\n\t            Returns:\n", "                weights `float` `[L, n_points, 2**dim]`: linear interpolation weights for each cell\n\t                                                         vertex\n\t            \"\"\"\n\t            # [dim]\n\t            pos_offset, _ = jnp.modf(pos_scaled)\n\t            # [2**dim, dim]\n\t            widths = jnp.clip(\n\t                # cell_vert_offsets: [2**dim, dim]\n\t                (1 - cell_vert_offsets[dim]) + (2 * cell_vert_offsets[dim] - 1) * pos_offset[None, :],\n\t                0,\n", "                1,\n\t            )\n\t            # [2**dim]\n\t            return jnp.prod(widths, axis=-1)\n\t        # [L]\n\t        scales = jnp.asarray(scales, dtype=jnp.float32)\n\t        # [L, n_points, dim]\n\t        pos_scaled = pos[None, :, :] * scales[:, None, None] + 0.5\n\t        # [L, n_points, 2**dim, dim]\n\t        vert_pos = make_vert_pos(pos_scaled)\n", "        # [L, n_points, 2**dim]\n\t        indices = make_indices(vert_pos, resolutions, first_hash_level)\n\t        # [L, n_points, 2**dim, F]\n\t        vert_latents = latents[indices]\n\t        # [L, n_points, 2**dim]\n\t        vert_weights = lerp_weights(pos_scaled)\n\t        # [L, n_points, F]\n\t        encodings = (vert_latents * vert_weights[..., None]).sum(axis=-2)\n\t        # [n_points, L*F]\n\t        encodings = encodings.transpose(1, 0, 2).reshape(-1, self.L * self.F)\n", "        ## Total variation\n\t        if self.tv_scale > 0:\n\t            # [L, n_points, dim * 2, dim]\n\t            adjacent_pos = make_adjacent_pos(pos_scaled)\n\t            # [L, n_points, dim * 2]\n\t            adjacent_indices = make_indices(adjacent_pos, resolutions, first_hash_level)\n\t            # [L, n_points, dim * 2, F]\n\t            adjacent_latents = latents[adjacent_indices]\n\t            # [L, n_points, dim * 2, F]\n\t            tv = self.tv_scale * jnp.square(adjacent_latents - vert_latents[:, :, :1, :])\n", "            # [L, n_points]\n\t            tv = tv.sum(axis=(-2, -1))\n\t            tv = tv.mean()\n\t        else:\n\t            tv = 0\n\t        return encodings, tv\n\t@empty_impl\n\tclass TCNNHashGridEncoder(HashGridEncoder):\n\t    @nn.compact\n\t    def __call__(self, pos: jax.Array, bound: float) -> jax.Array:\n", "        dim = pos.shape[-1]\n\t        # CAVEAT: hashgrid encoder is defined only in the unit cube [0, 1)^3\n\t        pos = (pos + bound) / (2 * bound)\n\t        scales, resolutions, first_hash_level, offsets = [], [], 0, [0]\n\t        for i in range(self.L):\n\t            scale = self.N_min * (self.b**i) - 1\n\t            scales.append(scale)\n\t            res = math.ceil(scale) + 1\n\t            resolutions.append(res)\n\t            n_entries = res ** dim\n", "            if n_entries <= self.T:\n\t                first_hash_level += 1\n\t            else:\n\t                n_entries = self.T\n\t            offsets.append(offsets[-1] + n_entries)\n\t        latents = self.param(\n\t            \"latent codes stored on grid vertices\",\n\t            # paper:\n\t            #   We initialize the hash table entries using the uniform distribution U(‚àí10^{‚àí4}, 10^{‚àí4})\n\t            #   to provide a small amount of randomness while encouraging initial predictions close\n", "            #   to zero.\n\t            lambda key, shape, dtype: jran.uniform(key, shape, dtype, -1e-4, 1e-4),\n\t            (offsets[-1], self.F),\n\t            self.param_dtype,\n\t        )\n\t        return hashgrid_encode(\n\t            desc=HashGridMetadata(\n\t                L=self.L,\n\t                F=self.F,\n\t                N_min=self.N_min,\n", "                per_level_scale=self.b,\n\t            ),\n\t            offset_table_data=jnp.asarray(offsets, dtype=jnp.uint32),\n\t            coords_rm=pos.T,\n\t            params=latents,\n\t        ).T, 0\n\t@empty_impl\n\tclass FrequencyEncoder(Encoder):\n\t    \"\"\"\n\t    Frequency encoding from Equation(4) of the NeRF paper, except the encoded frequency orders are\n", "    different.\n\t    \"\"\"\n\t    # number of frequencies\n\t    L: int\n\t    # NOTE:\n\t    #   adding @nn.compact makes this not directly callable (CallCompactUnboundModuleError)\n\t    #   See: <https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.CallCompactUnboundModuleError>\n\t    #\n\t    # @nn.compact\n\t    # TODO:\n", "    #   using a function for this (vmap, then jit the vmapped function) seems to be faster (~47ms vs\n\t    #   ~57ms)\n\t    def __call__(self, pos: jax.Array, bound: float) -> jax.Array:\n\t        \"\"\"\n\t        Inuts:\n\t            pos [..., dim]: `dim`-d coordinates to be frequency-encoded\n\t        Returns:\n\t            encodings [..., 2*dim*L]: frequency-encoded coordinates\n\t        \"\"\"\n\t        dim = pos.shape[-1]\n", "        pos = (pos + bound) / (2 * bound)\n\t        # [..., dim, L]: 2^{l} * pi * p\n\t        A = jnp.exp2(jnp.arange(self.L)) * jnp.pi * pos[..., None]\n\t        # [..., dim, L], [..., dim, L]\n\t        senc, cenc = jnp.sin(A), jnp.cos(A)\n\t        # [..., dim*L], [..., dim*L]\n\t        senc, cenc = senc.reshape(*A.shape[:-2], dim*self.L), cenc.reshape(*A.shape[:-2], dim*self.L)\n\t        # [..., 2*dim*L]\n\t        encodings = jnp.stack([senc, cenc], axis=-1).reshape(*A.shape[:-2], 2*dim*self.L)\n\t        return (\n", "            encodings,\n\t            0.,  # placeholder for the total variation loss\n\t        )\n\t@empty_impl\n\tclass SphericalHarmonicsEncoderCuda(Encoder):\n\t    # highest degree\n\t    L: int\n\t    def __call__(self, dirs: jax.Array) -> jax.Array:\n\t        \"Just a thin wrapper on top of :func:`shjax.spherical_harmonics_encoding()`\"\n\t        return shjax.spherical_harmonics_encoding(dirs, self.L)\n", "@empty_impl\n\tclass SphericalHarmonicsEncoder(Encoder):\n\t    # highest degree\n\t    L: int\n\t    def __call__(self, dirs: jax.Array) -> jax.Array:\n\t        \"\"\"\n\t        Adapted from <https://github.com/NVlabs/tiny-cuda-nn/blob/39df2387a684e4fe0cfa33542aebf5eab237716b/include/tiny-cuda-nn/encodings/spherical_harmonics.h#L52-L123>\n\t        Inputs:\n\t            dirs [..., 3]: **unit** vectors, representing directions.\n\t        Returns:\n", "            encodings [..., L**2]: real parts of the spherical harmonics up to the L-th degree.\n\t        \"\"\"\n\t        chex.assert_axis_dimension(dirs, -1, 3)\n\t        x, y, z = dirs[..., 0], dirs[..., 1], dirs[..., 2]\n\t        xy, xz, yz = x*y, x*z, y*z\n\t        x2, y2, z2 = x*x, y*y, z*z\n\t        x4, y4, z4 = x2*x2, y2*y2, z2*z2\n\t        x6, y6, z6 = x4*x2, y4*y2, z4*z2\n\t        encodings = jnp.empty((*dirs.shape[:-1], self.L**2))\n\t        encodings = encodings.at[..., 0].set(0.28209479177387814)  # 1/(2*sqrt(pi))\n", "        if self.L <= 1: return encodings\n\t        encodings = encodings.at[..., 1].set(-0.48860251190291987*y)  # -sqrt(3)*y/(2*sqrt(pi))\n\t        encodings = encodings.at[..., 2].set(0.48860251190291987*z)  # sqrt(3)*z/(2*sqrt(pi))\n\t        encodings = encodings.at[..., 3].set(-0.48860251190291987*x)  # -sqrt(3)*x/(2*sqrt(pi))\n\t        if self.L <= 2: return encodings;\n\t        encodings = encodings.at[..., 4].set(1.0925484305920792*xy)  # sqrt(15)*xy/(2*sqrt(pi))\n\t        encodings = encodings.at[..., 5].set(-1.0925484305920792*yz)  # -sqrt(15)*yz/(2*sqrt(pi))\n\t        encodings = encodings.at[..., 6].set(0.94617469575755997*z2 - 0.31539156525251999)  # sqrt(5)*(3*z2 - 1)/(4*sqrt(pi))\n\t        encodings = encodings.at[..., 7].set(-1.0925484305920792*xz)  # -sqrt(15)*xz/(2*sqrt(pi))\n\t        encodings = encodings.at[..., 8].set(0.54627421529603959*x2 - 0.54627421529603959*y2)  # sqrt(15)*(x2 - y2)/(4*sqrt(pi))\n", "        if self.L <= 3: return encodings\n\t        encodings = encodings.at[..., 9].set(0.59004358992664352*y*(-3.0*x2 + y2))  # sqrt(70)*y*(-3*x2 + y2)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 10].set(2.8906114426405538*xy*z)  # sqrt(105)*xy*z/(2*sqrt(pi))\n\t        encodings = encodings.at[..., 11].set(0.45704579946446572*y*(1.0 - 5.0*z2))  # sqrt(42)*y*(1 - 5*z2)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 12].set(0.3731763325901154*z*(5.0*z2 - 3.0))  # sqrt(7)*z*(5*z2 - 3)/(4*sqrt(pi))\n\t        encodings = encodings.at[..., 13].set(0.45704579946446572*x*(1.0 - 5.0*z2))  # sqrt(42)*x*(1 - 5*z2)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 14].set(1.4453057213202769*z*(x2 - y2))  # sqrt(105)*z*(x2 - y2)/(4*sqrt(pi))\n\t        encodings = encodings.at[..., 15].set(0.59004358992664352*x*(-x2 + 3.0*y2))  # sqrt(70)*x*(-x2 + 3*y2)/(8*sqrt(pi))\n\t        if self.L <= 4: return encodings\n\t        encodings = encodings.at[..., 16].set(2.5033429417967046*xy*(x2 - y2))  # 3*sqrt(35)*xy*(x2 - y2)/(4*sqrt(pi))\n", "        encodings = encodings.at[..., 17].set(1.7701307697799304*yz*(-3.0*x2 + y2))  # 3*sqrt(70)*yz*(-3*x2 + y2)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 18].set(0.94617469575756008*xy*(7.0*z2 - 1.0))  # 3*sqrt(5)*xy*(7*z2 - 1)/(4*sqrt(pi))\n\t        encodings = encodings.at[..., 19].set(0.66904654355728921*yz*(3.0 - 7.0*z2))  # 3*sqrt(10)*yz*(3 - 7*z2)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 20].set(-3.1735664074561294*z2 + 3.7024941420321507*z4 + 0.31735664074561293)  # 3*(-30*z2 + 35*z4 + 3)/(16*sqrt(pi))\n\t        encodings = encodings.at[..., 21].set(0.66904654355728921*xz*(3.0 - 7.0*z2))  # 3*sqrt(10)*xz*(3 - 7*z2)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 22].set(0.47308734787878004*(x2 - y2)*(7.0*z2 - 1.0))  # 3*sqrt(5)*(x2 - y2)*(7*z2 - 1)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 23].set(1.7701307697799304*xz*(-x2 + 3.0*y2))  # 3*sqrt(70)*xz*(-x2 + 3*y2)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 24].set(-3.7550144126950569*x2*y2 + 0.62583573544917614*x4 + 0.62583573544917614*y4)  # 3*sqrt(35)*(-6*x2*y2 + x4 + y4)/(16*sqrt(pi))\n\t        if self.L <= 5: return encodings\n\t        encodings = encodings.at[..., 25].set(0.65638205684017015*y*(10.0*x2*y2 - 5.0*x4 - y4))  # 3*sqrt(154)*y*(10*x2*y2 - 5*x4 - y4)/(32*sqrt(pi))\n", "        encodings = encodings.at[..., 26].set(8.3026492595241645*xy*z*(x2 - y2))  # 3*sqrt(385)*xy*z*(x2 - y2)/(4*sqrt(pi))\n\t        encodings = encodings.at[..., 27].set(-0.48923829943525038*y*(3.0*x2 - y2)*(9.0*z2 - 1.0))  # -sqrt(770)*y*(3*x2 - y2)*(9*z2 - 1)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 28].set(4.7935367849733241*xy*z*(3.0*z2 - 1.0))  # sqrt(1155)*xy*z*(3*z2 - 1)/(4*sqrt(pi))\n\t        encodings = encodings.at[..., 29].set(0.45294665119569694*y*(14.0*z2 - 21.0*z4 - 1.0))  # sqrt(165)*y*(14*z2 - 21*z4 - 1)/(16*sqrt(pi))\n\t        encodings = encodings.at[..., 30].set(0.1169503224534236*z*(-70.0*z2 + 63.0*z4 + 15.0))  # sqrt(11)*z*(-70*z2 + 63*z4 + 15)/(16*sqrt(pi))\n\t        encodings = encodings.at[..., 31].set(0.45294665119569694*x*(14.0*z2 - 21.0*z4 - 1.0))  # sqrt(165)*x*(14*z2 - 21*z4 - 1)/(16*sqrt(pi))\n\t        encodings = encodings.at[..., 32].set(2.3967683924866621*z*(x2 - y2)*(3.0*z2 - 1.0))  # sqrt(1155)*z*(x2 - y2)*(3*z2 - 1)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 33].set(-0.48923829943525038*x*(x2 - 3.0*y2)*(9.0*z2 - 1.0))  # -sqrt(770)*x*(x2 - 3*y2)*(9*z2 - 1)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 34].set(2.0756623148810411*z*(-6.0*x2*y2 + x4 + y4))  # 3*sqrt(385)*z*(-6*x2*y2 + x4 + y4)/(16*sqrt(pi))\n\t        encodings = encodings.at[..., 35].set(0.65638205684017015*x*(10.0*x2*y2 - x4 - 5.0*y4))  # 3*sqrt(154)*x*(10*x2*y2 - x4 - 5*y4)/(32*sqrt(pi))\n", "        if self.L <= 6: return encodings\n\t        encodings = encodings.at[..., 36].set(1.3663682103838286*xy*(-10.0*x2*y2 + 3.0*x4 + 3.0*y4))  # sqrt(6006)*xy*(-10*x2*y2 + 3*x4 + 3*y4)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 37].set(2.3666191622317521*yz*(10.0*x2*y2 - 5.0*x4 - y4))  # 3*sqrt(2002)*yz*(10*x2*y2 - 5*x4 - y4)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 38].set(2.0182596029148963*xy*(x2 - y2)*(11.0*z2 - 1.0))  # 3*sqrt(91)*xy*(x2 - y2)*(11*z2 - 1)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 39].set(-0.92120525951492349*yz*(3.0*x2 - y2)*(11.0*z2 - 3.0))  # -sqrt(2730)*yz*(3*x2 - y2)*(11*z2 - 3)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 40].set(0.92120525951492349*xy*(-18.0*z2 + 33.0*z4 + 1.0))  # sqrt(2730)*xy*(-18*z2 + 33*z4 + 1)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 41].set(0.58262136251873131*yz*(30.0*z2 - 33.0*z4 - 5.0))  # sqrt(273)*yz*(30*z2 - 33*z4 - 5)/(16*sqrt(pi))\n\t        encodings = encodings.at[..., 42].set(6.6747662381009842*z2 - 20.024298714302954*z4 + 14.684485723822165*z6 - 0.31784601133814211)  # sqrt(13)*(105*z2 - 315*z4 + 231*z6 - 5)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 43].set(0.58262136251873131*xz*(30.0*z2 - 33.0*z4 - 5.0))  # sqrt(273)*xz*(30*z2 - 33*z4 - 5)/(16*sqrt(pi))\n\t        encodings = encodings.at[..., 44].set(0.46060262975746175*(x2 - y2)*(11.0*z2*(3.0*z2 - 1.0) - 7.0*z2 + 1.0))  # sqrt(2730)*(x2 - y2)*(11*z2*(3*z2 - 1) - 7*z2 + 1)/(64*sqrt(pi))\n", "        encodings = encodings.at[..., 45].set(-0.92120525951492349*xz*(x2 - 3.0*y2)*(11.0*z2 - 3.0))  # -sqrt(2730)*xz*(x2 - 3*y2)*(11*z2 - 3)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 46].set(0.50456490072872406*(11.0*z2 - 1.0)*(-6.0*x2*y2 + x4 + y4))  # 3*sqrt(91)*(11*z2 - 1)*(-6*x2*y2 + x4 + y4)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 47].set(2.3666191622317521*xz*(10.0*x2*y2 - x4 - 5.0*y4))  # 3*sqrt(2002)*xz*(10*x2*y2 - x4 - 5*y4)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 48].set(10.247761577878714*x2*y4 - 10.247761577878714*x4*y2 + 0.6831841051919143*x6 - 0.6831841051919143*y6)  # sqrt(6006)*(15*x2*y4 - 15*x4*y2 + x6 - y6)/(64*sqrt(pi))\n\t        if self.L <= 7: return encodings\n\t        encodings = encodings.at[..., 49].set(0.70716273252459627*y*(-21.0*x2*y4 + 35.0*x4*y2 - 7.0*x6 + y6))  # 3*sqrt(715)*y*(-21*x2*y4 + 35*x4*y2 - 7*x6 + y6)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 50].set(5.2919213236038001*xy*z*(-10.0*x2*y2 + 3.0*x4 + 3.0*y4))  # 3*sqrt(10010)*xy*z*(-10*x2*y2 + 3*x4 + 3*y4)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 51].set(-0.51891557872026028*y*(13.0*z2 - 1.0)*(-10.0*x2*y2 + 5.0*x4 + y4))  # -3*sqrt(385)*y*(13*z2 - 1)*(-10*x2*y2 + 5*x4 + y4)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 52].set(4.1513246297620823*xy*z*(x2 - y2)*(13.0*z2 - 3.0))  # 3*sqrt(385)*xy*z*(x2 - y2)*(13*z2 - 3)/(8*sqrt(pi))\n\t        encodings = encodings.at[..., 53].set(-0.15645893386229404*y*(3.0*x2 - y2)*(13.0*z2*(11.0*z2 - 3.0) - 27.0*z2 + 3.0))  # -3*sqrt(35)*y*(3*x2 - y2)*(13*z2*(11*z2 - 3) - 27*z2 + 3)/(64*sqrt(pi))\n", "        encodings = encodings.at[..., 54].set(0.44253269244498261*xy*z*(-110.0*z2 + 143.0*z4 + 15.0))  # 3*sqrt(70)*xy*z*(-110*z2 + 143*z4 + 15)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 55].set(0.090331607582517306*y*(-135.0*z2 + 495.0*z4 - 429.0*z6 + 5.0))  # sqrt(105)*y*(-135*z2 + 495*z4 - 429*z6 + 5)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 56].set(0.068284276912004949*z*(315.0*z2 - 693.0*z4 + 429.0*z6 - 35.0))  # sqrt(15)*z*(315*z2 - 693*z4 + 429*z6 - 35)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 57].set(0.090331607582517306*x*(-135.0*z2 + 495.0*z4 - 429.0*z6 + 5.0))  # sqrt(105)*x*(-135*z2 + 495*z4 - 429*z6 + 5)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 58].set(0.07375544874083044*z*(x2 - y2)*(143.0*z2*(3.0*z2 - 1.0) - 187.0*z2 + 45.0))  # sqrt(70)*z*(x2 - y2)*(143*z2*(3*z2 - 1) - 187*z2 + 45)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 59].set(-0.15645893386229404*x*(x2 - 3.0*y2)*(13.0*z2*(11.0*z2 - 3.0) - 27.0*z2 + 3.0))  # -3*sqrt(35)*x*(x2 - 3*y2)*(13*z2*(11*z2 - 3) - 27*z2 + 3)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 60].set(1.0378311574405206*z*(13.0*z2 - 3.0)*(-6.0*x2*y2 + x4 + y4))  # 3*sqrt(385)*z*(13*z2 - 3)*(-6*x2*y2 + x4 + y4)/(32*sqrt(pi))\n\t        encodings = encodings.at[..., 61].set(-0.51891557872026028*x*(13.0*z2 - 1.0)*(-10.0*x2*y2 + x4 + 5.0*y4))  # -3*sqrt(385)*x*(13*z2 - 1)*(-10*x2*y2 + x4 + 5*y4)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 62].set(2.6459606618019*z*(15.0*x2*y4 - 15.0*x4*y2 + x6 - y6))  # 3*sqrt(10010)*z*(15*x2*y4 - 15*x4*y2 + x6 - y6)/(64*sqrt(pi))\n\t        encodings = encodings.at[..., 63].set(0.70716273252459627*x*(-35.0*x2*y4 + 21.0*x4*y2 - x6 + 7.0*y6))  # 3*sqrt(715)*x*(-35*x2*y4 + 21*x4*y2 - x6 + 7*y6)/(64*sqrt(pi))\n", "        if self.L <= 8: return encodings\n\t        raise NotImplementedError(\"Largest supported degree of spherical harmonics is 8, got {}\".format(self.L))\n\tdef bench_sh():\n\t    import time\n\t    L = 8\n\t    sh = SphericalHarmonicsEncoder(L=L)\n\t    shcuda = SphericalHarmonicsEncoderCuda(L=L)\n\t    @jax.jit\n\t    def shjax_jitted(x):\n\t        return sh(x)\n", "    @jit_jaxfn_with(static_argnames=[\"L\"])\n\t    # def shcuda_jitted(x):\n\t    #     return shcuda(x)\n\t    def shcuda_jitted(x, L):\n\t        return shjax.spherical_harmonics_encoding(x, L)\n\t    d = jnp.asarray([[.1, .5, -.7]])\n\t    d /= jnp.linalg.norm(d, axis=-1, keepdims=True) + 1e-15\n\t    result = sh(d)\n\t    result_cuda = shcuda(d)\n\t    print(abs(result - result_cuda).sum())\n", "    K = jran.PRNGKey(0xdeadbeef)\n\t    for i in range(100):\n\t        K, key = jran.split(K, 2)\n\t        n = 800*800\n\t        d = jran.normal(key, (n, 3))\n\t        d /= jnp.linalg.norm(d) + 1e-15\n\t        print(\"{:03d}-th check ({} coordinates, degree={}): \".format(i+1, n, L), end=\"\")\n\t        stime = time.time()\n\t        print(\"|jax...\", end=\"\")\n\t        result = shjax_jitted(d).block_until_ready()\n", "        etime = time.time()\n\t        durms = 1000 * (etime - stime)\n\t        print(\"{:.2f}ms|\".format(durms), end=\"\")\n\t        stime = time.time()\n\t        print(\"|cuda...\", end=\"\")\n\t        result_cuda = shcuda_jitted(d, L).block_until_ready()\n\t        etime = time.time()\n\t        durms = 1000 * (etime - stime)\n\t        print(\"{:.2f}ms|\".format(durms), end=\"\")\n\t        diff = abs(result - result_cuda).sum()\n", "        print(\"diff(total)={:.3e}|diff(mean)={:.3e}\".format(diff, diff/n))\n\tdef bench_hg():\n\t    import time\n\t    dim=3\n\t    L=16\n\t    F=2\n\t    T=2**19\n\t    N_min=16\n\t    per_level_scale=2.\n\t    N_max=int(N_min * per_level_scale**(L - 1))\n", "    hg = HashGridEncoder(\n\t        dim=dim,\n\t        L=L,\n\t        T=T,\n\t        F=F,\n\t        N_min=N_min,\n\t        N_max=N_max,\n\t    )\n\t    K = jran.PRNGKey(0xabcdef)\n\t    variables = hg.init(K, jnp.zeros([5, 3]))\n", "    (params_array,) = variables[\"params\"][\"latent codes stored on grid vertices\"],\n\t    print(params_array.shape)\n\t    @jax.jit\n\t    def hgjax_jitted(d):\n\t        return hg.apply(variables, d)\n\t    hgmeta = HashGridMetadata(\n\t        L=L,\n\t        F=F,\n\t        N_min=N_min,\n\t        per_level_scale=per_level_scale,\n", "    )\n\t    resolutions, offsets = [], [0]\n\t    for i in range(L):\n\t        res = int(N_min * (per_level_scale**i))\n\t        resolutions.append(res)\n\t        n_entries = (res + 1) ** dim\n\t        if n_entries <= T:\n\t            pass\n\t        else:\n\t            n_entries = T\n", "        offsets.append(offsets[-1] + n_entries)\n\t    assert offsets[-1] == params_array.shape[0]\n\t    @jax.jit\n\t    def hgtcnn_jitted(d_row_major):  # expects input coordinates to have shape (dim, n_coords)\n\t        return hashgrid_encode(\n\t            desc=hgmeta,\n\t            offset_table_data=jnp.asarray(offsets, dtype=jnp.uint32),\n\t            coords_rm=d_row_major,\n\t            params=params_array,\n\t        )\n", "    print(\"starting!\")\n\t    for i in range(100):\n\t        K, key = jran.split(K, 2)\n\t        n = 256_000\n\t        d = jran.uniform(key, (n, 3), minval=0, maxval=1.)\n\t        d_T = jran.uniform(key, (3, n), minval=0, maxval=1.)\n\t        print(\"{:03d}-th check ({} coordinates, degree={}): \".format(i+1, n, L), end=\"\")\n\t        stime = time.time()\n\t        print(\"|jax...\", end=\"\")\n\t        result = hgjax_jitted(d).block_until_ready()\n", "        etime = time.time()\n\t        durms = 1000 * (etime - stime)\n\t        print(\"{:.2f}ms\".format(durms), end=\"\")\n\t        stime = time.time()\n\t        print(\"|tcnn...\", end=\"\")\n\t        result = hgtcnn_jitted(d_T).block_until_ready()\n\t        etime = time.time()\n\t        durms = 1000 * (etime - stime)\n\t        print(\"{:.2f}ms|\".format(durms), end=\"\")\n\t        print()\n", "if __name__ == \"__main__\":\n\t    print(\"bench_hg\")\n\t    bench_hg()\n\t    # print()\n\t    # print(\"bench_sh:\")\n\t    # bench_sh()\n"]}
{"filename": "models/renderers/__init__.py", "chunked_list": ["from .cuda import render_image_inference, render_rays_train\n\t__all__ = [\n\t    \"render_rays_train\",\n\t    \"render_image_inference\",\n\t]\n"]}
{"filename": "models/renderers/cuda.py", "chunked_list": ["from dataclasses import dataclass\n\timport math\n\tfrom typing import Callable\n\tfrom flax.core.scope import FrozenVariableDict\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\tfrom volrendjax import (\n\t    integrate_rays,\n\t    integrate_rays_inference,\n", "    march_rays,\n\t    march_rays_inference,\n\t)\n\tfrom utils.common import jit_jaxfn_with\n\tfrom utils.data import f32_to_u8\n\tfrom utils.types import Camera, CameraOverrideOptions, NeRFState, RigidTransformation\n\t@jax.jit\n\tdef make_rays_worldspace(\n\t    camera: Camera,\n\t    transform_cw: RigidTransformation,\n", "):\n\t    \"\"\"\n\t    Generate world-space rays for each pixel in the given camera's projection plane.\n\t    Inputs:\n\t        camera: the camera model in use\n\t        transform_cw[rotation, translation]: camera to world transformation\n\t            rotation [3, 3]: rotation matrix\n\t            translation [3]: translation vector\n\t    Returns:\n\t        o_world [H*W, 3]: ray origins, in world-space\n", "        d_world [H*W, 3]: ray directions, in world-space\n\t    \"\"\"\n\t    # [H*W, 1]\n\t    d_cam_idcs = jnp.arange(camera.n_pixels)\n\t    x, y = (\n\t        jnp.mod(d_cam_idcs, camera.width),\n\t        jnp.floor_divide(d_cam_idcs, camera.width),\n\t    )\n\t    # [H*W, 3]\n\t    d_cam = camera.make_ray_directions_from_pixel_coordinates(x, y, use_pixel_center=True)\n", "    # [H*W, 3]\n\t    o_world = jnp.broadcast_to(transform_cw.translation, d_cam.shape)\n\t    # [H*W, 3]\n\t    d_world = d_cam @ transform_cw.rotation.T\n\t    return o_world, d_world\n\t@jax.jit\n\tdef make_near_far_from_bound(\n\t    bound: float,\n\t    o: jax.Array,  # [n_rays, 3]\n\t    d: jax.Array,  # [n_rays, 3]\n", "):\n\t    \"Calculates near and far intersections with the bounding box [-bound, bound]^3 for each ray.\"\n\t    # avoid d[j] being zero\n\t    eps = 1e-15\n\t    d = jnp.where(\n\t        jnp.signbit(d),  # True for negatives, False for non-negatives\n\t        jnp.clip(d, None, -eps * jnp.ones_like(d)),  # if negative, upper-bound is -eps\n\t        jnp.clip(d, eps * jnp.ones_like(d)),  # if non-negative, lower-bound is eps\n\t    )\n\t    # [n_rays]\n", "    tx0, tx1 = (\n\t        (-bound - o[:, 0]) / d[:, 0],\n\t        (bound - o[:, 0]) / d[:, 0],\n\t    )\n\t    ty0, ty1 = (\n\t        (-bound - o[:, 1]) / d[:, 1],\n\t        (bound - o[:, 1]) / d[:, 1],\n\t    )\n\t    tz0, tz1 = (\n\t        (-bound - o[:, 2]) / d[:, 2],\n", "        (bound - o[:, 2]) / d[:, 2],\n\t    )\n\t    tx_start, tx_end = jnp.minimum(tx0, tx1), jnp.maximum(tx0, tx1)\n\t    ty_start, ty_end = jnp.minimum(ty0, ty1), jnp.maximum(ty0, ty1)\n\t    tz_start, tz_end = jnp.minimum(tz0, tz1), jnp.maximum(tz0, tz1)\n\t    # when t_start<0, or t_start>t_end, ray does not intersect with aabb, these cases are handled in\n\t    # the `march_rays` implementation\n\t    t_start = jnp.maximum(jnp.maximum(tx_start, ty_start), tz_start)  # last axis that gose inside the bbox\n\t    t_end = jnp.minimum(jnp.minimum(tx_end, ty_end), tz_end)  # first axis that goes out of the bbox\n\t    t_start = jnp.maximum(0., t_start)\n", "    # [n_rays], [n_rays]\n\t    return t_start, t_end\n\t@jit_jaxfn_with(static_argnames=[\"total_samples\"])\n\tdef render_rays_train(\n\t    KEY: jran.KeyArray,\n\t    o_world: jax.Array,\n\t    d_world: jax.Array,\n\t    appearance_embeddings: jax.Array,\n\t    bg: jax.Array,\n\t    total_samples: int,\n", "    state: NeRFState,\n\t):\n\t    # skip the empty space between camera and scene bbox\n\t    # [n_rays], [n_rays]\n\t    t_starts, t_ends = make_near_far_from_bound(\n\t        bound=state.scene_meta.bound,\n\t        o=o_world,\n\t        d=d_world,\n\t    )\n\t    if state.raymarch.perturb:\n", "        KEY, key = jran.split(KEY, 2)\n\t        noises = jran.uniform(key, shape=t_starts.shape, dtype=t_starts.dtype, minval=0., maxval=1.)\n\t    else:\n\t        noises = 0.\n\t    measured_batch_size_before_compaction, ray_is_valid, rays_n_samples, rays_sample_startidx, ray_idcs, xyzs, dirs, dss, z_vals = march_rays(\n\t        total_samples=total_samples,\n\t        diagonal_n_steps=state.raymarch.diagonal_n_steps,\n\t        K=state.scene_meta.cascades,\n\t        G=state.raymarch.density_grid_res,\n\t        bound=state.scene_meta.bound,\n", "        stepsize_portion=state.scene_meta.stepsize_portion,\n\t        rays_o=o_world,\n\t        rays_d=d_world,\n\t        t_starts=t_starts.ravel(),\n\t        t_ends=t_ends.ravel(),\n\t        noises=noises,\n\t        occupancy_bitfield=state.ogrid.occupancy,\n\t    )\n\t    drgbs, tv = state.nerf_fn(\n\t        {\"params\": state.params[\"nerf\"]},\n", "        xyzs,\n\t        dirs,\n\t        appearance_embeddings[ray_idcs],\n\t    )\n\t    effective_samples, final_rgbds, final_opacities = integrate_rays(\n\t        near_distance=state.scene_meta.camera.near,\n\t        rays_sample_startidx=rays_sample_startidx,\n\t        rays_n_samples=rays_n_samples,\n\t        bgs=bg,\n\t        dss=dss,\n", "        z_vals=z_vals,\n\t        drgbs=drgbs,\n\t    )\n\t    batch_metrics = {\n\t        \"n_valid_rays\": ray_is_valid.sum(),\n\t        \"ray_is_valid\": ray_is_valid,\n\t        \"measured_batch_size_before_compaction\": measured_batch_size_before_compaction,\n\t        \"measured_batch_size\": jnp.where(effective_samples > 0, effective_samples, 0).sum(),\n\t    }\n\t    return batch_metrics, final_rgbds, tv\n", "@dataclass(frozen=True, kw_only=True)\n\tclass MarchAndIntegrateInferencePayload:\n\t    march_steps_cap: int\n\t    diagonal_n_steps: int\n\t    cascades: int\n\t    density_grid_res: int\n\t    bound: float\n\t    stepsize_portion: float\n\t    nerf_fn: Callable\n\t@jit_jaxfn_with(\n", "    static_argnames=[\"payload\"],\n\t    donate_argnums=tuple(range(7)),  # NOTE: this only works for positional arguments, see <https://jax.readthedocs.io/en/latest/faq.html#buffer-donation>\n\t)\n\tdef march_and_integrate_inference(\n\t    # rw (donated) fields (7)\n\t    next_ray_index: jax.Array,\n\t    terminated: jax.Array,\n\t    indices: jax.Array,\n\t    rays_rgbd: jax.Array,\n\t    rays_T: jax.Array,\n", "    rays_cost: jax.Array | None,\n\t    t_starts: jax.Array,\n\t    # static\n\t    payload: MarchAndIntegrateInferencePayload,\n\t    # ro fields\n\t    locked_nerf_params: FrozenVariableDict,\n\t    appearance_embedding: jax.Array,\n\t    rays_o: jax.Array,\n\t    rays_d: jax.Array,\n\t    t_ends: jax.Array,\n", "    rays_bg: jax.Array,\n\t    occupancy_bitfield: jax.Array,\n\t):\n\t    next_ray_index, indices, n_samples, t_starts, xyzs, dss, z_vals = march_rays_inference(\n\t        diagonal_n_steps=payload.diagonal_n_steps,\n\t        K=payload.cascades,\n\t        G=payload.density_grid_res,\n\t        march_steps_cap=payload.march_steps_cap,\n\t        bound=payload.bound,\n\t        stepsize_portion=payload.stepsize_portion,\n", "        rays_o=rays_o,\n\t        rays_d=rays_d,\n\t        t_starts=t_starts,\n\t        t_ends=t_ends,\n\t        occupancy_bitfield=occupancy_bitfield,\n\t        next_ray_index_in=next_ray_index,\n\t        terminated=terminated,\n\t        indices=indices,\n\t    )\n\t    if rays_cost is not None:\n", "        rays_cost = rays_cost.at[indices].set(rays_cost[indices] + n_samples)\n\t    xyzs = jax.lax.stop_gradient(xyzs)\n\t    drgbs, _ = payload.nerf_fn(\n\t        {\"params\": locked_nerf_params},\n\t        xyzs,\n\t        jnp.broadcast_to(rays_d[indices, None, :], xyzs.shape),\n\t        jax.lax.stop_gradient(appearance_embedding),\n\t    )\n\t    terminate_cnt, terminated, rays_rgbd, rays_T = integrate_rays_inference(\n\t        rays_bg=rays_bg,\n", "        rays_rgbd=rays_rgbd,\n\t        rays_T=rays_T,\n\t        n_samples=n_samples,\n\t        indices=indices,\n\t        dss=dss,\n\t        z_vals=z_vals,\n\t        drgbs=drgbs,\n\t    )\n\t    return terminate_cnt, next_ray_index, terminated, indices, rays_rgbd, rays_T, rays_cost, t_starts\n\tdef render_image_inference(\n", "    KEY: jran.KeyArray,\n\t    transform_cw: RigidTransformation,\n\t    state: NeRFState,\n\t    camera_override: None | CameraOverrideOptions=None,\n\t    render_cost: bool=False,\n\t    appearance_embedding_index: int=0,\n\t):\n\t    if isinstance(camera_override, Camera):\n\t        state = state.replace(scene_meta=state.scene_meta.replace(camera=camera_override))\n\t    elif isinstance(camera_override, CameraOverrideOptions):\n", "        state = state.replace(\n\t            scene_meta=state.scene_meta.replace(\n\t                camera=camera_override.update_camera(state.scene_meta.camera),\n\t            ),\n\t        )\n\t    elif camera_override is None:\n\t        pass\n\t    else:\n\t        raise RuntimeError(\n\t            \"expected `camera_override` to be of type `Camera` or `CameraOverrideOptions`, got {}\".format(\n", "                type(camera_override)\n\t            )\n\t        )\n\t    o_world, d_world = make_rays_worldspace(camera=state.scene_meta.camera, transform_cw=transform_cw)\n\t    appearance_embedding = (\n\t        state.locked_params[\"appearance_embeddings\"][appearance_embedding_index]\n\t            if \"appearance_embeddings\" in state.locked_params\n\t            else jnp.empty(0)\n\t    )\n\t    t_starts, t_ends = make_near_far_from_bound(state.scene_meta.bound, o_world, d_world)\n", "    rays_rgbd = jnp.zeros((state.scene_meta.camera.n_pixels, 4), dtype=jnp.float32)\n\t    rays_T = jnp.ones(state.scene_meta.camera.n_pixels, dtype=jnp.float32)\n\t    if render_cost:\n\t        rays_cost = jnp.zeros(state.scene_meta.camera.n_pixels, dtype=jnp.uint32)\n\t    else:\n\t        rays_cost = None\n\t    if state.use_background_model:\n\t        bg = state.bg_fn(\n\t            {\"params\": state.locked_params[\"bg\"]},\n\t            o_world,\n", "            d_world,\n\t            appearance_embedding,\n\t        )\n\t    elif state.render.random_bg:\n\t        KEY, key = jran.split(KEY, 2)\n\t        bg = jran.uniform(key, (3,), dtype=jnp.float32, minval=0, maxval=1)\n\t    else:\n\t        bg = state.render.bg\n\t    rays_bg = jnp.broadcast_to(jnp.asarray(bg), (state.scene_meta.camera.n_pixels, 3))\n\t    (\n", "        o_world,\n\t        d_world,\n\t        appearance_embedding,\n\t        t_starts,\n\t        t_ends,\n\t        rays_bg,\n\t        rays_rgbd,\n\t        rays_cost,\n\t        rays_T,\n\t    ) = jax.lax.stop_gradient((\n", "        o_world,\n\t        d_world,\n\t        appearance_embedding,\n\t        t_starts,\n\t        t_ends,\n\t        rays_bg,\n\t        rays_rgbd,\n\t        rays_cost,\n\t        rays_T,\n\t    ))\n", "    march_steps_cap = 8\n\t    n_rays = min(8192, state.scene_meta.camera.n_pixels)\n\t    next_ray_index = jnp.zeros(1, dtype=jnp.uint32)\n\t    terminated = jnp.ones(n_rays, dtype=jnp.bool_)  # all rays are terminated at the beginning\n\t    indices = jnp.zeros(n_rays, dtype=jnp.uint32)\n\t    n_rendered_rays = 0\n\t    while n_rendered_rays < state.scene_meta.camera.n_pixels:\n\t        iters = max(1, (state.scene_meta.camera.n_pixels - n_rendered_rays) // n_rays)\n\t        iters = 2 ** int(math.log2(iters) + 1)\n\t        for _ in range(iters):\n", "            terminate_cnt, next_ray_index, terminated, indices, rays_rgbd, rays_T, rays_cost, t_starts = march_and_integrate_inference(\n\t                # rw (donated) fields\n\t                next_ray_index,\n\t                terminated,\n\t                indices,\n\t                rays_rgbd,\n\t                rays_T,\n\t                rays_cost,\n\t                t_starts,\n\t                # static\n", "                payload=MarchAndIntegrateInferencePayload(\n\t                    march_steps_cap=march_steps_cap,\n\t                    diagonal_n_steps=state.raymarch.diagonal_n_steps,\n\t                    cascades=state.scene_meta.cascades,\n\t                    density_grid_res=state.raymarch.density_grid_res,\n\t                    bound=state.scene_meta.bound,\n\t                    stepsize_portion=state.scene_meta.stepsize_portion,\n\t                    nerf_fn=state.nerf_fn,\n\t                ),\n\t                # ro fields\n", "                locked_nerf_params=state.locked_params[\"nerf\"],\n\t                appearance_embedding=appearance_embedding,\n\t                rays_o=o_world,\n\t                rays_d=d_world,\n\t                t_ends=t_ends,\n\t                rays_bg=rays_bg,\n\t                occupancy_bitfield=state.ogrid.occupancy,\n\t            )\n\t            n_rendered_rays += terminate_cnt\n\t    bg_array_f32 = rays_bg.reshape((state.scene_meta.camera.height, state.scene_meta.camera.width, 3))\n", "    rays_rgb, rays_depth = jnp.array_split(rays_rgbd, [3], axis=-1)\n\t    image_array_u8 = f32_to_u8(rays_rgb).reshape((state.scene_meta.camera.height, state.scene_meta.camera.width, 3))\n\t    distance_array_u8 = f32_to_u8((rays_depth - rays_depth.min()) / (rays_depth.max() - rays_depth.min() + 1e-15))\n\t    distance_array_u8 = distance_array_u8.reshape((state.scene_meta.camera.height, state.scene_meta.camera.width))\n\t    if render_cost:\n\t        cost_array_u8 = f32_to_u8(rays_cost.astype(jnp.float32) / (rays_cost.astype(jnp.float32).max() + 1.)).reshape((state.scene_meta.camera.height, state.scene_meta.camera.width))\n\t    else:\n\t        cost_array_u8 = None\n\t    return bg_array_f32, image_array_u8, distance_array_u8, cost_array_u8\n"]}
{"filename": "app/imagefit.py", "chunked_list": ["#!/usr/bin/env python3\n\tfrom pathlib import Path\n\tfrom typing import Literal\n\tfrom PIL import Image\n\tfrom flax.training.train_state import TrainState\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\timport numpy as np\n\timport optax\n", "from tqdm import tqdm\n\timport tyro\n\tfrom models.imagefit import ImageFitter\n\tfrom utils import common, data\n\tfrom utils.args import ImageFitArgs\n\tlogger = common.setup_logging(\"imagefit\")\n\t@jax.jit\n\tdef train_step(state: TrainState, uvs, rgbs, perm):\n\t    def loss(params, x, y):\n\t        preds = state.apply_fn({\"params\": params}, x)\n", "        loss = jnp.square(preds - y).mean()\n\t        return loss\n\t    loss_grad_fn = jax.value_and_grad(loss)\n\t    loss, grads = loss_grad_fn(state.params, uvs[perm], rgbs[perm])\n\t    state = state.apply_gradients(grads=grads)\n\t    metrics = {\n\t        \"loss\": loss * perm.shape[0],\n\t    }\n\t    return state, metrics\n\tdef train_epoch(\n", "        image_metadata: data.ImageMetadata,\n\t        permutation: data.Dataset,\n\t        total_batches: int,\n\t        state: TrainState,\n\t        ep_log: int,\n\t    ):\n\t    loss = 0\n\t    for perm in tqdm(permutation, total=total_batches, desc=\"ep#{:03d}\".format(ep_log), bar_format=common.tqdm_format):\n\t        state, metrics = train_step(state, image_metadata.uvs, image_metadata.rgbs, perm)\n\t        loss += metrics[\"loss\"]\n", "    return loss, state\n\t@jax.jit\n\tdef eval_step(state, uvs, perm):\n\t    preds = state.apply_fn({\"params\": state.params}, uvs[perm])\n\t    return preds\n\tdef eval(\n\t        image_array,\n\t        image_metadata: data.ImageMetadata,\n\t        state: TrainState,\n\t    ):\n", "    H, W = image_array.shape[:2]\n\t    @common.jit_jaxfn_with(static_argnames=[\"chunk_size\"])\n\t    def get_perms(chunk_size: int) -> list[jax.Array]:\n\t        all_perms = jnp.arange(H*W)\n\t        if chunk_size >= H*W:\n\t            n_chunks = 1\n\t        else:\n\t            n_chunks = H*W // chunk_size\n\t        perms = jnp.array_split(all_perms, n_chunks)\n\t        return perms\n", "    for perm in tqdm(get_perms(chunk_size=2**15), desc=\"evaluating\", bar_format=common.tqdm_format):\n\t        # preds = state.apply_fn({\"params\": state.params}, uv)\n\t        preds = eval_step(state, image_metadata.uvs, perm)\n\t        image_array = data.set_pixels(image_array, image_metadata.xys, perm, preds)\n\t    return image_array\n\tdef main(\n\t        args: ImageFitArgs,\n\t        in_image: Path,\n\t        out_path: Path,\n\t        encoding: Literal[\"hashgrid\", \"frequency\"],\n", "        # Enable this to suppress prompt if out_path exists and directly overwrite the file.\n\t        overwrite: bool = False,\n\t        encoding_prec: int = 32,\n\t        model_summary: bool = False,\n\t    ):\n\t    logger.setLevel(args.common.logging.upper())\n\t    if not out_path.parent.is_dir():\n\t        logger.err(\"Output path's parent '{}' does not exist or is not a directory!\".format(out_path.parent))\n\t        exit(1)\n\t    if out_path.exists() and not overwrite:\n", "        logger.warn(\"Output path '{}' exists and will be overwritten!\".format(out_path))\n\t        try:\n\t            r = input(\"Continue? [y/N] \")\n\t            if (r.strip() + \"n\").lower()[0] != \"y\":\n\t                exit(0)\n\t        except EOFError:\n\t            print()\n\t            exit(0)\n\t        except KeyboardInterrupt:\n\t            print()\n", "            exit(0)\n\t    encoding_dtype = getattr(jnp, \"float{}\".format(encoding_prec))\n\t    dtype = getattr(jnp, \"float{}\".format(args.common.prec))\n\t    # deterministic\n\t    K = common.set_deterministic(args.common.seed)\n\t    # model parameters\n\t    K, key = jran.split(K, 2)\n\t    model, init_input = (\n\t        ImageFitter(encoding=encoding, encoding_dtype=encoding_dtype),\n\t        jnp.zeros((1, 2), dtype=dtype),\n", "    )\n\t    variables = model.init(key, init_input)\n\t    if model_summary:\n\t        print(model.tabulate(key, init_input))\n\t    # training state\n\t    state = TrainState.create(\n\t        apply_fn=model.apply,\n\t        params=variables[\"params\"],\n\t        tx=optax.adam(\n\t            learning_rate=args.train.lr,\n", "            b1=0.9,\n\t            b2=0.99,\n\t            # paper:\n\t            #   the small value of ùúñ = 10^{‚àí15} can significantly accelerate the convergence of the\n\t            #   hash table entries when their gradients are sparse and weak.\n\t            eps=1e-15,\n\t        ),\n\t    )\n\t    # data\n\t    in_image = np.asarray(Image.open(in_image))\n", "    image_metadata = data.make_image_metadata(\n\t        image=in_image,\n\t        bg=(1.0, 1.0, 1.0),\n\t    )\n\t    for ep in range(args.train.n_epochs):\n\t        ep_log = ep + 1\n\t        K, key = jran.split(K, 2)\n\t        permutation = data.make_permutation_dataset(\n\t            key,\n\t            size=image_metadata.W * image_metadata.H,\n", "            shuffle=True\n\t        )\\\n\t            .batch(args.train.bs, drop_remainder=True)\\\n\t            .repeat(args.data.loop)\n\t        loss, state = train_epoch(\n\t            image_metadata=image_metadata,\n\t            permutation=permutation.take(args.train.n_batches).as_numpy_iterator(),\n\t            total_batches=args.train.n_batches,\n\t            state=state,\n\t            ep_log=ep_log,\n", "        )\n\t        image = np.asarray(Image.new(\"RGB\", in_image.shape[:2][::-1]))\n\t        image = eval(image, image_metadata, state)\n\t        logger.debug(\"saving image of shape {} to {}\".format(image.shape, out_path))\n\t        Image.fromarray(np.asarray(image)).save(out_path)\n\t        logger.info(\n\t            \"epoch#{:03d}: per-pixel loss={:.2e}, psnr={}\".format(\n\t                ep_log,\n\t                loss / (image_metadata.H * image_metadata.W),\n\t                data.psnr(in_image, image),\n", "            )\n\t        )\n\tif __name__ == \"__main__\":\n\t    tyro.cli(main)\n"]}
{"filename": "app/nerf/__main__.py", "chunked_list": ["#!/usr/bin/env python3\n\tfrom typing import Annotated\n\tfrom typing_extensions import assert_never\n\timport tyro\n\tfrom utils.args import NeRFTrainingArgs,NeRFTestingArgs,NeRFGUIArgs\n\tfrom utils import common\n\tCmdTrain = Annotated[\n\t    NeRFTrainingArgs,\n\t    tyro.conf.subcommand(\n\t        name=\"train\",\n", "        prefix_name=False,\n\t    ),\n\t]\n\tCmdTest = Annotated[\n\t    NeRFTestingArgs,\n\t    tyro.conf.subcommand(\n\t        name=\"test\",\n\t        prefix_name=False,\n\t    ),\n\t]\n", "CmdGui = Annotated[\n\t    NeRFGUIArgs,\n\t    tyro.conf.subcommand(\n\t        name=\"gui\",\n\t        prefix_name=False,\n\t    ),\n\t]\n\tMainArgsType = CmdTrain | CmdTest | CmdGui\n\tdef main(args: MainArgsType):\n\t    logger = common.setup_logging(\"nerf\")\n", "    KEY = common.set_deterministic(args.common.seed)\n\t    if isinstance(args, NeRFTrainingArgs):\n\t        from app.nerf.train import train\n\t        return train(KEY, args, logger)\n\t    elif isinstance(args, NeRFTestingArgs):\n\t        from app.nerf.test import test\n\t        return test(KEY, args, logger)\n\t    elif isinstance(args, NeRFGUIArgs):\n\t        from app.nerf.gui import GuiWindow\n\t        return GuiWindow(KEY, args, logger)\n", "    else:\n\t        assert_never(args)\n\tif __name__ == \"__main__\":\n\t    args = tyro.cli(MainArgsType)\n\t    exit(main(args))\n"]}
{"filename": "app/nerf/train.py", "chunked_list": ["import dataclasses\n\timport functools\n\timport gc\n\timport time\n\tfrom typing import Any, Dict, List, Tuple\n\tfrom flax.training import checkpoints\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\timport tyro\n", "from models.nerfs import make_nerf_ngp, make_skysphere_background_model_ngp\n\tfrom models.renderers import render_image_inference\n\tfrom utils import common, data\n\tfrom utils.args import NeRFTrainingArgs\n\tfrom utils.types import (\n\t    NeRFState,\n\t    OccupancyDensityGrid,\n\t    RenderedImage,\n\t    RigidTransformation,\n\t    SceneData,\n", ")\n\tfrom ._utils import make_optimizer, train_step, format_metrics\n\tdef train_epoch(\n\t    KEY: jran.KeyArray,\n\t    state: NeRFState,\n\t    scene: SceneData,\n\t    iters: int,\n\t    total_samples: int,\n\t    ep_log: int,\n\t    total_epochs: int,\n", "    logger: common.Logger,\n\t) -> Tuple[NeRFState, Dict[str, Any]]:\n\t    n_processed_rays = 0\n\t    total_loss = None\n\t    interrupted = False\n\t    try:\n\t        with common.tqdm(range(iters), desc=\"Training epoch#{:03d}/{:d}\".format(ep_log, total_epochs)) as pbar:\n\t            start = int(state.step) % iters\n\t            pbar.update(start)\n\t            for _ in range(start, iters):\n", "                KEY, key_perm, key_train_step = jran.split(KEY, 3)\n\t                perm = jran.choice(key_perm, scene.n_pixels, shape=(total_samples,), replace=True)\n\t                state, metrics = train_step(\n\t                    state,\n\t                    KEY=key_train_step,\n\t                    total_samples=total_samples,\n\t                    scene=scene,\n\t                    perm=perm,\n\t                )\n\t                n_processed_rays += metrics[\"n_valid_rays\"]\n", "                loss = metrics[\"loss\"]\n\t                if total_loss is None:\n\t                    total_loss = loss\n\t                else:\n\t                    total_loss = jax.tree_util.tree_map(\n\t                        lambda total, new: total + new * metrics[\"n_valid_rays\"],\n\t                        total_loss,\n\t                        loss,\n\t                    )\n\t                pbar.set_description_str(\n", "                    desc=\"Training epoch#{:03d}/{:d} \".format(\n\t                        ep_log,\n\t                        total_epochs,\n\t                    ) + format_metrics(metrics),\n\t                )\n\t                pbar.update(1)\n\t                if state.should_call_update_ogrid:\n\t                    # update occupancy grid\n\t                    for cas in range(state.scene_meta.cascades):\n\t                        KEY, key = jran.split(KEY, 2)\n", "                        state = state.update_ogrid_density(\n\t                            KEY=key,\n\t                            cas=cas,\n\t                            update_all=bool(state.should_update_all_ogrid_cells),\n\t                            max_inference=total_samples,\n\t                        )\n\t                    state = state.threshold_ogrid()\n\t                if state.should_write_batch_metrics:\n\t                    logger.write_metrics_to_tensorboard(metrics, state.step)\n\t    except (InterruptedError, KeyboardInterrupt):\n", "        interrupted = True\n\t    return state, {\n\t        \"total_loss\": total_loss,\n\t        \"n_processed_rays\": n_processed_rays,\n\t        \"interrupted\": interrupted,\n\t    }\n\tdef train(KEY: jran.KeyArray, args: NeRFTrainingArgs, logger: common.Logger) -> int:\n\t    if args.ckpt is not None and not args.ckpt.exists():\n\t        logger.error(\"specified checkpoint '{}' does not exist\".format(args.ckpt))\n\t        return 2\n", "    args.exp_dir.mkdir(parents=True, exist_ok=True)\n\t    save_dir = common.backup_current_codebase(args.exp_dir, name_prefix=\"train-\", note=args.note)\n\t    config_save_path = save_dir.joinpath(\"config.yaml\")\n\t    config_save_path.write_text(tyro.to_yaml(args))\n\t    logs_dir = save_dir.joinpath(\"logs\")\n\t    logs_dir.mkdir(parents=True, exist_ok=True)\n\t    logger = common.setup_logging(\n\t        \"nerf.train\",\n\t        file=logs_dir.joinpath(\"train.log\"),\n\t        with_tensorboard=True,\n", "        level=args.common.logging.upper(),\n\t        file_level=\"DEBUG\",\n\t    )\n\t    logger.write_hparams(dataclasses.asdict(args))\n\t    logger.info(\"code saved to '{}', configurations saved at '{}'\".format(\n\t        save_dir,\n\t        config_save_path,\n\t    ))\n\t    # data\n\t    logger.info(\"loading training frames\")\n", "    scene_train = data.load_scene(\n\t        srcs=args.frames_train,\n\t        scene_options=args.scene,\n\t    )\n\t    logger.debug(\"sharpness_min={:.3f}, sharpness_max={:.3f}\".format(*scene_train.meta.sharpness_range))\n\t    if len(args.frames_val) > 0:\n\t        logger.info(\"loading validation frames\")\n\t        scene_val = data.load_scene(\n\t            srcs=args.frames_val,\n\t            scene_options=args.scene,\n", "        )\n\t        assert scene_train.meta.replace(frames=None) == scene_val.meta.replace(frames=None)\n\t    else:\n\t        logger.warn(\"got empty validation set, this run will not do validation\")\n\t    scene_meta = scene_train.meta\n\t    # model parameters\n\t    nerf_model, init_input = (\n\t        make_nerf_ngp(bound=scene_meta.bound, inference=False, tv_scale=args.train.tv_scale),\n\t        (\n\t            jnp.zeros((1, 3), dtype=jnp.float32),\n", "            jnp.zeros((1, 3), dtype=jnp.float32),\n\t            jnp.zeros((1, scene_meta.n_extra_learnable_dims), dtype=jnp.float32),\n\t        )\n\t    )\n\t    KEY, key = jran.split(KEY, 2)\n\t    nerf_variables = nerf_model.init(key, *init_input)\n\t    if args.common.summary:\n\t        print(nerf_model.tabulate(key, *init_input))\n\t    if scene_meta.bg:\n\t        bg_model, init_input = (\n", "            make_skysphere_background_model_ngp(bound=scene_meta.bound),\n\t            (\n\t                jnp.zeros((1, 3), dtype=jnp.float32),\n\t                jnp.zeros((1, 3), dtype=jnp.float32),\n\t                jnp.zeros((1, scene_meta.n_extra_learnable_dims), dtype=jnp.float32),\n\t            ),\n\t        )\n\t        KEY, key = jran.split(KEY, 2)\n\t        bg_variables = bg_model.init(key, *init_input)\n\t    KEY, key = jran.split(KEY, 2)\n", "    # training state\n\t    state = NeRFState.create(\n\t        ogrid=OccupancyDensityGrid.create(\n\t            cascades=scene_meta.cascades,\n\t            grid_resolution=args.raymarch.density_grid_res,\n\t        ),\n\t        raymarch=args.raymarch,\n\t        render=args.render,\n\t        scene_options=args.scene,\n\t        scene_meta=scene_meta,\n", "        # unfreeze the frozen dict so that the weight_decay mask can apply, see:\n\t        #   <https://github.com/deepmind/optax/issues/160>\n\t        #   <https://github.com/google/flax/issues/1223>\n\t        nerf_fn=nerf_model.apply,\n\t        bg_fn=bg_model.apply if scene_meta.bg else None,\n\t        params={\n\t            \"nerf\": nerf_variables[\"params\"].unfreeze(),\n\t            \"bg\": bg_variables[\"params\"].unfreeze() if scene_meta.bg else None,\n\t            \"appearance_embeddings\": jran.uniform(\n\t                key=key,\n", "                shape=(len(scene_meta.frames), scene_meta.n_extra_learnable_dims),\n\t                dtype=jnp.float32,\n\t                minval=-1,\n\t                maxval=1,\n\t            ),\n\t        },\n\t        tx=make_optimizer(args.train.lr),\n\t    )\n\t    if args.ckpt is not None:\n\t        state = checkpoints.restore_checkpoint(args.ckpt, target=state)\n", "        if state.step == 0:\n\t            logger.error(\"an empty checkpoint was loaded from '{}'\".format(args.ckpt))\n\t            return 3\n\t        logger.info(\"checkpoint loaded from '{}' (step={})\".format(args.ckpt, int(state.step)))\n\t    state = state.mark_untrained_density_grid()  # still needs to mark grids even if state is loaded\n\t    logger.info(\"starting training\")\n\t    # training loop\n\t    for ep in range(state.epoch(args.train.iters), args.train.epochs):\n\t        gc.collect()\n\t        ep_log = ep + 1\n", "        KEY, key_resample, key_train = jran.split(KEY, 3)\n\t        scene_train = scene_train.resample_pixels(\n\t            KEY=key_resample,\n\t            new_max_mem_mbytes=args.scene.max_mem_mbytes,\n\t        )\n\t        state, metrics = train_epoch(\n\t            KEY=key_train,\n\t            state=state,\n\t            scene=scene_train,\n\t            iters=args.train.iters,\n", "            total_samples=args.train.bs,\n\t            ep_log=ep_log,\n\t            total_epochs=args.train.epochs,\n\t            logger=logger,\n\t        )\n\t        if metrics[\"interrupted\"]:\n\t            logger.warn(\"aborted at epoch {}\".format(ep_log))\n\t            logger.info(\"saving training state ... \")\n\t            ckpt_name = checkpoints.save_checkpoint(logs_dir, state, step=\"ep{}aborted\".format(ep_log), overwrite=True, keep=2**30)\n\t            logger.info(\"training state of epoch {} saved to: {}\".format(ep_log, ckpt_name))\n", "            logger.info(\"exiting cleanly ...\")\n\t            return 0\n\t        mean_loss = jax.tree_util.tree_map(\n\t            lambda val: val / metrics[\"n_processed_rays\"],\n\t            metrics[\"total_loss\"],\n\t        )\n\t        logger.info(\"epoch#{:03d}: loss:{{rgb={:.3e}({:.2f}dB),tv={:.3e}}}\".format(\n\t            ep_log,\n\t            mean_loss[\"rgb\"],\n\t            data.linear_to_db(mean_loss[\"rgb\"], maxval=1,),\n", "            mean_loss[\"total_variation\"],\n\t        ))\n\t        logger.write_scalar(\"epoch/‚Üìloss (rgb)\", mean_loss[\"rgb\"], step=ep_log)\n\t        logger.write_scalar(\"epoch/‚Üëestimated PSNR (db)\", data.linear_to_db(mean_loss[\"rgb\"], maxval=1), step=ep_log)\n\t        logger.write_scalar(\"batch/‚Üìloss (total variation)\", mean_loss[\"total_variation\"], state.step)\n\t        logger.info(\"saving training state ... \")\n\t        ckpt_name = checkpoints.save_checkpoint(\n\t            logs_dir,\n\t            state,\n\t            step=ep_log * args.train.iters,\n", "            overwrite=True,\n\t            keep=args.train.keep,\n\t            keep_every_n_steps=args.train.keep_every_n_steps,\n\t        )\n\t        logger.info(\"training state of epoch {} saved to: {}\".format(ep_log, ckpt_name))\n\t        if ep_log % args.train.validate_every == 0:\n\t            if len(args.frames_val) == 0:\n\t                logger.warn(\"empty validation set, skipping validation\")\n\t                continue\n\t            val_start_time = time.time()\n", "            rendered_images: List[RenderedImage] = []\n\t            state_eval = state\\\n\t                .replace(raymarch=args.raymarch_eval)\\\n\t                .replace(render=args.render_eval)\n\t            for val_i, val_view in enumerate(common.tqdm(scene_val.all_views, desc=\"| validating\")):\n\t                logger.debug(\"validating on {}\".format(val_view.file))\n\t                val_transform = RigidTransformation(\n\t                    rotation=scene_val.transforms[val_i, :9].reshape(3, 3),\n\t                    translation=scene_val.transforms[val_i, -3:].reshape(3),\n\t                )\n", "                KEY, key = jran.split(KEY, 2)\n\t                bg, rgb, depth, _ = data.to_cpu(render_image_inference(\n\t                    KEY=key,\n\t                    transform_cw=val_transform,\n\t                    state=state_eval,\n\t                ))\n\t                rendered_images.append(RenderedImage(\n\t                    bg=bg,\n\t                    rgb=rgb,\n\t                    depth=depth,  # call to data.mono_to_rgb is deferred below so as to minimize impact on rendering speed\n", "                ))\n\t            val_end_time = time.time()\n\t            logger.write_scalar(\n\t                tag=\"validation/‚Üìrendering time (ms) per image\",\n\t                value=(val_end_time - val_start_time) / len(rendered_images) * 1000,\n\t                step=ep_log,\n\t            )\n\t            gt_rgbs_f32 = list(map(\n\t                lambda val_view, rendered_image: data.blend_rgba_image_array(\n\t                    val_view.image_rgba_u8.astype(jnp.float32) / 255,\n", "                    rendered_image.bg,\n\t                ),\n\t                scene_val.all_views,\n\t                rendered_images,\n\t            ))\n\t            logger.debug(\"calculating psnr\")\n\t            mean_psnr = sum(map(\n\t                data.psnr,\n\t                map(data.f32_to_u8, gt_rgbs_f32),\n\t                map(lambda ri: ri.rgb, rendered_images),\n", "            )) / len(rendered_images)\n\t            logger.info(\"validated {} images, mean psnr={}\".format(len(rendered_images), mean_psnr))\n\t            logger.write_scalar(\"validation/‚Üëmean psnr\", mean_psnr, step=ep_log)\n\t            logger.debug(\"writing images to tensorboard\")\n\t            concatenate_fn = lambda gt, rendered_image: data.add_border(functools.reduce(\n\t                functools.partial(\n\t                    data.side_by_side,\n\t                    width=scene_meta.camera.width,\n\t                    height=scene_meta.camera.height,\n\t                ),\n", "                [\n\t                    gt,\n\t                    rendered_image.rgb,\n\t                    common.compose(data.mono_to_rgb, data.f32_to_u8)(rendered_image.depth),\n\t                ],\n\t            ))\n\t            logger.write_image(\n\t                tag=\"validation/[gt|rendered|depth]\",\n\t                image=list(map(\n\t                    concatenate_fn,\n", "                    map(data.f32_to_u8, gt_rgbs_f32),\n\t                    rendered_images,\n\t                )),\n\t                step=ep_log,\n\t                max_outputs=len(rendered_images),\n\t            )\n\t            del state_eval\n\t            del gt_rgbs_f32\n\t            del rendered_images\n\t    return 0\n"]}
{"filename": "app/nerf/gui.py", "chunked_list": ["from copy import deepcopy\n\tfrom enum import Enum\n\timport logging\n\tfrom pathlib import Path\n\timport numpy as np\n\tfrom typing import List, Any, Tuple, Union\n\timport jax\n\timport jax.random as jran\n\timport jax.numpy as jnp\n\tfrom flax.training import checkpoints\n", "from dataclasses import dataclass, field\n\timport threading\n\timport dearpygui.dearpygui as dpg\n\timport ctypes\n\tfrom utils.args import NeRFGUIArgs\n\tfrom .train import *\n\tfrom utils.types import (RGBColor, SceneData, SceneMeta, Camera)\n\tfrom models.nerfs import (NeRF, SkySphereBg)\n\tfrom PIL import Image\n\timport time\n", "@dataclass\n\tclass CKPT():\n\t    need_load_ckpt = False\n\t    ckpt_file_path: Path = Path(\"\")\n\t    step: int = 0\n\t    def parse_ckpt(self, ckpt_name: str, ckpt_path: str) -> str:\n\t        success = False\n\t        s = ckpt_name.split(\"_\")\n\t        if s[0] == \"checkpoint\" and Path(ckpt_path).exists:\n\t            try:\n", "                self.step = int(s[1].split(\".\")[0])\n\t                self.ckpt_file_path = Path(ckpt_path)\n\t                self.need_load_ckpt = True\n\t                success = True\n\t            except TypeError or ValueError as e:\n\t                self.logger.error(e)\n\t            finally:\n\t                if success:\n\t                    return \"checkpoint loaded from '{}'\".format(self.ckpt_file_path)\n\t                return \"Fail to load checkpoint, causing the file is not a checkpoint\"\n", "@dataclass\n\tclass CameraPose():\n\t    theta: float = 160.0\n\t    phi: float = -30.0\n\t    radius: float = 4.0\n\t    tx: float = 0.0\n\t    ty: float = 0.0\n\t    centroid: np.ndarray = np.asarray([0., 0., 0.])\n\t    def pose_spherical(self, theta, phi, radius):\n\t        trans_t = lambda t: np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, t],\n", "                                      [0, 0, 0, 1]], np.float32)\n\t        rot_phi = lambda phi: np.array(\n\t            [[1, 0, 0, 0], [0, np.cos(phi), -np.sin(phi), 0],\n\t             [0, np.sin(phi), np.cos(phi), 0], [0, 0, 0, 1]], np.float32)\n\t        rot_theta = lambda theta: np.array(\n\t            [[np.cos(theta), 0, -np.sin(theta), 0], [0, 1, 0, 0],\n\t             [np.sin(theta), 0, np.cos(theta), 0], [0, 0, 0, 1]], np.float32)\n\t        c2w = trans_t(radius)\n\t        #rotate\n\t        c2w = np.matmul(rot_phi(phi / 180. * np.pi), c2w)\n", "        c2w = np.matmul(rot_theta(theta / 180. * np.pi), c2w)\n\t        return c2w\n\t    @property\n\t    def pose(self):\n\t        mod = lambda x: x % 360\n\t        self.theta = mod(self.theta)\n\t        self.phi = mod(self.phi)\n\t        c2w = self.pose_spherical(self.theta, self.phi, self.radius)\n\t        #translate\n\t        self.centroid = np.asarray(\n", "            self.centroid) + self.tx * c2w[:3, 0] + self.ty * c2w[:3, 1]\n\t        self.tx, self.ty = 0, 0\n\t        trans_centroid = np.array(\n\t            [[1, 0, 0, self.centroid[0]], [0, 1, 0, self.centroid[1]],\n\t             [0, 0, 1, self.centroid[2]], [0, 0, 0, 1]], np.float32)\n\t        c2w = np.matmul(trans_centroid, c2w)\n\t        c2w = np.matmul(np.array([[-1, 0, 0, 0], [0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 0,1]]), c2w)\n\t        return jnp.asarray(c2w)\n\t    def move(self, dx, dy):\n\t        self.theta += .3 * dx\n", "        self.phi -= .2 * dy\n\t        return self.pose\n\t    def trans(self, dx, dy):\n\t        velocity = 8e-4\n\t        self.tx -= dx * velocity * self.radius\n\t        self.ty += dy * velocity * self.radius\n\t        return self.pose\n\t    def change_radius(self, rate):\n\t        self.radius *= 1.1**(-rate)\n\t        return self.pose\n", "@dataclass\n\tclass Gui_trainer():\n\t    KEY: jran.KeyArray\n\t    args: NeRFGUIArgs\n\t    logger: common.Logger\n\t    camera_pose: jnp.array\n\t    back_color: RGBColor\n\t    scene_train: SceneData = field(init=False)\n\t    scene_meta: SceneMeta = field(init=False)\n\t    nerf_model_train: NeRF = field(init=False)\n", "    nerf_model_inference: NeRF = field(init=False)\n\t    nerf_variables: Any = field(init=False)\n\t    bg_model: SkySphereBg = field(init=False, default=None)\n\t    bg_variables: Any = field(init=False)\n\t    optimizer: Any = field(init=False)\n\t    state: NeRFState = field(init=False)\n\t    last_resample_step: int = -1\n\t    cur_step: int = 0\n\t    log_step: int = 0\n\t    loss_log: str = \"--\"\n", "    istraining: bool = field(init=False)\n\t    data_step: List[int] = field(default_factory=list, init=False)\n\t    data_pixel_quality: List[float] = field(default_factory=list, init=False)\n\t    compacted_batch: int = -1\n\t    not_compacted_batch: int = -1\n\t    rays_num: int = -1\n\t    mean_effective_samples_per_ray: int = -1\n\t    mean_samples_per_ray: int = -1\n\t    camera_near: float = 0.1\n\t    camera: Camera = field(init=False)\n", "    need_exit: bool = False\n\t    loading_ckpt: bool = False\n\t    ckpt: CKPT = CKPT()\n\t    def __post_init__(self):\n\t        self.data_step = []\n\t        self.data_pixel_quality = []\n\t        self.cur_step = 0\n\t        self.istraining = True\n\t        self.args.exp_dir.mkdir(parents=True, exist_ok=True)\n\t        save_dir = common.backup_current_codebase(self.args.exp_dir, name_prefix=\"gui-\",\n", "                                                  note=self.args.note)\n\t        config_save_path = save_dir.joinpath(\"config.yaml\")\n\t        config_save_path.write_text(tyro.to_yaml(self.args))\n\t        logs_dir = save_dir.joinpath(\"logs\")\n\t        logs_dir.mkdir(parents=True, exist_ok=True)\n\t        self.logger = common.setup_logging(\n\t            \"nerf.gui\",\n\t            file=logs_dir.joinpath(\"gui.log\"),\n\t            with_tensorboard=True,\n\t            level=self.args.common.logging.upper(),\n", "            file_level=\"DEBUG\",\n\t        )\n\t        self.logger.write_hparams(dataclasses.asdict(self.args))\n\t        self.logger.info(\"code saved to '{}', configurations saved at '{}'\".format(\n\t            save_dir,\n\t            config_save_path,\n\t        ))\n\t        # load data\n\t        self.scene_train = data.load_scene(\n\t            srcs=self.args.frames_train,\n", "            scene_options=self.args.scene,\n\t        )\n\t        self.scene_meta = self.scene_train.meta\n\t        # model parameters\n\t        self.nerf_model_train, self.nerf_model_inference, init_input = (\n\t            make_nerf_ngp(bound=self.scene_meta.bound,\n\t                          inference=False,\n\t                          tv_scale=self.args.train.tv_scale),\n\t            make_nerf_ngp(bound=self.scene_meta.bound,\n\t                          inference=True), (jnp.zeros((1, 3), dtype=jnp.float32),\n", "                                            jnp.zeros((1, 3), dtype=jnp.float32),\n\t                                            jnp.zeros((1, self.scene_meta.n_extra_learnable_dims),\n\t                                                      dtype=jnp.float32)))\n\t        self.KEY, key = jran.split(self.KEY, 2)\n\t        self.nerf_variables = self.nerf_model_train.init(key, *init_input)\n\t        if self.args.common.summary:\n\t            print(self.nerf_model_train.tabulate(key, *init_input))\n\t        if self.scene_meta.bg:\n\t            self.bg_model, init_input = (make_skysphere_background_model_ngp(\n\t                bound=self.scene_meta.bound), (jnp.zeros((1, 3),dtype=jnp.float32),\n", "                                               jnp.zeros((1, 3),dtype=jnp.float32),\n\t                                               jnp.zeros((1, self.scene_meta.n_extra_learnable_dims),\n\t                                                      dtype=jnp.float32)))\n\t            self.KEY, key = jran.split(self.KEY, 2)\n\t            self.bg_variables = self.bg_model.init(key, *init_input)\n\t        self.optimizer = make_optimizer(self.args.train.lr)\n\t        if self.ckpt.need_load_ckpt:\n\t            self.load_checkpoint(self.ckpt.ckpt_file_path, self.ckpt.step)\n\t        else:\n\t            self.KEY, key = jran.split(self.KEY, 2)\n", "            self.state = NeRFState.create(\n\t                ogrid=OccupancyDensityGrid.create(\n\t                    cascades=self.scene_meta.cascades,\n\t                    grid_resolution=self.args.raymarch.density_grid_res,\n\t                ),\n\t                raymarch=self.args.raymarch,\n\t                render=self.args.render,\n\t                scene_options=self.args.scene,\n\t                scene_meta=self.scene_meta,\n\t                # unfreeze the frozen dict so that the weight_decay mask can apply, see:\n", "                #   <https://github.com/deepmind/optax/issues/160>\n\t                #   <https://github.com/google/flax/issues/1223>\n\t                nerf_fn=self.nerf_model_train.apply,\n\t                bg_fn=self.bg_model.apply if self.scene_meta.bg else None,\n\t                params={\n\t                    \"nerf\":\n\t                    self.nerf_variables[\"params\"].unfreeze(),\n\t                    \"bg\":\n\t                    self.bg_variables[\"params\"].unfreeze()\n\t                    if self.scene_meta.bg else None,\n", "                    \"appearance_embeddings\": jran.uniform(\n\t                        key=key,\n\t                        shape=(len(self.scene_meta.frames), self.scene_meta.n_extra_learnable_dims),\n\t                        dtype=jnp.float32,\n\t                        minval=-1,\n\t                        maxval=1,\n\t                    )\n\t                },\n\t                tx=self.optimizer,\n\t            )\n", "            self.state = self.state.mark_untrained_density_grid()\n\t        self.camera = Camera(\n\t            width=self.args.viewport.W,\n\t            height=self.args.viewport.H,\n\t            fx=self.scene_meta.camera.fx,\n\t            fy=self.scene_meta.camera.fy,\n\t            cx=self.args.viewport.W / 2,\n\t            cy=self.args.viewport.H / 2,\n\t            near=self.camera_near,\n\t        )\n", "    def set_render_camera(self, _scale, _H, _W) -> Camera:\n\t        self.camera = Camera(\n\t            width=_W,\n\t            height=_H,\n\t            fx=self.scene_meta.camera.fx,\n\t            fy=self.scene_meta.camera.fy,\n\t            cx=_W / 2,\n\t            cy=_H / 2,\n\t            near=self.camera_near,\n\t        )\n", "        self.camera = self.camera.scale_resolution(_scale)\n\t    def render_frame(self, _scale: float, _H: int, _W: int, render_cost: bool):\n\t        self.set_render_camera(_scale, _H, _W)\n\t        #camera pose\n\t        transform = RigidTransformation(\n\t            rotation=self.camera_pose[:3, :3],\n\t            translation=jnp.squeeze(self.camera_pose[:3, 3].reshape(-1, 3),\n\t                                    axis=0))\n\t        self.KEY, key = jran.split(self.KEY, 2)\n\t        bg, rgb, depth, cost = render_image_inference(\n", "            KEY=key,\n\t            transform_cw=transform,\n\t            state=self.state.replace(\n\t                raymarch=self.args.raymarch_eval,\n\t                render=self.args.render_eval.replace(bg=self.back_color),\n\t                nerf_fn=self.nerf_model_inference.apply,\n\t            ),\n\t            camera_override=self.camera,\n\t            render_cost=render_cost)\n\t        bg = self.get_npf32_image(bg,\n", "                                  W=self.args.viewport.W,\n\t                                  H=self.args.viewport.H)\n\t        rgb = self.get_npf32_image(rgb,\n\t                                   W=self.args.viewport.W,\n\t                                   H=self.args.viewport.H)\n\t        depth = self.color_depth(depth,\n\t                                 W=self.args.viewport.W,\n\t                                 H=self.args.viewport.H)\n\t        if render_cost:\n\t            cost = self.get_cost_image(cost,\n", "                                       W=self.args.viewport.W,\n\t                                       H=self.args.viewport.H)\n\t        return (bg, rgb, depth, cost)\n\t    def get_cost_image(self, cost, W, H):\n\t        img = Image.fromarray(np.array(cost, dtype=np.uint8))\n\t        img = img.convert('RGB')\n\t        img = img.resize(size=(W, H), resample=Image.NEAREST)\n\t        cost = np.array(img, dtype=np.float32) / 255.\n\t        return cost\n\t    def color_depth(self, depth, W, H):\n", "        depth = np.array(data.f32_to_u8(data.mono_to_rgb(depth)),\n\t                         dtype=np.uint8)\n\t        img = Image.fromarray(depth, mode='RGBA')\n\t        img = img.convert('RGB')\n\t        img = img.resize(size=(W, H), resample=Image.NEAREST)\n\t        depth = np.array(img, dtype=np.float32) / 255.\n\t        return depth\n\t    def load_checkpoint(self, path: Path, step: int):\n\t        self.loading_ckpt = True\n\t        try:\n", "            if not path.exists():\n\t                raise FileNotFoundError(\"{} does not exist\".format(path))\n\t            self.logger.info(\"loading checkpoint from '{}'\".format(path))\n\t            state: NeRFState = checkpoints.restore_checkpoint(\n\t                path,\n\t                target=NeRFState.empty(\n\t                    raymarch=self.args.raymarch,\n\t                    render=self.args.render,\n\t                    scene_options=self.args.scene,\n\t                    scene_meta=self.scene_meta,\n", "                    nerf_fn=self.nerf_model_train.apply,\n\t                    bg_fn=self.bg_model.apply if self.scene_meta.bg else None,\n\t                    tx=self.optimizer,\n\t                ),\n\t            )\n\t            # WARN:\n\t            #   flax.checkpoints.restore_checkpoint() returns a pytree with all arrays of numpy's array type,\n\t            #   which slows down inference.  use jax.device_put() to move them to jax's default device.\n\t            # REF: <https://github.com/google/flax/discussions/1199#discussioncomment-635132>\n\t            self.state = jax.device_put(state)\n", "            self.state = self.state.mark_untrained_density_grid()\n\t            self.logger.info(\"checkpoint loaded from '{}'\".format(path))\n\t            self.cur_step = step\n\t            self.loading_ckpt = False\n\t            return \"checkpoint loaded from '{}'\".format(path)\n\t        except BaseException as e:\n\t            self.logger.error(e)\n\t            return e\n\t    def train_steps(self, steps: int) -> Tuple[np.array, np.array, np.array]:\n\t        if self.loading_ckpt:\n", "            return\n\t        gc.collect()\n\t        try:\n\t            if self.istraining:\n\t                if self.last_resample_step < 0 or self.state.step - self.last_resample_step >= 1024:\n\t                    self.KEY, key_resample = jran.split(self.KEY, 2)\n\t                    self.scene_train = self.scene_train.resample_pixels(\n\t                        KEY=key_resample,\n\t                        new_max_mem_mbytes=self.args.scene.max_mem_mbytes,\n\t                    )\n", "                    self.last_resample_step = self.state.step\n\t                self.KEY, key_train = jran.split(self.KEY, 2)\n\t                self.state = self.gui_train_epoch(\n\t                    KEY=key_train,\n\t                    state=self.state,\n\t                    scene=self.scene_train,\n\t                    iters=steps,\n\t                    total_samples=self.args.train.bs,\n\t                    #total_samples=self.args.train.bs,\n\t                    cur_steps=self.cur_step,\n", "                    logger=self.logger,\n\t                )\n\t                self.cur_step = self.cur_step + steps\n\t        except UnboundLocalError as e:\n\t            self.logger.exception(e)\n\t    def get_npf32_image(self, img: jnp.array, W, H) -> np.array:\n\t        img = Image.fromarray(np.array(img, dtype=np.uint8))\n\t        img = img.resize(size=(W, H), resample=Image.NEAREST)\n\t        img = np.array(img, dtype=np.float32) / 255.\n\t        return img\n", "    def gui_train_epoch(\n\t        self,\n\t        KEY: jran.KeyArray,\n\t        state: NeRFState,\n\t        scene: SceneData,\n\t        iters: int,\n\t        total_samples: int,\n\t        cur_steps: int,\n\t        logger: common.Logger,\n\t    ):\n", "        self.log_step = 0\n\t        for _ in (pbar := common.tqdm(range(iters),\n\t                                      desc=\"Training step#{:03d}\".format(cur_steps),\n\t                                      leave=False)):\n\t            if self.need_exit:\n\t                raise KeyboardInterrupt\n\t            if not self.istraining:\n\t                logger.warn(\"aborted at step {}\".format(cur_steps))\n\t                logger.debug(\"exiting cleanly ...\")\n\t                exit()\n", "            KEY, key_perm, key_train_step = jran.split(KEY, 3)\n\t            perm = jran.choice(key_perm,\n\t                               scene.n_pixels,\n\t                               shape=(total_samples, ),\n\t                               replace=True)\n\t            state, metrics = train_step(\n\t                state,\n\t                KEY=key_train_step,\n\t                total_samples=total_samples,\n\t                scene=scene,\n", "                perm=perm,\n\t            )\n\t            self.log_step += 1\n\t            cur_steps = cur_steps + 1\n\t            loss = metrics[\"loss\"]\n\t            self.data_step, self.data_pixel_quality = (  # the 2 lists are ploted so should be updated simultaneously\n\t                self.data_step + [self.log_step + self.cur_step],\n\t                self.data_pixel_quality +\n\t                [data.linear_to_db(loss[\"rgb\"], maxval=1)])\n\t            self.mean_effective_samples_per_ray = metrics[\"measured_batch_size\"] / metrics[\"n_valid_rays\"]\n", "            self.mean_samples_per_ray = metrics[\"measured_batch_size_before_compaction\"] / metrics[\"n_valid_rays\"]\n\t            pbar.set_description_str(\n\t                desc=\"Training step#{:03d} \".format(cur_steps) + format_metrics(metrics))\n\t            if state.should_call_update_ogrid:\n\t                # update occupancy grid\n\t                for cas in range(state.scene_meta.cascades):\n\t                    KEY, key = jran.split(KEY, 2)\n\t                    state = state.update_ogrid_density(\n\t                        KEY=key,\n\t                        cas=cas,\n", "                        update_all=bool(state.should_update_all_ogrid_cells),\n\t                        max_inference=total_samples,\n\t                    )\n\t                state = state.threshold_ogrid()\n\t            self.compacted_batch = metrics[\"measured_batch_size\"]\n\t            self.not_compacted_batch = metrics[\n\t                \"measured_batch_size_before_compaction\"]\n\t            self.rays_num = metrics[\"n_valid_rays\"]\n\t            if state.should_write_batch_metrics:\n\t                logger.write_metrics_to_tensorboard(metrics, state.step)\n", "        return state\n\t    def stop_trainer(self):\n\t        self.istraining = False\n\t    def setBackColor(self, color: RGBColor):\n\t        self.back_color = color\n\t    def get_currentStep(self):\n\t        return self.cur_step\n\t    def get_logStep(self):\n\t        return self.log_step\n\t    def get_state(self) -> NeRFState:\n", "        return self.state\n\t    def get_plotData(self):\n\t        return (self.data_step, self.data_pixel_quality)\n\t    def get_effective_samples_nums(self):\n\t        return self.mean_effective_samples_per_ray\n\t    def get_samples_nums(self):\n\t        return self.mean_samples_per_ray\n\t    def get_compactedBatch(self):\n\t        return self.compacted_batch\n\t    def get_notCompactedBatch(self):\n", "        return self.not_compacted_batch\n\t    def get_raysNum(self):\n\t        return self.rays_num\n\tclass TrainThread(threading.Thread):\n\t    def __init__(self, KEY, args: NeRFGUIArgs, logger, camera_pose, step,\n\t                 back_color, ckpt):\n\t        super(TrainThread, self).__init__()\n\t        self.KEY = KEY\n\t        self.args = args\n\t        self.logger = logger\n", "        self.camera_pose = camera_pose\n\t        self.istraining = True\n\t        self.needUpdate = True\n\t        self.istesting = False\n\t        self.needtesting = False\n\t        self.step = step\n\t        self.scale = self.args.viewport.resolution_scale\n\t        self.H, self.W = self.args.viewport.H, self.args.viewport.W\n\t        self.back_color = back_color\n\t        self.framebuff = None\n", "        self.rgb = None\n\t        self.depth = None\n\t        self.trainer = None\n\t        self.initFrame()\n\t        self.train_infer_time = -1\n\t        self.render_infer_time = -1\n\t        self.data_step = []\n\t        self.data_pixel_quality = []\n\t        self.compacted_batch = -1\n\t        self.not_compacted_batch = -1\n", "        self.rays_num = -1\n\t        self.frame_updated = False\n\t        self.mode = Mode.Render\n\t        self.havestart = False\n\t        self.ckpt = ckpt\n\t    def initFrame(self):\n\t        frame_init = np.tile(np.asarray(self.back_color, dtype=np.float32),\n\t                             (self.H, self.W, 1))\n\t        self.framebuff = frame_init.copy()\n\t        self.rgb = frame_init.copy()\n", "        self.depth = frame_init.copy()\n\t        self.cost = frame_init.copy()\n\t        self.frame_updated = True\n\t    def setMode(self, mode):\n\t        self.mode = mode\n\t    def setBackColor(self, color: RGBColor):\n\t        self.back_color = color\n\t        if self.trainer:\n\t            self.trainer.setBackColor(self.back_color)\n\t    def run(self):\n", "        try:\n\t            self.trainer = Gui_trainer(KEY=self.KEY,\n\t                                       args=self.args,\n\t                                       logger=self.logger,\n\t                                       camera_pose=self.camera_pose,\n\t                                       back_color=self.back_color,\n\t                                       ckpt=self.ckpt)\n\t        except Exception as e:\n\t            self.logger.exception(e)\n\t            self.needUpdate = False\n", "        while self.needUpdate:\n\t            try:\n\t                if self.istraining and self.trainer:\n\t                    start_time = time.time()\n\t                    self.trainer.train_steps(self.step)\n\t                    end_time = time.time()\n\t                    self.train_infer_time = end_time - start_time\n\t                    self.test()\n\t                if self.istesting and self.needtesting:\n\t                    self.havestart = True\n", "                    start_time = time.time()\n\t                    self.trainer.setBackColor(self.back_color)\n\t                    _, self.rgb, self.depth, self.cost = self.trainer.render_frame(\n\t                        self.scale, self.H, self.W, self.mode == Mode.Cost)\n\t                    if self.mode == Mode.Render:\n\t                        self.framebuff = self.rgb\n\t                    elif self.mode == Mode.depth:\n\t                        self.framebuff = self.depth\n\t                    elif self.mode == Mode.Cost:\n\t                        if self.cost is not None:\n", "                            self.framebuff = self.cost\n\t                        else:\n\t                            self.framebuff = np.tile(np.asarray(self.back_color, dtype=np.float32),\n\t                                                     (self.H, self.W, 1))\n\t                    else:\n\t                        raise NotImplementedError(\"visualization mode '{}' is not implemented\"\n\t                                                  .format(self.mode))\n\t                    self.frame_updated = True\n\t                    end_time = time.time()\n\t                    self.render_infer_time = end_time - start_time\n", "                    self.istesting = False\n\t            except Exception as e:\n\t                self.logger.exception(e)\n\t                break\n\t    def get_TrainInferTime(self):\n\t        if self.train_infer_time != -1:\n\t            return \"{:.6f}\".format(self.train_infer_time)\n\t        else:\n\t            return \"no data\"\n\t    def get_RenderInferTime(self):\n", "        if self.render_infer_time != -1:\n\t            return \"{:.6f}\".format(self.render_infer_time)\n\t        else:\n\t            return \"no data\"\n\t    def get_Fps(self):\n\t        if self.train_infer_time == -1 and self.render_infer_time == -1:\n\t            return \"no data\"\n\t        elif self.render_infer_time == -1:\n\t            return \"{:.3f}\".format(1.0 / (self.train_infer_time))\n\t        elif self.train_infer_time == -1 or not self.istraining:\n", "            return \"{:.3f}\".format(1.0 / (self.render_infer_time))\n\t        else:\n\t            return \"{:.3f}\".format(\n\t                1.0 / (self.render_infer_time + self.train_infer_time))\n\t    def get_compactedBatch(self):\n\t        if self.trainer:\n\t            self.compacted_batch = self.trainer.get_compactedBatch()\n\t            if self.compacted_batch != -1:\n\t                return \"{:d}\".format(self.compacted_batch)\n\t            else:\n", "                return \"no data\"\n\t        return \"no data\"\n\t    def get_notCompactedBatch(self):\n\t        if self.trainer:\n\t            self.not_compacted_batch = self.trainer.get_notCompactedBatch()\n\t            if self.not_compacted_batch != -1:\n\t                return \"{:d}\".format(self.not_compacted_batch)\n\t            else:\n\t                return \"no data\"\n\t        return \"no data\"\n", "    def get_raysNum(self):\n\t        if self.trainer:\n\t            self.rays_num = self.trainer.get_raysNum()\n\t            if self.rays_num != -1:\n\t                return \"{:d}\".format(self.rays_num)\n\t            else:\n\t                return \"no data\"\n\t        return \"no data\"\n\t    def stop(self):\n\t        self.istraining = False\n", "        self.needUpdate = False\n\t        if self.trainer:\n\t            self.trainer.stop_trainer()\n\t        thread_id = self.get_id()\n\t        self.logger.debug(\"throwing training thread exit Exception\")\n\t        res = ctypes.pythonapi.PyThreadState_SetAsyncExc(\n\t            thread_id, ctypes.py_object(SystemExit))\n\t        if res > 1:\n\t            ctypes.pythonapi.PyThreadState_SetAsyncExc(thread_id, 0)\n\t            self.logger.warn(\"exception raise failure\",\n", "                             category=None,\n\t                             stacklevel=1)\n\t    def set_scale(self, _scale):\n\t        self.scale = _scale\n\t    def get_scale(self):\n\t        return self.scale\n\t    def get_id(self):\n\t        # returns id of the respective thread\n\t        if hasattr(self, '_thread_id'):\n\t            return self._thread_id\n", "        for id, thread in threading._active.items():\n\t            if thread is self:\n\t                return id\n\t    def get_state(self) -> NeRFState:\n\t        return self.trainer.get_state()\n\t    def set_camera_pose(self, camera_pose):\n\t        if self.trainer:\n\t            self.trainer.camera_pose = camera_pose\n\t    def change_WH(self, W, H):\n\t        self.W = W\n", "        self.H = H\n\t    def get_logStep(self):\n\t        if self.trainer:\n\t            return self.trainer.get_logStep()\n\t        return 0\n\t    def get_currentStep(self):\n\t        if self.trainer:\n\t            return self.trainer.get_currentStep()\n\t        return 0\n\t    def get_plotData(self):\n", "        if self.trainer:\n\t            self.data_step, self.data_pixel_quality = self.trainer.get_plotData(\n\t            )\n\t        return (self.data_step, self.data_pixel_quality)\n\t    def get_effective_samples_nums(self):\n\t        if self.trainer:\n\t            return \"{:.3f}\".format(self.trainer.mean_effective_samples_per_ray)\n\t        else:\n\t            return \"no data\"\n\t    def get_samples_nums(self):\n", "        if self.trainer:\n\t            return \"{:.3f}\".format(\n\t                self.trainer.mean_samples_per_ray)\n\t        else:\n\t            return \"no data\"\n\t    def test(self):\n\t        self.istesting = True\n\t    def finishUpdate(self):\n\t        self.frame_updated = False\n\t    def canUpdate(self):\n", "        return self.frame_updated\n\t    def setStep(self, step):\n\t        self.step = step\n\t    def setCamNear(self, near):\n\t        if self.trainer:\n\t            self.trainer.camera_near = near\n\t    def getPinholeCam(self):\n\t        if self.trainer:\n\t            return self.trainer.camera\n\t        return None\n", "class Mode(Enum):\n\t    Render = 1\n\t    depth = 2\n\t    Cost = 3\n\t@dataclass\n\tclass NeRFGUI():\n\t    framebuff: Any = field(init=False)\n\t    H: int = field(init=False)\n\t    W: int = field(init=False)\n\t    need_train: bool = False\n", "    istesting: bool = False\n\t    train_thread: TrainThread = field(init=False)\n\t    args: NeRFGUIArgs = None\n\t    KEY: jran.KeyArray = None\n\t    logger: logging.Logger = None\n\t    cameraPose: CameraPose = CameraPose()\n\t    cameraPosePrev: CameraPose = CameraPose()\n\t    cameraPoseNext: CameraPose = CameraPose()\n\t    scale_slider: Union[int, str] = field(init=False)\n\t    back_color: RGBColor = field(init=False)\n", "    scale: float = field(init=False)\n\t    data_step: List[int] = field(default_factory=list, init=False)\n\t    data_pixel_quality: List[float] = field(default_factory=list, init=False)\n\t    texture_H: int = field(init=False)\n\t    texture_W: int = field(init=False)\n\t    View_H: int = field(init=False)\n\t    View_W: int = field(init=False)\n\t    exit_flag: bool = False\n\t    mode: Mode = Mode.Render\n\t    mouse_pressed: bool = False\n", "    need_test: bool = True\n\t    #ckpt\n\t    ckpt: CKPT = CKPT()\n\t    @property\n\t    def _effective_resolution_display(self) -> str:\n\t        return \"{}x{}\".format(\n\t            *map(lambda val: int(val * self.scale), (self.W, self.H)))\n\t    def __post_init__(self):\n\t        self.H, self.W = self.args.viewport.H, self.args.viewport.W\n\t        self.back_color = self.args.render_eval.bg\n", "        self.scale = self.args.viewport.resolution_scale\n\t        self.texture_H, self.texture_W = self.H, self.W\n\t        self.framebuff = np.tile(np.asarray(self.back_color, dtype=np.float32),\n\t                                 (self.H, self.W, 3))\n\t        radius_init = 4.\n\t        self.cameraPose, self.cameraPosePrev, self.cameraPoseNext = (\n\t            CameraPose(radius=radius_init),\n\t            CameraPose(radius=radius_init),\n\t            CameraPose(radius=radius_init),\n\t        )\n", "        dpg.create_context()\n\t        self.train_thread = None\n\t        self.ItemsLayout()\n\t    def ItemsLayout(self):\n\t        def callback_backgroundColor():\n\t            self.back_color = tuple(\n\t                map(lambda val: val / 255,\n\t                    dpg.get_value(\"_BackColor\")[:3]))\n\t            self.setFrameColor()\n\t        def callback_mouseDrag(_, app_data):\n", "            if not dpg.is_item_focused(\"_primary_window\"):\n\t                return\n\t            if not self.need_test:\n\t                return\n\t            dx = app_data[1]\n\t            dy = app_data[2]\n\t            self.cameraPoseNext = deepcopy(self.cameraPosePrev)\n\t            self.cameraPoseNext.move(dx, dy)\n\t            if self.train_thread:\n\t                self.train_thread.set_camera_pose(self.cameraPoseNext.pose)\n", "                self.train_thread.test()\n\t                self.show_cam_angle(self.cameraPoseNext.theta,\n\t                                    self.cameraPoseNext.phi)\n\t        def callback_midmouseDrag(_, app_data):\n\t            if not self.need_test:\n\t                return\n\t            dx = app_data[1]\n\t            dy = app_data[2]\n\t            self.cameraPoseNext = deepcopy(self.cameraPosePrev)\n\t            self.cameraPoseNext.trans(dx, dy)\n", "            if self.train_thread:\n\t                self.train_thread.set_camera_pose(self.cameraPoseNext.pose)\n\t                self.train_thread.test()\n\t                self.show_cam_centroid(self.cameraPoseNext.centroid[0],\n\t                                       self.cameraPoseNext.centroid[1],\n\t                                       self.cameraPoseNext.centroid[2])\n\t        def callback_mouseDown(_, app_data):\n\t            if not dpg.is_item_hovered(\"_primary_window\"):\n\t                return\n\t            if not self.need_test:\n", "                return\n\t            self.mouse_pressed = True\n\t            if app_data[1] < 1e-5:\n\t                self.cameraPosePrev = self.cameraPose\n\t            if self.train_thread:\n\t                self.train_thread.setStep(1)\n\t        def callback_mouseRelease():\n\t            if not self.need_test:\n\t                return\n\t            self.mouse_pressed = False\n", "            self.cameraPose = self.cameraPoseNext\n\t            if self.train_thread:\n\t                self.train_thread.setStep(self.args.train.iters)\n\t        def callback_mouseWheel(_, app_data):\n\t            if not dpg.is_item_hovered(\"_primary_window\"):\n\t                return\n\t            if not self.need_test:\n\t                return\n\t            if self.train_thread:\n\t                self.cameraPose.change_radius(app_data)\n", "                self.train_thread.set_camera_pose(self.cameraPose.pose)\n\t                self.train_thread.test()\n\t                self.show_cam_radius(self.cameraPose.radius)\n\t        def callback_train():\n\t            if self.need_train:\n\t                self.need_train = False\n\t                self.istesting = True\n\t                if self.train_thread:\n\t                    self.train_thread.istraining = False\n\t                _label = \"continue\" if (self.train_thread != None) else \"start\"\n", "                dpg.configure_item(\"_button_train\", label=_label)\n\t            else:\n\t                dpg.configure_item(\"_button_train\", label=\"pause\")\n\t                self.need_train = True\n\t                if self.train_thread:\n\t                    self.train_thread.istraining = True\n\t                else:\n\t                    self.train_thread = TrainThread(\n\t                        KEY=self.KEY,\n\t                        args=self.args,\n", "                        logger=self.logger,\n\t                        camera_pose=self.cameraPose.pose,\n\t                        step=self.args.train.iters,\n\t                        back_color=self.back_color,\n\t                        ckpt=self.ckpt)\n\t                    self.train_thread.setDaemon(True)\n\t                    self.train_thread.start()\n\t        def callback_checkpoint(sender):\n\t            if sender == \"_button_check_save\":\n\t                if self.train_thread and self.train_thread.trainer:\n", "                    self.logger.info(\"saving training state ... \")\n\t                    ckpt_name = checkpoints.save_checkpoint(\n\t                        self.args.exp_dir,\n\t                        self.train_thread.get_state(),\n\t                        step=self.train_thread.get_currentStep(),\n\t                        overwrite=True,\n\t                        keep=self.args.train.keep,\n\t                    )\n\t                    dpg.set_value(\n\t                        \"_log_ckpt\",\n", "                        \"Checkpoint saved path: {}\".format(ckpt_name))\n\t                    self.logger.info(\n\t                        \"training state saved to: {}\".format(ckpt_name))\n\t                else:\n\t                    dpg.set_value(\n\t                        \"_log_ckpt\",\n\t                        \"Checkpoint save path: failed ,cause no training\")\n\t                    self.logger.info(\n\t                        \"saving training state failed ,cause no training\")\n\t        def callback_change_scale(_, new_scale):\n", "            self.scale = new_scale\n\t            dpg.set_value(\"_cam_WH\", self._effective_resolution_display)\n\t            if self.train_thread:\n\t                self.train_thread.set_scale(self.scale)\n\t                if self.train_thread.havestart:\n\t                    self.train_thread.test()\n\t        def callback_reset():\n\t            self.need_train = False\n\t            if self.train_thread:\n\t                self.train_thread.stop()\n", "                dpg.configure_item(\"_button_train\", label=\"start\")\n\t                self.train_thread = None\n\t            self.framebuff = np.tile(\n\t                np.asarray(self.back_color, dtype=np.float32),\n\t                (self.texture_H, self.texture_W, 3))\n\t            self.clear_plot()\n\t            self.ckpt = CKPT()\n\t            dpg.set_value(\"_log_ckpt\", \"\")\n\t        def callback_Render():\n\t            if self.need_test:\n", "                dpg.configure_item(\"_button_Render\",\n\t                                   label=\"continue rendering\")\n\t            else:\n\t                dpg.configure_item(\"_button_Render\", label=\"pause rendering\")\n\t            self.need_test = not self.need_test\n\t        def callback_mode(_, app_data):\n\t            if app_data == \"render\":\n\t                self.mode = Mode.Render\n\t            elif app_data == \"depth\":\n\t                self.mode = Mode.depth\n", "            elif app_data == \"cost\":\n\t                self.mode = Mode.Cost\n\t            else:\n\t                raise NotImplementedError(\"visualization mode '{}' is not implemented\"\n\t                                          .format(self.mode))\n\t            if self.train_thread:\n\t                self.train_thread.test()\n\t        def callback_loadCheckpoint(_, app_data):\n\t            file_name = app_data['file_name']\n\t            file_path_name = app_data['file_path_name'][:-2]\n", "            dpg.set_value('_log_ckpt',\n\t                          self.ckpt.parse_ckpt(file_name, file_path_name))\n\t        self.View_W, self.View_H = self.W + self.args.viewport.control_window_width, self.H\n\t        dpg.create_viewport(title='NeRF',\n\t                            width=self.View_W,\n\t                            height=self.View_H,\n\t                            min_width=250 +\n\t                            self.args.viewport.control_window_width,\n\t                            min_height=250,\n\t                            x_pos=0,\n", "                            y_pos=0)\n\t        with dpg.window(tag=\"_main_window\", no_scrollbar=True):\n\t            dpg.set_primary_window(\"_main_window\", True)\n\t            with dpg.file_dialog(directory_selector=False,\n\t                                 show=False,\n\t                                 callback=callback_loadCheckpoint,\n\t                                 tag=\"checkpoint_file_dialog\",\n\t                                 width=700,\n\t                                 height=400):\n\t                dpg.add_file_extension(\".*\")\n", "                dpg.add_file_extension(\"\",\n\t                                       color=(150, 255, 150, 255),\n\t                                       custom_text=\"[Checkpoint]\")\n\t            with dpg.group(horizontal=True):\n\t                #texture\n\t                with dpg.group(tag=\"_render_texture\"):\n\t                    with dpg.texture_registry(show=False):\n\t                        dpg.add_raw_texture(width=self.W,\n\t                                            height=self.H,\n\t                                            default_value=self.framebuff,\n", "                                            format=dpg.mvFormat_Float_rgb,\n\t                                            tag=\"_texture\")\n\t                    with dpg.child_window(tag=\"_primary_window\",\n\t                                          width=self.W,\n\t                                          no_scrollbar=True):\n\t                        dpg.add_image(\"_texture\",\n\t                                      tag=\"_img\",\n\t                                      parent=\"_primary_window\",\n\t                                      width=self.W - 15,\n\t                                      height=self.H - 32)\n", "                #control panel\n\t                with dpg.child_window(tag=\"_control_window\",\n\t                                      no_scrollbar=True):\n\t                    with dpg.theme() as theme_head:\n\t                        with dpg.theme_component(dpg.mvAll):\n\t                            dpg.add_theme_color(dpg.mvThemeCol_Header,\n\t                                                (0, 62, 89))\n\t                    #control\n\t                    with dpg.collapsing_header(tag=\"_control_panel\",\n\t                                               label=\"Control Panel\",\n", "                                               default_open=True):\n\t                        dpg.bind_item_theme(\"_control_panel\", theme_head)\n\t                        #mode\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Visualization mode:   \")\n\t                            items = [\"render\", \"depth\", \"cost\"]\n\t                            dpg.add_combo(items=items,\n\t                                          callback=callback_mode,\n\t                                          width=max(map(len, items)) * 10,\n\t                                          default_value=\"render\")\n", "                        # train / stop/reset\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Train: \")\n\t                            dpg.add_button(label=\"start\",\n\t                                           tag=\"_button_train\",\n\t                                           callback=callback_train)\n\t                            dpg.add_button(label=\"reset\",\n\t                                           tag=\"_button_reset\",\n\t                                           callback=callback_reset)\n\t                        #need render\n", "                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Render: \")\n\t                            dpg.add_button(label=\"pause rendering\",\n\t                                           tag=\"_button_Render\",\n\t                                           callback=callback_Render)\n\t                        # save ckpt\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Checkpoint: \")\n\t                            dpg.add_button(label=\"save\",\n\t                                           tag=\"_button_check_save\",\n", "                                           callback=callback_checkpoint)\n\t                            dpg.add_button(label=\"load\",\n\t                                           tag=\"_button_check_load\",\n\t                                           callback=lambda: dpg.show_item(\n\t                                               'checkpoint_file_dialog'))\n\t                        dpg.add_text(\n\t                            \"\",\n\t                            tag=\"_log_ckpt\",\n\t                            wrap=self.args.viewport.control_window_width - 40)\n\t                        #resolution\n", "                        dpg.add_text(\"resolution scale:\")\n\t                        self.scale_slider = dpg.add_slider_float(\n\t                            tag=\"_resolutionScale\",\n\t                            label=\"\",\n\t                            default_value=self.args.viewport.resolution_scale,\n\t                            clamped=True,\n\t                            min_value=0.1,\n\t                            max_value=1.0,\n\t                            width=self.args.viewport.control_window_width - 40,\n\t                            format=\"%.1f\",\n", "                            callback=callback_change_scale,\n\t                        )\n\t                        dpg.add_text(\"Background color: \")\n\t                        dpg.add_color_edit(\n\t                            tag=\"_BackColor\",\n\t                            default_value=tuple(\n\t                                map(lambda val: int(val * 255 + .5),\n\t                                    self.args.render_eval.bg)),\n\t                            no_alpha=True,\n\t                            width=self.args.viewport.control_window_width - 40,\n", "                            callback=callback_backgroundColor)\n\t                        with dpg.value_registry():\n\t                            dpg.add_float_value(default_value=0.0,\n\t                                                tag=\"float_value\")\n\t                        #camera\n\t                        dpg.add_text(\"camera set:\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"near plane\")\n\t                            dpg.add_input_text(tag=\"_camera_near\",\n\t                                               width=40,\n", "                                               default_value=0.1,\n\t                                               decimal=True)\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"centroid:\")\n\t                            dpg.add_text(\"x\")\n\t                            dpg.add_input_text(\n\t                                tag=\"_centroid_x\",\n\t                                width=40,\n\t                                default_value=self.cameraPose.centroid[0],\n\t                                decimal=True)\n", "                            dpg.add_text(\"y\")\n\t                            dpg.add_input_text(\n\t                                tag=\"_centroid_y\",\n\t                                width=40,\n\t                                default_value=self.cameraPose.centroid[1],\n\t                                decimal=True)\n\t                            dpg.add_text(\"z\")\n\t                            dpg.add_input_text(\n\t                                tag=\"_centroid_z\",\n\t                                width=40,\n", "                                default_value=self.cameraPose.centroid[2],\n\t                                decimal=True)\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"theta\")\n\t                            dpg.add_input_text(\n\t                                tag=\"_theta\",\n\t                                width=40,\n\t                                default_value=self.cameraPose.theta,\n\t                                decimal=True)\n\t                            dpg.add_text(\"phi\")\n", "                            dpg.add_input_text(\n\t                                tag=\"_phi\",\n\t                                width=40,\n\t                                default_value=self.cameraPose.phi,\n\t                                decimal=True)\n\t                            dpg.add_text(\"radius\")\n\t                            dpg.add_input_text(\n\t                                tag=\"_radius\",\n\t                                width=40,\n\t                                default_value=self.cameraPose.radius,\n", "                                decimal=True)\n\t                    with dpg.collapsing_header(tag=\"_para_panel\",\n\t                                               label=\"Parameter Monitor\",\n\t                                               default_open=True):\n\t                        dpg.bind_item_theme(\"_para_panel\", theme_head)\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Resolution(W*H): \")\n\t                            dpg.add_text(self._effective_resolution_display,\n\t                                         tag=\"_cam_WH\")\n\t                        with dpg.group(horizontal=True):\n", "                            dpg.add_text(\"Current training step: \")\n\t                            dpg.add_text(\"no data\", tag=\"_cur_train_step\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Train time: \")\n\t                            dpg.add_text(\"no data\", tag=\"_log_train_time\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Infer time: \")\n\t                            dpg.add_text(\"no data\", tag=\"_log_infer_time\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"FPS: \")\n", "                            dpg.add_text(\"no data\", tag=\"_fps\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Mean samples/ray: \")\n\t                            dpg.add_text(\"no data\", tag=\"_samples\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Mean effective samples/ray: \")\n\t                            dpg.add_text(\"no data\", tag=\"_effective_samples\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Batch size: \")\n\t                            dpg.add_text(\"no data\",\n", "                                         tag=\"_not_compacted_batch_size\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Batch size(compacted): \")\n\t                            dpg.add_text(\"no data\",\n\t                                         tag=\"_compacted_batch_size\")\n\t                        with dpg.group(horizontal=True):\n\t                            dpg.add_text(\"Number of rays: \")\n\t                            dpg.add_text(\"no data\", tag=\"_rays_num\")\n\t                        # create plot\n\t                        with dpg.plot(\n", "                                label=\"pixel quality\",\n\t                                height=self.args.viewport.control_window_width\n\t                                - 40,\n\t                                width=self.args.viewport.control_window_width -\n\t                                40):\n\t                            # optionally create legend\n\t                            dpg.add_plot_legend()\n\t                            # REQUIRED: create x and y axes\n\t                            dpg.add_plot_axis(dpg.mvXAxis,\n\t                                              label=\"step\",\n", "                                              tag=\"x_axis\")\n\t                            dpg.add_plot_axis(dpg.mvYAxis,\n\t                                              label=\"PSNR (estimated)\",\n\t                                              tag=\"y_axis\")\n\t                            # series belong to a y axis\n\t                            dpg.add_line_series(self.data_step,\n\t                                                self.data_pixel_quality,\n\t                                                label=\"~PSNR\",\n\t                                                parent=\"y_axis\",\n\t                                                tag=\"_plot\")\n", "                    with dpg.collapsing_header(tag=\"_tip_panel\",\n\t                                               label=\"Tips\",\n\t                                               default_open=True):\n\t                        dpg.bind_item_theme(\"_tip_panel\", theme_head)\n\t                        tip1 = \"* Drag the left mouse button to rotate the camera\\n\"\n\t                        tip2 = \"* The mouse wheel zooms the distance between the camera and the object\\n\"\n\t                        tip3 = \"* Drag the window to resize\\n\"\n\t                        tip4 = \"* Drag the middle mouse button to translate the camera\\n\"\n\t                        dpg.add_text(\n\t                            tip1,\n", "                            wrap=self.args.viewport.control_window_width - 40)\n\t                        dpg.add_text(\n\t                            tip2,\n\t                            wrap=self.args.viewport.control_window_width - 40)\n\t                        dpg.add_text(\n\t                            tip3,\n\t                            wrap=self.args.viewport.control_window_width - 40)\n\t                        dpg.add_text(\n\t                            tip4,\n\t                            wrap=self.args.viewport.control_window_width - 40)\n", "        def callback_key(_, appdata):\n\t            if appdata == dpg.mvKey_Q:\n\t                self.exit_flag = True\n\t        #IO\n\t        with dpg.handler_registry():\n\t            dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Left,\n\t                                       callback=callback_mouseDrag)\n\t            dpg.add_mouse_release_handler(button=dpg.mvMouseButton_Left,\n\t                                          callback=callback_mouseRelease)\n\t            dpg.add_mouse_down_handler(button=dpg.mvMouseButton_Left,\n", "                                       callback=callback_mouseDown)\n\t            dpg.add_mouse_wheel_handler(callback=callback_mouseWheel)\n\t            #mouse middle\n\t            dpg.add_mouse_drag_handler(button=dpg.mvMouseButton_Middle,\n\t                                       callback=callback_midmouseDrag)\n\t            dpg.add_mouse_down_handler(button=dpg.mvMouseButton_Middle,\n\t                                       callback=callback_mouseDown)\n\t            dpg.add_mouse_release_handler(button=dpg.mvMouseButton_Middle,\n\t                                          callback=callback_mouseRelease)\n\t            dpg.add_key_release_handler(callback=callback_key)\n", "        dpg.setup_dearpygui()\n\t        dpg.show_viewport()\n\t    def update_frame(self):\n\t        self.framebuff = self.train_thread.framebuff\n\t        dpg.set_value(\"_texture\", self.framebuff)\n\t    def adapt_size(self):\n\t        if self.View_H != dpg.get_viewport_height(\n\t        ) or self.View_W != dpg.get_viewport_width():\n\t            self.View_H = dpg.get_viewport_height()\n\t            self.View_W = dpg.get_viewport_width()\n", "            self.H, self.W = self.View_H, self.View_W - self.args.viewport.control_window_width\n\t            dpg.set_item_width(\"_primary_window\", self.W)\n\t            dpg.delete_item(\"_img\")\n\t            dpg.add_image(\"_texture\",\n\t                          tag=\"_img\",\n\t                          parent=\"_primary_window\",\n\t                          width=self.W - 15,\n\t                          height=self.H - 32)\n\t            dpg.configure_item(\"_control_panel\",\n\t                               label=\"Control Panel\",\n", "                               default_open=True)\n\t            dpg.configure_item(\"_para_panel\",\n\t                               label=\"Parameter Monitor\",\n\t                               default_open=True)\n\t            dpg.configure_item(\"_tip_panel\", label=\"Tips\", default_open=True)\n\t            dpg.set_value(\"_cam_WH\", self._effective_resolution_display)\n\t            if self.train_thread:\n\t                self.train_thread.test()\n\t    def setFrameColor(self):\n\t        if self.train_thread:\n", "            self.train_thread.setBackColor(self.back_color)\n\t        if self.train_thread and self.train_thread.havestart:\n\t            self.train_thread.test()\n\t        else:\n\t            self.framebuff = np.tile(\n\t                np.asarray(self.back_color, dtype=np.float32),\n\t                (self.texture_H, self.texture_W, 1))\n\t        dpg.set_value(\"_texture\", self.framebuff)\n\t    def clear_plot(self):\n\t        self.data_step.clear()\n", "        self.data_pixel_quality.clear()\n\t        self.update_plot()\n\t    def update_plot(self):\n\t        if len(self.data_pixel_quality\n\t               ) > self.args.viewport.max_show_loss_step:\n\t            self.data_pixel_quality = self.data_pixel_quality[\n\t                -self.args.viewport.max_show_loss_step - 1:]\n\t            self.data_step = self.data_step[\n\t                -self.args.viewport.max_show_loss_step - 1:]\n\t        dpg.set_value('_plot', [self.data_step, self.data_pixel_quality])\n", "        dpg.fit_axis_data(\"y_axis\")\n\t        dpg.fit_axis_data(\"x_axis\")\n\t    def set_cam_angle(self):\n\t        try:\n\t            theta = float(dpg.get_value(\"_theta\"))\n\t            phi = float(dpg.get_value(\"_phi\"))\n\t            radius = float(dpg.get_value(\"_radius\"))\n\t            if theta != self.cameraPose.theta or phi != self.cameraPose.phi or radius != self.cameraPose.radius:\n\t                self.cameraPose.theta = theta\n\t                self.cameraPose.phi = phi\n", "                self.cameraPose.radius = radius\n\t                self.train_thread.set_camera_pose(self.cameraPose.pose)\n\t                self.train_thread.test()\n\t        except BaseException as e:\n\t            self.logger.error(e)\n\t    def show_cam_angle(self, _theta, _phi):\n\t        _theta = float('{:.3f}'.format(_theta))\n\t        _phi = float('{:.3f}'.format(_phi))\n\t        try:\n\t            theta = float(dpg.get_value(\"_theta\"))\n", "            phi = float(dpg.get_value(\"_phi\"))\n\t            if theta != _theta or phi != _phi:\n\t                dpg.set_value(\"_theta\", _theta)\n\t                dpg.set_value(\"_phi\", _phi)\n\t        except BaseException as e:\n\t            self.logger.error(e)\n\t    def show_cam_radius(self, _radius):\n\t        _radius = float('{:.3f}'.format(_radius))\n\t        try:\n\t            radius = float(dpg.get_value(\"_radius\"))\n", "            if radius != _radius:\n\t                dpg.set_value(\"_radius\", _radius)\n\t        except BaseException as e:\n\t            self.logger.error(e)\n\t    def set_cam_centroid(self):\n\t        try:\n\t            x = float(dpg.get_value(\"_centroid_x\"))\n\t            y = float(dpg.get_value(\"_centroid_y\"))\n\t            z = float(dpg.get_value(\"_centroid_z\"))\n\t            if x != self.cameraPose.centroid[\n", "                    0] or y != self.cameraPose.centroid[\n\t                        1] or z != self.cameraPose.centroid[2]:\n\t                self.cameraPose.centroid[0] = x\n\t                self.cameraPose.centroid[1] = y\n\t                self.cameraPose.centroid[2] = z\n\t                self.train_thread.set_camera_pose(self.cameraPose.pose)\n\t                self.train_thread.test()\n\t        except BaseException as e:\n\t            self.logger.error(e)\n\t    def set_cam_near(self):\n", "        try:\n\t            cam_near = float(dpg.get_value(\"_camera_near\"))\n\t            camera = self.train_thread.getPinholeCam()\n\t            if camera and camera.near != cam_near:\n\t                self.train_thread.setCamNear(cam_near)\n\t                self.train_thread.test()\n\t        except BaseException as e:\n\t            self.logger.exception(e)\n\t    def show_cam_centroid(self, _x, _y, _z):\n\t        _x = float('{:.3f}'.format(_x))\n", "        _y = float('{:.3f}'.format(_y))\n\t        _z = float('{:.3f}'.format(_z))\n\t        try:\n\t            x = float(dpg.get_value(\"_centroid_x\"))\n\t            y = float(dpg.get_value(\"_centroid_y\"))\n\t            z = float(dpg.get_value(\"_centroid_z\"))\n\t            if x != _x or y != _y or z != _z:\n\t                dpg.set_value(\"_centroid_x\", _x)\n\t                dpg.set_value(\"_centroid_y\", _y)\n\t                dpg.set_value(\"_centroid_z\", _z)\n", "        except BaseException as e:\n\t            self.logger.error(e)\n\t    def update_panel(self):\n\t        dpg.set_value(\n\t            \"_cur_train_step\",\n\t            \"{} (+{}/{})\".format(self.train_thread.get_currentStep(),\n\t                                 self.train_thread.get_logStep(),\n\t                                 self.train_thread.step))\n\t        dpg.set_value(\"_log_train_time\",\n\t                      \"{}\".format(self.train_thread.get_TrainInferTime()))\n", "        dpg.set_value(\"_log_infer_time\",\n\t                      \"{}\".format(self.train_thread.get_RenderInferTime()))\n\t        dpg.set_value(\"_fps\", \"{}\".format(self.train_thread.get_Fps()))\n\t        dpg.set_value(\"_samples\",\n\t                      \"{}\".format(self.train_thread.get_samples_nums()))\n\t        dpg.set_value(\n\t            \"_effective_samples\",\n\t            \"{}\".format(self.train_thread.get_effective_samples_nums()))\n\t        dpg.set_value(\"_compacted_batch_size\",\n\t                      \"{}\".format(self.train_thread.get_compactedBatch()))\n", "        dpg.set_value(\"_not_compacted_batch_size\",\n\t                      \"{}\".format(self.train_thread.get_notCompactedBatch()))\n\t        dpg.set_value(\"_rays_num\",\n\t                      \"{}\".format(self.train_thread.get_raysNum()))\n\t        self.data_step, self.data_pixel_quality = self.train_thread.get_plotData(\n\t        )\n\t        self.update_plot()\n\t    def load_ckpt(self):\n\t        if self.train_thread and self.train_thread.havestart:\n\t            self.train_thread.trainer.load_checkpoint(self.ckpt.ckpt_file_path,\n", "                                                      self.ckpt.step)\n\t            self.clear_plot()\n\t            self.ckpt.need_load_ckpt = False\n\t    def render(self) -> int:\n\t        while dpg.is_dearpygui_running():\n\t            self.adapt_size()\n\t            if self.train_thread:\n\t                if self.ckpt.need_load_ckpt:\n\t                    self.load_ckpt()\n\t                if not self.mouse_pressed:\n", "                    self.set_cam_angle()\n\t                    self.set_cam_centroid()\n\t                    self.set_cam_near()\n\t                self.train_thread.setMode(self.mode)\n\t                self.train_thread.set_scale(self.scale)\n\t                self.train_thread.change_WH(self.W, self.H)\n\t                self.update_panel()\n\t                if self.need_test:\n\t                    self.train_thread.needtesting = True\n\t                    if self.train_thread.canUpdate():\n", "                        self.update_frame()\n\t                        self.train_thread.finishUpdate()\n\t                else:\n\t                    self.train_thread.needtesting = False\n\t            else:\n\t                dpg.set_value(\"_texture\", self.framebuff)\n\t            dpg.render_dearpygui_frame()\n\t            time.sleep(.01 if self.train_thread and self.train_thread.istraining else .1)\n\t            if self.exit_flag:\n\t                if self.train_thread:\n", "                    self.train_thread.stop()\n\t                while self.train_thread and self.train_thread.is_alive():\n\t                    pass\n\t                self.logger.debug(\"thread killed successfully\")\n\t                self.logger.info(\"exiting cleanly ...\")\n\t                break\n\t        dpg.destroy_context()\n\t        return 0\n\tdef gui_exit():\n\t    import sys\n", "    sys.exit()\n\tdef GuiWindow(KEY: jran.KeyArray, args: NeRFGUIArgs, logger: logging.Logger) -> int:\n\t    nerfGui = NeRFGUI(args=args, KEY=KEY, logger=logger)\n\t    return nerfGui.render()\n"]}
{"filename": "app/nerf/_utils.py", "chunked_list": ["from typing import Dict, Tuple\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\timport optax\n\tfrom models.renderers import render_rays_train\n\tfrom utils import common, data\n\tfrom utils.types import NeRFState, SceneData\n\t__all__ = [\n\t    \"make_optimizer\",\n", "    \"train_step\",\n\t]\n\tdef make_optimizer(lr: float) -> optax.GradientTransformation:\n\t    lr_sch = optax.exponential_decay(\n\t        init_value=lr,\n\t        transition_steps=10_000,\n\t        decay_rate=1/3,  # decay to `1/3 * init_lr` after `transition_steps` steps\n\t        staircase=True,  # use integer division to determine lr drop step\n\t        transition_begin=10_000,  # hold the initial lr value for the initial 10k steps (but first lr drop happens at 20k steps because `staircase` is specified)\n\t        end_value=lr / 100,  # stop decaying at `1/100 * init_lr`\n", "    )\n\t    optimizer_network = optax.adam(\n\t        learning_rate=lr_sch,\n\t        b1=0.9,\n\t        b2=0.99,\n\t        # paper:\n\t        #   the small value of ùúñ = 10^{‚àí15} can significantly accelerate the convergence of the\n\t        #   hash table entries when their gradients are sparse and weak.\n\t        eps=1e-15,\n\t        eps_root=1e-15,\n", "    )\n\t    optimizer_ae = optax.adam(\n\t        learning_rate=1e-4,\n\t        b1=.9,\n\t        b2=.99,\n\t        eps=1e-8,\n\t        eps_root=0,\n\t    )\n\t    return optax.chain(\n\t        optax.multi_transform(\n", "            transforms={\n\t                \"network\": optimizer_network,\n\t                \"ae\": optimizer_ae,\n\t            },\n\t            param_labels={\n\t                \"nerf\": \"network\",\n\t                \"bg\": \"network\",\n\t                \"appearance_embeddings\": \"ae\",\n\t            },\n\t        ),\n", "        optax.add_decayed_weights(\n\t            # In NeRF experiments, the network can converge to a reasonably low loss during the\n\t            # first ~50k training steps (with 1024 rays per batch and 1024 samples per ray), but the\n\t            # loss becomes NaN after about 50~150k training steps.\n\t            # paper:\n\t            #   To prevent divergence after long training periods, we apply a weak L2 regularization\n\t            #   (factor 10^{‚àí6}) to the neural network weights, ...\n\t            weight_decay=1e-6,\n\t            # paper:\n\t            #   ... to the neural network weights, but not to the hash table entries.\n", "            mask={\n\t                \"nerf\": {\n\t                    \"density_mlp\": True,\n\t                    \"rgb_mlp\": True,\n\t                    \"position_encoder\": False,\n\t                },\n\t                \"bg\": True,\n\t                \"appearance_embeddings\": False,\n\t            },\n\t        ),\n", "    )\n\t@common.jit_jaxfn_with(\n\t    static_argnames=[\"total_samples\"],\n\t    donate_argnums=(0,),  # NOTE: this only works for positional arguments, see <https://jax.readthedocs.io/en/latest/faq.html#buffer-donation>\n\t)\n\tdef train_step(\n\t    state: NeRFState,\n\t    /,\n\t    KEY: jran.KeyArray,\n\t    total_samples: int,\n", "    scene: SceneData,\n\t    perm: jax.Array,\n\t) -> Tuple[NeRFState, Dict[str, jax.Array | float]]:\n\t    # indices of views and pixels\n\t    view_idcs, pixel_idcs = scene.get_view_indices(perm), scene.get_pixel_indices(perm)\n\t    # TODO:\n\t    #   merge this and `models.renderers.make_rays_worldspace` as a single function\n\t    def make_rays_worldspace() -> Tuple[jax.Array, jax.Array]:\n\t        # [N], [N]\n\t        x, y = (\n", "            jnp.mod(pixel_idcs, scene.meta.camera.width),\n\t            jnp.floor_divide(pixel_idcs, scene.meta.camera.width),\n\t        )\n\t        # [N, 3]\n\t        d_cam = scene.meta.camera.make_ray_directions_from_pixel_coordinates(x, y, use_pixel_center=True)\n\t        # [N, 3]\n\t        o_world = scene.transforms[view_idcs, -3:]\n\t        # [N, 3, 3]\n\t        R_cws = scene.transforms[view_idcs, :9].reshape(-1, 3, 3)\n\t        # [N, 3]\n", "        # equavalent to performing `d_cam[i] @ R_cws[i].T` for each i in [0, N)\n\t        d_world = (d_cam[:, None, :] * R_cws).sum(-1)\n\t        return o_world, d_world\n\t    # CAVEAT: gradient is only calculate w.r.t. the first parameter of this function\n\t    # (`params_to_optimize here`), any parameters that need to be optimized should be taken from\n\t    # this parameter, instead from the outer-scope `state.params`.\n\t    def loss_fn(params_to_optimize, gt_rgba_f32, KEY):\n\t        o_world, d_world = make_rays_worldspace()\n\t        appearance_embeddings = (\n\t            params_to_optimize[\"appearance_embeddings\"][view_idcs]\n", "                if \"appearance_embeddings\" in params_to_optimize\n\t                else jnp.empty(0)\n\t        )\n\t        if state.use_background_model:\n\t            bg = state.bg_fn(\n\t                {\"params\": params_to_optimize[\"bg\"]},\n\t                o_world,\n\t                d_world,\n\t                appearance_embeddings,\n\t            )\n", "        elif state.render.random_bg:\n\t            KEY, key = jran.split(KEY, 2)\n\t            bg = jran.uniform(key, shape=(o_world.shape[0], 3), dtype=jnp.float32, minval=0, maxval=1)\n\t        else:\n\t            bg = jnp.asarray(state.render.bg)\n\t        KEY, key = jran.split(KEY, 2)\n\t        batch_metrics, pred_rgbds, tv = render_rays_train(\n\t            KEY=key,\n\t            o_world=o_world,\n\t            d_world=d_world,\n", "            appearance_embeddings=appearance_embeddings,\n\t            bg=bg,\n\t            total_samples=total_samples,\n\t            state=state.replace(params=params_to_optimize),\n\t        )\n\t        pred_rgbs, pred_depths = jnp.array_split(pred_rgbds, [3], axis=-1)\n\t        gt_rgbs = data.blend_rgba_image_array(imgarr=gt_rgba_f32, bg=bg)\n\t        batch_metrics[\"loss\"] = {\n\t            \"rgb\": jnp.where(\n\t                batch_metrics[\"ray_is_valid\"],\n", "                optax.huber_loss(pred_rgbs, gt_rgbs, delta=0.1).mean(axis=-1),\n\t                0.,\n\t            ).sum() / batch_metrics[\"n_valid_rays\"],\n\t            \"total_variation\": tv,\n\t        }\n\t        loss = jax.tree_util.tree_reduce(lambda x, y: x + y, batch_metrics[\"loss\"])\n\t        return loss, batch_metrics\n\t    loss_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n\t    KEY, key = jran.split(KEY, 2)\n\t    (_, batch_metrics), grads = loss_grad_fn(\n", "        state.params,\n\t        scene.rgbas_u8[perm].astype(jnp.float32) / 255,\n\t        key,\n\t    )\n\t    state = state.apply_gradients(grads=grads)\n\t    return state, batch_metrics\n\tdef format_metrics(metrics: Dict[str, jax.Array | float]) -> str:\n\t    loss = metrics[\"loss\"]\n\t    return \"batch_size={}/{} samp./ray={:.1f}/{:.1f} n_rays={} loss:{{rgb={:.2e}({:.2f}dB),tv={:.2e}}}\".format(\n\t        metrics[\"measured_batch_size\"],\n", "        metrics[\"measured_batch_size_before_compaction\"],\n\t        metrics[\"measured_batch_size\"] / metrics[\"n_valid_rays\"],\n\t        metrics[\"measured_batch_size_before_compaction\"] / metrics[\"n_valid_rays\"],\n\t        metrics[\"n_valid_rays\"],\n\t        loss[\"rgb\"],\n\t        data.linear_to_db(loss[\"rgb\"], maxval=1.),\n\t        loss[\"total_variation\"],\n\t    )\n"]}
{"filename": "app/nerf/test.py", "chunked_list": ["from concurrent.futures import ThreadPoolExecutor\n\tfrom typing import List\n\tfrom typing_extensions import assert_never\n\tfrom PIL import Image\n\tfrom flax.training import checkpoints\n\timport jax\n\timport jax.numpy as jnp\n\timport jax.random as jran\n\timport numpy as np\n\tfrom models.nerfs import make_nerf_ngp, make_skysphere_background_model_ngp\n", "from models.renderers import render_image_inference\n\tfrom utils import common, data\n\tfrom utils.args import NeRFTestingArgs\n\tfrom utils.types import NeRFState, RenderedImage, RigidTransformation\n\tdef test(KEY: jran.KeyArray, args: NeRFTestingArgs, logger: common.Logger) -> int:\n\t    args.logs_dir.mkdir(parents=True, exist_ok=True)\n\t    logger = common.setup_logging(\n\t        \"nerf.test\",\n\t        file=args.logs_dir.joinpath(\"test.log\"),\n\t        level=args.common.logging.upper(),\n", "        file_level=\"DEBUG\",\n\t    )\n\t    if not args.ckpt.exists():\n\t        logger.error(\"specified checkpoint '{}' does not exist\".format(args.ckpt))\n\t        return 1\n\t    scene_data = data.load_scene(\n\t        srcs=args.frames,\n\t        scene_options=args.scene,\n\t        sort_frames=args.sort_frames,\n\t    )\n", "    scene_meta = scene_data.meta\n\t    if args.report_metrics:\n\t        logger.warn(\"will not load gt images because either the intrinsics or the extrinsics of the camera have been changed\")\n\t        if args.trajectory == \"orbit\":\n\t            scene_meta = scene_meta.make_frames_with_orbiting_trajectory(args.orbit)\n\t            logger.info(\"generated {} camera transforms for testing\".format(len(scene_meta.frames)))\n\t    else:\n\t        logger.debug(\"loading testing frames from {}\".format(args.frames))\n\t        logger.info(\"loaded {} camera transforms for testing\".format(len(scene_meta.frames)))\n\t    if args.camera_override.enabled:\n", "        scene_meta = scene_meta.replace(camera=args.camera_override.update_camera(scene_meta.camera))\n\t    # load parameters\n\t    logger.debug(\"loading checkpoint from '{}'\".format(args.ckpt))\n\t    state: NeRFState = checkpoints.restore_checkpoint(\n\t        args.ckpt,\n\t        target=NeRFState.empty(\n\t            raymarch=args.raymarch,\n\t            render=args.render,\n\t            scene_options=args.scene,\n\t            scene_meta=scene_meta,\n", "            nerf_fn=make_nerf_ngp(bound=scene_meta.bound, inference=True).apply,\n\t            bg_fn=make_skysphere_background_model_ngp(bound=scene_meta.bound).apply if scene_meta.bg else None,\n\t        ),\n\t    )\n\t    # WARN:\n\t    #   flax.checkpoints.restore_checkpoint() returns a pytree with all arrays of numpy's array type,\n\t    #   which slows down inference.  use jax.device_put() to move them to jax's default device.\n\t    # REF: <https://github.com/google/flax/discussions/1199#discussioncomment-635132>\n\t    state = jax.device_put(state)\n\t    if state.step == 0:\n", "        logger.error(\"an empty checkpoint was loaded from '{}'\".format(args.ckpt))\n\t        return 2\n\t    logger.info(\"checkpoint loaded from '{}' (step={})\".format(args.ckpt, int(state.step)))\n\t    rendered_images: List[RenderedImage] = []\n\t    try:\n\t        n_frames = len(scene_meta.frames)\n\t        logger.info(\"starting testing (totally {} transform(s) to test)\".format(n_frames))\n\t        for test_i in common.tqdm(range(n_frames), desc=\"testing (resolultion: {}x{})\".format(scene_meta.camera.width, scene_meta.camera.height)):\n\t            logger.debug(\"testing on frame {}\".format(scene_meta.frames[test_i]))\n\t            transform = RigidTransformation(\n", "                rotation=scene_meta.frames[test_i].transform_matrix_jax_array[:3, :3],\n\t                translation=scene_meta.frames[test_i].transform_matrix_jax_array[:3, 3],\n\t            )\n\t            KEY, key = jran.split(KEY, 2)\n\t            bg, rgb, depth, _ = data.to_cpu(render_image_inference(\n\t                KEY=key,\n\t                transform_cw=transform,\n\t                state=state,\n\t            ))\n\t            rendered_images.append(RenderedImage(\n", "                bg=bg,\n\t                rgb=rgb,\n\t                depth=depth,  # call to data.mono_to_rgb is deferred below so as to minimize impact on rendering speed\n\t            ))\n\t    except KeyboardInterrupt:\n\t        logger.warn(\"keyboard interrupt, tested {} images\".format(len(rendered_images)))\n\t    if args.trajectory == \"loaded\":\n\t        if len(rendered_images) == 0:\n\t            logger.warn(\"tested 0 image, not calculating psnr\")\n\t        else:\n", "            gt_rgbs_f32 = map(\n\t                lambda test_view, rendered_image: data.blend_rgba_image_array(\n\t                    test_view.image_rgba_u8.astype(jnp.float32) / 255,\n\t                    rendered_image.bg,\n\t                ),\n\t                scene_data.all_views,\n\t                rendered_images,\n\t            )\n\t            logger.debug(\"calculating psnr\")\n\t            mean_psnr = sum(map(\n", "                data.psnr,\n\t                map(data.f32_to_u8, gt_rgbs_f32),\n\t                map(lambda ri: ri.rgb, rendered_images),\n\t            )) / len(rendered_images)\n\t            logger.info(\"tested {} images, mean psnr={}\".format(len(rendered_images), mean_psnr))\n\t    elif args.trajectory == \"orbit\":\n\t        logger.debug(\"using generated orbiting trajectory, not calculating psnr\")\n\t    else:\n\t        assert_never(\"\")\n\t    save_dest = args.logs_dir.joinpath(\"test\")\n", "    save_dest.mkdir(parents=True, exist_ok=True)\n\t    if \"video\" in args.save_as:\n\t        dest_rgb_video = save_dest.joinpath(\"rgb.mp4\")\n\t        dest_depth_video = save_dest.joinpath(\"depth.mp4\")\n\t        logger.debug(\"saving predicted color images as a video at '{}'\".format(dest_rgb_video))\n\t        data.write_video(\n\t            save_dest.joinpath(\"rgb.mp4\"),\n\t            map(lambda img: img.rgb, rendered_images),\n\t            fps=args.fps,\n\t            loop=args.loop,\n", "        )\n\t        logger.debug(\"saving predicted disparities as a video at '{}'\".format(dest_depth_video))\n\t        data.write_video(\n\t            save_dest.joinpath(\"depth.mp4\"),\n\t            map(lambda img: common.compose(data.mono_to_rgb, data.f32_to_u8)(img.depth), rendered_images),\n\t            fps=args.fps,\n\t            loop=args.loop,\n\t        )\n\t    if \"image\" in args.save_as:\n\t        dest_rgb = save_dest.joinpath(\"rgb\")\n", "        dest_depth = save_dest.joinpath(\"depth\")\n\t        dest_rgb.mkdir(parents=True, exist_ok=True)\n\t        dest_depth.mkdir(parents=True, exist_ok=True)\n\t        logger.debug(\"saving as images\")\n\t        def save_rgb_and_depth(save_i: int, img: RenderedImage):\n\t            common.compose(\n\t                np.asarray,\n\t                Image.fromarray\n\t            )(img.rgb).save(dest_rgb.joinpath(\"{:04d}.png\".format(save_i)))\n\t            common.compose(\n", "                data.mono_to_rgb,\n\t                data.f32_to_u8,\n\t                np.asarray,\n\t                Image.fromarray\n\t            )(img.depth).save(dest_depth.joinpath(\"{:04d}.png\".format(save_i)))\n\t        for _ in common.tqdm(\n\t            ThreadPoolExecutor().map(\n\t                save_rgb_and_depth,\n\t                range(len(rendered_images)),\n\t                rendered_images,\n", "            ),\n\t            total=len(rendered_images),\n\t            desc=\"| saving images\",\n\t        ):\n\t            pass\n\t    return 0\n"]}
