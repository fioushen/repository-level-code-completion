{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\twith open(\"README.md\", \"r\") as fh:\n\t    long_description = fh.read()\n\tVERSION = \"0.2.0b0\"\n\tREQUIRES_PYTHON = \">=3.7.0\"\n\tREQUIRED = [\n\t    \"black==22.6\",\n\t    \"pytest==7.1\",\n\t    \"google-api-python-client==2.78\",\n\t    \"google-cloud-core==2.3\",\n", "    \"google-cloud-storage==2.7\",\n\t    \"google-auth\",\n\t    \"boto==2.49\",\n\t    \"botocore==1.29\",\n\t    \"boto3==1.26\",\n\t    \"docker==6.1\",\n\t    \"redis==4.5\",\n\t    \"seldon-core==1.16\",\n\t    \"pydantic==1.10.8\",\n\t    \"kubernetes\",\n", "    \"tabulate\"\n\t]\n\tDEV_REQUIRED = [\n\t    \"black==22.6.0\",\n\t    \"pytest==7.1.2\",\n\t    \"google-api-python-client==2.78.0\",\n\t    \"google-cloud-core==2.3.2\",\n\t    \"google-cloud-storage==2.7.0\",\n\t    \"google-auth\",\n\t    \"zstd==1.5.2.6\",\n", "    \"boto==2.49.0\",\n\t    \"botocore==1.29.127\",\n\t    \"boto3==1.26\",\n\t    \"docker==6.1.2\",\n\t    \"redis==4.5.5\",\n\t    \"seldon-core==1.16\",\n\t    \"pydantic==1.10.8\",\n\t    \"kubernetes==26.1.0\"\n\t]\n\tsetup(\n", "    name=\"airdot\",\n\t    url=\"https://github.com/airdot-io/airdot-Deploy/\",\n\t    author=\"airdot-io\",\n\t    author_email=\"abhhinav035991@gmail.com\",\n\t    packages=find_packages(),\n\t    version=VERSION,\n\t    description=\"A code base for deploying python functions\",\n\t    long_description=long_description,\n\t    python_requires=REQUIRES_PYTHON,\n\t    install_requires=REQUIRED,\n", ")\n"]}
{"filename": "test_gsutil.py", "chunked_list": ["from google.cloud import storage\n\tfrom google.oauth2 import service_account\n\tfrom googleapiclient import discovery\n\tfrom google import auth\n\tfrom datetime import datetime, timedelta\n\timport json\n\timport pickle\n\tfile_ = open(\"/Users/abhinav.singh-mbp/Downloads/token.json\", mode=\"rb\").read()\n\tprivate_json = json.loads(file_)\n\tfrom pprint import pprint\n", "def get_gcs_bucket(config):\n\t    storage_client = storage.Client()\n\t    return storage_client.bucket(\"gs://model-ml-deployer\")\n\tdef generate_upload_signed_url_v4(bucket_name, blob_name):\n\t    \"\"\"Generates a v4 signed URL for uploading a blob using HTTP PUT.\n\t    Note that this method requires a service account key file. You can not use\n\t    this if you are using Application Default Credentials from Google Compute\n\t    Engine or from the Google Cloud SDK.\n\t    \"\"\"\n\t    # bucket_name = 'your-bucket-name'\n", "    # blob_name = 'your-object-name'\n\t    storage_client = storage.Client()\n\t    bucket = storage_client.bucket(bucket_name)\n\t    blob = bucket.blob(blob_name)\n\t    url = blob.generate_signed_url(\n\t        version=\"v4\",\n\t        # This URL is valid for 15 minutes\n\t        expiration=datetime.timedelta(minutes=5),\n\t        # Allow PUT requests using this URL.\n\t        method=\"PUT\",\n", "        content_type=\"application/octet-stream\",\n\t    )\n\t    return url\n\t# credentials, project = auth.default(\n\t# )\n\t# credentials.refresh(auth.transport.requests.Request())\n\t# credentials = service_account.Credentials.from_service_account_info(private_json)\n\t# #credentials.refresh(auth.transport.requests.Request())\n\t# expiration_timedelta = timedelta(minutes=3)\n\t# storage_client = storage.Client(credentials=credentials)\n", "# bucket = storage_client.bucket('model-ml-deployer')\n\t# blob = bucket.blob(\"test-folder2/test_model.pkl\")\n\t# test_dict = {'test_key':'test_value'}\n\t# file = pickle.dumps(test_dict)\n\t# blob.upload_from_string(file)\n\t# signed_url = blob.generate_signed_url(\n\t#     expiration=expiration_timedelta,\n\t#     service_account_email=credentials.service_account_email,\n\t#     access_token=credentials.token,\n\t# )\n", "# print('signed url', signed_url)\n\t# pickle.dump()\n\tstorage_client = storage.Client()\n\tbucket = storage_client.bucket(\"model-ml-deployer\")\n\tblob = bucket.blob(\"116997773269197393190/lm.pkl\")\n\tlm = pickle.loads(blob.download_as_string())\n"]}
{"filename": "airdot/version.py", "chunked_list": ["VERSION=\"0.5.0b0\""]}
{"filename": "airdot/deployer.py", "chunked_list": ["import json\n\timport requests\n\timport docker\n\timport shutil\n\timport logging\n\tfrom time import sleep\n\tfrom typing_extensions import Literal\n\tfrom typing import Optional\n\tfrom docker.errors import APIError\n\tfrom typing import cast, Union, Callable, Any, Dict, List, Optional\n", "from airdot.helpers.version_helpers import get_python_default_version\n\tfrom airdot.helpers.pkg_helpers import get_environment_pkgs, get_pip_list\n\tfrom airdot.helpers.runtime_helper import get_function_properties\n\tfrom airdot.helpers.template_helpers import make_soruce_file, make_soruce_file_seldon\n\tfrom airdot.helpers.general_helpers import get_name, get_difference\n\tfrom airdot.helpers.data_object_helpers import (\n\t    make_and_upload_data_files,\n\t    upload_runtime_object,\n\t)\n\tfrom airdot.collection.collections import authentication\n", "from tabulate import tabulate\n\tfrom airdot.helpers.authentication import (\n\t    user_login,\n\t    verify_user,\n\t    get_function_status,\n\t)\n\tfrom airdot import URL, VERIFY\n\tfrom airdot.helpers.docker_helper import docker_helper\n\tfrom airdot.helpers.redis_helper import redis_helper\n\tfrom airdot.helpers.network_helper import find_available_port\n", "from airdot.helpers.seldon_helper import seldon_helpers\n\tfrom airdot.helpers.content_helper import content_helper\n\tfrom airdot.helpers.s2i_helper import s2i_python_helper\n\tfrom airdot.contants.runtime_images import seldon_images\n\tfrom airdot.helpers.kubernetes_helper import k8s\n\tauth_ = authentication()\n\t# python custom imports\n\tlog_fmt = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n\tlogging.basicConfig(level=logging.INFO, format=log_fmt)\n\tclass Deployer:\n", "    def __init__(\n\t        self,\n\t        minio_endpoint: str = \"http://127.0.0.1:9000\",\n\t        redis_endpoint: str = \"localhost:6379\",\n\t        deployment_configuration: dict = {\n\t            \"deployment_type\": \"test\",\n\t            \"bucket_type\": \"minio\",\n\t        },\n\t    ) -> None:\n\t        \"\"\"\n", "        Deployer class provides interface for user to create and deploy their ML Models\n\t        Args:\n\t            minio_endpoint (str, optional): Local minio endpoint. Defaults to \"http://127.0.0.1:9000\".\n\t            redis_endpoint (str, optional): Local redis endpoint. Defaults to \"localhost:6379\".\n\t            deployment_configuration (dict, optional): _description_. Defaults to { \"deployment_type\": \"test\", \"bucket_type\": \"minio\", }.\n\t        \"\"\"\n\t        self.minio_endpoint = minio_endpoint\n\t        self.redis_endpoint = redis_endpoint\n\t        self.deployment_type = deployment_configuration[\"deployment_type\"]\n\t        self.deployment_configuration = deployment_configuration\n", "        self.docker_client = docker_helper()\n\t        self.redis_helper_obj = redis_helper(\n\t            host=self.redis_endpoint.split(\":\")[0],\n\t            port=self.redis_endpoint.split(\":\")[1],\n\t        )\n\t        if self.deployment_type == \"test\":\n\t            self.minio_network = \"minio-network\"\n\t    def _perform_user_login(self):\n\t        login_uri = user_login(auth_=auth_)\n\t        if login_uri is None:\n", "            print(\"login failed please try again\")\n\t            return False\n\t        try_auth = 50\n\t        while not (self._is_user_authenticated()) and try_auth > 0:\n\t            sleep(1)\n\t            try_auth -= 1\n\t            continue\n\t        if self._is_user_authenticated(True):\n\t            self.user_login = True\n\t            return self.user_login\n", "        self.user_login = False\n\t        return self.user_login\n\t    def _is_user_authenticated(self, print_status=False):\n\t        if auth_.refresh_token is not None and verify_user(auth_=auth_):\n\t            if print_status:\n\t                print(\"User authenticated.\")\n\t            return True\n\t        return False\n\t    def build_deployment(\n\t        self,\n", "        func: Callable,\n\t        name: Optional[str] = None,\n\t        python_version: Optional[str] = \"3.8\",\n\t        python_packages: Optional[List[str]] = None,\n\t        system_packages: Optional[List[str]] = None,\n\t    ):\n\t        \"\"\"\n\t        Build the source code from user specified function, this is done by tracking call trace\n\t        of the function.\n\t        Args:\n", "            func (Callable): primary function which predicts, this can be model object itself.\n\t            name (Optional[str], optional): service name. Defaults to None.\n\t            python_version (Optional[str], optional): python version to be used for runtime. Defaults to \"3.8\".\n\t            python_packages (Optional[List[str]], optional): List of python pkgs \n\t                if not provided uses func to get user pakgs. Defaults to None.\n\t            system_packages (Optional[List[str]], optional): Not yet implemented. Defaults to None.\n\t        Raises:\n\t            Exception: if func is not callable\n\t        Returns:\n\t            dict: {\n", "                \"source_file\": source code,\n\t                \"value_files\": {},\n\t                \"name\": name of the function,\n\t                \"data_files\": object datafiles like datframes or model object files,\n\t                \"module\": service name,\n\t                \"arg_names\": args for primary function to call service,\n\t                \"arg_types\": types but only available if types defined in function definition,\n\t                \"requirements_txt\": list of python packages to be used in order to run the callable,\n\t                \"python_version\": python version,\n\t                \"system_packages\": Not Implemented\n", "                \"dockerRun\": Not implemented\n\t                \"func_props\": properties of function,\n\t        }\n\t        \"\"\"\n\t        data_files = None\n\t        dir_id = None\n\t        bucket_type = self.deployment_configuration[\"bucket_type\"]\n\t        if callable(func):\n\t            python_version = get_python_default_version(python_version)\n\t            env_python_packages = get_environment_pkgs(\n", "                python_packages, func.__globals__\n\t            )\n\t            if python_packages is not None:\n\t                env_python_packages = env_python_packages + python_packages\n\t            func_props = get_function_properties(func, env_python_packages)\n\t            name = get_name(name)\n\t            data_files = make_and_upload_data_files(\n\t                bucket_id=func_props.name.replace(\"_\", \"-\"),\n\t                open_id=dir_id,\n\t                py_state=func_props,\n", "                endpoint=self.minio_endpoint,\n\t                bucket_type=bucket_type,\n\t            )  # uploading of data objects.\n\t        elif (\n\t            hasattr(func, \"__module__\")\n\t            and \"sklearn\" in func.__module__\n\t            and hasattr(func, \"predict\")\n\t        ):\n\t            pass\n\t        else:\n", "            raise Exception(\"Passed object is not callable\")\n\t        if self.deployment_type == \"test\":\n\t            source_file = make_soruce_file(\n\t                dir=dir_id,\n\t                pyProps=func_props,\n\t                source_file_name=name,\n\t            )\n\t        if self.deployment_type == \"seldon\":\n\t            source_file = make_soruce_file_seldon(\n\t                dir=dir_id,\n", "                pyProps=func_props,\n\t                source_file_name=name,\n\t                bucket_type=bucket_type,\n\t            )\n\t        elif self.deployment_type == \"kserve\":\n\t            pass\n\t        return {\n\t            \"source_file\": source_file.as_dict(),\n\t            \"value_files\": {},\n\t            \"name\": func_props.name,\n", "            \"data_files\": data_files,\n\t            \"module\": name,\n\t            \"arg_names\": func_props.arg_names,\n\t            \"arg_types\": func_props.arg_types,\n\t            \"requirements_txt\": \"\\n\".join(env_python_packages),\n\t            \"python_version\": python_version,\n\t            \"system_packages\": None,  # need to see this\n\t            \"dockerRun\": None,  # need to build this\n\t            \"func_props\": func_props,\n\t        }\n", "    def _list_to_json(self, cld_function_string):\n\t        return dict(\n\t            line.strip().split(\":\", 1)\n\t            for line in cld_function_string.split(\"\\n\")\n\t            if \":\" in line\n\t        )\n\t    def _build_url(self, json_string, deploy_dict):\n\t        if json_string is None:\n\t            logging.error(\"failed to deploy. please try again\")\n\t        json_value = self._list_to_json(json_string)\n", "        url = json_value[\"url\"]\n\t        print(\"Generating Curl request\")\n\t        data_dict = {}\n\t        if len(deploy_dict[\"arg_names\"]) > 0:\n\t            for arg_name in deploy_dict[\"arg_names\"]:\n\t                data_dict[arg_name] = \"<value-for-argument>\"\n\t        curl = f\"curl -XPOST {url} -d '{json.dumps(data_dict)}' -H 'Content-Type: application/json' \"\n\t        return curl\n\t    def _run__test_function(self, port, image):\n\t        try:\n", "            self.container = self.docker_client.run_container(\n\t                image,\n\t                detach=True,\n\t                ports={f\"{8080}/tcp\": port},\n\t                network=self.minio_network,\n\t            )\n\t            return True\n\t        except Exception as e:\n\t            logging.error(f\"{e}\")\n\t            return False\n", "    def restart(self, function_id):\n\t        \"\"\"\n\t        To restart service. Currently only implemented for local deployment\n\t        Args:\n\t            function_id (str): name of service\n\t        \"\"\"\n\t        container_id = self.docker_client.get_container_id(function_id)\n\t        self.docker_client.restart_container(container_id=container_id)\n\t    def stop(self, image_name):\n\t        \"\"\"\n", "        To stop the service. Currently only implemented for local deployment\n\t        Args:\n\t            image_name (str):  name of service\n\t        \"\"\"\n\t        try:\n\t            container_id = self.docker_client.get_container_id(image_name=image_name)\n\t            container_status = self.docker_client.kill_container(\n\t                container_id=container_id\n\t            )\n\t            if container_status:\n", "                self.docker_client.delete_container(container_id=container_id)\n\t                print(\"deployment killed successfully\")\n\t        except APIError as e:\n\t            print(f\"{e}\")\n\t    def update_redis(self, function_curl, object_refresh=False):\n\t        self.redis_helper_obj.set_user_function(\n\t            self.deploy_dict[\"name\"],\n\t            self.deploy_dict,\n\t            function_curl_req=function_curl,\n\t            object_refresh=object_refresh,\n", "        )\n\t    def run(\n\t        self,\n\t        func: Callable,\n\t        name: Optional[str] = None,\n\t        python_version: Optional[str] = None,\n\t        python_packages: Optional[List[str]] = None,\n\t        system_packages: Optional[List[str]] = None,\n\t    ):\n\t        \"\"\"_summary_\n", "        Args:\n\t            func (Callable): primary function which predicts, this can be model object itself.\n\t            name (Optional[str], optional): service name. Defaults to None.\n\t            python_version (Optional[str], optional): python version to be used for runtime. Defaults to \"3.8\".\n\t            python_packages (Optional[List[str]], optional): List of python pkgs \n\t                if not provided uses func to get user pakgs. Defaults to None.\n\t            system_packages (Optional[List[str]], optional): Not yet implemented. Defaults to None.\n\t        Raises:\n\t            TypeError: raise if empty seldon uri supplied with seldon deployment.\n\t        \"\"\"\n", "        print(\"deployment started\")\n\t        self.deploy_dict = self.build_deployment(\n\t            func=func,\n\t            name=name,\n\t            python_packages=python_packages,\n\t            python_version=python_version,\n\t            system_packages=system_packages,\n\t        )\n\t        # changes for seldon deployment.\n\t        if self.deployment_type == \"test\":\n", "            print(\n\t                \"switching to test deployment no deployment configuration is provided.\"\n\t            )\n\t            port = find_available_port(8000)\n\t            content_helper_obj = content_helper(\n\t                deploy_dict=self.deploy_dict,\n\t                deployment_type=self.deployment_type,\n\t                seldon_configuration=None,\n\t            )\n\t            deployment_path = content_helper_obj.write_contents()\n", "            image = self.docker_client.build_image(\n\t                path=deployment_path, name=self.deploy_dict[\"name\"]\n\t            )\n\t            print(f\"deploying on port: {port}\")\n\t            function_status = self._run__test_function(port=port, image=image)\n\t            if function_status:\n\t                url = f\"http://127.0.0.1:{port}\"\n\t                function_curl = self.build_function_url(url=url)\n\t                print(\"deployment ready, access using the curl command below\")\n\t                print(function_curl)\n", "                self.update_redis(function_curl)\n\t        elif self.deployment_type == \"seldon\":\n\t            if self.deployment_configuration['image_uri'] is None:\n\t                raise TypeError('cannot provide empty image_uri for seldon deployment')\n\t            # building seldon deployment dictionary\n\t            seldon_helpers_obj = seldon_helpers(\n\t                deployment_configuration=self.deployment_configuration\n\t            )\n\t            seldon_configuration = seldon_helpers_obj.create_seldon_configuration(\n\t                deploy_dict=self.deploy_dict, image_uri = self.deployment_configuration['image_uri']\n", "            )\n\t            # building contents\n\t            content_helper_obj = content_helper(\n\t                deploy_dict=self.deploy_dict,\n\t                deployment_type=self.deployment_type,\n\t                seldon_configuration=seldon_configuration,\n\t            )\n\t            deployment_path = content_helper_obj.write_contents()\n\t            #raise TypeError\n\t            # building s2i image\n", "            base_image = seldon_images[self.deploy_dict[\"python_version\"]]\n\t            builder_image = self.deployment_configuration['image_uri']\n\t            s2i_python_helper_obj = s2i_python_helper(\n\t                base_image=base_image, builder_image=builder_image\n\t            )\n\t            #s2i_python_helper_obj.build_and_push_image(source_path=deployment_path)\n\t            s2i_python_helper_obj.build_and_push_image(source_path=deployment_path)\n\t            # k8s application\n\t            namespace = seldon_configuration['metadata']['namespace']\n\t            k8s_obj = k8s()\n", "            if k8s_obj.create_namespace(namespace=namespace):\n\t                _ = k8s_obj.apply_kubernetes_resources(\n\t                    resource_paths=deployment_path + \"/seldon_model.json\"\n\t                )\n\t        else:\n\t            print(\"failed to run function. Please try again.\")\n\t    def update_objects(self, object, function_id):\n\t        \"\"\"_summary_\n\t        Args:\n\t            object (list, tuple): Either list of tuple or just a tuple. tuple will contain two values\n", "                object name and object\n\t            function_id (str): service name\n\t        Returns:\n\t            bool: True if objects successfully updated else False. Currently only Implemented for local deployment.\n\t        \"\"\"\n\t        data_files: Dict[str, str] = {}\n\t        if (\n\t            isinstance(object, list)\n\t            and len(object) > 0\n\t            and isinstance(object[0], tuple)\n", "        ):\n\t            for item in object:\n\t                nName = item[0]\n\t                nVal = item[1]\n\t                data_files[f\"{nName}.pkl\"] = upload_runtime_object(\n\t                    function_id.replace(\"_\", \"-\"),\n\t                    None,\n\t                    nVal,\n\t                    nName,\n\t                    endpoint=self.minio_endpoint,\n", "                )\n\t            json_dict = {\n\t                \"data_files\": data_files,\n\t                \"function_id\": function_id,\n\t                \"auth_session_token\": auth_.refresh_token,\n\t            }\n\t            status = self.restart(function_id)\n\t            if status is not None:\n\t                return status\n\t        elif isinstance(object, tuple):\n", "            nName = object[0]\n\t            nVal = object[1]\n\t            data_files[f\"{nName}.pkl\"] = upload_runtime_object(\n\t                function_id.replace(\"_\", \"-\"),\n\t                None,\n\t                nVal,\n\t                nName,\n\t                endpoint=self.minio_endpoint,\n\t            )\n\t            json_dict = {\n", "                \"data_files\": data_files,\n\t                \"name\": function_id,\n\t                \"auth_session_token\": auth_.refresh_token,\n\t            }\n\t            status = self.restart(function_id)\n\t            if status is not None:\n\t                return status\n\t        else:\n\t            print(\"Please pass object tuple or list of tuples\")\n\t    def _check_function_status(self, deploy_dict):\n", "        payload = {\n\t            \"auth_session_token\": deploy_dict[\"auth_session_token\"],\n\t            \"name\": deploy_dict[\"name\"],\n\t        }\n\t        def function_status(payload):\n\t            status = get_function_status(payload=payload)\n\t            if status is not None and json.loads(status)[\"status\"] == \"DONE\":\n\t                return True\n\t            else:\n\t                return False\n", "        try_check_function_status = 10\n\t        function_status_flag = function_status(payload=payload)\n\t        while not (function_status_flag) and try_check_function_status > 0:\n\t            function_status_flag = function_status(payload=payload)\n\t            try_check_function_status -= 1\n\t            sleep(50)\n\t            continue\n\t        function_status_flag = function_status(payload=payload)\n\t        if function_status_flag:\n\t            return json.loads(get_function_status(payload=payload))\n", "        else:\n\t            return None\n\t    def build_function_url(self, url):\n\t        if url is None:\n\t            print(\"failed to generate url\")\n\t            exit(1)\n\t        url = url\n\t        data_dict = {}\n\t        if len(self.deploy_dict[\"arg_names\"]) > 0:\n\t            for arg_name in self.deploy_dict[\"arg_names\"]:\n", "                data_dict[arg_name] = \"<value-for-argument>\"\n\t        curl = f\"curl -XPOST {url} -H 'Content-Type: application/json' -d '{json.dumps(data_dict)}'  \"\n\t        return curl\n\t    def generate_arg_list(self, function_request):\n\t        arg_list = []\n\t        if len(function_request[\"metadata\"][\"arg_names\"]) > 0:\n\t            for arg_name in function_request[\"metadata\"][\"arg_names\"]:\n\t                arg_list.append(f\"{arg_name}\")\n\t            arg_list = arg_list if len(arg_list) > 0 else [\"None args defined\"]\n\t            return arg_list\n", "    def data_objects_list(self, function_request):\n\t        data_obj_keys = list(function_request[\"data_files\"].keys())\n\t        data_objects_list = []\n\t        for key in data_obj_keys:\n\t            update_keys = list(function_request[\"data_files\"][key].keys())\n\t            for update_key in update_keys:\n\t                data_objects_list.append(\n\t                    f\"name {update_key} update time {key.replace('$', ' ')}\"\n\t                )\n\t        data_objects_list = (\n", "            data_objects_list\n\t            if len(data_objects_list) > 0\n\t            else [\"None data objects found\"]\n\t        )\n\t        return data_objects_list\n\t    def list_deployments(self):\n\t        \"\"\"\n\t        List all deployments. Currently only Implemented for local deployment.\n\t        \"\"\"\n\t        user_functions = self.redis_helper_obj.get_keys(\"*\")\n", "        if user_functions is not None:\n\t            keys = [\"deployment-name\", \"args\", \"data-objects\"]\n\t            function_names = [value.decode() for value in user_functions]\n\t            for function_name in function_names:\n\t                table = []\n\t                json_value = json.loads(self.redis_helper_obj.get_key(function_name))\n\t                curl_request = json_value[function_name][\"curl\"]\n\t                table.append(\n\t                    [\n\t                        function_name,\n", "                        \"\\n\".join(self.generate_arg_list(json_value[function_name])),\n\t                        \"\\n\".join(self.data_objects_list(json_value[function_name])),\n\t                    ]\n\t                )\n\t                print(f\"curl-request {curl_request}\")\n\t                print(tabulate(table, keys, \"grid\"))\n"]}
{"filename": "airdot/__init__.py", "chunked_list": ["__version__ = \"0.0.1\"\n\t__authur__ = \"abhinav.singh\"\n\tURL = \"https://deploy.creatoronly.in/\"\n\tVERIFY = True\n\t# URL = \"https://127.0.0.1:8000/\"\n\t# VERIFY = False\n\tfrom airdot.deployer import Deployer\n\t__all__ = [\"Deployer\"]\n"]}
{"filename": "airdot/cli.py", "chunked_list": []}
{"filename": "airdot/contants/__init__.py", "chunked_list": []}
{"filename": "airdot/contants/runtime_images.py", "chunked_list": ["# python runtime images\n\tseldon_images = {\n\t    \"3.7\": \"seldonio/seldon-core-s2i-python37-ubi8:1.17.0-dev\",\n\t    \"3.8\": \"seldonio/seldon-core-s2i-python38-ubi8:1.17.0-dev\",\n\t}\n"]}
{"filename": "airdot/collection/__init__.py", "chunked_list": []}
{"filename": "airdot/collection/collections.py", "chunked_list": ["# python imports\n\tfrom typing import Dict, List, Any, Optional\n\tfrom datetime import datetime\n\tclass namespace:\n\t    def __init__(self):\n\t        self.functions: Dict[str, str] = {}\n\t        self.vars: Dict[str, Any] = {}\n\t        self.imports: Dict[str, str] = {}\n\t        self.froms: Dict[str, str] = {\"*\": \"typing\"}\n\t        self.all_modules: List[str] = []\n", "        self.custom_init_code: List[str] = []\n\tclass python_function_prop:\n\t    exclude_from_dict: List[str] = [\"errors\"]\n\t    def __init__(self):\n\t        self.source: Optional[str] = None\n\t        self.name: Optional[str] = None\n\t        self.arg_names: Optional[List[str]] = None\n\t        self.arg_types: Optional[Dict[str, str]] = None\n\t        self.namespace_vars_desc: Optional[Dict[str, str]] = None\n\t        self.namespace_functions: Optional[Dict[str, str]] = None\n", "        self.namespace_imports: Optional[Dict[str, str]] = None\n\t        self.namespace_froms: Optional[Dict[str, str]] = None\n\t        self.namespace_modules: Optional[List[str]] = None\n\t        self.errors: Optional[List[str]] = None\n\t        self.namespace_vars: Optional[Dict[str, Any]] = None\n\t        self.custom_init_code: Optional[List[str]] = None\n\tclass authentication:\n\t    def __init__(self) -> None:\n\t        self.refresh_token: Optional[str] = None\n\t        self.token_time: Optional[datetime] = datetime(2000, 1, 1)\n", "class source_file_props:\n\t    def __init__(\n\t        self, user_contents: str, seldon_contents: str = None, name: str = \"source.py\"\n\t    ):\n\t        self.seldon_name = \"seldon_wrapper.py\"\n\t        self.user_name = name\n\t        self.seldon_contents = seldon_contents\n\t        self.user_contents = user_contents\n\t    def as_dict(self):\n\t        return {\n", "            \"user_name\": self.user_name,\n\t            \"seldon_name\": self.seldon_name,\n\t            \"seldon_contents\": self.seldon_contents,\n\t            \"user_contents\": self.user_contents,\n\t        }\n"]}
{"filename": "airdot/helpers/docker_helper.py", "chunked_list": ["import os\n\timport string\n\timport random\n\timport docker\n\tfrom airdot.helpers.template_helpers import get_docker_template\n\tDEFAULT_PKG_LIST = [\"Flask\", \"gunicorn\", \"boto3\"]\n\tclass docker_helper:\n\t    def __init__(self):\n\t        self.client = docker.from_env()\n\t    def run_container(\n", "        self,\n\t        image_name,\n\t        command=None,\n\t        detach=True,\n\t        remove=True,\n\t        ports=None,\n\t        network=None,\n\t    ):\n\t        try:\n\t            container = self.client.containers.run(\n", "                image_name,\n\t                command,\n\t                detach=detach,\n\t                remove=remove,\n\t                ports=ports,\n\t                network=network,\n\t            )\n\t            return container\n\t        except docker.errors.ImageNotFound:\n\t            print(f\"Error: Image '{image_name}' not found\")\n", "        except docker.errors.APIError as e:\n\t            print(f\"Error starting container: {e}\")\n\t    def get_container(self, container_id):\n\t        try:\n\t            container = self.client.containers.get(container_id)\n\t            return container.id\n\t        except docker.errors.NotFound:\n\t            print(f\"Error: Container '{container_id}' not found\")\n\t        except docker.errors.APIError as e:\n\t            print(f\"Error getting container ID: {e}\")\n", "    def get_container_id(self, image_name):\n\t        container_id = None\n\t        containers = self.client.containers.list(all=True)\n\t        for container in containers:\n\t            if len(container.image.tags) > 0 and container.image.tags[0].split(\":\")[0] == image_name:\n\t                container_id = container.id\n\t        return container_id\n\t    def kill_container(self, container_id):\n\t        try:\n\t            container = self.client.containers.get(container_id)\n", "            container.kill()\n\t            return True\n\t        except docker.errors.NotFound:\n\t            print(f\"Error: Container '{container_id}' not found\")\n\t        except docker.errors.APIError as e:\n\t            print(f\"Error killing container: {e}\")\n\t            return False\n\t    def delete_container(self, container_id):\n\t        try:\n\t            container = self.client.containers.get(container_id)\n", "            container.remove()\n\t            return True\n\t        except docker.errors.NotFound:\n\t            print(f\"Error: Container '{container_id}' not found\")\n\t        except docker.errors.APIError as e:\n\t            print(f\"Error deleting container: {e}\")\n\t            return False\n\t    def restart_container(self, container_id):\n\t        try:\n\t            container = self.client.containers.get(container_id)\n", "            container.restart()\n\t            print(f\"Container '{container_id}' restarted successfully\")\n\t        except docker.errors.NotFound:\n\t            print(f\"Container '{container_id}' not found\")\n\t    def create_docker_runtime(self, deploy_dict):\n\t        dir = self.write_user_file(deploy_dict[\"source_file\"])\n\t        custum_req = self.get_custom_requirements(deploy_dict[\"requirements_txt\"])\n\t        success_flag = self.create_custom_docker_file(custum_req, dir)\n\t        if success_flag:\n\t            image, _ = self.client.images.build(\n", "                path=f\"/tmp/{dir}/\", tag=f\"{deploy_dict['name']}\"\n\t            )\n\t            return image, dir\n\t        return None, None\n\t    def build_image(self, path, name):\n\t        image, _ = self.client.images.build(\n\t                path=path, tag=f\"{name}\"\n\t            )\n\t        return image\n\t    def get_available_images(self):\n", "        try:\n\t            images = self.client.images.list()\n\t            return [image.tags[0] for image in images if image.tags]\n\t        except docker.errors.APIError as e:\n\t            print(f\"Failed to retrieve available images: {e}\")\n\t            return None\n\t    def pull_image(self, image_name):\n\t        try:\n\t            self.client.images.pull(image_name)\n\t            print(f\"Image '{image_name}' pulled successfully.\")\n", "        except docker.errors.APIError as e:\n\t            print(f\"Failed to pull image: {e}\")\n\t    def create_custom_docker_file(self, custum_req, dir):\n\t        try:\n\t            docker_template = get_docker_template(custum_req)\n\t            with open(f\"/tmp/{dir}/Dockerfile\", \"w\") as py_file:\n\t                py_file.write(\"\\n\".join(docker_template) + \"\\n\")\n\t            return True\n\t        except:\n\t            return False\n", "    def get_custom_requirements(self, requirements_txt):\n\t        pkg_list = requirements_txt.split(\"\\n\")\n\t        return \" \".join(pkg_list + DEFAULT_PKG_LIST)\n\t    def write_user_file(self, source_file):\n\t        try:\n\t            dir = self.id_generator()\n\t            os.mkdir(f\"/tmp/{dir}\")\n\t            with open(f\"/tmp/{dir}/app.py\", \"w\") as py_file:\n\t                py_file.write(\"\\n\".join(source_file[\"contents\"].split(\"\\n\")) + \"\\n\")\n\t            return dir\n", "        except:\n\t            return None\n\t    def id_generator(self, size=4, chars=string.ascii_lowercase + string.digits):\n\t        return \"\".join(random.choice(chars) for _ in range(size))\n"]}
{"filename": "airdot/helpers/data_object_helpers.py", "chunked_list": ["import sys\n\tfrom pathlib import Path\n\tsys.path.append(str(Path(__file__).parents[2]))\n\timport hashlib\n\timport pickle\n\tfrom google.cloud import storage\n\tfrom typing import Dict, Tuple, Any, cast\n\timport pprint\n\timport yaml\n\tfrom google.cloud import storage\n", "#from google.oauth2 import service_account\n\tfrom googleapiclient import discovery\n\tfrom datetime import timedelta\n\tSCHEMA_VERSION = 1\n\tMAX_DESCRIPTION_SIZE = 5000\n\tNULL_BYTE = b\"\\x00\"\n\tfrom airdot.helpers.minio_helper import minio_helper\n\tdef serialize_zstd(obj) -> Tuple[bytes, str, int]:\n\t    pkl_data = pickle.dumps(obj)\n\t    content_hash = f\"sha1:{hashlib.sha1(pkl_data).hexdigest()}\"\n", "    obj_size = len(pkl_data)\n\t    return (pkl_data, content_hash, obj_size)\n\tdef is_binary_file(content: bytes) -> bool:\n\t    return NULL_BYTE in content\n\tdef decode_string(b: bytes) -> str:\n\t    for encoding in (\"ascii\", \"utf8\", \"latin1\"):\n\t        try:\n\t            return b.decode(encoding)\n\t        except UnicodeDecodeError:\n\t            pass\n", "    return b.decode(\"ascii\", \"ignore\")\n\tdef describe_object(\n\t    obj: Any, max_depth: int, remaining_characters=MAX_DESCRIPTION_SIZE\n\t) -> Dict[str, Any]:\n\t    objT = type(obj)\n\t    if objT is dict and max_depth > 0:\n\t        ret = {}\n\t        for k, v in obj.items():\n\t            ret[k] = describe_object(v, max_depth - 1, max(0, remaining_characters))\n\t            remaining_characters -= len(str(ret[k]))\n", "        return ret\n\t    elif objT is bytes:\n\t        if is_binary_file(obj):\n\t            obj = \"Unknown binary file\"\n\t        else:\n\t            obj = decode_string(obj)\n\t            objT = type(obj)\n\t    description = (\n\t        obj[:remaining_characters].strip()\n\t        if type(obj) is str\n", "        else pprint.pformat(obj, depth=1, width=100, compact=True)[\n\t            :remaining_characters\n\t        ].strip()\n\t    )\n\t    return {\n\t        \"module\": objT.__module__,\n\t        \"class\": objT.__name__,\n\t        \"description\": description,\n\t    }\n\tdef repr_str(dumper: yaml.Dumper, data: str):\n", "    if \"\\n\" in data:\n\t        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"|\")\n\t    return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data)\n\tdef to_file_stub_dict(content_hash: str, obj_desc: Dict[str, Any]) -> Dict[str, Any]:\n\t    return {\n\t        \"_\": \"MBFileStub\",\n\t        \"content_hash\": content_hash,\n\t        \"metadata\": obj_desc,\n\t        \"schemaVersion\": SCHEMA_VERSION,\n\t    }\n", "def to_yaml(content_hash: str, fileSize: int, obj_desc: Dict[str, Any]) -> str:\n\t    metadata: Dict[str, Any] = {\"file_size\": fileSize, \"object\": obj_desc}\n\t    obj = to_file_stub_dict(content_hash, metadata)\n\t    yaml.add_representer(str, repr_str)\n\t    return yaml.dump(obj, width=1000)\n\tdef put_secure_data(\n\t    bucket_id, open_id, data: bytes, desc: str, endpoint: str, bucket_type\n\t):\n\t    try:\n\t        if bucket_type == 'minio':\n", "            minio_helper_obj = minio_helper(endpoint=endpoint)\n\t            minio_helper_obj.create_bucket(bucket_name=bucket_id)\n\t            minio_helper_obj.put_object(bucket=bucket_id, key=f\"{desc}.pkl\", data=data)\n\t            return True\n\t        # TODO add gcs and aws support\n\t    except Exception as e:\n\t        print(f\"failed to upload data object. Please try again {e}\")\n\t        return False\n\tdef upload_runtime_object(\n\t    bucket_id, open_id, obj, desc: str, endpoint: str, bucket_type: str\n", "):\n\t    (data, content_hash, obj_size) = serialize_zstd(obj)\n\t    response = put_secure_data(bucket_id, open_id, data, desc, endpoint, bucket_type)\n\t    if response:\n\t        yamlObj = to_yaml(content_hash, obj_size, describe_object(obj, 1))\n\t        return yamlObj  # need to think a way to save complete yamlObj\n\t    else:\n\t        return \"None\"\n\t# uploading\n\tdef make_and_upload_data_files(\n", "    bucket_id, open_id, py_state, endpoint, bucket_type=None\n\t):\n\t    dataFiles: Dict[str, str] = {}\n\t    if py_state.namespace_vars and py_state.namespace_vars_desc:\n\t        for nName, nVal in py_state.namespace_vars.items():\n\t            dataFiles[f\"{nName}.pkl\"] = upload_runtime_object(\n\t                bucket_id, open_id, nVal, nName, endpoint, bucket_type=bucket_type\n\t            )\n\t    return dataFiles\n"]}
{"filename": "airdot/helpers/kubernetes_helper.py", "chunked_list": ["import subprocess\n\timport yaml\n\tfrom kubernetes import config, client\n\tclass k8s:\n\t    def __init__(self):  # Optional: specify the context to use\n\t        self.context = self.get_current_cluster_config()\n\t    def get_current_cluster_config(self):\n\t        try:\n\t            current_context = config.list_kube_config_contexts()[1][\"context\"]\n\t            return current_context\n", "        except Exception as e:\n\t            print(f\"Failed to retrieve cluster configuration: {e}\")\n\t            return None\n\t    def apply_kubernetes_resources(self, resource_paths):\n\t        try:\n\t            subprocess.check_call(\n\t                    [\"kubectl\", \"apply\", \"-f\", resource_paths]\n\t                )\n\t            print(\"Resources applied successfully.\")\n\t        except subprocess.CalledProcessError as e:\n", "            print(f\"Failed to apply resources: {e}\")\n\t    def create_namespace(self, namespace):\n\t        try:\n\t            subprocess.check_call(\n\t                [\"kubectl\", \"create\", \"namespace\", namespace]\n\t            )\n\t            print(f\"Namespace '{namespace}' created successfully\")\n\t            return True\n\t        except subprocess.CalledProcessError as e:\n\t            print(f\"Failed to create namespace: {e}\")\n", "            return False\n\t    def delete_namespace(self, namespace):\n\t        try:\n\t            subprocess.check_call(\n\t                [\"kubectl\", \"delete\", \"namespace\", namespace, \"--context\", self.context]\n\t            )\n\t            print(f\"Namespace '{namespace}' deleted successfully.\")\n\t        except subprocess.CalledProcessError as e:\n\t            print(f\"Failed to delete namespace: {e}\")\n"]}
{"filename": "airdot/helpers/redis_helper.py", "chunked_list": ["import redis\n\timport json\n\tfrom airdot.helpers.general_helpers import get_datetime\n\tclass redis_helper:\n\t    def __init__(self, host, port, db=None):\n\t        self.host = host\n\t        self.port = port\n\t        self.db = db\n\t        self.redis = redis.Redis(host=self.host, port=self.port, db=self.db)\n\t    def set_key(self, key, value):\n", "        self.redis.set(key, value)\n\t    def get_key(self, key):\n\t        return self.redis.get(key)\n\t    def delete_key(self, key):\n\t        self.redis.delete(key)\n\t    def get_keys(self, pattern):\n\t        return self.redis.keys(pattern)\n\t    def increment_key(self, key):\n\t        self.redis.incr(key)\n\t    def decrement_key(self, key):\n", "        self.redis.decr(key)\n\t    def set_user_function(\n\t        self, id, deploy_dict, function_curl_req, object_refresh=False\n\t    ):\n\t        user_function = self.get_key(id)\n\t        if user_function is not None:  # for a old deployment\n\t            user_function = json.loads(user_function)\n\t            user_function[deploy_dict[\"name\"]][\"curl\"] = function_curl_req\n\t            user_function[deploy_dict[\"name\"]][\"version\"] = (\n\t                user_function[deploy_dict[\"name\"]][\"version\"] + 1\n", "                if not (object_refresh)\n\t                else user_function[deploy_dict[\"name\"]][\"version\"]\n\t            )\n\t            user_function[deploy_dict[\"name\"]][\"data_files\"][get_datetime()] = (\n\t                \"\" if deploy_dict[\"data_files\"] is None else deploy_dict[\"data_files\"]\n\t            )\n\t            user_function[deploy_dict[\"name\"]][\"metadata\"] = {\n\t                \"python_version\": deploy_dict[\"python_version\"]\n\t                if not (object_refresh)\n\t                else user_function[deploy_dict[\"name\"]][\"metadata\"][\"python_version\"],\n", "                \"arg_types\": deploy_dict[\"arg_types\"]\n\t                if not (object_refresh)\n\t                else user_function[deploy_dict[\"name\"]][\"metadata\"][\"arg_types\"],\n\t                \"arg_names\": deploy_dict[\"arg_names\"]\n\t                if not (object_refresh)\n\t                else user_function[deploy_dict[\"name\"]][\"metadata\"][\"arg_names\"],\n\t            }\n\t        else:\n\t            user_function = dict()\n\t            user_function[deploy_dict[\"name\"]] = {\n", "                \"curl\": function_curl_req,\n\t                \"version\": 1,\n\t                \"data_files\": {\n\t                    get_datetime(): \"\"\n\t                    if deploy_dict[\"data_files\"] is None\n\t                    else deploy_dict[\"data_files\"]\n\t                },\n\t                \"metadata\": {\n\t                    \"python_version\": deploy_dict[\"python_version\"],\n\t                    \"arg_types\": deploy_dict[\"arg_types\"],\n", "                    \"arg_names\": deploy_dict[\"arg_names\"],\n\t                },\n\t            }\n\t        try:\n\t            status = self.set_key(\n\t                id,\n\t                json.dumps(user_function),\n\t            )\n\t            return status\n\t        except Exception as e:\n", "            return None\n"]}
{"filename": "airdot/helpers/template_helpers.py", "chunked_list": ["from typing import List\n\tfrom airdot.collection.collections import python_function_prop, source_file_props\n\tfrom airdot.helpers.general_helpers import add_space\n\tbucket_type_import = {\n\t    \"gcs\": \"from google.cloud import storage\",\n\t    \"s3\": \"import boto3\",\n\t    \"azure\": \"import boto3\",\n\t    \"minio\": \"import boto3\",\n\t}\n\tobject_type_import = {}\n", "def make_soruce_file(\n\t    dir: str, pyProps: python_function_prop, source_file_name: str = \"source\"\n\t):\n\t    source_parts: List[str] = [\n\t        \"import sys\",\n\t        \"from flask import escape, jsonify, Flask, request\",\n\t        \"import pickle\",\n\t        \"import boto3\",\n\t        \"app = Flask('ml-deployer')\",\n\t    ]\n", "    if pyProps.namespace_froms:\n\t        for iAs, iModule in pyProps.namespace_froms.items():\n\t            source_parts.append(f\"from {iModule} import {iAs}\")\n\t    if pyProps.namespace_imports:\n\t        for iAs, iModule in pyProps.namespace_imports.items():\n\t            if iModule == iAs:\n\t                source_parts.append(f\"import {iModule}\")\n\t            else:\n\t                source_parts.append(f\"import {iModule} as {iAs}\")\n\t    add_space(source_parts)\n", "    source_parts.append(\n\t        \"client = boto3.resource('s3', endpoint_url='http://airdot-minio-1:9000', aws_access_key_id='minioadmin',aws_secret_access_key='miniopassword')\"\n\t    )\n\t    source_parts.append(f\"bucket = client.Bucket('{pyProps.name.replace('_','-')}')\")\n\t    if pyProps.namespace_vars and pyProps.namespace_vars_desc:\n\t        for nName, _ in pyProps.namespace_vars.items():\n\t            source_parts.append(\n\t                f\"{nName} = pickle.loads(bucket.Object('{nName}.pkl').get()['Body'].read())\"\n\t            )\n\t    if pyProps.custom_init_code:\n", "        source_parts.append(\"\\n\" + \"\\n\\n\".join(pyProps.custom_init_code))\n\t    add_space(source_parts)\n\t    if pyProps.namespace_functions:\n\t        for _, fSource in pyProps.namespace_functions.items():\n\t            source_parts.append(fSource)\n\t            add_space(source_parts)\n\t    add_space(source_parts)\n\t    if pyProps.source:\n\t        source_parts.append(\"# main function\")\n\t        source_parts.append(pyProps.source)\n", "    # add calling method\n\t    add_space(source_parts)\n\t    source_parts.append(\"@app.route('/', methods=['POST'])\")\n\t    source_parts.append(f\"def main_{pyProps.name}():\")\n\t    source_parts.append(\"\\tdata = request.get_json()\")\n\t    source_parts.append(\"\\tif data is None:\")\n\t    source_parts.append(f\"\\t\\treturn jsonify(str({pyProps.name}()))\")\n\t    source_parts.append(\"\\telse:\")\n\t    source_parts.append(f\"\\t\\treturn jsonify(str({pyProps.name}(**data)))\")\n\t    return source_file_props(\n", "        name=f\"{source_file_name}.py\", user_contents=\"\\n\".join(source_parts)\n\t    )\n\tdef build_source_template(\n\t    dir: str,\n\t    pyProps: python_function_prop,\n\t    source_file_name: str = \"source\",\n\t    bucket_type=\"gcs\",\n\t    bucket_name=\"seldon-test\",\n\t):\n\t    source_parts: List[str] = [\n", "        \"import sys\",\n\t        \"from flask import escape, jsonify, Flask, request\",\n\t        \"import pickle\",\n\t        \"import logging\",\n\t        \"from io import BytesIO, StringIO\",\n\t        bucket_type_import[bucket_type],\n\t    ]\n\t    # adding custom imports\n\t    if pyProps.namespace_froms:\n\t        for iAs, iModule in pyProps.namespace_froms.items():\n", "            source_parts.append(f\"from {iModule} import {iAs}\")\n\t    if pyProps.namespace_imports:\n\t        for iAs, iModule in pyProps.namespace_imports.items():\n\t            if iModule == iAs:\n\t                source_parts.append(f\"import {iModule}\")\n\t            else:\n\t                source_parts.append(f\"import {iModule} as {iAs}\")\n\t    for _ in range(4):\n\t        add_space(source_parts)\n\t    # adding bucket imports\n", "    if bucket_type is \"gcs\":\n\t        source_parts.append(\"storage_client = storage.Client()\")\n\t        source_parts.append(\n\t            f\"bucket = storage_client.bucket('{pyProps.name.replace('_','-')}')\"\n\t        )\n\t        if pyProps.namespace_vars and pyProps.namespace_vars_desc:\n\t            for nName, _ in pyProps.namespace_vars.items():\n\t                source_parts.append(f\"{nName}_blob = bucket.get_blob({nName}.pkl')\")\n\t                source_parts.append(f\"self.{nName} = BytesIO(blob.download_as_bytes())\")\n\t    elif bucket_type is \"minio\":\n", "        source_parts.append(\n\t            \"client = boto3.resource('s3', endpoint_url='http://airdot-minio-1:9000', aws_access_key_id='minioadmin',aws_secret_access_key='miniopassword')\"\n\t        )\n\t        source_parts.append(\n\t            f\"bucket = client.Bucket('{pyProps.name.replace('_','-')}')\"\n\t        )\n\t        if pyProps.namespace_vars and pyProps.namespace_vars_desc:\n\t            for nName, _ in pyProps.namespace_vars.items():\n\t                source_parts.append(\n\t                    f\"{nName} = pickle.loads(bucket.Object('{nName}.pkl').get()['Body'].read())\"\n", "                )\n\t    if pyProps.custom_init_code:\n\t        source_parts.append(\"\\n\" + \"\\n\\n\".join(pyProps.custom_init_code))\n\t    add_space(source_parts)\n\t    if pyProps.namespace_functions:\n\t        for _, fSource in pyProps.namespace_functions.items():\n\t            source_parts.append(fSource)\n\t            add_space(source_parts)\n\t    add_space(source_parts)\n\t    if pyProps.source:\n", "        source_parts.append(\"# main function\")\n\t        source_parts.append(pyProps.source)\n\t    return source_parts\n\tdef make_soruce_file_seldon(\n\t    dir: str,\n\t    pyProps: python_function_prop,\n\t    source_file_name: str = \"source\",\n\t    bucket_type=\"gcs\",\n\t    bucket_name=\"seldon-test\",\n\t):\n", "    source_parts: List[str] = [\n\t        \"import logging\",\n\t        f\"from {pyProps.name}_source import {pyProps.name}\",\n\t    ]\n\t    add_space(source_parts)\n\t    source_parts.append(f\"class {pyProps.name}_class(object):\")\n\t    source_parts.append(f\"\\tdef __init__(self):\")\n\t    source_parts.append(f\"\\t\\t logging.info('service created ready to serve')\")\n\t    add_space(source_parts)\n\t    # add calling method\n", "    add_space(source_parts)\n\t    source_parts.append(f\"\\tdef predict(self, data):\")\n\t    source_parts.append(f\"\\t\\treturn {pyProps.name}(**data)\")\n\t    user_source = build_source_template(\n\t        dir, pyProps, source_file_name, bucket_type, bucket_name\n\t    )\n\t    return source_file_props(\n\t        name=f\"{pyProps.name}_source.py\",\n\t        seldon_contents=\"\\n\".join(source_parts),\n\t        user_contents=\"\\n\".join(user_source),\n", "    )\n\tdef get_docker_template(req_string, source_name):\n\t    dockerBuildParts: List[str] = [\n\t        \"FROM python:3.8-slim\",\n\t        \"ENV APP_HOME /app\",\n\t        \"WORKDIR $APP_HOME\",\n\t        \"COPY . ./\",\n\t        f\"RUN pip install {req_string}\",\n\t        f\"CMD exec gunicorn --bind :8080 --workers 1 --threads 8 {source_name}:app\",\n\t    ]\n", "    return dockerBuildParts\n"]}
{"filename": "airdot/helpers/runtime_helper.py", "chunked_list": ["import inspect\n\tfrom typing import Callable, Any, List, Dict\n\timport tempfile, os\n\timport ast\n\timport re\n\tfrom airdot.collection.collections import namespace, python_function_prop\n\tdef get_function_properties(func, imported_modules):\n\t    props = python_function_prop()\n\t    if not callable(func):\n\t        raise Exception(\"Object is not a callable function\")\n", "    else:\n\t        props.name = func.__name__\n\t        props.source = get_function_source_code(func)\n\t        props.arg_names = get_func_args_name(func)\n\t        props.arg_types = annotation_to_type_str(func.__annotations__)\n\t        ns_collection = namespace()\n\t        ns_collection = get_function_dep(func, ns_collection, imported_modules)\n\t        props.namespace_functions = ns_collection.functions\n\t        props.namespace_vars = ns_collection.vars\n\t        props.namespace_vars_desc = get_string_values(ns_collection.vars)\n", "        props.namespace_imports = ns_collection.imports\n\t        props.namespace_froms = ns_collection.froms\n\t        props.namespace_modules = list(set(ns_collection.all_modules))\n\t        props.custom_init_code = ns_collection.custom_init_code\n\t    return props\n\tdef get_string_values(args: Dict[str, Any]):\n\t    new_dict: Dict[str, str] = {}\n\t    for k, v in args.items():\n\t        str_val = re.sub(r\"\\s+\", \" \", str(v))\n\t        if type(v) is bytes:\n", "            str_val = \"Binary data\"\n\t        elif len(str_val) > 200:\n\t            str_val = str_val[0:200] + \"...\"\n\t        new_dict[k] = str_val\n\t    return new_dict\n\tdef unindent(source: str) -> str:\n\t    leading_whitespaces = len(source) - len(source.lstrip())\n\t    if leading_whitespaces == 0:\n\t        return source\n\t    new_lines = [line[leading_whitespaces:] for line in source.split(\"\\n\")]\n", "    return \"\\n\".join(new_lines)\n\tdef get_function_source_code(func: Callable = None):\n\t    if not callable(func):\n\t        return None\n\t    return unindent(inspect.getsource(func))\n\tdef get_func_args_name(func: Callable = None):\n\t    arg_spec = inspect.getfullargspec(func)\n\t    if arg_spec.varargs:\n\t        return [\"...\"]\n\t    if arg_spec.args:\n", "        return arg_spec.args\n\t    noArgs: List[str] = []\n\t    return noArgs\n\tdef annotation_to_type_str(annotations: Dict[str, Any]):\n\t    anno_strs: Dict[str, str] = {}\n\t    for name, t_class in annotations.items():\n\t        try:\n\t            if t_class == Any:\n\t                anno_strs[name] = \"Any\"\n\t            else:\n", "                anno_strs[name] = t_class.__name__\n\t        except:\n\t            pass\n\t    return anno_strs\n\tdef has_state(obj: Any) -> bool:\n\t    try:\n\t        return len(obj.__dict__) > 0\n\t    except:\n\t        return False\n\tdef collect_byte_obj(\n", "    maybe_func_var: Any, maybe_func_var_name: str, collection: namespace\n\t):\n\t    tmp_file_path = os.path.join(tempfile.gettempdir(), \"btyd.pkl\")\n\t    maybe_func_var.save_model(tmp_file_path)\n\t    with open(tmp_file_path, \"rb\") as f:\n\t        collection.vars[maybe_func_var_name + \"_state\"] = f.read()\n\t    collection.custom_init_code.append(\n\t        f\"\"\"\n\t    with open('data/{maybe_func_var_name}_state.tmp', 'wb') as fo:\n\t    with open('data/{maybe_func_var_name}_state.pkl', 'rb') as fi:\n", "    fo.write(pickle.load(fi))\n\t    {maybe_func_var_name} = {maybe_func_var.__class__.__name__}()\n\t    {maybe_func_var_name}.load_model('data/{maybe_func_var_name}_state.tmp')\n\t    \"\"\".strip()\n\t    )\n\t    collection.froms[maybe_func_var.__class__.__name__] = maybe_func_var.__module__\n\t    collection.imports[\"pickle\"] = \"pickle\"\n\t    collection.imports[\"btyd\"] = \"btyd\"\n\tdef is_imported_module(imported_modules, module_name):\n\t    for item in imported_modules:\n", "        pkg_name, _ = item.split(\"==\")\n\t        if pkg_name == module_name:\n\t            return True\n\t    return False\n\tdef get_function_dep(func: Callable[..., Any], collection: namespace, imported_modules):\n\t    if not callable(func):\n\t        return collection\n\t    collection = get_function_args(func, collection)\n\t    globalsDict = func.__globals__  # type: ignore\n\t    allNames = func.__code__.co_names + func.__code__.co_freevars\n", "    for maybe_func_var_name in allNames:\n\t        if maybe_func_var_name in globalsDict:\n\t            maybe_func_var = globalsDict[maybe_func_var_name]\n\t            if \"__module__\" in dir(maybe_func_var):\n\t                if maybe_func_var.__module__ == \"__main__\":\n\t                    arg_names = list(maybe_func_var.__code__.co_varnames or [])\n\t                    funcSig = f\"{maybe_func_var.__name__}({', '.join(arg_names)})\"\n\t                    if funcSig not in collection.functions:\n\t                        collection.functions[funcSig] = inspect.getsource(\n\t                            maybe_func_var\n", "                        )\n\t                        get_function_dep(maybe_func_var, collection, imported_modules)\n\t                else:\n\t                    if inspect.isclass(maybe_func_var):\n\t                        collection.froms[\n\t                            maybe_func_var_name\n\t                        ] = maybe_func_var.__module__  #\n\t                        collection.all_modules.append(maybe_func_var.__module__)\n\t                    elif callable(maybe_func_var) and not has_state(maybe_func_var):\n\t                        collection.froms[\n", "                            maybe_func_var_name\n\t                        ] = maybe_func_var.__module__  #\n\t                        collection.all_modules.append(maybe_func_var.__module__)\n\t                    elif \"btyd.fitters\" in f\"{maybe_func_var.__module__}\":\n\t                        collection = collect_byte_obj(\n\t                            maybe_func_var, maybe_func_var_name, collection\n\t                        )\n\t                    elif isinstance(maybe_func_var, object):\n\t                        collection.froms[\n\t                            maybe_func_var.__class__.__name__\n", "                        ] = maybe_func_var.__module__\n\t                        collection.all_modules.append(maybe_func_var.__module__)\n\t                        collection.vars[maybe_func_var_name] = maybe_func_var\n\t                    else:\n\t                        collection.froms[\n\t                            maybe_func_var_name\n\t                        ] = f\"NYI: {maybe_func_var.__module__}\"\n\t            elif str(maybe_func_var).startswith(\"<module\"):\n\t                collection.imports[maybe_func_var_name] = maybe_func_var.__name__\n\t                collection.all_modules.append(maybe_func_var.__name__)\n", "            elif inspect.isclass(maybe_func_var):\n\t                collection.froms[maybe_func_var_name] = maybe_func_var.__module__  #\n\t                collection.all_modules.append(maybe_func_var.__module__)\n\t            else:\n\t                collection.vars[maybe_func_var_name] = maybe_func_var\n\t    return collection\n\tdef is_valid_package(pkg_str: str):\n\t    return len(pkg_str.split(\".\")) > 0\n\tdef collect_mod_name(func, modName: str, collection: namespace):\n\t    if modName in func.__globals__:\n", "        gMod = func.__globals__[modName]\n\t        if hasattr(gMod, \"__module__\"):\n\t            collection.froms[modName] = gMod.__module__\n\t        else:\n\t            collection.imports[modName] = gMod.__name__\n\t            collection.all_modules.append(func.__globals__[modName].__name__)\n\t    return collection\n\tdef parse_ast_name_to_id(astName: Any):\n\t    if hasattr(astName, \"attr\"):\n\t        return astName.value.id\n", "    else:\n\t        return astName.id\n\tdef get_function_args(func: Callable[..., Any], collection: namespace):\n\t    try:\n\t        sigAst = ast.parse(inspect.getsource(func)).body[0]  # type: ignore\n\t        for a in sigAst.args.args:  # type: ignore\n\t            if a.annotation is None:  # type: ignore\n\t                continue\n\t            collection = collect_mod_name(func, parse_ast_name_to_id(a.annotation), collection=collection)  # type: ignore\n\t        if sigAst.returns is not None:  # type: ignore\n", "            collection = collect_mod_name(func, parse_ast_name_to_id(sigAst.returns), collection=collection)  # type: ignore\n\t        return collection\n\t    except Exception as err:\n\t        strErr = f\"{err}\"\n\t        if (\n\t            strErr != \"could not get source code\"\n\t        ):  # triggers when deploying pure sklearn model\n\t            print(f\"Warning: failed parsing type annotations: {err}\")\n"]}
{"filename": "airdot/helpers/seldon_helper.py", "chunked_list": ["from airdot.data_models.deployment import Deployment\n\tclass seldon_helpers(object):\n\t    def __init__(self, deployment_configuration) -> None:\n\t        if deployment_configuration is None:\n\t            raise TypeError(\n\t                \"failed to build seldon deployment no deployment configuration is provided\"\n\t            )\n\t        self.seldon_configuration = Deployment(**deployment_configuration).dict()['seldon_configuration']\n\t    def create_seldon_configuration(self, deploy_dict, image_uri):\n\t        if self.seldon_configuration['apiVersion'] == \"None\":\n", "            self.seldon_configuration['apiVersion'] = \"machinelearning.seldon.io/v1\"\n\t            self.seldon_configuration['metadata']['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n\t            self.seldon_configuration['metadata']['namespace'] = f\"{deploy_dict['name'].replace('_','-')}\"\n\t            self.seldon_configuration['spec']['predictors'][0]['componentSpecs'][0]['spec']['containers'][0]['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n\t            self.seldon_configuration['spec']['predictors'][0]['componentSpecs'][0]['spec']['containers'][0]['image'] = f\"{image_uri}\"\n\t            self.seldon_configuration['spec']['predictors'][0]['graph']['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n\t            self.seldon_configuration['spec']['predictors'][0]['name'] = f\"{deploy_dict['name'].replace('_','-')}\"\n\t            self.seldon_configuration['spec']['predictors'][0]['replicas'] = 1\n\t        return self.seldon_configuration\n"]}
{"filename": "airdot/helpers/network_helper.py", "chunked_list": ["import socket\n\tdef find_available_port(start_port, max_port=9000):\n\t    for port in range(start_port, max_port + 1):\n\t        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n\t            try:\n\t                sock.bind((\"localhost\", port))\n\t                return port\n\t            except OSError:\n\t                pass\n\t    return None\n"]}
{"filename": "airdot/helpers/__init__.py", "chunked_list": []}
{"filename": "airdot/helpers/minio_helper.py", "chunked_list": ["import boto3\n\tfrom botocore.exceptions import ClientError\n\tclass minio_helper:\n\t    def __init__(self, endpoint, access_key=None, secret_key=None, secure=True):\n\t        self.client = boto3.resource(\n\t            \"s3\",\n\t            endpoint_url=\"http://127.0.0.1:9000\",\n\t            aws_access_key_id=\"minioadmin\",\n\t            aws_secret_access_key=\"miniopassword\",\n\t        )\n", "    def bucket_exists(self, bucket_name):\n\t        if self.client.Bucket(bucket_name) in self.client.buckets.all():\n\t            # print(f'Bucket {bucket_name} exists.')\n\t            return True\n\t        else:\n\t            # print(f'Bucket {bucket_name} does not exist.')\n\t            return False\n\t    def create_bucket(self, bucket_name):\n\t        try:\n\t            if not (self.bucket_exists(bucket_name=bucket_name)):\n", "                bucket = self.client.create_bucket(Bucket=bucket_name)\n\t                # print(f'Bucket {bucket_name} created successfully.')\n\t            else:\n\t                pass\n\t                # print('bucket already exists')\n\t        except Exception as e:\n\t            print(f\"Error creating bucket {bucket_name}: {str(e)}\")\n\t    def delete_bucket(self, bucket_name):\n\t        try:\n\t            self.client.Bucket(bucket_name).delete()\n", "            # print(f'Bucket {bucket_name} deleted successfully.')\n\t        except Exception as e:\n\t            print(f\"Error deleting bucket {bucket_name}: {str(e)}\")\n\t    def put_object(self, bucket, key, data):\n\t        try:\n\t            self.client.Object(bucket, key).put(Body=data)\n\t            print(f\"{key} uploaded successfully and available at {bucket}/{key}\")\n\t        except Exception as e:\n\t            print(f\"Error uploading object {key}: {str(e)}\")\n\t    def get_object(self, bucket, key):\n", "        try:\n\t            response = self.client.get_object(Bucket=bucket, Key=key)\n\t            return response[\"Body\"].read()\n\t        except ClientError as e:\n\t            # print(f\"Error getting object '{key}' from MinIO: {e}\")\n\t            return None\n"]}
{"filename": "airdot/helpers/version_helpers.py", "chunked_list": ["import os, sys\n\tfrom typing import Optional\n\tsupported_python_versions = [\"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n\tdef get_python_default_version(python_version: Optional[str] = None):\n\t    if python_version is None:\n\t        # add getting environment python version here\n\t        python_version = f\"{sys.version_info.major}.{sys.version_info.minor}\"\n\t        return verify_version(python_version)\n\t    else:\n\t        return verify_version(python_version)\n", "def verify_version(python_version):\n\t    if python_version.split(\".\")[-1][0] in supported_python_versions:\n\t        return python_version\n\t    else:\n\t        raise Exception(\n\t            f\"Unsupported python version passed {python_version}. current supported version {supported_python_versions}\"\n\t        )\n"]}
{"filename": "airdot/helpers/authentication.py", "chunked_list": ["from airdot.collection.collections import authentication\n\timport requests\n\tfrom datetime import datetime\n\tfrom airdot import URL, VERIFY\n\tdef get_authentication_token(auth_: authentication):\n\t    r = requests.get(f\"{URL}get_auth_token\", verify=VERIFY)\n\t    if r.status_code == 200:\n\t        auth_.refresh_token = r.content.decode()\n\t        auth_.token_time = datetime.now()\n\tdef verify_user(auth_: authentication):\n", "    json = {\"auth_session_token\": auth_.refresh_token}\n\t    r = requests.post(f\"{URL}check_authentication\", verify=VERIFY, json=json)\n\t    if r.status_code == 200:\n\t        if r.content.decode() == \"false\":\n\t            return False\n\t        return True\n\tdef user_login(auth_: authentication):\n\t    get_authentication_token(auth_=auth_)\n\t    json = {\"auth_session_token\": auth_.refresh_token}\n\t    r = requests.post(f\"{URL}login\", verify=VERIFY, json=json)\n", "    if r.status_code == 200:\n\t        return r.content.decode()\n\t    else:\n\t        return None\n\tdef get_user_function(auth_: authentication):\n\t    json = {\"auth_session_token\": auth_.refresh_token}\n\t    r = requests.post(f\"{URL}get_my_functions\", verify=VERIFY, json=json)\n\t    if r.status_code == 200:\n\t        return r.content.decode()\n\t    else:\n", "        return None\n\tdef get_function_status(payload: dict):\n\t    r = requests.post(f\"{URL}check_function_status\", verify=VERIFY, json=payload)\n\t    if r.status_code == 200:\n\t        return r.content.decode()\n\t    else:\n\t        return None\n\tdef get_gcs_signed_token(auth_: authentication):\n\t    json = {\"auth_session_token\": auth_.refresh_token}\n\t    r = requests.post(f\"{URL}get_gcs_token\", verify=VERIFY, json=json)\n", "    if r.status_code == 200:\n\t        return r.content.decode()\n\t    else:\n\t        return None\n\tdef push_refreshed_objects(json_dict):\n\t    r = requests.post(f\"{URL}update_objects\", verify=VERIFY, json=json_dict)\n\t    if r.status_code == 200:\n\t        return r.content.decode()\n\t    else:\n\t        return None\n"]}
{"filename": "airdot/helpers/general_helpers.py", "chunked_list": ["# python module\n\tfrom typing import List\n\tfrom datetime import datetime\n\tdef add_space(strList: List[str]):\n\t    if len(strList) > 0 and strList[-1] != \"\":\n\t        strList.append(\"\")\n\tdef get_name(name: str):\n\t    if name is None:\n\t        return \"source\"\n\t    return name\n", "def in_notebook() -> bool:\n\t    # From: https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n\t    # Tested in Jupyter, Hex, DeepNote and Colab\n\t    try:\n\t        import IPython\n\t        return (\n\t            hasattr(IPython.get_ipython(), \"config\")\n\t            and len(IPython.get_ipython().config) > 0\n\t        )\n\t    except (NameError, ModuleNotFoundError):\n", "        return False\n\tdef get_difference(time):\n\t    return (time - datetime.now()).seconds\n\tdef get_datetime():\n\t    return datetime.now().strftime(\"%Y-%m-%d$%H:%M\")\n"]}
{"filename": "airdot/helpers/pkg_helpers.py", "chunked_list": ["import os, sys\n\tfrom typing import Optional, List, Dict\n\timport pkg_resources\n\timport types\n\tfrom importlib.metadata import version\n\timport json\n\timport requests\n\timport warnings\n\tpkg_mapping = {\"sklearn\": \"scikit-learn\"}\n\tbase_pkg = []\n", "def get_environment_pkgs(\n\t    python_packages: Optional[List[str]] = None, func_globals=None\n\t):\n\t    if python_packages is None:\n\t        return verify_packages(\n\t            get_pip_list().split(\"\\n\"), func_globals\n\t        )  # get_pip_list().split(\"\\n\")\n\t    elif isinstance(python_packages, list):\n\t        return python_packages + base_pkg\n\t# only explicit install ?\n", "def get_pip_list() -> List[Dict[str, str]]:\n\t    return os.popen(\"python -m pip freeze | grep == \").read().strip()\n\tdef imports(func_globals):\n\t    module_list = []\n\t    for name, val in func_globals.items():\n\t        if isinstance(val, types.ModuleType):\n\t            module_list.append(val.__name__)\n\t    return module_list\n\tdef get_locally_installed_packages(encoding=None):\n\t    packages = []\n", "    ignore = [\"tests\", \"_tests\", \"egg\", \"EGG\", \"info\"]\n\t    for path in sys.path:\n\t        for root, dirs, files in os.walk(path):\n\t            for item in files:\n\t                if \"top_level\" in item:\n\t                    item = os.path.join(root, item)\n\t                    with open(item, \"r\", encoding=encoding) as f:\n\t                        package = root.split(os.sep)[-1].split(\"-\")\n\t                        try:\n\t                            package_import = f.read().strip().split(\"\\n\")\n", "                        except:  # NOQA\n\t                            # TODO: What errors do we intend to suppress here?\n\t                            continue\n\t                        for i_item in package_import:\n\t                            if (i_item not in ignore) and (package[0] not in ignore):\n\t                                version = None\n\t                                if len(package) > 1:\n\t                                    version = (\n\t                                        package[1]\n\t                                        .replace(\".dist\", \"\")\n", "                                        .replace(\".egg\", \"\")\n\t                                    )\n\t                                packages.append(f\"{package[0]}=={version}\")\n\t    return list(set(packages))\n\tdef get_root_pkgs(pkg_list):\n\t    root_pkg = []\n\t    for pkg in pkg_list:\n\t        if len(pkg.split(\".\")) > 1 and pkg.split(\".\")[0] in pkg_mapping:\n\t            root_pkg.append(pkg_mapping[pkg.split(\".\")[0]])\n\t        else:\n", "            root_pkg.append(pkg)\n\t    return root_pkg\n\tdef verify_packages(pkg_list, func_globals):\n\t    # get only used modules\n\t    used_pkg_list = []\n\t    used_imports = get_root_pkgs(imports(func_globals))\n\t    for item in pkg_list:\n\t        pkg_name, pkg_version = item.split(\"==\")\n\t        if pkg_name in used_imports:\n\t            used_pkg_list.append(f\"{pkg_name}=={pkg_version}\")\n", "    pypi_url = \"https://pypi.org/pypi/{}/{}/json\"\n\t    final_pkg_list = []\n\t    for item in used_pkg_list:\n\t        pkg_name, pkg_version = item.split(\"==\")\n\t        status_code = requests.get(pypi_url.format(pkg_name, pkg_version)).status_code\n\t        if status_code == 200:\n\t            final_pkg_list.append(item)\n\t        else:\n\t            warnings.warn(f\"Not a valid Pypi package {pkg_name}. ignoring the package\")\n\t    final_pkg_list = final_pkg_list + base_pkg\n", "    return final_pkg_list\n"]}
{"filename": "airdot/helpers/content_helper.py", "chunked_list": ["import os\n\timport json\n\timport yaml\n\timport random\n\timport string\n\tfrom airdot.helpers.template_helpers import get_docker_template\n\tfrom airdot.helpers.s2i_helper import get_s2i_environment\n\tDEFAULT_PKG_LIST = [\"Flask\", \"gunicorn\", \"boto3\"]\n\tclass content_helper:\n\t    def __init__(\n", "        self, deploy_dict=None, deployment_type=None, seldon_configuration=None\n\t    ) -> str:\n\t        self.deploy_dict = deploy_dict\n\t        self.deployment_type = deployment_type\n\t        self.seldon_configuration = seldon_configuration\n\t    def write_contents(self):\n\t        try:\n\t            deployment_path = self.create_tmp_directory()\n\t            if self.deployment_type == \"test\":\n\t                # write python file\n", "                user_contents = self.deploy_dict[\"source_file\"][\"user_contents\"]\n\t                file_name = self.deploy_dict[\"source_file\"][\"user_name\"]\n\t                py_path = os.path.join(deployment_path, file_name)\n\t                self.write_python_file(\n\t                    py_path, \"\\n\".join(user_contents.split(\"\\n\")) + \"\\n\"\n\t                )\n\t                # write docker file\n\t                requirements_file_content = self.get_custom_requirements(\n\t                    self.deploy_dict[\"requirements_txt\"], DEFAULT_PKG_LIST\n\t                )\n", "                docker_template = get_docker_template(requirements_file_content, self.deploy_dict[\"source_file\"][\"user_name\"].split('.')[0])\n\t                dockerfile_path = os.path.join(deployment_path, \"Dockerfile\")\n\t                self.write_custom_file(dockerfile_path, \"\\n\".join(docker_template))\n\t                return deployment_path\n\t            elif self.deployment_type == \"seldon\":\n\t                # write user python file\n\t                user_contents = self.deploy_dict[\"source_file\"][\"user_contents\"]\n\t                file_name = self.deploy_dict[\"source_file\"][\"user_name\"]\n\t                py_path = os.path.join(deployment_path, file_name)\n\t                self.write_python_file(\n", "                    py_path, \"\\n\".join(user_contents.split(\"\\n\")) + \"\\n\"\n\t                )\n\t                # write seldon contents file\n\t                user_contents = self.deploy_dict[\"source_file\"][\"seldon_contents\"]\n\t                file_name = self.deploy_dict[\"source_file\"][\"seldon_name\"]\n\t                py_path = os.path.join(deployment_path, file_name)\n\t                self.write_python_file(\n\t                    py_path, \"\\n\".join(user_contents.split(\"\\n\")) + \"\\n\"\n\t                )\n\t                # .s2i/env file\n", "                os.mkdir(os.path.join(deployment_path, \".s2i\"))\n\t                s2i_path = os.path.join(deployment_path, \".s2i/environment\")\n\t                self.write_custom_file(\n\t                    s2i_path, get_s2i_environment(self.deploy_dict[\"name\"])\n\t                )\n\t                # write kubect yaml file\n\t                yaml_path = os.path.join(deployment_path, \"seldon_model.json\")\n\t                self.write_json_file(yaml_path, self.seldon_configuration)\n\t                return deployment_path\n\t            elif self.deployment_type == \"kserve\":\n", "                pass\n\t        except Exception as e:\n\t            print(f\"{e}\")\n\t    def id_generator(self, size=4, chars=string.ascii_lowercase + string.digits):\n\t        return \"\".join(random.choice(chars) for _ in range(size))\n\t    def get_custom_requirements(self, requirements_txt, default_path: str = \"\"):\n\t        pkg_list = requirements_txt.split(\"\\n\")\n\t        return \" \".join(pkg_list + default_path)\n\t    def create_tmp_directory(self):\n\t        try:\n", "            dir = self.id_generator(size=9)\n\t            path = f\"/tmp/{dir}\"\n\t            os.mkdir(path)\n\t            return path\n\t        except Exception as e:\n\t            print(f\"failed to create temporary directory {e}\")\n\t            return None\n\t    def write_file(self, file_path, content):\n\t        \"\"\"\n\t        Write content to a file.\n", "        :param file_path: The path of the file to be written.\n\t        :param content: The content to write to the file.\n\t        \"\"\"\n\t        directory = os.path.dirname(file_path)\n\t        if not os.path.exists(directory):\n\t            os.makedirs(directory)\n\t        with open(file_path, \"w\") as file:\n\t            file.write(content)\n\t    def write_python_file(self, file_path, content):\n\t        \"\"\"\n", "        Write content to a Python (.py) file.\n\t        :param file_path: The path of the Python file to be written.\n\t        :param content: The content to write to the file.\n\t        \"\"\"\n\t        if not file_path.endswith(\".py\"):\n\t            file_path += \".py\"\n\t        self.write_file(file_path, content)\n\t    def write_text_file(self, file_path, content):\n\t        \"\"\"\n\t        Write content to a text (.txt) file.\n", "        :param file_path: The path of the text file to be written.\n\t        :param content: The content to write to the file.\n\t        \"\"\"\n\t        if not file_path.endswith(\".txt\"):\n\t            file_path += \".txt\"\n\t        self.write_file(file_path, content)\n\t    def write_json_file(self, file_path, data):\n\t        \"\"\"\n\t        Write JSON data to a JSON (.json) file.\n\t        :param file_path: The path of the JSON file to be written.\n", "        :param data: The JSON data to write to the file.\n\t        \"\"\"\n\t        if not file_path.endswith(\".json\"):\n\t            file_path += \".json\"\n\t        json_content = json.dumps(data, indent=4)\n\t        self.write_file(file_path, json_content)\n\t    def write_yaml_file(self, file_path, data):\n\t        \"\"\"\n\t        Write YAML data to a YAML (.yaml) file.\n\t        :param file_path: The path of the YAML file to be written.\n", "        :param data: The YAML data to write to the file.\n\t        \"\"\"\n\t        if not file_path.endswith(\".yaml\"):\n\t            file_path += \".yaml\"\n\t        yaml_content = yaml.dump(data, sort_keys=False)\n\t        self.write_file(file_path, yaml_content)\n\t    def write_custom_file(self, file_path, content):\n\t        \"\"\"\n\t        Write content to a custom file with no extension.\n\t        :param file_path: The path of the custom file to be written.\n", "        :param content: The content to write to the file.\n\t        \"\"\"\n\t        self.write_file(file_path, content)\n"]}
{"filename": "airdot/helpers/gcs_helper.py", "chunked_list": ["from google.cloud import storage\n\timport pandas as pd\n\tfrom io import BytesIO, StringIO\n\tclass gcs_utils:\n\t    \"\"\"\n\t    Gcs helpers utility module. This module enables to perfrom\n\t    operations on gcs [uploading, downloading, exists check, connecting to gcs bucket]\n\t    \"\"\"\n\t    def _init_(self, gcs_uri) -> None:\n\t        \"\"\"\n", "        Default constructor for gcs_utils modules\n\t        Args:\n\t            gcs_uri (str): gcs uri string for the blob\n\t        \"\"\"\n\t        self.gcs_uri = gcs_uri\n\t        self.bucket_name, self.blob_name = self.split_gcs_path(self.gcs_uri)\n\t        self.bucket = self.connect_bucket()\n\t        self.storage_client = storage.Client()\n\t    def split_gcs_path(self, path):\n\t        \"\"\"\n", "        Splits the gcs path into bucket name and blob uri string.\n\t        Args:\n\t            path (str): gcs uri string for the blob\n\t        Returns:\n\t            str : it returns two values for bucket name and blob name\n\t        \"\"\"\n\t        bucket_name = path.split(\"//\")[1].split(\"/\")[0]\n\t        blob_name = \"/\".join(path.split(\"//\")[1].split(\"/\")[1:])\n\t        return bucket_name, blob_name\n\t    def connect_bucket(self):\n", "        \"\"\"\n\t        Connects to gcs bucket\n\t        Returns:\n\t            gcs bucket: gcs bucket connection\n\t        \"\"\"\n\t        storage_client = storage.Client()\n\t        bucket = storage_client.bucket(self.bucket_name)\n\t        return bucket\n\t    def download_as_file_bytes(self, blob_name):\n\t        \"\"\"\n", "        Downloads gcs files as bytes\n\t        Args:\n\t            gcs_uri (str): gcs uri for the blob to download\n\t        Returns:\n\t            bytes: data of the blob at gcs uri\n\t        \"\"\"\n\t        blob = self.bucket.get_blob(blob_name)\n\t        data = blob.download_as_bytes()\n\t        return BytesIO(data)\n\t    def check_gcs_blob_existence(self, gcs_uri):\n", "        \"\"\"\n\t        checks if the blob exists at gcs uri\n\t        Args:\n\t            gcs_uri (str): gcs uri for the blob\n\t        Returns:\n\t            bool: True if Exists otherwise false\n\t        \"\"\"\n\t        _, blob_name = self.split_gcs_path(gcs_uri)\n\t        return storage.Blob(bucket=self.bucket, name=blob_name).exists(\n\t            self.storage_client\n", "        )\n\t    def file_name_filter(self, name, blob_name):\n\t        \"\"\"\n\t        checks if blob name starts with name\n\t        Args:\n\t            name (str):name string that needs to be checked\n\t            blob_name (str): string that needs to be present in name\n\t        Returns:\n\t            bool: True if blob name starts with name otherwise False\n\t        \"\"\"\n", "        return name.startswith(blob_name)\n\t    def get_file_list(self):\n\t        \"\"\"\n\t        returns list of files inside a bucket\n\t        Returns:\n\t            list: returns list of files name that starts with blob names in bucket.\n\t        \"\"\"\n\t        blobs = self.storage_client.list_blobs(self.bucket_name)\n\t        files = [\n\t            blob.name\n", "            for blob in blobs\n\t            if self.file_name_filter(blob.name, self.blob_name)\n\t        ]\n\t        return sorted(files)\n\t    def download_file_as_string(self, blob_name):\n\t        \"\"\"\n\t        Downloads gcs files as string\n\t        Args:\n\t            gcs_uri (str): gcs uri for the blob to download\n\t        Returns:\n", "            str: data of the blob at gcs uri\n\t        \"\"\"\n\t        blob = self.bucket.get_blob(blob_name)\n\t        file = blob.download_as_string()\n\t        return StringIO(file)\n\t    def put_objects(self, data, gcs_uri):\n\t        \"\"\"\n\t        Uploads file to desired gcs_uri\n\t        Args:\n\t            data (object): data that needs to be uploaded.\n", "            gcs_uri (str): gcs uri at which DataFrame needs to be dumped.\n\t        \"\"\"\n\t        bucket_name, blob_name = self.split_gcs_path(gcs_uri)\n\t        storage_client = storage.Client()\n\t        bucket = storage_client.bucket(bucket_name)\n\t        blob = bucket.blob(blob_name)\n\t        blob.upload_from_string(data)\n\t        return True\n"]}
{"filename": "airdot/helpers/s2i_helper.py", "chunked_list": ["import subprocess\n\tdef get_s2i_environment(name):\n\t    contents = [\n\t        f\"MODEL_NAME={name}\",\n\t        \"API_TYPE=REST\",\n\t        \"SERVICE_TYPE=MODEL\",\n\t        \"PERSISTENCE=0\",\n\t    ]\n\t    return \"\\n\".join(contents)\n\tclass s2i_python_helper:\n", "    def __init__(self, base_image, builder_image):\n\t        self.base_image = base_image\n\t        self.builder_image = builder_image\n\t    def build_image(self, source_path):\n\t        \"\"\"\n\t        Build a container image using S2I.\n\t        :param source_path: The path to the source code directory.\n\t        :param image_name: The name of the container image to be built.\n\t        \"\"\"\n\t        command = [\n", "            \"s2i\",\n\t            \"build\",\n\t            source_path,\n\t            self.base_image,\n\t            self.builder_image,\n\t        ]\n\t        subprocess.run(command, check=True)\n\t    def build_and_push_image(\n\t        self, source_path, registry_url=None, username=None, password=None\n\t    ):\n", "        \"\"\"\n\t        Build a container image using S2I and push it to a container registry.\n\t        :param source_path: The path to the source code directory.\n\t        :param image_name: The name of the container image to be built.\n\t        :param registry_url: The URL of the container registry to push the image to.\n\t        :param username: Optional username for the container registry.\n\t        :param password: Optional password for the container registry.\n\t        \"\"\"\n\t        self.build_image(source_path)\n\t        if username and password:\n", "            docker_login_command = [\n\t                \"docker\",\n\t                \"login\",\n\t                registry_url,\n\t                \"--username\",\n\t                username,\n\t                \"--password\",\n\t                password,\n\t            ]\n\t            subprocess.run(docker_login_command, check=True)\n", "        # docker_tag_command = [\n\t        #     \"docker\",\n\t        #     \"tag\",\n\t        #     f\"{self.builder_image}\",\n\t        # ]\n\t        # subprocess.run(docker_tag_command, check=True)\n\t        docker_push_command = [\"docker\", \"push\", f\"{self.builder_image}\"]\n\t        subprocess.run(docker_push_command, check=True)\n"]}
{"filename": "airdot/data_models/seldon_model.py", "chunked_list": ["from pydantic import BaseModel\n\tfrom typing_extensions import Literal\n\tfrom typing import Optional\n\tclass SeldonMetadata(BaseModel):\n\t    labels: Optional[dict] = {\"app\": \"seldon\"}\n\t    name: Optional[str] = \"seldon_app\"\n\tclass SeldonAnnotations(BaseModel):\n\t    project_name: Optional[str] = \"seldon_model\"\n\t    deployment_version: Optional[str] = \"0\"\n\tclass SeldonContainer(BaseModel):\n", "    name: str\n\t    imagePullPolicy: Optional[str] = \"IfNotPresent\"\n\t    resources: Optional[dict] = {\"requests\": {\"cpu\": \"1\", \"memory\": \"1M\"}}\n\tclass SeldonPodSpecs(BaseModel):\n\t    nodeSepector: Optional[dict]\n\t    containers: Optional[SeldonContainer] = {\n\t        \"name\": \"seldon_container\",\n\t        \"image\": \"None\",\n\t        \"imagePullPolicy\": \"IfNotPresent\",\n\t        \"resources\": {\"requests\": {\"cpu\": \"1\", \"memory\": \"1M\"}},\n", "    }\n\tclass SeldonComponentSpecs(BaseModel):\n\t    specs: SeldonPodSpecs\n\tclass SeldonSpecs(BaseModel):\n\t    annotations: Optional[SeldonAnnotations]\n\t    name: Optional[str] = \"seldon_model\"\n\t    predictors: Optional[SeldonComponentSpecs]\n\tclass SeldonGraph(BaseModel):\n\t    name: Optional[str] = {\"name\": \"seldon_model\"}\n\tclass SeldonConfiguration(BaseModel):\n", "    apiVersion: Optional[\n\t        Literal[\"machinelearning.seldon.io/v1\", \"machinelearning.seldon.io/v1alpha2\"]\n\t    ] = \"machinelearning.seldon.io/v1\"\n\t    kind: Optional[str] = \"SeldonDeployment\"\n\t    metadata: Optional[SeldonMetadata] = {\"name\": \"seldon_model\"}\n\t    specs: Optional[SeldonSpecs] = {\n\t        \"specs\":{\n\t            \"name\": \"None\",\n\t            \"predictors\": {\n\t                \"componentSpecs\": {\n", "                    \"specs\": {\n\t                        \"container\": {\n\t                            \"name\": \"None\",\n\t                            \"image\": \"None\",\n\t                            \"imagePullPolicy\": \"ifNotPresent\",\n\t                            \"resources\": {\"requests\": {\"cpu\": \"1\", \"memory\": \"1M\"}},\n\t                        }\n\t                    }\n\t                }\n\t            },\n", "        }\n\t    }\n\t    graph: Optional[SeldonGraph] = {\"name\": \"None\"}\n\t    name: Optional[str] = \"default\"\n\t    replicas: Optional[int] = 1\n"]}
{"filename": "airdot/data_models/kserve_model.py", "chunked_list": ["from pydantic import BaseModel\n\tfrom typing_extensions import Literal\n\tclass KServe(BaseModel):\n\t    pass\n"]}
{"filename": "airdot/data_models/__init__.py", "chunked_list": []}
{"filename": "airdot/data_models/deployment.py", "chunked_list": ["from pydantic import BaseModel\n\tfrom typing_extensions import Literal\n\tfrom typing import Optional\n\tfrom airdot.data_models.seldon_model import SeldonConfiguration\n\tfrom airdot.data_models.kserve_model import KServe\n\tclass Deployment(BaseModel):\n\t    deployment_type: Literal[\"seldon\", \"kserve\", \"local\"]\n\t    bucket_type: Optional[str] = None,\n\t    image_uri: Optional[str] = None\n\t    seldon_configuration: Optional[SeldonConfiguration] = {\n", "                \"apiVersion\": \"None\",\n\t                \"kind\": \"SeldonDeployment\",\n\t                \"metadata\": {\n\t                    \"name\": \"None\",\n\t                    \"namespace\":\"None\"\n\t                },\n\t                \"spec\": {\n\t                    \"name\": \"seldon-test\",\n\t                    \"predictors\": [{\n\t                    \"componentSpecs\":[{\n", "                        \"spec\": {\n\t                        \"containers\": [{\n\t                            \"name\": \"None\",\n\t                            \"image\": \"None\",\n\t                            \"imagePullPolicy\": \"Always\",\n\t                            \"resources\": {\n\t                            \"requests\": {\n\t                                \"cpu\": \"1\",\n\t                                \"memory\": \"1M\"\n\t                            }\n", "                            }\n\t                        }]\n\t                        }\n\t                    }],\n\t                    \"graph\":{\n\t                        \"children\":[],\n\t                        \"name\":\"None\",\n\t                        \"endpoint\": {\n\t                            \"type\":\"REST\"\n\t                        },\n", "                        \"type\": \"MODEL\"\n\t                    },\n\t                    \"name\":\"seldon-test\",\n\t                    \"replicas\":1\n\t                    }]\n\t                }\n\t            }\n\t    kserve_configuration: Optional[KServe]\n"]}
{"filename": "tests/deployer_test.py", "chunked_list": ["from airdot import Deployer\n\timport pandas as pd\n\tdeployer = Deployer() \n\t# declare a function\n\tdf2 = pd.DataFrame(data=[[10,20],[10,40]], columns=['1', '2'])\n\t# def func_one(value):\n\t#     return value\n\t# def func_two(value):\n\t#     return func_one(value)\n\tdef get_value_data(cl_idx='1'):\n", "    return df2[cl_idx].values.tolist()\n\tdeployer.run(get_value_data) # to deploy local\n\t# # \n\t# deployer.list_deployments() # to list all deployments\n\t# df2 = pd.DataFrame(data=[[3,4],[7,8]], columns=['1', '2'])\n\t# deployer.update_objects(('df2',df2), 'get_value_data')\n\t# #deployer.stop('get_value_data') # to stop container"]}
{"filename": "tests/__init__.py", "chunked_list": ["from tests.test_files.example_1 import func_4\n\t__all__ = [\"func_4\"]\n"]}
{"filename": "tests/test_files/__init__.py", "chunked_list": []}
{"filename": "tests/test_files/example_1.py", "chunked_list": ["import sys\n\tdef func_1(val):\n\t    print(val)\n\tdef func_2(val):\n\t    func_1(val)\n\tdef func_3(val):\n\t    func_2(val)\n\tdef func_4(val=4):\n\t    return func_3(val)\n\tif __name__ == \"__main__\":\n", "    func_4()\n"]}
