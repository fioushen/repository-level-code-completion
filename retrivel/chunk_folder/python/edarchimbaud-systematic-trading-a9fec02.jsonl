{"filename": "systematic_trading/helpers.py", "chunked_list": ["import json\n\timport subprocess\n\timport time\n\timport requests\n\tfrom requests.exceptions import ConnectionError, HTTPError, ReadTimeout\n\tfrom requests.models import Response\n\tdef is_valid_json(data):\n\t    try:\n\t        json.loads(data)\n\t        return True\n", "    except json.JSONDecodeError:\n\t        return False\n\tdef nasdaq_headers():\n\t    return {\n\t        \"authority\": \"api.nasdaq.com\",\n\t        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n\t        \"accept-language\": \"en-US,en;q=0.5\",\n\t        \"cache-control\": \"max-age=0\",\n\t        \"sec-ch-ua\": '\"Brave\";v=\"113\", \"Chromium\";v=\"113\", \"Not-A.Brand\";v=\"24\"',\n\t        \"sec-ch-ua-mobile\": \"?0\",\n", "        \"sec-ch-ua-platform\": '\"macOS\"',\n\t        \"sec-fetch-dest\": \"document\",\n\t        \"sec-fetch-mode\": \"navigate\",\n\t        \"sec-fetch-site\": \"none\",\n\t        \"sec-fetch-user\": \"?1\",\n\t        \"sec-gpc\": \"1\",\n\t        \"upgrade-insecure-requests\": \"1\",\n\t        \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36\",\n\t    }\n\tdef retry_get(\n", "    url,\n\t    headers={\n\t        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\",\n\t    },\n\t    retries=10,\n\t    delay=300,\n\t    mode=\"default\",\n\t):\n\t    if mode == \"curl\":\n\t        curl_headers = []\n", "        for k, v in headers.items():\n\t            curl_headers += [\"-H\", f\"{k}: {v}\"]\n\t        curl_command = [\n\t            \"curl\",\n\t            url,\n\t            *curl_headers,\n\t            \"--compressed\",\n\t        ]\n\t        for _ in range(retries):\n\t            result = subprocess.run(curl_command, capture_output=True, text=True)\n", "            content = result.stdout\n\t            if result.returncode == 0 and is_valid_json(content):\n\t                response = Response()\n\t                response.status_code = 200\n\t                response._content = content.encode(\"utf-8\")\n\t                return response\n\t            else:\n\t                print(f\"Connection error with {url}. Retrying in {delay} seconds...\")\n\t                time.sleep(delay)\n\t        raise ConnectionError(f\"Failed to connect to {url} after {retries} retries\")\n", "    for _ in range(retries):\n\t        try:\n\t            response = requests.get(url, headers=headers, timeout=10)\n\t            response.raise_for_status()  # Raise an exception for 4xx/5xx status codes\n\t            return response\n\t        except (ConnectionError, HTTPError, ReadTimeout):\n\t            print(f\"Connection error with {url}. Retrying in {delay} seconds...\")\n\t            time.sleep(delay)\n\t    raise ConnectionError(f\"Failed to connect to {url} after {retries} retries\")\n"]}
{"filename": "systematic_trading/strategies/momentum.py", "chunked_list": ["from datetime import datetime\n\tfrom dateutil import relativedelta\n\timport os\n\timport backtrader as bt\n\timport backtrader.feeds as btfeeds\n\tfrom datasets import load_dataset\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport pandas as pd\n\timport pickle\n", "from tqdm import tqdm\n\tclass MomentumStrategy(bt.Strategy):\n\t    \"\"\"\n\t    A momentum strategy that goes long the top quantile of stocks\n\t    and short the bottom quantile of stocks.\n\t    \"\"\"\n\t    params = (\n\t        (\"long_quantile\", 0.8),  # Long quantile threshold (e.g., top 80%)\n\t        (\"short_quantile\", 0.2),  # Short quantile threshold (e.g., bottom 20%)\n\t    )\n", "    def __init__(self):\n\t        self.ret = np.zeros(len(self.datas))\n\t    def log(self, txt, dt=None):\n\t        dt = dt or self.datas[0].datetime.date(0)\n\t        print(f\"{dt.isoformat()},{txt}\")\n\t    def is_first_business_day(self, dt=None):\n\t        dt = dt or self.datas[0].datetime.date(0)\n\t        first_day = dt.replace(day=1)\n\t        first_business_day = pd.date_range(first_day, periods=1, freq=\"BMS\")[0]\n\t        return dt == first_business_day.date()\n", "    def next(self):\n\t        \"\"\"\n\t        Execute trades based on the momentum strategy.\n\t        \"\"\"\n\t        if not self.is_first_business_day():\n\t            return\n\t        # Calculate returns for all stocks\n\t        self.ret = np.array(\n\t            [\n\t                (d.close[-20] / d.close[-252] - 1 if len(d) > 252 else np.NaN)\n", "                for d in self.datas\n\t            ]\n\t        )\n\t        self.log(self.broker.getvalue())\n\t        # Count the number of stocks that have a valid momentum predictor\n\t        num_stocks = np.count_nonzero(~np.isnan(self.ret))\n\t        # Compute the quantile thresholds\n\t        long_threshold = np.nanquantile(self.ret, self.params.long_quantile)\n\t        short_threshold = np.nanquantile(self.ret, self.params.short_quantile)\n\t        for i, d in enumerate(self.datas):\n", "            if self.ret[i] > long_threshold:  # Long the top quantile stocks\n\t                self.order_target_percent(\n\t                    data=d,\n\t                    target=0.7 / num_stocks,\n\t                )\n\t            elif self.ret[i] < short_threshold:  # Short the bottom quantile stocks\n\t                self.order_target_percent(\n\t                    data=d,\n\t                    target=-0.7 / num_stocks,\n\t                )\n", "            else:  # Close positions that don't meet the long or short criteria\n\t                self.close(data=d)\n\tclass CashNav(bt.analyzers.Analyzer):\n\t    \"\"\"\n\t    Analyzer returning cash and market values\n\t    \"\"\"\n\t    def create_analysis(self):\n\t        self.rets = {}\n\t        self.vals = 0.0\n\t    def notify_cashvalue(self, cash, value):\n", "        self.vals = (\n\t            self.strategy.datetime.datetime(),\n\t            cash,\n\t            value,\n\t        )\n\t        self.rets[len(self)] = self.vals\n\t    def get_analysis(self):\n\t        return self.rets\n\tdef main():\n\t    # Load Data\n", "    path = os.path.join(\"/tmp\", \"momentum.pkl\")\n\t    if os.path.exists(path):\n\t        df = pickle.load(open(path, \"rb\"))\n\t    else:\n\t        dataset = load_dataset(\"edarchimbaud/timeseries-daily-sp500\", split=\"train\")\n\t        df = pd.DataFrame(dataset)\n\t        pickle.dump(df, open(path, \"wb\"))\n\t    # Data Preparation\n\t    symbols = df[\"symbol\"].unique()\n\t    df[\"date\"] = pd.to_datetime(df[\"date\"])\n", "    # Create a Cerebro object\n\t    cerebro = bt.Cerebro()\n\t    cerebro.broker.set_cash(1000000)\n\t    starting_date = datetime(1990, 1, 1)\n\t    # Add Data Feeds to Cerebro\n\t    for _, symbol in enumerate(tqdm(symbols)):\n\t        df_symbol = df[df[\"symbol\"] == symbol].copy()\n\t        if df_symbol[\"date\"].min() > starting_date:\n\t            continue\n\t        factor = df_symbol[\"adj_close\"] / df_symbol[\"close\"]\n", "        df_symbol[\"open\"] = df_symbol[\"open\"] * factor\n\t        df_symbol[\"high\"] = df_symbol[\"high\"] * factor\n\t        df_symbol[\"low\"] = df_symbol[\"low\"] * factor\n\t        df_symbol[\"close\"] = df_symbol[\"close\"] * factor\n\t        df_symbol.drop([\"symbol\", \"adj_close\"], axis=1, inplace=True)\n\t        df_symbol.set_index(\"date\", inplace=True)\n\t        data = btfeeds.PandasData(dataname=df_symbol)\n\t        cerebro.adddata(data, name=symbol)\n\t    # Add Strategy to Cerebro\n\t    cerebro.addstrategy(\n", "        MomentumStrategy, long_quantile=0.8, short_quantile=0.2\n\t    )  # Adjust parameters as desired\n\t    cerebro.addanalyzer(CashNav, _name=\"cash_nav\")\n\t    # Run the Strategy\n\t    results = cerebro.run()\n\t    print(\"Final Portfolio Value: %.2f\" % cerebro.broker.getvalue())\n\t    dictionary = results[0].analyzers.getbyname(\"cash_nav\").get_analysis()\n\t    df = pd.DataFrame(dictionary).T\n\t    df.columns = [\"Date\", \"Cash\", \"Nav\"]\n\t    df.set_index(\"Date\", inplace=True)\n", "    df.loc[df.index >= starting_date, [\"Nav\"]].plot()\n\t    plt.show()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "systematic_trading/features/__main__.py", "chunked_list": ["from datetime import date, timedelta\n\timport click\n\tfrom systematic_trading.features.predictors.predictors_monthly import PredictorsMonthly\n\tfrom systematic_trading.features.targets.targets_monthly import TargetsMonthly\n\t@click.command()\n\t@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\n\tdef main(suffix: str, username: str):\n\t    tag_date = date.today() - timedelta(days=3)\n\t    print(\"Updating feature and target datasets...\")\n\t    features = {\n", "        \"predictors-monthly-stocks\": PredictorsMonthly(\n\t            suffix=\"stocks\", tag_date=tag_date, username=username\n\t        ),\n\t        \"targets-monthly-stocks\": TargetsMonthly(\n\t            suffix=\"stocks\", tag_date=tag_date, username=username\n\t        ),\n\t    }\n\t    for name in features:\n\t        features[name].set_dataset_df()\n\t        features[name].to_hf_datasets()\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "systematic_trading/features/targets/targets_monthly.py", "chunked_list": ["from datetime import date\n\tfrom datasets import load_dataset\n\timport numpy as np\n\timport pandas as pd\n\tfrom systematic_trading.datasets.dataset import Dataset\n\tclass TargetsMonthly(Dataset):\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"targets-monthly-{self.suffix}\"\n\t        self.expected_columns = [\"symbol\", \"date\", \"return\", \"return_quintile\"]\n", "    def __return_to_quintile(self, returns_arr):\n\t        # I am allowed to use the future to calculate the percentiles\n\t        percentiles = np.percentile(returns_arr, [20, 40, 60, 80])\n\t        quintile_id = []\n\t        for ret in returns_arr:\n\t            if ret <= percentiles[0]:\n\t                quintile_id.append(-2)\n\t            elif ret <= percentiles[1]:\n\t                quintile_id.append(-1)\n\t            elif ret <= percentiles[2]:\n", "                quintile_id.append(0)\n\t            elif ret <= percentiles[3]:\n\t                quintile_id.append(1)\n\t            else:\n\t                quintile_id.append(2)\n\t        return quintile_id\n\t    def set_dataset_df(self):\n\t        \"\"\"\n\t        Compute the dataset.\n\t        \"\"\"\n", "        timeseries_daily_df = pd.DataFrame(\n\t            load_dataset(\n\t                f\"{self.username}/timeseries-daily-{self.suffix}\",\n\t                revision=self.tag_date.isoformat(),\n\t                split=\"train\",\n\t            ),\n\t        )\n\t        timeseries_daily_df[\"date\"] = pd.to_datetime(timeseries_daily_df[\"date\"])\n\t        timeseries_daily_df.set_index(\"date\", inplace=True)\n\t        # Cross-sectional returns\n", "        monthly_df = (\n\t            timeseries_daily_df.groupby(\"symbol\")[\"close\"]\n\t            .resample(\"M\")\n\t            .last()\n\t            .pct_change()\n\t            .shift(-1)\n\t        )\n\t        monthly_df = monthly_df.reset_index(level=[\"symbol\", \"date\"]).dropna()\n\t        monthly_df.rename(columns={\"close\": \"return\"}, inplace=True)\n\t        monthly_df[\"return_quintile\"] = monthly_df.groupby(\"date\")[\"return\"].transform(\n", "            lambda x: pd.qcut(x, 5, labels=False)\n\t        )\n\t        monthly_df.reset_index(drop=True, inplace=True)\n\t        self.dataset_df = monthly_df\n"]}
{"filename": "systematic_trading/features/predictors/__main__.py", "chunked_list": ["from datetime import date, timedelta\n\timport click\n\tfrom systematic_trading.features.predictors.predictors_monthly import PredictorsMonthly\n\tfrom systematic_trading.features.targets.targets_monthly import TargetsMonthly\n\t@click.command()\n\t@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\n\tdef main(suffix: str, username: str):\n\t    tag_date = date.today() - timedelta(days=3)\n\t    print(\"Updating feature and target datasets...\")\n\t    features = {\n", "        \"predictors-monthly-stocks\": PredictorsMonthly(\n\t            suffix=\"stocks\", tag_date=tag_date, username=username\n\t        ),\n\t        \"targets-monthly-stocks\": TargetsMonthly(\n\t            suffix=\"stocks\", tag_date=tag_date, username=username\n\t        ),\n\t    }\n\t    for name in features:\n\t        features[name].set_dataset_df()\n\t        features[name].to_hf_datasets()\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "systematic_trading/features/predictors/predictors_monthly.py", "chunked_list": ["from datetime import date\n\timport os\n\tfrom datasets import load_dataset\n\tfrom numba import jit\n\timport numpy as np\n\timport pandas as pd\n\timport pickle\n\tfrom tqdm import tqdm\n\tfrom systematic_trading.datasets.dataset import Dataset\n\tfrom systematic_trading.datasets.predictors.estimators.slope import (\n", "    bayesian_slope,\n\t    linear_regression_slope,\n\t    median_of_local_slopes,\n\t    median_of_progressive_slopes,\n\t    barycentre_of_progressive_slopes,\n\t)\n\tclass SignalsMonthly(Dataset):\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"signals-monthly-{self.suffix}\"\n", "        self.expected_columns = [\n\t            \"symbol\",\n\t            \"date\",\n\t            \"bayesian_slope_12m\",\n\t            \"linear_regression_slope_12m\",\n\t            \"median_of_progressive_slopes_12m\",\n\t            \"median_of_local_slopes_12m\",\n\t            \"barycentre_of_progressive_slopes_12m\",\n\t            \"bayesian_slope_12m_quintile\",\n\t            \"linear_regression_slope_12m_quintile\",\n", "            \"median_of_progressive_slopes_12m_quintile\",\n\t            \"median_of_local_slopes_12m_quintile\",\n\t            \"barycentre_of_progressive_slopes_12m_quintile\",\n\t        ]\n\t    def set_dataset_df(self):\n\t        path = os.path.join(os.getenv(\"HOME\"), \"Downloads\", \"timeseries_daily_df.pkl\")\n\t        if os.path.exists(path):\n\t            with open(path, \"rb\") as handler:\n\t                timeseries_daily_df = pickle.load(handler)\n\t        else:\n", "            timeseries_daily_df = pd.DataFrame(\n\t                load_dataset(\n\t                    f\"{self.username}/timeseries-daily-{self.suffix}\",\n\t                    revision=self.tag_date.isoformat(),\n\t                    split=\"train\",\n\t                ),\n\t            )\n\t            with open(path, \"wb\") as handler:\n\t                pickle.dump(timeseries_daily_df, handler)\n\t        timeseries_daily_df[\"date\"] = pd.to_datetime(timeseries_daily_df[\"date\"])\n", "        timeseries_daily_df.set_index(\"date\", inplace=True)\n\t        BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M = \"barycentre_of_progressive_slopes_12m\"\n\t        signals = {\n\t            \"bayesian_slope_12m\": bayesian_slope,\n\t            \"linear_regression_slope_12m\": linear_regression_slope,\n\t            \"median_of_progressive_slopes_12m\": median_of_progressive_slopes,\n\t            \"median_of_local_slopes_12m\": median_of_local_slopes,\n\t            BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M: \"custom\",\n\t        }\n\t        frames = []\n", "        for signal_name, signal in tqdm(signals.items()):\n\t            if signal != \"custom\":\n\t                monthly_df = (\n\t                    timeseries_daily_df.groupby(\"symbol\")[\"close\"]\n\t                    .resample(\"M\")\n\t                    .last()\n\t                    .rolling(window=12)\n\t                    .apply(signal)\n\t                )\n\t                monthly_df.rename(signal_name, inplace=True)\n", "            elif signal_name == BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M:\n\t                monthly_close_df = (\n\t                    timeseries_daily_df.groupby(\"symbol\")[\"close\"].resample(\"M\").last()\n\t                )\n\t                monthly_volume_df = (\n\t                    timeseries_daily_df.groupby(\"symbol\")[\"volume\"].resample(\"M\").sum()\n\t                )\n\t                monthly_df = pd.concat([monthly_close_df, monthly_volume_df], axis=1)\n\t                monthly_df = monthly_df.rolling(window=12, method=\"table\").apply(\n\t                    barycentre_of_progressive_slopes,\n", "                    raw=True,\n\t                    engine=\"numba\",\n\t                )[[\"close\"]]\n\t                monthly_df.rename(\n\t                    columns={\"close\": BARYCENTRE_OF_PROGRESSIVE_SLOPES_12M},\n\t                    inplace=True,\n\t                )\n\t            frames.append(monthly_df)\n\t        monthly_df = pd.concat(frames, axis=1)\n\t        monthly_df = monthly_df.reset_index(level=[\"symbol\", \"date\"]).dropna()\n", "        for signal_name in signals.keys():\n\t            monthly_df[f\"{signal_name}_quintile\"] = monthly_df.groupby(\"date\")[\n\t                signal_name\n\t            ].transform(lambda x: pd.qcut(x, 5, labels=False))\n\t        monthly_df.reset_index(drop=True, inplace=True)\n\t        self.dataset_df = monthly_df\n"]}
{"filename": "systematic_trading/features/predictors/estimators/slope.py", "chunked_list": ["from numba import jit\n\timport numpy as np\n\tdef bayesian_slope(x) -> float:\n\t    \"\"\"\n\t    Bayesian slope: (S(12)-S(1)) / S(1)\n\t    \"\"\"\n\t    if len(x) < 12:\n\t        return np.NaN\n\t    else:\n\t        return (x[-1] - x[0]) / 12 / np.median(x)\n", "def linear_regression_slope(x) -> float:\n\t    \"\"\"\n\t    Slope of the linear regression: slope(close(1), ..., close(12))\n\t    \"\"\"\n\t    if len(x) < 12:\n\t        return np.NaN\n\t    else:\n\t        return np.polyfit(range(12), x, 1)[0] / np.median(x)\n\tdef median_of_local_slopes(x) -> float:\n\t    \"\"\"\n", "    Median of local slopes: median(close(12)-close(11), close(11)-close(10), ..., close(2)-close(1))\n\t    \"\"\"\n\t    if len(x) < 12:\n\t        return np.NaN\n\t    else:\n\t        return np.median(np.diff(x)) / np.median(x)\n\tdef median_of_progressive_slopes(x) -> float:\n\t    \"\"\"\n\t    Median of progressive slopes: median(close(2)-close(1), (close(3)-close(1)) / 2, ..., (close(12)-close(1)) / 11)\n\t    \"\"\"\n", "    if len(x) < 12:\n\t        return np.NaN\n\t    else:\n\t        return np.median((x[1:] - x[0]) / np.arange(1, 12)) / np.median(x)\n\t@jit(nopython=True)\n\tdef barycentre_of_progressive_slopes(x) -> float:\n\t    \"\"\"\n\t    Barycentre of progressive slopes: sum(volume(n)*(close(n)-close(1)/n)/sum(volume(n))\n\t    \"\"\"\n\t    if len(x) < 12:\n", "        return np.NaN\n\t    else:\n\t        returns = (x[1:, 0] - x[0, 0]) / np.arange(1, 12) / np.median(x[:, 0])\n\t        volumes = x[1:, 1]\n\t        return np.sum(returns / volumes) / np.sum(1 / volumes)\n"]}
{"filename": "systematic_trading/models/momentum.py", "chunked_list": ["import click\n\tfrom datasets import load_dataset\n\timport ffn\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport pandas as pd\n\timport seaborn as sns\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.ensemble import RandomForestClassifier\n\tfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n", "from sklearn.metrics import confusion_matrix\n\tdef train_test_split_v2(X, y, dt=None):\n\t    if dt is None:\n\t        return train_test_split(X, y, test_size=0.5, random_state=42)\n\t    X_train = X.index.get_level_values(\"date\") < dt\n\t    y_train = y.index.get_level_values(\"date\") < dt\n\t    return X.loc[X_train, :], X.loc[~X_train, :], y.loc[y_train, :], y.loc[~y_train, :]\n\t@click.command()\n\t@click.option(\n\t    \"--test_start_date\", help=\"Starting date of the out of sample (yyyy-mm-dd)\"\n", ")\n\tdef main(test_start_date: str):\n\t    predictors_df = pd.DataFrame(\n\t        load_dataset(\"edarchimbaud/signals-monthly-sp500\", split=\"train\")\n\t    )\n\t    targets_df = pd.DataFrame(\n\t        load_dataset(\"edarchimbaud/targets-monthly-sp500\", split=\"train\")\n\t    )\n\t    print(predictors_df.iloc[0, :])\n\t    print(targets_df.iloc[0, :])\n", "    # merging the two dataframes on the 'symbol' and 'date' columns\n\t    merged_df = pd.merge(predictors_df, targets_df, on=[\"symbol\", \"date\"])\n\t    merged_df.set_index([\"symbol\", \"date\"], inplace=True)\n\t    # filter the merged dataframe to include only \"*quintile\" columns from the first dataset\n\t    quintile_cols = [\n\t        col\n\t        for col in merged_df.columns\n\t        if \"quintile\" in col and col != \"return_quintile\"\n\t    ]\n\t    X = merged_df[quintile_cols]\n", "    # target variable is 'return_quintile' from the second dataset\n\t    y = merged_df[[\"return_quintile\", \"return\"]]\n\t    # split the data into training and test sets\n\t    X_train, X_test, y_train, y_test = train_test_split_v2(X, y, dt=test_start_date)\n\t    # use a random forest classifier\n\t    clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\t    clf.fit(X_train, y_train.return_quintile)\n\t    # Predict the 'return_quintile' for the test set\n\t    y_pred = clf.predict(X_test)\n\t    # Calculate metrics\n", "    accuracy = accuracy_score(y_test.return_quintile, y_pred)\n\t    precision = precision_score(\n\t        y_test.return_quintile, y_pred, average=\"weighted\"\n\t    )  # you can choose another averaging method if you prefer\n\t    recall = recall_score(y_test.return_quintile, y_pred, average=\"weighted\")\n\t    f1 = f1_score(y_test.return_quintile, y_pred, average=\"weighted\")\n\t    # Print metrics\n\t    print(f\"Accuracy: {accuracy}\")\n\t    print(f\"Precision: {precision}\")\n\t    print(f\"Recall: {recall}\")\n", "    print(f\"F1 Score: {f1}\")\n\t    # Confusion matrix\n\t    cm = confusion_matrix(y_test.return_quintile, y_pred)\n\t    plt.figure(figsize=(10, 7))\n\t    sns.heatmap(cm, annot=True, fmt=\"d\")\n\t    plt.xlabel(\"Predicted\")\n\t    plt.ylabel(\"Truth\")\n\t    plt.show()\n\t    # Plot cumulative returns\n\t    fig = plt.figure(figsize=(10, 7))\n", "    for n in range(5):\n\t        y_test.loc[y_pred == n, \"return\"].groupby(\"date\").mean().cumsum().plot()\n\t    plt.legend([f\"Predicted {n}\" for n in range(5)])\n\t    plt.show()\n\t    # Plot cumulative returns for long and short positions\n\t    fig = plt.figure(figsize=(10, 7))\n\t    df = pd.concat(\n\t        [\n\t            y_test.loc[y_pred == 4, \"return\"].groupby(\"date\").mean(),\n\t            y_test.loc[y_pred == 1, \"return\"].groupby(\"date\").mean(),\n", "        ],\n\t        axis=1,\n\t        keys=[\"long\", \"short\"],\n\t    )\n\t    df = df.fillna(0)\n\t    df[\"return\"] = df[\"long\"] - df[\"short\"]\n\t    df[\"return\"].cumsum().plot()\n\t    plt.show()\n\t    # Display performance metrics\n\t    nav = (df[\"return\"] + 1).cumprod()\n", "    stats = nav.calc_stats()\n\t    stats.display()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_abstract_crawler.py", "chunked_list": ["from collections import Counter\n\timport os\n\timport re\n\timport time\n\tfrom typing import Optional\n\tfrom bs4 import BeautifulSoup\n\timport requests\n\tfrom selenium import webdriver\n\tfrom selenium.webdriver.chrome.options import Options\n\timport textract\n", "from tqdm import tqdm\n\tfrom webdriver_manager.chrome import ChromeDriverManager\n\tfrom systematic_trading.strategy_ideas.ssrn_abstract import SsrnAbstract\n\tclass SsrnAbstractCrawler:\n\t    def __init__(\n\t        self,\n\t        kili_project_id: Optional[str] = None,\n\t        is_strategy: Optional[str] = None,\n\t    ):\n\t        self.kili_project_id = kili_project_id\n", "        self.is_strategy = is_strategy\n\t        self._driver = None\n\t    def __from_url(self, url: str):\n\t        return int(url.split(\"=\")[-1])\n\t    def __download_and_save_to_kili(self, abstract_ids: list):\n\t        for abstract_id in tqdm(abstract_ids):\n\t            abstract = SsrnAbstract(abstract_id)\n\t            if (\n\t                abstract.exists_in_kili(self.kili_project_id)\n\t                or not abstract.exists_in_ssrn()\n", "            ):\n\t                continue\n\t            abstract.from_ssrn()\n\t            if self.is_strategy is not None:\n\t                abstract.is_strategy = self.is_strategy\n\t            abstract.to_kili(self.kili_project_id)\n\t    def __go_to_page(self, page: int):\n\t        self._driver.find_element(\n\t            \"xpath\", '//input[@aria-label=\"Go to page\"]'\n\t        ).send_keys(page)\n", "        self._driver.find_element(\"xpath\", '//button[@aria-label=\"Go\"]').click()\n\t    def from_jel_code(self, jel_code: str, from_page: int = 1):\n\t        \"\"\"\n\t        List all abstract ids from SSRN\n\t        \"\"\"\n\t        headers = {\n\t            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n\t        }\n\t        options = Options()\n\t        options.headless = True\n", "        options.add_argument(\"user-agent=\" + headers[\"User-Agent\"])\n\t        self._driver = webdriver.Chrome(\n\t            ChromeDriverManager().install(), options=options\n\t        )\n\t        self._driver.get(\"https://papers.ssrn.com/sol3/DisplayAbstractSearch.cfm\")\n\t        self._driver.find_element_by_id(\"onetrust-accept-btn-handler\").click()\n\t        self._driver.find_element_by_id(\"advanced_search\").send_keys(jel_code)\n\t        self._driver.find_element_by_id(\"srchCrit2\").find_element_by_xpath(\"..\").click()\n\t        self._driver.find_element(\n\t            \"xpath\", '//button[contains(@class, \"primary\")]'\n", "        ).click()\n\t        for page in range(from_page, 200 + 1):\n\t            self.__go_to_page(page)\n\t            print(f\"{page} / 200\")\n\t            body = self._driver.find_element(\"xpath\", \"//body\")\n\t            body_html = body.get_attribute(\"innerHTML\")\n\t            soup = BeautifulSoup(body_html, \"html.parser\")\n\t            a_tags = soup.find_all(\"a\", {\"class\": \"title optClickTitle\"})\n\t            abstract_ids = [self.__from_url(a_tag[\"href\"]) for a_tag in a_tags]\n\t            abstract_ids = list(set(abstract_ids))\n", "            time.sleep(10)\n\t            self.__download_and_save_to_kili(abstract_ids)\n"]}
{"filename": "systematic_trading/strategy_ideas/__main__.py", "chunked_list": ["import click\n\tfrom systematic_trading.strategy_ideas.ssrn_abstract_crawler import SsrnAbstractCrawler\n\tfrom systematic_trading.strategy_ideas.ssrn_paper_crawler import SsrnPaperCrawler\n\tfrom systematic_trading.strategy_ideas.ssrn_paper_summary_crawler import (\n\t    SsrnPaperSummaryCrawler,\n\t)\n\t@click.command()\n\t@click.option(\"--mode\")\n\t@click.option(\"--kili-project-id\", help=\"Kili project id to save the data\")\n\t@click.option(\"--from-page\", default=1, help=\"Starting from 1\")\n", "@click.option(\"--jel-code\", default=\"G14\", help=\"JEL code: G14, G12, G11\")\n\t@click.option(\n\t    \"--src-kili-project-id\", default=\"\", help=\"Kili project id to read the data\"\n\t)\n\t@click.option(\n\t    \"--tgt-folder\", default=\"data/summaries\", help=\"Folder to save the summaries\"\n\t)\n\tdef main(\n\t    mode: str,\n\t    kili_project_id: str,\n", "    from_page: int,\n\t    jel_code: str,\n\t    src_kili_project_id: str,\n\t    target_folder: str,\n\t):\n\t    \"\"\"\n\t    Main entrypoint.\n\t    \"\"\"\n\t    if mode == \"abstract\":\n\t        SsrnAbstractCrawler(kili_project_id=kili_project_id).from_jel_code(\n", "            jel_code,\n\t            from_page,\n\t        )\n\t    elif mode == \"paper\":\n\t        SsrnPaperCrawler(tgt_kili_project_id=kili_project_id).from_kili(\n\t            src_kili_project_id=src_kili_project_id,\n\t        )\n\t    elif mode == \"summary\":\n\t        ssrn_paper_summarizer = SsrnPaperSummarizer()\n\t        ssrn_paper_summarizer.predict(\n", "            kili_project_id,\n\t            target_folder,\n\t        )\n\tif __name__ == \"__main__\":\n\t    main()  # pylint: disable=no-value-for-parameter\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_paper_summarizer.py", "chunked_list": ["\"\"\"\n\tSSRN Paper Summarizer.\n\t\"\"\"\n\timport os\n\timport json\n\tfrom pprint import pprint\n\tfrom kili.client import Kili\n\tfrom langchain.prompts import (\n\t    ChatPromptTemplate,\n\t    HumanMessagePromptTemplate,\n", "    SystemMessagePromptTemplate,\n\t)\n\tfrom langchain.chat_models import ChatOpenAI\n\tfrom tqdm import tqdm\n\tfrom systematic_trading.strategy_ideas.ssrn_abstract import SsrnAbstract\n\tfrom systematic_trading.strategy_ideas.ssrn_strategy import SsrnStrategy\n\tclass SsrnPaperSummarizer:\n\t    \"\"\"\n\t    SSRN Paper Summarizer.\n\t    \"\"\"\n", "    def __init__(self):\n\t        self._kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n\t        self._openai_client = ChatOpenAI(\n\t            model_name=\"gpt-3.5-turbo\",\n\t            openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n\t            temperature=0,\n\t        )\n\t    def __parse_label(self, asset):\n\t        label = asset[\"labels\"][-1][\"jsonResponse\"]\n\t        is_strategy = label[\"IS_STRATEGY\"][\"categories\"][0][\"name\"]\n", "        key_elements_annotations = label[\"KEY_ELEMENTS\"][\"annotations\"]\n\t        key_elements = []\n\t        for annotation in key_elements_annotations:\n\t            key_elements.append(\n\t                {\n\t                    \"category\": annotation[\"categories\"][0][\"name\"],\n\t                    \"content\": annotation[\"content\"],\n\t                    \"page_number\": min(annotation[\"annotations\"][0][\"pageNumberArray\"]),\n\t                }\n\t            )\n", "        key_elements = sorted(key_elements, key=lambda x: x[\"page_number\"])\n\t        aggregated_key_elements = {}\n\t        for item in key_elements:\n\t            category = item[\"category\"]\n\t            content = item[\"content\"]\n\t            if category in aggregated_key_elements:\n\t                aggregated_key_elements[category].append(content)\n\t            else:\n\t                aggregated_key_elements[category] = [content]\n\t        return aggregated_key_elements, is_strategy\n", "    def __query_chatgpt(self, instructions: str, text: str):\n\t        system_message_prompt = SystemMessagePromptTemplate.from_template(instructions)\n\t        human_template = \"{text}\"\n\t        human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n\t        chat_prompt = ChatPromptTemplate.from_messages(\n\t            [system_message_prompt, human_message_prompt]\n\t        )\n\t        response = self._openai_client(\n\t            chat_prompt.format_prompt(\n\t                text=text,\n", "            ).to_messages()\n\t        )\n\t        return response.content\n\t    def __predict_trading_rules(self, key_elements):\n\t        trading_rules = \" \".join(key_elements.get(\"TRADING_RULES\", []))\n\t        if trading_rules == \"\":\n\t            return \"\"\n\t        instructions = \"\"\"You are a helpful assistant that extract the rules of the following trading strategy as bullet points.\n\tHere is an example:\n\t- Investment universe: 54 countries' 10-year government bonds\n", "- Sort assets into quintiles based on past month return\n\t- Long top quintile assets (highest returns from previous month)\n\t- Short bottom quintile assets (lowest returns from previous month)\n\t- Utilize equal weighting for assets\n\t- Rebalance strategy on a monthly basis\"\"\"\n\t        return self.__query_chatgpt(instructions=instructions, text=trading_rules)\n\t    def __predict_backtrader(self, key_elements):\n\t        trading_rules = \" \".join(key_elements.get(\"TRADING_RULES\", []))\n\t        if trading_rules == \"\":\n\t            return \"\"\n", "        instructions = (\n\t            \"Write the python code with Backtrader for the following strategy.\"\n\t        )\n\t        return self.__query_chatgpt(instructions=instructions, text=trading_rules)\n\t    def __predict_markets_traded(self, key_elements):\n\t        markets_traded = \" \".join(key_elements.get(\"MARKETS_TRADED\", []))\n\t        if markets_traded == \"\":\n\t            return \"\"\n\t        instructions = (\n\t            \"Extract the list of markets traded.\"\n", "            \" It can be one or more of the following: equities, bonds, bills, commodities, currencies, cryptos.\"\n\t        )\n\t        return self.__query_chatgpt(instructions=instructions, text=markets_traded)\n\t    def __predict_period_of_rebalancing(self, key_elements):\n\t        period_of_rebalancing = \" \".join(key_elements.get(\"PERIOD_OF_REBALANCING\", []))\n\t        if period_of_rebalancing == \"\":\n\t            return \"\"\n\t        instructions = \"Extract the period of rebalancing. It can be: daily, weekly, quarterly, yearly.\"\n\t        return self.__query_chatgpt(\n\t            instructions=instructions, text=period_of_rebalancing\n", "        )\n\t    def __predict_backtest_period(self, key_elements):\n\t        backtest_period = \" \".join(key_elements.get(\"BACKTEST_PERIOD\", []))\n\t        if backtest_period == \"\":\n\t            return \"\"\n\t        instructions = \"Extract the backtest_period. Example: 1961-2018.\"\n\t        return self.__query_chatgpt(instructions=instructions, text=backtest_period)\n\t    def __format_percent(self, text):\n\t        if \"%\" not in text:\n\t            return f\"{text}%\"\n", "        return text\n\t    def predict(self, kili_project_id: str, target_folder: str):\n\t        \"\"\"\n\t        Run predictions.\n\t        \"\"\"\n\t        assets = self._kili_client.assets(\n\t            project_id=kili_project_id,\n\t            fields=[\"id\", \"externalId\", \"labels.jsonResponse\"],\n\t            status_in=[\"LABELED\"],\n\t            disable_tqdm=True,\n", "        )\n\t        if not os.path.exists(target_folder):\n\t            os.makedirs(target_folder)\n\t        for asset in tqdm(assets):\n\t            key_elements, is_strategy = self.__parse_label(asset)\n\t            if is_strategy == \"NO\":\n\t                continue\n\t            abstract_id = int(asset[\"externalId\"])\n\t            path = os.path.join(target_folder, f\"{abstract_id}.md\")\n\t            if os.path.exists(path):\n", "                continue\n\t            abstract = SsrnAbstract(abstract_id)\n\t            abstract.from_ssrn()\n\t            strategy = SsrnStrategy(abstract)\n\t            strategy.trading_rules = self.__predict_trading_rules(key_elements)\n\t            strategy.backtrader = self.__predict_backtrader(key_elements)\n\t            strategy.markets_traded = self.__predict_markets_traded(key_elements)\n\t            strategy.period_of_rebalancing = self.__predict_period_of_rebalancing(\n\t                key_elements\n\t            )\n", "            strategy.backtest_period = self.__predict_backtest_period(key_elements)\n\t            annual_return = \" \".join(key_elements.get(\"ANNUAL_RETURN\", []))\n\t            strategy.annual_return = self.__format_percent(annual_return)\n\t            maximum_drawdown = \" \".join(key_elements.get(\"MAXIMUM_DRAWDOWN\", []))\n\t            strategy.maximum_drawdown = self.__format_percent(maximum_drawdown)\n\t            strategy.sharpe_ratio = \" \".join(key_elements.get(\"SHARPE_RATIO\", []))\n\t            annual_standard_deviation = \" \".join(\n\t                key_elements.get(\"ANNUAL_STANDARD_DEVIATION\", [])\n\t            )\n\t            strategy.annual_standard_deviation = self.__format_percent(\n", "                annual_standard_deviation\n\t            )\n\t            print(path)\n\t            with open(path, \"w\", encoding=\"utf-8\") as f:\n\t                markdown = strategy.to_markdown()\n\t                f.write(markdown)\n\t                print(markdown)\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_abstract_classifier.py", "chunked_list": ["import os\n\timport requests\n\timport click\n\tfrom datasets import Dataset\n\timport evaluate\n\tfrom kili.client import Kili\n\timport numpy as np\n\tfrom tqdm import tqdm\n\tfrom transformers import AutoTokenizer\n\tfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n", "from transformers import DataCollatorWithPadding\n\tfrom transformers import pipeline\n\tfrom ssrn_abstract import SsrnAbstract\n\tos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\tclass SsrnAbstractClassifier:\n\t    def __init__(self, kili_project_id: str):\n\t        self.kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n\t        self.kili_project_id = kili_project_id\n\t        self.zeroshot_author_id = \"\"\n\t        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n", "        self.id2label = {0: \"NO\", 1: \"YES\"}\n\t        self.label2id = {\"NO\": 0, \"YES\": 1}\n\t        self.model = AutoModelForSequenceClassification.from_pretrained(\n\t            \"distilbert-base-uncased\",\n\t            num_labels=2,\n\t            id2label=self.id2label,\n\t            label2id=self.label2id,\n\t        )\n\t        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)\n\t        self.metric = evaluate.load(\"f1\")\n", "        self.model_name = \"ssrn-abstract-classifier\"\n\t    def __preprocess_function(self, examples):\n\t        return self.tokenizer(examples[\"text\"], truncation=True)\n\t    def __compute_metrics(self, eval_pred):\n\t        predictions, labels = eval_pred\n\t        predictions = np.argmax(predictions, axis=1)\n\t        return self.metric.compute(predictions=predictions, references=labels)\n\t    def train(self):\n\t        assets = self.kili_client.assets(\n\t            project_id=self.kili_project_id,\n", "            fields=[\"id\", \"externalId\", \"labels.jsonResponse\", \"labels.labelType\"],\n\t            status_in=[\"LABELED\"],\n\t        )\n\t        labels = []\n\t        texts = []\n\t        for asset in tqdm(assets):\n\t            groundtruth_labels = [\n\t                l for l in asset[\"labels\"] if l[\"labelType\"] == \"DEFAULT\"\n\t            ]\n\t            if len(groundtruth_labels) == 0:\n", "                continue\n\t            groundtruth_category = groundtruth_labels[-1][\"jsonResponse\"][\n\t                \"IS_STRATEGY\"\n\t            ][\"categories\"][0][\"name\"]\n\t            labels.append(self.label2id[groundtruth_category])\n\t            abstract_id = int(asset[\"externalId\"])\n\t            abstract = SsrnAbstract(abstract_id)\n\t            abstract.from_kili(project_id=self.kili_project_id)\n\t            text = str(abstract)\n\t            texts.append(text)\n", "        if len(labels) == 0 or len(texts) == 0:\n\t            print(\"There is no data for training. Please check the assets list.\")\n\t            return\n\t        dataset_dict = {\"label\": labels, \"text\": texts}\n\t        dataset = Dataset.from_dict(dataset_dict)\n\t        dataset = dataset.train_test_split(test_size=0.2)\n\t        tokenized_dataset = dataset.map(self.__preprocess_function, batched=True)\n\t        training_args = TrainingArguments(\n\t            output_dir=self.model_name,\n\t            learning_rate=2e-5,\n", "            per_device_train_batch_size=16,\n\t            per_device_eval_batch_size=16,\n\t            num_train_epochs=3,\n\t            weight_decay=0.01,\n\t            evaluation_strategy=\"epoch\",\n\t            save_strategy=\"epoch\",\n\t            load_best_model_at_end=True,\n\t            push_to_hub=True,\n\t        )\n\t        trainer = Trainer(\n", "            model=self.model,\n\t            args=training_args,\n\t            train_dataset=tokenized_dataset[\"train\"],\n\t            eval_dataset=tokenized_dataset[\"test\"],\n\t            tokenizer=self.tokenizer,\n\t            data_collator=self.data_collator,\n\t            compute_metrics=self.__compute_metrics,\n\t        )\n\t        trainer.train()\n\t    def predict(self):\n", "        \"\"\"\n\t        Predicts the category of a text.\n\t        \"\"\"\n\t        classifier = pipeline(\n\t            \"text-classification\", model=self.model_name, tokenizer=self.tokenizer\n\t        )\n\t        assets = self.kili_client.assets(\n\t            project_id=self.kili_project_id,\n\t            fields=[\"id\", \"externalId\"],\n\t            status_in=[\"TODO\"],\n", "        )\n\t        for asset in tqdm(assets):\n\t            abstract_id = int(asset[\"externalId\"])\n\t            abstract = SsrnAbstract(abstract_id)\n\t            abstract.from_kili(project_id=self.kili_project_id)\n\t            text = str(abstract)\n\t            try:\n\t                prediction = classifier(text)\n\t            except RuntimeError:\n\t                continue\n", "            predicted_label = {\n\t                \"IS_STRATEGY\": {\"categories\": [{\"name\": prediction[0][\"label\"]}]}\n\t            }\n\t            self.kili_client.append_labels(\n\t                asset_id_array=[asset[\"id\"]],\n\t                json_response_array=[predicted_label],\n\t                model_name=self.model_name,\n\t                disable_tqdm=True,\n\t                label_type=\"PREDICTION\",\n\t            )\n", "            priority = int(100 * (1 - prediction[0][\"score\"]))\n\t            self.kili_client.update_properties_in_assets(\n\t                asset_ids=[asset[\"id\"]],\n\t                priorities=[priority],\n\t            )\n\t@click.command()\n\t@click.option(\"--mode\", default=\"train\")\n\t@click.option(\"--kili-project-id\")\n\tdef main(mode, kili_project_id):\n\t    \"\"\"\n", "    Main function.\n\t    \"\"\"\n\t    ssrn_abstract_classifier = SsrnAbstractClassifier(kili_project_id=kili_project_id)\n\t    if mode == \"train\":\n\t        ssrn_abstract_classifier.train()\n\t    elif mode == \"predict\":\n\t        ssrn_abstract_classifier.predict()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_paper_crawler.py", "chunked_list": ["from collections import Counter\n\timport os\n\timport re\n\timport time\n\tfrom typing import Optional\n\tfrom bs4 import BeautifulSoup\n\tfrom kili.client import Kili\n\timport requests\n\tfrom selenium import webdriver\n\tfrom selenium.webdriver.chrome.options import Options\n", "import textract\n\tfrom tqdm import tqdm\n\tfrom webdriver_manager.chrome import ChromeDriverManager\n\tfrom systematic_trading.strategy_ideas.ssrn_paper import SsrnPaper\n\tclass SsrnPaperCrawler:\n\t    def __init__(\n\t        self,\n\t        project_id: Optional[str] = None,\n\t    ):\n\t        self.tgt_kili_project_id = tgt_kili_project_id\n", "    def __from_url(self, url: str):\n\t        return int(url.split(\"=\")[-1])\n\t    def from_kili(self, src_kili_project_id: str):\n\t        \"\"\"\n\t        List all abstract ids from Kili\n\t        \"\"\"\n\t        kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n\t        assets = kili_client.assets(\n\t            project_id=src_kili_project_id,\n\t            fields=[\"externalId\", \"labels.jsonResponse\", \"labels.labelType\"],\n", "            disable_tqdm=True,\n\t        )\n\t        for asset in assets:\n\t            labels = [\n\t                label\n\t                for label in asset[\"labels\"]\n\t                if label[\"labelType\"] in [\"DEFAULT\", \"REVIEW\"]\n\t            ]\n\t            if len(labels) == 0:\n\t                continue\n", "            is_strategy = labels[-1][\"jsonResponse\"][\"IS_STRATEGY\"][\"categories\"][0][\n\t                \"name\"\n\t            ]\n\t            if is_strategy != \"Yes\":\n\t                continue\n\t            abstract_id = int(asset[\"externalId\"])\n\t            paper = SsrnPaper(abstract_id)\n\t            if paper.exists_in_kili(self.tgt_kili_project_id):\n\t                continue\n\t            paper.from_ssrn()\n", "            if paper.pdf_path is None:\n\t                continue\n\t            paper.to_kili(self.tgt_kili_project_id, metadata={\"text\": filename})\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_strategy.py", "chunked_list": ["import os\n\tfrom kili.client import Kili\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tfrom ssrn_abstract import SsrnAbstract\n\tclass SsrnStrategy:\n\t    def __init__(self, abstract: SsrnAbstract):\n\t        self.abstract = abstract\n\t        self.trading_rules = \"\"\n\t        self.backtrader = \"\"\n", "        self.markets_traded = \"\"\n\t        self.period_of_rebalancing = \"\"\n\t        self.backtest_period = \"\"\n\t        self.annual_return = \"\"\n\t        self.maximum_drawdown = \"\"\n\t        self.sharpe_ratio = \"\"\n\t        self.annual_standard_deviation = \"\"\n\t    def to_markdown(self):\n\t        return f\"\"\"# {self.abstract.title}\n\tA python [Backtrader](https://www.backtrader.com/) implementation of the algorithmic trading strategy described in the following paper.\n", "# Original paper\n\t📕 [Paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id={self.abstract.abstract_id})\n\t# Trading rules\n\t{self.trading_rules}\n\t# Statistics\n\t- **Markets Traded:** {self.markets_traded}\n\t- **Period of Rebalancing:** {self.period_of_rebalancing}\n\t- **Backtest period:** {self.backtest_period}\n\t- **Annual Return:** {self.annual_return}\n\t- **Maximum Drawdown:** {self.maximum_drawdown}\n", "- **Sharpe Ratio:** {self.sharpe_ratio}\n\t- **Annual Standard Deviation:** {self.annual_standard_deviation}\n\t# Python code\n\t## Backtrader\n\t```python\n\t{self.backtrader}\n\t```\"\"\"\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_abstract.py", "chunked_list": ["import json\n\timport os\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tfrom kili.client import Kili\n\tclass SsrnAbstract:\n\t    def __init__(self, abstract_id: int):\n\t        self.abstract_id = abstract_id\n\t        self.url = (\n\t            f\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id={self.abstract_id}\"\n", "        )\n\t        self.abstract = \"\"\n\t        self.authors = []\n\t        self.jel_classification = []\n\t        self.keywords = []\n\t        self.online_date = \"\"\n\t        self.publication_date = \"\"\n\t        self.title = \"\"\n\t        self.is_strategy = \"\"\n\t        self._kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n", "        self._external_id = str(abstract_id)\n\t        self._soup = None\n\t    def __ssrn_page(self):\n\t        if self._soup is not None:\n\t            return self._soup\n\t        headers = {\n\t            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n\t        }\n\t        response = requests.get(self.url, headers=headers)\n\t        self._soup = BeautifulSoup(response.content, \"html.parser\")\n", "        return self._soup\n\t    def exists_in_ssrn(self):\n\t        soup = self.__ssrn_page()\n\t        abstract_tag = soup.find(\"div\", {\"class\": \"abstract-text\"})\n\t        title = \"The submitter of this work did not provide a PDF file for download\"\n\t        no_download_tag = soup.find(\"a\", {\"title\": title})\n\t        return abstract_tag is not None and no_download_tag is None\n\t    def from_ssrn(self):\n\t        soup = self.__ssrn_page()\n\t        # abstract\n", "        abstract_tag = soup.find(\"div\", {\"class\": \"abstract-text\"})\n\t        self.abstract = abstract_tag.find(\"p\").text.strip()\n\t        # authors\n\t        author_tags = soup.find_all(\"meta\", {\"name\": \"citation_author\"})\n\t        self.authors = [tag[\"content\"] for tag in author_tags]\n\t        # JEL classification\n\t        p_tags = soup.find_all(\"p\")\n\t        pattern = \"JEL Classification:\"\n\t        jel_tag = [tag for tag in p_tags if pattern in tag.text]\n\t        if len(jel_tag) == 1:\n", "            self.jel_classification = (\n\t                jel_tag[0].text.replace(pattern, \"\").strip().split(\", \")\n\t            )\n\t        # keywords\n\t        keywords_tag = soup.find(\"meta\", {\"name\": \"citation_keywords\"})\n\t        self.keywords = keywords_tag[\"content\"].split(\", \")\n\t        # online date\n\t        online_date_tag = soup.find(\"meta\", {\"name\": \"citation_online_date\"})\n\t        self.online_date = online_date_tag[\"content\"]\n\t        # publication date\n", "        publication_date_tag = soup.find(\"meta\", {\"name\": \"citation_publication_date\"})\n\t        self.publication_date = publication_date_tag[\"content\"]\n\t        # title\n\t        title_tag = soup.find(\"meta\", {\"name\": \"citation_title\"})\n\t        self.title = title_tag[\"content\"]\n\t    def __find_json_content_element(self, json_obj, target_id: str):\n\t        results = []\n\t        if isinstance(json_obj, dict):\n\t            if json_obj.get(\"id\") == target_id:\n\t                results.append(json_obj)\n", "            for value in json_obj.values():\n\t                if isinstance(value, (dict, list)):\n\t                    results.extend(self.__find_json_content_element(value, target_id))\n\t        elif isinstance(json_obj, list):\n\t            for item in json_obj:\n\t                if isinstance(item, (dict, list)):\n\t                    results.extend(self.__find_json_content_element(item, target_id))\n\t        return results\n\t    def from_kili(self, project_id: str):\n\t        assets = self._kili_client.assets(\n", "            project_id=project_id,\n\t            external_id_strictly_in=[self._external_id],\n\t            fields=[\"jsonContent\", \"labels.jsonResponse\", \"labels.labelType\"],\n\t            disable_tqdm=True,\n\t        )\n\t        assert len(assets) == 1\n\t        asset = assets[0]\n\t        asset[\"jsonContent\"] = json.loads(requests.get(asset[\"jsonContent\"]).content)\n\t        # abstract\n\t        abstract_blocks = self.__find_json_content_element(\n", "            asset[\"jsonContent\"], \"abstract\"\n\t        )\n\t        assert len(abstract_blocks) == 1\n\t        self.abstract = abstract_blocks[0][\"text\"]\n\t        # authors\n\t        authors_blocks = self.__find_json_content_element(\n\t            asset[\"jsonContent\"], \"authors\"\n\t        )\n\t        assert len(authors_blocks) == 1\n\t        self.authors = authors_blocks[0][\"text\"].split(\" | \")\n", "        # JEL classification\n\t        jel_classification_blocks = self.__find_json_content_element(\n\t            asset[\"jsonContent\"], \"jel-classification\"\n\t        )\n\t        assert len(jel_classification_blocks) == 1\n\t        self.jel_classification = jel_classification_blocks[0][\"text\"].split(\" | \")\n\t        # keywords\n\t        keywords_blocks = self.__find_json_content_element(\n\t            asset[\"jsonContent\"], \"keywords\"\n\t        )\n", "        assert len(keywords_blocks) == 1\n\t        self.keywords = keywords_blocks[0][\"text\"].split(\" | \")\n\t        # online date\n\t        online_date_blocks = self.__find_json_content_element(\n\t            asset[\"jsonContent\"], \"online-date\"\n\t        )\n\t        assert len(online_date_blocks) == 1\n\t        self.online_date = online_date_blocks[0][\"text\"]\n\t        # publication date\n\t        publication_date_blocks = self.__find_json_content_element(\n", "            asset[\"jsonContent\"], \"publication-date\"\n\t        )\n\t        assert len(publication_date_blocks) == 1\n\t        self.publication_date = publication_date_blocks[0][\"text\"]\n\t        # title\n\t        title_blocks = self.__find_json_content_element(asset[\"jsonContent\"], \"title\")\n\t        assert len(title_blocks) == 1\n\t        self.title = title_blocks[0][\"text\"]\n\t        labels = [\n\t            label\n", "            for label in asset[\"labels\"]\n\t            if label[\"labelType\"] in [\"DEFAULT\", \"REVIEW\"]\n\t        ]\n\t        if len(labels) > 0:\n\t            self.is_strategy = labels[-1][\"jsonResponse\"][\"IS_STRATEGY\"][\"categories\"][\n\t                0\n\t            ][\"name\"]\n\t    def __json_content_children(self, tag_id: str, title: str, text: str):\n\t        return [\n\t            {\n", "                \"type\": \"h3\",\n\t                \"children\": [\n\t                    {\n\t                        \"id\": f\"{tag_id}-h\",\n\t                        \"text\": title,\n\t                    }\n\t                ],\n\t            },\n\t            {\n\t                \"type\": \"p\",\n", "                \"children\": [\n\t                    {\n\t                        \"id\": tag_id,\n\t                        \"text\": text,\n\t                    }\n\t                ],\n\t            },\n\t        ]\n\t    def exists_in_folder(self, path: str):\n\t        return os.path.exists(os.path.join(path, f\"{self._external_id}.json\"))\n", "    def exists_in_kili(self, project_id: str):\n\t        assets = self._kili_client.assets(\n\t            project_id=project_id,\n\t            external_id_strictly_in=[self._external_id],\n\t            fields=[\"id\"],\n\t            disable_tqdm=True,\n\t        )\n\t        return len(assets) == 1\n\t    def to_folder(self, path: str):\n\t        with open(os.path.join(path, f\"{self._external_id}.json\"), \"w\") as f:\n", "            json.dump(self.__dict__(), f, indent=4, sort_keys=True)\n\t    def to_kili(self, project_id: str):\n\t        children = (\n\t            self.__json_content_children(\n\t                tag_id=\"title\",\n\t                title=\"Title\",\n\t                text=self.title,\n\t            )\n\t            + self.__json_content_children(\n\t                tag_id=\"abstract\",\n", "                title=\"Abstract\",\n\t                text=self.abstract,\n\t            )\n\t            + self.__json_content_children(\n\t                tag_id=\"keywords\",\n\t                title=\"Keywords\",\n\t                text=\" | \".join(self.keywords),\n\t            )\n\t            + self.__json_content_children(\n\t                tag_id=\"jel-classification\",\n", "                title=\"JEL classification\",\n\t                text=\" | \".join(self.jel_classification),\n\t            )\n\t            + self.__json_content_children(\n\t                tag_id=\"authors\",\n\t                title=\"Authors\",\n\t                text=\" | \".join(self.authors),\n\t            )\n\t            + self.__json_content_children(\n\t                tag_id=\"url\",\n", "                title=\"Url\",\n\t                text=self.url,\n\t            )\n\t            + self.__json_content_children(\n\t                tag_id=\"publication-date\",\n\t                title=\"Publication date\",\n\t                text=self.publication_date,\n\t            )\n\t            + self.__json_content_children(\n\t                tag_id=\"online-date\",\n", "                title=\"Online date\",\n\t                text=self.online_date,\n\t            )\n\t        )\n\t        json_content = [\n\t            {\n\t                \"children\": children,\n\t            }\n\t        ]\n\t        self._kili_client.append_many_to_dataset(\n", "            project_id=project_id,\n\t            json_content_array=[json_content],\n\t            external_id_array=[self._external_id],\n\t            disable_tqdm=True,\n\t        )\n\t        if self.is_strategy != \"\":\n\t            json_response = {\n\t                \"IS_STRATEGY\": {\"categories\": [{\"name\": self.is_strategy}]}\n\t            }\n\t            self._kili_client.append_labels(\n", "                project_id=project_id,\n\t                asset_external_id_array=[self._external_id],\n\t                json_response_array=[json_response],\n\t                label_type=\"DEFAULT\",\n\t                disable_tqdm=True,\n\t            )\n\t    def __dict__(self):\n\t        return {\n\t            \"abstract\": self.abstract,\n\t            \"authors\": self.authors,\n", "            \"external_id\": self._external_id,\n\t            \"jel_classification\": self.jel_classification,\n\t            \"keywords\": self.keywords,\n\t            \"online_date\": self.online_date,\n\t            \"publication_date\": self.publication_date,\n\t            \"title\": self.title,\n\t            \"url\": self.url,\n\t        }\n\t    def __str__(self):\n\t        text = \"\\n\\n\".join(\n", "            [\n\t                self.title,\n\t                self.abstract,\n\t                \" | \".join(self.keywords),\n\t                \" | \".join(self.jel_classification),\n\t                \" | \".join(self.authors),\n\t            ]\n\t        )\n\t        return text\n"]}
{"filename": "systematic_trading/strategy_ideas/ssrn_paper.py", "chunked_list": ["import os\n\tfrom kili.client import Kili\n\timport requests\n\tfrom bs4 import BeautifulSoup\n\tclass SsrnPaper:\n\t    def __init__(self, abstract_id: int):\n\t        self.abstract_id = abstract_id\n\t        self._external_id = str(abstract_id)\n\t        self.pdf_path = None\n\t        self.url = (\n", "            f\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id={self.abstract_id}\"\n\t        )\n\t        self._kili_client = Kili(api_key=os.getenv(\"KILI_API_KEY\"))\n\t    def exists_in_kili(self, project_id: str):\n\t        assets = self._kili_client.assets(\n\t            project_id=project_id,\n\t            external_id_strictly_in=[self._external_id],\n\t            fields=[\"id\"],\n\t            disable_tqdm=True,\n\t        )\n", "        return len(assets) == 1\n\t    def from_ssrn(self):\n\t        headers = {\n\t            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n\t        }\n\t        html = requests.get(self.url, headers=headers).content\n\t        soup = BeautifulSoup(html, \"html.parser\")\n\t        hrefs = [\n\t            tag[\"href\"]\n\t            for tag in soup.find_all(\"a\")\n", "            if tag.has_attr(\"href\") and tag[\"href\"].startswith(\"Delivery.cfm/\")\n\t        ]\n\t        if len(hrefs) == 0:\n\t            return\n\t        pdf_url = \"https://papers.ssrn.com/sol3/\" + hrefs[0]\n\t        folder = os.path.join(os.getenv(\"HOME\"), \"Downloads\")\n\t        filename = f\"{self.abstract_id}.pdf\"\n\t        self.pdf_path = os.path.join(folder, filename)\n\t        response = requests.get(pdf_url, headers={**headers, \"Referer\": self.url})\n\t        with open(self.pdf_path, \"wb\") as handler:\n", "            handler.write(response.content)\n\t    def to_kili(self, project_id: str, metadata: dict = {}):\n\t        self._kili_client.append_many_to_dataset(\n\t            project_id=project_id,\n\t            content_array=[self.pdf_path],\n\t            external_id_array=[self._external_id],\n\t            disable_tqdm=True,\n\t            json_metadata_array=[metadata],\n\t        )\n"]}
{"filename": "systematic_trading/datasets/__main__.py", "chunked_list": ["from datetime import date, datetime, timedelta\n\tfrom typing import List\n\timport click\n\tfrom tqdm import tqdm\n\tfrom systematic_trading.datasets.dataset import Dataset\n\tfrom systematic_trading.datasets.index_constituents import IndexConstituents\n\tfrom systematic_trading.datasets.index_constituents.sp500 import SP500\n\tfrom systematic_trading.datasets.knowledge_graph.stocks import Stocks\n\tfrom systematic_trading.datasets.raw.analysis.earnings_estimate import EarningsEstimate\n\tfrom systematic_trading.datasets.raw.analysis.eps_revisions import EPSRevisions\n", "from systematic_trading.datasets.raw.analysis.eps_trend import EPSTrend\n\tfrom systematic_trading.datasets.raw.analysis.revenue_estimate import RevenueEstimate\n\tfrom systematic_trading.datasets.raw.earnings import Earnings\n\tfrom systematic_trading.datasets.raw.earnings_forecast import EarningsForecast\n\tfrom systematic_trading.datasets.raw.earnings_surprise import EarningsSurprise\n\tfrom systematic_trading.datasets.raw.extended_trading import ExtendedTrading\n\tfrom systematic_trading.datasets.raw.news import News\n\tfrom systematic_trading.datasets.raw.short_interest import ShortInterest\n\tfrom systematic_trading.datasets.raw.timeseries_daily import TimeseriesDaily\n\tfrom systematic_trading.datasets.raw.timeseries_1mn import Timeseries1mn\n", "@click.command()\n\t@click.option(\"--mode\", default=\"\", help=\"Mode to use, daily / on-demand\")\n\t@click.option(\"--username\", default=\"edarchimbaud\", help=\"Username to use\")\n\tdef main(mode: str, username: str):\n\t    \"\"\"\n\t    Main function.\n\t    \"\"\"\n\t    if mode == \"daily\":\n\t        now = datetime.now()\n\t        if now.hour > 21:\n", "            tag_date = date.today()\n\t        elif now.hour < 10:\n\t            tag_date = date.today() - timedelta(days=1)\n\t        else:\n\t            raise ValueError(\"This script should be run between 21:00 and 10:00\")\n\t        tag = tag_date.isoformat()\n\t        print(\"Updating index constituents...\")\n\t        index_constituents = SP500(tag_date=tag_date, username=username)\n\t        if not index_constituents.check_file_exists(tag=tag):\n\t            index_constituents.set_dataset_df()\n", "            index_constituents.to_hf_datasets()\n\t        print(\"Updating raw datasets...\")\n\t        raw_datasets = {\n\t            \"earnings-stocks\": Earnings(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"earnings-estimate-stocks\": EarningsEstimate(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"earnings-forecast-stocks\": EarningsForecast(\n", "                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"earnings-surprise-stocks\": EarningsSurprise(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"extended-trading-stocks\": ExtendedTrading(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"eps-revisions-stocks\": EPSRevisions(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n", "            ),\n\t            \"eps-trend-stocks\": EPSTrend(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"news-stocks\": News(suffix=\"stocks\", tag_date=tag_date, username=username),\n\t            \"revenue-estimate-stocks\": RevenueEstimate(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"short-interest-stocks\": ShortInterest(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n", "            ),\n\t            \"timeseries-daily-stocks\": TimeseriesDaily(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t            \"timeseries-1mn-stocks\": Timeseries1mn(\n\t                suffix=\"stocks\", tag_date=tag_date, username=username\n\t            ),\n\t        }\n\t        dataset_names = [\n\t            name\n", "            for name in raw_datasets\n\t            if not raw_datasets[name].check_file_exists(tag=tag)\n\t        ]\n\t        for name in dataset_names:\n\t            raw_datasets[name].load_frames()\n\t        for symbol in tqdm(index_constituents.symbols):\n\t            for name in dataset_names:\n\t                if symbol in raw_datasets[name].frames:\n\t                    continue\n\t                raw_datasets[name].append_frame(symbol)\n", "                raw_datasets[name].save_frames()\n\t        for name in dataset_names:\n\t            raw_datasets[name].set_dataset_df()\n\t            raw_datasets[name].to_hf_datasets()\n\t    elif mode == \"on-demand\":\n\t        print(\"Updating list of stocks...\")\n\t        stocks = Stocks(username=username)\n\t        stocks.set_dataset_df()\n\t        stocks.to_hf_datasets()\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "systematic_trading/datasets/dataset.py", "chunked_list": ["from datetime import date, timedelta\n\tfrom typing import Optional\n\tfrom datasets import Dataset as HFDataset, load_dataset\n\timport huggingface_hub\n\timport pandas as pd\n\timport re\n\tclass Dataset:\n\t    \"\"\"\n\t    Dataset.\n\t    \"\"\"\n", "    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        self.suffix: str = suffix\n\t        self.tag_date = tag_date\n\t        self.username: str = username\n\t        self.expected_columns = []\n\t        self.dataset_df: pd.DataFrame = pd.DataFrame(columns=self.expected_columns)\n\t        self.name: str = None\n\t        self.symbols = self.get_scope_symbols()\n\t    def add_previous_data(self):\n\t        \"\"\"\n", "        Add previous data to the current data.\n\t        \"\"\"\n\t        prev_data = pd.DataFrame(\n\t            load_dataset(f\"{self.username}/{self.name}\")[\"train\"],\n\t        )\n\t        # filter out news that are not related to the index\n\t        still_in_scope = prev_data.symbol.isin(self.symbols)\n\t        prev_data = prev_data.loc[still_in_scope]\n\t        self.dataset_df = pd.concat([prev_data, self.dataset_df])\n\t        self.dataset_df.drop_duplicates(inplace=True)\n", "    def check_file_exists(self, tag: Optional[str] = None) -> bool:\n\t        \"\"\"\n\t        Check if file exists.\n\t        \"\"\"\n\t        try:\n\t            load_dataset(\n\t                f\"{self.username}/{self.name}\",\n\t                revision=tag,\n\t                verification_mode=\"no_checks\",\n\t            )\n", "            return True\n\t        except FileNotFoundError:\n\t            return False\n\t    def set_dataset_df(self):\n\t        \"\"\"\n\t        Frames to dataset.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    def get_scope_symbols(self) -> list:\n\t        if self.check_file_exists():\n", "            return load_dataset(f\"{self.username}/{self.suffix}\")[\"train\"][\"symbol\"]\n\t        return []\n\t    def symbol_to_ticker(self, symbol: str) -> str:\n\t        \"\"\"\n\t        Convert a symbol to a ticker.\n\t        \"\"\"\n\t        pattern = re.compile(r\"\\.B$\")\n\t        return pattern.sub(\"-B\", symbol)\n\t    def to_hf_datasets(self) -> None:\n\t        \"\"\"\n", "        To Hugging Face datasets.\n\t        \"\"\"\n\t        if self.dataset_df.columns.tolist() != self.expected_columns:\n\t            raise ValueError(\n\t                f\"self.dataset_df must have the right columns\\n{self.dataset_df.columns.tolist()}\\n!=\\n{self.expected_columns}\"\n\t            )\n\t        if len(self.dataset_df) == 0:\n\t            raise ValueError(\"self.dataset_df must be set\")\n\t        tag = self.tag_date.isoformat()\n\t        dataset = HFDataset.from_pandas(self.dataset_df)\n", "        repo_id: str = f\"edarchimbaud/{self.name}\"\n\t        dataset.push_to_hub(repo_id, private=False)\n\t        huggingface_hub.create_tag(repo_id, tag=tag, repo_type=\"dataset\")\n"]}
{"filename": "systematic_trading/datasets/knowledge_graph/wikipedia.py", "chunked_list": ["\"\"\"\n\tcurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream.xml.bz2\"\n\tcurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream-index.txt.bz2\"\n\t\"\"\"\n\timport bz2\n\timport os\n\timport pickle\n\timport re\n\tfrom bs4 import BeautifulSoup\n\timport click\n", "from tqdm import tqdm\n\tDRIVE = \"/Users/edba/Downloads\"\n\tclass Wikipedia:\n\t    def __init__(self):\n\t        path = os.path.join(\n\t            DRIVE,\n\t            \"Raw\",\n\t            \"Wikipedia\",\n\t            \"enwiki-20230620-pages-articles-multistream.xml.bz2\",\n\t        )\n", "        self.handler = bz2.BZ2File(path, \"r\")\n\t    def __del__(self):\n\t        self.handler.close()\n\t    def select_pages(self, titles: list[str]):\n\t        \"\"\"\n\t        Returns the Wikipedia pages of companies that are traded.\n\t        \"\"\"\n\t        pages = {}\n\t        for line in tqdm(self.handler, total=22962775):\n\t            line = line.decode(\"utf-8\")\n", "            if \"<page>\" in line:\n\t                page_content = []\n\t                page_content.append(line)\n\t                while \"</page>\" not in line:\n\t                    line = next(self.handler).decode(\"utf-8\")\n\t                    page_content.append(line)\n\t                page = \"\".join(page_content).strip()\n\t                soup = BeautifulSoup(page, \"xml\")\n\t                title_tag = soup.find(\"title\")\n\t                title = title_tag.get_text()\n", "                if title in titles:\n\t                    text_tag = soup.find(\"text\")\n\t                    text = text_tag.get_text()\n\t                    pages[title] = text\n\t        return pages\n"]}
{"filename": "systematic_trading/datasets/knowledge_graph/stocks.py", "chunked_list": ["\"\"\"\n\tcurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream.xml.bz2\"\n\tcurl -C - -O \"https://dumps.wikimedia.org/enwiki/20230620/enwiki-20230620-pages-articles-multistream-index.txt.bz2\"\n\t\"\"\"\n\tfrom datetime import date\n\timport json\n\timport os\n\timport pickle\n\timport re\n\timport time\n", "from urllib.parse import quote_plus\n\tfrom bs4 import BeautifulSoup\n\timport click\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport requests\n\tfrom selenium import webdriver\n\tfrom selenium.webdriver.chrome.options import Options\n\tfrom tqdm import tqdm\n\tfrom webdriver_manager.chrome import ChromeDriverManager\n", "from systematic_trading.helpers import nasdaq_headers\n\tfrom systematic_trading.datasets.knowledge_graph import KnowledgeGraph\n\tfrom systematic_trading.datasets.knowledge_graph.wikipedia import Wikipedia\n\tclass Stocks(KnowledgeGraph):\n\t    def __init__(self, tag_date: date = None, username: str = None):\n\t        super().__init__(\"stocks\", tag_date, username)\n\t        self.name = f\"stocks\"\n\t    def __download_nasdaq(self) -> pd.DataFrame:\n\t        \"\"\"\n\t        Returns a DataFrame of NASDAQ stocks\n", "        \"\"\"\n\t        url = \"https://api.nasdaq.com/api/screener/stocks?tableonly=true&download=true\"\n\t        response = requests.get(url, headers=nasdaq_headers())\n\t        json_data = response.json()\n\t        df = pd.DataFrame(data=json_data[\"data\"][\"rows\"])\n\t        df = df[[\"symbol\", \"name\", \"country\", \"sector\", \"industry\"]]\n\t        # filter common stocks\n\t        index = df.name.apply(lambda x: x.endswith(\"Common Stock\"))\n\t        df = df.loc[index, :]\n\t        df.reset_index(drop=True, inplace=True)\n", "        nasdaq_names = df.name.apply(\n\t            lambda x: x.replace(\" Common Stock\", \"\")\n\t            .replace(\" Inc.\", \"\")\n\t            .replace(\" Inc\", \"\")\n\t            .replace(\" Class A\", \"\")\n\t        )\n\t        df.name = nasdaq_names\n\t        df.rename(\n\t            columns={\n\t                \"name\": \"security\",\n", "                \"sector\": \"gics_sector\",\n\t                \"industry\": \"gics_sub_industry\",\n\t            },\n\t            inplace=True,\n\t        )\n\t        return df\n\t    def __download_sp500(self) -> pd.DataFrame:\n\t        dataset = load_dataset(\"edarchimbaud/index-constituents-sp500\")\n\t        df = dataset[\"train\"].to_pandas()\n\t        df = df[[\"symbol\", \"security\", \"gics_sector\", \"gics_sub_industry\"]]\n", "        df.loc[:, \"country\"] = \"United States\"\n\t        return df\n\t    def __download(self) -> pd.DataFrame:\n\t        path_tgt = os.path.join(\"data\", \"stocks.raw.csv\")\n\t        if os.path.exists(path_tgt):\n\t            return\n\t        self.dataset_df = pd.concat(\n\t            [\n\t                self.__download_nasdaq(),\n\t                self.__download_sp500(),\n", "            ]\n\t        )\n\t        self.dataset_df = self.dataset_df.drop_duplicates(\n\t            subset=[\"symbol\"], keep=\"first\"\n\t        )\n\t        self.dataset_df.sort_values(by=[\"symbol\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n\t        self.__save(path=path_tgt)\n\t    def __load(self, path):\n\t        if path.endswith(\".csv\"):\n", "            self.dataset_df = pd.read_csv(path)\n\t        elif path.endswith(\".pkl\"):\n\t            self.dataset_df = pd.read_pickle(path)\n\t    def __save(self, path):\n\t        if path.endswith(\".csv\"):\n\t            self.dataset_df.to_csv(\n\t                path,\n\t                index=False,\n\t            )\n\t        elif path.endswith(\".pkl\"):\n", "            self.dataset_df.to_pickle(path)\n\t    def __add_wikipedia_title(self):\n\t        \"\"\"\n\t        Add wikipedia title to the DataFrame.\n\t        \"\"\"\n\t        path_src = os.path.join(\"data\", \"stocks.raw.csv\")\n\t        path_tgt = os.path.join(\"data\", \"stocks.title.csv\")\n\t        if os.path.exists(path_tgt):\n\t            return\n\t        self.__load(path=path_src)\n", "        self.dataset_df.fillna(\"\", inplace=True)\n\t        self.dataset_df.loc[:, \"wikipedia_title\"] = \"\"\n\t        # Match with Google search\n\t        headers = {\n\t            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n\t        }\n\t        options = Options()\n\t        options.headless = False\n\t        options.add_argument(\"user-agent=\" + headers[\"User-Agent\"])\n\t        driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n", "        driver.get(\"https://www.google.com/\")\n\t        input(\"Cookies accepted?\")\n\t        path = os.path.join(\"data\", \"stocks.csv\")\n\t        for index, row in tqdm(self.dataset_df.iterrows(), total=len(self.dataset_df)):\n\t            if index < 6:\n\t                continue\n\t            if row[\"wikipedia_title\"]:\n\t                continue\n\t            encoded_query = quote_plus(row[\"security\"] + \" company\")\n\t            url = \"https://www.google.com/search?hl=en&q=\" + encoded_query\n", "            driver.get(url)\n\t            time.sleep(60)\n\t            body = driver.find_element(\"xpath\", \"//body\")\n\t            body_html = body.get_attribute(\"innerHTML\")\n\t            soup = BeautifulSoup(body_html, \"html.parser\")\n\t            hrefs = [\n\t                a[\"href\"]\n\t                for a in soup.find_all(\"a\")\n\t                if a.has_attr(\"href\")\n\t                and a[\"href\"].startswith(\"https://en.wikipedia.org/\")\n", "                and a.text.strip() == \"Wikipedia\"\n\t            ]\n\t            if len(hrefs) == 0:\n\t                continue\n\t            href = hrefs[0]\n\t            wikipedia_name = href.split(\"/\")[-1].replace(\"_\", \" \")\n\t            self.dataset_df.loc[index, \"wikipedia_title\"] = wikipedia_name\n\t            self.__save(path=path_tgt)\n\t        self.__save(path=path_tgt)\n\t    def __add_wikipedia_page(self):\n", "        \"\"\"\n\t        Add wikipedia page to the DataFrame.\n\t        \"\"\"\n\t        path_src = os.path.join(\"data\", \"stocks.title.csv\")\n\t        path_tgt = os.path.join(\"data\", \"stocks.page.pkl\")\n\t        if os.path.exists(path_tgt):\n\t            return\n\t        self.__load(path=path_src)\n\t        titles = self.dataset_df.wikipedia_title.tolist()\n\t        wikipedia = Wikipedia()\n", "        pages = wikipedia.select_pages(titles)\n\t        self.dataset_df.loc[:, \"wikipedia_page\"] = \"\"\n\t        for index, row in self.dataset_df.iterrows():\n\t            title = row[\"wikipedia_title\"]\n\t            if title == \"\" or title not in pages:\n\t                continue\n\t            row[\"wikipedia_page\"] = pages[title]\n\t        self.__save(path=path_tgt)\n\t    def __add_relationships(self):\n\t        \"\"\"\n", "        Add relationships to the DataFrame.\n\t        \"\"\"\n\t        path_src = os.path.join(\"data\", \"stocks.page.pkl\")\n\t        path_tgt = os.path.join(\"data\", \"stocks.csv\")\n\t        if os.path.exists(path_tgt):\n\t            return\n\t        self.__load(path=path_src)\n\t        self.dataset_df.loc[:, \"categories\"] = \"\"\n\t        pattern = r\"\\[\\[Category:(.*?)\\]\\]\"\n\t        for index, row in self.dataset_df.iterrows():\n", "            text = row[\"wikipedia_page\"]\n\t            if text == \"\":\n\t                continue\n\t            categories = list(re.findall(pattern, text))\n\t            self.dataset_df.loc[index, \"categories\"] = json.dumps(categories)\n\t        self.dataset_df = self.dataset_df[self.expected_columns]\n\t        self.__save(path=path_tgt)\n\t    def set_dataset_df(self):\n\t        \"\"\"\n\t        Frames to dataset.\n", "        \"\"\"\n\t        self.dataset_df = self.__download()\n\t        self.__add_wikipedia_title()\n\t        self.__add_wikipedia_page()\n\t        self.__add_relationships()\n"]}
{"filename": "systematic_trading/datasets/knowledge_graph/__init__.py", "chunked_list": ["\"\"\"\n\tIndex constituents data.\n\t\"\"\"\n\tfrom datetime import date\n\timport pandas as pd\n\tfrom systematic_trading.datasets.dataset import Dataset\n\tclass KnowledgeGraph(Dataset):\n\t    \"\"\"\n\t    Index constituents data.\n\t    \"\"\"\n", "    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.expected_columns = [\n\t            \"symbol\",\n\t            \"security\",\n\t            \"country\",\n\t            \"gics_sector\",\n\t            \"gics_sub_industry\",\n\t            \"categories\",\n\t        ]\n", "        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n"]}
{"filename": "systematic_trading/datasets/index_constituents/__init__.py", "chunked_list": ["\"\"\"\n\tIndex constituents data.\n\t\"\"\"\n\tfrom datetime import date\n\timport pandas as pd\n\tfrom systematic_trading.datasets.dataset import Dataset\n\tclass IndexConstituents(Dataset):\n\t    \"\"\"\n\t    Index constituents data.\n\t    \"\"\"\n", "    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.expected_columns = [\n\t            \"symbol\",\n\t            \"security\",\n\t            \"gics_sector\",\n\t            \"gics_sub_industry\",\n\t            \"headquarters_location\",\n\t            \"date_added\",\n\t            \"cik\",\n", "            \"founded\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n"]}
{"filename": "systematic_trading/datasets/index_constituents/sp500.py", "chunked_list": ["\"\"\"\n\tIndex constituents S&P 500.\n\t\"\"\"\n\tfrom datetime import date\n\tfrom bs4 import BeautifulSoup\n\timport pandas as pd\n\timport requests\n\tfrom tqdm import tqdm\n\tfrom systematic_trading.datasets.index_constituents import IndexConstituents\n\tfrom systematic_trading.helpers import retry_get\n", "class SP500(IndexConstituents):\n\t    \"\"\"\n\t    Index constituents S&P 500.\n\t    \"\"\"\n\t    def __init__(self, tag_date: date = None, username: str = None):\n\t        super().__init__(\"stocks\", tag_date, username)\n\t        self.name = f\"index-constituents-sp500\"\n\t    def set_dataset_df(self):\n\t        \"\"\"\n\t        Download the list of S&P 500 constituents from Wikipedia.\n", "        \"\"\"\n\t        url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n\t        response = retry_get(url)\n\t        body_html = response.content.decode(\"utf-8\")\n\t        soup = BeautifulSoup(body_html, features=\"lxml\")\n\t        table = soup.find(\"table\", {\"id\": \"constituents\"})\n\t        header = [th.text.strip() for th in table.find_all(\"th\")]\n\t        assert len(header) == 8, f\"len(header)=={len(header)}\"\n\t        tbody = table.find(\"tbody\")\n\t        data = []\n", "        for row in tqdm(tbody.find_all(\"tr\")):\n\t            td_tags = row.find_all(\"td\")\n\t            if len(td_tags) != len(header):\n\t                continue\n\t            data.append(\n\t                {\n\t                    \"symbol\": td_tags[header.index(\"Symbol\")].text.strip(),\n\t                    \"security\": td_tags[header.index(\"Security\")].text.strip(),\n\t                    \"gics_sector\": td_tags[header.index(\"GICS Sector\")].text.strip(),\n\t                    \"gics_sub_industry\": td_tags[\n", "                        header.index(\"GICS Sub-Industry\")\n\t                    ].text.strip(),\n\t                    \"headquarters_location\": td_tags[\n\t                        header.index(\"Headquarters Location\")\n\t                    ].text.strip(),\n\t                    \"date_added\": td_tags[header.index(\"Date added\")].text.strip(),\n\t                    \"cik\": td_tags[header.index(\"CIK\")].text.strip(),\n\t                    \"founded\": td_tags[header.index(\"Founded\")].text.strip(),\n\t                }\n\t            )\n", "        self.dataset_df = pd.DataFrame(data=data)\n\t        self.dataset_df.sort_values(by=[\"symbol\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
{"filename": "systematic_trading/datasets/raw/short_interest.py", "chunked_list": ["\"\"\"\n\tShort interest from Nasdaq.\n\t\"\"\"\n\tfrom datetime import date, datetime\n\timport time\n\timport urllib\n\tfrom datasets import load_dataset\n\timport numpy as np\n\timport pandas as pd\n\timport requests\n", "from systematic_trading.datasets.raw import Raw\n\tfrom systematic_trading.helpers import nasdaq_headers, retry_get\n\tclass ShortInterest(Raw):\n\t    \"\"\"\n\t    Short interest from Nasdaq.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"short-interest-{suffix}\"\n\t        self.expected_columns = [\n", "            \"symbol\",\n\t            \"date\",\n\t            \"id\",\n\t            \"settlement_date\",\n\t            \"interest\",\n\t            \"avg_daily_share_volume\",\n\t            \"days_to_cover\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def append_frame(self, symbol: str):\n", "        ticker = self.symbol_to_ticker(symbol)\n\t        url = f\"https://api.nasdaq.com/api/quote/{ticker}/short-interest?assetClass=stocks\"\n\t        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n\t        json_data = response.json()\n\t        if json_data[\"data\"] is None:\n\t            self.frames[symbol] = None\n\t            return\n\t        short_interest_table = json_data[\"data\"][\"shortInterestTable\"]\n\t        if short_interest_table is None:\n\t            self.frames[symbol] = None\n", "            return\n\t        data = short_interest_table[\"rows\"]\n\t        df = pd.DataFrame(data=data)\n\t        df[\"settlementDate\"] = pd.to_datetime(df[\"settlementDate\"])\n\t        df[\"interest\"] = df[\"interest\"].apply(\n\t            lambda x: int(x.replace(\",\", \"\")) if x != \"N/A\" else None\n\t        )\n\t        df[\"avgDailyShareVolume\"] = df[\"avgDailyShareVolume\"].apply(\n\t            lambda x: int(x.replace(\",\", \"\")) if x != \"N/A\" else None\n\t        )\n", "        df.rename(\n\t            columns={\n\t                \"settlementDate\": \"settlement_date\",\n\t                \"avgDailyShareVolume\": \"avg_daily_share_volume\",\n\t                \"daysToCover\": \"days_to_cover\",\n\t            },\n\t            inplace=True,\n\t        )\n\t        df[\"id\"] = range(len(df))\n\t        df[\"symbol\"] = symbol\n", "        df[\"date\"] = self.tag_date.isoformat()\n\t        df = df.reindex(columns=self.expected_columns)\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n\tif __name__ == \"__main__\":\n", "    symbol = \"AAPL\"\n\t    suffix = \"stocks\"\n\t    tag_date = datetime(2023, 5, 26).date()\n\t    username = \"edarchimbaud\"\n\t    dataset = ShortInterest(suffix=suffix, tag_date=tag_date, username=username)\n\t    dataset.append_frame(symbol)\n"]}
{"filename": "systematic_trading/datasets/raw/timeseries_daily.py", "chunked_list": ["\"\"\"\n\tTimeseries daily from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import date, datetime\n\timport time\n\timport urllib\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport requests\n\tfrom systematic_trading.datasets.raw import Raw\n", "class TimeseriesDaily(Raw):\n\t    \"\"\"\n\t    Timeseries daily from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"timeseries-daily-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n\t            \"date\",\n", "            \"open\",\n\t            \"high\",\n\t            \"low\",\n\t            \"close\",\n\t            \"adj_close\",\n\t            \"volume\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def __get_timeseries_daily_with_retry(self, ticker: str, retries=10, delay=300):\n\t        from_timestamp = int(datetime.timestamp(datetime(1980, 1, 1)))\n", "        to_timestamp = int(datetime.timestamp(datetime.now()))\n\t        url = f\"https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1={from_timestamp}&period2={to_timestamp}&interval=1d&events=history&includeAdjustedClose=true\"\n\t        for _ in range(retries):\n\t            try:\n\t                df = pd.read_csv(url)\n\t                df[\"Date\"] = pd.to_datetime(df[\"Date\"]).dt.date.apply(\n\t                    lambda x: x.isoformat()\n\t                )\n\t                return df\n\t            except urllib.error.HTTPError:\n", "                print(f\"Connection error with {url}. Retrying in {delay} seconds...\")\n\t                time.sleep(delay)\n\t        raise ConnectionError(f\"Failed to connect to {url} after {retries} retries\")\n\t    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n\t        df = self.__get_timeseries_daily_with_retry(ticker)\n\t        if df is None:\n\t            self.frames[symbol] = None\n\t            return\n\t        df.rename(\n", "            columns={\n\t                \"Date\": \"date\",\n\t                \"Open\": \"open\",\n\t                \"High\": \"high\",\n\t                \"Low\": \"low\",\n\t                \"Close\": \"close\",\n\t                \"Adj Close\": \"adj_close\",\n\t                \"Volume\": \"volume\",\n\t            },\n\t            inplace=True,\n", "        )\n\t        df[\"symbol\"] = symbol\n\t        # use reindex() to set 'symbol' as the first column\n\t        df = df.reindex(columns=[\"symbol\"] + list(df.columns[:-1]))\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
{"filename": "systematic_trading/datasets/raw/earnings.py", "chunked_list": ["\"\"\"\n\tEarnings data from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import datetime, date\n\tfrom typing import Union\n\tfrom bs4 import BeautifulSoup\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport pytz\n\timport requests\n", "from systematic_trading.datasets.raw import Raw\n\tfrom systematic_trading.helpers import retry_get\n\tclass Earnings(Raw):\n\t    \"\"\"\n\t    Earnings data from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"earnings-{suffix}\"\n\t        self.expected_columns = [\n", "            \"symbol\",\n\t            \"date\",\n\t            \"eps_estimate\",\n\t            \"reported_eps\",\n\t            \"surprise\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def __format_field(self, key: str, value: str):\n\t        \"\"\"\n\t        Format a field.\n", "        \"\"\"\n\t        if key == \"Earnings Date\":\n\t            date_str = value[:-3]\n\t            datetime_obj = datetime.strptime(date_str, \"%b %d, %Y, %I %p\")\n\t            tz_str = value[-3:]\n\t            if tz_str in [\"EST\", \"EDT\"]:\n\t                tz = pytz.timezone(\"EST\")\n\t            else:\n\t                raise ValueError(f\"Unknown timezone: {tz_str}\")\n\t            return tz.localize(datetime_obj)\n", "        elif key in [\"EPS Estimate\", \"Reported EPS\", \"Surprise(%)\"]:\n\t            if value == \"-\":\n\t                return None\n\t            return float(value)\n\t        elif key in [\"Symbol\", \"Company\"]:\n\t            return value\n\t        else:\n\t            raise ValueError(f\"Unknown key: {key}\")\n\t    def __get_earnings(self, ticker: str) -> Union[pd.DataFrame, None]:\n\t        \"\"\"\n", "        Get earnings for a given ticker.\n\t        \"\"\"\n\t        url = f\"https://finance.yahoo.com/calendar/earnings?symbol={ticker}\"\n\t        response = retry_get(url)\n\t        soup = BeautifulSoup(response.text, features=\"lxml\")\n\t        div = soup.find(\"div\", {\"id\": \"cal-res-table\"})\n\t        if div is None:\n\t            raise ValueError(f\"Could not find earnings for {ticker}\")\n\t        table = div.find(\"table\")\n\t        thead = table.find(\"thead\")\n", "        header = [th.text for th in thead.find_all(\"th\")]\n\t        expected_header = [\n\t            \"Symbol\",\n\t            \"Company\",\n\t            \"Earnings Date\",\n\t            \"EPS Estimate\",\n\t            \"Reported EPS\",\n\t            \"Surprise(%)\",\n\t        ]\n\t        assert header == expected_header\n", "        tbody = table.find(\"tbody\")\n\t        data = []\n\t        for row in tbody.find_all(\"tr\"):\n\t            data.append(\n\t                {\n\t                    key: self.__format_field(\n\t                        key,\n\t                        value=row.find_all(\"td\")[index].text,\n\t                    )\n\t                    for index, key in enumerate(header)\n", "                }\n\t            )\n\t        assert len(data) > 0\n\t        df = pd.DataFrame(data)\n\t        return df\n\t    def append_frame(self, symbol: str):\n\t        \"\"\"\n\t        Append a dataframe for a given symbol.\n\t        \"\"\"\n\t        ticker = self.symbol_to_ticker(symbol)\n", "        try:\n\t            df = self.__get_earnings(ticker)\n\t        except ValueError as e:\n\t            print(f\"Exception for {self.name}: {symbol}: {e}\")\n\t            self.frames[symbol] = None\n\t            return\n\t        df.drop(columns=[\"Symbol\", \"Company\"], inplace=True)\n\t        df.rename(\n\t            columns={\n\t                \"Earnings Date\": \"date\",\n", "                \"EPS Estimate\": \"eps_estimate\",\n\t                \"Reported EPS\": \"reported_eps\",\n\t                \"Surprise(%)\": \"surprise\",\n\t            },\n\t            inplace=True,\n\t        )\n\t        df[\"symbol\"] = symbol\n\t        # use reindex() to set 'symbol' as the first column\n\t        df = df.reindex(columns=[\"symbol\"] + list(df.columns[:-1]))\n\t        self.frames[symbol] = df\n", "    def set_dataset_df(self):\n\t        \"\"\"\n\t        Set the dataset dataframe.\n\t        \"\"\"\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
{"filename": "systematic_trading/datasets/raw/news.py", "chunked_list": ["\"\"\"\n\tNews from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import datetime, date\n\tfrom bs4 import BeautifulSoup\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport pytz\n\timport requests\n\tfrom systematic_trading.datasets.raw import Raw\n", "from systematic_trading.helpers import retry_get\n\tclass Article:\n\t    \"\"\"\n\t    Article.\n\t    \"\"\"\n\t    def __init__(self, url, uuid, title):\n\t        self.url = url\n\t        self.uuid = uuid\n\t        self.title = title\n\t        soup = self.__get_soup(url)\n", "        publisher_tag = soup.find(\"span\", {\"class\": \"caas-attr-provider\"})\n\t        self.publisher = publisher_tag.text if publisher_tag is not None else None\n\t        self.publish_time = self.__format_date(\n\t            soup.find(\"div\", {\"class\": \"caas-attr-time-style\"}).find(\"time\")[\n\t                \"datetime\"\n\t            ],\n\t        )\n\t        self.text = soup.find(\"div\", {\"class\": \"caas-body\"}).text\n\t    def __format_date(self, date_str):\n\t        \"\"\"\n", "        Format date.\n\t        \"\"\"\n\t        date_obj = datetime.strptime(date_str, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n\t        tz = pytz.timezone(\"GMT\")\n\t        date_obj = tz.localize(date_obj)\n\t        return date_obj\n\t    def __get_soup(self, url):\n\t        headers = {\n\t            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36\"\n\t        }\n", "        response = retry_get(url)\n\t        soup = BeautifulSoup(response.text, \"html.parser\")\n\t        return soup\n\t    def to_json(self):\n\t        return {\n\t            \"body\": self.text,\n\t            \"publisher\": self.publisher,\n\t            \"publish_time\": self.publish_time,\n\t            \"title\": self.title,\n\t            \"url\": self.url,\n", "            \"uuid\": self.uuid,\n\t        }\n\tclass News(Raw):\n\t    \"\"\"\n\t    News from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"news-{self.suffix}\"\n\t        self.expected_columns = [\n", "            \"symbol\",\n\t            \"body\",\n\t            \"publisher\",\n\t            \"publish_time\",\n\t            \"title\",\n\t            \"url\",\n\t            \"uuid\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def __get_news(self, ticker: str) -> pd.DataFrame:\n", "        \"\"\"\n\t        Get news for a given ticker.\n\t        \"\"\"\n\t        url = f\"https://finance.yahoo.com/quote/{ticker}/?p={ticker}\"\n\t        response = retry_get(url)\n\t        soup = BeautifulSoup(response.text, \"html.parser\")\n\t        news_tag = soup.find(\"div\", {\"id\": \"quoteNewsStream-0-Stream\"})\n\t        data = []\n\t        for h3_tag in news_tag.find_all(\"h3\"):\n\t            a_tag = h3_tag.find(\"a\")\n", "            if not a_tag.has_attr(\"data-uuid\"):\n\t                continue\n\t            article = Article(\n\t                url=\"https://finance.yahoo.com\" + a_tag[\"href\"],\n\t                uuid=a_tag[\"data-uuid\"],\n\t                title=a_tag.text,\n\t            )\n\t            data.append(article.to_json())\n\t        df = pd.DataFrame(data)\n\t        return df\n", "    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n\t        try:\n\t            df = self.__get_news(ticker)\n\t        except AttributeError as e:\n\t            print(f\"Exception for {self.name}: {symbol}: {e}\")\n\t            self.frames[symbol] = None\n\t            return\n\t        df[\"symbol\"] = symbol\n\t        # use reindex() to set 'symbol' as the first column\n", "        df = df.reindex(columns=[\"symbol\"] + list(df.columns[:-1]))\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"publish_time\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
{"filename": "systematic_trading/datasets/raw/earnings_forecast.py", "chunked_list": ["\"\"\"\n\tEarnings forecast from Nasdaq.\n\t\"\"\"\n\tfrom datetime import date, datetime\n\timport time\n\timport urllib\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport requests\n\tfrom systematic_trading.datasets.raw import Raw\n", "from systematic_trading.helpers import nasdaq_headers, retry_get\n\tclass EarningsForecast(Raw):\n\t    \"\"\"\n\t    Earnings forecast from Nasdaq.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"earnings-forecast-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n", "            \"date\",\n\t            \"id\",\n\t            \"fiscal_end\",\n\t            \"consensus_eps_forecast\",\n\t            \"high_eps_forecast\",\n\t            \"low_eps_forecast\",\n\t            \"no_of_estimates\",\n\t            \"up\",\n\t            \"down\",\n\t        ]\n", "        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n\t        url = f\"https://api.nasdaq.com/api/analyst/{ticker}/earnings-forecast\"\n\t        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n\t        json_data = response.json()\n\t        if json_data[\"data\"] is None:\n\t            self.frames[symbol] = None\n\t            return\n\t        quarterly_forecast = json_data[\"data\"][\"quarterlyForecast\"]\n", "        if quarterly_forecast is None:\n\t            self.frames[symbol] = None\n\t            return\n\t        df = pd.DataFrame(data=quarterly_forecast[\"rows\"])\n\t        df.rename(\n\t            columns={\n\t                \"fiscalEnd\": \"fiscal_end\",\n\t                \"consensusEPSForecast\": \"consensus_eps_forecast\",\n\t                \"highEPSForecast\": \"high_eps_forecast\",\n\t                \"lowEPSForecast\": \"low_eps_forecast\",\n", "                \"noOfEstimates\": \"no_of_estimates\",\n\t            },\n\t            inplace=True,\n\t        )\n\t        df[\"id\"] = range(len(df))\n\t        df[\"symbol\"] = symbol\n\t        df[\"date\"] = self.tag_date.isoformat()\n\t        df = df.reindex(columns=self.expected_columns)\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n", "        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n\tif __name__ == \"__main__\":\n\t    symbol = \"AAPL\"\n\t    suffix = \"stocks\"\n\t    tag_date = datetime(2023, 5, 26).date()\n\t    username = \"edarchimbaud\"\n", "    dataset = EarningsForecast(suffix=suffix, tag_date=tag_date, username=username)\n\t    dataset.append_frame(symbol)\n"]}
{"filename": "systematic_trading/datasets/raw/__init__.py", "chunked_list": ["from datetime import date\n\timport os\n\timport pickle\n\tfrom systematic_trading.datasets.dataset import Dataset\n\tclass Raw(Dataset):\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.cache_dir = os.getenv(\"CACHE_DIR\", \"/tmp\")\n\t        self.frames = {}\n\t    def append_frame(self, symbol: str):\n", "        \"\"\"\n\t        Append frame.\n\t        \"\"\"\n\t        raise NotImplementedError\n\t    def get_cache_path(self):\n\t        \"\"\"\n\t        Get cache path.\n\t        \"\"\"\n\t        tag = self.tag_date.isoformat()\n\t        return os.path.join(\n", "            self.cache_dir,\n\t            self.username,\n\t            self.name,\n\t            f\"{tag}.pkl\",\n\t        )\n\t    def load_frames(self):\n\t        file_path = self.get_cache_path()\n\t        if os.path.exists(file_path):\n\t            with open(file_path, \"rb\") as file:\n\t                self.frames = pickle.load(file)\n", "    def save_frames(self):\n\t        file_path = self.get_cache_path()\n\t        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\t        with open(file_path, \"wb\") as file:\n\t            pickle.dump(self.frames, file)\n"]}
{"filename": "systematic_trading/datasets/raw/extended_trading.py", "chunked_list": ["\"\"\"\n\tExtended trading from Nasdaq.\n\t\"\"\"\n\tfrom datetime import date, datetime\n\timport time\n\timport urllib\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport requests\n\tfrom systematic_trading.datasets.raw import Raw\n", "from systematic_trading.helpers import nasdaq_headers, retry_get\n\tclass ExtendedTrading(Raw):\n\t    \"\"\"\n\t    Extended trading from Nasdaq.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"extended-trading-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n", "            \"date\",\n\t            \"time\",\n\t            \"price\",\n\t            \"share_volume\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n\t        data = []\n\t        for n in range(1, 12):\n", "            url = f\"https://api.nasdaq.com/api/quote/{ticker}/extended-trading?markettype=pre&assetclass=stocks&time={n}\"\n\t            response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n\t            json_data = response.json()\n\t            if json_data[\"data\"] is None:\n\t                continue\n\t            trade_detail_table = json_data[\"data\"][\"tradeDetailTable\"]\n\t            if trade_detail_table[\"rows\"] is None:\n\t                continue\n\t            data += trade_detail_table[\"rows\"]\n\t        if len(data) == 0:\n", "            self.frames[symbol] = None\n\t            return\n\t        df = pd.DataFrame(data=data)\n\t        df.rename(\n\t            columns={\n\t                \"shareVolume\": \"share_volume\",\n\t            },\n\t            inplace=True,\n\t        )\n\t        df[\"price\"] = df[\"price\"].replace(\"\\$\", \"\", regex=True).astype(float)\n", "        df[\"symbol\"] = symbol\n\t        df[\"date\"] = self.tag_date.isoformat()\n\t        df = df.reindex(columns=self.expected_columns)\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"time\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n", "if __name__ == \"__main__\":\n\t    symbol = \"AAPL\"\n\t    suffix = \"stocks\"\n\t    tag_date = datetime(2023, 5, 26).date()\n\t    username = \"edarchimbaud\"\n\t    dataset = ExtendedTrading(suffix=suffix, tag_date=tag_date, username=username)\n\t    dataset.append_frame(symbol)\n"]}
{"filename": "systematic_trading/datasets/raw/timeseries_1mn.py", "chunked_list": ["\"\"\"\n\tTimeseries 1mn from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import date, datetime\n\timport time\n\timport urllib\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport requests\n\tfrom systematic_trading.datasets.raw import Raw\n", "from systematic_trading.helpers import retry_get\n\tclass Timeseries1mn(Raw):\n\t    \"\"\"\n\t    Timeseries 1mn from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"timeseries-1mn-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n", "            \"datetime\",\n\t            \"open\",\n\t            \"high\",\n\t            \"low\",\n\t            \"close\",\n\t            \"volume\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n", "        url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{ticker}?region=US&lang=en-US&includePrePost=false&interval=1m&useYfid=true&range=1d&corsDomain=finance.yahoo.com&.tsrc=finance\"\n\t        response = retry_get(url)\n\t        json_data = response.json()\n\t        result = json_data[\"chart\"][\"result\"][0]\n\t        if \"timestamp\" not in result:\n\t            self.frames[symbol] = None\n\t            return\n\t        timestamp = result[\"timestamp\"]\n\t        indicators = result[\"indicators\"][\"quote\"][0]\n\t        data = {\"datetime\": [datetime.fromtimestamp(t) for t in timestamp]}\n", "        data.update(indicators)\n\t        df = pd.DataFrame(data=data)\n\t        df[\"symbol\"] = symbol\n\t        df = df.reindex(columns=self.expected_columns)\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"datetime\"], inplace=True)\n", "        self.dataset_df.reset_index(drop=True, inplace=True)\n\tif __name__ == \"__main__\":\n\t    symbol = \"AAPL\"\n\t    suffix = \"stocks\"\n\t    tag_date = datetime(2023, 5, 26).date()\n\t    username = \"edarchimbaud\"\n\t    dataset = Timeseries1mn(suffix=suffix, tag_date=tag_date, username=username)\n\t    dataset.append_frame(symbol)\n"]}
{"filename": "systematic_trading/datasets/raw/earnings_surprise.py", "chunked_list": ["\"\"\"\n\tEarnings surprise from Nasdaq.\n\t\"\"\"\n\tfrom datetime import date, datetime\n\timport time\n\timport urllib\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\timport requests\n\tfrom systematic_trading.datasets.raw import Raw\n", "from systematic_trading.helpers import nasdaq_headers, retry_get\n\tclass EarningsSurprise(Raw):\n\t    \"\"\"\n\t    Earnings surprise from Nasdaq.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"earnings-surprise-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n", "            \"date\",\n\t            \"id\",\n\t            \"fiscal_qtr_end\",\n\t            \"date_reported\",\n\t            \"eps\",\n\t            \"consensus_forecast\",\n\t            \"percentage_surprise\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def append_frame(self, symbol: str):\n", "        ticker = self.symbol_to_ticker(symbol)\n\t        url = f\"https://api.nasdaq.com/api/company/{ticker}/earnings-surprise\"\n\t        response = retry_get(url, headers=nasdaq_headers(), mode=\"curl\")\n\t        json_data = response.json()\n\t        if json_data[\"data\"] is None:\n\t            self.frames[symbol] = None\n\t            return\n\t        earnings_surprise = json_data[\"data\"][\"earningsSurpriseTable\"]\n\t        if earnings_surprise is None:\n\t            self.frames[symbol] = None\n", "            return\n\t        df = pd.DataFrame(data=earnings_surprise[\"rows\"])\n\t        df.rename(\n\t            columns={\n\t                \"fiscalQtrEnd\": \"fiscal_qtr_end\",\n\t                \"dateReported\": \"date_reported\",\n\t                \"consensusForecast\": \"consensus_forecast\",\n\t                \"percentageSurprise\": \"percentage_surprise\",\n\t            },\n\t            inplace=True,\n", "        )\n\t        df[\"date_reported\"] = pd.to_datetime(df[\"date_reported\"])\n\t        df[\"id\"] = range(len(df))\n\t        df[\"symbol\"] = symbol\n\t        df[\"date\"] = self.tag_date.isoformat()\n\t        df = df.reindex(columns=self.expected_columns)\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n", "            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\", \"id\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n\tif __name__ == \"__main__\":\n\t    symbol = \"AAPL\"\n\t    suffix = \"stocks\"\n\t    tag_date = datetime(2023, 5, 26).date()\n\t    username = \"edarchimbaud\"\n\t    dataset = EarningsSurprise(suffix=suffix, tag_date=tag_date, username=username)\n\t    dataset.append_frame(symbol)\n"]}
{"filename": "systematic_trading/datasets/raw/analysis/earnings_estimate.py", "chunked_list": ["\"\"\"\n\tEarnings estimate from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import date\n\tfrom typing import Union\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\tfrom systematic_trading.datasets.raw.analysis import Analysis\n\tclass EarningsEstimate(Analysis):\n\t    \"\"\"\n", "    Earnings estimate from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"earnings-estimate-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n\t            \"date\",\n\t            \"current_qtr\",\n\t            \"no_of_analysts_current_qtr\",\n", "            \"next_qtr\",\n\t            \"no_of_analysts_next_qtr\",\n\t            \"current_year\",\n\t            \"no_of_analysts_current_year\",\n\t            \"next_year\",\n\t            \"no_of_analysts_next_year\",\n\t            \"avg_estimate_current_qtr\",\n\t            \"avg_estimate_next_qtr\",\n\t            \"avg_estimate_current_year\",\n\t            \"avg_estimate_next_year\",\n", "            \"low_estimate_current_qtr\",\n\t            \"low_estimate_next_qtr\",\n\t            \"low_estimate_current_year\",\n\t            \"low_estimate_next_year\",\n\t            \"high_estimate_current_qtr\",\n\t            \"high_estimate_next_qtr\",\n\t            \"high_estimate_current_year\",\n\t            \"high_estimate_next_year\",\n\t            \"year_ago_eps_current_qtr\",\n\t            \"year_ago_eps_next_qtr\",\n", "            \"year_ago_eps_current_year\",\n\t            \"year_ago_eps_next_year\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def format_value(self, key: str, value: str) -> Union[int, float]:\n\t        \"\"\"\n\t        Format value.\n\t        \"\"\"\n\t        if value == \"N/A\":\n\t            return None\n", "        elif key.startswith(\"no_of_analysts\"):\n\t            return int(value)\n\t        elif key.startswith(\"avg_estimate\"):\n\t            return float(value)\n\t        elif key.startswith(\"low_estimate\"):\n\t            return float(value)\n\t        elif key.startswith(\"high_estimate\"):\n\t            return float(value)\n\t        elif key.startswith(\"year_ago_eps\"):\n\t            return float(value)\n", "        elif key == \"current_qtr\" or key == \"next_qtr\":\n\t            return value\n\t        elif key == \"current_year\" or key == \"next_year\":\n\t            return int(value)\n\t        else:\n\t            raise ValueError(f\"Unknown key: {key}\")\n\t    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n\t        try:\n\t            data = self.get_analysis(ticker)\n", "            df = self.data_to_df(\n\t                data=data[0][\"Earnings Estimate\"],\n\t                field=\"Earnings Estimate\",\n\t                symbol=symbol,\n\t            )\n\t        except (IndexError, ValueError) as e:\n\t            print(f\"Exception for {self.name}: {symbol}: {e}\")\n\t            self.frames[symbol] = None\n\t            return\n\t        self.frames[symbol] = df\n", "    def set_dataset_df(self):\n\t        \"\"\"\n\t        Convert the frames to a dataset.\n\t        \"\"\"\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
{"filename": "systematic_trading/datasets/raw/analysis/revenue_estimate.py", "chunked_list": ["\"\"\"\n\tRevenue estimate from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import date\n\tfrom typing import Union\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\tfrom systematic_trading.datasets.raw.analysis import Analysis\n\tclass RevenueEstimate(Analysis):\n\t    \"\"\"\n", "    Revenue estimate from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"revenue-estimate-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n\t            \"date\",\n\t            \"current_qtr\",\n\t            \"no_of_analysts_current_qtr\",\n", "            \"next_qtr\",\n\t            \"no_of_analysts_next_qtr\",\n\t            \"current_year\",\n\t            \"no_of_analysts_current_year\",\n\t            \"next_year\",\n\t            \"no_of_analysts_next_year\",\n\t            \"avg_estimate_current_qtr\",\n\t            \"avg_estimate_next_qtr\",\n\t            \"avg_estimate_current_year\",\n\t            \"avg_estimate_next_year\",\n", "            \"low_estimate_current_qtr\",\n\t            \"low_estimate_next_qtr\",\n\t            \"low_estimate_current_year\",\n\t            \"low_estimate_next_year\",\n\t            \"high_estimate_current_qtr\",\n\t            \"high_estimate_next_qtr\",\n\t            \"high_estimate_current_year\",\n\t            \"high_estimate_next_year\",\n\t            \"year_ago_sales_current_qtr\",\n\t            \"year_ago_sales_next_qtr\",\n", "            \"year_ago_sales_current_year\",\n\t            \"year_ago_sales_next_year\",\n\t            \"sales_growth_yearest_current_qtr\",\n\t            \"sales_growth_yearest_next_qtr\",\n\t            \"sales_growth_yearest_current_year\",\n\t            \"sales_growth_yearest_next_year\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def format_value(self, key: str, value: str) -> Union[int, float]:\n\t        \"\"\"\n", "        Format value.\n\t        \"\"\"\n\t        if value == \"N/A\":\n\t            return None\n\t        elif key.startswith(\"no_of_analysts\"):\n\t            return int(value)\n\t        elif (\n\t            key.startswith(\"avg_estimate\")\n\t            or key.startswith(\"low_estimate\")\n\t            or key.startswith(\"high_estimate\")\n", "            or key.startswith(\"year_ago_sales\")\n\t            or key.startswith(\"sales_growth_yearest\")\n\t        ):\n\t            return value\n\t        elif key == \"current_qtr\" or key == \"next_qtr\":\n\t            return value\n\t        elif key == \"current_year\" or key == \"next_year\":\n\t            return int(value)\n\t        else:\n\t            raise ValueError(f\"Unknown key: {key}\")\n", "    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n\t        try:\n\t            data = self.get_analysis(ticker)\n\t            df = self.data_to_df(\n\t                data=data[1][\"Revenue Estimate\"],\n\t                field=\"Revenue Estimate\",\n\t                symbol=symbol,\n\t            )\n\t        except (IndexError, ValueError) as e:\n", "            print(f\"Exception for {self.name}: {symbol}: {e}\")\n\t            self.frames[symbol] = None\n\t            return\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
{"filename": "systematic_trading/datasets/raw/analysis/__init__.py", "chunked_list": ["\"\"\"\n\tEarnings data.\n\t\"\"\"\n\tfrom typing import Union\n\tfrom bs4 import BeautifulSoup\n\timport pandas as pd\n\timport re\n\timport requests\n\tfrom systematic_trading.datasets.raw import Raw\n\tfrom systematic_trading.helpers import retry_get\n", "class Analysis(Raw):\n\t    \"\"\"\n\t    Analysis data from Yahoo Finance.\n\t    \"\"\"\n\t    def get_analysis(self, ticker: str) -> pd.DataFrame:\n\t        \"\"\"\n\t        Get analysis for a given ticker.\n\t        \"\"\"\n\t        url = f\"https://finance.yahoo.com/quote/{ticker}/analysis?p={ticker}\"\n\t        response = retry_get(url)\n", "        soup = BeautifulSoup(response.text, features=\"lxml\")\n\t        div = soup.find(\"div\", {\"id\": \"Col1-0-AnalystLeafPage-Proxy\"})\n\t        tables = div.find_all(\"table\") if div is not None else None\n\t        if tables is None:\n\t            raise ValueError(f\"No tables found for {ticker}\")\n\t        expected_table_names = [\n\t            \"Earnings Estimate\",\n\t            \"Revenue Estimate\",\n\t            \"Earnings History\",\n\t            \"EPS Trend\",\n", "            \"EPS Revisions\",\n\t            \"Growth Estimates\",\n\t        ]\n\t        data = []\n\t        for index, expected_table_name in enumerate(expected_table_names):\n\t            table = tables[index]\n\t            thead = table.find(\"thead\")\n\t            header = [th.text for th in thead.find_all(\"th\")]\n\t            assert len(header) == 5\n\t            assert header[0] == expected_table_name\n", "            tbody = table.find(\"tbody\")\n\t            table_data = []\n\t            for row in tbody.find_all(\"tr\"):\n\t                table_data.append(\n\t                    {k: row.find_all(\"td\")[i].text for i, k in enumerate(header)}\n\t                )\n\t            data.append(\n\t                {\n\t                    expected_table_name: table_data,\n\t                }\n", "            )\n\t        assert len(data) > 0\n\t        return data\n\t    def __format_column(self, column: str) -> str:\n\t        \"\"\"\n\t        Format column.\n\t        \"\"\"\n\t        return re.sub(r\"[^a-z0-9_]\", \"\", column.replace(\" \", \"_\").lower())\n\t    def format_value(self, column: str, value: str) -> Union[float, None]:\n\t        raise NotImplementedError\n", "    def data_to_df(\n\t        self,\n\t        data,\n\t        field: str,\n\t        symbol: str,\n\t    ) -> pd.DataFrame:\n\t        df = pd.DataFrame(data=data)\n\t        df.set_index(field, inplace=True, drop=True)\n\t        data_dict = {}\n\t        for index, row in df.iterrows():\n", "            for col in df.columns:\n\t                period_column = self.__format_column(col.split(\" (\")[0])\n\t                period = col.split(\" (\")[1].replace(\")\", \"\")\n\t                data_dict[period_column] = self.format_value(period_column, period)\n\t                column = self.__format_column(index + \" \" + period_column)\n\t                data_dict[column] = self.format_value(column, row[col])\n\t        df = pd.DataFrame(data=[data_dict])\n\t        df[\"date\"] = self.tag_date.isoformat()\n\t        df[\"symbol\"] = symbol\n\t        df = df.reindex(columns=[\"symbol\", \"date\"] + list(df.columns[:-2]))\n", "        return df\n"]}
{"filename": "systematic_trading/datasets/raw/analysis/eps_revisions.py", "chunked_list": ["\"\"\"\n\tEPS Revisions from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import date\n\tfrom typing import Union\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\tfrom systematic_trading.datasets.raw.analysis import Analysis\n\tclass EPSRevisions(Analysis):\n\t    \"\"\"\n", "    EPS Revisions from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"eps-revisions-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n\t            \"date\",\n\t            \"current_qtr\",\n\t            \"up_last_7_days_current_qtr\",\n", "            \"next_qtr\",\n\t            \"up_last_7_days_next_qtr\",\n\t            \"current_year\",\n\t            \"up_last_7_days_current_year\",\n\t            \"next_year\",\n\t            \"up_last_7_days_next_year\",\n\t            \"up_last_30_days_current_qtr\",\n\t            \"up_last_30_days_next_qtr\",\n\t            \"up_last_30_days_current_year\",\n\t            \"up_last_30_days_next_year\",\n", "            \"down_last_7_days_current_qtr\",\n\t            \"down_last_7_days_next_qtr\",\n\t            \"down_last_7_days_current_year\",\n\t            \"down_last_7_days_next_year\",\n\t            \"down_last_30_days_current_qtr\",\n\t            \"down_last_30_days_next_qtr\",\n\t            \"down_last_30_days_current_year\",\n\t            \"down_last_30_days_next_year\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n", "    def format_value(self, key: str, value: str) -> Union[int, float]:\n\t        \"\"\"\n\t        Format value.\n\t        \"\"\"\n\t        if value == \"N/A\":\n\t            return None\n\t        elif \"_last_\" in key:\n\t            return int(value)\n\t        elif key == \"current_qtr\" or key == \"next_qtr\":\n\t            return value\n", "        elif key == \"current_year\" or key == \"next_year\":\n\t            return int(value)\n\t        else:\n\t            raise ValueError(f\"Unknown key: {key}\")\n\t    def append_frame(self, symbol: str) -> None:\n\t        ticker = self.symbol_to_ticker(symbol)\n\t        try:\n\t            data = self.get_analysis(ticker)\n\t            df = self.data_to_df(\n\t                data=data[4][\"EPS Revisions\"],\n", "                field=\"EPS Revisions\",\n\t                symbol=symbol,\n\t            )\n\t        except (IndexError, ValueError) as e:\n\t            print(f\"Exception for {self.name}: {symbol}: {e}\")\n\t            self.frames[symbol] = None\n\t            return\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n", "        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n\t        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
{"filename": "systematic_trading/datasets/raw/analysis/eps_trend.py", "chunked_list": ["\"\"\"\n\tEPS Trend from Yahoo Finance.\n\t\"\"\"\n\tfrom datetime import date\n\tfrom typing import Union\n\tfrom datasets import load_dataset\n\timport pandas as pd\n\tfrom systematic_trading.datasets.raw.analysis import Analysis\n\tclass EPSTrend(Analysis):\n\t    \"\"\"\n", "    EPS Trend from Yahoo Finance.\n\t    \"\"\"\n\t    def __init__(self, suffix: str = None, tag_date: date = None, username: str = None):\n\t        super().__init__(suffix, tag_date, username)\n\t        self.name = f\"eps-trend-{suffix}\"\n\t        self.expected_columns = [\n\t            \"symbol\",\n\t            \"date\",\n\t            \"current_qtr\",\n\t            \"current_estimate_current_qtr\",\n", "            \"next_qtr\",\n\t            \"current_estimate_next_qtr\",\n\t            \"current_year\",\n\t            \"current_estimate_current_year\",\n\t            \"next_year\",\n\t            \"current_estimate_next_year\",\n\t            \"7_days_ago_current_qtr\",\n\t            \"7_days_ago_next_qtr\",\n\t            \"7_days_ago_current_year\",\n\t            \"7_days_ago_next_year\",\n", "            \"30_days_ago_current_qtr\",\n\t            \"30_days_ago_next_qtr\",\n\t            \"30_days_ago_current_year\",\n\t            \"30_days_ago_next_year\",\n\t            \"60_days_ago_current_qtr\",\n\t            \"60_days_ago_next_qtr\",\n\t            \"60_days_ago_current_year\",\n\t            \"60_days_ago_next_year\",\n\t            \"90_days_ago_current_qtr\",\n\t            \"90_days_ago_next_qtr\",\n", "            \"90_days_ago_current_year\",\n\t            \"90_days_ago_next_year\",\n\t        ]\n\t        self.dataset_df = pd.DataFrame(columns=self.expected_columns)\n\t    def format_value(self, key: str, value: str) -> Union[int, float]:\n\t        \"\"\"\n\t        Format value.\n\t        \"\"\"\n\t        if value == \"N/A\":\n\t            return None\n", "        elif key.startswith(\"current_estimate\") or \"_days_ago_\" in key:\n\t            return float(value)\n\t        elif key == \"current_qtr\" or key == \"next_qtr\":\n\t            return value\n\t        elif key == \"current_year\" or key == \"next_year\":\n\t            return int(value)\n\t        else:\n\t            raise ValueError(f\"Unknown key: {key}\")\n\t    def append_frame(self, symbol: str):\n\t        ticker = self.symbol_to_ticker(symbol)\n", "        try:\n\t            data = self.get_analysis(ticker)\n\t            df = self.data_to_df(\n\t                data=data[3][\"EPS Trend\"],\n\t                field=\"EPS Trend\",\n\t                symbol=symbol,\n\t            )\n\t        except (IndexError, ValueError) as e:\n\t            print(f\"Exception for {self.name}: {symbol}: {e}\")\n\t            self.frames[symbol] = None\n", "            return\n\t        self.frames[symbol] = df\n\t    def set_dataset_df(self):\n\t        \"\"\"\n\t        Download the EPS trend data from Yahoo Finance.\n\t        \"\"\"\n\t        self.dataset_df = pd.concat([f for f in self.frames.values() if f is not None])\n\t        if self.check_file_exists():\n\t            self.add_previous_data()\n\t        self.dataset_df.sort_values(by=[\"symbol\", \"date\"], inplace=True)\n", "        self.dataset_df.reset_index(drop=True, inplace=True)\n"]}
