{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tfrom pathlib import Path\n\tthis_directory = Path(__file__).parent\n\tlong_description = (this_directory / \"README.md\").read_text()\n\tsetup(\n\t    name='complex-network-link-prediction',\n\t    version='1.3',\n\t    license='MIT',\n\t    description='A python library for link prediction in social networks',\n\t    author=\"Cristian Cosci, Fabrizio Fagiolo, Nicolò Vescera, Nicolò Posta, Tommaso Romani\",\n", "    packages=find_packages(where='.', include=['cnlp*']),\n\t    long_description=long_description,\n\t    long_description_content_type='text/markdown',\n\t    url='https://github.com/Typing-Monkeys/social-network-link-prediction',\n\t    keywords='Link Prediction, Social Network, Complex Network Analisys',\n\t    install_requires=[\n\t        'networkx',\n\t        'scipy',\n\t        'numpy',\n\t    ],\n", ")\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["from .Configs import Configs\n"]}
{"filename": "tests/Configs.py", "chunked_list": ["import networkx as nx\n\tfrom sknetwork.data import miserables\n\tclass Configs:\n\t    __dataset = miserables(metadata=True)\n\t    __graph_normal = None\n\t    __graph_labels = None\n\t    @staticmethod\n\t    def load_normal_dataset():\n\t        if Configs.__graph_normal is None:\n\t            Configs.__graph_normal = nx.to_networkx_graph(\n", "                Configs.__dataset['adjacency'])\n\t        return Configs.__graph_normal\n\t    @staticmethod\n\t    def load_labels_dataset():\n\t        if Configs.__graph_labels is None:\n\t            if Configs.__graph_normal is None:\n\t                Configs.load_normal_dataset()\n\t            names = Configs.__dataset['names']\n\t            map = {idx: name for idx, name in enumerate(names)}\n\t            Configs.__graph_labels = nx.relabel_nodes(Configs.__graph_normal,\n", "                                                      map)\n\t        return Configs.__graph_labels\n"]}
{"filename": "tests/unittests/test_othermethods.py", "chunked_list": ["import unittest\n\tfrom cnlp import other_methods\n\tfrom tests import Configs\n\timport numpy as np\n\tfrom scipy import sparse\n\tclass TestDimensionalityReductionMethods(unittest.TestCase):\n\t    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n\t        g = Configs.load_normal_dataset()\n\t        g_labels = Configs.load_labels_dataset()\n\t        res = None\n", "        res_labels = None\n\t        with self.subTest('Int Labels'):\n\t            res = fun(g, **params)\n\t            if debug:\n\t                print(res)\n\t                print(type(res))\n\t            self.assertIsNotNone(res, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n\t                \"Wrong return type\")\n", "        with self.subTest('String Labels'):\n\t            res_labels = fun(g_labels, **params)\n\t            if debug:\n\t                print(res_labels)\n\t                print(type(res_labels))\n\t            self.assertIsNotNone(res_labels, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res_labels) is sparse.csr_matrix\n\t                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\t        with self.subTest('CMP Results'):\n", "            try:\n\t                self.assertTrue(\n\t                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n\t                    \"Results are different !\")\n\t            except AssertionError as e:\n\t                print(e)\n\t        return res\n\t    def test_MI(self):\n\t        self.__perform_test(other_methods.information_theory.MI)\n\t    def test_pathentropy(self):\n", "        self.__perform_test(other_methods.information_theory.path_entropy)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/unittests/__init__.py", "chunked_list": []}
{"filename": "tests/unittests/test_dimreduction.py", "chunked_list": ["import unittest\n\tfrom cnlp import dimensionality_reduction_methods\n\tfrom tests import Configs\n\timport numpy as np\n\tfrom scipy import sparse\n\tclass TestDimensionalityReductionMethods(unittest.TestCase):\n\t    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n\t        g = Configs.load_normal_dataset()\n\t        g_labels = Configs.load_labels_dataset()\n\t        res = None\n", "        res_labels = None\n\t        with self.subTest('Int Labels'):\n\t            res = fun(g, **params)\n\t            if debug:\n\t                print(res)\n\t                print(type(res))\n\t            self.assertIsNotNone(res, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n\t                \"Wrong return type\")\n", "        with self.subTest('String Labels'):\n\t            res_labels = fun(g_labels, **params)\n\t            if debug:\n\t                print(res_labels)\n\t                print(type(res_labels))\n\t            self.assertIsNotNone(res_labels, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res_labels) is sparse.csr_matrix\n\t                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\t        with self.subTest('CMP Results'):\n", "            try:\n\t                self.assertTrue(\n\t                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n\t                    \"Results are different !\")\n\t            except AssertionError as e:\n\t                print(e)\n\t        return res\n\t    def test_SVD(self):\n\t        self.__perform_test(\n\t            dimensionality_reduction_methods.link_prediction_svd, {'k': 40})\n", "    def test_NMF(self):\n\t        self.__perform_test(\n\t            dimensionality_reduction_methods.link_prediction_nmf, {\n\t                'num_features': 10,\n\t                'num_iterations': 100\n\t            })\n\t    # def setUp(self):\n\t    #     self.start_time = time()\n\t    # def tearDown(self):\n\t    #     t = time() - self.start_time\n", "    #     print(f\"{round(t, 2)} s\")\n\t    # # TODO: @ncvesvera @Cosci @posta ricontrollare\n\t    # # @unittest.skip(\"Metodo non ancora implementato\")\n\t    # def test_svd_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     k = 100\n\t    #     svd = SVD(n_components=k)\n\t    #     embedding = svd.fit_transform(adjacency)\n\t    #     our_embedding = dimensionality_reduction_methods.link_prediction_svd(\n\t    #         g, k=k)\n", "    #     print()\n\t    #     print(svd.predict(adjacency))\n\t    #     print()\n\t    #     print()\n\t    #     print(embedding)\n\t    #     print(our_embedding)\n\t    #     self.assertEqual(embedding, our_embedding)\n\t    # @timeout(Configs.timeout)\n\t    # def test_svd_time(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n", "    #     k = 100\n\t    #     our_embedding = dimensionality_reduction_methods.link_prediction_svd(\n\t    #         g, k=k)\n\t    #     self.assertIsNotNone(our_embedding)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/unittests/test_probabilisticmethods.py", "chunked_list": ["import unittest\n\tfrom cnlp import probabilistic_methods\n\tfrom tests import Configs\n\timport numpy as np\n\tfrom scipy import sparse\n\tclass TestDimensionalityReductionMethods(unittest.TestCase):\n\t    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n\t        # g = Configs.load_normal_dataset()\n\t        g_labels = Configs.load_labels_dataset()\n\t        res = None\n", "        res_labels = None\n\t        # with self.subTest('Int Labels'):\n\t        #     res = fun(g, **params)\n\t        #\n\t        #     if debug:\n\t        #         print(res)\n\t        #         print(type(res))\n\t        #\n\t        #     self.assertIsNotNone(res, \"None result is returned\")\n\t        #     self.assertTrue(\n", "        #         type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n\t        #         \"Wrong return type\")\n\t        with self.subTest('String Labels'):\n\t            res_labels = fun(g_labels, **params)\n\t            if debug:\n\t                print(res_labels)\n\t                print(type(res_labels))\n\t            self.assertIsNotNone(res_labels, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res_labels) is sparse.csr_matrix\n", "                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\t        # with self.subTest('CMP Results'):\n\t        #     try:\n\t        #         self.assertTrue((res.__round__(4)\n\t        #                          != res_labels.__round__(4)).nnz == 0,\n\t        #                         \"Results are different !\")\n\t        #     except AssertionError as e:\n\t        #         print(e)\n\t        return res\n\t    def test_sbm(self):\n", "        self.__perform_test(probabilistic_methods.stochastic_block_model, {\n\t            'n': 1,\n\t        })\n"]}
{"filename": "tests/unittests/similarity_methods/test_global.py", "chunked_list": ["import unittest\n\tfrom cnlp.similarity_methods import global_similarity\n\tfrom tests import Configs\n\tfrom scipy import sparse\n\timport numpy as np\n\tclass TestGlobalSimilarityMethods(unittest.TestCase):\n\t    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n\t        g = Configs.load_normal_dataset()\n\t        g_labels = Configs.load_labels_dataset()\n\t        res = None\n", "        res_labels = None\n\t        with self.subTest('Int Labels'):\n\t            res = fun(g, **params)\n\t            if debug:\n\t                print(res)\n\t                print(type(res))\n\t            self.assertIsNotNone(res, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n\t                \"Wrong return type\")\n", "        with self.subTest('String Labels'):\n\t            res_labels = fun(g_labels, **params)\n\t            if debug:\n\t                print(res_labels)\n\t                print(type(res_labels))\n\t            self.assertIsNotNone(res_labels, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res_labels) is sparse.csr_matrix\n\t                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\t        with self.subTest('CMP Results'):\n", "            try:\n\t                self.assertTrue(\n\t                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n\t                    \"Results are different !\")\n\t            except AssertionError as e:\n\t                print(e)\n\t        return res\n\t    def test_katz(self):\n\t        self.__perform_test(global_similarity.katz_index, {'beta': .001})\n\t    def test_randwalk(self):\n", "        self.__perform_test(global_similarity.link_prediction_rwr, {\n\t            'c': .05,\n\t            'max_iters': 10\n\t        })\n\t    def test_rootedpage(self):\n\t        self.__perform_test(global_similarity.rooted_page_rank, {'alpha': .5})\n\t    def test_shortestpath(self):\n\t        self.__perform_test(global_similarity.shortest_path, {'cutoff': None})\n\t    def test_simrank(self):\n\t        self.__perform_test(global_similarity.sim_rank)\n", "    # @timeout(Configs.timeout)\n\t    # def test_katz_time(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     beta = .001\n\t    #     res = global_similarity.katz_index(g, beta=beta)\n\t    #     # print(res)\n\t    #     self.assertIsNotNone(res)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/unittests/similarity_methods/test_local.py", "chunked_list": ["import unittest\n\tfrom cnlp.similarity_methods import local_similarity\n\tfrom tests import Configs\n\tfrom scipy import sparse\n\timport numpy as np\n\tclass TestLocalSimilarityMethods(unittest.TestCase):\n\t    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n\t        g = Configs.load_normal_dataset()\n\t        g_labels = Configs.load_labels_dataset()\n\t        res = None\n", "        res_labels = None\n\t        with self.subTest('Int Labels'):\n\t            res = fun(g, **params)\n\t            if debug:\n\t                print(res)\n\t                print(type(res))\n\t            self.assertIsNotNone(res, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n\t                \"Wrong return type\")\n", "        with self.subTest('String Labels'):\n\t            res_labels = fun(g_labels, **params)\n\t            if debug:\n\t                print(res_labels)\n\t                print(type(res_labels))\n\t            self.assertIsNotNone(res_labels, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res_labels) is sparse.csr_matrix\n\t                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\t        with self.subTest('CMP Results'):\n", "            try:\n\t                self.assertTrue(\n\t                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n\t                    \"Results are different !\")\n\t            except AssertionError as e:\n\t                print(e)\n\t        return res\n\t    def test_common_neighbors(self):\n\t        self.__perform_test(local_similarity.common_neighbors)\n\t    def test_adamicadar(self):\n", "        self.__perform_test(local_similarity.adamic_adar)\n\t    def test_jaccard(self):\n\t        self.__perform_test(local_similarity.jaccard)\n\t    def test_sorensen(self):\n\t        self.__perform_test(local_similarity.sorensen)\n\t    def test_hubpromoted(self):\n\t        self.__perform_test(local_similarity.hub_promoted)\n\t    def test_hubdepressed(self):\n\t        self.__perform_test(local_similarity.hub_depressed)\n\t    def test_resourceallocation(self):\n", "        self.__perform_test(local_similarity.resource_allocation)\n\t    def test_prefattachment(self):\n\t        self.__perform_test(local_similarity.preferential_attachment)\n\t    def test_cosine(self):\n\t        self.__perform_test(local_similarity.cosine_similarity)\n\t    def test_nodeclustering(self):\n\t        self.__perform_test(local_similarity.node_clustering)\n\t    # def setUp(self):\n\t    #     self.start_time = time()\n\t    # def tearDown(self):\n", "    #     t = time() - self.start_time\n\t    #     print(f\"{round(t, 2)} s\")\n\t    # def __perform_sktest(self,\n\t    #                      our_values,\n\t    #                      skfunction,\n\t    #                      samples_range,\n\t    #                      samples_number=20,\n\t    #                      decimal_precision=3):\n\t    #     indxes = [(random.randrange(samples_range),\n\t    #                random.randrange(samples_range))\n", "    #               for a in range(samples_number)]\n\t    #     for i, j in indxes:\n\t    #         expected = skfunction.predict((i, j))\n\t    #         our_res = our_values[i, j]\n\t    #         self.assertAlmostEqual(expected, our_res, decimal_precision)\n\t    # def test_common_neighbors_1(self):\n\t    #     g, adjacency = Configs.load_easy_dataset()\n\t    #     cn = CommonNeighbors()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.common_neighbors(g)\n", "    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0], 20)\n\t    # def test_common_neighbors_2(self):\n\t    #     g, adjacency = Configs.load_medium_dataset()\n\t    #     cn = CommonNeighbors()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.common_neighbors(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @timeout(Configs.timeout)\n\t    # def test_common_neighbors_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n", "    #     cn = CommonNeighbors()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.common_neighbors(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_jaccard_1(self):\n\t    #     g, adjacency = Configs.load_easy_dataset()\n\t    #     cn = JaccardIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.jaccard(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n", "    # def test_jaccard_2(self):\n\t    #     g, adjacency = Configs.load_medium_dataset()\n\t    #     cn = JaccardIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.jaccard(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @timeout(Configs.timeout)\n\t    # def test_jaccard_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     cn = JaccardIndex()\n", "    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.jaccard(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_sorensen_1(self):\n\t    #     g, adjacency = Configs.load_easy_dataset()\n\t    #     cn = SorensenIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.sorensen(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_sorensen_2(self):\n", "    #     g, adjacency = Configs.load_medium_dataset()\n\t    #     cn = SorensenIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.sorensen(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @timeout(Configs.timeout)\n\t    # def test_sorensen_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     cn = SorensenIndex()\n\t    #     cn.fit(adjacency)\n", "    #     our_sim = local_similarity.sorensen(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_hubpromoted_1(self):\n\t    #     g, adjacency = Configs.load_easy_dataset()\n\t    #     cn = HubPromotedIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.hub_promoted(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_hubpromoted_2(self):\n\t    #     g, adjacency = Configs.load_medium_dataset()\n", "    #     cn = HubPromotedIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.hub_promoted(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @timeout(Configs.timeout)\n\t    # def test_hubpromoted_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     cn = HubPromotedIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.hub_promoted(g)\n", "    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_hubdepressed_1(self):\n\t    #     g, adjacency = Configs.load_easy_dataset()\n\t    #     cn = HubDepressedIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.hub_depressed(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_hubdepressed_2(self):\n\t    #     g, adjacency = Configs.load_medium_dataset()\n\t    #     cn = HubDepressedIndex()\n", "    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.hub_depressed(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @timeout(Configs.timeout)\n\t    # def test_hubdepressede_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     cn = HubDepressedIndex()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.hub_depressed(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n", "    # def test_adamicadar_1(self):\n\t    #     g, adjacency = Configs.load_easy_dataset()\n\t    #     cn = AdamicAdar()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.adamic_adar(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_adamicadar_2(self):\n\t    #     g, adjacency = Configs.load_medium_dataset()\n\t    #     cn = AdamicAdar()\n\t    #     cn.fit(adjacency)\n", "    #     our_sim = local_similarity.adamic_adar(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @timeout(Configs.timeout)\n\t    # def test_adamicadar_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     cn = AdamicAdar()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.adamic_adar(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_resourceallocation_1(self):\n", "    #     g, adjacency = Configs.load_easy_dataset()\n\t    #     cn = ResourceAllocation()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.resource_allocation(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_resourceallocation_2(self):\n\t    #     g, adjacency = Configs.load_medium_dataset()\n\t    #     cn = ResourceAllocation()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.resource_allocation(g)\n", "    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @timeout(Configs.timeout)\n\t    # def test_resourceallocation_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     cn = ResourceAllocation()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.resource_allocation(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_preferentialattachment_1(self):\n\t    #     g, adjacency = Configs.load_easy_dataset()\n", "    #     cn = PreferentialAttachment()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.preferential_attachment(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # def test_preferentialattachment_2(self):\n\t    #     g, adjacency = Configs.load_medium_dataset()\n\t    #     cn = PreferentialAttachment()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.preferential_attachment(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n", "    # @timeout(Configs.timeout)\n\t    # def test_preferentialattachment_3(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     cn = PreferentialAttachment()\n\t    #     cn.fit(adjacency)\n\t    #     our_sim = local_similarity.preferential_attachment(g)\n\t    #     self.__perform_sktest(our_sim, cn, adjacency.shape[0])\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_cosine_1(self):\n\t    #     pass\n", "    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_cosine_2(self):\n\t    #     pass\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_cosine_3(self):\n\t    #     pass\n\t    # @timeout(Configs.timeout)\n\t    # def test_cosine_time(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     our_sim = local_similarity.cosine_similarity(g)\n", "    #     self.assertIsNotNone(our_sim)\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_nodeclustering_1(self):\n\t    #     pass\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_nodeclustering_2(self):\n\t    #     pass\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_nodeclustering_3(self):\n\t    #     pass\n", "    # @timeout(Configs.timeout)\n\t    # def test_nodeclustering_time(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     our_sim = local_similarity.node_clustering(g)\n\t    #     self.assertIsNotNone(our_sim)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/unittests/similarity_methods/__init__.py", "chunked_list": []}
{"filename": "tests/unittests/similarity_methods/test_quasi_local.py", "chunked_list": ["import unittest\n\tfrom cnlp.similarity_methods import quasi_local_similarity\n\tfrom tests import Configs\n\tfrom scipy import sparse\n\timport numpy as np\n\tclass TestQuasiGlobalSimilarityMethods(unittest.TestCase):\n\t    def __perform_test(self, fun, params: dict = {}, debug: bool = False):\n\t        g = Configs.load_normal_dataset()\n\t        g_labels = Configs.load_labels_dataset()\n\t        res = None\n", "        res_labels = None\n\t        with self.subTest('Int Labels'):\n\t            res = fun(g, **params)\n\t            if debug:\n\t                print(res)\n\t                print(type(res))\n\t            self.assertIsNotNone(res, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res) is sparse.csr_matrix or type(res) is np.ndarray,\n\t                \"Wrong return type\")\n", "        with self.subTest('String Labels'):\n\t            res_labels = fun(g_labels, **params)\n\t            if debug:\n\t                print(res_labels)\n\t                print(type(res_labels))\n\t            self.assertIsNotNone(res_labels, \"None result is returned\")\n\t            self.assertTrue(\n\t                type(res_labels) is sparse.csr_matrix\n\t                or type(res_labels) is np.ndarray, \"Wrong return type\")\n\t        with self.subTest('CMP Results'):\n", "            try:\n\t                self.assertTrue(\n\t                    (res.__round__(4) != res_labels.__round__(4)).nnz == 0,\n\t                    \"Results are different !\")\n\t            except AssertionError as e:\n\t                print(e)\n\t        return res\n\t    def test_LPI(self):\n\t        self.__perform_test(quasi_local_similarity.local_path_index, {\n\t            'epsilon': .1,\n", "            'n': 10\n\t        })\n\t    def test_PL3(self):\n\t        self.__perform_test(quasi_local_similarity.path_of_length_three)\n\t    # def setUp(self):\n\t    #     self.start_time = time()\n\t    # def tearDown(self):\n\t    #     t = time() - self.start_time\n\t    #     print(f\"{round(t, 2)} s\")\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n", "    # def test_lpi_1(self):\n\t    #     pass\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_lpi_2(self):\n\t    #     pass\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_lpi_3(self):\n\t    #     pass\n\t    # # TODO: @ncvesvera @fagiolo ricontrollare\n\t    # @timeout(Configs.timeout)\n", "    # def test_lpi_time(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     epsilon = .1\n\t    #     n = 10\n\t    #     res = quasi_local_similarity.local_path_index(g, epsilon, n)\n\t    #     # print(res)\n\t    #     self.assertIsNotNone(res)\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_pol3_1(self):\n\t    #     pass\n", "    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_pol3_2(self):\n\t    #     pass\n\t    # @unittest.skip(\"Non è presente in scikit-network\")\n\t    # def test_pol3_3(self):\n\t    #     pass\n\t    # @timeout(Configs.timeout)\n\t    # def test_pol3_time(self):\n\t    #     g, adjacency = Configs.load_hard_dataset()\n\t    #     res = quasi_local_similarity.path_of_length_three(g)\n", "    #     # print(res)\n\t    #     self.assertIsNotNone(res)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "cnlp/dimensionality_reduction_methods.py", "chunked_list": ["\"\"\"Dimentionality Reduction based Methods for Link Prediction.\n\tThe curse of dimensionality is a well-known\n\tproblem in machine learning.\n\tSome researchers employ dimension reduction techniques\n\tto tackle the above problem and apply it in the link prediction scenario.\n\t\"\"\"\n\timport networkx as nx\n\timport numpy as np\n\tfrom scipy.sparse import csr_matrix, spdiags\n\tfrom cnlp.utils import to_adjacency_matrix, only_unconnected\n", "from scipy.sparse.linalg import svds\n\tdef link_prediction_svd(G: nx.Graph,\n\t                        k: int = 5,\n\t                        normalize: bool = False) -> csr_matrix:\n\t    \"\"\"Compute the SVD Decomposition for the Graph Adjacency Matrix.\n\t    The similarity decinoisutuin is defined as:\n\t    .. math::\n\t        X_\\\\pm \\\\approx F G^T\n\t    where \\\\(F \\\\in \\\\mathbb{R}^{p \\\\times k}\\\\) contains\n\t    the bases of the latent space and is called the basis matrix;\n", "    \\\\(G \\\\in \\\\mathbb{R}^{n \\\\times k}\\\\) contains combination of coefficients\n\t    of the bases for reconstructing the matrix \\\\(X\\\\), and is called\n\t    the coefficient matrix; \\\\(k\\\\) is the dimention of the latent space\n\t    (\\\\(k<n\\\\)) and \\\\(n\\\\) is the nunber of data vector\n\t    (as columns) in \\\\(X\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    k: int :\n", "        dimention of the latent space (must be \\\\(< n\\\\))\n\t         (Default value = 5)\n\t    normalize: bool :\n\t        if True, normalize the output values\n\t         (Default value = False)\n\t    Returns\n\t    -------\n\t    predicted_adj_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n", "    Typically, the latent features are extracted and using these features,\n\t    each vertex is represented in latent space, and such representations are\n\t    used in a supervised or unsupervised framework for link prediction.\n\t    To further improve the prediction results, some additional node/link or\n\t    other attribute information can be used.\n\t    In most of the works, non-negative matrix factorization has been used.\n\t    Some authors also applied the singular value decomposition technique.\n\t    \"\"\"\n\t    # Create the adjacency matrix of the graph\n\t    adj_matrix = to_adjacency_matrix(G)\n", "    # Perform the singular value decomposition\n\t    U, S, Vt = svds(csr_matrix(adj_matrix).astype(float), k=k)\n\t    # Make the diagonal matrix sparse\n\t    S = spdiags(S, [0], k, k)\n\t    # Compute the predicted adj_matrix\n\t    predicted_adj_matrix = ((U @ S) @ Vt)\n\t    if normalize:\n\t        # Normalize the predicted edge weights to the range [0, 1]\n\t        rows, cols = np.nonzero(predicted_adj_matrix)\n\t        max_weight = np.max(predicted_adj_matrix[rows, cols])\n", "        for i in range(len(rows)):\n\t            predicted_adj_matrix[rows[i], cols[i]] /= max_weight\n\t    return only_unconnected(G, csr_matrix(predicted_adj_matrix))\n\tdef link_prediction_nmf(graph: nx.Graph,\n\t                        num_features: int = 2,\n\t                        num_iterations: int = 100,\n\t                        seed: int = 69) -> csr_matrix:\n\t    \"\"\"Compute the _Non-negative Matrix Factorization_ Decomposition for the Graph Adjacency Matrix.\n\t    The similarity decinoisutuin is defined as:\n\t    .. math::\n", "        X_\\\\pm \\\\approx F_+ G^T_+\n\t    where \\\\(F \\\\in \\\\mathbb{R}^{p \\\\times k}\\\\) contains\n\t    the bases of the latent space and is called the basis matrix;\n\t    \\\\(G \\\\in \\\\mathbb{R}^{n \\\\times k}\\\\) contains combination of coefficients\n\t    of the bases for reconstructing the matrix \\\\(X\\\\), and is called\n\t    the coefficient matrix; \\\\(k\\\\) is the dimention of the latent space\n\t    ( \\\\(k<n\\\\) ) and \\\\(n\\\\) is the nunber of data vector\n\t    (as columns) in \\\\(X\\\\).\n\t    Parameters\n\t    ----------\n", "    graph: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    num_features: int :\n\t        dimention of the latent space (must be \\\\(< n\\\\))\n\t         (Default value = 2)\n\t    num_iterations: int :\n\t        max number of iteration for the algorithm convergence\n\t         (Default value = 100)\n\t    sedd: int :\n\t        the seed for the random initialization\n", "         (Default value = 69)\n\t    Returns\n\t    -------\n\t    predicted_adj_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    Typically, the latent features are extracted and using these features,\n\t    each vertex is represented in latent space, and such representations are\n\t    used in a supervised or unsupervised framework for link prediction.\n\t    To further improve the prediction results, some additional node/link or\n", "    other attribute information can be used.\n\t    In most of the works, non-negative matrix factorization has been used.\n\t    Some authors also applied the singular value decomposition technique.\n\t    \"\"\"\n\t    adj_matrix = to_adjacency_matrix(graph, sparse=False)\n\t    # Initialize the feature and coefficient matrices randomly\n\t    num_nodes = adj_matrix.shape[0]\n\t    np.random.seed(seed)\n\t    feature_matrix = np.random.rand(num_nodes, num_features)\n\t    coefficient_matrix = np.random.rand(num_features, num_nodes)\n", "    # Perform NMF using multiplicative updates\n\t    for i in range(num_iterations):\n\t        # Update the coefficient matrix\n\t        numerator = feature_matrix.T @ adj_matrix\n\t        denominator = (\n\t            (feature_matrix.T @ feature_matrix) @ coefficient_matrix)\n\t        coefficient_matrix *= (numerator / denominator)\n\t        numerator = adj_matrix @ coefficient_matrix.T\n\t        denominator = (\n\t            feature_matrix @ (coefficient_matrix @ coefficient_matrix.T))\n", "        feature_matrix *= (numerator / denominator)\n\t    # Compute the predicted adjacency matrix\n\t    predicted_adj_matrix = feature_matrix @ coefficient_matrix\n\t    return only_unconnected(graph, csr_matrix(predicted_adj_matrix))\n\tif __name__ == \"__main__\":\n\t    # Load a test graph\n\t    graph = nx.karate_club_graph()\n\t    # Compute the SVD of the similarity matrix and do link prediction on it\n\t    predicted_adj_matrix = link_prediction_nmf(graph)\n\t    adj_matrix = nx.to_numpy_array(graph)\n", "    # Initialize the feature and coefficient matrices randomly\n\t    num_nodes = adj_matrix.shape[0]\n\t    # Get the edges with the highest predicted probabilities\n\t    edges = []\n\t    for i in range(num_nodes):\n\t        for j in range(i + 1, num_nodes):\n\t            prob = predicted_adj_matrix[i, j]\n\t            edges.append((i, j, prob))\n\t    edges = sorted(edges, key=lambda x: x[2], reverse=True)[:10]\n\t    # Normalize the probabilities to be between 0 and 100\n", "    max_prob = max([e[2] for e in edges])\n\t    for edge in edges:\n\t        edge_prob = (edge[2] / max_prob) * 100\n\t        print(f\"({edge[0]}, {edge[1]}) - Probability: {edge_prob:.2f}%\")\n"]}
{"filename": "cnlp/probabilistic_methods.py", "chunked_list": ["\"\"\"Collection of Probabilistic Methods for Link Prediction.\"\"\"\n\timport networkx as nx\n\timport numpy as np\n\tfrom networkx.algorithms.community import louvain_communities\n\tfrom cnlp.utils import to_adjacency_matrix\n\tfrom scipy.sparse import csr_matrix, lil_matrix\n\tfrom scipy.special import comb\n\tfrom typing import List, Set\n\tfrom itertools import product, combinations_with_replacement\n\tfrom cnlp.utils import nodes_to_indexes\n", "def stochastic_block_model(G: nx.Graph,\n\t                           n: int,\n\t                           p: float = .05,\n\t                           seed: int = 42) -> csr_matrix:\n\t    \"\"\"Compute the Sotchastic Block Model Similarity for\n\t    all the nodes in the network.\n\t    This similarity is defined as:\n\t    .. math::\n\t        R_{xy} = \\\\frac{1}{Z} \\\\sum_{P \\\\in P^*}\n\t            ( \\\\frac{l^1_{\\\\sigma_x \\\\sigma_y}+1}{r^0_{\\\\sigma_x \\\\sigma_y} +2}) exp[-H(P)]\n", "    where the sum is over all possible partitions \\\\(P^*\\\\) of the network\n\t    into groups, \\\\( \\\\sigma_x \\\\) and \\\\( \\\\sigma_y \\\\) are vertices \\\\(x\\\\)\n\t    and \\\\(y\\\\) groups in partition \\\\(P\\\\) respectively.\n\t    Moreover, \\\\(l^0_{ \\\\sigma_{\\\\alpha} \\\\sigma_{\\\\beta}} \\\\)\n\t    and \\\\(r^0_{ \\\\sigma_{\\\\alpha} \\\\sigma_{\\\\beta}} \\\\)\n\t    are the number of links and maximum possible links in the observed network\n\t    between groups \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n", "        input Graph (a networkx Graph)\n\t    n: int :\n\t        number of samples\n\t    p: float :\n\t        chaos probability, higher means more chaos in\n\t        the matrix\n\t         (Default value = .05)\n\t    seed: int :\n\t        seed for the random generated values\n\t         (Default value = 42)\n", "    Returns\n\t    -------\n\t    R: csr_matrix : the Similarity Matrix (in sparse format)\n\t    \"\"\"\n\t    def __random_changes(A_0: csr_matrix, p: float = .05) -> lil_matrix:\n\t        \"\"\"Perform random changes in the Adjacency Matrix\n\t        This function is used to generate chaos in the network\n\t        for the sample (blocks) collection.\n\t        Parameters\n\t        ----------\n", "        A_0: csr_matrix :\n\t            the original Adjacency Matrix\n\t        p: float :\n\t            chaos probability, higher means more\n\t            chaos\n\t             (Default value = 0.05)\n\t        Returns\n\t        -------\n\t        A_chaos: lil_matrix : the new chaos matrix\n\t        \"\"\"\n", "        A_chaos = A_0.copy().tolil()\n\t        num_changes = int(A_chaos.shape[0]**2 * p)\n\t        indexes = []\n\t        while len(indexes) < num_changes:\n\t            x = np.random.randint(0, high=A_chaos.shape[0])\n\t            y = np.random.randint(0, high=A_chaos.shape[0])\n\t            if x == y:\n\t                continue\n\t            if (x, y) in indexes or (y, x) in indexes:\n\t                continue\n", "            indexes.append((x, y))\n\t        for x, y in indexes:\n\t            A_chaos[x, y] = abs(A_0[x, y] - 1)\n\t            A_chaos[y, x] = A_chaos[x, y]\n\t        return A_chaos\n\t    def __generate_samples(A_0: csr_matrix,\n\t                           n: int,\n\t                           p: np.float32 = .05,\n\t                           seed: int = 42) -> List[List[Set]]:\n\t        \"\"\"Generate the samples (block) given the orginal\n", "        adjacecy matrix.\n\t        Parameters\n\t        ----------\n\t        A_0: csr_matrix :\n\t            the original Adjacency Matrix\n\t        n: int :\n\t            number of samples\n\t        p: float :\n\t            chaos probability, higher means more\n\t            chaos\n", "             (Default value = 0.05)\n\t        seed: int :\n\t            seed for random generated values\n\t             (Default values = 42)\n\t        Returns\n\t        -------\n\t        samples_list: List[List[Set]] : all the samples\n\t        \"\"\"\n\t        samples_list = []\n\t        for _ in range(n):\n", "            A_chaos = __random_changes(A_0, p)\n\t            G_chaos = nx.Graph(A_chaos)\n\t            res = louvain_communities(G_chaos, seed)\n\t            samples_list.append(res)\n\t        return samples_list\n\t    def __get_node_block(sample: List[Set], x: int) -> Set:\n\t        \"\"\"Return the block that node X belongs to\n\t        Parameters\n\t        ----------\n\t        samole: List[Set] :\n", "            current sample to analize\n\t        x: int :\n\t            node\n\t        Returns\n\t        -------\n\t        Set : the block\n\t        \"\"\"\n\t        for i in sample:\n\t            if x in i:\n\t                return i\n", "    def __l(A_0: lil_matrix, alpha: Set, beta: Set) -> int:\n\t        \"\"\"Number of links between groups Alpha and Beta\"\"\"\n\t        return np.sum([A_0[x, y] for x, y in product(alpha, beta)])\n\t    def __r(alpha: Set, beta: Set) -> int:\n\t        \"\"\"Maxmimum possible links between groups Alpha and Beta\"\"\"\n\t        len_a = len(alpha)\n\t        len_b = len(beta)\n\t        if alpha == beta:\n\t            return len_a * (len_a - 1)\n\t        return (len_a * len_b)\n", "    def __H(A_0: csr_matrix, P: List[Set]) -> float:\n\t        res = 0\n\t        for a, b in combinations_with_replacement(P, 2):\n\t            r = __r(a, b)\n\t            l0 = __l(A_0, a, b)\n\t            res += np.log(r + 1) + np.log(comb(r, l0))\n\t        return res\n\t    def __link_reliability(A_0: csr_matrix, samples_list: List[List[Set]],\n\t                           x: int, y: int) -> float:\n\t        summing_result = 0\n", "        HP_mem = []\n\t        for sample in samples_list:\n\t            x_block = __get_node_block(sample, x)\n\t            y_block = __get_node_block(sample, y)\n\t            l_xy = __l(A_0, x_block, y_block) + 1\n\t            r_xy = __r(x_block, y_block) + 2\n\t            second_term = np.exp(-__H(A_0, sample))\n\t            HP_mem.append(second_term)\n\t            tmp_ratio = l_xy / r_xy\n\t            summing_result += tmp_ratio * second_term\n", "        return summing_result / np.sum(HP_mem)\n\t    np.random.seed(seed)\n\t    A_0 = to_adjacency_matrix(G)\n\t    samples_list = __generate_samples(A_0, n, p, seed=seed)\n\t    nodes_to_indexes_map = nodes_to_indexes(G)\n\t    R = lil_matrix(A_0.shape)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = nodes_to_indexes_map[x]\n\t        _y = nodes_to_indexes_map[y]\n\t        R[_x, _y] = __link_reliability(A_0, samples_list, _x, _y)\n", "    return R.tocsr()\n\tif __name__ == \"__main__\":\n\t    import matplotlib.pyplot as plt\n\t    G = nx.karate_club_graph()\n\t    A = to_adjacency_matrix(G)\n\t    res = stochastic_block_model(G, 10)\n\t    predicted_edges = []\n\t    for u in range(res.shape[0]):\n\t        for v in range(u + 1, res.shape[1]):\n\t            if G.has_edge(u, v):\n", "                continue\n\t            w = res[u, v]\n\t            predicted_edges.append((u, v, w))\n\t    # Sort the predicted edges by weight in descending order\n\t    predicted_edges.sort(key=lambda x: x[2], reverse=True)\n\t    # Print the predicted edges with their weight score\n\t    print(\"Top Predicted edges:\")\n\t    for edge in predicted_edges[:50]:\n\t        print(f\"({edge[0]}, {edge[1]}): {edge[2]}\")\n\t    nx.draw(G)\n", "    plt.show()\n"]}
{"filename": "cnlp/__init__.py", "chunked_list": []}
{"filename": "cnlp/utils.py", "chunked_list": ["\"\"\"Some utilities functions.\"\"\"\n\timport networkx as nx\n\timport numpy as np\n\tfrom typing import Union\n\tfrom scipy.sparse import csr_matrix, csc_matrix, lil_matrix\n\tfrom typing import Dict, List, Tuple\n\tdef nodes_to_indexes(G: nx.Graph) -> dict[any, int]:\n\t    \"\"\"Node Label - Index encoder\n\t    Associate, for each node label, and index starting from 0.\n\t    Parameters\n", "    ----------\n\t    G: nx.Graph :\n\t        the graph from which you want the node-to-index mapping\n\t    Returns\n\t    -------\n\t    Dict[Any, int]: the encoding Node Label - Index dictionary\n\t    Notes\n\t    -----\n\t    The method `Graph.nodes` return the nodes in the exactly same order, and\n\t    the first node (at index 0) represent the index 0 in the Adjacency Matrix\n", "    obtained with the method `Graph.to_adjacency_matrix` or\n\t    `Graph.to_numpy_array`.\n\t    \"\"\"\n\t    return {node_name: index for index, node_name in enumerate(G.nodes)}\n\tdef to_adjacency_matrix(G: nx.Graph,\n\t                        sparse: bool = True) -> Union[csc_matrix, np.ndarray]:\n\t    \"\"\"Convert a ginven Graph in to its Adjacency Matrix\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n", "        input Graph (a networkx Graph)\n\t    sparse: bool:\n\t        if True, return the Adjacency Matrix in sparse format,\n\t        otherwise in full format.\n\t         (Default value = True)\n\t    Returns\n\t    -------\n\t    csc_matrix | np.ndarray: the Adjacency Matrix\n\t    \"\"\"\n\t    # TODO: ricontrollare se i pesi servono\n", "    return nx.adjacency_matrix(\n\t        G, weight=None) if sparse else nx.to_numpy_array(G, weight=None)\n\tdef only_unconnected(graph: nx.Graph, sim_matrix: lil_matrix) -> csr_matrix:\n\t    \"\"\"Filter the given matrix and return only previously unconnected\n\t    nodes \"similarity\" values\n\t    Parameters\n\t    ----------\n\t    graph: nx.Graph :\n\t        input graph\n\t    sim_matrix: csr_matrix :\n", "        similarity matrix\n\t    Returns\n\t    -------\n\t    sim_matrix: csr_matrix : the similarity matrix without the previously\n\t    connected nodes similarity\n\t    \"\"\"\n\t    node_idexies_map = nodes_to_indexes(graph)\n\t    for x, y in graph.edges():\n\t        sim_matrix[node_idexies_map[x], node_idexies_map[y]] = 0\n\t    sim_matrix = sim_matrix.tocsr()\n", "    sim_matrix.eliminate_zeros()\n\t    return sim_matrix.tocsr()\n\tdef get_top_predicted_link(predicted_adj_matrix: csr_matrix,\n\t                           number_of_nodes: int,\n\t                           pct_new_link: float,\n\t                           name_index_map: Dict[any, int],\n\t                           verbose: bool = False) -> List[Tuple[any, any]]:\n\t    \"\"\"Get the edges (new link) with the highest predicted probabilities\n\t                           verbose: bool = False):\n\t    Parameters\n", "    ----------\n\t    predicted_adj_matrix: csr_matrix,\n\t        input Similarity Matrix\n\t    number_of_nodes: int :\n\t        number of node in the graph\n\t    pct_new_link: float :\n\t        top x% best new links\n\t    name_index_map: Dict[any, int] :\n\t        node to index (starting from 0) mapping\n\t    verbose: bool :\n", "        if True print some usefull outputs\n\t         (Default value = False)\n\t    Returns\n\t    -------\n\t    new_link: List[Tuple[any, any]] : top % new link predicted\n\t    \"\"\"\n\t    max_possible_edges = predicted_adj_matrix.nnz\n\t    number_of_new_link = int(np.ceil(max_possible_edges / 100 * pct_new_link))\n\t    edges = []\n\t    new_link = []\n", "    for i in range(number_of_nodes):\n\t        for j in range(i + 1, number_of_nodes):\n\t            prob = predicted_adj_matrix[i, j]\n\t            edges.append((i, j, prob))\n\t    edges = sorted(edges, key=lambda x: x[2],\n\t                   reverse=True)[:number_of_new_link]\n\t    new_link = []\n\t    for edge in edges:\n\t        if verbose:\n\t            print(\n", "                f\"({name_index_map[edge[0]][0]}, {name_index_map[edge[1]][0]}) - Similarity: {edge[2]:.2f}\"\n\t            )\n\t        new_link.append(\n\t            (name_index_map[edge[0]][0], name_index_map[edge[1]][0]))\n\t    return new_link\n"]}
{"filename": "cnlp/similarity_methods/quasi_local_similarity.py", "chunked_list": ["\"\"\"Collection of Quasi-local Similarity Methods for Link Prediction.\n\tQuasi-local indices have been introduced as a trade-off between\n\tlocal and global approaches or performance and complexity.\n\tThese metrics are as efficient to compute as local indices.\n\tSome of these indices extract the entire topological information\n\tof the network.\n\tThe time complexities of these indices are still below compared\n\tto the global approaches.\n\t\"\"\"\n\timport networkx as nx\n", "import numpy as np\n\tfrom cnlp.utils import to_adjacency_matrix, nodes_to_indexes, only_unconnected\n\tfrom scipy.sparse import csr_matrix, lil_matrix\n\tdef local_path_index(G: nx.Graph, epsilon: float, n: int) -> csr_matrix:\n\t    \"\"\"Compute the Local Path  Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S^{LP} = A^{2} + \\\\epsilon A^{3} +\n\t        \\\\epsilon^{2} A^{4} + \\\\ldots + \\\\epsilon^{n - 2} A^{n}\n\t    where \\\\(\\\\epsilon\\\\) is a free parameter,\n", "    \\\\(A\\\\) is the Adjacency Matrix and \\\\(n\\\\) is the maximal order.\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    epsilon: float :\n\t        free parameter\n\t    n: int :\n\t        maximal order\n\t    Returns\n", "    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    This metric has the intent to furnish a good trade-off\n\t    between accuracy and computational complexity.\n\t    Clearly, the measurement converges to common neighbor when\n\t    \\\\(\\\\epsilon = 0\\\\). If there is no direct connection between\n\t    \\\\(x\\\\) and \\\\(y\\\\), \\\\((A^{3})_{x,y}\\\\) is equated to the total\n\t    different paths of length 3 between \\\\(x\\\\) and \\\\(y\\\\).\n", "    Computing this index becomes more complicated with the increasing\n\t    value of \\\\(n\\\\). The LP index outperforms the proximity-based indices,\n\t    such as RA, AA, and CN.\n\t    \"\"\"\n\t    A = to_adjacency_matrix(G)\n\t    A = A @ A\n\t    S = np.power(epsilon, 0) * (A)\n\t    # Calculate the remaining terms of the sum\n\t    for i in range(1, n - 2):\n\t        A = A @ A\n", "        S += np.power(epsilon, i) * (A)\n\t    return only_unconnected(G, lil_matrix(S))\n\tdef path_of_length_three(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Path of Length Three Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = \\\\sum \\\\frac{a_{x,u}a_{u,v}a_{v,y}}{k_u k_v}\n\t    where \\\\(k_x\\\\) is the degree of node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n", "    G: nx.Graph :\n\t        grafo da analizzare\n\t    Returns\n\t    -------\n\t    S: csr_matrix : matrice di Similarità\n\t    Notes\n\t    -----\n\t    Georg Simmel, a German sociologist, first coined the concept\n\t    “triadic closure” and made popular by Mark Granovetter in his work\n\t    “The Strength of Weak Ties”. The authors proposed a similarity index\n", "    in protein-protein interaction (PPI) network,\n\t    called path of length 3 (or L3) published in the\n\t    Nature Communication. They experimentally show that the triadic closure\n\t    principle (TCP) does not work well with PPI networks. They showed the\n\t    paradoxical behavior of the TCP (i.e., the path of length 2),\n\t    which does not follow the structural and evolutionary mechanism that\n\t    governs protein interaction. The TCP predicts well to the interaction of\n\t    self-interaction proteins (SIPs), which are very small (4%) in PPI networks\n\t    and fails in prediction between SIP and non SIP that amounts to 96%.\n\t    \"\"\"\n", "    def __path_of_length_three_iter(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Path of Length Three Index for 2 given nodes.\"\"\"\n\t        k_x = G.degree(x)\n\t        k_y = G.degree(y)\n\t        score = 0\n\t        # Enroll all neighbors\n\t        for u in G[x]:\n\t            for v in G[y]:\n\t                # Base case\n\t                if u == v:\n", "                    continue\n\t                # Calcolate the score with the multiply of\n\t                # value of node and divide for degree\n\t                if G.has_edge(u, v):\n\t                    a_xu = G[x][u].get(\n\t                        'weight',\n\t                        1)  # prende il 'peso' dell'arco (1 se non ha peso)\n\t                    a_uv = G[u][v].get('weight', 1)\n\t                    a_vy = G[v][y].get('weight', 1)\n\t                    score += (a_xu * a_uv * a_vy) / (k_x * k_y)\n", "        return score\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    name_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = name_index_map[x]\n\t        _y = name_index_map[y]\n\t        S[_x, _y] = __path_of_length_three_iter(G, x, y)\n\t        S[_y, _x] = S[_x, _y]\n\t    return S.tocsr()\n"]}
{"filename": "cnlp/similarity_methods/local_similarity.py", "chunked_list": ["\"\"\"Collection of Local Similarity Methods for Link Prediction.\n\tLocal indices are generally calculated using information about\n\tcommon neighbors and node degree.\n\tThese indices **consider immediate neighbors of a node**.\n\t\"\"\"\n\timport networkx as nx\n\timport numpy as np\n\tfrom cnlp.utils import nodes_to_indexes\n\tfrom scipy.sparse import lil_matrix, csr_matrix\n\tdef adamic_adar(G: nx.Graph) -> csr_matrix:\n", "    \"\"\"Compute the Adamic and Adar Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = \\\\sum_{z \\\\in \\\\Gamma(x) \\\\cap \\\\Gamma(y)}\n\t        \\\\frac{1}{\\\\log k_z}\n\t    where \\\\(k_z\\\\) is the degree of node \\\\(z\\\\)\n\t    and \\\\(\\\\Gamma(x)\\\\) are the neighbors of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n", "        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    It is clear from the equation that more weights are assigned to\n\t    the common neighbors having smaller degrees.\n\t    This is also intuitive in the real-world scenario, for example,\n\t    a person with more number of friends spend less time/resource\n", "    with an individual friend as compared to the less number of friends.\n\t    \"\"\"\n\t    def __adamic_adar(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Adamic and Adar Index for 2 given nodes.\"\"\"\n\t        return sum([1 / np.log(G.degree[z]) for z in set(G[x]) & set(G[y])])\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n", "        _y = node_index_map[y]\n\t        S[_x, _y] = __adamic_adar(G, x, y)\n\t    return S.tocsr()\n\tdef __common_neighbors(G: nx.Graph, x, y) -> int:\n\t    \"\"\"Compute the Common Neighbors Index for 2 given nodes.\"\"\"\n\t    return len(set(G[x]).intersection(set(G[y])))\n\tdef common_neighbors(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Common Neighbors Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n", "        S(x, y) = |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|\n\t    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n", "    -----\n\t    The likelihood of the existence of a link between \\\\(x\\\\)\n\t    and \\\\(y\\\\) increases with the number of common neighbors between them.\n\t    \"\"\"\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n", "        S[_x, _y] = __common_neighbors(G, x, y)\n\t        # S[_y, _x] = S[_x, _y]\n\t    return S.tocsr()\n\tdef cosine_similarity(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Cosine Similarity Index\n\t    (a.k.a. Salton Index) for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = \\\\frac{|\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{\\\\sqrt{k_x k_y}}\n\t    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n", "    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n", "    This similarity index between two nodes is measured by\n\t    calculating the Cosine of the angle between them.\n\t    The metric is all about the orientation and not magnitude.\n\t    \"\"\"\n\t    def __cosine_similarity(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Cosine Similarity Index for 2 given nodes.\"\"\"\n\t        return __common_neighbors(G, x, y) / np.sqrt(G.degree[x] * G.degree[y])\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n", "    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __cosine_similarity(G, x, y)\n\t        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n\tdef hub_depressed(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Hub Depressed Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n", "        S(x, y) = \\\\frac{2 |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{\\\\max(k_x, k_y)}\n\t    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n\t    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n", "    Notes\n\t    -----\n\t    This index is the same as the previous one but with the\n\t    opposite goal as it avoids the formation of\n\t    links between hubs and low degree nodes in the networks.\n\t    The Hub depressed index promotes the links evolution\n\t    between the hubs as well as the low degree nodes.\n\t    \"\"\"\n\t    def __hub_depressed(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Hub Depressed Index for 2 given nodes.\"\"\"\n", "        return __common_neighbors(G, x, y) / max(G.degree[x], G.degree[y])\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __hub_depressed(G, x, y)\n\t        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n", "def hub_promoted(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Hub Promoted Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = \\\\frac{2 |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{\\\\min(k_x, k_y)}\n\t    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n\t    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n", "        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    This similarity index promotes the formation of links between\n\t    the sparsely connected nodes and hubs.\n\t    It also tries to prevent links formation between the hub nodes.\n\t    \"\"\"\n", "    def __hub_promoted(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Hub Promoted Index for 2 given nodes.\"\"\"\n\t        return __common_neighbors(G, x, y) / min(G.degree[x], G.degree[y])\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __hub_promoted(G, x, y)\n", "        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n\tdef jaccard(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Jaccard Coefficient for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = \\\\frac{| \\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{| \\\\Gamma(x) \\\\cup \\\\Gamma(y)|}\n\t    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n", "    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    The Jaccard coefficient is defined as the probability of selection\n\t    of common neighbors of pairwise vertices from all the neighbors of\n\t    either vertex. The pairwise Jaccard score increases with the number of\n", "    common neighbors between the two vertices considered. Some researcher\n\t    (**Liben-Nowell et al.**) demonstrated that this similarity metric\n\t    performs worse as compared to Common Neighbors.\n\t    \"\"\"\n\t    def __jaccard(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Jaccard Coefficient for 2 given nodes.\"\"\"\n\t        total_neighbor_number = len(set(G[x]).union(set(G[y])))\n\t        if total_neighbor_number == 0:\n\t            return 0\n\t        return __common_neighbors(G, x, y) / total_neighbor_number\n", "    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __jaccard(G, x, y)\n\t        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n\tdef node_clustering(G: nx.Graph) -> csr_matrix:\n", "    \"\"\"Compute the Hub Depressed Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = \\\\sum_{z \\\\in \\\\Gamma(x) \\\\cap \\\\Gamma(y)} C(z)\n\t    where\n\t    .. math::\n\t        C(z) = \\\\frac{t(z)}{k_z(k_z - 1)}\n\t    is the clustering coefficient of node \\\\(z\\\\), \\\\(t(z)\\\\)\n\t    is the total triangles passing through the node \\\\(z\\\\),\n\t    \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n", "    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n", "    This index is also based on the clustering coefficient property\n\t    of the network in which the clustering coefficients of all\n\t    the common neighbors of a seed node pair are computed\n\t    and summed to find the final similarity score of the pair.\n\t    \"\"\"\n\t    def __t(G: nx.Graph, z) -> int:\n\t        \"\"\"Number of triangles passing through the node z\"\"\"\n\t        return nx.triangles(G, z)\n\t    def __C(G: nx.Graph, z) -> float:\n\t        \"\"\"Clustering Coefficient\"\"\"\n", "        z_degree = G.degree[z]\n\t        # avoiding 0 divition error\n\t        if z_degree == 1:\n\t            return 0\n\t        return __t(G, z) / (z_degree * (z_degree - 1))\n\t    def __node_clustering(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Node Clustering Coefficient for 2 given nodes.\"\"\"\n\t        return sum([__C(G, z) for z in (set(G[x]) & set(G[y]))])\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n", "    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __node_clustering(G, x, y)\n\t        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n\tdef preferential_attachment(G: nx.Graph, sum: bool = False) -> csr_matrix:\n\t    \"\"\"Compute the Preferential Attachment Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n", "    .. math::\n\t        S(x, y) = k_x k_y\n\t    where \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    sum: bool :\n\t        Replace multiplication with summation when computing the index.\n\t         (Default value = False)\n", "    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    The idea of preferential attachment is applied to generate a growing\n\t    scale-free network. The term growing represents the incremental nature\n\t    of nodes over time in the network. The likelihood incrementing new\n\t    connection associated with a node \\\\(x\\\\) is proportional to\n\t    \\\\(k_x\\\\), the degree of the node.\n", "    This index shows the worst performance on most networks.\n\t    The **simplicity** (as it requires the least information\n\t    for the score calculation) and the computational time of this metric\n\t    are the main advantages. PA shows better results if larger\n\t    degree nodes are densely connected,\n\t    and lower degree nodes are rarely connected.\n\t    In the above equation, summation can also be used instead of\n\t    multiplication as an aggregate function (`sum = True`).\n\t    \"\"\"\n\t    def __preferential_attachment(G: nx.Graph,\n", "                                  x,\n\t                                  y,\n\t                                  sum: bool = False) -> float:\n\t        \"\"\"Compute the Preferential Attachment Index for 2 given nodes.\"\"\"\n\t        return G.degree[x] * G.degree[y] if not sum else G.degree[\n\t            x] + G.degree[y]\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n", "        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __preferential_attachment(G, x, y, sum=sum)\n\t        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n\tdef resource_allocation(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Resource Allocation Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = \\\\sum_{z \\\\in \\\\Gamma(x) \\\\cap \\\\Gamma(y)} \\\\frac{1}{k_z}\n", "    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\) and\n\t    \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n", "    -----\n\t    Consider two non-adjacent vertices \\\\(x\\\\) and \\\\(y\\\\).\n\t    Suppose node \\\\(x\\\\) sends some resources to \\\\(y\\\\)\n\t    through the common nodes of both \\\\(x\\\\) and \\\\(y\\\\)\n\t    then the similarity between the two vertices is computed in terms\n\t    of resources sent from \\\\(x\\\\) to \\\\(y\\\\).\n\t    The difference between Resource Allocation (**RA**) and\n\t    Adamic and Adar (**AA**) is that the RA index\n\t    heavily penalizes to higher degree nodes compared to the AA index.\n\t    Prediction results of these indices become almost the same\n", "    for smaller average degree networks.\n\t    This index shows good performance on heterogeneous\n\t    networks with a high clustering coefficient, especially\n\t    on transportation networks.\n\t    \"\"\"\n\t    def __resource_allocation(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Resource Allocation Index for 2 given nodes.\"\"\"\n\t        return sum([1 / G.degree[z] for z in set(G[x]) & set(G[y])])\n\t    size = G.number_of_nodes()\n\t    S = lil_matrix((size, size))\n", "    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __resource_allocation(G, x, y)\n\t        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n\tdef sorensen(G: nx.Graph) -> csr_matrix:\n\t    \"\"\"Compute the Sorensen Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n", "    .. math::\n\t        S(x, y) = \\\\frac{2 |\\\\Gamma(x) \\\\cap \\\\Gamma(y)|}{k_x + k_y}\n\t    where \\\\(\\\\Gamma(x)\\\\) are the neighbors of node \\\\(x\\\\)\n\t    and \\\\(k_x\\\\) is the degree of the node \\\\(x\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n\t    -------\n", "    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    It is very similar to the Jaccard index. **McCune et al.** show\n\t    that it is more robust than Jaccard against the outliers.\n\t    \"\"\"\n\t    def __sorensen(G: nx.Graph, x, y) -> float:\n\t        \"\"\"Compute the Sorensen Index for 2 given nodes.\"\"\"\n\t        return (2 * __common_neighbors(G, x, y)) / (G.degree[x] + G.degree[y])\n\t    size = G.number_of_nodes()\n", "    S = lil_matrix((size, size))\n\t    node_index_map = nodes_to_indexes(G)\n\t    for x, y in nx.complement(G).edges():\n\t        _x = node_index_map[x]\n\t        _y = node_index_map[y]\n\t        S[_x, _y] = __sorensen(G, x, y)\n\t        # S[y, x] = S[x, y]\n\t    return S.tocsr()\n"]}
{"filename": "cnlp/similarity_methods/__init__.py", "chunked_list": ["\"\"\"Similarity based Methods for Link Prediction.\n\tSimilarity-based metrics are the simplest one in link prediction,\n\tin which for each pair \\\\(x\\\\) and \\\\(y\\\\) , a similarity score\n\t\\\\(S(x, y)\\\\) is calculated.\n\tThe score \\\\(S(x, y)\\\\) is based on the structural or node’s properties of\n\tthe considered pair. The non-observed links (i.e., \\\\(U - E^T\\\\)) are assigned\n\tscores according to their similarities. **The pair of nodes having a higher\n\tscore represents the predicted link between them**.\n\tThe similarity measures between every pair can be _calculated using several\n\tproperties of the network_, one of which is structural property.\n", "Scores based on this property can be grouped in several categories\n\tlike **local** and **global**, and so on.\n\t\"\"\"\n"]}
{"filename": "cnlp/similarity_methods/global_similarity.py", "chunked_list": ["\"\"\"Collection of Global Similarity Methods for Link Prediction.\n\tGlobal indices are computed using entire\n\ttopological information of a network.\n\tThe computational complexities of such methods\n\tare higher and seem to be infeasible for large networks.\n\t\"\"\"\n\timport networkx as nx\n\timport numpy as np\n\tfrom scipy.sparse import csr_matrix, linalg, identity, lil_matrix, hstack, lil_array\n\tfrom cnlp.utils import nodes_to_indexes, only_unconnected, to_adjacency_matrix\n", "def katz_index(G: nx.Graph, beta: int = 1) -> csr_matrix:\n\t    \"\"\"Compute the Katz Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = (I - \\\\beta A)^{-1} - I\n\t    where \\\\(I\\\\) is the Identity Matrix,\n\t    \\\\(\\\\beta\\\\) is a dumping factor that controls the path weights\n\t    and \\\\(A\\\\) is the Adjacency Matrix\n\t    Parameters\n\t    ----------\n", "    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    beta: int :\n\t        Dumping Factor\n\t         (Default value = 1)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n", "    For the convergence of above equation,\n\t    .. math::\n\t        \\\\beta < \\\\frac{1}{\\\\lambda_1}\n\t    where \\\\(\\\\lambda_1\\\\) is the maximum eighenvalue of the matrix \\\\(A\\\\).\n\t    The computational complexity of the given metric is high,\n\t    and it can be roughly estimated to be cubic complexity\n\t    which is not feasible for a large network.\n\t    \"\"\"\n\t    def __power_method(A: csr_matrix,\n\t                       max_iterations: int = 100,\n", "                       tol: float = 1e-12,\n\t                       verbose: bool = False):\n\t        \"\"\"Perfome the Power Method\"\"\"\n\t        n = A.shape[0]\n\t        x = np.ones(n) / np.sqrt(n)  # initialize a vector x\n\t        # r = A @ x - np.dot(A @ x, x) * x # residual initialization\n\t        r = A @ x - ((A @ x) @ x) * x\n\t        eigenvalue = x @ (A @ x)  # residual eigenvalue\n\t        # eigenvalue = np.dot(x, A @ x) # residual eigenvalue\n\t        for i in range(max_iterations):\n", "            # Compute the new vector x\n\t            x = A @ x\n\t            # vector normalization\n\t            x = x / np.linalg.norm(x)\n\t            # Residual and eigenvalue computation\n\t            r = A @ x - ((A @ x) @ x) * x\n\t            eigenvalue = x @ (A @ x)\n\t            # r = A @ x - np.dot(A @ x, x) * x\n\t            # eigenvalue = np.dot(x, A @ x)\n\t            # If the norm of r is less than the tolerance, break out of the loop.\n", "            if np.linalg.norm(r) < tol:\n\t                if verbose:\n\t                    print(f'Computation done after {i} steps')\n\t                break\n\t        return eigenvalue, x\n\t    A = to_adjacency_matrix(G)\n\t    largest_eigenvalue = __power_method(A)  # lambda_1\n\t    if beta >= (1 / largest_eigenvalue[0]):\n\t        print(f'Warning, Beta should be less than {largest_eigenvalue}')\n\t    eye = identity(A.shape[0], format='csc')\n", "    S = linalg.inv((eye - beta * A.tocsc())) - eye\n\t    return only_unconnected(G, csr_matrix(S))\n\tdef link_prediction_rwr(G: nx.Graph,\n\t                        c: int = 0.05,\n\t                        max_iters: int = 10) -> csr_matrix:\n\t    \"\"\"Compute the Random Walk with Restart Algorithm.\n\t    The similarity between two nodes is defined as:\n\t    .. math::\n\t        S(x, y) = q_{xy} + q_{yx}\n\t    where \\\\(q_x\\\\) is defined as \\\\( (1-\\\\alpha) (I - \\\\alpha P^T)^{-1} e_x\\\\)\n", "    and \\\\(e_x\\\\) is the seed vector of length \\\\(|V|\\\\).\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    c: int :\n\t        TODO\n\t         (Default value = 0.05)\n\t    max_iters: int :\n\t        max number of iteration for the algorithm convergence\n", "         (Default value = 10)\n\t    Returns\n\t    -------\n\t    similarity_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    Let \\\\(\\\\alpha\\\\) be a probability that a random walker\n\t    iteratively moves to an arbitrary neighbor and returns to the same\n\t    starting vertex with probability \\\\( (1 - \\\\alpha )\\\\).\n\t    Consider \\\\(q_{xy}\\\\) to be the probability that a random walker\n", "    who starts walking from vertex x and located at the vertex y in steady-state.\n\t    The seed vector \\\\(e_x\\\\) consists of zeros for all components except the\n\t    elements \\\\(x\\\\) itself.\n\t    The transition matrix \\\\(P\\\\) can be expressed as\n\t    .. math::\n\t        P_{xy} = \\\\begin{cases}\n\t                \\\\frac{1}{k_x} & \\\\text{if } x \\\\text{ and } y \\\\text{ are connected,} \\\\\\\\\n\t                0 & \\\\text{otherwise.}\n\t            \\\\end{cases}\n\t    \"\"\"\n", "    def random_walk_with_restart(e: lil_array,\n\t                                 W_normalized: csr_matrix,\n\t                                 c: int = 0.05,\n\t                                 max_iters: int = 100) -> lil_array:\n\t        \"\"\"Generates the probability vector\n\t        Parameters\n\t        ----------\n\t        e: lil_array :\n\t            input probability vector\n\t        W_normalized: csr_matrix :\n", "            TODO\n\t        c: int :\n\t            TODO\n\t             (Default value = 0.05)\n\t        max_iters: int :\n\t            max number of iteration for the algorithm convergence\n\t             (Default value = 100)\n\t        Returns\n\t        -------\n\t        e: lil_array : the updated probability vector\n", "        \"\"\"\n\t        # Initialize the current probability vector to the initial one and the error to 1\n\t        old_e = e\n\t        err = 1.\n\t        # Perform the random walk with restart until the maximum number\n\t        # of iterations is reached or the error becomes less than 1e-6\n\t        for _ in range(max_iters):\n\t            e = (c * (W_normalized @ old_e)) + ((1 - c) * e)\n\t            err = linalg.norm(e - old_e, 1)\n\t            if err <= 1e-6:\n", "                break\n\t            old_e = e\n\t        # Return the current probability vector\n\t        return e\n\t    # Convert the graph G into an adjacency matrix A\n\t    A = to_adjacency_matrix(G)\n\t    # Extract the number of nodes of matrix A\n\t    m = A.shape[0]\n\t    # Initialize the diagonal matrix D as a sparse lil_matrix\n\t    D = lil_matrix(A.shape)\n", "    # Create a map that associates each node with a row index in matrix A\n\t    nodes_to_indexes_map = nodes_to_indexes(G)\n\t    # Build the diagonal matrix D so that the elements on the diagonal\n\t    # are equal to the degree of the corresponding node\n\t    for node in G.nodes():\n\t        D[nodes_to_indexes_map[node],\n\t          nodes_to_indexes_map[node]] = G.degree[node]\n\t    # Convert the diagonal matrix D into csc_matrix format\n\t    D = D.tocsc()\n\t    try:\n", "        # Build the normalized transition matrix W_normalized\n\t        W_normalized = linalg.inv(D) @ A.tocsc()\n\t    except RuntimeError as e:\n\t        print('Possible presence of singleton nodes in the graph G')\n\t        print(e)\n\t        exit(1)\n\t    # Initialize an matrix to hold the similarities between node pairs\n\t    # We put an initial column made of Zeros so we can use the hstack\n\t    # method later on and keep the code more clean\n\t    similarity_matrix = csr_matrix((m, 1))\n", "    # For each node i, create a probability vector and perform the\n\t    # random walk with restart starting from that node\n\t    for i in range(m):\n\t        e = lil_array((m, 1))\n\t        e[i, 0] = 1\n\t        # Concatenate the similarity vectors into a similarity matrix\n\t        # The use of hstack allows the lil_array returned from the\n\t        # random walk function to be transposed and added to the\n\t        # similarity matrix as a new column in just one line of code\n\t        similarity_matrix = hstack([\n", "            similarity_matrix,\n\t            random_walk_with_restart(e=e,\n\t                                     W_normalized=W_normalized,\n\t                                     c=c,\n\t                                     max_iters=max_iters)\n\t        ])\n\t    # Return the similarity matrix and remove the fisrt column\n\t    # In order to keep the results consistent without the added column of zeros at the beginning\n\t    return only_unconnected(G, csr_matrix(similarity_matrix)[:, 1:])\n\tdef rooted_page_rank(G: nx.Graph, alpha: float = .5) -> csr_matrix:\n", "    \"\"\"Compute the Rooted Page Rank for all nodes in the Graph.\n\t    This score is defined as:\n\t    .. math::\n\t        S = (1 - \\\\alpha) (I - \\\\alpha \\\\hat{N})^{-1}\n\t    where \\\\(\\\\hat{N} = D^{-1}A\\\\) is the normalized\n\t    Adjacency Matrix with the diagonal degree matrix\n\t    \\\\(D[i,i] = \\\\sum_j A[i,j]\\\\)\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n", "        input Graph (a networkx Graph)\n\t    alpha: float :\n\t        random walk probability\n\t         (Default value = 0.5)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n\t    Notes\n\t    -----\n\t    The idea of PageRank was originally proposed to rank the web pages based on\n", "    the importance of those pages. The algorithm is based on the assumption that\n\t    a random walker randomly goes to a web page with probability \\\\(\\\\alpha\\\\)\n\t    and follows hyper-link embedded in the page with probability \\\\( (1 - \\\\alpha ) \\\\).\n\t    Chung et al. used this concept incorporated with a random walk in\n\t    link prediction framework. The importance of web pages, in a random walk,\n\t    can be replaced by stationary distribution. The similarity between two vertices\n\t    \\\\(x\\\\) and \\\\(y\\\\) can be measured by the stationary probability of\n\t    \\\\(x\\\\) from \\\\(y\\\\) in a random walk where the walker moves to an\n\t    arbitrary neighboring vertex with probability \\\\(\\\\alpha\\\\)\n\t    and returns to \\\\(x\\\\) with probability \\\\( ( 1 - \\\\alpha )\\\\).\n", "    \"\"\"\n\t    A = to_adjacency_matrix(G)\n\t    D = lil_matrix(A.shape)\n\t    nodes_to_indexes_map = nodes_to_indexes(G)\n\t    for node in G.nodes():\n\t        D[nodes_to_indexes_map[node],\n\t          nodes_to_indexes_map[node]] = G.degree[node]\n\t    D = D.tocsc()\n\t    N_hat = linalg.inv(D) @ A.tocsc()\n\t    eye = identity(A.shape[0], format='csc')\n", "    S = (1 - alpha) * linalg.inv(eye - alpha * N_hat)\n\t    return only_unconnected(G, S.tocsr())\n\tdef shortest_path(G: nx.Graph, cutoff: int = None) -> csr_matrix:\n\t    \"\"\"Compute the Shortest Path Index for all nodes in the Graph.\n\t    Each similarity value is defined as:\n\t    .. math::\n\t        S(x, y) = - |d(x,y)|\n\t    where Dijkstra algorithm  is applied to efficiently\n\t    compute the shortest path \\\\(d(x, y)\\\\) between the\n\t    node pair \\\\( (x, y) \\\\).\n", "    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    cutoff: int :\n\t        max path length\n\t         (Default value = None)\n\t    Returns\n\t    -------\n\t    S: csr_matrix : the Similarity Matrix (in sparse format)\n", "    Notes\n\t    -----\n\t    Liben-Nowell et al. provided the shortest path with its negation as a\n\t    metric to link prediction.\n\t    The prediction accuracy\n\t    of this index is low compared to most local indices.\n\t    \"\"\"\n\t    dim = G.number_of_nodes()\n\t    if cutoff is None:\n\t        cutoff = dim\n", "    lengths = dict(nx.all_pairs_shortest_path_length(G, cutoff))\n\t    nodes_to_indexes_map = nodes_to_indexes(G)\n\t    prexisting_links = list(G.edges())\n\t    S = lil_matrix((dim, dim))\n\t    for source_node in lengths.keys():\n\t        for dest_node in lengths[source_node].keys():\n\t            # If the link already exists in the starting graph the computation is skipped\n\t            if (nodes_to_indexes_map[source_node],\n\t                    nodes_to_indexes_map[dest_node]) not in prexisting_links:\n\t                S[nodes_to_indexes_map[source_node],\n", "                  nodes_to_indexes_map[dest_node]] = -lengths[source_node][\n\t                      dest_node]\n\t    return S.tocsr()\n\tdef sim_rank(G: nx.Graph,\n\t             k: int = 5,\n\t             cutoff: int = 4,\n\t             c: int = 0.8) -> csr_matrix:\n\t    \"\"\"Compute the SimRank index for all the nodes in the Graph.\n\t    This method is defined as:\n\t    .. math::\n", "        S(x, y) = \\\\begin{cases}\n\t                \\\\frac{\\\\alpha}{k_x k_y} \\\\sum_{i=1}^{k_x} \\\\sum_{j=1}^{k_y}\n\t                    S( \\\\Gamma_i(x), \\\\Gamma_j(y)) & x \\\\neq y \\\\\\\\\n\t                1 & x = y\n\t            \\\\end{cases}\n\t    where \\\\( \\\\alpha \\\\in (0,1) \\\\) is a constant. \\\\(\\\\Gamma_i(x)\\\\) and \\\\( \\\\Gamma_j(y) \\\\)\n\t    are the ith and jth elements in the neighborhood sets.\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n", "        input Graph (a networkx Graph)\n\t    k: int :\n\t         (Default value = 5)\n\t    cutoff: int :\n\t         (Default value = 4)\n\t    c: int :\n\t         (Default value = 0.8)\n\t    Returns\n\t    -------\n\t    sim_matrix: csr_matrix : the Similarity Matrix (in sparse format)\n", "    \"\"\"\n\t    def init_similarity_matrix(G: nx.Graph, n: int) -> lil_matrix:\n\t        \"\"\"Generate an Identity matrix: the starting Similarity\n\t        Matrix.\n\t        Parameters\n\t        ----------\n\t        G: nx.Graph :\n\t            input Graph (a networkx Graph)\n\t        n: int :\n\t           the new matrix size\n", "        Returns\n\t        -------\n\t        sim_matrix: lil_matrix : the starting Similarity Matrix\n\t        \"\"\"\n\t        # inizializzo la matrice similarity\n\t        # gli elementi con loro stessi (lungo la diagonale) hanno similarità massima\n\t        sim_matrix = identity(n).tolil()\n\t        return sim_matrix\n\t    def compute_sim_rank(G: nx.Graph,\n\t                         a,\n", "                         b,\n\t                         sim_matrix: lil_matrix,\n\t                         C: int = 0.8) -> float:\n\t        \"\"\"Compute the Sim Rank method between the given\n\t        nodes a and b.\n\t        Parameters\n\t        ----------\n\t        G: nx.Graph :\n\t            input Graph (a networkx Graph)\n\t        a :\n", "           first node\n\t        b :\n\t           second node\n\t        sim_matrix: lil_matrix :\n\t            the similarity matrix\n\t        C: int :\n\t            free parameter\n\t             (Default value = 0.8)\n\t        Returns\n\t        -------\n", "        new_SimRank: float : the SimRank value between a and b\n\t        \"\"\"\n\t        # se i nodi sono uguali allora similarità massima\n\t        if (a == b):\n\t            return 1\n\t        a_neigh = list(G.neighbors(a))\n\t        b_neigh = list(G.neighbors(b))\n\t        len_a = len(a_neigh)\n\t        len_b = len(b_neigh)\n\t        # nodi isolati hanno similarità 0\n", "        if (len_a == 0 or len_b == 0):\n\t            return 0\n\t        # mi recupero e sommo i valori di similarità calcolati in precedenza\n\t        simRank_sum = 0\n\t        for i in a_neigh:\n\t            for j in b_neigh:\n\t                simRank_sum += sim_matrix[i, j]\n\t        # moltiplico secondo la definizione del paper\n\t        scale = C / (len_a * len_b)\n\t        new_SimRank = scale * simRank_sum\n", "        return new_SimRank\n\t    G = nx.convert_node_labels_to_integers(G, 0)\n\t    nodes_num = G.number_of_nodes()\n\t    sim_matrix = init_similarity_matrix(G, nodes_num)\n\t    for a in range(nodes_num):\n\t        for b in range(nodes_num):\n\t            # fa pruning evitando di calcolare la similarità di archi a distanza maggiore di 5\n\t            if (nx.has_path(G, a, b)\n\t                    and (nx.shortest_path_length(G, a, b) > cutoff)):\n\t                sim_matrix[a, b] = 0\n", "            else:\n\t                # se non deve fare pruning si calcola il valore di similarità per i nodi a e b\n\t                for i in range(k):\n\t                    sim_matrix[a, b] = compute_sim_rank(G,\n\t                                                        a,\n\t                                                        b,\n\t                                                        sim_matrix=sim_matrix,\n\t                                                        C=c)\n\t    # imposta a 0 gli elementi della diagonale che prima avevano similarità uguale ad 1\n\t    for a in range(nodes_num):\n", "        sim_matrix[a, a] = 0\n\t    return only_unconnected(G, sim_matrix)\n"]}
{"filename": "cnlp/other_methods/__init__.py", "chunked_list": ["import cnlp.other_methods.information_theory\n"]}
{"filename": "cnlp/other_methods/information_theory.py", "chunked_list": ["import networkx as nx\n\timport numpy as np\n\timport scipy.sparse as scipy\n\timport math\n\timport itertools\n\tfrom cnlp.utils import nodes_to_indexes\n\tfrom scipy.sparse import lil_matrix, csr_matrix\n\tfrom typing import Generator\n\tdef MI(G: nx.Graph) -> scipy.csr_matrix:\n\t    \"\"\"Neighbor Set Information\n", "    Il modello di link prediction basato su information theory che sfrutta la neighbor\n\t    set information è un approccio utilizzato per prevedere la probabilità di esistenza\n\t    di un link tra due nodi in una rete. In questo modello, l'informazione contenuta nei\n\t    neighbor set dei due nodi in questione viene utilizzata per stimare la probabilità\n\t    di connessione.\n\t    L'idea alla base di questo modello è che i nodi che hanno molti neighbor in comune\n\t    sono più propensi a essere connessi tra loro rispetto a nodi con neighbor set diversi.\n\t    Questo perché i nodi con neighbor set simili tendono a essere coinvolti in attività\n\t    simili all'interno della rete, come ad esempio\n\t    partecipare agli stessi gruppi o condividere gli stessi interessi.\n", "    Per utilizzare questa informazione per prevedere la probabilità di connessione tra due nodi,\n\t    il modello utilizza l'entropia di Shannon, una misura dell'incertezza di una\n\t    distribuzione di probabilità.\n\t    In particolare, l'entropia viene calcolata sui neighbor set dei due nodi, e la differenza tra le\n\t    entropie dei due set viene utilizzata per stimare la probabilità di connessione.\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    Returns\n", "    -------\n\t    res_sparse: csr_matrix : the Similarity Matrix (in sparse format)\n\t    \"\"\"\n\t    def overlap_info(G: nx.Graph, x, y, edge_num: int) -> float:\n\t        \"\"\"Two Information Definition.\n\t        Overlapping nodes of different sets and\n\t        the existence of link across different sets\n\t        Parameters\n\t        ----------\n\t        G: nx.Graph :\n", "            input graph\n\t        x :\n\t            first node\n\t        y :\n\t            second node\n\t        edge_num: int :\n\t            TODO !\n\t        Returns\n\t        -------\n\t        s_Overlap: float : TODO\n", "        \"\"\"\n\t        # ottenimento dei dati da cui ottenere le informazioni\n\t        o_nodes = nx.common_neighbors(G, x, y)\n\t        p_prob_overlap = -np.log2(prior(x, y, G, edge_num))\n\t        # utilizzo delle informazioni per stimarsi la likelihood\n\t        # con gli overlapping nodes\n\t        coeff = 0\n\t        overlap_info_value = 0\n\t        overlap = 0\n\t        for z in o_nodes:\n", "            # degree of z\n\t            kz = G.degree(z)\n\t            coeff = 1 / (kz * (kz - 1))\n\t            # sum over edges = neighbors of z\n\t            overlap = 0\n\t            for m, n in itertools.combinations(G.neighbors(z), 2):\n\t                priorInfo = -np.log2(prior(m, n, G, edge_num))\n\t                likelihoodInfo = -np.log2(likelihood(z, G))\n\t                # print(f\"a = {x}, b = {y}, priorInfo = { priorInfo},\n\t                #   lilelihoodInfo = {likelihoodInfo}\")\n", "                # combine mutual information\n\t                overlap += 2 * (priorInfo - likelihoodInfo)\n\t                # print(f\"a = {x}, b = {y}, zOverlap = { 2*(priorInfo -likelihoodInfo)}\")\n\t        # add average mutual information per neighbor\n\t        overlap_info_value += coeff * overlap\n\t        s_Overlap = overlap_info_value - p_prob_overlap\n\t        return s_Overlap\n\t    def prior(m, n, G: nx.Graph, edge_num: int) -> float:\n\t        \"\"\"Calcola la probabilità a priori dati due nodi e\n\t        un grafo riferita alla probabilità con cui non si forma un cammino\n", "        tra i due nodi\n\t        Parameters\n\t        ----------\n\t        m :\n\t            first node\n\t        n :\n\t            second node\n\t        G: nx.Graph :\n\t            input graph\n\t        edge_num: int :\n", "            TODO\n\t        Returns\n\t        -------\n\t        float: the prior probability\n\t        \"\"\"\n\t        kn = G.degree(n)\n\t        km = G.degree(m)\n\t        return 1 - math.comb(edge_num - kn, km) / math.comb(edge_num, km)\n\t    def likelihood(z, G: nx.Graph) -> float:\n\t        \"\"\"probabilità condizionata che in questo caso è definita come il clustering\n", "        coefficient dei common neighbor dei nodi x e y\n\t        Parameters\n\t        ----------\n\t        z :\n\t            input node\n\t        G: nx.Graph :\n\t            input graph\n\t        Returns\n\t        -------\n\t        float: TODO\n", "        \"\"\"\n\t        kz = G.degree(z)\n\t        N_triangles = nx.triangles(G, z)\n\t        N_triads = math.comb(kz, 2)\n\t        return N_triangles / N_triads\n\t    I_Oxy = 0\n\t    edge_num = G.number_of_edges()\n\t    node_num = G.number_of_nodes()\n\t    edge_num = G.number_of_edges()\n\t    res_sparse = scipy.lil_matrix((node_num, node_num))\n", "    nodes_to_indexes_map = nodes_to_indexes(G)\n\t    for i, j in nx.complement(G).edges():\n\t        I_Oxy = overlap_info(G, i, j, edge_num)\n\t        res_sparse[nodes_to_indexes_map[i], nodes_to_indexes_map[j]] = I_Oxy\n\t    return res_sparse.tocsr()\n\tdef path_entropy(G: nx.Graph, max_path: int = 3) -> csr_matrix:\n\t    \"\"\"Compute the Path Entropy Measure for all nodes in the Graph.\n\t    This Similarity measure between two nodes \\\\(X\\\\) and \\\\(Y\\\\)\n\t    is calculated with:\n\t    .. math::\n", "        S_{x,y} = -I(L^1_{xy}|U_{i=2}^{maxlen} D_{xy}^i)\n\t    where \\\\(D^i_{xy}\\\\) represents the set consisting of all simple\n\t    paths of length i between the two vertices and maxlen is the maximum\n\t    length of simple path of the network.\n\t    Parameters\n\t    ----------\n\t    G: nx.Graph :\n\t        input Graph (a networkx Graph)\n\t    max_path: int :\n\t        maximal path length\n", "         (Default value = 3)\n\t    Returns\n\t    -------\n\t    similarity_matrix: csr_matix: the Similarity Matrix (in sparse format)\n\t    \"\"\"\n\t    def simple_path_entropy(paths: Generator[list, None, None],\n\t                            G: nx.Graph) -> float:\n\t        \"\"\"Calcola l'entropia data dalla probabilità che si vengano a creare\n\t        i vari simple paths tra i nodi tra cui si\n\t        vuole fare link prediction\n", "        Parameters\n\t        ----------\n\t        paths: Generator[List] :\n\t            generator ritornato dalla funzione nx.all_simple_paths()\n\t        G: nx.Graph :\n\t            input graph\n\t        Returns\n\t        -------\n\t        float: simple path entropy\n\t        \"\"\"\n", "        tmp = .0\n\t        for path in paths:\n\t            for a, b in list(nx.utils.pairwise(path)):\n\t                tmp += new_link_entropy(G, a, b)\n\t        return tmp\n\t    def new_link_entropy(G: nx.Graph, a, b) -> float:\n\t        \"\"\"Calcola l'entropia basata sulla probabilità\n\t        a priori della creazione del link diretto\n\t        tra le coppie di noti senza link diretti\n\t        Parameters\n", "        ----------\n\t        G: nx.Graph :\n\t            input graph\n\t        a :\n\t            first node\n\t        b :\n\t            second node\n\t        Returns\n\t        -------\n\t        float: entropy between node A and B\n", "        \"\"\"\n\t        deg_a = G.degree(a)\n\t        deg_b = G.degree(b)\n\t        M = G.number_of_edges()\n\t        return -1 * np.log2(1 - (math.comb(M - deg_a, deg_b) /\n\t                                   math.comb(M, deg_b)))\n\t    similarity_matrix = lil_matrix((G.number_of_nodes(), G.number_of_nodes()))\n\t    nodes_to_indexes_map = nodes_to_indexes(G)\n\t    missing_edges = list(nx.complement(G).edges())\n\t    for elem in missing_edges:\n", "        paths = nx.all_simple_paths(G, elem[0], elem[1], max_path)\n\t        tmp = 0\n\t        for i in range(2, (max_path + 1)):\n\t            tmp += (1 / (i - 1)) * simple_path_entropy(paths=paths, G=G)\n\t        tmp = tmp - new_link_entropy(G, elem[0], elem[1])\n\t        similarity_matrix[nodes_to_indexes_map[elem[0]],\n\t                          nodes_to_indexes_map[elem[1]]] = tmp\n\t    return similarity_matrix.tocsr()\n\tif __name__ == \"__main__\":\n\t    import matplotlib.pyplot as plt\n", "    G = nx.karate_club_graph()\n\t    # converte gli id dei nodi in interi affinche possano essere usati come indici\n\t    # G_to_int = nx.convert_node_labels_to_integers(G, 0)\n\t    nx.draw(G, with_labels=True)\n\t    ranking = MI(G)\n\t    # da aggiungere informazioni dei nodi che hanno fatto ottentere il\n\t    # ranking migliore\n\t    # va preso il risultato più piccolo perchè si tratta di entropia\n\t    print(ranking)\n\t    for i, j in nx.complement(G).edges():\n", "        if (ranking[i, j] == ranking.toarray().min()):\n\t            print(\n\t                f\"Il link più probabile tra quelli possibili è tra {i} e {j}, con un valore di {ranking[i,j]}\"\n\t            )\n\t    plt.show()\n"]}
