{"filename": "main.py", "chunked_list": ["\"\"\"Main function for this repo\n\t\"\"\"\n\timport ast\n\timport argparse\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    #data\n\t    parser.add_argument('--phase', type=str, default='graphtrain', choices=['pretrain', 'finetune',\n\t                                                                            'prototrain', 'protoeval',\n\t                                                                            'mptitrain', 'mptieval'])\n", "    parser.add_argument('--dataset', type=str, default='s3dis', help='Dataset name: s3dis|scannet')\n\t    parser.add_argument('--cvfold', type=int, default=0, help='Fold left-out for testing in leave-one-out setting '\n\t                                                              'Options:{0,1}')\n\t    parser.add_argument('--data_path', type=str, default='./datasets/S3DIS/blocks_bs1_s1',\n\t                                                    help='Directory to the source data')\n\t    parser.add_argument('--pretrain_checkpoint_path', type=str, default=None,\n\t                        help='Path to the checkpoint of pre model for resuming')\n\t    parser.add_argument('--model_checkpoint_path', type=str, default=None,\n\t                        help='Path to the checkpoint of model for resuming')\n\t    parser.add_argument('--save_path', type=str, default='./log_s3dis/',\n", "                        help='Directory to the save log and checkpoints')\n\t    parser.add_argument('--eval_interval', type=int, default=1500,\n\t                        help='iteration/epoch inverval to evaluate model')\n\t    #optimization\n\t    parser.add_argument('--batch_size', type=int, default=32, help='Number of samples/tasks in one batch')\n\t    parser.add_argument('--n_workers', type=int, default=16, help='number of workers to load data')\n\t    parser.add_argument('--n_iters', type=int, default=30000, help='number of iterations/epochs to train')\n\t    parser.add_argument('--lr', type=float, default=0.001,\n\t                        help='Model (eg. protoNet or MPTI) learning rate [default: 0.001]')\n\t    parser.add_argument('--step_size', type=int, default=5000, help='Iterations of learning rate decay')\n", "    parser.add_argument('--gamma', type=float, default=0.5, help='Multiplicative factor of learning rate decay')\n\t    parser.add_argument('--pretrain_lr', type=float, default=0.001, help='pretrain learning rate [default: 0.001]')\n\t    parser.add_argument('--pretrain_weight_decay', type=float, default=0., help='weight decay for regularization')\n\t    parser.add_argument('--pretrain_step_size', type=int, default=50, help='Period of learning rate decay')\n\t    parser.add_argument('--pretrain_gamma', type=float, default=0.5, help='Multiplicative factor of learning rate decay')\n\t    #few-shot episode setting\n\t    parser.add_argument('--n_way', type=int, default=2, help='Number of classes for each episode: 1|3')\n\t    parser.add_argument('--k_shot', type=int, default=1, help='Number of samples/shots for each class: 1|5')\n\t    parser.add_argument('--n_queries', type=int, default=1, help='Number of queries for each class')\n\t    parser.add_argument('--n_episode_test', type=int, default=100,\n", "                        help='Number of episode per configuration during testing')\n\t    # Point cloud processing\n\t    parser.add_argument('--pc_npts', type=int, default=2048, help='Number of input points for PointNet.')\n\t    parser.add_argument('--pc_attribs', default='xyzrgbXYZ',\n\t                        help='Point attributes fed to PointNets, if empty then all possible. '\n\t                             'xyz = coordinates, rgb = color, XYZ = normalized xyz')\n\t    parser.add_argument('--pc_augm', action='store_true', help='Training augmentation for points in each superpoint')\n\t    parser.add_argument('--pc_augm_scale', type=float, default=0,\n\t                        help='Training augmentation: Uniformly random scaling in [1/scale, scale]')\n\t    parser.add_argument('--pc_augm_rot', type=int, default=1,\n", "                        help='Training augmentation: Bool, random rotation around z-axis')\n\t    parser.add_argument('--pc_augm_mirror_prob', type=float, default=0,\n\t                        help='Training augmentation: Probability of mirroring about x or y axes')\n\t    parser.add_argument('--pc_augm_jitter', type=int, default=1,\n\t                        help='Training augmentation: Bool, Gaussian jittering of all attributes')\n\t    parser.add_argument('--pc_augm_shift', type=float, default=0,\n\t                        help='Training augmentation: Probability of shifting points')\n\t    parser.add_argument('--pc_augm_color', type=int, default=0,\n\t                        help='Training augmentation: Bool, random color of all attributes')\n\t    # feature extraction network configuration\n", "    parser.add_argument('--dgcnn_k', type=int, default=20, help='Number of nearest neighbors in Edgeconv')\n\t    parser.add_argument('--edgeconv_widths', default='[[64,64], [64,64], [64,64]]', help='DGCNN Edgeconv widths')\n\t    parser.add_argument('--dgcnn_mlp_widths', default='[512, 256]', help='DGCNN MLP (following stacked Edgeconv) widths')\n\t    parser.add_argument('--base_widths', default='[128, 64]', help='BaseLearner widths')\n\t    parser.add_argument('--output_dim', type=int, default=64,\n\t                        help='The dimension of the final output of attention learner or linear mapper')\n\t    parser.add_argument('--use_attention', action='store_true', help='if incorporate attention learner')\n\t    # protoNet configuration\n\t    parser.add_argument('--dist_method', default='euclidean',\n\t                        help='Method to compute distance between query feature maps and prototypes.[Option: cosine|euclidean]')\n", "    # PAPFZS3D configuration\n\t    parser.add_argument('--use_align', action='store_true', help='if incorporate alignment process')\n\t    parser.add_argument('--use_high_dgcnn', action='store_true', help='if incorporate another dgcnn')\n\t    parser.add_argument('--use_supervise_prototype', action='store_true', help='if incorporate self-reconstruction process')\n\t    parser.add_argument('--use_transformer', action='store_true', help='if incorporate transformer process')\n\t    parser.add_argument('--use_linear_proj', action='store_true', help='if incorporate linear projection process')\n\t    parser.add_argument('--embedding_type', type=str, default='word2vec', help='semantic input')\n\t    parser.add_argument('--use_zero', action='store_true', help='if incorporate zero-shot learning process')\n\t    parser.add_argument('--trans_lr', type=float, default=0.0001, help='transformer learning rate')\n\t    parser.add_argument('--generator_lr', type=float, default=0.0002, help='generator learning rate')\n", "    parser.add_argument('--noise_dim', type=int, default=300, help='noise dim for generator')\n\t    parser.add_argument('--gmm_dropout', type=float, default=0.1, help='drop out rate for generator')\n\t    parser.add_argument('--gmm_weight', type=float, default=0.1, help='training weight for generator')\n\t    parser.add_argument('--train_dim', type=int, default=320, help='training dim for transformer')\n\t    # MPTI configuration\n\t    parser.add_argument('--n_subprototypes', type=int, default=100,\n\t                        help='Number of prototypes for each class in support set')\n\t    parser.add_argument('--k_connect', type=int, default=200,\n\t                        help='Number of nearest neighbors to construct local-constrained affinity matrix')\n\t    parser.add_argument('--sigma', type=float, default=1., help='hyeprparameter in gaussian similarity function')\n", "    args = parser.parse_args()\n\t    args.edgeconv_widths = ast.literal_eval(args.edgeconv_widths)\n\t    args.dgcnn_mlp_widths = ast.literal_eval(args.dgcnn_mlp_widths)\n\t    args.base_widths = ast.literal_eval(args.base_widths)\n\t    args.pc_in_dim = len(args.pc_attribs)\n\t    # Start trainer for pre-train, proto-train, proto-eval, mpti-train, mpti-test\n\t    if args.phase == 'mptitrain':\n\t        args.log_dir = args.save_path + 'log_mpti_%s_S%d_N%d_K%d_Att%d' % (args.dataset, args.cvfold,\n\t                                                                             args.n_way, args.k_shot,\n\t                                                                             args.use_attention)\n", "        from runs.mpti_train import train\n\t        train(args)\n\t    elif args.phase == 'prototrain':\n\t        args.log_dir = args.save_path + 'log_proto_%s_S%d_N%d_K%d_Att%d' % (args.dataset, args.cvfold,\n\t                                                                             args.n_way, args.k_shot,\n\t                                                                             args.use_attention)\n\t        from runs.proto_train import train\n\t        train(args)\n\t    elif args.phase == 'protoeval' or args.phase == 'mptieval':\n\t        args.log_dir = args.model_checkpoint_path\n", "        from runs.eval import eval\n\t        eval(args)\n\t    elif args.phase == 'pretrain':\n\t        args.log_dir = args.save_path + 'log_pretrain_%s_S%d' % (args.dataset, args.cvfold)\n\t        from runs.pre_train import pretrain\n\t        pretrain(args)\n\t    elif args.phase == 'finetune':\n\t        args.log_dir = args.save_path + 'log_finetune_%s_S%d_N%d_K%d' % (args.dataset, args.cvfold,\n\t                                                                            args.n_way, args.k_shot)\n\t        from runs.fine_tune import finetune\n", "        finetune(args)\n\t    else:\n\t        raise ValueError('Please set correct phase.')"]}
{"filename": "runs/proto_train.py", "chunked_list": ["\"\"\" Prototypical Network for Few-shot 3D Point Cloud Semantic Segmentation [Baseline]\n\t\"\"\"\n\timport os\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom runs.eval import test_few_shot\n\tfrom dataloaders.loader import MyDataset, MyTestDataset, batch_test_task_collate\n\tfrom models.proto_learner import ProtoLearner\n\tfrom models.proto_learner_FZ import ProtoLearnerFZ\n", "from utils.cuda_util import cast_cuda\n\tfrom utils.logger import init_logger\n\tdef train(args):\n\t    logger = init_logger(args.log_dir, args)\n\t    # init model and optimizer\n\t    if args.use_zero:\n\t        PL = ProtoLearnerFZ(args)\n\t    else:\n\t        PL = ProtoLearner(args)\n\t    #Init datasets, dataloaders, and writer\n", "    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n\t                         'rot': args.pc_augm_rot,\n\t                         'mirror_prob': args.pc_augm_mirror_prob,\n\t                         'jitter': args.pc_augm_jitter,\n\t                         'shift': args.pc_augm_shift,\n\t                         'random_color': args.pc_augm_color,\n\t                         }\n\t    TRAIN_DATASET = MyDataset(args.data_path, args.dataset, cvfold=args.cvfold, num_episode=args.n_iters,\n\t                              n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n\t                              phase=args.phase, mode='train',\n", "                              num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n\t                              pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\t    VALID_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n\t                                  num_episode_per_comb=args.n_episode_test,\n\t                                  n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n\t                                  num_point=args.pc_npts, pc_attribs=args.pc_attribs)\n\t    VALID_CLASSES = list(VALID_DATASET.classes)\n\t    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\t    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\t    WRITER = SummaryWriter(log_dir=args.log_dir)\n", "    # train\n\t    best_iou = 0\n\t    import time\n\t    for batch_idx, (data, sampled_classes) in enumerate(TRAIN_LOADER):\n\t        if torch.cuda.is_available():\n\t            data = cast_cuda(data)\n\t        loss, accuracy = PL.train(data, sampled_classes)\n\t        if (batch_idx+1) % 100 == 0:\n\t            logger.cprint('=====[Train] Iter: %d | Loss: %.4f | Accuracy: %f =====' % (batch_idx, loss, accuracy))\n\t            WRITER.add_scalar('Train/loss', loss, batch_idx)\n", "            WRITER.add_scalar('Train/accuracy', accuracy, batch_idx)\n\t        if (batch_idx+1) % args.eval_interval == 0:\n\t            valid_loss, mean_IoU = test_few_shot(VALID_LOADER, PL, logger, VALID_CLASSES, args.use_zero)\n\t            WRITER.add_scalar('Valid/loss', valid_loss, batch_idx)\n\t            WRITER.add_scalar('Valid/meanIoU', mean_IoU, batch_idx)\n\t            if mean_IoU > best_iou:\n\t                best_iou = mean_IoU\n\t                logger.cprint('*******************Model Saved*******************')\n\t                save_dict = {'iteration': batch_idx + 1,\n\t                             'model_state_dict': PL.model.state_dict(),\n", "                             'loss': valid_loss,\n\t                             'IoU': best_iou\n\t                             }\n\t                torch.save(save_dict, os.path.join(args.log_dir, 'checkpoint.tar'))\n\t            logger.cprint('=====Best IoU Is: %f =====' % (best_iou))\n\t    WRITER.close()\n"]}
{"filename": "runs/fine_tune.py", "chunked_list": ["\"\"\" Finetune Baseline for Few-shot 3D Point Cloud Semantic Segmentation\n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch import optim\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom runs.eval import evaluate_metric\n\tfrom runs.pre_train import DGCNNSeg\n", "from models.dgcnn import DGCNN\n\tfrom dataloaders.loader import MyTestDataset, batch_test_task_collate, augment_pointcloud\n\tfrom utils.logger import init_logger\n\tfrom utils.cuda_util import cast_cuda\n\tfrom utils.checkpoint_util import load_pretrain_checkpoint\n\tclass FineTuner(object):\n\t    def __init__(self, args):\n\t        self.n_way = args.n_way\n\t        self.k_shot = args.k_shot\n\t        self.n_queries = args.n_queries\n", "        self.n_points = args.pc_npts\n\t        # init model and optimizer\n\t        self.model = DGCNNSeg(args, self.n_way+1)\n\t        print(self.model)\n\t        if torch.cuda.is_available():\n\t            self.model.cuda()\n\t        self.optimizer = torch.optim.Adam(self.model.segmenter.parameters(), lr=args.lr)\n\t        # load pretrained model for point cloud encoding\n\t        self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n\t    def train(self, support_x, support_y):\n", "        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way*k_shot, in_channels, num_points)\n\t            support_y: support masks (foreground) with shape (n_way*k_shot, num_points), each point \\in {0,..., n_way}\n\t        \"\"\"\n\t        support_logits = self.model(support_x)\n\t        train_loss = F.cross_entropy(support_logits, support_y)\n\t        self.optimizer.zero_grad()\n\t        train_loss.backward()\n\t        self.optimizer.step()\n", "        return train_loss\n\t    def test(self, query_x, query_y):\n\t        \"\"\"\n\t        Args:\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n\t        \"\"\"\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            query_logits = self.model(query_x)\n", "            test_loss = F.cross_entropy(query_logits, query_y)\n\t            pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n\t            correct = torch.eq(pred, query_y).sum().item()\n\t            accuracy = correct / (self.n_queries*self.n_points)\n\t        return pred, test_loss, accuracy\n\tdef support_mask_to_label(support_masks, n_way, k_shot, num_points):\n\t    \"\"\"\n\t    Args:\n\t        support_masks: binary (foreground/background) masks with shape (n_way, k_shot, num_points)\n\t    \"\"\"\n", "    support_masks = support_masks.view(n_way, k_shot*num_points)\n\t    support_labels = []\n\t    for n in range(support_masks.shape[0]):\n\t        support_mask = support_masks[n, :] #(k_shot*num_points)\n\t        support_label = torch.zeros_like(support_mask)\n\t        mask_index = torch.nonzero(support_mask).squeeze(1)\n\t        support_label= support_label.scatter_(0, mask_index, n+1)\n\t        support_labels.append(support_label)\n\t    support_labels = torch.stack(support_labels, dim=0)\n\t    support_labels = support_labels.view(n_way, k_shot, num_points)\n", "    return support_labels.long()\n\tdef finetune(args):\n\t    num_iters = args.n_iters\n\t    logger = init_logger(args.log_dir, args)\n\t    #Init datasets, dataloaders, and writer\n\t    DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n\t                                 num_episode_per_comb=args.n_episode_test,\n\t                                 n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n\t                                 num_point=args.pc_npts, pc_attribs=args.pc_attribs, mode='test')\n\t    CLASSES = list(DATASET.classes)\n", "    DATA_LOADER = DataLoader(DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\t    WRITER = SummaryWriter(log_dir=args.log_dir)\n\t    #Init model and optimizer\n\t    FT = FineTuner(args)\n\t    predicted_label_total = []\n\t    gt_label_total = []\n\t    label2class_total = []\n\t    global_iter = 0\n\t    for batch_idx, (data, sampled_classes) in enumerate(DATA_LOADER):\n\t        query_label = data[-1]\n", "        data[1] = support_mask_to_label(data[1], args.n_way, args.k_shot, args.pc_npts)\n\t        if torch.cuda.is_available():\n\t            data = cast_cuda(data)\n\t        [support_x, support_y, query_x, query_y] = data\n\t        support_x = support_x.view(args.n_way * args.k_shot, -1, args.pc_npts)\n\t        support_y = support_y.view(args.n_way * args.k_shot, args.pc_npts)\n\t        # train on support set\n\t        for i in range(num_iters):\n\t            train_loss = FT.train(support_x, support_y)\n\t            WRITER.add_scalar('Train/loss', train_loss, global_iter)\n", "            logger.cprint('=====[Train] Batch_idx: %d | Iter: %d | Loss: %.4f =====' % (batch_idx, i, train_loss.item()))\n\t            global_iter += 1\n\t        # test on query set\n\t        query_pred, test_loss, accuracy = FT.test(query_x, query_y)\n\t        WRITER.add_scalar('Test/loss', test_loss, global_iter)\n\t        WRITER.add_scalar('Test/accuracy', accuracy, global_iter)\n\t        logger.cprint(\n\t            '=====[Valid] Batch_idx: %d | Loss: %.4f =====' % (batch_idx, test_loss.item()))\n\t        #compute metric for predictions\n\t        predicted_label_total.append(query_pred.cpu().detach().numpy())\n", "        gt_label_total.append(query_label.numpy())\n\t        label2class_total.append(sampled_classes)\n\t    mean_IoU = evaluate_metric(logger, predicted_label_total, gt_label_total, label2class_total, CLASSES)\n\t    logger.cprint('\\n=====[Test] Mean IoU: %f =====\\n' % mean_IoU)\n"]}
{"filename": "runs/__init__.py", "chunked_list": []}
{"filename": "runs/mpti_train.py", "chunked_list": ["\"\"\" Attetion-aware Multi-Prototype Transductive Inference for Few-shot 3D Point Cloud Semantic Segmentation [Our method]\n\t\"\"\"\n\timport os\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.tensorboard import SummaryWriter\n\tfrom runs.eval import test_few_shot\n\tfrom dataloaders.loader import MyDataset, MyTestDataset, batch_test_task_collate\n\tfrom models.mpti_learner import MPTILearner\n\tfrom utils.cuda_util import cast_cuda\n", "from utils.logger import init_logger\n\tdef train(args):\n\t    logger = init_logger(args.log_dir, args)\n\t    # os.system('cp models/mpti_learner.py %s' % (args.log_dir))\n\t    # os.system('cp models/mpti.py %s' % (args.log_dir))\n\t    # os.system('cp models/dgcnn.py %s' % (args.log_dir))\n\t    # init model and optimizer\n\t    MPTI = MPTILearner(args)\n\t    #Init datasets, dataloaders, and writer\n\t    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n", "                         'rot': args.pc_augm_rot,\n\t                         'mirror_prob': args.pc_augm_mirror_prob,\n\t                         'shift': args.pc_augm_shift,\n\t                         'jitter': args.pc_augm_jitter\n\t                         }\n\t    TRAIN_DATASET = MyDataset(args.data_path, args.dataset, cvfold=args.cvfold, num_episode=args.n_iters,\n\t                              n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n\t                              phase=args.phase, mode='train',\n\t                              num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n\t                              pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n", "    VALID_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n\t                                  num_episode_per_comb=args.n_episode_test,\n\t                                  n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n\t                                  num_point=args.pc_npts, pc_attribs=args.pc_attribs)\n\t    VALID_CLASSES = list(VALID_DATASET.classes)\n\t    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\t    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\t    WRITER = SummaryWriter(log_dir=args.log_dir)\n\t    # train\n\t    best_iou = 0\n", "    for batch_idx, (data, sampled_classes) in enumerate(TRAIN_LOADER):\n\t        if torch.cuda.is_available():\n\t            data = cast_cuda(data)\n\t        loss, accuracy = MPTI.train(data)\n\t        if (batch_idx+1) % 100 == 0:\n\t            logger.cprint('==[Train] Iter: %d | Loss: %.4f |  Accuracy: %f  ==' % (batch_idx, loss, accuracy))\n\t            WRITER.add_scalar('Train/loss', loss, batch_idx)\n\t            WRITER.add_scalar('Train/accuracy', accuracy, batch_idx)\n\t        if (batch_idx+1) % args.eval_interval == 0:\n\t            valid_loss, mean_IoU = test_few_shot(VALID_LOADER, MPTI, logger, VALID_CLASSES)\n", "            logger.cprint('\\n=====[VALID] Loss: %.4f | Mean IoU: %f  =====\\n' % (valid_loss, mean_IoU))\n\t            WRITER.add_scalar('Valid/loss', valid_loss, batch_idx)\n\t            WRITER.add_scalar('Valid/meanIoU', mean_IoU, batch_idx)\n\t            if mean_IoU > best_iou:\n\t                best_iou = mean_IoU\n\t                logger.cprint('*******************Model Saved*******************')\n\t                save_dict = {'iteration': batch_idx + 1,\n\t                             'model_state_dict': MPTI.model.state_dict(),\n\t                             'optimizer_state_dict': MPTI.optimizer.state_dict(),\n\t                             'loss': valid_loss,\n", "                             'IoU': best_iou\n\t                             }\n\t                torch.save(save_dict, os.path.join(args.log_dir, 'checkpoint.tar'))\n\t    WRITER.close()"]}
{"filename": "runs/eval.py", "chunked_list": ["\"\"\"Evaluating functions for Few-shot 3D Point Cloud Semantic Segmentation\n\t\"\"\"\n\timport os\n\timport numpy as np\n\tfrom datetime import datetime\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom dataloaders.loader import MyTestDataset, batch_test_task_collate\n\tfrom models.proto_learner import ProtoLearner\n\tfrom models.proto_learner_FZ import ProtoLearnerFZ\n", "from models.mpti_learner import MPTILearner\n\tfrom utils.cuda_util import cast_cuda\n\tfrom utils.logger import init_logger\n\tdef evaluate_metric(logger, pred_labels_list, gt_labels_list, label2class_list, test_classes):\n\t    \"\"\"\n\t    :param pred_labels_list: a list of np array, each entry with shape (n_queries*n_way, num_points).\n\t    :param gt_labels_list: a list of np array, each entry with shape (n_queries*n_way, num_points).\n\t    :param test_classes: a list of np array, each entry with shape (n_way,)\n\t    :return: iou: scaler\n\t    \"\"\"\n", "    assert len(pred_labels_list) == len(gt_labels_list) == len(label2class_list)\n\t    logger.cprint('*****Test Classes: {0}*****'.format(test_classes))\n\t    NUM_CLASS = len(test_classes) + 1  # add 1 to consider background class\n\t    gt_classes = [0 for _ in range(NUM_CLASS)]\n\t    positive_classes = [0 for _ in range(NUM_CLASS)]\n\t    true_positive_classes = [0 for _ in range(NUM_CLASS)]\n\t    for i, batch_gt_labels in enumerate(gt_labels_list):\n\t        batch_pred_labels = pred_labels_list[i]  # (n_queries*n_way, num_points)\n\t        label2class = label2class_list[i]  # (n_way,)\n\t        for j in range(batch_pred_labels.shape[0]):\n", "            for k in range(batch_pred_labels.shape[1]):\n\t                gt = int(batch_gt_labels[j, k])\n\t                pred = int(batch_pred_labels[j, k])\n\t                if gt == 0:  # 0 indicate background class\n\t                    gt_index = 0\n\t                else:\n\t                    gt_class = label2class[gt - 1]  # the ground truth class in the dataset\n\t                    gt_index = test_classes.index(gt_class) + 1\n\t                gt_classes[gt_index] += 1\n\t                if pred == 0:\n", "                    pred_index = 0\n\t                else:\n\t                    pred_class = label2class[pred - 1]\n\t                    pred_index = test_classes.index(pred_class) + 1\n\t                positive_classes[pred_index] += 1\n\t                true_positive_classes[gt_index] += int(gt == pred)\n\t    iou_list = []\n\t    for c in range(NUM_CLASS):\n\t        iou = true_positive_classes[c] / float(gt_classes[c] + positive_classes[c] - true_positive_classes[c])\n\t        logger.cprint('----- [class %d]  IoU: %f -----' % (c, iou))\n", "        iou_list.append(iou)\n\t    mean_IoU = np.array(iou_list[1:]).mean()\n\t    return mean_IoU\n\tdef test_few_shot(test_loader, learner, logger, test_classes, use_zero=False):\n\t    total_loss = 0\n\t    predicted_label_total = []\n\t    gt_label_total = []\n\t    label2class_total = []\n\t    start_time = time.time()\n\t    print(str(datetime.now()))\n", "    for batch_idx, (data, sampled_classes) in enumerate(test_loader):\n\t        query_label = data[-1]\n\t        if torch.cuda.is_available():\n\t            data = cast_cuda(data)\n\t        if use_zero:\n\t            query_pred, loss, accuracy = learner.test_semantic(data, sampled_classes)\n\t        else:\n\t            query_pred, loss, accuracy = learner.test(data)\n\t        total_loss += loss.detach().item()\n\t        if (batch_idx + 1) % 100 == 0:\n", "            logger.cprint('[Eval] Iter: %d | Loss: %.4f | %s' % (batch_idx + 1, loss.detach().item(), str(datetime.now())))\n\t        predicted_label_total.append(query_pred.cpu().detach().numpy())\n\t        gt_label_total.append(query_label.numpy())\n\t        label2class_total.append(sampled_classes)\n\t    mean_loss = total_loss / len(test_loader)\n\t    mean_IoU = evaluate_metric(logger, predicted_label_total, gt_label_total, label2class_total, test_classes)\n\t    return mean_loss, mean_IoU\n\timport time\n\tdef eval(args):\n\t    logger = init_logger(args.log_dir, args)\n", "    if args.phase == 'protoeval':\n\t        if args.use_zero:\n\t            learner = ProtoLearnerFZ(args, mode='test')\n\t        else:\n\t            learner = ProtoLearner(args, mode='test')\n\t    elif args.phase == 'mptieval':\n\t        learner = MPTILearner(args, mode='test')\n\t    # Init dataset, dataloader\n\t    TEST_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n\t                                 num_episode_per_comb=args.n_episode_test,\n", "                                 n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n\t                                 num_point=args.pc_npts, pc_attribs=args.pc_attribs, mode='test')\n\t    TEST_CLASSES = list(TEST_DATASET.classes)\n\t    TEST_LOADER = DataLoader(TEST_DATASET, batch_size=1, shuffle=False, collate_fn=batch_test_task_collate)\n\t    test_loss, mean_IoU = test_few_shot(TEST_LOADER, learner, logger, TEST_CLASSES, args.use_zero)\n\t    logger.cprint('\\n=====[TEST] Loss: %.4f | Mean IoU: %f =====\\n' % (test_loss, mean_IoU))\n"]}
{"filename": "runs/pre_train.py", "chunked_list": ["\"\"\" Pre-train phase\n\t\"\"\"\n\timport os\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch import optim\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.tensorboard import SummaryWriter\n", "from dataloaders.loader import MyPretrainDataset\n\tfrom models.dgcnn import DGCNN\n\tfrom models.dgcnn_new import DGCNN_semseg\n\tfrom utils.logger import init_logger\n\tfrom utils.checkpoint_util import save_pretrain_checkpoint\n\tclass DGCNNSeg(nn.Module):\n\t    def __init__(self, args, num_classes):\n\t        super(DGCNNSeg, self).__init__()\n\t        if args.use_high_dgcnn:\n\t            self.encoder = DGCNN_semseg(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n", "        else:\n\t            self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n\t        in_dim = args.dgcnn_mlp_widths[-1]\n\t        for edgeconv_width in args.edgeconv_widths:\n\t            in_dim += edgeconv_width[-1]\n\t        self.segmenter = nn.Sequential(\n\t                            nn.Conv1d(in_dim, 256, 1, bias=False),\n\t                            nn.BatchNorm1d(256),\n\t                            nn.LeakyReLU(0.2),\n\t                            nn.Conv1d(256, 128, 1),\n", "                            nn.BatchNorm1d(128),\n\t                            nn.LeakyReLU(0.2),\n\t                            nn.Dropout(0.3),\n\t                            nn.Conv1d(128, num_classes, 1)\n\t                         )\n\t    def forward(self, pc):\n\t        num_points = pc.shape[2]\n\t        edgeconv_feats, point_feat, _ = self.encoder(pc)\n\t        global_feat = point_feat.max(dim=-1, keepdim=True)[0]\n\t        edgeconv_feats.append(global_feat.expand(-1,-1,num_points))\n", "        pc_feat = torch.cat(edgeconv_feats, dim=1)\n\t        logits = self.segmenter(pc_feat)\n\t        return logits\n\tdef metric_evaluate(predicted_label, gt_label, NUM_CLASS):\n\t    \"\"\"\n\t    :param predicted_label: (B,N) tensor\n\t    :param gt_label: (B,N) tensor\n\t    :return: iou: scaler\n\t    \"\"\"\n\t    gt_classes = [0 for _ in range(NUM_CLASS)]\n", "    positive_classes = [0 for _ in range(NUM_CLASS)]\n\t    true_positive_classes = [0 for _ in range(NUM_CLASS)]\n\t    for i in range(gt_label.size()[0]):\n\t        pred_pc = predicted_label[i]\n\t        gt_pc = gt_label[i]\n\t        for j in range(gt_pc.shape[0]):\n\t            gt_l = int(gt_pc[j])\n\t            pred_l = int(pred_pc[j])\n\t            gt_classes[gt_l] += 1\n\t            positive_classes[pred_l] += 1\n", "            true_positive_classes[gt_l] += int(gt_l == pred_l)\n\t    oa = sum(true_positive_classes)/float(sum(positive_classes))\n\t    print('Overall accuracy: {0}'.format(oa))\n\t    iou_list = []\n\t    for i in range(NUM_CLASS):\n\t        iou_class = true_positive_classes[i] / float(gt_classes[i]+positive_classes[i]-true_positive_classes[i])\n\t        print('Class_%d: iou_class is %f' % (i, iou_class))\n\t        iou_list.append(iou_class)\n\t    mean_IoU = np.array(iou_list[1:]).mean()\n\t    return oa, mean_IoU, iou_list\n", "def pretrain(args):\n\t    logger = init_logger(args.log_dir, args)\n\t    # Init datasets, dataloaders, and writer\n\t    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n\t                         'rot': args.pc_augm_rot,\n\t                         'mirror_prob': args.pc_augm_mirror_prob,\n\t                         'jitter': args.pc_augm_jitter,\n\t                         'shift': args.pc_augm_shift,\n\t                         'random_color': args.pc_augm_color,\n\t                         }\n", "    if args.dataset == 's3dis':\n\t        from dataloaders.s3dis import S3DISDataset\n\t        DATASET = S3DISDataset(args.cvfold, args.data_path)\n\t    elif args.dataset == 'scannet':\n\t        from dataloaders.scannet import ScanNetDataset\n\t        DATASET = ScanNetDataset(args.cvfold, args.data_path)\n\t    else:\n\t        raise NotImplementedError('Unknown dataset %s!' % args.dataset)\n\t    CLASSES = DATASET.train_classes\n\t    NUM_CLASSES = len(CLASSES) + 1\n", "    CLASS2SCANS = {c: DATASET.class2scans[c] for c in CLASSES}\n\t    TRAIN_DATASET = MyPretrainDataset(args.data_path, CLASSES, CLASS2SCANS, mode='train',\n\t                                      num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n\t                                      pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\t    VALID_DATASET = MyPretrainDataset(args.data_path, CLASSES, CLASS2SCANS, mode='test',\n\t                                      num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n\t                                      pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\t    logger.cprint('=== Pre-train Dataset (classes: {0}) | Train: {1} blocks | Valid: {2} blocks ==='.format(\n\t                                                     CLASSES, len(TRAIN_DATASET), len(VALID_DATASET)))\n\t    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=args.batch_size, num_workers=args.n_workers, shuffle=True,\n", "                              drop_last=True)\n\t    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=args.batch_size, num_workers=args.n_workers, shuffle=False,\n\t                              drop_last=True)\n\t    WRITER = SummaryWriter(log_dir=args.log_dir)\n\t    # Init model and optimizer\n\t    model = DGCNNSeg(args, num_classes=NUM_CLASSES)\n\t    print(model)\n\t    if torch.cuda.is_available():\n\t        model.cuda()\n\t    optimizer = optim.Adam([{'params': model.encoder.parameters(), 'lr': args.pretrain_lr}, \\\n", "                           {'params': model.segmenter.parameters(), 'lr': args.pretrain_lr}], \\\n\t                            weight_decay=args.pretrain_weight_decay)\n\t    # Set learning rate scheduler\n\t    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.pretrain_step_size, gamma=args.pretrain_gamma)\n\t    # train\n\t    best_iou = 0\n\t    global_iter = 0\n\t    for epoch in range(args.n_iters):\n\t        model.train()\n\t        for batch_idx, (ptclouds, labels) in enumerate(TRAIN_LOADER):\n", "            if torch.cuda.is_available():\n\t                ptclouds = ptclouds.cuda()\n\t                labels = labels.cuda()\n\t            logits = model(ptclouds)\n\t            loss = F.cross_entropy(logits, labels)\n\t            # Loss backwards and optimizer updates\n\t            optimizer.zero_grad()\n\t            loss.backward()\n\t            optimizer.step()\n\t            if (batch_idx + 1) % 100 == 0:\n", "                WRITER.add_scalar('Train/loss', loss, global_iter)\n\t                logger.cprint('=====[Train] Epoch: %d | Iter: %d | Loss: %.4f =====' % (epoch, batch_idx, loss.item()))\n\t            global_iter += 1\n\t        lr_scheduler.step()\n\t        if (epoch+1) % args.eval_interval == 0:\n\t            pred_total = []\n\t            gt_total = []\n\t            model.eval()\n\t            with torch.no_grad():\n\t                for i, (ptclouds, labels) in enumerate(VALID_LOADER):\n", "                    gt_total.append(labels.detach())\n\t                    if torch.cuda.is_available():\n\t                        ptclouds = ptclouds.cuda()\n\t                        labels = labels.cuda()\n\t                    logits = model(ptclouds)\n\t                    loss = F.cross_entropy(logits, labels)\n\t                    # 　Compute predictions\n\t                    _, preds = torch.max(logits.detach(), dim=1, keepdim=False)\n\t                    pred_total.append(preds.cpu().detach())\n\t                    WRITER.add_scalar('Valid/loss', loss, global_iter)\n", "                    # logger.cprint(\n\t                    #     '=====[Valid] Epoch: %d | Iter: %d | Loss: %.4f =====' % (epoch, i, loss.item()))\n\t            pred_total = torch.stack(pred_total, dim=0).view(-1, args.pc_npts)\n\t            gt_total = torch.stack(gt_total, dim=0).view(-1, args.pc_npts)\n\t            accuracy, mIoU, iou_perclass = metric_evaluate(pred_total, gt_total, NUM_CLASSES)\n\t            logger.cprint('===== EPOCH [%d]: Accuracy: %f | mIoU: %f =====\\n' % (epoch, accuracy, mIoU))\n\t            WRITER.add_scalar('Valid/overall_accuracy', accuracy, global_iter)\n\t            WRITER.add_scalar('Valid/meanIoU', mIoU, global_iter)\n\t            if mIoU > best_iou:\n\t                best_iou = mIoU\n", "                logger.cprint('*******************Model Saved*******************')\n\t                save_pretrain_checkpoint(model, args.log_dir)\n\t            logger.cprint('=====Best IoU Is: %f =====' % (best_iou))\n\t    WRITER.close()\n"]}
{"filename": "dataloaders/loader.py", "chunked_list": ["\"\"\" Data Loader for Generating Tasks\n\t\"\"\"\n\timport os\n\timport random\n\timport math\n\timport glob\n\timport numpy as np\n\timport h5py as h5\n\timport transforms3d\n\tfrom itertools import combinations\n", "import torch\n\tfrom torch.utils.data import Dataset\n\tdef sample_K_pointclouds(data_path, num_point, pc_attribs, pc_augm, pc_augm_config,\n\t                         scan_names, sampled_class, sampled_classes, is_support=False):\n\t    '''sample K pointclouds and the corresponding labels for one class (one_way)'''\n\t    ptclouds  = []\n\t    labels = []\n\t    for scan_name in scan_names:\n\t        ptcloud, label = sample_pointcloud(data_path, num_point, pc_attribs, pc_augm, pc_augm_config,\n\t                                           scan_name, sampled_classes, sampled_class, support=is_support)\n", "        ptclouds.append(ptcloud)\n\t        labels.append(label)\n\t    ptclouds = np.stack(ptclouds, axis=0)\n\t    labels = np.stack(labels, axis=0)\n\t    return ptclouds, labels\n\tdef sample_pointcloud(data_path, num_point, pc_attribs, pc_augm, pc_augm_config, scan_name,\n\t                      sampled_classes, sampled_class=0, support=False, random_sample=False):\n\t    sampled_classes = list(sampled_classes)\n\t    data = np.load(os.path.join(data_path, 'data', '%s.npy' %scan_name))\n\t    N = data.shape[0] #number of points in this scan\n", "    if random_sample:\n\t        sampled_point_inds = np.random.choice(np.arange(N), num_point, replace=(N < num_point))\n\t    else:\n\t        # If this point cloud is for support/query set, make sure that the sampled points contain target class\n\t        valid_point_inds = np.nonzero(data[:,6] == sampled_class)[0]  # indices of points belonging to the sampled class\n\t        if N < num_point:\n\t            sampled_valid_point_num = len(valid_point_inds)\n\t        else:\n\t            valid_ratio = len(valid_point_inds)/float(N)\n\t            sampled_valid_point_num = int(valid_ratio*num_point)\n", "        sampled_valid_point_inds = np.random.choice(valid_point_inds, sampled_valid_point_num, replace=False)\n\t        sampled_other_point_inds = np.random.choice(np.arange(N), num_point-sampled_valid_point_num,\n\t                                                    replace=(N<num_point))\n\t        sampled_point_inds = np.concatenate([sampled_valid_point_inds, sampled_other_point_inds])\n\t    data = data[sampled_point_inds]\n\t    xyz = data[:, 0:3]\n\t    rgb = data[:, 3:6]\n\t    labels = data[:, 6].astype(np.int)\n\t    xyz_min = np.amin(xyz, axis=0)\n\t    xyz -= xyz_min\n", "    if pc_augm:\n\t        xyz = augment_pointcloud(xyz, pc_augm_config)\n\t    if 'XYZ' in pc_attribs:\n\t        xyz_min = np.amin(xyz, axis=0)\n\t        XYZ = xyz - xyz_min\n\t        xyz_max = np.amax(XYZ, axis=0)\n\t        XYZ = XYZ/xyz_max\n\t    ptcloud = []\n\t    if 'xyz' in pc_attribs: ptcloud.append(xyz)\n\t    if 'rgb' in pc_attribs: ptcloud.append(rgb/255.)\n", "    if 'XYZ' in pc_attribs: ptcloud.append(XYZ)\n\t    ptcloud = np.concatenate(ptcloud, axis=1)\n\t    if support:\n\t        groundtruth = labels == sampled_class\n\t    else:\n\t        groundtruth = np.zeros_like(labels)\n\t        for i, label in enumerate(labels):\n\t            if label in sampled_classes:\n\t                groundtruth[i] = sampled_classes.index(label)+1\n\t    return ptcloud, groundtruth\n", "def augment_pointcloud(P, pc_augm_config):\n\t    \"\"\"\" Augmentation on XYZ and jittering of everything \"\"\"\n\t    M = transforms3d.zooms.zfdir2mat(1)\n\t    if pc_augm_config['scale'] > 1:\n\t        s = random.uniform(1 / pc_augm_config['scale'], pc_augm_config['scale'])\n\t        M = np.dot(transforms3d.zooms.zfdir2mat(s), M)\n\t    if pc_augm_config['rot'] == 1:\n\t        angle = random.uniform(0, 2 * math.pi)\n\t        M = np.dot(transforms3d.axangles.axangle2mat([0, 0, 1], angle), M)  # z=upright assumption\n\t    if pc_augm_config['mirror_prob'] > 0:  # mirroring x&y, not z\n", "        if random.random() < pc_augm_config['mirror_prob'] / 2:\n\t            M = np.dot(transforms3d.zooms.zfdir2mat(-1, [1, 0, 0]), M)\n\t        if random.random() < pc_augm_config['mirror_prob'] / 2:\n\t            M = np.dot(transforms3d.zooms.zfdir2mat(-1, [0, 1, 0]), M)\n\t    P[:, :3] = np.dot(P[:, :3], M.T)\n\t    if pc_augm_config['shift'] > 0:\n\t        shift = np.random.uniform(-pc_augm_config['shift'], pc_augm_config['shift'], 3)\n\t        P[:, :3] += shift\n\t    if pc_augm_config['jitter']:\n\t        sigma, clip = 0.01, 0.05  # https://github.com/charlesq34/pointnet/blob/master/provider.py#L74\n", "        P = P + np.clip(sigma * np.random.randn(*P.shape), -1 * clip, clip).astype(np.float32)\n\t    return P\n\tclass MyDataset(Dataset):\n\t    def __init__(self, data_path, dataset_name, cvfold=0, num_episode=50000, n_way=3, k_shot=5, n_queries=1,\n\t                 phase=None, mode='train', num_point=4096, pc_attribs='xyz', pc_augm=False, pc_augm_config=None):\n\t        super(MyDataset).__init__()\n\t        self.data_path = data_path\n\t        self.n_way = n_way\n\t        self.k_shot = k_shot\n\t        self.n_queries = n_queries\n", "        self.num_episode = num_episode\n\t        self.phase = phase\n\t        self.mode = mode\n\t        self.num_point = num_point\n\t        self.pc_attribs = pc_attribs\n\t        self.pc_augm = pc_augm\n\t        self.pc_augm_config = pc_augm_config\n\t        if dataset_name == 's3dis':\n\t            from dataloaders.s3dis import S3DISDataset\n\t            self.dataset = S3DISDataset(cvfold, data_path)\n", "        elif dataset_name == 'scannet':\n\t            from dataloaders.scannet import ScanNetDataset\n\t            self.dataset = ScanNetDataset(cvfold, data_path)\n\t        else:\n\t            raise NotImplementedError('Unknown dataset %s!' % dataset_name)\n\t        if mode == 'train':\n\t            self.classes = np.array(self.dataset.train_classes)\n\t        elif mode == 'test':\n\t            self.classes = np.array(self.dataset.test_classes)\n\t        else:\n", "            raise NotImplementedError('Unkown mode %s! [Options: train/test]' % mode)\n\t        print('MODE: {0} | Classes: {1}'.format(mode, self.classes))\n\t        self.class2scans = self.dataset.class2scans\n\t    def __len__(self):\n\t        return self.num_episode\n\t    def __getitem__(self, index, n_way_classes=None):\n\t        if n_way_classes is not None:\n\t            sampled_classes = np.array(n_way_classes)\n\t        else:\n\t            sampled_classes = np.random.choice(self.classes, self.n_way, replace=False)\n", "        support_ptclouds, support_masks, query_ptclouds, query_labels = self.generate_one_episode(sampled_classes)\n\t        if self.mode == 'train' and self.phase == 'metatrain':\n\t            remain_classes = list(set(self.classes) - set(sampled_classes))\n\t            try:\n\t                sampled_valid_classes = np.random.choice(np.array(remain_classes), self.n_way, replace=False)\n\t            except:\n\t                raise NotImplementedError('Error! The number remaining classes is less than %d_way' %self.n_way)\n\t            valid_support_ptclouds, valid_support_masks, valid_query_ptclouds, \\\n\t                                            valid_query_labels = self.generate_one_episode(sampled_valid_classes)\n\t            return support_ptclouds.astype(np.float32), \\\n", "                   support_masks.astype(np.int32), \\\n\t                   query_ptclouds.astype(np.float32), \\\n\t                   query_labels.astype(np.int64), \\\n\t                   valid_support_ptclouds.astype(np.float32), \\\n\t                   valid_support_masks.astype(np.int32), \\\n\t                   valid_query_ptclouds.astype(np.float32), \\\n\t                   valid_query_labels.astype(np.int64)\n\t        else:\n\t            return support_ptclouds.astype(np.float32), \\\n\t                   support_masks.astype(np.int32), \\\n", "                   query_ptclouds.astype(np.float32), \\\n\t                   query_labels.astype(np.int64), \\\n\t                   sampled_classes.astype(np.int32)\n\t    def generate_one_episode(self, sampled_classes):\n\t        support_ptclouds = []\n\t        support_masks = []\n\t        query_ptclouds = []\n\t        query_labels = []\n\t        black_list = []  # to store the sampled scan names, in order to prevent sampling one scan several times...\n\t        for sampled_class in sampled_classes:\n", "            all_scannames = self.class2scans[sampled_class].copy()\n\t            if len(black_list) != 0:\n\t                all_scannames = [x for x in all_scannames if x not in black_list]\n\t            selected_scannames = np.random.choice(all_scannames, self.k_shot+self.n_queries, replace=False)\n\t            black_list.extend(selected_scannames)\n\t            query_scannames = selected_scannames[:self.n_queries]\n\t            support_scannames = selected_scannames[self.n_queries:]\n\t            query_ptclouds_one_way, query_labels_one_way = sample_K_pointclouds(self.data_path, self.num_point,\n\t                                                                                self.pc_attribs, self.pc_augm,\n\t                                                                                self.pc_augm_config,\n", "                                                                                query_scannames,\n\t                                                                                sampled_class,\n\t                                                                                sampled_classes,\n\t                                                                                is_support=False)\n\t            support_ptclouds_one_way, support_masks_one_way = sample_K_pointclouds(self.data_path, self.num_point,\n\t                                                                                self.pc_attribs, self.pc_augm,\n\t                                                                                self.pc_augm_config,\n\t                                                                                support_scannames,\n\t                                                                                sampled_class,\n\t                                                                                sampled_classes,\n", "                                                                                is_support=True)\n\t            query_ptclouds.append(query_ptclouds_one_way)\n\t            query_labels.append(query_labels_one_way)\n\t            support_ptclouds.append(support_ptclouds_one_way)\n\t            support_masks.append(support_masks_one_way)\n\t        support_ptclouds = np.stack(support_ptclouds, axis=0)\n\t        support_masks = np.stack(support_masks, axis=0)\n\t        query_ptclouds = np.concatenate(query_ptclouds, axis=0)\n\t        query_labels = np.concatenate(query_labels, axis=0)\n\t        return support_ptclouds, support_masks, query_ptclouds, query_labels\n", "def batch_train_task_collate(batch):\n\t    task_train_support_ptclouds, task_train_support_masks, task_train_query_ptclouds, task_train_query_labels, \\\n\t    task_valid_support_ptclouds, task_valid_support_masks, task_valid_query_ptclouds, task_valid_query_labels = list(zip(*batch))\n\t    task_train_support_ptclouds = np.stack(task_train_support_ptclouds)\n\t    task_train_support_masks = np.stack(task_train_support_masks)\n\t    task_train_query_ptclouds = np.stack(task_train_query_ptclouds)\n\t    task_train_query_labels = np.stack(task_train_query_labels)\n\t    task_valid_support_ptclouds = np.stack(task_valid_support_ptclouds)\n\t    task_valid_support_masks = np.stack(task_valid_support_masks)\n\t    task_valid_query_ptclouds = np.array(task_valid_query_ptclouds)\n", "    task_valid_query_labels = np.stack(task_valid_query_labels)\n\t    data = [torch.from_numpy(task_train_support_ptclouds).transpose(3,4), torch.from_numpy(task_train_support_masks),\n\t            torch.from_numpy(task_train_query_ptclouds).transpose(2,3), torch.from_numpy(task_train_query_labels),\n\t            torch.from_numpy(task_valid_support_ptclouds).transpose(3,4), torch.from_numpy(task_valid_support_masks),\n\t            torch.from_numpy(task_valid_query_ptclouds).transpose(2,3), torch.from_numpy(task_valid_query_labels)]\n\t    return data\n\t################################################ Static Testing Dataset ################################################\n\tclass MyTestDataset(Dataset):\n\t    def __init__(self, data_path, dataset_name, cvfold=0, num_episode_per_comb=100, n_way=3, k_shot=5, n_queries=1,\n\t                       num_point=4096, pc_attribs='xyz', mode='valid'):\n", "        super(MyTestDataset).__init__()\n\t        dataset = MyDataset(data_path, dataset_name, cvfold=cvfold, n_way=n_way, k_shot=k_shot, n_queries=n_queries,\n\t                            mode='test', num_point=num_point, pc_attribs=pc_attribs, pc_augm=False)\n\t        self.classes = dataset.classes\n\t        if mode == 'valid':\n\t            test_data_path = os.path.join(data_path, 'S_%d_N_%d_K_%d_episodes_%d_pts_%d' % (\n\t                                                    cvfold, n_way, k_shot, num_episode_per_comb, num_point))\n\t        elif mode == 'test':\n\t            test_data_path = os.path.join(data_path, 'S_%d_N_%d_K_%d_test_episodes_%d_pts_%d' % (\n\t                                                    cvfold, n_way, k_shot, num_episode_per_comb, num_point))\n", "        else:\n\t            raise NotImplementedError('Mode (%s) is unknown!' %mode)\n\t        if os.path.exists(test_data_path):\n\t            self.file_names = glob.glob(os.path.join(test_data_path, '*.h5'))\n\t            self.num_episode = len(self.file_names)\n\t        else:\n\t            print('Test dataset (%s) does not exist...\\n Constructing...' %test_data_path)\n\t            os.mkdir(test_data_path)\n\t            class_comb = list(combinations(self.classes, n_way))  # [(),(),(),...]\n\t            self.num_episode = len(class_comb) * num_episode_per_comb\n", "            episode_ind = 0\n\t            self.file_names = []\n\t            for sampled_classes in class_comb:\n\t                sampled_classes = list(sampled_classes)\n\t                for i in range(num_episode_per_comb):\n\t                    data = dataset.__getitem__(episode_ind, sampled_classes)\n\t                    out_filename = os.path.join(test_data_path, '%d.h5' % episode_ind)\n\t                    write_episode(out_filename, data)\n\t                    self.file_names.append(out_filename)\n\t                    episode_ind += 1\n", "    def __len__(self):\n\t        return self.num_episode\n\t    def __getitem__(self, index):\n\t        file_name = self.file_names[index]\n\t        return read_episode(file_name)\n\tdef batch_test_task_collate(batch):\n\t    batch_support_ptclouds, batch_support_masks, batch_query_ptclouds, batch_query_labels, batch_sampled_classes = batch[0]\n\t    data = [torch.from_numpy(batch_support_ptclouds).transpose(2,3), torch.from_numpy(batch_support_masks),\n\t            torch.from_numpy(batch_query_ptclouds).transpose(1,2), torch.from_numpy(batch_query_labels.astype(np.int64))]\n\t    return data, batch_sampled_classes\n", "def write_episode(out_filename, data):\n\t    support_ptclouds, support_masks, query_ptclouds, query_labels, sampled_classes = data\n\t    data_file = h5.File(out_filename, 'w')\n\t    data_file.create_dataset('support_ptclouds', data=support_ptclouds, dtype='float32')\n\t    data_file.create_dataset('support_masks', data=support_masks, dtype='int32')\n\t    data_file.create_dataset('query_ptclouds', data=query_ptclouds, dtype='float32')\n\t    data_file.create_dataset('query_labels', data=query_labels, dtype='int64')\n\t    data_file.create_dataset('sampled_classes', data=sampled_classes, dtype='int32')\n\t    data_file.close()\n\t    print('\\t {0} saved! | classes: {1}'.format(out_filename, sampled_classes))\n", "def read_episode(file_name):\n\t    data_file = h5.File(file_name, 'r')\n\t    support_ptclouds = data_file['support_ptclouds'][:]\n\t    support_masks = data_file['support_masks'][:]\n\t    query_ptclouds = data_file['query_ptclouds'][:]\n\t    query_labels = data_file['query_labels'][:]\n\t    sampled_classes = data_file['sampled_classes'][:]\n\t    return support_ptclouds, support_masks, query_ptclouds, query_labels, sampled_classes\n\t################################################  Pre-train Dataset ################################################\n\tclass MyPretrainDataset(Dataset):\n", "    def __init__(self, data_path, classes, class2scans, mode='train', num_point=4096, pc_attribs='xyz',\n\t                       pc_augm=False, pc_augm_config=None):\n\t        super(MyPretrainDataset).__init__()\n\t        self.data_path = data_path\n\t        self.classes = classes\n\t        self.num_point = num_point\n\t        self.pc_attribs = pc_attribs\n\t        self.pc_augm = pc_augm\n\t        self.pc_augm_config = pc_augm_config\n\t        train_block_names = []\n", "        all_block_names = []\n\t        for k, v in sorted(class2scans.items()):\n\t            all_block_names.extend(v)\n\t            n_blocks = len(v)\n\t            n_test_blocks = int(n_blocks * 0.1)\n\t            n_train_blocks = n_blocks - n_test_blocks\n\t            train_block_names.extend(v[:n_train_blocks])\n\t        if mode == 'train':\n\t            self.block_names = list(set(train_block_names))\n\t        elif mode == 'test':\n", "            self.block_names = list(set(all_block_names) - set(train_block_names))\n\t        else:\n\t            raise NotImplementedError('Mode is unknown!')\n\t        print('[Pretrain Dataset] Mode: {0} | Num_blocks: {1}'.format(mode, len(self.block_names)))\n\t    def __len__(self):\n\t        return len(self.block_names)\n\t    def __getitem__(self, index):\n\t        block_name = self.block_names[index]\n\t        ptcloud, label = sample_pointcloud(self.data_path, self.num_point, self.pc_attribs, self.pc_augm,\n\t                                           self.pc_augm_config, block_name, self.classes, random_sample=True)\n", "        return torch.from_numpy(ptcloud.transpose().astype(np.float32)), torch.from_numpy(label.astype(np.int64))"]}
{"filename": "dataloaders/scannet.py", "chunked_list": ["\"\"\" Data Preprocess and Loader for ScanNetV2 Dataset\n\t\"\"\"\n\timport os\n\timport glob\n\timport numpy as np\n\timport pickle\n\tclass ScanNetDataset(object):\n\t    def __init__(self, cvfold, data_path):\n\t        self.data_path = data_path\n\t        self.classes = 21\n", "        # self.class2type = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk', 6:'bed', 7:'bookshelf',\n\t        #                    8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet', 12:'curtain', 13:'counter', 14:'door',\n\t        #                    15:'window', 16:'shower curtain', 17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}\n\t        class_names = open(os.path.join(os.path.dirname(data_path), 'meta', 'scannet_classnames.txt')).readlines()\n\t        self.class2type = {i: name.strip() for i, name in enumerate(class_names)}\n\t        self.type2class = {self.class2type[t]: t for t in self.class2type}\n\t        self.types = self.type2class.keys()\n\t        self.fold_0 = ['bathtub', 'bed', 'bookshelf', 'cabinet', 'chair','counter', 'curtain', 'desk', 'door', 'floor']\n\t        self.fold_1 = ['otherfurniture', 'picture', 'refridgerator', 'shower curtain', 'sink', 'sofa', 'table', 'toilet', 'wall', 'window']\n\t        if cvfold == 0:\n", "            self.test_classes = [self.type2class[i] for i in self.fold_0]\n\t        elif cvfold == 1:\n\t            self.test_classes = [self.type2class[i] for i in self.fold_1]\n\t        else:\n\t            raise NotImplementedError('Unknown cvfold (%s). [Options: 0,1]' %cvfold)\n\t        all_classes = [i for i in range(1, self.classes)]\n\t        self.train_classes = [c for c in all_classes if c not in self.test_classes]\n\t        self.class2scans = self.get_class2scans()\n\t    def get_class2scans(self):\n\t        class2scans_file = os.path.join(self.data_path, 'class2scans.pkl')\n", "        if os.path.exists(class2scans_file):\n\t            #load class2scans (dictionary)\n\t            with open(class2scans_file, 'rb') as f:\n\t                class2scans = pickle.load(f)\n\t        else:\n\t            min_ratio = .05  # to filter out scans with only rare labelled points\n\t            min_pts = 100  # to filter out scans with only rare labelled points\n\t            class2scans = {k:[] for k in range(self.classes)}\n\t            for file in glob.glob(os.path.join(self.data_path, 'data', '*.npy')):\n\t                scan_name = os.path.basename(file)[:-4]\n", "                data = np.load(file)\n\t                labels = data[:,6].astype(np.int)\n\t                classes = np.unique(labels)\n\t                print('{0} | shape: {1} | classes: {2}'.format(scan_name, data.shape, list(classes)))\n\t                for class_id in classes:\n\t                    #if the number of points for the target class is too few, do not add this sample into the dictionary\n\t                    num_points = np.count_nonzero(labels == class_id)\n\t                    threshold = max(int(data.shape[0]*min_ratio), min_pts)\n\t                    if num_points > threshold:\n\t                        class2scans[class_id].append(scan_name)\n", "            print('==== class to scans mapping is done ====')\n\t            for class_id in range(self.classes):\n\t                print('\\t class_id: {0} | min_ratio: {1} | min_pts: {2} | class_name: {3} | num of scans: {4}'.format(\n\t                          class_id,  min_ratio, min_pts, self.class2type[class_id], len(class2scans[class_id])))\n\t            with open(class2scans_file, 'wb') as f:\n\t                pickle.dump(class2scans, f, pickle.HIGHEST_PROTOCOL)\n\t        return class2scans\n\tif __name__ == '__main__':\n\t    import argparse\n\t    parser = argparse.ArgumentParser(description='Pre-training on ShapeNet')\n", "    parser.add_argument('--cvfold', type=int, default=0, help='Fold left-out for testing in leave-one-out setting '\n\t                                                              'Options: {0,1}')\n\t    parser.add_argument('--data_path', type=str, default='../datasets/ScanNet/blocks_bs1_s1', help='Directory to source data')\n\t    args = parser.parse_args()\n\t    dataset = ScanNetDataset(args.cvfold, args.data_path)"]}
{"filename": "dataloaders/get_embedding.py", "chunked_list": ["import gensim.downloader as api\n\timport numpy as np\n\tmodel = api.load('glove-wiki-gigaword-300')  # download trained model\n\tcategories = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase', 'board', 'clutter']\n\tw2vectors = []\n\tfor word in categories:\n\t    w2vectors.append(model[word])\n\tw2v = [np.expand_dims(x, axis=0) for x in w2vectors]\n\tembeddings = np.concatenate(w2v, axis=0)\n\tnp.save('dataloaders/S3DIS_glove.npy', embeddings)\n", "# 'word2vec-ruscorpora-300',glove-wiki-gigaword-300, 'word2vec-google-news-300','conceptnet-numberbatch-17-06-300','fasttext-wiki-news-subwords-300'\n\tcategories = ['clutter', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf',\n\t                   'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door',\n\t                   'window', 'curtain', 'refrigerator', 'picture', 'cabinet', 'furniture']\n\tw2vectors = []\n\tfor word in categories:\n\t    w2vectors.append(model[word])\n\tw2v = [np.expand_dims(x, axis=0) for x in w2vectors]\n\tembeddings = np.concatenate(w2v, axis=0)\n\tnp.save('dataloaders/ScanNet_glove.npy', embeddings)"]}
{"filename": "dataloaders/s3dis.py", "chunked_list": ["\"\"\" Data Preprocess and Loader for S3DIS Dataset\n\t\"\"\"\n\timport os\n\timport glob\n\timport numpy as np\n\timport pickle\n\tclass S3DISDataset(object):\n\t    def __init__(self, cvfold, data_path):\n\t        self.data_path = data_path\n\t        self.classes = 13\n", "        # self.class2type = {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',\n\t        #                    8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}\n\t        class_names = open(os.path.join(os.path.dirname(data_path), 'meta', 's3dis_classnames.txt')).readlines()\n\t        self.class2type = {i: name.strip() for i, name in enumerate(class_names)}\n\t        print(self.class2type)\n\t        self.type2class = {self.class2type[t]: t for t in self.class2type}\n\t        self.types = self.type2class.keys()\n\t        self.fold_0 = ['beam', 'board', 'bookcase', 'ceiling', 'chair', 'column']\n\t        self.fold_1 = ['door', 'floor', 'sofa', 'table', 'wall', 'window']\n\t        if cvfold == 0:\n", "            self.test_classes = [self.type2class[i] for i in self.fold_0]\n\t        elif cvfold == 1:\n\t            self.test_classes = [self.type2class[i] for i in self.fold_1]\n\t        else:\n\t            raise NotImplementedError('Unknown cvfold (%s). [Options: 0,1]' %cvfold)\n\t        all_classes = [i for i in range(0, self.classes-1)]\n\t        self.train_classes = [c for c in all_classes if c not in self.test_classes]\n\t        # print('train_class:{0}'.format(self.train_classes))\n\t        # print('test_class:{0}'.format(self.test_classes))\n\t        self.class2scans = self.get_class2scans()\n", "    def get_class2scans(self):\n\t        class2scans_file = os.path.join(self.data_path, 'class2scans.pkl')\n\t        if os.path.exists(class2scans_file):\n\t            #load class2scans (dictionary)\n\t            with open(class2scans_file, 'rb') as f:\n\t                class2scans = pickle.load(f)\n\t        else:\n\t            min_ratio = .05  # to filter out scans with only rare labelled points\n\t            min_pts = 100  # to filter out scans with only rare labelled points\n\t            class2scans = {k:[] for k in range(self.classes)}\n", "            for file in glob.glob(os.path.join(self.data_path, 'data', '*.npy')):\n\t                scan_name = os.path.basename(file)[:-4]\n\t                data = np.load(file)\n\t                labels = data[:,6].astype(np.int)\n\t                classes = np.unique(labels)\n\t                print('{0} | shape: {1} | classes: {2}'.format(scan_name, data.shape, list(classes)))\n\t                for class_id in classes:\n\t                    #if the number of points for the target class is too few, do not add this sample into the dictionary\n\t                    num_points = np.count_nonzero(labels == class_id)\n\t                    threshold = max(int(data.shape[0]*min_ratio), min_pts)\n", "                    if num_points > threshold:\n\t                        class2scans[class_id].append(scan_name)\n\t            print('==== class to scans mapping is done ====')\n\t            for class_id in range(self.classes):\n\t                print('\\t class_id: {0} | min_ratio: {1} | min_pts: {2} | class_name: {3} | num of scans: {4}'.format(\n\t                          class_id,  min_ratio, min_pts, self.class2type[class_id], len(class2scans[class_id])))\n\t            with open(class2scans_file, 'wb') as f:\n\t                pickle.dump(class2scans, f, pickle.HIGHEST_PROTOCOL)\n\t        return class2scans"]}
{"filename": "dataloaders/__init__.py", "chunked_list": []}
{"filename": "utils/checkpoint_util.py", "chunked_list": ["\"\"\" Util functions for loading and saving checkpoints\n\t\"\"\"\n\timport os\n\timport torch\n\tdef load_pretrain_checkpoint(model, pretrain_checkpoint_path):\n\t    # load pretrained model for point cloud encoding\n\t    model_dict = model.state_dict()\n\t    if pretrain_checkpoint_path is not None:\n\t        print('Load encoder module from pretrained checkpoint...')\n\t        pretrained_dict = torch.load(os.path.join(pretrain_checkpoint_path, 'checkpoint.tar'))['params']\n", "        pretrained_dict = {'encoder.' + k: v for k, v in pretrained_dict.items()}\n\t        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n\t        model_dict.update(pretrained_dict)\n\t        model.load_state_dict(model_dict)\n\t    else:\n\t        raise ValueError('Pretrained checkpoint must be given.')\n\t    return model\n\tdef load_model_checkpoint(model, model_checkpoint_path, optimizer=None, mode='test'):\n\t    try:\n\t        checkpoint = torch.load(os.path.join(model_checkpoint_path, 'checkpoint.tar'))\n", "        start_iter = checkpoint['iteration']\n\t        start_iou = checkpoint['IoU']\n\t    except:\n\t        raise ValueError('Model checkpoint file must be correctly given (%s).' %model_checkpoint_path)\n\t    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n\t    if mode == 'test':\n\t        print('Load model checkpoint at Iteration %d (IoU %f)...' % (start_iter, start_iou))\n\t        return model\n\t    else:\n\t        try:\n", "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\t        except:\n\t            print('Checkpoint does not include optimizer state dict...')\n\t        print('Resume from checkpoint at Iteration %d (IoU %f)...' % (start_iter, start_iou))\n\t        return model, optimizer\n\tdef save_pretrain_checkpoint(model, output_path):\n\t    torch.save(dict(params=model.encoder.state_dict()), os.path.join(output_path, 'checkpoint.tar'))"]}
{"filename": "utils/logger.py", "chunked_list": ["\"\"\" Util functions for writing logs\n\t\"\"\"\n\timport os\n\tclass IOStream():\n\t    def __init__(self, path):\n\t        self.f = open(path, 'a')\n\t    def cprint(self, text):\n\t        print(text)\n\t        self.f.write(text+'\\n')\n\t        self.f.flush()\n", "    def close(self):\n\t        self.f.close()\n\tdef mkdir(path):\n\t    if not os.path.exists(path):\n\t        os.makedirs(path)\n\tdef print_args(logger, args):\n\t    opt = vars(args)\n\t    logger.cprint('------------ Options -------------')\n\t    for k, v in sorted(opt.items()):\n\t        logger.cprint('%s: %s' % (str(k), str(v)))\n", "    logger.cprint('-------------- End ----------------\\n')\n\tdef init_logger(log_dir, args):\n\t    mkdir(log_dir)\n\t    log_file = os.path.join(log_dir, 'log_%s.txt' %args.phase)\n\t    logger = IOStream(log_file)\n\t    # logger.cprint(str(args))\n\t    ## print arguments in format\n\t    print_args(logger, args)\n\t    return logger"]}
{"filename": "utils/cuda_util.py", "chunked_list": ["\"\"\" Cuda util function\n\t\"\"\"\n\tdef cast_cuda(input):\n\t    if type(input) == type([]):\n\t        for i in range(len(input)):\n\t            input[i] = cast_cuda(input[i])\n\t    else:\n\t        return input.cuda()\n\t    return input"]}
{"filename": "utils/__init__.py", "chunked_list": []}
{"filename": "preprocess/room2blocks.py", "chunked_list": ["\"\"\" Processing step 1, split room into blocks\n\t\"\"\"\n\timport os\n\timport glob\n\timport numpy as np\n\t# -----------------------------------------------------------------------------\n\t# PREPARE BLOCK DATA FOR SUPERPOINT GRAPH GENERATION\n\t# -----------------------------------------------------------------------------\n\tdef room2blocks(data, block_size, stride, min_npts):\n\t    \"\"\" Prepare block data.\n", "    Args:\n\t        data: N x 7 numpy array, 012 are XYZ in meters, 345 are RGB in [0,255], 6 is the labels\n\t            assumes the data is not shifted (min point is not origin),\n\t        block_size: float, physical size of the block in meters\n\t        stride: float, stride for block sweeping\n\t    Returns:\n\t        blocks_list: a list of blocks, each block is a num_point x 7 np array\n\t    \"\"\"\n\t    assert (stride <= block_size)\n\t    xyz = data[:,:3]\n", "    xyz_min = np.amin(xyz, axis=0)\n\t    xyz -= xyz_min\n\t    xyz_max = np.amax(xyz, axis=0)\n\t    # Get the corner location for our sampling blocks\n\t    xbeg_list = []\n\t    ybeg_list = []\n\t    num_block_x = int(np.ceil((xyz_max[0] - block_size) / stride)) + 1\n\t    num_block_y = int(np.ceil((xyz_max[1] - block_size) / stride)) + 1\n\t    for i in range(num_block_x):\n\t        for j in range(num_block_y):\n", "            xbeg_list.append(i * stride)\n\t            ybeg_list.append(j * stride)\n\t    # Collect blocks\n\t    blocks_list = []\n\t    for idx in range(len(xbeg_list)):\n\t        xbeg = xbeg_list[idx]\n\t        ybeg = ybeg_list[idx]\n\t        xcond = (xyz[:, 0] <= xbeg + block_size) & (xyz[:, 0] >= xbeg)\n\t        ycond = (xyz[:, 1] <= ybeg + block_size) & (xyz[:, 1] >= ybeg)\n\t        cond = xcond & ycond\n", "        if np.sum(cond) < min_npts:  # discard block if there are less than 100 pts.\n\t            continue\n\t        block = data[cond, :]\n\t        blocks_list.append(block)\n\t    return blocks_list\n\tdef room2blocks_wrapper(room_path, block_size, stride, min_npts):\n\t    if room_path[-3:] == 'txt':\n\t        data = np.loadtxt(room_path)\n\t    elif room_path[-3:] == 'npy':\n\t        data = np.load(room_path)\n", "    else:\n\t        print('Unknown file type! exiting.')\n\t        exit()\n\t    return room2blocks(data, block_size, stride, min_npts)\n\tif __name__ == '__main__':\n\t    import argparse\n\t    parser = argparse.ArgumentParser(description='[Preprocessing] Split rooms into blocks')\n\t    parser.add_argument('--data_path', default='../datasets/S3DIS/scenes')\n\t    parser.add_argument('--dataset', default='s3dis', metavar='bs', help='s3dis|scannet')\n\t    parser.add_argument('--block_size', type=float, default=1, metavar='s', help='size of each block')\n", "    parser.add_argument('--stride', type=float, default=1, help='stride of sliding window for splitting rooms, '\n\t                                                                'stride should be not larger than block size')\n\t    parser.add_argument('--min_npts', type=int, default=1000, help='the minimum number of points in a block,'\n\t                                                                  'if less than this threshold, the block is discarded')\n\t    args = parser.parse_args()\n\t    DATA_PATH = args.data_path\n\t    BLOCK_SIZE = args.block_size\n\t    STRIDE = args.stride\n\t    MIN_NPTS = args.min_npts\n\t    SAVE_PATH = os.path.join(os.path.dirname(DATA_PATH), 'blocks_bs{0}_s{1}'.format(BLOCK_SIZE, STRIDE), 'data')\n", "    if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n\t    file_paths = glob.glob(os.path.join(DATA_PATH, 'data', '*.npy'))\n\t    print('{} scenes to be split...'.format(len(file_paths)))\n\t    block_cnt = 0\n\t    for file_path in file_paths:\n\t        room_name = os.path.basename(file_path)[:-4]\n\t        blocks_list = room2blocks_wrapper(file_path, block_size=BLOCK_SIZE, stride=STRIDE, min_npts=MIN_NPTS)\n\t        print('{0} is split into {1} blocks.'.format(room_name, len(blocks_list)))\n\t        block_cnt += len(blocks_list)\n\t        for i, block_data in enumerate(blocks_list):\n", "            block_filename = room_name + '_block_' + str(i) + '.npy'\n\t            np.save(os.path.join(SAVE_PATH, block_filename), block_data)\n\t    print(\"Total samples: {0}\".format(block_cnt))"]}
{"filename": "preprocess/collect_scannet_data.py", "chunked_list": ["\"\"\" Collect point clouds and the corresponding labels from original ScanNetV2 dataset, and save into numpy files.\n\t\"\"\"\n\timport os\n\timport sys\n\timport json\n\timport numpy as np\n\tfrom plyfile import PlyData\n\tBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\tROOT_DIR = os.path.dirname(BASE_DIR)\n\tsys.path.append(ROOT_DIR)\n", "def get_raw2scannet_label_map(label_mapping_file):\n\t    lines = [line.rstrip() for line in open(label_mapping_file)]\n\t    lines = lines[1:]\n\t    raw2scannet = {}\n\t    label_classes_set = set(CLASS_NAMES)\n\t    for i in range(len(lines)):\n\t        elements = lines[i].split('\\t')\n\t        raw_name = elements[1]\n\t        nyu40_name = elements[7]\n\t        if nyu40_name not in label_classes_set:\n", "            raw2scannet[raw_name] = 'unannotated'\n\t        else:\n\t            raw2scannet[raw_name] = nyu40_name\n\t    return raw2scannet\n\tdef read_ply_xyzrgb(filename):\n\t    \"\"\" read XYZRGB point cloud from filename PLY file \"\"\"\n\t    assert(os.path.isfile(filename))\n\t    with open(filename, 'rb') as f:\n\t        plydata = PlyData.read(f)\n\t        num_verts = plydata['vertex'].count\n", "        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n\t        vertices[:,0] = plydata['vertex'].data['x']\n\t        vertices[:,1] = plydata['vertex'].data['y']\n\t        vertices[:,2] = plydata['vertex'].data['z']\n\t        vertices[:,3] = plydata['vertex'].data['red']\n\t        vertices[:,4] = plydata['vertex'].data['green']\n\t        vertices[:,5] = plydata['vertex'].data['blue']\n\t    return vertices\n\tdef collect_point_label(scene_path, scene_name, out_filename):\n\t    # Over-segmented segments: maps from segment to vertex/point IDs\n", "    mesh_seg_filename = os.path.join(scene_path, '%s_vh_clean_2.0.010000.segs.json' % (scene_name))\n\t    # print mesh_seg_filename\n\t    with open(mesh_seg_filename) as jsondata:\n\t        d = json.load(jsondata)\n\t        seg = d['segIndices']\n\t        # print len(seg)\n\t    segid_to_pointid = {}\n\t    for i in range(len(seg)):\n\t        if seg[i] not in segid_to_pointid:\n\t            segid_to_pointid[seg[i]] = []\n", "        segid_to_pointid[seg[i]].append(i)\n\t    # Raw points in XYZRGBA\n\t    ply_filename = os.path.join(scene_path, '%s_vh_clean_2.ply' % (scene_name))\n\t    points = read_ply_xyzrgb(ply_filename)\n\t    print('{0}: {1} points'.format(scene_name, points.shape[0]))\n\t    # Instances over-segmented segment IDs: annotation on segments\n\t    instance_segids = []\n\t    labels = []\n\t    annotation_filename = os.path.join(scene_path, '%s.aggregation.json' % (scene_name))\n\t    # print annotation_filename\n", "    with open(annotation_filename) as jsondata:\n\t        d = json.load(jsondata)\n\t        for x in d['segGroups']:\n\t            instance_segids.append(x['segments'])\n\t            labels.append(x['label'])\n\t    # print len(instance_segids)\n\t    # print labels\n\t    # Each instance's points\n\t    instance_points_list = []\n\t    # instance_labels_list = []\n", "    semantic_labels_list = []\n\t    for i in range(len(instance_segids)):\n\t        segids = instance_segids[i]\n\t        pointids = []\n\t        for segid in segids:\n\t            pointids += segid_to_pointid[segid]\n\t        instance_points = points[np.array(pointids), :]\n\t        instance_points_list.append(instance_points)\n\t        # instance_labels_list.append(np.ones((instance_points.shape[0], 1)) * i)\n\t        if labels[i] not in RAW2SCANNET:\n", "            label = 'unannotated'\n\t        else:\n\t            label = RAW2SCANNET[labels[i]]\n\t        label = CLASS_NAMES.index(label)\n\t        semantic_labels_list.append(np.ones((instance_points.shape[0], 1)) * label)\n\t    # Refactor data format\n\t    scene_points = np.concatenate(instance_points_list, 0)\n\t    scene_points = scene_points[:, 0:6]  # XYZRGB, disregarding the A\n\t    # instance_labels = np.concatenate(instance_labels_list, 0)\n\t    semantic_labels = np.concatenate(semantic_labels_list, 0)\n", "    # data = np.concatenate((scene_points, instance_labels, semantic_labels), 1)\n\t    data = np.concatenate((scene_points, semantic_labels), 1)\n\t    np.save(out_filename, data)\n\tif __name__ == '__main__':\n\t    import argparse\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--data_path', default='datasets/ScanNet/scans',\n\t                        help='Directory to dataset')\n\t    args = parser.parse_args()\n\t    DATA_PATH = args.data_path\n", "    DST_PATH = os.path.join(ROOT_DIR, 'datasets/ScanNet')\n\t    SAVE_PATH = os.path.join(DST_PATH, 'scenes', 'data')\n\t    if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n\t    meta_path = os.path.join(DST_PATH, 'meta')\n\t    CLASS_NAMES = [x.rstrip() for x in open(os.path.join(meta_path, 'scannet_classnames.txt'))]\n\t    label_mapping_file = os.path.join(meta_path, 'scannetv2-labels.combined.tsv')\n\t    RAW2SCANNET = get_raw2scannet_label_map(label_mapping_file)\n\t    scene_paths = [os.path.join(DATA_PATH, o) for o in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, o))]\n\t    n_scenes = len(scene_paths)\n\t    if (n_scenes == 0):\n", "        raise ValueError('%s is empty' % DATA_PATH)\n\t    else:\n\t        print('%d scenes to be processed...' % n_scenes)\n\t    for scene_path in scene_paths:\n\t        scene_name = os.path.basename(scene_path)\n\t        try:\n\t            out_filename = scene_name+'.npy' # scene0000_00.npy\n\t            collect_point_label(scene_path, scene_name, os.path.join(SAVE_PATH, out_filename))\n\t        except:\n\t            raise ValueError('ERROR {}!!'.format(scene_path))\n"]}
{"filename": "preprocess/collect_s3dis_data.py", "chunked_list": ["\"\"\" Collect point clouds and the corresponding labels from original S3DID dataset, and save into numpy files.\n\t\"\"\"\n\timport os\n\timport glob\n\timport numpy as np\n\timport sys\n\tBASE_DIR = os.path.dirname(os.path.abspath(__file__))\n\tROOT_DIR = os.path.dirname(BASE_DIR)\n\tsys.path.append(ROOT_DIR)\n\tdef collect_point_label(anno_path, out_filename, file_format='numpy'):\n", "    \"\"\" Convert original dataset files to data_label file (each line is XYZRGBL).\n\t        We aggregated all the points from each instance in the room.\n\t    Args:\n\t        anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n\t        out_filename: path to save collected points and labels (each line is XYZRGBL)\n\t        file_format: txt or numpy, determines what file format to save.\n\t    Returns:\n\t        None\n\t    Note:\n\t        the points are shifted before save, the most negative point is now at origin.\n", "    \"\"\"\n\t    points_list = []\n\t    for f in glob.glob(os.path.join(anno_path, '*.txt')):\n\t        cls = os.path.basename(f).split('_')[0]\n\t        if cls not in CLASS_NAMES:  # note: in some room there is 'staris' class..\n\t            cls = 'clutter'\n\t        points = np.loadtxt(f)\n\t        labels = np.ones((points.shape[0], 1)) * CLASS2LABEL[cls]\n\t        points_list.append(np.concatenate([points, labels], 1))  # Nx7\n\t    data_label = np.concatenate(points_list, 0)\n", "    # xyz_min = np.amin(data_label, axis=0)[0:3]\n\t    # data_label[:, 0:3] -= xyz_min\n\t    if file_format == 'txt':\n\t        fout = open(out_filename, 'w')\n\t        for i in range(data_label.shape[0]):\n\t            fout.write('%f %f %f %d %d %d %d\\n' % \\\n\t                       (data_label[i, 0], data_label[i, 1], data_label[i, 2],\n\t                        data_label[i, 3], data_label[i, 4], data_label[i, 5],\n\t                        data_label[i, 6]))\n\t        fout.close()\n", "    elif file_format == 'numpy':\n\t        np.save(out_filename, data_label)\n\t    else:\n\t        print('ERROR!! Unknown file format: %s, please use txt or numpy.' % \\\n\t              (file_format))\n\t        exit()\n\tif __name__ == '__main__':\n\t    import argparse\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--data_path', default='datasets/S3DIS/Stanford3dDataset_v1.2_Aligned_Version',\n", "                        help='Directory to dataset')\n\t    args = parser.parse_args()\n\t    DATA_PATH = args.data_path\n\t    folders = [\"Area_1\", \"Area_2\", \"Area_3\", \"Area_4\", \"Area_5\", \"Area_6\"]\n\t    DST_PATH = os.path.join(ROOT_DIR, 'datasets/S3DIS')\n\t    SAVE_PATH = os.path.join(DST_PATH, 'scenes', 'data')\n\t    if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n\t    CLASS_NAMES = [x.rstrip() for x in open(os.path.join(ROOT_DIR, 'datasets/S3DIS/meta', 's3dis_classnames.txt'))]\n\t    CLASS2LABEL = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n\t    for folder in folders:\n", "        print(\"=================\\n   \" + folder + \"\\n=================\")\n\t        data_folder = os.path.join(DATA_PATH, folder)\n\t        if not os.path.isdir(data_folder):\n\t            raise ValueError(\"%s does not exist\" % data_folder)\n\t        # all the scenes in current Area\n\t        scene_paths = [os.path.join(data_folder, o) for o in os.listdir(data_folder)\n\t                                                if os.path.isdir(os.path.join(data_folder, o))]\n\t        n_scenes = len(scene_paths)\n\t        if (n_scenes == 0):\n\t            raise ValueError('%s is empty' % data_folder)\n", "        else:\n\t            print('%d files are under this folder' % n_scenes)\n\t        for scene_path in scene_paths:\n\t            # Note: there is an extra character in the v1.2 data in Area_5/hallway_6. It's fixed manually.\n\t            anno_path = os.path.join(scene_path, \"Annotations\")\n\t            print(anno_path)\n\t            elements = scene_path.split('/')\n\t            out_filename = '{}_{}.npy'.format(elements[-2], elements[-1]) # Area_1_hallway_1.npy\n\t            try:\n\t                collect_point_label(anno_path, os.path.join(SAVE_PATH, out_filename))\n", "            except:\n\t                print(anno_path, 'ERROR!!')\n"]}
{"filename": "models/protonet_QGPA.py", "chunked_list": ["\"\"\" Prototypical Network \n\t\"\"\"\n\timport pdb\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom models.dgcnn import DGCNN\n\tfrom models.dgcnn_new import DGCNN_semseg\n\tfrom models.attention import SelfAttention, QGPA\n\tfrom models.gmmn import GMMNnetwork\n", "class BaseLearner(nn.Module):\n\t    \"\"\"The class for inner loop.\"\"\"\n\t    def __init__(self, in_channels, params):\n\t        super(BaseLearner, self).__init__()\n\t        self.num_convs = len(params)\n\t        self.convs = nn.ModuleList()\n\t        for i in range(self.num_convs):\n\t            if i == 0:\n\t                in_dim = in_channels\n\t            else:\n", "                in_dim = params[i-1]\n\t            self.convs.append(nn.Sequential(\n\t                              nn.Conv1d(in_dim, params[i], 1),\n\t                              nn.BatchNorm1d(params[i])))\n\t    def forward(self, x):\n\t        for i in range(self.num_convs):\n\t            x = self.convs[i](x)\n\t            if i != self.num_convs-1:\n\t                x = F.relu(x)\n\t        return x\n", "class ProtoNetAlignQGPASR(nn.Module):\n\t    def __init__(self, args):\n\t        super(ProtoNetAlignQGPASR, self).__init__()\n\t        self.n_way = args.n_way\n\t        self.k_shot = args.k_shot\n\t        self.dist_method = 'cosine'\n\t        self.in_channels = args.pc_in_dim\n\t        self.n_points = args.pc_npts\n\t        self.use_attention = args.use_attention\n\t        self.use_align = args.use_align\n", "        self.use_linear_proj = args.use_linear_proj\n\t        self.use_supervise_prototype = args.use_supervise_prototype\n\t        if args.use_high_dgcnn:\n\t            self.encoder = DGCNN_semseg(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n\t        else:\n\t            self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n\t        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\t        if self.use_attention:\n\t            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n\t        else:\n", "            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\t        if self.use_linear_proj:\n\t            self.conv_1 = nn.Sequential(nn.Conv1d(args.train_dim, args.train_dim, kernel_size=1, bias=False),\n\t                                   nn.BatchNorm1d(args.train_dim),\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.use_transformer = args.use_transformer\n\t        if self.use_transformer:\n\t            self.transformer = QGPA()\n\t    def forward(self, support_x, support_y, query_x, query_y):\n\t        \"\"\"\n", "        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n\t        Return:\n\t            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n\t        \"\"\"\n\t        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n\t        support_feat, _ = self.getFeatures(support_x)\n", "        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points)\n\t        query_feat, xyz = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n\t        fg_mask = support_y\n\t        bg_mask = torch.logical_not(support_y)\n\t        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n\t        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask)\n\t        # prototype learning\n\t        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n\t        prototypes = [bg_prototype] + fg_prototypes\n\t        self_regulize_loss = 0\n", "        if self.use_supervise_prototype:\n\t            self_regulize_loss = self.sup_regulize_Loss(prototypes, support_feat, fg_mask, bg_mask)\n\t        if self.use_transformer:\n\t            prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0).repeat(query_feat.shape[0], 1, 1)\n\t            support_feat_ = support_feat.mean(1)\n\t            prototypes_all_post = self.transformer(query_feat, support_feat_, prototypes_all)\n\t            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n\t            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n\t            query_pred = torch.stack(similarity, dim=1)\n\t            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n", "        else:\n\t            similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n\t            query_pred = torch.stack(similarity, dim=1)\n\t            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\t        align_loss = 0\n\t        if self.use_align:\n\t            align_loss_epi = self.alignLoss_trans(query_feat, query_pred, support_feat, fg_mask, bg_mask)\n\t            align_loss += align_loss_epi\n\t        prototypes_all_post = prototypes_all_post.clone().detach()\n\t        return query_pred, loss + align_loss + self_regulize_loss, prototypes_all_post\n", "    def forward_test_semantic(self, support_x, support_y, query_x, query_y, embeddings=None):\n\t        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n\t        Return:\n\t            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n\t        \"\"\"\n", "        query_feat, xyz = self.getFeatures(query_x)\n\t        # prototype learning\n\t        if self.use_transformer:\n\t            prototypes_all_post = embeddings\n\t            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n\t            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n\t            query_pred = torch.stack(similarity, dim=1)\n\t            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\t        return query_pred, loss\n\t    def sup_regulize_Loss(self, prototype_supp, supp_fts, fore_mask, back_mask):\n", "        \"\"\"\n\t        Compute the loss for the prototype suppoort self alignment branch\n\t        Args:\n\t            prototypes: embedding features for query images\n\t                expect shape: N x C x num_points\n\t            supp_fts: embedding features for support images\n\t                expect shape: (Wa x Shot) x C x num_points\n\t            fore_mask: foreground masks for support images\n\t                expect shape: (way x shot) x num_points\n\t            back_mask: background masks for support images\n", "                expect shape: (way x shot) x num_points\n\t        \"\"\"\n\t        n_ways, n_shots = self.n_way, self.k_shot\n\t        # Compute the support loss\n\t        loss = 0\n\t        for way in range(n_ways):\n\t            prototypes = [prototype_supp[0], prototype_supp[way + 1]]\n\t            for shot in range(n_shots):\n\t                img_fts = supp_fts[way, shot].unsqueeze(0)\n\t                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes]\n", "                supp_pred = torch.stack(supp_dist, dim=1)\n\t                # Construct the support Ground-Truth segmentation\n\t                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\t                supp_label[fore_mask[way, shot] == 1] = 1\n\t                supp_label[back_mask[way, shot] == 1] = 0\n\t                # Compute Loss\n\t                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n\t        return loss\n\t    def getFeatures(self, x):\n\t        \"\"\"\n", "        Forward the input data to network and generate features\n\t        :param x: input data with shape (B, C_in, L)\n\t        :return: features with shape (B, C_out, L)\n\t        \"\"\"\n\t        if self.use_attention:\n\t            feat_level1, feat_level2, xyz = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            att_feat = self.att_learner(feat_level2)\n\t            if self.use_linear_proj:\n\t                return self.conv_1(torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1)), xyz\n", "            else:\n\t                return torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1), xyz\n\t        else:\n\t            # return self.base_learner(self.encoder(x))\n\t            feat_level1, feat_level2 = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            map_feat = self.linear_mapper(feat_level2)\n\t            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n\t    def getMaskedFeatures(self, feat, mask):\n\t        \"\"\"\n", "        Extract foreground and background features via masked average pooling\n\t        Args:\n\t            feat: input features, shape: (n_way, k_shot, feat_dim, num_points)\n\t            mask: binary mask, shape: (n_way, k_shot, num_points)\n\t        Return:\n\t            masked_feat: masked features, shape: (n_way, k_shot, feat_dim)\n\t        \"\"\"\n\t        mask = mask.unsqueeze(2)\n\t        masked_feat = torch.sum(feat * mask, dim=3) / (mask.sum(dim=3) + 1e-5)\n\t        return masked_feat\n", "    def getPrototype(self, fg_feat, bg_feat):\n\t        \"\"\"\n\t        Average the features to obtain the prototype\n\t        Args:\n\t            fg_feat: foreground features for each way/shot, shape: (n_way, k_shot, feat_dim)\n\t            bg_feat: background features for each way/shot, shape: (n_way, k_shot, feat_dim)\n\t        Returns:\n\t            fg_prototypes: a list of n_way foreground prototypes, each prototype is a vector with shape (feat_dim,)\n\t            bg_prototype: background prototype, a vector with shape (feat_dim,)\n\t        \"\"\"\n", "        fg_prototypes = [fg_feat[way, ...].sum(dim=0) / self.k_shot for way in range(self.n_way)]\n\t        bg_prototype = bg_feat.sum(dim=(0,1)) / (self.n_way * self.k_shot)\n\t        return fg_prototypes, bg_prototype\n\t    def calculateSimilarity(self, feat,  prototype, method='cosine', scaler=10):\n\t        \"\"\"\n\t        Calculate the Similarity between query point-level features and prototypes\n\t        Args:\n\t            feat: input query point-level features\n\t                  shape: (n_queries, feat_dim, num_points)\n\t            prototype: prototype of one semantic class\n", "                       shape: (feat_dim,)\n\t            method: 'cosine' or 'euclidean', different ways to calculate similarity\n\t            scaler: used when 'cosine' distance is computed.\n\t                    By multiplying the factor with cosine distance can achieve comparable performance\n\t                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n\t        Return:\n\t            similarity: similarity between query point to prototype\n\t                        shape: (n_queries, 1, num_points)\n\t        \"\"\"\n\t        if method == 'cosine':\n", "            similarity = F.cosine_similarity(feat, prototype[None, ..., None], dim=1) * scaler\n\t        elif method == 'euclidean':\n\t            similarity = - F.pairwise_distance(feat, prototype[None, ..., None], p=2)**2\n\t        else:\n\t            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n\t        return similarity\n\t    def calculateSimilarity_trans(self, feat,  prototype, method='cosine', scaler=10):\n\t        \"\"\"\n\t        Calculate the Similarity between query point-level features and prototypes\n\t        Args:\n", "            feat: input query point-level features\n\t                  shape: (n_queries, feat_dim, num_points)\n\t            prototype: prototype of one semantic class\n\t                       shape: (feat_dim,)\n\t            method: 'cosine' or 'euclidean', different ways to calculate similarity\n\t            scaler: used when 'cosine' distance is computed.\n\t                    By multiplying the factor with cosine distance can achieve comparable performance\n\t                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n\t        Return:\n\t            similarity: similarity between query point to prototype\n", "                        shape: (n_queries, 1, num_points)\n\t        \"\"\"\n\t        if method == 'cosine':\n\t            similarity = F.cosine_similarity(feat, prototype[..., None], dim=1) * scaler\n\t        elif method == 'euclidean':\n\t            similarity = - F.pairwise_distance(feat, prototype[..., None], p=2)**2\n\t        else:\n\t            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n\t        return similarity\n\t    def computeCrossEntropyLoss(self, query_logits, query_labels):\n", "        \"\"\" Calculate the CrossEntropy Loss for query set\n\t        \"\"\"\n\t        return F.cross_entropy(query_logits, query_labels)\n\t    def alignLoss_trans(self, qry_fts, pred, supp_fts, fore_mask, back_mask):\n\t        \"\"\"\n\t        Compute the loss for the prototype alignment branch\n\t        Args:\n\t            qry_fts: embedding features for query images\n\t                expect shape: N x C x num_points\n\t            pred: predicted segmentation score\n", "                expect shape: N x (1 + Wa) x num_points\n\t            supp_fts: embedding features for support images\n\t                expect shape: (Wa x Shot) x C x num_points\n\t            fore_mask: foreground masks for support images\n\t                expect shape: (way x shot) x num_points\n\t            back_mask: background masks for support images\n\t                expect shape: (way x shot) x num_points\n\t        \"\"\"\n\t        n_ways, n_shots = self.n_way, self.k_shot\n\t        # Mask and get query prototype\n", "        pred_mask = pred.argmax(dim=1, keepdim=True)  # N x 1 x H' x W'\n\t        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n\t        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n\t        pred_mask = torch.stack(binary_masks, dim=1).float()  # N x (1 + Wa) x 1 x H' x W'\n\t        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3)) / (pred_mask.sum(dim=(0, 3)) + 1e-5)\n\t        # Compute the support loss\n\t        loss = 0\n\t        for way in range(n_ways):\n\t            if way in skip_ways:\n\t                continue\n", "            # Get the query prototypes\n\t            prototypes = [qry_prototypes[0], qry_prototypes[way + 1]]\n\t            for shot in range(n_shots):\n\t                img_fts = supp_fts[way, shot].unsqueeze(0)\n\t                prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0)\n\t                prototypes_all_post = self.transformer(img_fts, qry_fts.mean(0).unsqueeze(0), prototypes_all)\n\t                prototypes_new = [prototypes_all_post[0, 0], prototypes_all_post[0, 1]]\n\t                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes_new]\n\t                supp_pred = torch.stack(supp_dist, dim=1)\n\t                # Construct the support Ground-Truth segmentation\n", "                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\t                supp_label[fore_mask[way, shot] == 1] = 1\n\t                supp_label[back_mask[way, shot] == 1] = 0\n\t                # Compute Loss\n\t                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n\t        return loss\n"]}
{"filename": "models/mpti.py", "chunked_list": ["\"\"\" Multi-prototype transductive inference\n\t\"\"\"\n\timport numpy as np\n\timport faiss\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch_cluster import fps\n\tfrom models.dgcnn import DGCNN\n\tfrom models.attention import SelfAttention\n", "class BaseLearner(nn.Module):\n\t    \"\"\"The class for inner loop.\"\"\"\n\t    def __init__(self, in_channels, params):\n\t        super(BaseLearner, self).__init__()\n\t        self.num_convs = len(params)\n\t        self.convs = nn.ModuleList()\n\t        for i in range(self.num_convs):\n\t            if i == 0:\n\t                in_dim = in_channels\n\t            else:\n", "                in_dim = params[i-1]\n\t            self.convs.append(nn.Sequential(\n\t                              nn.Conv1d(in_dim, params[i], 1),\n\t                              nn.BatchNorm1d(params[i])))\n\t    def forward(self, x):\n\t        for i in range(self.num_convs):\n\t            x = self.convs[i](x)\n\t            if i != self.num_convs-1:\n\t                x = F.relu(x)\n\t        return x\n", "class MultiPrototypeTransductiveInference(nn.Module):\n\t    def __init__(self, args):\n\t        super(MultiPrototypeTransductiveInference, self).__init__()\n\t        # self.gpu_id = args.gpu_id\n\t        self.n_way = args.n_way\n\t        self.k_shot = args.k_shot\n\t        self.in_channels = args.pc_in_dim\n\t        self.n_points = args.pc_npts\n\t        self.use_attention = args.use_attention\n\t        self.n_subprototypes = args.n_subprototypes\n", "        self.k_connect = args.k_connect\n\t        self.sigma = args.sigma\n\t        self.n_classes = self.n_way+1\n\t        self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k)\n\t        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\t        if self.use_attention:\n\t            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n\t        else:\n\t            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\t        self.feat_dim = args.edgeconv_widths[0][-1] + args.output_dim + args.base_widths[-1]\n", "    def forward(self, support_x, support_y, query_x, query_y):\n\t        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n\t        Return:\n\t            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n\t        \"\"\"\n", "        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n\t        support_feat = self.getFeatures(support_x)\n\t        support_feat = support_feat.view(self.n_way, self.k_shot, self.feat_dim, self.n_points)\n\t        query_feat = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n\t        query_feat = query_feat.transpose(1,2).contiguous().view(-1, self.feat_dim) #(n_queries*num_points, feat_dim)\n\t        fg_mask = support_y\n\t        bg_mask = torch.logical_not(support_y)\n\t        fg_prototypes, fg_labels = self.getForegroundPrototypes(support_feat, fg_mask, k=self.n_subprototypes)\n\t        bg_prototype, bg_labels = self.getBackgroundPrototypes(support_feat, bg_mask, k=self.n_subprototypes)\n\t        # prototype learning\n", "        if bg_prototype is not None and bg_labels is not None:\n\t            prototypes = torch.cat((bg_prototype, fg_prototypes), dim=0) #(*, feat_dim)\n\t            prototype_labels = torch.cat((bg_labels, fg_labels), dim=0) #(*,n_classes)\n\t        else:\n\t            prototypes = fg_prototypes\n\t            prototype_labels = fg_labels\n\t        self.num_prototypes = prototypes.shape[0]\n\t        # construct label matrix Y, with Y_ij = 1 if x_i is from the support set and labeled as y_i = j, otherwise Y_ij = 0.\n\t        self.num_nodes = self.num_prototypes + query_feat.shape[0] # number of node of partial observed graph\n\t        Y = torch.zeros(self.num_nodes, self.n_classes).cuda()\n", "        Y[:self.num_prototypes] = prototype_labels\n\t        # construct feat matrix F\n\t        node_feat = torch.cat((prototypes, query_feat), dim=0) #(num_nodes, feat_dim)\n\t        # label propagation\n\t        A = self.calculateLocalConstrainedAffinity(node_feat, k=self.k_connect)\n\t        Z = self.label_propagate(A, Y) #(num_nodes, n_way+1)\n\t        query_pred = Z[self.num_prototypes:, :] #(n_queries*num_points, n_way+1)\n\t        query_pred = query_pred.view(-1, query_y.shape[1], self.n_classes).transpose(1,2) #(n_queries, n_way+1, num_points)\n\t        loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\t        return query_pred, loss\n", "    def getFeatures(self, x):\n\t        \"\"\"\n\t        Forward the input data to network and generate features\n\t        :param x: input data with shape (B, C_in, L)\n\t        :return: features with shape (B, C_out, L)\n\t        \"\"\"\n\t        if self.use_attention:\n\t            feat_level1, feat_level2 = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            att_feat = self.att_learner(feat_level2)\n", "            return torch.cat((feat_level1, att_feat, feat_level3), dim=1)\n\t        else:\n\t            # return self.base_learner(self.encoder(x))\n\t            feat_level1, feat_level2 = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            map_feat = self.linear_mapper(feat_level2)\n\t            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n\t    def getMutiplePrototypes(self, feat, k):\n\t        \"\"\"\n\t        Extract multiple prototypes by points separation and assembly\n", "        Args:\n\t            feat: input point features, shape:(n_points, feat_dim)\n\t        Return:\n\t            prototypes: output prototypes, shape: (n_prototypes, feat_dim)\n\t        \"\"\"\n\t        # sample k seeds as initial centers with Farthest Point Sampling (FPS)\n\t        n = feat.shape[0]\n\t        assert n > 0\n\t        ratio = k / n\n\t        if ratio < 1:\n", "            fps_index = fps(feat, None, ratio=ratio, random_start=False).unique()\n\t            num_prototypes = len(fps_index)\n\t            farthest_seeds = feat[fps_index]\n\t            # compute the point-to-seed distance\n\t            distances = F.pairwise_distance(feat[..., None], farthest_seeds.transpose(0, 1)[None, ...],\n\t                                            p=2)  # (n_points, n_prototypes)\n\t            # hard assignment for each point\n\t            assignments = torch.argmin(distances, dim=1)  # (n_points,)\n\t            # aggregating each cluster to form prototype\n\t            prototypes = torch.zeros((num_prototypes, self.feat_dim)).cuda()\n", "            for i in range(num_prototypes):\n\t                selected = torch.nonzero(assignments == i).squeeze(1)\n\t                selected = feat[selected, :]\n\t                prototypes[i] = selected.mean(0)\n\t            return prototypes\n\t        else:\n\t            return feat\n\t    def getForegroundPrototypes(self, feats, masks, k=100):\n\t        \"\"\"\n\t        Extract foreground prototypes for each class via clustering point features within that class\n", "        Args:\n\t            feats: input support features, shape: (n_way, k_shot, feat_dim, num_points)\n\t            masks: foreground binary masks, shape: (n_way, k_shot, num_points)\n\t        Return:\n\t            prototypes: foreground prototypes, shape: (n_way*k, feat_dim)\n\t            labels: foreground prototype labels (one-hot), shape: (n_way*k, n_way+1)\n\t        \"\"\"\n\t        prototypes = []\n\t        labels = []\n\t        for i in range(self.n_way):\n", "            # extract point features belonging to current foreground class\n\t            feat = feats[i, ...].transpose(1,2).contiguous().view(-1, self.feat_dim) #(k_shot*num_points, feat_dim)\n\t            index = torch.nonzero(masks[i, ...].view(-1)).squeeze(1) #(k_shot*num_points,)\n\t            feat = feat[index]\n\t            class_prototypes = self.getMutiplePrototypes(feat, k)\n\t            prototypes.append(class_prototypes)\n\t            # construct label matrix\n\t            class_labels = torch.zeros(class_prototypes.shape[0], self.n_classes)\n\t            class_labels[:, i+1] = 1\n\t            labels.append(class_labels)\n", "        prototypes = torch.cat(prototypes, dim=0)\n\t        labels = torch.cat(labels, dim=0)\n\t        return prototypes, labels\n\t    def getBackgroundPrototypes(self, feats, masks, k=100):\n\t        \"\"\"\n\t        Extract background prototypes via clustering point features within background class\n\t        Args:\n\t            feats: input support features, shape: (n_way, k_shot, feat_dim, num_points)\n\t            masks: background binary masks, shape: (n_way, k_shot, num_points)\n\t        Return:\n", "            prototypes: background prototypes, shape: (k, feat_dim)\n\t            labels: background prototype labels (one-hot), shape: (k, n_way+1)\n\t        \"\"\"\n\t        feats = feats.transpose(2,3).contiguous().view(-1, self.feat_dim)\n\t        index = torch.nonzero(masks.view(-1)).squeeze(1)\n\t        feat = feats[index]\n\t        # in case this support set does not contain background points..\n\t        if feat.shape[0] != 0:\n\t            prototypes = self.getMutiplePrototypes(feat, k)\n\t            labels = torch.zeros(prototypes.shape[0], self.n_classes)\n", "            labels[:, 0] = 1\n\t            return prototypes, labels\n\t        else:\n\t            return None, None\n\t    def calculateLocalConstrainedAffinity(self, node_feat, k=200, method='gaussian'):\n\t        \"\"\"\n\t        Calculate the Affinity matrix of the nearest neighbor graph constructed by prototypes and query points,\n\t        It is a efficient way when the number of nodes in the graph is too large.\n\t        Args:\n\t            node_feat: input node features\n", "                  shape: (num_nodes, feat_dim)\n\t            k: the number of nearest neighbors for each node to compute the similarity\n\t            method: 'cosine' or 'gaussian', different similarity function\n\t        Return:\n\t            A: Affinity matrix with zero diagonal, shape: (num_nodes, num_nodes)\n\t        \"\"\"\n\t        # kNN search for the graph\n\t        X = node_feat.detach().cpu().numpy()\n\t        # build the index with cpu version\n\t        index = faiss.IndexFlatL2(self.feat_dim)\n", "        index.add(X)\n\t        _, I = index.search(X, k + 1)\n\t        I = torch.from_numpy(I[:, 1:]).cuda() #(num_nodes, k)\n\t        # create the affinity matrix\n\t        knn_idx = I.unsqueeze(2).expand(-1, -1, self.feat_dim).contiguous().view(-1, self.feat_dim)\n\t        knn_feat = torch.gather(node_feat, dim=0, index=knn_idx).contiguous().view(self.num_nodes, k, self.feat_dim)\n\t        if method == 'cosine':\n\t            knn_similarity = F.cosine_similarity(node_feat[:,None,:], knn_feat, dim=2)\n\t        elif method == 'gaussian':\n\t            dist = F.pairwise_distance(node_feat[:,:,None], knn_feat.transpose(1,2), p=2)\n", "            knn_similarity = torch.exp(-0.5*(dist/self.sigma)**2)\n\t        else:\n\t            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n\t        A = torch.zeros(self.num_nodes, self.num_nodes, dtype=torch.float).cuda()\n\t        A = A.scatter_(1, I, knn_similarity)\n\t        A = A + A.transpose(0,1)\n\t        identity_matrix = torch.eye(self.num_nodes, requires_grad=False).cuda()\n\t        A = A * (1 - identity_matrix)\n\t        return A\n\t    def label_propagate(self, A, Y, alpha=0.99):\n", "        \"\"\" Label Propagation, refer to \"Learning with Local and Global Consistency\" NeurIPs 2003\n\t        Args:\n\t            A: Affinity matrix with zero diagonal, shape: (num_nodes, num_nodes)\n\t            Y: initial label matrix, shape: (num_nodes, n_way+1)\n\t            alpha: a parameter to control the amount of propagated info.\n\t        Return:\n\t            Z: label predictions, shape: (num_nodes, n_way+1)\n\t        \"\"\"\n\t        #compute symmetrically normalized matrix S\n\t        eps = np.finfo(float).eps\n", "        D = A.sum(1) #(num_nodes,)\n\t        D_sqrt_inv = torch.sqrt(1.0/(D+eps))\n\t        D_sqrt_inv = torch.diag_embed(D_sqrt_inv).cuda()\n\t        S = D_sqrt_inv @ A @ D_sqrt_inv\n\t        #close form solution\n\t        Z = torch.inverse(torch.eye(self.num_nodes).cuda() - alpha*S + eps) @ Y\n\t        return Z\n\t    def computeCrossEntropyLoss(self, query_logits, query_labels):\n\t        \"\"\" Calculate the CrossEntropy Loss for query set\n\t        \"\"\"\n", "        return F.cross_entropy(query_logits, query_labels)\n"]}
{"filename": "models/protonet_FZ.py", "chunked_list": ["\"\"\" Prototypical Network \n\t\"\"\"\n\timport pdb\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom models.dgcnn import DGCNN\n\tfrom models.dgcnn_new import DGCNN_semseg\n\tfrom models.attention import SelfAttention, QGPA\n\tfrom models.gmmn import GMMNnetwork\n", "class BaseLearner(nn.Module):\n\t    \"\"\"The class for inner loop.\"\"\"\n\t    def __init__(self, in_channels, params):\n\t        super(BaseLearner, self).__init__()\n\t        self.num_convs = len(params)\n\t        self.convs = nn.ModuleList()\n\t        for i in range(self.num_convs):\n\t            if i == 0:\n\t                in_dim = in_channels\n\t            else:\n", "                in_dim = params[i-1]\n\t            self.convs.append(nn.Sequential(\n\t                              nn.Conv1d(in_dim, params[i], 1),\n\t                              nn.BatchNorm1d(params[i])))\n\t    def forward(self, x):\n\t        for i in range(self.num_convs):\n\t            x = self.convs[i](x)\n\t            if i != self.num_convs-1:\n\t                x = F.relu(x)\n\t        return x\n", "class ProtoNetAlignFZ(nn.Module):\n\t    def __init__(self, args):\n\t        super(ProtoNetAlignFZ, self).__init__()\n\t        self.n_way = args.n_way\n\t        self.k_shot = args.k_shot\n\t        self.dist_method = 'cosine'\n\t        self.in_channels = args.pc_in_dim\n\t        self.n_points = args.pc_npts\n\t        self.use_attention = args.use_attention\n\t        self.use_align = args.use_align\n", "        self.use_linear_proj = args.use_linear_proj\n\t        self.use_supervise_prototype = args.use_supervise_prototype\n\t        if args.use_high_dgcnn:\n\t            self.encoder = DGCNN_semseg(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n\t        else:\n\t            self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n\t        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\t        if self.use_attention:\n\t            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n\t        else:\n", "            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\t        if self.use_linear_proj:\n\t            self.conv_1 = nn.Sequential(nn.Conv1d(args.train_dim, args.train_dim, kernel_size=1, bias=False),\n\t                                   nn.BatchNorm1d(args.train_dim),\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.use_transformer = args.use_transformer\n\t        if self.use_transformer:\n\t            self.transformer = QGPA()\n\t        if args.use_zero:\n\t            self.generator = GMMNnetwork(args.noise_dim, args.noise_dim, args.train_dim, args.train_dim, args.gmm_dropout)\n", "    def forward(self, support_x, support_y, query_x, query_y):\n\t        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n\t        Return:\n\t            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n\t        \"\"\"\n", "        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n\t        support_feat, _ = self.getFeatures(support_x)\n\t        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points)\n\t        query_feat, xyz = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n\t        fg_mask = support_y\n\t        bg_mask = torch.logical_not(support_y)\n\t        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n\t        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask)\n\t        # prototype learning\n\t        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n", "        prototypes = [bg_prototype] + fg_prototypes\n\t        self_regulize_loss = 0\n\t        if self.use_supervise_prototype:\n\t            self_regulize_loss = self.sup_regulize_Loss(prototypes, support_feat, fg_mask, bg_mask)\n\t        if self.use_transformer:\n\t            prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0).repeat(query_feat.shape[0], 1, 1)\n\t            prototypes_all_post = self.transformer(query_feat, query_feat, prototypes_all)\n\t            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n\t            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n\t            query_pred = torch.stack(similarity, dim=1)\n", "            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\t        else:\n\t            similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n\t            query_pred = torch.stack(similarity, dim=1)\n\t            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\t        align_loss = 0\n\t        if self.use_align:\n\t            align_loss_epi = self.alignLoss_trans(query_feat, query_pred, support_feat, fg_mask, bg_mask)\n\t            align_loss += align_loss_epi\n\t        prototypes_all = prototypes_all.clone().detach()\n", "        return query_pred, loss + align_loss + self_regulize_loss, prototypes_all\n\t    def forward_test_semantic(self, support_x, support_y, query_x, query_y, embeddings=None):\n\t        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n\t        Return:\n\t            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n", "        \"\"\"\n\t        query_feat, xyz = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n\t        # prototype learning\n\t        if self.use_transformer:\n\t            prototypes_all = embeddings\n\t            prototypes_all_post = self.transformer(query_feat, query_feat, prototypes_all)\n\t            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n\t            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n\t            query_pred = torch.stack(similarity, dim=1)\n\t            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n", "        return query_pred, loss\n\t    def sup_regulize_Loss(self, prototype_supp, supp_fts, fore_mask, back_mask):\n\t        \"\"\"\n\t        Compute the loss for the prototype suppoort self alignment branch\n\t        Args:\n\t            prototypes: embedding features for query images\n\t                expect shape: N x C x num_points\n\t            supp_fts: embedding features for support images\n\t                expect shape: (Wa x Shot) x C x num_points\n\t            fore_mask: foreground masks for support images\n", "                expect shape: (way x shot) x num_points\n\t            back_mask: background masks for support images\n\t                expect shape: (way x shot) x num_points\n\t        \"\"\"\n\t        n_ways, n_shots = self.n_way, self.k_shot\n\t        # Compute the support loss\n\t        loss = 0\n\t        for way in range(n_ways):\n\t            prototypes = [prototype_supp[0], prototype_supp[way + 1]]\n\t            for shot in range(n_shots):\n", "                img_fts = supp_fts[way, shot].unsqueeze(0)\n\t                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes]\n\t                supp_pred = torch.stack(supp_dist, dim=1)\n\t                # Construct the support Ground-Truth segmentation\n\t                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\t                supp_label[fore_mask[way, shot] == 1] = 1\n\t                supp_label[back_mask[way, shot] == 1] = 0\n\t                # Compute Loss\n\t                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n\t        return loss\n", "    def getFeatures(self, x):\n\t        \"\"\"\n\t        Forward the input data to network and generate features\n\t        :param x: input data with shape (B, C_in, L)\n\t        :return: features with shape (B, C_out, L)\n\t        \"\"\"\n\t        if self.use_attention:\n\t            feat_level1, feat_level2, xyz = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            att_feat = self.att_learner(feat_level2)\n", "            if self.use_linear_proj:\n\t                return self.conv_1(torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1)), xyz\n\t            else:\n\t                return torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1), xyz\n\t        else:\n\t            # return self.base_learner(self.encoder(x))\n\t            feat_level1, feat_level2 = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            map_feat = self.linear_mapper(feat_level2)\n\t            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n", "    def getMaskedFeatures(self, feat, mask):\n\t        \"\"\"\n\t        Extract foreground and background features via masked average pooling\n\t        Args:\n\t            feat: input features, shape: (n_way, k_shot, feat_dim, num_points)\n\t            mask: binary mask, shape: (n_way, k_shot, num_points)\n\t        Return:\n\t            masked_feat: masked features, shape: (n_way, k_shot, feat_dim)\n\t        \"\"\"\n\t        mask = mask.unsqueeze(2)\n", "        masked_feat = torch.sum(feat * mask, dim=3) / (mask.sum(dim=3) + 1e-5)\n\t        return masked_feat\n\t    def getPrototype(self, fg_feat, bg_feat):\n\t        \"\"\"\n\t        Average the features to obtain the prototype\n\t        Args:\n\t            fg_feat: foreground features for each way/shot, shape: (n_way, k_shot, feat_dim)\n\t            bg_feat: background features for each way/shot, shape: (n_way, k_shot, feat_dim)\n\t        Returns:\n\t            fg_prototypes: a list of n_way foreground prototypes, each prototype is a vector with shape (feat_dim,)\n", "            bg_prototype: background prototype, a vector with shape (feat_dim,)\n\t        \"\"\"\n\t        fg_prototypes = [fg_feat[way, ...].sum(dim=0) / self.k_shot for way in range(self.n_way)]\n\t        bg_prototype = bg_feat.sum(dim=(0,1)) / (self.n_way * self.k_shot)\n\t        return fg_prototypes, bg_prototype\n\t    def calculateSimilarity(self, feat,  prototype, method='cosine', scaler=10):\n\t        \"\"\"\n\t        Calculate the Similarity between query point-level features and prototypes\n\t        Args:\n\t            feat: input query point-level features\n", "                  shape: (n_queries, feat_dim, num_points)\n\t            prototype: prototype of one semantic class\n\t                       shape: (feat_dim,)\n\t            method: 'cosine' or 'euclidean', different ways to calculate similarity\n\t            scaler: used when 'cosine' distance is computed.\n\t                    By multiplying the factor with cosine distance can achieve comparable performance\n\t                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n\t        Return:\n\t            similarity: similarity between query point to prototype\n\t                        shape: (n_queries, 1, num_points)\n", "        \"\"\"\n\t        if method == 'cosine':\n\t            similarity = F.cosine_similarity(feat, prototype[None, ..., None], dim=1) * scaler\n\t        elif method == 'euclidean':\n\t            similarity = - F.pairwise_distance(feat, prototype[None, ..., None], p=2)**2\n\t        else:\n\t            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n\t        return similarity\n\t    def calculateSimilarity_trans(self, feat,  prototype, method='cosine', scaler=10):\n\t        \"\"\"\n", "        Calculate the Similarity between query point-level features and prototypes\n\t        Args:\n\t            feat: input query point-level features\n\t                  shape: (n_queries, feat_dim, num_points)\n\t            prototype: prototype of one semantic class\n\t                       shape: (feat_dim,)\n\t            method: 'cosine' or 'euclidean', different ways to calculate similarity\n\t            scaler: used when 'cosine' distance is computed.\n\t                    By multiplying the factor with cosine distance can achieve comparable performance\n\t                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n", "        Return:\n\t            similarity: similarity between query point to prototype\n\t                        shape: (n_queries, 1, num_points)\n\t        \"\"\"\n\t        if method == 'cosine':\n\t            similarity = F.cosine_similarity(feat, prototype[..., None], dim=1) * scaler\n\t        elif method == 'euclidean':\n\t            similarity = - F.pairwise_distance(feat, prototype[..., None], p=2)**2\n\t        else:\n\t            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n", "        return similarity\n\t    def computeCrossEntropyLoss(self, query_logits, query_labels):\n\t        \"\"\" Calculate the CrossEntropy Loss for query set\n\t        \"\"\"\n\t        return F.cross_entropy(query_logits, query_labels)\n\t    def alignLoss_trans(self, qry_fts, pred, supp_fts, fore_mask, back_mask):\n\t        \"\"\"\n\t        Compute the loss for the prototype alignment branch\n\t        Args:\n\t            qry_fts: embedding features for query images\n", "                expect shape: N x C x num_points\n\t            pred: predicted segmentation score\n\t                expect shape: N x (1 + Wa) x num_points\n\t            supp_fts: embedding features for support images\n\t                expect shape: (Wa x Shot) x C x num_points\n\t            fore_mask: foreground masks for support images\n\t                expect shape: (way x shot) x num_points\n\t            back_mask: background masks for support images\n\t                expect shape: (way x shot) x num_points\n\t        \"\"\"\n", "        n_ways, n_shots = self.n_way, self.k_shot\n\t        # Mask and get query prototype\n\t        pred_mask = pred.argmax(dim=1, keepdim=True)  # N x 1 x H' x W'\n\t        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n\t        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n\t        pred_mask = torch.stack(binary_masks, dim=1).float()  # N x (1 + Wa) x 1 x H' x W'\n\t        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3)) / (pred_mask.sum(dim=(0, 3)) + 1e-5)\n\t        # Compute the support loss\n\t        loss = 0\n\t        for way in range(n_ways):\n", "            if way in skip_ways:\n\t                continue\n\t            # Get the query prototypes\n\t            prototypes = [qry_prototypes[0], qry_prototypes[way + 1]]\n\t            for shot in range(n_shots):\n\t                img_fts = supp_fts[way, shot].unsqueeze(0)\n\t                prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0)\n\t                prototypes_all_post = self.transformer(img_fts, img_fts, prototypes_all)\n\t                # prototypes_all_post = self.transformer(img_fts, qry_fts[0:1], prototypes_all)\n\t                prototypes_new = [prototypes_all_post[0, 0], prototypes_all_post[0, 1]]\n", "                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes_new]\n\t                supp_pred = torch.stack(supp_dist, dim=1)\n\t                # Construct the support Ground-Truth segmentation\n\t                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\t                supp_label[fore_mask[way, shot] == 1] = 1\n\t                supp_label[back_mask[way, shot] == 1] = 0\n\t                # Compute Loss\n\t                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n\t        return loss\n"]}
{"filename": "models/mpti_learner.py", "chunked_list": ["\"\"\" MPTI with/without attention Learner for Few-shot 3D Point Cloud Semantic Segmentation\n\t\"\"\"\n\timport os\n\timport torch\n\tfrom torch import optim\n\tfrom torch.nn import functional as F\n\tfrom models.mpti import MultiPrototypeTransductiveInference\n\tfrom utils.checkpoint_util import load_pretrain_checkpoint, load_model_checkpoint\n\tclass MPTILearner(object):\n\t    def __init__(self, args, mode='train'):\n", "        # init model and optimizer\n\t        self.model = MultiPrototypeTransductiveInference(args)\n\t        print(self.model)\n\t        if torch.cuda.is_available():\n\t            self.model.cuda()\n\t        if mode=='train':\n\t            if args.use_attention:\n\t                self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n\t                     {'params': self.model.base_learner.parameters()},\n", "                     {'params': self.model.att_learner.parameters()}], lr=args.lr)\n\t            else:\n\t                self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n\t                     {'params': self.model.base_learner.parameters()},\n\t                     {'params': self.model.linear_mapper.parameters()}], lr=args.lr)\n\t            #set learning rate scheduler\n\t            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.step_size,\n\t                                                          gamma=args.gamma)\n\t            if args.model_checkpoint_path is None:\n", "                # load pretrained model for point cloud encoding\n\t                self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n\t            else:\n\t                # resume from model checkpoint\n\t                self.model, self.optimizer = load_model_checkpoint(self.model, args.model_checkpoint_path,\n\t                                                                   optimizer=self.optimizer, mode='train')\n\t        elif mode=='test':\n\t            # Load model checkpoint\n\t            self.model = load_model_checkpoint(self.model, args.model_checkpoint_path, mode='test')\n\t        else:\n", "            raise ValueError('Wrong GraphLearner mode (%s)! Option:train/test' %mode)\n\t    def train(self, data):\n\t        \"\"\"\n\t        Args:\n\t            data: a list of torch tensors wit the following entries.\n\t            - support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            - support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n\t            - query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            - query_y: query labels with shape (n_queries, num_points)\n\t        \"\"\"\n", "        [support_x, support_y, query_x, query_y] = data\n\t        self.model.train()\n\t        query_logits, loss= self.model(support_x, support_y, query_x, query_y)\n\t        self.optimizer.zero_grad()\n\t        loss.backward()\n\t        self.optimizer.step()\n\t        self.lr_scheduler.step()\n\t        query_pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n\t        correct = torch.eq(query_pred, query_y).sum().item()  # including background class\n\t        accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n", "        return loss, accuracy\n\t    def test(self, data):\n\t        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points), each point \\in {0,1}.\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n\t        \"\"\"\n\t        [support_x, support_y, query_x, query_y] = data\n", "        self.model.eval()\n\t        with torch.no_grad():\n\t            logits, loss= self.model(support_x, support_y, query_x, query_y)\n\t            pred = F.softmax(logits, dim=1).argmax(dim=1)\n\t            correct = torch.eq(pred, query_y).sum().item()\n\t            accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\t        return pred, loss, accuracy\n"]}
{"filename": "models/proto_learner_FZ.py", "chunked_list": ["\"\"\" ProtoNet with/without attention learner for Few-shot 3D Point Cloud Semantic Segmentation\n\t\"\"\"\n\timport torch\n\tfrom torch import optim\n\tfrom torch.nn import functional as F\n\tfrom models.protonet_FZ import ProtoNetAlignFZ\n\tfrom utils.checkpoint_util import load_pretrain_checkpoint, load_model_checkpoint\n\tfrom models.gmmn import GMMNLoss\n\timport numpy as np\n\tclass ProtoLearnerFZ(object):\n", "    def __init__(self, args, mode='train'):\n\t        self.model = ProtoNetAlignFZ(args)\n\t        print(self.model)\n\t        if torch.cuda.is_available():\n\t            self.model.cuda()\n\t        if mode == 'train':\n\t            if args.use_attention:\n\t                if args.use_transformer:\n\t                    self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n", "                     {'params': self.model.base_learner.parameters()},\n\t                     {'params': self.model.transformer.parameters(), 'lr': args.trans_lr},\n\t                     {'params': self.model.att_learner.parameters()}\n\t                     ], lr=args.lr)\n\t                else:\n\t                    self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n\t                     {'params': self.model.base_learner.parameters()},\n\t                     {'params': self.model.att_learner.parameters()}\n\t                     ], lr=args.lr)\n", "            else:\n\t                self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n\t                     {'params': self.model.base_learner.parameters()},\n\t                     {'params': self.model.linear_mapper.parameters()}], lr=args.lr)\n\t            #set learning rate scheduler\n\t            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.step_size,\n\t                                                          gamma=args.gamma)\n\t            # load pretrained model for point cloud encoding\n\t            self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n", "        elif mode == 'test':\n\t            # Load model checkpoint\n\t            self.model = load_model_checkpoint(self.model, args.model_checkpoint_path, mode='test')\n\t        else:\n\t            raise ValueError('Wrong GMMLearner mode (%s)! Option:train/test' %mode)\n\t        self.optimizer_generator = torch.optim.Adam(self.model.generator.parameters(), lr=args.generator_lr)\n\t        self.criterion_generator = GMMNLoss(\n\t            sigma=[2, 5, 10, 20, 40, 80], cuda=True).build_loss()\n\t        self.noise_dim = args.noise_dim\n\t        self.gmm_weight = args.gmm_weight\n", "        self.n_way = args.n_way\n\t        vec_name = 'glove'\n\t        if args.dataset == 's3dis':\n\t            self.bg_id = 12\n\t            self.embeddings = torch.from_numpy(np.load('dataloaders/S3DIS_{}.npy'.format(vec_name))) # S3DIS_fasttext.npy S3DIS_word2vec_google.npy\n\t        elif args.dataset == 'scannet':\n\t            self.bg_id = 0\n\t            self.embeddings = torch.from_numpy(np.load('dataloaders/ScanNet_{}.npy'.format(vec_name))) # S3DIS_fasttext.npy S3DIS_word2vec_google.npy\n\t        self.embeddings = torch.nn.functional.normalize(self.embeddings, p=2, dim=1)\n\t    def train(self, data, sampled_classes):\n", "        \"\"\"\n\t        Args:\n\t            data: a list of torch tensors wit the following entries.\n\t            - support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            - support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n\t            - query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            - query_y: query labels with shape (n_queries, num_points)\n\t        \"\"\"\n\t        [support_x, support_y, query_x, query_y] = data\n\t        self.model.train()\n", "        query_logits, loss, prototype_gts = self.model(support_x, support_y, query_x, query_y)\n\t        self.optimizer.zero_grad()\n\t        loss.backward()\n\t        self.optimizer.step()\n\t        self.lr_scheduler.step()\n\t        query_pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n\t        correct = torch.eq(query_pred, query_y).sum().item()  # including background class\n\t        accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\t        self.optimizer_generator.zero_grad()\n\t        support_embeddings = torch.cat([self.embeddings[self.bg_id].unsqueeze(0), self.embeddings[sampled_classes]], dim=0).to(query_pred.device)\n", "        prototype_fakes = []\n\t        for i in range(self.n_way):\n\t            z_g = torch.rand((support_embeddings.shape[0], self.noise_dim)).cuda()\n\t            prototype_fakes.append(self.model.generator(support_embeddings, z_g.float()))\n\t        prototype_fakes = torch.stack(prototype_fakes, dim=0)\n\t        g_loss_bg = self.criterion_generator(prototype_fakes[:, :1, :].flatten(0, 1), prototype_gts[:, :1, :].flatten(0, 1))\n\t        g_loss_fg = self.criterion_generator(prototype_fakes[:, 1:, :].flatten(0, 1), prototype_gts[:, 1:, :].flatten(0, 1))\n\t        g_loss = g_loss_bg * self.gmm_weight + g_loss_fg\n\t        g_loss.backward()\n\t        self.optimizer_generator.step()\n", "        return loss, accuracy\n\t    def test_semantic(self, data, sampled_classes):\n\t        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points), each point \\in {0,1}.\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n\t        \"\"\"\n\t        [_, _, query_x, query_y] = data\n", "        self.model.eval()\n\t        support_embeddings = torch.cat([self.embeddings[self.bg_id].unsqueeze(0), self.embeddings[sampled_classes]], dim=0).cuda()\n\t        with torch.no_grad():\n\t            prototype_fakes = []\n\t            for i in range(self.n_way):\n\t                z_g = torch.rand((support_embeddings.shape[0], self.noise_dim)).cuda()\n\t                prototype_fakes.append(self.model.generator(support_embeddings, z_g.float()))\n\t            prototype_fakes = torch.stack(prototype_fakes, dim=0)\n\t            logits, loss = self.model.forward_test_semantic(_, _, query_x, query_y, prototype_fakes)\n\t            pred = F.softmax(logits, dim=1).argmax(dim=1)\n", "            correct = torch.eq(pred, query_y).sum().item()\n\t            accuracy = correct / (query_y.shape[0] * query_y.shape[1])\n\t        return pred, loss, accuracy"]}
{"filename": "models/protonet.py", "chunked_list": ["\"\"\" Prototypical Network \n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom models.dgcnn import DGCNN\n\tfrom models.attention import SelfAttention\n\tclass BaseLearner(nn.Module):\n\t    \"\"\"The class for inner loop.\"\"\"\n\t    def __init__(self, in_channels, params):\n", "        super(BaseLearner, self).__init__()\n\t        self.num_convs = len(params)\n\t        self.convs = nn.ModuleList()\n\t        for i in range(self.num_convs):\n\t            if i == 0:\n\t                in_dim = in_channels\n\t            else:\n\t                in_dim = params[i-1]\n\t            self.convs.append(nn.Sequential(\n\t                              nn.Conv1d(in_dim, params[i], 1),\n", "                              nn.BatchNorm1d(params[i])))\n\t    def forward(self, x):\n\t        for i in range(self.num_convs):\n\t            x = self.convs[i](x)\n\t            if i != self.num_convs-1:\n\t                x = F.relu(x)\n\t        return x\n\tclass ProtoNet(nn.Module):\n\t    def __init__(self, args):\n\t        super(ProtoNet, self).__init__()\n", "        self.n_way = args.n_way\n\t        self.k_shot = args.k_shot\n\t        self.dist_method = 'cosine'\n\t        self.in_channels = args.pc_in_dim\n\t        self.n_points = args.pc_npts\n\t        self.use_attention = args.use_attention\n\t        self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k)\n\t        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\t        if self.use_attention:\n\t            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n", "        else:\n\t            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\t    def forward(self, support_x, support_y, query_x, query_y):\n\t        \"\"\"\n\t        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n\t        Return:\n", "            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n\t        \"\"\"\n\t        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n\t        support_feat = self.getFeatures(support_x)\n\t        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points) # [2, 1, 192, 2048]\n\t        query_feat = self.getFeatures(query_x) #(n_queries, feat_dim, num_points) # [2, 192, 2048]\n\t        fg_mask = support_y # [2, 1, 2048]\n\t        bg_mask = torch.logical_not(support_y)\n\t        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n\t        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask) # [2, 1, 192]\n", "        # prototype learning\n\t        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n\t        prototypes = [bg_prototype] + fg_prototypes\n\t        # non-parametric metric learning\n\t        similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n\t        query_pred = torch.stack(similarity, dim=1) #(n_queries, n_way+1, num_points)\n\t        loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\t        return query_pred, loss\n\t    def forward_dummy(self, support_x):\n\t        \"\"\"\n", "        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n\t        Return:\n\t            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n\t        \"\"\"\n\t        support_x = support_x.squeeze(0)\n\t        query_x = support_x\n", "        support_y = torch.randn(support_x.shape[0], 1, support_x.shape[2]).cuda()\n\t        query_y = torch.randn(support_x.shape[0], support_x.shape[2]).cuda()\n\t        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n\t        support_feat = self.getFeatures(support_x)\n\t        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points) # [2, 1, 192, 2048]\n\t        query_feat = self.getFeatures(query_x) #(n_queries, feat_dim, num_points) # [2, 192, 2048]\n\t        fg_mask = support_y # [2, 1, 2048]\n\t        bg_mask = torch.logical_not(support_y)\n\t        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n\t        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask) # [2, 1, 192]\n", "        # prototype learning\n\t        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n\t        prototypes = [bg_prototype] + fg_prototypes\n\t        # non-parametric metric learning\n\t        similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n\t        query_pred = torch.stack(similarity, dim=1) #(n_queries, n_way+1, num_points)\n\t        return query_pred\n\t    def getFeatures(self, x):\n\t        \"\"\"\n\t        Forward the input data to network and generate features\n", "        :param x: input data with shape (B, C_in, L)\n\t        :return: features with shape (B, C_out, L)\n\t        \"\"\"\n\t        if self.use_attention:\n\t            feat_level1, feat_level2 = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            att_feat = self.att_learner(feat_level2)\n\t            return torch.cat((feat_level1, att_feat, feat_level3), dim=1)\n\t        else:\n\t            # return self.base_learner(self.encoder(x))\n", "            feat_level1, feat_level2 = self.encoder(x)\n\t            feat_level3 = self.base_learner(feat_level2)\n\t            map_feat = self.linear_mapper(feat_level2)\n\t            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n\t    def getMaskedFeatures(self, feat, mask):\n\t        \"\"\"\n\t        Extract foreground and background features via masked average pooling\n\t        Args:\n\t            feat: input features, shape: (n_way, k_shot, feat_dim, num_points)\n\t            mask: binary mask, shape: (n_way, k_shot, num_points)\n", "        Return:\n\t            masked_feat: masked features, shape: (n_way, k_shot, feat_dim)\n\t        \"\"\"\n\t        mask = mask.unsqueeze(2)\n\t        masked_feat = torch.sum(feat * mask, dim=3) / (mask.sum(dim=3) + 1e-5)\n\t        return masked_feat\n\t    def getPrototype(self, fg_feat, bg_feat):\n\t        \"\"\"\n\t        Average the features to obtain the prototype\n\t        Args:\n", "            fg_feat: foreground features for each way/shot, shape: (n_way, k_shot, feat_dim)\n\t            bg_feat: background features for each way/shot, shape: (n_way, k_shot, feat_dim)\n\t        Returns:\n\t            fg_prototypes: a list of n_way foreground prototypes, each prototype is a vector with shape (feat_dim,)\n\t            bg_prototype: background prototype, a vector with shape (feat_dim,)\n\t        \"\"\"\n\t        fg_prototypes = [fg_feat[way, ...].sum(dim=0) / self.k_shot for way in range(self.n_way)]\n\t        bg_prototype = bg_feat.sum(dim=(0,1)) / (self.n_way * self.k_shot)\n\t        return fg_prototypes, bg_prototype\n\t    def calculateSimilarity(self, feat,  prototype, method='cosine', scaler=10):\n", "        \"\"\"\n\t        Calculate the Similarity between query point-level features and prototypes\n\t        Args:\n\t            feat: input query point-level features\n\t                  shape: (n_queries, feat_dim, num_points)\n\t            prototype: prototype of one semantic class\n\t                       shape: (feat_dim,)\n\t            method: 'cosine' or 'euclidean', different ways to calculate similarity\n\t            scaler: used when 'cosine' distance is computed.\n\t                    By multiplying the factor with cosine distance can achieve comparable performance\n", "                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n\t        Return:\n\t            similarity: similarity between query point to prototype\n\t                        shape: (n_queries, 1, num_points)\n\t        \"\"\"\n\t        if method == 'cosine':\n\t            similarity = F.cosine_similarity(feat, prototype[None, ..., None], dim=1) * scaler\n\t        elif method == 'euclidean':\n\t            similarity = - F.pairwise_distance(feat, prototype[None, ..., None], p=2)**2\n\t        else:\n", "            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n\t        return similarity\n\t    def computeCrossEntropyLoss(self, query_logits, query_labels):\n\t        \"\"\" Calculate the CrossEntropy Loss for query set\n\t        \"\"\"\n\t        return F.cross_entropy(query_logits, query_labels)\n"]}
{"filename": "models/__init__.py", "chunked_list": []}
{"filename": "models/dgcnn.py", "chunked_list": ["﻿\"\"\"DGCNN as Backbone to extract point-level features\n\t   Adapted from https://github.com/WangYueFt/dgcnn/blob/master/pytorch/model.py\n\t\"\"\"\n\timport os\n\timport sys\n\timport copy\n\timport math\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n", "import torch.nn.functional as F\n\tdef knn(x, k):\n\t    inner = -2 * torch.matmul(x.transpose(2, 1), x) #(B,N,N)\n\t    xx = torch.sum(x ** 2, dim=1, keepdim=True) #(B,1,N)\n\t    pairwise_distance = -xx - inner - xx.transpose(2, 1) #(B,N,N)\n\t    idx = pairwise_distance.topk(k=k, dim=-1)[1]  # (B,N,k)\n\t    return idx\n\tdef get_edge_feature(x, K=20, idx=None):\n\t    \"\"\"Construct edge feature for each point\n\t      Args:\n", "        x: point clouds (B, C, N)\n\t        K: int\n\t        idx: knn index, if not None, the shape is (B, N, K)\n\t      Returns:\n\t        edge feat: (B, 2C, N, K)\n\t    \"\"\"\n\t    B, C, N = x.size()\n\t    if idx is None:\n\t        idx = knn(x, k=K)  # (batch_size, num_points, k)\n\t    central_feat = x.unsqueeze(-1).expand(-1,-1,-1,K)\n", "    idx = idx.unsqueeze(1).expand(-1, C, -1, -1).contiguous().view(B,C,N*K)\n\t    knn_feat = torch.gather(x, dim=2, index=idx).contiguous().view(B,C,N,K)\n\t    edge_feat = torch.cat((knn_feat-central_feat, central_feat), dim=1)\n\t    return edge_feat\n\tclass conv2d(nn.Module):\n\t    def __init__(self, in_feat, layer_dims, batch_norm=True, relu=True, bias=False):\n\t        super().__init__()\n\t        self.layer_dims = layer_dims\n\t        layers = []\n\t        for i in range(len(layer_dims)):\n", "            in_dim = in_feat if i==0 else layer_dims[i-1]\n\t            out_dim = layer_dims[i]\n\t            layers.append(nn.Conv2d(in_dim, out_dim, kernel_size=1, bias=bias))\n\t            if batch_norm:\n\t                layers.append(nn.BatchNorm2d(out_dim))\n\t            if relu:\n\t                layers.append(nn.LeakyReLU(0.2))\n\t        self.layer = nn.Sequential(*layers)\n\t    def forward(self, x):\n\t        return self.layer(x)\n", "class conv1d(nn.Module):\n\t    def __init__(self, in_feat, layer_dims, batch_norm=True, relu=True, bias=False):\n\t        super().__init__()\n\t        self.layer_dims = layer_dims\n\t        layers = []\n\t        for i in range(len(layer_dims)):\n\t            in_dim = in_feat if i==0 else layer_dims[i-1]\n\t            out_dim = layer_dims[i]\n\t            layers.append(nn.Conv1d(in_dim, out_dim, kernel_size=1, bias=bias))\n\t            if batch_norm:\n", "                layers.append(nn.BatchNorm1d(out_dim))\n\t            if relu:\n\t                layers.append(nn.LeakyReLU(0.2))\n\t        self.layer = nn.Sequential(*layers)\n\t    def forward(self, x):\n\t        return self.layer(x)\n\tclass DGCNN(nn.Module):\n\t    \"\"\"\n\t    DGCNN with only stacked EdgeConv, return intermediate features if use attention\n\t    Parameters:\n", "      edgeconv_widths: list of layer widths of edgeconv blocks [[],[],...]\n\t      mlp_widths: list of layer widths of mlps following Edgeconv blocks\n\t      nfeat: number of input features\n\t      k: number of neighbors\n\t      conv_aggr: neighbor information aggregation method, Option:['add', 'mean', 'max', None]\n\t    \"\"\"\n\t    def __init__(self, edgeconv_widths, mlp_widths, nfeat, k=20, return_edgeconvs=False):\n\t        super(DGCNN, self).__init__()\n\t        self.n_edgeconv = len(edgeconv_widths)\n\t        self.k = k\n", "        self.return_edgeconvs = return_edgeconvs\n\t        self.edge_convs = nn.ModuleList()\n\t        for i in range(self.n_edgeconv):\n\t            if i==0:\n\t                in_feat = nfeat*2\n\t            else:\n\t                in_feat = edgeconv_widths[i-1][-1]*2\n\t            self.edge_convs.append(conv2d(in_feat, edgeconv_widths[i]))\n\t        in_dim = 0\n\t        for edgeconv_width in edgeconv_widths:\n", "            in_dim += edgeconv_width[-1]\n\t        self.conv = conv1d(in_dim, mlp_widths)\n\t    def forward(self, x):\n\t        edgeconv_outputs = []\n\t        for i in range(self.n_edgeconv):\n\t            x = get_edge_feature(x, K=self.k)\n\t            x = self.edge_convs[i](x)\n\t            x = x.max(dim=-1, keepdim=False)[0]\n\t            edgeconv_outputs.append(x)\n\t        out = torch.cat(edgeconv_outputs, dim=1) # [16, 192, 2048]\n", "        out = self.conv(out) # [16, 256, 2048]\n\t        if self.return_edgeconvs:\n\t            return edgeconv_outputs, out\n\t        else:\n\t            return edgeconv_outputs[0], out\n"]}
{"filename": "models/proto_learner.py", "chunked_list": ["\"\"\" ProtoNet with/without attention learner for Few-shot 3D Point Cloud Semantic Segmentation\n\t\"\"\"\n\timport torch\n\tfrom torch import optim\n\tfrom torch.nn import functional as F\n\tfrom models.protonet_QGPA import ProtoNetAlignQGPASR\n\tfrom models.protonet import ProtoNet\n\tfrom utils.checkpoint_util import load_pretrain_checkpoint, load_model_checkpoint\n\tclass ProtoLearner(object):\n\t    def __init__(self, args, mode='train'):\n", "        # init model and optimizer\n\t        if args.use_transformer:\n\t            self.model = ProtoNetAlignQGPASR(args)\n\t        else:\n\t            self.model = ProtoNet(args)\n\t        print(self.model)\n\t        if torch.cuda.is_available():\n\t            self.model.cuda()\n\t        if mode == 'train':\n\t            if args.use_attention:\n", "                if args.use_transformer:\n\t                    self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n\t                     {'params': self.model.base_learner.parameters()},\n\t                     {'params': self.model.transformer.parameters(), 'lr': args.trans_lr},\n\t                     {'params': self.model.att_learner.parameters()}\n\t                     ], lr=args.lr)\n\t                else:\n\t                    self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n", "                     {'params': self.model.base_learner.parameters()},\n\t                     {'params': self.model.att_learner.parameters()}\n\t                     ], lr=args.lr)\n\t            else:\n\t                self.optimizer = torch.optim.Adam(\n\t                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n\t                     {'params': self.model.base_learner.parameters()},\n\t                     {'params': self.model.linear_mapper.parameters()}], lr=args.lr)\n\t            #set learning rate scheduler\n\t            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.step_size,\n", "                                                          gamma=args.gamma)\n\t            # load pretrained model for point cloud encoding\n\t            self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n\t        elif mode == 'test':\n\t            # Load model checkpoint\n\t            self.model = load_model_checkpoint(self.model, args.model_checkpoint_path, mode='test')\n\t        else:\n\t            raise ValueError('Wrong GMMLearner mode (%s)! Option:train/test' %mode)\n\t    def train(self, data, sampled_classes):\n\t        \"\"\"\n", "        Args:\n\t            data: a list of torch tensors wit the following entries.\n\t            - support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            - support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n\t            - query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            - query_y: query labels with shape (n_queries, num_points)\n\t        \"\"\"\n\t        [support_x, support_y, query_x, query_y] = data\n\t        self.model.train()\n\t        query_logits, loss, _ = self.model(support_x, support_y, query_x, query_y)\n", "        self.optimizer.zero_grad()\n\t        loss.backward()\n\t        self.optimizer.step()\n\t        self.lr_scheduler.step()\n\t        query_pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n\t        correct = torch.eq(query_pred, query_y).sum().item()  # including background class\n\t        accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\t        return loss, accuracy\n\t    def test(self, data):\n\t        \"\"\"\n", "        Args:\n\t            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n\t            support_y: support masks (foreground) with shape (n_way, k_shot, num_points), each point \\in {0,1}.\n\t            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n\t            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n\t        \"\"\"\n\t        [support_x, support_y, query_x, query_y] = data\n\t        self.model.eval()\n\t        with torch.no_grad():\n\t            logits, loss, fg_prototypes = self.model(support_x, support_y, query_x, query_y)\n", "            pred = F.softmax(logits, dim=1).argmax(dim=1)\n\t            correct = torch.eq(pred, query_y).sum().item()\n\t            accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\t        return pred, loss, accuracy"]}
{"filename": "models/attention.py", "chunked_list": ["\"\"\"Self Attention Module\n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass SelfAttention(nn.Module):\n\t    def __init__(self, in_channel, out_channel=None, attn_dropout=0.1):\n\t        \"\"\"\n\t        :param in_channel: previous layer's output feature dimension\n\t        :param out_channel: size of output vector, defaults to in_channel\n", "        \"\"\"\n\t        super(SelfAttention, self).__init__()\n\t        self.in_channel = in_channel\n\t        if out_channel is not None:\n\t            self.out_channel = out_channel\n\t        else:\n\t            self.out_channel = in_channel\n\t        self.temperature = self.out_channel ** 0.5\n\t        self.q_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n\t        self.k_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n", "        self.v_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n\t        self.dropout = nn.Dropout(attn_dropout)\n\t    def forward(self, x):\n\t        \"\"\"\n\t        :param x: the feature maps from previous layer,\n\t                      shape: (batch_size, in_channel, num_points)\n\t        :return: y: attentioned features maps,\n\t                        shape： (batch_size, out_channel, num_points)\n\t        \"\"\"\n\t        q = self.q_map(x)  # (batch_size, out_channel, num_points)\n", "        k = self.k_map(x)  # (batch_size, out_channel, num_points)\n\t        v = self.v_map(x)  # (batch_size, out_channel, num_points)\n\t        attn = torch.matmul(q.transpose(1,2) / self.temperature, k)\n\t        attn = self.dropout(F.softmax(attn, dim=-1))\n\t        y = torch.matmul(attn, v.transpose(1,2)) # (batch_size, num_points, out_channel)\n\t        return y.transpose(1, 2)\n\tclass QGPA(nn.Module):\n\t    def __init__(self, attn_dropout=0.1):\n\t        super(QGPA, self).__init__()\n\t        self.in_channel = self.out_channel = 320\n", "        self.temperature = self.out_channel ** 0.5\n\t        self.layer_norm = nn.LayerNorm(self.in_channel)\n\t        proj_dim = 512\n\t        self.q_map = nn.Conv1d(2048, proj_dim, 1, bias=False)\n\t        self.k_map = nn.Conv1d(2048, proj_dim, 1, bias=False)\n\t        self.v_map = nn.Linear(self.in_channel, self.out_channel)\n\t        self.fc = nn.Conv1d(self.in_channel, self.out_channel, 1, bias=False)\n\t        self.dropout = nn.Dropout(attn_dropout)\n\t    def forward(self, query, support, prototype):\n\t        batch, dim = query.shape[0], query.shape[1]\n", "        way = support.shape[0] + 1\n\t        residual = prototype\n\t        q = self.q_map(query.transpose(1, 2))\n\t        if len(support.shape) == 4:\n\t            support = support.squeeze()\n\t        support = torch.cat([support.mean(0).unsqueeze(0), support], dim=0)\n\t        k = self.k_map(support.transpose(1, 2))\n\t        v = self.v_map(prototype)\n\t        q = q.view(q.shape[1], q.shape[2] * q.shape[0])\n\t        k = k.view(k.shape[1], k.shape[2] * k.shape[0])\n", "        attn = torch.matmul(q.transpose(0, 1) / self.temperature, k)\n\t        attn = attn.reshape(batch, way, dim, dim)\n\t        attn = F.softmax(attn, dim=-1)\n\t        v = v.unsqueeze(2)\n\t        output = torch.matmul(attn, v.transpose(-2, -1)).squeeze(-1).transpose(1, 2)\n\t        output = self.dropout(self.fc(output)).transpose(1, 2)\n\t        output = self.layer_norm(output + residual)\n\t        return output\n"]}
{"filename": "models/gmmn.py", "chunked_list": ["import torch\n\tfrom torch import nn\n\tclass GMMNnetwork(nn.Module):\n\t    def __init__(\n\t        self,\n\t        noise_dim,\n\t        embed_dim,\n\t        hidden_size,\n\t        feature_dim,\n\t        drop_out_gmm,\n", "        semantic_reconstruction=False,\n\t    ):\n\t        super().__init__()\n\t        def block(in_feat, out_feat):\n\t            layers = [nn.Linear(in_feat, out_feat)]\n\t            layers.append(nn.LeakyReLU(0.2, inplace=True))\n\t            layers.append(nn.Dropout(p=drop_out_gmm))\n\t            return layers\n\t        def init_weights(m):\n\t            if type(m) == nn.Linear:\n", "                torch.nn.init.xavier_uniform_(m.weight)\n\t                m.bias.data.fill_(0.01)\n\t        if hidden_size:\n\t            self.model = nn.Sequential(\n\t                *block(noise_dim + embed_dim, hidden_size),\n\t                nn.Linear(hidden_size, feature_dim),\n\t            )\n\t        else:\n\t            self.model = nn.Linear(noise_dim + embed_dim, feature_dim)\n\t        self.model.apply(init_weights)\n", "        self.semantic_reconstruction = semantic_reconstruction\n\t        if self.semantic_reconstruction:\n\t            self.semantic_reconstruction_layer = nn.Linear(\n\t                feature_dim, noise_dim + embed_dim\n\t            )\n\t    def forward(self, embd, noise):\n\t        features = self.model(torch.cat((embd, noise), 1))\n\t        if self.semantic_reconstruction:\n\t            semantic = self.semantic_reconstruction_layer(features)\n\t            return features, semantic\n", "        else:\n\t            return features\n\tclass GMMNLoss:\n\t    def __init__(self, sigma=[2, 5, 10, 20, 40, 80], cuda=False):\n\t        self.sigma = sigma\n\t        self.cuda = cuda\n\t    def build_loss(self):\n\t        return self.moment_loss\n\t    def get_scale_matrix(self, M, N):\n\t        s1 = torch.ones((N, 1)) * 1.0 / N\n", "        s2 = torch.ones((M, 1)) * -1.0 / M\n\t        if self.cuda:\n\t            s1, s2 = s1.cuda(), s2.cuda()\n\t        return torch.cat((s1, s2), 0)\n\t    def moment_loss(self, gen_samples, x):\n\t        X = torch.cat((gen_samples, x), 0)\n\t        XX = torch.matmul(X, X.t())\n\t        X2 = torch.sum(X * X, 1, keepdim=True)\n\t        exp = XX - 0.5 * X2 - 0.5 * X2.t()\n\t        M = gen_samples.size()[0]\n", "        N = x.size()[0]\n\t        s = self.get_scale_matrix(M, N)\n\t        S = torch.matmul(s, s.t())\n\t        loss = 0\n\t        for v in self.sigma:\n\t            kernel_val = torch.exp(exp / v)\n\t            loss += torch.sum(S * kernel_val)\n\t        loss = torch.sqrt(loss)\n\t        return loss\n"]}
{"filename": "models/dgcnn_new.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\t@Author: Yue Wang\n\t@Contact: yuewangx@mit.edu\n\t@File: model.py\n\t@Time: 2018/10/13 6:35 PM\n\tModified by \n\t@Author: An Tao\n\t@Contact: ta19@mails.tsinghua.edu.cn\n", "@Time: 2020/3/9 9:32 PM\n\t\"\"\"\n\timport os\n\timport sys\n\timport copy\n\timport math\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.init as init\n", "import torch.nn.functional as F\n\tdef knn(x, k):\n\t    inner = -2*torch.matmul(x.transpose(2, 1), x)\n\t    xx = torch.sum(x**2, dim=1, keepdim=True)\n\t    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n\t    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n\t    return idx\n\tdef get_graph_feature(x, k=20, idx=None, dim9=False):\n\t    batch_size = x.size(0)\n\t    num_points = x.size(2)\n", "    x = x.view(batch_size, -1, num_points)\n\t    if idx is None:\n\t        if dim9 == False:\n\t            idx = knn(x, k=k)   # (batch_size, num_points, k)\n\t        else:\n\t            idx = knn(x[:, 6:], k=k)\n\t    device = torch.device('cuda')\n\t    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n\t    idx = idx + idx_base\n\t    idx = idx.view(-1)\n", "    _, num_dims, _ = x.size()\n\t    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n\t    feature = x.view(batch_size*num_points, -1)[idx, :]\n\t    feature = feature.view(batch_size, num_points, k, num_dims) \n\t    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n\t    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n\t    return feature      # (batch_size, 2*num_dims, num_points, k)\n\tclass PointNet(nn.Module):\n\t    def __init__(self, args, output_channels=40):\n\t        super(PointNet, self).__init__()\n", "        self.args = args\n\t        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n\t        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n\t        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n\t        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n\t        self.conv5 = nn.Conv1d(128, args.emb_dims, kernel_size=1, bias=False)\n\t        self.bn1 = nn.BatchNorm1d(64)\n\t        self.bn2 = nn.BatchNorm1d(64)\n\t        self.bn3 = nn.BatchNorm1d(64)\n\t        self.bn4 = nn.BatchNorm1d(128)\n", "        self.bn5 = nn.BatchNorm1d(args.emb_dims)\n\t        self.linear1 = nn.Linear(args.emb_dims, 512, bias=False)\n\t        self.bn6 = nn.BatchNorm1d(512)\n\t        self.dp1 = nn.Dropout()\n\t        self.linear2 = nn.Linear(512, output_channels)\n\t    def forward(self, x):\n\t        x = F.relu(self.bn1(self.conv1(x)))\n\t        x = F.relu(self.bn2(self.conv2(x)))\n\t        x = F.relu(self.bn3(self.conv3(x)))\n\t        x = F.relu(self.bn4(self.conv4(x)))\n", "        x = F.relu(self.bn5(self.conv5(x)))\n\t        x = F.adaptive_max_pool1d(x, 1).squeeze()\n\t        x = F.relu(self.bn6(self.linear1(x)))\n\t        x = self.dp1(x)\n\t        x = self.linear2(x)\n\t        return x\n\tclass DGCNN_cls(nn.Module):\n\t    def __init__(self, args, output_channels=40):\n\t        super(DGCNN_cls, self).__init__()\n\t        self.args = args\n", "        self.k = args.k\n\t        self.bn1 = nn.BatchNorm2d(64)\n\t        self.bn2 = nn.BatchNorm2d(64)\n\t        self.bn3 = nn.BatchNorm2d(128)\n\t        self.bn4 = nn.BatchNorm2d(256)\n\t        self.bn5 = nn.BatchNorm1d(args.emb_dims)\n\t        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n\t                                   self.bn1,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n", "                                   self.bn2,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n\t                                   self.bn3,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n\t                                   self.bn4,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv5 = nn.Sequential(nn.Conv1d(512, args.emb_dims, kernel_size=1, bias=False),\n\t                                   self.bn5,\n", "                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.linear1 = nn.Linear(args.emb_dims*2, 512, bias=False)\n\t        self.bn6 = nn.BatchNorm1d(512)\n\t        self.dp1 = nn.Dropout(p=args.dropout)\n\t        self.linear2 = nn.Linear(512, 256)\n\t        self.bn7 = nn.BatchNorm1d(256)\n\t        self.dp2 = nn.Dropout(p=args.dropout)\n\t        self.linear3 = nn.Linear(256, output_channels)\n\t    def forward(self, x):\n\t        batch_size = x.size(0)\n", "        x = get_graph_feature(x, k=self.k)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n\t        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n\t        x = self.conv2(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n\t        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n\t        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n\t        x = get_graph_feature(x3, k=self.k)     # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n", "        x = self.conv4(x)                       # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n\t        x4 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n\t        x = torch.cat((x1, x2, x3, x4), dim=1)  # (batch_size, 64+64+128+256, num_points)\n\t        x = self.conv5(x)                       # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n\t        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n\t        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n\t        x = torch.cat((x1, x2), 1)              # (batch_size, emb_dims*2)\n\t        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2) # (batch_size, emb_dims*2) -> (batch_size, 512)\n\t        x = self.dp1(x)\n\t        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2) # (batch_size, 512) -> (batch_size, 256)\n", "        x = self.dp2(x)\n\t        x = self.linear3(x)                                             # (batch_size, 256) -> (batch_size, output_channels)\n\t        return x\n\tclass Transform_Net(nn.Module):\n\t    def __init__(self, args):\n\t        super(Transform_Net, self).__init__()\n\t        self.args = args\n\t        self.k = 3\n\t        self.bn1 = nn.BatchNorm2d(64)\n\t        self.bn2 = nn.BatchNorm2d(128)\n", "        self.bn3 = nn.BatchNorm1d(1024)\n\t        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n\t                                   self.bn1,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=1, bias=False),\n\t                                   self.bn2,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv3 = nn.Sequential(nn.Conv1d(128, 1024, kernel_size=1, bias=False),\n\t                                   self.bn3,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n", "        self.linear1 = nn.Linear(1024, 512, bias=False)\n\t        self.bn3 = nn.BatchNorm1d(512)\n\t        self.linear2 = nn.Linear(512, 256, bias=False)\n\t        self.bn4 = nn.BatchNorm1d(256)\n\t        self.transform = nn.Linear(256, 3*3)\n\t        init.constant_(self.transform.weight, 0)\n\t        init.eye_(self.transform.bias.view(3, 3))\n\t    def forward(self, x):\n\t        batch_size = x.size(0)\n\t        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n", "        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 128, num_points, k)\n\t        x = x.max(dim=-1, keepdim=False)[0]     # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n\t        x = self.conv3(x)                       # (batch_size, 128, num_points) -> (batch_size, 1024, num_points)\n\t        x = x.max(dim=-1, keepdim=False)[0]     # (batch_size, 1024, num_points) -> (batch_size, 1024)\n\t        x = F.leaky_relu(self.bn3(self.linear1(x)), negative_slope=0.2)     # (batch_size, 1024) -> (batch_size, 512)\n\t        x = F.leaky_relu(self.bn4(self.linear2(x)), negative_slope=0.2)     # (batch_size, 512) -> (batch_size, 256)\n\t        x = self.transform(x)                   # (batch_size, 256) -> (batch_size, 3*3)\n\t        x = x.view(batch_size, 3, 3)            # (batch_size, 3*3) -> (batch_size, 3, 3)\n\t        return x\n\tclass DGCNN_partseg(nn.Module):\n", "    def __init__(self, args, seg_num_all):\n\t        super(DGCNN_partseg, self).__init__()\n\t        self.args = args\n\t        self.seg_num_all = seg_num_all\n\t        self.k = args.k\n\t        self.transform_net = Transform_Net(args)\n\t        self.bn1 = nn.BatchNorm2d(64)\n\t        self.bn2 = nn.BatchNorm2d(64)\n\t        self.bn3 = nn.BatchNorm2d(64)\n\t        self.bn4 = nn.BatchNorm2d(64)\n", "        self.bn5 = nn.BatchNorm2d(64)\n\t        self.bn6 = nn.BatchNorm1d(args.emb_dims)\n\t        self.bn7 = nn.BatchNorm1d(64)\n\t        self.bn8 = nn.BatchNorm1d(256)\n\t        self.bn9 = nn.BatchNorm1d(256)\n\t        self.bn10 = nn.BatchNorm1d(128)\n\t        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n\t                                   self.bn1,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n", "                                   self.bn2,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n\t                                   self.bn3,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n\t                                   self.bn4,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n\t                                   self.bn5,\n", "                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv6 = nn.Sequential(nn.Conv1d(192, args.emb_dims, kernel_size=1, bias=False),\n\t                                   self.bn6,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv7 = nn.Sequential(nn.Conv1d(16, 64, kernel_size=1, bias=False),\n\t                                   self.bn7,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv8 = nn.Sequential(nn.Conv1d(1280, 256, kernel_size=1, bias=False),\n\t                                   self.bn8,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n", "        self.dp1 = nn.Dropout(p=args.dropout)\n\t        self.conv9 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=1, bias=False),\n\t                                   self.bn9,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.dp2 = nn.Dropout(p=args.dropout)\n\t        self.conv10 = nn.Sequential(nn.Conv1d(256, 128, kernel_size=1, bias=False),\n\t                                   self.bn10,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv11 = nn.Conv1d(128, self.seg_num_all, kernel_size=1, bias=False)\n\t    def forward(self, x, l):\n", "        batch_size = x.size(0)\n\t        num_points = x.size(2)\n\t        x0 = get_graph_feature(x, k=self.k)     # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n\t        t = self.transform_net(x0)              # (batch_size, 3, 3)\n\t        x = x.transpose(2, 1)                   # (batch_size, 3, num_points) -> (batch_size, num_points, 3)\n\t        x = torch.bmm(x, t)                     # (batch_size, num_points, 3) * (batch_size, 3, 3) -> (batch_size, num_points, 3)\n\t        x = x.transpose(2, 1)                   # (batch_size, num_points, 3) -> (batch_size, 3, num_points)\n\t        x = get_graph_feature(x, k=self.k)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n\t        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n", "        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n\t        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x = self.conv4(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n\t        x = self.conv5(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = torch.cat((x1, x2, x3), dim=1)      # (batch_size, 64*3, num_points)\n\t        x = self.conv6(x)                       # (batch_size, 64*3, num_points) -> (batch_size, emb_dims, num_points)\n", "        x = x.max(dim=-1, keepdim=True)[0]      # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims, 1)\n\t        l = l.view(batch_size, -1, 1)           # (batch_size, num_categoties, 1)\n\t        l = self.conv7(l)                       # (batch_size, num_categoties, 1) -> (batch_size, 64, 1)\n\t        x = torch.cat((x, l), dim=1)            # (batch_size, 1088, 1)\n\t        x = x.repeat(1, 1, num_points)          # (batch_size, 1088, num_points)\n\t        x = torch.cat((x, x1, x2, x3), dim=1)   # (batch_size, 1088+64*3, num_points)\n\t        x = self.conv8(x)                       # (batch_size, 1088+64*3, num_points) -> (batch_size, 256, num_points)\n\t        x = self.dp1(x)\n\t        x = self.conv9(x)                       # (batch_size, 256, num_points) -> (batch_size, 256, num_points)\n\t        x = self.dp2(x)\n", "        x = self.conv10(x)                      # (batch_size, 256, num_points) -> (batch_size, 128, num_points)\n\t        x = self.conv11(x)                      # (batch_size, 256, num_points) -> (batch_size, seg_num_all, num_points)\n\t        return x\n\tclass DGCNN_semseg(nn.Module):\n\t    def __init__(self, edgeconv_widths, mlp_widths, nfeat, k=20, return_edgeconvs=False):\n\t        super(DGCNN_semseg, self).__init__()\n\t        self.k = k\n\t        self.return_edgeconvs = return_edgeconvs\n\t        self.bn1 = nn.BatchNorm2d(64)\n\t        self.bn2 = nn.BatchNorm2d(64)\n", "        self.bn3 = nn.BatchNorm2d(64)\n\t        self.bn4 = nn.BatchNorm2d(64)\n\t        self.bn5 = nn.BatchNorm2d(64)\n\t        self.bn6 = nn.BatchNorm1d(256)\n\t        self.bn7 = nn.BatchNorm1d(512)\n\t        self.bn8 = nn.BatchNorm1d(256)\n\t        self.conv1 = nn.Sequential(nn.Conv2d(18, 64, kernel_size=1, bias=False),\n\t                                   self.bn1,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n", "                                   self.bn2,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n\t                                   self.bn3,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n\t                                   self.bn4,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n\t                                   self.bn5,\n", "                                   nn.LeakyReLU(negative_slope=0.2))\n\t        self.conv6 = nn.Sequential(nn.Conv1d(192, 256, kernel_size=1, bias=False),\n\t                                   self.bn6,\n\t                                   nn.LeakyReLU(negative_slope=0.2))\n\t    def forward(self, x):\n\t        batch_size = x.size(0)\n\t        num_points = x.size(2)\n\t        xyz = x[:, :3, :]\n\t        x = get_graph_feature(x, k=self.k, dim9=True)   # (batch_size, 9, num_points) -> (batch_size, 9*2, num_points, k)\n\t        x = self.conv1(x)                       # (batch_size, 9*2, num_points, k) -> (batch_size, 64, num_points, k)\n", "        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n\t        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x = self.conv4(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n\t        x = self.conv5(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n\t        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\t        x = torch.cat((x1, x2, x3), dim=1)      # (batch_size, 64*3, num_points)\n", "        out = self.conv6(x)                       # (batch_size, 64*3, num_points) -> (batch_size, emb_dims, num_points)\n\t        if self.return_edgeconvs:\n\t            return [x1, x2, x3], out, xyz.transpose(1, 2)\n\t        else:\n\t            return x1, out, xyz.transpose(1, 2)\n"]}
