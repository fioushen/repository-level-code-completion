{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(\n\t    name=\"protorl\",\n\t    version=\"0.1\",\n\t    author=\"Phil Tabor\",\n\t    author_email=\"phil@neuralnet.ai\",\n\t    url=\"https://www.github.com/philtabor/protorl\",\n\t    license=\"MIT\",\n\t    packages=find_packages(),\n\t    install_requires=[\n", "        \"numpy==1.21.*\",\n\t        \"torch==1.11.*\",\n\t        \"gym==0.26.*\",\n\t        \"gym[box2d]\",\n\t        \"atari-py==0.2.6\",\n\t        \"mpi4py\",\n\t        \"opencv-python\",\n\t        \"matplotlib\",\n\t        \"ale-py\"\n\t    ],\n", "    description=\"Torch based deep RL framework for rapid prototyping\",\n\t    python_requires=\">=3.8\",\n\t)\n"]}
{"filename": "protorl/agents/base.py", "chunked_list": ["import torch as T\n\tfrom protorl.utils.common import convert_arrays_to_tensors\n\tclass Agent:\n\t    def __init__(self, memory, policy, gamma=0.99, tau=0.001):\n\t        self.memory = memory\n\t        self.policy = policy\n\t        self.gamma = gamma\n\t        self.tau = tau\n\t        self.networks = []\n\t        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n", "    def store_transition(self, *args):\n\t        self.memory.store_transition(*args)\n\t    def sample_memory(self, mode='uniform'):\n\t        memory_batch = self.memory.sample_buffer(mode=mode)\n\t        memory_tensors = convert_arrays_to_tensors(memory_batch, self.device)\n\t        return memory_tensors\n\t    def save_models(self):\n\t        for network in self.networks:\n\t            for x in network:\n\t                x.save_checkpoint()\n", "    def load_models(self):\n\t        for network in self.networks:\n\t            for x in network:\n\t                x.load_checkpoint()\n\t    def update_network_parameters(self, src, dest, tau=None):\n\t        if tau is None:\n\t            tau = self.tau\n\t        for param, target in zip(src.parameters(), dest.parameters()):\n\t            target.data.copy_(tau * param.data + (1 - tau) * target.data)\n\t    def update(self):\n", "        raise(NotImplementedError)\n\t    def choose_action(self, observation):\n\t        raise(NotImplementedError)\n"]}
{"filename": "protorl/agents/dueling.py", "chunked_list": ["from protorl.agents.base import Agent\n\timport numpy as np\n\timport torch as T\n\tclass DuelingDQNAgent(Agent):\n\t    def __init__(self, online_net, target_net, memory, policy,\n\t                 use_double=False, gamma=0.99, lr=1e-4, replace=1000):\n\t        super().__init__(memory, policy, gamma)\n\t        self.replace_target_cnt = replace\n\t        self.learn_step_counter = 0\n\t        self.use_double = use_double\n", "        self.q_eval = online_net\n\t        self.q_next = target_net\n\t        self.networks = [net for net in [self.q_eval, self.q_next]]\n\t        self.optimizer = T.optim.Adam(self.q_eval.parameters(), lr=lr)\n\t        self.loss = T.nn.MSELoss()\n\t    def choose_action(self, observation):\n\t        state = T.tensor(observation, dtype=T.float, device=self.device)\n\t        _, advantage = self.q_eval(state)\n\t        action = self.policy(advantage)\n\t        return action\n", "    def replace_target_network(self):\n\t        if self.learn_step_counter % self.replace_target_cnt == 0:\n\t            self.update_network_parameters(self.q_eval, self.q_next, tau=1.0)\n\t    def update(self):\n\t        if not self.memory.ready():\n\t            return\n\t        self.optimizer.zero_grad()\n\t        self.replace_target_network()\n\t        states, actions, rewards, states_, dones = self.sample_memory()\n\t        indices = np.arange(len(states))\n", "        V_s, A_s = self.q_eval(states)\n\t        V_s_, A_s_ = self.q_next(states_)\n\t        q_pred = T.add(V_s,\n\t                       (A_s - A_s.mean(dim=1,\n\t                                       keepdim=True)))[indices, actions]\n\t        q_next = T.add(V_s_, (A_s_ - A_s_.mean(dim=1, keepdim=True)))\n\t        q_next[dones] = 0.0\n\t        if self.use_double:\n\t            V_s_eval, A_s_eval = self.q_eval(states_)\n\t            q_eval = T.add(V_s_eval,\n", "                           (A_s_eval - A_s_eval.mean(dim=1, keepdim=True)))\n\t            max_actions = T.argmax(q_eval, dim=1)\n\t            q_next = q_next[indices, max_actions]\n\t        else:\n\t            q_next = q_next.max(dim=1)[0]\n\t        q_target = rewards + self.gamma * q_next\n\t        loss = self.loss(q_target, q_pred).to(self.device)\n\t        loss.backward()\n\t        self.optimizer.step()\n\t        self.learn_step_counter += 1\n"]}
{"filename": "protorl/agents/td3.py", "chunked_list": ["from protorl.agents.base import Agent\n\timport torch as T\n\timport torch.nn.functional as F\n\tclass TD3Agent(Agent):\n\t    def __init__(self, actor_network, critic_network_1, critic_network_2,\n\t                 target_actor_network, target_critic_1_network,\n\t                 target_critic_2_network, memory, policy, tau=0.005,\n\t                 gamma=0.99, critic_lr=1e-3, actor_lr=1e-4, warmup=1000,\n\t                 actor_update_interval=1):\n\t        super().__init__(memory, policy, gamma, tau)\n", "        self.warmup = warmup\n\t        self.actor_update_interval = actor_update_interval\n\t        self.learn_step_counter = 0\n\t        self.actor = actor_network\n\t        self.critic_1 = critic_network_1\n\t        self.critic_2 = critic_network_2\n\t        self.target_actor = target_actor_network\n\t        self.target_critic_1 = target_critic_1_network\n\t        self.target_critic_2 = target_critic_2_network\n\t        self.networks = [net for net in [self.actor, self.critic_1,\n", "                                         self.critic_2,\n\t                                         self.target_actor,\n\t                                         self.target_critic_1,\n\t                                         self.target_critic_2]]\n\t        self.actor_optimizer = T.optim.Adam(self.actor.parameters(),\n\t                                            lr=actor_lr)\n\t        self.critic_1_optimizer = T.optim.Adam(self.critic_1.parameters(),\n\t                                               lr=critic_lr)\n\t        self.critic_2_optimizer = T.optim.Adam(self.critic_2.parameters(),\n\t                                               lr=critic_lr)\n", "        self.update_network_parameters(self.actor, self.target_actor, tau=1.0)\n\t        self.update_network_parameters(self.critic_1,\n\t                                       self.target_critic_1, tau=1.0)\n\t        self.update_network_parameters(self.critic_2,\n\t                                       self.target_critic_2, tau=1.0)\n\t    def choose_action(self, observation):\n\t        state = T.tensor(observation, dtype=T.float, device=self.device)\n\t        mu = self.actor(state)\n\t        if self.learn_step_counter < self.warmup:\n\t            mu = T.zeros(size=mu.shape)\n", "        mu_prime = self.policy(mu)\n\t        return mu_prime.cpu().detach().numpy()\n\t    def update(self):\n\t        if not self.memory.ready():\n\t            return\n\t        states, actions, rewards, states_, dones = self.sample_memory()\n\t        target_mu = self.target_actor(states_)\n\t        target_actions = self.policy(target_mu, scale=0.2,\n\t                                     noise_bounds=[-0.5, 0.5])\n\t        q1_ = self.target_critic_1([states_, target_actions]).squeeze()\n", "        q2_ = self.target_critic_2([states_, target_actions]).squeeze()\n\t        q1 = self.critic_1([states, actions]).squeeze()\n\t        q2 = self.critic_2([states, actions]).squeeze()\n\t        q1_[dones] = 0.0\n\t        q2_[dones] = 0.0\n\t        critic_value_ = T.min(q1_, q2_)\n\t        target = rewards + self.gamma * critic_value_\n\t        target = target.squeeze()\n\t        self.critic_1_optimizer.zero_grad()\n\t        self.critic_2_optimizer.zero_grad()\n", "        q1_loss = F.mse_loss(target, q1)\n\t        q2_loss = F.mse_loss(target, q2)\n\t        critic_loss = q1_loss + q2_loss\n\t        critic_loss.backward()\n\t        self.critic_1_optimizer.step()\n\t        self.critic_2_optimizer.step()\n\t        self.learn_step_counter += 1\n\t        if self.learn_step_counter % self.actor_update_interval != 0:\n\t            return\n\t        self.actor_optimizer.zero_grad()\n", "        actor_q1_loss = self.critic_1([states, self.actor(states)]).squeeze()\n\t        actor_loss = -T.mean(actor_q1_loss)\n\t        actor_loss.backward()\n\t        self.actor_optimizer.step()\n\t        self.update_network_parameters(self.actor, self.target_actor)\n\t        self.update_network_parameters(self.critic_1, self.target_critic_1)\n\t        self.update_network_parameters(self.critic_2, self.target_critic_2)\n"]}
{"filename": "protorl/agents/ddpg.py", "chunked_list": ["from protorl.agents.base import Agent\n\timport torch as T\n\timport torch.nn.functional as F\n\tclass DDPGAgent(Agent):\n\t    def __init__(self, actor_network, critic_network, target_actor_network,\n\t                 target_critic_network, memory, policy,\n\t                 gamma=0.99, actor_lr=1e-4, critic_lr=1e-3, tau=0.001):\n\t        super().__init__(memory, policy, gamma, tau)\n\t        self.actor = actor_network\n\t        self.critic = critic_network\n", "        self.target_actor = target_actor_network\n\t        self.target_critic = target_critic_network\n\t        self.networks = [net for net in [self.actor, self.critic,\n\t                                         self.target_actor, self.target_critic,\n\t                                         ]]\n\t        self.actor_optimizer = T.optim.Adam(self.actor.parameters(),\n\t                                            lr=actor_lr)\n\t        self.critic_optimizer = T.optim.Adam(self.critic.parameters(),\n\t                                             lr=critic_lr)\n\t        self.update_network_parameters(self.actor, self.target_actor, tau=1.0)\n", "        self.update_network_parameters(self.critic,\n\t                                       self.target_critic, tau=1.0)\n\t    def choose_action(self, observation):\n\t        state = T.tensor(observation, dtype=T.float, device=self.device)\n\t        mu = self.actor(state)\n\t        actions = self.policy(mu)\n\t        return actions.cpu().detach().numpy()\n\t    def update(self):\n\t        if not self.memory.ready():\n\t            return\n", "        states, actions, rewards, states_, dones = self.sample_memory()\n\t        target_actions = self.target_actor(states_)\n\t        critic_value_ = self.target_critic([states_, target_actions]).view(-1)\n\t        critic_value = self.critic([states, actions]).view(-1)\n\t        critic_value_[dones] = 0.0\n\t        target = rewards + self.gamma * critic_value_\n\t        self.critic_optimizer.zero_grad()\n\t        critic_loss = F.mse_loss(target, critic_value)\n\t        critic_loss.backward()\n\t        self.critic_optimizer.step()\n", "        self.actor_optimizer.zero_grad()\n\t        actor_loss = -self.critic([states, self.actor(states)])\n\t        actor_loss = T.mean(actor_loss)\n\t        actor_loss.backward()\n\t        self.actor_optimizer.step()\n\t        self.update_network_parameters(self.actor, self.target_actor)\n\t        self.update_network_parameters(self.critic, self.target_critic)\n"]}
{"filename": "protorl/agents/__init__.py", "chunked_list": []}
{"filename": "protorl/agents/dqn.py", "chunked_list": ["from protorl.agents.base import Agent\n\timport numpy as np\n\timport torch as T\n\tclass DQNAgent(Agent):\n\t    def __init__(self, eval_net, target_net, memory, policy, use_double=False,\n\t                 gamma=0.99, lr=1e-4, replace=1000, prioritized=False):\n\t        super().__init__(memory, policy, gamma)\n\t        self.replace_target_cnt = replace\n\t        self.learn_step_counter = 0\n\t        self.use_double = use_double\n", "        self.prioritized = prioritized\n\t        self.q_eval = eval_net\n\t        self.q_next = target_net\n\t        self.networks = [net for net in [self.q_eval, self.q_next]]\n\t        self.optimizer = T.optim.Adam(self.q_eval.parameters(), lr=lr)\n\t        self.loss = T.nn.MSELoss()\n\t    def choose_action(self, observation):\n\t        state = T.tensor(observation, dtype=T.float).to(self.device)\n\t        q_values = self.q_eval(state)\n\t        action = self.policy(q_values)\n", "        return action\n\t    def replace_target_network(self):\n\t        if self.learn_step_counter % self.replace_target_cnt == 0:\n\t            self.update_network_parameters(self.q_eval, self.q_next, tau=1.0)\n\t    def update(self):\n\t        if not self.memory.ready():\n\t            return\n\t        self.optimizer.zero_grad()\n\t        self.replace_target_network()\n\t        if self.prioritized:\n", "            sample_idx, states, actions, rewards, states_, dones, weights =\\\n\t                    self.sample_memory(mode='prioritized')\n\t        else:\n\t            states, actions, rewards, states_, dones = self.sample_memory()\n\t        indices = np.arange(len(states))\n\t        q_pred = self.q_eval.forward(states)[indices, actions]\n\t        q_next = self.q_next(states_)\n\t        q_next[dones] = 0.0\n\t        if self.use_double:\n\t            q_eval = self.q_eval(states_)\n", "            max_actions = T.argmax(q_eval, dim=1)\n\t            q_next = q_next[indices, max_actions]\n\t        else:\n\t            q_next = q_next.max(dim=1)[0]\n\t        q_target = rewards + self.gamma * q_next\n\t        if self.prioritized:\n\t            td_error = np.abs((q_target.detach().cpu().numpy() -\n\t                               q_pred.detach().cpu().numpy()))\n\t            td_error = np.clip(td_error, 0., 1.)\n\t            self.memory.sum_tree.update_priorities(sample_idx, td_error)\n", "            q_target *= weights\n\t            q_pred *= weights\n\t        loss = self.loss(q_target, q_pred).to(self.device)\n\t        loss.backward()\n\t        self.optimizer.step()\n\t        self.learn_step_counter += 1\n"]}
{"filename": "protorl/agents/sac.py", "chunked_list": ["from protorl.agents.base import Agent\n\timport torch as T\n\timport torch.nn.functional as F\n\tclass SACAgent(Agent):\n\t    def __init__(self, actor_network, critic_network_1, critic_network_2,\n\t                 value_network, target_value_network, memory, policy,\n\t                 reward_scale=2, gamma=0.99, actor_lr=3e-4, critic_lr=3e-4,\n\t                 value_lr=3e-4, tau=0.005):\n\t        super().__init__(memory, policy, gamma, tau)\n\t        self.reward_scale = reward_scale\n", "        self.actor = actor_network\n\t        self.critic_1 = critic_network_1\n\t        self.critic_2 = critic_network_2\n\t        self.value = value_network\n\t        self.target_value = target_value_network\n\t        self.networks = [net for net in [self.actor, self.critic_1,\n\t                                         self.critic_2, self.value,\n\t                                         self.target_value]]\n\t        self.actor_optimizer = T.optim.Adam(self.actor.parameters(),\n\t                                            lr=actor_lr)\n", "        self.critic_1_optimizer = T.optim.Adam(self.critic_1.parameters(),\n\t                                               lr=critic_lr)\n\t        self.critic_2_optimizer = T.optim.Adam(self.critic_2.parameters(),\n\t                                               lr=critic_lr)\n\t        self.value_optimizer = T.optim.Adam(self.value.parameters(),\n\t                                            lr=value_lr)\n\t        self.update_network_parameters(self.value, self.target_value, tau=1.0)\n\t    def choose_action(self, observation):\n\t        state = T.tensor(observation, dtype=T.float).to(self.device)\n\t        mu, sigma = self.actor(state)\n", "        actions, _ = self.policy(mu, sigma)\n\t        return actions.cpu().detach().numpy()\n\t    def update(self):\n\t        if not self.memory.ready():\n\t            return\n\t        states, actions, rewards, states_, dones = self.sample_memory()\n\t        value = self.value(states).view(-1)\n\t        value_ = self.target_value(states_).view(-1)\n\t        value_[dones] = 0.0\n\t        # CALCULATE VALUE LOSS #\n", "        mu, sigma = self.actor(states)\n\t        new_actions, log_probs = self.policy(mu, sigma, False)\n\t        log_probs -= T.log(1 - new_actions.pow(2) + 1e-6)\n\t        log_probs = log_probs.sum(1, keepdim=True)\n\t        log_probs = log_probs.view(-1)\n\t        q1_new_policy = self.critic_1([states, new_actions])\n\t        q2_new_policy = self.critic_2([states, new_actions])\n\t        critic_value = T.min(q1_new_policy, q2_new_policy)\n\t        critic_value = critic_value.view(-1)\n\t        self.value_optimizer.zero_grad()\n", "        value_target = critic_value - log_probs\n\t        value_loss = 0.5 * (F.mse_loss(value, value_target))\n\t        value_loss.backward(retain_graph=True)\n\t        self.value_optimizer.step()\n\t        # CACULATE ACTOR LOSS #\n\t        mu, sigma = self.actor(states)\n\t        new_actions, log_probs = self.policy(mu, sigma, True)\n\t        log_probs -= T.log(1 - new_actions.pow(2) + 1e-6)\n\t        log_probs = log_probs.sum(1, keepdim=True)\n\t        log_probs = log_probs.view(-1)\n", "        q1_new_policy = self.critic_1([states, new_actions])\n\t        q2_new_policy = self.critic_2([states, new_actions])\n\t        critic_value = T.min(q1_new_policy, q2_new_policy)\n\t        critic_value = critic_value.view(-1)\n\t        actor_loss = log_probs - critic_value\n\t        actor_loss = T.mean(actor_loss)\n\t        self.actor_optimizer.zero_grad()\n\t        actor_loss.backward(retain_graph=True)\n\t        self.actor_optimizer.step()\n\t        # CALCULATE CRITIC LOSS #\n", "        self.critic_1_optimizer.zero_grad()\n\t        self.critic_2_optimizer.zero_grad()\n\t        q_hat = self.reward_scale * rewards + self.gamma * value_\n\t        q1_old_policy = self.critic_1([states, actions]).view(-1)\n\t        q2_old_policy = self.critic_2([states, actions]).view(-1)\n\t        critic_1_loss = 0.5 * F.mse_loss(q1_old_policy, q_hat)\n\t        critic_2_loss = 0.5 * F.mse_loss(q2_old_policy, q_hat)\n\t        critic_loss = critic_1_loss + critic_2_loss\n\t        critic_loss.backward()\n\t        self.critic_1_optimizer.step()\n", "        self.critic_2_optimizer.step()\n\t        self.update_network_parameters(self.value, self.target_value)\n"]}
{"filename": "protorl/agents/ppo.py", "chunked_list": ["import torch as T\n\tfrom protorl.agents.base import Agent\n\tfrom protorl.utils.common import convert_arrays_to_tensors\n\tfrom protorl.utils.common import calc_adv_and_returns\n\tclass PPOAgent(Agent):\n\t    def __init__(self, actor_net, critic_net, action_type, memory, policy, N,\n\t                 gamma=0.99, lr=1E-4, gae_lambda=0.95, entropy_coeff=0,\n\t                 policy_clip=0.2, n_epochs=10):\n\t        super().__init__(memory, policy, gamma)\n\t        self.policy_clip = policy_clip\n", "        self.n_epochs = n_epochs\n\t        self.gae_lambda = gae_lambda\n\t        self.T = N\n\t        self.step_counter = 0\n\t        self.entropy_coefficient = entropy_coeff\n\t        self.action_type = action_type\n\t        self.policy_clip_start = policy_clip\n\t        self.actor = actor_net\n\t        self.critic = critic_net\n\t        self.networks = [net for net in [self.actor, self.critic]]\n", "        self.actor_optimizer = T.optim.Adam(self.actor.parameters(), lr=lr)\n\t        self.critic_optimizer = T.optim.Adam(self.critic.parameters(), lr=lr)\n\t    def choose_action(self, observation):\n\t        state = T.tensor(observation, dtype=T.float, device=self.device)\n\t        with T.no_grad():\n\t            if self.action_type == 'continuous':\n\t                alpha, beta = self.actor(state)\n\t                action, log_probs = self.policy(alpha, beta)\n\t            elif self.action_type == 'discrete':\n\t                probs = self.actor(state)\n", "                action, log_probs = self.policy(probs)\n\t        self.step_counter += 1\n\t        return action.cpu().numpy(), log_probs.cpu().numpy()\n\t    def update(self, n_steps):\n\t        if self.step_counter % self.T != 0:\n\t            return\n\t        s, a, r, s_, d, lp = self.memory.sample_buffer(mode='all')\n\t        s, s_, r = convert_arrays_to_tensors([s, s_, r], device=self.device)\n\t        with T.no_grad():\n\t            values = self.critic(s).squeeze()\n", "            values_ = self.critic(s_).squeeze()\n\t        adv, returns = calc_adv_and_returns(values, values_, r, d)\n\t        for epoch in range(self.n_epochs):\n\t            batches = self.memory.sample_buffer(mode='batch')\n\t            for batch in batches:\n\t                indices, states, actions, rewards, states_, dones, old_probs =\\\n\t                    convert_arrays_to_tensors(batch, device=self.device)\n\t                if self.action_type == 'continuous':\n\t                    alpha, beta = self.actor(states)\n\t                    _, new_probs, entropy = self.policy(alpha, beta,\n", "                                                        old_action=actions,\n\t                                                        with_entropy=True)\n\t                    last_dim = int(len(new_probs.shape) - 1)\n\t                    prob_ratio = T.exp(\n\t                        new_probs.sum(last_dim, keepdims=True) -\n\t                        old_probs.sum(last_dim, keepdims=True))\n\t                    # a = adv[indices]\n\t                    entropy = entropy.sum(last_dim, keepdims=True)\n\t                elif self.action_type == 'discrete':\n\t                    probs = self.actor(states)\n", "                    _, new_probs, entropy = self.policy(probs,\n\t                                                        old_action=actions,\n\t                                                        with_entropy=True)\n\t                    prob_ratio = T.exp(new_probs - old_probs)\n\t                a = adv[indices].view(prob_ratio.shape)\n\t                weighted_probs = a * prob_ratio\n\t                weighted_clipped_probs = T.clamp(\n\t                        prob_ratio, 1-self.policy_clip, 1+self.policy_clip) * a\n\t                actor_loss = -T.min(weighted_probs,\n\t                                    weighted_clipped_probs)\n", "                actor_loss -= self.entropy_coefficient * entropy\n\t                self.actor_optimizer.zero_grad()\n\t                actor_loss.mean().backward()\n\t                T.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n\t                self.actor_optimizer.step()\n\t                critic_value = self.critic(states).squeeze()\n\t                critic_loss = (critic_value - returns[indices].squeeze()).\\\n\t                    pow(2).mean()\n\t                self.critic_optimizer.zero_grad()\n\t                critic_loss.backward()\n", "                self.critic_optimizer.step()\n\t        self.step_counter = 0\n\t    def anneal_policy_clip(self, n_ep, max_ep):\n\t        self.policy_clip = self.policy_clip_start * (1 - n_ep / max_ep)\n"]}
{"filename": "protorl/loops/ppo_episode.py", "chunked_list": ["import numpy as np\n\tfrom protorl.utils.common import action_adapter, clip_reward\n\tclass EpisodeLoop:\n\t    def __init__(self, agent, env, n_threads=1, adapt_actions=False,\n\t                 load_checkpoint=False, clip_reward=False,\n\t                 extra_functionality=None, seed=None):\n\t        self.agent = agent\n\t        self.seed = seed\n\t        self.env = env\n\t        self.load_checkpoint = load_checkpoint\n", "        self.clip_reward = clip_reward\n\t        self.n_threads = n_threads\n\t        self.adapt_actions = adapt_actions\n\t        self.functions = extra_functionality or []\n\t    def run(self, n_episodes=1):\n\t        if self.load_checkpoint:\n\t            self.agent.load_models()\n\t        n_steps = 0\n\t        best_score = self.env.reward_range[0]\n\t        scores, steps = [], []\n", "        if self.adapt_actions:\n\t            max_a = self.env.action_space.high[0]\n\t        for i in range(n_episodes):\n\t            done = [False] * self.n_threads\n\t            trunc = [False] * self.n_threads\n\t            if self.seed is not None:\n\t                observation, info = self.env.reset(self.seed)\n\t                self.seed = None\n\t            else:\n\t                observation, info = self.env.reset()\n", "            score = [0] * self.n_threads\n\t            while not (any(done) or any(trunc)):\n\t                action, log_prob = self.agent.choose_action(observation)\n\t                if self.adapt_actions:\n\t                    act = action_adapter(action, max_a).reshape(\n\t                        action.shape)\n\t                else:\n\t                    act = action\n\t                observation_, reward, done, trunc, info = self.env.step(act)\n\t                score += reward\n", "                r = clip_reward(reward) if self.clip_reward else reward\n\t                mask = [0.0 if d or t else 1.0 for d, t in zip(done, trunc)]\n\t                if not self.load_checkpoint:\n\t                    self.agent.store_transition([observation, action,\n\t                                                r, observation_, mask,\n\t                                                log_prob])\n\t                    self.agent.update(n_steps)\n\t                observation = observation_\n\t                n_steps += 1\n\t            # score = np.mean(score)\n", "            scores.append(np.mean(score))\n\t            steps.append(n_steps)\n\t            avg_score = np.mean(scores[-100:])\n\t            print('episode {} average score {:.1f} n steps {}'.\n\t                  format(i+1, avg_score,  n_steps))\n\t            if avg_score > best_score:\n\t                if not self.load_checkpoint:\n\t                    self.agent.save_models()\n\t                best_score = avg_score\n\t            # self.handle_extra_functionality(i, n_episodes)\n", "        self.env.close()\n\t        return scores, steps\n\t    def handle_extra_functionality(self, *args):\n\t        for func in self.functions:\n\t            func(*args)\n"]}
{"filename": "protorl/loops/__init__.py", "chunked_list": []}
{"filename": "protorl/loops/single.py", "chunked_list": ["import numpy as np\n\tfrom protorl.utils.common import clip_reward\n\tclass EpisodeLoop:\n\t    def __init__(self, agent, env,\n\t                 load_checkpoint=False, clip_reward=False):\n\t        self.agent = agent\n\t        self.env = env\n\t        self.load_checkpoint = load_checkpoint\n\t        self.clip_reward = clip_reward\n\t        self.functions = []\n", "    def run(self, n_episodes=1):\n\t        if self.load_checkpoint:\n\t            self.agent.load_models()\n\t        n_steps = 0\n\t        best_score = self.env.reward_range[0]\n\t        scores, steps = [], []\n\t        for i in range(n_episodes):\n\t            done, trunc = False, False\n\t            observation, info = self.env.reset()\n\t            score = 0\n", "            while not (done or trunc):\n\t                action = self.agent.choose_action(observation)\n\t                observation_, reward, done, trunc, info = self.env.step(action)\n\t                score += reward\n\t                r = clip_reward(reward) if self.clip_reward else reward\n\t                if not self.load_checkpoint:\n\t                    ep_end = done or trunc\n\t                    self.agent.store_transition([observation, action,\n\t                                                r, observation_, ep_end])\n\t                    self.agent.update()\n", "                observation = observation_\n\t                n_steps += 1\n\t            score = np.mean(score)\n\t            scores.append(score)\n\t            steps.append(n_steps)\n\t            avg_score = np.mean(scores[-100:])\n\t            print('episode {} ep score {:.1f} average score {:.1f} n steps {}'.\n\t                  format(i, score, avg_score,  n_steps))\n\t            if avg_score > best_score:\n\t                if not self.load_checkpoint:\n", "                    self.agent.save_models()\n\t                best_score = avg_score\n\t            self.handle_extra_functionality()\n\t        return scores, steps\n\t    def handle_extra_functionality(self):\n\t        for func in self.functions:\n\t            continue\n"]}
{"filename": "protorl/loops/episode.py", "chunked_list": ["import numpy as np\n\tfrom protorl.utils.common import clip_reward\n\tclass EpisodeLoop:\n\t    def __init__(self, agent, env, n_threads=1, seed=None,\n\t                 load_checkpoint=False, clip_reward=False):\n\t        self.agent = agent\n\t        self.env = env\n\t        self.load_checkpoint = load_checkpoint\n\t        self.clip_reward = clip_reward\n\t        self.n_threads = n_threads\n", "        self.seed = seed\n\t        self.functions = []\n\t    def run(self, n_episodes=1):\n\t        if self.load_checkpoint:\n\t            self.agent.load_models()\n\t        n_steps = 0\n\t        best_score = self.env.reward_range[0]\n\t        scores, steps = [], []\n\t        for i in range(n_episodes):\n\t            done = [False] * self.n_threads\n", "            trunc = [False] * self.n_threads\n\t            if self.seed is not None:\n\t                observation, info = self.env.reset(self.seed)\n\t                self.seed = None\n\t            else:\n\t                observation, info = self.env.reset()\n\t            score = [0] * self.n_threads\n\t            while not (any(done) or any(trunc)):\n\t                action = self.agent.choose_action(observation)\n\t                observation_, reward, done, trunc, info = self.env.step(action)\n", "                score += reward\n\t                r = clip_reward(reward) if self.clip_reward else reward\n\t                if not self.load_checkpoint:\n\t                    ep_end = [d or t for d, t in zip(done, trunc)]\n\t                    self.agent.store_transition([observation, action,\n\t                                                r, observation_, ep_end])\n\t                    self.agent.update()\n\t                observation = observation_\n\t                n_steps += 1\n\t            score = np.mean(score)\n", "            scores.append(score)\n\t            steps.append(n_steps)\n\t            avg_score = np.mean(scores[-100:])\n\t            print('episode {} ep score {:.1f} average score {:.1f} n steps {}'.\n\t                  format(i, score, avg_score,  n_steps))\n\t            if avg_score > best_score:\n\t                if not self.load_checkpoint:\n\t                    self.agent.save_models()\n\t                best_score = avg_score\n\t            self.handle_extra_functionality()\n", "        return scores, steps\n\t    def handle_extra_functionality(self):\n\t        for func in self.functions:\n\t            continue\n"]}
{"filename": "protorl/utils/__init__.py", "chunked_list": []}
{"filename": "protorl/utils/network_utils.py", "chunked_list": ["import math\n\timport torch.nn as nn\n\tfrom torch.nn import Sequential\n\tfrom protorl.networks.base import AtariBase, CriticBase\n\tfrom protorl.networks.base import LinearBase, LinearTanhBase\n\tfrom protorl.networks.head import DuelingHead, QHead\n\tfrom protorl.networks.head import BetaHead, SoftmaxHead\n\tfrom protorl.networks.head import DeterministicHead, MeanAndSigmaHead\n\tfrom protorl.networks.head import ValueHead\n\tfrom protorl.utils.common import calculate_conv_output_dims\n", "def make_dqn_networks(env, use_double=False,\n\t                      use_atari=False, use_dueling=False):\n\t    algo = 'dueling_' if use_dueling else ''\n\t    algo += 'ddqn' if use_double else 'dqn'\n\t    head_fn = DuelingHead if use_dueling else QHead\n\t    base_fn = AtariBase if use_atari else LinearBase\n\t    input_dims = calculate_conv_output_dims() if use_atari else [256]\n\t    base = base_fn(name=algo+'_q_base',\n\t                   input_dims=env.observation_space.shape)\n\t    target_base = base_fn(name='target_'+algo+'_q_base',\n", "                          input_dims=env.observation_space.shape)\n\t    head = head_fn(n_actions=env.action_space.n,\n\t                   input_dims=input_dims,\n\t                   name=algo+'_q_head')\n\t    target_head = head_fn(n_actions=env.action_space.n,\n\t                          input_dims=input_dims,\n\t                          name='target_'+algo+'_q_head')\n\t    actor = Sequential(base, head)\n\t    target_actor = Sequential(target_base, target_head)\n\t    return actor, target_actor\n", "def make_ddpg_networks(env):\n\t    actor_base = LinearBase(name='ddpg_actor_base',\n\t                            input_dims=env.observation_space.shape)\n\t    actor_head = DeterministicHead(n_actions=env.action_space.shape[0],\n\t                                   name='ddpg_actor_head')\n\t    actor = Sequential(actor_base, actor_head)\n\t    target_actor_base = LinearBase(name='ddpg_target_actor_base',\n\t                                   input_dims=env.observation_space.shape)\n\t    target_actor_head = DeterministicHead(n_actions=env.action_space.shape[0],\n\t                                          name='ddpg_target_actor_head')\n", "    target_actor = Sequential(target_actor_base, target_actor_head)\n\t    input_dims = [env.observation_space.shape[0] + env.action_space.shape[0]]\n\t    critic_base = CriticBase(name='ddpg_critic_base',\n\t                             input_dims=input_dims)\n\t    critic_head = ValueHead(name='ddpg_critic_head')\n\t    critic = Sequential(critic_base, critic_head)\n\t    target_critic_base = CriticBase(name='ddpg_target_critic_base',\n\t                                    input_dims=input_dims)\n\t    target_critic_head = ValueHead(name='ddpg_target_critic_head')\n\t    target_critic = Sequential(target_critic_base, target_critic_head)\n", "    return actor, critic, target_actor, target_critic\n\tdef make_td3_networks(env):\n\t    actor_base = LinearBase(name='td3_actor_base', hidden_dims=[400, 300],\n\t                            input_dims=env.observation_space.shape)\n\t    actor_head = DeterministicHead(n_actions=env.action_space.shape[0],\n\t                                   input_dims=[300],\n\t                                   name='td3_actor_head')\n\t    actor = Sequential(actor_base, actor_head)\n\t    target_actor_base = LinearBase(name='td3_target_actor_base',\n\t                                   hidden_dims=[400, 300],\n", "                                   input_dims=env.observation_space.shape)\n\t    target_actor_head = DeterministicHead(n_actions=env.action_space.shape[0],\n\t                                          input_dims=[300],\n\t                                          name='td3_target_actor_head')\n\t    target_actor = Sequential(target_actor_base, target_actor_head)\n\t    input_dims = [env.observation_space.shape[0] + env.action_space.shape[0]]\n\t    critic_base_1 = CriticBase(name='td3_critic_1_base',\n\t                               hidden_dims=[400, 300],\n\t                               input_dims=input_dims)\n\t    critic_head_1 = ValueHead(name='td3_critic_1_head', input_dims=[300])\n", "    critic_1 = Sequential(critic_base_1, critic_head_1)\n\t    critic_base_2 = CriticBase(name='td3_critic_2_base',\n\t                               hidden_dims=[400, 300],\n\t                               input_dims=input_dims)\n\t    critic_head_2 = ValueHead(name='td3_critic_2_head', input_dims=[300])\n\t    critic_2 = Sequential(critic_base_2, critic_head_2)\n\t    target_critic_1_base = CriticBase(name='td3_target_critic_1_base',\n\t                                      input_dims=input_dims,\n\t                                      hidden_dims=[400, 300])\n\t    target_critic_1_head = ValueHead(name='td3_target_critic_1_head',\n", "                                     input_dims=[300])\n\t    target_critic_1 = Sequential(target_critic_1_base, target_critic_1_head)\n\t    target_critic_2_base = CriticBase(name='td3_target_critic_2_base',\n\t                                      input_dims=input_dims,\n\t                                      hidden_dims=[400, 300])\n\t    target_critic_2_head = ValueHead(name='td3_target_critic_2_head',\n\t                                     input_dims=[300])\n\t    target_critic_2 = Sequential(target_critic_2_base, target_critic_2_head)\n\t    return actor, critic_1, critic_2, target_actor,\\\n\t        target_critic_1, target_critic_2\n", "def make_sac_networks(env):\n\t    actor_base = LinearBase(name='sac_actor_base',\n\t                            input_dims=env.observation_space.shape)\n\t    actor_head = MeanAndSigmaHead(n_actions=env.action_space.shape[0],\n\t                                  name='sac_actor_head')\n\t    actor = Sequential(actor_base, actor_head)\n\t    input_dims = [env.observation_space.shape[0] + env.action_space.shape[0]]\n\t    critic_base_1 = CriticBase(name='sac_critic_1_base',\n\t                               input_dims=input_dims)\n\t    critic_head_1 = ValueHead(name='sac_critic_1_head')\n", "    critic_1 = Sequential(critic_base_1, critic_head_1)\n\t    critic_base_2 = CriticBase(name='sac_critic_2_base',\n\t                               input_dims=input_dims)\n\t    critic_head_2 = ValueHead(name='sac_critic_2_head')\n\t    critic_2 = Sequential(critic_base_2, critic_head_2)\n\t    value_base = LinearBase(name='sac_value_base',\n\t                            input_dims=env.observation_space.shape)\n\t    value_head = ValueHead(name='sac_value_head')\n\t    value = Sequential(value_base, value_head)\n\t    target_value_base = LinearBase(name='sac_target_value_base',\n", "                                   input_dims=env.observation_space.shape)\n\t    target_value_head = ValueHead(name='sac_target_value_head')\n\t    target_value = Sequential(target_value_base, target_value_head)\n\t    return actor, critic_1, critic_2, value, target_value\n\tdef make_ppo_networks(env, action_space='discrete'):\n\t    actor_base = LinearTanhBase(name='ppo_actor_base',\n\t                                input_dims=env.observation_space.shape,\n\t                                hidden_dims=[128, 128])\n\t    critic_base = LinearTanhBase(name='ppo_critic_base',\n\t                                 hidden_dims=[128, 128],\n", "                                 input_dims=env.observation_space.shape)\n\t    critic_head = ValueHead(name='ppo_critic_head', input_dims=[128])\n\t    if action_space == 'discrete':\n\t        actor_head = SoftmaxHead(n_actions=env.action_space.n,\n\t                                 name='ppo_actor_head', input_dims=[128])\n\t    elif action_space == 'continuous':\n\t        actor_head = BetaHead(name='ppo_actor_head', input_dims=[128],\n\t                              n_actions=env.action_space.shape[0])\n\t    actor = Sequential(actor_base, actor_head)\n\t    critic = Sequential(critic_base, critic_head)\n", "    for m in actor.modules():\n\t        if isinstance(m, nn.Linear):\n\t            stdv = 1. / math.sqrt(m.weight.size(1))\n\t            nn.init.uniform_(m.weight, -stdv, stdv)\n\t            if m.bias is not None:\n\t                m.bias.data.uniform_(-stdv, stdv)\n\t    for m in critic.modules():\n\t        if isinstance(m, nn.Linear):\n\t            stdv = 1. / math.sqrt(m.weight.size(1))\n\t            nn.init.uniform_(m.weight, -stdv, stdv)\n", "            if m.bias is not None:\n\t                m.bias.data.uniform_(-stdv, stdv)\n\t    return actor, critic\n"]}
{"filename": "protorl/utils/common.py", "chunked_list": ["import numpy as np\n\timport torch as T\n\timport matplotlib.pyplot as plt\n\tdef plot_learning_curve(x, scores, figure_file):\n\t    running_avg = np.zeros(len(scores))\n\t    for i in range(len(running_avg)):\n\t        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n\t    plt.plot(x, running_avg)\n\t    plt.title('Running average of previous 100 scores')\n\t    plt.savefig(figure_file)\n", "def calculate_conv_output_dims(channels=(32, 64, 64),\n\t                               kernels=(8, 4, 3),\n\t                               input_dims=(4, 84, 84)):\n\t    # assume input_dims is [channels, height, width]\n\t    # assume channels is (x, y, z)\n\t    # assume kernels is (a, b, c)\n\t    _, h, w = input_dims\n\t    ch1, ch2, ch3 = channels\n\t    k1, k2, k3 = kernels\n\t    output_dims = ch3 * w // (k2 * k3) * w // (k2 * k3)\n", "    return [output_dims]\n\tdef convert_arrays_to_tensors(array, device):\n\t    tensors = []\n\t    for arr in array:\n\t        tensors.append(T.tensor(arr).to(device))\n\t    return tensors\n\tdef action_adapter(a, max_a):\n\t    return 2 * (a - 0.5) * max_a\n\tdef clip_reward(x):\n\t    if type(x) is np.ndarray:\n", "        rewards = []\n\t        for r in x:\n\t            if r < -1:\n\t                rewards.append(-1)\n\t            elif r > 1:\n\t                rewards.append(1)\n\t            else:\n\t                rewards.append(r)\n\t        return np.array(rewards)\n\t    else:\n", "        if r > 1:\n\t            return 1\n\t        elif r < -1:\n\t            return -1\n\t        else:\n\t            return r\n\tdef calc_adv_and_returns(values, values_, rewards, dones,\n\t                         gamma=0.99, gae_lambda=0.95):\n\t    # TODO - multi gpu support\n\t    # TODO - support for different vector shapes\n", "    device = 'cuda:0' if T.cuda.is_available() else 'cpu'\n\t    deltas = rewards + gamma * values_ - values\n\t    deltas = deltas.cpu().numpy()\n\t    adv = [0]\n\t    for step in reversed(range(deltas.shape[0])):\n\t        advantage = deltas[step] +\\\n\t            gamma * gae_lambda * adv[-1] * dones[step]\n\t        adv.append(advantage)\n\t    adv.reverse()\n\t    adv = adv[:-1]\n", "    adv = T.tensor(adv, dtype=T.double,\n\t                   device=device, requires_grad=False).unsqueeze(2)\n\t    returns = adv + values.unsqueeze(2)\n\t    adv = (adv - adv.mean()) / (adv.std() + 1e-4)\n\t    return adv, returns\n"]}
{"filename": "protorl/networks/base.py", "chunked_list": ["import torch as T\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom protorl.networks.core import NetworkCore\n\tclass CriticBase(NetworkCore, nn.Module):\n\t    def __init__(self, name, input_dims, hidden_dims=[256, 256],\n\t                 chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.fc1 = nn.Linear(*input_dims, hidden_dims[0])\n\t        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n", "        self.to(self.device)\n\t    def forward(self, sa):\n\t        state, action = sa\n\t        x = T.cat([state, action], dim=1)\n\t        x = F.relu(self.fc1(x))\n\t        x = F.relu(self.fc2(x))\n\t        return x\n\tclass LinearBase(NetworkCore, nn.Module):\n\t    def __init__(self, name, input_dims, hidden_dims=[256, 256],\n\t                 chkpt_dir='models'):\n", "        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.fc1 = nn.Linear(*input_dims, hidden_dims[0])\n\t        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n\t        self.to(self.device)\n\t    def forward(self, state):\n\t        f1 = F.relu(self.fc1(state))\n\t        f2 = F.relu(self.fc2(f1))\n\t        return f2\n\tclass AtariBase(NetworkCore, nn.Module):\n\t    def __init__(self, name, input_dims, channels=(32, 64, 64),\n", "                 kernels=(8, 4, 3), strides=(4, 2, 1),\n\t                 chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        assert len(channels) == 3, \"Must supply 3 channels for AtariBase\"\n\t        assert len(kernels) == 3, \"Must supply 3 kernels for AtariBase\"\n\t        assert len(strides) == 3, \"Must supply 3 strides for AtariBase\"\n\t        self.input_dims = input_dims\n\t        self.channels = channels\n\t        self.kernels = kernels\n\t        self.strides = strides\n", "        self.conv1 = nn.Conv2d(input_dims[0], channels[0],\n\t                               kernels[0], strides[0])\n\t        self.conv2 = nn.Conv2d(channels[0], channels[1],\n\t                               kernels[1], strides[1])\n\t        self.conv3 = nn.Conv2d(channels[1], channels[2],\n\t                               kernels[2], strides[2])\n\t        self.flat = nn.Flatten()\n\t        self.to(self.device)\n\t    def forward(self, state):\n\t        conv1 = F.relu(self.conv1(state))\n", "        conv2 = F.relu(self.conv2(conv1))\n\t        conv3 = F.relu(self.conv3(conv2))\n\t        conv_state = self.flat(conv3)\n\t        return conv_state\n\tclass LinearTanhBase(NetworkCore, nn.Module):\n\t    def __init__(self, name, input_dims, hidden_dims=[256, 256],\n\t                 chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.fc1 = nn.Linear(*input_dims, hidden_dims[0])\n\t        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n", "        self.to(self.device)\n\t    def forward(self, state):\n\t        f1 = T.tanh(self.fc1(state))\n\t        f2 = T.tanh(self.fc2(f1))\n\t        return f2\n\tclass PPOCritic(NetworkCore, nn.Module):\n\t    def __init__(self, name, input_dims, hidden_dims=[128, 128],\n\t                 chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.fc1 = nn.Linear(*input_dims, hidden_dims[0])\n", "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n\t        self.v = nn.Linear(hidden_dims[1], 1)\n\t        self.to(self.device)\n\t    def forward(self, state):\n\t        f1 = T.tanh(self.fc1(state))\n\t        f2 = T.tanh(self.fc2(f1))\n\t        v = self.v(f2)\n\t        return v\n\tclass PPOActor(NetworkCore, nn.Module):\n\t    def __init__(self, name, input_dims, n_actions, hidden_dims=[128, 128],\n", "                 chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.fc1 = nn.Linear(*input_dims, hidden_dims[0])\n\t        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n\t        self.alpha = nn.Linear(hidden_dims[1], n_actions)\n\t        self.beta = nn.Linear(hidden_dims[1], n_actions)\n\t        self.to(self.device)\n\t    def forward(self, state):\n\t        f1 = T.tanh(self.fc1(state))\n\t        f2 = T.tanh(self.fc2(f1))\n", "        alpha = F.relu(self.alpha(f2)) + 1.0\n\t        beta = F.relu(self.beta(f2)) + 1.0\n\t        return alpha, beta\n"]}
{"filename": "protorl/networks/head.py", "chunked_list": ["import torch as T\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom protorl.networks.core import NetworkCore\n\tclass QHead(NetworkCore, nn.Module):\n\t    def __init__(self, name,  n_actions,\n\t                 input_dims=[256], hidden_layers=[512], chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        assert len(hidden_layers) == 1, \"Must supply 1 hidden layer size\"\n\t        self.fc1 = nn.Linear(*input_dims, hidden_layers[0])\n", "        self.fc2 = nn.Linear(hidden_layers[0], n_actions)\n\t        self.to(self.device)\n\t    def forward(self, x):\n\t        f1 = F.relu(self.fc1(x))\n\t        q_values = self.fc2(f1)\n\t        return q_values\n\tclass DuelingHead(NetworkCore, nn.Module):\n\t    def __init__(self, name, n_actions,\n\t                 input_dims=[256], hidden_layers=[512], chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n", "        assert len(hidden_layers) == 1, \"Must supply 1 hidden layer size\"\n\t        self.fc1 = nn.Linear(*input_dims, hidden_layers[0])\n\t        self.V = nn.Linear(hidden_layers[0], 1)\n\t        self.A = nn.Linear(hidden_layers[0], n_actions)\n\t        self.to(self.device)\n\t    def forward(self, x):\n\t        f1 = F.relu(self.fc1(x))\n\t        V = self.V(f1)\n\t        A = self.A(f1)\n\t        return V, A\n", "class DeterministicHead(NetworkCore, nn.Module):\n\t    def __init__(self, name, n_actions, chkpt_dir='models', input_dims=[256]):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.fc1 = nn.Linear(*input_dims, n_actions)\n\t        self.to(self.device)\n\t    def forward(self, x):\n\t        mu = T.tanh(self.fc1(x))\n\t        return mu\n\tclass MeanAndSigmaHead(NetworkCore, nn.Module):\n\t    def __init__(self, name, n_actions,\n", "                 input_dims=[256], chkpt_dir='models', std_min=1e-6):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.std_min = std_min\n\t        self.mu = nn.Linear(*input_dims, n_actions)\n\t        self.sigma = nn.Linear(*input_dims, n_actions)\n\t        self.to(self.device)\n\t    def forward(self, x):\n\t        mu = self.mu(x)\n\t        sigma = self.sigma(x)\n\t        sigma = T.clamp(sigma, min=self.std_min, max=1)\n", "        return mu, sigma\n\tclass ValueHead(NetworkCore, nn.Module):\n\t    def __init__(self, name, input_dims=[256], chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.v = nn.Linear(*input_dims, 1)\n\t        self.to(self.device)\n\t    def forward(self, x):\n\t        value = self.v(x)\n\t        return value\n\tclass SoftmaxHead(NetworkCore, nn.Module):\n", "    def __init__(self, name, n_actions,\n\t                 input_dims=[256], chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.probs = nn.Linear(*input_dims, n_actions)\n\t        self.to(self.device)\n\t    def forward(self, x):\n\t        probs = F.softmax(self.probs(x), dim=1)\n\t        return probs\n\tclass BetaHead(NetworkCore, nn.Module):\n\t    def __init__(self, name, n_actions,\n", "                 input_dims=[256], chkpt_dir='models'):\n\t        super().__init__(name=name, chkpt_dir=chkpt_dir)\n\t        self.alpha = nn.Linear(*input_dims, n_actions)\n\t        self.beta = nn.Linear(*input_dims, n_actions)\n\t        self.to(self.device)\n\t    def forward(self, state):\n\t        alpha = F.relu(self.alpha(state)) + 1.0\n\t        beta = F.relu(self.beta(state)) + 1.0\n\t        return alpha, beta\n"]}
{"filename": "protorl/networks/__init__.py", "chunked_list": []}
{"filename": "protorl/networks/core.py", "chunked_list": ["import os\n\timport torch as T\n\tclass NetworkCore:\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__()\n\t        self.checkpoint_dir = kwargs['chkpt_dir']\n\t        self.checkpoint_file = os.path.join(self.checkpoint_dir,\n\t                                            kwargs['name'])\n\t        self.name = kwargs['name']\n\t        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n", "    def save_checkpoint(self):\n\t        T.save(self.state_dict(), self.checkpoint_file)\n\t    def load_checkpoint(self):\n\t        self.load_state_dict(T.load(self.checkpoint_file))\n"]}
{"filename": "protorl/networks/factory.py", "chunked_list": ["from protorl.core import NetworkCore\n\timport torch.nn as nn\n\tclass NetworkFactory(NetworkCore, nn.Module):\n\t    def __init__(self, input_dims, hidden_layer_dims, activations,\n\t                 name='network', chkpt_dir='models/'):\n\t        super(NetworkFactory, self).__init__()\n\t        c_dim = input_dims\n\t        self.activations = activations\n\t        self.layers = nn.ModuleList()\n\t        for dim in hidden_layer_dims:\n", "            self.layers.append(nn.Linear(*c_dim, dim))\n\t            c_dim = [dim]\n\t    def forward(self, x):\n\t        for idx, layer in enumerate(self.layers[:-1]):\n\t            x = self.activations[idx](layer(x))\n\t        out = self.activations[-1](self.layers[-1](x))\n\t        return out\n"]}
{"filename": "protorl/policies/gaussian.py", "chunked_list": ["import torch as T\n\tclass GaussianPolicy:\n\t    def __init__(self, min_action=-1, max_action=1):\n\t        self.min_action = min_action\n\t        self.max_action = max_action\n\t    def __call__(self, mu, sigma,\n\t                 reparam=False, with_entropy=False, old_action=None):\n\t        probs = T.distributions.Normal(mu, sigma)\n\t        actions = probs.rsample() if reparam else probs.sample()\n\t        if old_action is None:\n", "            a = actions\n\t        else:\n\t            a = old_action\n\t        log_probs = probs.log_prob(a)\n\t        actions = T.tanh(actions)*T.tensor(self.max_action).to(actions.device)\n\t        if with_entropy:\n\t            entropy = probs.entropy()\n\t            return actions, log_probs, entropy\n\t        return actions, log_probs\n"]}
{"filename": "protorl/policies/discrete.py", "chunked_list": ["import torch as T\n\tclass DiscretePolicy:\n\t    def __call__(self, probs, with_entropy=False, old_action=None):\n\t        dist = T.distributions.Categorical(probs)\n\t        actions = old_action if old_action is not None else dist.sample()\n\t        log_probs = dist.log_prob(actions)\n\t        if with_entropy:\n\t            entropy = dist.entropy()\n\t            return actions, log_probs, entropy\n\t        return actions, log_probs\n"]}
{"filename": "protorl/policies/noisy_deterministic.py", "chunked_list": ["import numpy as np\n\timport torch as T\n\tclass NoisyDeterministicPolicy:\n\t    def __init__(self, n_actions, noise=0.1, min_action=-1, max_action=1):\n\t        self.noise = noise\n\t        self.min_action = min_action\n\t        self.max_action = max_action\n\t    def __call__(self, mu, scale=None, noise_bounds=None):\n\t        scale = scale or self.noise\n\t        mu = mu.detach()\n", "        # Some environments have max action outside the range of +/- 1 that we\n\t        # get from the tanh activation function\n\t        mu *= T.abs(T.tensor(self.max_action))\n\t        noise = T.tensor(np.random.normal(scale=scale), dtype=T.float)\n\t        if noise_bounds:\n\t            noise = T.clamp(noise, noise_bounds[0], noise_bounds[1])\n\t        mu = mu + noise\n\t        mu = T.clamp(mu, self.min_action, self.max_action)\n\t        return mu\n"]}
{"filename": "protorl/policies/__init__.py", "chunked_list": []}
{"filename": "protorl/policies/epsilon_greedy.py", "chunked_list": ["import numpy as np\n\timport torch as T\n\tclass EpsilonGreedyPolicy:\n\t    def __init__(self, n_actions, eps_start=1.0,\n\t                 eps_dec=1e-5, eps_min=0.01, n_threads=1):\n\t        self.epsilon = eps_start\n\t        self.eps_dec = eps_dec\n\t        self.eps_min = eps_min\n\t        self.action_space = [[i for i in range(n_actions)]\n\t                             for x in range(n_threads)]\n", "    def decrement_epsilon(self):\n\t        self.epsilon = self.epsilon - self.eps_dec \\\n\t                           if self.epsilon > self.eps_min else self.eps_min\n\t    def __call__(self, q_values):\n\t        if np.random.random() > self.epsilon:\n\t            action = T.argmax(q_values, dim=-1).cpu().detach().numpy()\n\t        else:\n\t            action = np.array([np.random.choice(a) for a in self.action_space])\n\t        self.decrement_epsilon()\n\t        return action\n"]}
{"filename": "protorl/policies/beta.py", "chunked_list": ["import torch as T\n\tclass BetaPolicy:\n\t    def __init__(self, min_action=-1, max_action=1):\n\t        self.min_action = min_action\n\t        self.max_action = max_action\n\t    def __call__(self, alpha, beta, with_entropy=False, old_action=None):\n\t        probs = T.distributions.Beta(alpha, beta)\n\t        actions = probs.sample()\n\t        if old_action is None:\n\t            a = actions\n", "        else:\n\t            a = old_action\n\t        log_probs = probs.log_prob(a)\n\t        if with_entropy:\n\t            entropy = probs.entropy()\n\t            return actions, log_probs, entropy\n\t        return actions, log_probs\n"]}
{"filename": "protorl/wrappers/vec_env.py", "chunked_list": ["from mpi4py import MPI\n\timport multiprocessing as mp\n\timport gym\n\timport numpy as np\n\timport torch as T\n\tfrom protorl.wrappers.atari import RepeatActionAndMaxFrame\n\tfrom protorl.wrappers.atari import PreprocessFrame, StackFrames\n\tdef worker(remote, parent_remote, env_fn_wrapper):\n\t    parent_remote.close()\n\t    env = env_fn_wrapper.x()\n", "    while True:\n\t        cmd, data = remote.recv()\n\t        if cmd == 'step':\n\t            ob, reward, done, trunc, info = env.step(data)\n\t            if done:\n\t                ob, info = env.reset()\n\t            remote.send((ob, reward, done, trunc, info))\n\t        elif cmd == 'reset':\n\t            ob, info = env.reset()\n\t            remote.send([ob, info])\n", "        elif cmd == 'close':\n\t            remote.close()\n\t            break\n\t        elif cmd == 'get_spaces':\n\t            remote.send((env.observation_space, env.action_space,\n\t                        env.reward_range))\n\t        else:\n\t            raise NotImplementedError\n\tclass SubprocVecEnv:\n\t    def __init__(self, env_fns, spaces=None):\n", "        self.closed = False\n\t        nenvs = len(env_fns)\n\t        mp.set_start_method('forkserver')\n\t        self.remotes, self.work_remotes = zip(*[mp.Pipe()\n\t                                                for _ in range(nenvs)])\n\t        self.ps = [mp.Process(target=worker, args=(work_remote, remote,\n\t                              CloudpickleWrapper(env_fn)))\n\t                   for (work_remote, remote, env_fn) in\n\t                   zip(self.work_remotes, self.remotes, env_fns)]\n\t        for p in self.ps:\n", "            p.daemon = True\n\t            p.start()\n\t        for remote in self.work_remotes:\n\t            remote.close()\n\t        self.remotes[0].send(('get_spaces', None))\n\t        observation_space, action_space, reward_range = self.remotes[0].recv()\n\t        self.observation_space = observation_space\n\t        self.action_space = action_space\n\t        self.reward_range = reward_range\n\t    def step_async(self, actions):\n", "        assert not self.closed, \"trying to operate after calling close()\"\n\t        for remote, action in zip(self.remotes, actions):\n\t            remote.send(('step', action))\n\t        results = [remote.recv() for remote in self.remotes]\n\t        obs, rews, dones, trunc, infos = zip(*results)\n\t        return np.stack(obs), np.stack(rews), np.stack(dones),\\\n\t            np.stack(trunc), infos\n\t    def reset(self):\n\t        assert not self.closed, \"trying to operate after calling close()\"\n\t        for remote in self.remotes:\n", "            remote.send(('reset', None))\n\t        obs, info = [], []\n\t        for remote in self.remotes:\n\t            o, i = remote.recv()\n\t            obs.append(o)\n\t            info.append(i)\n\t        obs = np.array(obs)\n\t        info = np.array(info)\n\t        return obs, info\n\t    def close_extras(self):\n", "        if self.closed:\n\t            return\n\t        for remote in self.remotes:\n\t            remote.send(('close', None))\n\t        for p in self.ps:\n\t            p.join()\n\t        self.closed = True\n\t    def close(self):\n\t        if self.closed:\n\t            return\n", "        self.close_extras()\n\t        self.closed = True\n\t    def step(self, actions):\n\t        obs, reward, dones, trunc, info = self.step_async(actions)\n\t        return obs, reward, dones, trunc, info\n\t    def __del__(self):\n\t        if not self.closed:\n\t            self.close()\n\tclass CloudpickleWrapper:\n\t    def __init__(self, x):\n", "        self.x = x\n\t    def __getstate__(self):\n\t        import cloudpickle\n\t        return cloudpickle.dumps(self.x)\n\t    def __setstate__(self, ob):\n\t        import pickle\n\t        self.x = pickle.loads(ob)\n\tdef make_single_env(env_id, rank, use_atari, repeat=4,\n\t                    clip_rewards=False, no_ops=0, fire_first=False,\n\t                    shape=(84, 84, 1)):\n", "    def _thunk():\n\t        env = gym.make(env_id)\n\t        if use_atari:\n\t            env = RepeatActionAndMaxFrame(env, repeat, clip_rewards,\n\t                                          no_ops, fire_first)\n\t            env = PreprocessFrame(shape, env)\n\t            env = StackFrames(env, repeat)\n\t        return env\n\t    return _thunk\n\tdef make_vec_envs(env_name, use_atari=False, seed=None, n_threads=2):\n", "    mpi_rank = MPI.COMM_WORLD.Get_rank() if MPI else 0\n\t    seed = seed + 10000 * mpi_rank if seed is not None else None\n\t    set_global_seeds(seed)\n\t    envs = [make_single_env(env_name, i, use_atari)\n\t            for i in range(n_threads)]\n\t    envs = SubprocVecEnv(envs, seed)\n\t    return envs\n\tdef set_global_seeds(seed):\n\t    if seed is not None:\n\t        import random\n", "        np.random.seed(seed)\n\t        random.seed(seed)\n\t        T.manual_seed(seed)\n"]}
{"filename": "protorl/wrappers/__init__.py", "chunked_list": []}
{"filename": "protorl/wrappers/single_threaded.py", "chunked_list": ["import numpy as np\n\timport gym\n\tclass SingleThreadedWrapper(gym.Wrapper):\n\t    def __init__(self, env):\n\t        self.env = env\n\t    def step(self, action):\n\t        try:\n\t            obs, reward, done, trunc, info = self.env.step(action.item())\n\t        # we may not have an extra dimension around the action\n\t        except ValueError:\n", "            obs, reward, done, trunc, info = self.env.step(action)\n\t        return obs, reward, done, trunc, info\n\tclass BatchDimensionWrapper(gym.ObservationWrapper):\n\t    def observation(self, observation):\n\t        observation = observation[np.newaxis, :]\n\t        return observation\n"]}
{"filename": "protorl/wrappers/common.py", "chunked_list": ["import gym\n\tfrom protorl.wrappers.atari import PreprocessFrame\n\tfrom protorl.wrappers.atari import RepeatActionAndMaxFrame, StackFrames\n\tfrom protorl.wrappers.single_threaded import SingleThreadedWrapper\n\tfrom protorl.wrappers.single_threaded import BatchDimensionWrapper\n\tdef make_env(env_name, use_atari=False, shape=(84, 84, 1),\n\t             repeat=4, clip_rewards=False, no_ops=0,\n\t             fire_first=False, n_threads=1):\n\t    env = gym.make(env_name)\n\t    if use_atari:\n", "        env = RepeatActionAndMaxFrame(env, repeat, clip_rewards,\n\t                                      no_ops, fire_first)\n\t        env = PreprocessFrame(shape, env)\n\t        env = StackFrames(env, repeat)\n\t        if n_threads == 1:\n\t            env = BatchDimensionWrapper(env)\n\t            env = SingleThreadedWrapper(env)\n\t            return env\n\t    if n_threads == 1:\n\t        env = SingleThreadedWrapper(env)\n", "    return env\n"]}
{"filename": "protorl/wrappers/atari.py", "chunked_list": ["import collections\n\timport cv2\n\timport numpy as np\n\timport gym\n\tclass RepeatActionAndMaxFrame(gym.Wrapper):\n\t    def __init__(self, env=None, repeat=4, clip_reward=False, no_ops=0,\n\t                 fire_first=False):\n\t        super(RepeatActionAndMaxFrame, self).__init__(env)\n\t        self.repeat = repeat\n\t        self.shape = env.observation_space.low.shape\n", "        self.frame_buffer = np.zeros(shape=(2, *self.shape), dtype=np.float16)\n\t        self.clip_reward = clip_reward\n\t        self.no_ops = no_ops\n\t        self.fire_first = fire_first\n\t    def step(self, action):\n\t        t_reward = 0.0\n\t        done = False\n\t        for i in range(self.repeat):\n\t            obs, reward, done, trunc, info = self.env.step(action)\n\t            if self.clip_reward:\n", "                reward = np.clip(np.array([reward]), -1, 1)[0]\n\t            t_reward += reward\n\t            idx = i % 2\n\t            self.frame_buffer[idx] = obs\n\t            if done:\n\t                break\n\t        max_frame = np.maximum(self.frame_buffer[0], self.frame_buffer[1])\n\t        return max_frame, t_reward, done, trunc, info\n\t    def reset(self):\n\t        obs, info = self.env.reset()\n", "        no_ops = np.random.randint(self.no_ops)+1 if self.no_ops > 0 else 0\n\t        for _ in range(no_ops):\n\t            _, _, done, _ = self.env.step(0)\n\t            if done:\n\t                self.env.reset()\n\t        if self.fire_first:\n\t            assert self.env.unwrapped.get_action_meanings()[1] == 'FIRE'\n\t            obs, _, _, _ = self.env.step(1)\n\t        self.frame_buffer = np.zeros(shape=(2, *self.shape), dtype=np.float16)\n\t        self.frame_buffer[0] = obs\n", "        return obs, info\n\tclass PreprocessFrame(gym.ObservationWrapper):\n\t    def __init__(self, shape, env=None):\n\t        super(PreprocessFrame, self).__init__(env)\n\t        self.shape = (shape[2], shape[0], shape[1])\n\t        self.observation_space = gym.spaces.Box(low=0.0, high=1.0,\n\t                                                shape=self.shape,\n\t                                                dtype=np.float16)\n\t    def observation(self, obs):\n\t        new_frame = cv2.cvtColor(obs.astype(np.uint8), cv2.COLOR_RGB2GRAY)\n", "        resized_screen = cv2.resize(new_frame, self.shape[1:],\n\t                                    interpolation=cv2.INTER_AREA)\n\t        new_obs = np.array(resized_screen, dtype=np.uint8).reshape(self.shape)\n\t        new_obs = new_obs / 255.0\n\t        return new_obs.astype(np.float16)\n\tclass StackFrames(gym.ObservationWrapper):\n\t    def __init__(self, env, repeat):\n\t        super(StackFrames, self).__init__(env)\n\t        self.observation_space = gym.spaces.Box(\n\t                            env.observation_space.low.repeat(repeat, axis=0),\n", "                            env.observation_space.high.repeat(repeat, axis=0),\n\t                            dtype=np.float16)\n\t        self.stack = collections.deque(maxlen=repeat)\n\t    def reset(self):\n\t        self.stack.clear()\n\t        observation, info = self.env.reset()\n\t        for _ in range(self.stack.maxlen):\n\t            self.stack.append(observation)\n\t        return np.array(self.stack, dtype=np.float16).reshape(\n\t            self.observation_space.low.shape), info\n", "    def observation(self, observation):\n\t        self.stack.append(observation)\n\t        return np.array(self.stack).reshape(self.observation_space.low.shape)\n\t\"\"\"\n\tdef make_env(env_name, shape=(84, 84, 1), repeat=4, clip_rewards=False,\n\t             no_ops=0, fire_first=False, n_threads=1):\n\t    env = gym.make(env_name)\n\t    env = RepeatActionAndMaxFrame(env, repeat, clip_rewards,\n\t                                  no_ops, fire_first)\n\t    env = PreprocessFrame(shape, env)\n", "    env = StackFrames(env, repeat)\n\t    return env\n\t\"\"\"\n"]}
{"filename": "protorl/examples/dueling_ddqn.py", "chunked_list": ["from protorl.agents.dueling import DuelingDQNAgent as Agent\n\tfrom protorl.loops.single import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_dqn_networks\n\tfrom protorl.policies.epsilon_greedy import EpsilonGreedyPolicy\n\tfrom protorl.wrappers.common import make_env\n\tdef main():\n\t    # env_name = 'PongNoFrameskip-v4'\n\t    env_name = 'CartPole-v1'\n\t    use_double = True\n", "    use_atari = False\n\t    use_prioritization = True\n\t    use_dueling = True\n\t    env = make_env(env_name,  use_atari=use_atari)\n\t    n_games = 1500\n\t    bs = 256\n\t    q_online, q_target = make_dqn_networks(env, use_dueling=use_dueling,\n\t                                           use_double=use_double,\n\t                                           use_atari=use_atari)\n\t    memory = initialize_memory(max_size=100_000,\n", "                               obs_shape=env.observation_space.shape,\n\t                               batch_size=bs,\n\t                               n_actions=env.action_space.n,\n\t                               action_space='discrete',\n\t                               prioritized=use_prioritization\n\t                               )\n\t    policy = EpsilonGreedyPolicy(n_actions=env.action_space.n, eps_dec=1e-4)\n\t    agent = Agent(q_online, q_target, memory, policy, use_double=True)\n\t    ep_loop = EpisodeLoop(agent, env)\n\t    scores, steps_array = ep_loop.run(n_games)\n", "if __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/ddqn.py", "chunked_list": ["from protorl.agents.dqn import DQNAgent as Agent\n\tfrom protorl.loops.single import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_dqn_networks\n\tfrom protorl.policies.epsilon_greedy import EpsilonGreedyPolicy\n\tfrom protorl.wrappers.common import make_env\n\tdef main():\n\t    env_name = 'CartPole-v1'\n\t    env = make_env(env_name)\n\t    n_games = 1500\n", "    bs = 256\n\t    q_online, q_target = make_dqn_networks(env, use_double=True)\n\t    memory = initialize_memory(max_size=100_000,\n\t                               obs_shape=env.observation_space.shape,\n\t                               batch_size=bs,\n\t                               n_actions=env.action_space.n,\n\t                               action_space='discrete',\n\t                               )\n\t    policy = EpsilonGreedyPolicy(n_actions=env.action_space.n, eps_dec=1e-4)\n\t    agent = Agent(q_online, q_target, memory, policy, use_double=True)\n", "    ep_loop = EpisodeLoop(agent, env)\n\t    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/td3.py", "chunked_list": ["from protorl.agents.td3 import TD3Agent as Agent\n\tfrom protorl.loops.single import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_td3_networks\n\tfrom protorl.policies.noisy_deterministic import NoisyDeterministicPolicy\n\tfrom protorl.wrappers.common import make_env\n\tdef main():\n\t    env_name = 'LunarLanderContinuous-v2'\n\t    n_games = 1500\n\t    bs = 100\n", "    env = make_env(env_name)\n\t    actor, critic_1, critic_2,\\\n\t        target_actor, target_critic_1, target_critic_2 = make_td3_networks(env)\n\t    memory = initialize_memory(max_size=1_000_000,\n\t                               obs_shape=env.observation_space.shape,\n\t                               batch_size=bs,\n\t                               n_actions=env.action_space.shape[0],\n\t                               action_space='continuous',\n\t                               )\n\t    policy = NoisyDeterministicPolicy(n_actions=env.action_space.shape[0],\n", "                                      min_action=env.action_space.low[0],\n\t                                      max_action=env.action_space.high[0])\n\t    agent = Agent(actor, critic_1, critic_2, target_actor,\n\t                  target_critic_1, target_critic_2, memory, policy)\n\t    ep_loop = EpisodeLoop(agent, env)\n\t    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/ddpg.py", "chunked_list": ["from protorl.agents.ddpg import DDPGAgent as Agent\n\tfrom protorl.loops.single import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_ddpg_networks\n\tfrom protorl.policies.noisy_deterministic import NoisyDeterministicPolicy\n\tfrom protorl.wrappers.common import make_env\n\tdef main():\n\t    env_name = 'LunarLanderContinuous-v2'\n\t    n_games = 1500\n\t    bs = 64\n", "    env = make_env(env_name)\n\t    actor, critic, target_actor, target_critic = make_ddpg_networks(env)\n\t    memory = initialize_memory(max_size=100_000,\n\t                               obs_shape=env.observation_space.shape,\n\t                               batch_size=bs,\n\t                               n_actions=env.action_space.shape[0],\n\t                               action_space='continuous',\n\t                               )\n\t    policy = NoisyDeterministicPolicy(n_actions=env.action_space.shape[0],\n\t                                      min_action=env.action_space.low[0],\n", "                                      max_action=env.action_space.high[0])\n\t    agent = Agent(actor, critic, target_actor, target_critic, memory, policy)\n\t    ep_loop = EpisodeLoop(agent, env)\n\t    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/__init__.py", "chunked_list": []}
{"filename": "protorl/examples/ppo_continuous.py", "chunked_list": ["import numpy as np\n\tfrom protorl.agents.ppo import PPOAgent as Agent\n\tfrom protorl.loops.ppo_episode import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_ppo_networks\n\tfrom protorl.policies.beta import BetaPolicy\n\tfrom protorl.wrappers.vec_env import make_vec_envs\n\tdef main():\n\t    env_name = 'LunarLanderContinuous-v2'\n\t    n_games = 4000\n", "    bs = 64\n\t    n_threads = 8\n\t    n_epochs = 10\n\t    horizon = 16384\n\t    T = int(horizon // n_threads)\n\t    env = make_vec_envs(env_name, n_threads=n_threads, seed=0)\n\t    actor, critic = make_ppo_networks(env, action_space='continuous')\n\t    fields = ['states', 'actions', 'rewards', 'states_',\n\t              'mask', 'log_probs']\n\t    state_shape = (T, n_threads, *env.observation_space.shape)\n", "    action_shape = probs_shape = (T, n_threads, env.action_space.shape[0])\n\t    reward_shape = mask_shape = (T, n_threads)\n\t    vals = [np.zeros(state_shape, dtype=np.float32),\n\t            np.zeros(action_shape, dtype=np.float32),\n\t            np.zeros(reward_shape, dtype=np.float32),\n\t            np.zeros(state_shape, dtype=np.float32),\n\t            np.zeros(mask_shape, dtype=np.float32),\n\t            np.zeros(probs_shape, dtype=np.float32)]\n\t    memory = initialize_memory(max_size=T,\n\t                               obs_shape=env.observation_space.shape,\n", "                               batch_size=bs,\n\t                               n_actions=env.action_space.shape[0],\n\t                               action_space='continuous',\n\t                               n_threads=n_threads,\n\t                               fields=fields,\n\t                               vals=vals\n\t                               )\n\t    policy = BetaPolicy(min_action=env.action_space.low[0],\n\t                        max_action=env.action_space.high[0])\n\t    agent = Agent(actor, critic, memory=memory, policy=policy,\n", "                  action_type='continuous', N=T, n_epochs=n_epochs)\n\t    ep_loop = EpisodeLoop(agent, env, n_threads=n_threads, clip_reward=True,\n\t                          extra_functionality=[agent.anneal_policy_clip],\n\t                          adapt_actions=True)\n\t    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/dqn.py", "chunked_list": ["from protorl.agents.dqn import DQNAgent as Agent\n\tfrom protorl.loops.single import EpisodeLoop\n\tfrom protorl.policies.epsilon_greedy import EpsilonGreedyPolicy\n\tfrom protorl.utils.network_utils import make_dqn_networks\n\tfrom protorl.wrappers.common import make_env\n\tfrom protorl.memory.generic import initialize_memory\n\tdef main():\n\t    # env_name = 'CartPole-v1'\n\t    env_name = 'PongNoFrameskip-v4'\n\t    use_prioritization = True\n", "    use_double = False\n\t    use_dueling = False\n\t    use_atari = True\n\t    # use_atari = False\n\t    env = make_env(env_name, use_atari=use_atari)\n\t    n_games = 1500\n\t    bs = 64\n\t    q_eval, q_target = make_dqn_networks(env, use_double=use_double,\n\t                                         use_dueling=use_dueling,\n\t                                         use_atari=use_atari)\n", "    memory = initialize_memory(max_size=100_000,\n\t                               obs_shape=env.observation_space.shape,\n\t                               batch_size=bs,\n\t                               n_actions=env.action_space.n,\n\t                               action_space='discrete',\n\t                               prioritized=use_prioritization\n\t                               )\n\t    policy = EpsilonGreedyPolicy(n_actions=env.action_space.n, eps_dec=1e-4)\n\t    agent = Agent(q_eval, q_target, memory, policy, use_double=use_double,\n\t                  prioritized=use_prioritization)\n", "    ep_loop = EpisodeLoop(agent, env)\n\t    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/sac.py", "chunked_list": ["import gym\n\tfrom protorl.agents.sac import SACAgent as Agent\n\tfrom protorl.loops.single import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_sac_networks\n\tfrom protorl.policies.gaussian import GaussianPolicy\n\tdef main():\n\t    env_name = 'LunarLanderContinuous-v2'\n\t    env = gym.make(env_name)\n\t    n_games = 1500\n", "    bs = 256\n\t    actor, critic_1, critic_2, value, target_value = \\\n\t        make_sac_networks(env)\n\t    memory = initialize_memory(max_size=100_000,\n\t                               obs_shape=env.observation_space.shape,\n\t                               batch_size=bs,\n\t                               n_actions=env.action_space.shape[0],\n\t                               action_space='continuous',\n\t                               )\n\t    policy = GaussianPolicy(min_action=env.action_space.low,\n", "                            max_action=env.action_space.high)\n\t    agent = Agent(actor, critic_1, critic_2, value,\n\t                  target_value, memory, policy)\n\t    ep_loop = EpisodeLoop(agent, env)\n\t    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/dueling_dqn.py", "chunked_list": ["from protorl.agents.dueling import DuelingDQNAgent as Agent\n\tfrom protorl.loops.single import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_dqn_networks\n\tfrom protorl.policies.epsilon_greedy import EpsilonGreedyPolicy\n\tfrom protorl.wrappers.common import make_env\n\tdef main():\n\t    env_name = 'PongNoFrameskip-v4'\n\t    # env_name = 'CartPole-v1'\n\t    use_double = True\n", "    use_atari = True\n\t    # use_atari = False\n\t    use_dueling = True\n\t    use_prioritization = True\n\t    env = make_env(env_name, use_atari=use_atari)\n\t    n_games = 1500\n\t    bs = 256\n\t    q_online, q_target = make_dqn_networks(env, use_double=use_double,\n\t                                           use_atari=use_atari,\n\t                                           use_dueling=use_dueling)\n", "    memory = initialize_memory(max_size=100_000,\n\t                               obs_shape=env.observation_space.shape,\n\t                               batch_size=bs,\n\t                               n_actions=env.action_space.n,\n\t                               action_space='discrete',\n\t                               prioritized=use_prioritization\n\t                               )\n\t    policy = EpsilonGreedyPolicy(n_actions=env.action_space.n, eps_dec=1e-4)\n\t    agent = Agent(q_online, q_target, memory, policy, use_double=False)\n\t    ep_loop = EpisodeLoop(agent, env)\n", "    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/examples/ppo_discrete.py", "chunked_list": ["import numpy as np\n\tfrom protorl.agents.ppo import PPOAgent as Agent\n\tfrom protorl.loops.ppo_episode import EpisodeLoop\n\tfrom protorl.memory.generic import initialize_memory\n\tfrom protorl.utils.network_utils import make_ppo_networks\n\tfrom protorl.policies.discrete import DiscretePolicy\n\tfrom protorl.wrappers.vec_env import make_vec_envs\n\tdef main():\n\t    env_name = 'CartPole-v1'\n\t    n_games = 4000\n", "    bs = 64\n\t    n_threads = 8\n\t    n_epochs = 10\n\t    horizon = 16384\n\t    T = int(horizon // n_threads)\n\t    env = make_vec_envs(env_name, n_threads=n_threads, seed=0)\n\t    actor, critic = make_ppo_networks(env, action_space='discrete')\n\t    fields = ['states', 'actions', 'rewards', 'states_',\n\t              'mask', 'log_probs']\n\t    state_shape = (T, n_threads, *env.observation_space.shape)\n", "    action_shape = probs_shape = (T, n_threads)\n\t    reward_shape = mask_shape = (T, n_threads)\n\t    vals = [np.zeros(state_shape, dtype=np.float32),\n\t            np.zeros(action_shape, dtype=np.float32),\n\t            np.zeros(reward_shape, dtype=np.float32),\n\t            np.zeros(state_shape, dtype=np.float32),\n\t            np.zeros(mask_shape, dtype=np.float32),\n\t            np.zeros(probs_shape, dtype=np.float32)]\n\t    memory = initialize_memory(max_size=T,\n\t                               obs_shape=env.observation_space.shape,\n", "                               batch_size=bs,\n\t                               n_actions=env.action_space.n,\n\t                               action_space='discrete',\n\t                               n_threads=n_threads,\n\t                               fields=fields,\n\t                               vals=vals\n\t                               )\n\t    policy = DiscretePolicy()\n\t    agent = Agent(actor, critic, memory=memory, policy=policy,\n\t                  action_type='discrete', N=T, n_epochs=n_epochs)\n", "    ep_loop = EpisodeLoop(agent, env, n_threads=n_threads, clip_reward=True,\n\t                          extra_functionality=[agent.anneal_policy_clip],\n\t                          adapt_actions=True)\n\t    scores, steps_array = ep_loop.run(n_games)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "protorl/memory/generic.py", "chunked_list": ["import numpy as np\n\tfrom protorl.memory.sum_tree import SumTree\n\tclass GenericBuffer:\n\t    def __init__(self, max_size, batch_size, fields, prioritized=False):\n\t        self.mem_size = max_size\n\t        self.mem_cntr = 0\n\t        self.batch_size = batch_size\n\t        self.fields = fields\n\t        self.prioritized = prioritized\n\t        if prioritized:\n", "            self.sum_tree = SumTree(max_size, batch_size)\n\t    def store_transition(self, items):\n\t        index = self.mem_cntr % self.mem_size\n\t        for item, field in zip(items, self.fields):\n\t            getattr(self, field)[index] = item\n\t        self.mem_cntr += 1\n\t        if self.prioritized:\n\t            self.sum_tree.store_transition()\n\t    def sample_buffer(self, mode='uniform'):\n\t        max_mem = min(self.mem_cntr, self.mem_size)\n", "        if mode == 'uniform':\n\t            batch = np.random.choice(max_mem, self.batch_size, replace=False)\n\t            arr = []\n\t            for field in self.fields:\n\t                arr.append(getattr(self, field)[batch])\n\t        elif mode == 'batch':\n\t            n_batches = int(self.mem_size // self.batch_size)\n\t            indices = np.arange(self.mem_size, dtype=np.int64)\n\t            np.random.shuffle(indices)\n\t            batches = [indices[i * self.batch_size: (i+1) * self.batch_size]\n", "                       for i in range(n_batches)]\n\t            arr = []\n\t            for batch in batches:\n\t                transition = [batch]\n\t                for field in self.fields:\n\t                    transition.append(getattr(self, field)[batch])\n\t                arr.append(transition)\n\t        elif mode == 'all':\n\t            arr = [getattr(self, field)[:max_mem] for field in self.fields]\n\t        elif mode == 'prioritized':\n", "            indices, weights = self.sum_tree.sample()\n\t            arr = [indices]\n\t            for field in self.fields:\n\t                arr.append(getattr(self, field)[indices])\n\t            arr.append(weights)\n\t        return arr\n\t    def ready(self):\n\t        return self.mem_cntr >= self.batch_size\n\tdef initialize_memory(obs_shape, n_actions, max_size, batch_size,\n\t                      n_threads=1, extra_fields=None, extra_vals=None,\n", "                      action_space='discrete', fields=None, vals=None,\n\t                      prioritized=False):\n\t    if n_threads > 1:\n\t        # state_shape = [max_size, *obs_shape, n_threads]\n\t        state_shape = [max_size, n_threads, *obs_shape]\n\t        reward_shape = [max_size, n_threads]\n\t        done_shape = [max_size, n_threads]\n\t        if action_space == 'continuous':\n\t            action_space = [max_size, n_threads, n_actions]\n\t            a_dtype = np.float32\n", "        elif action_space == 'discrete':\n\t            action_shape = [max_size, n_threads]\n\t            a_dtype = np.int64\n\t    else:\n\t        state_shape = [max_size, *obs_shape]\n\t        reward_shape = max_size\n\t        done_shape = max_size\n\t        if action_space == 'continuous':\n\t            action_shape = [max_size, n_actions]\n\t            a_dtype = np.float32\n", "        elif action_space == 'discrete':\n\t            action_shape = max_size\n\t            a_dtype = np.int64\n\t    fields = fields or ['states', 'actions', 'rewards', 'states_', 'dones']\n\t    vals = vals or [np.zeros(state_shape, dtype=np.float32),\n\t                    np.zeros(action_shape, dtype=a_dtype),\n\t                    np.zeros(reward_shape, dtype=np.float32),\n\t                    np.zeros(state_shape, dtype=np.float32),\n\t                    np.zeros(done_shape, dtype=bool)]\n\t    if extra_fields is not None:\n", "        fields += extra_fields\n\t        vals += extra_vals\n\t    Memory = type('ReplayBuffer', (GenericBuffer,),\n\t                  {field: value for field, value in zip(fields, vals)})\n\t    memory_buffer = Memory(max_size, batch_size, fields, prioritized)\n\t    return memory_buffer\n"]}
{"filename": "protorl/memory/__init__.py", "chunked_list": []}
{"filename": "protorl/memory/sum_tree.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom typing import List\n\timport numpy as np\n\t@dataclass\n\tclass Node:\n\t    value: float = 0.01\n\t    total: float = 0.01\n\t    def update_priority(self, priority: float):\n\t        delta = priority - self.value\n\t        self.value = priority\n", "        self.total += delta\n\t        return delta\n\t    def update_total(self, delta: float):\n\t        self.total += delta\n\tclass SumTree:\n\t    def __init__(self, max_size: int = 1_00_000, batch_size: int = 32,\n\t                 alpha: float = 0.5, beta: float = 0.5):\n\t        self.counter = 0\n\t        self.max_size = max_size\n\t        self.batch_size = batch_size\n", "        self.alpha = alpha\n\t        self.beta = beta\n\t        self.alpha_start = alpha\n\t        self.beta_start = beta\n\t        self.sum_tree = []\n\t    def _insert(self):\n\t        if self.counter < self.max_size:\n\t            self.sum_tree.append(Node())\n\t        self.counter += 1\n\t    def store_transition(self):\n", "        self._insert()\n\t    def _calculate_parents(self, index: int):\n\t        parents = []\n\t        index = index.item()\n\t        while index > 0:\n\t            parents.append(int((index-1)//2))\n\t            index = int((index-1)//2)\n\t        return parents\n\t    def update_priorities(self, indices: List, priorities: List):\n\t        self._propagate_changes(indices, priorities)\n", "    def _propagate_changes(self, indices: List, priorities: List):\n\t        for idx, p in zip(indices, priorities):\n\t            delta = self.sum_tree[idx].update_priority(p**self.alpha)\n\t            parents = self._calculate_parents(idx)\n\t            for parent in parents:\n\t                self.sum_tree[parent].update_total(delta)\n\t    def _sample(self):\n\t        total_weight = self.sum_tree[0].total\n\t        if total_weight == 0.01:\n\t            samples = np.random.choice(self.batch_size, self.batch_size,\n", "                                       replace=False)\n\t            probs = [1 / self.batch_size for _ in range(self.batch_size)]\n\t            return samples, probs\n\t        samples, probs, n_samples = [], [], 1\n\t        index = self.counter % self.max_size - 1\n\t        samples.append(index)\n\t        probs.append(self.sum_tree[index].value / self.sum_tree[0].total)\n\t        while n_samples < self.batch_size:\n\t            index = 0\n\t            target = total_weight * np.random.random()\n", "            while True:\n\t                left = 2 * index + 1\n\t                right = 2 * index + 2\n\t                if left > len(self.sum_tree) - 1\\\n\t                   or right > len(self.sum_tree) - 1:\n\t                    break\n\t                left_sum = self.sum_tree[left].total\n\t                if target < left_sum:\n\t                    index = left\n\t                    continue\n", "                target -= left_sum\n\t                right_sum = self.sum_tree[right].total\n\t                if target < right_sum:\n\t                    index = right\n\t                    continue\n\t                target -= right_sum\n\t                break\n\t            samples.append(index)\n\t            n_samples += 1\n\t            probs.append(self.sum_tree[index].value / self.sum_tree[0].total)\n", "        return samples, probs\n\t    def sample(self):\n\t        samples, probs = self._sample()\n\t        weights = self._calculate_weights(probs)\n\t        return samples, weights\n\t    def _calculate_weights(self, probs: List):\n\t        weights = np.array([(1 / self.counter * 1 / prob)**self.beta\n\t                            for prob in probs])\n\t        weights *= 1 / max(weights)\n\t        return weights\n", "    def anneal_beta(self, ep: int, ep_max: int):\n\t        self.beta = self.beta_start + ep / ep_max * (1 - self.beta_start)\n\t    def anneal_alpha(self, ep: int, ep_max: int):\n\t        self.alpha = self.alpha_start * (1 - ep / ep_max)\n"]}
