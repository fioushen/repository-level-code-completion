{"filename": "main.py", "chunked_list": ["import os\n\timport time\n\timport random\n\timport argparse\n\timport datetime\n\timport numpy as np\n\timport subprocess\n\timport torch\n\timport torch.backends.cudnn as cudnn\n\timport torch.distributed as dist\n", "from timm.utils import ModelEma\n\tfrom timm.utils import AverageMeter\n\tfrom config import get_config\n\tfrom models import build_model\n\tfrom models import BinaryCrossEntropyLoss, CrossEntropyLoss, DiceLoss\n\tfrom dataset import build_loader\n\tfrom lr_scheduler import build_scheduler\n\tfrom optimizer import build_optimizer\n\tfrom logger import create_logger\n\tfrom utils import accuracy_SBM\n", "from utils import NativeScalerWithGradNormCount as NativeScaler\n\tfrom utils import (load_checkpoint, load_pretrained, save_checkpoint,\n\t                   auto_resume_helper, reduce_tensor, load_ema_checkpoint)\n\tfrom ddp_hooks import fp16_compress_hook\n\ttry:\n\t    if getattr(torch.cuda.amp, 'autocast') is not None:\n\t        has_native_amp = True\n\t    else:\n\t        has_native_amp = False\n\texcept AttributeError:\n", "    has_native_amp = False\n\tdef parse_option():\n\t    parser = argparse.ArgumentParser(\n\t        'GPTrans training and evaluation script', add_help=False)\n\t    parser.add_argument('--cfg', type=str, required=True, metavar=\"FILE\", help='path to config file')\n\t    parser.add_argument(\"--opts\", help=\"Modify config options by adding 'KEY VALUE' pairs. \", default=None, nargs='+')\n\t    # easy config modification\n\t    parser.add_argument('--batch-size', type=int, help=\"batch size for single GPU\")\n\t    parser.add_argument('--dataset', type=str, help='dataset name', default=None)\n\t    parser.add_argument('--data-path', type=str, help='path to dataset')\n", "    parser.add_argument('--pretrained', help='pretrained weight from checkpoint, could be imagenet22k pretrained weight')\n\t    parser.add_argument('--resume', help='resume from checkpoint')\n\t    parser.add_argument('--accumulation-steps', type=int, default=1, help=\"gradient accumulation steps\")\n\t    parser.add_argument('--use-checkpoint', action='store_true', help=\"whether to use gradient checkpointing to save memory\")\n\t    parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],\n\t        help='mixed precision opt level, if O0, no amp is used')\n\t    parser.add_argument('--output', default='work_dirs', type=str, metavar='PATH',\n\t        help='root of output folder, the full path is <output>/<model_name>/<tag> (default: output)')\n\t    parser.add_argument('--tag', help='tag of experiment')\n\t    parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n", "    parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n\t    parser.add_argument('--save-ckpt-num', default=1, type=int)\n\t    parser.add_argument('--use-zero', action='store_true', help=\"whether to use ZeroRedundancyOptimizer (ZeRO) to save memory\")\n\t    # distributed training\n\t    parser.add_argument(\"--local_rank\", type=int, required=True, help='local rank for DistributedDataParallel')\n\t    args, unparsed = parser.parse_known_args()\n\t    config = get_config(args)\n\t    return args, config\n\t@torch.no_grad()\n\tdef throughput(data_loader, model, logger):\n", "    model.eval()\n\t    for idx, data in enumerate(data_loader):\n\t        batch_size = data['x'].shape[0]\n\t        for i in range(100):\n\t            model(data)\n\t        torch.cuda.synchronize()\n\t        logger.info(f\"throughput averaged with 100 times\")\n\t        tic1 = time.time()\n\t        for i in range(100):\n\t            model(data)\n", "        torch.cuda.synchronize()\n\t        tic2 = time.time()\n\t        logger.info(\n\t            f\"batch_size {batch_size} throughput {100 * batch_size / (tic2 - tic1)}\")\n\t        return\n\tdef build_criterion(config):\n\t    if config.TRAIN.CRITERION == 'mse':\n\t        criterion = torch.nn.L1Loss()\n\t    elif config.TRAIN.CRITERION == 'bce':\n\t        criterion = BinaryCrossEntropyLoss(config.TRAIN.CLASS_WEIGHTS, config.TRAIN.REDUCE_ZERO_LABEL)\n", "    elif config.TRAIN.CRITERION == 'ce':\n\t        criterion = CrossEntropyLoss(config.TRAIN.CLASS_WEIGHTS, config.TRAIN.REDUCE_ZERO_LABEL)\n\t    elif config.TRAIN.CRITERION == 'dice':\n\t        criterion = DiceLoss(config.TRAIN.REDUCE_ZERO_LABEL)\n\t    else:\n\t        raise ValueError(f'unknown {config.TRAIN.CRITERION}')\n\t    return criterion\n\tdef main(config):\n\t    # prepare data loaders\n\t    dataset_train, dataset_val, dataset_test, data_loader_train, \\\n", "        data_loader_val, data_loader_test = build_loader(config)\n\t    # build runner\n\t    logger.info(f\"Creating model:{config.MODEL.TYPE}/{config.MODEL.NAME}\")\n\t    model = build_model(config)\n\t    model.cuda()\n\t    logger.info(str(model))\n\t    # build optimizer\n\t    optimizer = build_optimizer(config, model)\n\t    if config.AMP_OPT_LEVEL != \"O0\":\n\t        config.defrost()\n", "        if has_native_amp:\n\t            config.native_amp = True\n\t            use_amp = 'native'\n\t        config.freeze()\n\t    # setup automatic mixed-precision (AMP) loss scaling and op casting\n\t    loss_scaler = None\n\t    if config.AMP_OPT_LEVEL != \"O0\":\n\t        if use_amp == 'native':\n\t            loss_scaler = NativeScaler()\n\t            if config.LOCAL_RANK == 0:\n", "                logger.info('Using native Torch AMP. Training in mixed precision.')\n\t        else:\n\t            if config.LOCAL_RANK == 0:\n\t                logger.info('AMP not enabled. Training in float32.')\n\t    # put model on gpus\n\t    model = torch.nn.parallel.DistributedDataParallel(\n\t        model, device_ids=[config.LOCAL_RANK], broadcast_buffers=False,\n\t        gradient_as_bucket_view=True)\n\t    try:\n\t        model.register_comm_hook(state=None, hook=fp16_compress_hook)\n", "        logger.info('using fp16_compress_hook!')\n\t    except:\n\t        logger.info(\"cannot register fp16_compress_hook!\")\n\t    model_without_ddp = model.module\n\t    n_parameters = sum(p.numel() for p in model.parameters()\n\t                       if p.requires_grad)\n\t    logger.info(f\"number of params: {n_parameters / 1000 / 1000}M\")\n\t    if hasattr(model_without_ddp, 'flops'):\n\t        flops = model_without_ddp.flops()\n\t        logger.info(f\"number of GFLOPs: {flops / 1e9}\")\n", "    # build learning rate scheduler\n\t    lr_scheduler = build_scheduler(config, optimizer, len(data_loader_train)) \\\n\t        if not config.EVAL_MODE else None\n\t    # build criterion\n\t    criterion = build_criterion(config)\n\t    if config.DATA.METRIC in ['MAE']:\n\t        best_performance, best_performance_ema = 99999, 99999\n\t    else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n\t        best_performance, best_performance_ema = 0, 0\n\t    # set auto resume\n", "    if config.MODEL.RESUME == '' and config.TRAIN.AUTO_RESUME:\n\t        resume_file = auto_resume_helper(config.OUTPUT)\n\t        if resume_file:\n\t            if config.MODEL.RESUME:\n\t                logger.warning(f\"auto-resume changing resume file from {config.MODEL.RESUME} to {resume_file}\")\n\t            config.defrost()\n\t            config.MODEL.RESUME = resume_file\n\t            config.freeze()\n\t            logger.info(f'auto resuming from {resume_file}')\n\t        else:\n", "            logger.info(f'no checkpoint found in {config.OUTPUT}, ignoring auto resume')\n\t    # set resume and pretrain\n\t    if config.MODEL.RESUME:\n\t        best_performance = load_checkpoint(config, model_without_ddp, optimizer,\n\t                                           lr_scheduler, loss_scaler, logger)\n\t        performance, loss = validate(config, data_loader_val, model)\n\t        logger.info(f\"{config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n\t        performance, loss = validate(config, data_loader_test, model)\n\t        logger.info(f\"{config.DATA.METRIC} on the {len(dataset_test)} test graphs: {performance:.4f}\")\n\t    elif config.MODEL.PRETRAINED:\n", "        load_pretrained(config, model_without_ddp, logger)\n\t        if data_loader_val is not None:\n\t            performance, loss = validate(config, data_loader_val, model)\n\t            logger.info(f\"{config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n\t    # evaluate EMA\n\t    model_ema = None\n\t    if config.TRAIN.EMA.ENABLE:\n\t        # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n\t        model_ema = ModelEma(model, decay=config.TRAIN.EMA.DECAY)\n\t        logger.info(\"Using EMA with decay = %.8f\" % config.TRAIN.EMA.DECAY)\n", "        if config.MODEL.RESUME:\n\t            best_performance_ema = load_ema_checkpoint(config, model_ema, logger)\n\t            performance, loss = validate(config, data_loader_val, model_ema.ema)\n\t            logger.info(f\"[EMA] {config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n\t            performance, loss = validate(config, data_loader_test, model_ema.ema)\n\t            logger.info(f\"[EMA] {config.DATA.METRIC} on the {len(dataset_test)} test graphs: {performance:.4f}\")\n\t    if config.THROUGHPUT_MODE:\n\t        throughput(data_loader_val, model, logger)\n\t    if config.EVAL_MODE:\n\t        exit()\n", "    # train\n\t    logger.info(\"Start training\")\n\t    start_time = time.time()\n\t    for epoch in range(config.TRAIN.START_EPOCH, config.TRAIN.EPOCHS):\n\t        data_loader_train.sampler.set_epoch(epoch)\n\t        train_one_epoch(config, model, criterion, data_loader_train, optimizer,\n\t                        epoch, lr_scheduler, loss_scaler, model_ema=model_ema)\n\t        if (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)) and config.TRAIN.OPTIMIZER.USE_ZERO:\n\t            optimizer.consolidate_state_dict(to=0)\n\t        if dist.get_rank() == 0 and (epoch % config.SAVE_FREQ == 0 or epoch == (config.TRAIN.EPOCHS - 1)):\n", "            save_checkpoint(config, epoch, model_without_ddp, optimizer, lr_scheduler,\n\t                            loss_scaler, logger, best_performance=best_performance,\n\t                            best_performance_ema=best_performance_ema, model_ema=model_ema)\n\t        if data_loader_val is not None and epoch % config.EVAL_FREQ == 0:\n\t            performance, loss = validate(config, data_loader_val, model, epoch)\n\t            logger.info(f\"{config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n\t            if config.DATA.METRIC in ['MAE']:\n\t                best_flag = performance < best_performance\n\t            else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n\t                best_flag = performance > best_performance\n", "            if dist.get_rank() == 0 and best_flag:\n\t                save_checkpoint(config, epoch, model_without_ddp, optimizer, lr_scheduler,\n\t                                loss_scaler, logger, best_performance=best_performance,\n\t                                best_performance_ema=best_performance_ema,\n\t                                model_ema=model_ema, best='best')\n\t            if config.DATA.METRIC in ['MAE']:\n\t                best_performance = min(best_performance, performance)\n\t            else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n\t                best_performance = max(best_performance, performance)\n\t            logger.info(f'Best {config.DATA.METRIC}: {best_performance:.4f}')\n", "            if config.TRAIN.EMA.ENABLE:\n\t                performance, loss = validate(config, data_loader_val, model_ema.ema, epoch)\n\t                logger.info(f\"[EMA] {config.DATA.METRIC} on the {len(dataset_val)} val graphs: {performance:.4f}\")\n\t                if config.DATA.METRIC in ['MAE']:\n\t                    best_flag = performance < best_performance_ema\n\t                else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n\t                    best_flag = performance > best_performance_ema\n\t                if dist.get_rank() == 0 and best_flag:\n\t                    save_checkpoint(config, epoch, model_without_ddp, optimizer, lr_scheduler,\n\t                                    loss_scaler, logger, best_performance=best_performance,\n", "                                    best_performance_ema=best_performance_ema,\n\t                                    model_ema=model_ema, best='ema_best')\n\t                if config.DATA.METRIC in ['MAE']:\n\t                    best_performance_ema = min(best_performance_ema, performance)\n\t                else: # ['ROC_AUC', 'AP', 'Accuracy', 'F1_Score']\n\t                    best_performance_ema = max(best_performance_ema, performance)\n\t                logger.info(f'Best EMA {config.DATA.METRIC}: {best_performance_ema:.4f}')\n\t    total_time = time.time() - start_time\n\t    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n\t    logger.info('Training time {}'.format(total_time_str))\n", "def train_one_epoch(config, model, criterion, data_loader, optimizer, epoch,\n\t                    lr_scheduler, loss_scaler=None, model_ema=None):\n\t    model.train()\n\t    optimizer.zero_grad()\n\t    num_steps = len(data_loader)\n\t    batch_time = AverageMeter()\n\t    model_time = AverageMeter()\n\t    loss_meter = AverageMeter()\n\t    start = time.time()\n\t    end = time.time()\n", "    for idx, data in enumerate(data_loader):\n\t        iter_begin_time = time.time()\n\t        samples = data\n\t        targets = data['y'].cuda(non_blocking=True)\n\t        targets = targets.reshape(-1) # for compatibility with node classification\n\t        with torch.cuda.amp.autocast():\n\t            outputs = model(samples)\n\t        loss = criterion(outputs, targets)\n\t        if config.TRAIN.ACCUMULATION_STEPS > 1: # with gradient accumulation\n\t            loss = loss / config.TRAIN.ACCUMULATION_STEPS\n", "            if config.AMP_OPT_LEVEL != \"O0\":\n\t                is_second_order = hasattr(optimizer, 'is_second_order') and \\\n\t                    optimizer.is_second_order\n\t                loss_scaler(loss,\n\t                            optimizer,\n\t                            clip_grad=config.TRAIN.CLIP_GRAD,\n\t                            parameters=model.parameters(),\n\t                            create_graph=is_second_order,\n\t                            update_grad=(idx + 1) %\n\t                            config.TRAIN.ACCUMULATION_STEPS == 0)\n", "                if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n\t                    optimizer.zero_grad()\n\t                    if model_ema is not None:\n\t                        model_ema.update(model)\n\t            else:\n\t                loss.backward()\n\t                if config.TRAIN.CLIP_GRAD:\n\t                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n\t                if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n\t                    optimizer.step()\n", "                    optimizer.zero_grad()\n\t                    if model_ema is not None:\n\t                        model_ema.update(model)\n\t            if (idx + 1) % config.TRAIN.ACCUMULATION_STEPS == 0:\n\t                lr_scheduler.step_update(epoch * num_steps + idx)\n\t        else:  # without gradient accumulation\n\t            optimizer.zero_grad()\n\t            if config.AMP_OPT_LEVEL != \"O0\":\n\t                is_second_order = hasattr(optimizer, 'is_second_order') and \\\n\t                    optimizer.is_second_order\n", "                loss_scaler(loss,\n\t                            optimizer,\n\t                            clip_grad=config.TRAIN.CLIP_GRAD,\n\t                            parameters=model.parameters(),\n\t                            create_graph=is_second_order,\n\t                            update_grad=(idx + 1) %\n\t                            config.TRAIN.ACCUMULATION_STEPS == 0)\n\t                if model_ema is not None:\n\t                    model_ema.update(model)\n\t            else:\n", "                loss.backward()\n\t                if config.TRAIN.CLIP_GRAD:\n\t                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.TRAIN.CLIP_GRAD)\n\t                optimizer.step()\n\t                if model_ema is not None:\n\t                    model_ema.update(model)\n\t            lr_scheduler.step_update(epoch * num_steps + idx)\n\t        if idx % config.PRINT_FREQ == 0:\n\t            torch.cuda.synchronize()\n\t            loss_meter.update(loss.item(), targets.size(0))\n", "            batch_time.update((time.time() - end) / config.PRINT_FREQ)\n\t            model_time.update(time.time() - iter_begin_time)\n\t            end = time.time()\n\t            lr = optimizer.param_groups[0]['lr']\n\t            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n\t            etas = batch_time.avg * (num_steps - idx)\n\t            logger.info(\n\t                f'Train: [{epoch}/{config.TRAIN.EPOCHS}][{idx}/{num_steps}]\\t'\n\t                f'eta {datetime.timedelta(seconds=int(etas))} lr {lr:.6f}\\t'\n\t                f'time {batch_time.val:.4f} ({batch_time.avg:.4f})\\t'\n", "                f'model_time {model_time.val:.4f} ({model_time.avg:.4f})\\t'\n\t                f'loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n\t                f'mem {memory_used:.0f}MB')\n\t    epoch_time = time.time() - start\n\t    logger.info(f\"EPOCH {epoch} training takes {datetime.timedelta(seconds=int(epoch_time))}\")\n\t@torch.no_grad()\n\tdef calculate_performance(config, output, target):\n\t    if config.DATA.METRIC == 'MAE':\n\t        import ogb\n\t        evaluator = ogb.lsc.PCQM4Mv2Evaluator()\n", "        input_dict = {'y_pred': output, 'y_true': target}\n\t        performance = evaluator.eval(input_dict)['mae']\n\t        if output.shape[0] == 147037:\n\t            print(\"save the output for the test set!\")\n\t            input_dict = {'y_pred': output.cpu().numpy()}\n\t            evaluator.save_test_submission(input_dict=input_dict, dir_path=\"./submit\", mode='test-dev')\n\t    elif config.DATA.METRIC == 'Accuracy':\n\t        mask = ~torch.isnan(target)\n\t        if config.TRAIN.REDUCE_ZERO_LABEL:\n\t            target = target - 1\n", "        performance = accuracy_SBM(output[mask], target[mask])\n\t    elif config.DATA.METRIC == 'ROC_AUC':\n\t        from ogb.graphproppred import Evaluator\n\t        evaluator = Evaluator(name=\"ogbg-molhiv\")\n\t        input_dict = {'y_pred': output[:, None].sigmoid(), 'y_true': target[:, None]}\n\t        performance = evaluator.eval(input_dict)['rocauc']\n\t    elif config.DATA.METRIC == 'AP':\n\t        from ogb.graphproppred import Evaluator\n\t        evaluator = Evaluator(name=\"ogbg-molpcba\")\n\t        if config.TRAIN.REDUCE_ZERO_LABEL:\n", "            target = target - 1\n\t        target = target.reshape(output.shape)\n\t        input_dict = {'y_pred': output.sigmoid(), 'y_true': target}\n\t        performance = evaluator.eval(input_dict)['ap']\n\t    elif config.DATA.METRIC == 'F1_Score':\n\t        # TODO: add F1-score\n\t        performance = None\n\t    else:\n\t        raise NotImplementedError\n\t    performance = torch.tensor(performance, device=output.device)\n", "    performance = reduce_tensor(performance).item()\n\t    return performance\n\t@torch.no_grad()\n\tdef validate(config, data_loader, model, epoch=None):\n\t    criterion = build_criterion(config)\n\t    model.eval()\n\t    batch_time = AverageMeter()\n\t    loss_meter = AverageMeter()\n\t    end = time.time()\n\t    outputs, targets = [], []\n", "    for idx, data in enumerate(data_loader):\n\t        target = data['y'].cuda(non_blocking=True)\n\t        target = target.reshape(-1) # for compatibility with node classification\n\t        output = model(data)\n\t        outputs.append(output)\n\t        targets.append(target)\n\t        # measure accuracy and record loss\n\t        loss = criterion(output, target)\n\t        loss = reduce_tensor(loss)\n\t        loss_meter.update(loss.item(), target.size(0))\n", "        # measure elapsed time\n\t        batch_time.update(time.time() - end)\n\t        end = time.time()\n\t        if idx % config.PRINT_FREQ == 0:\n\t            memory_used = torch.cuda.max_memory_allocated() / (1024.0 * 1024.0)\n\t            logger.info(f'Test: [{idx}/{len(data_loader)}]\\t'\n\t                        f'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n\t                        f'Loss {loss_meter.val:.4f} ({loss_meter.avg:.4f})\\t'\n\t                        f'Mem {memory_used:.0f}MB')\n\t    outputs = torch.cat(outputs, dim=0)\n", "    targets = torch.cat(targets, dim=0)\n\t    performance = calculate_performance(config, outputs, targets)\n\t    if epoch is not None:\n\t        logger.info(f'[Epoch:{epoch}] * {config.DATA.METRIC} {performance:.4f}')\n\t    else:\n\t        logger.info(f' * {config.DATA.METRIC} {performance:.4f}')\n\t    return performance, loss_meter.avg\n\tif __name__ == '__main__':\n\t    _, config = parse_option()\n\t    if config.AMP_OPT_LEVEL != \"O0\":\n", "        assert has_native_amp, \"Please update pytorch(1.6+) to support amp!\"\n\t    # init distributed env\n\t    if 'SLURM_PROCID' in os.environ and int(os.environ['SLURM_NNODES']) != 1:\n\t        print(\"\\nDist init: SLURM\")\n\t        rank = int(os.environ['SLURM_PROCID'])\n\t        gpu = rank % torch.cuda.device_count()\n\t        config.defrost()\n\t        config.LOCAL_RANK = gpu\n\t        config.freeze()\n\t        world_size = int(os.environ[\"SLURM_NTASKS\"])\n", "        if \"MASTER_PORT\" not in os.environ:\n\t            os.environ[\"MASTER_PORT\"] = \"29501\"\n\t        node_list = os.environ[\"SLURM_NODELIST\"]\n\t        addr = subprocess.getoutput(f\"scontrol show hostname {node_list} | head -n1\")\n\t        if \"MASTER_ADDR\" not in os.environ:\n\t            os.environ[\"MASTER_ADDR\"] = addr\n\t        os.environ['RANK'] = str(rank)\n\t        os.environ['LOCAL_RANK'] = str(gpu)\n\t        os.environ['LOCAL_SIZE'] = str(torch.cuda.device_count())\n\t        os.environ['WORLD_SIZE'] = str(world_size)\n", "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n\t        rank = int(os.environ[\"RANK\"])\n\t        world_size = int(os.environ['WORLD_SIZE'])\n\t        print(f\"RANK and WORLD_SIZE in environ: {rank}/{world_size}\")\n\t    else:\n\t        rank = -1\n\t        world_size = -1\n\t    torch.cuda.set_device(config.LOCAL_RANK)\n\t    torch.distributed.init_process_group(backend='nccl',\n\t                                         init_method='env://',\n", "                                         world_size=world_size,\n\t                                         rank=rank)\n\t    torch.distributed.barrier()\n\t    seed = config.SEED + dist.get_rank()\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    torch.backends.cuda.matmul.allow_tf32 = True\n\t    torch.backends.cudnn.allow_tf32 = True\n", "    cudnn.benchmark = False\n\t    # linear scale the learning rate according to total batch size, may not be optimal\n\t    linear_scaled_lr = config.TRAIN.BASE_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n\t    linear_scaled_warmup_lr = config.TRAIN.WARMUP_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n\t    linear_scaled_min_lr = config.TRAIN.MIN_LR * config.DATA.BATCH_SIZE * dist.get_world_size() / 512.0\n\t    # gradient accumulation also need to scale the learning rate\n\t    if config.TRAIN.ACCUMULATION_STEPS > 1:\n\t        linear_scaled_lr = linear_scaled_lr * config.TRAIN.ACCUMULATION_STEPS\n\t        linear_scaled_warmup_lr = linear_scaled_warmup_lr * config.TRAIN.ACCUMULATION_STEPS\n\t        linear_scaled_min_lr = linear_scaled_min_lr * config.TRAIN.ACCUMULATION_STEPS\n", "    config.defrost()\n\t    config.TRAIN.BASE_LR = linear_scaled_lr\n\t    config.TRAIN.WARMUP_LR = linear_scaled_warmup_lr\n\t    config.TRAIN.MIN_LR = linear_scaled_min_lr\n\t    print(config.AMP_OPT_LEVEL, _.amp_opt_level)\n\t    config.freeze()\n\t    os.makedirs(config.OUTPUT, exist_ok=True)\n\t    logger = create_logger(output_dir=config.OUTPUT,\n\t                           dist_rank=dist.get_rank(),\n\t                           name=f\"{config.MODEL.NAME}\")\n", "    if dist.get_rank() == 0:\n\t        path = os.path.join(config.OUTPUT, \"config.json\")\n\t        with open(path, \"w\") as f:\n\t            f.write(config.dump())\n\t        logger.info(f\"Full config saved to {path}\")\n\t    # print config\n\t    logger.info(config.dump())\n\t    main(config)\n"]}
{"filename": "config.py", "chunked_list": ["import os\n\timport yaml\n\tfrom yacs.config import CfgNode as CN\n\t_C = CN()\n\t# Base config files\n\t_C.BASE = ['']\n\t# -----------------------------------------------------------------------------\n\t# Data settings\n\t# -----------------------------------------------------------------------------\n\t_C.DATA = CN()\n", "# Batch size for a single GPU, could be overwritten by command line argument\n\t_C.DATA.BATCH_SIZE = 128\n\t# Path to dataset, could be overwritten by command line argument\n\t_C.DATA.DATA_PATH = ''\n\t# Dataset name\n\t_C.DATA.DATASET = 'pcqm4m'\n\t# Dataset source\n\t_C.DATA.SOURCE = 'ogb'\n\t# Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.\n\t_C.DATA.PIN_MEMORY = True\n", "# Number of data loading threads\n\t_C.DATA.NUM_WORKERS = 16\n\t# Max nodes\n\t_C.DATA.TRAIN_MAX_NODES = 512\n\t_C.DATA.INFER_MAX_NODES = 512\n\t# Multi hop max distance\n\t_C.DATA.MULTI_HOP_MAX_DIST = 20\n\t# spatial pos max\n\t_C.DATA.SPATIAL_POS_MAX = 1024\n\t# number of atoms\n", "_C.DATA.NUM_ATOMS = 4608\n\t# numebr of edges\n\t_C.DATA.NUM_EDGES = 1536\n\t# number of in degree\n\t_C.DATA.NUM_IN_DEGREE = 512\n\t# number of out degree\n\t_C.DATA.NUM_OUT_DEGREE = 512\n\t# number of edge distance\n\t_C.DATA.NUM_EDGE_DIST = 128\n\t# number of spatial\n", "_C.DATA.NUM_SPATIAL = 512\n\t# edge type\n\t_C.DATA.EDGE_TYPE = 'multi_hop'\n\t# metric name\n\t_C.DATA.METRIC = 'MAE'\n\t# task type\n\t_C.DATA.TASK_TYPE = 'graph_regression'\n\t# -----------------------------------------------------------------------------\n\t# Model settings\n\t# -----------------------------------------------------------------------------\n", "_C.MODEL = CN()\n\t# Model type\n\t_C.MODEL.TYPE = 'GPTrans'\n\t# Model name\n\t_C.MODEL.NAME = 'GPTrans'\n\t# Pretrained weight from checkpoint, could be pcqm4m pretrained weight\n\t_C.MODEL.PRETRAINED = ''\n\t# Checkpoint to resume, could be overwritten by command line argument\n\t_C.MODEL.RESUME = ''\n\t# Number of classes, overwritten in data preparation\n", "_C.MODEL.NUM_CLASSES = 1\n\t# Dropout rate\n\t_C.MODEL.DROP_RATE = 0.0\n\t# Drop path rate\n\t_C.MODEL.DROP_PATH_RATE = 0.0\n\t# Drop path type\n\t_C.MODEL.DROP_PATH_TYPE = 'linear'  # linear, uniform\n\t# Attention drop rate\n\t_C.MODEL.ATTN_DROP_RATE = 0.0\n\t# GPTRANS parameters\n", "_C.MODEL.GPTRANS = CN()\n\t_C.MODEL.GPTRANS.NUM_LAYERS = 24\n\t_C.MODEL.GPTRANS.NUM_HEADS = 23\n\t_C.MODEL.GPTRANS.NODE_DIM = 736\n\t_C.MODEL.GPTRANS.EDGE_DIM = 92\n\t_C.MODEL.GPTRANS.LAYER_SCALE = None\n\t_C.MODEL.GPTRANS.MLP_RATIO = 1.0\n\t_C.MODEL.GPTRANS.POST_NORM = False\n\t_C.MODEL.GPTRANS.NUM_CLASSES = 1\n\t# -----------------------------------------------------------------------------\n", "# Training settings\n\t# -----------------------------------------------------------------------------\n\t_C.TRAIN = CN()\n\t_C.TRAIN.START_EPOCH = 0\n\t_C.TRAIN.EPOCHS = 300\n\t_C.TRAIN.WARMUP_EPOCHS = 20\n\t_C.TRAIN.WEIGHT_DECAY = 0.05\n\t_C.TRAIN.BASE_LR = 5e-4\n\t_C.TRAIN.WARMUP_LR = 5e-7\n\t_C.TRAIN.MIN_LR = 5e-6\n", "# Clip gradient norm\n\t_C.TRAIN.CLIP_GRAD = None\n\t# Auto resume from latest checkpoint\n\t_C.TRAIN.AUTO_RESUME = True\n\t# Gradient accumulation steps\n\t# could be overwritten by command line argument\n\t_C.TRAIN.ACCUMULATION_STEPS = 0\n\t# Whether to use gradient checkpointing to save memory\n\t# could be overwritten by command line argument\n\t_C.TRAIN.USE_CHECKPOINT = False\n", "# criterion type\n\t_C.TRAIN.CRITERION = 'mse'\n\t# class weights\n\t_C.TRAIN.CLASS_WEIGHTS = None\n\t# reduce zero label\n\t_C.TRAIN.REDUCE_ZERO_LABEL = False\n\t# LR scheduler\n\t_C.TRAIN.LR_SCHEDULER = CN()\n\t_C.TRAIN.LR_SCHEDULER.NAME = 'cosine'\n\t# Epoch interval to decay LR, used in StepLRScheduler\n", "_C.TRAIN.LR_SCHEDULER.DECAY_EPOCHS = 30\n\t# LR decay rate, used in StepLRScheduler\n\t_C.TRAIN.LR_SCHEDULER.DECAY_RATE = 0.1\n\t# Optimizer\n\t_C.TRAIN.OPTIMIZER = CN()\n\t_C.TRAIN.OPTIMIZER.NAME = 'adamw'\n\t# Optimizer Epsilon\n\t_C.TRAIN.OPTIMIZER.EPS = 1e-8\n\t# Optimizer Betas\n\t_C.TRAIN.OPTIMIZER.BETAS = (0.9, 0.999)\n", "# SGD momentum\n\t_C.TRAIN.OPTIMIZER.MOMENTUM = 0.9\n\t# ZeRO\n\t_C.TRAIN.OPTIMIZER.USE_ZERO = False\n\t# EMA\n\t_C.TRAIN.EMA = CN()\n\t_C.TRAIN.EMA.ENABLE = False\n\t_C.TRAIN.EMA.DECAY = 0.9998\n\t# FLAG\n\t# self.flag_m = cfg.flag_m\n", "# self.flag_step_size = cfg.flag_step_size\n\t# self.flag_mag = cfg.flag_mag\n\t_C.TRAIN.FLAG = CN()\n\t_C.TRAIN.FLAG.ENABLE = False\n\t_C.TRAIN.FLAG.FLAG_M = 3\n\t_C.TRAIN.FLAG.FLAG_STEP_SIZE = 0.01\n\t_C.TRAIN.FLAG.FLAG_MAG = 0\n\t# LR_LAYER_DECAY\n\t_C.TRAIN.LR_LAYER_DECAY = False\n\t_C.TRAIN.LR_LAYER_DECAY_RATIO = 0.875\n", "# FT head init weights\n\t_C.TRAIN.RAND_INIT_FT_HEAD = False\n\t_C.TRAIN.PARAM_EFFICIENT_TUNING = False\n\t# -----------------------------------------------------------------------------\n\t# Augmentation settings\n\t# -----------------------------------------------------------------------------\n\t_C.AUG = CN()\n\t_C.AUG.RANDOM_FEATURE = False\n\t# -----------------------------------------------------------------------------\n\t# Testing settings\n", "# -----------------------------------------------------------------------------\n\t_C.TEST = CN()\n\t# Whether to use SequentialSampler as validation sampler\n\t_C.TEST.SEQUENTIAL = False\n\t# -----------------------------------------------------------------------------\n\t# Misc\n\t# -----------------------------------------------------------------------------\n\t# Mixed precision opt level, if O0, no amp is used ('O0', 'O1', 'O2')\n\t# overwritten by command line argument\n\t_C.AMP_OPT_LEVEL = ''\n", "# Path to output folder, overwritten by command line argument\n\t_C.OUTPUT = ''\n\t# Tag of experiment, overwritten by command line argument\n\t_C.TAG = 'default'\n\t# Frequency to save checkpoint\n\t_C.SAVE_FREQ = 1\n\t# Frequency to logging info\n\t_C.PRINT_FREQ = 10\n\t# eval freq\n\t_C.EVAL_FREQ = 1\n", "# Fixed random seed\n\t_C.SEED = 0\n\t# Perform evaluation only, overwritten by command line argument\n\t_C.EVAL_MODE = False\n\t# Test throughput only, overwritten by command line argument\n\t_C.THROUGHPUT_MODE = False\n\t# local rank for DistributedDataParallel, given by command line argument\n\t_C.LOCAL_RANK = 0\n\t_C.AMP_TYPE = 'float16'\n\tdef _update_config_from_file(config, cfg_file):\n", "    config.defrost()\n\t    with open(cfg_file, 'r') as f:\n\t        yaml_cfg = yaml.load(f, Loader=yaml.FullLoader)\n\t    for cfg in yaml_cfg.setdefault('BASE', ['']):\n\t        if cfg:\n\t            _update_config_from_file(\n\t                config, os.path.join(os.path.dirname(cfg_file), cfg))\n\t    print('=> merge config from {}'.format(cfg_file))\n\t    config.merge_from_file(cfg_file)\n\t    config.freeze()\n", "def update_config(config, args):\n\t    _update_config_from_file(config, args.cfg)\n\t    config.defrost()\n\t    if hasattr(args, 'opts') and args.opts:\n\t        config.merge_from_list(args.opts)\n\t    # merge from specific arguments\n\t    if hasattr(args, 'batch_size') and args.batch_size:\n\t        config.DATA.BATCH_SIZE = args.batch_size\n\t    if hasattr(args, 'dataset') and args.dataset:\n\t        config.DATA.DATASET = args.dataset\n", "    if hasattr(args, 'source') and args.source:\n\t        config.DATA.SOURCE = args.source\n\t    if hasattr(args, 'data_path') and args.data_path:\n\t        config.DATA.DATA_PATH = args.data_path\n\t    if hasattr(args, 'pretrained') and args.pretrained:\n\t        config.MODEL.PRETRAINED = args.pretrained\n\t    if hasattr(args, 'resume') and args.resume:\n\t        config.MODEL.RESUME = args.resume\n\t    if hasattr(args, 'accumulation_steps') and args.accumulation_steps:\n\t        config.TRAIN.ACCUMULATION_STEPS = args.accumulation_steps\n", "    if hasattr(args, 'use_checkpoint') and args.use_checkpoint:\n\t        config.TRAIN.USE_CHECKPOINT = True\n\t    if hasattr(args, 'amp_opt_level') and args.amp_opt_level:\n\t        config.AMP_OPT_LEVEL = args.amp_opt_level\n\t    if hasattr(args, 'output') and args.output:\n\t        config.OUTPUT = args.output\n\t    if hasattr(args, 'tag') and args.tag:\n\t        config.TAG = args.tag\n\t    if hasattr(args, 'eval') and args.eval:\n\t        config.EVAL_MODE = True\n", "    if hasattr(args, 'throughput') and args.throughput:\n\t        config.THROUGHPUT_MODE = True\n\t    if hasattr(args, 'save_ckpt_num') and args.save_ckpt_num:\n\t        config.SAVE_CKPT_NUM = args.save_ckpt_num\n\t    if hasattr(args, 'use_zero') and args.use_zero:\n\t        config.TRAIN.OPTIMIZER.USE_ZERO = True\n\t    # set local rank for distributed training\n\t    if hasattr(args, 'local_rank') and args.local_rank:\n\t        config.LOCAL_RANK = args.local_rank\n\t    # output folder\n", "    config.MODEL.NAME = args.cfg.split('/')[-1].replace('.yaml', '')\n\t    config.OUTPUT = os.path.join(config.OUTPUT, config.MODEL.NAME)\n\t    config.freeze()\n\tdef get_config(args):\n\t    \"\"\"Get a yacs CfgNode object with default values.\"\"\"\n\t    # Return a clone so that the defaults will not be altered\n\t    # This is for the \"local variable\" use pattern\n\t    config = _C.clone()\n\t    update_config(config, args)\n\t    return config\n"]}
{"filename": "optimizer.py", "chunked_list": ["from torch import optim as optim\n\tfrom torch.distributed.optim import ZeroRedundancyOptimizer\n\tfrom apex.optimizers import FusedAdam\n\timport logging\n\tlogger = logging.getLogger(__name__)\n\tdef build_optimizer(config, model):\n\t    \"\"\"\n\t    Build optimizer, set weight decay of normalization to 0 by default.\n\t    \"\"\"\n\t    skip = {}\n", "    skip_keywords = {}\n\t    if hasattr(model, 'no_weight_decay'):\n\t        skip = model.no_weight_decay()\n\t    if hasattr(model, 'no_weight_decay_keywords'):\n\t        skip_keywords = model.no_weight_decay_keywords()\n\t    parameters = set_weight_decay_and_lr(\n\t        model,\n\t        config.TRAIN.WEIGHT_DECAY,\n\t        config.TRAIN.BASE_LR,\n\t        skip,\n", "        skip_keywords,\n\t        lr_layer_decay=config.TRAIN.LR_LAYER_DECAY,\n\t        lr_layer_decay_ratio=config.TRAIN.LR_LAYER_DECAY_RATIO,\n\t    )\n\t    opt_lower = config.TRAIN.OPTIMIZER.NAME.lower()\n\t    optimizer = None\n\t    use_zero = config.TRAIN.OPTIMIZER.USE_ZERO\n\t    if use_zero:\n\t        logger.info(f\"\\nUse Zero!\")\n\t        if opt_lower == 'sgd':\n", "            # an ugly implementation\n\t            # https://github.com/pytorch/pytorch/issues/71347\n\t            optimizer = ZeroRedundancyOptimizer(\n\t                parameters[0]['params'],\n\t                optimizer_class=optim.SGD,\n\t                momentum=config.TRAIN.OPTIMIZER.MOMENTUM,\n\t                nesterov=True,\n\t                lr=config.TRAIN.BASE_LR,\n\t                weight_decay=config.TRAIN.WEIGHT_DECAY)\n\t            if len(parameters[1]['params']) > 0:\n", "                optimizer.add_param_group({\n\t                    \"params\": parameters[1]['params'],\n\t                    'weight_decay': 0.\n\t                })\n\t        elif opt_lower == 'adamw':\n\t            optimizer = ZeroRedundancyOptimizer(\n\t                parameters[0]['params'],\n\t                optimizer_class=optim.AdamW,\n\t                eps=config.TRAIN.OPTIMIZER.EPS,\n\t                betas=config.TRAIN.OPTIMIZER.BETAS,\n", "                lr=config.TRAIN.BASE_LR,\n\t                weight_decay=config.TRAIN.WEIGHT_DECAY)\n\t            if len(parameters[1]['params']) > 0:\n\t                optimizer.add_param_group({\n\t                    \"params\": parameters[1]['params'],\n\t                    'weight_decay': 0.\n\t                })\n\t    else:\n\t        if opt_lower == 'sgd':\n\t            optimizer = optim.SGD(parameters,\n", "                                  momentum=config.TRAIN.OPTIMIZER.MOMENTUM,\n\t                                  nesterov=True,\n\t                                  lr=config.TRAIN.BASE_LR,\n\t                                  weight_decay=config.TRAIN.WEIGHT_DECAY)\n\t        elif opt_lower == 'adamw':\n\t            optimizer = optim.AdamW(parameters,\n\t                                    eps=config.TRAIN.OPTIMIZER.EPS,\n\t                                    betas=config.TRAIN.OPTIMIZER.BETAS,\n\t                                    lr=config.TRAIN.BASE_LR,\n\t                                    weight_decay=config.TRAIN.WEIGHT_DECAY)\n", "        elif opt_lower == 'fused_adamw':\n\t            optimizer = FusedAdam(parameters,\n\t                                  eps=config.TRAIN.OPTIMIZER.EPS,\n\t                                  betas=config.TRAIN.OPTIMIZER.BETAS,\n\t                                  lr=config.TRAIN.BASE_LR,\n\t                                  weight_decay=config.TRAIN.WEIGHT_DECAY)\n\t    return optimizer\n\tdef check_keywords_in_name(name, keywords=()):\n\t    isin = False\n\t    for keyword in keywords:\n", "        if keyword in name:\n\t            isin = True\n\t    return isin\n\tdef check_keywords_in_dict(name, keywords_dict):\n\t    for k, v in keywords_dict.items():\n\t        if k in name:\n\t            return v\n\t    return None\n\tdef set_weight_decay_and_lr(model, weight_decay, base_lr, skip_list=(), skip_keywords=(),\n\t                            lr_layer_decay=None, lr_layer_decay_ratio=None, layerwise_lr=True):\n", "    parameters = {}\n\t    no_decay_name = []\n\t    lr_ratio_log = {}\n\t    for name, param in model.named_parameters():\n\t        if not param.requires_grad:\n\t            continue  # frozen weights\n\t        # 1. check wd\n\t        if len(param.shape) == 1 or name.endswith(\".bias\") or (\n\t                name in skip_list) or check_keywords_in_name(\n\t                    name, skip_keywords):\n", "            wd = 0.\n\t            no_decay_name.append(name)\n\t        else:\n\t            wd = weight_decay\n\t        # 2. set weight_decay\n\t        if lr_layer_decay:\n\t            logger.info('layer-wise lr decay is used !')\n\t            assert hasattr(model, 'lr_decay_keywords')\n\t            lr_ratio_keywords = model.lr_decay_keywords(lr_layer_decay_ratio)\n\t            # 2. check lr\n", "            ratio = check_keywords_in_dict(name, lr_ratio_keywords)\n\t            if ratio is not None:\n\t                lr = ratio * base_lr\n\t            else:\n\t                lr = base_lr\n\t            lr_ratio_log[name] = (base_lr, ratio, wd, param.requires_grad)\n\t        else:\n\t            lr = base_lr\n\t        group_name = f\"weight_decay_{str(wd)}_lr_{str(lr)}\"\n\t        if group_name not in parameters:\n", "            parameters[group_name] = {'params': [param], 'weight_decay': wd, 'lr': lr}\n\t        else:\n\t            parameters[group_name]['params'].append(param)\n\t    logger.info(f'no decay params: {no_decay_name}')\n\t    if layerwise_lr:\n\t        logger.info('lr_ratio_params:')\n\t        for k, v in lr_ratio_log.items():\n\t            print(k, v)\n\t    parameters = list(parameters.values())\n\t    return parameters\n"]}
{"filename": "logger.py", "chunked_list": ["import os\n\timport sys\n\timport logging\n\timport functools\n\tfrom termcolor import colored\n\t@functools.lru_cache()\n\tdef create_logger(output_dir, dist_rank=0, name=''):\n\t    # create logger\n\t    logger = logging.getLogger(name)\n\t    logger.setLevel(logging.DEBUG)\n", "    logger.propagate = False\n\t    # create formatter\n\t    fmt = '[%(asctime)s %(name)s] (%(filename)s %(lineno)d): %(levelname)s %(message)s'\n\t    color_fmt = colored('[%(asctime)s %(name)s]', 'green') + \\\n\t        colored('(%(filename)s %(lineno)d)', 'yellow') + \\\n\t        ': %(levelname)s %(message)s'\n\t    # create console handlers for master process\n\t    if dist_rank == 0:\n\t        console_handler = logging.StreamHandler(sys.stdout)\n\t        console_handler.setLevel(logging.DEBUG)\n", "        console_handler.setFormatter(\n\t            logging.Formatter(fmt=color_fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n\t        logger.addHandler(console_handler)\n\t    # create file handlers\n\t    file_handler = logging.FileHandler(os.path.join(\n\t        output_dir, f'log_rank{dist_rank}.txt'),\n\t                                       mode='a')\n\t    file_handler.setLevel(logging.DEBUG)\n\t    file_handler.setFormatter(\n\t        logging.Formatter(fmt=fmt, datefmt='%Y-%m-%d %H:%M:%S'))\n", "    logger.addHandler(file_handler)\n\t    return logger\n"]}
{"filename": "ddp_hooks.py", "chunked_list": ["from typing import Any, Callable\n\timport torch\n\timport torch.distributed as dist\n\tdef _allreduce_fut(process_group: dist.ProcessGroup,\n\t                   tensor: torch.Tensor) -> torch.futures.Future[torch.Tensor]:\n\t    \"Averages the input gradient tensor by allreduce and returns a future.\"\n\t    group_to_use = process_group if process_group is not None else dist.group.WORLD\n\t    # Apply the division first to avoid overflow, especially for FP16.\n\t    tensor.div_(group_to_use.size())\n\t    return (dist.all_reduce(\n", "        tensor, group=group_to_use,\n\t        async_op=True).get_future().then(lambda fut: fut.value()[0]))\n\tdef allreduce_hook(\n\t        process_group: dist.ProcessGroup,\n\t        bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n\t    \"\"\"\n\t    This DDP communication hook just calls ``allreduce`` using ``GradBucket``\n\t    tensors. Once gradient tensors are aggregated across all workers, its ``then``\n\t    callback takes the mean and returns the result. If user registers this hook,\n\t    DDP results is expected to be same as the case where no hook was registered.\n", "    Hence, this won't change behavior of DDP and user can use this as a reference\n\t    or modify this hook to log useful information or any other purposes while\n\t    unaffecting DDP behavior.\n\t    Example::\n\t        >>> ddp_model.register_comm_hook(process_group, allreduce_hook)\n\t    \"\"\"\n\t    return _allreduce_fut(process_group, bucket.buffer())\n\tdef fp16_compress_hook(\n\t        process_group: dist.ProcessGroup,\n\t        bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n", "    \"\"\"\n\t    This DDP communication hook implements a simple gradient compression\n\t    approach that casts ``GradBucket`` tensor to half-precision floating-point format (``torch.float16``)\n\t    and then divides it by the process group size.\n\t    It allreduces those ``float16`` gradient tensors. Once compressed gradient\n\t    tensors are allreduced, the chained callback ``decompress`` casts it back to the input data type (such as ``float32``).\n\t    Example::\n\t        >>> ddp_model.register_comm_hook(process_group, fp16_compress_hook)\n\t    \"\"\"\n\t    group_to_use = process_group if process_group is not None else dist.group.WORLD\n", "    world_size = group_to_use.size()\n\t    compressed_tensor = bucket.buffer().to(torch.float16).div_(world_size)\n\t    fut = dist.all_reduce(compressed_tensor, group=group_to_use,\n\t                          async_op=True).get_future()\n\t    def decompress(fut):\n\t        decompressed_tensor = bucket.buffer()\n\t        # Decompress in place to reduce the peak memory.\n\t        # See: https://github.com/pytorch/pytorch/issues/45968\n\t        decompressed_tensor.copy_(fut.value()[0])\n\t        return decompressed_tensor\n", "    return fut.then(decompress)\n\t# TODO: create an internal helper function and extract the duplicate code in FP16_compress and BF16_compress.\n\tdef bf16_compress_hook(\n\t        process_group: dist.ProcessGroup,\n\t        bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n\t    \"\"\"\n\t    Warning: This API is experimental, and it requires NCCL version later than 2.9.6.\n\t    This DDP communication hook implements a simple gradient compression\n\t    approach that casts ``GradBucket`` tensor to half-precision\n\t    `Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format>`_ (``torch.bfloat16``)\n", "    and then divides it by the process group size.\n\t    It allreduces those ``bfloat16`` gradient tensors. Once compressed gradient\n\t    tensors are allreduced, the chained callback ``decompress`` casts it back to the input data type (such as ``float32``).\n\t    Example::\n\t        >>> ddp_model.register_comm_hook(process_group, bf16_compress_hook)\n\t    \"\"\"\n\t    group_to_use = process_group if process_group is not None else dist.group.WORLD\n\t    world_size = group_to_use.size()\n\t    compressed_tensor = bucket.buffer().to(torch.bfloat16).div_(world_size)\n\t    fut = dist.all_reduce(compressed_tensor, group=group_to_use,\n", "                          async_op=True).get_future()\n\t    def decompress(fut):\n\t        decompressed_tensor = bucket.buffer()\n\t        # Decompress in place to reduce the peak memory.\n\t        # See: https://github.com/pytorch/pytorch/issues/45968\n\t        decompressed_tensor.copy_(fut.value()[0])\n\t        return decompressed_tensor\n\t    return fut.then(decompress)\n\tdef fp16_compress_wrapper(\n\t    hook: Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]\n", ") -> Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]:\n\t    \"\"\"\n\t    This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision\n\t    floating point format (``torch.float16``), and casts the resulting tensor of the given hook back to\n\t    the input data type, such as ``float32``.\n\t    Therefore, ``fp16_compress_hook`` is equivalent to ``fp16_compress_wrapper(allreduce_hook)``.\n\t    Example::\n\t        >>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n\t        >>> ddp_model.register_comm_hook(state, fp16_compress_wrapper(powerSGD_hook))\n\t    \"\"\"\n", "    def fp16_compress_wrapper_hook(\n\t            hook_state,\n\t            bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n\t        # Cast bucket tensor to FP16.\n\t        bucket.set_buffer(bucket.buffer().to(torch.float16))\n\t        fut = hook(hook_state, bucket)\n\t        def decompress(fut):\n\t            decompressed_tensor = bucket.buffer()\n\t            # Decompress in place to reduce the peak memory.\n\t            # See: https://github.com/pytorch/pytorch/issues/45968\n", "            decompressed_tensor.copy_(fut.value())\n\t            return decompressed_tensor\n\t        # Decompress after hook has run.\n\t        return fut.then(decompress)\n\t    return fp16_compress_wrapper_hook\n\tdef bf16_compress_wrapper(\n\t    hook: Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]\n\t) -> Callable[[Any, dist.GradBucket], torch.futures.Future[torch.Tensor]]:\n\t    \"\"\"\n\t    Warning: This API is experimental, and it requires NCCL version later than 2.9.6.\n", "    This wrapper casts the input gradient tensor of a given DDP communication hook to half-precision\n\t    `Brain floating point format <https://en.wikipedia.org/wiki/Bfloat16_floating-point_format> `_  (``torch.bfloat16``),\n\t    and casts the resulting tensor of the given hook back to the input data type, such as ``float32``.\n\t    Therefore, ``bf16_compress_hook`` is equivalent to ``bf16_compress_wrapper(allreduce_hook)``.\n\t    Example::\n\t        >>> state = PowerSGDState(process_group=process_group, matrix_approximation_rank=1, start_powerSGD_iter=10)\n\t        >>> ddp_model.register_comm_hook(state, bf16_compress_wrapper(powerSGD_hook))\n\t    \"\"\"\n\t    def bf16_compress_wrapper_hook(\n\t            hook_state,\n", "            bucket: dist.GradBucket) -> torch.futures.Future[torch.Tensor]:\n\t        # Cast bucket tensor to BF16.\n\t        bucket.set_buffer(bucket.buffer().to(torch.bfloat16))\n\t        fut = hook(hook_state, bucket)\n\t        def decompress(fut):\n\t            decompressed_tensor = bucket.buffer()\n\t            # Decompress in place to reduce the peak memory.\n\t            # See: https://github.com/pytorch/pytorch/issues/45968\n\t            decompressed_tensor.copy_(fut.value())\n\t            return decompressed_tensor\n", "        # Decompress after hook has run.\n\t        return fut.then(decompress)\n\t    return bf16_compress_wrapper_hook\n"]}
{"filename": "utils.py", "chunked_list": ["import os\n\timport math\n\timport torch\n\timport numpy as np\n\timport torch.distributed as dist\n\tfrom collections import OrderedDict\n\tfrom timm.utils import get_state_dict\n\tfrom sklearn.metrics import confusion_matrix\n\ttry:\n\t    # noinspection PyUnresolvedReferences\n", "    from apex import amp\n\texcept ImportError:\n\t    amp = None\n\tdef load_ema_checkpoint(config, model_ema, logger):\n\t    logger.info(\n\t        f'==============> Resuming form {config.MODEL.RESUME}....................'\n\t    )\n\t    if config.MODEL.RESUME.startswith('https'):\n\t        checkpoint = torch.hub.load_state_dict_from_url(config.MODEL.RESUME,\n\t                                                        map_location='cpu',\n", "                                                        check_hash=True)\n\t    else:\n\t        checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n\t    assert isinstance(checkpoint, dict)\n\t    if 'model_ema' in checkpoint:\n\t        new_state_dict = OrderedDict()\n\t        for k, v in checkpoint['model_ema'].items():\n\t            if model_ema.ema_has_module:\n\t                name = 'module.' + k if not k.startswith('module') else k\n\t            else:\n", "                name = k\n\t            new_state_dict[name] = v\n\t        msg = model_ema.ema.load_state_dict(new_state_dict, strict=False)\n\t        logger.info(msg)\n\t        logger.info('Loaded state_dict_ema')\n\t    else:\n\t        logger.warning(\n\t            'Failed to find state_dict_ema, starting from loaded model weights'\n\t        )\n\t    best_performance_ema = 0\n", "    if 'best_performance_ema' in checkpoint:\n\t        best_performance_ema = checkpoint['best_performance_ema']\n\t    if 'ema_decay' in checkpoint:\n\t        model_ema.decay = checkpoint['ema_decay']\n\t    return best_performance_ema\n\tdef load_checkpoint(config, model, optimizer, lr_scheduler, scaler, logger):\n\t    logger.info(\n\t        f'==============> Resuming form {config.MODEL.RESUME}....................'\n\t    )\n\t    if config.MODEL.RESUME.startswith('https'):\n", "        checkpoint = torch.hub.load_state_dict_from_url(config.MODEL.RESUME,\n\t                                                        map_location='cpu',\n\t                                                        check_hash=True)\n\t    else:\n\t        checkpoint = torch.load(config.MODEL.RESUME, map_location='cpu')\n\t    print('resuming model')\n\t    msg = model.load_state_dict(checkpoint['model'], strict=False)\n\t    logger.info(msg)\n\t    best_performance = 0.0\n\t    if not config.EVAL_MODE and 'optimizer' in checkpoint and 'lr_scheduler' in checkpoint and 'epoch' in checkpoint:\n", "        if optimizer is not None:\n\t            print('resuming optimizer')\n\t            try:\n\t                optimizer.load_state_dict(checkpoint['optimizer'])\n\t            except:\n\t                print('resume optimizer failed')\n\t        if lr_scheduler is not None:\n\t            print('resuming lr_scheduler')\n\t            lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n\t        config.defrost()\n", "        config.TRAIN.START_EPOCH = checkpoint['epoch'] + 1\n\t        config.freeze()\n\t        if 'amp' in checkpoint and config.AMP_OPT_LEVEL != 'O0' and checkpoint[\n\t                'config'].AMP_OPT_LEVEL != 'O0':\n\t            scaler.load_state_dict(checkpoint['amp'])\n\t        logger.info(\n\t            f\"=> loaded successfully {config.MODEL.RESUME} (epoch {checkpoint['epoch']})\"\n\t        )\n\t        if 'best_performance' in checkpoint:\n\t            best_performance = checkpoint['best_performance']\n", "    del checkpoint\n\t    torch.cuda.empty_cache()\n\t    return best_performance\n\tdef load_pretrained(config, model, logger):\n\t    logger.info(\n\t        f'==============> Loading weight {config.MODEL.PRETRAINED} for fine-tuning......'\n\t    )\n\t    checkpoint = torch.load(config.MODEL.PRETRAINED, map_location='cpu')\n\t    if \"ema\" in config.MODEL.PRETRAINED:\n\t        state_dict = checkpoint['model_ema']\n", "        logger.info(f'==============> Loading ema weight......')\n\t    else:\n\t        if 'model' in checkpoint:\n\t            state_dict = checkpoint['model']\n\t        elif 'module' in checkpoint:\n\t            state_dict = checkpoint['module']\n\t    if config.TRAIN.RAND_INIT_FT_HEAD:\n\t        model.head.weight.data = model.head.weight.data * 0.001\n\t        model.head.bias.data = model.head.bias.data * 0.001\n\t        del state_dict['head.weight']\n", "        del state_dict['head.bias']\n\t        logger.warning(f'Re-init classifier head to 0!')\n\t    msg = model.load_state_dict(state_dict, strict=False)\n\t    logger.warning(msg)\n\t    logger.info(f'=> loaded successfully {config.MODEL.PRETRAINED}')\n\t    del checkpoint\n\t    torch.cuda.empty_cache()\n\tdef save_checkpoint(config, epoch, model, optimizer, lr_scheduler, scaler, logger, best_performance,\n\t                    model_ema=None, ema_decay=None, best_performance_ema=None, best=None):\n\t    save_state = {\n", "        'model': model.state_dict(),\n\t        'optimizer': optimizer.state_dict(),\n\t        'lr_scheduler': lr_scheduler.state_dict(),\n\t        'epoch': epoch,\n\t        'config': config\n\t    }\n\t    if model_ema is not None:\n\t        save_state['model_ema'] = get_state_dict(model_ema)\n\t    if ema_decay is not None:\n\t        save_state['ema_decay'] = ema_decay\n", "    if best_performance is not None:\n\t        save_state['best_performance'] = best_performance\n\t    if best_performance_ema is not None:\n\t        save_state['best_performance_ema'] = best_performance_ema\n\t    if config.AMP_OPT_LEVEL != 'O0':\n\t        # save_state['amp'] = amp.state_dict()\n\t        save_state['amp'] = scaler.state_dict()\n\t    if best is None:\n\t        save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{epoch}.pth')\n\t    else:\n", "        save_path = os.path.join(config.OUTPUT, f'ckpt_epoch_{best}.pth')\n\t    logger.info(f'{save_path} saving......')\n\t    torch.save(save_state, save_path)\n\t    logger.info(f'{save_path} saved !!!')\n\t    if dist.get_rank() == 0 and isinstance(epoch, int):\n\t        to_del = epoch - config.SAVE_CKPT_NUM * config.SAVE_FREQ\n\t        old_ckpt = os.path.join(config.OUTPUT, f'ckpt_epoch_{to_del}.pth')\n\t        if os.path.exists(old_ckpt):\n\t            os.remove(old_ckpt)\n\tdef get_grad_norm(parameters, norm_type=2):\n", "    if isinstance(parameters, torch.Tensor):\n\t        parameters = [parameters]\n\t    parameters = list(filter(lambda p: p.grad is not None, parameters))\n\t    norm_type = float(norm_type)\n\t    total_norm = 0\n\t    for p in parameters:\n\t        param_norm = p.grad.data.norm(norm_type)\n\t        total_norm += param_norm.item()**norm_type\n\t    total_norm = total_norm**(1. / norm_type)\n\t    return total_norm\n", "def auto_resume_helper(output_dir):\n\t    checkpoints = os.listdir(output_dir)\n\t    checkpoints = [ckpt for ckpt in checkpoints if ckpt.endswith('pth')]\n\t    print(f'All checkpoints founded in {output_dir}: {checkpoints}')\n\t    if len(checkpoints) > 0:\n\t        latest_checkpoint = max(\n\t            [os.path.join(output_dir, d) for d in checkpoints],\n\t            key=os.path.getmtime)\n\t        print(f'The latest checkpoint founded: {latest_checkpoint}')\n\t        resume_file = latest_checkpoint\n", "    else:\n\t        resume_file = None\n\t    return resume_file\n\tdef reduce_tensor(tensor):\n\t    rt = tensor.clone()\n\t    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n\t    rt /= dist.get_world_size()\n\t    return rt\n\tdef all_gather_tensor(tensor_list, tensor):\n\t    dist.all_gather(tensor_list, tensor)\n", "    return tensor_list\n\tclass ApexScalerWithGradNormCount:\n\t    state_dict_key = \"amp\"\n\t    def __call__(self, loss, optimizer, clip_grad=None, clip_mode='norm', parameters=None,\n\t                 create_graph=False, update_grad=True):\n\t        with amp.scale_loss(loss, optimizer) as scaled_loss:\n\t            scaled_loss.backward(create_graph=create_graph)\n\t        if update_grad:\n\t            if clip_grad is not None:\n\t                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), clip_grad)\n", "            optimizer.step()\n\t    def state_dict(self):\n\t        if 'state_dict' in amp.__dict__:\n\t            return amp.state_dict()\n\t    def load_state_dict(self, state_dict):\n\t        if 'load_state_dict' in amp.__dict__:\n\t            amp.load_state_dict(state_dict)\n\t# https://github.com/facebookresearch/ConvNeXt/blob/main/utils.py\n\tclass NativeScalerWithGradNormCount:\n\t    state_dict_key = 'amp_scaler'\n", "    def __init__(self):\n\t        self._scaler = torch.cuda.amp.GradScaler()\n\t    def __call__(self, loss, optimizer, clip_grad=None, parameters=None, create_graph=False, update_grad=True):\n\t        self._scaler.scale(loss).backward(create_graph=create_graph)\n\t        if update_grad:\n\t            if clip_grad is not None:\n\t                self._scaler.unscale_(optimizer)  # unscale the gradients of optimizer's assigned params in-place\n\t                torch.nn.utils.clip_grad_norm_(parameters, clip_grad)\n\t            self._scaler.step(optimizer)\n\t            self._scaler.update()\n", "    def state_dict(self):\n\t        return self._scaler.state_dict()\n\t    def load_state_dict(self, state_dict):\n\t        self._scaler.load_state_dict(state_dict)\n\tclass MyAverageMeter(object):\n\t    \"\"\"Computes and stores the average and current value.\"\"\"\n\t    def __init__(self, max_len=-1):\n\t        self.val_list = []\n\t        self.count = []\n\t        self.max_len = max_len\n", "        self.val = 0\n\t        self.avg = 0\n\t        self.var = 0\n\t    def update(self, val):\n\t        self.val = val\n\t        self.avg = 0\n\t        self.var = 0\n\t        if not math.isnan(val) and not math.isinf(val):\n\t            self.val_list.append(val)\n\t        if self.max_len > 0 and len(self.val_list) > self.max_len:\n", "            self.val_list = self.val_list[-self.max_len:]\n\t        if len(self.val_list) > 0:\n\t            self.avg = np.mean(np.array(self.val_list))\n\t            self.var = np.std(np.array(self.val_list))\n\tdef accuracy_SBM(scores, targets):\n\t    # build confusion matrix\n\t    S = targets.cpu().numpy()\n\t    if scores.size(-1) == 1:\n\t        C = torch.sigmoid(scores) < 0.5\n\t        C = (torch.where(C, 0, 1)).cpu().numpy()\n", "        C = C.squeeze(-1)\n\t    else:\n\t        C = scores.argmax(-1).cpu().numpy()\n\t    CM = confusion_matrix(S, C).astype(np.float32)\n\t    # calculate accuracy\n\t    nb_classes = CM.shape[0]\n\t    targets = targets.cpu().detach().numpy()\n\t    nb_non_empty_classes = 0\n\t    pr_classes = np.zeros(nb_classes)\n\t    for r in range(nb_classes):\n", "        cluster = np.where(targets==r)[0]\n\t        if cluster.shape[0] != 0:\n\t            pr_classes[r] = CM[r,r] / float(cluster.shape[0])\n\t            if CM[r,r] > 0:\n\t                nb_non_empty_classes += 1\n\t        else:\n\t            pr_classes[r] = 0.0\n\t    print(\"pre classes acc:\", pr_classes)\n\t    acc = 100.* np.sum(pr_classes)/ float(nb_classes)\n\t    return torch.tensor(acc, device=scores.device)\n"]}
{"filename": "lr_scheduler.py", "chunked_list": ["import torch\n\tfrom timm.scheduler.cosine_lr import CosineLRScheduler\n\tfrom timm.scheduler.step_lr import StepLRScheduler\n\tfrom timm.scheduler.scheduler import Scheduler\n\tdef build_scheduler(config, optimizer, n_iter_per_epoch):\n\t    num_steps = int(config.TRAIN.EPOCHS * n_iter_per_epoch)\n\t    warmup_steps = int(config.TRAIN.WARMUP_EPOCHS * n_iter_per_epoch)\n\t    decay_steps = int(config.TRAIN.LR_SCHEDULER.DECAY_EPOCHS *\n\t                      n_iter_per_epoch)\n\t    lr_scheduler = None\n", "    if config.TRAIN.LR_SCHEDULER.NAME == 'cosine':\n\t        lr_scheduler = CosineLRScheduler(\n\t            optimizer,\n\t            t_initial=num_steps,\n\t            # t_mul=1.,\n\t            lr_min=config.TRAIN.MIN_LR,\n\t            warmup_lr_init=config.TRAIN.WARMUP_LR,\n\t            warmup_t=warmup_steps,\n\t            cycle_limit=1,\n\t            t_in_epochs=False,\n", "        )\n\t    elif config.TRAIN.LR_SCHEDULER.NAME == 'linear':\n\t        lr_scheduler = LinearLRScheduler(\n\t            optimizer,\n\t            t_initial=num_steps,\n\t            lr_min_rate=0.01,\n\t            warmup_lr_init=config.TRAIN.WARMUP_LR,\n\t            warmup_t=warmup_steps,\n\t            t_in_epochs=False,\n\t        )\n", "    elif config.TRAIN.LR_SCHEDULER.NAME == 'step':\n\t        lr_scheduler = StepLRScheduler(\n\t            optimizer,\n\t            decay_t=decay_steps,\n\t            decay_rate=config.TRAIN.LR_SCHEDULER.DECAY_RATE,\n\t            warmup_lr_init=config.TRAIN.WARMUP_LR,\n\t            warmup_t=warmup_steps,\n\t            t_in_epochs=False,\n\t        )\n\t    return lr_scheduler\n", "class LinearLRScheduler(Scheduler):\n\t    def __init__(\n\t        self,\n\t        optimizer: torch.optim.Optimizer,\n\t        t_initial: int,\n\t        lr_min_rate: float,\n\t        warmup_t=0,\n\t        warmup_lr_init=0.,\n\t        t_in_epochs=True,\n\t        noise_range_t=None,\n", "        noise_pct=0.67,\n\t        noise_std=1.0,\n\t        noise_seed=42,\n\t        initialize=True,\n\t    ) -> None:\n\t        super().__init__(optimizer,\n\t                         param_group_field=\"lr\",\n\t                         noise_range_t=noise_range_t,\n\t                         noise_pct=noise_pct,\n\t                         noise_std=noise_std,\n", "                         noise_seed=noise_seed,\n\t                         initialize=initialize)\n\t        self.t_initial = t_initial\n\t        self.lr_min_rate = lr_min_rate\n\t        self.warmup_t = warmup_t\n\t        self.warmup_lr_init = warmup_lr_init\n\t        self.t_in_epochs = t_in_epochs\n\t        if self.warmup_t:\n\t            self.warmup_steps = [(v - warmup_lr_init) / self.warmup_t\n\t                                 for v in self.base_values]\n", "            super().update_groups(self.warmup_lr_init)\n\t        else:\n\t            self.warmup_steps = [1 for _ in self.base_values]\n\t    def _get_lr(self, t):\n\t        if t < self.warmup_t:\n\t            lrs = [self.warmup_lr_init + t * s for s in self.warmup_steps]\n\t        else:\n\t            t = t - self.warmup_t\n\t            total_t = self.t_initial - self.warmup_t\n\t            lrs = [\n", "                v - ((v - v * self.lr_min_rate) * (t / total_t))\n\t                for v in self.base_values\n\t            ]\n\t        return lrs\n\t    def get_epoch_values(self, epoch: int):\n\t        if self.t_in_epochs:\n\t            return self._get_lr(epoch)\n\t        else:\n\t            return None\n\t    def get_update_values(self, num_updates: int):\n", "        if not self.t_in_epochs:\n\t            return self._get_lr(num_updates)\n\t        else:\n\t            return None\n"]}
{"filename": "dataset/dataset.py", "chunked_list": ["from functools import lru_cache\n\tfrom typing import Optional\n\timport torch\n\timport torch.utils.data as data\n\tfrom .collator import collator\n\tfrom .dgl_datasets import DGLDatasetLookupTable\n\tfrom .pyg_datasets import PYGDatasetLookupTable\n\tfrom .ogb_datasets import OGBDatasetLookupTable\n\tclass BatchedDataDataset(data.Dataset):\n\t    def __init__(self, dataset, max_nodes=128, multi_hop_max_dist=5, spatial_pos_max=1024):\n", "        super().__init__()\n\t        self.dataset = dataset\n\t        self.max_nodes = max_nodes\n\t        self.multi_hop_max_dist = multi_hop_max_dist\n\t        self.spatial_pos_max = spatial_pos_max\n\t    def __getitem__(self, index):\n\t        item = self.dataset[int(index)]\n\t        return item\n\t    def __len__(self):\n\t        return len(self.dataset)\n", "    def collater(self, samples):\n\t        return collator(\n\t            samples,\n\t            max_node=self.max_nodes,\n\t            multi_hop_max_dist=self.multi_hop_max_dist,\n\t            spatial_pos_max=self.spatial_pos_max,\n\t        )\n\tdef pad_1d_unsqueeze_nan(x, padlen):\n\t    x = x + 1  # pad id = 0\n\t    xlen = x.size(0)\n", "    if xlen < padlen:\n\t        new_x = x.new_zeros([padlen], dtype=x.dtype).float()\n\t        new_x[:] = float('nan')\n\t        new_x[:xlen] = x\n\t        x = new_x\n\t    return x.unsqueeze(0)\n\tclass TargetDataset(data.Dataset):\n\t    def __init__(self, dataset):\n\t        super().__init__()\n\t        self.dataset = dataset\n", "    @lru_cache(maxsize=16)\n\t    def __getitem__(self, index):\n\t        return self.dataset[index].y\n\t    def __len__(self):\n\t        return len(self.dataset)\n\t    def collater(self, samples):\n\t        try:\n\t            return torch.stack(samples, dim=0)\n\t        except: # only for PATTERN and CLUSTER now\n\t            max_node_num = max(sample.size(0) for sample in samples)\n", "            samples = torch.cat([pad_1d_unsqueeze_nan(i, max_node_num) for i in samples])\n\t            samples = samples - 1 # for PATTERN and CLUSTER here\n\t            return samples\n\tclass GraphDataset:\n\t    def __init__(self, dataset_spec: Optional[str] = None,\n\t                 dataset_source: Optional[str] = None, seed: int = 0):\n\t        super().__init__()\n\t        if dataset_source == \"dgl\":\n\t            self.dataset = DGLDatasetLookupTable.GetDGLDataset(dataset_spec, seed=seed)\n\t        elif dataset_source == \"pyg\":\n", "            self.dataset = PYGDatasetLookupTable.GetPYGDataset(dataset_spec, seed=seed)\n\t        elif dataset_source == \"ogb\":\n\t            self.dataset = OGBDatasetLookupTable.GetOGBDataset(dataset_spec, seed=seed)\n\t        else:\n\t            raise ValueError(f\"Unknown dataset source {dataset_source}\")\n\t        self.setup()\n\t    def setup(self):\n\t        self.train_idx = self.dataset.train_idx\n\t        self.valid_idx = self.dataset.valid_idx\n\t        self.test_idx = self.dataset.test_idx\n", "        self.dataset_train = self.dataset.train_data\n\t        self.dataset_val = self.dataset.valid_data\n\t        self.dataset_test = self.dataset.test_data\n"]}
{"filename": "dataset/__init__.py", "chunked_list": ["from .build import build_loader\n\t__all__ = ['build_loader']"]}
{"filename": "dataset/collator.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\timport torch\n\tdef pad_1d_unsqueeze(x, padlen):\n\t    x = x + 1  # pad id = 0\n\t    xlen = x.size(0)\n\t    if xlen < padlen:\n\t        new_x = x.new_zeros([padlen], dtype=x.dtype)\n\t        new_x[:xlen] = x\n\t        x = new_x\n", "    return x.unsqueeze(0)\n\tdef pad_2d_unsqueeze(x, padlen):\n\t    x = x + 1  # pad id = 0\n\t    xlen, xdim = x.size()\n\t    if xlen < padlen:\n\t        new_x = x.new_zeros([padlen, xdim], dtype=x.dtype)\n\t        new_x[:xlen, :] = x\n\t        x = new_x\n\t    return x.unsqueeze(0)\n\tdef pad_attn_bias_unsqueeze(x, padlen):\n", "    xlen = x.size(0)\n\t    if xlen < padlen:\n\t        new_x = x.new_zeros([padlen, padlen], dtype=x.dtype).fill_(float(\"-inf\"))\n\t        new_x[:xlen, :xlen] = x\n\t        new_x[xlen:, :xlen] = 0\n\t        x = new_x\n\t    return x.unsqueeze(0)\n\tdef pad_edge_type_unsqueeze(x, padlen):\n\t    xlen = x.size(0)\n\t    if xlen < padlen:\n", "        new_x = x.new_zeros([padlen, padlen, x.size(-1)], dtype=x.dtype)\n\t        new_x[:xlen, :xlen, :] = x\n\t        x = new_x\n\t    return x.unsqueeze(0)\n\tdef pad_spatial_pos_unsqueeze(x, padlen):\n\t    x = x + 1\n\t    xlen = x.size(0)\n\t    if xlen < padlen:\n\t        new_x = x.new_zeros([padlen, padlen], dtype=x.dtype)\n\t        new_x[:xlen, :xlen] = x\n", "        x = new_x\n\t    return x.unsqueeze(0)\n\tdef pad_3d_unsqueeze(x, padlen1, padlen2, padlen3):\n\t    x = x + 1\n\t    xlen1, xlen2, xlen3, xlen4 = x.size()\n\t    if xlen1 < padlen1 or xlen2 < padlen2 or xlen3 < padlen3:\n\t        new_x = x.new_zeros([padlen1, padlen2, padlen3, xlen4], dtype=x.dtype)\n\t        new_x[:xlen1, :xlen2, :xlen3, :] = x\n\t        x = new_x\n\t    return x.unsqueeze(0)\n", "def pad_1d_unsqueeze_nan(x, padlen):\n\t    x = x + 1  # pad id = 0\n\t    xlen = x.size(0)\n\t    if xlen < padlen:\n\t        new_x = x.new_zeros([padlen], dtype=x.dtype).float()\n\t        new_x[:] = float('nan')\n\t        new_x[:xlen] = x\n\t        x = new_x\n\t    return x.unsqueeze(0)\n\tdef collator(items, max_node=512, multi_hop_max_dist=20, spatial_pos_max=20):\n", "    items = [item for item in items if item is not None and item.x.size(0) <= max_node]\n\t    items = [\n\t        (\n\t            item.idx,\n\t            item.edge_feature,\n\t            item.attn_edge_type,\n\t            item.spatial_pos,\n\t            item.in_degree,\n\t            item.out_degree,\n\t            item.edge_attr,\n", "            item.edge_index,\n\t            item.x,\n\t            item.edge_input[:, :, :multi_hop_max_dist, :],\n\t            item.y,\n\t        )\n\t        for item in items\n\t    ]\n\t    (\n\t        idxs,\n\t        attn_biases,\n", "        attn_edge_types,\n\t        spatial_poses,\n\t        in_degrees,\n\t        out_degrees,\n\t        edge_attrs,\n\t        edge_indexes,\n\t        xs,\n\t        edge_inputs,\n\t        ys,\n\t    ) = zip(*items)\n", "    for idx, _ in enumerate(attn_biases):\n\t        attn_biases[idx][1:, 1:][spatial_poses[idx] >= spatial_pos_max] = float(\"-inf\")\n\t    max_node_num = max(i.size(0) for i in xs)\n\t    max_dist = max(i.size(-2) for i in edge_inputs)\n\t    if ys[0].size(0) == 1:\n\t        y = torch.cat(ys)\n\t    else:\n\t        try:\n\t            max_edge_num = max([y.size(0) for y in ys])\n\t            y = torch.cat([pad_1d_unsqueeze_nan(i, max_edge_num) for i in ys])\n", "        except:\n\t            y = torch.cat([pad_1d_unsqueeze_nan(i, max_node_num) for i in ys])\n\t    x = torch.cat([pad_2d_unsqueeze(i, max_node_num) for i in xs])\n\t    edge_input = torch.cat(\n\t        [pad_3d_unsqueeze(i, max_node_num, max_node_num, max_dist) for i in edge_inputs]\n\t    )\n\t    attn_bias = torch.cat(\n\t        [pad_attn_bias_unsqueeze(i, max_node_num + 1) for i in attn_biases]\n\t    )\n\t    attn_edge_type = torch.cat(\n", "        [pad_edge_type_unsqueeze(i, max_node_num) for i in attn_edge_types]\n\t    )\n\t    spatial_pos = torch.cat(\n\t        [pad_spatial_pos_unsqueeze(i, max_node_num) for i in spatial_poses]\n\t    )\n\t    in_degree = torch.cat([pad_1d_unsqueeze(i, max_node_num) for i in in_degrees])\n\t    # max_edge_num = max([edge_attr.shape[0] for edge_attr in edge_attrs])\n\t    # consider the god node\n\t    # edge_index = torch.cat([pad_2d_unsqueeze(i.transpose(-1, -2), max_edge_num) for i in edge_indexes])\n\t    # edge_index = edge_index.transpose(-1, -2)\n", "    return dict(\n\t        idx=torch.LongTensor(idxs),\n\t        attn_bias=attn_bias,\n\t        attn_edge_type=attn_edge_type,\n\t        spatial_pos=spatial_pos,\n\t        in_degree=in_degree,\n\t        out_degree=in_degree,  # for undirected graph\n\t        # edge_index=torch.LongTensor(edge_index),\n\t        x=x.long(),\n\t        edge_input=edge_input,\n", "        y=y,\n\t    )\n"]}
{"filename": "dataset/wrapper.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\timport torch\n\timport numpy as np\n\timport torch_geometric\n\tfrom ogb.graphproppred import PygGraphPropPredDataset\n\tfrom ogb.lsc.pcqm4mv2_pyg import PygPCQM4Mv2Dataset\n\tfrom functools import lru_cache\n\timport pyximport\n\timport torch.distributed as dist\n", "pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n\tfrom . import algos\n\t@torch.jit.script\n\tdef convert_to_single_emb(x, offset: int = 512):\n\t    feature_num = x.size(1) if len(x.size()) > 1 else 1\n\t    feature_offset = 1 + torch.arange(0, feature_num * offset, offset, dtype=torch.long)\n\t    x = x + feature_offset\n\t    return x\n\tdef preprocess_item(item):\n\t    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n", "    N = x.size(0)\n\t    x = convert_to_single_emb(x)\n\t    # node adj matrix [N, N] bool\n\t    adj = torch.zeros([N, N], dtype=torch.bool)\n\t    adj[edge_index[0, :], edge_index[1, :]] = True\n\t    # edge feature here\n\t    if edge_attr is None:\n\t        edge_attr = torch.zeros(edge_index.size(1), 3).long()\n\t    if len(edge_attr.size()) == 1:\n\t        edge_attr = edge_attr[:, None]\n", "    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n\t    attn_edge_type[edge_index[0, :], edge_index[1, :]] = (\n\t        convert_to_single_emb(edge_attr) + 1\n\t    )\n\t    shortest_path_result, path = algos.floyd_warshall(adj.numpy())\n\t    max_dist = np.amax(shortest_path_result)\n\t    edge_input = algos.gen_edge_input(max_dist, path, attn_edge_type.numpy())\n\t    spatial_pos = torch.from_numpy((shortest_path_result)).long()\n\t    attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)  # with graph token\n\t    # combine\n", "    item.x = x\n\t    item.edge_feature = attn_bias\n\t    item.attn_edge_type = attn_edge_type\n\t    item.spatial_pos = spatial_pos\n\t    item.in_degree = adj.long().sum(dim=1).view(-1)\n\t    item.out_degree = item.in_degree  # for undirected graph\n\t    item.edge_input = torch.from_numpy(edge_input).long()\n\t    return item\n\tdef preprocess_item_tsp(item):\n\t    edge_attr, edge_index, x = item.edge_attr, item.edge_index, item.x\n", "    N = edge_index.max() + 1\n\t    x = torch.zeros(N, 1)\n\t    x = convert_to_single_emb(x)\n\t    # node adj matrix [N, N] bool\n\t    adj = torch.zeros([N, N], dtype=torch.bool)\n\t    adj[edge_index[0, :], edge_index[1, :]] = True\n\t    # edge feature here\n\t    if edge_attr is None:\n\t        edge_attr = torch.zeros(edge_index.size(1), 3).long()\n\t    if len(edge_attr.size()) == 1:\n", "        edge_attr = edge_attr[:, None]\n\t    attn_edge_type = torch.zeros([N, N, edge_attr.size(-1)], dtype=torch.long)\n\t    attn_edge_type[edge_index[0, :], edge_index[1, :]] = (\n\t        (convert_to_single_emb(edge_attr) + 1).long()\n\t    )\n\t    shortest_path_result, path = algos.floyd_warshall(adj.numpy())\n\t    max_dist = np.amax(shortest_path_result)\n\t    edge_input = algos.gen_edge_input(max_dist, path, attn_edge_type.numpy())\n\t    spatial_pos = torch.from_numpy((shortest_path_result)).long()\n\t    attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)  # with graph token\n", "    # combine\n\t    item.x = x\n\t    item.edge_feature = attn_bias\n\t    item.attn_edge_type = attn_edge_type\n\t    item.spatial_pos = spatial_pos\n\t    item.in_degree = adj.long().sum(dim=1).view(-1)\n\t    item.out_degree = item.in_degree  # for undirected graph\n\t    item.edge_input = torch.from_numpy(edge_input).long()\n\t    return item\n\tclass MyPygPCQM4MDataset(PygPCQM4Mv2Dataset):\n", "    def download(self):\n\t        super(MyPygPCQM4MDataset, self).download()\n\t    def process(self):\n\t        super(MyPygPCQM4MDataset, self).process()\n\t    @lru_cache(maxsize=16)\n\t    def __getitem__(self, idx):\n\t        item = self.get(self.indices()[idx])\n\t        item.idx = idx\n\t        return preprocess_item(item)\n\tclass MyPygGraphPropPredDataset(PygGraphPropPredDataset):\n", "    def download(self):\n\t        if dist.get_rank() == 0:\n\t            super(MyPygGraphPropPredDataset, self).download()\n\t        dist.barrier()\n\t    def process(self):\n\t        if dist.get_rank() == 0:\n\t            super(MyPygGraphPropPredDataset, self).process()\n\t        dist.barrier()\n\t    @lru_cache(maxsize=16)\n\t    def __getitem__(self, idx):\n", "        item = self.get(self.indices()[idx])\n\t        item.idx = idx\n\t        item.y = item.y.reshape(-1)\n\t        return preprocess_item(item)\n"]}
{"filename": "dataset/build.py", "chunked_list": ["import torch\n\timport torch.distributed as dist\n\tfrom .dataset import GraphDataset\n\tfrom .dataset import BatchedDataDataset\n\tdef build_loader(config):\n\t    config.defrost()\n\t    dataset = GraphDataset(dataset_spec=config.DATA.DATASET, dataset_source=config.DATA.SOURCE)\n\t    config.freeze()\n\t    dataset_train = BatchedDataDataset(dataset.dataset_train, config.DATA.TRAIN_MAX_NODES,\n\t                                       config.DATA.MULTI_HOP_MAX_DIST, config.DATA.SPATIAL_POS_MAX)\n", "    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()}\"\n\t          f\"successfully build train dataset, with #samples: {len(dataset_train)}\")\n\t    dataset_val = BatchedDataDataset(dataset.dataset_val, config.DATA.INFER_MAX_NODES,\n\t                                      config.DATA.MULTI_HOP_MAX_DIST, config.DATA.SPATIAL_POS_MAX)\n\t    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()}\"\n\t          f\"successfully build val dataset, with #samples: {len(dataset_val)}\")\n\t    dataset_test = BatchedDataDataset(dataset.dataset_test, config.DATA.INFER_MAX_NODES,\n\t                                      config.DATA.MULTI_HOP_MAX_DIST, config.DATA.SPATIAL_POS_MAX)\n\t    print(f\"local rank {config.LOCAL_RANK} / global rank {dist.get_rank()}\"\n\t          f\"successfully build test dataset, with #samples: {len(dataset_test)}\")\n", "    num_tasks = dist.get_world_size()\n\t    global_rank = dist.get_rank()\n\t    if dataset_train is not None:\n\t        sampler_train = torch.utils.data.DistributedSampler(\n\t            dataset_train,\n\t            num_replicas=num_tasks,\n\t            rank=global_rank,\n\t            shuffle=True)\n\t    if dataset_val is not None:\n\t        if config.TEST.SEQUENTIAL:\n", "            sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n\t        else:\n\t            sampler_val = torch.utils.data.distributed.DistributedSampler(\n\t                dataset_val, shuffle=False)\n\t    if dataset_test is not None:\n\t        if config.TEST.SEQUENTIAL:\n\t            sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n\t        else:\n\t            sampler_test = torch.utils.data.distributed.DistributedSampler(\n\t                dataset_test, shuffle=False)\n", "    data_loader_train = torch.utils.data.DataLoader(\n\t        dataset_train,\n\t        collate_fn=dataset_train.collater,\n\t        sampler=sampler_train,\n\t        batch_size=config.DATA.BATCH_SIZE,\n\t        num_workers=config.DATA.NUM_WORKERS,\n\t        pin_memory=config.DATA.PIN_MEMORY,\n\t        drop_last=True,\n\t        persistent_workers=True) if dataset_train is not None else None\n\t    data_loader_val = torch.utils.data.DataLoader(\n", "        dataset_val,\n\t        collate_fn=dataset_val.collater,\n\t        sampler=sampler_val,\n\t        batch_size=config.DATA.BATCH_SIZE,\n\t        shuffle=False,\n\t        num_workers=config.DATA.NUM_WORKERS,\n\t        pin_memory=config.DATA.PIN_MEMORY,\n\t        drop_last=False,\n\t        persistent_workers=True) if dataset_val is not None else None\n\t    data_loader_test = torch.utils.data.DataLoader(\n", "        dataset_test,\n\t        collate_fn=dataset_test.collater,\n\t        sampler=sampler_test,\n\t        batch_size=config.DATA.BATCH_SIZE,\n\t        shuffle=False,\n\t        num_workers=config.DATA.NUM_WORKERS,\n\t        pin_memory=config.DATA.PIN_MEMORY,\n\t        drop_last=False,\n\t        persistent_workers=True) if dataset_test is not None else None\n\t    return dataset_train, dataset_val, dataset_test, data_loader_train, \\\n", "        data_loader_val, data_loader_test\n"]}
{"filename": "dataset/dgl_datasets/dgl_dataset.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\timport torch\n\timport numpy as np\n\tfrom copy import copy\n\tfrom typing import List\n\tfrom typing import Optional, Tuple\n\tfrom dgl import DGLGraph\n\tfrom dgl.data import DGLDataset\n\tfrom torch_geometric.data import Dataset\n", "from sklearn.model_selection import train_test_split\n\tfrom torch_geometric.data import Data as PYGGraph\n\tfrom ..wrapper import convert_to_single_emb\n\tfrom .. import algos\n\tclass DGLDataset(Dataset):\n\t    def __init__(self,\n\t        dataset: DGLDataset,\n\t        seed: int = 0,\n\t        train_idx=None,\n\t        valid_idx=None,\n", "        test_idx=None,\n\t    ):\n\t        self.dataset = dataset\n\t        num_data = len(self.dataset)\n\t        self.seed = seed\n\t        if train_idx is None:\n\t            train_valid_idx, test_idx = train_test_split(\n\t                np.arange(num_data), test_size=num_data // 10, random_state=seed\n\t            )\n\t            train_idx, valid_idx = train_test_split(\n", "                train_valid_idx, test_size=num_data // 5, random_state=seed\n\t            )\n\t        self.train_idx = train_idx\n\t        self.valid_idx = valid_idx\n\t        self.test_idx = test_idx\n\t        self.__indices__ = None\n\t        self.train_data = self.index_select(train_idx)\n\t        self.valid_data = self.index_select(valid_idx)\n\t        self.test_data = self.index_select(test_idx)\n\t    def index_select(self, indices: List[int]):\n", "        subset = copy(self)\n\t        subset.__indices__ = indices\n\t        subset.train_idx = None\n\t        subset.valid_idx = None\n\t        subset.test_idx = None\n\t        subset.train_data = None\n\t        subset.valid_data = None\n\t        subset.test_data = None\n\t        return subset\n\t    def __extract_edge_and_node_features(\n", "        self, graph_data: DGLGraph\n\t    ) -> Tuple[\n\t        Optional[torch.Tensor],\n\t        Optional[torch.Tensor],\n\t        Optional[torch.Tensor],\n\t        Optional[torch.Tensor],\n\t    ]:\n\t        def extract_tensor_from_node_or_edge_data(\n\t            feature_dict: dict, num_nodes_or_edges\n\t        ):\n", "            float_feature_list = []\n\t            int_feature_list = []\n\t            def extract_tensor_from_dict(feature: torch.Tensor):\n\t                if feature.dtype == torch.int32 or feature.dtype == torch.long:\n\t                    int_feature_list.append(feature.unsqueeze(1))\n\t                elif feature.dtype == torch.float32 or feature.dtype == torch.float64:\n\t                    float_feature_list.append(feature.unsqueeze(1))\n\t            for feature_or_dict in feature_dict:\n\t                if isinstance(feature_or_dict, torch.Tensor):\n\t                    extract_tensor_from_dict(feature_or_dict)\n", "                elif isinstance(feature_or_dict, dict):\n\t                    for feature in feature_or_dict:\n\t                        extract_tensor_from_dict(feature)\n\t            int_feature_tensor = (\n\t                torch.from_numpy(np.zeros(shape=[num_nodes_or_edges, 1])).long()\n\t                if len(int_feature_list) == 0\n\t                else torch.cat(int_feature_list)\n\t            )\n\t            float_feature_tensor = (\n\t                None if len(float_feature_list) == 0 else torch.cat(float_feature_list)\n", "            )\n\t            return int_feature_tensor, float_feature_tensor\n\t        node_int_feature, node_float_feature = extract_tensor_from_node_or_edge_data(\n\t            graph_data.ndata, graph_data.num_nodes()\n\t        )\n\t        edge_int_feature, edge_float_feature = extract_tensor_from_node_or_edge_data(\n\t            graph_data.edata, graph_data.num_edges()\n\t        )\n\t        return (\n\t            node_int_feature,\n", "            node_float_feature,\n\t            edge_int_feature,\n\t            edge_float_feature,\n\t        )\n\t    def __preprocess_dgl_graph(\n\t        self, graph_data: DGLGraph, y: torch.Tensor, idx: int\n\t    ) -> PYGGraph:\n\t        if not graph_data.is_homogeneous:\n\t            raise ValueError(\n\t                \"Heterogeneous DGLGraph is found. Only homogeneous graph is supported.\"\n", "            )\n\t        N = graph_data.num_nodes()\n\t        (\n\t            node_int_feature,\n\t            node_float_feature,\n\t            edge_int_feature,\n\t            edge_float_feature,\n\t        ) = self.__extract_edge_and_node_features(graph_data)\n\t        edge_index = graph_data.edges()\n\t        attn_edge_type = torch.zeros(\n", "            [N, N, edge_int_feature.shape[1]], dtype=torch.long\n\t        )\n\t        attn_edge_type[\n\t            edge_index[0].long(), edge_index[1].long()\n\t        ] = convert_to_single_emb(edge_int_feature)\n\t        dense_adj = graph_data.adj().to_dense().type(torch.int)\n\t        shortest_path_result, path = algos.floyd_warshall(dense_adj.numpy())\n\t        max_dist = np.amax(shortest_path_result)\n\t        edge_input = algos.gen_edge_input(max_dist, path, attn_edge_type.numpy())\n\t        spatial_pos = torch.from_numpy((shortest_path_result)).long()\n", "        attn_bias = torch.zeros([N + 1, N + 1], dtype=torch.float)  # with graph token\n\t        pyg_graph = PYGGraph()\n\t        pyg_graph.x = convert_to_single_emb(node_int_feature)\n\t        pyg_graph.adj = dense_adj\n\t        pyg_graph.attn_bias = attn_bias\n\t        pyg_graph.attn_edge_type = attn_edge_type\n\t        pyg_graph.spatial_pos = spatial_pos\n\t        pyg_graph.in_degree = dense_adj.long().sum(dim=1).view(-1)\n\t        pyg_graph.out_degree = pyg_graph.in_degree\n\t        pyg_graph.edge_input = torch.from_numpy(edge_input).long()\n", "        if y.dim() == 0:\n\t            y = y.unsqueeze(-1)\n\t        pyg_graph.y = y\n\t        pyg_graph.idx = idx\n\t        return pyg_graph\n\t    def __getitem__(self, idx):\n\t        if isinstance(idx, int):\n\t            if self.__indices__ is not None:\n\t                idx = self.__indices__[idx]\n\t            graph, y = self.dataset[idx]\n", "            return self.__preprocess_dgl_graph(graph, y, idx)\n\t        else:\n\t            raise TypeError(\"index to a DGLDataset can only be an integer.\")\n\t    def __len__(self) -> int:\n\t        return len(self.dataset) if self.__indices__ is None else len(self.__indices__)\n"]}
{"filename": "dataset/dgl_datasets/dgl_dataset_lookup_table.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\tfrom typing import Optional\n\tfrom dgl.data import (\n\t    QM7bDataset,\n\t    QM9Dataset,\n\t    QM9EdgeDataset,\n\t    MiniGCDataset,\n\t    TUDataset,\n\t    GINDataset,\n", "    FakeNewsDataset,\n\t)\n\timport torch.distributed as dist\n\tfrom .dgl_dataset import DGLDataset\n\tclass MyQM7bDataset(QM7bDataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyQM7bDataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n", "class MyQM9Dataset(QM9Dataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyQM9Dataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyQM9EdgeDataset(QM9EdgeDataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyQM9EdgeDataset, self).download()\n", "        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyMiniGCDataset(MiniGCDataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyMiniGCDataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyTUDataset(TUDataset):\n\t    def download(self):\n", "        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyTUDataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyGINDataset(GINDataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyGINDataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n", "class MyFakeNewsDataset(FakeNewsDataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyFakeNewsDataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass DGLDatasetLookupTable:\n\t    @staticmethod\n\t    def GetDGLDataset(dataset_name: str, seed: int) -> Optional[DGLDataset]:\n\t        params = dataset_name.split(\":\")[-1].split(\",\")\n", "        inner_dataset = None\n\t        if dataset_name == \"qm7b\":\n\t            inner_dataset = MyQM7bDataset()\n\t        elif dataset_name.startswith(\"qm9\"):\n\t            label_keys = None\n\t            cutoff = 5.0\n\t            for param in params:\n\t                name, value = param.split(\"=\")\n\t                if name == \"label_keys\":\n\t                    label_keys = value.split(\"+\")\n", "                elif name == \"cutoff\":\n\t                    cutoff = float(value)\n\t            inner_dataset = MyQM9Dataset(label_keys=label_keys, cutoff=cutoff)\n\t        elif dataset_name.startswith(\"qm9edge\"):\n\t            label_keys = None\n\t            for param in params:\n\t                name, value = param.split(\"=\")\n\t                if name == \"label_keys\":\n\t                    label_keys = value.split(\"+\")\n\t            inner_dataset = MyQM9EdgeDataset(label_keys=label_keys)\n", "        elif dataset_name.startswith(\"minigc\"):\n\t            num_graphs = None\n\t            min_num_v = None\n\t            max_num_v = None\n\t            data_seed = seed\n\t            for param in params:\n\t                name, value = param.split(\"=\")\n\t                if name == \"num_graphs\":\n\t                    num_graphs = int(value)\n\t                elif name == \"min_num_v\":\n", "                    min_num_v = int(value)\n\t                elif name == \"max_num_v\":\n\t                    max_num_v = int(value)\n\t                elif name == \"seed\":\n\t                    data_seed = int(value)\n\t            inner_dataset = MyMiniGCDataset(\n\t                num_graphs, min_num_v, max_num_v, seed=data_seed\n\t            )\n\t        elif dataset_name.startswith(\"tu\"):\n\t            nm = None\n", "            for param in params:\n\t                name, value = param.split(\"=\")\n\t                if name == \"name\":\n\t                    nm = value\n\t            inner_dataset = MyTUDataset(name=nm)\n\t        elif dataset_name.startswith(\"gin\"):\n\t            nm = None\n\t            self_loop = None\n\t            degree_as_nlabel = False\n\t            for param in params:\n", "                name, value = param.split(\"=\")\n\t                if name == \"name\":\n\t                    nm = value\n\t                elif name == \"self_loop\":\n\t                    if value.lower() == \"false\":\n\t                        self_loop = False\n\t                    elif value.lower() == \"true\":\n\t                        self_loop = True\n\t                elif name == \"degree_as_nlabel\":\n\t                    if value.lower() == \"false\":\n", "                        degree_as_nlabel = False\n\t                    elif value.lower() == \"true\":\n\t                        degree_as_nlabel = True\n\t            inner_dataset = MyGINDataset(\n\t                name=nm, self_loop=self_loop, degree_as_nlabel=degree_as_nlabel\n\t            )\n\t        elif dataset_name.startswith(\"fakenews\"):\n\t            nm = None\n\t            feature_name = None\n\t            for param in params:\n", "                name, value = param.split(\"=\")\n\t                if name == \"name\":\n\t                    nm = value\n\t                elif name == \"feature_name\":\n\t                    feature_name = value\n\t            inner_dataset = MyFakeNewsDataset(name=nm, feature_name=feature_name)\n\t        else:\n\t            raise ValueError(f\"Unknown dataset specificaion {dataset_name}\")\n\t        return (\n\t            None\n", "            if inner_dataset is None\n\t            else DGLDataset(inner_dataset, seed)\n\t        )\n"]}
{"filename": "dataset/dgl_datasets/__init__.py", "chunked_list": ["from .dgl_dataset_lookup_table import DGLDatasetLookupTable\n\tfrom .dgl_dataset import DGLDataset\n\t__all__ = ['DGLDataset', 'DGLDatasetLookupTable']"]}
{"filename": "dataset/pyg_datasets/pyg_dataset.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\timport copy\n\timport torch\n\timport numpy as np\n\tfrom functools import lru_cache\n\tfrom torch_geometric.data import Dataset\n\tfrom sklearn.model_selection import train_test_split\n\tfrom ..wrapper import preprocess_item\n\tfrom ..wrapper import preprocess_item_tsp\n", "class PYGDataset(Dataset):\n\t    def __init__(\n\t        self,\n\t        dataset: Dataset,\n\t        seed: int = 0,\n\t        train_idx=None,\n\t        valid_idx=None,\n\t        test_idx=None,\n\t        train_set=None,\n\t        valid_set=None,\n", "        test_set=None,\n\t    ):\n\t        self.dataset = dataset\n\t        if self.dataset is not None:\n\t            self.num_data = len(self.dataset)\n\t        self.seed = seed\n\t        if train_idx is None and train_set is None:\n\t            train_valid_idx, test_idx = train_test_split(\n\t                np.arange(self.num_data),\n\t                test_size=self.num_data // 10,\n", "                random_state=seed,\n\t            )\n\t            train_idx, valid_idx = train_test_split(\n\t                train_valid_idx, test_size=self.num_data // 5, random_state=seed\n\t            )\n\t            self.train_idx = torch.from_numpy(train_idx)\n\t            self.valid_idx = torch.from_numpy(valid_idx)\n\t            self.test_idx = torch.from_numpy(test_idx)\n\t            self.train_data = self.index_select(self.train_idx)\n\t            self.valid_data = self.index_select(self.valid_idx)\n", "            self.test_data = self.index_select(self.test_idx)\n\t        elif train_set is not None:\n\t            self.num_data = len(train_set) + len(valid_set) + len(test_set)\n\t            self.train_data = self.create_subset(train_set)\n\t            self.valid_data = self.create_subset(valid_set)\n\t            self.test_data = self.create_subset(test_set)\n\t            self.train_idx = None\n\t            self.valid_idx = None\n\t            self.test_idx = None\n\t        else:\n", "            self.num_data = len(train_idx) + len(valid_idx) + len(test_idx)\n\t            self.train_idx = train_idx\n\t            self.valid_idx = valid_idx\n\t            self.test_idx = test_idx\n\t            self.train_data = self.index_select(self.train_idx)\n\t            self.valid_data = self.index_select(self.valid_idx)\n\t            self.test_data = self.index_select(self.test_idx)\n\t        self.__indices__ = None\n\t    def index_select(self, idx):\n\t        dataset = copy.copy(self)\n", "        dataset.dataset = self.dataset.index_select(idx)\n\t        if isinstance(idx, torch.Tensor):\n\t            dataset.num_data = idx.size(0)\n\t        else:\n\t            dataset.num_data = idx.shape[0]\n\t        dataset.__indices__ = idx\n\t        dataset.train_data = None\n\t        dataset.valid_data = None\n\t        dataset.test_data = None\n\t        dataset.train_idx = None\n", "        dataset.valid_idx = None\n\t        dataset.test_idx = None\n\t        return dataset\n\t    def create_subset(self, subset):\n\t        dataset = copy.copy(self)\n\t        dataset.dataset = subset\n\t        dataset.num_data = len(subset)\n\t        dataset.__indices__ = None\n\t        dataset.train_data = None\n\t        dataset.valid_data = None\n", "        dataset.test_data = None\n\t        dataset.train_idx = None\n\t        dataset.valid_idx = None\n\t        dataset.test_idx = None\n\t        return dataset\n\t    @lru_cache(maxsize=16)\n\t    def __getitem__(self, idx):\n\t        if isinstance(idx, int):\n\t            item = self.dataset[idx]\n\t            item.idx = idx\n", "            item.y = item.y.reshape(-1)\n\t            if item.x is not None:\n\t                return preprocess_item(item)\n\t            else:\n\t                return preprocess_item_tsp(item)\n\t        else:\n\t            raise TypeError(\"index to a PYGDataset can only be an integer.\")\n\t    def __len__(self):\n\t        return self.num_data\n"]}
{"filename": "dataset/pyg_datasets/__init__.py", "chunked_list": ["from .pyg_dataset_lookup_table import PYGDatasetLookupTable\n\tfrom .pyg_dataset import PYGDataset\n\t__all__ = ['PYGDatasetLookupTable', 'PYGDataset']"]}
{"filename": "dataset/pyg_datasets/pyg_dataset_lookup_table.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\tfrom typing import Optional\n\tfrom torch_geometric.datasets import *\n\tfrom torch_geometric.data import Dataset\n\timport torch.distributed as dist\n\tfrom .pyg_dataset import PYGDataset\n\tclass MyQM7b(QM7b):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n", "            super(MyQM7b, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyQM7b, self).process()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyQM9(QM9):\n\t    def download(self):\n", "        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyQM9, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyQM9, self).process()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyZINC(ZINC):\n", "    def __init__(self, subset=True, **kwargs):\n\t        super(MyZINC, self).__init__(subset=subset, **kwargs)\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyZINC, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyZINC, self).process()\n", "        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyMoleculeNet(MoleculeNet):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyMoleculeNet, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n", "            super(MyMoleculeNet, self).process()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyTSP(GNNBenchmarkDataset):\n\t    def __init__(self, **kwargs):\n\t        super(MyTSP, self).__init__(name=\"TSP\", **kwargs)\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyTSP, self).download()\n\t        if dist.is_initialized():\n", "            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyTSP, self).process()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyPattern(GNNBenchmarkDataset):\n\t    def __init__(self, **kwargs):\n\t        super(MyPattern, self).__init__(name=\"PATTERN\", **kwargs)\n\t    def download(self):\n", "        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPattern, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPattern, self).process()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyCluster(GNNBenchmarkDataset):\n", "    def __init__(self, **kwargs):\n\t        super(MyCluster, self).__init__(name=\"CLUSTER\", **kwargs)\n\t        self.url = 'https://github.com/czczup/storage/releases/download/v0.1.0/'\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyCluster, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n", "            super(MyCluster, self).process()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass PYGDatasetLookupTable:\n\t    @staticmethod\n\t    def GetPYGDataset(dataset_spec: str, seed: int) -> Optional[Dataset]:\n\t        split_result = dataset_spec.split(\":\")\n\t        if len(split_result) == 2:\n\t            name, params = split_result[0], split_result[1]\n\t            params = params.split(\",\")\n", "        elif len(split_result) == 1:\n\t            name = dataset_spec\n\t            params = []\n\t        inner_dataset = None\n\t        num_class = 1\n\t        train_set = None\n\t        valid_set = None\n\t        test_set = None\n\t        root = \"data\"\n\t        if name == \"qm7b\":\n", "            inner_dataset = MyQM7b(root=root)\n\t        elif name == \"qm9\":\n\t            inner_dataset = MyQM9(root=root)\n\t        elif name == \"zinc-subset\":\n\t            inner_dataset = MyZINC(root=root, subset=True)\n\t            train_set = MyZINC(root=root, split=\"train\", subset=True)\n\t            valid_set = MyZINC(root=root, split=\"val\", subset=True)\n\t            test_set = MyZINC(root=root, split=\"test\", subset=True)\n\t        elif name == \"zinc-full\":\n\t            inner_dataset = MyZINC(root=root, subset=False)\n", "            train_set = MyZINC(root=root, split=\"train\", subset=False)\n\t            valid_set = MyZINC(root=root, split=\"val\", subset=False)\n\t            test_set = MyZINC(root=root, split=\"test\", subset=False)\n\t        elif name == \"pattern\":\n\t            inner_dataset = MyPattern(root=root)\n\t            train_set = MyPattern(root=root, split=\"train\")\n\t            valid_set = MyPattern(root=root, split=\"val\")\n\t            test_set = MyPattern(root=root, split=\"test\")\n\t        elif name == \"cluster\":\n\t            inner_dataset = MyCluster(root=root)\n", "            train_set = MyCluster(root=root, split=\"train\")\n\t            valid_set = MyCluster(root=root, split=\"val\")\n\t            test_set = MyCluster(root=root, split=\"test\")\n\t        elif name == \"tsp\":\n\t            inner_dataset = MyTSP(root=root)\n\t            train_set = MyTSP(root=root, split=\"train\")\n\t            valid_set = MyTSP(root=root, split=\"val\")\n\t            test_set = MyTSP(root=root, split=\"test\")\n\t        elif name == \"moleculenet\":\n\t            nm = None\n", "            for param in params:\n\t                name, value = param.split(\"=\")\n\t                if name == \"name\":\n\t                    nm = value\n\t            inner_dataset = MyMoleculeNet(root=root, name=nm)\n\t        else:\n\t            raise ValueError(f\"Unknown dataset name {name} for pyg source.\")\n\t        if train_set is not None:\n\t            return PYGDataset(\n\t                    None,\n", "                    seed,\n\t                    None,\n\t                    None,\n\t                    None,\n\t                    train_set,\n\t                    valid_set,\n\t                    test_set,\n\t                )\n\t        else:\n\t            return (\n", "                None\n\t                if inner_dataset is None\n\t                else PYGDataset(inner_dataset, seed)\n\t            )\n"]}
{"filename": "dataset/smiles/smiles_dataset.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\timport torch\n\timport numpy as np\n\tfrom sklearn.model_selection import train_test_split\n\tfrom ogb.utils.mol import smiles2graph\n\tfrom .. import algos\n\tfrom ..wrapper import preprocess_item\n\tfrom ..pyg_datasets import PYGDataset\n\tclass SMILESDataset(PYGDataset):\n", "    def __init__(\n\t        self,\n\t        dataset: str,\n\t        num_class: int,\n\t        max_node: int,\n\t        multi_hop_max_dist: int,\n\t        spatial_pos_max: int,\n\t    ):\n\t        self.dataset = np.genfromtxt(dataset, delimiter=\",\", dtype=str)\n\t        num_data = len(self.dataset)\n", "        self.num_class = num_class\n\t        self.__get_graph_metainfo(max_node, multi_hop_max_dist, spatial_pos_max)\n\t        train_valid_idx, test_idx = train_test_split(num_data // 10)\n\t        train_idx, valid_idx = train_test_split(train_valid_idx, num_data // 5)\n\t        self.train_idx = train_idx\n\t        self.valid_idx = valid_idx\n\t        self.test_idx = test_idx\n\t        self.__indices__ = None\n\t        self.train_data = self.index_select(train_idx)\n\t        self.valid_data = self.index_select(valid_idx)\n", "        self.test_data = self.index_select(test_idx)\n\t    def __get_graph_metainfo(\n\t        self, max_node: int, multi_hop_max_dist: int, spatial_pos_max: int\n\t    ):\n\t        self.max_node = min(\n\t            max_node,\n\t            torch.max(self.dataset[i][0].num_nodes() for i in range(len(self.dataset))),\n\t        )\n\t        max_dist = 0\n\t        for i in range(len(self.dataset)):\n", "            pyg_graph = smiles2graph(self.dataset[i])\n\t            dense_adj = pyg_graph.adj().to_dense().type(torch.int)\n\t            shortest_path_result, _ = algos.floyd_warshall(dense_adj.numpy())\n\t            max_dist = max(max_dist, np.amax(shortest_path_result))\n\t        self.multi_hop_max_dist = min(multi_hop_max_dist, max_dist)\n\t        self.spatial_pos_max = min(spatial_pos_max, max_dist)\n\t    def __getitem__(self, idx):\n\t        if isinstance(idx, int):\n\t            item = smiles2graph(self.dataset[idx])\n\t            item.idx = idx\n", "            return preprocess_item(item)\n\t        else:\n\t            raise TypeError(\"index to a PYGDataset can only be an integer.\")\n"]}
{"filename": "dataset/ogb_datasets/__init__.py", "chunked_list": ["from .ogb_dataset_lookup_table import OGBDatasetLookupTable\n\t__all__ = ['OGBDatasetLookupTable']"]}
{"filename": "dataset/ogb_datasets/ogb_dataset_lookup_table.py", "chunked_list": ["# Copyright (c) Microsoft Corporation.\n\t# Licensed under the MIT License.\n\tfrom typing import Optional\n\tfrom ogb.lsc.pcqm4mv2_pyg import PygPCQM4Mv2Dataset\n\tfrom ogb.lsc.pcqm4m_pyg import PygPCQM4MDataset\n\tfrom ogb.graphproppred import PygGraphPropPredDataset\n\tfrom torch_geometric.data import Dataset\n\tfrom ..pyg_datasets import PYGDataset\n\timport torch.distributed as dist\n\timport os\n", "class MyPygPCQM4Mv2Dataset(PygPCQM4Mv2Dataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPygPCQM4Mv2Dataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPygPCQM4Mv2Dataset, self).process()\n\t        if dist.is_initialized():\n", "            dist.barrier()\n\tclass MyPygPCQM4MDataset(PygPCQM4MDataset):\n\t    def download(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPygPCQM4MDataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPygPCQM4MDataset, self).process()\n", "        if dist.is_initialized():\n\t            dist.barrier()\n\tclass MyPygGraphPropPredDataset(PygGraphPropPredDataset):\n\t    def download(self):\n\t        # self.meta_info['url'] = 'https://github.com/czczup/storage/releases/download/v0.1.0/pcba.zip'\n\t        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPygGraphPropPredDataset, self).download()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def process(self):\n", "        if not dist.is_initialized() or dist.get_rank() == 0:\n\t            super(MyPygGraphPropPredDataset, self).process()\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\tclass OGBDatasetLookupTable:\n\t    @staticmethod\n\t    def GetOGBDataset(dataset_name: str, seed: int) -> Optional[Dataset]:\n\t        inner_dataset = None\n\t        train_idx = None\n\t        valid_idx = None\n", "        test_idx = None\n\t        if dataset_name == \"ogbg-molhiv\":\n\t            folder_name = dataset_name.replace(\"-\", \"_\")\n\t            os.system(f\"mkdir -p data/{folder_name}/\")\n\t            os.system(f\"touch data/{folder_name}/RELEASE_v1.txt\")\n\t            inner_dataset = MyPygGraphPropPredDataset(dataset_name, root='data')\n\t            idx_split = inner_dataset.get_idx_split()\n\t            train_idx = idx_split[\"train\"]\n\t            valid_idx = idx_split[\"valid\"]\n\t            test_idx = idx_split[\"test\"]\n", "        elif dataset_name == \"ogbg-molpcba\":\n\t            folder_name = dataset_name.replace(\"-\", \"_\")\n\t            os.system(f\"mkdir -p data/{folder_name}/\")\n\t            os.system(f\"touch data/{folder_name}/RELEASE_v1.txt\")\n\t            inner_dataset = MyPygGraphPropPredDataset(dataset_name, root='data')\n\t            idx_split = inner_dataset.get_idx_split()\n\t            train_idx = idx_split[\"train\"]\n\t            valid_idx = idx_split[\"valid\"]\n\t            test_idx = idx_split[\"test\"]\n\t        elif dataset_name == \"pcqm4mv2\":\n", "            os.system(\"mkdir -p data/pcqm4m-v2/\")\n\t            os.system(\"touch data/pcqm4m-v2/RELEASE_v1.txt\")\n\t            inner_dataset = MyPygPCQM4Mv2Dataset(root='data')\n\t            idx_split = inner_dataset.get_idx_split()\n\t            train_idx = idx_split[\"train\"]\n\t            valid_idx = idx_split[\"valid\"]\n\t            test_idx = idx_split[\"test-dev\"]\n\t        elif dataset_name == \"pcqm4m\":\n\t            os.system(\"mkdir -p data/pcqm4m_kddcup2021/\")\n\t            os.system(\"touch data/pcqm4m_kddcup2021/RELEASE_v1.txt\")\n", "            inner_dataset = MyPygPCQM4MDataset(root='data')\n\t            idx_split = inner_dataset.get_idx_split()\n\t            train_idx = idx_split[\"train\"]\n\t            valid_idx = idx_split[\"valid\"]\n\t            test_idx = idx_split[\"test\"]\n\t        else:\n\t            raise ValueError(f\"Unknown dataset name {dataset_name} for ogb source.\")\n\t        return (\n\t            None\n\t            if inner_dataset is None\n", "            else PYGDataset(\n\t                inner_dataset, seed, train_idx, valid_idx, test_idx\n\t            )\n\t        )\n"]}
{"filename": "models/losses.py", "chunked_list": ["import torch.nn.functional as F\n\timport torch.nn as nn\n\timport torch\n\tclass DiceLoss(nn.Module):\n\t    def __init__(self, reduce_zero_label=False):\n\t        super(DiceLoss, self).__init__()\n\t        print(\"reduce_zero_label:\", reduce_zero_label)\n\t        self.reduce_zero_label = reduce_zero_label\n\t    def forward(self, input, target, reduce=True):\n\t        input = torch.sigmoid(input)\n", "        input = input.reshape(-1)\n\t        target = target.reshape(-1).float()\n\t        mask = ~torch.isnan(target)\n\t        if self.reduce_zero_label:\n\t            target = target - 1  # start from zero\n\t        input = input[mask]\n\t        target = target[mask]\n\t        a = torch.sum(input * target)\n\t        b = torch.sum(input * input) + 0.001\n\t        c = torch.sum(target * target) + 0.001\n", "        d = (2 * a) / (b + c)\n\t        loss = 1 - d\n\t        if reduce:\n\t            loss = torch.mean(loss)\n\t        return loss\n\tclass BinaryCrossEntropyLoss(nn.Module):\n\t    def __init__(self, weight, reduce_zero_label=False):\n\t        super(BinaryCrossEntropyLoss, self).__init__()\n\t        print(\"weight:\", weight, \"reduce_zero_label:\", reduce_zero_label)\n\t        if weight is not None:\n", "            self.weight = torch.tensor(weight).cuda()\n\t        else:\n\t            self.weight = None\n\t        self.reduce_zero_label = reduce_zero_label\n\t    def forward(self, outputs, targets):\n\t        # if outputs.size(-1) == 1 and len(outputs.shape) > 1:\n\t        #     outputs = outputs.squeeze(-1)\n\t        outputs = outputs.reshape(-1)\n\t        targets = targets.reshape(-1)\n\t        mask = ~torch.isnan(targets)\n", "        if self.reduce_zero_label:\n\t            targets = targets - 1  # start from zero\n\t        if self.weight is not None:\n\t            loss = F.binary_cross_entropy_with_logits(\n\t                outputs[mask].float(), targets[mask].float(), self.weight[targets[mask].long()], reduction=\"sum\")\n\t        else:\n\t            loss = F.binary_cross_entropy_with_logits(\n\t                outputs[mask].float(), targets[mask].float(), reduction=\"sum\")\n\t        sample_size = torch.sum(mask.type(torch.int64))\n\t        return loss / sample_size\n", "class CrossEntropyLoss(nn.Module):\n\t    def __init__(self, weight, reduce_zero_label=False):\n\t        super(CrossEntropyLoss, self).__init__()\n\t        print(\"weight:\", weight, \"reduce_zero_label:\", reduce_zero_label)\n\t        if weight is not None:\n\t            self.weight = torch.tensor(weight).cuda()\n\t        else:\n\t            self.weight = None\n\t        self.reduce_zero_label = reduce_zero_label\n\t    def forward(self, outputs, targets):\n", "        mask = ~torch.isnan(targets)\n\t        if self.reduce_zero_label:\n\t            targets = targets - 1  # start from zero\n\t        if self.weight is not None:\n\t            loss = F.cross_entropy(\n\t                outputs[mask].float(), targets[mask].long(), self.weight, reduction=\"sum\")\n\t        else:\n\t            loss = F.cross_entropy(\n\t                outputs[mask].float(), targets[mask].long(), reduction=\"sum\")\n\t        sample_size = torch.sum(mask.type(torch.int64))\n", "        return loss / sample_size\n"]}
{"filename": "models/__init__.py", "chunked_list": ["from .build import build_model\n\tfrom .losses import BinaryCrossEntropyLoss, CrossEntropyLoss, DiceLoss\n\t__all__ = ['build_model', 'BinaryCrossEntropyLoss', 'CrossEntropyLoss',\n\t           'DiceLoss']"]}
{"filename": "models/build.py", "chunked_list": ["from .gptrans import GPTrans\n\tdef build_model(config):\n\t    model_type = config.MODEL.TYPE\n\t    if model_type == 'GPTrans':\n\t        model = GPTrans(\n\t            num_layers=config.MODEL.GPTRANS.NUM_LAYERS,\n\t            num_heads=config.MODEL.GPTRANS.NUM_HEADS,\n\t            node_dim=config.MODEL.GPTRANS.NODE_DIM,\n\t            edge_dim=config.MODEL.GPTRANS.EDGE_DIM,\n\t            layer_scale=config.MODEL.GPTRANS.LAYER_SCALE,\n", "            mlp_ratio=config.MODEL.GPTRANS.MLP_RATIO,\n\t            num_classes=config.MODEL.GPTRANS.NUM_CLASSES,\n\t            drop_rate=config.MODEL.DROP_RATE,\n\t            attn_drop_rate=config.MODEL.ATTN_DROP_RATE,\n\t            drop_path_rate=config.MODEL.DROP_PATH_RATE,\n\t            num_atoms=config.DATA.NUM_ATOMS,\n\t            num_edges=config.DATA.NUM_EDGES,\n\t            num_in_degree=config.DATA.NUM_IN_DEGREE,\n\t            num_out_degree=config.DATA.NUM_OUT_DEGREE,\n\t            num_spatial=config.DATA.NUM_SPATIAL,\n", "            num_edge_dist=config.DATA.NUM_EDGE_DIST,\n\t            multi_hop_max_dist=config.DATA.MULTI_HOP_MAX_DIST,\n\t            edge_type=config.DATA.EDGE_TYPE,\n\t            task_type=config.DATA.TASK_TYPE,\n\t            with_cp=config.TRAIN.USE_CHECKPOINT,\n\t            random_feature=config.AUG.RANDOM_FEATURE,\n\t        )\n\t    else:\n\t        raise NotImplementedError(f\"Unkown model: {model_type}\")\n\t    return model\n"]}
{"filename": "models/gptrans.py", "chunked_list": ["import logging\n\timport math\n\timport torch\n\timport torch.nn as nn\n\timport torch.utils.checkpoint as cp\n\tfrom timm.models.layers import DropPath\n\tlogger = logging.getLogger(__name__)\n\tdef init_params(module, num_layers):\n\t    if isinstance(module, nn.Linear):\n\t        module.weight.data.normal_(mean=0.0, std=0.02 / math.sqrt(num_layers))\n", "        if module.bias is not None:\n\t            module.bias.data.zero_()\n\t    if isinstance(module, nn.Embedding):\n\t        module.weight.data.normal_(mean=0.0, std=0.02)\n\tclass GraphNodeFeature(nn.Module):\n\t    def __init__(self, num_heads, num_atoms, num_in_degree, num_out_degree, hidden_dim, num_layers):\n\t        super(GraphNodeFeature, self).__init__()\n\t        self.num_heads = num_heads\n\t        self.num_atoms = num_atoms\n\t        self.atom_encoder = nn.Embedding(num_atoms + 1, hidden_dim, padding_idx=0)\n", "        self.in_degree_encoder = nn.Embedding(num_in_degree, hidden_dim, padding_idx=0)\n\t        self.out_degree_encoder = nn.Embedding(\n\t            num_out_degree, hidden_dim, padding_idx=0\n\t        )\n\t        self.graph_token = nn.Embedding(1, hidden_dim)\n\t        self.apply(lambda module: init_params(module, num_layers=num_layers))\n\t    def forward(self, batched_data):\n\t        x, in_degree, out_degree = (\n\t            batched_data[\"x\"],\n\t            batched_data[\"in_degree\"],\n", "            batched_data[\"out_degree\"],\n\t        )\n\t        n_graph, n_node = x.size()[:2]  # [B, T, 9]\n\t        # node feauture + graph token\n\t        node_feature = self.atom_encoder(x).sum(dim=-2)  # [n_graph, n_node, n_hidden]\n\t        node_feature = (\n\t                node_feature\n\t                + self.in_degree_encoder(in_degree)  # [n_graph, n_node, n_hidden]\n\t                + self.out_degree_encoder(out_degree)  # [n_graph, n_node, n_hidden]\n\t        )\n", "        graph_token_feature = self.graph_token.weight.unsqueeze(0).repeat(n_graph, 1, 1)\n\t        graph_node_feature = torch.cat([graph_token_feature, node_feature], dim=1)\n\t        return graph_node_feature\n\tclass GraphEdgeFeature(nn.Module):\n\t    def __init__(self, num_heads, num_edges, num_spatial, num_edge_dist, edge_type,\n\t                 multi_hop_max_dist, num_layers, edge_dim):\n\t        super(GraphEdgeFeature, self).__init__()\n\t        self.num_heads = num_heads\n\t        self.multi_hop_max_dist = multi_hop_max_dist\n\t        self.edge_embedding_dim = edge_dim\n", "        self.edge_encoder = nn.Embedding(num_edges + 1, edge_dim, padding_idx=0)\n\t        self.edge_type = edge_type\n\t        if self.edge_type == \"multi_hop\":\n\t            self.edge_dis_encoder = nn.Embedding(\n\t                num_edge_dist * edge_dim * edge_dim, 1)\n\t        self.spatial_pos_encoder = nn.Embedding(num_spatial, edge_dim, padding_idx=0)\n\t        self.graph_token_virtual_distance = nn.Embedding(1, edge_dim)\n\t        self.apply(lambda module: init_params(module, num_layers=num_layers))\n\t    def forward(self, batched_data):\n\t        attn_bias, spatial_pos, x = (\n", "            batched_data[\"attn_bias\"],\n\t            batched_data[\"spatial_pos\"],\n\t            batched_data[\"x\"],\n\t        )\n\t        attn_bias = torch.zeros_like(attn_bias) # avoid nan\n\t        # in_degree, out_degree = batched_data.in_degree, batched_data.in_degree\n\t        edge_input, attn_edge_type = (\n\t            batched_data[\"edge_input\"],\n\t            batched_data[\"attn_edge_type\"],\n\t        )\n", "        n_graph, n_node = x.size()[:2]\n\t        graph_attn_bias = attn_bias.clone()\n\t        graph_attn_bias = graph_attn_bias.unsqueeze(1).repeat(\n\t            1, self.edge_embedding_dim, 1, 1\n\t        )  # [n_graph, edge_dim, n_node+1, n_node+1]\n\t        # spatial pos\n\t        # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n\t        spatial_pos_bias = self.spatial_pos_encoder(spatial_pos).permute(0, 3, 1, 2)\n\t        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + spatial_pos_bias\n\t        # reset spatial pos here\n", "        t = self.graph_token_virtual_distance.weight.view(1, self.edge_embedding_dim, 1)\n\t        graph_attn_bias[:, :, 1:, 0] = graph_attn_bias[:, :, 1:, 0] + t\n\t        graph_attn_bias[:, :, 0, :] = graph_attn_bias[:, :, 0, :] + t\n\t        # edge feature\n\t        if self.edge_type == \"multi_hop\": # here\n\t            spatial_pos_ = spatial_pos.clone()\n\t            spatial_pos_[spatial_pos_ == 0] = 1  # set pad to 1\n\t            # set 1 to 1, node_embeds > 1 to node_embeds - 1\n\t            spatial_pos_ = torch.where(spatial_pos_ > 1, spatial_pos_ - 1, spatial_pos_)\n\t            if self.multi_hop_max_dist > 0:\n", "                spatial_pos_ = spatial_pos_.clamp(0, self.multi_hop_max_dist)\n\t                edge_input = edge_input[:, :, :, : self.multi_hop_max_dist, :]\n\t            # [n_graph, n_node, n_node, max_dist, n_head]\n\t            edge_input = self.edge_encoder(edge_input).mean(-2)\n\t            max_dist = edge_input.size(-2)\n\t            edge_input_flat = edge_input.permute(3, 0, 1, 2, 4).reshape(\n\t                max_dist, -1, self.edge_embedding_dim)\n\t            edge_input_flat = torch.bmm(\n\t                edge_input_flat,\n\t                self.edge_dis_encoder.weight.reshape(\n", "                    -1, self.edge_embedding_dim, self.edge_embedding_dim\n\t                )[:max_dist, :, :])\n\t            edge_input = edge_input_flat.reshape(\n\t                max_dist, n_graph, n_node, n_node, self.edge_embedding_dim\n\t            ).permute(1, 2, 3, 0, 4)\n\t            edge_input = (\n\t                edge_input.sum(-2) / (spatial_pos_.float().unsqueeze(-1))\n\t            ).permute(0, 3, 1, 2)\n\t        else:\n\t            # [n_graph, n_node, n_node, n_head] -> [n_graph, n_head, n_node, n_node]\n", "            edge_input = self.edge_encoder(attn_edge_type).mean(-2).permute(0, 3, 1, 2)\n\t        graph_attn_bias[:, :, 1:, 1:] = graph_attn_bias[:, :, 1:, 1:] + edge_input.to(graph_attn_bias.dtype)\n\t        graph_attn_bias = graph_attn_bias + attn_bias.unsqueeze(1)  # [B, n_heads, N, N]\n\t        return graph_attn_bias\n\tclass GraphPropagationAttention(nn.Module):\n\t    def __init__(self, node_dim, edge_dim, num_heads=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n\t        super().__init__()\n\t        self.num_heads = num_heads\n\t        head_dim = node_dim // num_heads\n\t        self.scale = head_dim ** -0.5\n", "        self.qkv = nn.Linear(node_dim, node_dim * 3, bias=qkv_bias)\n\t        self.attn_drop = nn.Dropout(attn_drop)\n\t        self.proj = nn.Linear(node_dim, node_dim)\n\t        self.reduce = nn.Conv2d(edge_dim, num_heads, kernel_size=1)\n\t        self.expand = nn.Conv2d(num_heads, edge_dim, kernel_size=1)\n\t        if edge_dim != node_dim:\n\t            self.fc = nn.Linear(edge_dim, node_dim)\n\t        else:\n\t            self.fc = nn.Identity()\n\t        self.proj_drop = nn.Dropout(proj_drop)\n", "    def forward(self, node_embeds, edge_embeds, padding_mask):\n\t        # node-to-node propagation\n\t        B, N, C = node_embeds.shape\n\t        qkv = self.qkv(node_embeds).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n\t        q, k, v = qkv.unbind(0)\n\t        attn = (q @ k.transpose(-2, -1)) * self.scale # [B, n_head, 1+N, 1+N]\n\t        attn_bias = self.reduce(edge_embeds) # [B, C, 1+N, 1+N] -> [B, n_head, 1+N, 1+N]\n\t        attn = attn + attn_bias # [B, n_head, 1+N, 1+N]\n\t        residual = attn\n\t        attn = attn.masked_fill(padding_mask, float(\"-inf\"))\n", "        attn = attn.softmax(dim=-1) # [B, C, N, N]\n\t        attn = self.attn_drop(attn)\n\t        node_embeds = (attn @ v).transpose(1, 2).reshape(B, N, C)\n\t        # node-to-edge propagation\n\t        edge_embeds = self.expand(attn + residual)  # [B, n_head, 1+N, 1+N] -> [B, C, 1+N, 1+N]\n\t        # edge-to-node propagation\n\t        w = edge_embeds.masked_fill(padding_mask, float(\"-inf\"))\n\t        w = w.softmax(dim=-1)\n\t        w = (w * edge_embeds).sum(-1).transpose(-1, -2)\n\t        node_embeds = node_embeds + self.fc(w)\n", "        node_embeds = self.proj(node_embeds)\n\t        node_embeds = self.proj_drop(node_embeds)\n\t        return node_embeds, edge_embeds\n\tclass FFN(nn.Module):\n\t    def __init__(self, in_features, hidden_features=None, out_features=None, drop=0., drop_act=0.):\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        self.fc1 = nn.Linear(in_features, hidden_features)\n\t        self.act = nn.GELU()\n", "        self.fc2 = nn.Linear(hidden_features, out_features)\n\t        self.drop = nn.Dropout(drop)\n\t        self.drop_act = nn.Dropout(drop_act)\n\t    def forward(self, x):\n\t        x = self.fc1(x)\n\t        x = self.act(x)\n\t        x = self.drop_act(x)\n\t        x = self.fc2(x)\n\t        x = self.drop(x)\n\t        return x\n", "class GPTransBlock(nn.Module):\n\t    def __init__(self, node_dim, edge_dim, num_heads, mlp_ratio=1., qkv_bias=True, drop=0., drop_act=0.,\n\t                 with_cp=False, attn_drop=0., drop_path=0., init_values=None):\n\t        super().__init__()\n\t        self.with_cp = with_cp\n\t        self.norm1 = nn.LayerNorm(node_dim)\n\t        self.gpa = GraphPropagationAttention(node_dim, edge_dim, num_heads=num_heads, qkv_bias=qkv_bias,\n\t                                             attn_drop=attn_drop, proj_drop=drop)\n\t        self.norm2 = nn.LayerNorm(node_dim)\n\t        self.ffn = FFN(in_features=node_dim, hidden_features=int(node_dim * mlp_ratio),\n", "                       drop=drop, drop_act=drop_act)\n\t        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n\t        if init_values is not None:\n\t            self.gamma1 = nn.Parameter(init_values * torch.ones((node_dim)), requires_grad=True)\n\t            self.gamma2 = nn.Parameter(init_values * torch.ones((node_dim)), requires_grad=True)\n\t        else:\n\t            self.gamma1 = None\n\t            self.gamma2 = None\n\t    def forward(self, node_embeds, edge_embeds, padding_mask):\n\t        def _inner_forward(x, edge_embeds):\n", "            if self.gamma1 is not None:\n\t                attn, edge_embeds_ = self.gpa(self.norm1(x), edge_embeds, padding_mask)\n\t                edge_embeds = edge_embeds + edge_embeds_\n\t                x = x + self.drop_path(self.gamma1 * attn)\n\t                x = x + self.drop_path(self.gamma2 * self.ffn(self.norm2(x)))\n\t            else:\n\t                attn, edge_embeds_ = self.gpa(self.norm1(x), edge_embeds, padding_mask)\n\t                edge_embeds = edge_embeds + edge_embeds_\n\t                x = x + self.drop_path(attn)\n\t                x = x + self.drop_path(self.ffn(self.norm2(x)))\n", "            return x, edge_embeds\n\t        if self.with_cp and node_embeds.requires_grad:\n\t            node_embeds, edge_embeds = cp.checkpoint(_inner_forward, node_embeds, edge_embeds)\n\t        else:\n\t            node_embeds, edge_embeds = _inner_forward(node_embeds, edge_embeds)\n\t        return node_embeds, edge_embeds\n\tclass GraphEmbedding(nn.Module):\n\t    def __init__(self, num_atoms, num_in_degree, num_out_degree, num_edges, num_spatial,\n\t                 num_edge_dist, edge_type, multi_hop_max_dist, num_layers, node_dim,\n\t                 edge_dim, num_heads, dropout):\n", "        super().__init__()\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.embedding_dim = node_dim\n\t        self.emb_layer_norm = nn.LayerNorm(node_dim)\n\t        self.graph_node_feature = GraphNodeFeature(\n\t            num_heads=num_heads,\n\t            num_atoms=num_atoms,\n\t            num_in_degree=num_in_degree,\n\t            num_out_degree=num_out_degree,\n\t            hidden_dim=node_dim,\n", "            num_layers=num_layers,\n\t        )\n\t        self.graph_edge_feature = GraphEdgeFeature(\n\t            num_heads=num_heads,\n\t            num_edges=num_edges,\n\t            num_spatial=num_spatial,\n\t            num_edge_dist=num_edge_dist,\n\t            edge_type=edge_type,\n\t            multi_hop_max_dist=multi_hop_max_dist,\n\t            num_layers=num_layers,\n", "            edge_dim=edge_dim,\n\t        )\n\t    def forward(self, batched_data, perturb=None):\n\t        # compute padding mask. This is needed for multi-head attention\n\t        data_x = batched_data[\"x\"]  # [B, 18 or 19, 9]\n\t        n_graph, n_node = data_x.size()[:2]\n\t        padding_mask = (data_x[:, :, 0]).eq(0)  # B node_embeds T\n\t        padding_mask_cls = torch.zeros(  # not mask\n\t            n_graph, 1, device=padding_mask.device, dtype=padding_mask.dtype\n\t        )  # B node_embeds 1\n", "        padding_mask = torch.cat((padding_mask_cls, padding_mask), dim=1)\n\t        # B node_embeds (1+T)\n\t        node_embeds = self.graph_node_feature(batched_data)\n\t        if perturb is not None:  # perturb is None\n\t            node_embeds[:, 1:, :] += perturb\n\t        # node_embeds: B node_embeds T node_embeds C\n\t        edge_embeds = self.graph_edge_feature(batched_data)\n\t        node_embeds = self.emb_layer_norm(node_embeds)\n\t        node_embeds = self.dropout(node_embeds)\n\t        return node_embeds, edge_embeds, padding_mask\n", "class GPTrans(nn.Module):\n\t    def __init__(self, num_layers=24, num_heads=23, node_dim=736, edge_dim=92, layer_scale=1.0, mlp_ratio=1.0,\n\t                 drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0, num_atoms=4608, num_edges=1536,\n\t                 num_in_degree=512, num_out_degree=512, num_spatial=512, num_edge_dist=128, multi_hop_max_dist=20,\n\t                 edge_type='multi_hop', qkv_bias=True, num_classes=1, task_type=\"graph_regression\",\n\t                 random_feature=False, with_cp=False):\n\t        super(GPTrans, self).__init__()\n\t        logger.info(f\"drop: {drop_rate}, drop_path_rate: {drop_path_rate}, attn_drop_rate: {attn_drop_rate}\")\n\t        self.task_type = task_type\n\t        self.random_feature = random_feature\n", "        self.graph_embedding = GraphEmbedding(\n\t            num_atoms=num_atoms,\n\t            num_in_degree=num_in_degree,\n\t            num_out_degree=num_out_degree,\n\t            num_edges=num_edges,\n\t            num_spatial=num_spatial,\n\t            num_edge_dist=num_edge_dist,\n\t            edge_type=edge_type,\n\t            multi_hop_max_dist=multi_hop_max_dist,\n\t            num_layers=num_layers,\n", "            node_dim=node_dim,\n\t            edge_dim=edge_dim,\n\t            num_heads=num_heads,\n\t            dropout=drop_rate,\n\t        )\n\t        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, num_layers)]  # stochastic depth decay rule\n\t        self.blocks = nn.Sequential(*[\n\t            GPTransBlock(node_dim=node_dim, edge_dim=edge_dim, num_heads=num_heads, mlp_ratio=mlp_ratio,\n\t                         drop_act=drop_rate, drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i],\n\t                         qkv_bias=qkv_bias, init_values=layer_scale, with_cp=with_cp) for i in range(num_layers)\n", "        ])\n\t        self.fc_layer = nn.Sequential(\n\t            nn.Linear(node_dim + edge_dim, node_dim),\n\t            nn.LayerNorm(node_dim),\n\t            nn.GELU(),\n\t        )\n\t        self.head = nn.Linear(node_dim, num_classes, bias=True)\n\t    def forward(self, batched_data, perturb=None):\n\t        node_embeds, edge_embeds, padding_mask = self.graph_embedding(\n\t            batched_data,\n", "            perturb=perturb,\n\t        )\n\t        if self.random_feature and self.training:\n\t            node_embeds += torch.rand_like(node_embeds)\n\t            edge_embeds += torch.rand_like(edge_embeds)\n\t        padding_mask = padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool)\n\t        for blk in self.blocks:\n\t            node_embeds, edge_embeds = blk(node_embeds,  # [B, 1+N, C]\n\t                                           edge_embeds,  # [B, C, 1+N, 1+N]\n\t                                           padding_mask) # [B, 1+N, 1]\n", "        if self.task_type == \"graph_regression\" or self.task_type == \"graph_classification\":\n\t            x = torch.cat([node_embeds[:, :1, :], edge_embeds[:, :, 0:1, 0].transpose(-1, -2)], dim=2)\n\t            x = self.fc_layer(x)\n\t            x = self.head(x)[:, 0, :] # select the virtual node\n\t            if x.size(-1) == 1: x = x.squeeze(-1)\n\t        elif self.task_type == \"node_classification\":\n\t            diag = torch.diagonal(edge_embeds, dim1=-1, dim2=-2).transpose(-1, -2)\n\t            x = torch.cat([node_embeds, diag], dim=2)[:, 1:, :]\n\t            x = x.reshape(-1, x.shape[-1])\n\t            x = self.fc_layer(x)\n", "            x = self.head(x)\n\t        return x\n\t    @torch.jit.ignore\n\t    def lr_decay_keywords(self, decay_ratio=0.87):\n\t        lr_ratios = {}\n\t        depth = len(self.blocks) + 1\n\t        for k, v in self.named_parameters():\n\t            if \"graph_embedding.\" in k:\n\t                lr_ratios[k] = decay_ratio ** depth\n\t            elif \"blocks.\" in k:\n", "                block_id = int(k.split(\".\")[1])\n\t                lr_ratios[k] = decay_ratio ** (depth - block_id - 1)\n\t            elif \"fc_layer.\" in k or \"head.\" in k:\n\t                lr_ratios[k] = 1\n\t        return lr_ratios"]}
