{"filename": "setup.py", "chunked_list": ["from distutils.core import setup\n\tsetup(name='Apollo',\n\t      version='1.0',\n\t      packages=[\"cdm_processing\", \"utils\", \"simulating\", \"data_generating\"],\n\t     )"]}
{"filename": "extras/eval_mapping_solutions.py", "chunked_list": ["# Some code to see if we can speed up concept mapping\n\timport random\n\timport pandas as pd\n\timport timeit\n\trandom.seed(0)\n\tids_to_map = [random.randint(0, int(1e7)) for i in range(1000)]\n\tsource_ids = [random.randint(0, int(1e7)) for i in range(100000)]\n\ttarget_ids = [random.randint(0, int(1e7)) for i in range(100000)]\n\t# Using pandas\n\tmapping = pd.DataFrame({\"source_id\": source_ids, \"target_id\": target_ids})\n", "mapping.set_index(\"source_id\", drop=True, inplace=True)\n\tdf = pd.DataFrame({\"source_id\": ids_to_map})\n\tstmt = '''\n\tresult = df.merge(mapping, how=\"inner\", left_on=\"source_id\", right_index=True)\n\t'''\n\ttimeit.timeit(stmt=stmt, globals=globals(), number=10)\n\t# 0.13781319199999942\n\t# Using a dictionary\n\tmapping = dict(zip(source_ids, target_ids))\n\tdef do_map(x):\n", "    if x in mapping:\n\t        return mapping[x]\n\t    else:\n\t        return -1\n\tstmt = '''\n\tresult = [do_map(x) for x in ids_to_map]\n\t'''\n\ttimeit.timeit(stmt=stmt, globals=globals(), number=10)\n\t# 0.0017271530000471103\n"]}
{"filename": "extras/analyse_profile_stats.py", "chunked_list": ["# Code to display the results of profiling\n\timport pstats\n\tfrom pstats import SortKey\n\tp = pstats.Stats('../stats')\n\tp.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(25)\n"]}
{"filename": "extras/read_stats.py", "chunked_list": ["import pstats\n\tfrom pstats import SortKey\n\tp = pstats.Stats('stats')\n\tp.strip_dirs().sort_stats(SortKey.CUMULATIVE).print_stats(25)"]}
{"filename": "extras/explore_all_parquet_ops.py", "chunked_list": ["import cdm_processing.cdm_processor_utils as cdm_utils\n\tcdm_folder = \"D:/GPM_CCAE\"\n\tcdm_tables = cdm_utils.get_cdm_tables(cdm_folder, 1)\n\tcdm_tables[\"person\"] = cdm_utils.add_date_of_birth(cdm_tables[\"person\"])\n\tdrug_mapping = cdm_utils.load_mapping_to_ingredients(cdm_folder=cdm_folder)\n\tcdm_tables[\"drug_exposure\"] = cdm_utils.map_concepts(cdm_table=cdm_tables[\"drug_exposure\"],\n\t                                                     concept_id_field=\"drug_concept_id\",\n\t                                                     mapping=drug_mapping)\n\tcombined_table = cdm_utils.union_domain_tables(cdm_tables)\n\tcombined_table, removed_concepts = cdm_utils.remove_concepts(combined_table=combined_table,\n", "                                                             concept_ids=[0, 900000010])\n\tcombined_table, removed_duplicates = cdm_utils.remove_duplicates(combined_table=combined_table)\n\tcombined_table, visit_occurrence, mapping_stats = cdm_utils.link_events_to_visits(combined_table=combined_table,\n\t                                                                                  visit_occurrence=cdm_tables[\n\t                                                                                      \"visit_occurrence\"],\n\t                                                                                  mising_visit_concept_id=1)\n\t# Create CEHR-BERT sequence format\n\timport duckdb\n\tcon = duckdb.connect(database=':memory:', read_only=False)\n\tcon.register(\"visit_occurrence\", visit_occurrence)\n", "con.register(\"observation_period_table\", cdm_tables[\"observation_period\"])\n\tcon.register(\"person\", cdm_tables[\"person\"])\n\tcon.register(\"combined_table\", combined_table)\n\tsql = \"CREATE TABLE visits AS \" \\\n\t      \"SELECT visit_occurrence.*, \" \\\n\t      \"  observation_period_id, \" \\\n\t      \"  ROW_NUMBER() OVER (PARTITION BY observation_period_id ORDER BY visit_start_date) AS visit_rank \" \\\n\t      \"FROM visit_occurrence \" \\\n\t      \"INNER JOIN observation_period_table \" \\\n\t      \"  ON visit_occurrence.person_id = observation_period_table.person_id \" \\\n", "      \"    AND visit_occurrence.visit_start_date >= observation_period_table.observation_period_start_date \" \\\n\t      \"    AND visit_occurrence.visit_start_date <= observation_period_table.observation_period_end_date\"\n\tcon.execute(sql)\n\tsql = \"CREATE TABLE interval_tokens AS \" \\\n\t      \"SELECT CASE \" \\\n\t      \"    WHEN days < 0 THEN 'W-1' \" \\\n\t      \"    WHEN days < 28 THEN 'W' || CAST(CAST(FLOOR(days / 7) AS INT) AS VARCHAR) \" \\\n\t      \"    WHEN days < 360 THEN 'M' || CAST(CAST(FLOOR(days / 30) AS INT) AS VARCHAR) \" \\\n\t      \"    ELSE 'LT' \" \\\n\t      \"  END AS concept_id, \" \\\n", "      \"  0 AS visit_segments, \" \\\n\t      \"  0 AS dates, \" \\\n\t      \" -1 AS ages, \" \\\n\t      \"  visit_rank AS visit_concept_orders, \" \\\n\t      \"  0 AS visit_concept_ids, \" \\\n\t      \"  -2 AS sort_order, \" \\\n\t      \"  observation_period_id \" \\\n\t      \"FROM (\" \\\n\t      \"  SELECT visits.visit_start_date - previous_visit.visit_end_date AS days,\" \\\n\t      \"    visits.* \" \\\n", "      \"  FROM visits \" \\\n\t      \"  INNER JOIN visits previous_visit\" \\\n\t      \"    ON visits.observation_period_id = previous_visit.observation_period_id \" \\\n\t      \"      AND visits.visit_rank = previous_visit.visit_rank + 1\" \\\n\t      \") intervals\"\n\tcon.execute(sql)\n\tsql = \"CREATE TABLE start_tokens AS \" \\\n\t      \"SELECT 'VS' AS concept_id, \" \\\n\t      \"  visit_rank % 2 + 1 AS visit_segments, \" \\\n\t      \"  DATE_DIFF('week', DATE '1970-01-01', visit_start_date) AS dates, \" \\\n", "      \"  DATE_DIFF('month', date_of_birth, visit_start_date) AS ages, \" \\\n\t      \"  visit_rank AS visit_concept_orders, \" \\\n\t      \"  visit_concept_id AS visit_concept_ids, \" \\\n\t      \"  -1 AS sort_order, \" \\\n\t      \"  observation_period_id \" \\\n\t      \"FROM visits \" \\\n\t      \"INNER JOIN person \" \\\n\t      \"  ON visits.person_id = person.person_id\"\n\tcon.execute(sql)\n\tsql = \"CREATE TABLE event_tokens AS \" \\\n", "      \"SELECT CAST(concept_id AS VARCHAR) AS concept_id, \" \\\n\t      \"  visit_rank % 2 + 1 AS visit_segments, \" \\\n\t      \"  DATE_DIFF('week', DATE '1970-01-01', start_date) AS dates, \" \\\n\t      \"  DATE_DIFF('month', date_of_birth,start_date) AS ages, \" \\\n\t      \"  visit_rank AS visit_concept_orders, \" \\\n\t      \"  visit_concept_id AS visit_concept_ids, \" \\\n\t      \"  concept_id AS sort_order, \" \\\n\t      \"  observation_period_id \" \\\n\t      \"FROM combined_table \" \\\n\t      \"INNER JOIN visits \" \\\n", "      \"  ON combined_table.internal_visit_id = visits.internal_visit_id \" \\\n\t      \"INNER JOIN person \" \\\n\t      \"  ON visits.person_id = person.person_id\"\n\tcon.execute(sql)\n\tsql = \"CREATE TABLE end_tokens AS \" \\\n\t      \"SELECT 'VE' AS concept_id, \" \\\n\t      \"  visit_rank % 2 + 1 AS visit_segments, \" \\\n\t      \"  DATE_DIFF('week', DATE '1970-01-01', visit_end_date) AS dates, \" \\\n\t      \"  DATE_DIFF('month', date_of_birth, visit_end_date) AS ages, \" \\\n\t      \"  visit_rank AS visit_concept_orders, \" \\\n", "      \"  visit_concept_id AS visit_concept_ids, \" \\\n\t      \"  9223372036854775807 AS sort_order, \" \\\n\t      \"  observation_period_id \" \\\n\t      \"FROM visits \" \\\n\t      \"INNER JOIN person \" \\\n\t      \"  ON visits.person_id = person.person_id\"\n\tcon.execute(sql)\n\tsql = \"SELECT *, \" \\\n\t      \"  ROW_NUMBER() OVER (PARTITION BY observation_period_id ORDER BY sort_order) AS orders \" \\\n\t      \"FROM (\" \\\n", "      \"  SELECT * FROM interval_tokens \" \\\n\t      \"  UNION ALL \" \\\n\t      \"  SELECT * FROM start_tokens \" \\\n\t      \"  UNION ALL \" \\\n\t      \"  SELECT * FROM event_tokens \" \\\n\t      \"  UNION ALL \" \\\n\t      \"  SELECT * FROM end_tokens\" \\\n\t      \") tokens \" \\\n\t      \"ORDER BY observation_period_id, visit_concept_orders, sort_order\"\n\tunion_tokens = con.execute(sql).arrow()\n", "con.execute(\"DROP TABLE interval_tokens\")\n\tcon.execute(\"DROP TABLE start_tokens\")\n\tcon.execute(\"DROP TABLE event_tokens\")\n\tcon.execute(\"DROP TABLE end_tokens\")\n\tcehr_bert_input = union_tokens.group_by(\"observation_period_id\").aggregate(\n\t    [(\"concept_id\", \"list\"), (\"visit_segments\", \"list\"), (\"dates\", \"list\"), (\"ages\", \"list\"),\n\t     (\"visit_concept_orders\", \"list\"), (\"visit_concept_ids\", \"list\"), (\"orders\", \"list\"), (\"concept_id\", \"count\"),\n\t     (\"visit_concept_orders\", \"max\")]).rename_columns([\"observation_period_id\", \"concept_id\", \"visit_segments\", \"dates\",\n\t                                                       \"ages\", \"visit_concept_orders\", \"visit_concept_ids\", \"orders\",\n\t                                                       \"num_of_concepts\", \"num_of_visits\"])\n"]}
{"filename": "extras/analyze_cehr_bert_sequence_format.py", "chunked_list": ["# Some code to analyze the sequence format used by cehr-bert\n\timport pandas as pd\n\timport pyarrow.parquet as pq\n\timport os\n\t# folder = \"D:/omopSynthea/cehr-bert/patient_sequence\"\n\t# file = pq.read_table(os.path.join(folder, \"part-00000-c0fda67a-757c-41ba-8c31-a69d1f7bf530-c000.snappy.parquet\"))\n\tfolder = \"D:/GPM_CCAE/patient_sequence\"\n\tfile = \"part0001.parquet\"\n\tfolder = \"D:/GPM_MDCD/cehr-bert/hospitalization\"\n\tfile = \"part-00000-41b43fb8-f5c0-4d94-8455-830e730bc13d-c000.snappy.parquet\"\n", "folder = \"D:/GPM_MDCD/cehr-bert/hopitalization_outcome\"\n\tfile = \"part-00000-eac1df12-7972-46d8-8b3f-e98f7bab9090-c000.snappy.parquet\"\n\tfolder = \"D:/GPM_MDCD/cehr-bert/hospitalization_target\"\n\tfile = \"part-00000-b6bd2591-3b0b-4582-a1ed-b00bb9d49963-c000.snappy.parquet\"\n\tpfile = pq.read_table(os.path.join(folder, file))\n\tx = pfile.to_pandas()\n\tprint(x.dtypes)\n\tfor column in x.columns:\n\t  print(f\"column: {column}\")\n\t  print(x[column].iat[0])\n", "  # print(x[column].iat[0].dtype)\n\tlen(x)\n\tfull = pd.read_parquet(folder)\n\tcids = full[\"concept_ids\"]\n\tcids = full[\"visit_concept_ids\"]\n\tcids = full[\"dates\"]\n\tx = [j for i in cids for j in i]\n\tx = set(x)\n\tx = list(x)\n\tx.sort()\n", "x[:25]\n\tx[-25:-1]\n\tfor i in range(len(full)):\n\t  cidLen = full[\"num_of_concepts\"].iat[i]\n\t  if len(full[\"concept_ids\"].iat[i]) != cidLen:\n\t    print(f\"Issue with concept_ids for {i}\")\n\t  if len(full[\"visit_segments\"].iat[i]) != cidLen:\n\t    print(f\"Issue with visit_segments for {i}\")\n\t  if len(full[\"orders\"].iat[i]) != cidLen:\n\t    print(f\"Issue with orders for {i}\")\n", "  if len(full[\"dates\"].iat[i]) != cidLen:\n\t    print(f\"Issue with dates for {i}\")\n\t  if len(full[\"ages\"].iat[i]) != cidLen:\n\t    print(f\"Issue with ages for {i}\")\n\t  if len(full[\"visit_concept_orders\"].iat[i]) != cidLen:\n\t    print(f\"Issue visit_concept_orders ages for {i}\")\n\t  max([max(x) for x in full[\"visit_concept_orders\"]])\n\t  min([len(x) for x in full[\"visit_concept_orders\"]])\n\ttotal_visits = 0\n\tshady_visits = 0\n", "for i in range(len(full)):\n\t  dates = full[\"dates\"].iat[i]\n\t  days = (max(dates) - min(dates)) * 7\n\t  vcos = full[\"visit_concept_orders\"].iat[i]\n\t  visits = max(vcos)\n\t  total_visits += visits\n\t  if visits > days / 2:\n\t    shady_visits += visits\n\tprint(f\"Total visits: {total_visits}\")\n\tprint(f\"Shady visits: {shady_visits}\")\n", "print(f\"Shady visits: {shady_visits / total_visits}\")\n"]}
{"filename": "extras/problems_with_pandas.py", "chunked_list": ["# For my own sanity, a documentation of issues I've had with pandas:\n\timport pandas as pd\n\t# Storing and retrieving integers --------------------------------------------------------------------------------------\n\t# By default, pandas stores integers as int64, which can't store nulls. This leads to silent conversion to float64:\n\tdf = pd.DataFrame({\"int\": [1, 2]})\n\tdf[\"int\"].dtype\n\t# dtype('int64')\n\tdf.iloc[0, 0] = None\n\tdf[\"int\"].dtype\n\t# dtype('float64')\n", "# To avoid this, we can use the nullable integer type:\n\tdf = pd.DataFrame({\"int\": pd.array([1, 2], dtype=pd.Int64Dtype())})\n\tdf[\"int\"].dtype\n\t# Int64Dtype()\n\tdf.iloc[0, 0] = None\n\tdf[\"int\"].dtype\n\t# Int64Dtype()\n\t# Slicing and indexing -------------------------------------------------------------------------------------------------\n\t# When slicing a DataFrame, the index is preserved, even when creating a copy:\n\tdf = pd.DataFrame({\"int\": [1, 2, 3, 4, 5]})\n", "part_df = df[df.int > 3].copy()\n\tpart_df.int[0]\n\t# Key error\n\t# Instead, we need to reset the index:\n\tpart_df = df[df.int > 3].copy().reset_index(drop=True)\n\tpart_df.int[0]\n\t# 4\n"]}
{"filename": "simulating/simulator.py", "chunked_list": ["\"\"\"\n\tSimulate CDM data for testing purposes. Persons are simulated to have hidden disease states. Some states are fixed at\n\tthe start, to simulate fixed traits such as genetics, while the remaining states are dynamic. The probability to enter\n\ta dynamic disease state depending on the current disease states in a non-linear way (i.e. using interactions between\n\tstates). Concepts are simulated to be observed with probabilities depending on the current disease states, again in a\n\tnon-linear way. Concepts imply visits, and visits in return increase the probability of observing concepts, thus causing\n\tconcepts to cluster in time. The simulated data is saved in the CDM format.\n\tTo generate patient-level predicition problems with a gold standard, the model is executed. For each person, an index\n\tdate is randomly selected, and Monte-Carlo simulations are performed to simulate the future.\n\t\"\"\"\n", "import cProfile\n\timport configparser\n\timport json\n\timport logging\n\timport multiprocessing\n\timport os\n\timport sys\n\tfrom typing import List\n\tfrom dataclasses import dataclass\n\timport numpy as np\n", "import tqdm as tqdm\n\timport cdm_data\n\timport utils.logger as logger\n\tLOGGER_FILE_NAME = \"_simulation_log.txt\"\n\tQUIET = 0\n\tCHATTY = 1\n\tOBSESSIVE = 2\n\tJSON_FILE_NAME = \"_simulation.json\"\n\tPRETRAINING_FOLDER = \"pretraining\"\n\tTRAIN_FOLDER = \"train\"\n", "TEST_FOLDER = \"test\"\n\tSTATE_TRANSITION_INTERCEPT = -6  # Log probability intercept of transitioning to a new state.\n\tCONCEPT_OBSERVATION_INTERCEPT = -8  # Log probability intercept of observing a concept.\n\tAGE_INDEX = -2\n\tGENDER_INDEX = -1\n\tdef logistic(x):\n\t    return 1 / (1 + np.exp(-x))\n\t@dataclass\n\tclass SimulationSettings:\n\t    dynamic_state_count: int = 10\n", "    fixed_state_count: int = 5\n\t    concept_count: int = 100\n\t    serious_concept_count: int = 10\n\t    visit_multiplier: int = 2\n\t    days_to_simulate: int = 365 * 2\n\t@dataclass\n\tclass SimulationTask:\n\t    partition_count: int\n\t@dataclass\n\tclass PreTrainingTask(SimulationTask):\n", "    person_count: int\n\t@dataclass\n\tclass PredictionTask(SimulationTask):\n\t    train_person_count: int\n\t    test_person_count: int\n\t    prediction_window: int\n\tdef _simulate_date_of_birth(current_date: np.datetime64, age_indicator: int) -> tuple[int, int, int]:\n\t    \"\"\"\n\t    Simulate a date of birth for a person.\n\t    Args:\n", "        current_date: The current date.\n\t        age_indicator: An indicator of the age of the person.\n\t    Returns:\n\t        A tuple of the year, month and day of birth.\n\t    \"\"\"\n\t    if age_indicator == 0:\n\t        # Simulate a date of birth for a child:\n\t        birth_date = current_date - np.random.randint(0, 365 * 18)\n\t    else:\n\t        # Simulate a date of birth for an adult:\n", "        birth_date = current_date - np.random.randint(365 * 18, 365 * 100)\n\t    birth_date = np.datetime64(birth_date, 'D')\n\t    return birth_date.astype('datetime64[Y]').astype(int) + 1970, \\\n\t           birth_date.astype('datetime64[M]').astype(int) % 12 + 1, \\\n\t           birth_date.astype('datetime64[D]').astype(int) % 31 + 1\n\tclass Simulator:\n\t    # Note: Tried using scipy.sparse.csr_matrix, but it was 100 times slower than using a dense matrix.\n\t    def __init__(self,\n\t                 root_folder: str,\n\t                 settings: SimulationSettings = None,\n", "                 json_file_name: str = None,\n\t                 log_verbosity: int = QUIET,\n\t                 max_cores: int = 1):\n\t        self._root_folder = root_folder\n\t        if not os.path.exists(root_folder):\n\t            os.makedirs(root_folder)\n\t        self._profile = False\n\t        self._log_verbosity = log_verbosity\n\t        self._max_cores = max_cores\n\t        self._task = SimulationTask(0)\n", "        self._configure_logger()\n\t        if settings is None:\n\t            self._init_from_json(json_file_name)\n\t        else:\n\t            self._init_from_settings(settings)\n\t            self.save_to_json(os.path.join(self._root_folder, JSON_FILE_NAME))\n\t    def _init_from_json(self, json_file_name: str):\n\t        logging.info(\"Loading simulation configuration from %s\", json_file_name)\n\t        with open(json_file_name, \"r\") as f:\n\t            loaded = json.load(f)\n", "        self._settings = SimulationSettings(**loaded[\"simulation_settings\"])\n\t        self._state_count = loaded[\"state_count\"]\n\t        self._concept_ids = np.array(loaded[\"concept_ids\"])\n\t        self._serious_concept_idx = np.array(loaded[\"serious_concept_idx\"])\n\t        self._initial_state_probabilities = np.array(loaded[\"initial_state_probabilities\"])\n\t        self._dynamic_state_entry_coefs = [np.array(matrix) for matrix in loaded[\"dynamic_state_entry_coefs\"]]\n\t        self._dynamic_state_exit_probabilities = np.array(loaded[\"dynamic_state_exit_probabilities\"])\n\t        self._concept_emmision_coefs = [np.array(matrix) for matrix in loaded[\"concept_emmision_coefs\"]]\n\t    def _init_from_settings(self, settings: SimulationSettings):\n\t        self._settings = settings\n", "        self._state_count = settings.dynamic_state_count + settings.fixed_state_count + 2  # + 2 for age and sex\n\t        self._concept_ids = np.arange(settings.concept_count) + 1000000\n\t        # Initialize probabilities and coefficients:\n\t        self._initial_state_probabilities = np.concatenate((\n\t            np.random.beta(size=settings.dynamic_state_count, a=1, b=4) / settings.dynamic_state_count,\n\t            np.random.uniform(size=settings.fixed_state_count + 2)\n\t        ))\n\t        # Note: really only need half of these matrices to be filled, because interaction matrix will be symmetrical\n\t        # over diagonal, but it's easier to just fill them completely:\n\t        self._dynamic_state_entry_coefs = []\n", "        for i in range(settings.dynamic_state_count):\n\t            matrix = np.zeros(self._state_count * self._state_count)\n\t            non_zero_count = round(len(matrix) / self._state_count)\n\t            matrix[np.random.choice(len(matrix), size=non_zero_count, replace=False)] = \\\n\t                np.random.laplace(loc=0, scale=0.1, size=non_zero_count)\n\t            self._dynamic_state_entry_coefs.append(matrix.reshape(self._state_count, self._state_count))\n\t        self._dynamic_state_exit_probabilities = \\\n\t            np.random.beta(size=settings.dynamic_state_count, a=0.2, b=3) / settings.dynamic_state_count\n\t        self._concept_emmision_coefs = []\n\t        for i in range(settings.concept_count):\n", "            matrix = np.zeros(self._state_count * self._state_count)\n\t            non_zero_count = round(len(matrix) / np.sqrt(self._settings.concept_count))\n\t            matrix[np.random.choice(len(matrix), size=non_zero_count, replace=False)] = \\\n\t                np.random.laplace(loc=0, scale=0.5, size=non_zero_count)\n\t            self._concept_emmision_coefs.append(matrix.reshape(self._state_count, self._state_count))\n\t        self._serious_concept_idx = np.zeros(settings.concept_count, dtype=bool)\n\t        self._serious_concept_idx[np.random.choice(settings.concept_count,\n\t                                                   size=settings.serious_concept_count,\n\t                                                   replace=False)] = True\n\t    def set_profile(self, profile: bool):\n", "        self._profile = profile\n\t    def get_profile(self):\n\t        return self._profile\n\t    def _configure_logger(self):\n\t        logger.create_logger(os.path.join(self._root_folder, LOGGER_FILE_NAME))\n\t    def _simulate_person(self, person_id: int):\n\t        if isinstance(self._task, PredictionTask):\n\t            prediction_labels = np.zeros(self._settings.concept_count, dtype=bool)\n\t            # Currently just using full prediction window, but could change to make index day random:\n\t            index_day = self._settings.days_to_simulate - self._task.prediction_window\n", "            is_prediction = True\n\t        else:\n\t            prediction_labels = None\n\t            index_day = 0\n\t            is_prediction = False\n\t        start_date = np.datetime64(\"2010-01-01\") + np.random.randint(0, 365 * 10)\n\t        state = np.random.binomial(n=1, p=self._initial_state_probabilities)\n\t        self._cdm_data.add_observation_period(person_id=person_id,\n\t                                              observation_period_id=person_id,\n\t                                              observation_period_start_date=start_date,\n", "                                              observation_period_end_date=start_date + self._settings.days_to_simulate)\n\t        year_of_birth, month_of_birth, day_of_birth = _simulate_date_of_birth(start_date, state[AGE_INDEX])\n\t        gender_concept_id = 8507 if state[GENDER_INDEX] == 0 else 8532\n\t        self._cdm_data.add_person(person_id=person_id,\n\t                                  year_of_birth=year_of_birth,\n\t                                  month_of_birth=month_of_birth,\n\t                                  day_of_birth=day_of_birth,\n\t                                  gender_concept_id=gender_concept_id)\n\t        visit_occurrence_id = person_id * 100000\n\t        for t in range(self._settings.days_to_simulate):\n", "            if self._log_verbosity == OBSESSIVE:\n\t                logging.debug(\"Person: %s, Day: %s, State: %s\", person_id, t, state)\n\t            state_interaction_matrix = np.outer(state, state)\n\t            # Roll dice to change state:\n\t            flip_to_one = logistic(\n\t                np.sum(np.asarray(self._dynamic_state_entry_coefs) * state_interaction_matrix[np.newaxis, :, :],\n\t                       axis=(1, 2)) - 6) * (state[:self._settings.dynamic_state_count] == 0)\n\t            flip_to_zero = (self._dynamic_state_exit_probabilities * (state[:self._settings.dynamic_state_count] == 1))\n\t            state_flip_probabilities = flip_to_one + flip_to_zero\n\t            state[:self._settings.dynamic_state_count] = np.logical_xor(state[:self._settings.dynamic_state_count],\n", "                                                                        np.random.binomial(n=1,\n\t                                                                                           p=state_flip_probabilities))\n\t            # Roll dice to observe a concept:\n\t            concept_probabilities = logistic(CONCEPT_OBSERVATION_INTERCEPT + np.sum(\n\t                np.asarray(self._concept_emmision_coefs) * state_interaction_matrix[np.newaxis, :, :], axis=(1, 2)))\n\t            admission_concept_idx = (np.random.binomial(n=1, p=concept_probabilities) != 0)\n\t            visit = admission_concept_idx.any()\n\t            if visit:\n\t                # Roll dice again to observe a concept (to simulate the fact that some concepts are more likely to be\n\t                # observed during a visit):\n", "                observed_concept_idx = admission_concept_idx | (np.random.binomial(n=self._settings.visit_multiplier,\n\t                                                                                   p=concept_probabilities) != 0)\n\t                if is_prediction and t > index_day:\n\t                    prediction_labels = prediction_labels | observed_concept_idx\n\t                else:\n\t                    concept_ids = self._concept_ids[observed_concept_idx]\n\t                    for concept_id in concept_ids:\n\t                        self._cdm_data.add_condition_occurrence(person_id=person_id,\n\t                                                                condition_start_date=start_date + t,\n\t                                                                condition_concept_id=concept_id,\n", "                                                                visit_occurrence_id=visit_occurrence_id)\n\t                    # If admission concept is serious, make the visit an emergency room visit, otherwise make it an\n\t                    # outpatient visit:\n\t                    if (admission_concept_idx & self._serious_concept_idx).any():\n\t                        visit_concept_id = 9203  # Emergency room visit\n\t                    else:\n\t                        visit_concept_id = 9201  # Outpatient visit\n\t                    self._cdm_data.add_visit_occurrence(person_id=person_id,\n\t                                                        visit_start_date=start_date + t,\n\t                                                        visit_end_date=start_date + t,\n", "                                                        visit_concept_id=visit_concept_id,\n\t                                                        visit_occurrence_id=visit_occurrence_id)\n\t                    visit_occurrence_id += 1\n\t                    if self._log_verbosity == OBSESSIVE:\n\t                        logging.debug(\"Person %s visit on day %s with concept IDs: %s\", person_id, t, concept_ids)\n\t        if isinstance(self._cdm_data, cdm_data.CdmDataWithLabels):\n\t            for i in range(self._settings.concept_count):\n\t                self._cdm_data.add_label(person_id=person_id,\n\t                                         concept_id=self._concept_ids[i],\n\t                                         label=prediction_labels[i])\n", "    def simulate(self, task: SimulationTask):\n\t        \"\"\"\n\t        Process the CDM data in the provided cdm_data_path.\n\t        \"\"\"\n\t        self._task = task\n\t        if isinstance(task, PreTrainingTask):\n\t            logging.info(\"Simulating data for pre-training task\")\n\t            output_folder = os.path.join(self._root_folder, PRETRAINING_FOLDER)\n\t            if not os.path.exists(output_folder):\n\t                os.makedirs(output_folder)\n", "        elif isinstance(task, PredictionTask):\n\t            logging.info(\"Simulating data for prediction task\")\n\t            train_folder = os.path.join(self._root_folder, TRAIN_FOLDER)\n\t            if not os.path.exists(train_folder):\n\t                os.makedirs(train_folder)\n\t            test_folder = os.path.join(self._root_folder, TEST_FOLDER)\n\t            if not os.path.exists(test_folder):\n\t                os.makedirs(test_folder)\n\t        else:\n\t            raise ValueError(\"Unknown task type: %s\" % type(task))\n", "        if self._profile:\n\t            cProfile.runctx(statement=\"self._simulate()\",\n\t                            locals={\"self\": self},\n\t                            globals={},\n\t                            filename=\"../stats\")\n\t        else:\n\t            self._simulate()\n\t    def _simulate(self):\n\t        if self._profile:\n\t            logging.info(\"Profiling mode enabled, running first partition in single thread\")\n", "            self._simulate_partition(0)\n\t        elif self._max_cores == 1:\n\t            # Run single thread in main thread for easier debugging:\n\t            for partition_i in range(self._task.partition_count):\n\t                self._simulate_partition(partition_i)\n\t        else:\n\t            pool = multiprocessing.get_context(\"spawn\").Pool(processes=self._max_cores)\n\t            tasks = range(self._task.partition_count)\n\t            work = self._simulate_partition\n\t            for _ in tqdm.tqdm(pool.imap_unordered(work, tasks), total=len(tasks)):\n", "                pass\n\t            pool.close()\n\t        logging.info(\"Finished simulation\")\n\t    def _simulate_partition(self, partition_i: int):\n\t        # This function is executed within a thread\n\t        # Need to re-configure logger because we're in a thread:\n\t        self._configure_logger()\n\t        logging.debug(\"Starting partition %s of %s\", partition_i, self._task.partition_count)\n\t        if isinstance(self._task, PreTrainingTask):\n\t            self._simulate_person_set(partition_i=partition_i,\n", "                                      persons_per_partition=self._task.person_count // self._task.partition_count,\n\t                                      output_folder=os.path.join(self._root_folder, PRETRAINING_FOLDER))\n\t        elif isinstance(self._task, PredictionTask):\n\t            self._simulate_person_set(partition_i=partition_i,\n\t                                      persons_per_partition=self._task.train_person_count // self._task.partition_count,\n\t                                      output_folder=os.path.join(self._root_folder, TRAIN_FOLDER))\n\t            self._simulate_person_set(partition_i=partition_i,\n\t                                      persons_per_partition=self._task.test_person_count // self._task.partition_count,\n\t                                      output_folder=os.path.join(self._root_folder, TEST_FOLDER))\n\t        else:\n", "            raise ValueError(\"Unknown task type: %s\" % type(self._task))\n\t        logging.debug(\"Finished partition %s of %s\", partition_i, self._task.partition_count)\n\t    def _simulate_person_set(self,\n\t                             partition_i: int,\n\t                             persons_per_partition: int,\n\t                             output_folder: str):\n\t        if isinstance(self._task, PredictionTask):\n\t            self._cdm_data = cdm_data.CdmDataWithLabels()\n\t        else:\n\t            self._cdm_data = cdm_data.CdmData()\n", "        for i in range(persons_per_partition * partition_i, persons_per_partition * (partition_i + 1)):\n\t            self._simulate_person(person_id=i)\n\t        self._cdm_data.log_statistics(partition_i=partition_i)\n\t        self._cdm_data.write_to_parquet(output_folder, partition_i)\n\t    def save_to_json(self, file_name: str):\n\t        with open(file_name, \"w\") as f:\n\t            to_save = {\n\t                \"simulation_settings\": self._settings.__dict__,\n\t                \"state_count\": self._state_count,\n\t                \"concept_ids\": self._concept_ids.tolist(),\n", "                \"serious_concept_idx\": self._serious_concept_idx.tolist(),\n\t                \"initial_state_probabilities\": self._initial_state_probabilities.tolist(),\n\t                \"dynamic_state_exit_probabilities\": self._dynamic_state_exit_probabilities.tolist(),\n\t                \"dynamic_state_entry_coefs\": [matrix.tolist() for matrix in self._dynamic_state_entry_coefs],\n\t                \"concept_emmision_coefs\": [matrix.tolist() for matrix in self._concept_emmision_coefs]\n\t            }\n\t            json.dump(to_save, f)\n\tdef main(args: List[str]):\n\t    config = configparser.ConfigParser()\n\t    with open(args[0]) as file:  # Explicitly opening file so error is thrown when not found\n", "        config.read_file(file)\n\t    if config[\"simulation\"].get(\"json_file_name\") == \"\":\n\t        settings = SimulationSettings(\n\t            dynamic_state_count=config[\"simulation\"].getint(\"dynamic_state_count\"),\n\t            fixed_state_count=config[\"simulation\"].getint(\"fixed_state_count\"),\n\t            concept_count=config[\"simulation\"].getint(\"concept_count\"),\n\t            serious_concept_count=config[\"simulation\"].getint(\"serious_concept_count\"),\n\t            visit_multiplier=config[\"simulation\"].getint(\"visit_multiplier\"),\n\t            days_to_simulate=config[\"simulation\"].getint(\"days_to_simulate\"))\n\t        json_file_name = None\n", "    else:\n\t        settings = None\n\t        json_file_name = config[\"simulation\"].get(\"json_file_name\")\n\t        config.remove_section(\"simulation\")\n\t    simulator = Simulator(root_folder=config[\"system\"].get(\"root_folder\"),\n\t                          settings=settings,\n\t                          json_file_name=json_file_name,\n\t                          log_verbosity=config[\"debug\"].getint(\"log_verbosity\"),\n\t                          max_cores=config[\"system\"].getint(\"max_cores\"))\n\t    # Log config after initializing cdm_data_processor so logger is initialized:\n", "    logger.log_config(config)\n\t    if config[\"debug\"].getboolean(\"profile\"):\n\t        simulator.set_profile(True)\n\t    if config[\"pre-training data generation\"].getboolean(\"generate_pre_training_data\"):\n\t        task = PreTrainingTask(partition_count=config[\"pre-training data generation\"].getint(\"partition_count\"),\n\t                               person_count=config[\"pre-training data generation\"].getint(\"person_count\"))\n\t        simulator.simulate(task)\n\t    if config[\"prediction data generation\"].getboolean(\"generate_prediction_data\"):\n\t        task = PredictionTask(partition_count=config[\"prediction data generation\"].getint(\"partition_count\"),\n\t                              train_person_count=config[\"prediction data generation\"].getint(\"train_person_count\"),\n", "                              test_person_count=config[\"prediction data generation\"].getint(\"test_person_count\"),\n\t                              prediction_window=config[\"prediction data generation\"].getint(\"prediction_window\"))\n\t        simulator.simulate(task)\n\tif __name__ == \"__main__\":\n\t    if len(sys.argv) != 2:\n\t        raise Exception(\"Must provide path to ini file as argument\")\n\t    else:\n\t        main(sys.argv[1:])\n"]}
{"filename": "simulating/__init__.py", "chunked_list": []}
{"filename": "simulating/cdm_data.py", "chunked_list": ["\"\"\"\n\tThis module contains the CdmData class, which is used to store the data that is generated by the simulation. The\n\tdata is stored in dynamic arrays, which are then concatenated into a single numpy array when the simulation is\n\tcomplete. The data is then written to parquet files.\n\t\"\"\"\n\timport os\n\tfrom collections import defaultdict\n\tfrom typing import Type\n\timport logging\n\timport numpy as np\n", "import pyarrow as pa\n\timport pyarrow.parquet as pq\n\tclass DynamicArray:\n\t    \"\"\"\n\t    A dynamic array that can be appended to one at a time. The array is split into blocks of a fixed size, and the\n\t    blocks are concatenated when the array is collected.\n\t    \"\"\"\n\t    BLOCK_SIZE = 1000\n\t    def __init__(self, dtype: Type = np.float64):\n\t        self._dtype = dtype\n", "        self._blocks = []\n\t        self._current_block = self._create_block()\n\t        self._cursor = -1\n\t    def _create_block(self):\n\t        if self._dtype == np.datetime64:\n\t            return np.full(shape=self.BLOCK_SIZE, fill_value=np.datetime64(\"2010-01-01\"))\n\t        else:\n\t            return np.zeros(self.BLOCK_SIZE, dtype=self._dtype)\n\t    def append(self, value):\n\t        self._cursor += 1\n", "        if self._cursor >= self.BLOCK_SIZE:\n\t            self._blocks.append(self._current_block)\n\t            self._current_block = self._create_block()\n\t            self._cursor = 0\n\t        self._current_block[self._cursor] = value\n\t    def __len__(self):\n\t        return len(self._blocks) * self.BLOCK_SIZE + self._cursor + 1\n\t    def collect(self) -> np.ndarray:\n\t        return np.concatenate(self._blocks + [self._current_block[:self._cursor + 1]])\n\tdef _create_folder_if_not_exists(folder: str):\n", "    if not os.path.exists(folder):\n\t        os.makedirs(folder, exist_ok=True)\n\tclass CdmData:\n\t    \"\"\"\n\t    The data that is generated by the simulation.\n\t    \"\"\"\n\t    def __init__(self):\n\t        self._person_person_id = DynamicArray(np.int64)\n\t        self._person_year_of_birth = DynamicArray(np.int32)\n\t        self._person_month_of_birth = DynamicArray(np.int32)\n", "        self._person_day_of_birth = DynamicArray(np.int32)\n\t        self._person_gender_concept_id = DynamicArray(np.int32)\n\t        self._visit_occurrence_person_id = DynamicArray(np.int64)\n\t        self._visit_occurrence_visit_occurrence_id = DynamicArray(np.int64)\n\t        self._visit_occurrence_visit_start_date = DynamicArray(np.datetime64)\n\t        self._visit_occurrence_visit_end_date = DynamicArray(np.datetime64)\n\t        self._visit_occurrence_visit_concept_id = DynamicArray(np.int32)\n\t        self._condition_occurrence_person_id = DynamicArray(np.int64)\n\t        self._condition_occurrence_visit_occurrence_id = DynamicArray(np.int64)\n\t        self._condition_occurrence_condition_start_date = DynamicArray(np.datetime64)\n", "        self._condition_occurrence_condition_concept_id = DynamicArray(np.int32)\n\t        self._observation_period_person_id = DynamicArray(np.int64)\n\t        self._observation_period_observation_period_id = DynamicArray(np.int64)\n\t        self._observation_period_observation_period_start_date = DynamicArray(np.datetime64)\n\t        self._observation_period_observation_period_end_date = DynamicArray(np.datetime64)\n\t    def add_person(self, person_id: int, year_of_birth: int, month_of_birth: int, day_of_birth: int,\n\t                   gender_concept_id: int):\n\t        self._person_person_id.append(person_id)\n\t        self._person_year_of_birth.append(year_of_birth)\n\t        self._person_month_of_birth.append(month_of_birth)\n", "        self._person_day_of_birth.append(day_of_birth)\n\t        self._person_gender_concept_id.append(gender_concept_id)\n\t    def add_visit_occurrence(self, person_id: int, visit_occurrence_id: int, visit_start_date: np.datetime64,\n\t                             visit_end_date: np.datetime64, visit_concept_id: int):\n\t        self._visit_occurrence_person_id.append(person_id)\n\t        self._visit_occurrence_visit_occurrence_id.append(visit_occurrence_id)\n\t        self._visit_occurrence_visit_start_date.append(visit_start_date)\n\t        self._visit_occurrence_visit_end_date.append(visit_end_date)\n\t        self._visit_occurrence_visit_concept_id.append(visit_concept_id)\n\t    def add_condition_occurrence(self, person_id: int, visit_occurrence_id: int, condition_start_date: np.datetime64,\n", "                                 condition_concept_id: int):\n\t        self._condition_occurrence_person_id.append(person_id)\n\t        self._condition_occurrence_visit_occurrence_id.append(visit_occurrence_id)\n\t        self._condition_occurrence_condition_start_date.append(condition_start_date)\n\t        self._condition_occurrence_condition_concept_id.append(condition_concept_id)\n\t    def add_observation_period(self, person_id: int, observation_period_id: int,\n\t                               observation_period_start_date: np.datetime64,\n\t                               observation_period_end_date: np.datetime64):\n\t        self._observation_period_person_id.append(person_id)\n\t        self._observation_period_observation_period_id.append(observation_period_id)\n", "        self._observation_period_observation_period_start_date.append(observation_period_start_date)\n\t        self._observation_period_observation_period_end_date.append(observation_period_end_date)\n\t    def write_to_parquet(self, root_folder: str, partition_i: int):\n\t        person = pa.Table.from_arrays(arrays=[\n\t            self._person_person_id.collect(),\n\t            self._person_year_of_birth.collect(),\n\t            self._person_month_of_birth.collect(),\n\t            self._person_day_of_birth.collect(),\n\t            self._person_gender_concept_id.collect()],\n\t            names=[\"person_id\", \"year_of_birth\", \"month_of_birth\", \"day_of_birth\", \"gender_concept_id\"])\n", "        visit_occurrence = pa.Table.from_arrays(arrays=[\n\t            self._visit_occurrence_person_id.collect(),\n\t            self._visit_occurrence_visit_occurrence_id.collect(),\n\t            self._visit_occurrence_visit_start_date.collect(),\n\t            self._visit_occurrence_visit_end_date.collect(),\n\t            self._visit_occurrence_visit_concept_id.collect()],\n\t            names=[\"person_id\", \"visit_occurrence_id\", \"visit_start_date\", \"visit_end_date\", \"visit_concept_id\"])\n\t        condition_occurrence = pa.Table.from_arrays(arrays=[\n\t            self._condition_occurrence_person_id.collect(),\n\t            self._condition_occurrence_visit_occurrence_id.collect(),\n", "            self._condition_occurrence_condition_start_date.collect(),\n\t            self._condition_occurrence_condition_concept_id.collect()],\n\t            names=[\"person_id\", \"visit_occurrence_id\", \"condition_start_date\", \"condition_concept_id\"])\n\t        observation_period = pa.Table.from_arrays(arrays=[\n\t            self._observation_period_person_id.collect(),\n\t            self._observation_period_observation_period_id.collect(),\n\t            self._observation_period_observation_period_start_date.collect(),\n\t            self._observation_period_observation_period_end_date.collect()],\n\t            names=[\"person_id\", \"observation_period_id\", \"observation_period_start_date\",\n\t                   \"observation_period_end_date\"])\n", "        file_name = \"part{:04d}.parquet\".format(partition_i + 1)\n\t        _create_folder_if_not_exists(os.path.join(root_folder, \"person\"))\n\t        pq.write_table(person, os.path.join(root_folder, \"person\", file_name))\n\t        _create_folder_if_not_exists(os.path.join(root_folder, \"visit_occurrence\"))\n\t        pq.write_table(visit_occurrence, os.path.join(root_folder, \"visit_occurrence\", file_name))\n\t        _create_folder_if_not_exists(os.path.join(root_folder, \"condition_occurrence\"))\n\t        pq.write_table(condition_occurrence, os.path.join(root_folder, \"condition_occurrence\", file_name))\n\t        _create_folder_if_not_exists(os.path.join(root_folder, \"observation_period\"))\n\t        pq.write_table(observation_period, os.path.join(root_folder, \"observation_period\", file_name))\n\t    def log_statistics(self, partition_i: int):\n", "        logging.debug(\"Partition %s persons: %s\", partition_i, len(self._person_person_id))\n\t        logging.debug(\"Partition %s visit_occurrences: %s\", partition_i, len(self._visit_occurrence_person_id))\n\t        total_days = np.sum(np.subtract(self._observation_period_observation_period_end_date.collect(),\n\t                                        self._observation_period_observation_period_start_date.collect()).view(\n\t            np.int64))\n\t        logging.debug(\"Partition %s mean days between visits: %.1f\", partition_i,\n\t                      total_days / len(self._visit_occurrence_person_id))\n\t        logging.debug(\"Partition %s mean concepts per visit: %.1f\", partition_i,\n\t                      len(self._condition_occurrence_person_id) / len(self._visit_occurrence_person_id))\n\t        logging.debug(\"Partition %s unique concept count: %s\", partition_i,\n", "                      len(np.unique(self._condition_occurrence_condition_concept_id.collect())))\n\t        logging.debug(\"Partition %s percent ER visits: %.1f%%\", partition_i,\n\t                      100 * sum(self._visit_occurrence_visit_concept_id.collect() == 9203) /\n\t                      len(self._visit_occurrence_person_id))\n\tclass _Labels:\n\t    person_id: DynamicArray\n\t    label: DynamicArray\n\t    def __init__(self):\n\t        super().__init__()\n\t        self.person_id = DynamicArray(np.int64)\n", "        self.label = DynamicArray(bool)\n\tclass CdmDataWithLabels(CdmData):\n\t    def __init__(self):\n\t        super().__init__()\n\t        self._concept_id_to_labels: defaultdict[int, _Labels] = defaultdict(_Labels)\n\t    def add_label(self, person_id: int, concept_id: int, label: bool):\n\t        labels = self._concept_id_to_labels[concept_id]\n\t        labels.person_id.append(person_id)\n\t        labels.label.append(label)\n\t    def write_to_parquet(self, root_folder: str, partition_i: int):\n", "        super().write_to_parquet(root_folder, partition_i)\n\t        for concept_id, labels in self._concept_id_to_labels.items():\n\t            table = pa.Table.from_arrays(arrays=[\n\t                labels.person_id.collect(),\n\t                labels.label.collect()],\n\t                names=[\"person_id\", \"label\"])\n\t            folder_name = \"label_c{:04d}\".format(concept_id)\n\t            _create_folder_if_not_exists(os.path.join(root_folder, folder_name))\n\t            file_name = \"part{:04d}.parquet\".format(partition_i + 1)\n\t            pq.write_table(table, os.path.join(root_folder, folder_name, file_name))\n", "    def log_statistics(self, partition_i: int):\n\t        super().log_statistics(partition_i)\n\t        means = DynamicArray(np.float64)\n\t        for labels in self._concept_id_to_labels.values():\n\t            means.append(np.mean(labels.label.collect()))\n\t        logging.debug(\"Partition %s percent labels positive: %.1f%%\", partition_i, 100 * np.mean(means.collect()))\n"]}
{"filename": "cdm_processing/cdm_processor_utils.py", "chunked_list": ["import os\n\tfrom typing import List, Dict\n\timport pyarrow.parquet as pq\n\timport pyarrow.compute as pc\n\timport pyarrow as pa\n\timport duckdb\n\tPERSON = \"person\"\n\tPERSON_ID = \"person_id\"\n\tLOGGER_FILE_NAME = \"_cdm_processing_log.txt\"  # Start with underscore so ignored by Parquet\n\tPROFILE_MAX_PERSONS = 1000\n", "START_DATE_FIELDS = {\n\t    \"observation_period\": \"observation_period_start_date\",\n\t    \"visit_occurrence\": \"visit_start_date\",\n\t    \"condition_occurrence\": \"condition_start_date\",\n\t    \"drug_exposure\": \"drug_exposure_start_date\",\n\t    \"procedure_occurrence\": \"procedure_date\",\n\t    \"device_exposure\": \"device_exposure_start_date\",\n\t    \"measurement\": \"measurement_date\",\n\t    \"observation\": \"observation_date\",\n\t    \"death\": \"death_date\",\n", "}\n\tCONCEPT_ID_FIELDS = {\n\t    \"visit_occurrence\": \"visit_concept_id\",\n\t    \"condition_occurrence\": \"condition_concept_id\",\n\t    \"drug_exposure\": \"drug_concept_id\",\n\t    \"procedure_occurrence\": \"procedure_concept_id\",\n\t    \"device_exposure\": \"device_concept_id\",\n\t    \"measurement\": \"measurement_concept_id\",\n\t    \"observation\": \"observation_concept_id\",\n\t    \"death\": \"death_concept_id\",\n", "}\n\tSTART_DATE = \"start_date\"\n\tCONCEPT_ID = \"concept_id\"\n\tDOMAIN_TABLES = [\n\t    \"condition_occurrence\",\n\t    \"drug_exposure\",\n\t    \"procedure_occurrence\",\n\t    \"device_exposure\",\n\t    \"measurement\",\n\t    \"observation\",\n", "    \"death\",\n\t]\n\tDEATH = \"death\"\n\tDEATH_CONCEPT_ID = \"death_concept_id\"\n\tDEATH_CONCEPT_ID_VALUE = 4306655\n\tINTERNAL_VISIT_ID = \"internal_visit_id\"\n\tVISIT_OCCURRENCE = \"visit_occurrence\"\n\tVISIT_OCCURRENCE_ID = \"visit_occurrence_id\"\n\tCONCEPT = \"concept\"\n\tCONCEPT_ANCESTOR = \"concept_ancestor\"\n", "YEAR_OF_BIRTH = \"year_of_birth\"\n\tMONTH_OF_BIRTH = \"month_of_birth\"\n\tDAY_OF_BIRTH = \"day_of_birth\"\n\tDATE_OF_BIRTH = \"date_of_birth\"\n\tSOURCE_CONCEPT_ID = \"source_concept_id\"\n\tTARGET_CONCEPT_ID = \"target_concept_id\"\n\tdef add_date_of_birth(person: pa.Table) -> pa.Table:\n\t    \"\"\"\n\t    Compute the date of birth from a person table, and add it as a column called 'date_of_birth'.\n\t    \"\"\"\n", "    dob = pc.strptime(\n\t        pc.binary_join_element_wise(\n\t            pc.cast(person[YEAR_OF_BIRTH], pa.string()),\n\t            pc.cast(pc.coalesce(person[MONTH_OF_BIRTH], 1), pa.string()),\n\t            pc.cast(pc.coalesce(person[DAY_OF_BIRTH], 1), pa.string()),\n\t            \"-\"\n\t        ),\n\t        format=\"%Y-%m-%d\",\n\t        unit=\"s\"\n\t    )\n", "    return person.append_column(DATE_OF_BIRTH, dob)\n\tdef union_domain_tables(cdm_tables: Dict[str, pa.Table]) -> pa.Table:\n\t    \"\"\"\n\t    Combines all domain tables into a single table. For this, column names will be normalized first.\n\t    Entries in the death table will automatically be assigned a concept ID (4306655).\n\t    Args:\n\t        cdm_tables: A dictionary, mapping from CDM table name to table data.\n\t    Returns:\n\t        A table with a person_id, concept_id, visit_occurrence_id, and a start_date column.\n\t    \"\"\"\n", "    available_domain_tables = list(set(DOMAIN_TABLES) & set(cdm_tables))\n\t    normalized_tables = []\n\t    if DEATH in cdm_tables:\n\t        death = cdm_tables[DEATH]\n\t        death = death.select([PERSON_ID, START_DATE_FIELDS[DEATH]]).rename_columns(\n\t            [PERSON_ID, START_DATE]\n\t        ).append_column(\n\t            CONCEPT_ID,\n\t            pa.array([DEATH_CONCEPT_ID_VALUE] * len(death), pa.int32())\n\t        ).append_column(\n", "            VISIT_OCCURRENCE_ID,\n\t            pa.nulls(len(death), pa.int64())\n\t        )\n\t        available_domain_tables.remove(DEATH)\n\t        normalized_tables.append(death)\n\t    for table in available_domain_tables:\n\t        normalized_table = cdm_tables[table].select(\n\t            [PERSON_ID, START_DATE_FIELDS[table], CONCEPT_ID_FIELDS[table], VISIT_OCCURRENCE_ID]).rename_columns(\n\t            [PERSON_ID, START_DATE, CONCEPT_ID, VISIT_OCCURRENCE_ID]\n\t        )\n", "        normalized_tables.append(normalized_table)\n\t    event_table = pa.concat_tables(normalized_tables)\n\t    return event_table\n\tdef remove_concepts(event_table: pa.Table, concept_ids: List[int]) -> tuple[pa.Table, int]:\n\t    \"\"\"\n\t    Removes all rows from a table that have a concept ID in the given list.\n\t    Args:\n\t        event_table: A table with a person_id, concept_id and a start_date column.\n\t        concept_ids: A list of concept IDs to remove.\n\t    Returns:\n", "        A tuple with the count of removed rows, and a table with the same schema as the input table, but without the\n\t            rows with the given concept IDs.\n\t    \"\"\"\n\t    count_before = len(event_table)\n\t    result = event_table.filter(~pc.is_in(pc.field(CONCEPT_ID), pa.array(concept_ids)))\n\t    return result, count_before - len(result)\n\tdef remove_duplicates(event_table: pa.Table) -> tuple[pa.Table, int]:\n\t    \"\"\"\n\t    Removes all rows with duplicate person_id, concept_id, and start_date combinations.\n\t    Args:\n", "        event_table: A table with a person_id, concept_id and a start_date column.\n\t    Returns:\n\t        A tuple with the count of removed rows, and a table with the same schema as the input table, but without the\n\t            duplicate rows.\n\t    \"\"\"\n\t    count_before = len(event_table)\n\t    con = duckdb.connect(database=':memory:', read_only=False)\n\t    con.execute(\"SET enable_progress_bar = false\")\n\t    con.register(\"event_table\", event_table)\n\t    sql = \"SELECT DISTINCT * FROM event_table\"\n", "    result = con.execute(sql).arrow()\n\t    duckdb.close(con)\n\t    return result, count_before - len(result)\n\tdef link_events_to_visits(event_table: pa.Table,\n\t                          visit_occurrence: pa.Table,\n\t                          mising_visit_concept_id: int = 0) -> tuple[pa.Table, pa.Table, Dict[str, int]]:\n\t    \"\"\"\n\t    Links events to visits by finding the visit that contains the event's start date.\n\t    Args:\n\t        event_table: A table with a person_id, concept_id and a start_date column.\n", "        visit_occurrence: The CDM visit_occurrence table\n\t        mising_visit_concept_id: The concept ID to use for visits that were generated because events could not be\n\t            linked to an existing visit.\n\t    Returns:\n\t        A tuple of 3 items: (1) the combined table with an additional internal_visit_id column, (2) the visit_occurrence\n\t        table with an additional internal_visit_id column and added visits if some events could not be mapped to\n\t        existing visits, and (3) a dataframe with the number of visits mapped by ID, data, or to new visits.\n\t    \"\"\"\n\t    # DuckDb seems to be the fastest way (by far) to do these join, especially the one on dates\n\t    visit_occurrence = visit_occurrence.append_column(\"internal_visit_id\",\n", "                                                      pa.array(range(len(visit_occurrence)), pa.int64()))\n\t    con = duckdb.connect(database=':memory:', read_only=False)\n\t    con.execute(\"SET enable_progress_bar = false\")\n\t    con.register(\"event_table\", event_table)\n\t    con.register(\"visit_occurrence\", visit_occurrence)\n\t    # Join by visit_occurrence_id:\n\t    sql = \"CREATE TABLE joined_1 AS \" \\\n\t          \"SELECT event_table.*, \" \\\n\t          \"   internal_visit_id AS id_from_id \" \\\n\t          \"FROM event_table \" \\\n", "          \"LEFT JOIN visit_occurrence \" \\\n\t          \"  ON event_table.visit_occurrence_id = visit_occurrence.visit_occurrence_id \"\n\t    con.execute(sql)\n\t    # Join by date:\n\t    sql = \"CREATE TABLE joined_2 AS \" \\\n\t          \"SELECT joined_1.person_id, \" \\\n\t          \"  joined_1.start_date, \" \\\n\t          \"  joined_1.concept_id, \" \\\n\t          \"  joined_1.id_from_id, \" \\\n\t          \"  MIN(visit_occurrence.internal_visit_id) AS id_from_date \" \\\n", "          \"FROM joined_1 \" \\\n\t          \"LEFT JOIN visit_occurrence \" \\\n\t          \"  ON joined_1.person_id = visit_occurrence.person_id \" \\\n\t          \"    AND start_date >= visit_start_date \" \\\n\t          \"    AND start_date <= visit_end_date \" \\\n\t          \"GROUP BY joined_1.person_id, \" \\\n\t          \"  joined_1.start_date,\" \\\n\t          \"  joined_1.concept_id, \" \\\n\t          \"  joined_1.id_from_id\"\n\t    con.execute(sql)\n", "    # Create missing visits from unmapped event dates:\n\t    sql = \"CREATE TABLE missing_visits AS \" \\\n\t          \"SELECT person_id, \" \\\n\t          \"  start_date AS visit_start_date, \" \\\n\t          \"  start_date AS visit_end_date,\" \\\n\t          \"  {max_internal_visit_id} + ROW_NUMBER() OVER(ORDER BY person_id, start_date) AS internal_visit_id, \" \\\n\t          \"  CAST({mising_visit_concept_id} AS BIGINT) AS visit_concept_id \" \\\n\t          \"FROM ( \" \\\n\t          \"  SELECT DISTINCT person_id,\" \\\n\t          \"    start_date \" \\\n", "          \"  FROM joined_2 \" \\\n\t          \"  WHERE id_from_id IS NULL\" \\\n\t          \"     AND id_from_date IS NULL \" \\\n\t          \") AS missing_visits\"\n\t    sql = sql.format(max_internal_visit_id=len(visit_occurrence) - 1, mising_visit_concept_id=mising_visit_concept_id)\n\t    con.execute(sql)\n\t    new_visits = con.execute(\"SELECT COUNT(*) FROM missing_visits\").fetchone()[0]\n\t    existing_visits = len(visit_occurrence)\n\t    # Join to the missing visits:\n\t    sql = \"CREATE TABLE joined_3 AS \" \\\n", "          \"SELECT joined_2.*, \" \\\n\t          \"   missing_visits.internal_visit_id AS id_from_new_visit \" \\\n\t          \"FROM joined_2 \" \\\n\t          \"LEFT JOIN missing_visits \" \\\n\t          \"  ON joined_2.person_id = missing_visits.person_id \" \\\n\t          \"    AND start_date = visit_start_date\"\n\t    con.execute(sql)\n\t    sql = \"SELECT person_id, \" \\\n\t          \"  concept_id, \" \\\n\t          \"  start_date, \" \\\n", "          \"  COALESCE(id_from_id, id_from_date, id_from_new_visit) AS internal_visit_id \" \\\n\t          \"FROM joined_3\"\n\t    event_table = con.execute(sql).arrow()\n\t    sql = \"SELECT person_id, \" \\\n\t          \"  visit_start_date, \" \\\n\t          \"  visit_end_date, \" \\\n\t          \"  visit_concept_id, \" \\\n\t          \"  internal_visit_id \" \\\n\t          \"FROM visit_occurrence \" \\\n\t          \"\" \\\n", "          \"UNION ALL \" \\\n\t          \"\" \\\n\t          \"SELECT person_id, \" \\\n\t          \"  visit_start_date, \" \\\n\t          \"  visit_end_date, \" \\\n\t          \"  visit_concept_id, \" \\\n\t          \"  internal_visit_id \" \\\n\t          \"FROM missing_visits\"\n\t    visit_occurrence = con.execute(sql).arrow()\n\t    sql = \"SELECT CAST(SUM(mapped_by_id) AS INT) AS mapped_by_id, \" \\\n", "          \"  CAST(SUM(mapped_by_date) AS INT) AS mapped_by_date, \" \\\n\t          \"  CAST(SUM(mapped_by_new_visit) AS INT) AS mapped_to_new_visit \" \\\n\t          \"FROM ( \" \\\n\t          \"  SELECT CASE WHEN id_from_id IS NOT NULL THEN 1 ELSE 0 END AS mapped_by_id, \" \\\n\t          \"    CASE WHEN id_from_date IS NOT NULL AND id_from_id IS NULL THEN 1 ELSE 0 END AS mapped_by_date, \" \\\n\t          \"    CASE WHEN id_from_new_visit IS NOT NULL THEN 1 ELSE 0 END AS mapped_by_new_visit \" \\\n\t          \"  FROM joined_3 \" \\\n\t          \") AS counts\"\n\t    statistics = con.execute(sql).fetchdf().iloc[0].to_dict()\n\t    statistics[\"new_visits\"] = new_visits\n", "    statistics[\"existing_visits\"] = existing_visits\n\t    con.execute(\"DROP TABLE joined_1\")\n\t    con.execute(\"DROP TABLE joined_2\")\n\t    con.execute(\"DROP TABLE joined_3\")\n\t    con.execute(\"DROP TABLE missing_visits\")\n\t    duckdb.close(con)\n\t    return event_table, visit_occurrence, statistics\n\tdef load_mapping_to_ingredients(cdm_folder: str) -> pa.Table:\n\t    \"\"\"\n\t    Uses the concept and concept_ancestor table to construct a mapping from drugs to ingredients.\n", "    Args:\n\t        cdm_folder: The path where the CDM Parquet files are saved (using the GeneralPretrainModelTools packages).\n\t    Returns:\n\t        A dictionary from drug concept ID to ingredient concept ID.\n\t    \"\"\"\n\t    ingredients = pq.read_table(\n\t        os.path.join(cdm_folder, CONCEPT),\n\t        columns=[\"concept_id\"],\n\t        filters=[(\"concept_class_id\", \"==\", \"Ingredient\")],\n\t    )\n", "    concept_ancestor = pq.read_table(os.path.join(cdm_folder, CONCEPT_ANCESTOR))\n\t    concept_ancestor = concept_ancestor.join(\n\t        ingredients,\n\t        keys=[\"ancestor_concept_id\"],\n\t        right_keys=[\"concept_id\"],\n\t        join_type=\"inner\",\n\t    ).select([\"descendant_concept_id\", \"ancestor_concept_id\"]).rename_columns(\n\t        [SOURCE_CONCEPT_ID, TARGET_CONCEPT_ID])\n\t    return concept_ancestor\n\tdef map_concepts(cdm_table: pa.Table, concept_id_field: str, mapping: pa.Table) -> pa.Table:\n", "    \"\"\"\n\t    Maps a concept ID field to another concept ID using a mapping table.\n\t    Args:\n\t        cdm_table: The table to map.\n\t        concept_id_field: The name of the field containing the concept ID.\n\t        mapping: A table with two columns: source_concept_id and target_concept_id.\n\t    Returns:\n\t        A table with the same columns as cdm_table, but with the values in the concept ID field replaced by the\n\t        target_concept_id. Any records that did not have a matching concept were removed. Any records that map to\n\t        multiple concepts are duplicated.\n", "    \"\"\"\n\t    intermediate_columns = cdm_table.column_names\n\t    intermediate_columns.remove(concept_id_field)\n\t    intermediate_columns.append(TARGET_CONCEPT_ID)\n\t    final_columns = cdm_table.column_names\n\t    final_columns.remove(concept_id_field)\n\t    final_columns.append(concept_id_field)\n\t    return cdm_table.join(\n\t        mapping,\n\t        keys=[concept_id_field],\n", "        right_keys=[SOURCE_CONCEPT_ID],\n\t        join_type=\"inner\",\n\t    ).select(intermediate_columns).rename_columns(final_columns)\n"]}
{"filename": "cdm_processing/abstract_cdm_processor.py", "chunked_list": ["import cProfile\n\tfrom abc import ABC, abstractmethod\n\timport multiprocessing\n\timport os\n\tfrom typing import Dict\n\timport logging\n\timport pyarrow.parquet as pq\n\timport pyarrow as pa\n\timport tqdm\n\tfrom utils.logger import create_logger\n", "PERSON = \"person\"\n\tCDM_TABLES = [\n\t    \"person\",\n\t    \"observation_period\",\n\t    \"visit_occurrence\",\n\t    \"condition_occurrence\",\n\t    \"drug_exposure\",\n\t    \"procedure_occurrence\",\n\t    \"device_exposure\",\n\t    \"measurement\",\n", "    \"observation\",\n\t    \"death\",\n\t]\n\tLOGGER_FILE_NAME = \"_cdm_processing_log.txt\"  # Start with underscore so ignored by Parquet\n\tclass AbstractCdmDataProcessor(ABC):\n\t    \"\"\"\n\t    An abstract class that implements iterating over partitioned data as generated by the\n\t    GeneralPretrainedModelTools R package. It divides the partitions over various threads,\n\t    and calls the _process_parition_cdm_data() function with all data of a partition.\n\t    Args:\n", "        cdm_data_path: The path where the CDM Parquet files are saved (using the GeneralPretrainModelTools packages).\n\t        max_cores: The maximum number of CPU cores to use. If set to -1, all multihreading code will be bypassed for\n\t                   easier debugging.\n\t    \"\"\"\n\t    def __init__(self, cdm_data_path: str, output_path: str, max_cores: int = 1):\n\t        if not os.path.exists(output_path):\n\t            os.makedirs(output_path)\n\t        self._cdm_data_path = cdm_data_path\n\t        self._max_cores = max_cores\n\t        self._person_partition_count = 0\n", "        self._output_path = output_path\n\t        self._profile = False\n\t        self._configure_logger()\n\t    def set_profile(self, profile: bool):\n\t        self._profile = profile\n\t    def get_profile(self):\n\t        return self._profile\n\t    def _configure_logger(self):\n\t        create_logger(os.path.join(self._output_path, LOGGER_FILE_NAME))\n\t    @abstractmethod\n", "    def _process_parition_cdm_data(self, cdm_tables: Dict[str, pa.Table], partition_i: int):\n\t        # This functon is called for every parition (It is executed within a thread.)\n\t        pass\n\t    def process_cdm_data(self):\n\t        \"\"\"\n\t        Process the CDM data in the provided cdm_data_path.\n\t        \"\"\"\n\t        if self._profile:\n\t            cProfile.runctx(statement=\"self._process_cdm_data()\",\n\t                            locals={\"self\": self},\n", "                            globals={},\n\t                            filename=\"../stats\")\n\t        else:\n\t            self._process_cdm_data()\n\t    def _process_cdm_data(self):\n\t        self._get_partition_counts()\n\t        if self._profile:\n\t            logging.info(\"Profiling mode enabled, running first partition in single thread\")\n\t            self._process_partition(0)\n\t        elif self._max_cores == 1:\n", "            # Run single thread in main thread for easier debugging:\n\t            for partition_i in range(self._person_partition_count):\n\t                self._process_partition(partition_i)\n\t        else:\n\t            pool = multiprocessing.get_context(\"spawn\").Pool(processes=self._max_cores)\n\t            tasks = range(self._person_partition_count)\n\t            work = self._process_partition\n\t            for _ in tqdm.tqdm(pool.imap_unordered(work, tasks), total=len(tasks)):\n\t                pass\n\t            pool.close()\n", "        logging.info(\"Finished processing data\")\n\t    def _get_partition_counts(self):\n\t        files = os.listdir(os.path.join(self._cdm_data_path, PERSON))\n\t        self._person_partition_count = len(\n\t            list(filter(lambda x: \".parquet\" in x, files))\n\t        )\n\t        logging.info(\"Found %s partitions\", self._person_partition_count)\n\t    def _process_partition(self, partition_i: int):\n\t        # This function is executed within a thread\n\t        # Need to re-configure logger because we're in a thread:\n", "        self._configure_logger()\n\t        logging.debug(\"Starting partition %s of %s\", partition_i, self._person_partition_count)\n\t        file_name = \"part{:04d}.parquet\".format(partition_i + 1)\n\t        available_tables = [table for table in CDM_TABLES if table in os.listdir(self._cdm_data_path)]\n\t        cdm_tables = {table: pq.read_table(os.path.join(self._cdm_data_path, table, file_name)) for table in\n\t                      available_tables}\n\t        self._process_parition_cdm_data(cdm_tables, partition_i)\n\t        logging.debug(\"Finished partition %s of %s\", partition_i, self._person_partition_count)\n"]}
{"filename": "cdm_processing/__init__.py", "chunked_list": []}
{"filename": "cdm_processing/cehr_bert_cdm_processor.py", "chunked_list": ["import os\n\timport sys\n\tfrom typing import Dict, List\n\timport logging\n\timport configparser\n\timport duckdb\n\timport pyarrow as pa\n\timport pyarrow.parquet as pq\n\tfrom cdm_processing.abstract_cdm_processor import AbstractCdmDataProcessor\n\timport cdm_processing.cdm_processor_utils as cdm_utils\n", "import utils.logger as logger\n\tdef _create_cehr_bert_tables(cdm_tables: Dict[str, pa.Table], event_table: pa.Table) -> pa.Table:\n\t    \"\"\"\n\t    Creates the table needed for the CEHR-BERT model.\n\t    Args:\n\t        cdm_tables: A dictionary of CDM tables, with the table name as key and the table as value. The person table\n\t            should have the date_of_birth field added using the add_date_of_birth_to_person_table() function.\n\t        event_table: The table with the clinical events. This is a combination across the CDM domain tables, as created\n\t            by the union_cdm_tables() function.\n\t    Returns:\n", "        A table with the data needed for the CEHR-BERT model.\n\t    \"\"\"\n\t    con = duckdb.connect(database=':memory:', read_only=False)\n\t    con.execute(\"SET enable_progress_bar = false\")\n\t    con.register(\"visit_occurrence\", cdm_tables[\"visit_occurrence\"])\n\t    con.register(\"observation_period_table\", cdm_tables[\"observation_period\"])\n\t    con.register(\"person\", cdm_tables[\"person\"])\n\t    con.register(\"event_table\", event_table)\n\t    sql = \"CREATE TABLE visits AS \" \\\n\t          \"SELECT visit_occurrence.*, \" \\\n", "          \"  observation_period_id, \" \\\n\t          \"  ROW_NUMBER() OVER (PARTITION BY observation_period_id ORDER BY visit_start_date) AS visit_rank \" \\\n\t          \"FROM visit_occurrence \" \\\n\t          \"INNER JOIN observation_period_table \" \\\n\t          \"  ON visit_occurrence.person_id = observation_period_table.person_id \" \\\n\t          \"    AND visit_occurrence.visit_start_date >= observation_period_table.observation_period_start_date \" \\\n\t          \"    AND visit_occurrence.visit_start_date <= observation_period_table.observation_period_end_date\"\n\t    con.execute(sql)\n\t    sql = \"CREATE TABLE interval_tokens AS \" \\\n\t          \"SELECT CASE \" \\\n", "          \"    WHEN days < 0 THEN 'W-1' \" \\\n\t          \"    WHEN days < 28 THEN 'W' || CAST(CAST(FLOOR(days / 7) AS INT) AS VARCHAR) \" \\\n\t          \"    WHEN days < 360 THEN 'M' || CAST(CAST(FLOOR(days / 30) AS INT) AS VARCHAR) \" \\\n\t          \"    ELSE 'LT' \" \\\n\t          \"  END AS concept_ids, \" \\\n\t          \"  0 AS visit_segments, \" \\\n\t          \"  0 AS dates, \" \\\n\t          \" -1 AS ages, \" \\\n\t          \"  visit_rank AS visit_concept_orders, \" \\\n\t          \"  CAST('0' AS VARCHAR) AS visit_concept_ids, \" \\\n", "          \"  -2 AS sort_order, \" \\\n\t          \"  observation_period_id, \" \\\n\t          \"  person_id \" \\\n\t          \"FROM (\" \\\n\t          \"  SELECT visits.visit_start_date - previous_visit.visit_end_date AS days,\" \\\n\t          \"    visits.* \" \\\n\t          \"  FROM visits \" \\\n\t          \"  INNER JOIN visits previous_visit\" \\\n\t          \"    ON visits.observation_period_id = previous_visit.observation_period_id \" \\\n\t          \"      AND visits.visit_rank = previous_visit.visit_rank + 1\" \\\n", "          \") intervals\"\n\t    con.execute(sql)\n\t    sql = \"CREATE TABLE start_tokens AS \" \\\n\t          \"SELECT 'VS' AS concept_ids, \" \\\n\t          \"  visit_rank % 2 + 1 AS visit_segments, \" \\\n\t          \"  DATE_DIFF('week', DATE '1970-01-01', visit_start_date) AS dates, \" \\\n\t          \"  DATE_DIFF('month', date_of_birth, visit_start_date) AS ages, \" \\\n\t          \"  visit_rank AS visit_concept_orders, \" \\\n\t          \"  CAST(visit_concept_id AS VARCHAR) AS visit_concept_ids, \" \\\n\t          \"  -1 AS sort_order, \" \\\n", "          \"  observation_period_id, \" \\\n\t          \"  person.person_id \" \\\n\t          \"FROM visits \" \\\n\t          \"INNER JOIN person \" \\\n\t          \"  ON visits.person_id = person.person_id\"\n\t    con.execute(sql)\n\t    sql = \"CREATE TABLE event_tokens AS \" \\\n\t          \"SELECT CAST(concept_id AS VARCHAR) AS concept_ids, \" \\\n\t          \"  visit_rank % 2 + 1 AS visit_segments, \" \\\n\t          \"  DATE_DIFF('week', DATE '1970-01-01', start_date) AS dates, \" \\\n", "          \"  DATE_DIFF('month', date_of_birth,start_date) AS ages, \" \\\n\t          \"  visit_rank AS visit_concept_orders, \" \\\n\t          \"  CAST(visit_concept_id AS VARCHAR) AS visit_concept_ids, \" \\\n\t          \"  concept_id AS sort_order, \" \\\n\t          \"  observation_period_id, \" \\\n\t          \"  person.person_id \" \\\n\t          \"FROM event_table \" \\\n\t          \"INNER JOIN visits \" \\\n\t          \"  ON event_table.internal_visit_id = visits.internal_visit_id \" \\\n\t          \"INNER JOIN person \" \\\n", "          \"  ON visits.person_id = person.person_id\"\n\t    con.execute(sql)\n\t    sql = \"CREATE TABLE end_tokens AS \" \\\n\t          \"SELECT 'VE' AS concept_ids, \" \\\n\t          \"  visit_rank % 2 + 1 AS visit_segments, \" \\\n\t          \"  DATE_DIFF('week', DATE '1970-01-01', visit_end_date) AS dates, \" \\\n\t          \"  DATE_DIFF('month', date_of_birth, visit_end_date) AS ages, \" \\\n\t          \"  visit_rank AS visit_concept_orders, \" \\\n\t          \"  CAST(visit_concept_id AS VARCHAR) AS visit_concept_ids, \" \\\n\t          \"  9223372036854775807 AS sort_order, \" \\\n", "          \"  observation_period_id, \" \\\n\t          \"  person.person_id \" \\\n\t          \"FROM visits \" \\\n\t          \"INNER JOIN person \" \\\n\t          \"  ON visits.person_id = person.person_id\"\n\t    con.execute(sql)\n\t    sql = \"SELECT *, \" \\\n\t          \"  ROW_NUMBER() OVER \" \\\n\t          \"    (PARTITION BY observation_period_id ORDER BY visit_concept_orders, sort_order) - 1  AS orders \" \\\n\t          \"FROM (\" \\\n", "          \"  SELECT * FROM interval_tokens \" \\\n\t          \"  UNION ALL \" \\\n\t          \"  SELECT * FROM start_tokens \" \\\n\t          \"  UNION ALL \" \\\n\t          \"  SELECT * FROM event_tokens \" \\\n\t          \"  UNION ALL \" \\\n\t          \"  SELECT * FROM end_tokens\" \\\n\t          \") tokens \" \\\n\t          \"ORDER BY observation_period_id, visit_concept_orders, sort_order\"\n\t    union_tokens = con.execute(sql).arrow()\n", "    con.execute(\"DROP TABLE visits\")\n\t    con.execute(\"DROP TABLE interval_tokens\")\n\t    con.execute(\"DROP TABLE start_tokens\")\n\t    con.execute(\"DROP TABLE event_tokens\")\n\t    con.execute(\"DROP TABLE end_tokens\")\n\t    duckdb.close(con)\n\t    cehr_bert_input = union_tokens.group_by(\"observation_period_id\").aggregate([\n\t        (\"person_id\", \"max\"),\n\t        (\"concept_ids\", \"list\"),\n\t        (\"visit_segments\", \"list\"),\n", "        (\"dates\", \"list\"),\n\t        (\"ages\", \"list\"),\n\t        (\"visit_concept_orders\", \"list\"),\n\t        (\"visit_concept_ids\", \"list\"),\n\t        (\"orders\", \"list\"),\n\t        (\"concept_ids\", \"count\"),\n\t        (\"visit_concept_orders\", \"max\")]).rename_columns(\n\t        [\"observation_period_id\",\n\t         \"person_id\",\n\t         \"concept_ids\",\n", "         \"visit_segments\",\n\t         \"dates\",\n\t         \"ages\",\n\t         \"visit_concept_orders\",\n\t         \"visit_concept_ids\",\n\t         \"orders\",\n\t         \"num_of_concepts\",\n\t         \"num_of_visits\"])\n\t    return cehr_bert_input\n\tclass CehrBertCdmDataProcessor(AbstractCdmDataProcessor):\n", "    \"\"\"\n\t    A re-implementation of the processor for CEHR-BERT (https://github.com/cumc-dbmi/cehr-bert)\n\t    \"\"\"\n\t    def __init__(self, cdm_data_path: str, output_path: str, max_cores: int = -1,\n\t                 map_drugs_to_ingredients: bool = False, concepts_to_remove: List[int] = [0]):\n\t        super().__init__(\n\t            cdm_data_path=cdm_data_path, output_path=output_path, max_cores=max_cores\n\t        )\n\t        self._map_drugs_to_ingredients = map_drugs_to_ingredients\n\t        self._concepts_to_remove = concepts_to_remove\n", "        if self._map_drugs_to_ingredients:\n\t            self._drug_mapping = cdm_utils.load_mapping_to_ingredients(self._cdm_data_path)\n\t    def _process_parition_cdm_data(self, cdm_tables: Dict[str, pa.Table], partition_i: int):\n\t        \"\"\"\n\t        Process a single partition of CDM data, and save the result to disk.\n\t        \"\"\"\n\t        cdm_tables[\"person\"] = cdm_utils.add_date_of_birth(cdm_tables[\"person\"])\n\t        if self._map_drugs_to_ingredients:\n\t            cdm_tables[\"drug_exposure\"] = cdm_utils.map_concepts(cdm_table=cdm_tables[\"drug_exposure\"],\n\t                                                                 concept_id_field=\"drug_concept_id\",\n", "                                                                 mapping=self._drug_mapping)\n\t        event_table = cdm_utils.union_domain_tables(cdm_tables)\n\t        event_table, removed_concepts = cdm_utils.remove_concepts(event_table=event_table,\n\t                                                                  concept_ids=self._concepts_to_remove)\n\t        event_table, removed_duplicates = cdm_utils.remove_duplicates(event_table=event_table)\n\t        event_table, visit_occurrence, mapping_stats = cdm_utils.link_events_to_visits(event_table=event_table,\n\t                                                                                       visit_occurrence=cdm_tables[\n\t                                                                                           \"visit_occurrence\"],\n\t                                                                                       mising_visit_concept_id=1)\n\t        cdm_tables[\"visit_occurrence\"] = visit_occurrence\n", "        cehr_bert_input = _create_cehr_bert_tables(cdm_tables=cdm_tables, event_table=event_table)\n\t        file_name = \"part{:04d}.parquet\".format(partition_i + 1)\n\t        pq.write_table(cehr_bert_input, os.path.join(self._output_path, file_name))\n\t        logging.debug(\"Partition %s persons: %s\", partition_i, len(cdm_tables[\"person\"]))\n\t        logging.debug(\"Partition %s observation periods: %s\", partition_i, len(cdm_tables[\"observation_period\"]))\n\t        logging.debug(\"Partition %s removed events having unwanted concept ID: %s\", partition_i, removed_concepts)\n\t        logging.debug(\"Partition %s removed duplicate events: %s\", partition_i, removed_duplicates)\n\t        logging.debug(\"Partition %s events mapped to visit by ID: %s\", partition_i, mapping_stats[\"mapped_by_id\"])\n\t        logging.debug(\"Partition %s events mapped to visit by date: %s\", partition_i, mapping_stats[\"mapped_by_date\"])\n\t        logging.debug(\"Partition %s events mapped to new visits: %s\", partition_i, mapping_stats[\"mapped_to_new_visit\"])\n", "        logging.debug(\"Partition %s existing visits: %s\", partition_i, mapping_stats[\"existing_visits\"])\n\t        logging.debug(\"Partition %s newly created visits: %s\", partition_i, mapping_stats[\"new_visits\"])\n\tdef main(args: List[str]):\n\t    config = configparser.ConfigParser()\n\t    with open(args[0]) as file:  # Explicitly opening file so error is thrown when not found\n\t        config.read_file(file)\n\t    cdm_data_processor = CehrBertCdmDataProcessor(\n\t        cdm_data_path=config[\"system\"].get(\"cdm_data_path\"),\n\t        max_cores=config[\"system\"].getint(\"max_cores\"),\n\t        output_path=config[\"system\"].get(\"output_path\"),\n", "        map_drugs_to_ingredients=config[\"mapping\"].getboolean(\"map_drugs_to_ingredients\"),\n\t        concepts_to_remove=[int(x) for x in config[\"mapping\"].get(\"concepts_to_remove\").split(\",\")],\n\t    )\n\t    # Log config after initializing cdm_data_processor so logger is initialized:\n\t    logger.log_config(config)\n\t    if config[\"debug\"].getboolean(\"profile\"):\n\t        cdm_data_processor.set_profile(True)\n\t    cdm_data_processor.process_cdm_data()\n\tif __name__ == \"__main__\":\n\t    if len(sys.argv) != 2:\n", "        raise Exception(\"Must provide path to ini file as argument\")\n\t    else:\n\t        main(sys.argv[1:])\n"]}
{"filename": "utils/logger.py", "chunked_list": ["import logging\n\timport sys\n\tdef _add_stream_handler(logger: logging.Logger):\n\t    stream_handler = logging.StreamHandler()\n\t    stream_handler.setLevel(logging.INFO)\n\t    logger.addHandler(stream_handler)\n\t    return logger\n\tdef _add_file_handler(logger: logging.Logger, log_file_name: str):\n\t    file_handler = logging.FileHandler(log_file_name, mode=\"a\")\n\t    formatter = logging.Formatter(\n", "        fmt=\"%(asctime)s - %(levelname)-8s %(message)s\", datefmt=\"%m-%d %H:%M\")\n\t    file_handler.setFormatter(formatter)\n\t    file_handler.setLevel(logging.DEBUG)\n\t    logger.addHandler(file_handler)\n\t    return logger\n\tdef create_logger(log_file_name: str):\n\t    \"\"\"\n\t    Sets up the root logger where it writes all logging events to file, and writing events at or above 'info' to\n\t    console. Events are appended to the log file.\n\t    Args:\n", "        log_file_name: The name of the file where the log will be written to.\n\t    \"\"\"\n\t    logger = logging.getLogger()\n\t    logger.setLevel(logging.DEBUG)\n\t    if not len(logger.handlers):\n\t        _add_file_handler(logger=logger, log_file_name=log_file_name)\n\t        _add_stream_handler(logger=logger)\n\t    sys.excepthook = handle_exception\n\tclass _ConfigLogger(object):\n\t    def log_config(self, config):\n", "        logging.info(\"Config:\")\n\t        config.write(self)\n\t    def write(self, data):\n\t        line = data.strip()\n\t        logging.info(line)\n\tdef log_config(config):\n\t    config_logger = _ConfigLogger()\n\t    config_logger.log_config(config)\n\tdef handle_exception(exc_type, exc_value, exc_traceback):\n\t    if not issubclass(exc_type, KeyboardInterrupt):\n", "        logger = logging.getLogger()\n\t        logger.critical(\"Uncaught exception\", exc_info=(exc_type, exc_value, exc_traceback))\n\t    sys.__excepthook__(exc_type, exc_value, exc_traceback)\n"]}
{"filename": "tests/test_cdm_data.py", "chunked_list": ["import shutil\n\timport tempfile\n\timport os\n\timport unittest\n\timport numpy as np\n\tfrom simulating import cdm_data\n\tclass TestCdmData(unittest.TestCase):\n\t    def setUp(self) -> None:\n\t        self.temp_folder = tempfile.mkdtemp()\n\t    def tearDown(self):\n", "        shutil.rmtree(self.temp_folder)\n\t    def test_dynamic_array(self):\n\t        x = cdm_data.DynamicArray()\n\t        for value in range(2000):\n\t            x.append(value)\n\t        assert len(x) == 2000\n\t        assert (x.collect() == np.asarray(range(2000))).all()\n"]}
{"filename": "tests/test_cdm_processor_utils.py", "chunked_list": ["from typing import List\n\timport unittest\n\timport pyarrow as pa\n\timport pyarrow.compute as pc\n\timport datetime as dt\n\timport cdm_processing.cdm_processor_utils as cdm_utils\n\tdef d(dates_as_strings: List[str]):\n\t    \"\"\"\n\t    Helper function: convert a list of dates as strings to a list of dates as datetime objects.\n\t    Args:\n", "        dates_as_strings: A list of dates as strings in the format YYYY-MM-DD.\n\t    Returns:\n\t        A list of dates as datetime objects.\n\t    \"\"\"\n\t    return pc.strptime(\n\t        dates_as_strings,\n\t        format=\"%Y-%m-%d\",\n\t        unit=\"s\"\n\t    )\n\tclass TestCdmProcessorUtils(unittest.TestCase):\n", "    def setUp(self) -> None:\n\t        person = pa.Table.from_pydict(\n\t            {\n\t                \"person_id\": [1],\n\t                \"year_of_birth\": [1970],\n\t                \"month_of_birth\": [5],\n\t                \"day_of_birth\": [7],\n\t                \"gender_concept_id\": [8507],\n\t            },\n\t            schema=pa.schema(\n", "                [\n\t                    (\"person_id\", pa.int64()),\n\t                    (\"year_of_birth\", pa.int32()),\n\t                    (\"month_of_birth\", pa.int32()),\n\t                    (\"day_of_birth\", pa.int32()),\n\t                    (\"gender_concept_id\", pa.int32()),\n\t                ]\n\t            )\n\t        )\n\t        observation_period = pa.Table.from_pydict(\n", "            {\n\t                \"person_id\": [1, 1, 1],\n\t                \"observation_period_id\": [1, 2, 3],\n\t                \"observation_period_start_date\": d([\"2000-01-01\", \"2001-01-01\", \"2002-01-01\"]),\n\t                \"observation_period_end_date\": d([\"2000-07-01\", \"2001-07-01\", \"2002-07-01\"]),\n\t            },\n\t            schema=pa.schema(\n\t                [\n\t                    (\"person_id\", pa.int64()),\n\t                    (\"observation_period_id\", pa.int64()),\n", "                    (\"observation_period_start_date\", pa.date32()),\n\t                    (\"observation_period_end_date\", pa.date32()),\n\t                ]\n\t            )\n\t        )\n\t        visit_occurrence = pa.Table.from_pydict(\n\t            {\n\t                \"person_id\": pa.array([1, 1, 1]),\n\t                \"visit_occurrence_id\": [1, 2, 3],\n\t                \"visit_concept_id\": [9201, 9202, 9201],\n", "                \"visit_start_date\": d([\"2000-01-01\", \"2000-02-01\", \"2002-07-01\"]),\n\t                \"visit_end_date\": d([\"2000-01-01\", \"2000-02-05\", \"2002-07-01\"]),\n\t            },\n\t            schema=pa.schema(\n\t                [\n\t                    (\"person_id\", pa.int64()),\n\t                    (\"visit_occurrence_id\", pa.int64()),\n\t                    (\"visit_concept_id\", pa.int32()),\n\t                    (\"visit_start_date\", pa.date32()),\n\t                    (\"visit_end_date\", pa.date32()),\n", "                ]\n\t            )\n\t        )\n\t        condition_occurrence = pa.Table.from_pydict(\n\t            {\n\t                \"person_id\": [1, 1, 1],\n\t                \"condition_concept_id\": [123, 456, 0],\n\t                \"condition_start_date\": d([\"2000-01-01\", \"2000-02-01\", \"2000-03-01\"]),\n\t                \"condition_end_date\": d([\"2000-01-01\", \"2000-02-01\", \"2000-03-01\"]),\n\t                \"visit_occurrence_id\": [1, None, None],\n", "            },\n\t            schema=pa.schema(\n\t                [\n\t                    (\"person_id\", pa.int64()),\n\t                    (\"condition_concept_id\", pa.int32()),\n\t                    (\"condition_start_date\", pa.date32()),\n\t                    (\"condition_end_date\", pa.date32()),\n\t                    (\"visit_occurrence_id\", pa.int64()),\n\t                ]\n\t            )\n", "        )\n\t        death = pa.Table.from_pydict(\n\t            {\n\t                \"person_id\": [1],\n\t                \"death_date\": d([\"2020-07-01\"]),\n\t            },\n\t            schema=pa.schema(\n\t                [\n\t                    (\"person_id\", pa.int64()),\n\t                    (\"death_date\", pa.date32()),\n", "                ]\n\t            )\n\t        )\n\t        self.cdm_tables = {\"person\": person,\n\t                           \"observation_period\": observation_period,\n\t                           \"visit_occurrence\": visit_occurrence,\n\t                           \"condition_occurrence\": condition_occurrence,\n\t                           \"death\": death}\n\t    def test_union_domain_tables(self):\n\t        event_table = cdm_utils.union_domain_tables(self.cdm_tables)\n", "        # First record should be death:\n\t        assert event_table[\"concept_id\"].to_pylist()[0] == 4306655\n\t    def test_remove_concepts(self):\n\t        event_table = cdm_utils.union_domain_tables(self.cdm_tables)\n\t        new_cdm_tables, removed_count = cdm_utils.remove_concepts(event_table=event_table, concept_ids=[0])\n\t        assert removed_count == 1\n\t        assert len(event_table) == 4\n\t    def test_add_date_of_birth(self):\n\t        person = pa.Table.from_pydict(\n\t            {\n", "                \"person_id\": [1, 2, 3],\n\t                \"year_of_birth\": [1970, 1980, 1990],\n\t                \"month_of_birth\": [5, 4, None],\n\t                \"day_of_birth\": [7, None, None],\n\t                \"gender_concept_id\": [8507, 8507, 8507],\n\t            }\n\t        )\n\t        dob = cdm_utils.add_date_of_birth(person)\n\t        assert dob[\"date_of_birth\"].to_pylist()[0] == dt.datetime(1970, 5, 7)\n\t        assert dob[\"date_of_birth\"].to_pylist()[1] == dt.datetime(1980, 4, 1)\n", "        assert dob[\"date_of_birth\"].to_pylist()[2] == dt.datetime(1990, 1, 1)\n\t    def test_group_by_visit(self):\n\t        event_table = cdm_utils.union_domain_tables(self.cdm_tables)\n\t        visits = self.cdm_tables[\"visit_occurrence\"]\n\t        event_table, visits, stats = cdm_utils.link_events_to_visits(event_table=event_table,\n\t                                                                     visit_occurrence=visits,\n\t                                                                     mising_visit_concept_id=0)\n\t        assert pc.max(visits[\"internal_visit_id\"]).as_py() == 4\n\t        # First visit in CDM data, linked by ID:\n\t        visit_group = event_table.filter(pc.equal(event_table[\"internal_visit_id\"], 0))\n", "        assert len(visit_group) == 1\n\t        assert visit_group[\"concept_id\"].to_pylist()[0] == 123\n\t        # Second visit in CDM data, linked by date\n\t        visit_group = event_table.filter(pc.equal(event_table[\"internal_visit_id\"], 1))\n\t        assert len(visit_group) == 1\n\t        assert visit_group[\"concept_id\"].to_pylist()[0] == 456\n\t        # Third visit in CDM data, has no events:\n\t        visit_group = event_table.filter(pc.equal(event_table[\"internal_visit_id\"], 2))\n\t        assert len(visit_group) == 0\n\t        # New visit, derived from condition occurrence\n", "        visit_group = event_table.filter(pc.equal(event_table[\"internal_visit_id\"], 3))\n\t        assert len(visit_group) == 1\n\t        assert visit_group[\"concept_id\"].to_pylist()[0] == 0\n\t        visit = visits.filter(pc.equal(visits[\"internal_visit_id\"], 3))\n\t        assert visit[\"visit_concept_id\"].to_pylist()[0] == 0\n\t        # New visit, derived from death\n\t        visit_group = event_table.filter(pc.equal(event_table[\"internal_visit_id\"], 4))\n\t        assert len(visit_group) == 1\n\t        assert visit_group[\"concept_id\"].to_pylist()[0] == 4306655\n\t    def test_remove_duplicate_events(self):\n", "        event_table = pa.Table.from_pydict(\n\t            {\n\t                \"concept_id\": [1000, 2000, 2000, 2000],\n\t                \"start_date\": d([\"2000-01-01\", \"2001-01-01\", \"2001-01-01\", \"2001-01-02\"]),\n\t            }\n\t        )\n\t        event_table, removed_count = cdm_utils.remove_duplicates(event_table)\n\t        assert removed_count == 1\n\t        assert len(event_table) == 3\n\t        assert event_table[\"concept_id\"].to_pylist() == [1000, 2000, 2000]\n", "if __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_data_generating.py", "chunked_list": ["import shutil\n\timport tempfile\n\timport os\n\timport pandas as pd\n\timport numpy as np\n\timport unittest\n\timport tensorflow as tf\n\tfrom data_generating import tokenizer, learning_objective, data_generator, parquet_data_iterator\n\tclass TestDataGenerating(unittest.TestCase):\n\t    def setUp(self) -> None:\n", "        self.parquet_folder = tempfile.mkdtemp()\n\t        row = pd.DataFrame(\n\t            {\n\t                \"cohort_member_id\": [1],\n\t                \"person_id\": [1],\n\t                \"concept_ids\": [np.array([\"VS\", \"123\", \"456\", \"VE\", \"W1\", \"VS\", \"456\", \"VE\"], dtype=str)],\n\t                \"visit_segments\": [np.array([2, 2, 2, 2, 0, 1, 1, 1], dtype=np.int32)],\n\t                \"orders\": [np.array([0, 1, 2, 3, 4, 5, 6, 7], dtype=np.int32)],\n\t                \"dates\": [np.array([1800, 1800, 1800, 1800, 0, 1801, 1801, 1801], dtype=np.int32)],\n\t                \"ages\": [np.array([75, 75, 75, 75, 0, 76, 76, 76], dtype=np.int32)],\n", "                \"visit_concept_orders\": [np.array([1, 1, 1, 1, 2, 2, 2, 2], dtype=np.int32)],\n\t                \"num_of_concepts\": [8],\n\t                \"num_of_visits\": [2],\n\t                \"visit_concept_ids\": [np.array([\"9202\", \"9202\", \"9202\", \"9202\", \"0\", \"9202\", \"9202\", \"9202\"], dtype=str)],\n\t            }\n\t        )\n\t        row.to_parquet(os.path.join(self.parquet_folder, \"test.parquet\"))\n\t    def tearDown(self):\n\t        shutil.rmtree(self.parquet_folder)\n\t    def test_parquest_data_iterator(self):\n", "        data = parquet_data_iterator.ParquetDataIterator(parquet_folder_name=self.parquet_folder)\n\t        row_count = 0\n\t        for row in data:\n\t            assert row[\"cohort_member_id\"] == 1\n\t            assert row[\"person_id\"] == 1\n\t            assert row[\"concept_ids\"].shape == (8,)\n\t            assert row[\"visit_segments\"].shape == (8,)\n\t            assert row[\"orders\"].shape == (8,)\n\t            assert row[\"dates\"].shape == (8,)\n\t            assert row[\"ages\"].shape == (8,)\n", "            assert row[\"visit_concept_orders\"].shape == (8,)\n\t            assert row[\"num_of_concepts\"] == 8\n\t            assert row[\"num_of_visits\"] == 2\n\t            assert row[\"visit_concept_ids\"].shape == (8,)\n\t            row_count += 1\n\t            break\n\t        assert row_count == 1\n\t    def test_concept_tokenizer(self):\n\t        data = parquet_data_iterator.ParquetDataIterator(parquet_folder_name=self.parquet_folder)\n\t        concept_tokenizer = tokenizer.ConceptTokenizer()\n", "        concept_tokenizer.fit_on_concept_sequences(parquet_data_iterator=data,\n\t                                                   column_name=\"concept_ids\")\n\t        assert concept_tokenizer.encode([tokenizer.OUT_OF_VOCABULARY_TOKEN]) == \\\n\t               [concept_tokenizer.get_out_of_vocabulary_token_id()]\n\t        assert concept_tokenizer.encode([tokenizer.MASK_TOKEN]) == [concept_tokenizer.get_mask_token_id()]\n\t        assert concept_tokenizer.encode([tokenizer.UNUSED_TOKEN]) == [concept_tokenizer.get_unused_token_id()]\n\t        test_concept_ids = np.array([\"VS\", \"123\", \"456\", \"VE\", \"W1\", \"VS\", \"456\", \"VE\"], dtype=str)\n\t        encoding = concept_tokenizer.encode(test_concept_ids)\n\t        decoding = concept_tokenizer.decode(encoding)\n\t        assert decoding == test_concept_ids.tolist()\n", "        json_filename = os.path.join(self.parquet_folder, \"_test.json\")\n\t        concept_tokenizer.save_to_json(json_filename)\n\t        concept_tokenizer_2 = tokenizer.load_from_json(json_filename)\n\t        os.remove(json_filename)\n\t        encoding_2 = concept_tokenizer_2.encode(test_concept_ids)\n\t        assert encoding == encoding_2\n\t    def test_data_generator(self):\n\t        learning_objectives = [learning_objective.MaskedLanguageModelLearningObjective(work_folder=self.parquet_folder),\n\t                               learning_objective.VisitPredictionLearningObjective(work_folder=self.parquet_folder)]\n\t        bert_data_generator = data_generator.DataGenerator(training_data_path=self.parquet_folder,\n", "                                                           batch_size=4,\n\t                                                           max_sequence_length=10,\n\t                                                           min_sequence_length=5,\n\t                                                           is_training=False,\n\t                                                           learning_objectives=learning_objectives)\n\t        assert len(bert_data_generator) == 1\n\t        batch_count = 0\n\t        for batch_input, batch_output in bert_data_generator.generator():\n\t            # TODO: add some specific tests on output\n\t            batch_count += 1\n", "            break\n\t        assert batch_count == 1\n\t    def test_dataset_from_generator(self):\n\t        learning_objectives = [learning_objective.MaskedLanguageModelLearningObjective(work_folder=self.parquet_folder),\n\t                               learning_objective.VisitPredictionLearningObjective(work_folder=self.parquet_folder)]\n\t        bert_data_generator = data_generator.DataGenerator(training_data_path=self.parquet_folder,\n\t                                                           batch_size=4,\n\t                                                           max_sequence_length=10,\n\t                                                           min_sequence_length=5,\n\t                                                           is_training=True,\n", "                                                           learning_objectives=learning_objectives)\n\t        # from_generator() verifies if the output of the generator matches the schema:\n\t        dataset = tf.data.Dataset.from_generator(generator=bert_data_generator.generator,\n\t                                                 output_types=bert_data_generator.get_tf_dataset_schema())\n\t        batch_count = 0\n\t        for batch in dataset:\n\t            # TODO: add some specific tests on output\n\t            batch_count += 1\n\t            break\n\t        assert batch_count == 1\n"]}
{"filename": "data_generating/parquet_data_iterator.py", "chunked_list": ["from typing import Iterator\n\timport pandas as pd\n\timport pyarrow.parquet as pq\n\tclass ParquetDataIterator:\n\t    \"\"\"Iterate over rows in parquet files\"\"\"\n\t    def __init__(self, parquet_folder_name: str):\n\t        \"\"\"\n\t        Initialization\n\t        Args:\n\t            parquet_folder_name: Path to the folder containing the parquet files.\n", "        \"\"\"\n\t        self._parquet_path = parquet_folder_name\n\t        self._dataset = pq.ParquetDataset(parquet_folder_name)\n\t        self._nrows = sum(fragment.count_rows() for fragment in self._dataset.fragments)\n\t        self._iterator = self.__iter__()  # Used by __next__\n\t    def __iter__(self) -> Iterator[pd.DataFrame]:\n\t        for fragment in self._dataset.fragments:\n\t            for batch in fragment.to_batches():\n\t                for index, row in batch.to_pandas().iterrows():\n\t                    yield row\n", "    def __len__(self) -> int:\n\t        \"\"\"The number of batches per epoch\"\"\"\n\t        return self._nrows\n\t    def __next__(self) -> pd.DataFrame:\n\t        return next(self._iterator)\n"]}
{"filename": "data_generating/abstract_data_generator.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom data_generating.parquet_data_iterator import ParquetDataIterator\n\tclass AbstractDataGenerator(ABC):\n\t    \"\"\"\n\t    The interface of the data generator that will be visible to the learning objectives.\n\t    \"\"\"\n\t    @abstractmethod\n\t    def get_parquet_data_iterator(self) -> ParquetDataIterator:\n\t        pass\n\t    @abstractmethod\n", "    def get_is_training(self) -> bool:\n\t        pass\n\t    @abstractmethod\n\t    def get_max_sequence_length(self) -> int:\n\t        pass\n"]}
{"filename": "data_generating/data_generator.py", "chunked_list": ["import random\n\tfrom collections import ChainMap, defaultdict\n\tfrom typing import Iterator, Dict\n\timport numpy as np\n\timport pandas as pd\n\tfrom data_generating.abstract_data_generator import AbstractDataGenerator\n\tfrom data_generating.learning_objective import LearningObjective\n\tfrom data_generating.parquet_data_iterator import ParquetDataIterator\n\tSEQUENCE_LENGTH_COLUMN_NAME = \"num_of_concepts\"\n\tclass DataGenerator(AbstractDataGenerator):\n", "    \"\"\"\n\t    Generate data for tensorflow from parquet files. Iterates over the sequence data created using the CDM\n\t    processing. To be used with tf.data.Dataset.from_generator\n\t    \"\"\"\n\t    def __init__(self,\n\t                 training_data_path: str,\n\t                 batch_size: int,\n\t                 max_sequence_length: int,\n\t                 min_sequence_length: int,\n\t                 is_training: bool,\n", "                 learning_objectives: list[LearningObjective] = None):\n\t        \"\"\"\n\t        Initialization\n\t        Args:\n\t            training_data_path: Path to the folder containing the parquet files.\n\t            batch_size: Number of examples in a batch.\n\t            max_sequence_length: The maximum length of a sequence.\n\t            min_sequence_length: The minimum length of a sequence. Persons with sequences shorter than this will be\n\t                ignored.\n\t            is_training: If true, the generated data is intended for training, and for example subsequences will be\n", "                sampled and tokens will be masked.\n\t            learning_objectives: One or more learning objectives for which to generate data.\n\t        \"\"\"\n\t        self._parquet_path = training_data_path\n\t        self._batch_size = batch_size\n\t        self._max_sequence_length = max_sequence_length\n\t        self._min_sequence_length = min_sequence_length\n\t        self._is_training = is_training\n\t        self._parquet_data_iterator = ParquetDataIterator(training_data_path)\n\t        self._nrows = len(self._parquet_data_iterator)\n", "        self._learning_objectives = learning_objectives\n\t        for learning_objective in self._learning_objectives:\n\t            learning_objective.initialize(self)\n\t    def __len__(self) -> int:\n\t        \"\"\"The number of batches per epoch\"\"\"\n\t        return int(np.ceil(self._nrows / self._batch_size))\n\t    def get_parquet_data_iterator(self) -> ParquetDataIterator:\n\t        return self._parquet_data_iterator\n\t    def get_is_training(self) -> bool:\n\t        return self._is_training\n", "    def get_max_sequence_length(self) -> int:\n\t        return self._max_sequence_length\n\t    def generator(self) -> Iterator[tuple[Dict, Dict]]:\n\t        \"\"\"Generate data for tensorflow\"\"\"\n\t        while True:\n\t            try:\n\t                yield self._get_batch()\n\t            except StopIteration:\n\t                break\n\t    def _get_batch(self) -> tuple[Dict, Dict]:\n", "        \"\"\"Get a batch of data for tensorflow\"\"\"\n\t        row_batch = self._get_row_batch()\n\t        input_dicts = defaultdict(list)\n\t        output_dicts = defaultdict(list)\n\t        for row in row_batch:\n\t            begin_index, end_index = self._create_begin_end_indices(row)\n\t            for learning_objective in self._learning_objectives:\n\t                inputs, outputs = learning_objective.process_row(row, begin_index, end_index)\n\t                for key, value in inputs.items():\n\t                    input_dicts[key].append(value)\n", "                for key, value in outputs.items():\n\t                    output_dicts[key].append(value)\n\t        return dict(input_dicts), dict(output_dicts)\n\t    def _get_row_batch(self) -> list[pd.DataFrame]:\n\t        \"\"\"Get a batch of rows\"\"\"\n\t        row_batch = []\n\t        while len(row_batch) < self._batch_size:\n\t            try:\n\t                row = next(self._parquet_data_iterator)\n\t            except StopIteration:\n", "                break\n\t            if row[SEQUENCE_LENGTH_COLUMN_NAME] >= self._min_sequence_length:\n\t                row_batch.append(row)\n\t        if len(row_batch) == 0:\n\t            raise StopIteration\n\t        return row_batch\n\t    def _create_begin_end_indices(self, row: pd.DataFrame) -> tuple[int, int]:\n\t        \"\"\"Create begin and end indices for a row, either by sampling a sequence or using the whole sequence\"\"\"\n\t        seq_length = row[SEQUENCE_LENGTH_COLUMN_NAME]\n\t        if self._is_training:\n", "            cursor = random.randint(0, seq_length - 1)\n\t            half_window_size = int(self._max_sequence_length / 2)\n\t            start_index = max(0, cursor - half_window_size)\n\t            end_index = min(cursor + half_window_size, seq_length)\n\t            if start_index < end_index:\n\t                return start_index, end_index\n\t            else:\n\t                return 0, seq_length\n\t        else:\n\t            return 0, seq_length\n", "    def get_tf_dataset_schema(self):\n\t        \"\"\"\n\t        Combine the input and output tensorflow data schema from multiple learning objectives\n\t        :return: A tuple of input and output tensorflow data schema.\n\t        \"\"\"\n\t        input_dict_schemas = []\n\t        output_dict_schemas = []\n\t        for learning_objective in self._learning_objectives:\n\t            input_dict_schema, output_dict_schema = learning_objective.get_tf_dataset_schema()\n\t            input_dict_schemas.append(input_dict_schema)\n", "            output_dict_schemas.append(output_dict_schema)\n\t        return dict(ChainMap(*input_dict_schemas)), dict(ChainMap(*output_dict_schemas))\n"]}
{"filename": "data_generating/tokenizer.py", "chunked_list": ["import json\n\tfrom data_generating.parquet_data_iterator import ParquetDataIterator\n\tfrom typing import List, Union\n\timport numpy as np\n\tUNUSED_TOKEN = \"[UNUSED]\"\n\tMASK_TOKEN = \"[MASK]\"\n\tOUT_OF_VOCABULARY_TOKEN = \"[OOV]\"\n\tclass ConceptTokenizer:\n\t    \"\"\"\n\t    Maps concept ID strings and special tokens to integer indexes and vice versa.\n", "    \"\"\"\n\t    def __init__(self):\n\t        self._word_index = {}\n\t        self._index_word = {}\n\t        self._oov_token_index = 1\n\t        self._unused_token_index = 2\n\t        self._mask_token_index = 3\n\t    def fit_on_concept_sequences(self, parquet_data_iterator: ParquetDataIterator, column_name: str):\n\t        \"\"\"\n\t        Fit the tokenizer on the concept IDs in the given column of the given parquet data iterator.\n", "        Args:\n\t            parquet_data_iterator: A parquet data iterator.\n\t            column_name: The name of the column containing the concept IDs.\n\t        \"\"\"\n\t        words = set()\n\t        for row in parquet_data_iterator:\n\t            for concept_id in row[column_name]:\n\t                words.add(concept_id)\n\t        vocabulary = [OUT_OF_VOCABULARY_TOKEN, UNUSED_TOKEN, MASK_TOKEN]\n\t        vocabulary.extend(words)\n", "        self._word_index = dict(zip(vocabulary, list(range(1, len(vocabulary) + 1))))\n\t        self._index_word = {index: word for word, index in self._word_index.items()}\n\t        self._oov_token_index = self._word_index[OUT_OF_VOCABULARY_TOKEN]\n\t        self._unused_token_index = self._word_index[UNUSED_TOKEN]\n\t        self._mask_token_index = self._word_index[MASK_TOKEN]\n\t    def encode(self, concept_ids: Union[List[str], np.ndarray[str]]) -> List[int]:\n\t        result = []\n\t        for word in concept_ids:\n\t            idx = self._word_index.get(word)\n\t            if idx is None:\n", "                result.append(self._oov_token_index)\n\t            else:\n\t                result.append(idx)\n\t        return result\n\t    def decode(self, concept_token_ids: List[int]) -> List[str]:\n\t        return [self._index_word.get(i) for i in concept_token_ids]\n\t    def get_vocab_size(self):\n\t        return len(self._word_index)\n\t    def get_unused_token_id(self):\n\t        return self._unused_token_index\n", "    def get_mask_token_id(self):\n\t        return self._mask_token_index\n\t    def get_out_of_vocabulary_token_id(self):\n\t        return self._oov_token_index\n\t    def get_first_token_id(self):\n\t        return 4\n\t    def get_last_token_id(self):\n\t        return self.get_vocab_size() - 1\n\t    def save_to_json(self, file_name: str):\n\t        with open(file_name, \"w\") as f:\n", "            json.dump(self._word_index, f)\n\tdef load_from_json(file_name: str):\n\t    self = ConceptTokenizer()\n\t    with open(file_name, \"r\") as f:\n\t        self._word_index = json.load(f)\n\t    self._index_word = {index: word for word, index in self._word_index.items()}\n\t    self._oov_token_index = self._word_index[OUT_OF_VOCABULARY_TOKEN]\n\t    self._unused_token_index = self._word_index[UNUSED_TOKEN]\n\t    self._mask_token_index = self._word_index[MASK_TOKEN]\n\t    return self\n"]}
{"filename": "data_generating/learning_objective.py", "chunked_list": ["import os\n\timport random\n\tfrom abc import ABC, abstractmethod\n\tfrom typing import Dict\n\timport numpy as np\n\timport pandas as pd\n\timport tensorflow as tf\n\tfrom data_generating import tokenizer\n\tfrom data_generating.abstract_data_generator import AbstractDataGenerator\n\tclass LayerInputNames:\n", "    \"\"\"\n\t    Names of the inputs to the model. These inputs are generated by the data generator using the learning objectives.\n\t    \"\"\"\n\t    LABEL = \"label\"\n\t    MASKED_VISIT_CONCEPTS = \"masked_visit_concepts\"\n\t    MASK_VISIT = \"mask_visit\"\n\t    VISIT_PREDICTIONS = \"visit_predictions\"\n\t    MASKED_CONCEPT_IDS = \"masked_concept_ids\"\n\t    CONCEPT_IDS = \"concept_ids\"\n\t    MASK = \"mask\"\n", "    TIME_STAMPS = \"time_stamps\"\n\t    VISIT_SEGMENTS = \"visit_segments\"\n\t    AGES = \"ages\"\n\t    VISIT_CONCEPT_ORDERS = \"visit_concept_orders\"\n\t    CONCEPT_PREDICTIONS = \"concept_predictions\"\n\tdef _pad_sequence(sequence: np.ndarray[any], padding_value: any, max_sequence_length: int) -> np.ndarray[any]:\n\t    \"\"\"\n\t    Pad a sequence to a given length.\n\t    Args\n\t        sequence: The sequence to pad.\n", "        max_sequence_length: The length to pad to.\n\t        adding_value: The value to pad with.\n\t    Returns\n\t        The padded sequence.\n\t    \"\"\"\n\t    n_to_pad = max_sequence_length - len(sequence)\n\t    if n_to_pad > 0:\n\t        sequence = np.append(sequence, [padding_value] * n_to_pad)\n\t    return sequence\n\tclass LearningObjective(ABC):\n", "    \"\"\"\n\t    A learning objective is a task that can be learned from the data. For example, predicting the next visit. This\n\t    class is used to generate the data for the learning objective.\n\t    \"\"\"\n\t    @abstractmethod\n\t    def initialize(self, data_generator: AbstractDataGenerator):\n\t        \"\"\"\n\t        An initializer called by the DataGenerator. Other, objective-specific initialization, can be done in __init__.\n\t        Args\n\t            data_generator: The calling data generator.\n", "        \"\"\"\n\t        pass\n\t    @abstractmethod\n\t    def process_row(self, row: pd.DataFrame, start_index: int, end_index: int) -> tuple[Dict, Dict]:\n\t        \"\"\"\n\t        Process a row to generate input and output data for the learning objective\n\t        Args\n\t            row: The row to process, as generated by the CDM processing.\n\t            start_index: Any sequence in the row should start at this index.\n\t            end_index: Any sequence in the row should end at this index.\n", "        Returns\n\t            Two dictonaries to be used by Tensorflow. The first is the input, the second is the output.\n\t        \"\"\"\n\t        pass\n\t    @abstractmethod\n\t    def get_tf_dataset_schema(self) -> tuple[Dict, Dict]:\n\t        \"\"\"\n\t        Get the schema for the input and output to the tensorflow Dataset\n\t        Returns\n\t            A tuple of two dictionaries. The first is the input schema, the second is the output schema.\n", "        \"\"\"\n\t        pass\n\tclass BertFineTuningLearningObjective(LearningObjective):\n\t    def initialize(self, data_generator: AbstractDataGenerator):\n\t        pass\n\t    def get_tf_dataset_schema(self) -> tuple[Dict, Dict]:\n\t        output_dict_schema = {LayerInputNames.LABEL: tf.int32}\n\t        return {}, output_dict_schema\n\t    def process_row(self, row: pd.DataFrame, start_index: int, end_index: int) -> tuple[Dict, Dict]:\n\t        output_dict = {LayerInputNames.LABEL: row.label}\n", "        return {}, output_dict\n\tclass VisitPredictionLearningObjective(LearningObjective):\n\t    def __init__(self, work_folder: str, reuse_tokenizer: bool = True):\n\t        \"\"\"\n\t        Initialization\n\t        Args:\n\t            work_folder: The folder where the tokenizer will be saved.\n\t            reuse_tokenizer: If true, the tokenizer will be loaded from the work_folder if it exists.\n\t        \"\"\"\n\t        self._work_folder = work_folder\n", "        self._visit_tokenizer = None\n\t        self._reuse_tokenizer = reuse_tokenizer\n\t        self._max_sequence_length = None\n\t    def initialize(self, data_generator: AbstractDataGenerator):\n\t        json_file = os.path.join(self._work_folder, \"_visit_tokenizer.json\")\n\t        if self._reuse_tokenizer and os.path.exists(json_file):\n\t            self._visit_tokenizer = tokenizer.load_from_json(json_file)\n\t        else:\n\t            self._visit_tokenizer = tokenizer.ConceptTokenizer()\n\t            self._visit_tokenizer.fit_on_concept_sequences(data_generator.get_parquet_data_iterator(),\n", "                                                           \"visit_concept_ids\")\n\t            self._visit_tokenizer.save_to_json(json_file)\n\t        self._max_sequence_length = data_generator.get_max_sequence_length()\n\t    def get_tf_dataset_schema(self):\n\t        input_dict_schema = {\n\t            LayerInputNames.MASKED_VISIT_CONCEPTS: tf.int32,\n\t            LayerInputNames.MASK_VISIT: tf.int32\n\t        }\n\t        output_dict_schema = {LayerInputNames.VISIT_PREDICTIONS: tf.int32}\n\t        return input_dict_schema, output_dict_schema\n", "    def process_row(self, row: pd.DataFrame, start_index: int, end_index: int) -> tuple[Dict, Dict]:\n\t        visit_concept_ids = row.visit_concept_ids[start_index:end_index]\n\t        visit_token_ids = self._visit_tokenizer.encode(visit_concept_ids)\n\t        masked_visit_token_ids, output_mask = self._mask_visit_concepts(visit_token_ids)\n\t        masked_visit_token_ids = _pad_sequence(sequence=masked_visit_token_ids,\n\t                                               padding_value=self._visit_tokenizer.get_unused_token_id(),\n\t                                               max_sequence_length=self._max_sequence_length)\n\t        visit_token_ids = _pad_sequence(sequence=masked_visit_token_ids,\n\t                                        padding_value=self._visit_tokenizer.get_unused_token_id(),\n\t                                        max_sequence_length=self._max_sequence_length)\n", "        visit_mask = (visit_token_ids == self._visit_tokenizer.get_unused_token_id()).astype(int)\n\t        combined_label = np.stack([visit_token_ids, output_mask], axis=-1)\n\t        input_dict = {\n\t            LayerInputNames.MASKED_VISIT_CONCEPTS: masked_visit_token_ids,\n\t            LayerInputNames.MASK_VISIT: visit_mask\n\t        }\n\t        output_dict = {LayerInputNames.VISIT_PREDICTIONS: combined_label}\n\t        return input_dict, output_dict\n\t    def _mask_visit_concepts(self, visit_concepts):\n\t        masked_visit_concepts = np.asarray(visit_concepts).copy()\n", "        output_mask = np.zeros((self._max_sequence_length,), dtype=int)\n\t        for word_pos in range(0, len(visit_concepts)):\n\t            if random.random() < 0.5:\n\t                output_mask[word_pos] = 1\n\t                masked_visit_concepts[word_pos] = self._visit_tokenizer.get_mask_token_id()\n\t        return masked_visit_concepts, output_mask\n\tclass MaskedLanguageModelLearningObjective(LearningObjective):\n\t    def __init__(self, work_folder: str, reuse_tokenizer: bool = True):\n\t        \"\"\"\n\t        Initialization\n", "        Args:\n\t            work_folder: The folder where the tokenizer will be saved.\n\t            reuse_tokenizer: If true, the tokenizer will be loaded from the work_folder if it exists.\n\t        \"\"\"\n\t        self._work_folder = work_folder\n\t        self._reuse_tokenizer = reuse_tokenizer\n\t        self._concept_tokenizer = None\n\t        self._max_sequence_length = None\n\t        self._is_training = None\n\t    def initialize(self, data_generator: AbstractDataGenerator):\n", "        json_file = os.path.join(self._work_folder, \"_concept_tokenizer.json\")\n\t        if self._reuse_tokenizer and os.path.exists(json_file):\n\t            self._concept_tokenizer = tokenizer.load_from_json(json_file)\n\t        else:\n\t            self._concept_tokenizer = tokenizer.ConceptTokenizer()\n\t            self._concept_tokenizer.fit_on_concept_sequences(data_generator.get_parquet_data_iterator(), \"concept_ids\")\n\t            self._concept_tokenizer.save_to_json(json_file)\n\t        self._max_sequence_length = data_generator.get_max_sequence_length()\n\t        self._is_training = data_generator.get_is_training()\n\t    def get_tf_dataset_schema(self):\n", "        input_dict_schema = {\n\t            LayerInputNames.MASKED_CONCEPT_IDS: tf.int32,\n\t            LayerInputNames.CONCEPT_IDS: tf.int32,\n\t            LayerInputNames.MASK: tf.int32,\n\t            LayerInputNames.TIME_STAMPS: tf.int32,\n\t            LayerInputNames.VISIT_SEGMENTS: tf.int32,\n\t            LayerInputNames.AGES: tf.int32,\n\t            LayerInputNames.VISIT_CONCEPT_ORDERS: tf.int32\n\t        }\n\t        output_dict_schema = {LayerInputNames.CONCEPT_PREDICTIONS: tf.int32}\n", "        return input_dict_schema, output_dict_schema\n\t    def process_row(self, row: pd.DataFrame, start_index: int, end_index: int) -> tuple[Dict, Dict]:\n\t        concept_ids = row.concept_ids[start_index:end_index]\n\t        visit_segments = row.visit_segments[start_index:end_index]\n\t        dates = row.dates[start_index:end_index]\n\t        ages = row.ages[start_index:end_index]\n\t        visit_concept_orders = row.visit_concept_orders[start_index:end_index]\n\t        token_ids = self._concept_tokenizer.encode(concept_ids)\n\t        # Normalize the visit_orders using the smallest visit_concept_orders\n\t        visit_concept_orders = visit_concept_orders - min(visit_concept_orders)\n", "        masked_token_ids, output_mask = self._mask_concepts(token_ids)\n\t        token_ids = _pad_sequence(sequence=token_ids,\n\t                                  padding_value=self._concept_tokenizer.get_unused_token_id(),\n\t                                  max_sequence_length=self._max_sequence_length)\n\t        masked_token_ids = _pad_sequence(sequence=masked_token_ids,\n\t                                         padding_value=self._concept_tokenizer.get_unused_token_id(),\n\t                                         max_sequence_length=self._max_sequence_length)\n\t        visit_segments = _pad_sequence(sequence=visit_segments,\n\t                                       padding_value=self._max_sequence_length,\n\t                                       max_sequence_length=self._max_sequence_length)\n", "        dates = _pad_sequence(sequence=dates,\n\t                              padding_value=self._max_sequence_length,\n\t                              max_sequence_length=self._max_sequence_length)\n\t        ages = _pad_sequence(sequence=ages,\n\t                             padding_value=self._max_sequence_length,\n\t                             max_sequence_length=self._max_sequence_length)\n\t        visit_concept_orders = _pad_sequence(sequence=visit_concept_orders,\n\t                                             padding_value=self._max_sequence_length - 1,\n\t                                             max_sequence_length=self._max_sequence_length)\n\t        output_mask = (token_ids == self._concept_tokenizer.get_unused_token_id()).astype(int)\n", "        combined_label = np.stack([token_ids, output_mask], axis=-1)\n\t        input_dict = {LayerInputNames.MASKED_CONCEPT_IDS: masked_token_ids,\n\t                      LayerInputNames.CONCEPT_IDS: token_ids,\n\t                      LayerInputNames.MASK: output_mask,\n\t                      LayerInputNames.TIME_STAMPS: dates,\n\t                      LayerInputNames.AGES: ages,\n\t                      LayerInputNames.VISIT_SEGMENTS: visit_segments,\n\t                      LayerInputNames.VISIT_CONCEPT_ORDERS: visit_concept_orders}\n\t        output_dict = {LayerInputNames.CONCEPT_PREDICTIONS: combined_label}\n\t        return input_dict, output_dict\n", "    def _mask_concepts(self, concepts):\n\t        masked_concepts = concepts.copy()\n\t        output_mask = np.zeros((self._max_sequence_length,), dtype=int)\n\t        if self._is_training:\n\t            for word_pos in range(0, len(concepts)):\n\t                if concepts[word_pos] == self._concept_tokenizer.get_unused_token_id():\n\t                    break\n\t                if random.random() < 0.15:\n\t                    dice = random.random()\n\t                    if dice < 0.8:\n", "                        masked_concepts[word_pos] = self._concept_tokenizer.get_mask_token_id()\n\t                    elif dice < 0.9:\n\t                        masked_concepts[word_pos] = random.randint(\n\t                            self._concept_tokenizer.get_first_token_id(),\n\t                            self._concept_tokenizer.get_last_token_id())\n\t                    # else: 10% of the time we just leave the token as is\n\t                    output_mask[word_pos] = 1\n\t        return masked_concepts, output_mask\n"]}
{"filename": "data_generating/__init__.py", "chunked_list": []}
