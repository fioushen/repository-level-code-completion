{"filename": "api/src/fewshot_examples.py", "chunked_list": ["def get_fewshot_examples(openai_api_key):\n\t    return f\"\"\"\n\t#How is Emil Eifrem connected to Michael Hunger?\n\tMATCH (p1:Person {{name:\"Emil Eifrem\"}}), (p2:Person {{name:\"Michael Hunger\"}})\n\tMATCH p=shortestPath((p1)-[*]-(p2))\n\tRETURN p\n\t#What are the latest news regarding Google?\n\tMATCH (o:Organization {{name:\"Google\"}})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c)\n\tRETURN a.title AS title, c.text AS text, c.date AS date\n\tORDER BY date DESC LIMIT 3\n", "#Are there any news regarding return to office policies?\n\tCALL apoc.ml.openai.embedding([\"Are there any news regarding return to office policies?\"], \n\t   \"{openai_api_key}\") YIELD embedding\n\tMATCH (c:Chunk)\n\tWITH c, gds.similarity.cosine(c.embedding, embedding) AS score\n\tORDER BY score DESC LIMIT 3\n\tRETURN c.text, score\n\t#What is Microsoft policy regarding to the return to office?\n\tCALL apoc.ml.openai.embedding([\"What is Microsoft policy regarding to the return to office?\"], \"{openai_api_key}\") YIELD embedding\n\tMATCH (o:Organization {{name:\"Microsoft\"}})<-[:MENTIONS]-()-[:HAS_CHUNK]->(c)\n", "WITH distinct c, embedding\n\tWITH c, gds.similarity.cosine(c.embedding, embedding) AS score\n\tORDER BY score DESC LIMIT 3\n\tRETURN c.text, score\n\tWhen searching for specific information in the text chunks, never use the CONTAINS clause, but always use the apoc.ml.openai.embedding\n\tand gds.similarity.cosine functions as shown in the examples.\n\tWhen returning text chunks, always return exactly three chunks, no more, no less.\n\tRemember, instead of using CONTAINS to find information within text chunks use the apoc.ml.openai.embedding and gds.similarity.cosine functions.\n\t\"\"\"\n"]}
{"filename": "api/src/main.py", "chunked_list": ["import os\n\tfrom typing import Optional\n\tfrom components.company_report import CompanyReport\n\tfrom components.data_disambiguation import DataDisambiguation\n\tfrom components.question_proposal_generator import (\n\t    QuestionProposalGenerator,\n\t)\n\tfrom components.summarize_cypher_result import SummarizeCypherResult\n\tfrom components.text2cypher import Text2Cypher\n\tfrom components.unstructured_data_extractor import (\n", "    DataExtractor,\n\t    DataExtractorWithSchema,\n\t)\n\tfrom driver.neo4j import Neo4jDatabase\n\tfrom fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect\n\tfrom fastapi.middleware.cors import CORSMiddleware\n\tfrom fastapi.responses import JSONResponse\n\tfrom fewshot_examples import get_fewshot_examples\n\tfrom llm.openai import OpenAIChat\n\tfrom pydantic import BaseModel\n", "class Payload(BaseModel):\n\t    question: str\n\t    api_key: Optional[str]\n\t    model_name: Optional[str]\n\tclass ImportPayload(BaseModel):\n\t    input: str\n\t    neo4j_schema: Optional[str]\n\t    api_key: Optional[str]\n\tclass questionProposalPayload(BaseModel):\n\t    api_key: Optional[str]\n", "# Maximum number of records used in the context\n\tHARD_LIMIT_CONTEXT_RECORDS = 10\n\tneo4j_connection = Neo4jDatabase(\n\t    host=os.environ.get(\"NEO4J_URL\", \"neo4j+s://demo.neo4jlabs.com\"),\n\t    user=os.environ.get(\"NEO4J_USER\", \"companies\"),\n\t    password=os.environ.get(\"NEO4J_PASS\", \"companies\"),\n\t    database=os.environ.get(\"NEO4J_DATABASE\", \"companies\"),\n\t)\n\t# Initialize LLM modules\n\topenai_api_key = os.environ.get(\"OPENAI_API_KEY\", None)\n", "# Define FastAPI endpoint\n\tapp = FastAPI()\n\torigins = [\n\t    \"*\",\n\t]\n\tapp.add_middleware(\n\t    CORSMiddleware,\n\t    allow_origins=origins,\n\t    allow_credentials=True,\n\t    allow_methods=[\"*\"],\n", "    allow_headers=[\"*\"],\n\t)\n\t@app.post(\"/questionProposalsForCurrentDb\")\n\tasync def questionProposalsForCurrentDb(payload: questionProposalPayload):\n\t    if not openai_api_key and not payload.api_key:\n\t        raise HTTPException(\n\t            status_code=422,\n\t            detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n\t        )\n\t    api_key = openai_api_key if openai_api_key else payload.api_key\n", "    questionProposalGenerator = QuestionProposalGenerator(\n\t        database=neo4j_connection,\n\t        llm=OpenAIChat(\n\t            openai_api_key=api_key,\n\t            model_name=\"gpt-3.5-turbo-0613\",\n\t            max_tokens=512,\n\t            temperature=0.8,\n\t        ),\n\t    )\n\t    return questionProposalGenerator.run()\n", "@app.get(\"/hasapikey\")\n\tasync def hasApiKey():\n\t    return JSONResponse(content={\"output\": openai_api_key is not None})\n\t@app.websocket(\"/text2text\")\n\tasync def websocket_endpoint(websocket: WebSocket):\n\t    async def sendDebugMessage(message):\n\t        await websocket.send_json({\"type\": \"debug\", \"detail\": message})\n\t    async def sendErrorMessage(message):\n\t        await websocket.send_json({\"type\": \"error\", \"detail\": message})\n\t    async def onToken(token):\n", "        delta = token[\"choices\"][0][\"delta\"]\n\t        if \"content\" not in delta:\n\t            return\n\t        content = delta[\"content\"]\n\t        if token[\"choices\"][0][\"finish_reason\"] == \"stop\":\n\t            await websocket.send_json({\"type\": \"end\", \"output\": content})\n\t        else:\n\t            await websocket.send_json({\"type\": \"stream\", \"output\": content})\n\t        # await websocket.send_json({\"token\": token})\n\t    await websocket.accept()\n", "    await sendDebugMessage(\"connected\")\n\t    chatHistory = []\n\t    try:\n\t        while True:\n\t            data = await websocket.receive_json()\n\t            if not openai_api_key and not data.get(\"api_key\"):\n\t                raise HTTPException(\n\t                    status_code=422,\n\t                    detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n\t                )\n", "            api_key = openai_api_key if openai_api_key else data.get(\"api_key\")\n\t            default_llm = OpenAIChat(\n\t                openai_api_key=api_key,\n\t                model_name=data.get(\"model_name\", \"gpt-3.5-turbo-0613\"),\n\t            )\n\t            summarize_results = SummarizeCypherResult(\n\t                llm=OpenAIChat(\n\t                    openai_api_key=api_key,\n\t                    model_name=\"gpt-3.5-turbo-0613\",\n\t                    max_tokens=128,\n", "                )\n\t            )\n\t            text2cypher = Text2Cypher(\n\t                database=neo4j_connection,\n\t                llm=default_llm,\n\t                cypher_examples=get_fewshot_examples(api_key),\n\t            )\n\t            if \"type\" not in data:\n\t                await websocket.send_json({\"error\": \"missing type\"})\n\t                continue\n", "            if data[\"type\"] == \"question\":\n\t                try:\n\t                    question = data[\"question\"]\n\t                    chatHistory.append({\"role\": \"user\", \"content\": question})\n\t                    await sendDebugMessage(\"received question: \" + question)\n\t                    results = None\n\t                    try:\n\t                        results = text2cypher.run(question, chatHistory)\n\t                        print(\"results\", results)\n\t                    except Exception as e:\n", "                        await sendErrorMessage(str(e))\n\t                        continue\n\t                    if results == None:\n\t                        await sendErrorMessage(\"Could not generate Cypher statement\")\n\t                        continue\n\t                    await websocket.send_json(\n\t                        {\n\t                            \"type\": \"start\",\n\t                        }\n\t                    )\n", "                    output = await summarize_results.run_async(\n\t                        question,\n\t                        results[\"output\"][:HARD_LIMIT_CONTEXT_RECORDS],\n\t                        callback=onToken,\n\t                    )\n\t                    chatHistory.append({\"role\": \"system\", \"content\": output})\n\t                    await websocket.send_json(\n\t                        {\n\t                            \"type\": \"end\",\n\t                            \"output\": output,\n", "                            \"generated_cypher\": results[\"generated_cypher\"],\n\t                        }\n\t                    )\n\t                except Exception as e:\n\t                    await sendErrorMessage(str(e))\n\t                await sendDebugMessage(\"output done\")\n\t    except WebSocketDisconnect:\n\t        print(\"disconnected\")\n\t@app.post(\"/data2cypher\")\n\tasync def root(payload: ImportPayload):\n", "    \"\"\"\n\t    Takes an input and created a Cypher query\n\t    \"\"\"\n\t    if not openai_api_key and not payload.api_key:\n\t        raise HTTPException(\n\t            status_code=422,\n\t            detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n\t        )\n\t    api_key = openai_api_key if openai_api_key else payload.api_key\n\t    try:\n", "        result = \"\"\n\t        llm = OpenAIChat(\n\t            openai_api_key=api_key, model_name=\"gpt-3.5-turbo-16k\", max_tokens=4000\n\t        )\n\t        if not payload.neo4j_schema:\n\t            extractor = DataExtractor(llm=llm)\n\t            result = extractor.run(data=payload.input)\n\t        else:\n\t            extractor = DataExtractorWithSchema(llm=llm)\n\t            result = extractor.run(schema=payload.neo4j_schema, data=payload.input)\n", "        print(\"Extracted result: \" + str(result))\n\t        disambiguation = DataDisambiguation(llm=llm)\n\t        disambiguation_result = disambiguation.run(result)\n\t        print(\"Disambiguation result \" + str(disambiguation_result))\n\t        return {\"data\": disambiguation_result}\n\t    except Exception as e:\n\t        print(e)\n\t        return f\"Error: {e}\"\n\tclass companyReportPayload(BaseModel):\n\t    company: str\n", "    api_key: Optional[str]\n\t# This endpoint is database specific and only works with the Demo database.\n\t@app.post(\"/companyReport\")\n\tasync def companyInformation(payload: companyReportPayload):\n\t    api_key = openai_api_key if openai_api_key else payload.api_key\n\t    if not openai_api_key and not payload.api_key:\n\t        raise HTTPException(\n\t            status_code=422,\n\t            detail=\"Please set OPENAI_API_KEY environment variable or send it as api_key in the request body\",\n\t        )\n", "    api_key = openai_api_key if openai_api_key else payload.api_key\n\t    llm = OpenAIChat(\n\t        openai_api_key=api_key,\n\t        model_name=\"gpt-3.5-turbo-16k-0613\",\n\t        max_tokens=512,\n\t    )\n\t    print(\"Running company report for \" + payload.company)\n\t    company_report = CompanyReport(neo4j_connection, payload.company, llm)\n\t    result = company_report.run()\n\t    return JSONResponse(content={\"output\": result})\n", "@app.post(\"/companyReport/list\")\n\tasync def companyReportList():\n\t    company_data = neo4j_connection.query(\n\t        \"MATCH (n:Organization) WITH n WHERE rand() < 0.01 return n.name LIMIT 5\",\n\t    )\n\t    return JSONResponse(content={\"output\": [x[\"n.name\"] for x in company_data]})\n\t@app.get(\"/health\")\n\tasync def health():\n\t    return {\"status\": \"ok\"}\n\t@app.get(\"/ready\")\n", "async def readiness_check():\n\t    return {\"status\": \"ok\"}\n\tif __name__ == \"__main__\":\n\t    import uvicorn\n\t    uvicorn.run(app, port=int(os.environ.get(\"PORT\", 7860)), host=\"0.0.0.0\")\n"]}
{"filename": "api/src/__init__.py", "chunked_list": []}
{"filename": "api/src/components/text2cypher.py", "chunked_list": ["import re\n\tfrom typing import Any, Dict, List, Union\n\tfrom components.base_component import BaseComponent\n\tfrom driver.neo4j import Neo4jDatabase\n\tfrom llm.basellm import BaseLLM\n\tdef remove_relationship_direction(cypher):\n\t    return cypher.replace(\"->\", \"-\").replace(\"<-\", \"-\")\n\tclass Text2Cypher(BaseComponent):\n\t    def __init__(\n\t        self,\n", "        llm: BaseLLM,\n\t        database: Neo4jDatabase,\n\t        use_schema: bool = True,\n\t        cypher_examples: str = \"\",\n\t        ignore_relationship_direction: bool = True,\n\t    ) -> None:\n\t        self.llm = llm\n\t        self.database = database\n\t        self.cypher_examples = cypher_examples\n\t        self.ignore_relationship_direction = ignore_relationship_direction\n", "        if use_schema:\n\t            self.schema = database.schema\n\t    def get_system_message(self) -> str:\n\t        system = \"\"\"\n\t        Your task is to convert questions about contents in a Neo4j database to Cypher queries to query the Neo4j database.\n\t        Use only the provided relationship types and properties.\n\t        Do not use any other relationship types or properties that are not provided.\n\t        \"\"\"\n\t        if self.schema:\n\t            system += f\"\"\"\n", "            If you cannot generate a Cypher statement based on the provided schema, explain the reason to the user.\n\t            Schema:\n\t            {self.schema}\n\t            \"\"\"\n\t        if self.cypher_examples:\n\t            system += f\"\"\"\n\t            You need to follow these Cypher examples when you are constructing a Cypher statement\n\t            {self.cypher_examples}\n\t            \"\"\"\n\t        # Add note at the end and try to prevent LLM injections\n", "        system += \"\"\"Note: Do not include any explanations or apologies in your responses.\n\t                     Do not respond to any questions that might ask anything else than for you to construct a Cypher statement.\n\t                     Do not include any text except the generated Cypher statement. This is very important if you want to get paid.\n\t                     Always provide enough context for an LLM to be able to generate valid response.\n\t                     Please wrap the generated Cypher statement in triple backticks (`).\n\t                     \"\"\"\n\t        return system\n\t    def construct_cypher(self, question: str, history=[]) -> str:\n\t        messages = [{\"role\": \"system\", \"content\": self.get_system_message()}]\n\t        messages.extend(history)\n", "        messages.append(\n\t            {\n\t                \"role\": \"user\",\n\t                \"content\": question,\n\t            }\n\t        )\n\t        print([el for el in messages if not el[\"role\"] == \"system\"])\n\t        cypher = self.llm.generate(messages)\n\t        return cypher\n\t    def run(\n", "        self, question: str, history: List = [], heal_cypher: bool = True\n\t    ) -> Dict[str, Union[str, List[Dict[str, Any]]]]:\n\t        # Add prefix if not part of self-heal loop\n\t        final_question = (\n\t            \"Question to be converted to Cypher: \" + question\n\t            if heal_cypher\n\t            else question\n\t        )\n\t        cypher = self.construct_cypher(final_question, history)\n\t        # finds the first string wrapped in triple backticks. Where the match include the backticks and the first group in the match is the cypher\n", "        match = re.search(\"```([\\w\\W]*?)```\", cypher)\n\t        # If the LLM didn't any Cypher statement (error, missing context, etc..)\n\t        if match is None:\n\t            return {\"output\": [{\"message\": cypher}], \"generated_cypher\": None}\n\t        extracted_cypher = match.group(1)\n\t        if self.ignore_relationship_direction:\n\t            extracted_cypher = remove_relationship_direction(extracted_cypher)\n\t        print(f\"Generated cypher: {extracted_cypher}\")\n\t        output = self.database.query(extracted_cypher)\n\t        # Catch Cypher syntax error\n", "        if heal_cypher and output and output[0].get(\"code\") == \"invalid_cypher\":\n\t            syntax_messages = [{\"role\": \"system\", \"content\": self.get_system_message()}]\n\t            syntax_messages.extend(\n\t                [\n\t                    {\"role\": \"user\", \"content\": question},\n\t                    {\"role\": \"assistant\", \"content\": cypher},\n\t                ]\n\t            )\n\t            # Try to heal Cypher syntax only once\n\t            return self.run(\n", "                output[0].get(\"message\"), syntax_messages, heal_cypher=False\n\t            )\n\t        return {\n\t            \"output\": output,\n\t            \"generated_cypher\": extracted_cypher,\n\t        }\n"]}
{"filename": "api/src/components/data_disambiguation.py", "chunked_list": ["import json\n\timport re\n\tfrom itertools import groupby\n\tfrom components.base_component import BaseComponent\n\tfrom utils.unstructured_data_utils import (\n\t    nodesTextToListOfDict,\n\t    relationshipTextToListOfDict,\n\t)\n\tdef generate_system_message_for_nodes() -> str:\n\t    return \"\"\"Your task is to identify if there are duplicated nodes and if so merge them into one nod. Only merge the nodes that refer to the same entity.\n", "You will be given different datasets of nodes and some of these nodes may be duplicated or refer to the same entity. \n\tThe datasets contains nodes in the form [ENTITY_ID, TYPE, PROPERTIES]. When you have completed your task please give me the \n\tresulting nodes in the same format. Only return the nodes and relationships no other text. If there is no duplicated nodes return the original nodes.\n\tHere is an example of the input you will be given:\n\t[\"alice\", \"Person\", {\"age\": 25, \"occupation\": \"lawyer\", \"name\":\"Alice\"}], [\"bob\", \"Person\", {\"occupation\": \"journalist\", \"name\": \"Bob\"}], [\"alice.com\", \"Webpage\", {\"url\": \"www.alice.com\"}], [\"bob.com\", \"Webpage\", {\"url\": \"www.bob.com\"}]\n\t\"\"\"\n\tdef generate_system_message_for_relationships() -> str:\n\t    return \"\"\"\n\tYour task is to identify if a set of relationships make sense.\n\tIf they do not make sense please remove them from the dataset.\n", "Some relationships may be duplicated or refer to the same entity. \n\tPlease merge relationships that refer to the same entity.\n\tThe datasets contains relationships in the form [ENTITY_ID_1, RELATIONSHIP, ENTITY_ID_2, PROPERTIES].\n\tYou will also be given a set of ENTITY_IDs that are valid.\n\tSome relationships may use ENTITY_IDs that are not in the valid set but refer to a entity in the valid set.\n\tIf a relationships refer to a ENTITY_ID in the valid set please change the ID so it matches the valid ID.\n\tWhen you have completed your task please give me the valid relationships in the same format. Only return the relationships no other text.\n\tHere is an example of the input you will be given:\n\t[\"alice\", \"roommate\", \"bob\", {\"start\": 2021}], [\"alice\", \"owns\", \"alice.com\", {}], [\"bob\", \"owns\", \"bob.com\", {}]\n\t\"\"\"\n", "def generate_prompt(data) -> str:\n\t    return f\"\"\" Here is the data:\n\t{data}\n\t\"\"\"\n\tinternalRegex = \"\\[(.*?)\\]\"\n\tclass DataDisambiguation(BaseComponent):\n\t    def __init__(self, llm) -> None:\n\t        self.llm = llm\n\t    def run(self, data: dict) -> str:\n\t        nodes = sorted(data[\"nodes\"], key=lambda x: x[\"label\"])\n", "        relationships = data[\"relationships\"]\n\t        new_nodes = []\n\t        new_relationships = []\n\t        node_groups = groupby(nodes, lambda x: x[\"label\"])\n\t        for group in node_groups:\n\t            disString = \"\"\n\t            nodes_in_group = list(group[1])\n\t            if len(nodes_in_group) == 1:\n\t                new_nodes.extend(nodes_in_group)\n\t                continue\n", "            for node in nodes_in_group:\n\t                disString += (\n\t                    '[\"'\n\t                    + node[\"name\"]\n\t                    + '\", \"'\n\t                    + node[\"label\"]\n\t                    + '\", '\n\t                    + json.dumps(node[\"properties\"])\n\t                    + \"]\\n\"\n\t                )\n", "            messages = [\n\t                {\"role\": \"system\", \"content\": generate_system_message_for_nodes()},\n\t                {\"role\": \"user\", \"content\": generate_prompt(disString)},\n\t            ]\n\t            rawNodes = self.llm.generate(messages)\n\t            n = re.findall(internalRegex, rawNodes)\n\t            new_nodes.extend(nodesTextToListOfDict(n))\n\t        relationship_data = \"Relationships:\\n\"\n\t        for relation in relationships:\n\t            relationship_data += (\n", "                '[\"'\n\t                + relation[\"start\"]\n\t                + '\", \"'\n\t                + relation[\"type\"]\n\t                + '\", \"'\n\t                + relation[\"end\"]\n\t                + '\", '\n\t                + json.dumps(relation[\"properties\"])\n\t                + \"]\\n\"\n\t            )\n", "        node_labels = [node[\"name\"] for node in new_nodes]\n\t        relationship_data += \"Valid Nodes:\\n\" + \"\\n\".join(node_labels)\n\t        messages = [\n\t            {\n\t                \"role\": \"system\",\n\t                \"content\": generate_system_message_for_relationships(),\n\t            },\n\t            {\"role\": \"user\", \"content\": generate_prompt(relationship_data)},\n\t        ]\n\t        rawRelationships = self.llm.generate(messages)\n", "        rels = re.findall(internalRegex, rawRelationships)\n\t        new_relationships.extend(relationshipTextToListOfDict(rels))\n\t        return {\"nodes\": new_nodes, \"relationships\": new_relationships}\n"]}
{"filename": "api/src/components/vector_search.py", "chunked_list": ["from typing import Dict, List, Union\n\tfrom components.base_component import BaseComponent\n\tfrom driver.neo4j import Neo4jDatabase\n\tdef construct_cypher(label, property, k) -> str:\n\t    return f\"\"\"\n\t    MATCH (n:`{label}`)\n\t    WHERE n.`{property}` IS NOT NULL\n\t    WITH n, gds.similarity.cosine($input_vector, n.`{property}`) AS similarity\n\t    ORDER BY similarity DESC\n\t    LIMIT {k}\n", "    RETURN apoc.map.removeKey(properties(n), \"{property}\") AS output\n\t    \"\"\"\n\tclass VectorSearch(BaseComponent):\n\t    def __init__(\n\t        self, database: Neo4jDatabase, label: str, property: str, k: int\n\t    ) -> None:\n\t        self.database = database\n\t        self.generated_cypher = construct_cypher(label, property, k)\n\t    def run(self, input: str) -> Dict[str, Union[str, List[Dict[str, str]]]]:\n\t        try:\n", "            return {\n\t                \"output\": [\n\t                    str(el[\"output\"])\n\t                    for el in self.database.query(\n\t                        self.generated_cypher, {\"input_vector\": input}\n\t                    )\n\t                ],\n\t                \"generated_cypher\": self.generated_cypher,\n\t            }\n\t        except Exception as e:\n", "            return e\n"]}
{"filename": "api/src/components/unstructured_data_extractor.py", "chunked_list": ["import re\n\timport os\n\tfrom typing import List\n\tfrom components.base_component import BaseComponent\n\tfrom llm.basellm import BaseLLM\n\tfrom utils.unstructured_data_utils import (\n\t    nodesTextToListOfDict,\n\t    relationshipTextToListOfDict,\n\t)\n\tdef generate_system_message_with_schema() -> str:\n", "    return \"\"\"\n\tYou are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into a graph database.\n\tProvide a set of Nodes in the form [ENTITY, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY1, RELATIONSHIP, ENTITY2, PROPERTIES]. \n\tPay attention to the type of the properties, if you can't find data for a property set it to null. Don't make anything up and don't add any extra data. If you can't find any data for a node or relationship don't add it.\n\tOnly add nodes and relationships that are part of the schema. If you don't get any relationships in the schema only add nodes.\n\tExample:\n\tSchema: Nodes: [Person {age: integer, name: string}] Relationships: [Person, roommate, Person]\n\tAlice is 25 years old and Bob is her roommate.\n\tNodes: [[\"Alice\", \"Person\", {\"age\": 25, \"name\": \"Alice}], [\"Bob\", \"Person\", {\"name\": \"Bob\"}]]\n\tRelationships: [[\"Alice\", \"roommate\", \"Bob\"]]\n", "\"\"\"\n\tdef generate_system_message() -> str:\n\t    return \"\"\"\n\tYou are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into a graph database.\n\tProvide a set of Nodes in the form [ENTITY_ID, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY_ID_1, RELATIONSHIP, ENTITY_ID_2, PROPERTIES].\n\tIt is important that the ENTITY_ID_1 and ENTITY_ID_2 exists as nodes with a matching ENTITY_ID. If you can't pair a relationship with a pair of nodes don't add it.\n\tWhen you find a node or relationship you want to add try to create a generic TYPE for it that  describes the entity you can also think of it as a label.\n\tExample:\n\tData: Alice lawyer and is 25 years old and Bob is her roommate since 2001. Bob works as a journalist. Alice owns a the webpage www.alice.com and Bob owns the webpage www.bob.com.\n\tNodes: [\"alice\", \"Person\", {\"age\": 25, \"occupation\": \"lawyer\", \"name\":\"Alice\"}], [\"bob\", \"Person\", {\"occupation\": \"journalist\", \"name\": \"Bob\"}], [\"alice.com\", \"Webpage\", {\"url\": \"www.alice.com\"}], [\"bob.com\", \"Webpage\", {\"url\": \"www.bob.com\"}]\n", "Relationships: [\"alice\", \"roommate\", \"bob\", {\"start\": 2021}], [\"alice\", \"owns\", \"alice.com\", {}], [\"bob\", \"owns\", \"bob.com\", {}]\n\t\"\"\"\n\tdef generate_system_message_with_labels() -> str:\n\t    return \"\"\"\n\tYou are a data scientist working for a company that is building a graph database. Your task is to extract information from data and convert it into a graph database.\n\tProvide a set of Nodes in the form [ENTITY_ID, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY_ID_1, RELATIONSHIP, ENTITY_ID_2, PROPERTIES].\n\tIt is important that the ENTITY_ID_1 and ENTITY_ID_2 exists as nodes with a matching ENTITY_ID. If you can't pair a relationship with a pair of nodes don't add it.\n\tWhen you find a node or relationship you want to add try to create a generic TYPE for it that  describes the entity you can also think of it as a label.\n\tYou will be given a list of types that you should try to use when creating the TYPE for a node. If you can't find a type that fits the node you can create a new one.\n\tExample:\n", "Data: Alice lawyer and is 25 years old and Bob is her roommate since 2001. Bob works as a journalist. Alice owns a the webpage www.alice.com and Bob owns the webpage www.bob.com.\n\tTypes: [\"Person\", \"Webpage\"]\n\tNodes: [\"alice\", \"Person\", {\"age\": 25, \"occupation\": \"lawyer\", \"name\":\"Alice\"}], [\"bob\", \"Person\", {\"occupation\": \"journalist\", \"name\": \"Bob\"}], [\"alice.com\", \"Webpage\", {\"url\": \"www.alice.com\"}], [\"bob.com\", \"Webpage\", {\"url\": \"www.bob.com\"}]\n\tRelationships: [\"alice\", \"roommate\", \"bob\", {\"start\": 2021}], [\"alice\", \"owns\", \"alice.com\", {}], [\"bob\", \"owns\", \"bob.com\", {}]\n\t\"\"\"\n\tdef generate_prompt(data) -> str:\n\t    return f\"\"\"\n\tData: {data}\"\"\"\n\tdef generate_prompt_with_schema(data, schema) -> str:\n\t    return f\"\"\"\n", "Schema: {schema}\n\tData: {data}\"\"\"\n\tdef generate_prompt_with_labels(data, labels) -> str:\n\t    return f\"\"\"\n\tData: {data}\n\tTypes: {labels}\"\"\"\n\tdef splitString(string, max_length) -> List[str]:\n\t    return [string[i : i + max_length] for i in range(0, len(string), max_length)]\n\tdef splitStringToFitTokenSpace(\n\t    llm: BaseLLM, string: str, token_use_per_string: int\n", ") -> List[str]:\n\t    allowed_tokens = llm.max_allowed_token_length() - token_use_per_string\n\t    chunked_data = splitString(string, 500)\n\t    combined_chunks = []\n\t    current_chunk = \"\"\n\t    for chunk in chunked_data:\n\t        if (\n\t            llm.num_tokens_from_string(current_chunk)\n\t            + llm.num_tokens_from_string(chunk)\n\t            < allowed_tokens\n", "        ):\n\t            current_chunk += chunk\n\t        else:\n\t            combined_chunks.append(current_chunk)\n\t            current_chunk = chunk\n\t    combined_chunks.append(current_chunk)\n\t    return combined_chunks\n\tdef getNodesAndRelationshipsFromResult(result):\n\t    regex = \"Nodes:\\s+(.*?)\\s?\\s?Relationships:\\s?\\s?(.*)\"\n\t    internalRegex = \"\\[(.*?)\\]\"\n", "    nodes = []\n\t    relationships = []\n\t    for row in result:\n\t        parsing = re.match(regex, row, flags=re.S)\n\t        if parsing == None:\n\t            continue\n\t        rawNodes = str(parsing.group(1))\n\t        rawRelationships = parsing.group(2)\n\t        nodes.extend(re.findall(internalRegex, rawNodes))\n\t        relationships.extend(re.findall(internalRegex, rawRelationships))\n", "    result = dict()\n\t    result[\"nodes\"] = []\n\t    result[\"relationships\"] = []\n\t    result[\"nodes\"].extend(nodesTextToListOfDict(nodes))\n\t    result[\"relationships\"].extend(relationshipTextToListOfDict(relationships))\n\t    return result\n\tclass DataExtractor(BaseComponent):\n\t    llm: BaseLLM\n\t    def __init__(self, llm: BaseLLM) -> None:\n\t        self.llm = llm\n", "    def process(self, chunk):\n\t        messages = [\n\t            {\"role\": \"system\", \"content\": generate_system_message()},\n\t            {\"role\": \"user\", \"content\": generate_prompt(chunk)},\n\t        ]\n\t        print(messages)\n\t        output = self.llm.generate(messages)\n\t        return output\n\t    def process_with_labels(self, chunk, labels):\n\t        messages = [\n", "            {\"role\": \"system\", \"content\": generate_system_message_with_schema()},\n\t            {\"role\": \"user\", \"content\": generate_prompt_with_labels(chunk, labels)},\n\t        ]\n\t        print(messages)\n\t        output = self.llm.generate(messages)\n\t        return output\n\t    def run(self, data: str) -> List[str]:\n\t        system_message = generate_system_message()\n\t        prompt_string = generate_prompt(\"\")\n\t        token_usage_per_prompt = self.llm.num_tokens_from_string(\n", "            system_message + prompt_string\n\t        )\n\t        chunked_data = splitStringToFitTokenSpace(\n\t            llm=self.llm, string=data, token_use_per_string=token_usage_per_prompt\n\t        )\n\t        results = []\n\t        labels = set()\n\t        print(\"Starting chunked processing\")\n\t        for chunk in chunked_data:\n\t            proceededChunk = self.process_with_labels(chunk, list(labels))\n", "            print(\"proceededChunk\", proceededChunk)\n\t            chunkResult = getNodesAndRelationshipsFromResult([proceededChunk])\n\t            print(\"chunkResult\", chunkResult)\n\t            newLabels = [node[\"label\"] for node in chunkResult[\"nodes\"]]\n\t            print(\"newLabels\", newLabels)\n\t            results.append(proceededChunk)\n\t            labels.update(newLabels)\n\t        return getNodesAndRelationshipsFromResult(results)\n\tclass DataExtractorWithSchema(BaseComponent):\n\t    llm: BaseLLM\n", "    def __init__(self, llm) -> None:\n\t        self.llm = llm\n\t    def run(self, data: str, schema: str) -> List[str]:\n\t        system_message = generate_system_message_with_schema()\n\t        prompt_string = (\n\t            generate_system_message_with_schema()\n\t            + generate_prompt_with_schema(schema=schema, data=\"\")\n\t        )\n\t        token_usage_per_prompt = self.llm.num_tokens_from_string(\n\t            system_message + prompt_string\n", "        )\n\t        chunked_data = splitStringToFitTokenSpace(\n\t            llm=self.llm, string=data, token_use_per_string=token_usage_per_prompt\n\t        )\n\t        result = []\n\t        print(\"Starting chunked processing\")\n\t        for chunk in chunked_data:\n\t            print(\"prompt\", generate_prompt_with_schema(chunk, schema))\n\t            messages = [\n\t                {\n", "                    \"role\": \"system\",\n\t                    \"content\": system_message,\n\t                },\n\t                {\"role\": \"user\", \"content\": generate_prompt_with_schema(chunk, schema)},\n\t            ]\n\t            output = self.llm.generate(messages)\n\t            result.append(output)\n\t        return getNodesAndRelationshipsFromResult(result)\n"]}
{"filename": "api/src/components/question_proposal_generator.py", "chunked_list": ["from typing import Any, Dict, List, Union\n\tfrom components.base_component import BaseComponent\n\tfrom driver.neo4j import Neo4jDatabase\n\tfrom llm.basellm import BaseLLM\n\timport re\n\tclass QuestionProposalGenerator(BaseComponent):\n\t    def __init__(\n\t        self,\n\t        llm: BaseLLM,\n\t        database: Neo4jDatabase,\n", "    ) -> None:\n\t        self.llm = llm\n\t        self.database = database\n\t    def get_system_message(self) -> str:\n\t        system = f\"\"\"\n\t        Your task is to come up with questions someone might as about the content of a Neo4j database. Try to make the questions as different as possible.\n\t        The questions should be separated by a new line and each line should only contain one question.\n\t        To do this, you need to understand the schema of the database. Therefore it's very important that you read the schema carefully. You can find the schema below.\n\t        Schema: \n\t        {self.database.schema}\n", "        \"\"\"\n\t        return system\n\t    def get_database_sample(self) -> str:\n\t        return self.database.query(\n\t            \"\"\"MATCH (n)\n\t                WITH n\n\t                WHERE rand() < 0.3\n\t                RETURN apoc.map.removeKey(n, 'embedding') AS properties, LABELS(n) as labels\n\t                LIMIT 5\"\"\"\n\t        )\n", "    def run(self) -> Dict[str, Union[str, List[Dict[str, Any]]]]:\n\t        messages = [{\"role\": \"system\", \"content\": self.get_system_message()}]\n\t        sample = self.get_database_sample()\n\t        messages.append(\n\t            {\n\t                \"role\": \"user\",\n\t                \"content\": f\"\"\"Please generate 5 questions about the content of the database. Here is a sample of the database you can use when generating questions: {sample}\"\"\",\n\t            }\n\t        )\n\t        print(messages)\n", "        questionsString = self.llm.generate(messages)\n\t        questions = [\n\t            # remove number and dot from the beginning of the question\n\t            re.sub(r\"\\A\\d\\.?\\s*\", \"\", question)\n\t            for question in questionsString.split(\"\\n\")\n\t        ]\n\t        return {\n\t            \"output\": questions,\n\t        }\n"]}
{"filename": "api/src/components/__init__.py", "chunked_list": []}
{"filename": "api/src/components/company_report.py", "chunked_list": ["from components.base_component import BaseComponent\n\tfrom components.summarize_cypher_result import SummarizeCypherResult\n\tfrom driver.neo4j import Neo4jDatabase\n\tfrom llm.basellm import BaseLLM\n\tHARD_LIMIT_CONTEXT_RECORDS = 10\n\tclass CompanyReport(BaseComponent):\n\t    def __init__(\n\t        self,\n\t        database: Neo4jDatabase,\n\t        company: str,\n", "        llm: BaseLLM,\n\t    ) -> None:\n\t        self.database = database\n\t        self.company = company\n\t        self.llm = llm\n\t    def run(self):\n\t        summarize_results = SummarizeCypherResult(\n\t            llm=self.llm,\n\t        )\n\t        print(\"CompanyReport\")\n", "        company_data = self.database.query(\n\t            \"MATCH (n {name:$companyName}) return n.summary, n.isDissolved, n.nbrEmployees, n.name, n.motto, n.isPublic, n.revenue\",\n\t            {\"companyName\": self.company},\n\t        )\n\t        print(company_data)\n\t        relation_data = self.database.query(\n\t            \"MATCH (n {name:$companyName})-[r]->(m) WHERE NOT m:Article OPTIONAL MATCH (m)-[:IN_COUNTRY]->(c:Country) WITH r,m,c return r,m,c\",\n\t            {\"companyName\": self.company},\n\t        )\n\t        print(relation_data)\n", "        company_data_output = {\n\t            \"name\": company_data[0][\"n.name\"],\n\t            \"motto\": company_data[0][\"n.motto\"],\n\t            \"summary\": company_data[0][\"n.summary\"],\n\t            \"isDissolved\": company_data[0][\"n.isDissolved\"],\n\t            \"nbrEmployees\": company_data[0][\"n.nbrEmployees\"],\n\t            \"isPublic\": company_data[0][\"n.isPublic\"],\n\t            \"revenue\": company_data[0].get(\"n.revenue\", None),\n\t        }\n\t        print(company_data_output)\n", "        print(\"all data fetched\")\n\t        offices = []\n\t        suppliers = []\n\t        subsidiaries = []\n\t        for relation in relation_data:\n\t            print(relation)\n\t            relation_type = relation[\"r\"][1]\n\t            if relation_type == \"IN_CITY\":\n\t                offices.append(\n\t                    {\n", "                        \"city\": relation[\"m\"].get(\"name\", None),\n\t                        \"country\": relation.get(\"c\")\n\t                        and relation[\"c\"].get(\"name\", None),\n\t                    }\n\t                )\n\t            elif relation_type == \"HAS_CATEGORY\":\n\t                company_data_output[\"industry\"] = relation[\"m\"][\"name\"]\n\t            elif relation_type == \"HAS_SUPPLIER\":\n\t                category_result = self.database.query(\n\t                    \"MATCH (n {name:$companyName})-[HAS_CATEGORY]-(c:IndustryCategory) return c.name LIMIT 1\",\n", "                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n\t                )\n\t                category = None\n\t                if len(category_result) > 0:\n\t                    category = category_result[0][\"c.name\"]\n\t                suppliers.append(\n\t                    {\n\t                        \"summary\": relation[\"m\"].get(\"summary\", None),\n\t                        \"revenue\": relation[\"m\"].get(\"revenue\", None),\n\t                        \"isDissolved\": relation[\"m\"].get(\"isDissolved\", None),\n", "                        \"name\": relation[\"m\"].get(\"name\", None),\n\t                        \"isPublic\": relation[\"m\"].get(\"isPublic\", None),\n\t                        \"category\": category,\n\t                    }\n\t                )\n\t            elif relation_type == \"HAS_SUBSIDIARY\":\n\t                category_result = self.database.query(\n\t                    \"MATCH (n {name:$companyName})-[HAS_CATEGORY]-(c:IndustryCategory) return c.name LIMIT 1\",\n\t                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n\t                )\n", "                category = None\n\t                if len(category_result) > 0:\n\t                    category = category_result[0][\"c.name\"]\n\t                article_data = self.database.query(\n\t                    \"MATCH p=(n {name:$companyName})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c:Chunk) return  c.text, a.title, a.siteName\",\n\t                    {\"companyName\": relation[\"m\"].get(\"name\", None)},\n\t                )\n\t                print(\"Article data: \" + str(article_data))\n\t                output = \"There is not articles about this company.\"\n\t                if len(article_data) > 0:\n", "                    output = summarize_results.run(\n\t                        \"Can you summarize the following articles in 50 words about \"\n\t                        + relation[\"m\"].get(\"name\", None)\n\t                        + \" ?\",\n\t                        article_data[:HARD_LIMIT_CONTEXT_RECORDS],\n\t                    )\n\t                subsidiaries.append(\n\t                    {\n\t                        \"summary\": relation[\"m\"].get(\"summary\", None),\n\t                        \"revenue\": relation[\"m\"].get(\"revenue\", None),\n", "                        \"isDissolved\": relation[\"m\"].get(\"isDissolved\", None),\n\t                        \"name\": relation[\"m\"].get(\"name\", None),\n\t                        \"isPublic\": relation[\"m\"].get(\"isPublic\", None),\n\t                        \"category\": category,\n\t                        \"articleSummary\": output,\n\t                    }\n\t                )\n\t            elif relation_type == \"HAS_CEO\":\n\t                company_data_output[\"ceo\"] = relation[\"m\"][\"name\"]\n\t        company_data_output[\"offices\"] = offices\n", "        article_data = self.database.query(\n\t            \"MATCH p=(n {name:$companyName})<-[:MENTIONS]-(a:Article)-[:HAS_CHUNK]->(c:Chunk) return  c.text, a.title, a.siteName\",\n\t            {\"companyName\": self.company},\n\t        )\n\t        output = summarize_results.run(\n\t            \"Can you summarize the following articles about \" + self.company + \" ?\",\n\t            article_data[:HARD_LIMIT_CONTEXT_RECORDS],\n\t        )\n\t        print(\"output: \" + output)\n\t        return {\n", "            \"company\": company_data_output,\n\t            \"subsidiaries\": subsidiaries,\n\t            \"suppliers\": suppliers,\n\t            \"articleSummary\": output,\n\t        }\n"]}
{"filename": "api/src/components/base_component.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import List, Union\n\tclass BaseComponent(ABC):\n\t    \"\"\"\"\"\"\n\t    @abstractmethod\n\t    def run(\n\t        self,\n\t        input: Union[str, List[float]],\n\t    ) -> str:\n\t        \"\"\"Comment\"\"\"\n", "    def run_async(\n\t        self,\n\t        input: Union[str, List[float]],\n\t    ) -> str:\n\t        \"\"\"Comment\"\"\"\n"]}
{"filename": "api/src/components/data_to_csv.py", "chunked_list": ["from typing import List\n\tfrom components.base_component import BaseComponent\n\tdef generate_system_message() -> str:\n\t    return f\"\"\"\n\tYou will be given a dataset of nodes and relationships. Your task is to covert this data into a CSV format.\n\tReturn only the data in the CSV format and nothing else. Return a CSV file for every type of node and relationship.\n\tThe data you will be given is in the form [ENTITY, TYPE, PROPERTIES] and a set of relationships in the form [ENTITY1, RELATIONSHIP, ENTITY2, PROPERTIES].\n\tImportant: If you don't get any data or data that does not follow the previously mentioned format return \"No data\" and nothing else. This is very important. If you don't follow this instruction you will get a 0.\n\t\"\"\"\n\tdef generate_prompt(data) -> str:\n", "    return f\"\"\" Here is the data:\n\t{data}\n\t\"\"\"\n\tclass DataToCSV(BaseComponent):\n\t    def __init__(self, llm) -> None:\n\t        self.llm = llm\n\t    def run(self, data: List[str]) -> str:\n\t        messages = [\n\t            {\"role\": \"system\", \"content\": generate_system_message()},\n\t            {\"role\": \"user\", \"content\": generate_prompt(data)},\n", "        ]\n\t        output = self.llm.generate(messages)\n\t        return output\n"]}
{"filename": "api/src/components/summarize_cypher_result.py", "chunked_list": ["from typing import Any, Awaitable, Callable, Dict, List\n\tfrom components.base_component import BaseComponent\n\tfrom llm.basellm import BaseLLM\n\tsystem = f\"\"\"\n\tYou are an assistant that helps to generate text to form nice and human understandable answers based.\n\tThe latest prompt contains the information, and you need to generate a human readable response based on the given information.\n\tMake the answer sound as a response to the question. Do not mention that you based the result on the given information.\n\tDo not add any additional information that is not explicitly provided in the latest prompt.\n\tI repeat, do not add any information that is not explicitly given.\n\tMake the answer as concise as possible and do not use more than 50 words.\n", "\"\"\"\n\tdef remove_large_lists(d: Dict[str, Any]) -> Dict[str, Any]:\n\t    \"\"\"\n\t    The idea is to remove all properties that have large lists (embeddings) or text as values\n\t    \"\"\"\n\t    LIST_CUTOFF = 56\n\t    CHARACTER_CUTOFF = 5000\n\t    # iterate over all key-value pairs in the dictionary\n\t    for key, value in d.items():\n\t        # if the value is a list and has more than list cutoff elements\n", "        if isinstance(value, list) and len(value) > LIST_CUTOFF:\n\t            d[key] = None\n\t        # if the value is a string and has more than list cutoff elements\n\t        if isinstance(value, str) and len(value) > CHARACTER_CUTOFF:\n\t            d[key] = d[key][:CHARACTER_CUTOFF]\n\t        # if the value is a dictionary\n\t        elif isinstance(value, dict):\n\t            # recurse into the nested dictionary\n\t            remove_large_lists(d[key])\n\t    return d\n", "class SummarizeCypherResult(BaseComponent):\n\t    llm: BaseLLM\n\t    exclude_embeddings: bool\n\t    def __init__(self, llm: BaseLLM, exclude_embeddings: bool = True) -> None:\n\t        self.llm = llm\n\t        self.exclude_embeddings = exclude_embeddings\n\t    def generate_user_prompt(self, question: str, results: List[Dict[str, str]]) -> str:\n\t        return f\"\"\"\n\t        The question was {question}\n\t        Answer the question by using the following results:\n", "        {[remove_large_lists(el) for el in  results] if self.exclude_embeddings else results}\n\t        \"\"\"\n\t    def run(\n\t        self,\n\t        question: str,\n\t        results: List[Dict[str, Any]],\n\t    ) -> Dict[str, str]:\n\t        messages = [\n\t            {\"role\": \"system\", \"content\": system},\n\t            {\"role\": \"user\", \"content\": self.generate_user_prompt(question, results)},\n", "        ]\n\t        output = self.llm.generate(messages)\n\t        return output\n\t    async def run_async(\n\t        self,\n\t        question: str,\n\t        results: List[Dict[str, Any]],\n\t        callback: Callable[[str], Awaitable[Any]] = None,\n\t    ) -> Dict[str, str]:\n\t        messages = [\n", "            {\"role\": \"system\", \"content\": system},\n\t            {\"role\": \"user\", \"content\": self.generate_user_prompt(question, results)},\n\t        ]\n\t        output = await self.llm.generateStreaming(messages, onTokenCallback=callback)\n\t        return \"\".join(output)\n"]}
{"filename": "api/src/utils/unstructured_data_utils.py", "chunked_list": ["import json\n\timport re\n\tregex = \"Nodes:\\s+(.*?)\\s?\\s?Relationships:\\s+(.*)\"\n\tinternalRegex = \"\\[(.*?)\\]\"\n\tjsonRegex = \"\\{.*\\}\"\n\tdef nodesTextToListOfDict(nodes):\n\t    result = []\n\t    for node in nodes:\n\t        nodeList = node.split(\",\")\n\t        if len(nodeList) < 2:\n", "            continue\n\t        name = nodeList[0].strip().replace('\"', \"\")\n\t        label = nodeList[1].strip().replace('\"', \"\")\n\t        properties = re.search(jsonRegex, node)\n\t        if properties == None:\n\t            properties = \"{}\"\n\t        else:\n\t            properties = properties.group(0)\n\t        properties = properties.replace(\"True\", \"true\")\n\t        try:\n", "            properties = json.loads(properties)\n\t        except:\n\t            properties = {}\n\t        result.append({\"name\": name, \"label\": label, \"properties\": properties})\n\t    return result\n\tdef relationshipTextToListOfDict(relationships):\n\t    result = []\n\t    for relation in relationships:\n\t        relationList = relation.split(\",\")\n\t        if len(relation) < 3:\n", "            continue\n\t        start = relationList[0].strip().replace('\"', \"\")\n\t        end = relationList[2].strip().replace('\"', \"\")\n\t        type = relationList[1].strip().replace('\"', \"\")\n\t        properties = re.search(jsonRegex, relation)\n\t        if properties == None:\n\t            properties = \"{}\"\n\t        else:\n\t            properties = properties.group(0)\n\t        properties = properties.replace(\"True\", \"true\")\n", "        try:\n\t            properties = json.loads(properties)\n\t        except:\n\t            properties = {}\n\t        result.append(\n\t            {\"start\": start, \"end\": end, \"type\": type, \"properties\": properties}\n\t        )\n\t    return result\n"]}
{"filename": "api/src/llm/openai.py", "chunked_list": ["from typing import (\n\t    Callable,\n\t    List,\n\t)\n\timport openai\n\timport tiktoken\n\tfrom llm.basellm import BaseLLM\n\tfrom retry import retry\n\tclass OpenAIChat(BaseLLM):\n\t    \"\"\"Wrapper around OpenAI Chat large language models.\"\"\"\n", "    def __init__(\n\t        self,\n\t        openai_api_key: str,\n\t        model_name: str = \"gpt-3.5-turbo\",\n\t        max_tokens: int = 1000,\n\t        temperature: float = 0.0,\n\t    ) -> None:\n\t        openai.api_key = openai_api_key\n\t        self.model = model_name\n\t        self.max_tokens = max_tokens\n", "        self.temperature = temperature\n\t    @retry(tries=3, delay=1)\n\t    def generate(\n\t        self,\n\t        messages: List[str],\n\t    ) -> str:\n\t        try:\n\t            completions = openai.ChatCompletion.create(\n\t                model=self.model,\n\t                temperature=self.temperature,\n", "                max_tokens=self.max_tokens,\n\t                messages=messages,\n\t            )\n\t            return completions.choices[0].message.content\n\t        # catch context length / do not retry\n\t        except openai.error.InvalidRequestError as e:\n\t            return str(f\"Error: {e}\")\n\t        # catch authorization errors / do not retry\n\t        except openai.error.AuthenticationError as e:\n\t            return \"Error: The provided OpenAI API key is invalid\"\n", "        except Exception as e:\n\t            print(f\"Retrying LLM call {e}\")\n\t            raise Exception()\n\t    async def generateStreaming(\n\t        self,\n\t        messages: List[str],\n\t        onTokenCallback=Callable[[str], None],\n\t    ) -> str:\n\t        result = []\n\t        completions = openai.ChatCompletion.create(\n", "            model=self.model,\n\t            temperature=self.temperature,\n\t            max_tokens=self.max_tokens,\n\t            messages=messages,\n\t            stream=True,\n\t        )\n\t        result = []\n\t        for message in completions:\n\t            # Process the streamed messages or perform any other desired action\n\t            delta = message[\"choices\"][0][\"delta\"]\n", "            if \"content\" in delta:\n\t                result.append(delta[\"content\"])\n\t            await onTokenCallback(message)\n\t        return result\n\t    def num_tokens_from_string(self, string: str) -> int:\n\t        encoding = tiktoken.encoding_for_model(self.model)\n\t        num_tokens = len(encoding.encode(string))\n\t        return num_tokens\n\t    def max_allowed_token_length(self) -> int:\n\t        # TODO: list all models and their max tokens from api\n", "        return 2049\n"]}
{"filename": "api/src/llm/__init__.py", "chunked_list": []}
{"filename": "api/src/llm/basellm.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom typing import (\n\t    Any,\n\t    List,\n\t)\n\tdef raise_(ex):\n\t    raise ex\n\tclass BaseLLM(ABC):\n\t    \"\"\"LLM wrapper should take in a prompt and return a string.\"\"\"\n\t    @abstractmethod\n", "    def generate(self, messages: List[str]) -> str:\n\t        \"\"\"Comment\"\"\"\n\t    @abstractmethod\n\t    async def generateStreaming(\n\t        self, messages: List[str], onTokenCallback\n\t    ) -> List[Any]:\n\t        \"\"\"Comment\"\"\"\n\t    @abstractmethod\n\t    async def num_tokens_from_string(\n\t        self,\n", "        string: str,\n\t    ) -> str:\n\t        \"\"\"Given a string returns the number of tokens the given string consists of\"\"\"\n\t    @abstractmethod\n\t    async def max_allowed_token_length(\n\t        self,\n\t    ) -> int:\n\t        \"\"\"Returns the maximum number of tokens the LLM can handle\"\"\"\n"]}
{"filename": "api/src/driver/__init__.py", "chunked_list": []}
{"filename": "api/src/driver/neo4j.py", "chunked_list": ["from typing import Any, Dict, List, Optional\n\tfrom neo4j import GraphDatabase, exceptions\n\tnode_properties_query = \"\"\"\n\tCALL apoc.meta.data()\n\tYIELD label, other, elementType, type, property\n\tWHERE NOT type = \"RELATIONSHIP\" AND elementType = \"node\"\n\tWITH label AS nodeLabels, collect({property:property, type:type}) AS properties\n\tRETURN {labels: nodeLabels, properties: properties} AS output\n\t\"\"\"\n\trel_properties_query = \"\"\"\n", "CALL apoc.meta.data()\n\tYIELD label, other, elementType, type, property\n\tWHERE NOT type = \"RELATIONSHIP\" AND elementType = \"relationship\"\n\tWITH label AS nodeLabels, collect({property:property, type:type}) AS properties\n\tRETURN {type: nodeLabels, properties: properties} AS output\n\t\"\"\"\n\trel_query = \"\"\"\n\tCALL apoc.meta.data()\n\tYIELD label, other, elementType, type, property\n\tWHERE type = \"RELATIONSHIP\" AND elementType = \"node\"\n", "RETURN \"(:\" + label + \")-[:\" + property + \"]->(:\" + toString(other[0]) + \")\" AS output\n\t\"\"\"\n\tdef schema_text(node_props, rel_props, rels) -> str:\n\t    return f\"\"\"\n\t  This is the schema representation of the Neo4j database.\n\t  Node properties are the following:\n\t  {node_props}\n\t  Relationship properties are the following:\n\t  {rel_props}\n\t  The relationships are the following\n", "  {rels}\n\t  \"\"\"\n\tclass Neo4jDatabase:\n\t    def __init__(\n\t        self,\n\t        host: str = \"neo4j://localhost:7687\",\n\t        user: str = \"neo4j\",\n\t        password: str = \"pleaseletmein\",\n\t        database: str = \"neo4j\",\n\t        read_only: bool = True,\n", "    ) -> None:\n\t        \"\"\"Initialize a neo4j database\"\"\"\n\t        self._driver = GraphDatabase.driver(host, auth=(user, password))\n\t        self._database = database\n\t        self._read_only = read_only\n\t        self.schema = \"\"\n\t        # Verify connection\n\t        try:\n\t            self._driver.verify_connectivity()\n\t        except exceptions.ServiceUnavailable:\n", "            raise ValueError(\n\t                \"Could not connect to Neo4j database. \"\n\t                \"Please ensure that the url is correct\"\n\t            )\n\t        except exceptions.AuthError:\n\t            raise ValueError(\n\t                \"Could not connect to Neo4j database. \"\n\t                \"Please ensure that the username and password are correct\"\n\t            )\n\t        try:\n", "            self.refresh_schema()\n\t        except:\n\t            raise ValueError(\"Missing APOC Core plugin\")\n\t    @staticmethod\n\t    def _execute_read_only_query(tx, cypher_query: str, params: Optional[Dict] = {}):\n\t        result = tx.run(cypher_query, params)\n\t        return [r.data() for r in result]\n\t    def query(\n\t        self, cypher_query: str, params: Optional[Dict] = {}\n\t    ) -> List[Dict[str, Any]]:\n", "        with self._driver.session(database=self._database) as session:\n\t            try:\n\t                if self._read_only:\n\t                    result = session.read_transaction(\n\t                        self._execute_read_only_query, cypher_query, params\n\t                    )\n\t                    return result\n\t                else:\n\t                    result = session.run(cypher_query, params)\n\t                    # Limit to at most 10 results\n", "                    return [r.data() for r in result]\n\t            # Catch Cypher syntax errors\n\t            except exceptions.CypherSyntaxError as e:\n\t                return [\n\t                    {\n\t                        \"code\": \"invalid_cypher\",\n\t                        \"message\": f\"Invalid Cypher statement due to an error: {e}\",\n\t                    }\n\t                ]\n\t            except exceptions.ClientError as e:\n", "                # Catch access mode errors\n\t                if e.code == \"Neo.ClientError.Statement.AccessMode\":\n\t                    return [\n\t                        {\n\t                            \"code\": \"error\",\n\t                            \"message\": \"Couldn't execute the query due to the read only access to Neo4j\",\n\t                        }\n\t                    ]\n\t                else:\n\t                    return [{\"code\": \"error\", \"message\": e}]\n", "    def refresh_schema(self) -> None:\n\t        node_props = [el[\"output\"] for el in self.query(node_properties_query)]\n\t        rel_props = [el[\"output\"] for el in self.query(rel_properties_query)]\n\t        rels = [el[\"output\"] for el in self.query(rel_query)]\n\t        schema = schema_text(node_props, rel_props, rels)\n\t        self.schema = schema\n\t        print(schema)\n\t    def check_if_empty(self) -> bool:\n\t        data = self.query(\n\t            \"\"\"\n", "        MATCH (n)\n\t        WITH count(n) as c\n\t        RETURN CASE WHEN c > 0 THEN true ELSE false END AS output\n\t        \"\"\"\n\t        )\n\t        return data[0][\"output\"]\n"]}
{"filename": "api/src/embedding/base_embedding.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tclass BaseEmbedding(ABC):\n\t    \"\"\"\"\"\"\n\t    @abstractmethod\n\t    async def generate(\n\t        self,\n\t        input: str,\n\t    ) -> str:\n\t        \"\"\"Comment\"\"\"\n"]}
{"filename": "api/src/embedding/openai.py", "chunked_list": ["import openai\n\tfrom embedding.base_embedding import BaseEmbedding\n\tclass OpenAIEmbedding(BaseEmbedding):\n\t    \"\"\"Wrapper around OpenAI embedding models.\"\"\"\n\t    def __init__(\n\t        self, openai_api_key: str, model_name: str = \"text-embedding-ada-002\"\n\t    ) -> None:\n\t        openai.api_key = openai_api_key\n\t        self.model = model_name\n\t    def generate(\n", "        self,\n\t        input: str,\n\t    ) -> str:\n\t        embedding = openai.Embedding.create(input=input, model=self.model)\n\t        return embedding[\"data\"][0][\"embedding\"]\n"]}
{"filename": "api/src/embedding/__init__.py", "chunked_list": []}
