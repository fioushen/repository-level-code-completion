{"filename": "setup.py", "chunked_list": ["import re\n\tfrom pathlib import Path\n\timport setuptools\n\tfrom setuptools import find_packages\n\tFILE = Path(__file__).resolve()\n\tPARENT = FILE.parent  # root directory\n\tREADME = (PARENT / \"README.md\").read_text(encoding=\"utf-8\")\n\tdef get_version():\n\t    file = PARENT / \"evaluations/__init__.py\"\n\t    return re.search(\n", "        r'^__version__ = [\\'\"]([^\\'\"]*)[\\'\"]', file.read_text(encoding=\"utf-8\"), re.M\n\t    )[1]\n\tsetuptools.setup(\n\t    name=\"evaluations\",\n\t    version=get_version(),\n\t    author=\"Roboflow, Inc\",\n\t    author_email=\"support@roboflow.com\",\n\t    license=\"MIT\",\n\t    description=\"Evaluate ground truth and model predictions from Roboflow and supported zero-shot models\",\n\t    long_description=README,\n", "    long_description_content_type=\"text/markdown\",\n\t    url=\"https://github.com/roboflow/cvevals\",\n\t    install_requires=[\"numpy>=1.20.0\", \"opencv-python\", \"matplotlib\"],\n\t    packages=find_packages(exclude=(\"tests\",)),\n\t    extras_require={\n\t        \"dev\": [\n\t            \"flake8\",\n\t            \"black==22.3.0\",\n\t            \"isort\",\n\t            \"twine\",\n", "            \"pytest\",\n\t            \"wheel\",\n\t            \"mkdocs-material\",\n\t            \"mkdocstrings[python]\",\n\t        ],\n\t    },\n\t    classifiers=[\n\t        \"Intended Audience :: Developers\",\n\t        \"Intended Audience :: Science/Research\",\n\t        \"License :: OSI Approved :: BSD License\",\n", "        \"Programming Language :: Python :: 3\",\n\t        \"Programming Language :: Python :: 3.7\",\n\t        \"Programming Language :: Python :: 3.8\",\n\t        \"Programming Language :: Python :: 3.9\",\n\t        \"Programming Language :: Python :: 3.10\",\n\t        \"Programming Language :: Python :: 3 :: Only\",\n\t        \"Topic :: Software Development\",\n\t        \"Topic :: Scientific/Engineering\",\n\t        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n\t        \"Topic :: Scientific/Engineering :: Image Recognition\",\n", "    ],\n\t    keywords=\"Roboflow, computer vision, CV, computer vision evaluation\",\n\t    python_requires=\">=3.7\",\n\t)\n"]}
{"filename": "scripts/tighten_bounding_boxes.py", "chunked_list": ["import argparse\n\timport os\n\timport json\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\t# translate above to argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n", "    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n", ")\n\tparser.add_argument(\n\t    \"--increase_bounding_box_size\",\n\t    type=float,\n\t    required=False,\n\t    default=0.1,\n\t    help=\"Increase bounding box size by this percentage\",\n\t)\n\tparser.add_argument(\n\t    \"--sam_checkpoint\",\n", "    type=str,\n\t    required=True,\n\t    help=\"Path to SAM checkpoint\",\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tif args.increase_bounding_box_size > 1.0:\n", "    raise ValueError(\"Increase bounding box size must be less than 1.0\")\n\tclass_names, data, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"object-detection\",\n\t).download_dataset()\n\timages = data.keys()\n\timport cv2\n", "import supervision as sv\n\tfrom segment_anything import SamPredictor, sam_model_registry\n\tfrom evaluations.iou import box_iou\n\tsam = sam_model_registry[\"default\"](\n\t    checkpoint=SAM_CHECKPOINT\n\t)\n\timport numpy as np\n\trecommended_boxes = {}\n\tpredictor = SamPredictor(sam)\n\tfor i in images:\n", "    print(i)\n\t    bboxes = data[i][\"ground_truth\"]\n\t    img = cv2.imread(i)\n\t    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\t    predictor.set_image(img_rgb)\n\t    input_box = bboxes[0]\n\t    annotator = sv.BoxAnnotator()\n\t    input_box = list(map(int, input_box[:4]))\n\t    # cast box as 2d np.ndarray\n\t    box = np.array([input_box])\n", "    box = list(map(int, input_box[:4]))\n\t    # make box bigger\n\t    box[0] = box[0] - int(box[2] * args.increase_bounding_box_size)\n\t    box[1] = box[1] - int(box[3] * args.increase_bounding_box_size)\n\t    box[2] = box[2] + int(box[2] * args.increase_bounding_box_size)\n\t    box[3] = box[3] + int(box[3] * args.increase_bounding_box_size)\n\t    masks, scores, logits = predictor.predict(\n\t        point_coords=None,\n\t        point_labels=None,\n\t        box=np.array(box[:4]),\n", "        multimask_output=False,\n\t    )\n\t    box_annotator = sv.BoxAnnotator(color=sv.Color.red())\n\t    mask_annotator = sv.MaskAnnotator(color=sv.Color.red())\n\t    detections = sv.Detections(xyxy=sv.mask_to_xyxy(masks=masks), mask=masks)\n\t    detections = detections[detections.area == np.max(detections.area)]\n\t    recommended_box = detections.xyxy[0].tolist()\n\t    recommended_boxes[i] = {}\n\t    recommended_boxes[i][\"rec\"] = recommended_box\n\t    recommended_boxes[i][\"gt\"] = input_box\n", "    recommended_boxes[i][\"iou\"] = box_iou(input_box, recommended_box)\n\t    annotator = sv.BoxAnnotator()\n\t    # merge two detections\n\t    detections = sv.Detections(\n\t        xyxy=np.concatenate([np.array([input_box]), detections.xyxy]),\n\t    )\n\t    annotated_image = annotator.annotate(\n\t        img,\n\t        detections=detections,\n\t    )\n", "    sv.plot_image(annotated_image)\n\twith open(\"recommended_boxes.json\", \"w\") as f:\n\t    json.dump(recommended_boxes, f)\n\t# calculate avg iou\n\tious = [v[\"iou\"] for v in recommended_boxes.values()]\n\tprint(np.mean(ious))\n"]}
{"filename": "scripts/cutout.py", "chunked_list": ["import argparse\n\timport csv\n\timport os\n\timport pypandoc\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\timport clip\n\timport cv2\n\timport torch\n\tfrom PIL import Image\n\tparser = argparse.ArgumentParser()\n", "parser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path where your dataset will be saved\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n", "    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\tparser.add_argument(\n\t    \"--generate_pdf\", type=bool, required=False, default=False, help=\"Generate PDF\"\n\t)\n\tparser.add_argument(\n\t    \"--interactive\", type=bool, required=False, default=False, help=\"Show false positives\"\n", ")\n\tparser.add_argument(\n\t    \"--fp_threshold\",\n\t    type=float,\n\t    required=False,\n\t    default=0.7,\n\t    help=\"False positive threshold\",\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n", "ROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tclass_names, data, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"object-detection\",\n\t    dataset=\"train\"\n", ").download_dataset()\n\tmask_vectors = {}\n\tclip_vectors_by_class = {}\n\tbbox_size = []\n\tclass_distribution = {}\n\twhole_image_vectors = []\n\tvectors_by_image = {}\n\timages = data.keys()\n\tdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\tclip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n", "def run_clip_inference(mask: str) -> torch.Tensor:\n\t    image = preprocess(Image.fromarray(mask)).unsqueeze(0).to(device)\n\t    with torch.no_grad():\n\t        image_features = clip_model.encode_image(image)\n\t    image_features /= image_features.norm(dim=-1, keepdim=True)\n\t    return image_features\n\tdef save_report():\n\t    print(\"Computing images with potential false positives. This may take some time, depending on with how many images you are working.\")\n\t    potential_false_positives = []\n\t    for i in data.keys():\n", "        img = cv2.imread(i)\n\t        print(\"Evaluating\", i)\n\t        for j in data[i][\"ground_truth\"]:\n\t            x1 = int(j[0])\n\t            y1 = int(j[1])\n\t            x2 = int(j[2])\n\t            y2 = int(j[3])\n\t            class_name = class_names[int(j[4])]\n\t            if (x1, y1, x2, y2) not in mask_vectors[class_name]:\n\t                continue\n", "            img = cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\t            clip_vector = mask_vectors[class_name][(x1, y1, x2, y2)]\n\t            # show mask\n\t            mask = img[y1:y2, x1:x2]\n\t            # calculate distance between clip vector and class vector\n\t            distance = torch.dist(clip_vector, clip_vectors_by_class[class_name])\n\t            if distance > 0.7:\n\t                print(f\"{i} has a false positive at {x1, y1, x2, y2}\")\n\t                image = cv2.rectangle(img, (x1, y1), (x2, y2), (0, 0, 255), 2)\n\t                cv2.imshow(\"image\", image)\n", "                potential_false_positives.append(i)\n\t                if args.interactive:\n\t                    cv2.imshow(\"mask\", mask)\n\t                    if cv2.waitKey(0) == ord(\"q\"):\n\t                        args.interactive = False\n\t                    cv2.destroyAllWindows()\n\t    potential_false_positives = \"\\n\\n\".join(potential_false_positives)\n\t    report = f\"\"\"\n\t    # Bounding Box Size Report\n\t    ## Stats\n", "    - Dataset Used: {ROBOFLOW_PROJECT_URL}/1\n\t    - Number of Images: {len(images)}\n\t    - Number of Classes: {len(class_names)}\n\t    ## Class Distribution\n\t    {class_distribution_as_str}\n\t    ## Images with Potential False Positives\n\t    {potential_false_positives}\n\t    \"\"\"\n\t    with open(\"report.md\", \"w\") as f:\n\t        f.write(report)\n", "    if args.generate_pdf:\n\t        pypandoc.convert_file(\n\t            \"report.md\",\n\t            \"pdf\",\n\t            outputfile=\"report.pdf\",\n\t            extra_args=[\"-V\", \"geometry:margin=1in\", \"-V\", \"mainfont=Times New Roman\"],\n\t        )\n\t# find duplicate clip vectors\n\tdef find_duplicate_images():\n\t    # show progress bar\n", "    fps = 0\n\t    images = os.listdir(EVAL_DATA_PATH + \"train/images\")\n\t    for i in images:\n\t        i = EVAL_DATA_PATH + \"train/images/\" + i\n\t        print(\"Comparing\", i)\n\t        if vectors_by_image.get(i) is None:\n\t            vectors_by_image[i] = run_clip_inference(cv2.imread(i))\n\t        for j in vectors_by_image.keys():\n\t            if i != j:\n\t                distance = torch.dist(vectors_by_image[i], vectors_by_image[j])\n", "                if distance < 0.1:\n\t                    print(f\"{i} and {j} are duplicates\")\n\tfor i in data.keys():\n\t    img = cv2.imread(i)\n\t    print(\"Computing ground truth vectors for\", i)\n\t    for j in data[i][\"ground_truth\"]:\n\t        x1 = int(j[0])\n\t        y1 = int(j[1])\n\t        x2 = int(j[2])\n\t        y2 = int(j[3])\n", "        class_name = class_names[int(j[4])]\n\t        if x1 == x2 or y1 == y2:\n\t            print(f\"{i} has a 0x0 bounding box\")\n\t            continue\n\t        mask = img[y1:y2, x1:x2]\n\t        if class_name not in clip_vectors_by_class:\n\t            clip_vectors_by_class[class_name] = []\n\t        vector = run_clip_inference(mask)\n\t        class_distribution[class_name] = class_distribution.get(class_name, 0) + 1\n\t        if mask_vectors.get(class_name) is None:\n", "            mask_vectors[class_name] = {}\n\t        mask_vectors[class_name][(x1, y1, x2, y2)] = vector\n\t        clip_vectors_by_class[class_name].append(vector)\n\t        bbox_size.append(mask.shape[0] * mask.shape[1])\n\t        whole_image_clip = run_clip_inference(img)\n\t        whole_image_vectors.append(whole_image_clip)\n\t        vectors_by_image[i] = whole_image_clip\n\tprint(\"Computing mean class vectors\")\n\tfor i in clip_vectors_by_class.keys():\n\t    clip_vectors_by_class[i] = torch.mean(torch.stack(clip_vectors_by_class[i]), dim=0)\n", "whole_image_vectors = torch.mean(torch.stack(whole_image_vectors), dim=0)\n\tavg_frame_size = sum(bbox_size) / len(bbox_size)\n\tmega_pixels = round(avg_frame_size / 1000000, 2)\n\tclass_distribution_as_str = \"\"\"\n\t| Class Name | Count |\n\t| ---------- | ----- |\n\t{}\"\"\".format(\n\t    \"\\n\".join(\n\t        [\n\t            \"| {} | {} |\".format(class_name, count)\n", "            for class_name, count in class_distribution.items()\n\t        ]\n\t    )\n\t)\n\tif __name__ == \"__main__\":\n\t    # find_duplicate_images()\n\t    print(\"Saving report\")\n\t    save_report()\n"]}
{"filename": "scripts/bbox_sizes.py", "chunked_list": ["import cv2\n\tdef find_small_bounding_boxes(data):\n\t    for img in data.keys():\n\t        for j in data[img][\"ground_truth\"]:\n\t            x1 = int(j[0])\n\t            y1 = int(j[1])\n\t            x2 = int(j[2])\n\t            y2 = int(j[3])\n\t            if x1 == x2 or y1 == y2:\n\t                print(f\"{img} has a 0x0 bounding box\")\n", "                continue\n\t            if x2 - x1 < 10 or y2 - y1 < 10:\n\t                print(f\"{img} has a small bounding box\")\n\tdef find_large_bounding_boxes(data):\n\t    for img in data.keys():\n\t        width, height = cv2.imread(img).shape[:2]\n\t        for j in data[img][\"ground_truth\"]:\n\t            x1 = int(j[0])\n\t            y1 = int(j[1])\n\t            x2 = int(j[2])\n", "            y2 = int(j[3])\n\t            if x1 < 0 or y1 < 0 or x2 > width or y2 > height:\n\t                print(f\"{img} has a large bounding box\")\n"]}
{"filename": "examples/dino_example.py", "chunked_list": ["import argparse\n\timport os\n\timport cv2\n\tfrom groundingdino.util.inference import Model\n\tfrom evaluations import Evaluator\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n", "    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n", "    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\tparser.add_argument(\n\t    \"--config_path\", type=str, required=True, help=\"Path to GroundingDINO config\"\n\t)\n\tparser.add_argument(\n\t    \"--weights_path\", type=str, required=True, help=\"Path to GroundingDINO weights\"\n\t)\n\tparser.add_argument(\"--box_threshold\", type=float, required=False, help=\"Box threshold\")\n\tparser.add_argument(\n", "    \"--text_threshold\", type=float, required=False, help=\"Text threshold\"\n\t)\n\targs = parser.parse_args()\n\tif not args.box_threshold:\n\t    BOX_THRESHOLD = 0.35\n\tif not args.text_threshold:\n\t    TEXT_THRESHOLD = 0.25\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n", "ROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tCONFIG_PATH = args.config_path\n\tWEIGHTS_PATH = args.weights_path\n\tIMAGE_PATH = EVAL_DATA_PATH + \"/images/valid\"\n\tclass_names, data, _ = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t).download_dataset()\n", "model = Model(\n\t    model_config_path=CONFIG_PATH, model_checkpoint_path=WEIGHTS_PATH, device=\"cpu\"\n\t)\n\tall_predictions = {}\n\tfor file in os.listdir(IMAGE_PATH)[:2]:\n\t    file = os.path.join(IMAGE_PATH, file)\n\t    if file.endswith(\".jpg\"):\n\t        image = cv2.imread(file)\n\t        detections = model.predict_with_classes(\n\t            image=image,\n", "            classes=class_names,\n\t            box_threshold=BOX_THRESHOLD,\n\t            text_threshold=TEXT_THRESHOLD,\n\t        )\n\t        all_predictions[file] = {\"filename\": file, \"predictions\": detections}\n\tevaluator = Evaluator(\n\t    ground_truth=data,\n\t    predictions=all_predictions,\n\t    class_names=class_names,\n\t    confidence_threshold=0.2,\n", "    mode=\"batch\",\n\t)\n\tcf = evaluator.eval_model_predictions()\n\tdata = evaluator.calculate_statistics()\n\tprint(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/roboflow_example.py", "chunked_list": ["import argparse\n\timport os\n\tfrom evaluations.dataloaders import (RoboflowDataLoader,\n\t                                     RoboflowPredictionsDataLoader)\n\tfrom evaluations.roboflow import RoboflowEvaluator\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n", "    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n", ")\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tclass_names, data, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n", "    image_files=EVAL_DATA_PATH,\n\t).download_dataset()\n\tpredictions = RoboflowPredictionsDataLoader(\n\t    model=model,\n\t    model_type=\"object-detection\",\n\t    image_files=EVAL_DATA_PATH,\n\t    class_names=class_names,\n\t).process_files()\n\tevaluator = RoboflowEvaluator(\n\t    ground_truth=data, predictions=predictions, class_names=class_names, mode=\"batch\"\n", ")\n\tcf = evaluator.eval_model_predictions()\n\tprint(cf)\n\tdata = evaluator.calculate_statistics()\n\tprint(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/compare_roboflow_models.py", "chunked_list": ["import os\n\timport pypandoc\n\tfrom evaluations import CompareEvaluations\n\tfrom evaluations.confusion_matrices import plot_confusion_matrix\n\tfrom evaluations.dataloaders import (RoboflowDataLoader,\n\t                                     RoboflowPredictionsDataLoader)\n\tfrom evaluations.roboflow import RoboflowEvaluator\n\tMODEL_TYPE = \"object-detection\"\n\tmodels = [\n\t    {\n", "        \"workspace_url\": \"\",\n\t        \"project_url\": \"\",\n\t        \"model_version\": 8,\n\t    },\n\t]\n\tevals = []\n\tdatasets = []\n\tfor m in models:\n\t    loader = RoboflowDataLoader(\n\t        workspace_url=m[\"workspace_url\"],\n", "        project_url=m[\"project_url\"],\n\t        project_version=m[\"model_version\"],\n\t        image_files=m[\"project_url\"],\n\t        model_type=MODEL_TYPE,\n\t    )\n\t    class_names, data, model = loader.download_dataset()\n\t    predictions = RoboflowPredictionsDataLoader(\n\t        model=model,\n\t        model_type=\"object-detection\",\n\t        image_files=m[\"project_url\"],\n", "        class_names=class_names,\n\t    ).process_files()\n\t    long_name = f\"{m['workspace_url']}/{m['project_url']}/{m['model_version']}\"\n\t    evals.append(\n\t        RoboflowEvaluator(\n\t            ground_truth=data,\n\t            predictions=predictions,\n\t            class_names=class_names,\n\t            mode=\"batch\",\n\t            name=long_name,\n", "            model_type=MODEL_TYPE,\n\t        )\n\t    )\n\t    print(loader.dataset_content)\n\t    datasets.append(loader.dataset_content)\n\tbest = CompareEvaluations(evals)\n\tfull_path = os.path.join(os.getcwd(), \"output\", \"matrices\")\n\tbest, evaluations = best.compare()\n\tplot_confusion_matrix(\n\t    best[4], class_names=best[3], file_name=f\"Best Confusion Matrix\", mode=\"return\"\n", ")\n\tbest_cf = (\n\t    f\"![Confusion Matrix]({full_path}/Best Confusion Matrix.png)\" + \"{ width=400px }\"\n\t)\n\tcfs = [\n\t    f\"![Confusion Matrix]({full_path}/{c[6].replace('/', '_')}.png)\" + \"{ width=400px }\"\n\t    for c in evaluations\n\t]\n\tcfs = \"\\n\\n\".join(cfs)\n\tdataset_meta = \"\"\n", "for d in datasets:\n\t    preprocessing = d.preprocessing\n\t    augmentations = d.augmentation\n\t    # convert to str\n\t    augmentations = str(augmentations)\n\t    preprocessing = str(preprocessing)\n\t    splits = d.splits\n\t    dataset_meta += f\"### {d.name} (Version {d.version})\\n\\n\"\n\t    dataset_meta += f\"#### Preprocessing\\n\\n\"\n\t    dataset_meta += f\"> ```python\\n{preprocessing}\\n```\\n\\n\"\n", "    dataset_meta += f\"\\n\\n#### Augmentations\\n\\n\"\n\t    dataset_meta += f\"> ```python\\n{augmentations}\\n```\\n\\n\"\n\t    dataset_meta += f\"\\n\\n#### Splits\\n\\n\"\n\t    dataset_meta += \"Valid: \" + str(splits[\"valid\"]) + \"\\n\\n\"\n\t    dataset_meta += \"Train: \" + str(splits[\"train\"]) + \"\\n\\n\"\n\t    dataset_meta += \"Test: \" + str(splits[\"test\"]) + \"\\n\\n\"\n\tmodel_table_data = \"\"\n\tfor c in evaluations:\n\t    plot_confusion_matrix(\n\t        c[4],\n", "        file_name=f\"{c[6].replace('/', '_')}\",\n\t        class_names=c[3],\n\t        mode=\"return\",\n\t    )\n\t    model_table_data += f\"| {c[6]} | {round(c[0], 2)} | {round(c[1], 2)} | {round(c[2], 2)} | {c[5]} |\\n\"\n\treport = f\"\"\"\n\t# Roboflow Model Comparison\n\tTotal evaluations run: {len(evaluations)}\n\t## Models\n\t| Model | F1 Score | Precision | Recall | Confidence Threshold |\n", "| --- | --- | --- | --- | --- |\n\t{model_table_data}\n\t## Datasets\n\t{dataset_meta}\n\t## Best Model (measured by f1 score)\n\t{best_cf}\n\t## Confusion Matrices by Model\n\t{cfs}\n\t\"\"\"\n\twith open(\"roboflow_model_comparison.md\", \"w\") as f:\n", "    f.write(report)\n\tpypandoc.convert_file(\n\t    \"roboflow_model_comparison.md\",\n\t    \"pdf\",\n\t    outputfile=\"roboflow_model_comparison.pdf\",\n\t    extra_args=[\"--extract-media\", \".\", \"-V\", \"geometry:margin=1in\"],\n\t)\n"]}
{"filename": "examples/clip_blip_albef_compare.py", "chunked_list": ["import argparse\n\timport torch\n\tfrom models.blip import run_blip_albef_inference\n\tfrom models.clip import run_clip_inference\n\tfrom evaluations.classification import ClassificationEvaluator\n\tfrom evaluations.compare import CompareEvaluations\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\tfrom evaluations.dataloaders.classification import \\\n\t    ClassificationFolderDataLoader\n\tdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "parser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n", "parser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n", "ROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tIMAGE_PATH = EVAL_DATA_PATH + \"/valid\"\n\tclass_names, ground_truth, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"classification\",\n\t).download_dataset()\n\t# dedupe class names\n", "class_names = list(set(class_names))\n\tall_clip_predictions = {}\n\tfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n\t    # print(file)\n\t    all_clip_predictions.update(run_clip_inference(file, class_names))\n\tall_blip_predictions = {}\n\tfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n\t    all_blip_predictions.update(\n\t        run_blip_albef_inference(file, class_names, \"blip\", device)\n\t    )\n", "all_albef_predictions = {}\n\tfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n\t    all_albef_predictions.update(\n\t        run_blip_albef_inference(file, class_names, \"albef\", device)\n\t    )\n\tevals = [\n\t    {\"name\": \"clip\", \"predictions\": all_clip_predictions},\n\t    {\"name\": \"blip\", \"predictions\": all_blip_predictions},\n\t    {\"name\": \"albef\", \"predictions\": all_albef_predictions},\n\t]\n", "best = CompareEvaluations(\n\t    [\n\t        ClassificationEvaluator(\n\t            predictions=cn[\"predictions\"],\n\t            ground_truth=ground_truth,\n\t            class_names=class_names,\n\t            mode=\"batch\",\n\t            name=cn[\"name\"],\n\t        )\n\t    ]\n", "    for cn in evals\n\t)\n\tcf = best.compare()\n\tprint(cf)\n"]}
{"filename": "examples/google_cloud_vision_example.py", "chunked_list": ["import argparse\n\timport base64\n\timport glob\n\timport os\n\tfrom typing import Sequence\n\timport cv2\n\timport numpy as np\n\timport supervision as sv\n\tfrom google.cloud import vision\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n", "from evaluations.roboflow import RoboflowEvaluator\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n", ")\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n", "ROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tVALIDATION_SET_PATH = EVAL_DATA_PATH + \"/valid/images/*.jpg\"\n\t# map class names to class ids\n\tclass_mappings = {\n\t    \"Mug\": 0,\n\t}\n\tdef analyze_image_from_uri(\n\t    image_uri: str,\n\t    feature_types: Sequence,\n", ") -> vision.AnnotateImageResponse:\n\t    client = vision.ImageAnnotatorClient()\n\t    image = vision.Image()\n\t    loaded_image = cv2.imread(image_uri)\n\t    base64_image = base64.b64encode(open(image_uri, \"rb\").read())\n\t    image.source.image_uri = \"base64://\" + base64_image.decode()\n\t    features = [vision.Feature(type_=feature_type) for feature_type in feature_types]\n\t    request = {\n\t        \"image\": {\"content\": base64_image.decode()},\n\t        \"features\": features,\n", "    }\n\t    response = client.annotate_image(request)\n\t    localized_object_annotations = response.localized_object_annotations\n\t    localized_object_annotations = [\n\t        annotation\n\t        for annotation in localized_object_annotations\n\t        if annotation.name in class_mappings.keys()\n\t    ]\n\t    if len(localized_object_annotations) == 0:\n\t        return sv.detection.core.Detections(\n", "            xyxy=np.array([[0, 0, 0, 0]]),\n\t            class_id=np.array([None]),\n\t            confidence=np.array([100]),\n\t        )\n\t    xyxys = []\n\t    image_size = loaded_image.shape\n\t    for obj in localized_object_annotations:\n\t        # scale up to full image size\n\t        x0 = obj.bounding_poly.normalized_vertices[0].x * image_size[1]\n\t        y0 = obj.bounding_poly.normalized_vertices[0].y * image_size[0]\n", "        x1 = obj.bounding_poly.normalized_vertices[2].x * image_size[1]\n\t        y1 = obj.bounding_poly.normalized_vertices[2].y * image_size[0]\n\t        xyxys.append([x0, y0, x1, y1])\n\t    annotations = sv.detection.core.Detections(\n\t        xyxy=np.array(xyxys).astype(np.float32),\n\t        class_id=np.array(\n\t            [class_mappings[obj.name] for obj in localized_object_annotations]\n\t        ),\n\t        confidence=np.array([obj.score for obj in localized_object_annotations]),\n\t    )\n", "    return annotations\n\tclass_names, data, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t).download_dataset()\n\tall_predictions = {}\n\tfor file in glob.glob(VALIDATION_SET_PATH):\n\t    # add base dir\n", "    current_dir = os.getcwd()\n\t    response = analyze_image_from_uri(file, [vision.Feature.Type.OBJECT_LOCALIZATION])\n\t    all_predictions[file] = {\"filename\": file, \"predictions\": response}\n\tevaluator = RoboflowEvaluator(\n\t    ground_truth=data,\n\t    predictions=all_predictions,\n\t    class_names=class_names,\n\t    mode=\"batch\",\n\t)\n\tcf = evaluator.eval_model_predictions()\n", "data = evaluator.calculate_statistics()\n\tprint(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/blip_albef_compare_example.py", "chunked_list": ["import argparse\n\timport torch\n\tfrom models.blip import run_blip_albef_inference\n\tfrom evaluations.classification import ClassificationEvaluator\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\tfrom evaluations.dataloaders.classification import \\\n\t    ClassificationFolderDataLoader\n\tdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n", "    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n", ")\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\tparser.add_argument(\n\t    \"--feature_extractor\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Feature extractor to use ('blip' or 'albef')\",\n\t)\n", "args = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tFEATURE_EXTRACTOR = args.feature_extractor\n\tif FEATURE_EXTRACTOR not in (\"blip\", \"albef\"):\n\t    raise ValueError(\"feature_extractor must be either 'blip' or 'albef'\")\n\t# use validation set\n\tIMAGE_PATH = EVAL_DATA_PATH + \"/images/valid\"\n", "class_names, ground_truth, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"classification\",\n\t).download_dataset()\n\tall_predictions = {}\n\tfor file in ClassificationFolderDataLoader(IMAGE_PATH).get_files()[1]:\n\t    all_predictions.update(\n", "        run_blip_albef_inference(file, class_names, FEATURE_EXTRACTOR, device)\n\t    )\n\tevaluator = ClassificationEvaluator(\n\t    ground_truth=ground_truth,\n\t    predictions=all_predictions,\n\t    class_names=[\"banana\", \"apple\"],\n\t    mode=\"batch\",\n\t)\n\tcf = evaluator.eval_model_predictions()\n\tdata = evaluator.calculate_statistics()\n", "print(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/imagebind_example.py", "chunked_list": ["import data\n\timport torch\n\tfrom models import imagebind_model\n\tfrom models.imagebind_model import ModalityType\n\timport argparse\n\timport os\n\timport clip\n\timport torch\n\tfrom PIL import Image\n\tfrom evaluations.classification import ClassificationEvaluator\n", "from evaluations.dataloaders import RoboflowDataLoader\n\tfrom evaluations.dataloaders.classification import ClassificationDetections\n\tdevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n\tmodel = imagebind_model.imagebind_huge(pretrained=True)\n\tmodel.eval()\n\tmodel.to(device)\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n", "    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n", "    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\t# use validation set\n\tIMAGE_PATH = EVAL_DATA_PATH + \"/valid\"\n\tclass_names, ground_truth, model = RoboflowDataLoader(\n", "    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"multiclass\",\n\t).download_dataset()\n\tall_predictions = {}\n\tdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\tclip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\tdef run_imagebind_inference(class_names: str, img_filename: str) -> str:\n", "    \"\"\"\n\t    Run inference on an image using ImageBind.\n\t    Args:\n\t        class_names: The class names to use for inference.\n\t        img_filename: The filename of the image on which to run inference.\n\t    Returns:\n\t        The predicted class name.\n\t    \"\"\"\n\t    image_paths=[\".assets/dog_image.jpg\"]\n\t    # Load data\n", "    inputs = {\n\t        ModalityType.TEXT: data.load_and_transform_text(class_names, device),\n\t        ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n\t    }\n\t    with torch.no_grad():\n\t        embeddings = model(inputs)\n\t    text_result = torch.softmax(embeddings[ModalityType.VISION] @ embeddings[ModalityType.TEXT].T, dim=-1)\n\t    top_k = torch.topk(text_result, k=1, dim=-1)\n\t    confidence = top_k.values[0][0].item()\n\t    return class_names[top_k.indices[0][0]], confidence\n", "for file in ground_truth.keys():\n\t    # add base dir\n\t    current_dir = os.getcwd()\n\t    file = os.path.join(current_dir, file)\n\t    # run inference\n\t    class_name, confidence = run_imagebind_inference(class_names, file)\n\t    result = ClassificationDetections(\n\t        image_id=file,\n\t        predicted_class_names=[class_name],\n\t        predicted_class_ids=[class_names.index(class_name)],\n", "        confidence=[1.0, confidence],\n\t    )\n\t    all_predictions[file] = {\"filename\": file, \"predictions\": result}\n\tevaluator = ClassificationEvaluator(\n\t    ground_truth=ground_truth,\n\t    predictions=all_predictions,\n\t    class_names=class_names,\n\t    mode=\"batch\",\n\t    model_type=\"multiclass\",\n\t)\n", "cf = evaluator.eval_model_predictions()\n\tdata = evaluator.calculate_statistics()\n\tprint(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/azure_example.py", "chunked_list": ["import argparse\n\timport glob\n\timport os\n\timport cv2\n\timport numpy as np\n\timport requests\n\timport supervision as sv\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\tfrom evaluations.roboflow import RoboflowEvaluator\n\tparser = argparse.ArgumentParser()\n", "parser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n", "    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\tparser.add_argument(\"--azure_endpoint\", type=str, required=True, help=\"Azure endpoint\")\n\tparser.add_argument(\"--azure_api_key\", type=str, required=True, help=\"Azure API key\")\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n", "ROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tAZURE_ENDPOINT = args.azure_endpoint\n\tAZURE_API_KEY = args.azure_api_key\n\tVALIDATION_SET_PATH = EVAL_DATA_PATH + \"/valid/images/*.jpg\"\n\t# map class names to class ids\n\tclass_mappings = {\n\t    \"cup\": 0,\n\t}\n\tdef run_inference_azure(image_filename):\n", "    with open(image_filename, \"rb\") as image_file:\n\t        data = image_file.read()\n\t    image = cv2.imread(image_filename)\n\t    headers = {\n\t        \"Content-Type\": \"application/octet-stream\",\n\t        \"Ocp-Apim-Subscription-Key\": AZURE_API_KEY,\n\t    }\n\t    response = requests.request(\"POST\", AZURE_ENDPOINT, headers=headers, data=data)\n\t    predictions = response.json()\n\t    if \"error\" in predictions.keys():\n", "        print(predictions[\"error\"][\"message\"])\n\t        return sv.detection.core.Detections(\n\t            xyxy=np.array([[0, 0, 0, 0]]),\n\t            class_id=np.array([None]),\n\t            confidence=np.array([100]),\n\t        )\n\t    predictions = predictions[\"objects\"]\n\t    # turn predictions into xyxys\n\t    xyxys = []\n\t    predictions = [\n", "        prediction\n\t        for prediction in predictions\n\t        if prediction[\"object\"] in class_mappings.keys()\n\t    ]\n\t    if len(predictions) == 0:\n\t        return sv.detection.core.Detections(\n\t            xyxy=np.array([[0, 0, 0, 0]]),\n\t            class_id=np.array([None]),\n\t            confidence=np.array([100]),\n\t        )\n", "    for obj in predictions:\n\t        # scale up to full image size\n\t        x0 = obj[\"rectangle\"][\"x\"] * image.shape[1]\n\t        y0 = obj[\"rectangle\"][\"y\"] * image.shape[0]\n\t        x1 = (obj[\"rectangle\"][\"x\"] + obj[\"rectangle\"][\"w\"]) * image.shape[1]\n\t        y1 = (obj[\"rectangle\"][\"y\"] + obj[\"rectangle\"][\"h\"]) * image.shape[0]\n\t        xyxys.append([x0, y0, x1, y1])\n\t    annotations = sv.detection.core.Detections(\n\t        xyxy=np.array(xyxys).astype(np.float32),\n\t        class_id=np.array([class_mappings[obj[\"object\"]] for obj in predictions]),\n", "        confidence=np.array([obj[\"confidence\"] for obj in predictions]),\n\t    )\n\t    return annotations\n\tclass_names, data, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t).download_dataset()\n\tall_predictions = {}\n", "for file in glob.glob(VALIDATION_SET_PATH):\n\t    response = run_inference_azure(file)\n\t    all_predictions[file] = {\"filename\": file, \"predictions\": response}\n\tprint(all_predictions)\n\tprint(data)\n\tevaluator = RoboflowEvaluator(\n\t    ground_truth=data,\n\t    predictions=all_predictions,\n\t    class_names=class_names,\n\t    mode=\"batch\",\n", ")\n\tcf = evaluator.eval_model_predictions()\n\tdata = evaluator.calculate_statistics()\n\tprint(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/segmentation.py", "chunked_list": ["import argparse\n\timport os\n\timport supervision as sv\n\tfrom evaluations.dataloaders import (RoboflowDataLoader,\n\t                                     RoboflowPredictionsDataLoader)\n\tfrom evaluations.segmentation import SegmentationEvaluator\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n", "    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n", "    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tclass_names, _, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n", "    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"segmentation\",\n\t).download_dataset()\n\tds = sv.DetectionDataset.from_yolo(\n\t    images_directory_path=os.path.join(EVAL_DATA_PATH, \"test/images\"),\n\t    annotations_directory_path=os.path.join(EVAL_DATA_PATH, \"test/labels\"),\n\t    data_yaml_path=os.path.join(EVAL_DATA_PATH, \"data.yaml\"),\n\t)\n\tpredictions = RoboflowPredictionsDataLoader(\n", "    model=model,\n\t    model_type=\"segmentation\",\n\t    image_files=EVAL_DATA_PATH,\n\t    class_names=class_names,\n\t).process_files()\n\t# add EVAL_DATA_PATH to predictions\n\tevaluator = SegmentationEvaluator(\n\t    ground_truth=ds, predictions=predictions, class_names=class_names, eval_data_path = EVAL_DATA_PATH\n\t)\n\tcf = evaluator.eval_model_predictions()\n", "from evaluations.confusion_matrices import plot_confusion_matrix\n\tplot_confusion_matrix(\n\t    cf,\n\t    class_names=class_names,\n\t    aggregate=True,\n\t)\n\tdata = evaluator.calculate_statistics()\n\tprint(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/clip_compare_example.py", "chunked_list": ["import argparse\n\timport os\n\tfrom models.clip import run_clip_inference\n\tfrom evaluations.classification import ClassificationEvaluator\n\tfrom evaluations.compare import CompareEvaluations\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\tfrom evaluations.dataloaders.classification import \\\n\t    ClassificationFolderDataLoader\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n", "    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n", ")\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tIMAGE_PATH = EVAL_DATA_PATH + \"/images/valid\"\n", "class_names, ground_truth, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"classification\",\n\t).download_dataset()\n\tevals = [\n\t    {\"classes\": [\"orange\", \"background\"], \"confidence\": 1},\n\t    {\"classes\": [\"pear\", \"background\"], \"confidence\": 0.9},\n", "]\n\tevaluations = []\n\tfor cn in evals:\n\t    all_predictions = {}\n\t    for file in ClassificationFolderDataLoader(IMAGE_PATH).get_files():\n\t        all_predictions.update(run_clip_inference(file, class_names))\n\t        evaluations.append(\n\t            ClassificationEvaluator(\n\t                predictions=all_predictions,\n\t                ground_truth=ground_truth,\n", "                confidence_threshold=cn[\"confidence\"],\n\t                class_names=class_names + cn[\"classes\"],\n\t                mode=\"batch\",\n\t            )\n\t        )\n\tbest = CompareEvaluations(evaluations)\n\tcf = best.compare()\n\tprint(cf)\n"]}
{"filename": "examples/dinov2_example.py", "chunked_list": ["import argparse\n\timport os\n\timport clip\n\tfrom evaluations import CompareEvaluations\n\tfrom evaluations.classification import ClassificationEvaluator\n\tfrom evaluations.confusion_matrices import plot_confusion_matrix\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\timport models.clip as clip\n\timport models.dinov2 as dinov2\n\tparser = argparse.ArgumentParser()\n", "parser.add_argument(\n\t    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to Classification dataset (if you don't have one, this is the path in which your Roboflow dataset will be saved)\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n", "    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n", "IMAGE_PATH = EVAL_DATA_PATH + \"/test\"\n\tclass_names, ground_truth, _ = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"classification\",\n\t).download_dataset()\n\tlabels = {}\n\tdinov2_predictions = {}\n", "clip_predictions = {}\n\tfor folder in os.listdir(IMAGE_PATH):\n\t    for file in os.listdir(os.path.join(IMAGE_PATH, folder)):\n\t        if file.endswith(\".jpg\"):\n\t            full_name = os.path.join(IMAGE_PATH, folder, file)\n\t            labels[full_name] = folder\n\tfiles = labels.keys()\n\tmodel = dinov2.train_dinov2_svm_model(IMAGE_PATH)\n\tprint(\n\t    \"DINOv2 Model Trained. Starting inference (this may take a while depending on how many images you are using).\"\n", ")\n\tall_predictions = {}\n\tfor file in list(ground_truth.keys())[:3]:\n\t    print(\"Running inference on\", file)\n\t    dinov2_result = dinov2.run_dinov2_inference(model, file, class_names)\n\t    clip_result = clip.run_clip_inference(file, class_names)\n\t    dinov2_predictions[file] = dinov2_result\n\t    clip_predictions[file] = clip_result\n\tprint(\"Running comparison.\")\n\tbest = CompareEvaluations(\n", "    [\n\t        ClassificationEvaluator(\n\t            ground_truth=ground_truth,\n\t            predictions=dinov2_predictions,\n\t            class_names=class_names,\n\t            mode=\"batch\",\n\t            model_type=\"multiclass\",\n\t            name=\"DINOv2\",\n\t        ),\n\t        ClassificationEvaluator(\n", "            ground_truth=ground_truth,\n\t            predictions=clip_predictions,\n\t            class_names=class_names,\n\t            mode=\"batch\",\n\t            model_type=\"multiclass\",\n\t            name=\"CLIP\",\n\t        ),\n\t    ]\n\t)\n\thighest_eval, comparison = best.compare()\n", "plot_confusion_matrix(highest_eval[4], class_names)\n\tprint(\"F1 Score:\", highest_eval[0])\n\tprint(\"Precision:\", highest_eval[1])\n\tprint(\"Recall:\", highest_eval[2])\n\tprint(\"Class Names:\", highest_eval[3])\n\tprint(\"Confidence Threshold:\", highest_eval[5])\n"]}
{"filename": "examples/clip_example.py", "chunked_list": ["import argparse\n\timport os\n\timport clip\n\timport torch\n\tfrom PIL import Image\n\tfrom evaluations.classification import ClassificationEvaluator\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\tfrom evaluations.dataloaders.classification import ClassificationDetections\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n", "    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n", ")\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\targs = parser.parse_args()\n\tEVAL_DATA_PATH = args.eval_data_path\n\tROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\t# use validation set\n", "IMAGE_PATH = EVAL_DATA_PATH + \"/valid\"\n\tclass_names, ground_truth, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n\t    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"multiclass\",\n\t).download_dataset()\n\tall_predictions = {}\n\tdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n", "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\tfor file in ground_truth.keys():\n\t    # add base dir\n\t    current_dir = os.getcwd()\n\t    file = os.path.join(current_dir, file)\n\t    image = preprocess(Image.open(file)).unsqueeze(0).to(device)\n\t    text = clip.tokenize(class_names).to(device)\n\t    with torch.no_grad():\n\t        image_features = clip_model.encode_image(image)\n\t        text_features = clip_model.encode_text(text)\n", "    image_features /= image_features.norm(dim=-1, keepdim=True)\n\t    text_features /= text_features.norm(dim=-1, keepdim=True)\n\t    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\t    values, indices = similarity[0].topk(1)\n\t    data = ClassificationDetections(\n\t        image_id=file,\n\t        predicted_class_names=[class_names[indices[0]]],\n\t        predicted_class_ids=[indices[0]],\n\t        confidence=values[0],\n\t    )\n", "    all_predictions[file] = {\"filename\": file, \"predictions\": data}\n\tevaluator = ClassificationEvaluator(\n\t    ground_truth=ground_truth,\n\t    predictions=all_predictions,\n\t    class_names=class_names,\n\t    mode=\"batch\",\n\t    model_type=\"multiclass\",\n\t)\n\tcf = evaluator.eval_model_predictions()\n\tdata = evaluator.calculate_statistics()\n", "print(\"Precision:\", data.precision)\n\tprint(\"Recall:\", data.recall)\n\tprint(\"f1 Score:\", data.f1)\n"]}
{"filename": "examples/dino_compare_example.py", "chunked_list": ["import argparse\n\timport os\n\timport numpy as np\n\timport cv2\n\tfrom groundingdino.util.inference import Model\n\timport supervision as sv\n\tfrom evaluations import CompareEvaluations, Evaluator\n\tfrom evaluations.dataloaders import RoboflowDataLoader\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n", "    \"--eval_data_path\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to YOLOv5 PyTorch TXT or Classification dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_workspace_url\", type=str, required=True, help=\"Roboflow workspace ID\"\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_project_url\", type=str, required=True, help=\"Roboflow project ID\"\n", ")\n\tparser.add_argument(\n\t    \"--roboflow_model_version\", type=int, required=True, help=\"Roboflow model version\"\n\t)\n\tparser.add_argument(\n\t    \"--config_path\", type=str, required=True, help=\"Path to GroundingDINO config\"\n\t)\n\tparser.add_argument(\n\t    \"--weights_path\", type=str, required=True, help=\"Path to GroundingDINO weights\"\n\t)\n", "parser.add_argument(\"--box_threshold\", type=float, required=False, help=\"Box threshold\")\n\tparser.add_argument(\n\t    \"--text_threshold\", type=float, required=False, help=\"Text threshold\"\n\t)\n\targs = parser.parse_args()\n\tif not args.box_threshold:\n\t    BOX_THRESHOLD = 0.35\n\tif not args.text_threshold:\n\t    TEXT_THRESHOLD = 0.25\n\tEVAL_DATA_PATH = args.eval_data_path\n", "ROBOFLOW_WORKSPACE_URL = args.roboflow_workspace_url\n\tROBOFLOW_PROJECT_URL = args.roboflow_project_url\n\tROBOFLOW_MODEL_VERSION = args.roboflow_model_version\n\tCONFIG_PATH = args.config_path\n\tWEIGHTS_PATH = args.weights_path\n\t# use validation set\n\tIMAGE_PATH = EVAL_DATA_PATH + \"/train/images\"\n\tclass_names, ground_truth, model = RoboflowDataLoader(\n\t    workspace_url=ROBOFLOW_WORKSPACE_URL,\n\t    project_url=ROBOFLOW_PROJECT_URL,\n", "    project_version=ROBOFLOW_MODEL_VERSION,\n\t    image_files=EVAL_DATA_PATH,\n\t    model_type=\"object-detection\",\n\t    dataset=\"train\",\n\t).download_dataset()\n\tground_truth = sv.DetectionDataset.from_yolo(\n\t    images_directory_path=os.path.join(EVAL_DATA_PATH, \"train/images\"),\n\t    annotations_directory_path=os.path.join(EVAL_DATA_PATH, \"train/labels\"),\n\t    data_yaml_path=os.path.join(EVAL_DATA_PATH, \"data.yaml\"),\n\t)\n", "model = Model(\n\t    model_config_path=CONFIG_PATH, model_checkpoint_path=WEIGHTS_PATH, device=\"cpu\"\n\t)\n\tdef get_dino_predictions(class_names: dict) -> dict:\n\t    all_predictions = {}\n\t    class_names = [pred[\"inference\"] for pred in class_names]\n\t    for file in os.listdir(IMAGE_PATH):\n\t        file = os.path.join(IMAGE_PATH, file)\n\t        if file.endswith(\".jpg\"):\n\t            image = cv2.imread(file)\n", "            detections = model.predict_with_classes(\n\t                image=image,\n\t                classes=class_names,\n\t                box_threshold=BOX_THRESHOLD,\n\t                text_threshold=TEXT_THRESHOLD,\n\t            )\n\t            annotations = sv.detection.core.Detections(\n\t                xyxy=np.array(detections.xyxy).astype(np.float32),\n\t                class_id=np.array(detections.class_id),\n\t                confidence=np.array(detections.confidence)\n", "            )\n\t            all_predictions[file] = {\"filename\": file, \"predictions\": annotations}\n\t    return all_predictions\n\tevals = [\n\t    {\"classes\": [{\"ground_truth\": \"trash\", \"inference\": \"trash\"}], \"confidence\": 0.5},\n\t    {\"classes\": [{\"ground_truth\": \"trash\", \"inference\": \"rubbish\"}], \"confidence\": 0.5},\n\t    {\"classes\": [{\"ground_truth\": \"trash\", \"inference\": \"waste\"}], \"confidence\": 0.5},\n\t]\n\t# map eval inferences to ground truth\n\tfor e in evals:\n", "    evals[evals.index(e)][\"ground_truth\"] = ground_truth\n\t    evals[evals.index(e)][\"inference_class_names\"] = [e[\"classes\"][0][\"inference\"], \"background\"]\n\tbest = CompareEvaluations(\n\t    [\n\t        Evaluator(\n\t            predictions=get_dino_predictions(class_names=cn[\"classes\"]),\n\t            ground_truth=cn[\"ground_truth\"].annotations,\n\t            confidence_threshold=cn[\"confidence\"],\n\t            class_names=cn[\"inference_class_names\"],\n\t            name=cn[\"classes\"][0][\"inference\"],\n", "            mode=\"batch\",\n\t            model_type=\"object-detection\"\n\t        )\n\t        for cn in evals\n\t    ]\n\t)\n\tcomparison = best.compare()\n\tprint(\"F1 Score:\", comparison[0])\n\tprint(\"Precision:\", comparison[1])\n\tprint(\"Recall:\", comparison[2])\n", "print(\"Class Names:\", comparison[3])\n\tprint(\"Confidence Threshold:\", comparison[5])"]}
{"filename": "examples/models/clip.py", "chunked_list": ["import os\n\timport clip\n\timport torch\n\tfrom PIL import Image\n\tfrom evaluations.dataloaders.classification import ClassificationDetections\n\tdef run_clip_inference(file: str, class_names: list) -> dict:\n\t    \"\"\"\n\t    Run inference on a single image using the CLIP model by OpenAI.\n\t    Args:\n\t        file (str): Path to the image file.\n", "        class_names (list): List of class names.\n\t    Returns:\n\t        dict: Dictionary containing the filename and the predictions.\n\t    \"\"\"\n\t    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\t    image = preprocess(Image.open(file)).unsqueeze(0).to(device)\n\t    text = clip.tokenize(class_names).to(device)\n\t    with torch.no_grad():\n\t        image_features = clip_model.encode_image(image)\n", "        text_features = clip_model.encode_text(text)\n\t    image_features /= image_features.norm(dim=-1, keepdim=True)\n\t    text_features /= text_features.norm(dim=-1, keepdim=True)\n\t    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\t    values, indices = similarity[0].topk(1)\n\t    data = ClassificationDetections(\n\t        image_id=file,\n\t        predicted_class_names=[class_names[indices[0]]],\n\t        predicted_class_ids=[indices[0]],\n\t        confidence=values[0],\n", "    )\n\t    return {\"filename\": file, \"predictions\": data}\n"]}
{"filename": "examples/models/blip.py", "chunked_list": ["import os\n\timport torch\n\tfrom lavis.models import load_model_and_preprocess\n\tfrom PIL import Image\n\tfrom evaluations.dataloaders.classification import ClassificationDetections\n\tSUPPORTED_MODELS = (\"blip\", \"albef\")\n\tdef run_blip_albef_inference(file: str, class_names: list, model_type: str, device: str) -> dict:\n\t    \"\"\"\n\t    Run inference on a single image using the BLIP or ALBEF model by Salesforce.\n\t    Args:\n", "        file (str): Path to the image file.\n\t        class_names (list): List of class names.\n\t        model_type (str): Model type to use. Either \"blip\" or \"albef\".\n\t        device (str): Device to run the model on.\n\t    Returns:\n\t        dict: Dictionary containing the filename and the predictions.\n\t    \"\"\"\n\t    current_dir = os.getcwd()\n\t    file = os.path.join(current_dir, file)\n\t    if model_type not in SUPPORTED_MODELS:\n", "        raise ValueError(f\"Model type {model_type} is not supported by the run_blip_albef_inference function. Supported models are {SUPPORTED_MODELS}\")\n\t    raw_image = Image.open(\"./IMG_3322.jpeg\").convert(\"RGB\")\n\t    classes = [\"A picture of \" + c for c in class_names]\n\t    model, vis_processors, _ = load_model_and_preprocess(\n\t        name=model_type + \"_classification\",\n\t        model_type=\"base\",\n\t        is_eval=True,\n\t        device=device,\n\t    )\n\t    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n", "    sample = {\"image\": image, \"text_input\": classes}\n\t    image_features = model.extract_features(sample, mode=\"image\").image_embeds_proj[\n\t        :, 0\n\t    ]\n\t    text_features = model.extract_features(sample, mode=\"text\").text_embeds_proj[:, 0]\n\t    sims = (image_features @ text_features.t())[0] / model.temp\n\t    probs = torch.nn.Softmax(dim=0)(sims).tolist()\n\t    for cls_nm, prob in zip(classes, probs):\n\t        print(f\"{cls_nm}: \\t {prob:.3%}\")\n\t    top_prob = None\n", "    top_prob_confidence = -1\n\t    for cls_nm, prob in zip(classes, probs):\n\t        if prob > top_prob_confidence:\n\t            top_prob = cls_nm\n\t            top_prob_confidence = prob\n\t    top_prob = top_prob.lstrip(\"A picture of \")\n\t    data = ClassificationDetections(\n\t        image_id=file,\n\t        predicted_class_names=[class_names[top_prob[0]]],\n\t        predicted_class_ids=[top_prob[0]],\n", "        confidence=top_prob_confidence[0],\n\t    )\n\t    return {\"filename\": file, \"predictions\": data}\n"]}
{"filename": "examples/models/dinov2.py", "chunked_list": ["import json\n\timport os\n\timport numpy as np\n\timport torch\n\timport torchvision.transforms as T\n\tfrom PIL import Image\n\tfrom sklearn import svm\n\tfrom tqdm import tqdm\n\tfrom evaluations.dataloaders.classification import ClassificationDetections\n\tcwd = os.getcwd()\n", "ROOT_DIR = os.path.join(cwd, \"MIT-Indoor-Scene-Recognition-5/train\")\n\tdinov2_vits14 = torch.hub.load(\"facebookresearch/dinov2\", \"dinov2_vits14\")\n\tdevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\tdinov2_vits14.to(device)\n\ttransform_image = T.Compose(\n\t    [T.ToTensor(), T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])],\n\t)\n\tdef load_image(img: str) -> torch.Tensor:\n\t    \"\"\"\n\t    Load an image and return a tensor that can be used as an input to DINOv2.\n", "    \"\"\"\n\t    img = Image.open(img)\n\t    transformed_img = transform_image(img)[:3].unsqueeze(0)\n\t    return transformed_img\n\tdef embed_image(img: str) -> torch.Tensor:\n\t    \"\"\"\n\t    Embed an image using DINOv2.\n\t    \"\"\"\n\t    with torch.no_grad():\n\t        return dinov2_vits14(load_image(img).to(device))\n", "def compute_embeddings(files: list) -> dict:\n\t    \"\"\"\n\t    Create an index that contains all of the images in the specified list of files.\n\t    \"\"\"\n\t    all_embeddings = {}\n\t    with torch.no_grad():\n\t        for i, file in enumerate(tqdm(files)):\n\t            embeddings = dinov2_vits14(load_image(file).to(device))\n\t            all_embeddings[file] = (\n\t                np.array(embeddings[0].cpu().numpy()).reshape(1, -1).tolist()\n", "            )\n\t    with open(\"all_embeddings.json\", \"w\") as f:\n\t        f.write(json.dumps(all_embeddings))\n\t    return all_embeddings\n\tdef train_dinov2_svm_model(training_dir):\n\t    \"\"\"\n\t    Train an SVM model using the embeddings from DINOv2.\n\t    \"\"\"\n\t    labels = {}\n\t    for folder in os.listdir(training_dir):\n", "        for file in os.listdir(os.path.join(training_dir, folder)):\n\t            if file.endswith(\".jpg\"):\n\t                full_name = os.path.join(training_dir, folder, file)\n\t                labels[full_name] = folder\n\t    files = labels.keys()\n\t    embeddings = compute_embeddings(files)\n\t    clf = svm.SVC(gamma=\"scale\")\n\t    y = [labels[file] for file in files]\n\t    embedding_list = list(embeddings.values())\n\t    clf.fit(np.array(embedding_list).reshape(-1, 384), y)\n", "    return clf\n\tdef run_dinov2_inference(model, image: str, class_names: list) -> dict:\n\t    \"\"\"\n\t    Run inference on a single image using the DINOv2 model.\n\t    \"\"\"\n\t    result = model.predict(embed_image(image))\n\t    data = ClassificationDetections(\n\t        image_id=image,\n\t        predicted_class_names=result,\n\t        predicted_class_ids=[class_names.index(result[0])],\n", "        confidence=1.0,\n\t    )\n\t    return {\"filename\": image, \"predictions\": data}"]}
{"filename": "examples/annotation_check/run_evaluator.py", "chunked_list": ["from annotation_eval import AnnotationEval\n\timport argparse\n\t'''\n\tfrom roboflow import Roboflow\n\trf = Roboflow(api_key=\"\")\n\tproject = rf.workspace(\"\").project(\"\")\n\tdataset = project.version().download(\"voc\")\n\t'''\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n", "    \"--local_data_folder\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to local data folder of images and annotations \",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_data_folder\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to roboflow uploads of images and annotations \",\n", ")\n\targs = parser.parse_args()\n\tLOCAL_FOLDER = args.local_data_folder\n\tROBOFLOW_FOLDER = args.roboflow_data_folder\n\tif __name__ == \"__main__\":\n\t    eval = AnnotationEval(\n\t        local_folder = LOCAL_FOLDER,\n\t        roboflow_folder = ROBOFLOW_FOLDER)\n\t    eval.collect_images_labels()\n\t    eval.run_eval_loop()\n"]}
{"filename": "examples/annotation_check/annotation_eval.py", "chunked_list": ["import json\n\timport yaml\n\timport pandas as pd\n\timport os, glob\n\timport yaml\n\timport json\n\timport csv\n\timport xml.etree.ElementTree as ET\n\tfrom roboflow import Roboflow\n\timport os\n", "import argparse\n\tclass AnnotationEval():\n\t    def __init__(\n\t        self,\n\t        local_folder: str,\n\t        roboflow_folder: str\n\t    ):\n\t        \"\"\"\n\t        Loads local images + labels and roboflow images + labels. \n\t        Checks that each label in each image locally matches the label in roboflow\n", "            Args:\n\t                local_folder (str): local folder\n\t                roboflow_folder (str): data downloaded/exported from roboflow.\n\t            Returns:\n\t                Similarity Score\n\t        \"\"\"\n\t        self.local_folder = local_folder\n\t        self.roboflow_folder = roboflow_folder\n\t    def extract_text_after_last_slash(self,text):\n\t        last_slash_index = text.rfind('/')\n", "        if last_slash_index != -1:\n\t            text_after_last_slash = text[last_slash_index + 1:]\n\t            return text_after_last_slash\n\t        return None\n\t    def parse_xml(self,xml_string):\n\t        tree = ET.parse(xml_string)\n\t        root = tree.getroot()\n\t        bbox_dict = {}\n\t        # Find all <object> elements\n\t        object_elements = root.findall('.//object')\n", "        # Iterate over each <object> element\n\t        for i, object_element in enumerate(object_elements):\n\t            bbox = {}\n\t            # Extract bounding box coordinates\n\t            bndbox_element = object_element.find('bndbox')\n\t            bbox['xmin'] = round(float(bndbox_element.find('xmin').text))\n\t            bbox['xmax'] = round(float(bndbox_element.find('xmax').text))\n\t            bbox['ymin'] = round(float(bndbox_element.find('ymin').text))\n\t            bbox['ymax'] = round(float(bndbox_element.find('ymax').text))\n\t            # Extract name\n", "            name_element = object_element.find('name')\n\t            bbox['name'] = name_element.text\n\t            # Add bbox to the dictionary with counter as the key\n\t            bbox_dict[i] = bbox\n\t        return bbox_dict\n\t    def collect_images_labels(self):\n\t        self.local_images = []\n\t        self.local_annotations = []\n\t        self.roboflow_annotations = []\n\t        # Loop through each subfolder in image_folder\n", "        for root, dirs, files in os.walk(self.local_folder):\n\t            # Check if 'xml' folder exists in the current subfolder\n\t            if 'xml' in dirs:\n\t                xml_folder = os.path.join(root, 'xml')\n\t                # Get all XML files in xml_folder\n\t                #xml_files = [file for file in os.listdir(xml_folder) if file.endswith('.xml')]\n\t                xml_files = sorted(glob.glob(os.path.join(xml_folder, \"*.xml\")))\n\t                # Add the XML files to the master_list\n\t                self.local_annotations.extend(xml_files)\n\t        # Loop through each subfolder in image_folder\n", "        for root, dirs, files in os.walk(self.local_folder):\n\t            # Check if 'images' folder exists in the current subfolder\n\t            if 'images' in dirs:\n\t                images_folder = os.path.join(root, 'images')\n\t                # Get all JPEG files in the images_folder\n\t                #jpeg_files = [file for file in os.listdir(images_folder) if file.endswith('.jpg') or file.endswith('.jpg')]\n\t                jpeg_files = sorted(glob.glob(os.path.join(images_folder, \"*.jpg\")))\n\t                # Add the JPEG files to the local_images list\n\t                self.local_images.extend(jpeg_files)\n\t        name_requirements = [\"train\",\"test\",\"valid\"]\n", "        # Loop through each subfolder in image_folder\n\t        for root, dirs, files in os.walk(self.roboflow_folder):\n\t            # Check if 'images' folder exists in the current subfolder\n\t            for folder in dirs:\n\t                # Check if the subfolder meets the name requirement\n\t                if any(req in folder for req in name_requirements):\n\t                    xml_path = os.path.join(root, folder)\n\t                    # Get all XML files in the subfolder\n\t                    #xml_files = [file for file in os.listdir(xml_path) if file.endswith('.xml')]\n\t                    xml_files = sorted(glob.glob(os.path.join(xml_path, \"*.xml\")))\n", "                    # Add the XML files to the local_images list\n\t                    self.roboflow_annotations.extend(xml_files)\n\t        # Print the local_images list\n\t        print('local image count',len(self.local_images))\n\t        print('local annotation count',len(self.local_annotations))\n\t        print('robfolow annotation count',len(self.roboflow_annotations))\n\t        return self.local_images,self.local_annotations,self.roboflow_annotations\n\t    def run_eval_loop(self):\n\t        count = 0\n\t        loop_count = 0\n", "        roboflow_count = 0\n\t        match1 = 0\n\t        overall_accuracy = []\n\t        no_difference_count = 0\n\t        roboflow_key_count = 0\n\t        local_key_count = 0\n\t        key_match = 0\n\t        for image in self.local_images:\n\t            if count < len(self.local_images):\n\t                f = os.path.join(image)\n", "                image_hash = self.extract_text_after_last_slash(image.split(\".\")[0].replace(\"#\",\"-\"))\n\t                # split the image path to the hash\n\t                current_annotation = self.local_annotations[count]\n\t                annotation_hash = self.extract_text_after_last_slash(current_annotation.split(\".\")[0]).replace(\"#\",\"-\")\n\t                if image_hash == annotation_hash:\n\t                    match1 +=1 \n\t                    for roboflow_annotation in self.roboflow_annotations:\n\t                        #Roboflow labels and hash\n\t                        roboflow_hash = ((roboflow_annotation.split(\"/\"))[-1].split('.')[0][:-4])\n\t                        if roboflow_hash == image_hash:\n", "                            roboflow_count +=1\n\t                            local_parsed = self.parse_xml(current_annotation)\n\t                            roboflow_parsed = self.parse_xml(roboflow_annotation)\n\t                            label_count_local = len(local_parsed)\n\t                            roboflow_count_local = len(roboflow_parsed)\n\t                            local_key_count += label_count_local\n\t                            for key in local_parsed: \n\t                                if key in roboflow_parsed:\n\t                                    roboflow_key_count+=1\n\t                                    difference = 0\n", "                                    for sub_key in local_parsed[key]:\n\t                                        if sub_key in roboflow_parsed[key] and type(local_parsed[key][sub_key]) == int and (local_parsed[key][sub_key]-local_parsed[key][sub_key]) >1:\n\t                                            difference += (local_parsed[key][sub_key] - roboflow_parsed[key][sub_key])\n\t                                        if type(local_parsed[key][sub_key]) == str:\n\t                                            if local_parsed[key][sub_key] != roboflow_parsed[key][sub_key]:\n\t                                                difference += 1\n\t                                    if difference <=1:\n\t                                        no_difference_count +=1\n\t                                    elif difference >1:\n\t                                        print('PIXEL MISMATCH')\n", "                                        print(image_hash,annotation_hash,roboflow_hash)\n\t                                        print(difference)\n\t                    count+=1\n\t                    if loop_count > len(self.local_images)*len(self.local_annotations):\n\t                        break\n\t        print('\\n')\n\t        print('KEY_MATCH %',str((roboflow_key_count/local_key_count)*100)+'%')\n\t        print('\\n')\n\t        print('LABEL SIMILARITY %',str((no_difference_count/roboflow_key_count)*100)+'%')\n\t        print('\\n')\n", "        print('TOTAL LABELS',local_key_count)\n\t        print('\\n')\n\t        print('TOTAL IMAGE MATCH',match1)\n"]}
{"filename": "evaluations/roboflow.py", "chunked_list": ["from .evaluator import Evaluator\n\tclass RoboflowEvaluator(Evaluator):\n\t    \"\"\"\n\t    Evaluate models hosted on the Roboflow platform.\n\t    \"\"\"\n\t    pass\n"]}
{"filename": "evaluations/compare.py", "chunked_list": ["from tabulate import tabulate\n\tSUPPORTED_COMPARATORS = [\"f1\", \"precision\", \"recall\"]\n\tclass CompareEvaluations:\n\t    \"\"\"\n\t    Compare multiple evaluations and return the best one.\n\t    \"\"\"\n\t    def __init__(self, evaluations, comparator: str = \"f1\"):\n\t        self.evaluations = evaluations\n\t        self.comparator = comparator\n\t    def compare(self):\n", "        \"\"\"\n\t        Compare the evaluations and return the best one according to the specified comparator.\n\t        \"\"\"\n\t        highest = -1\n\t        highest_eval = None\n\t        evaluations = []\n\t        for evaluation in self.evaluations:\n\t            cf = evaluation.eval_model_predictions()\n\t            cf = evaluation.combined_cf\n\t            data = evaluation.calculate_statistics()\n", "            results = (\n\t                data.f1,\n\t                data.precision,\n\t                data.recall,\n\t                evaluation.class_names,\n\t                cf,\n\t                evaluation.confidence_threshold,\n\t                evaluation.name,\n\t            )\n\t            if self.comparator not in SUPPORTED_COMPARATORS:\n", "                raise Exception(\n\t                    f\"Comparator {self.comparator} not supported. Please use one of {SUPPORTED_COMPARATORS}.\"\n\t                )\n\t            if self.comparator == \"f1\":\n\t                if data.f1 > highest:\n\t                    highest = data.f1\n\t                    highest_eval = results\n\t            elif self.comparator == \"precision\":\n\t                if data.precision > highest:\n\t                    highest = data.precision\n", "                    highest_eval = results\n\t            elif self.comparator == \"recall\":\n\t                if data.recall > highest:\n\t                    highest = data.recall\n\t                    highest_eval = results\n\t            evaluations.append(results)\n\t        table = tabulate(\n\t            [\n\t                [\n\t                    \"F1\",\n", "                    \"Precision\",\n\t                    \"Recall\",\n\t                    \"Class Names\",\n\t                    \"Confusion Matrix\",\n\t                    \"Confidence\",\n\t                    \"Name\",\n\t                ]\n\t            ]\n\t            + evaluations,\n\t            headers=\"firstrow\",\n", "            tablefmt=\"fancy_grid\",\n\t        )\n\t        print(table)\n\t        return highest_eval, evaluations\n"]}
{"filename": "evaluations/classification.py", "chunked_list": ["from .evaluator import Evaluator\n\tclass ClassificationEvaluator(Evaluator):\n\t    \"\"\"\n\t    Evaluate classification models.\n\t    \"\"\"\n\t    def compute_confusion_matrix(self, result: dict, class_names: list) -> dict:\n\t        \"\"\"\n\t        Compute a confusion matrix for a classification model.\n\t        Args:\n\t            result (dict): A dictionary containing the ground truth and predictions.\n", "            class_names (list): A list of class names.\n\t        \"\"\"\n\t        confusion_data = {}\n\t        # matrix is\n\t        # [[tp, fp]]\n\t        for i, _ in enumerate(class_names):\n\t            for j, _ in enumerate(class_names):\n\t                confusion_data[(i, j)] = 0\n\t        if self.model_type == \"multiclass\":\n\t            for i, _ in enumerate(result[0]):\n", "                if result[0][i] in result[1].predicted_class_names:\n\t                    r0index = class_names.index(result[0][i])\n\t                    r1index = class_names.index(result[0][i])\n\t                    confusion_data[(r0index, r1index)] += 1\n\t                else:\n\t                    r0index = class_names.index(result[0][i])\n\t                    r1index = class_names.index(result[1].predicted_class_names[i])\n\t                    confusion_data[(r0index, r1index)] += 1\n\t        else:\n\t            is_match = result[0][0] == result[1].predicted_class_names[0]\n", "            if is_match:\n\t                r0index = class_names.index(result[0][0])\n\t                confusion_data[(r0index, r0index)] += 1\n\t            else:\n\t                r0index = class_names.index(result[0][0])\n\t                r1index = class_names.index(result[1].predicted_class_names[0])\n\t                confusion_data[(r0index, r1index)] += 1\n\t        return confusion_data\n"]}
{"filename": "evaluations/__init__.py", "chunked_list": ["from .compare import CompareEvaluations\n\tfrom .evaluator import Evaluator\n\t__version__ = \"0.1.0\"\n\t__all__ = [\"Evaluator\", \"CompareEvaluations\"]\n"]}
{"filename": "evaluations/confusion_matrices.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport seaborn as sns\n\tdef plot_confusion_matrix(\n\t    image_eval_results: dict,\n\t    class_names: list,\n\t    aggregate: bool = False,\n\t    file_name: str = \"\",\n\t    mode: str = \"interactive\",\n\t) -> None:\n\t    \"\"\"\n", "    Plot an aggregate confusion matrix showing the results of the evaluation.\n\t    \"\"\"\n\t    confusion = []\n\t    # get base name\n\t    file_name = file_name.split(\"/\")[-1]\n\t    for x in range(len(class_names)):\n\t        row = []\n\t        for y in range(len(class_names)):\n\t            row.append(image_eval_results[(x, y)])\n\t        confusion.append(row)\n", "    fig = plt.figure(figsize=(10, 10))\n\t    plt.title(\"Confusion Matrix for \" + file_name)\n\t    plt.xlabel(\"Predicted\")\n\t    plt.ylabel(\"Actual\")\n\t    if aggregate:\n\t        plt.title(\"Confusion Matrix (Aggregated)\")\n\t    heatmap = sns.heatmap(\n\t        confusion, annot=True, xticklabels=class_names, yticklabels=class_names, fmt=\"g\"\n\t    )\n\t    # axis names\n", "    heatmap.set_xlabel(\"Predicted\")\n\t    heatmap.set_ylabel(\"Actual\")\n\t    if mode == \"interactive\":\n\t        plt.title(file_name)\n\t        plt.show()\n\t    # save to ./output/matrices\n\t    plt.savefig(\"./output/matrices/\" + file_name + \".png\")\n\t    plt.close(fig)\n"]}
{"filename": "evaluations/eval_images.py", "chunked_list": ["import cv2\n\timport numpy as np\n\t# from supervision.metrics.iou import box_iou\n\tfrom .iou import box_iou\n\tdef find_best_prediction(\n\t    gt_box: list, predictions: list, iou_threshold: int = 0.5\n\t) -> tuple:\n\t    \"\"\"\n\t    Given ground truth data and associated predictions, find the prediction with the highest IOU.\n\t    Args:\n", "        gt_box (list): Ground truth box coordinates.\n\t        predictions (list): A list of model predictions.\n\t        iou_threshold (list): The Intersection Over Union threshold above which the ground truth and top prediction must be to be returned.\n\t    \"\"\"\n\t    if len(predictions) == 0:\n\t        return None, None\n\t    ious = [box_iou(gt_box, pred_box) for pred_box in predictions]\n\t    selected_pred_idx = np.argmax(ious)\n\t    selected_pred = predictions[selected_pred_idx]\n\t    if ious[selected_pred_idx] > iou_threshold:\n", "        return [selected_pred_idx, selected_pred]\n\t    else:\n\t        return None, None\n\tdef draw_bounding_boxes(\n\t    image: np.ndarray, ground_truth: list, predictions: list, class_names: list\n\t) -> np.ndarray:\n\t    \"\"\"\n\t    Draw bounding boxes on an image to show the ground truth and predictions.\n\t    Args:\n\t        image (np.ndarray): The image to draw bounding boxes on.\n", "        ground_truth (list): A list of ground truth bounding boxes.\n\t        predictions (list): A list of predicted bounding boxes.\n\t        class_names (list): A list of class names.\n\t    Returns:\n\t        np.ndarray: The image with bounding boxes drawn on it.\n\t    \"\"\"\n\t    output_image = image.copy()\n\t    # Draw ground truth bounding boxes with green color\n\t    for x0, y0, x1, y1, label in ground_truth:\n\t        cv2.rectangle(output_image, (x0, y0), (x1, y1), (0, 255, 0), 2)\n", "        cv2.putText(\n\t            output_image,\n\t            str(label),\n\t            (x0, y0 - 5),\n\t            cv2.FONT_HERSHEY_SIMPLEX,\n\t            0.5,\n\t            (0, 255, 255),\n\t            2,\n\t        )\n\t    # Draw predicted bounding boxes with red color\n", "    for pred in range(len(predictions.xyxy)):\n\t        prediction = predictions.xyxy[pred].tolist()\n\t        x0, y0, x1, y1 = prediction\n\t        class_name = class_names[int(predictions.class_id[pred])]\n\t        # scale to image\n\t        cv2.rectangle(output_image, (x0, y0), (x1, y1), (0, 0, 255), 2)\n\t        cv2.putText(\n\t            output_image,\n\t            str(class_name),\n\t            (x0, y0 - 5),\n", "            cv2.FONT_HERSHEY_SIMPLEX,\n\t            0.5,\n\t            (255, 0, 255),\n\t            2,\n\t        )\n\t    # show\n\t    cv2.imshow(\"image\", output_image)\n\t    return output_image\n\tdef visualize_image_experiment(img_eval_data: dict, class_names: list) -> None:\n\t    \"\"\"\n", "    Show the ground truth and predictions of an image using OpenCV.\n\t    Args:\n\t        img_eval_data: Evaluation data with ground truth and predictions.\n\t        class_names: The list of class names.\n\t    Returns:\n\t        None\n\t    \"\"\"\n\t    img_filename = img_eval_data[\"filename\"]\n\t    predictions = img_eval_data[\"predictions\"]\n\t    ground_truth = img_eval_data[\"ground_truth\"]\n", "    print(predictions)\n\t    image = cv2.imread(img_filename)\n\t    # output_image = draw_bounding_boxes(image, ground_truth, predictions, class_names)\n\t    base_file_name = img_filename.split(\"/\")[-1]\n\t    # save to ./output/images\n\t    print(f\"Saving image to ./output/images/{base_file_name}\")\n\t    # cv2.imwrite(f\"./output/images/{base_file_name}\", output_image)\n"]}
{"filename": "evaluations/iou.py", "chunked_list": ["def box_iou(boxA: list, boxB: list) -> float:\n\t    \"\"\"\n\t    Determine the Intersection over Union (IoU) of two bounding boxes.\n\t    Args:\n\t        :boxA (list): First bounding box\n\t        :boxB (list): Second bounding box\n\t    \"\"\"\n\t    # determine the (x, y)-coordinates of the intersection rectangle\n\t    xA = max(boxA[0], boxB[0])\n\t    yA = max(boxA[1], boxB[1])\n", "    xB = min(boxA[2], boxB[2])\n\t    yB = min(boxA[3], boxB[3])\n\t    # compute the area of intersection rectangle\n\t    interArea = (xB - xA) * (yB - yA)\n\t    # compute the area of both the prediction and ground-truth\n\t    # rectangles\n\t    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n\t    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n\t    # compute the intersection over union by taking the intersection\n\t    # area and dividing it by the sum of prediction + ground-truth\n", "    # areas - the interesection area\n\t    iou = interArea / float(boxAArea + boxBArea - interArea)\n\t    # return the intersection over union value\n\t    return iou\n"]}
{"filename": "evaluations/segmentation.py", "chunked_list": ["import numpy as np\n\tfrom .evaluator import Evaluator\n\tfrom .confusion_matrices import plot_confusion_matrix\n\timport cv2\n\tclass SegmentationEvaluator(Evaluator):\n\t    def __init__(self, ground_truth, predictions, class_names, eval_data_path, dataset = \"test\") -> None:\n\t        self.ground_truth = ground_truth\n\t        self.predictions = predictions\n\t        self.class_names = class_names\n\t        self.eval_data_path = eval_data_path\n", "        self.dataset = dataset\n\t    def eval_model_predictions(self) -> dict:\n\t        confusion_matrices = {}\n\t        class_names = self.class_names\n\t        predictions = self.predictions\n\t        combined_cf = {\"fn\": 0, \"fp\": 0}\n\t        combined_cf = {}\n\t        for i, _ in enumerate(class_names):\n\t            for j, _ in enumerate(class_names):\n\t                combined_cf[(i, j)] = 0\n", "        for image_name, prediction in self.ground_truth.annotations.items():\n\t            mask = prediction.mask\n\t            image_name = self.eval_data_path + \"/test/images/\" + image_name\n\t            best_mask = 0\n\t            best_iou = 0\n\t            cf = {}\n\t            for i, _ in enumerate(class_names):\n\t                for j, _ in enumerate(class_names):\n\t                    cf[(i, j)] = 0\n\t            if image_name not in predictions:\n", "                continue\n\t            for pred in predictions[image_name][\"predictions\"]:\n\t                class_id = pred[3]\n\t                pred = pred[1]\n\t                # show two masks on image\n\t                image = cv2.imread(image_name)\n\t                # convert img to 3 channels\n\t                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\t                # convert mask to 1 channel\n\t                mask = mask.astype(np.uint8)\n", "                for m in mask:\n\t                    intersection = np.logical_and(m, pred)\n\t                    union = np.logical_or(m, pred)\n\t                    iou = np.sum(intersection) / np.sum(union)\n\t                    if iou > best_iou:\n\t                        best_iou = iou\n\t                        best_mask = class_id\n\t            # if best mask has iou > threshold, add to confusion matrix\n\t            if best_iou > 0.5:\n\t                cf[(best_mask, best_mask)] += 1\n", "            else:\n\t                background_class_id = class_names.index(\"background\")\n\t                cf[(background_class_id, background_class_id)] += 1\n\t            for i, _ in enumerate(class_names):\n\t                for j, _ in enumerate(class_names):\n\t                    combined_cf[(i, j)] += cf[(i, j)]\n\t            confusion_matrices[image_name] = cf\n\t            plot_confusion_matrix(cf, self.class_names, False, key, self.mode)\n\t        plot_confusion_matrix(\n\t            combined_cf, self.class_names, True, \"aggregate\", self.mode\n", "        )\n\t        self.combined_cf = combined_cf\n\t        return combined_cf"]}
{"filename": "evaluations/evaluator.py", "chunked_list": ["import copy\n\timport os\n\tfrom dataclasses import dataclass\n\tfrom .confusion_matrices import plot_confusion_matrix\n\tfrom .eval_images import find_best_prediction, visualize_image_experiment\n\tACCEPTED_MODES = [\"interactive\", \"batch\"]\n\t@dataclass\n\tclass EvaluatorResponse:\n\t    true_positives: int\n\t    false_positives: int\n", "    false_negatives: int\n\t    precision: float\n\t    recall: float\n\t    f1: float\n\tclass Evaluator:\n\t    \"\"\"\n\t    Evaluates the output of a model on a dataset.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        predictions: dict = {},\n\t        ground_truth: dict = {},\n\t        class_names: list = [],\n\t        confidence_threshold: int = 0.2,\n\t        mode: str = \"interactive\",\n\t        name: str = \"\",\n\t        model_type: str = \"\",\n\t    ) -> None:\n\t        if mode not in ACCEPTED_MODES:\n\t            raise ValueError(f\"mode must be one of {ACCEPTED_MODES}\")\n", "        if model_type not in [\"multiclass\", \"classification\", \"object-detection\", \"segmentation\"]:\n\t            raise ValueError(\n\t                f\"model_type must be one of ['multiclass', 'classification', 'object-detection', 'segmentation']\"\n\t            )\n\t        self.confidence_threshold = confidence_threshold\n\t        self.mode = mode\n\t        self.class_names = class_names\n\t        self.name = name\n\t        self.model_type = model_type\n\t        if not os.path.exists(\"./output/images\"):\n", "            os.makedirs(\"./output/images\")\n\t        if not os.path.exists(\"./output/matrices\"):\n\t            os.makedirs(\"./output/matrices\")\n\t        merged_data = {}\n\t        for key in predictions.keys():\n\t            gt = ground_truth[key.split(\"/\")[-1]]\n\t            gts = [list(item) for item in gt.xyxy]\n\t            for idx, item in enumerate(gt.class_id):\n\t                gts[idx].append(item)\n\t            merged_data[key] = {\n", "                \"ground_truth\": gts,\n\t                \"predictions\": predictions[key][\"predictions\"],\n\t                \"filename\": key,\n\t            }\n\t        self.data = merged_data\n\t    def eval_model_predictions(self) -> dict:\n\t        \"\"\"\n\t        Compute confusion matrices for a Roboflow, Grounding DINO, and CLIP model.\n\t        Returns:\n\t            combined_cf (dict): A dictionary with a confusion matrix composed of the result of all inferences.\n", "        \"\"\"\n\t        confusion_matrices = {}\n\t        combined_cf = {\"fn\": 0, \"fp\": 0}\n\t        combined_cf = {}\n\t        for i, _ in enumerate(self.class_names):\n\t            for j, _ in enumerate(self.class_names):\n\t                combined_cf[(i, j)] = 0\n\t        for key, value in self.data.items():\n\t            print(\n\t                \"evaluating image predictions against ground truth\",\n", "                key,\n\t                \"...\",\n\t                self.class_names,\n\t            )\n\t            gt = value[\"ground_truth\"]\n\t            pred = value[\"predictions\"]\n\t            cf = self.compute_confusion_matrix([gt, pred], self.class_names)\n\t            for k, v in cf.items():\n\t                combined_cf[k] += v\n\t            confusion_matrices[key] = cf\n", "            if self.model_type == \"object-detection\":\n\t                visualize_image_experiment(value, self.class_names)\n\t            plot_confusion_matrix(cf, self.class_names, False, key, self.mode)\n\t        plot_confusion_matrix(\n\t            combined_cf, self.class_names, True, \"aggregate\", self.mode\n\t        )\n\t        self.combined_cf = combined_cf\n\t        return combined_cf\n\t    def compute_confusion_matrix(\n\t        self,\n", "        image_eval_data,\n\t        class_names,\n\t    ) -> dict:\n\t        # image eval data looks like:\n\t        # {filename: \"path/to/image.jpg\", ground_truth: [{}, {}, {}, ...], predictions: [{},{},{},...]}\n\t        # each ground truth is a tuple/list of (x_min, y_min, x_max, y_max, label)\n\t        # each prediction is a tuple/list of (x_min, y_min, x_max, y_max, label, confidence)\n\t        predictions = copy.deepcopy(image_eval_data[1])\n\t        all_predictions = []\n\t        for i in range(len(predictions)):\n", "            if (\n\t                predictions.confidence[i] > self.confidence_threshold\n\t                and predictions.class_id[i] is not None\n\t            ):\n\t                merged_prediction = predictions.xyxy[i].tolist() + [\n\t                    predictions.class_id[i]\n\t                ]\n\t                all_predictions.append(merged_prediction)\n\t        ground_truths = copy.deepcopy(image_eval_data[0])\n\t        confusion_data = {}\n", "        for i, _ in enumerate(class_names):\n\t            for j, _ in enumerate(class_names):\n\t                confusion_data[(i, j)] = 0\n\t        for gt_box in ground_truths:\n\t            match_idx, match = find_best_prediction(gt_box, all_predictions)\n\t            if match is None:\n\t                confusion_data[gt_box[4], len(class_names) - 1] += 1\n\t            else:\n\t                all_predictions.pop(match_idx)\n\t                confusion_data[gt_box[4], match[4]] += 1\n", "        for p in all_predictions:\n\t            confusion_data[len(class_names) - 1, p[4]] += 1\n\t        return confusion_data\n\t    def calculate_statistics(self) -> tuple:\n\t        \"\"\"\n\t        Calculate precision, recall, and f1 score for the evaluation.\n\t        Returns:\n\t            precision: The precision of the model\n\t            recall: The recall of the the model\n\t            f1: The f1 score of the model\n", "        \"\"\"\n\t        cf = self.combined_cf\n\t        # compute precision, recall, and f1 score\n\t        tp = 0\n\t        fp = 0\n\t        fn = 0\n\t        for x in range(len(self.class_names)):  # ground truth\n\t            for y in range(len(self.class_names)):  # predictions\n\t                if (\n\t                    x == len(self.class_names) - 1\n", "                ):  # last column / prediction with no ground truth\n\t                    fp += cf[(x, y)]\n\t                elif (\n\t                    y == len(self.class_names) - 1\n\t                ):  # bottom row / ground truth with no prediction\n\t                    fn += cf[(x, y)]\n\t                elif x == y:  # true positives across the diagonal\n\t                    tp += cf[(x, y)]\n\t                else:  # misclassification\n\t                    fp += cf[(x, y)]\n", "        if tp + fp == 0:\n\t            precision = 1\n\t        else:\n\t            precision = tp / (tp + fp)\n\t        if tp + fn == 0:\n\t            recall = 1\n\t        else:\n\t            recall = tp / (tp + fn)\n\t        if precision + recall == 0:\n\t            f1 = 0\n", "        else:\n\t            f1 = 2 * (precision * recall) / (precision + recall)\n\t        return EvaluatorResponse(\n\t            true_positives=tp,\n\t            false_positives=fp,\n\t            false_negatives=fn,\n\t            precision=precision,\n\t            recall=recall,\n\t            f1=f1,\n\t        )\n"]}
{"filename": "evaluations/dataloaders/yolov5.py", "chunked_list": ["import cv2\n\tdef get_ground_truth_for_image(img_filename):\n\t    labels = []\n\t    masks = []\n\t    label_file = img_filename.replace(\"/images/\", \"/labels/\").replace(\".jpg\", \".txt\")\n\t    with open(label_file, \"r\") as file:\n\t        for line in file:\n\t            # Split the line into a list of values and convert them to floats\n\t            values = list(map(float, line.strip().split()))\n\t            print(values)\n", "            if len(values) > 5:\n\t                # normalize to bbox\n\t                max_x = max(values[1::2])\n\t                min_x = min(values[1::2])\n\t                max_y = max(values[2::2])\n\t                min_y = min(values[2::2])\n\t                cx = (max_x + min_x) / 2\n\t                cy = (max_y + min_y) / 2\n\t                width = max_x - min_x\n\t                height = max_y - min_y\n", "                label = values[0]\n\t            else:\n\t                # Extract the label, and scale the coordinates and dimensions\n\t                label = int(values[0])\n\t                cx = values[1]\n\t                cy = values[2]\n\t                width = values[3]\n\t                height = values[4]\n\t            image = cv2.imread(img_filename)\n\t            x0 = cx - width / 2\n", "            y0 = cy - height / 2\n\t            x1 = cx + width / 2\n\t            y1 = cy + height / 2\n\t            # scale to non-floats\n\t            x0 = int(x0 * image.shape[1])\n\t            y0 = int(y0 * image.shape[0])\n\t            x1 = int(x1 * image.shape[1])\n\t            y1 = int(y1 * image.shape[0])\n\t            # Add the extracted data to the output list\n\t            labels.append((x0, y0, x1, y1, label))\n", "            if len(values) > 5:\n\t                # Extract the mask\n\t                mask = values[1:]\n\t                # Add the mask to the output list\n\t                masks.append(mask)\n\t    return labels, masks\n"]}
{"filename": "evaluations/dataloaders/roboflow.py", "chunked_list": ["import csv\n\timport os\n\timport cv2\n\timport numpy as np\n\timport roboflow\n\timport yaml\n\tfrom supervision.detection.core import Detections\n\tfrom .dataloader import DataLoader\n\tfrom .yolov5 import get_ground_truth_for_image\n\tclass RoboflowDataLoader(DataLoader):\n", "    def __init__(\n\t        self,\n\t        workspace_url: str,\n\t        project_url: str,\n\t        project_version: int,\n\t        image_files: str,\n\t        model_type: str = \"object-detection\",\n\t        dataset: str = \"test\",\n\t    ):\n\t        \"\"\"\n", "        Load a dataset from Roboflow. Saves the result to ./dataset/\n\t        Args:\n\t            workspace_url (str): The Roboflow workspace URL\n\t            project_url (str): The Roboflow project URL\n\t            project_version (int): The Roboflow project version\n\t            model_type (str): The model type. Either \"object-detection\" or \"classification\"\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        self.workspace_url = workspace_url\n", "        self.project_url = project_url\n\t        self.project_version = project_version\n\t        self.model_type = model_type\n\t        self.data = {}\n\t        self.image_files = image_files\n\t        self.dataset = dataset\n\t        self.model = None\n\t        self.dataset_version = None\n\t    def download_dataset(self) -> None:\n\t        \"\"\"\n", "        Download a dataset from Roboflow. Saves the result to ./dataset/\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        roboflow.login()\n\t        rf = roboflow.Roboflow()\n\t        self.data = {}\n\t        project = rf.workspace(self.workspace_url).project(self.project_url)\n\t        self.dataset_version = project.version(self.project_version)\n\t        self.dataset_content = self.dataset_version\n", "        self.model = project.version(self.project_version).model\n\t        if self.model_type == \"classification\":\n\t            data_format = \"folder\"\n\t        elif self.model_type == \"multiclass\":\n\t            data_format = \"multiclass\"\n\t        elif self.model_type == \"object-detection\":\n\t            data_format = \"yolov5\"\n\t        elif self.model_type == \"segmentation\":\n\t            data_format = \"yolov5\"\n\t        else:\n", "            raise ValueError(\"Model type not supported\")\n\t        root_path = self.image_files\n\t        # download if needed\n\t        if not os.path.exists(root_path):\n\t            self.dataset_version.download(data_format, root_path)\n\t        if data_format == \"yolov5\":\n\t            yaml_data = os.path.join(root_path, \"data.yaml\")\n\t            if os.path.exists(yaml_data):\n\t                # load class names map\n\t                with open(yaml_data, \"r\") as file:\n", "                    dataset_yaml = yaml.safe_load(file)\n\t                    self.class_names = [\n\t                        i.replace(\"-\", \" \") for i in dataset_yaml[\"names\"]\n\t                    ]\n\t        elif data_format == \"multiclass\":\n\t            with open(os.path.join(root_path, \"valid/\", \"_classes.csv\")) as f:\n\t                reader = csv.reader(f)\n\t                results = list(reader)\n\t                class_names = results[0]\n\t                # first item will be \"filename\", so we need to remove it\n", "                self.class_names = [c.strip() for c in class_names][1:]\n\t                self.class_names.append(\"background\")\n\t                for row in results[1:]:\n\t                    self.data[os.path.join(root_path, \"valid/\", row[0])] = {\n\t                        \"filename\": os.path.join(root_path, \"valid/\", row[0]),\n\t                        \"predictions\": [],\n\t                        \"ground_truth\": [\n\t                            self.class_names[c - 1].strip()\n\t                            for c in range(1, len(row))\n\t                            if row[c].strip() == \"1\"\n", "                        ],\n\t                    }\n\t                return self.class_names, self.data, self.model\n\t        else:\n\t            # class names are folder names in test/\n\t            self.class_names = [\n\t                name\n\t                for name in os.listdir(os.path.join(root_path, \"valid\"))\n\t                if os.path.isdir(os.path.join(root_path, \"valid\", name))\n\t            ]\n", "        for root, dirs, files in os.walk(\n\t            self.image_files.rstrip(\"/\") + f\"/{self.dataset}/\"\n\t        ):\n\t            for file in files:\n\t                if file.endswith(\".jpg\"):\n\t                    if self.model_type == \"object-detection\" or self.model_type == \"segmentation\":\n\t                        ground_truth, masks = get_ground_truth_for_image(\n\t                            os.path.join(root, file)\n\t                        )\n\t                        if masks != []:\n", "                            ground_truth = masks\n\t                    else:\n\t                        # folder name\n\t                        ground_truth = [os.path.basename(root)]\n\t                    self.data[os.path.join(root, file)] = {\n\t                        \"filename\": os.path.join(root, file),\n\t                        \"predictions\": [],\n\t                        \"ground_truth\": ground_truth,\n\t                    }\n\t        self.class_names.append(\"background\")\n", "        return self.class_names, self.data, self.model\n\tclass RoboflowPredictionsDataLoader(DataLoader):\n\t    def __init__(self, model, model_type, image_files, class_names):\n\t        self.model = model\n\t        self.model_type = model_type\n\t        self.image_files = image_files\n\t        self.data = {}\n\t        self.class_names = class_names\n\t    def get_predictions_for_image(self, img_filename: str) -> list:\n\t        \"\"\"\n", "        Retrieve predictions for a Roboflow, Grounding DINO, and CLIP model for a single image.\n\t        Args:\n\t            img_filename (str): Path to image file\n\t        Returns:\n\t            predictions (list): List of predictions\n\t        \"\"\"\n\t        prediction_group = self.model.predict(img_filename)\n\t        image_dimensions = prediction_group.image_dims\n\t        width, height = float(image_dimensions[\"width\"]), float(\n\t            image_dimensions[\"height\"]\n", "        )\n\t        prediction_group = [p.json() for p in prediction_group.predictions]\n\t        predictions = []\n\t        class_names = []\n\t        confidence = []\n\t        masks = []\n\t        for p in prediction_group:\n\t            # scale predictions to 0 to 1\n\t            cx = p[\"x\"] / width\n\t            cy = p[\"y\"] / height\n", "            w = p[\"width\"] / width\n\t            h = p[\"height\"] / height\n\t            x0 = cx - w / 2\n\t            y0 = cy - h / 2\n\t            x1 = cx + w / 2\n\t            y1 = cy + h / 2\n\t            # multiply by image dimensions to get pixel values\n\t            x0 = int(x0 * width)\n\t            y0 = int(y0 * height)\n\t            x1 = int(x1 * width)\n", "            y1 = int(y1 * height)\n\t            if p.get(\"points\"):\n\t                # points are {x, y} pairs, turn into mask\n\t                points = p[\"points\"]\n\t                # print(points)\n\t                mask = np.zeros((int(height), int(width)), dtype=np.uint8)\n\t                points = np.array(\n\t                    [(p[\"x\"], p[\"y\"]) for p in points], dtype=np.int32\n\t                ).reshape((-1, 1, 2))\n\t                # entire points should be filled\n", "                cv2.fillPoly(mask, [points], 1)\n\t                # should be T or F\n\t                mask = mask.astype(bool)\n\t                # print # of True bools\n\t                print(np.count_nonzero(mask))\n\t                # cv2.imshow(\"mask\", mask.astype(np.uint8) * 255)\n\t                # if cv2.waitKey(0) == ord(\"q\"):\n\t                #     break\n\t                masks.append(mask)\n\t            predictions.append(\n", "                (\n\t                    x0,\n\t                    y0,\n\t                    x1,\n\t                    y1,\n\t                )\n\t            )\n\t            class_names.append(self.class_names.index(p[\"class\"]))\n\t            confidence.append(p[\"confidence\"])\n\t        if len(predictions) == 0:\n", "            return Detections.empty()\n\t        predictions = Detections(\n\t            xyxy=np.array(predictions),\n\t            class_id=np.array(class_names),\n\t            mask=np.array(masks),\n\t            confidence=np.array(confidence),\n\t        )\n\t        return predictions\n"]}
{"filename": "evaluations/dataloaders/dataloader.py", "chunked_list": ["import os\n\tclass DataLoader:\n\t    def __init__(self):\n\t        self.eval_data = {}\n\t        self.data = {}\n\t        self.image_files = None\n\t    def process_files(self) -> None:\n\t        \"\"\"\n\t        Process all input files in the dataset.\n\t        Returns:\n", "            None\n\t        \"\"\"\n\t        for root, dirs, files in os.walk(self.image_files.rstrip(\"/\") + \"/test/\"):\n\t            for file in files:\n\t                if file.endswith(\".jpg\"):\n\t                    file_name = os.path.join(root, file)\n\t                    if file_name not in self.data:\n\t                        self.data[file_name] = {}\n\t                    self.data[file_name][\n\t                        \"predictions\"\n", "                    ] = self.get_predictions_for_image(file_name)\n\t        return self.data\n"]}
{"filename": "evaluations/dataloaders/classification.py", "chunked_list": ["from dataclasses import dataclass\n\t@dataclass\n\tclass ClassificationDetections:\n\t    image_id: str\n\t    predicted_class_names: list\n\t    predicted_class_ids: list\n\t    confidence: float\n"]}
{"filename": "evaluations/dataloaders/cliploader.py", "chunked_list": ["import glob\n\timport os\n\timport clip\n\timport torch\n\tfrom PIL import Image\n\tfrom ..evaluator import Evaluator\n\tdevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\tclass CLIPDataLoader(Evaluator):\n\t    \"\"\"\n\t    Evaluate CLIP prompts for classification tasks.\n", "    \"\"\"\n\t    def __init__(self, eval_data_path, class_names, data):\n\t        self.eval_data_path = eval_data_path\n\t        self.class_names = class_names\n\t        self.data = data\n\t        self.load_model()\n\t    def load_model(self):\n\t        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t        clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\t        self.clip_model = clip_model\n", "        self.preprocess = preprocess\n\t    def get_ground_truth_for_image(self, img_filename: str) -> list:\n\t        \"\"\"\n\t        Get the ground truth for image in an object detection dataset.\n\t        Args:\n\t            img_filename (str): Path to image file\n\t        Returns:\n\t            list: List of ground truth labels\n\t        \"\"\"\n\t        return self.class_names\n", "    def run_clip_inference(self, filename: str) -> tuple:\n\t        \"\"\"\n\t        Run inference on an image using CLIP.\n\t        Args:\n\t            filename (str): path to image file\n\t        Returns:\n\t            top (str): Top prediction from CLIP\n\t            top_rf (str): Top prediction from roboflow\n\t        \"\"\"\n\t        image = self.preprocess(Image.open(filename)).unsqueeze(0).to(device)\n", "        text = clip.tokenize(self.class_names).to(device)\n\t        with torch.no_grad():\n\t            image_features = self.clip_model.encode_image(image)\n\t            text_features = self.clip_model.encode_text(text)\n\t        image_features /= image_features.norm(dim=-1, keepdim=True)\n\t        text_features /= text_features.norm(dim=-1, keepdim=True)\n\t        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\t        _, indices = similarity[0].topk(1)\n\t        top = self.class_names[indices[0]]\n\t        return top\n", "    def get_predictions_for_image(self, img_filename: str) -> str:\n\t        \"\"\"\n\t        Retrieve predictions for a Roboflow, Grounding DINO, and CLIP model for a single image.\n\t        Args:\n\t            img_filename (str): Path to image file\n\t        Returns:\n\t            predictions (list): List of predictions\n\t        \"\"\"\n\t        label = self.run_clip_inference(img_filename)\n\t        return label\n", "    def process_files(self) -> None:\n\t        \"\"\"\n\t        Process all input files in the dataset.\n\t        Returns:\n\t            None\n\t        \"\"\"\n\t        for root, dirs, files in os.walk(self.eval_data_path):\n\t            for file in files:\n\t                if file.endswith(\".jpg\"):\n\t                    file_name = os.path.join(root, file)\n", "                    if file_name not in self.data:\n\t                        self.data[file_name] = {}\n\t                    self.data[file_name][\n\t                        \"predictions\"\n\t                    ] = self.get_predictions_for_image(file_name)\n\t                    self.data[file_name][\n\t                        \"ground_truth\"\n\t                    ] = self.get_ground_truth_for_image(file_name)\n\t        return self.data\n"]}
{"filename": "evaluations/dataloaders/__init__.py", "chunked_list": ["from .roboflow import RoboflowDataLoader, RoboflowPredictionsDataLoader\n\t__all__ = [\n\t    \"RoboflowDataLoader\",\n\t    \"JSONDataLoader\",\n\t    \"RoboflowPredictionsDataLoader\",\n\t]\n"]}
{"filename": "evaluations/annotation_check/run_evaluator.py", "chunked_list": ["from annotation_eval import AnnotationEval\n\timport argparse\n\t'''\n\tfrom roboflow import Roboflow\n\trf = Roboflow(api_key=\"\")\n\tproject = rf.workspace(\"\").project(\"\")\n\tdataset = project.version().download(\"voc\")\n\t'''\n\t# translate above to argparse\n\tparser = argparse.ArgumentParser()\n", "parser.add_argument(\n\t    \"--local_data_folder\",\n\t    type=str,\n\t    required=True,\n\t    help=\"Absolute path to local data folder of images and annotations \",\n\t)\n\tparser.add_argument(\n\t    \"--roboflow_data_folder\",\n\t    type=str,\n\t    required=True,\n", "    help=\"Absolute path to roboflow uploads of images and annotations \",\n\t)\n\targs = parser.parse_args()\n\tLOCAL_FOLDER = args.local_data_folder\n\tROBOFLOW_FOLDER = args.roboflow_data_folder\n\tif __name__ == \"__main__\":\n\t    eval = AnnotationEval(\n\t        local_folder = LOCAL_FOLDER,\n\t        roboflow_folder = ROBOFLOW_FOLDER)\n\t    eval.collect_images_labels()\n", "    eval.run_eval_loop()\n"]}
{"filename": "evaluations/annotation_check/annotation_eval.py", "chunked_list": ["import json\n\timport yaml\n\timport pandas as pd\n\timport os, glob\n\timport yaml\n\timport json\n\timport csv\n\timport xml.etree.ElementTree as ET\n\tfrom roboflow import Roboflow\n\timport os\n", "import argparse\n\tclass AnnotationEval():\n\t    def __init__(\n\t        self,\n\t        local_folder: str,\n\t        roboflow_folder: str\n\t    ):\n\t        \"\"\"\n\t        Loads local images + labels and roboflow images + labels. \n\t        Checks that each label in each image locally matches the label in roboflow\n", "            Args:\n\t                local_folder (str): local folder\n\t                roboflow_folder (str): data downloaded/exported from roboflow.\n\t            Returns:\n\t                Similarity Score\n\t        \"\"\"\n\t        self.local_folder = local_folder\n\t        self.roboflow_folder = roboflow_folder\n\t    def extract_text_after_last_slash(self,text):\n\t        last_slash_index = text.rfind('/')\n", "        if last_slash_index != -1:\n\t            text_after_last_slash = text[last_slash_index + 1:]\n\t            return text_after_last_slash\n\t        return None\n\t    def parse_xml(self,xml_string):\n\t        tree = ET.parse(xml_string)\n\t        root = tree.getroot()\n\t        bbox_dict = {}\n\t        # Find all <object> elements\n\t        object_elements = root.findall('.//object')\n", "        # Iterate over each <object> element\n\t        for i, object_element in enumerate(object_elements):\n\t            bbox = {}\n\t            # Extract bounding box coordinates\n\t            bndbox_element = object_element.find('bndbox')\n\t            bbox['xmin'] = round(float(bndbox_element.find('xmin').text))\n\t            bbox['xmax'] = round(float(bndbox_element.find('xmax').text))\n\t            bbox['ymin'] = round(float(bndbox_element.find('ymin').text))\n\t            bbox['ymax'] = round(float(bndbox_element.find('ymax').text))\n\t            # Extract name\n", "            name_element = object_element.find('name')\n\t            bbox['name'] = name_element.text\n\t            # Add bbox to the dictionary with counter as the key\n\t            bbox_dict[i] = bbox\n\t        return bbox_dict\n\t    def collect_images_labels(self):\n\t        self.local_images = []\n\t        self.local_annotations = []\n\t        self.roboflow_annotations = []\n\t        # Loop through each subfolder in image_folder\n", "        for root, dirs, files in os.walk(self.local_folder):\n\t            # Check if 'xml' folder exists in the current subfolder\n\t            if 'xml' in dirs:\n\t                xml_folder = os.path.join(root, 'xml')\n\t                # Get all XML files in xml_folder\n\t                #xml_files = [file for file in os.listdir(xml_folder) if file.endswith('.xml')]\n\t                xml_files = sorted(glob.glob(os.path.join(xml_folder, \"*.xml\")))\n\t                # Add the XML files to the master_list\n\t                self.local_annotations.extend(xml_files)\n\t        # Loop through each subfolder in image_folder\n", "        for root, dirs, files in os.walk(self.local_folder):\n\t            # Check if 'images' folder exists in the current subfolder\n\t            if 'images' in dirs:\n\t                images_folder = os.path.join(root, 'images')\n\t                # Get all JPEG files in the images_folder\n\t                #jpeg_files = [file for file in os.listdir(images_folder) if file.endswith('.jpg') or file.endswith('.jpg')]\n\t                jpeg_files = sorted(glob.glob(os.path.join(images_folder, \"*.jpg\")))\n\t                # Add the JPEG files to the local_images list\n\t                self.local_images.extend(jpeg_files)\n\t        name_requirements = [\"train\",\"test\",\"valid\"]\n", "        # Loop through each subfolder in image_folder\n\t        for root, dirs, files in os.walk(self.roboflow_folder):\n\t            # Check if 'images' folder exists in the current subfolder\n\t            for folder in dirs:\n\t                # Check if the subfolder meets the name requirement\n\t                if any(req in folder for req in name_requirements):\n\t                    xml_path = os.path.join(root, folder)\n\t                    # Get all XML files in the subfolder\n\t                    #xml_files = [file for file in os.listdir(xml_path) if file.endswith('.xml')]\n\t                    xml_files = sorted(glob.glob(os.path.join(xml_path, \"*.xml\")))\n", "                    # Add the XML files to the local_images list\n\t                    self.roboflow_annotations.extend(xml_files)\n\t        # Print the local_images list\n\t        print('local image count',len(self.local_images))\n\t        print('local annotation count',len(self.local_annotations))\n\t        print('robfolow annotation count',len(self.roboflow_annotations))\n\t        return self.local_images,self.local_annotations,self.roboflow_annotations\n\t    def run_eval_loop(self):\n\t        count = 0\n\t        loop_count = 0\n", "        roboflow_count = 0\n\t        match1 = 0\n\t        overall_accuracy = []\n\t        no_difference_count = 0\n\t        roboflow_key_count = 0\n\t        local_key_count = 0\n\t        key_match = 0\n\t        for image in self.local_images:\n\t            if count < len(self.local_images):\n\t                f = os.path.join(image)\n", "                image_hash = self.extract_text_after_last_slash(image.split(\".\")[0].replace(\"#\",\"-\"))\n\t                # split the image path to the hash\n\t                current_annotation = self.local_annotations[count]\n\t                annotation_hash = self.extract_text_after_last_slash(current_annotation.split(\".\")[0]).replace(\"#\",\"-\")\n\t                if image_hash == annotation_hash:\n\t                    match1 +=1 \n\t                    for roboflow_annotation in self.roboflow_annotations:\n\t                        #Roboflow labels and hash\n\t                        roboflow_hash = ((roboflow_annotation.split(\"/\"))[-1].split('.')[0][:-4])\n\t                        if roboflow_hash == image_hash:\n", "                            roboflow_count +=1\n\t                            local_parsed = self.parse_xml(current_annotation)\n\t                            roboflow_parsed = self.parse_xml(roboflow_annotation)\n\t                            label_count_local = len(local_parsed)\n\t                            roboflow_count_local = len(roboflow_parsed)\n\t                            local_key_count += label_count_local\n\t                            for key in local_parsed: \n\t                                if key in roboflow_parsed:\n\t                                    roboflow_key_count+=1\n\t                                    difference = 0\n", "                                    for sub_key in local_parsed[key]:\n\t                                        if sub_key in roboflow_parsed[key] and type(local_parsed[key][sub_key]) == int and (local_parsed[key][sub_key]-local_parsed[key][sub_key]) >1:\n\t                                            difference += (local_parsed[key][sub_key] - roboflow_parsed[key][sub_key])\n\t                                        if type(local_parsed[key][sub_key]) == str:\n\t                                            if local_parsed[key][sub_key] != roboflow_parsed[key][sub_key]:\n\t                                                difference += 1\n\t                                    if difference <=1:\n\t                                        no_difference_count +=1\n\t                                    elif difference >1:\n\t                                        print('PIXEL MISMATCH')\n", "                                        print(image_hash,annotation_hash,roboflow_hash)\n\t                                        print(difference)\n\t                    count+=1\n\t                    if loop_count > len(self.local_images)*len(self.local_annotations):\n\t                        break\n\t        print('\\n')\n\t        print('KEY_MATCH %',str((roboflow_key_count/local_key_count)*100)+'%')\n\t        print('\\n')\n\t        print('LABEL SIMILARITY %',str((no_difference_count/roboflow_key_count)*100)+'%')\n\t        print('\\n')\n", "        print('TOTAL LABELS',local_key_count)\n\t        print('\\n')\n\t        print('TOTAL IMAGE MATCH',match1)\n"]}
