{"filename": "toy/toy.py", "chunked_list": ["from copy import deepcopy\n\tfrom mpl_toolkits.mplot3d import Axes3D\n\tfrom matplotlib import cm, ticker\n\tfrom matplotlib.colors import LogNorm\n\tfrom tqdm import tqdm\n\tfrom scipy.optimize import minimize, Bounds, minimize_scalar\n\timport matplotlib.pyplot as plt\n\timport numpy as np\n\timport time\n\timport torch\n", "import torch.nn as nn\n\tfrom torch.optim.lr_scheduler import ExponentialLR\n\timport seaborn as sns\n\timport sys\n\t################################################################################\n\t#\n\t# Define the Optimization Problem\n\t#\n\t################################################################################\n\tLOWER = 0.000005\n", "class Toy(nn.Module):\n\t    def __init__(self):\n\t        super(Toy, self).__init__()\n\t        self.centers = torch.Tensor([[-3.0, 0], [3.0, 0]])\n\t    def forward(self, x, compute_grad=False):\n\t        x1 = x[0]\n\t        x2 = x[1]\n\t        f1 = torch.clamp((0.5 * (-x1 - 7) - torch.tanh(-x2)).abs(), LOWER).log() + 6\n\t        f2 = torch.clamp((0.5 * (-x1 + 3) + torch.tanh(-x2) + 2).abs(), LOWER).log() + 6\n\t        c1 = torch.clamp(torch.tanh(x2 * 0.5), 0)\n", "        f1_sq = ((-x1 + 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n\t        f2_sq = ((-x1 - 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n\t        c2 = torch.clamp(torch.tanh(-x2 * 0.5), 0)\n\t        f1 = f1 * c1 + f1_sq * c2\n\t        f2 = f2 * c1 + f2_sq * c2\n\t        f = torch.tensor([f1, f2])\n\t        if compute_grad:\n\t            g11 = torch.autograd.grad(f1, x1, retain_graph=True)[0].item()\n\t            g12 = torch.autograd.grad(f1, x2, retain_graph=True)[0].item()\n\t            g21 = torch.autograd.grad(f2, x1, retain_graph=True)[0].item()\n", "            g22 = torch.autograd.grad(f2, x2, retain_graph=True)[0].item()\n\t            g = torch.Tensor([[g11, g21], [g12, g22]])\n\t            return f, g\n\t        else:\n\t            return f\n\t    def batch_forward(self, x):\n\t        x1 = x[:, 0]\n\t        x2 = x[:, 1]\n\t        f1 = torch.clamp((0.5 * (-x1 - 7) - torch.tanh(-x2)).abs(), LOWER).log() + 6\n\t        f2 = torch.clamp((0.5 * (-x1 + 3) + torch.tanh(-x2) + 2).abs(), LOWER).log() + 6\n", "        c1 = torch.clamp(torch.tanh(x2 * 0.5), 0)\n\t        f1_sq = ((-x1 + 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n\t        f2_sq = ((-x1 - 7).pow(2) + 0.1 * (-x2 - 8).pow(2)) / 10 - 20\n\t        c2 = torch.clamp(torch.tanh(-x2 * 0.5), 0)\n\t        f1 = f1 * c1 + f1_sq * c2\n\t        f2 = f2 * c1 + f2_sq * c2\n\t        f = torch.cat([f1.view(-1, 1), f2.view(-1, 1)], -1)\n\t        return f\n\t################################################################################\n\t#\n", "# Plot Utils\n\t#\n\t################################################################################\n\tdef plotme(F, all_traj=None, xl=11):\n\t    n = 500\n\t    x = np.linspace(-xl, xl, n)\n\t    y = np.linspace(-xl, xl, n)\n\t    X, Y = np.meshgrid(x, y)\n\t    Xs = torch.Tensor(np.transpose(np.array([list(X.flat), list(Y.flat)]))).double()\n\t    Ys = F.batch_forward(Xs)\n", "    colormaps = {\n\t        \"sgd\": \"tab:blue\",\n\t        \"pcgrad\": \"tab:orange\",\n\t        \"mgd\": \"tab:cyan\",\n\t        \"cagrad\": \"tab:red\",\n\t        \"sdmgrad\": \"tab:green\"\n\t    }\n\t    plt.figure(figsize=(12, 5))\n\t    plt.subplot(131)\n\t    c = plt.contour(X, Y, Ys[:, 0].view(n, n))\n", "    if all_traj is not None:\n\t        for i, (k, v) in enumerate(all_traj.items()):\n\t            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n\t    plt.title(\"L1(x)\")\n\t    plt.subplot(132)\n\t    c = plt.contour(X, Y, Ys[:, 1].view(n, n))\n\t    if all_traj is not None:\n\t        for i, (k, v) in enumerate(all_traj.items()):\n\t            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n\t    plt.title(\"L2(x)\")\n", "    plt.subplot(133)\n\t    c = plt.contour(X, Y, Ys.mean(1).view(n, n))\n\t    if all_traj is not None:\n\t        for i, (k, v) in enumerate(all_traj.items()):\n\t            plt.plot(all_traj[k][:, 0], all_traj[k][:, 1], '--', c=colormaps[k], label=k)\n\t    plt.legend()\n\t    plt.title(\"0.5*(L1(x)+L2(x))\")\n\t    plt.tight_layout()\n\t    plt.savefig(f\"toy_ct.png\")\n\tdef plot3d(F, xl=11):\n", "    n = 500\n\t    x = np.linspace(-xl, xl, n)\n\t    y = np.linspace(-xl, xl, n)\n\t    X, Y = np.meshgrid(x, y)\n\t    Xs = torch.Tensor(np.transpose(np.array([list(X.flat), list(Y.flat)]))).double()\n\t    Ys = F.batch_forward(Xs)\n\t    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\t    ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n\t    ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n\t    ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n", "    ax.grid(False)\n\t    Yv = Ys.mean(1).view(n, n)\n\t    surf = ax.plot_surface(X, Y, Yv.numpy(), cmap=cm.viridis)\n\t    print(Ys.mean(1).min(), Ys.mean(1).max())\n\t    ax.set_zticks([-16, -8, 0, 8])\n\t    ax.set_zlim(-20, 10)\n\t    ax.set_xticks([-10, 0, 10])\n\t    ax.set_yticks([-10, 0, 10])\n\t    for tick in ax.xaxis.get_major_ticks():\n\t        tick.label.set_fontsize(15)\n", "    for tick in ax.yaxis.get_major_ticks():\n\t        tick.label.set_fontsize(15)\n\t    for tick in ax.zaxis.get_major_ticks():\n\t        tick.label.set_fontsize(15)\n\t    ax.view_init(25)\n\t    plt.tight_layout()\n\t    plt.savefig(f\"3d-obj.png\", dpi=1000)\n\tdef plot_contour(F, task=1, traj=None, xl=11, plotbar=False, name=\"tmp\"):\n\t    n = 500\n\t    x = np.linspace(-xl, xl, n)\n", "    y = np.linspace(-xl, xl, n)\n\t    X, Y = np.meshgrid(x, y)\n\t    fig = plt.figure()\n\t    ax = fig.add_subplot(111)\n\t    Xs = torch.Tensor(np.transpose(np.array([list(X.flat), list(Y.flat)]))).double()\n\t    Ys = F.batch_forward(Xs)\n\t    cmap = cm.get_cmap('viridis')\n\t    yy = -8.3552\n\t    if task == 0:\n\t        Yv = Ys.mean(1)\n", "        plt.plot(-8.5, 7.5, marker='o', markersize=10, zorder=5, color='k')\n\t        plt.plot(-8.5, -5, marker='o', markersize=10, zorder=5, color='k')\n\t        plt.plot(9, 9, marker='o', markersize=10, zorder=5, color='k')\n\t        plt.plot([-7, 7], [yy, yy], linewidth=8.0, zorder=0, color='gray')\n\t        plt.plot(0, yy, marker='*', markersize=15, zorder=5, color='k')\n\t    elif task == 1:\n\t        Yv = Ys[:, 0]\n\t        plt.plot(7, yy, marker='*', markersize=15, zorder=5, color='k')\n\t    else:\n\t        Yv = Ys[:, 1]\n", "        plt.plot(-7, yy, marker='*', markersize=15, zorder=5, color='k')\n\t    c = plt.contour(X, Y, Yv.view(n, n), cmap=cm.viridis, linewidths=4.0)\n\t    if traj is not None:\n\t        for tt in traj:\n\t            l = tt.shape[0]\n\t            color_list = np.zeros((l, 3))\n\t            color_list[:, 0] = 1.\n\t            color_list[:, 1] = np.linspace(0, 1, l)\n\t            #color_list[:,2] = 1-np.linspace(0, 1, l)\n\t            ax.scatter(tt[:, 0], tt[:, 1], color=color_list, s=6, zorder=10)\n", "    if plotbar:\n\t        cbar = fig.colorbar(c, ticks=[-15, -10, -5, 0, 5])\n\t        cbar.ax.tick_params(labelsize=15)\n\t    ax.set_aspect(1.0 / ax.get_data_ratio(), adjustable='box')\n\t    plt.xticks([-10, -5, 0, 5, 10], fontsize=15)\n\t    plt.yticks([-10, -5, 0, 5, 10], fontsize=15)\n\t    plt.tight_layout()\n\t    plt.savefig(f\"{name}.png\", dpi=100)\n\t    plt.close()\n\tdef smooth(x, n=20):\n", "    l = len(x)\n\t    y = []\n\t    for i in range(l):\n\t        ii = max(0, i - n)\n\t        jj = min(i + n, l - 1)\n\t        v = np.array(x[ii:jj]).astype(np.float64)\n\t        if i < 3:\n\t            y.append(x[i])\n\t        else:\n\t            y.append(v.mean())\n", "    return y\n\tdef plot_loss(trajs, name=\"tmp\"):\n\t    fig = plt.figure()\n\t    ax = fig.add_subplot(111)\n\t    colormaps = {\n\t        \"sgd\": \"tab:blue\",\n\t        \"pcgrad\": \"tab:orange\",\n\t        \"mgd\": \"tab:purple\",\n\t        \"cagrad\": \"tab:red\",\n\t        \"sdmgrad\": \"tab:cyan\"\n", "    }\n\t    maps = {\"sgd\": \"Adam\", \"pcgrad\": \"PCGrad\", \"mgd\": \"MGDA\", \"cagrad\": \"CAGrad\", \"sdmgrad\": \"SDMGrad (Ours)\"}\n\t    for method in [\"sgd\", \"mgd\", \"pcgrad\", \"cagrad\", \"sdmgrad\"]:\n\t        traj = trajs[method][::100]\n\t        Ys = F.batch_forward(traj)\n\t        x = np.arange(traj.shape[0])\n\t        #y = torch.cummin(Ys.mean(1), 0)[0]\n\t        y = Ys.mean(1)\n\t        ax.plot(x, smooth(list(y)), color=colormaps[method], linestyle='-', label=maps[method], linewidth=4.)\n\t    plt.xticks([0, 200, 400, 600, 800, 1000], [\"0\", \"20K\", \"40K\", \"60K\", \"80K\", \"100K\"], fontsize=15)\n", "    plt.yticks(fontsize=15)\n\t    ax.grid()\n\t    plt.legend(fontsize=15)\n\t    ax.set_aspect(1.0 / ax.get_data_ratio(), adjustable='box')\n\t    plt.tight_layout()\n\t    plt.savefig(f\"{name}.png\", dpi=100)\n\t    plt.close()\n\t################################################################################\n\t#\n\t# Multi-Objective Optimization Solver\n", "#\n\t################################################################################\n\tdef mean_grad(grads):\n\t    return grads.mean(1)\n\tdef pcgrad(grads):\n\t    g1 = grads[:, 0]\n\t    g2 = grads[:, 1]\n\t    g11 = g1.dot(g1).item()\n\t    g12 = g1.dot(g2).item()\n\t    g22 = g2.dot(g2).item()\n", "    if g12 < 0:\n\t        return ((1 - g12 / g11) * g1 + (1 - g12 / g22) * g2) / 2\n\t    else:\n\t        return (g1 + g2) / 2\n\tdef mgd(grads):\n\t    g1 = grads[:, 0]\n\t    g2 = grads[:, 1]\n\t    g11 = g1.dot(g1).item()\n\t    g12 = g1.dot(g2).item()\n\t    g22 = g2.dot(g2).item()\n", "    if g12 < min(g11, g22):\n\t        x = (g22 - g12) / (g11 + g22 - 2 * g12 + 1e-8)\n\t    elif g11 < g22:\n\t        x = 1\n\t    else:\n\t        x = 0\n\t    g_mgd = x * g1 + (1 - x) * g2  # mgd gradient g_mgd\n\t    return g_mgd\n\tdef cagrad(grads, c=0.5):\n\t    g1 = grads[:, 0]\n", "    g2 = grads[:, 1]\n\t    g0 = (g1 + g2) / 2\n\t    g11 = g1.dot(g1).item()\n\t    g12 = g1.dot(g2).item()\n\t    g22 = g2.dot(g2).item()\n\t    g0_norm = 0.5 * np.sqrt(g11 + g22 + 2 * g12 + 1e-4)\n\t    # want to minimize g_w^Tg_0 + c*||g_0||*||g_w||\n\t    coef = c * g0_norm\n\t    def obj(x):\n\t        # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n", "        # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n\t        return coef * np.sqrt(x**2*(g11+g22-2*g12)+2*x*(g12-g22)+g22+1e-4) + \\\n\t                0.5*x*(g11+g22-2*g12)+(0.5+x)*(g12-g22)+g22\n\t    res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n\t    x = res.x\n\t    gw = x * g1 + (1 - x) * g2\n\t    gw_norm = np.sqrt(x**2 * g11 + (1 - x)**2 * g22 + 2 * x * (1 - x) * g12 + 1e-4)\n\t    lmbda = coef / (gw_norm + 1e-4)\n\t    g = g0 + lmbda * gw\n\t    return g / (1 + c)\n", "### Our SDMGrad ###\n\tdef sdmgrad(grads, lmbda):\n\t    g1 = grads[:, 0]\n\t    g2 = grads[:, 1]\n\t    g0 = (g1 + g2) / 2\n\t    g11 = g1.dot(g1).item()\n\t    g12 = g1.dot(g2).item()\n\t    g22 = g2.dot(g2).item()\n\t    def obj(x):\n\t        # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n", "        # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n\t        return (x**2*(g11+g22-2*g12)+2*x*(g12-g22)+g22+1e-4) + \\\n\t                2 * lmbda * (0.5*x*(g11+g22-2*g12)+(0.5+x)*(g12-g22)+g22) + \\\n\t                lmbda**2 * 0.25 * (g11+g22+2*g12+1e-4)\n\t    res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n\t    x = res.x\n\t    gw = x * g1 + (1 - x) * g2\n\t    g = lmbda * g0 + gw\n\t    return g / (1 + lmbda)\n\t### Add noise ###\n", "def add_noise(grads, coef=0.2):\n\t    grads_ = grads + coef * torch.randn_like(grads)\n\t    return grads_\n\t### Define the problem ###\n\tF = Toy()\n\tmaps = {\"sgd\": mean_grad, \"cagrad\": cagrad, \"mgd\": mgd, \"pcgrad\": pcgrad, \"sdmgrad\": sdmgrad}\n\t### Start experiments ###\n\tdef run_all():\n\t    all_traj = {}\n\t    # the initial positions\n", "    inits = [\n\t        torch.Tensor([-8.5, 7.5]),\n\t        torch.Tensor([-8.5, -5.]),\n\t        torch.Tensor([9., 9.]),\n\t    ]\n\t    for i, init in enumerate(inits):\n\t        for m in tqdm([\"sgd\", \"mgd\", \"pcgrad\", \"cagrad\", \"sdmgrad\"]):\n\t            all_traj[m] = None\n\t            traj = []\n\t            solver = maps[m]\n", "            x = init.clone()\n\t            x.requires_grad = True\n\t            n_iter = 70000\n\t            opt = torch.optim.Adam([x], lr=0.002)\n\t            # scheduler = ExponentialLR(opt, gamma = 0.9999)\n\t            for it in range(n_iter):\n\t                traj.append(x.detach().numpy().copy())\n\t                # if it % 1000 == 0:\n\t                #     print(f'\\niteration {it}, before update x: ', x.detach().numpy().copy())\n\t                f, grads = F(x, True)\n", "                grads = add_noise(grads, coef=0.2)\n\t                # grads = add_element_noise(grads, coef=1.0, it=it)\n\t                if m == \"cagrad\":\n\t                    g = solver(grads, c=0.5)\n\t                elif m == \"sdmgrad\":\n\t                    g = solver(grads, lmbda=0.01)\n\t                else:\n\t                    g = solver(grads)\n\t                opt.zero_grad()\n\t                x.grad = g\n", "                opt.step()\n\t                # scheduler.step()\n\t            all_traj[m] = torch.tensor(np.array(traj))\n\t        torch.save(all_traj, f\"toy{i}.pt\")\n\t    plot_loss(all_traj)\n\t    plot_results()\n\tdef plot_results():\n\t    plot3d(F)\n\t    plot_contour(F, 1, name=\"toy_task_1\")\n\t    plot_contour(F, 2, name=\"toy_task_2\")\n", "    t1 = torch.load(f\"toy0.pt\")\n\t    t2 = torch.load(f\"toy1.pt\")\n\t    t3 = torch.load(f\"toy2.pt\")\n\t    length = t1[\"sdmgrad\"].shape[0]\n\t    for method in [\"sgd\", \"mgd\", \"pcgrad\", \"cagrad\", \"sdmgrad\"]:\n\t        ranges = list(range(10, length, 1000))\n\t        ranges.append(length - 1)\n\t        for t in tqdm(ranges):\n\t            plot_contour(\n\t                F,\n", "                task=0,  # task == 0 meeas plot for both tasks\n\t                traj=[t1[method][:t], t2[method][:t], t3[method][:t]],\n\t                plotbar=(method == \"sdmgrad\"),\n\t                name=f\"./imgs/toy_{method}_{t}\")\n\tif __name__ == \"__main__\":\n\t    run_all()\n"]}
{"filename": "nyuv2/model_segnet_stan.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Single-task: Attention Network')\n\tparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')\n\tparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n", "opt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 13\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n", "        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n", "            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n\t        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n\t        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n\t        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n", "        for j in range(1):\n\t            for i in range(4):\n\t                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n\t                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\t        for i in range(4):\n\t            if i < 3:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t            else:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n", "                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t        if opt.task == 'semantic':\n\t            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        if opt.task == 'depth':\n\t            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n\t        if opt.task == 'normal':\n\t            self.pred_task = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n", "        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n", "    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n", "                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n\t    def att_layer(self, channel):\n\t        att_block = nn.Sequential(\n\t            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[1]),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[2]),\n", "            nn.Sigmoid(),\n\t        )\n\t        return att_block\n\t    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define attention list for two tasks\n\t        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n\t        for i in range(3):\n", "            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n\t        for i in range(3):\n\t            for j in range(5):\n\t                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\t        # define global shared network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n", "            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n", "                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t        # define task dependent attention module\n\t        for i in range(1):\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n", "                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t                else:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n\t                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n", "                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t                else:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n\t                                                           scale_factor=2,\n", "                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t        # define task prediction layers\n\t        if opt.task == 'semantic':\n\t            pred = F.log_softmax(self.pred_task(atten_decoder[0][-1][-1]), dim=1)\n\t        if opt.task == 'depth':\n", "            pred = self.pred_task(atten_decoder[0][-1][-1])\n\t        if opt.task == 'normal':\n\t            pred = self.pred_task(atten_decoder[0][-1][-1])\n\t            pred = pred / torch.norm(pred, p=2, dim=1, keepdim=True)\n\t        return pred\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_STAN = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_STAN.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n", "print('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_STAN),\n\t                                                         count_parameters(SegNet_STAN) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation on NYUv2.')\n\telse:\n", "    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\tnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\tbatch_size = 2\n\tnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\tnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate single-task network\n\tsingle_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_STAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/model_segnet_mtan.py", "chunked_list": ["import numpy as np\n\timport random\n\timport torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Attention Network')\n", "parser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n\tparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\n\tparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--seed', default=0, type=int, help='the seed')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n", "        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 13\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n", "        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n", "                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task attention layers\n\t        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n\t        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n\t        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n\t        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for j in range(3):\n\t            if j < 2:\n\t                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n\t                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n", "            for i in range(4):\n\t                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n\t                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\t        for i in range(4):\n\t            if i < 3:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t            else:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n", "        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n\t        self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n", "                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n", "                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n", "    def att_layer(self, channel):\n\t        att_block = nn.Sequential(\n\t            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[1]),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[2]),\n\t            nn.Sigmoid(),\n\t        )\n\t        return att_block\n", "    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define attention list for tasks\n\t        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n\t        for i in range(3):\n\t            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n\t        for i in range(3):\n\t            for j in range(5):\n", "                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\t        # define global shared network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n", "                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n", "        # define task dependent attention module\n\t        for i in range(3):\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t                else:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n", "                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n", "                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t                else:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n", "                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n\t        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n\t        t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n\t        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\t# control seed\n", "torch.backends.cudnn.enabled = False\n\ttorch.manual_seed(opt.seed)\n\tnp.random.seed(opt.seed)\n\trandom.seed(opt.seed)\n\ttorch.cuda.manual_seed_all(opt.seed)\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_MTAN = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n", "print('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n\t                                                         count_parameters(SegNet_MTAN) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation on NYUv2.')\n\telse:\n", "    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\tnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\tbatch_size = 2\n\tnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\tnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate multi-task network\n\tmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/model_segnet_split.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Split')\n\tparser.add_argument('--type', default='standard', type=str, help='split type: standard, wide, deep')\n\tparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n", "parser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\n\tparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--seed', default=0, type=int, help='the seed')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        if opt.type == 'wide':\n", "            filter = [64, 128, 256, 512, 1024]\n\t        else:\n\t            filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 13\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n", "        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n", "                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task specific layers\n\t        self.pred_task1 = nn.Sequential(\n\t            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n\t            nn.Conv2d(in_channels=filter[0], out_channels=self.class_nb, kernel_size=1, padding=0))\n\t        self.pred_task2 = nn.Sequential(\n\t            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n\t            nn.Conv2d(in_channels=filter[0], out_channels=1, kernel_size=1, padding=0))\n", "        self.pred_task3 = nn.Sequential(\n\t            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n\t            nn.Conv2d(in_channels=filter[0], out_channels=3, kernel_size=1, padding=0))\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n", "                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    # define convolutional block\n\t    def conv_layer(self, channel):\n\t        if opt.type == 'deep':\n", "            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n", "                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]), nn.ReLU(inplace=True))\n\t        return conv_block\n\t    def forward(self, x):\n\t        import pdb\n\t        pdb.set_trace()\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # global shared encoder-decoder network\n", "        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n", "            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(g_decoder[i][1]), dim=1)\n", "        t2_pred = self.pred_task2(g_decoder[i][1])\n\t        t3_pred = self.pred_task3(g_decoder[i][1])\n\t        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\t# control seed\n\ttorch.backends.cudnn.enabled = False\n\ttorch.manual_seed(opt.seed)\n\tnp.random.seed(opt.seed)\n\trandom.seed(opt.seed)\n\ttorch.cuda.manual_seed_all(opt.seed)\n", "# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_SPLIT = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_SPLIT.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_SPLIT),\n\t                                                         count_parameters(SegNet_SPLIT) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n", "dataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation on NYUv2.')\n\telse:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\tnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\tbatch_size = 2  ###org 2\n\tnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n", "nyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\timport pdb\n\tpdb.set_trace()\n\t# Train and evaluate multi-task network\n\tmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_SPLIT, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/evaluate.py", "chunked_list": ["import matplotlib\n\timport matplotlib.pyplot as plt\n\timport seaborn as sns\n\timport numpy as np\n\timport torch\n\timport itertools\n\tmethods = [\n\t    \"sdmgrad-1e-1\", \"sdmgrad-2e-1\", \"sdmgrad-3e-1\", \"sdmgrad-4e-1\", \"sdmgrad-5e-1\", \"sdmgrad-6e-1\", \"sdmgrad-7e-1\",\n\t    \"sdmgrad-8e-1\", \"sdmgrad-9e-1\", \"sdmgrad-1e0\"\n\t]\n", "colors = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\", \"tab:green\", \"tab:cyan\", \"tab:blue\", \"tab:red\"]\n\tstats = [\n\t    \"semantic loss\", \"mean iou\", \"pix acc\", \"depth loss\", \"abs err\", \"rel err\", \"normal loss\", \"mean\", \"median\",\n\t    \"<11.25\", \"<22.5\", \"<30\"\n\t]\n\tdelta_stats = [\"mean iou\", \"pix acc\", \"abs err\", \"rel err\", \"mean\", \"median\", \"<11.25\", \"<22.5\", \"<30\"]\n\tstats_idx_map = [4, 5, 6, 8, 9, 10, 12, 13, 14, 15, 16, 17]\n\ttime_idx = 34\n\t# change random seeds used in the experiments here\n\tseeds = [0, 1, 2]\n", "logs = {}\n\tmin_epoch = 100000\n\tfor m in methods:\n\t    logs[m] = {\"train\": [None for _ in range(3)], \"test\": [None for _ in range(3)]}\n\t    for seed in seeds:\n\t        logs[m][\"train\"][seed] = {}\n\t        logs[m][\"test\"][seed] = {}\n\t    for stat in stats:\n\t        for seed in seeds:\n\t            logs[m][\"train\"][seed][stat] = []\n", "            logs[m][\"test\"][seed][stat] = []\n\t    for seed in seeds:\n\t        logs[m][\"train\"][seed][\"time\"] = []\n\t    for seed in seeds:\n\t        fname = f\"logs/{m}-sd{seed}.log\"\n\t        with open(fname, \"r\") as f:\n\t            lines = f.readlines()\n\t            for line in lines:\n\t                if line.startswith(\"Epoch\"):\n\t                    ws = line.split(\" \")\n", "                    for i, stat in enumerate(stats):\n\t                        logs[m][\"train\"][seed][stat].append(float(ws[stats_idx_map[i]]))\n\t                        logs[m][\"test\"][seed][stat].append(float(ws[stats_idx_map[i] + 15]))\n\t                    logs[m][\"train\"][seed][\"time\"].append(float(ws[time_idx]))\n\t            min_epoch = min(min(min_epoch, len(logs[m][\"train\"][seed][\"semantic loss\"])),\n\t                            len(logs[m][\"test\"][seed][\"semantic loss\"]))\n\ttest_stats = {}\n\ttrain_stats = {}\n\tlearning_time = {}\n\tprint(\" \" * 25 + \" | \".join([f\"{s:5s}\" for s in stats]))\n", "for mi, mode in enumerate([\"train\", \"test\"]):\n\t    if mi == 1:\n\t        print(mode)\n\t    for mmi, m in enumerate(methods):\n\t        if m not in test_stats:\n\t            test_stats[m] = {}\n\t            train_stats[m] = {}\n\t        string = f\"{m:30s} \"\n\t        for stat in stats:\n\t            x = []\n", "            for seed in seeds:\n\t                x.append(np.array(logs[m][mode][seed][stat][min_epoch - 10:min_epoch]).mean())\n\t            x = np.array(x)\n\t            if mode == \"test\":\n\t                test_stats[m][stat] = x.copy()\n\t            else:\n\t                train_stats[m][stat] = x.copy()\n\t            mu = x.mean()\n\t            std = x.std() / np.sqrt(3)\n\t            string += f\" | {mu:5.4f}\"\n", "        if mode == \"test\":\n\t            print(string)\n\tfor m in methods:\n\t    learning_time[m] = np.array([np.array(logs[m][\"train\"][sd][\"time\"]).mean() for sd in seeds])\n\t### print average training loss\n\tfor method in methods:\n\t    average_loss = np.mean([\n\t        train_stats[method][\"semantic loss\"].mean(), train_stats[method][\"depth loss\"].mean(),\n\t        train_stats[method][\"normal loss\"].mean()\n\t    ])\n", "    print(f\"{method} average training loss {average_loss}\")\n\t### print delta M\n\tbase = np.array([0.3830, 0.6376, 0.6754, 0.2780, 25.01, 19.21, 0.3014, 0.5720, 0.6915])\n\tsign = np.array([1, 1, 0, 0, 0, 0, 1, 1, 1])\n\tkk = np.ones(9) * -1\n\tdef delta_fn(a):\n\t    return (kk**sign * (a - base) / base).mean() * 100.  # *100 for percentage\n\tdeltas = {}\n\tfor method in methods:\n\t    tmp = np.zeros(9)\n", "    for i, stat in enumerate(delta_stats):\n\t        tmp[i] = test_stats[method][stat].mean()\n\t    deltas[method] = delta_fn(tmp)\n\t    print(f\"{method:30s} delta: {deltas[method]:4.3f}\")\n"]}
{"filename": "nyuv2/utils.py", "chunked_list": ["import numpy as np\n\timport time\n\timport torch\n\timport torch.nn.functional as F\n\tfrom copy import deepcopy\n\tfrom min_norm_solvers import MinNormSolver\n\tfrom scipy.optimize import minimize, Bounds, minimize_scalar\n\tdef euclidean_proj_simplex(v, s=1):\n\t    \"\"\" Compute the Euclidean projection on a positive simplex\n\t    Solves the optimisation problem (using the algorithm from [1]):\n", "        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n\t    Parameters\n\t    ----------\n\t    v: (n,) numpy array,\n\t       n-dimensional vector to project\n\t    s: int, optional, default: 1,\n\t       radius of the simplex\n\t    Returns\n\t    -------\n\t    w: (n,) numpy array,\n", "       Euclidean projection of v on the simplex\n\t    Notes\n\t    -----\n\t    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n\t    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n\t    However, this implementation still easily scales to millions of dimensions.\n\t    References\n\t    ----------\n\t    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n\t        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n", "        International Conference on Machine Learning (ICML 2008)\n\t        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n\t    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n\t        Weiran Wang, Miguel . Carreira-Perpin. arXiv:1309.1541\n\t        https://arxiv.org/pdf/1309.1541.pdf\n\t    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n\t    \"\"\"\n\t    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n\t    v = v.astype(np.float64)\n\t    n, = v.shape  # will raise ValueError if v is not 1-D\n", "    # check if we are already on the simplex\n\t    if v.sum() == s and np.alltrue(v >= 0):\n\t        # best projection: itself!\n\t        return v\n\t    # get the array of cumulative sums of a sorted (decreasing) copy of v\n\t    u = np.sort(v)[::-1]\n\t    cssv = np.cumsum(u)\n\t    # get the number of > 0 components of the optimal solution\n\t    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n\t    # compute the Lagrange multiplier associated to the simplex constraint\n", "    theta = float(cssv[rho] - s) / (rho + 1)\n\t    # compute the projection by thresholding v using theta\n\t    w = (v - theta).clip(min=0)\n\t    return w\n\t\"\"\"\n\tDefine task metrics, loss functions and model trainer here.\n\t\"\"\"\n\tdef count_parameters(model):\n\t    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\tdef model_fit(x_pred, x_output, task_type):\n", "    device = x_pred.device\n\t    # binary mark to mask out undefined pixel space\n\t    binary_mask = (torch.sum(x_output, dim=1) != 0).float().unsqueeze(1).to(device)\n\t    if task_type == 'semantic':\n\t        # semantic loss: depth-wise cross entropy\n\t        loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n\t    if task_type == 'depth':\n\t        # depth loss: l1 norm\n\t        loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask,\n\t                                                                                     as_tuple=False).size(0)\n", "    if task_type == 'normal':\n\t        # normal loss: dot product\n\t        loss = 1 - torch.sum((x_pred * x_output) * binary_mask) / torch.nonzero(binary_mask, as_tuple=False).size(0)\n\t    return loss\n\t# Legacy: compute mIoU and Acc. for each image and average across all images.\n\t# def compute_miou(x_pred, x_output):\n\t#     _, x_pred_label = torch.max(x_pred, dim=1)\n\t#     x_output_label = x_output\n\t#     batch_size = x_pred.size(0)\n\t#     class_nb = x_pred.size(1)\n", "#     device = x_pred.device\n\t#     for i in range(batch_size):\n\t#         true_class = 0\n\t#         first_switch = True\n\t#         invalid_mask = (x_output[i] >= 0).float()\n\t#         for j in range(class_nb):\n\t#             pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).long().to(device))\n\t#             true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).long().to(device))\n\t#             mask_comb = pred_mask.float() + true_mask.float()\n\t#             union = torch.sum((mask_comb > 0).float() * invalid_mask)  # remove non-defined pixel predictions\n", "#             intsec = torch.sum((mask_comb > 1).float())\n\t#             if union == 0:\n\t#                 continue\n\t#             if first_switch:\n\t#                 class_prob = intsec / union\n\t#                 first_switch = False\n\t#             else:\n\t#                 class_prob = intsec / union + class_prob\n\t#             true_class += 1\n\t#         if i == 0:\n", "#             batch_avg = class_prob / true_class\n\t#         else:\n\t#             batch_avg = class_prob / true_class + batch_avg\n\t#     return batch_avg / batch_size\n\t#\n\t#\n\t# def compute_iou(x_pred, x_output):\n\t#     _, x_pred_label = torch.max(x_pred, dim=1)\n\t#     x_output_label = x_output\n\t#     batch_size = x_pred.size(0)\n", "#     for i in range(batch_size):\n\t#         if i == 0:\n\t#             pixel_acc = torch.div(\n\t#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n\t#                 torch.sum((x_output_label[i] >= 0).float()))\n\t#         else:\n\t#             pixel_acc = pixel_acc + torch.div(\n\t#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n\t#                 torch.sum((x_output_label[i] >= 0).float()))\n\t#     return pixel_acc / batch_size\n", "# New mIoU and Acc. formula: accumulate every pixel and average across all pixels in all images\n\tclass ConfMatrix(object):\n\t    def __init__(self, num_classes):\n\t        self.num_classes = num_classes\n\t        self.mat = None\n\t    def update(self, pred, target):\n\t        n = self.num_classes\n\t        if self.mat is None:\n\t            self.mat = torch.zeros((n, n), dtype=torch.int64, device=pred.device)\n\t        with torch.no_grad():\n", "            k = (target >= 0) & (target < n)\n\t            inds = n * target[k].to(torch.int64) + pred[k]\n\t            self.mat += torch.bincount(inds, minlength=n**2).reshape(n, n)\n\t    def get_metrics(self):\n\t        h = self.mat.float()\n\t        acc = torch.diag(h).sum() / h.sum()\n\t        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n\t        return torch.mean(iu).item(), acc.item()\n\tdef depth_error(x_pred, x_output):\n\t    device = x_pred.device\n", "    binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n\t    x_pred_true = x_pred.masked_select(binary_mask)\n\t    x_output_true = x_output.masked_select(binary_mask)\n\t    abs_err = torch.abs(x_pred_true - x_output_true)\n\t    rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n\t    return (torch.sum(abs_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item(), \\\n\t           (torch.sum(rel_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item()\n\tdef normal_error(x_pred, x_output):\n\t    binary_mask = (torch.sum(x_output, dim=1) != 0)\n\t    error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1,\n", "                                   1)).detach().cpu().numpy()\n\t    error = np.degrees(error)\n\t    return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n\t\"\"\"\n\t=========== Universal Multi-task Trainer ===========\n\t\"\"\"\n\tdef multi_task_trainer(train_loader, test_loader, multi_task_model, device, optimizer, scheduler, opt, total_epoch=200):\n\t    start_time = time.time()\n\t    train_batch = len(train_loader)\n\t    test_batch = len(test_loader)\n", "    T = opt.temp\n\t    avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\n\t    lambda_weight = np.ones([3, total_epoch])\n\t    for index in range(total_epoch):\n\t        epoch_start_time = time.time()\n\t        cost = np.zeros(24, dtype=np.float32)\n\t        # apply Dynamic Weight Average\n\t        if opt.weight == 'dwa':\n\t            if index == 0 or index == 1:\n\t                lambda_weight[:, index] = 1.0\n", "            else:\n\t                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n\t                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n\t                w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n\t                lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\t                lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\t                lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\t        # iteration for all batches\n\t        multi_task_model.train()\n\t        train_dataset = iter(train_loader)\n", "        conf_mat = ConfMatrix(multi_task_model.class_nb)\n\t        for k in range(train_batch):\n\t            train_data, train_label, train_depth, train_normal = train_dataset.next()\n\t            train_data, train_label = train_data.to(device), train_label.long().to(device)\n\t            train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\t            train_pred, logsigma = multi_task_model(train_data)\n\t            optimizer.zero_grad()\n\t            train_loss = [\n\t                model_fit(train_pred[0], train_label, 'semantic'),\n\t                model_fit(train_pred[1], train_depth, 'depth'),\n", "                model_fit(train_pred[2], train_normal, 'normal')\n\t            ]\n\t            if opt.weight == 'equal' or opt.weight == 'dwa':\n\t                loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(3)])\n\t                #loss = sum([w[i] * train_loss[i] for i in range(3)])\n\t            else:\n\t                loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(3))\n\t            loss.backward()\n\t            optimizer.step()\n\t            # accumulate label prediction for every pixel in training images\n", "            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\t            cost[0] = train_loss[0].item()\n\t            cost[3] = train_loss[1].item()\n\t            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n\t            cost[6] = train_loss[2].item()\n\t            cost[7], cost[8], cost[9], cost[10], cost[11] = normal_error(train_pred[2], train_normal)\n\t            avg_cost[index, :12] += cost[:12] / train_batch\n\t        # compute mIoU and acc\n\t        avg_cost[index, 1:3] = conf_mat.get_metrics()\n\t        # evaluating test data\n", "        multi_task_model.eval()\n\t        conf_mat = ConfMatrix(multi_task_model.class_nb)\n\t        with torch.no_grad():  # operations inside don't track history\n\t            test_dataset = iter(test_loader)\n\t            for k in range(test_batch):\n\t                test_data, test_label, test_depth, test_normal = test_dataset.next()\n\t                test_data, test_label = test_data.to(device), test_label.long().to(device)\n\t                test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\t                test_pred, _ = multi_task_model(test_data)\n\t                test_loss = [\n", "                    model_fit(test_pred[0], test_label, 'semantic'),\n\t                    model_fit(test_pred[1], test_depth, 'depth'),\n\t                    model_fit(test_pred[2], test_normal, 'normal')\n\t                ]\n\t                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\t                cost[12] = test_loss[0].item()\n\t                cost[15] = test_loss[1].item()\n\t                cost[16], cost[17] = depth_error(test_pred[1], test_depth)\n\t                cost[18] = test_loss[2].item()\n\t                cost[19], cost[20], cost[21], cost[22], cost[23] = normal_error(test_pred[2], test_normal)\n", "                avg_cost[index, 12:] += cost[12:] / test_batch\n\t            # compute mIoU and acc\n\t            avg_cost[index, 13:15] = conf_mat.get_metrics()\n\t        scheduler.step()\n\t        epoch_end_time = time.time()\n\t        print(\n\t            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} ||'\n\t            'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} | {:.4f}'.\n\t            format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n\t                   avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n", "                   avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12],\n\t                   avg_cost[index, 13], avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16],\n\t                   avg_cost[index, 17], avg_cost[index, 18], avg_cost[index, 19], avg_cost[index, 20],\n\t                   avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23], epoch_end_time - epoch_start_time))\n\t    end_time = time.time()\n\t    print(\"Training time: \", end_time - start_time)\n\t\"\"\"\n\t=========== Universal Single-task Trainer ===========\n\t\"\"\"\n\tdef single_task_trainer(train_loader,\n", "                        test_loader,\n\t                        single_task_model,\n\t                        device,\n\t                        optimizer,\n\t                        scheduler,\n\t                        opt,\n\t                        total_epoch=200):\n\t    train_batch = len(train_loader)\n\t    test_batch = len(test_loader)\n\t    avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\n", "    for index in range(total_epoch):\n\t        cost = np.zeros(24, dtype=np.float32)\n\t        # iteration for all batches\n\t        single_task_model.train()\n\t        train_dataset = iter(train_loader)\n\t        conf_mat = ConfMatrix(single_task_model.class_nb)\n\t        for k in range(train_batch):\n\t            train_data, train_label, train_depth, train_normal = train_dataset.next()\n\t            train_data, train_label = train_data.to(device), train_label.long().to(device)\n\t            train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n", "            train_pred = single_task_model(train_data)\n\t            optimizer.zero_grad()\n\t            if opt.task == 'semantic':\n\t                train_loss = model_fit(train_pred, train_label, opt.task)\n\t                train_loss.backward()\n\t                optimizer.step()\n\t                conf_mat.update(train_pred.argmax(1).flatten(), train_label.flatten())\n\t                cost[0] = train_loss.item()\n\t            if opt.task == 'depth':\n\t                train_loss = model_fit(train_pred, train_depth, opt.task)\n", "                train_loss.backward()\n\t                optimizer.step()\n\t                cost[3] = train_loss.item()\n\t                cost[4], cost[5] = depth_error(train_pred, train_depth)\n\t            if opt.task == 'normal':\n\t                train_loss = model_fit(train_pred, train_normal, opt.task)\n\t                train_loss.backward()\n\t                optimizer.step()\n\t                cost[6] = train_loss.item()\n\t                cost[7], cost[8], cost[9], cost[10], cost[11] = normal_error(train_pred, train_normal)\n", "            avg_cost[index, :12] += cost[:12] / train_batch\n\t        if opt.task == 'semantic':\n\t            avg_cost[index, 1:3] = conf_mat.get_metrics()\n\t        # evaluating test data\n\t        single_task_model.eval()\n\t        conf_mat = ConfMatrix(single_task_model.class_nb)\n\t        with torch.no_grad():  # operations inside don't track history\n\t            test_dataset = iter(test_loader)\n\t            for k in range(test_batch):\n\t                test_data, test_label, test_depth, test_normal = test_dataset.next()\n", "                test_data, test_label = test_data.to(device), test_label.long().to(device)\n\t                test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\t                test_pred = single_task_model(test_data)\n\t                if opt.task == 'semantic':\n\t                    test_loss = model_fit(test_pred, test_label, opt.task)\n\t                    conf_mat.update(test_pred.argmax(1).flatten(), test_label.flatten())\n\t                    cost[12] = test_loss.item()\n\t                if opt.task == 'depth':\n\t                    test_loss = model_fit(test_pred, test_depth, opt.task)\n\t                    cost[15] = test_loss.item()\n", "                    cost[16], cost[17] = depth_error(test_pred, test_depth)\n\t                if opt.task == 'normal':\n\t                    test_loss = model_fit(test_pred, test_normal, opt.task)\n\t                    cost[18] = test_loss.item()\n\t                    cost[19], cost[20], cost[21], cost[22], cost[23] = normal_error(test_pred, test_normal)\n\t                avg_cost[index, 12:] += cost[12:] / test_batch\n\t            if opt.task == 'semantic':\n\t                avg_cost[index, 13:15] = conf_mat.get_metrics()\n\t        scheduler.step()\n\t        if opt.task == 'semantic':\n", "            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n\t                index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 12],\n\t                avg_cost[index, 13], avg_cost[index, 14]))\n\t        if opt.task == 'depth':\n\t            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n\t                index, avg_cost[index, 3], avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 15],\n\t                avg_cost[index, 16], avg_cost[index, 17]))\n\t        if opt.task == 'normal':\n\t            print(\n\t                'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f}'\n", "                .format(index, avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8], avg_cost[index, 9],\n\t                        avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 18], avg_cost[index, 19],\n\t                        avg_cost[index, 20], avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23]))\n\t''' ===== multi task MGD trainer ==== '''\n\tdef multi_task_mgd_trainer(train_loader,\n\t                           test_loader,\n\t                           multi_task_model,\n\t                           device,\n\t                           optimizer,\n\t                           scheduler,\n", "                           opt,\n\t                           total_epoch=200,\n\t                           method='sumloss',\n\t                           alpha=0.5,\n\t                           seed=0):\n\t    start_time = time.time()\n\t    niter = opt.niter\n\t    def graddrop(grads):\n\t        P = 0.5 * (1. + grads.sum(1) / (grads.abs().sum(1) + 1e-8))\n\t        U = torch.rand_like(grads[:, 0])\n", "        M = P.gt(U).view(-1, 1) * grads.gt(0) + P.lt(U).view(-1, 1) * grads.lt(0)\n\t        g = (grads * M.float()).mean(1)\n\t        return g\n\t    def mgd(grads):\n\t        grads_cpu = grads.t().cpu()\n\t        sol, min_norm = MinNormSolver.find_min_norm_element([grads_cpu[t] for t in range(grads.shape[-1])])\n\t        w = torch.FloatTensor(sol).to(grads.device)\n\t        g = grads.mm(w.view(-1, 1)).view(-1)\n\t        return g\n\t    def pcgrad(grads, rng):\n", "        grad_vec = grads.t()\n\t        num_tasks = 3\n\t        shuffled_task_indices = np.zeros((num_tasks, num_tasks - 1), dtype=int)\n\t        for i in range(num_tasks):\n\t            task_indices = np.arange(num_tasks)\n\t            task_indices[i] = task_indices[-1]\n\t            shuffled_task_indices[i] = task_indices[:-1]\n\t            rng.shuffle(shuffled_task_indices[i])\n\t        shuffled_task_indices = shuffled_task_indices.T\n\t        normalized_grad_vec = grad_vec / (grad_vec.norm(dim=1, keepdim=True) + 1e-8)  # num_tasks x dim\n", "        modified_grad_vec = deepcopy(grad_vec)\n\t        for task_indices in shuffled_task_indices:\n\t            normalized_shuffled_grad = normalized_grad_vec[task_indices]  # num_tasks x dim\n\t            dot = (modified_grad_vec * normalized_shuffled_grad).sum(dim=1, keepdim=True)  # num_tasks x dim\n\t            modified_grad_vec -= torch.clamp_max(dot, 0) * normalized_shuffled_grad\n\t        g = modified_grad_vec.mean(dim=0)\n\t        return g\n\t    def cagrad(grads, alpha=0.5, rescale=1):\n\t        GG = grads.t().mm(grads).cpu()  # [num_tasks, num_tasks]\n\t        g0_norm = (GG.mean() + 1e-8).sqrt()  # norm of the average gradient\n", "        x_start = np.ones(3) / 3\n\t        bnds = tuple((0, 1) for x in x_start)\n\t        cons = ({'type': 'eq', 'fun': lambda x: 1 - sum(x)})\n\t        A = GG.numpy()\n\t        b = x_start.copy()\n\t        c = (alpha * g0_norm + 1e-8).item()\n\t        def objfn(x):\n\t            return (x.reshape(1, 3).dot(A).dot(b.reshape(3, 1)) +\n\t                    c * np.sqrt(x.reshape(1, 3).dot(A).dot(x.reshape(3, 1)) + 1e-8)).sum()\n\t        res = minimize(objfn, x_start, bounds=bnds, constraints=cons)\n", "        w_cpu = res.x\n\t        ww = torch.Tensor(w_cpu).to(grads.device)\n\t        gw = (grads * ww.view(1, -1)).sum(1)\n\t        gw_norm = gw.norm()\n\t        lmbda = c / (gw_norm + 1e-8)\n\t        g = grads.mean(1) + lmbda * gw\n\t        if rescale == 0:\n\t            return g\n\t        elif rescale == 1:\n\t            return g / (1 + alpha**2)\n", "        else:\n\t            return g / (1 + alpha)\n\t    def sdmgrad(w, grads, alpha, niter=20):\n\t        GG = torch.mm(grads.t(), grads)\n\t        scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n\t        GG = GG / scale.pow(2)\n\t        Gg = torch.mean(GG, dim=1)\n\t        gg = torch.mean(Gg)\n\t        w.requires_grad = True\n\t        optimizer = torch.optim.SGD([w], lr=10, momentum=0.5)\n", "        for i in range(niter):\n\t            optimizer.zero_grad()\n\t            obj = torch.dot(w, torch.mv(GG, w)) + 2 * alpha * torch.dot(w, Gg) + alpha**2 * gg\n\t            obj.backward()\n\t            optimizer.step()\n\t            proj = euclidean_proj_simplex(w.data.cpu().numpy())\n\t            w.data.copy_(torch.from_numpy(proj).data)\n\t        w.requires_grad = False\n\t        g0 = torch.mean(grads, dim=1)\n\t        gw = torch.mv(grads, w)\n", "        g = (gw + alpha * g0) / (1 + alpha)\n\t        return g\n\t    def grad2vec(m, grads, grad_dims, task):\n\t        # store the gradients\n\t        grads[:, task].fill_(0.0)\n\t        cnt = 0\n\t        for mm in m.shared_modules():\n\t            for p in mm.parameters():\n\t                grad = p.grad\n\t                if grad is not None:\n", "                    grad_cur = grad.data.detach().clone()\n\t                    beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n\t                    en = sum(grad_dims[:cnt + 1])\n\t                    grads[beg:en, task].copy_(grad_cur.data.view(-1))\n\t                cnt += 1\n\t    def overwrite_grad(m, newgrad, grad_dims):\n\t        newgrad = newgrad * 3  # to match the sum loss\n\t        cnt = 0\n\t        for mm in m.shared_modules():\n\t            for param in mm.parameters():\n", "                beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n\t                en = sum(grad_dims[:cnt + 1])\n\t                this_grad = newgrad[beg:en].contiguous().view(param.data.size())\n\t                param.grad = this_grad.data.clone()\n\t                cnt += 1\n\t    rng = np.random.default_rng()\n\t    grad_dims = []\n\t    for mm in multi_task_model.shared_modules():\n\t        for param in mm.parameters():\n\t            grad_dims.append(param.data.numel())\n", "    grads = torch.Tensor(sum(grad_dims), 3).cuda()\n\t    w = 1 / 3 * torch.ones(3).cuda()\n\t    train_batch = len(train_loader)\n\t    test_batch = len(test_loader)\n\t    T = opt.temp\n\t    avg_cost = np.zeros([total_epoch, 24], dtype=np.float32)\n\t    lambda_weight = np.ones([3, total_epoch])\n\t    neg_trace = []\n\t    obj_trace = []\n\t    for index in range(total_epoch):\n", "        epoch_start_time = time.time()\n\t        cost = np.zeros(24, dtype=np.float32)\n\t        # apply Dynamic Weight Average\n\t        if opt.weight == 'dwa':\n\t            if index == 0 or index == 1:\n\t                lambda_weight[:, index] = 1.0\n\t            else:\n\t                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n\t                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n\t                w_3 = avg_cost[index - 1, 6] / avg_cost[index - 2, 6]\n", "                lambda_weight[0, index] = 3 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\t                lambda_weight[1, index] = 3 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\t                lambda_weight[2, index] = 3 * np.exp(w_3 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T) + np.exp(w_3 / T))\n\t        # iteration for all batches\n\t        multi_task_model.train()\n\t        train_dataset = iter(train_loader)\n\t        conf_mat = ConfMatrix(multi_task_model.class_nb)\n\t        for k in range(train_batch):\n\t            train_data, train_label, train_depth, train_normal = train_dataset.next()\n\t            train_data, train_label = train_data.to(device), train_label.long().to(device)\n", "            train_depth, train_normal = train_depth.to(device), train_normal.to(device)\n\t            train_pred, logsigma = multi_task_model(train_data)\n\t            train_loss = [\n\t                model_fit(train_pred[0], train_label, 'semantic'),\n\t                model_fit(train_pred[1], train_depth, 'depth'),\n\t                model_fit(train_pred[2], train_normal, 'normal')\n\t            ]\n\t            train_loss_tmp = [0, 0, 0]\n\t            if opt.weight == 'equal' or opt.weight == 'dwa':\n\t                for i in range(3):\n", "                    train_loss_tmp[i] = train_loss[i] * lambda_weight[i, index]\n\t            else:\n\t                for i in range(3):\n\t                    train_loss_tmp[i] = 1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2\n\t            optimizer.zero_grad()\n\t            if method == \"graddrop\":\n\t                for i in range(3):\n\t                    if i < 3:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n", "                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = graddrop(grads)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n\t            elif method == \"mgd\":\n\t                for i in range(3):\n\t                    if i < 3:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n", "                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = mgd(grads)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n\t            elif method == \"pcgrad\":\n\t                for i in range(3):\n\t                    if i < 3:\n", "                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = pcgrad(grads, rng)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n\t            elif method == \"cagrad\":\n\t                for i in range(3):\n", "                    if i < 3:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = cagrad(grads, alpha, rescale=1)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n\t            elif method == \"sdmgrad\":\n", "                for i in range(3):\n\t                    if i < 3:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = sdmgrad(w, grads, alpha, niter=niter)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n", "            # accumulate label prediction for every pixel in training images\n\t            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\t            cost[0] = train_loss[0].item()\n\t            cost[3] = train_loss[1].item()\n\t            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n\t            cost[6] = train_loss[2].item()\n\t            cost[7], cost[8], cost[9], cost[10], cost[11] = normal_error(train_pred[2], train_normal)\n\t            avg_cost[index, :12] += cost[:12] / train_batch\n\t        # compute mIoU and acc\n\t        avg_cost[index, 1:3] = conf_mat.get_metrics()\n", "        # evaluating test data\n\t        multi_task_model.eval()\n\t        conf_mat = ConfMatrix(multi_task_model.class_nb)\n\t        with torch.no_grad():  # operations inside don't track history\n\t            test_dataset = iter(test_loader)\n\t            for k in range(test_batch):\n\t                test_data, test_label, test_depth, test_normal = test_dataset.next()\n\t                test_data, test_label = test_data.to(device), test_label.long().to(device)\n\t                test_depth, test_normal = test_depth.to(device), test_normal.to(device)\n\t                test_pred, _ = multi_task_model(test_data)\n", "                test_loss = [\n\t                    model_fit(test_pred[0], test_label, 'semantic'),\n\t                    model_fit(test_pred[1], test_depth, 'depth'),\n\t                    model_fit(test_pred[2], test_normal, 'normal')\n\t                ]\n\t                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\t                cost[12] = test_loss[0].item()\n\t                cost[15] = test_loss[1].item()\n\t                cost[16], cost[17] = depth_error(test_pred[1], test_depth)\n\t                cost[18] = test_loss[2].item()\n", "                cost[19], cost[20], cost[21], cost[22], cost[23] = normal_error(test_pred[2], test_normal)\n\t                avg_cost[index, 12:] += cost[12:] / test_batch\n\t            # compute mIoU and acc\n\t            avg_cost[index, 13:15] = conf_mat.get_metrics()\n\t        scheduler.step()\n\t        if method == \"mean\":\n\t            torch.save(torch.Tensor(neg_trace), \"trace.pt\")\n\t        if \"debug\" in method:\n\t            torch.save(torch.Tensor(obj_trace), f\"{method}_obj.pt\")\n\t        epoch_end_time = time.time()\n", "        print(\n\t            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} ||'\n\t            'TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} {:.4f} | {:.4f}'.\n\t            format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n\t                   avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n\t                   avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], avg_cost[index, 12],\n\t                   avg_cost[index, 13], avg_cost[index, 14], avg_cost[index, 15], avg_cost[index, 16],\n\t                   avg_cost[index, 17], avg_cost[index, 18], avg_cost[index, 19], avg_cost[index, 20],\n\t                   avg_cost[index, 21], avg_cost[index, 22], avg_cost[index, 23], epoch_end_time - epoch_start_time))\n\t        if \"cagrad\" in method:\n", "            torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{alpha}-{seed}.pt\")\n\t        elif \"sdmgrad\" in method:\n\t            torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{alpha}-{seed}-{niter}.pt\")\n\t        else:\n\t            torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{seed}.pt\")\n\t    end_time = time.time()\n\t    print(\"Training time: \", end_time - start_time)\n"]}
{"filename": "nyuv2/min_norm_solvers.py", "chunked_list": ["# This code is from\n\t# Multi-Task Learning as Multi-Objective Optimization\n\t# Ozan Sener, Vladlen Koltun\n\t# Neural Information Processing Systems (NeurIPS) 2018\n\t# https://github.com/intel-isl/MultiObjectiveOptimization\n\timport numpy as np\n\timport torch\n\tclass MinNormSolver:\n\t    MAX_ITER = 20\n\t    STOP_CRIT = 1e-5\n", "    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n\t        \"\"\"\n\t        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n\t        d is the distance (objective) optimzed\n\t        v1v1 = <x1,x1>\n\t        v1v2 = <x1,x2>\n\t        v2v2 = <x2,x2>\n\t        \"\"\"\n\t        if v1v2 >= v1v1:\n\t            # Case: Fig 1, third column\n", "            gamma = 0.999\n\t            cost = v1v1\n\t            return gamma, cost\n\t        if v1v2 >= v2v2:\n\t            # Case: Fig 1, first column\n\t            gamma = 0.001\n\t            cost = v2v2\n\t            return gamma, cost\n\t        # Case: Fig 1, second column\n\t        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n", "        cost = v2v2 + gamma * (v1v2 - v2v2)\n\t        return gamma, cost\n\t    def _min_norm_2d(vecs, dps):\n\t        \"\"\"\n\t        Find the minimum norm solution as combination of two points\n\t        This is correct only in 2D\n\t        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n\t        \"\"\"\n\t        dmin = np.inf\n\t        for i in range(len(vecs)):\n", "            for j in range(i + 1, len(vecs)):\n\t                if (i, j) not in dps:\n\t                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n\t                    dps[(j, i)] = dps[(i, j)]\n\t                if (i, i) not in dps:\n\t                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n\t                if (j, j) not in dps:\n\t                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n\t                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n\t                if d < dmin:\n", "                    dmin = d\n\t                    sol = [(i, j), c, d]\n\t        return sol, dps\n\t    def _projection2simplex(y):\n\t        \"\"\"\n\t        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n\t        \"\"\"\n\t        m = len(y)\n\t        sorted_y = np.flip(np.sort(y), axis=0)\n\t        tmpsum = 0.0\n", "        tmax_f = (np.sum(y) - 1.0) / m\n\t        for i in range(m - 1):\n\t            tmpsum += sorted_y[i]\n\t            tmax = (tmpsum - 1) / (i + 1.0)\n\t            if tmax > sorted_y[i + 1]:\n\t                tmax_f = tmax\n\t                break\n\t        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\t    def _next_point(cur_val, grad, n):\n\t        proj_grad = grad - (np.sum(grad) / n)\n", "        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n\t        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\t        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n\t        t = 1\n\t        if len(tm1[tm1 > 1e-7]) > 0:\n\t            t = np.min(tm1[tm1 > 1e-7])\n\t        if len(tm2[tm2 > 1e-7]) > 0:\n\t            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\t        next_point = proj_grad * t + cur_val\n\t        next_point = MinNormSolver._projection2simplex(next_point)\n", "        return next_point\n\t    def find_min_norm_element(vecs):\n\t        \"\"\"\n\t        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n\t        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n\t        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n\t        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n\t        \"\"\"\n\t        # Solution lying at the combination of two points\n\t        dps = {}\n", "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\t        n = len(vecs)\n\t        sol_vec = np.zeros(n)\n\t        sol_vec[init_sol[0][0]] = init_sol[1]\n\t        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\t        if n < 3:\n\t            # This is optimal for n=2, so return the solution\n\t            return sol_vec, init_sol[2]\n\t        iter_count = 0\n\t        grad_mat = np.zeros((n, n))\n", "        for i in range(n):\n\t            for j in range(n):\n\t                grad_mat[i, j] = dps[(i, j)]\n\t        while iter_count < MinNormSolver.MAX_ITER:\n\t            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n\t            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n\t            # Re-compute the inner products for line search\n\t            v1v1 = 0.0\n\t            v1v2 = 0.0\n\t            v2v2 = 0.0\n", "            for i in range(n):\n\t                for j in range(n):\n\t                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n\t                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n\t                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n\t            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n\t            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n\t            change = new_sol_vec - sol_vec\n\t            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n\t                return sol_vec, nd\n", "            sol_vec = new_sol_vec\n\t    def find_min_norm_element_FW(vecs):\n\t        \"\"\"\n\t        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n\t        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n\t        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n\t        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n\t        \"\"\"\n\t        # Solution lying at the combination of two points\n\t        dps = {}\n", "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\t        n = len(vecs)\n\t        sol_vec = np.zeros(n)\n\t        sol_vec[init_sol[0][0]] = init_sol[1]\n\t        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\t        if n < 3:\n\t            # This is optimal for n=2, so return the solution\n\t            return sol_vec, init_sol[2]\n\t        iter_count = 0\n\t        grad_mat = np.zeros((n, n))\n", "        for i in range(n):\n\t            for j in range(n):\n\t                grad_mat[i, j] = dps[(i, j)]\n\t        while iter_count < MinNormSolver.MAX_ITER:\n\t            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\t            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n\t            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n\t            v2v2 = grad_mat[t_iter, t_iter]\n\t            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n\t            new_sol_vec = nc * sol_vec\n", "            new_sol_vec[t_iter] += 1 - nc\n\t            change = new_sol_vec - sol_vec\n\t            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n\t                return sol_vec, nd\n\t            sol_vec = new_sol_vec\n\tdef gradient_normalizers(grads, losses, normalization_type):\n\t    gn = {}\n\t    if normalization_type == 'l2':\n\t        for t in grads:\n\t            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n", "    elif normalization_type == 'loss':\n\t        for t in grads:\n\t            gn[t] = losses[t]\n\t    elif normalization_type == 'loss+':\n\t        for t in grads:\n\t            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n\t    elif normalization_type == 'none':\n\t        for t in grads:\n\t            gn[t] = 1.0\n\t    else:\n", "        print('ERROR: Invalid Normalization Type')\n\t    return gn\n"]}
{"filename": "nyuv2/model_segnet_cross.py", "chunked_list": ["import numpy as np\n\timport random\n\timport torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Cross')\n", "parser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n\tparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\n\tparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--seed', default=0, type=int, help='the seed')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n", "        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 13\n\t        # define encoder decoder layers\n\t        self.encoder_block_t = nn.ModuleList(\n\t            [nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])])\n\t        self.decoder_block_t = nn.ModuleList(\n\t            [nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)])])\n\t        for j in range(3):\n\t            if j < 2:\n\t                self.encoder_block_t.append(\n", "                    nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)]))\n\t                self.decoder_block_t.append(\n\t                    nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)]))\n\t            for i in range(4):\n\t                if i == 0:\n\t                    self.encoder_block_t[j].append(\n\t                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n\t                    self.decoder_block_t[j].append(\n\t                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n\t                else:\n", "                    self.encoder_block_t[j].append(\n\t                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n\t                    self.decoder_block_t[j].append(\n\t                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\t        # define cross-stitch units\n\t        self.cs_unit_encoder = nn.Parameter(data=torch.ones(4, 3))\n\t        self.cs_unit_decoder = nn.Parameter(data=torch.ones(5, 3))\n\t        # define task specific layers\n\t        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], bottle_neck=True, pred_layer=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], bottle_neck=True, pred_layer=True)\n", "        self.pred_task3 = self.conv_layer([filter[0], 3], bottle_neck=True, pred_layer=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_uniform_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n", "                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_uniform_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Parameter):\n\t                nn.init.constant(m.weight, 1)\n\t    def conv_layer(self, channel, bottle_neck, pred_layer=False):\n\t        if bottle_neck:\n\t            if not pred_layer:\n", "                conv_block = nn.Sequential(\n\t                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                    nn.BatchNorm2d(channel[1]),\n\t                    nn.ReLU(inplace=True),\n\t                    nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n\t                    nn.BatchNorm2d(channel[2]),\n\t                    nn.ReLU(inplace=True),\n\t                )\n\t            else:\n\t                conv_block = nn.Sequential(\n", "                    nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t                )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(channel[1]),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(channel[1]),\n", "                nn.ReLU(inplace=True),\n\t                nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(channel[2]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        return conv_block\n\t    def forward(self, x):\n\t        encoder_conv_t, decoder_conv_t, encoder_samp_t, decoder_samp_t, indices_t = ([0] * 3 for _ in range(5))\n\t        for i in range(3):\n\t            encoder_conv_t[i], decoder_conv_t[i], encoder_samp_t[i], decoder_samp_t[i], indices_t[i] = (\n", "                [0] * 5 for _ in range(5))\n\t        # task branch 1\n\t        for i in range(5):\n\t            for j in range(3):\n\t                if i == 0:\n\t                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](x)\n\t                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\t                else:\n\t                    encoder_cross_stitch = self.cs_unit_encoder[i - 1][0] * encoder_samp_t[0][i - 1] + \\\n\t                                           self.cs_unit_encoder[i - 1][1] * encoder_samp_t[1][i - 1] + \\\n", "                                           self.cs_unit_encoder[i - 1][2] * encoder_samp_t[2][i - 1]\n\t                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](encoder_cross_stitch)\n\t                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\t        for i in range(5):\n\t            for j in range(3):\n\t                if i == 0:\n\t                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * encoder_samp_t[0][-1] + \\\n\t                                           self.cs_unit_decoder[i][1] * encoder_samp_t[1][-1] + \\\n\t                                           self.cs_unit_decoder[i][2] * encoder_samp_t[2][-1]\n\t                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n", "                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\t                else:\n\t                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * decoder_conv_t[0][i - 1] + \\\n\t                                           self.cs_unit_decoder[i][1] * decoder_conv_t[1][i - 1] + \\\n\t                                           self.cs_unit_decoder[i][2] * decoder_conv_t[2][i - 1]\n\t                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n\t                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(decoder_conv_t[0][-1]), dim=1)\n\t        t2_pred = self.pred_task2(decoder_conv_t[1][-1])\n", "        t3_pred = self.pred_task3(decoder_conv_t[2][-1])\n\t        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\t# control seed\n\ttorch.backends.cudnn.enabled = False\n\ttorch.manual_seed(opt.seed)\n\tnp.random.seed(opt.seed)\n\trandom.seed(opt.seed)\n\ttorch.cuda.manual_seed_all(opt.seed)\n\t# define model, optimiser and scheduler\n", "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_CROSS = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_CROSS.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_CROSS),\n\t                                                         count_parameters(SegNet_CROSS) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n", "if opt.apply_augmentation:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation on NYUv2.')\n\telse:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\tnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\tbatch_size = 2\n\tnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\tnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n", "# Train and evaluate multi-task network\n\tmulti_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_CROSS, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "nyuv2/model_segnet_mt.py", "chunked_list": ["import numpy as np\n\timport random\n\timport torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Split')\n", "parser.add_argument('--type', default='standard', type=str, help='split type: standard, wide, deep')\n\tparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n\tparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\n\tparser.add_argument('--method', default='sdmgrad', type=str, help='optimization method')\n\tparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--alpha', default=0.3, type=float, help='the alpha')\n\tparser.add_argument('--lr', default=1e-4, type=float, help='the learning rate')\n\tparser.add_argument('--seed', default=1, type=int, help='the seed')\n\tparser.add_argument('--niter', default=20, type=int, help='number of inner iteration')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n", "opt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 13\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n", "        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n", "            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task attention layers\n\t        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n\t        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n\t        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n", "        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for j in range(3):\n\t            if j < 2:\n\t                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n\t                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n\t            for i in range(4):\n\t                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n\t                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\t        for i in range(4):\n\t            if i < 3:\n", "                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t            else:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n\t        self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n", "        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n", "                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def shared_modules(self):\n\t        return [\n\t            self.encoder_block,\n\t            self.decoder_block,\n\t            self.conv_block_enc,\n\t            self.conv_block_dec,\n\t            #self.encoder_att, self.decoder_att,\n\t            self.encoder_block_att,\n", "            self.decoder_block_att,\n\t            self.down_sampling,\n\t            self.up_sampling\n\t        ]\n\t    def zero_grad_shared_modules(self):\n\t        for mm in self.shared_modules():\n\t            mm.zero_grad()\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n", "                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n", "    def att_layer(self, channel):\n\t        att_block = nn.Sequential(\n\t            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[1]),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[2]),\n\t            nn.Sigmoid(),\n\t        )\n\t        return att_block\n", "    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define attention list for tasks\n\t        atten_encoder, atten_decoder = ([0] * 3 for _ in range(2))\n\t        for i in range(3):\n\t            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n\t        for i in range(3):\n\t            for j in range(5):\n", "                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\t        # define global shared network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n", "                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n", "        # define task dependent attention module\n\t        for i in range(3):\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t                else:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n", "                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n", "                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t                else:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n", "                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n\t        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n\t        t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n\t        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\tclass SegNetSplit(nn.Module):\n", "    def __init__(self):\n\t        super(SegNetSplit, self).__init__()\n\t        # initialise network parameters\n\t        if opt.type == 'wide':\n\t            filter = [64, 128, 256, 512, 1024]\n\t        else:\n\t            filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 13\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n", "        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n", "                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task specific layers\n\t        self.pred_task1 = nn.Sequential(\n\t            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n", "            nn.Conv2d(in_channels=filter[0], out_channels=self.class_nb, kernel_size=1, padding=0))\n\t        self.pred_task2 = nn.Sequential(\n\t            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n\t            nn.Conv2d(in_channels=filter[0], out_channels=1, kernel_size=1, padding=0))\n\t        self.pred_task3 = nn.Sequential(\n\t            nn.Conv2d(in_channels=filter[0], out_channels=filter[0], kernel_size=3, padding=1),\n\t            nn.Conv2d(in_channels=filter[0], out_channels=3, kernel_size=1, padding=0))\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n", "        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n", "                nn.init.constant_(m.bias, 0)\n\t    # define convolutional block\n\t    def conv_layer(self, channel):\n\t        if opt.type == 'deep':\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n", "                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]), nn.ReLU(inplace=True))\n\t        return conv_block\n\t    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n", "            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # global shared encoder-decoder network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n", "                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n", "        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(g_decoder[i][1]), dim=1)\n\t        t2_pred = self.pred_task2(g_decoder[i][1])\n\t        t3_pred = self.pred_task3(g_decoder[i][1])\n\t        t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred, t3_pred], self.logsigma\n\t# control seed\n\ttorch.backends.cudnn.enabled = False\n\ttorch.manual_seed(opt.seed)\n\tnp.random.seed(opt.seed)\n", "random.seed(opt.seed)\n\ttorch.cuda.manual_seed_all(opt.seed)\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_MTAN = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=opt.lr)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n\t                                                         count_parameters(SegNet_MTAN) / 24981069))\n\tprint(\n", "    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation on NYUv2.')\n\telse:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\tnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n", "batch_size = 2\n\tnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\tnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate multi-task network\n\tmulti_task_mgd_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200,\n\t                       opt.method, opt.alpha, opt.seed)\n"]}
{"filename": "nyuv2/create_dataset.py", "chunked_list": ["from torch.utils.data.dataset import Dataset\n\timport os\n\timport torch\n\timport torch.nn.functional as F\n\timport fnmatch\n\timport numpy as np\n\timport random\n\tclass RandomScaleCrop(object):\n\t    \"\"\"\n\t    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n", "    \"\"\"\n\t    def __init__(self, scale=[1.0, 1.2, 1.5]):\n\t        self.scale = scale\n\t    def __call__(self, img, label, depth, normal):\n\t        height, width = img.shape[-2:]\n\t        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n\t        h, w = int(height / sc), int(width / sc)\n\t        i = random.randint(0, height - h)\n\t        j = random.randint(0, width - w)\n\t        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n", "                             align_corners=True).squeeze(0)\n\t        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n\t                               mode='nearest').squeeze(0).squeeze(0)\n\t        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n\t        normal_ = F.interpolate(normal[None, :, i:i + h, j:j + w],\n\t                                size=(height, width),\n\t                                mode='bilinear',\n\t                                align_corners=True).squeeze(0)\n\t        return img_, label_, depth_ / sc, normal_\n\tclass NYUv2(Dataset):\n", "    \"\"\"\n\t    We could further improve the performance with the data augmentation of NYUv2 defined in:\n\t        [1] PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing\n\t        [2] Pattern affinitive propagation across depth, surface normal and semantic segmentation\n\t        [3] Mti-net: Multiscale task interaction networks for multi-task learning\n\t        1. Random scale in a selected raio 1.0, 1.2, and 1.5.\n\t        2. Random horizontal flip.\n\t    Please note that: all baselines and MTAN did NOT apply data augmentation in the original paper.\n\t    \"\"\"\n\t    def __init__(self, root, train=True, augmentation=False):\n", "        self.train = train\n\t        self.root = os.path.expanduser(root)\n\t        self.augmentation = augmentation\n\t        # read the data file\n\t        if train:\n\t            self.data_path = root + '/train'\n\t        else:\n\t            self.data_path = root + '/val'\n\t        # calculate data length\n\t        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n", "    def __getitem__(self, index):\n\t        # load data from the pre-processed npy files\n\t        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n\t        semantic = torch.from_numpy(np.load(self.data_path + '/label/{:d}.npy'.format(index)))\n\t        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n\t        normal = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/normal/{:d}.npy'.format(index)), -1, 0))\n\t        # apply data augmentation if required\n\t        if self.augmentation:\n\t            image, semantic, depth, normal = RandomScaleCrop()(image, semantic, depth, normal)\n\t            if torch.rand(1) < 0.5:\n", "                image = torch.flip(image, dims=[2])\n\t                semantic = torch.flip(semantic, dims=[1])\n\t                depth = torch.flip(depth, dims=[2])\n\t                normal = torch.flip(normal, dims=[2])\n\t                normal[0, :, :] = -normal[0, :, :]\n\t        return image.float(), semantic.float(), depth.float(), normal.float()\n\t    def __len__(self):\n\t        return self.data_len\n"]}
{"filename": "nyuv2/model_segnet_single.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Single-task: One Task')\n\tparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')\n\tparser.add_argument('--dataroot', default='nyuv2', type=str, help='dataset root')\n\tparser.add_argument('--seed', default=0, type=int, help='the seed')\n", "parser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 13\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n", "        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n", "                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        if opt.task == 'semantic':\n\t            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        if opt.task == 'depth':\n", "            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n\t        if opt.task == 'normal':\n\t            self.pred_task = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n", "            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n", "                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n\t    def forward(self, x):\n", "        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define global shared network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n", "                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n", "                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t        # define task prediction layers\n\t        if opt.task == 'semantic':\n\t            pred = F.log_softmax(self.pred_task(g_decoder[-1][-1]), dim=1)\n\t        if opt.task == 'depth':\n\t            pred = self.pred_task(g_decoder[-1][-1])\n\t        if opt.task == 'normal':\n\t            pred = self.pred_task(g_decoder[-1][-1])\n\t            pred = pred / torch.norm(pred, p=2, dim=1, keepdim=True)\n", "        return pred\n\t# control seed\n\ttorch.backends.cudnn.enabled = False\n\ttorch.manual_seed(opt.seed)\n\tnp.random.seed(opt.seed)\n\trandom.seed(opt.seed)\n\ttorch.cuda.manual_seed_all(opt.seed)\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet = SegNet().to(device)\n", "optimizer = optim.Adam(SegNet.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet), count_parameters(SegNet) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation on NYUv2.')\n", "else:\n\t    nyuv2_train_set = NYUv2(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\tnyuv2_test_set = NYUv2(root=dataset_path, train=False)\n\tbatch_size = 2\n\tnyuv2_train_loader = torch.utils.data.DataLoader(dataset=nyuv2_train_set, batch_size=batch_size, shuffle=True)\n\tnyuv2_test_loader = torch.utils.data.DataLoader(dataset=nyuv2_test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate single-task network\n\tsingle_task_trainer(nyuv2_train_loader, nyuv2_test_loader, SegNet, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "consistency/train.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.utils.data\n\tfrom torch import linalg as LA\n\tfrom torch.autograd import Variable\n\tfrom model_lenet import RegressionModel, RegressionTrain\n\tfrom model_resnet import MnistResNet, RegressionTrainResNet\n\tfrom utils import *\n\timport pickle\n\timport argparse\n", "parser = argparse.ArgumentParser(description='Multi-Fashion-MNIST')\n\tparser.add_argument('--base', default='lenet', type=str, help='base model')\n\tparser.add_argument('--solver', default='sdmgrad', type=str, help='which optimization algorithm to use')\n\tparser.add_argument('--alpha', default=0.5, type=float, help='the alpha used in cagrad')\n\tparser.add_argument('--lmbda', default=0.5, type=float, help='the lmbda used in sdmgrad')\n\tparser.add_argument('--seed', default=0, type=int, help='the seed')\n\tparser.add_argument('--niter', default=100, type=int, help='step of (outer) iteration')\n\tparser.add_argument('--initer', default=20, type=int, help='step of inner itration')\n\targs = parser.parse_args()\n\ttorch.manual_seed(args.seed)\n", "np.random.seed(args.seed)\n\ttorch.cuda.manual_seed_all(args.seed)\n\tdef train(dataset, base_model, solver, alpha, lmbda, niter, initer):\n\t    # generate #npref preference vectors\n\t    n_tasks = 2\n\t    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\t    # load dataset\n\t    # MultiMNIST: multi_mnist.pickle\n\t    if dataset == 'mnist':\n\t        with open('./data/multi_mnist.pickle', 'rb') as f:\n", "            trainX, trainLabel, testX, testLabel = pickle.load(f)\n\t    # MultiFashionMNIST: multi_fashion.pickle\n\t    if dataset == 'fashion':\n\t        with open('./data/multi_fashion.pickle', 'rb') as f:\n\t            trainX, trainLabel, testX, testLabel = pickle.load(f)\n\t    # Multi-(Fashion+MNIST): multi_fashion_and_mnist.pickle\n\t    if dataset == 'fashion_and_mnist':\n\t        with open('./data/multi_fashion_and_mnist.pickle', 'rb') as f:\n\t            trainX, trainLabel, testX, testLabel = pickle.load(f)\n\t    trainX = torch.from_numpy(trainX.reshape(120000, 1, 36, 36)).float()\n", "    trainLabel = torch.from_numpy(trainLabel).long()\n\t    testX = torch.from_numpy(testX.reshape(20000, 1, 36, 36)).float()\n\t    testLabel = torch.from_numpy(testLabel).long()\n\t    train_set = torch.utils.data.TensorDataset(trainX, trainLabel)\n\t    test_set = torch.utils.data.TensorDataset(testX, testLabel)\n\t    batch_size = 256\n\t    train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\t    test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\t    print('==>>> total trainning batch number: {}'.format(len(train_loader)))\n\t    print('==>>> total testing batch number: {}'.format(len(test_loader)))\n", "    # define the base model for ParetoMTL\n\t    if base_model == 'lenet':\n\t        model = RegressionModel(n_tasks).to(device)\n\t    if base_model == 'resnet18':\n\t        model = MnistResNet(n_tasks).to(device)\n\t    # choose different optimizer for different base model\n\t    if base_model == 'lenet':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n\t        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[15, 30, 45, 60, 75, 90], gamma=0.5)\n\t    if base_model == 'resnet18':\n", "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\t        scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20], gamma=0.1)\n\t    # store infomation during optimization\n\t    task_train_losses = []\n\t    train_accs = []\n\t    # grad\n\t    grad_dims = []\n\t    for mm in model.shared_modules():\n\t        for param in mm.parameters():\n\t            grad_dims.append(param.data.numel())\n", "    grads = torch.Tensor(sum(grad_dims), n_tasks).to(device)\n\t    w = torch.ones(n_tasks).to(device) / n_tasks\n\t    # run niter epochs\n\t    for t in range(niter):\n\t        model.train()\n\t        for it, (X, ts) in enumerate(train_loader):\n\t            X, ts = X.to(device), ts.to(device)\n\t            optimizer.zero_grad()\n\t            # compute stochastic gradient\n\t            task_loss = model.forward_loss(X, ts)\n", "            # \\nabla F, grads [n_model, n_tasks]\n\t            for i in range(n_tasks):\n\t                if i == 0:\n\t                    task_loss[i].backward(retain_graph=True)\n\t                else:\n\t                    task_loss[i].backward()\n\t                grad2vec(model, grads, grad_dims, i)\n\t                model.zero_grad_shared_modules()\n\t            if solver == 'cagrad':\n\t                g = cagrad(grads, alpha, rescale=1)\n", "            elif solver == 'mgd':\n\t                g = mgd(grads)\n\t            elif solver == 'sgd':\n\t                g = mean_grad(grads)\n\t            elif solver == 'sdmgrad':\n\t                g = sdmgrad(w, grads, lmbda, initer)\n\t            else:\n\t                raise ValueError('Not supported solver.')\n\t            overwrite_grad(model, g, grad_dims)\n\t            # optimization step\n", "            optimizer.step()\n\t            scheduler.step()\n\t        # calculate and record performance\n\t        if t == 0 or (t + 1) % 2 == 0:\n\t            model.eval()\n\t            with torch.no_grad():\n\t                total_train_loss = []\n\t                train_acc = []\n\t                correct1_train = 0\n\t                correct2_train = 0\n", "                for it, (X, ts) in enumerate(train_loader):\n\t                    X, ts = X.to(device), ts.to(device)\n\t                    valid_train_loss = model.forward_loss(X, ts)\n\t                    total_train_loss.append(valid_train_loss)\n\t                    output1 = model(X).max(2, keepdim=True)[1][:, 0]\n\t                    output2 = model(X).max(2, keepdim=True)[1][:, 1]\n\t                    correct1_train += output1.eq(ts[:, 0].view_as(output1)).sum().item()\n\t                    correct2_train += output2.eq(ts[:, 1].view_as(output2)).sum().item()\n\t                train_acc = np.stack([\n\t                    1.0 * correct1_train / len(train_loader.dataset), 1.0 * correct2_train / len(train_loader.dataset)\n", "                ])\n\t                total_train_loss = torch.stack(total_train_loss)\n\t                average_train_loss = torch.mean(total_train_loss, dim=0)\n\t            # record and print\n\t            task_train_losses.append(average_train_loss.data.cpu().numpy())\n\t            train_accs.append(train_acc)\n\t            print('{}/{}: train_loss={}, train_acc={}'.format(t + 1, niter, task_train_losses[-1], train_accs[-1]))\n\t    save_path = './saved_model/%s_%s_solver_%s_niter_%d_seed_%d.pickle' % (dataset, base_model, solver, niter,\n\t                                                                           args.seed)\n\t    torch.save(model.state_dict(), save_path)\n", "def run(dataset='mnist', base_model='lenet', solver='sdmgrad', alpha=0.5, lmbda=0.5, niter=100, initer=20):\n\t    \"\"\"\n\t    run stochatic moo algorithms\n\t    \"\"\"\n\t    train(dataset, base_model, solver, alpha, lmbda, niter, initer)\n\trun(dataset='fashion_and_mnist',\n\t    base_model=args.base,\n\t    solver=args.solver,\n\t    alpha=args.alpha,\n\t    lmbda=args.lmbda,\n", "    niter=args.niter,\n\t    initer=args.initer)\n"]}
{"filename": "consistency/utils.py", "chunked_list": ["import numpy as np\n\tfrom min_norm_solvers import MinNormSolver\n\tfrom scipy.optimize import minimize, Bounds, minimize_scalar\n\timport torch\n\tfrom torch import linalg as LA\n\tfrom torch.nn import functional as F\n\tdef euclidean_proj_simplex(v, s=1):\n\t    \"\"\" Compute the Euclidean projection on a positive simplex\n\t    Solves the optimisation problem (using the algorithm from [1]):\n\t        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0\n", "    Parameters\n\t    ----------\n\t    v: (n,) numpy array,\n\t       n-dimensional vector to project\n\t    s: int, optional, default: 1,\n\t       radius of the simplex\n\t    Returns\n\t    -------\n\t    w: (n,) numpy array,\n\t       Euclidean projection of v on the simplex\n", "    Notes\n\t    -----\n\t    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n\t    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n\t    However, this implementation still easily scales to millions of dimensions.\n\t    References\n\t    ----------\n\t    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n\t        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n\t        International Conference on Machine Learning (ICML 2008)\n", "        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n\t    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n\t        Weiran Wang, Miguel . Carreira-Perpin. arXiv:1309.1541\n\t        https://arxiv.org/pdf/1309.1541.pdf\n\t    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n\t    \"\"\"\n\t    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n\t    v = v.astype(np.float64)\n\t    n, = v.shape  # will raise ValueError if v is not 1-D\n\t    # check if we are already on the simplex\n", "    if v.sum() == s and np.alltrue(v >= 0):\n\t        # best projection: itself!\n\t        return v\n\t    # get the array of cumulative sums of a sorted (decreasing) copy of v\n\t    u = np.sort(v)[::-1]\n\t    cssv = np.cumsum(u)\n\t    # get the number of > 0 components of the optimal solution\n\t    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n\t    # compute the Lagrange multiplier associated to the simplex constraint\n\t    theta = float(cssv[rho] - s) / (rho + 1)\n", "    # compute the projection by thresholding v using theta\n\t    w = (v - theta).clip(min=0)\n\t    return w\n\tdef grad2vec(m, grads, grad_dims, task):\n\t    # store the gradients\n\t    grads[:, task].fill_(0.0)\n\t    cnt = 0\n\t    for mm in m.shared_modules():\n\t        for p in mm.parameters():\n\t            grad = p.grad\n", "            if grad is not None:\n\t                grad_cur = grad.data.detach().clone()\n\t                beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n\t                en = sum(grad_dims[:cnt + 1])\n\t                grads[beg:en, task].copy_(grad_cur.data.view(-1))\n\t            cnt += 1\n\tdef overwrite_grad(m, newgrad, grad_dims):\n\t    # newgrad = newgrad * 2 # to match the sum loss\n\t    cnt = 0\n\t    for mm in m.shared_modules():\n", "        for param in mm.parameters():\n\t            beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n\t            en = sum(grad_dims[:cnt + 1])\n\t            this_grad = newgrad[beg:en].contiguous().view(param.data.size())\n\t            param.grad = this_grad.data.clone()\n\t            cnt += 1\n\tdef mean_grad(grads):\n\t    return grads.mean(1)\n\tdef mgd(grads):\n\t    grads_cpu = grads.t().cpu()\n", "    sol, min_norm = MinNormSolver.find_min_norm_element([grads_cpu[t] for t in range(grads.shape[-1])])\n\t    w = torch.FloatTensor(sol).to(grads.device)\n\t    g = grads.mm(w.view(-1, 1)).view(-1)\n\t    return g\n\tdef cagrad(grads, alpha=0.5, rescale=0):\n\t    g1 = grads[:, 0]\n\t    g2 = grads[:, 1]\n\t    g11 = g1.dot(g1).item()\n\t    g12 = g1.dot(g2).item()\n\t    g22 = g2.dot(g2).item()\n", "    g0_norm = 0.5 * np.sqrt(g11 + g22 + 2 * g12)\n\t    # want to minimize g_w^Tg_0 + c*||g_0||*||g_w||\n\t    coef = alpha * g0_norm\n\t    def obj(x):\n\t        # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n\t        # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n\t        return coef * np.sqrt(x**2 * (g11 + g22 - 2 * g12) + 2 * x * (g12 - g22) + g22 +\n\t                              1e-8) + 0.5 * x * (g11 + g22 - 2 * g12) + (0.5 + x) * (g12 - g22) + g22\n\t    res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n\t    x = res.x\n", "    gw_norm = np.sqrt(x**2 * g11 + (1 - x)**2 * g22 + 2 * x * (1 - x) * g12 + 1e-8)\n\t    lmbda = coef / (gw_norm + 1e-8)\n\t    g = (0.5 + lmbda * x) * g1 + (0.5 + lmbda * (1 - x)) * g2  # g0 + lmbda*gw\n\t    if rescale == 0:\n\t        return g\n\t    elif rescale == 1:\n\t        return g / (1 + alpha**2)\n\t    else:\n\t        return g / (1 + alpha)\n\tdef sdmgrad(w, grads, lmbda, niter=20):\n", "    \"\"\"\n\t    our proposed sdmgrad\n\t    \"\"\"\n\t    GG = torch.mm(grads.t(), grads)\n\t    scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n\t    GG = GG / scale.pow(2)\n\t    Gg = torch.mean(GG, dim=1)\n\t    gg = torch.mean(Gg)\n\t    w.requires_grad = True\n\t    optimizer = torch.optim.SGD([w], lr=10, momentum=0.5)\n", "    for i in range(niter):\n\t        optimizer.zero_grad()\n\t        obj = torch.dot(w, torch.mv(GG, w)) + 2 * lmbda * torch.dot(w, Gg) + lmbda**2 * gg\n\t        obj.backward()\n\t        optimizer.step()\n\t        proj = euclidean_proj_simplex(w.data.cpu().numpy())\n\t        w.data.copy_(torch.from_numpy(proj).data)\n\t    w.requires_grad = False\n\t    g0 = torch.mean(grads, dim=1)\n\t    gw = torch.mv(grads, w)\n", "    g = (gw + lmbda * g0) / (1 + lmbda)\n"]}
{"filename": "consistency/min_norm_solvers.py", "chunked_list": ["# This code is from\n\t# Multi-Task Learning as Multi-Objective Optimization\n\t# Ozan Sener, Vladlen Koltun\n\t# Neural Information Processing Systems (NeurIPS) 2018\n\t# https://github.com/intel-isl/MultiObjectiveOptimization\n\timport numpy as np\n\timport torch\n\tclass MinNormSolver:\n\t    MAX_ITER = 20\n\t    STOP_CRIT = 1e-5\n", "    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n\t        \"\"\"\n\t        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n\t        d is the distance (objective) optimzed\n\t        v1v1 = <x1,x1>\n\t        v1v2 = <x1,x2>\n\t        v2v2 = <x2,x2>\n\t        \"\"\"\n\t        if v1v2 >= v1v1:\n\t            # Case: Fig 1, third column\n", "            gamma = 0.999\n\t            cost = v1v1\n\t            return gamma, cost\n\t        if v1v2 >= v2v2:\n\t            # Case: Fig 1, first column\n\t            gamma = 0.001\n\t            cost = v2v2\n\t            return gamma, cost\n\t        # Case: Fig 1, second column\n\t        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n", "        cost = v2v2 + gamma * (v1v2 - v2v2)\n\t        return gamma, cost\n\t    def _min_norm_2d(vecs, dps):\n\t        \"\"\"\n\t        Find the minimum norm solution as combination of two points\n\t        This is correct only in 2D\n\t        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n\t        \"\"\"\n\t        dmin = np.inf\n\t        for i in range(len(vecs)):\n", "            for j in range(i + 1, len(vecs)):\n\t                if (i, j) not in dps:\n\t                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n\t                    dps[(j, i)] = dps[(i, j)]\n\t                if (i, i) not in dps:\n\t                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n\t                if (j, j) not in dps:\n\t                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n\t                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n\t                if d < dmin:\n", "                    dmin = d\n\t                    sol = [(i, j), c, d]\n\t        return sol, dps\n\t    def _projection2simplex(y):\n\t        \"\"\"\n\t        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n\t        \"\"\"\n\t        m = len(y)\n\t        sorted_y = np.flip(np.sort(y), axis=0)\n\t        tmpsum = 0.0\n", "        tmax_f = (np.sum(y) - 1.0) / m\n\t        for i in range(m - 1):\n\t            tmpsum += sorted_y[i]\n\t            tmax = (tmpsum - 1) / (i + 1.0)\n\t            if tmax > sorted_y[i + 1]:\n\t                tmax_f = tmax\n\t                break\n\t        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\t    def _next_point(cur_val, grad, n):\n\t        proj_grad = grad - (np.sum(grad) / n)\n", "        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n\t        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\t        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n\t        t = 1\n\t        if len(tm1[tm1 > 1e-7]) > 0:\n\t            t = np.min(tm1[tm1 > 1e-7])\n\t        if len(tm2[tm2 > 1e-7]) > 0:\n\t            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\t        next_point = proj_grad * t + cur_val\n\t        next_point = MinNormSolver._projection2simplex(next_point)\n", "        return next_point\n\t    def find_min_norm_element(vecs):\n\t        \"\"\"\n\t        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n\t        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n\t        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n\t        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n\t        \"\"\"\n\t        # Solution lying at the combination of two points\n\t        dps = {}\n", "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\t        n = len(vecs)\n\t        sol_vec = np.zeros(n)\n\t        sol_vec[init_sol[0][0]] = init_sol[1]\n\t        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\t        if n < 3:\n\t            # This is optimal for n=2, so return the solution\n\t            return sol_vec, init_sol[2]\n\t        iter_count = 0\n\t        grad_mat = np.zeros((n, n))\n", "        for i in range(n):\n\t            for j in range(n):\n\t                grad_mat[i, j] = dps[(i, j)]\n\t        while iter_count < MinNormSolver.MAX_ITER:\n\t            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n\t            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n\t            # Re-compute the inner products for line search\n\t            v1v1 = 0.0\n\t            v1v2 = 0.0\n\t            v2v2 = 0.0\n", "            for i in range(n):\n\t                for j in range(n):\n\t                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n\t                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n\t                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n\t            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n\t            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n\t            change = new_sol_vec - sol_vec\n\t            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n\t                return sol_vec, nd\n", "            sol_vec = new_sol_vec\n\t    def find_min_norm_element_FW(vecs):\n\t        \"\"\"\n\t        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n\t        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n\t        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n\t        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n\t        \"\"\"\n\t        # Solution lying at the combination of two points\n\t        dps = {}\n", "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\t        n = len(vecs)\n\t        sol_vec = np.zeros(n)\n\t        sol_vec[init_sol[0][0]] = init_sol[1]\n\t        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\t        if n < 3:\n\t            # This is optimal for n=2, so return the solution\n\t            return sol_vec, init_sol[2]\n\t        iter_count = 0\n\t        grad_mat = np.zeros((n, n))\n", "        for i in range(n):\n\t            for j in range(n):\n\t                grad_mat[i, j] = dps[(i, j)]\n\t        while iter_count < MinNormSolver.MAX_ITER:\n\t            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\t            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n\t            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n\t            v2v2 = grad_mat[t_iter, t_iter]\n\t            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n\t            new_sol_vec = nc * sol_vec\n", "            new_sol_vec[t_iter] += 1 - nc\n\t            change = new_sol_vec - sol_vec\n\t            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n\t                return sol_vec, nd\n\t            sol_vec = new_sol_vec\n\tdef gradient_normalizers(grads, losses, normalization_type):\n\t    gn = {}\n\t    if normalization_type == 'l2':\n\t        for t in grads:\n\t            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n", "    elif normalization_type == 'loss':\n\t        for t in grads:\n\t            gn[t] = losses[t]\n\t    elif normalization_type == 'loss+':\n\t        for t in grads:\n\t            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n\t    elif normalization_type == 'none':\n\t        for t in grads:\n\t            gn[t] = 1.0\n\t    else:\n", "        print('ERROR: Invalid Normalization Type')\n\t    return gn(base)\n"]}
{"filename": "consistency/model_lenet.py", "chunked_list": ["# lenet base model for Pareto MTL\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.nn.modules.loss import CrossEntropyLoss\n\tclass RegressionTrain(torch.nn.Module):\n\t    def __init__(self, model, init_weight):\n\t        super(RegressionTrain, self).__init__()\n\t        self.model = model\n\t        self.weights = torch.nn.Parameter(torch.from_numpy(init_weight).float())\n", "        self.ce_loss = CrossEntropyLoss()\n\t    def forward(self, x, ts):\n\t        n_tasks = 2\n\t        ys = self.model(x)\n\t        task_loss = []\n\t        for i in range(n_tasks):\n\t            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n\t        task_loss = torch.stack(task_loss)\n\t        return task_loss\n\tclass RegressionModel(torch.nn.Module):\n", "    def __init__(self, n_tasks):\n\t        super(RegressionModel, self).__init__()\n\t        self.n_tasks = n_tasks\n\t        self.conv1 = nn.Conv2d(1, 10, 9, 1)\n\t        self.conv2 = nn.Conv2d(10, 20, 5, 1)\n\t        self.fc1 = nn.Linear(5 * 5 * 20, 50)\n\t        self.ce_loss = CrossEntropyLoss()\n\t        for i in range(self.n_tasks):\n\t            setattr(self, 'task_{}'.format(i), nn.Linear(50, 10))\n\t    def shared_modules(self):\n", "        return [self.conv1, self.conv2, self.fc1]\n\t    def zero_grad_shared_modules(self):\n\t        for mm in self.shared_modules():\n\t            mm.zero_grad()\n\t    def forward(self, x):\n\t        x = F.relu(self.conv1(x))\n\t        x = F.max_pool2d(x, 2, 2)\n\t        x = F.relu(self.conv2(x))\n\t        x = F.max_pool2d(x, 2, 2)\n\t        x = x.view(-1, 5 * 5 * 20)\n", "        x = F.relu(self.fc1(x))\n\t        outs = []\n\t        for i in range(self.n_tasks):\n\t            layer = getattr(self, 'task_{}'.format(i))\n\t            outs.append(layer(x))\n\t        return torch.stack(outs, dim=1)\n\t    def forward_loss(self, x, ts):\n\t        ys = self.forward(x)\n\t        task_loss = []\n\t        for i in range(self.n_tasks):\n", "            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n\t        task_loss = torch.stack(task_loss)\n\t        return task_loss\n"]}
{"filename": "consistency/model_resnet.py", "chunked_list": ["# resnet18 base model for Pareto MTL\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.nn.modules.loss import CrossEntropyLoss\n\tfrom torchvision import models\n\tclass RegressionTrainResNet(torch.nn.Module):\n\t    def __init__(self, model, init_weight):\n\t        super(RegressionTrainResNet, self).__init__()\n\t        self.model = model\n", "        self.weights = torch.nn.Parameter(torch.from_numpy(init_weight).float())\n\t        self.ce_loss = CrossEntropyLoss()\n\t    def forward(self, x, ts):\n\t        n_tasks = 2\n\t        ys = self.model(x)\n\t        task_loss = []\n\t        for i in range(n_tasks):\n\t            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n\t        task_loss = torch.stack(task_loss)\n\t        return task_loss\n", "class MnistResNet(torch.nn.Module):\n\t    def __init__(self, n_tasks):\n\t        super(MnistResNet, self).__init__()\n\t        self.n_tasks = n_tasks\n\t        self.feature_extractor = models.resnet18(pretrained=False)\n\t        self.feature_extractor.conv1 = torch.nn.Conv2d(1,\n\t                                                       64,\n\t                                                       kernel_size=(7, 7),\n\t                                                       stride=(2, 2),\n\t                                                       padding=(3, 3),\n", "                                                       bias=False)\n\t        fc_in_features = self.feature_extractor.fc.in_features\n\t        self.feature_extractor.fc = torch.nn.Linear(fc_in_features, 100)\n\t        self.ce_loss = CrossEntropyLoss()\n\t        for i in range(self.n_tasks):\n\t            setattr(self, 'task_{}'.format(i), nn.Linear(100, 10))\n\t    def shared_modules(self):\n\t        return [self.feature_extractor]\n\t    def zero_grad_shared_modules(self):\n\t        for mm in self.shared_modules():\n", "            mm.zero_grad()\n\t    def forward(self, x):\n\t        x = F.relu(self.feature_extractor(x))\n\t        outs = []\n\t        for i in range(self.n_tasks):\n\t            layer = getattr(self, 'task_{}'.format(i))\n\t            outs.append(layer(x))\n\t        return torch.stack(outs, dim=1)\n\t    def forward_loss(self, x, ts):\n\t        ys = self.forward(x)\n", "        task_loss = []\n\t        for i in range(self.n_tasks):\n\t            task_loss.append(self.ce_loss(ys[:, i], ts[:, i]))\n\t        task_loss = torch.stack(task_loss)\n\t        return task_loss\n"]}
{"filename": "mtrl/mtrl_files/sdmgrad.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\tfrom copy import deepcopy\n\tfrom typing import Iterable, List, Optional, Tuple\n\timport numpy as np\n\timport time\n\timport torch\n\tfrom omegaconf import OmegaConf\n\tfrom mtrl.agent import grad_manipulation as grad_manipulation_agent\n\tfrom mtrl.utils.types import ConfigType, TensorType\n\t#from mtrl.agent.mgda import MinNormSolver\n", "def euclidean_proj_simplex(v, s=1):\n\t    \"\"\" Compute the Euclidean projection on a positive simplex\n\t    Solves the optimisation problem (using the algorithm from [1]):\n\t        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n\t    Parameters\n\t    ----------\n\t    v: (n,) numpy array,\n\t       n-dimensional vector to project\n\t    s: int, optional, default: 1,\n\t       radius of the simplex\n", "    Returns\n\t    -------\n\t    w: (n,) numpy array,\n\t       Euclidean projection of v on the simplex\n\t    Notes\n\t    -----\n\t    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n\t    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n\t    However, this implementation still easily scales to millions of dimensions.\n\t    References\n", "    ----------\n\t    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n\t        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n\t        International Conference on Machine Learning (ICML 2008)\n\t        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n\t    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n\t        Weiran Wang, Miguel . Carreira-Perpin. arXiv:1309.1541\n\t        https://arxiv.org/pdf/1309.1541.pdf\n\t    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n\t    \"\"\"\n", "    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n\t    v = v.astype(np.float64)\n\t    n, = v.shape  # will raise ValueError if v is not 1-D\n\t    # check if we are already on the simplex\n\t    if v.sum() == s and np.alltrue(v >= 0):\n\t        # best projection: itself!\n\t        return v\n\t    # get the array of cumulative sums of a sorted (decreasing) copy of v\n\t    u = np.sort(v)[::-1]\n\t    cssv = np.cumsum(u)\n", "    # get the number of > 0 components of the optimal solution\n\t    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n\t    # compute the Lagrange multiplier associated to the simplex constraint\n\t    theta = float(cssv[rho] - s) / (rho + 1)\n\t    # compute the projection by thresholding v using theta\n\t    w = (v - theta).clip(min=0)\n\t    return w\n\tdef _check_param_device(param: TensorType, old_param_device: Optional[int]) -> int:\n\t    \"\"\"This helper function is to check if the parameters are located\n\t        in the same device. Currently, the conversion between model parameters\n", "        and single vector form is not supported for multiple allocations,\n\t        e.g. parameters in different GPUs, or mixture of CPU/GPU.\n\t        The implementation is taken from: https://github.com/pytorch/pytorch/blob/22a34bcf4e5eaa348f0117c414c3dd760ec64b13/torch/nn/utils/convert_parameters.py#L57\n\t    Args:\n\t        param ([TensorType]): a Tensor of a parameter of a model.\n\t        old_param_device ([int]): the device where the first parameter\n\t            of a model is allocated.\n\t    Returns:\n\t        old_param_device (int): report device for the first time\n\t    \"\"\"\n", "    # Meet the first parameter\n\t    if old_param_device is None:\n\t        old_param_device = param.get_device() if param.is_cuda else -1\n\t    else:\n\t        warn = False\n\t        if param.is_cuda:  # Check if in same GPU\n\t            warn = param.get_device() != old_param_device\n\t        else:  # Check if in CPU\n\t            warn = old_param_device != -1\n\t        if warn:\n", "            raise TypeError(\"Found two parameters on different devices, \"\n\t                            \"this is currently not supported.\")\n\t    return old_param_device\n\tdef apply_vector_grad_to_parameters(vec: TensorType, parameters: Iterable[TensorType], accumulate: bool = False):\n\t    \"\"\"Apply vector gradients to the parameters\n\t    Args:\n\t        vec (TensorType): a single vector represents the gradients of a model.\n\t        parameters (Iterable[TensorType]): an iterator of Tensors that are the\n\t            parameters of a model.\n\t    \"\"\"\n", "    # Ensure vec of type Tensor\n\t    if not isinstance(vec, torch.Tensor):\n\t        raise TypeError(\"expected torch.Tensor, but got: {}\".format(torch.typename(vec)))\n\t    # Flag for the device where the parameter is located\n\t    param_device = None\n\t    # Pointer for slicing the vector for each parameter\n\t    pointer = 0\n\t    for param in parameters:\n\t        # Ensure the parameters are located in the same device\n\t        param_device = _check_param_device(param, param_device)\n", "        # The length of the parameter\n\t        num_param = param.numel()\n\t        # Slice the vector, reshape it, and replace the old grad of the parameter\n\t        if accumulate:\n\t            param.grad = (param.grad + vec[pointer:pointer + num_param].view_as(param).data)\n\t        else:\n\t            param.grad = vec[pointer:pointer + num_param].view_as(param).data\n\t        # Increment the pointer\n\t        pointer += num_param\n\tclass Agent(grad_manipulation_agent.Agent):\n", "    def __init__(\n\t        self,\n\t        env_obs_shape: List[int],\n\t        action_shape: List[int],\n\t        action_range: Tuple[int, int],\n\t        device: torch.device,\n\t        agent_cfg: ConfigType,\n\t        multitask_cfg: ConfigType,\n\t        cfg_to_load_model: Optional[ConfigType] = None,\n\t        should_complete_init: bool = True,\n", "    ):\n\t        \"\"\"Regularized gradient algorithm.\"\"\"\n\t        agent_cfg_copy = deepcopy(agent_cfg)\n\t        del agent_cfg_copy['sdmgrad_lmbda']\n\t        del agent_cfg_copy['sdmgrad_method']\n\t        OmegaConf.set_struct(agent_cfg_copy, False)\n\t        agent_cfg_copy.cfg_to_load_model = None\n\t        agent_cfg_copy.should_complete_init = False\n\t        agent_cfg_copy.loss_reduction = \"none\"\n\t        OmegaConf.set_struct(agent_cfg_copy, True)\n", "        super().__init__(\n\t            env_obs_shape=env_obs_shape,\n\t            action_shape=action_shape,\n\t            action_range=action_range,\n\t            multitask_cfg=multitask_cfg,\n\t            agent_cfg=agent_cfg_copy,\n\t            device=device,\n\t        )\n\t        self.agent._compute_gradient = self._compute_gradient\n\t        self._rng = np.random.default_rng()\n", "        self.sdmgrad_lmbda = agent_cfg['sdmgrad_lmbda']\n\t        self.sdmgrad_method = agent_cfg['sdmgrad_method']\n\t        fn_maps = {\n\t            \"sdmgrad\": self.sdmgrad,\n\t        }\n\t        for k in range(2, 50):\n\t            fn_maps[f\"sdmgrad_os{k}\"] = self.sdmgrad_os\n\t        fn_names = \", \".join(fn_maps.keys())\n\t        assert self.sdmgrad_method in fn_maps, \\\n\t                f\"[error] unrealized fn {self.sdmgrad_method}, currently we have {fn_names}\"\n", "        self.sdmgrad_fn = fn_maps[self.sdmgrad_method]\n\t        self.wi_map = {}\n\t        self.num_param_block = -1\n\t        self.conflicts = []\n\t        self.last_w = None\n\t        self.save_target = 500000\n\t        if \"os\" in self.sdmgrad_method:\n\t            num_tasks = multitask_cfg['num_envs']\n\t            self.os_n = int(self.sdmgrad_method[self.sdmgrad_method.find(\"os\") + 2:])\n\t        if should_complete_init:\n", "            self.complete_init(cfg_to_load_model=cfg_to_load_model)\n\t    def _compute_gradient(\n\t        self,\n\t        loss: TensorType,  # batch x 1\n\t        parameters: List[TensorType],\n\t        step: int,\n\t        component_names: List[str],\n\t        env_metadata: grad_manipulation_agent.EnvMetadata,\n\t        retain_graph: bool = False,\n\t        allow_unused: bool = False,\n", "    ) -> None:\n\t        #t0 = time.time()\n\t        task_loss = self._convert_loss_into_task_loss(loss=loss, env_metadata=env_metadata)\n\t        num_tasks = task_loss.shape[0]\n\t        grad = []\n\t        if \"os\" in self.sdmgrad_method:\n\t            n = self.os_n\n\t            while True:\n\t                idx = np.random.binomial(1, n / num_tasks, num_tasks)\n\t                sample_idx = np.where(idx == 1)[0]\n", "                n_sample = sample_idx.shape[0]\n\t                if n_sample:\n\t                    break\n\t            losses = [0] * n_sample\n\t            for j in range(n_sample):\n\t                losses[j] = task_loss[sample_idx[j]]\n\t            for loss in losses:\n\t                grad.append(\n\t                    tuple(_grad.contiguous() for _grad in torch.autograd.grad(\n\t                        loss,\n", "                        parameters,\n\t                        retain_graph=True,\n\t                        allow_unused=allow_unused,\n\t                    )))\n\t        else:\n\t            for index in range(num_tasks):\n\t                grad.append(\n\t                    tuple(_grad.contiguous() for _grad in torch.autograd.grad(\n\t                        task_loss[index],\n\t                        parameters,\n", "                        retain_graph=(retain_graph or index != num_tasks - 1),\n\t                        allow_unused=allow_unused,\n\t                    )))\n\t        grad_vec = torch.cat(\n\t            list(map(lambda x: torch.nn.utils.parameters_to_vector(x).unsqueeze(0), grad)),\n\t            dim=0,\n\t        )  # num_tasks x dim\n\t        regularized_grad = self.sdmgrad_fn(grad_vec, num_tasks)\n\t        apply_vector_grad_to_parameters(regularized_grad, parameters)\n\t    def sdmgrad(self, grad_vec, num_tasks):\n", "        \"\"\"\n\t        grad_vec: [num_tasks, dim]\n\t        \"\"\"\n\t        grads = grad_vec\n\t        GG = torch.mm(grads, grads.t()).cpu()\n\t        scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n\t        GG = GG / scale.pow(2)\n\t        Gg = torch.mean(GG, dim=1)\n\t        gg = torch.mean(Gg)\n\t        w = torch.ones(num_tasks) / num_tasks\n", "        w.requires_grad = True\n\t        if num_tasks == 50:\n\t            w_opt = torch.optim.SGD([w], lr=50, momentum=0.5)\n\t        else:\n\t            w_opt = torch.optim.SGD([w], lr=25, momentum=0.5)\n\t        lmbda = self.sdmgrad_lmbda\n\t        w_best = None\n\t        obj_best = np.inf\n\t        for i in range(21):\n\t            w_opt.zero_grad()\n", "            obj = torch.dot(w, torch.mv(GG, w)) + 2 * lmbda * torch.dot(w, Gg) + lmbda**2 * gg\n\t            if obj.item() < obj_best:\n\t                obj_best = obj.item()\n\t                w_best = w.clone()\n\t            if i < 20:\n\t                obj.backward()\n\t                w_opt.step()\n\t                proj = euclidean_proj_simplex(w.data.cpu().numpy())\n\t                w.data.copy_(torch.from_numpy(proj).data)\n\t        g0 = torch.mean(grads, dim=0)\n", "        gw = torch.mv(grads.t(), w_best.to(grads.device))\n\t        g = (gw + lmbda * g0) / (1 + lmbda)\n\t        return g\n\t    def sdmgrad_os(self, grad_vec, num_tasks):\n\t        \"\"\"\n\t        objective sampling\n\t        grad_vec: [num_tasks, dim]\n\t        \"\"\"\n\t        grads = grad_vec\n\t        n = grads.size(0)\n", "        GG = torch.mm(grads, grads.t()).cpu()\n\t        scale = (torch.diag(GG) + 1e-4).sqrt().mean()\n\t        GG = GG / scale.pow(2)\n\t        Gg = torch.mean(GG, dim=1)\n\t        gg = torch.mean(Gg)\n\t        w = torch.ones(n) / n\n\t        w.requires_grad = True\n\t        w_opt = torch.optim.SGD([w], lr=50, momentum=0.5)\n\t        lmbda = self.sdmgrad_lmbda\n\t        w_best = None\n", "        obj_best = np.inf\n\t        for i in range(21):\n\t            w_opt.zero_grad()\n\t            obj = torch.dot(w, torch.mv(GG, w)) + 2 * lmbda * torch.dot(w, Gg) + lmbda**2 * gg\n\t            if obj.item() < obj_best:\n\t                obj_best = obj.item()\n\t                w_best = w.clone()\n\t            if i < 20:\n\t                obj.backward()\n\t                w_opt.step()\n", "                proj = euclidean_proj_simplex(w.data.cpu().numpy())\n\t                w.data.copy_(torch.from_numpy(proj).data)\n\t        g0 = torch.mean(grads, dim=0)\n\t        gw = torch.mv(grads.t(), w_best.to(grads.device))\n\t        g = (gw + lmbda * g0) / (1 + lmbda)\n\t        return g\n"]}
{"filename": "mtrl/mtrl_files/config.py", "chunked_list": ["# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n\t\"\"\"Code to interface with the config.\"\"\"\n\timport datetime\n\timport hashlib\n\timport os\n\tfrom copy import deepcopy\n\tfrom typing import Any, Dict, cast\n\timport hydra\n\tfrom omegaconf import OmegaConf\n\tfrom mtrl.utils import utils\n", "from mtrl.utils.types import ConfigType\n\tdef dict_to_config(dictionary: Dict) -> ConfigType:\n\t    \"\"\"Convert the dictionary to a config.\n\t    Args:\n\t        dictionary (Dict): dictionary to convert.\n\t    Returns:\n\t        ConfigType: config made from the dictionary.\n\t    \"\"\"\n\t    return OmegaConf.create(dictionary)\n\tdef make_config_mutable(config: ConfigType) -> ConfigType:\n", "    \"\"\"Set the config to be mutable.\n\t    Args:\n\t        config (ConfigType):\n\t    Returns:\n\t        ConfigType:\n\t    \"\"\"\n\t    OmegaConf.set_readonly(config, False)\n\t    return config\n\tdef make_config_immutable(config: ConfigType) -> ConfigType:\n\t    \"\"\"Set the config to be immutable.\n", "    Args:\n\t        config (ConfigType):\n\t    Returns:\n\t        ConfigType:\n\t    \"\"\"\n\t    OmegaConf.set_readonly(config, True)\n\t    return config\n\tdef set_struct(config: ConfigType) -> ConfigType:\n\t    \"\"\"Set the struct flag in the config.\n\t    Args:\n", "        config (ConfigType):\n\t    Returns:\n\t        ConfigType:\n\t    \"\"\"\n\t    OmegaConf.set_struct(config, True)\n\t    return config\n\tdef unset_struct(config: ConfigType) -> ConfigType:\n\t    \"\"\"Unset the struct flag in the config.\n\t    Args:\n\t        config (ConfigType):\n", "    Returns:\n\t        ConfigType:\n\t    \"\"\"\n\t    OmegaConf.set_struct(config, False)\n\t    return config\n\tdef to_dict(config: ConfigType) -> Dict[str, Any]:\n\t    \"\"\"Convert config to a dictionary.\n\t    Args:\n\t        config (ConfigType):\n\t    Returns:\n", "        Dict:\n\t    \"\"\"\n\t    dict_config = cast(Dict[str, Any], OmegaConf.to_container(deepcopy(config), resolve=False))\n\t    return dict_config\n\tdef process_config(config: ConfigType, should_make_dir: bool = True) -> ConfigType:\n\t    \"\"\"Process the config.\n\t    Args:\n\t        config (ConfigType): config object to process.\n\t        should_make_dir (bool, optional): should make dir for saving logs, models etc? Defaults to True.\n\t    Returns:\n", "        ConfigType: processed config.\n\t    \"\"\"\n\t    config = _process_setup_config(config=config)\n\t    config = _process_experiment_config(config=config, should_make_dir=should_make_dir)\n\t    return set_struct(make_config_immutable(config))\n\tdef read_config_from_file(config_path: str) -> ConfigType:\n\t    \"\"\"Read the config from filesystem.\n\t    Args:\n\t        config_path (str): path to read config from.\n\t    Returns:\n", "        ConfigType:\n\t    \"\"\"\n\t    config = OmegaConf.load(config_path)\n\t    assert isinstance(config, ConfigType)\n\t    return set_struct(make_config_immutable(config))\n\tdef _process_setup_config(config: ConfigType) -> ConfigType:\n\t    \"\"\"Process the `setup` node of the config.\n\t    Args:\n\t        config (ConfigType): config object.\n\t    Returns:\n", "        [ConfigType]: processed config.\n\t    \"\"\"\n\t    setup_config = config.setup\n\t    if setup_config.base_path is None:\n\t        setup_config.base_path = hydra.utils.get_original_cwd()\n\t    if not setup_config.debug.should_enable:\n\t        #setup_config.id = f\"{hashlib.sha224(setup_config.description.encode()).hexdigest()}_issue_{setup_config.git.issue_id}_seed_{setup_config.seed}\"\n\t        if \"sdmgrad\" in config.agent.name:\n\t            setup_config.id = f\"{config.env.name}_{config.agent.name}_\"+\\\n\t                    f\"{config.agent.builder.agent_cfg.sdmgrad_method}_\"+\\\n", "                    f\"c{config.agent.builder.agent_cfg.sdmgrad_lmbda}_seed_{setup_config.seed}\"\n\t        else:\n\t            setup_config.id = f\"{config.env.name}_{config.agent.name}_seed_{setup_config.seed}\"\n\t    current_commit_id = utils.get_current_commit_id()\n\t    if not setup_config.git.commit_id:\n\t        setup_config.git.commit_id = current_commit_id\n\t    else:\n\t        # if the commit id is already set, assert that the commit id (in the\n\t        # config) is the same as the current commit id.\n\t        if setup_config.git.commit_id != current_commit_id:\n", "            raise RuntimeError(f\"\"\"The current commit id ({current_commit_id}) does\n\t                 not match the commit id from the config\n\t                 ({setup_config.git.commit_id})\"\"\")\n\t    if setup_config.git.has_uncommitted_changes == \"\":\n\t        setup_config.git.has_uncommitted_changes = utils.has_uncommitted_changes()\n\t    if not setup_config.date:\n\t        setup_config.date = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\t    slurm_id = []\n\t    env_var_names = [\"SLURM_JOB_ID\", \"SLURM_STEP_ID\"]\n\t    for var_name in env_var_names:\n", "        if var_name in os.environ:\n\t            slurm_id.append(str(os.environ[var_name]))\n\t    if slurm_id:\n\t        setup_config.slurm_id = \"-\".join(slurm_id)\n\t    else:\n\t        setup_config.slurm_id = \"-1\"\n\t    return config\n\tdef _process_experiment_config(config: ConfigType, should_make_dir: bool) -> ConfigType:\n\t    \"\"\"Process the `experiment` section of the config.\n\t    Args:\n", "        config (ConfigType): config object.\n\t        should_make_dir (bool): should make dir.\n\t    Returns:\n\t        ConfigType: Processed config\n\t    \"\"\"\n\t    if should_make_dir:\n\t        utils.make_dir(path=config.experiment.save_dir)\n\t    return config\n\tdef pretty_print(config, resolve: bool = True):\n\t    \"\"\"Prettyprint the config.\n", "    Args:\n\t        config ([type]):\n\t        resolve (bool, optional): should resolve the config before printing. Defaults to True.\n\t    \"\"\"\n\t    print(OmegaConf.to_yaml(config, resolve=resolve))\n\tdef get_env_params_from_config(config: ConfigType) -> ConfigType:\n\t    \"\"\"Get the params needed for building the environment from a config.\n\t    Args:\n\t        config (ConfigType):\n\t    Returns:\n", "        ConfigType: params for building the environment, encoded as a config.\n\t    \"\"\"\n\t    env_params = deepcopy(config.env.builder)\n\t    env_params = make_config_mutable(env_params)\n\t    env_params = unset_struct(env_params)\n\t    env_params.pop(\"_target_\")\n\t    return env_params\n"]}
{"filename": "cityscapes/model_segnet_stan.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Single-task: Attention Network')\n\tparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth, normal')\n\tparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n", "opt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 7\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n", "        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n", "            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task attention layers\n\t        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n\t        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n\t        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n", "        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for j in range(2):\n\t            if j < 1:\n\t                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n\t                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n\t            for i in range(4):\n\t                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n\t                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\t        for i in range(4):\n\t            if i < 3:\n", "                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t            else:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n\t        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n", "        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n", "                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n", "            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n\t    def att_layer(self, channel):\n\t        att_block = nn.Sequential(\n\t            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[1]),\n\t            nn.ReLU(inplace=True),\n", "            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[2]),\n\t            nn.Sigmoid(),\n\t        )\n\t        return att_block\n\t    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define attention list for tasks\n", "        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n\t        for i in range(2):\n\t            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n\t        for i in range(2):\n\t            for j in range(5):\n\t                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\t        # define global shared network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n", "                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n", "                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t        # define task dependent attention module\n\t        for i in range(2):\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n", "                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t                else:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n\t                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t            for j in range(5):\n", "                if j == 0:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t                else:\n", "                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n", "        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n\t        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n\t        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred], self.logsigma\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_STAN = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_STAN.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_STAN),\n", "                                                         count_parameters(SegNet_STAN) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation.')\n\telse:\n\t    train_set = CityScapes(root=dataset_path, train=True)\n", "    print('Standard training strategy without data augmentation.')\n\ttest_set = CityScapes(root=dataset_path, train=False)\n\tbatch_size = 8\n\ttrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ttest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate single-task network\n\tsingle_task_trainer(train_loader, test_loader, SegNet_STAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "cityscapes/model_segnet_mtan.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Attention Network')\n\tparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n\tparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\n", "parser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--seed', default=0, type=int, help='control seed')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 7\n", "        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n", "            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task attention layers\n", "        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n\t        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n\t        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n\t        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for j in range(2):\n\t            if j < 1:\n\t                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n\t                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n\t            for i in range(4):\n\t                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n", "                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\t        for i in range(4):\n\t            if i < 3:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t            else:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n", "        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n", "                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n", "                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n\t    def att_layer(self, channel):\n\t        att_block = nn.Sequential(\n", "            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[1]),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[2]),\n\t            nn.Sigmoid(),\n\t        )\n\t        return att_block\n\t    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n", "        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define attention list for tasks\n\t        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n\t        for i in range(2):\n\t            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n\t        for i in range(2):\n\t            for j in range(5):\n\t                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\t        # define global shared network\n", "        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n", "            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t        # define task dependent attention module\n\t        for i in range(2):\n", "            for j in range(5):\n\t                if j == 0:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t                else:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n\t                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n", "                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n", "                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t                else:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n", "                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n\t        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n\t        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n\t        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred], self.logsigma\n\tcontrol_seed(opt.seed)\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n", "SegNet_MTAN = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n\t                                                         count_parameters(SegNet_MTAN) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n", "    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation.')\n\telse:\n\t    train_set = CityScapes(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\ttest_set = CityScapes(root=dataset_path, train=False)\n\tbatch_size = 8\n\ttrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ttest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate multi-task network\n", "multi_task_trainer(train_loader, test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "cityscapes/model_segnet_split.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Split')\n\tparser.add_argument('--type', default='standard', type=str, help='split type: standard, wide, deep')\n\tparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n", "parser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\n\tparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 7\n", "        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n", "            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task attention layers\n", "        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n\t        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n\t        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n\t        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for j in range(2):\n\t            if j < 1:\n\t                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n\t                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n\t            for i in range(4):\n\t                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n", "                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\t        for i in range(4):\n\t            if i < 3:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t            else:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n", "        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n", "                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n", "                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n\t    def att_layer(self, channel):\n\t        att_block = nn.Sequential(\n", "            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[1]),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[2]),\n\t            nn.Sigmoid(),\n\t        )\n\t        return att_block\n\t    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n", "        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define attention list for tasks\n\t        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n\t        for i in range(2):\n\t            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n\t        for i in range(2):\n\t            for j in range(5):\n\t                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\t        # define global shared network\n", "        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n", "            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t        # define task dependent attention module\n\t        for i in range(2):\n", "            for j in range(5):\n\t                if j == 0:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t                else:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n\t                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n", "                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n", "                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t                else:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n", "                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n\t        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n\t        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n\t        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred], self.logsigma\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_SPLIT = SegNet().to(device)\n", "optimizer = optim.Adam(SegNet_SPLIT.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_SPLIT),\n\t                                                         count_parameters(SegNet_SPLIT) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n", "    print('Applying data augmentation.')\n\telse:\n\t    train_set = CityScapes(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\ttest_set = CityScapes(root=dataset_path, train=False)\n\tbatch_size = 8\n\ttrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ttest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate multi-task network\n\tmulti_task_trainer(train_loader, test_loader, SegNet_SPLIT, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "cityscapes/evaluate.py", "chunked_list": ["import matplotlib.pyplot as plt\n\timport seaborn as sns\n\timport numpy as np\n\timport torch\n\tmethods = [\n\t    \"sdmgrad-1e-1\", \"sdmgrad-2e-1\", \"sdmgrad-3e-1\", \"sdmgrad-4e-1\", \"sdmgrad-5e-1\", \"sdmgrad-6e-1\", \"sdmgrad-7e-1\",\n\t    \"sdmgrad-8e-1\", \"sdmgrad-9e-1\", \"sdmgrad-1e0\"\n\t]\n\tcolors = [\"C0\", \"C1\", \"C2\", \"C3\", \"C4\", \"C5\", \"C6\", \"C7\", \"C8\", \"C9\", \"tab:green\", \"tab:cyan\", \"tab:blue\", \"tab:red\"]\n\tstats = [\"semantic loss\", \"mean iou\", \"pix acc\", \"depth loss\", \"abs err\", \"rel err\"]\n", "stats_idx_map = [4, 5, 6, 8, 9, 10]\n\tdelta_stats = [\"mean iou\", \"pix acc\", \"abs err\", \"rel err\"]\n\ttime_idx = 22\n\t# change random seeds used in the experiments here\n\tseeds = [0, 1, 2]\n\tlogs = {}\n\tmin_epoch = 100000\n\tfor m in methods:\n\t    logs[m] = {\"train\": [None for _ in range(3)], \"test\": [None for _ in range(3)]}\n\t    for seed in seeds:\n", "        logs[m][\"train\"][seed] = {}\n\t        logs[m][\"test\"][seed] = {}\n\t    for stat in stats:\n\t        for seed in seeds:\n\t            logs[m][\"train\"][seed][stat] = []\n\t            logs[m][\"test\"][seed][stat] = []\n\t    for seed in seeds:\n\t        logs[m][\"train\"][seed][\"time\"] = []\n\t    for seed in seeds:\n\t        fname = f\"logs/{m}-sd{seed}.log\"\n", "        with open(fname, \"r\") as f:\n\t            lines = f.readlines()\n\t            for line in lines:\n\t                if line.startswith(\"Epoch\"):\n\t                    ws = line.split(\" \")\n\t                    for i, stat in enumerate(stats):\n\t                        logs[m][\"train\"][seed][stat].append(float(ws[stats_idx_map[i]]))\n\t                        logs[m][\"test\"][seed][stat].append(float(ws[stats_idx_map[i] + 9]))\n\t                    logs[m][\"train\"][seed][\"time\"].append(float(ws[time_idx]))\n\t            n_epoch = min(len(logs[m][\"train\"][seed][\"semantic loss\"]), len(logs[m][\"test\"][seed][\"semantic loss\"]))\n", "            if n_epoch < min_epoch:\n\t                min_epoch = n_epoch\n\t                print(m, n_epoch)\n\ttest_stats = {}\n\ttrain_stats = {}\n\tlearning_time = {}\n\tprint(\" \" * 25 + \" | \".join([f\"{s:5s}\" for s in stats]))\n\tfor mi, mode in enumerate([\"train\", \"test\"]):\n\t    if mi == 1:\n\t        print(mode)\n", "    for mmi, m in enumerate(methods):\n\t        if m not in test_stats:\n\t            test_stats[m] = {}\n\t            train_stats[m] = {}\n\t        string = f\"{m:30s} \"\n\t        for stat in stats:\n\t            x = []\n\t            for seed in seeds:\n\t                x.append(np.array(logs[m][mode][seed][stat][min_epoch - 10:min_epoch]).mean())\n\t            x = np.array(x)\n", "            if mode == \"test\":\n\t                test_stats[m][stat] = x.copy()\n\t            else:\n\t                train_stats[m][stat] = x.copy()\n\t            mu = x.mean()\n\t            std = x.std() / np.sqrt(3)\n\t            string += f\" | {mu:5.4f}\"\n\t        if mode == \"test\":\n\t            print(string)\n\tfor m in methods:\n", "    learning_time[m] = np.array([np.array(logs[m][\"train\"][sd][\"time\"]).mean() for sd in seeds])\n\t### print average training loss\n\tfor method in methods:\n\t    average_loss = np.mean([train_stats[method][\"semantic loss\"].mean(), train_stats[method][\"depth loss\"].mean()])\n\t    print(f\"{method} average training loss {average_loss}\")\n\t### print delta M\n\tbase = np.array([0.7401, 0.9316, 0.0125, 27.77])\n\tsign = np.array([1, 1, 0, 0])\n\tkk = np.ones(4) * -1\n\tdef delta_fn(a):\n", "    return (kk**sign * (a - base) / base).mean() * 100.  # *100 for percentage\n\tdeltas = {}\n\tfor method in methods:\n\t    tmp = np.zeros(4)\n\t    for i, stat in enumerate(delta_stats):\n\t        tmp[i] = test_stats[method][stat].mean()\n\t    deltas[method] = delta_fn(tmp)\n\t    print(f\"{method:30s} delta: {deltas[method]:4.3f}\")\n"]}
{"filename": "cityscapes/utils.py", "chunked_list": ["import torch\n\timport torch.nn.functional as F\n\timport numpy as np\n\timport random\n\timport time\n\tfrom copy import deepcopy\n\tfrom min_norm_solvers import MinNormSolver\n\tfrom scipy.optimize import minimize, Bounds, minimize_scalar\n\tdef euclidean_proj_simplex(v, s=1):\n\t    \"\"\" Compute the Euclidean projection on a positive simplex\n", "    Solves the optimisation problem (using the algorithm from [1]):\n\t        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0 \n\t    Parameters\n\t    ----------\n\t    v: (n,) numpy array,\n\t       n-dimensional vector to project\n\t    s: int, optional, default: 1,\n\t       radius of the simplex\n\t    Returns\n\t    -------\n", "    w: (n,) numpy array,\n\t       Euclidean projection of v on the simplex\n\t    Notes\n\t    -----\n\t    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n\t    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n\t    However, this implementation still easily scales to millions of dimensions.\n\t    References\n\t    ----------\n\t    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n", "        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n\t        International Conference on Machine Learning (ICML 2008)\n\t        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n\t    [2] Projection onto the probability simplex: An efficient algorithm with a simple proof, and an application\n\t        Weiran Wang, Miguel . Carreira-Perpin. arXiv:1309.1541\n\t        https://arxiv.org/pdf/1309.1541.pdf\n\t    [3] https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246#file-simplex_projection-py\n\t    \"\"\"\n\t    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n\t    v = v.astype(np.float64)\n", "    n, = v.shape  # will raise ValueError if v is not 1-D\n\t    # check if we are already on the simplex\n\t    if v.sum() == s and np.alltrue(v >= 0):\n\t        # best projection: itself!\n\t        return v\n\t    # get the array of cumulative sums of a sorted (decreasing) copy of v\n\t    u = np.sort(v)[::-1]\n\t    cssv = np.cumsum(u)\n\t    # get the number of > 0 components of the optimal solution\n\t    rho = np.nonzero(u * np.arange(1, n + 1) > (cssv - s))[0][-1]\n", "    # compute the Lagrange multiplier associated to the simplex constraint\n\t    theta = float(cssv[rho] - s) / (rho + 1)\n\t    # compute the projection by thresholding v using theta\n\t    w = (v - theta).clip(min=0)\n\t    return w\n\t\"\"\"\n\tDefine task metrics, loss functions and model trainer here.\n\t\"\"\"\n\tdef control_seed(seed):\n\t    torch.backends.cudnn.enabled = False\n", "    torch.manual_seed(seed)\n\t    np.random.seed(seed)\n\t    random.seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n\tdef count_parameters(model):\n\t    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\tdef model_fit(x_pred, x_output, task_type):\n\t    device = x_pred.device\n\t    # binary mark to mask out undefined pixel space\n\t    binary_mask = (torch.sum(x_output, dim=1) != 0).float().unsqueeze(1).to(device)\n", "    if task_type == 'semantic':\n\t        # semantic loss: depth-wise cross entropy\n\t        loss = F.nll_loss(x_pred, x_output, ignore_index=-1)\n\t    if task_type == 'depth':\n\t        # depth loss: l1 norm\n\t        loss = torch.sum(torch.abs(x_pred - x_output) * binary_mask) / torch.nonzero(binary_mask,\n\t                                                                                     as_tuple=False).size(0)\n\t    if task_type == 'normal':\n\t        # normal loss: dot product\n\t        loss = 1 - torch.sum((x_pred * x_output) * binary_mask) / torch.nonzero(binary_mask, as_tuple=False).size(0)\n", "    return loss\n\t# Legacy: compute mIoU and Acc. for each image and average across all images.\n\t# def compute_miou(x_pred, x_output):\n\t#     _, x_pred_label = torch.max(x_pred, dim=1)\n\t#     x_output_label = x_output\n\t#     batch_size = x_pred.size(0)\n\t#     class_nb = x_pred.size(1)\n\t#     device = x_pred.device\n\t#     for i in range(batch_size):\n\t#         true_class = 0\n", "#         first_switch = True\n\t#         invalid_mask = (x_output[i] >= 0).float()\n\t#         for j in range(class_nb):\n\t#             pred_mask = torch.eq(x_pred_label[i], j * torch.ones(x_pred_label[i].shape).long().to(device))\n\t#             true_mask = torch.eq(x_output_label[i], j * torch.ones(x_output_label[i].shape).long().to(device))\n\t#             mask_comb = pred_mask.float() + true_mask.float()\n\t#             union = torch.sum((mask_comb > 0).float() * invalid_mask)  # remove non-defined pixel predictions\n\t#             intsec = torch.sum((mask_comb > 1).float())\n\t#             if union == 0:\n\t#                 continue\n", "#             if first_switch:\n\t#                 class_prob = intsec / union\n\t#                 first_switch = False\n\t#             else:\n\t#                 class_prob = intsec / union + class_prob\n\t#             true_class += 1\n\t#         if i == 0:\n\t#             batch_avg = class_prob / true_class\n\t#         else:\n\t#             batch_avg = class_prob / true_class + batch_avg\n", "#     return batch_avg / batch_size\n\t#\n\t#\n\t# def compute_iou(x_pred, x_output):\n\t#     _, x_pred_label = torch.max(x_pred, dim=1)\n\t#     x_output_label = x_output\n\t#     batch_size = x_pred.size(0)\n\t#     for i in range(batch_size):\n\t#         if i == 0:\n\t#             pixel_acc = torch.div(\n", "#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n\t#                 torch.sum((x_output_label[i] >= 0).float()))\n\t#         else:\n\t#             pixel_acc = pixel_acc + torch.div(\n\t#                 torch.sum(torch.eq(x_pred_label[i], x_output_label[i]).float()),\n\t#                 torch.sum((x_output_label[i] >= 0).float()))\n\t#     return pixel_acc / batch_size\n\t# New mIoU and Acc. formula: accumulate every pixel and average across all pixels in all images\n\tclass ConfMatrix(object):\n\t    def __init__(self, num_classes):\n", "        self.num_classes = num_classes\n\t        self.mat = None\n\t    def update(self, pred, target):\n\t        n = self.num_classes\n\t        if self.mat is None:\n\t            self.mat = torch.zeros((n, n), dtype=torch.int64, device=pred.device)\n\t        with torch.no_grad():\n\t            k = (target >= 0) & (target < n)\n\t            inds = n * target[k].to(torch.int64) + pred[k]\n\t            self.mat += torch.bincount(inds, minlength=n**2).reshape(n, n)\n", "    def get_metrics(self):\n\t        h = self.mat.float()\n\t        acc = torch.diag(h).sum() / h.sum()\n\t        iu = torch.diag(h) / (h.sum(1) + h.sum(0) - torch.diag(h))\n\t        return torch.mean(iu).item(), acc.item()\n\tdef depth_error(x_pred, x_output):\n\t    device = x_pred.device\n\t    binary_mask = (torch.sum(x_output, dim=1) != 0).unsqueeze(1).to(device)\n\t    x_pred_true = x_pred.masked_select(binary_mask)\n\t    x_output_true = x_output.masked_select(binary_mask)\n", "    abs_err = torch.abs(x_pred_true - x_output_true)\n\t    rel_err = torch.abs(x_pred_true - x_output_true) / x_output_true\n\t    return (torch.sum(abs_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item(), \\\n\t           (torch.sum(rel_err) / torch.nonzero(binary_mask, as_tuple=False).size(0)).item()\n\tdef normal_error(x_pred, x_output):\n\t    binary_mask = (torch.sum(x_output, dim=1) != 0)\n\t    error = torch.acos(torch.clamp(torch.sum(x_pred * x_output, 1).masked_select(binary_mask), -1,\n\t                                   1)).detach().cpu().numpy()\n\t    error = np.degrees(error)\n\t    return np.mean(error), np.median(error), np.mean(error < 11.25), np.mean(error < 22.5), np.mean(error < 30)\n", "\"\"\"\n\t=========== Universal Multi-task Trainer ===========\n\t\"\"\"\n\tdef multi_task_trainer(train_loader, test_loader, multi_task_model, device, optimizer, scheduler, opt, total_epoch=200):\n\t    train_batch = len(train_loader)\n\t    test_batch = len(test_loader)\n\t    T = opt.temp\n\t    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n\t    lambda_weight = np.ones([2, total_epoch])\n\t    for index in range(total_epoch):\n", "        t0 = time.time()\n\t        cost = np.zeros(12, dtype=np.float32)\n\t        # apply Dynamic Weight Average\n\t        if opt.weight == 'dwa':\n\t            if index == 0 or index == 1:\n\t                lambda_weight[:, index] = 1.0\n\t            else:\n\t                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n\t                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n\t                lambda_weight[0, index] = 2 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n", "                lambda_weight[1, index] = 2 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n\t        # iteration for all batches\n\t        multi_task_model.train()\n\t        train_dataset = iter(train_loader)\n\t        conf_mat = ConfMatrix(multi_task_model.class_nb)\n\t        for k in range(train_batch):\n\t            train_data, train_label, train_depth = train_dataset.next()\n\t            train_data, train_label = train_data.to(device), train_label.long().to(device)\n\t            train_depth = train_depth.to(device)\n\t            train_pred, logsigma = multi_task_model(train_data)\n", "            optimizer.zero_grad()\n\t            train_loss = [\n\t                model_fit(train_pred[0], train_label, 'semantic'),\n\t                model_fit(train_pred[1], train_depth, 'depth')\n\t            ]\n\t            if opt.weight == 'equal' or opt.weight == 'dwa':\n\t                loss = sum([lambda_weight[i, index] * train_loss[i] for i in range(2)])\n\t            else:\n\t                loss = sum(1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2 for i in range(2))\n\t            loss.backward()\n", "            optimizer.step()\n\t            # accumulate label prediction for every pixel in training images\n\t            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\t            cost[0] = train_loss[0].item()\n\t            cost[3] = train_loss[1].item()\n\t            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n\t            avg_cost[index, :6] += cost[:6] / train_batch\n\t        # compute mIoU and acc\n\t        avg_cost[index, 1:3] = conf_mat.get_metrics()\n\t        # evaluating test data\n", "        multi_task_model.eval()\n\t        conf_mat = ConfMatrix(multi_task_model.class_nb)\n\t        with torch.no_grad():  # operations inside don't track history\n\t            test_dataset = iter(test_loader)\n\t            for k in range(test_batch):\n\t                test_data, test_label, test_depth = test_dataset.next()\n\t                test_data, test_label = test_data.to(device), test_label.long().to(device)\n\t                test_depth = test_depth.to(device)\n\t                test_pred, _ = multi_task_model(test_data)\n\t                test_loss = [\n", "                    model_fit(test_pred[0], test_label, 'semantic'),\n\t                    model_fit(test_pred[1], test_depth, 'depth')\n\t                ]\n\t                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\t                cost[6] = test_loss[0].item()\n\t                cost[9] = test_loss[1].item()\n\t                cost[10], cost[11] = depth_error(test_pred[1], test_depth)\n\t                avg_cost[index, 6:] += cost[6:] / test_batch\n\t            # compute mIoU and acc\n\t            avg_cost[index, 7:9] = conf_mat.get_metrics()\n", "        scheduler.step()\n\t        t1 = time.time()\n\t        print(\n\t            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} || TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | TIME: {:.4f}'\n\t            .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n\t                    avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n\t                    avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], t1 - t0))\n\t\"\"\"\n\t=========== Universal Single-task Trainer ===========\n\t\"\"\"\n", "def single_task_trainer(train_loader,\n\t                        test_loader,\n\t                        single_task_model,\n\t                        device,\n\t                        optimizer,\n\t                        scheduler,\n\t                        opt,\n\t                        total_epoch=200):\n\t    train_batch = len(train_loader)\n\t    test_batch = len(test_loader)\n", "    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n\t    for index in range(total_epoch):\n\t        cost = np.zeros(12, dtype=np.float32)\n\t        # iteration for all batches\n\t        single_task_model.train()\n\t        train_dataset = iter(train_loader)\n\t        conf_mat = ConfMatrix(single_task_model.class_nb)\n\t        for k in range(train_batch):\n\t            train_data, train_label, train_depth = train_dataset.next()\n\t            train_data, train_label = train_data.to(device), train_label.long().to(device)\n", "            train_depth = train_depth.to(device)\n\t            train_pred = single_task_model(train_data)\n\t            optimizer.zero_grad()\n\t            if opt.task == 'semantic':\n\t                train_loss = model_fit(train_pred, train_label, opt.task)\n\t                train_loss.backward()\n\t                optimizer.step()\n\t                conf_mat.update(train_pred.argmax(1).flatten(), train_label.flatten())\n\t                cost[0] = train_loss.item()\n\t            if opt.task == 'depth':\n", "                train_loss = model_fit(train_pred, train_depth, opt.task)\n\t                train_loss.backward()\n\t                optimizer.step()\n\t                cost[3] = train_loss.item()\n\t                cost[4], cost[5] = depth_error(train_pred, train_depth)\n\t            avg_cost[index, :6] += cost[:6] / train_batch\n\t        if opt.task == 'semantic':\n\t            avg_cost[index, 1:3] = conf_mat.get_metrics()\n\t        # evaluating test data\n\t        single_task_model.eval()\n", "        conf_mat = ConfMatrix(single_task_model.class_nb)\n\t        with torch.no_grad():  # operations inside don't track history\n\t            test_dataset = iter(test_loader)\n\t            for k in range(test_batch):\n\t                test_data, test_label, test_depth = test_dataset.next()\n\t                test_data, test_label = test_data.to(device), test_label.long().to(device)\n\t                test_depth = test_depth.to(device)\n\t                test_pred = single_task_model(test_data)\n\t                if opt.task == 'semantic':\n\t                    test_loss = model_fit(test_pred, test_label, opt.task)\n", "                    conf_mat.update(test_pred.argmax(1).flatten(), test_label.flatten())\n\t                    cost[6] = test_loss.item()\n\t                if opt.task == 'depth':\n\t                    test_loss = model_fit(test_pred, test_depth, opt.task)\n\t                    cost[9] = test_loss.item()\n\t                    cost[10], cost[11] = depth_error(test_pred, test_depth)\n\t                avg_cost[index, 6:] += cost[6:] / test_batch\n\t            if opt.task == 'semantic':\n\t                avg_cost[index, 7:9] = conf_mat.get_metrics()\n\t        scheduler.step()\n", "        if opt.task == 'semantic':\n\t            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n\t                index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 6],\n\t                avg_cost[index, 7], avg_cost[index, 8]))\n\t        if opt.task == 'depth':\n\t            print('Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} TEST: {:.4f} {:.4f} {:.4f}'.format(\n\t                index, avg_cost[index, 3], avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 9],\n\t                avg_cost[index, 10], avg_cost[index, 11]))\n\t        torch.save(single_task_model.state_dict(), f\"models/single-{opt.task}-{opt.seed}.pt\")\n\t\"\"\"\n", "=========== Universal Gradient Manipulation Multi-task Trainer ===========\n\t\"\"\"\n\tdef multi_task_rg_trainer(train_loader,\n\t                          test_loader,\n\t                          multi_task_model,\n\t                          device,\n\t                          optimizer,\n\t                          scheduler,\n\t                          opt,\n\t                          total_epoch=200):\n", "    method = opt.method\n\t    alpha = opt.alpha\n\t    niter = opt.niter\n\t    # warm_niter = opt.warm_niter\n\t    def graddrop(grads):\n\t        P = 0.5 * (1. + grads.sum(1) / (grads.abs().sum(1) + 1e-8))\n\t        U = torch.rand_like(grads[:, 0])\n\t        M = P.gt(U).view(-1, 1) * grads.gt(0) + P.lt(U).view(-1, 1) * grads.lt(0)\n\t        g = (grads * M.float()).mean(1)\n\t        return g\n", "    def mgd(grads):\n\t        grads_cpu = grads.t().cpu()\n\t        sol, min_norm = MinNormSolver.find_min_norm_element([grads_cpu[t] for t in range(grads.shape[-1])])\n\t        w = torch.FloatTensor(sol).to(grads.device)\n\t        g = grads.mm(w.view(-1, 1)).view(-1)\n\t        return g\n\t    def pcgrad(grads, rng):\n\t        grad_vec = grads.t()\n\t        num_tasks = 2\n\t        shuffled_task_indices = np.zeros((num_tasks, num_tasks - 1), dtype=int)\n", "        for i in range(num_tasks):\n\t            task_indices = np.arange(num_tasks)\n\t            task_indices[i] = task_indices[-1]\n\t            shuffled_task_indices[i] = task_indices[:-1]\n\t            rng.shuffle(shuffled_task_indices[i])\n\t        shuffled_task_indices = shuffled_task_indices.T\n\t        normalized_grad_vec = grad_vec / (grad_vec.norm(dim=1, keepdim=True) + 1e-8)  # num_tasks x dim\n\t        modified_grad_vec = deepcopy(grad_vec)\n\t        for task_indices in shuffled_task_indices:\n\t            normalized_shuffled_grad = normalized_grad_vec[task_indices]  # num_tasks x dim\n", "            dot = (modified_grad_vec * normalized_shuffled_grad).sum(dim=1, keepdim=True)  # num_tasks x dim\n\t            modified_grad_vec -= torch.clamp_max(dot, 0) * normalized_shuffled_grad\n\t        g = modified_grad_vec.mean(dim=0)\n\t        return g\n\t    def cagrad(grads, alpha=0.5, rescale=0):\n\t        g1 = grads[:, 0]\n\t        g2 = grads[:, 1]\n\t        g11 = g1.dot(g1).item()\n\t        g12 = g1.dot(g2).item()\n\t        g22 = g2.dot(g2).item()\n", "        g0_norm = 0.5 * np.sqrt(g11 + g22 + 2 * g12)\n\t        # want to minimize g_w^Tg_0 + c*||g_0||*||g_w||\n\t        coef = alpha * g0_norm\n\t        def obj(x):\n\t            # g_w^T g_0: x*0.5*(g11+g22-2g12)+(0.5+x)*(g12-g22)+g22\n\t            # g_w^T g_w: x^2*(g11+g22-2g12)+2*x*(g12-g22)+g22\n\t            return coef * np.sqrt(x**2 * (g11 + g22 - 2 * g12) + 2 * x * (g12 - g22) + g22 +\n\t                                  1e-8) + 0.5 * x * (g11 + g22 - 2 * g12) + (0.5 + x) * (g12 - g22) + g22\n\t        res = minimize_scalar(obj, bounds=(0, 1), method='bounded')\n\t        x = res.x\n", "        gw_norm = np.sqrt(x**2 * g11 + (1 - x)**2 * g22 + 2 * x * (1 - x) * g12 + 1e-8)\n\t        lmbda = coef / (gw_norm + 1e-8)\n\t        g = (0.5 + lmbda * x) * g1 + (0.5 + lmbda * (1 - x)) * g2  # g0 + lmbda*gw\n\t        if rescale == 0:\n\t            return g\n\t        elif rescale == 1:\n\t            return g / (1 + alpha**2)\n\t        else:\n\t            return g / (1 + alpha)\n\t    def sdmgrad(w, grads, alpha, niter=20):\n", "        GG = torch.mm(grads.t(), grads)\n\t        scale = torch.mean(torch.sqrt(torch.diag(GG) + 1e-4))\n\t        GG = GG / scale.pow(2)\n\t        Gg = torch.mean(GG, dim=1)\n\t        gg = torch.mean(Gg)\n\t        w.requires_grad = True\n\t        optimizer = torch.optim.SGD([w], lr=10, momentum=0.5)\n\t        for i in range(niter):\n\t            optimizer.zero_grad()\n\t            obj = torch.dot(w, torch.mv(GG, w)) + 2 * alpha * torch.dot(w, Gg) + alpha**2 * gg\n", "            obj.backward()\n\t            optimizer.step()\n\t            proj = euclidean_proj_simplex(w.data.cpu().numpy())\n\t            w.data.copy_(torch.from_numpy(proj).data)\n\t        w.requires_grad = False\n\t        g0 = torch.mean(grads, dim=1)\n\t        gw = torch.mv(grads, w)\n\t        g = (gw + alpha * g0) / (1 + alpha)\n\t        return g\n\t    def grad2vec(m, grads, grad_dims, task):\n", "        # store the gradients\n\t        grads[:, task].fill_(0.0)\n\t        cnt = 0\n\t        for mm in m.shared_modules():\n\t            for p in mm.parameters():\n\t                grad = p.grad\n\t                if grad is not None:\n\t                    grad_cur = grad.data.detach().clone()\n\t                    beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n\t                    en = sum(grad_dims[:cnt + 1])\n", "                    grads[beg:en, task].copy_(grad_cur.data.view(-1))\n\t                cnt += 1\n\t    def overwrite_grad(m, newgrad, grad_dims):\n\t        newgrad = newgrad * 2  # to match the sum loss\n\t        cnt = 0\n\t        for mm in m.shared_modules():\n\t            for param in mm.parameters():\n\t                beg = 0 if cnt == 0 else sum(grad_dims[:cnt])\n\t                en = sum(grad_dims[:cnt + 1])\n\t                this_grad = newgrad[beg:en].contiguous().view(param.data.size())\n", "                param.grad = this_grad.data.clone()\n\t                cnt += 1\n\t    rng = np.random.default_rng()\n\t    grad_dims = []\n\t    for mm in multi_task_model.shared_modules():\n\t        for param in mm.parameters():\n\t            grad_dims.append(param.data.numel())\n\t    grads = torch.Tensor(sum(grad_dims), 2).cuda()\n\t    w = 1 / 2 * torch.ones(2).cuda()\n\t    train_batch = len(train_loader)\n", "    test_batch = len(test_loader)\n\t    T = opt.temp\n\t    avg_cost = np.zeros([total_epoch, 12], dtype=np.float32)\n\t    lambda_weight = np.ones([2, total_epoch])\n\t    for index in range(total_epoch):\n\t        t0 = time.time()\n\t        cost = np.zeros(12, dtype=np.float32)\n\t        # apply Dynamic Weight Average\n\t        if opt.weight == 'dwa':\n\t            if index == 0 or index == 1:\n", "                lambda_weight[:, index] = 1.0\n\t            else:\n\t                w_1 = avg_cost[index - 1, 0] / avg_cost[index - 2, 0]\n\t                w_2 = avg_cost[index - 1, 3] / avg_cost[index - 2, 3]\n\t                lambda_weight[0, index] = 2 * np.exp(w_1 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n\t                lambda_weight[1, index] = 2 * np.exp(w_2 / T) / (np.exp(w_1 / T) + np.exp(w_2 / T))\n\t        # iteration for all batches\n\t        multi_task_model.train()\n\t        train_dataset = iter(train_loader)\n\t        conf_mat = ConfMatrix(multi_task_model.class_nb)\n", "        for k in range(train_batch):\n\t            train_data, train_label, train_depth = train_dataset.next()\n\t            train_data, train_label = train_data.to(device), train_label.long().to(device)\n\t            train_depth = train_depth.to(device)\n\t            train_pred, logsigma = multi_task_model(train_data)\n\t            train_loss = [\n\t                model_fit(train_pred[0], train_label, 'semantic'),\n\t                model_fit(train_pred[1], train_depth, 'depth')\n\t            ]\n\t            train_loss_tmp = [0, 0]\n", "            if opt.weight == 'equal' or opt.weight == 'dwa':\n\t                for i in range(2):\n\t                    train_loss_tmp[i] = train_loss[i] * lambda_weight[i, index]\n\t            else:\n\t                for i in range(2):\n\t                    train_loss_tmp[i] = 1 / (2 * torch.exp(logsigma[i])) * train_loss[i] + logsigma[i] / 2\n\t            optimizer.zero_grad()\n\t            if method == \"graddrop\":\n\t                for i in range(2):\n\t                    if i == 0:\n", "                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = graddrop(grads)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n\t            elif method == \"pcgrad\":\n\t                for i in range(2):\n", "                    if i == 0:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = pcgrad(grads, rng)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n\t            elif method == \"mgd\":\n", "                for i in range(2):\n\t                    if i == 0:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = mgd(grads)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n", "            elif method == \"cagrad\":\n\t                for i in range(2):\n\t                    if i == 0:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = cagrad(grads, alpha, rescale=1)\n\t                overwrite_grad(multi_task_model, g, grad_dims)\n", "                optimizer.step()\n\t            elif method == \"sdmgrad\":\n\t                for i in range(2):\n\t                    if i == 0:\n\t                        train_loss_tmp[i].backward(retain_graph=True)\n\t                    else:\n\t                        train_loss_tmp[i].backward()\n\t                    grad2vec(multi_task_model, grads, grad_dims, i)\n\t                    multi_task_model.zero_grad_shared_modules()\n\t                g = sdmgrad(w, grads, alpha, niter=niter)\n", "                overwrite_grad(multi_task_model, g, grad_dims)\n\t                optimizer.step()\n\t            # accumulate label prediction for every pixel in training images\n\t            conf_mat.update(train_pred[0].argmax(1).flatten(), train_label.flatten())\n\t            cost[0] = train_loss[0].item()\n\t            cost[3] = train_loss[1].item()\n\t            cost[4], cost[5] = depth_error(train_pred[1], train_depth)\n\t            avg_cost[index, :6] += cost[:6] / train_batch\n\t        # compute mIoU and acc\n\t        avg_cost[index, 1:3] = conf_mat.get_metrics()\n", "        # evaluating test data\n\t        multi_task_model.eval()\n\t        conf_mat = ConfMatrix(multi_task_model.class_nb)\n\t        with torch.no_grad():  # operations inside don't track history\n\t            test_dataset = iter(test_loader)\n\t            for k in range(test_batch):\n\t                test_data, test_label, test_depth = test_dataset.next()\n\t                test_data, test_label = test_data.to(device), test_label.long().to(device)\n\t                test_depth = test_depth.to(device)\n\t                test_pred, _ = multi_task_model(test_data)\n", "                test_loss = [\n\t                    model_fit(test_pred[0], test_label, 'semantic'),\n\t                    model_fit(test_pred[1], test_depth, 'depth')\n\t                ]\n\t                conf_mat.update(test_pred[0].argmax(1).flatten(), test_label.flatten())\n\t                cost[6] = test_loss[0].item()\n\t                cost[9] = test_loss[1].item()\n\t                cost[10], cost[11] = depth_error(test_pred[1], test_depth)\n\t                avg_cost[index, 6:] += cost[6:] / test_batch\n\t            # compute mIoU and acc\n", "            avg_cost[index, 7:9] = conf_mat.get_metrics()\n\t        scheduler.step()\n\t        t1 = time.time()\n\t        print(\n\t            'Epoch: {:04d} | TRAIN: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} || TEST: {:.4f} {:.4f} {:.4f} | {:.4f} {:.4f} {:.4f} | TIME: {:.4f}'\n\t            .format(index, avg_cost[index, 0], avg_cost[index, 1], avg_cost[index, 2], avg_cost[index, 3],\n\t                    avg_cost[index, 4], avg_cost[index, 5], avg_cost[index, 6], avg_cost[index, 7], avg_cost[index, 8],\n\t                    avg_cost[index, 9], avg_cost[index, 10], avg_cost[index, 11], t1 - t0))\n\t        torch.save(multi_task_model.state_dict(), f\"models/{method}-{opt.weight}-{alpha}-{opt.seed}.pt\")\n"]}
{"filename": "cityscapes/min_norm_solvers.py", "chunked_list": ["# This code is from\n\t# Multi-Task Learning as Multi-Objective Optimization\n\t# Ozan Sener, Vladlen Koltun\n\t# Neural Information Processing Systems (NeurIPS) 2018\n\t# https://github.com/intel-isl/MultiObjectiveOptimization\n\timport numpy as np\n\timport torch\n\tclass MinNormSolver:\n\t    MAX_ITER = 20\n\t    STOP_CRIT = 1e-5\n", "    def _min_norm_element_from2(v1v1, v1v2, v2v2):\n\t        \"\"\"\n\t        Analytical solution for min_{c} |cx_1 + (1-c)x_2|_2^2\n\t        d is the distance (objective) optimzed\n\t        v1v1 = <x1,x1>\n\t        v1v2 = <x1,x2>\n\t        v2v2 = <x2,x2>\n\t        \"\"\"\n\t        if v1v2 >= v1v1:\n\t            # Case: Fig 1, third column\n", "            gamma = 0.999\n\t            cost = v1v1\n\t            return gamma, cost\n\t        if v1v2 >= v2v2:\n\t            # Case: Fig 1, first column\n\t            gamma = 0.001\n\t            cost = v2v2\n\t            return gamma, cost\n\t        # Case: Fig 1, second column\n\t        gamma = -1.0 * ((v1v2 - v2v2) / (v1v1 + v2v2 - 2 * v1v2))\n", "        cost = v2v2 + gamma * (v1v2 - v2v2)\n\t        return gamma, cost\n\t    def _min_norm_2d(vecs, dps):\n\t        \"\"\"\n\t        Find the minimum norm solution as combination of two points\n\t        This is correct only in 2D\n\t        ie. min_c |\\sum c_i x_i|_2^2 st. \\sum c_i = 1 , 1 >= c_1 >= 0 for all i, c_i + c_j = 1.0 for some i, j\n\t        \"\"\"\n\t        dmin = np.inf\n\t        for i in range(len(vecs)):\n", "            for j in range(i + 1, len(vecs)):\n\t                if (i, j) not in dps:\n\t                    dps[(i, j)] = (vecs[i] * vecs[j]).sum().item()\n\t                    dps[(j, i)] = dps[(i, j)]\n\t                if (i, i) not in dps:\n\t                    dps[(i, i)] = (vecs[i] * vecs[i]).sum().item()\n\t                if (j, j) not in dps:\n\t                    dps[(j, j)] = (vecs[j] * vecs[j]).sum().item()\n\t                c, d = MinNormSolver._min_norm_element_from2(dps[(i, i)], dps[(i, j)], dps[(j, j)])\n\t                if d < dmin:\n", "                    dmin = d\n\t                    sol = [(i, j), c, d]\n\t        return sol, dps\n\t    def _projection2simplex(y):\n\t        \"\"\"\n\t        Given y, it solves argmin_z |y-z|_2 st \\sum z = 1 , 1 >= z_i >= 0 for all i\n\t        \"\"\"\n\t        m = len(y)\n\t        sorted_y = np.flip(np.sort(y), axis=0)\n\t        tmpsum = 0.0\n", "        tmax_f = (np.sum(y) - 1.0) / m\n\t        for i in range(m - 1):\n\t            tmpsum += sorted_y[i]\n\t            tmax = (tmpsum - 1) / (i + 1.0)\n\t            if tmax > sorted_y[i + 1]:\n\t                tmax_f = tmax\n\t                break\n\t        return np.maximum(y - tmax_f, np.zeros(y.shape))\n\t    def _next_point(cur_val, grad, n):\n\t        proj_grad = grad - (np.sum(grad) / n)\n", "        tm1 = -1.0 * cur_val[proj_grad < 0] / proj_grad[proj_grad < 0]\n\t        tm2 = (1.0 - cur_val[proj_grad > 0]) / (proj_grad[proj_grad > 0])\n\t        skippers = np.sum(tm1 < 1e-7) + np.sum(tm2 < 1e-7)\n\t        t = 1\n\t        if len(tm1[tm1 > 1e-7]) > 0:\n\t            t = np.min(tm1[tm1 > 1e-7])\n\t        if len(tm2[tm2 > 1e-7]) > 0:\n\t            t = min(t, np.min(tm2[tm2 > 1e-7]))\n\t        next_point = proj_grad * t + cur_val\n\t        next_point = MinNormSolver._projection2simplex(next_point)\n", "        return next_point\n\t    def find_min_norm_element(vecs):\n\t        \"\"\"\n\t        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n\t        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n\t        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n\t        Hence, we find the best 2-task solution, and then run the projected gradient descent until convergence\n\t        \"\"\"\n\t        # Solution lying at the combination of two points\n\t        dps = {}\n", "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\t        n = len(vecs)\n\t        sol_vec = np.zeros(n)\n\t        sol_vec[init_sol[0][0]] = init_sol[1]\n\t        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\t        if n < 3:\n\t            # This is optimal for n=2, so return the solution\n\t            return sol_vec, init_sol[2]\n\t        iter_count = 0\n\t        grad_mat = np.zeros((n, n))\n", "        for i in range(n):\n\t            for j in range(n):\n\t                grad_mat[i, j] = dps[(i, j)]\n\t        while iter_count < MinNormSolver.MAX_ITER:\n\t            grad_dir = -1.0 * np.dot(grad_mat, sol_vec)\n\t            new_point = MinNormSolver._next_point(sol_vec, grad_dir, n)\n\t            # Re-compute the inner products for line search\n\t            v1v1 = 0.0\n\t            v1v2 = 0.0\n\t            v2v2 = 0.0\n", "            for i in range(n):\n\t                for j in range(n):\n\t                    v1v1 += sol_vec[i] * sol_vec[j] * dps[(i, j)]\n\t                    v1v2 += sol_vec[i] * new_point[j] * dps[(i, j)]\n\t                    v2v2 += new_point[i] * new_point[j] * dps[(i, j)]\n\t            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n\t            new_sol_vec = nc * sol_vec + (1 - nc) * new_point\n\t            change = new_sol_vec - sol_vec\n\t            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n\t                return sol_vec, nd\n", "            sol_vec = new_sol_vec\n\t    def find_min_norm_element_FW(vecs):\n\t        \"\"\"\n\t        Given a list of vectors (vecs), this method finds the minimum norm element in the convex hull\n\t        as min |u|_2 st. u = \\sum c_i vecs[i] and \\sum c_i = 1.\n\t        It is quite geometric, and the main idea is the fact that if d_{ij} = min |u|_2 st u = c x_i + (1-c) x_j; the solution lies in (0, d_{i,j})\n\t        Hence, we find the best 2-task solution, and then run the Frank Wolfe until convergence\n\t        \"\"\"\n\t        # Solution lying at the combination of two points\n\t        dps = {}\n", "        init_sol, dps = MinNormSolver._min_norm_2d(vecs, dps)\n\t        n = len(vecs)\n\t        sol_vec = np.zeros(n)\n\t        sol_vec[init_sol[0][0]] = init_sol[1]\n\t        sol_vec[init_sol[0][1]] = 1 - init_sol[1]\n\t        if n < 3:\n\t            # This is optimal for n=2, so return the solution\n\t            return sol_vec, init_sol[2]\n\t        iter_count = 0\n\t        grad_mat = np.zeros((n, n))\n", "        for i in range(n):\n\t            for j in range(n):\n\t                grad_mat[i, j] = dps[(i, j)]\n\t        while iter_count < MinNormSolver.MAX_ITER:\n\t            t_iter = np.argmin(np.dot(grad_mat, sol_vec))\n\t            v1v1 = np.dot(sol_vec, np.dot(grad_mat, sol_vec))\n\t            v1v2 = np.dot(sol_vec, grad_mat[:, t_iter])\n\t            v2v2 = grad_mat[t_iter, t_iter]\n\t            nc, nd = MinNormSolver._min_norm_element_from2(v1v1, v1v2, v2v2)\n\t            new_sol_vec = nc * sol_vec\n", "            new_sol_vec[t_iter] += 1 - nc\n\t            change = new_sol_vec - sol_vec\n\t            if np.sum(np.abs(change)) < MinNormSolver.STOP_CRIT:\n\t                return sol_vec, nd\n\t            sol_vec = new_sol_vec\n\tdef gradient_normalizers(grads, losses, normalization_type):\n\t    gn = {}\n\t    if normalization_type == 'l2':\n\t        for t in grads:\n\t            gn[t] = np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n", "    elif normalization_type == 'loss':\n\t        for t in grads:\n\t            gn[t] = losses[t]\n\t    elif normalization_type == 'loss+':\n\t        for t in grads:\n\t            gn[t] = losses[t] * np.sqrt(np.sum([gr.pow(2).sum().data[0] for gr in grads[t]]))\n\t    elif normalization_type == 'none':\n\t        for t in grads:\n\t            gn[t] = 1.0\n\t    else:\n", "        print('ERROR: Invalid Normalization Type')\n\t    return gn\n"]}
{"filename": "cityscapes/model_segnet_cross.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Cross')\n\tparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n\tparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\n", "parser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--seed', default=0, type=int, help='control seed')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 7\n", "        # define encoder decoder layers\n\t        self.encoder_block_t = nn.ModuleList(\n\t            [nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)])])\n\t        self.decoder_block_t = nn.ModuleList(\n\t            [nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)])])\n\t        for j in range(2):\n\t            if j < 1:\n\t                self.encoder_block_t.append(\n\t                    nn.ModuleList([self.conv_layer([3, filter[0], filter[0]], bottle_neck=True)]))\n\t                self.decoder_block_t.append(\n", "                    nn.ModuleList([self.conv_layer([filter[0], filter[0], filter[0]], bottle_neck=True)]))\n\t            for i in range(4):\n\t                if i == 0:\n\t                    self.encoder_block_t[j].append(\n\t                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=True))\n\t                    self.decoder_block_t[j].append(\n\t                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=True))\n\t                else:\n\t                    self.encoder_block_t[j].append(\n\t                        self.conv_layer([filter[i], filter[i + 1], filter[i + 1]], bottle_neck=False))\n", "                    self.decoder_block_t[j].append(\n\t                        self.conv_layer([filter[i + 1], filter[i], filter[i]], bottle_neck=False))\n\t        # define cross-stitch units\n\t        self.cs_unit_encoder = nn.Parameter(data=torch.ones(4, 2))\n\t        self.cs_unit_decoder = nn.Parameter(data=torch.ones(5, 2))\n\t        # define task specific layers\n\t        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], bottle_neck=True, pred_layer=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], bottle_neck=True, pred_layer=True)\n\t        #self.pred_task3 = self.conv_layer([filter[0], 3], bottle_neck=True, pred_layer=True)\n\t        # define pooling and unpooling functions\n", "        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5]))\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_uniform_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n", "            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_uniform_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Parameter):\n\t                nn.init.constant(m.weight, 1)\n\t    def conv_layer(self, channel, bottle_neck, pred_layer=False):\n\t        if bottle_neck:\n\t            if not pred_layer:\n\t                conv_block = nn.Sequential(\n\t                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n", "                    nn.BatchNorm2d(channel[1]),\n\t                    nn.ReLU(inplace=True),\n\t                    nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n\t                    nn.BatchNorm2d(channel[2]),\n\t                    nn.ReLU(inplace=True),\n\t                )\n\t            else:\n\t                conv_block = nn.Sequential(\n\t                    nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                    nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n", "                )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(channel[1]),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(in_channels=channel[1], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(channel[1]),\n\t                nn.ReLU(inplace=True),\n\t                nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=3, padding=1),\n", "                nn.BatchNorm2d(channel[2]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        return conv_block\n\t    def forward(self, x):\n\t        encoder_conv_t, decoder_conv_t, encoder_samp_t, decoder_samp_t, indices_t = ([0] * 2 for _ in range(5))\n\t        for i in range(2):\n\t            encoder_conv_t[i], decoder_conv_t[i], encoder_samp_t[i], decoder_samp_t[i], indices_t[i] = (\n\t                [0] * 5 for _ in range(5))\n\t        # task branch 1\n", "        for i in range(5):\n\t            for j in range(2):\n\t                if i == 0:\n\t                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](x)\n\t                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\t                else:\n\t                    encoder_cross_stitch = self.cs_unit_encoder[i - 1][0] * encoder_samp_t[0][i - 1] + \\\n\t                                           self.cs_unit_encoder[i - 1][1] * encoder_samp_t[1][i - 1]\n\t                    #self.cs_unit_encoder[i - 1][2] * encoder_samp_t[2][i - 1]\n\t                    encoder_conv_t[j][i] = self.encoder_block_t[j][i](encoder_cross_stitch)\n", "                    encoder_samp_t[j][i], indices_t[j][i] = self.down_sampling(encoder_conv_t[j][i])\n\t        for i in range(5):\n\t            for j in range(2):\n\t                if i == 0:\n\t                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * encoder_samp_t[0][-1] + \\\n\t                                           self.cs_unit_decoder[i][1] * encoder_samp_t[1][-1]\n\t                    #self.cs_unit_decoder[i][2] * encoder_samp_t[2][-1]\n\t                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n\t                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\t                else:\n", "                    decoder_cross_stitch = self.cs_unit_decoder[i][0] * decoder_conv_t[0][i - 1] + \\\n\t                                           self.cs_unit_decoder[i][1] * decoder_conv_t[1][i - 1]\n\t                    #self.cs_unit_decoder[i][2] * decoder_conv_t[2][i - 1]\n\t                    decoder_samp_t[j][i] = self.up_sampling(decoder_cross_stitch, indices_t[j][-i - 1])\n\t                    decoder_conv_t[j][i] = self.decoder_block_t[j][-i - 1](decoder_samp_t[j][i])\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(decoder_conv_t[0][-1]), dim=1)\n\t        t2_pred = self.pred_task2(decoder_conv_t[1][-1])\n\t        #t3_pred = self.pred_task3(decoder_conv_t[2][-1])\n\t        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n", "        return [t1_pred, t2_pred], self.logsigma\n\tcontrol_seed(opt.seed)\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_CROSS = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_CROSS.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_CROSS),\n\t                                                         count_parameters(SegNet_CROSS) / 24981069))\n\tprint(\n", "    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation on CityScapes.')\n\telse:\n\t    train_set = CityScapes(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\ttest_set = CityScapes(root=dataset_path, train=False)\n", "batch_size = 8\n\ttrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ttest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate multi-task network\n\tmulti_task_trainer(train_loader, test_loader, SegNet_CROSS, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "cityscapes/model_segnet_mt.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\timport torch.utils.data.sampler as sampler\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Multi-task: Attention Network')\n\tparser.add_argument('--method', default='sdmgrad', type=str, help='which optimization algorithm to use')\n\tparser.add_argument('--weight', default='equal', type=str, help='multi-task weighting: equal, uncert, dwa')\n", "parser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\n\tparser.add_argument('--temp', default=2.0, type=float, help='temperature for DWA (must be positive)')\n\tparser.add_argument('--alpha', default=0.3, type=float, help='the alpha')\n\tparser.add_argument('--lr', default=1e-4, type=float, help='the learning rate')\n\tparser.add_argument('--seed', default=1, type=int, help='control seed')\n\tparser.add_argument('--niter', default=20, type=int, help='number of inner iteration')\n\tparser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n", "        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 7\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n\t        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n", "        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n", "                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        # define task attention layers\n\t        self.encoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])])])\n\t        self.decoder_att = nn.ModuleList([nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])])])\n\t        self.encoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[1]])])\n\t        self.decoder_block_att = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for j in range(2):\n\t            if j < 1:\n", "                self.encoder_att.append(nn.ModuleList([self.att_layer([filter[0], filter[0], filter[0]])]))\n\t                self.decoder_att.append(nn.ModuleList([self.att_layer([2 * filter[0], filter[0], filter[0]])]))\n\t            for i in range(4):\n\t                self.encoder_att[j].append(self.att_layer([2 * filter[i + 1], filter[i + 1], filter[i + 1]]))\n\t                self.decoder_att[j].append(self.att_layer([filter[i + 1] + filter[i], filter[i], filter[i]]))\n\t        for i in range(4):\n\t            if i < 3:\n\t                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 2]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t            else:\n", "                self.encoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t                self.decoder_block_att.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n\t        self.pred_task1 = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        self.pred_task2 = self.conv_layer([filter[0], 1], pred=True)\n\t        #self.pred_task3 = self.conv_layer([filter[0], 3], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        self.logsigma = nn.Parameter(torch.FloatTensor([-0.5, -0.5, -0.5]))\n\t        for m in self.modules():\n", "            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def shared_modules(self):\n", "        return [\n\t            self.encoder_block, self.decoder_block, self.conv_block_enc, self.conv_block_dec, self.encoder_block_att,\n\t            self.decoder_block_att, self.down_sampling, self.up_sampling\n\t        ]\n\t    def zero_grad_shared_modules(self):\n\t        for mm in self.shared_modules():\n\t            mm.zero_grad()\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n", "                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n\t            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n", "    def att_layer(self, channel):\n\t        att_block = nn.Sequential(\n\t            nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[1]),\n\t            nn.ReLU(inplace=True),\n\t            nn.Conv2d(in_channels=channel[1], out_channels=channel[2], kernel_size=1, padding=0),\n\t            nn.BatchNorm2d(channel[2]),\n\t            nn.Sigmoid(),\n\t        )\n\t        return att_block\n", "    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n\t            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define attention list for tasks\n\t        atten_encoder, atten_decoder = ([0] * 2 for _ in range(2))\n\t        for i in range(2):\n\t            atten_encoder[i], atten_decoder[i] = ([0] * 5 for _ in range(2))\n\t        for i in range(2):\n\t            for j in range(5):\n", "                atten_encoder[i][j], atten_decoder[i][j] = ([0] * 3 for _ in range(2))\n\t        # define global shared network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n", "                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n", "        # define task dependent attention module\n\t        for i in range(2):\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](g_encoder[j][0])\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t                else:\n\t                    atten_encoder[i][j][0] = self.encoder_att[i][j](torch.cat(\n", "                        (g_encoder[j][0], atten_encoder[i][j - 1][2]), dim=1))\n\t                    atten_encoder[i][j][1] = (atten_encoder[i][j][0]) * g_encoder[j][1]\n\t                    atten_encoder[i][j][2] = self.encoder_block_att[j](atten_encoder[i][j][1])\n\t                    atten_encoder[i][j][2] = F.max_pool2d(atten_encoder[i][j][2], kernel_size=2, stride=2)\n\t            for j in range(5):\n\t                if j == 0:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_encoder[i][-1][-1],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n", "                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n\t                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t                else:\n\t                    atten_decoder[i][j][0] = F.interpolate(atten_decoder[i][j - 1][2],\n\t                                                           scale_factor=2,\n\t                                                           mode='bilinear',\n\t                                                           align_corners=True)\n\t                    atten_decoder[i][j][0] = self.decoder_block_att[-j - 1](atten_decoder[i][j][0])\n", "                    atten_decoder[i][j][1] = self.decoder_att[i][-j - 1](torch.cat(\n\t                        (g_upsampl[j], atten_decoder[i][j][0]), dim=1))\n\t                    atten_decoder[i][j][2] = (atten_decoder[i][j][1]) * g_decoder[j][-1]\n\t        # define task prediction layers\n\t        t1_pred = F.log_softmax(self.pred_task1(atten_decoder[0][-1][-1]), dim=1)\n\t        t2_pred = self.pred_task2(atten_decoder[1][-1][-1])\n\t        #t3_pred = self.pred_task3(atten_decoder[2][-1][-1])\n\t        #t3_pred = t3_pred / torch.norm(t3_pred, p=2, dim=1, keepdim=True)\n\t        return [t1_pred, t2_pred], self.logsigma\n\tcontrol_seed(opt.seed)\n", "# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet_MTAN = SegNet().to(device)\n\toptimizer = optim.Adam(SegNet_MTAN.parameters(), lr=opt.lr)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet_MTAN),\n\t                                                         count_parameters(SegNet_MTAN) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n", "dataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation.')\n\telse:\n\t    train_set = CityScapes(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\ttest_set = CityScapes(root=dataset_path, train=False)\n\tbatch_size = 8\n\ttrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n", "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate multi-task network\n\tmulti_task_rg_trainer(train_loader, test_loader, SegNet_MTAN, device, optimizer, scheduler, opt, 200)\n"]}
{"filename": "cityscapes/create_dataset.py", "chunked_list": ["from torch.utils.data.dataset import Dataset\n\timport os\n\timport torch\n\timport torch.nn.functional as F\n\timport fnmatch\n\timport numpy as np\n\timport random\n\tclass RandomScaleCrop(object):\n\t    \"\"\"\n\t    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n", "    \"\"\"\n\t    def __init__(self, scale=[1.0, 1.2, 1.5]):\n\t        self.scale = scale\n\t    def __call__(self, img, label, depth, normal):\n\t        height, width = img.shape[-2:]\n\t        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n\t        h, w = int(height / sc), int(width / sc)\n\t        i = random.randint(0, height - h)\n\t        j = random.randint(0, width - w)\n\t        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n", "                             align_corners=True).squeeze(0)\n\t        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n\t                               mode='nearest').squeeze(0).squeeze(0)\n\t        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n\t        normal_ = F.interpolate(normal[None, :, i:i + h, j:j + w],\n\t                                size=(height, width),\n\t                                mode='bilinear',\n\t                                align_corners=True).squeeze(0)\n\t        return img_, label_, depth_ / sc, normal_\n\tclass RandomScaleCropCityScapes(object):\n", "    \"\"\"\n\t    Credit to Jialong Wu from https://github.com/lorenmt/mtan/issues/34.\n\t    \"\"\"\n\t    def __init__(self, scale=[1.0, 1.2, 1.5]):\n\t        self.scale = scale\n\t    def __call__(self, img, label, depth):\n\t        height, width = img.shape[-2:]\n\t        sc = self.scale[random.randint(0, len(self.scale) - 1)]\n\t        h, w = int(height / sc), int(width / sc)\n\t        i = random.randint(0, height - h)\n", "        j = random.randint(0, width - w)\n\t        img_ = F.interpolate(img[None, :, i:i + h, j:j + w], size=(height, width), mode='bilinear',\n\t                             align_corners=True).squeeze(0)\n\t        label_ = F.interpolate(label[None, None, i:i + h, j:j + w], size=(height, width),\n\t                               mode='nearest').squeeze(0).squeeze(0)\n\t        depth_ = F.interpolate(depth[None, :, i:i + h, j:j + w], size=(height, width), mode='nearest').squeeze(0)\n\t        return img_, label_, depth_ / sc\n\tclass NYUv2(Dataset):\n\t    \"\"\"\n\t    We could further improve the performance with the data augmentation of NYUv2 defined in:\n", "        [1] PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing\n\t        [2] Pattern affinitive propagation across depth, surface normal and semantic segmentation\n\t        [3] Mti-net: Multiscale task interaction networks for multi-task learning\n\t        1. Random scale in a selected raio 1.0, 1.2, and 1.5.\n\t        2. Random horizontal flip.\n\t    Please note that: all baselines and MTAN did NOT apply data augmentation in the original paper.\n\t    \"\"\"\n\t    def __init__(self, root, train=True, augmentation=False):\n\t        self.train = train\n\t        self.root = os.path.expanduser(root)\n", "        self.augmentation = augmentation\n\t        # read the data file\n\t        if train:\n\t            self.data_path = root + '/train'\n\t        else:\n\t            self.data_path = root + '/val'\n\t        # calculate data length\n\t        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n\t    def __getitem__(self, index):\n\t        # load data from the pre-processed npy files\n", "        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n\t        semantic = torch.from_numpy(np.load(self.data_path + '/label/{:d}.npy'.format(index)))\n\t        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n\t        normal = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/normal/{:d}.npy'.format(index)), -1, 0))\n\t        # apply data augmentation if required\n\t        if self.augmentation:\n\t            image, semantic, depth, normal = RandomScaleCrop()(image, semantic, depth, normal)\n\t            if torch.rand(1) < 0.5:\n\t                image = torch.flip(image, dims=[2])\n\t                semantic = torch.flip(semantic, dims=[1])\n", "                depth = torch.flip(depth, dims=[2])\n\t                normal = torch.flip(normal, dims=[2])\n\t                normal[0, :, :] = -normal[0, :, :]\n\t        return image.float(), semantic.float(), depth.float(), normal.float()\n\t    def __len__(self):\n\t        return self.data_len\n\tclass CityScapes(Dataset):\n\t    \"\"\"\n\t    We could further improve the performance with the data augmentation of NYUv2 defined in:\n\t        [1] PAD-Net: Multi-Tasks Guided Prediction-and-Distillation Network for Simultaneous Depth Estimation and Scene Parsing\n", "        [2] Pattern affinitive propagation across depth, surface normal and semantic segmentation\n\t        [3] Mti-net: Multiscale task interaction networks for multi-task learning\n\t        1. Random scale in a selected raio 1.0, 1.2, and 1.5.\n\t        2. Random horizontal flip.\n\t    Please note that: all baselines and MTAN did NOT apply data augmentation in the original paper.\n\t    \"\"\"\n\t    def __init__(self, root, train=True, augmentation=False):\n\t        self.train = train\n\t        self.root = os.path.expanduser(root)\n\t        self.augmentation = augmentation\n", "        # read the data file\n\t        if train:\n\t            self.data_path = root + '/train'\n\t        else:\n\t            self.data_path = root + '/val'\n\t        # calculate data length\n\t        self.data_len = len(fnmatch.filter(os.listdir(self.data_path + '/image'), '*.npy'))\n\t    def __getitem__(self, index):\n\t        # load data from the pre-processed npy files\n\t        image = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/image/{:d}.npy'.format(index)), -1, 0))\n", "        semantic = torch.from_numpy(np.load(self.data_path + '/label_7/{:d}.npy'.format(index)))\n\t        depth = torch.from_numpy(np.moveaxis(np.load(self.data_path + '/depth/{:d}.npy'.format(index)), -1, 0))\n\t        # apply data augmentation if required\n\t        if self.augmentation:\n\t            image, semantic, depth = RandomScaleCropCityScapes()(image, semantic, depth)\n\t            if torch.rand(1) < 0.5:\n\t                image = torch.flip(image, dims=[2])\n\t                semantic = torch.flip(semantic, dims=[1])\n\t                depth = torch.flip(depth, dims=[2])\n\t        return image.float(), semantic.float(), depth.float()\n", "    def __len__(self):\n\t        return self.data_len\n"]}
{"filename": "cityscapes/model_segnet_single.py", "chunked_list": ["import torch.nn as nn\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\timport argparse\n\tfrom create_dataset import *\n\tfrom utils import *\n\tparser = argparse.ArgumentParser(description='Single-task: One Task')\n\tparser.add_argument('--task', default='semantic', type=str, help='choose task: semantic, depth')\n\tparser.add_argument('--dataroot', default='cityscapes', type=str, help='dataset root')\n\tparser.add_argument('--seed', default=0, type=int, help='control seed')\n", "parser.add_argument('--apply_augmentation', action='store_true', help='toggle to apply data augmentation on NYUv2')\n\topt = parser.parse_args()\n\tclass SegNet(nn.Module):\n\t    def __init__(self):\n\t        super(SegNet, self).__init__()\n\t        # initialise network parameters\n\t        filter = [64, 128, 256, 512, 512]\n\t        self.class_nb = 7\n\t        # define encoder decoder layers\n\t        self.encoder_block = nn.ModuleList([self.conv_layer([3, filter[0]])])\n", "        self.decoder_block = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            self.encoder_block.append(self.conv_layer([filter[i], filter[i + 1]]))\n\t            self.decoder_block.append(self.conv_layer([filter[i + 1], filter[i]]))\n\t        # define convolution layer\n\t        self.conv_block_enc = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        self.conv_block_dec = nn.ModuleList([self.conv_layer([filter[0], filter[0]])])\n\t        for i in range(4):\n\t            if i == 0:\n\t                self.conv_block_enc.append(self.conv_layer([filter[i + 1], filter[i + 1]]))\n", "                self.conv_block_dec.append(self.conv_layer([filter[i], filter[i]]))\n\t            else:\n\t                self.conv_block_enc.append(\n\t                    nn.Sequential(self.conv_layer([filter[i + 1], filter[i + 1]]),\n\t                                  self.conv_layer([filter[i + 1], filter[i + 1]])))\n\t                self.conv_block_dec.append(\n\t                    nn.Sequential(self.conv_layer([filter[i], filter[i]]), self.conv_layer([filter[i], filter[i]])))\n\t        if opt.task == 'semantic':\n\t            self.pred_task = self.conv_layer([filter[0], self.class_nb], pred=True)\n\t        if opt.task == 'depth':\n", "            self.pred_task = self.conv_layer([filter[0], 1], pred=True)\n\t        # define pooling and unpooling functions\n\t        self.down_sampling = nn.MaxPool2d(kernel_size=2, stride=2, return_indices=True)\n\t        self.up_sampling = nn.MaxUnpool2d(kernel_size=2, stride=2)\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n", "                nn.init.constant_(m.bias, 0)\n\t            elif isinstance(m, nn.Linear):\n\t                nn.init.xavier_normal_(m.weight)\n\t                nn.init.constant_(m.bias, 0)\n\t    def conv_layer(self, channel, pred=False):\n\t        if not pred:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=3, padding=1),\n\t                nn.BatchNorm2d(num_features=channel[1]),\n\t                nn.ReLU(inplace=True),\n", "            )\n\t        else:\n\t            conv_block = nn.Sequential(\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[0], kernel_size=3, padding=1),\n\t                nn.Conv2d(in_channels=channel[0], out_channels=channel[1], kernel_size=1, padding=0),\n\t            )\n\t        return conv_block\n\t    def forward(self, x):\n\t        g_encoder, g_decoder, g_maxpool, g_upsampl, indices = ([0] * 5 for _ in range(5))\n\t        for i in range(5):\n", "            g_encoder[i], g_decoder[-i - 1] = ([0] * 2 for _ in range(2))\n\t        # define global shared network\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_encoder[i][0] = self.encoder_block[i](x)\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n\t                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t            else:\n\t                g_encoder[i][0] = self.encoder_block[i](g_maxpool[i - 1])\n\t                g_encoder[i][1] = self.conv_block_enc[i](g_encoder[i][0])\n", "                g_maxpool[i], indices[i] = self.down_sampling(g_encoder[i][1])\n\t        for i in range(5):\n\t            if i == 0:\n\t                g_upsampl[i] = self.up_sampling(g_maxpool[-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n\t            else:\n\t                g_upsampl[i] = self.up_sampling(g_decoder[i - 1][-1], indices[-i - 1])\n\t                g_decoder[i][0] = self.decoder_block[-i - 1](g_upsampl[i])\n\t                g_decoder[i][1] = self.conv_block_dec[-i - 1](g_decoder[i][0])\n", "        # define task prediction layers\n\t        if opt.task == 'semantic':\n\t            pred = F.log_softmax(self.pred_task(g_decoder[-1][-1]), dim=1)\n\t        if opt.task == 'depth':\n\t            pred = self.pred_task(g_decoder[-1][-1])\n\t        return pred\n\tcontrol_seed(opt.seed)\n\t# define model, optimiser and scheduler\n\tdevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\tSegNet = SegNet().to(device)\n", "optimizer = optim.Adam(SegNet.parameters(), lr=1e-4)\n\tscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.5)\n\tprint('Parameter Space: ABS: {:.1f}, REL: {:.4f}'.format(count_parameters(SegNet), count_parameters(SegNet) / 24981069))\n\tprint(\n\t    'LOSS FORMAT: SEMANTIC_LOSS MEAN_IOU PIX_ACC | DEPTH_LOSS ABS_ERR REL_ERR | NORMAL_LOSS MEAN MED <11.25 <22.5 <30')\n\t# define dataset\n\tdataset_path = opt.dataroot\n\tif opt.apply_augmentation:\n\t    train_set = CityScapes(root=dataset_path, train=True, augmentation=True)\n\t    print('Applying data augmentation.')\n", "else:\n\t    train_set = CityScapes(root=dataset_path, train=True)\n\t    print('Standard training strategy without data augmentation.')\n\ttest_set = CityScapes(root=dataset_path, train=False)\n\tbatch_size = 8\n\ttrain_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n\ttest_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n\t# Train and evaluate single-task network\n\tsingle_task_trainer(train_loader, test_loader, SegNet, device, optimizer, scheduler, opt, 200)\n"]}
