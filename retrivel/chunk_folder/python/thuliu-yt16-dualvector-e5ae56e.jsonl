{"filename": "train.py", "chunked_list": ["import argparse\n\timport os\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--config')\n\tparser.add_argument('--name', default=None)\n\tparser.add_argument('--tag', default=None)\n\tparser.add_argument('--gpu', default='0')\n\tparser.add_argument('--resume', default=None)\n\tparser.add_argument('--seed', default=None, type=int)\n\targs = parser.parse_args()\n", "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n\timport sys\n\timport copy\n\timport random\n\timport time\n\timport shutil\n\tfrom PIL import Image\n\timport yaml\n\timport numpy as np\n\timport torch\n", "import torch.nn as nn\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import DataLoader\n\timport torch.nn.functional as F\n\timport math\n\timport torchvision\n\tfrom einops import repeat, rearrange, reduce, parse_shape\n\timport datasets\n\timport losses\n\timport models\n", "import utils\n\tfrom models import make_lr_scheduler\n\timport warnings\n\twarnings.filterwarnings(\"ignore\")\n\tdef seed_all(seed):\n\t    log(f'Global seed set to {seed}')\n\t    random.seed(seed) # Python\n\t    np.random.seed(seed) # cpu vars\n\t    torch.manual_seed(seed) # cpu vars\n\t    if torch.cuda.is_available(): \n", "        torch.cuda.manual_seed(seed)\n\t        torch.cuda.manual_seed_all(seed) # gpu vars\n\t        torch.backends.cudnn.deterministic = True  #needed\n\t        torch.backends.cudnn.benchmark = False\n\tdef make_data_loader(spec, tag=''):\n\t    if spec is None:\n\t        return None\n\t    dataset = datasets.make(spec['dataset'])\n\t    log('{} dataset: size={}'.format(tag, len(dataset)))\n\t    for k, v in dataset[0].items():\n", "        if type(v) is torch.Tensor:\n\t            log('  {}: shape={}, dtype={}'.format(k, tuple(v.shape), v.dtype))\n\t        else:\n\t            log('  {}: type={}'.format(k, type(v)))\n\t    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n\t        shuffle=spec['shuffle'], num_workers=min(spec['batch_size'], os.cpu_count(), 32), pin_memory=True)\n\t    return loader\n\tdef make_data_loaders():\n\t    train_loader = make_data_loader(config.get('train_dataset'), tag='train')\n\t    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n", "    return train_loader, val_loader\n\tdef prepare_training():\n\t    if config.get('resume') is not None:\n\t        log('resume from {}'.format(config['resume']))\n\t        sv_file = torch.load(config['resume'])\n\t        if not utils.same_dict(sv_file['model'], config['model'], {'sd'}) or \\\n\t           not utils.same_dict(sv_file['optimizer'], config['optimizer'], {'sd'}):\n\t            print('from ckpt:')\n\t            print(yaml.dump(utils.without(sv_file['model'], {'sd'})))\n\t            print('from config:')\n", "            print(yaml.dump(config['model']))\n\t            which_one, _ = utils.input_checkbox(['ckpt', 'config'], msg='Model/Optimizer configs are different')\n\t            print('you select', which_one)\n\t            if which_one == 'config':\n\t                sv_file['model'].update(config['model'])\n\t                sv_file['optimizer'].update(config['optimizer'])\n\t        model = models.make(sv_file['model'], load_sd=True).cuda()\n\t        optimizer = utils.make_optimizer(\n\t            model.parameters(), sv_file['optimizer'], load_sd=config['load_optimizer'])\n\t        lr_scheduler = make_lr_scheduler(optimizer, config.get('scheduler'))\n", "        if config.get('run_step') is not None and config['run_step']:\n\t            epoch_start = sv_file['epoch'] + 1\n\t            for _ in range(epoch_start - 1):\n\t                lr_scheduler.step()\n\t        else:\n\t            epoch_start = 1\n\t    else:\n\t        model = models.make(config['model']).cuda()\n\t        optimizer = utils.make_optimizer(\n\t            model.parameters(), config['optimizer'])\n", "        epoch_start = 1\n\t        lr_scheduler = make_lr_scheduler(optimizer, config.get('scheduler'))\n\t    log('model: #params={}'.format(utils.compute_num_params(model, text=True)))\n\t    loss_fn = losses.make(config['loss'])\n\t    return model, optimizer, epoch_start, lr_scheduler, loss_fn\n\tdef debug(model):\n\t    has_nan = False\n\t    v_n = []\n\t    v_v = []\n\t    v_g = []\n", "    for name, parameter in model.named_parameters():\n\t        v_n.append(name)\n\t        v_v.append(parameter.detach().cpu() if parameter is not None else torch.zeros(1))\n\t        v_g.append(parameter.grad.detach().cpu() if parameter.grad is not None else torch.zeros(1))\n\t        has_nan = has_nan or \\\n\t            torch.any(torch.isnan(v_v[-1])) or \\\n\t            torch.any(torch.isnan(v_g[-1]))\n\t    if has_nan:\n\t        for i in range(len(v_n)):\n\t            print(f'value {v_n[i]}: {v_v[i].min().item():.3e} ~ {v_v[i].max().item():.3e}')\n", "            print(f'grad  {v_n[i]}: {v_g[i].min().item():.3e} ~ {v_g[i].max().item():.3e}')\n\t        exit(0)\n\tdef train(train_loader, model, optimizer, loss_fn, epoch):\n\t    global global_step\n\t    model.train()\n\t    train_loss = utils.Averager()\n\t    with tqdm(train_loader, leave=False, desc='train') as pbar:\n\t        for batch in pbar:\n\t            for k, v in batch.items():\n\t                if type(v) is torch.Tensor:\n", "                    batch[k] = v.cuda()\n\t            # with torch.autograd.detect_anomaly():\n\t            out = model(batch, epoch=epoch, train=True)\n\t            list_of_loss = loss_fn(out, batch, epoch=epoch)\n\t            loss = list_of_loss['loss']\n\t            train_loss.add(loss.item())\n\t            optimizer.zero_grad()\n\t            loss.backward()\n\t            optimizer.step()\n\t            loss = None\n", "            writer.add_scalars('step loss', list_of_loss, global_step)\n\t            global_step += 1\n\t            for k, v in list_of_loss.items():\n\t                list_of_loss[k] = f'{v.item():.6f}'\n\t            pbar.set_postfix({\n\t                **list_of_loss,\n\t                'avg': train_loss.item()\n\t            })\n\t    return train_loss.item()\n\tdef val(val_loader, model, img_path, epoch):\n", "    os.makedirs(img_path, exist_ok=True)\n\t    model.eval()\n\t    dim = 128\n\t    _xy = utils.make_coord((dim, dim)).cuda()\n\t    with torch.no_grad():\n\t        with tqdm(val_loader, leave=False, desc='val') as pbar:\n\t            for batch in val_loader:\n\t                b = 0\n\t                for k, v in batch.items():\n\t                    if type(v) is torch.Tensor:\n", "                        batch[k] = v.cuda()\n\t                        b = v.size(0)\n\t                xy = repeat(_xy, 'n d -> b n d', b=b)\n\t                batch['xy'] = xy\n\t                out = model(batch, epoch=epoch, train=False)\n\t                curves = out['curves']\n\t                curves_np = curves.detach().cpu().numpy()\n\t                if 'occ' in out:\n\t                    occ_img = rearrange(out['occ'], 'b (dim1 dim2) -> b () dim1 dim2', dim1=dim).detach().cpu()\n\t                if 'iter_occs' in out:\n", "                    iters_occ_img = rearrange(out['iter_occs'][-1], 'b (dim1 dim2) -> b dim1 dim2', dim1=dim).detach().cpu()\n\t                for i in range(b):\n\t                    curve_np = curves_np[i]\n\t                    filename = os.path.join(img_path, f\"{batch['index'][i]}.svg\")\n\t                    shutil.copyfile(batch['img_path'][i], filename.replace('.svg', '.png'))\n\t                    if 'img' in batch:\n\t                        utils.tensor_to_image(batch['img'][i, 0], filename.replace('.svg', '_inp.png'))\n\t                    if 'refs' in batch:\n\t                        n_ref = batch['n_ref'][i]\n\t                        grid = torchvision.utils.make_grid(batch['refs'][i], nrow=5)\n", "                        utils.tensor_to_image(grid[0], filename.replace('.svg', '_refs.png'))\n\t                    if 'full_img' in batch:\n\t                        utils.tensor_to_image(batch['full_img'][i, 0], filename.replace('.svg', '_full.png'))\n\t                    if 'img_origin_res' in batch:\n\t                        utils.tensor_to_image(batch['img_origin_res'][i, 0], filename.replace('.svg', '_origin_res.png'))\n\t                    if 'rec' in out:\n\t                        utils.tensor_to_image(out['rec'][i, 0], filename.replace('.svg', '_rec.png'))\n\t                    if 'dis' in out:\n\t                        utils.tensor_to_image(out['dis'][i].view(dim, dim) + 0.5, filename.replace('.svg', '_dis.png'))\n\t                    if 'rendered' in out:\n", "                        Image.fromarray((out['rendered'][i, 0].cpu().numpy() * 255).astype(np.uint8)).save(filename.replace('.svg', '_render.png'))\n\t                    if 'occ' in out:\n\t                        Image.fromarray((occ_img[i, 0].numpy() * 255).astype(np.uint8)).save(filename.replace('.svg', '_occ.png'))\n\t                    if 'iter_occs' in out:\n\t                        utils.tensor_to_image(iters_occ_img[i], filename.replace('.svg', '_occ_iter.png'))\n\t                    if 'sdf' in out:\n\t                        sdf_img = rearrange(torch.sigmoid(out['sdf']), 'b (dim1 dim2) -> b dim1 dim2', dim1=dim).detach().cpu().numpy()\n\t                        Image.fromarray((sdf_img[i] * 255).astype(np.uint8)).save(filename.replace('.svg', '_sdf.png'))\n\t                    if hasattr(model, 'write_paths_to_svg'):\n\t                        utils.curves_to_svg(curve_np, filename)\n", "                        model.write_paths_to_svg(curve_np, os.path.join(img_path, f\"{batch['index'][i]}_mask.svg\"))\n\t                break\n\t    return None\n\tdef main(config_, save_path):\n\t    global config, log, writer, global_step\n\t    config = config_\n\t    log, writer = utils.set_save_path(save_path)\n\t    global_step = 0\n\t    seed_all(config['seed'])\n\t    with open(os.path.join(save_path, 'config.yaml'), 'w') as f:\n", "        yaml.dump(config, f, sort_keys=False)\n\t    ckpt_path = os.path.join(save_path, 'ckpt')\n\t    img_path = os.path.join(save_path, 'img')\n\t    os.makedirs(ckpt_path, exist_ok=True)\n\t    os.makedirs(img_path, exist_ok=True)\n\t    train_loader, val_loader = make_data_loaders()\n\t    model, optimizer, epoch_start, lr_scheduler, loss_fn = prepare_training()\n\t    model.init()\n\t    n_gpus = len(os.environ['CUDA_VISIBLE_DEVICES'].split(','))\n\t    if n_gpus > 1:\n", "        model = nn.parallel.DataParallel(model)\n\t    epoch_max = config['epoch_max']\n\t    epoch_val = config.get('epoch_val')\n\t    epoch_save = config.get('epoch_save')\n\t    timer = utils.Timer()\n\t    val(val_loader, model, os.path.join(img_path, 'test'), epoch=0)\n\t    for epoch in range(epoch_start, epoch_max + 1):\n\t        t_epoch_start = timer.t()\n\t        log_info = ['epoch {}/{}'.format(epoch, epoch_max)]\n\t        writer.add_scalar('lr', optimizer.param_groups[0]['lr'], epoch)\n", "        train_loss = train(train_loader, model, optimizer, loss_fn, epoch)\n\t        if lr_scheduler is not None:\n\t            lr_scheduler.step()\n\t        log_info.append('train: loss={:.4f}'.format(train_loss))\n\t        writer.add_scalars('loss', {'train': train_loss}, epoch)\n\t        if n_gpus > 1:\n\t            model_ = model.module\n\t        else:\n\t            model_ = model\n\t        model_spec = copy.deepcopy(config['model'])\n", "        model_spec['sd'] = model_.state_dict()\n\t        optimizer_spec = copy.deepcopy(config['optimizer'])\n\t        optimizer_spec['sd'] = optimizer.state_dict()\n\t        sv_file = {\n\t            'model': model_spec,\n\t            'optimizer': optimizer_spec,\n\t            'epoch': epoch,\n\t            'config': config,\n\t        }\n\t        torch.save(sv_file, os.path.join(ckpt_path, 'epoch-last.pth'))\n", "        if (epoch_save is not None) and (epoch % epoch_save == 0):\n\t            torch.save(sv_file,\n\t                os.path.join(ckpt_path, 'epoch-{}.pth'.format(epoch)))\n\t        if (epoch_val is not None) and (epoch % epoch_val == 0):\n\t            if n_gpus > 1:\n\t                model_ = model.module\n\t            else:\n\t                model_ = model\n\t            val(val_loader, model_, os.path.join(img_path, str(epoch)), epoch)\n\t        t = timer.t()\n", "        prog = (epoch - epoch_start + 1) / (epoch_max - epoch_start + 1)\n\t        t_epoch = utils.time_text(t - t_epoch_start)\n\t        t_elapsed, t_all = utils.time_text(t), utils.time_text(t / prog)\n\t        log_info.append('{} {}/{}'.format(t_epoch, t_elapsed, t_all))\n\t        log(', '.join(log_info))\n\t        writer.flush()\n\tif __name__ == '__main__':\n\t    with open(args.config, 'r') as f:\n\t        config = yaml.load(f, Loader=yaml.FullLoader)\n\t        print('config loaded.')\n", "    save_name = args.name\n\t    if save_name is None:\n\t        save_name = '_' + args.config.split('/')[-1][:-len('.yaml')]\n\t    if args.tag is not None:\n\t        save_name += '_' + args.tag\n\t    save_path = os.path.join('./save', save_name)\n\t    if args.seed is None:\n\t        if 'seed' not in config:\n\t            config['seed'] = int(time.time() * 1000) % 1000\n\t    else:\n", "        config['seed'] = args.seed\n\t    config['cmd_args'] = sys.argv\n\t    config['resume'] = args.resume\n\t    main(config, save_path)\n"]}
{"filename": "utils.py", "chunked_list": ["import os\n\timport time\n\timport shutil\n\timport math\n\timport numpy as np\n\tfrom PIL import Image\n\timport torch\n\tfrom torch import nn\n\timport numpy as np\n\tfrom torch.optim import SGD, Adam\n", "# from optims import NAdam\n\tfrom tensorboardX import SummaryWriter\n\timport fnmatch\n\tclass Averager():\n\t    def __init__(self):\n\t        self.n = 0.0\n\t        self.v = 0.0\n\t    def add(self, v, n=1.0):\n\t        self.v = (self.v * self.n + v * n) / (self.n + n)\n\t        self.n += n\n", "    def item(self):\n\t        return self.v\n\tclass Timer():\n\t    def __init__(self):\n\t        self.v = time.time()\n\t    def s(self):\n\t        self.v = time.time()\n\t    def t(self):\n\t        return time.time() - self.v\n\tdef time_text(t):\n", "    if t >= 3600:\n\t        return '{:.1f}h'.format(t / 3600)\n\t    elif t >= 60:\n\t        return '{:.1f}m'.format(t / 60)\n\t    else:\n\t        return '{:.1f}s'.format(t)\n\t_log_path = None\n\tdef set_log_path(path):\n\t    global _log_path\n\t    _log_path = path\n", "def log(obj, filename='log.txt'):\n\t    print(obj)\n\t    if _log_path is not None:\n\t        with open(os.path.join(_log_path, filename), 'a') as f:\n\t            print(obj, file=f)\n\tdef without(d, exc_keys):\n\t    dct1 = {k: v for k, v in d.items() if k not in exc_keys}\n\t    return dct1\n\tdef same_dict(d1, d2, exc_keys):\n\t    return without(d1, exc_keys) == without(d2, exc_keys)\n", "def input_checkbox(opts, keys=None, default='0', msg=''):\n\t    prompts = [msg, f'Default: {default}'] + [f'\\t{i}. {opt}' for i, opt in enumerate(opts)] + [f'Please select: ']\n\t    if keys is None:\n\t        keys = list(map(str, range(len(opts))))\n\t    ind = input('\\n'.join(prompts))\n\t    while(ind not in keys and ind != ''):\n\t        ind = input('Illegal input, input again: ')\n\t    ind = keys.index(default if ind == '' else ind)\n\t    return opts[ind], ind\n\tdef ensure_path(path, remove=True):\n", "    basename = os.path.basename(path.rstrip('/'))\n\t    if os.path.exists(path):\n\t        if remove and (basename.startswith('_') or input('{} exists, remove? (y/[n]): '.format(path)) == 'y'):\n\t            shutil.rmtree(path)\n\t            os.makedirs(path)\n\t        else:\n\t            os.makedirs(path, exist_ok=True)\n\t    else:\n\t        os.makedirs(path)\n\tdef include_patterns(*patterns):\n", "    \"\"\"Factory function that can be used with copytree() ignore parameter.\n\t    Arguments define a sequence of glob-style patterns\n\t    that are used to specify what files to NOT ignore.\n\t    Creates and returns a function that determines this for each directory\n\t    in the file hierarchy rooted at the source directory when used with\n\t    shutil.copytree().\n\t    \"\"\"\n\t    def _ignore_patterns(path, names):\n\t        keep = set(name for pattern in patterns\n\t                            for name in fnmatch.filter(names, pattern))\n", "        ignore = set(name for name in names\n\t                        if name not in keep and not os.path.isdir(os.path.join(path, name)))\n\t        return ignore\n\t    return _ignore_patterns\n\tdef set_save_path(save_path, remove=True):\n\t    ensure_path(save_path, remove=remove)\n\t    set_log_path(save_path)\n\t    # copy tree\n\t    if os.path.exists(os.path.join(save_path, 'src')):\n\t        shutil.rmtree(os.path.join(save_path, 'src'))\n", "    shutil.copytree(os.getcwd(), os.path.join(save_path, 'src'), ignore=shutil.ignore_patterns('__pycache__*', 'data*', 'save*', '.git*', 'datasetprocess*'))\n\t    writer = SummaryWriter(os.path.join(save_path, 'tensorboard'))\n\t    return log, writer\n\tdef compute_num_params(model, text=False):\n\t    tot = int(sum([np.prod(p.shape) for p in model.parameters()]))\n\t    if text:\n\t        if tot >= 1e6:\n\t            return '{:.1f}M'.format(tot / 1e6)\n\t        else:\n\t            return '{:.1f}K'.format(tot / 1e3)\n", "    else:\n\t        return tot\n\tdef make_optimizer(param_list, optimizer_spec, load_sd=False):\n\t    Optimizer = {\n\t        'SGD': SGD,\n\t        'Adam': Adam,\n\t        # 'NAdam': NAdam,\n\t    }[optimizer_spec['name']]\n\t    optimizer = Optimizer(param_list, **optimizer_spec['args'])\n\t    if load_sd:\n", "        optimizer.load_state_dict(optimizer_spec['sd'])\n\t    return optimizer\n\tdef make_coord(shape, ranges=None, flatten=True):\n\t    \"\"\" Make coordinates at grid centers.\n\t    \"\"\"\n\t    coord_seqs = []\n\t    for i, n in enumerate(shape):\n\t        if ranges is None:\n\t            v0, v1 = -1, 1\n\t        else:\n", "            v0, v1 = ranges[i]\n\t        r = (v1 - v0) / (2 * n)\n\t        seq = v0 + r + (2 * r) * torch.arange(n).float()\n\t        coord_seqs.append(seq)\n\t    ret = torch.stack(torch.meshgrid(*coord_seqs), dim=-1)\n\t    if flatten:\n\t        ret = ret.view(-1, ret.shape[-1])\n\t    return ret\n\tdef to_pixel_samples(img):\n\t    \"\"\" Convert the image to coord-RGB pairs.\n", "        img: Tensor, (3, H, W)\n\t    \"\"\"\n\t    coord = make_coord(img.shape[-2:])\n\t    rgb = img.view(3, -1).permute(1, 0)\n\t    return coord, rgb\n\tdef init_params(net):\n\t    '''Init layer parameters.'''\n\t    for m in net.modules():\n\t        if isinstance(m, nn.Conv2d):\n\t            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n", "            if m.bias:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.BatchNorm2d):\n\t            nn.init.constant_(m.weight, 1)\n\t            nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.Linear):\n\t            nn.init.kaiming_normal_(m.weight, mode='fan_out')\n\t            if m.bias:\n\t                nn.init.constant_(m.bias, 0)\n\tfrom svgpathtools import Path, Line, QuadraticBezier, CubicBezier, Arc, parse_path, wsvg\n", "colors = []\n\tdef curves_to_svg(curves, filename, box=256, control_polygon=True):\n\t    if not hasattr(curves_to_svg, \"colors\"):\n\t        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n\t        curves_to_svg.colors = colors\n\t    if isinstance(curves, torch.Tensor):\n\t        curves = curves.detach().cpu().numpy()\n\t    n_paths, n_curves, n_cp = curves.shape\n\t    n_cp = n_cp // 2\n\t    bez_paths = []\n", "    seg_paths = []\n\t    for i in range(n_paths):\n\t        bez = []\n\t        segs = []\n\t        for j in range(n_curves):\n\t            cps = (curves[i, j] + 1) * box / 2\n\t            if n_cp == 4:\n\t                bez.append(CubicBezier(cps[1] + cps[0]*1j,\n\t                                       cps[3] + cps[2]*1j,\n\t                                       cps[5] + cps[4]*1j,\n", "                                       cps[7] + cps[6]*1j))\n\t                segs.append(Line(cps[1] + cps[0] * 1j, cps[3] + cps[2] * 1j))\n\t                segs.append(Line(cps[3] + cps[2] * 1j, cps[5] + cps[4] * 1j))\n\t                segs.append(Line(cps[5] + cps[4] * 1j, cps[7] + cps[6] * 1j))\n\t            elif n_cp == 3:\n\t                bez.append(QuadraticBezier(cps[1] + cps[0]*1j,\n\t                                       cps[3] + cps[2]*1j,\n\t                                       cps[5] + cps[4]*1j))\n\t                segs.append(Line(cps[1] + cps[0] * 1j, cps[3] + cps[2] * 1j))\n\t                segs.append(Line(cps[3] + cps[2] * 1j, cps[5] + cps[4] * 1j))\n", "            else:\n\t                raise NotImplementedError('not implemented order of bezier path')\n\t        bez_paths.append(Path(*bez))\n\t        seg_paths.append(Path(*segs))\n\t    dimensions = (200, 200)\n\t    viewbox = (0, 0, box, box)\n\t    # colors = ['black'] * len(paths)\n\t    # colors = curves_to_svg.colors[:len(bez_paths)]\n\t    colors = curves_to_svg.colors\n\t    attributes = [\n", "        {\n\t            'fill': colors[i % len(colors)],\n\t            'fill-opacity': 0.3,\n\t        } for i in range(len(bez_paths)) \n\t    ]\n\t    if control_polygon:\n\t        attributes += [\n\t            {\n\t                'stroke-dasharray': '10,10',\n\t                'stroke': colors[i % len(colors)],\n", "                'fill': 'none',\n\t                'stroke-width': '0.1',\n\t            } for i in range(len(seg_paths))\n\t        ] \n\t    if control_polygon:\n\t        wsvg(bez_paths + seg_paths, \n\t            # colors=colors, \n\t            # stroke_widths=stroke_widths, \n\t            dimensions=dimensions, \n\t            viewbox=viewbox, \n", "            attributes=attributes, \n\t            filename=filename)\n\t    else:\n\t        wsvg(bez_paths,\n\t            # colors=colors, \n\t            # stroke_widths=stroke_widths, \n\t            dimensions=dimensions, \n\t            viewbox=viewbox, \n\t            attributes=attributes, \n\t            filename=filename)\n", "def importance_sampling_create_image(idx, img_values, size = 128):\n\t    idx_np = idx.detach().cpu().numpy()\n\t    img_values_np = img_values.detach().cpu().numpy()\n\t    idx_int = (((idx_np+1)/2)*size).astype(int)\n\t    img = np.zeros([size, size])\n\t    img[idx_int[:,0], idx_int[:,1]] = img_values_np\n\t    return img\n\tdef tensor_to_image(ten, fname=None):\n\t    if isinstance(ten, torch.Tensor):\n\t        ten = ten.detach().cpu().numpy()\n", "    if ten.dtype == np.float64 or ten.dtype == np.float32:\n\t        ten = np.clip(ten, 0, 1)\n\t        ten = (ten * 255).astype(np.uint8)\n\t    if fname is None:\n\t        return ten\n\t    if len(ten.shape) == 3:\n\t        if ten.shape[0] == 3:\n\t            ten = ten.transpose(1, 2, 0)\n\t        elif ten.shape[0] == 1:\n\t            ten = ten[0]\n", "    Image.fromarray(ten).save(fname)\n\tdef batched_ssim(pred, gt):\n\t    '''\n\t    pred: n x 1 x dim x dim\n\t    gt: n x 1 x dim x dim\n\t    '''\n\t    from skimage.metrics import structural_similarity as ssim\n\t    pred = pred.detach().cpu().numpy()\n\t    gt = gt.detach().cpu().numpy()\n\t    s = 0\n", "    for i in range(pred.shape[0]):\n\t        sv = ssim(pred[i, 0], gt[i, 0], data_range=1, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, multichannel=False)\n\t        s += sv\n\t    return s / pred.shape[0]"]}
{"filename": "losses/bezier_sdf_loss.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport math\n\timport losses\n\tfrom losses import BaseLoss\n\timport lpips\n\tdef gradient(y, x, grad_outputs=None):\n\t    if grad_outputs is None:\n\t        grad_outputs = torch.ones_like(y)\n", "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n\t    return grad\n\t@losses.register('img-loss')\n\tclass ImgLoss(BaseLoss):\n\t    def __init__(self, lams):\n\t        super().__init__()\n\t        self.lams = lams\n\t        if 'pct' in self.lams:\n\t            self.lpips = lpips.LPIPS(net='vgg').cuda()\n\t    def loss_terms(self, out, batch, **kwargs):\n", "        l = {}\n\t        if 'l2' in self.lams:\n\t            l['l2'] = self.lams['l2'] * torch.mean((out['rec'] - batch['full_img']) ** 2)\n\t        if 'pct' in self.lams:\n\t            norm_rec = out['rec'].repeat(1, 3, 1, 1) * 2 - 1\n\t            norm_gt = batch['full_img'].repeat(1, 3, 1, 1) * 2 - 1\n\t            l['pct'] = self.lams['pct'] * torch.mean(self.lpips(norm_rec, norm_gt))\n\t        return l\n\t@losses.register('latent-loss')\n\tclass LatentLoss(BaseLoss):\n", "    def __init__(self, lam):\n\t        super().__init__()\n\t        self.lam = lam\n\t    def loss_terms(self, out, batch, epoch):\n\t        l = {}\n\t        l['z'] = self.lam * nn.MSELoss()(out['z'], out['z_detach'])\n\t        return l\n\t@losses.register('sdf-loss')\n\tclass SdfLoss(BaseLoss):\n\t    def __init__(self, lams, udf_warmup=20):\n", "        super().__init__()\n\t        self.lams = lams\n\t        self.udf_warmup = udf_warmup\n\t        self.lpips = None\n\t    def loss_terms(self, out, batch, epoch):\n\t        lams = self.lams\n\t        l = {}\n\t        # rendered with diffvg loss\n\t        if 'img' in lams:\n\t            if out['rendered'].shape == batch['full_img'].shape:\n", "                l['img_l2'] = lams['img'] * torch.mean((out['rendered'] - batch['full_img']) ** 2)\n\t            elif out['rendered'].shape == batch['img_origin_res'].shape:\n\t                l['img_l2'] = lams['img'] * torch.mean((out['rendered'] - batch['img_origin_res']) ** 2)\n\t        if 'pct' in lams:\n\t            if self.lpips is None:\n\t                self.lpips = lpips.LPIPS(net='vgg').cuda()\n\t            norm_rec = out['rendered'].repeat(1, 3, 1, 1) * 2 - 1\n\t            norm_gt = batch['full_img'].repeat(1, 3, 1, 1) * 2 - 1\n\t            l['img_pct'] = lams['pct'] * torch.mean(self.lpips(norm_rec, norm_gt))\n\t        # rendered with occupancy loss \n", "        if 'occ' in lams:\n\t            l['occ'] = lams['occ'] * torch.mean((out['occ'] - batch['pixel']) ** 2)\n\t        if 'occ_l1' in lams:\n\t            l['occ_l1'] = lams['occ_l1'] * torch.mean(torch.abs(out['occ'] - batch['pixel']))\n\t        # unsigned distance loss\n\t        if self.udf_warmup is not None and epoch <= self.udf_warmup:\n\t            if 'l1' in lams:\n\t                if 'weight' in batch:\n\t                    l['l1'] = lams['l1'] * torch.sum(torch.abs(out['dis'] - batch['dis']) * batch['weight']) / torch.sum(batch['weight'])\n\t                else:\n", "                    l['l1'] = lams['l1'] * nn.L1Loss()(out['dis'], batch['dis'])\n\t        # signed distance loss\n\t        if 'sdf' in lams:\n\t            gt_sdf = batch['dis'].clone()\n\t            gt_sdf[batch['pixel'] < 0.5] *= -1\n\t            l['sdf'] = lams['sdf'] * torch.abs(out['sdf'] - gt_sdf).mean()\n\t        # gradient of sdf loss\n\t        if 'eik' in lams:\n\t            grad = gradient(out['sdf'], batch['xy'])\n\t            l['eik'] = lams['eik'] * torch.abs(grad.norm(dim=-1) - 1).mean()\n", "        return l"]}
{"filename": "losses/vae_loss.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport math\n\timport losses\n\tfrom losses import BaseLoss\n\t@losses.register('kl-loss')\n\tclass KLLoss(BaseLoss):\n\t    def __init__(self, lam):\n\t        self.lam = lam\n", "    def loss_terms(self, out, batch, **kwargs):\n\t        mu = out['mu']\n\t        log_var = out['log_var']\n\t        return {\n\t            'kl': self.lam * torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n\t        }\n\t@losses.register('kl-loss-two')\n\tclass KLLossTwo(BaseLoss):\n\t    def __init__(self, lam):\n\t        self.lam = lam\n", "    def loss_terms(self, out, batch, **kwargs):\n\t        mu = out['mu']\n\t        log_var = out['log_var']\n\t        tgt_mu = out['mu_detach']\n\t        tgt_log_var = out['log_var_detach']\n\t        return {\n\t            'kl': self.lam * torch.mean(0.5 * torch.sum(-1 + tgt_log_var - log_var + (log_var.exp() +  (mu - tgt_mu) ** 2) / torch.clamp(tgt_log_var.exp(), min=0.1), dim=1), dim=0)\n\t        }\n"]}
{"filename": "losses/__init__.py", "chunked_list": ["from .tools import register, make, BaseLoss, ListLoss\n\tfrom . import bezier_sdf_loss\n\tfrom . import local_loss\n\tfrom . import vae_loss"]}
{"filename": "losses/tools.py", "chunked_list": ["import copy\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom abc import abstractmethod\n\tlosses = {}\n\tdef register(name):\n\t    def decorator(cls):\n\t        losses[name] = cls\n\t        return cls\n", "    return decorator\n\tdef make(loss_spec, args=None):\n\t    loss_args = copy.deepcopy(loss_spec.get('args', {}))\n\t    args = args or {}\n\t    loss_args.update(args)\n\t    loss = losses[loss_spec['name']](**loss_args)\n\t    return loss\n\tclass BaseLoss(nn.Module):\n\t    @abstractmethod\n\t    def loss_terms(self, *args, **kwargs):\n", "        pass\n\t    def forward(self, *args, **kwargs):\n\t        l = self.loss_terms(*args, **kwargs)\n\t        loss = 0\n\t        for k, v in l.items():\n\t            loss += v\n\t        l['loss'] = loss\n\t        return l\n\t@register('list-loss')\n\tclass ListLoss(BaseLoss):\n", "    def __init__(self, loss_list):\n\t        super().__init__()\n\t        self.loss_list = [make(spec) for spec in loss_list]\n\t    def loss_terms(self, *args, **kwargs):\n\t        l = {}\n\t        for loss_ins in self.loss_list:\n\t            terms = loss_ins.loss_terms(*args, **kwargs)\n\t            l.update(terms)\n\t        return l\n"]}
{"filename": "losses/local_loss.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport math\n\timport losses\n\tfrom losses import BaseLoss\n\tfrom models.gutils import bezier_length\n\tfrom einops import reduce, rearrange, repeat\n\tfrom objprint import objprint as op\n\t# import lpips\n", "@losses.register('local-loss')\n\tclass LocalLoss(BaseLoss):\n\t    def __init__(self, lam_render, lam_reg=0):\n\t        super().__init__()\n\t        self.lam_render = lam_render\n\t        self.lam_reg = lam_reg\n\t    def get_curve_length(self, shape):\n\t        points = shape.points\n\t        num_control_points = shape.num_control_points\n\t        n_curve = len(num_control_points)\n", "        control_points = []\n\t        start_index = 0\n\t        total_len = 0\n\t        assert(num_control_points.sum() + n_curve == points.shape[0])\n\t        for i in range(n_curve - 1):\n\t            num_p = num_control_points[i].item()\n\t            assert(num_p == 1 or num_p == 0)\n\t            if num_p == 1: # bezier curve, start_index, start_index + 1, start_index + 2\n\t                control_points.append(points[start_index : start_index + num_p + 2])\n\t            else: # length\n", "                total_len += (points[start_index + 1] - points[start_index]).norm()\n\t            start_index += num_p + 1\n\t        if num_control_points[-1] == 1:\n\t            index = [start_index, start_index + 1, 0]\n\t            control_points.append(points[index])\n\t        elif num_control_points[-1] == 0:\n\t            total_len += (points[-1] - points[0]).norm()\n\t        else:\n\t            op(shape)\n\t            exit(0)\n", "        if len(control_points) > 0:\n\t            control_points = rearrange(control_points, 'b n d -> b n d')\n\t            curve_len = bezier_length(control_points[:, 0, :], control_points[:, 1, :], control_points[:, 2, :]).sum()\n\t            total_len += curve_len\n\t        return total_len \n\t    def loss_terms(self, img_rendered, target, shapes):\n\t        loss_render = (img_rendered.mean(dim=-1) - target).pow(2).mean()\n\t        l = {\n\t            'render': self.lam_render * loss_render,\n\t        }\n", "        if self.lam_reg > 0:\n\t            loss_reg = sum([self.get_curve_length(shape) for shape in shapes])\n\t            l.update({\n\t                'len': self.lam_reg * loss_reg,\n\t            })\n\t        return l\n"]}
{"filename": "eval/refine_svg.py", "chunked_list": ["import argparse, os, sys, subprocess, copy, random, time, shutil, math\n\tfrom PIL import Image\n\timport yaml\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import DataLoader\n\timport torch.nn.functional as F\n\timport torchvision\n", "from einops import repeat, rearrange, reduce, parse_shape\n\timport utils, models\n\tfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\n\timport svgwrite\n\tdef to_quadratic_bezier_segments(seg):\n\t    if isinstance(seg, CubicBezier):\n\t        p1 = seg.start\n\t        p2 = seg.control1\n\t        p3 = seg.control2\n\t        p4 = seg.end\n", "        return QuadraticBezier(p1, 0.75 * (p2 + p3) - 0.25 * (p1 + p4), p4)\n\t    elif isinstance(seg, Line):\n\t        return QuadraticBezier(seg.start, (seg.start + seg.end) / 2, seg.end)\n\t    elif isinstance(seg, QuadraticBezier):\n\t        return seg\n\t    else:\n\t        raise NotImplementedError('not expected type of segment')\n\tdef convert_path_to_control_points(path, pruned=False):\n\t    sub_paths = path.continuous_subpaths()\n\t    if pruned:\n", "        sub_paths = prune_paths(sub_paths)\n\t    cps_list = []\n\t    for sub_path in sub_paths:\n\t        cps = np.array([[i.start, i.control, i.end] for i in sub_path])\n\t        cps = np.stack((cps.real, cps.imag), axis=-1)\n\t        n_curve, n, _ = cps.shape\n\t        cps = cps.reshape(n_curve, n * 2)\n\t        cps_list.append(cps)\n\t    return cps_list\n\tdef merge_d_string_single_channel(d_string_list):\n", "    args = ['node', 'js/merge_sin.js'] + [f'\"{ds}\"'for ds in d_string_list]\n\t    res = subprocess.run(args, check=True, encoding='utf-8', capture_output=True)\n\t    d_string = res.stdout\n\t    path = parse_path(d_string)\n\t    new_segs = []\n\t    for i in range(len(path)):\n\t        new_segs.append(to_quadratic_bezier_segments(path[i]))\n\t    new_path = Path(*new_segs)\n\t    return new_path, d_string\n\tdef merge_d_string(d_string_list):\n", "    args = ['node', 'js/merge.js'] + [f'\"{ds}\"'for ds in d_string_list]\n\t    res = subprocess.run(args, check=True, encoding='utf-8', capture_output=True)\n\t    d_string = res.stdout\n\t    path = parse_path(d_string)\n\t    new_segs = []\n\t    for i in range(len(path)):\n\t        new_segs.append(to_quadratic_bezier_segments(path[i]))\n\t    new_path = Path(*new_segs)\n\t    return new_path, d_string\n\tdef write_path_to_svg(curve_tensor_list, filename):\n", "    sl = 256\n\t    canvas = svgwrite.Drawing(filename=filename, debug=True)\n\t    canvas.viewbox(0, 0, sl, sl)\n\t    path_d = []\n\t    for curve_tensor in curve_tensor_list:\n\t        path_d.append(models.gutils.path_d_from_control_points(curve_tensor, xy_flip=False))\n\t    path_d = ' '.join(path_d)\n\t    path = canvas.path(   \n\t        d=path_d, \n\t        fill='#000000',\n", "    )\n\t    canvas.add(path)\n\t    canvas.save()\n\t    return canvas\n\tdef simplify_path(path):\n\t    new_segs = []\n\t    last_end = None\n\t    accumulated_length = 0\n\t    for seg in path:\n\t        if last_end is None:\n", "            last_end = seg.start\n\t        sl = seg.length()\n\t        if sl + accumulated_length < 1e-3 * 256:\n\t            accumulated_length += sl\n\t            continue\n\t        accumulated_length = 0\n\t        seg.start = last_end\n\t        new_segs.append(seg)\n\t        last_end = seg.end\n\t    if len(new_segs) >= 2:\n", "        return Path(*new_segs)\n\t    else:\n\t        return None\n\tdef prune_paths(paths, length=1.0, area=50):\n\t    # list of path / something from path.continuous_subpaths()\n\t    pruned_paths = []\n\t    for path in paths:\n\t        if path.length() < length or abs(path.area()) < area:\n\t            continue\n\t        new_path = simplify_path(path)\n", "        if new_path is not None:\n\t            pruned_paths.append(new_path)\n\t    return pruned_paths\n\tdef connect_cp_for_tensor_list(cp_tensor_list, norm=False, sidelength=0):\n\t    cp_connected_tensor_list = []\n\t    for cp in cp_tensor_list:\n\t        cp_connected = torch.cat([cp, cp[:, :2].roll(shifts=-1, dims=0)], dim=-1)\n\t        if norm:\n\t            cp_connected = 2 * cp_connected / sidelength - 1\n\t        cp_connected_tensor_list.append(cp_connected)\n", "    return cp_connected_tensor_list\n\tdef refine_svg(control_points_list, target, w, h, num_iter, loss_fn, verbose=False, prog_bar=False):\n\t    assert(w == h)\n\t    target = torch.as_tensor(target).cuda()\n\t    cp_tensor_list = []\n\t    for control_points in control_points_list:\n\t        cp_tensor = torch.tensor(control_points[:, :-2]).cuda()\n\t        cp_tensor.requires_grad = True\n\t        cp_tensor_list.append(cp_tensor)\n\t    optim = torch.optim.Adam(cp_tensor_list, lr=2, betas=(0.9, 0.999))\n", "    renderer = models.OccRender(sidelength=w).cuda()\n\t    imgs = []\n\t    with tqdm(range(num_iter), leave=False, desc='Refine', disable=not prog_bar) as iters:\n\t        for i in iters:\n\t            optim.zero_grad()\n\t            cp_norm_list = connect_cp_for_tensor_list(cp_tensor_list, norm=True, sidelength=w)\n\t            img_render = renderer(cp_norm_list, kernel=4)\n\t            if verbose:\n\t                imgs.append(utils.tensor_to_image(img_render))\n\t            list_of_loss = loss_fn(img_render, target, cp_norm_list)\n", "            loss = list_of_loss['loss']\n\t            loss.backward()\n\t            optim.step()\n\t            for k, v in list_of_loss.items():\n\t                list_of_loss[k] = f'{v.item():.6f}'\n\t            iters.set_postfix({\n\t                **list_of_loss,\n\t            })\n\t            loss = None\n\t    cp_connected = connect_cp_for_tensor_list(cp_tensor_list, norm=False)\n", "    return {\n\t        'control_points': cp_connected,\n\t        'rendered_images' : imgs, \n\t    }\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('--svg', type=str, required=True)\n\t    parser.add_argument('--inter', type=str, default=None)\n\t    parser.add_argument('--target', type=str, required=True)\n\t    parser.add_argument('--num_iter', type=int, default=100)\n", "    args = parser.parse_args()\n\t    assert(os.path.exists(args.svg))\n\t    _, attributes, svg_attributes = svg2paths(args.svg, return_svg_attributes=True)\n\t    viewbox = svg_attributes['viewBox']\n\t    viewbox = list(map(int, viewbox.split(' ')))\n\t    svg_w, svg_h = viewbox[2], viewbox[3]\n\t    d_string_list = [a['d'] + ' Z' for a in attributes]\n\t    path, d_string = merge_d_string(d_string_list)\n\t    if args.inter is not None:\n\t        wsvg(paths=[path], filename=args.inter, attributes=[{'fill': '#000000'}], svg_attributes=svg_attributes)\n", "    cps = convert_path_to_control_points(path, pruned=True)\n\t    img = np.asarray(Image.open(args.target).convert('L').resize((256, 256), resample=Image.BILINEAR)) / 255.\n\t    refined_path = refine_svg(cps, img, svg_w, svg_h, num_iter=args.num_iter, verbose=True)['control_points']\n\t    write_path_to_svg(refined_path, f'refine/final_refined.svg')\n"]}
{"filename": "eval/eval_reconstruction.py", "chunked_list": ["import argparse\n\timport os\n\timport sys\n\tsys.path.append(os.getcwd())\n\timport time\n\timport numpy as np\n\timport random\n\tfrom PIL import Image\n\timport imageio\n\timport yaml, shutil\n", "import torch\n\timport torch.nn as nn\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.optim.lr_scheduler import MultiStepLR\n\timport torch.nn.functional as F\n\timport torchvision\n\timport datasets\n\timport models, losses\n\timport utils\n", "import matplotlib.pyplot as plt\n\tfrom einops import rearrange, repeat, reduce, parse_shape\n\timport refine_svg\n\tfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--outdir', required=True, type=str)\n\tparser.add_argument('--resume', required=True, type=str)\n\tparser.add_argument('--seed', default=42, type=int)\n\targs = parser.parse_args()\n", "def seed_all(seed):\n\t    random.seed(seed) # Python\n\t    np.random.seed(seed) # cpu vars\n\t    torch.manual_seed(seed) # cpu vars\n\t    if torch.cuda.is_available(): \n\t        torch.cuda.manual_seed(seed)\n\t        torch.cuda.manual_seed_all(seed) # gpu vars\n\t        torch.backends.cudnn.deterministic = True  #needed\n\t        torch.backends.cudnn.benchmark = False\n\tdef make_data_loader(spec, tag=''):\n", "    if spec is None:\n\t        return None\n\t    dataset = datasets.make(spec['dataset'])\n\t    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n\t        shuffle=spec['shuffle'], num_workers=spec['batch_size'], pin_memory=True)\n\t    return loader\n\tdef make_data_loaders(config):\n\t    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n\t    return val_loader\n\tconfig_str = f'''\n", "val_dataset:\n\t  dataset:\n\t    name: deepvecfont-sdf\n\t    args:\n\t      data_root: ./data/dvf_png/font_pngs/test\n\t      img_res: 128\n\t      char_list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51]\n\t      include_lower_case: true\n\t      val: true\n\t      use_cache: false\n", "      valid_list: null\n\t      ratio: 1\n\t      valid_list: ./data/dvf_png/test_valid.txt\n\t  batch_size: 32\n\t  shuffle: false\n\t'''\n\tseed = args.seed\n\tseed_all(seed)\n\tprint('seed:', seed)\n\tconfig = yaml.load(config_str, Loader=yaml.FullLoader)\n", "output_dir = args.outdir\n\tos.makedirs(output_dir, exist_ok=True)\n\tsv_file = torch.load(args.resume) \n\tsystem = models.make(sv_file['model'], load_sd=True).cuda()\n\tsystem.init()\n\tsystem.eval()\n\tmodels.freeze(system)\n\tsidelength = 256\n\tdataloader = make_data_loaders(config)\n\twith open(os.path.join(output_dir, 'seed.txt'), 'w') as f:\n", "    f.write(str(seed))\n\tfor batch in tqdm(dataloader):\n\t    for k, v in batch.items():\n\t        if type(v) is torch.Tensor:\n\t            batch[k] = v.cuda()\n\t    with torch.no_grad():\n\t        img = batch['img']\n\t        z = system.encoder(batch)\n\t        curves = system.decoder(z)\n\t        img_rec = torch.clamp(system.decode_image_from_latent_vector(z)['rec'], 0, 1)\n", "    n = curves.shape[0]\n\t    curves_np_raw = curves.detach().cpu().numpy()\n\t    curves_np = (curves_np_raw + 1) * sidelength / 2\n\t    targets = img_rec\n\t    for i in range(n):\n\t        font_name = batch['font_name'][i]\n\t        save_dir = os.path.join(output_dir, 'rec_init', font_name)\n\t        os.makedirs(save_dir, exist_ok=True)\n\t        char_name = batch['char'][i].item()\n\t        svg_path = os.path.join(save_dir, f'{char_name:02d}_init.svg')\n", "        raw_path = os.path.join(save_dir, f'{char_name:02d}_raw.svg')\n\t        img_path = os.path.join(save_dir, f'{char_name:02d}_rec.png')\n\t        if os.path.exists(svg_path) and os.path.exists(raw_path) and os.path.exists(img_path):\n\t            continue\n\t        system.write_paths_to_svg(curves_np_raw[i], raw_path)\n\t        utils.tensor_to_image(img_rec[i, 0], img_path)\n\t        curve_np = curves_np[i]\n\t        d_string_list = [models.gutils.path_d_from_control_points(cp, xy_flip=True) for cp in curve_np]\n\t        path, d_string = refine_svg.merge_d_string(d_string_list)\n\t        cps_list = refine_svg.convert_path_to_control_points(path, pruned=True)\n", "        if len(cps_list) == 0:\n\t            continue\n\t        refine_svg.write_path_to_svg(cps_list, svg_path)\n"]}
{"filename": "eval/sample_font.py", "chunked_list": ["import argparse\n\timport os\n\timport sys\n\tsys.path.append(os.getcwd())\n\timport time\n\timport numpy as np\n\timport random\n\tfrom PIL import Image\n\timport imageio\n\timport yaml, shutil\n", "import torch\n\timport torch.nn as nn\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.optim.lr_scheduler import MultiStepLR\n\timport torch.nn.functional as F\n\timport torchvision\n\timport datasets\n\timport models, losses\n\timport utils\n", "import matplotlib.pyplot as plt\n\tfrom einops import rearrange, repeat, reduce, parse_shape\n\timport refine_svg\n\tfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\n\timport pydiffvg\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--outdir', required=True, type=str)\n\tparser.add_argument('--resume', required=True, type=str)\n\tparser.add_argument('--seed', default=42, type=int)\n\tparser.add_argument('--n-sample', default=20, type=int)\n", "parser.add_argument('--begin-index', default=0, type=int)\n\targs = parser.parse_args()\n\tdef seed_all(seed):\n\t    random.seed(seed) # Python\n\t    np.random.seed(seed) # cpu vars\n\t    torch.manual_seed(seed) # cpu vars\n\t    if torch.cuda.is_available(): \n\t        torch.cuda.manual_seed(seed)\n\t        torch.cuda.manual_seed_all(seed) # gpu vars\n\t        torch.backends.cudnn.deterministic = True  #needed\n", "        torch.backends.cudnn.benchmark = False\n\tdef make_data_loader(spec, tag=''):\n\t    if spec is None:\n\t        return None\n\t    dataset = datasets.make(spec['dataset'])\n\t    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n\t        shuffle=spec['shuffle'], num_workers=12, pin_memory=True)\n\t    return loader\n\tdef make_data_loaders(config):\n\t    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n", "    return val_loader\n\tref_char_list = [0,1]\n\tconfig_str = f'''\n\tloss:\n\t  name: local-loss\n\t  args:\n\t    lam_render: 1\n\t    lam_reg: 1.e-6\n\t'''\n\tseed = args.seed\n", "seed_all(seed)\n\tprint('seed:', seed)\n\tconfig = yaml.load(config_str, Loader=yaml.FullLoader)\n\toutput_dir = args.outdir\n\tos.makedirs(output_dir, exist_ok=True)\n\tos.makedirs(os.path.join(output_dir, 'all_rec'), exist_ok=True)\n\tsv_file = torch.load(args.resume)\n\tsystem = models.make(sv_file['model'], load_sd=True).cuda()\n\tsystem.init()\n\tsystem.eval()\n", "models.freeze(system)\n\tsidelength = 256\n\tloss_fn = losses.make(config['loss'])\n\tchar_nums = 52\n\tn_sample = args.n_sample\n\tsample_begin = args.begin_index\n\twith open(os.path.join(output_dir, 'seed.txt'), 'w') as f:\n\t    f.write(str(seed))\n\tfor sample_i in tqdm(range(sample_begin, sample_begin + n_sample)):\n\t    parent_dir = os.path.join(output_dir, f'{sample_i:04d}')\n", "    os.makedirs(parent_dir, exist_ok=True)\n\t    with torch.no_grad():\n\t        tgt_char_idx = torch.arange(char_nums).cuda()\n\t        z_style = torch.randn(1, 256).cuda() * 1.5\n\t        torch.save(z_style.detach().cpu(), os.path.join(parent_dir, 'z_style.pt'))\n\t        z_style = z_style.expand(char_nums, -1)\n\t        emb_char = system.cls_token(tgt_char_idx)\n\t        z = system.merge(torch.cat([z_style, emb_char], dim=-1))\n\t        curves = system.decoder(z)\n\t        img_rec = system.decode_image_from_latent_vector(z)['rec']\n", "    n = curves.shape[0]\n\t    curves_np = curves.detach().cpu().numpy()\n\t    curves_np = (curves_np + 1) * sidelength / 2\n\t    targets = img_rec\n\t    torchvision.utils.save_image(targets, os.path.join(output_dir, 'all_rec', f'{sample_i:04d}_rec.png'), nrow=13)\n\t    for i in range(n):\n\t        path_prefix = os.path.join(output_dir, f'{sample_i:04d}', f'{i:02d}')\n\t        if os.path.exists(path_prefix + '_init.svg'):\n\t            continue\n\t        target = targets[i, 0]\n", "        utils.tensor_to_image(img_rec[i, 0], path_prefix + '_rec.png')\n\t        curve_np = curves_np[i]\n\t        d_string_list = [models.gutils.path_d_from_control_points(cp, xy_flip=True) for cp in curve_np]\n\t        path, d_string = refine_svg.merge_d_string(d_string_list)\n\t        cps_list = refine_svg.convert_path_to_control_points(path, pruned=True)\n\t        if len(cps_list) == 0:\n\t            continue\n\t        refine_svg.write_path_to_svg(cps_list, path_prefix + '_init.svg')\n\t        continue"]}
{"filename": "eval/eval_generation_multiref.py", "chunked_list": ["import argparse\n\timport os\n\timport sys\n\tsys.path.append(os.getcwd())\n\timport numpy as np\n\timport random\n\tfrom PIL import Image\n\timport imageio\n\timport yaml, shutil\n\timport torch\n", "import torch.nn as nn\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.optim.lr_scheduler import MultiStepLR\n\timport torch.nn.functional as F\n\timport torchvision\n\timport datasets\n\timport models, losses\n\timport utils\n\timport matplotlib.pyplot as plt\n", "from einops import rearrange, repeat, reduce, parse_shape\n\timport refine_svg\n\tfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--outdir', required=True, type=str)\n\tparser.add_argument('--resume', required=True, type=str)\n\tparser.add_argument('--seed', default=42, type=int)\n\targs = parser.parse_args()\n\tdef seed_all(seed):\n\t    random.seed(seed) # Python\n", "    np.random.seed(seed) # cpu vars\n\t    torch.manual_seed(seed) # cpu vars\n\t    if torch.cuda.is_available(): \n\t        torch.cuda.manual_seed(seed)\n\t        torch.cuda.manual_seed_all(seed) # gpu vars\n\t        torch.backends.cudnn.deterministic = True  #needed\n\t        torch.backends.cudnn.benchmark = False\n\tdef make_data_loader(spec, tag=''):\n\t    if spec is None:\n\t        return None\n", "    dataset = datasets.make(spec['dataset'])\n\t    loader = DataLoader(dataset, batch_size=spec['batch_size'],\n\t        shuffle=spec['shuffle'], num_workers=12, pin_memory=True)\n\t    return loader\n\tdef make_data_loaders(config):\n\t    val_loader = make_data_loader(config.get('val_dataset'), tag='val')\n\t    return val_loader\n\tseed_all(args.seed)\n\tref_char_list = [0,1,26,27]\n\tconfig_str = f'''\n", "val_dataset:\n\t  dataset:\n\t    name: dvf-eval\n\t    args:\n\t      data_root: ./data/dvf_png/font_pngs/test\n\t      img_res: 128\n\t      valid_list: null\n\t      ref_list: {ref_char_list}\n\t  batch_size: 1\n\t  shuffle: false\n", "'''\n\tconfig = yaml.load(config_str, Loader=yaml.FullLoader)\n\toutput_dir = args.outdir\n\tsv_file = torch.load(args.resume)\n\tsystem = models.make(sv_file['model'], load_sd=True).cuda()\n\tsystem.init()\n\tsystem.eval()\n\tmodels.freeze(system)\n\tdataloader = make_data_loaders(config)\n\tsidelength = 256\n", "char_nums = 52\n\tfor batch in tqdm(dataloader):\n\t    for k, v in batch.items():\n\t        if type(v) is torch.Tensor:\n\t            batch[k] = v.cuda()\n\t    with torch.no_grad():\n\t        refs = batch['refs'] # 1 x 2 x 1 x 64 x 64\n\t        ref_char_idx = batch['ref_char_idx'] # 1 x 2\n\t        n_ref = batch['n_ref']\n\t        tgt_char_idx = torch.arange(char_nums).cuda()\n", "        mu, _ = system.ref_img_encode(batch)\n\t        mu_r = mu.expand(char_nums, -1) \n\t        emb_char = system.cls_token(tgt_char_idx)\n\t        z = system.merge(torch.cat([mu_r, emb_char], dim=-1))\n\t        curves = system.decoder(z)\n\t        img_rec = system.decode_image_from_latent_vector(z)['rec']\n\t    n = curves.shape[0]\n\t    curves_np_raw = curves.detach().cpu().numpy()\n\t    curves_np = (curves_np_raw + 1) * sidelength / 2\n\t    targets = img_rec\n", "    assert(n == char_nums)\n\t    font_name = batch['font_name'][0]\n\t    save_dir = os.path.join(output_dir, 'rec_init', font_name)\n\t    os.makedirs(save_dir, exist_ok=True)\n\t    for i in range(n):\n\t        char_name = i\n\t        svg_path = os.path.join(save_dir, f'{char_name:02d}_init.svg')\n\t        raw_path = os.path.join(save_dir, f'{char_name:02d}_raw.svg')\n\t        img_path = os.path.join(save_dir, f'{char_name:02d}_rec.png')\n\t        if os.path.exists(svg_path) and os.path.exists(raw_path) and os.path.exists(img_path):\n", "            continue\n\t        system.write_paths_to_svg(curves_np_raw[i], raw_path)\n\t        utils.tensor_to_image(img_rec[i, 0], img_path)\n\t        curve_np = curves_np[i]\n\t        d_string_list = [models.gutils.path_d_from_control_points(cp, xy_flip=True) for cp in curve_np]\n\t        path, d_string = refine_svg.merge_d_string(d_string_list)\n\t        cps_list = refine_svg.convert_path_to_control_points(path, pruned=True)\n\t        if len(cps_list) == 0:\n\t            continue\n\t        refine_svg.write_path_to_svg(cps_list, svg_path)"]}
{"filename": "eval/diffvg_parse_svg.py", "chunked_list": ["import torch\n\timport xml.etree.ElementTree as etree\n\timport numpy as np\n\timport diffvg\n\timport os\n\timport pydiffvg\n\timport svgpathtools\n\timport svgpathtools.parser\n\timport re\n\timport warnings\n", "import cssutils\n\timport logging\n\timport matplotlib.colors \n\tcssutils.log.setLevel(logging.ERROR)\n\tdef remove_namespaces(s):\n\t    \"\"\"\n\t        {...} ... -> ...\n\t    \"\"\"\n\t    return re.sub('{.*}', '', s)\n\tdef parse_style(s, defs):\n", "    style_dict = {}\n\t    for e in s.split(';'):\n\t        key_value = e.split(':')\n\t        if len(key_value) == 2:\n\t            key = key_value[0].strip()\n\t            value = key_value[1].strip()\n\t            if key == 'fill' or key == 'stroke':\n\t                # Special case: convert colors into tensor in definitions so\n\t                # that different shapes can share the same color\n\t                value = parse_color(value, defs)\n", "            style_dict[key] = value\n\t    return style_dict\n\tdef parse_hex(s):\n\t    \"\"\"\n\t        Hex to tuple\n\t    \"\"\"\n\t    s = s.lstrip('#')\n\t    if len(s) == 3:\n\t        s = s[0] + s[0] + s[1] + s[1] + s[2] + s[2]\n\t    rgb = tuple(int(s[i:i+2], 16) for i in (0, 2, 4))\n", "    # sRGB to RGB\n\t    # return torch.pow(torch.tensor([rgb[0] / 255.0, rgb[1] / 255.0, rgb[2] / 255.0]), 2.2)\n\t    return torch.pow(torch.tensor([rgb[0] / 255.0, rgb[1] / 255.0, rgb[2] / 255.0]), 1.0)\n\tdef parse_int(s):\n\t    \"\"\"\n\t        trim alphabets\n\t    \"\"\"\n\t    return int(float(''.join(i for i in s if (not i.isalpha()))))\n\tdef parse_color(s, defs):\n\t    if s is None:\n", "        return None\n\t    if isinstance(s, torch.Tensor):\n\t        return s\n\t    s = s.lstrip(' ')\n\t    color = torch.tensor([0.0, 0.0, 0.0, 1.0])\n\t    if s[0] == '#':\n\t        color[:3] = parse_hex(s)\n\t    elif s[:3] == 'url':\n\t        # url(#id)\n\t        color = defs[s[4:-1].lstrip('#')]\n", "    elif s == 'none':\n\t        color = None\n\t    elif s[:4] == 'rgb(':\n\t        rgb = s[4:-1].split(',')\n\t        color = torch.tensor([int(rgb[0]) / 255.0, int(rgb[1]) / 255.0, int(rgb[2]) / 255.0, 1.0])\n\t    elif s == 'none':\n\t        return None\n\t    else:\n\t        try : \n\t            rgba = matplotlib.colors.to_rgba(s)\n", "            color = torch.tensor(rgba)\n\t        except ValueError : \n\t            warnings.warn('Unknown color command ' + s)\n\t    return color\n\t# https://github.com/mathandy/svgpathtools/blob/7ebc56a831357379ff22216bec07e2c12e8c5bc6/svgpathtools/parser.py\n\tdef _parse_transform_substr(transform_substr):\n\t    type_str, value_str = transform_substr.split('(')\n\t    value_str = value_str.replace(',', ' ')\n\t    values = list(map(float, filter(None, value_str.split(' '))))\n\t    transform = np.identity(3)\n", "    if 'matrix' in type_str:\n\t        transform[0:2, 0:3] = np.array([values[0:6:2], values[1:6:2]])\n\t    elif 'translate' in transform_substr:\n\t        transform[0, 2] = values[0]\n\t        if len(values) > 1:\n\t            transform[1, 2] = values[1]\n\t    elif 'scale' in transform_substr:\n\t        x_scale = values[0]\n\t        y_scale = values[1] if (len(values) > 1) else x_scale\n\t        transform[0, 0] = x_scale\n", "        transform[1, 1] = y_scale\n\t    elif 'rotate' in transform_substr:\n\t        angle = values[0] * np.pi / 180.0\n\t        if len(values) == 3:\n\t            offset = values[1:3]\n\t        else:\n\t            offset = (0, 0)\n\t        tf_offset = np.identity(3)\n\t        tf_offset[0:2, 2:3] = np.array([[offset[0]], [offset[1]]])\n\t        tf_rotate = np.identity(3)\n", "        tf_rotate[0:2, 0:2] = np.array([[np.cos(angle), -np.sin(angle)], [np.sin(angle), np.cos(angle)]])\n\t        tf_offset_neg = np.identity(3)\n\t        tf_offset_neg[0:2, 2:3] = np.array([[-offset[0]], [-offset[1]]])\n\t        transform = tf_offset.dot(tf_rotate).dot(tf_offset_neg)\n\t    elif 'skewX' in transform_substr:\n\t        transform[0, 1] = np.tan(values[0] * np.pi / 180.0)\n\t    elif 'skewY' in transform_substr:\n\t        transform[1, 0] = np.tan(values[0] * np.pi / 180.0)\n\t    else:\n\t        # Return an identity matrix if the type of transform is unknown, and warn the user\n", "        warnings.warn('Unknown SVG transform type: {0}'.format(type_str))\n\t    return transform\n\tdef parse_transform(transform_str):\n\t    \"\"\"\n\t        Converts a valid SVG transformation string into a 3x3 matrix.\n\t        If the string is empty or null, this returns a 3x3 identity matrix\n\t    \"\"\"\n\t    if not transform_str:\n\t        return np.identity(3)\n\t    elif not isinstance(transform_str, str):\n", "        raise TypeError('Must provide a string to parse')\n\t    total_transform = np.identity(3)\n\t    transform_substrs = transform_str.split(')')[:-1]  # Skip the last element, because it should be empty\n\t    for substr in transform_substrs:\n\t        total_transform = total_transform.dot(_parse_transform_substr(substr))\n\t    return torch.from_numpy(total_transform).type(torch.float32)\n\tdef parse_linear_gradient(node, transform, defs):\n\t    begin = torch.tensor([0.0, 0.0])\n\t    end = torch.tensor([0.0, 0.0])\n\t    offsets = []\n", "    stop_colors = []\n\t    # Inherit from parent\n\t    for key in node.attrib:\n\t        if remove_namespaces(key) == 'href':\n\t            value = node.attrib[key]\n\t            parent = defs[value.lstrip('#')]\n\t            begin = parent.begin\n\t            end = parent.end\n\t            offsets = parent.offsets\n\t            stop_colors = parent.stop_colors\n", "    for attrib in node.attrib:\n\t        attrib = remove_namespaces(attrib)\n\t        if attrib == 'x1':\n\t            begin[0] = float(node.attrib['x1'])\n\t        elif attrib == 'y1':\n\t            begin[1] = float(node.attrib['y1'])\n\t        elif attrib == 'x2':\n\t            end[0] = float(node.attrib['x2'])\n\t        elif attrib == 'y2':\n\t            end[1] = float(node.attrib['y2'])\n", "        elif attrib == 'gradientTransform':\n\t            transform = transform @ parse_transform(node.attrib['gradientTransform'])\n\t    begin = transform @ torch.cat((begin, torch.ones([1])))\n\t    begin = begin / begin[2]\n\t    begin = begin[:2]\n\t    end = transform @ torch.cat((end, torch.ones([1])))\n\t    end = end / end[2]\n\t    end = end[:2]\n\t    for child in node:\n\t        tag = remove_namespaces(child.tag)\n", "        if tag == 'stop':\n\t            offset = float(child.attrib['offset'])\n\t            color = [0.0, 0.0, 0.0, 1.0]\n\t            if 'stop-color' in child.attrib:\n\t                c = parse_color(child.attrib['stop-color'], defs)\n\t                color[:3] = [c[0], c[1], c[2]]\n\t            if 'stop-opacity' in child.attrib:\n\t                color[3] = float(child.attrib['stop-opacity'])\n\t            if 'style' in child.attrib:\n\t                style = parse_style(child.attrib['style'], defs)\n", "                if 'stop-color' in style:\n\t                    c = parse_color(style['stop-color'], defs)\n\t                    color[:3] = [c[0], c[1], c[2]]\n\t                if 'stop-opacity' in style:\n\t                    color[3] = float(style['stop-opacity'])\n\t            offsets.append(offset)\n\t            stop_colors.append(color)\n\t    if isinstance(offsets, list):\n\t        offsets = torch.tensor(offsets)\n\t    if isinstance(stop_colors, list):\n", "        stop_colors = torch.tensor(stop_colors)\n\t    return pydiffvg.LinearGradient(begin, end, offsets, stop_colors)\n\tdef parse_radial_gradient(node, transform, defs):\n\t    begin = torch.tensor([0.0, 0.0])\n\t    end = torch.tensor([0.0, 0.0])\n\t    center = torch.tensor([0.0, 0.0])\n\t    radius = torch.tensor([0.0, 0.0])\n\t    offsets = []\n\t    stop_colors = []\n\t    # Inherit from parent\n", "    for key in node.attrib:\n\t        if remove_namespaces(key) == 'href':\n\t            value = node.attrib[key]\n\t            parent = defs[value.lstrip('#')]\n\t            begin = parent.begin\n\t            end = parent.end\n\t            offsets = parent.offsets\n\t            stop_colors = parent.stop_colors\n\t    for attrib in node.attrib:\n\t        attrib = remove_namespaces(attrib)\n", "        if attrib == 'cx':\n\t            center[0] = float(node.attrib['cx'])\n\t        elif attrib == 'cy':\n\t            center[1] = float(node.attrib['cy'])\n\t        elif attrib == 'fx':\n\t            radius[0] = float(node.attrib['fx'])\n\t        elif attrib == 'fy':\n\t            radius[1] = float(node.attrib['fy'])\n\t        elif attrib == 'fr':\n\t            radius[0] = float(node.attrib['fr'])\n", "            radius[1] = float(node.attrib['fr'])\n\t        elif attrib == 'gradientTransform':\n\t            transform = transform @ parse_transform(node.attrib['gradientTransform'])\n\t    # TODO: this is incorrect\n\t    center = transform @ torch.cat((center, torch.ones([1])))\n\t    center = center / center[2]\n\t    center = center[:2]\n\t    for child in node:\n\t        tag = remove_namespaces(child.tag)\n\t        if tag == 'stop':\n", "            offset = float(child.attrib['offset'])\n\t            color = [0.0, 0.0, 0.0, 1.0]\n\t            if 'stop-color' in child.attrib:\n\t                c = parse_color(child.attrib['stop-color'], defs)\n\t                color[:3] = [c[0], c[1], c[2]]\n\t            if 'stop-opacity' in child.attrib:\n\t                color[3] = float(child.attrib['stop-opacity'])\n\t            if 'style' in child.attrib:\n\t                style = parse_style(child.attrib['style'], defs)\n\t                if 'stop-color' in style:\n", "                    c = parse_color(style['stop-color'], defs)\n\t                    color[:3] = [c[0], c[1], c[2]]\n\t                if 'stop-opacity' in style:\n\t                    color[3] = float(style['stop-opacity'])\n\t            offsets.append(offset)\n\t            stop_colors.append(color)\n\t    if isinstance(offsets, list):\n\t        offsets = torch.tensor(offsets)\n\t    if isinstance(stop_colors, list):\n\t        stop_colors = torch.tensor(stop_colors)\n", "    return pydiffvg.RadialGradient(begin, end, offsets, stop_colors)\n\tdef parse_stylesheet(node, transform, defs):\n\t    # collect CSS classes\n\t    sheet = cssutils.parseString(node.text)\n\t    for rule in sheet:\n\t        if hasattr(rule, 'selectorText') and hasattr(rule, 'style'):\n\t            name = rule.selectorText\n\t            if len(name) >= 2 and name[0] == '.':\n\t                defs[name[1:]] = parse_style(rule.style.getCssText(), defs)\n\t    return defs\n", "def parse_defs(node, transform, defs):\n\t    for child in node:\n\t        tag = remove_namespaces(child.tag)\n\t        if tag == 'linearGradient':\n\t            if 'id' in child.attrib:\n\t                defs[child.attrib['id']] = parse_linear_gradient(child, transform, defs)\n\t        elif tag == 'radialGradient':\n\t            if 'id' in child.attrib:\n\t                defs[child.attrib['id']] = parse_radial_gradient(child, transform, defs)\n\t        elif tag == 'style':\n", "            defs = parse_stylesheet(child, transform, defs)\n\t    return defs\n\tdef parse_common_attrib(node, transform, fill_color, defs):\n\t    attribs = {}\n\t    if 'class' in node.attrib:\n\t        attribs.update(defs[node.attrib['class']])\n\t    attribs.update(node.attrib)\n\t    name = ''\n\t    if 'id' in node.attrib:\n\t        name = node.attrib['id']\n", "    stroke_color = None\n\t    stroke_width = torch.tensor(0.5)\n\t    use_even_odd_rule = False\n\t    new_transform = transform\n\t    if 'transform' in attribs:\n\t        new_transform = transform @ parse_transform(attribs['transform'])\n\t    if 'fill' in attribs:\n\t        fill_color = parse_color(attribs['fill'], defs)\n\t    fill_opacity = 1.0\n\t    if 'fill-opacity' in attribs:\n", "        fill_opacity *= float(attribs['fill-opacity'])\n\t    if 'opacity' in attribs:\n\t        fill_opacity *= float(attribs['opacity'])\n\t    # Ignore opacity if the color is a gradient\n\t    if isinstance(fill_color, torch.Tensor):\n\t        fill_color[3] = fill_opacity\n\t    if 'fill-rule' in attribs:\n\t        if attribs['fill-rule'] == \"evenodd\":\n\t            use_even_odd_rule = True\n\t        elif attribs['fill-rule'] == \"nonzero\":\n", "            use_even_odd_rule = False\n\t        else:\n\t            warnings.warn('Unknown fill-rule: {}'.format(attribs['fill-rule']))\n\t    if 'stroke' in attribs:\n\t        stroke_color = parse_color(attribs['stroke'], defs)\n\t    if 'stroke-width' in attribs:\n\t        stroke_width = attribs['stroke-width']\n\t        if stroke_width[-2:] == 'px':\n\t            stroke_width = stroke_width[:-2]\n\t        stroke_width = torch.tensor(float(stroke_width) / 2.0)\n", "    if 'style' in attribs:\n\t        style = parse_style(attribs['style'], defs)\n\t        if 'fill' in style:\n\t            fill_color = parse_color(style['fill'], defs)\n\t        fill_opacity = 1.0\n\t        if 'fill-opacity' in style:\n\t            fill_opacity *= float(style['fill-opacity'])\n\t        if 'opacity' in style:\n\t            fill_opacity *= float(style['opacity'])\n\t        if 'fill-rule' in style:\n", "            if style['fill-rule'] == \"evenodd\":\n\t                use_even_odd_rule = True\n\t            elif style['fill-rule'] == \"nonzero\":\n\t                use_even_odd_rule = False\n\t            else:\n\t                warnings.warn('Unknown fill-rule: {}'.format(style['fill-rule']))\n\t        # Ignore opacity if the color is a gradient\n\t        if isinstance(fill_color, torch.Tensor):\n\t            fill_color[3] = fill_opacity\n\t        if 'stroke' in style:\n", "            if style['stroke'] != 'none':\n\t                stroke_color = parse_color(style['stroke'], defs)\n\t                # Ignore opacity if the color is a gradient\n\t                if isinstance(stroke_color, torch.Tensor):\n\t                    if 'stroke-opacity' in style:\n\t                        stroke_color[3] = float(style['stroke-opacity'])\n\t                    if 'opacity' in style:\n\t                        stroke_color[3] *= float(style['opacity'])\n\t                if 'stroke-width' in style:\n\t                    stroke_width = style['stroke-width']\n", "                    if stroke_width[-2:] == 'px':\n\t                        stroke_width = stroke_width[:-2]\n\t                    stroke_width = torch.tensor(float(stroke_width) / 2.0)\n\t        if isinstance(fill_color, pydiffvg.LinearGradient):\n\t            fill_color.begin = new_transform @ torch.cat((fill_color.begin, torch.ones([1])))\n\t            fill_color.begin = fill_color.begin / fill_color.begin[2]\n\t            fill_color.begin = fill_color.begin[:2]\n\t            fill_color.end = new_transform @ torch.cat((fill_color.end, torch.ones([1])))\n\t            fill_color.end = fill_color.end / fill_color.end[2]\n\t            fill_color.end = fill_color.end[:2]\n", "        if isinstance(stroke_color, pydiffvg.LinearGradient):\n\t            stroke_color.begin = new_transform @ torch.cat((stroke_color.begin, torch.ones([1])))\n\t            stroke_color.begin = stroke_color.begin / stroke_color.begin[2]\n\t            stroke_color.begin = stroke_color.begin[:2]\n\t            stroke_color.end = new_transform @ torch.cat((stroke_color.end, torch.ones([1])))\n\t            stroke_color.end = stroke_color.end / stroke_color.end[2]\n\t            stroke_color.end = stroke_color.end[:2]\n\t        if 'filter' in style:\n\t            print('*** WARNING ***: Ignoring filter for path with id \"{}\"'.format(name))\n\t    return new_transform, fill_color, stroke_color, stroke_width, use_even_odd_rule\n", "def is_shape(tag):\n\t    return tag == 'path' or tag == 'polygon' or tag == 'line' or tag == 'circle' or tag == 'rect'\n\tdef parse_shape(node, transform, fill_color, shapes, shape_groups, defs):\n\t    tag = remove_namespaces(node.tag)\n\t    new_transform, new_fill_color, stroke_color, stroke_width, use_even_odd_rule = \\\n\t        parse_common_attrib(node, transform, fill_color, defs)\n\t    if tag == 'path':\n\t        d = node.attrib['d']\n\t        name = ''\n\t        if 'id' in node.attrib:\n", "            name = node.attrib['id']\n\t        force_closing = new_fill_color is not None\n\t        paths = pydiffvg.from_svg_path(d, new_transform, force_closing)\n\t        for idx, path in enumerate(paths):\n\t            assert(path.points.shape[1] == 2)\n\t            path.stroke_width = stroke_width\n\t            path.source_id = name\n\t            path.id = \"{}-{}\".format(name,idx) if len(paths)>1 else name\n\t        prev_shapes_size = len(shapes)\n\t        shapes = shapes + paths\n", "        shape_ids = torch.tensor(list(range(prev_shapes_size, len(shapes))))\n\t        shape_groups.append(pydiffvg.ShapeGroup(\\\n\t            shape_ids = shape_ids,\n\t            fill_color = new_fill_color,\n\t            stroke_color = stroke_color,\n\t            use_even_odd_rule = use_even_odd_rule,\n\t            id = name))\n\t    elif tag == 'polygon':\n\t        name = ''\n\t        if 'id' in node.attrib:\n", "            name = node.attrib['id']\n\t        force_closing = new_fill_color is not None\n\t        pts = node.attrib['points'].strip()\n\t        pts = pts.split(' ')\n\t        # import ipdb; ipdb.set_trace()\n\t        pts = [[float(y) for y in re.split(',| ', x)] for x in pts if x]\n\t        pts = torch.tensor(pts, dtype=torch.float32).view(-1, 2)\n\t        polygon = pydiffvg.Polygon(pts, force_closing)\n\t        polygon.stroke_width = stroke_width\n\t        shape_ids = torch.tensor([len(shapes)])\n", "        shapes.append(polygon)\n\t        shape_groups.append(pydiffvg.ShapeGroup(\\\n\t            shape_ids = shape_ids,\n\t            fill_color = new_fill_color,\n\t            stroke_color = stroke_color,\n\t            use_even_odd_rule = use_even_odd_rule,\n\t            shape_to_canvas = new_transform,\n\t            id = name))\n\t    elif tag == 'line':\n\t        x1 = float(node.attrib['x1'])\n", "        y1 = float(node.attrib['y1'])\n\t        x2 = float(node.attrib['x2'])\n\t        y2 = float(node.attrib['y2'])\n\t        p1 = torch.tensor([x1, y1])\n\t        p2 = torch.tensor([x2, y2])\n\t        points = torch.stack((p1, p2))\n\t        line = pydiffvg.Polygon(points, False)\n\t        line.stroke_width = stroke_width\n\t        shape_ids = torch.tensor([len(shapes)])\n\t        shapes.append(line)\n", "        shape_groups.append(pydiffvg.ShapeGroup(\\\n\t            shape_ids = shape_ids,\n\t            fill_color = new_fill_color,\n\t            stroke_color = stroke_color,\n\t            use_even_odd_rule = use_even_odd_rule,\n\t            shape_to_canvas = new_transform))\n\t    elif tag == 'circle':\n\t        radius = float(node.attrib['r'])\n\t        cx = float(node.attrib['cx'])\n\t        cy = float(node.attrib['cy'])\n", "        name = ''\n\t        if 'id' in node.attrib:\n\t            name = node.attrib['id']\n\t        center = torch.tensor([cx, cy])\n\t        circle = pydiffvg.Circle(radius = torch.tensor(radius),\n\t                                 center = center)\n\t        circle.stroke_width = stroke_width\n\t        shape_ids = torch.tensor([len(shapes)])\n\t        shapes.append(circle)\n\t        shape_groups.append(pydiffvg.ShapeGroup(\\\n", "            shape_ids = shape_ids,\n\t            fill_color = new_fill_color,\n\t            stroke_color = stroke_color,\n\t            use_even_odd_rule = use_even_odd_rule,\n\t            shape_to_canvas = new_transform))\n\t    elif tag == 'ellipse':\n\t        rx = float(node.attrib['rx'])\n\t        ry = float(node.attrib['ry'])\n\t        cx = float(node.attrib['cx'])\n\t        cy = float(node.attrib['cy'])\n", "        name = ''\n\t        if 'id' in node.attrib:\n\t            name = node.attrib['id']\n\t        center = torch.tensor([cx, cy])\n\t        circle = pydiffvg.Circle(radius = torch.tensor(radius),\n\t                                 center = center)\n\t        circle.stroke_width = stroke_width\n\t        shape_ids = torch.tensor([len(shapes)])\n\t        shapes.append(circle)\n\t        shape_groups.append(pydiffvg.ShapeGroup(\\\n", "            shape_ids = shape_ids,\n\t            fill_color = new_fill_color,\n\t            stroke_color = stroke_color,\n\t            use_even_odd_rule = use_even_odd_rule,\n\t            shape_to_canvas = new_transform))\n\t    elif tag == 'rect':\n\t        x = 0.0\n\t        y = 0.0\n\t        if x in node.attrib:\n\t            x = float(node.attrib['x'])\n", "        if y in node.attrib:\n\t            y = float(node.attrib['y'])\n\t        w = float(node.attrib['width'])\n\t        h = float(node.attrib['height'])\n\t        p_min = torch.tensor([x, y])\n\t        p_max = torch.tensor([x + w, x + h])\n\t        rect = pydiffvg.Rect(p_min = p_min, p_max = p_max)\n\t        rect.stroke_width = stroke_width\n\t        shape_ids = torch.tensor([len(shapes)])\n\t        shapes.append(rect)\n", "        shape_groups.append(pydiffvg.ShapeGroup(\\\n\t            shape_ids = shape_ids,\n\t            fill_color = new_fill_color,\n\t            stroke_color = stroke_color,\n\t            use_even_odd_rule = use_even_odd_rule,\n\t            shape_to_canvas = new_transform))\n\t    return shapes, shape_groups\n\tdef parse_group(node, transform, fill_color, shapes, shape_groups, defs):\n\t    if 'transform' in node.attrib:\n\t        transform = transform @ parse_transform(node.attrib['transform'])\n", "    if 'fill' in node.attrib:\n\t        fill_color = parse_color(node.attrib['fill'], defs)\n\t    for child in node:\n\t        tag = remove_namespaces(child.tag)\n\t        if is_shape(tag):\n\t            shapes, shape_groups = parse_shape(\\\n\t                child, transform, fill_color, shapes, shape_groups, defs)\n\t        elif tag == 'g':\n\t            shapes, shape_groups = parse_group(\\\n\t                child, transform, fill_color, shapes, shape_groups, defs)\n", "    return shapes, shape_groups\n\tdef parse_scene(node):\n\t    canvas_width = -1\n\t    canvas_height = -1\n\t    defs = {}\n\t    shapes = []\n\t    shape_groups = []\n\t    fill_color = torch.tensor([0.0, 0.0, 0.0, 1.0])\n\t    transform = torch.eye(3)\n\t    if 'viewBox' in node.attrib:\n", "        view_box_array_comma = node.attrib['viewBox'].split(',')\n\t        view_box_array = node.attrib['viewBox'].split()\n\t        if len(view_box_array) < len(view_box_array_comma):\n\t            view_box_array = view_box_array_comma\n\t        canvas_width = parse_int(view_box_array[2])\n\t        canvas_height = parse_int(view_box_array[3])\n\t    else:\n\t        if 'width' in node.attrib:\n\t            canvas_width = parse_int(node.attrib['width'])\n\t        else:\n", "            print('Warning: Can\\'t find canvas width.')\n\t        if 'height' in node.attrib:\n\t            canvas_height = parse_int(node.attrib['height'])\n\t        else:\n\t            print('Warning: Can\\'t find canvas height.')\n\t    for child in node:\n\t        tag = remove_namespaces(child.tag)\n\t        if tag == 'defs':\n\t            defs = parse_defs(child, transform, defs)\n\t        elif tag == 'style':\n", "            defs = parse_stylesheet(child, transform, defs)\n\t        elif tag == 'linearGradient':\n\t            if 'id' in child.attrib:\n\t                defs[child.attrib['id']] = parse_linear_gradient(child, transform, defs)\n\t        elif tag == 'radialGradient':\n\t            if 'id' in child.attrib:\n\t                defs[child.attrib['id']] = parse_radial_gradient(child, transform, defs)\n\t        elif is_shape(tag):\n\t            shapes, shape_groups = parse_shape(\\\n\t                child, transform, fill_color, shapes, shape_groups, defs)\n", "        elif tag == 'g':\n\t            shapes, shape_groups = parse_group(\\\n\t                child, transform, fill_color, shapes, shape_groups, defs)\n\t    return canvas_width, canvas_height, shapes, shape_groups\n\tdef svg_to_scene(filename):\n\t    \"\"\"\n\t        Load from a SVG file and convert to PyTorch tensors.\n\t    \"\"\"\n\t    tree = etree.parse(filename)\n\t    root = tree.getroot()\n", "    cwd = os.getcwd()\n\t    if (os.path.dirname(filename) != ''):\n\t        os.chdir(os.path.dirname(filename))\n\t    ret = parse_scene(root)\n\t    os.chdir(cwd)\n\t    return ret\n"]}
{"filename": "eval/post_refinement.py", "chunked_list": ["import argparse\n\timport os\n\timport sys\n\tsys.path.append(os.getcwd())\n\timport numpy as np\n\timport random\n\tfrom PIL import Image\n\timport imageio\n\timport yaml, shutil\n\timport torch\n", "import torch.nn as nn\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.optim.lr_scheduler import MultiStepLR\n\timport torch.nn.functional as F\n\timport torchvision\n\timport datasets\n\timport models, losses\n\timport utils\n\timport matplotlib.pyplot as plt\n", "from einops import rearrange, repeat, reduce, parse_shape\n\timport refine_svg\n\tfrom svgpathtools import svg2paths, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\n\timport pydiffvg\n\timport itertools\n\tfrom tqdm import tqdm\n\tfrom svg_simplification import svg_file_simplification\n\timport argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--outdir', required=True, type=str)\n", "parser.add_argument('--input', required=True, type=str)\n\tparser.add_argument('--fmin', default=0, type=int)\n\tparser.add_argument('--fmax', default=200, type=int)\n\targs = parser.parse_args()\n\texp_dir = args.outdir\n\texp_name = 'p4'\n\tconfig_str = '''\n\tloss:\n\t  name: local-loss\n\t  args:\n", "    lam_render: 1\n\t    lam_reg: 1.e-6\n\t'''\n\tconfig = yaml.load(config_str, Loader=yaml.FullLoader)\n\tloss_fn = losses.make(config['loss'])\n\tdef get_reference_image_path(path, font, char):\n\t    return os.path.join(path, font, f'{char:02d}_rec.png')\n\tdef get_init_svg_path(path, font, char):\n\t    return os.path.join(path, font, f'{char:02d}_init.svg')\n\tdef get_dst_svg_path(path, font, char):\n", "    return os.path.join(get_dst_font_dir(path, font), f'{char:02d}_{exp_name}.svg')\n\tdef get_dst_font_dir(path, font):\n\t    return os.path.join(exp_dir, font)\n\tdef post_refinement(path, font, glyph):\n\t    dst_dir = get_dst_font_dir(path, font)\n\t    os.makedirs(dst_dir, exist_ok=True)\n\t    init_svg_path = get_init_svg_path(path, font, glyph)\n\t    dst_svg_path = get_dst_svg_path(path, font, glyph)\n\t    image_path = get_reference_image_path(path, font, glyph)\n\t    target = torchvision.transforms.ToTensor()(Image.open(image_path).convert('L')).to(pydiffvg.get_device())\n", "    n_times = 4\n\t    if not os.path.exists(init_svg_path):\n\t        return None\n\t    if os.path.exists(dst_svg_path):\n\t        # skip\n\t        return None\n\t    for i in range(n_times):\n\t        if i == 0:\n\t            canvas_width, canvas_height, shapes, shape_groups = \\\n\t                pydiffvg.svg_to_scene(init_svg_path)\n", "        else:\n\t            if i == 1:\n\t                group, quad_line, merge, split = True, True, False, True\n\t            elif i == 2:\n\t                group, quad_line, merge, split = True, True, False, False\n\t            elif i == 3:\n\t                group, quad_line, merge, split = True, True, True, False\n\t            success = svg_file_simplification(dst_svg_path.replace('.svg', f'_{i-1}.svg'), dst_svg_path.replace('.svg', f'_{i-1}_sim.svg'),\\\n\t                    group=group, quad_line=quad_line, merge=merge, split=split)\n\t            if not success:\n", "                return None\n\t            canvas_width, canvas_height, shapes, shape_groups = \\\n\t                pydiffvg.svg_to_scene(dst_svg_path.replace('.svg', f'_{i-1}_sim.svg'))\n\t        iter_steps = 50 \n\t        scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n\t            canvas_width, canvas_height, shapes, shape_groups)\n\t        render = pydiffvg.RenderFunction.apply\n\t        # The output image is in linear RGB space. Do Gamma correction before saving the image.\n\t        points_vars = []\n\t        for path in shapes:\n", "            path.points.requires_grad = True\n\t            points_vars.append(path.points)\n\t        # Optimize\n\t        points_optim = torch.optim.Adam(points_vars, lr=0.5, betas=(0.9, 0.999))\n\t        # Adam iterations.\n\t        with tqdm(range(iter_steps), leave=False, desc='Refine') as iters:\n\t            for _ in iters:\n\t                points_optim.zero_grad()\n\t                # Forward pass: render the image.\n\t                scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n", "                    canvas_width, canvas_height, shapes, shape_groups)\n\t                try:\n\t                    img = render(canvas_width, # width\n\t                                canvas_height, # height\n\t                                2,   # num_samples_x\n\t                                2,   # num_samples_y\n\t                                42,   # seed\n\t                                None, # bg\n\t                                *scene_args)\n\t                except:\n", "                    return None\n\t                # Compose img with white background\n\t                img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n\t                img = img[:, :, :3]\n\t                loss_dict = loss_fn(img, target, shapes)\n\t                # Backpropagate the gradients.\n\t                loss_dict['loss'].backward()\n\t                # Take a gradient descent step.\n\t                points_optim.step()\n\t                for k, v in loss_dict.items():\n", "                    loss_dict[k] = f'{v.item():.6f}'\n\t                iters.set_postfix({\n\t                    **loss_dict,\n\t                })\n\t        if i < n_times - 1:\n\t            pydiffvg.save_svg_paths_only(dst_svg_path.replace('.svg', f'_{i}.svg'), canvas_width, canvas_height, shapes, shape_groups)\n\t        else:\n\t            pydiffvg.save_svg_paths_only(dst_svg_path, canvas_width, canvas_height, shapes, shape_groups)\n\t    return float(loss_dict['render'])\n\tdef main():\n", "    origin_path = args.input\n\t    font_list = [f'{i:04d}' for i in range(args.fmin, args.fmax)]\n\t    glyph_list = list(range(52))\n\t    font_list = font_list if font_list else os.listdir(origin_path)\n\t    os.makedirs(exp_dir, exist_ok=True)\n\t    task = sorted([(origin_path, f, g) for f, g in itertools.product(font_list, glyph_list)])\n\t    losses = []\n\t    for path, font, glyph in tqdm(task):\n\t        loss_render = post_refinement(path, font, glyph)\n\t        if loss_render is not None:\n", "            losses.append(loss_render)\n\t    print(sum(losses) / len(losses))\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "eval/run_metrics.py", "chunked_list": ["import os\n\timport sys\n\tsys.path.append(os.getcwd())\n\timport argparse\n\timport numpy as np\n\timport torch\n\tfrom skimage.metrics import structural_similarity as ssim\n\tfrom PIL import Image\n\tfrom tqdm import tqdm\n\timport cairosvg\n", "import math\n\tfrom svgpathtools import svg2paths2\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('--name', type=str, required=True)\n\tparser.add_argument('--gt', type=str, default='../data/dvf_png/font_pngs/test')\n\tparser.add_argument('--pred', type=str, required=True)\n\tparser.add_argument('--fontmin', type=int, default=0)\n\tparser.add_argument('--fontmax', type=int, default=100)\n\tparser.add_argument('--ff', type=str, default='{0:02d}_rec.png')\n\tparser.add_argument('--mode', type=str, default='svgrender', choices=['svgrender', 'svgcount', 'imgrec', 'multisvg'])\n", "parser.add_argument('--res', type=int, default=128)\n\tparser.add_argument('--gt_lowercase', action='store_true', default=False)\n\tparser.add_argument('--pred_lowercase', action='store_true', default=False) # crop larger to cover most lowercase letters and then scale back\n\tparser.add_argument('--glyph', type=int, default=52)\n\targs = parser.parse_args()\n\tclass Averager():\n\t    def __init__(self):\n\t        self.n = 0.0\n\t        self.v = 0.0\n\t    def add(self, v, n=1.0):\n", "        self.v = (self.v * self.n + v * n) / (self.n + n)\n\t        self.n += n\n\t    def item(self):\n\t        return self.v\n\tclass PNGMetric:\n\t    def __init__(self):\n\t        self.m_ssim = Averager()\n\t        self.m_l1 = Averager()\n\t        self.m_l2 = Averager()\n\t        self.m_siou = Averager()\n", "    def update(self, v_ssim, v_l1, v_l2, inter, union):\n\t        self.m_ssim.add(v_ssim)\n\t        self.m_l1.add(v_l1)\n\t        self.m_l2.add(v_l2)\n\t        self.m_siou.add(inter / union, union)\n\t    def info_dict(self):\n\t        return {\n\t            'ssim': self.m_ssim.item(),\n\t            'l1': self.m_l1.item(),\n\t            'l2': self.m_l2.item(),\n", "            'siou': self.m_siou.item(),\n\t            'siou_n': self.m_siou.n,\n\t        }\n\tclass SVGMetric:\n\t    def __init__(self):\n\t        pass\n\t    def update(self, v_ssim, v_l1, v_l2, v_siou):\n\t        pass\n\t    def info_dict(self):\n\t        pass\n", "def calc_ssim(pred, gt):\n\t    '''\n\t    pred: 1 x dim x dim\n\t    gt: 1 x dim x dim\n\t    '''\n\t    return ssim(pred, gt, data_range=1, gaussian_weights=True, sigma=1.5, use_sample_covariance=False, multichannel=False)\n\tdef calc_l1(pred, gt):\n\t    return np.mean(np.abs(pred - gt))\n\tdef calc_l2(pred, gt):\n\t    return np.mean(np.power(pred - gt, 2))\n", "def calc_siou(pred, gt):\n\t    pred = 1. - pred\n\t    gt = 1. - gt\n\t    return np.sum(np.abs(pred * gt)), np.sum(np.clip(pred + gt, 0, 1))\n\tdef surface_to_npim(surface):\n\t    \"\"\" Transforms a Cairo surface into a numpy array. \"\"\"\n\t    im = +np.frombuffer(surface.get_data(), np.uint8)\n\t    H,W = surface.get_height(), surface.get_width()\n\t    im.shape = (H,W,4) # for RGBA\n\t    return im\n", "def svg_to_npim(svg_bytestring, w, h):\n\t    \"\"\" Renders a svg bytestring as a RGB image in a numpy array \"\"\"\n\t    tree = cairosvg.parser.Tree(bytestring=svg_bytestring)\n\t    surf = cairosvg.surface.PNGSurface(tree,None,50,output_width=w, output_height=h).cairo\n\t    return surface_to_npim(surf)\n\tdef transform_path(path, lowercase=False, resolution=128, origin=256):\n\t    if lowercase:\n\t        return path.scaled(1.25*resolution/origin).translated(-resolution/8.)\n\t    else:\n\t        return path.scaled(resolution/origin)\n", "def render_svg_path(paths, svg_str=None, lowercase=False, resolution=128):\n\t    if svg_str is not None:\n\t        im = svg_to_npim(svg_str.encode('utf-8'), resolution, resolution)\n\t        return im[:, :, :3].mean(axis=-1) / 255.\n\t    else:\n\t        HEAD = f'''<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n\t    <svg baseProfile=\"full\" height=\"100%\" version=\"1.1\" viewBox=\"0,0,{resolution},{resolution}\" width=\"100%\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n\t    <defs />\n\t    '''\n\t        TAIL = '\\n</svg>'\n", "        path_template = '''<path d=\"{0}\" fill=\"black\" stroke=\"none\" />'''\n\t        strs = []\n\t        for path in paths:\n\t            d = transform_path(path, lowercase=lowercase, resolution=resolution).d()\n\t            strs.append(path_template.format(d))\n\t        svg_str = HEAD + '\\n'.join(strs) + TAIL\n\t        im = svg_to_npim(svg_str.encode('utf-8'), resolution, resolution)\n\t        return 1. - im[:, :, 3] / 255.\n\tdef read_png_gt(path, lowercase=False, resolution=128):\n\t    if lowercase:\n", "        cropped = Image.open(path).convert('L').crop((0, 0, 1024, 1280))\n\t        origin = Image.new('L', (1280, 1280), 255)\n\t        origin.paste(cropped, (128, 0, 1152, 1280))\n\t    else:\n\t        origin = Image.open(path).convert('L').crop((0, 0, 1024, 1024))\n\t    img_np = np.asarray(origin.resize((resolution, resolution), resample=Image.BICUBIC)) / 255.\n\t    return img_np\n\tdef read_png_pred(path, lowercase=False, resolution=128):\n\t    origin = Image.open(path).convert('L')\n\t    img_np = np.asarray(origin.resize((resolution, resolution), resample=Image.BICUBIC)) / 255.\n", "    return img_np\n\tdef get_gt_png(font, glyph, lowercase=False, path_only=False, resolution=128):\n\t    '''\n\t    path: str\n\t    img_np: np.array, resolution x resolution\n\t    pred: 0 - black - in glyph\n\t    gt:   1 - white - out glyph\n\t    '''\n\t    path = os.path.join(args.gt, font, f'{glyph}_1024.png')\n\t    if path_only:\n", "        return path\n\t    if not os.path.exists(path):\n\t        print(\"not found gt png, \", path)\n\t        return path, None\n\t    img_np = read_png_gt(path, lowercase=lowercase, resolution=resolution)\n\t    return path, img_np\n\tdef get_gt_svg(font, glyph, path_only=False, string_only=False):\n\t    '''\n\t    dvf_png/font_svgs/test/0000/svgs\n\t    '''\n", "    path = os.path.join(args.gt, font, 'svgs', f'gt_{glyph:02d}.svg')\n\t    if path_only:\n\t        return path\n\t    if not os.path.exists(path):\n\t        print(\"not found gt svg, \", path)\n\t        return path, None\n\t    paths, attributes, svg_attributes = svg2paths2(path)\n\t    return path, paths\n\tdef get_pred_png(font, glyph, lowercase=False, path_only=False, resolution=128):\n\t    '''\n", "    path: str\n\t    img_np: np.array, resolution x resolution\n\t    pred: 0 - black - in glyph\n\t    gt:   1 - white - out glyph\n\t    '''\n\t    path = os.path.join(args.pred, font, args.ff.format(glyph))\n\t    if path_only:\n\t        return path\n\t    if not os.path.exists(path):\n\t        print(\"not found pred png, \", path)\n", "        return path, None\n\t    img_np = read_png_pred(path, lowercase=lowercase, resolution=resolution)\n\t    return path, img_np\n\tdef get_pred_svg(font, glyph, path_only=False):\n\t    '''\n\t    dvf_png/font_svgs/test/0000/svgs\n\t    '''\n\t    path = os.path.join(args.pred, font, args.ff.format(glyph))\n\t    if path_only:\n\t        return path\n", "    if not os.path.exists(path):\n\t        print(\"not found pred svg, \", path)\n\t        return path, None\n\t    paths, attributes, svg_attributes = svg2paths2(path)\n\t    return path, paths\n\tif __name__ == '__main__':\n\t    font_list = [f'{i:04d}' for i in range(args.fontmin, args.fontmax)]\n\t    glyph_list = list(range(args.glyph))\n\t    result_dir = os.path.join('eval', 'save', args.name, args.mode)\n\t    dst_txt = os.path.join(result_dir, os.path.abspath(args.pred).replace('/', '_') + f'_{args.res:04d}.txt')\n", "    if os.path.exists(dst_txt):\n\t        print('please consider', 'rm', dst_txt)\n\t        exit(0)\n\t    print('Compare', args.gt, args.pred)\n\t    print('Mode', args.mode)\n\t    os.makedirs(result_dir, exist_ok=True)\n\t    if args.mode in ['svgrender', 'imgrec', 'multisvg']:\n\t        all_metric = PNGMetric()\n\t    else:\n\t        all_metric = SVGMetric()\n", "    font_metrics = []\n\t    with tqdm(font_list) as pbar:\n\t        for font in pbar:\n\t            if args.mode in ['svgrender', 'imgrec', 'multisvg']:\n\t                font_metric = PNGMetric()\n\t            else:\n\t                font_metric = SVGMetric()\n\t            for i in glyph_list:\n\t                if args.mode == 'svgrender':\n\t                    svg_fpath, svgpaths = get_pred_svg(font, i, path_only=False)\n", "                    if svgpaths is not None:\n\t                        pred_np = render_svg_path(svgpaths, lowercase=args.pred_lowercase, resolution=args.res)\n\t                        png_fpath, gt_np = get_gt_png(font, i, path_only=False, lowercase=args.gt_lowercase, resolution=args.res)\n\t                        if pred_np is not None and gt_np is not None:\n\t                            v_ssim = calc_ssim(pred_np, gt_np)\n\t                            v_l1 = calc_l1(pred_np, gt_np)\n\t                            v_l2 = calc_l2(pred_np, gt_np)\n\t                            inter, union = calc_siou(pred_np, gt_np)\n\t                            font_metric.update(v_ssim, v_l1, v_l2, inter, union)\n\t                            all_metric.update(v_ssim, v_l1, v_l2, inter, union)\n", "                elif args.mode == 'imgrec':\n\t                    _, pred_np = get_pred_png(font, i, path_only=False, resolution=args.res)\n\t                    _, gt_np = get_gt_png(font, i, path_only=False, lowercase=args.gt_lowercase, resolution=args.res)\n\t                    if pred_np is not None and gt_np is not None:\n\t                        v_ssim = calc_ssim(pred_np, gt_np)\n\t                        v_l1 = calc_l1(pred_np, gt_np)\n\t                        v_l2 = calc_l2(pred_np, gt_np)\n\t                        inter, union = calc_siou(pred_np, gt_np)\n\t                        font_metric.update(v_ssim, v_l1, v_l2, inter, union)\n\t                        all_metric.update(v_ssim, v_l1, v_l2, inter, union)\n", "                elif args.mode == 'multisvg':\n\t                    svg_fpath = get_pred_svg(font, i, path_only=True)\n\t                    if svg_fpath is not None:\n\t                        with open(svg_fpath) as svgf:\n\t                            svg_str = svgf.read()\n\t                        pred_np = render_svg_path(None, svg_str=svg_str, lowercase=args.pred_lowercase, resolution=args.res)\n\t                        # Image.fromarray((pred_np*255).astype(np.uint8)).save('test.png')\n\t                        png_fpath, gt_np = get_gt_png(font, i, path_only=False, lowercase=args.gt_lowercase, resolution=args.res)\n\t                        # Image.fromarray((gt_np*255).astype(np.uint8)).save('gt.png')\n\t                        # import pdb;pdb.set_trace()\n", "                        if pred_np is not None and gt_np is not None:\n\t                            v_ssim = calc_ssim(pred_np, gt_np)\n\t                            v_l1 = calc_l1(pred_np, gt_np)\n\t                            v_l2 = calc_l2(pred_np, gt_np)\n\t                            inter, union = calc_siou(pred_np, gt_np)\n\t                            font_metric.update(v_ssim, v_l1, v_l2, inter, union)\n\t                            all_metric.update(v_ssim, v_l1, v_l2, inter, union)\n\t            font_metrics.append((font, font_metric.info_dict()))\n\t            pbar.set_postfix(all_metric.info_dict())\n\t    with open(dst_txt, 'w') as f:\n", "        for font, md in font_metrics:\n\t            print(font, *md.values(), file=f)\n\t        print('all', *all_metric.info_dict().values(), file=f)\n"]}
{"filename": "eval/svg_simplification.py", "chunked_list": ["import argparse\n\timport os\n\timport sys\n\tsys.path.append(os.getcwd())\n\timport numpy as np\n\timport random\n\tfrom PIL import Image\n\timport imageio\n\timport yaml, shutil\n\timport torch\n", "import torch.nn as nn\n\tfrom tqdm import tqdm\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.optim.lr_scheduler import MultiStepLR\n\timport torch.nn.functional as F\n\timport torchvision\n\tfrom svgpathtools import svg2paths2, parse_path, wsvg, Line, QuadraticBezier, CubicBezier, Path\n\timport re\n\timport glob\n\tdef implicitize_bezier_curve(a, b, c, norm=True):\n", "    '''\n\t    Bt = a(1-t)^2 + bt(1-t) + ct^2\n\t    '''\n\t    x1, x2, x3 = a.real, b.real, c.real\n\t    y1, y2, y3 = a.imag, b.imag, c.imag\n\t    A = y1**2 - 4*y1*y2 + 2*y1*y3 + 4*y2**2 - 4*y2*y3 + y3**2\n\t    B = x1**2 - 4*x1*x2 + 2*x1*x3 + 4*x2**2 - 4*x2*x3 + x3**2\n\t    C = -2*x1*y1 + 4*x1*y2 - 2*x1*y3 + 4*x2*y1 - 8*x2*y2 + 4*x2*y3 - 2*x3*y1 + 4*x3*y2 - 2*x3*y3\n\t    D = 2*x1*y1*y3 - 4*x1*y2**2 + 4*x1*y2*y3 - 2*x1*y3**2 + 4*x2*y1*y2 -8*x2*y1*y3 + \\\n\t        2*x3*y1*y3 - 4*x3*y2**2 + 4*x3*y1*y2 - 2*x3*y1**2  + 4*x2*y2*y3\n", "    E = 2*y1*x1*x3 - 4*y1*x2**2 + 4*y1*x2*x3 - 2*y1*x3**2 + 4*y2*x1*x2 -8*y2*x1*x3 + \\\n\t        2*y3*x1*x3 - 4*y3*x2**2 + 4*y3*x1*x2 - 2*y3*x1**2  + 4*y2*x2*x3\n\t    F = (x1*y3)**2 - 4*x1*x2*y2*y3 -2*x1*x3*y1*y3 + 4*x1*x3*y2**2 + 4*x2**2*y1*y3 - 4*x2*x3*y1*y2 + (x3*y1)**2\n\t    eff = np.array([A,B,C,D,E,F])\n\t    if norm:\n\t        norm_eff = eff / np.linalg.norm(eff, ord=np.inf)\n\t        return norm_eff\n\t    else:\n\t        return eff\n\tdef quad_to_line(path, angle_threshold=171, length_threshold=1):\n", "    new_segs = []\n\t    cos_t = np.cos(angle_threshold/180*np.pi)\n\t    for seg in path:\n\t        if isinstance(seg, QuadraticBezier):\n\t            # A = seg.start\n\t            # B = seg.control\n\t            # C = seg.end\n\t            ab = seg.control - seg.start\n\t            cb = seg.control - seg.end\n\t            if abs(ab) < length_threshold or abs(cb) <  length_threshold:\n", "                new_segs.append(Line(seg.start, seg.end))\n\t            else:\n\t                cos_abc = (ab.real * cb.real + ab.imag * cb.imag) / abs(ab) / abs(cb)\n\t                if cos_abc < cos_t:\n\t                    new_segs.append(Line(seg.start, seg.end))\n\t                else:\n\t                    new_segs.append(seg)\n\t        else:\n\t            new_segs.append(seg)\n\t    return Path(*new_segs)\n", "def cos_complex(a, b):\n\t    return (a.real * b.real + a.imag * b.imag) / abs(a) / abs(b)\n\tdef merge_two_line(a, b):\n\t    return Line(a.start, b.end)\n\tdef vec_intersect(s1, s2, a, b):\n\t    '''\n\t    A = s1 + a*t\n\t    B = s2 + b*t\n\t    '''\n\t    ax, ay = a.real, a.imag\n", "    bx, by = b.real, b.imag\n\t    x1, y1 = s1.real, s1.imag\n\t    x2, y2 = s2.real, s2.imag\n\t    l = np.array([[ay, -ax], [by, -bx]])\n\t    r = np.array([ay*x1-ax*y1, by*x2-bx*y2])\n\t    x = np.linalg.solve(l, r)\n\t    return complex(x[0] + x[1]*1j)\n\tdef merge_two_quad(a, b):\n\t    s = a.start\n\t    e = b.end\n", "    c = vec_intersect(s, e, a.control - s, b.control - e)\n\t    return QuadraticBezier(s, c, e)\n\tdef group_near_points(path, size, threshold=3):\n\t    new_segs = []\n\t    last_end = None\n\t    accumulated_length = 0\n\t    for seg in path:\n\t        if last_end is None:\n\t            last_end = seg.start\n\t        sl = seg.length()\n", "        if sl + accumulated_length < threshold:\n\t            accumulated_length += sl\n\t            continue\n\t        accumulated_length = 0\n\t        seg.start = last_end\n\t        new_segs.append(seg)\n\t        last_end = seg.end\n\t    if accumulated_length > 0:\n\t        if len(new_segs) == 0:\n\t            return None\n", "        new_segs[0].start = last_end\n\t    if len(new_segs) >= 2:\n\t        return new_segs\n\t    else:\n\t        return None\n\tdef get_wh(svg_attributes):\n\t    if 'viewBox' in svg_attributes:\n\t        vb = svg_attributes['viewBox']\n\t        view_box_array_comma = vb.split(',')\n\t        view_box_array = vb.split()\n", "        if len(view_box_array) < len(view_box_array_comma):\n\t            view_box_array = view_box_array_comma\n\t        w = int(view_box_array[2])\n\t        h = int(view_box_array[3])\n\t        return w, h\n\t    return int(svg_attributes['width']), int(svg_attributes['height'])\n\tdef split_segments(path, size, length_threshold=0.1, angle_threshold=90):\n\t    new_segs = []\n\t    w, h = size\n\t    cos_th = np.cos(angle_threshold*np.pi/180)\n", "    for seg in path:\n\t        if isinstance(seg, QuadraticBezier):\n\t            s = seg.start\n\t            c = seg.control\n\t            e = seg.end\n\t            if seg.length() > length_threshold * w:\n\t                c1 = (s + c) / 2\n\t                c2 = (c + e) / 2\n\t                mid = (c1 + c2) / 2\n\t                new_segs.append(QuadraticBezier(s, c1, mid))\n", "                new_segs.append(QuadraticBezier(mid, c2, e))\n\t            else:\n\t                new_segs.append(seg)\n\t        elif isinstance(seg, Line):\n\t            if seg.length() > length_threshold * w:\n\t                s = seg.start\n\t                e = seg.end\n\t                mid = (s + e) / 2\n\t                new_segs.append(Line(s, mid))\n\t                new_segs.append(Line(mid, e))\n", "            else:\n\t                new_segs.append(seg)\n\t    return new_segs\n\tdef merge_segments(path, size, angle_threshold=175, eff_threshold=0.02):\n\t    '''\n\t    coord norm to [-0.5, 0.5]\n\t    consider only adjust segments\n\t    '''\n\t    w, h = size\n\t    last_vec = None\n", "    last_imp = None\n\t    lp = len(path)\n\t    cos_t = np.cos(angle_threshold/180*np.pi)\n\t    new_segs = []\n\t    buf = []\n\t    def merge_buf(buf):\n\t        if len(buf) == 1:\n\t            return buf[0]\n\t        fn = merge_two_quad if isinstance(buf[0], QuadraticBezier) else merge_two_line\n\t        return fn(buf[0], buf[-1])\n", "    for seg in path:\n\t        if len(buf) == 0:\n\t            buf.append(seg)\n\t        elif not (type(seg) is type(buf[0])):\n\t            res = merge_buf(buf)\n\t            new_segs.append(res)\n\t            buf = [seg]\n\t        else:\n\t            std = buf[0]\n\t            if isinstance(seg, Line):\n", "                vec = seg.end - seg.start\n\t                vec_std = std.end - std.start\n\t                cos_vec = cos_complex(vec, vec_std)\n\t                if abs(cos_vec) > abs(cos_t):\n\t                    buf.append(seg)\n\t                else:\n\t                    res = merge_buf(buf)\n\t                    new_segs.append(res)\n\t                    buf = [seg]\n\t            else:\n", "                eff_std = implicitize_bezier_curve(std.start / w, std.control / w, std.end / w, norm=True)\n\t                eff = implicitize_bezier_curve(seg.start / w, seg.control / w, seg.end / w, norm=True)\n\t                err = min(np.linalg.norm(eff_std - eff), np.linalg.norm(eff_std + eff))\n\t                if err < eff_threshold:\n\t                    buf.append(seg)\n\t                else:\n\t                    res = merge_buf(buf)\n\t                    new_segs.append(res)\n\t                    buf = [seg]\n\t    if len(buf) > 0:\n", "        res = merge_buf(buf)\n\t        new_segs.append(res)\n\t    return new_segs\n\tdef svg_file_simplification(svg_path, dst_path, group=True, quad_line=True, merge=True, split=True):\n\t    '''\n\t    svg_path: [path]\n\t    path may contain multiple isolated paths\n\t    '''\n\t    paths, attributes, svg_attributes = svg2paths2(svg_path)\n\t    assert(len(paths) == 1)\n", "    l = len(paths[0])\n\t    iso_paths = paths[0].continuous_subpaths()\n\t    w, h = get_wh(svg_attributes=svg_attributes)\n\t    assert(w == h and w == 256)\n\t    new_path = Path()\n\t    for path in iso_paths:\n\t        if group:\n\t            path = group_near_points(path, size=(w, h))\n\t        if path is not None:\n\t            if quad_line:\n", "                path = quad_to_line(path)\n\t            if merge:\n\t                path = merge_segments(path, size=(w, h))\n\t            if split:\n\t                path = split_segments(path, size=(w,h))\n\t            new_path += Path(*path)\n\t    if len(new_path) > 0:\n\t        wsvg(paths=[new_path], attributes=attributes, svg_attributes=svg_attributes, forceZ=True, filename=dst_path)\n\t        return True\n\t    else:\n", "        return False"]}
{"filename": "models/decoder.py", "chunked_list": ["from typing import Type, Any, Callable, Union, List, Optional\n\timport math\n\timport torch\n\timport torch.nn as nn\n\timport numpy as np\n\tfrom torch import Tensor\n\timport models\n\tfrom einops import rearrange, reduce, repeat\n\tdef get_timestep_embedding(timesteps, embedding_dim):\n\t    \"\"\"\n", "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n\t    From Fairseq.\n\t    Build sinusoidal embeddings.\n\t    This matches the implementation in tensor2tensor, but differs slightly\n\t    from the description in Section 3.5 of \"Attention Is All You Need\".\n\t    \"\"\"\n\t    assert len(timesteps.shape) == 1\n\t    half_dim = embedding_dim // 2\n\t    emb = math.log(10000) / (half_dim - 1)\n\t    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n", "    emb = emb.to(device=timesteps.device)\n\t    emb = timesteps.float()[:, None] * emb[None, :]\n\t    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n\t    if embedding_dim % 2 == 1:  # zero pad\n\t        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n\t    return emb\n\tdef nonlinearity(x):\n\t    # swish\n\t    return x*torch.sigmoid(x)\n\tdef Normalize(in_channels):\n", "    return torch.nn.GroupNorm(num_groups=min(in_channels, 32), num_channels=in_channels, eps=1e-6, affine=True)\n\tclass Upsample(nn.Module):\n\t    def __init__(self, in_channels, with_conv):\n\t        super().__init__()\n\t        self.with_conv = with_conv\n\t        if self.with_conv:\n\t            self.conv = torch.nn.Conv2d(in_channels,\n\t                                        in_channels,\n\t                                        kernel_size=3,\n\t                                        stride=1,\n", "                                        padding=1)\n\t    def forward(self, x):\n\t        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n\t        if self.with_conv:\n\t            x = self.conv(x)\n\t        return x\n\tclass Downsample(nn.Module):\n\t    def __init__(self, in_channels, with_conv):\n\t        super().__init__()\n\t        self.with_conv = with_conv\n", "        if self.with_conv:\n\t            # no asymmetric padding in torch conv, must do it ourselves\n\t            self.conv = torch.nn.Conv2d(in_channels,\n\t                                        in_channels,\n\t                                        kernel_size=3,\n\t                                        stride=2,\n\t                                        padding=0)\n\t    def forward(self, x):\n\t        if self.with_conv:\n\t            pad = (0,1,0,1)\n", "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n\t            x = self.conv(x)\n\t        else:\n\t            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n\t        return x\n\tclass ResnetBlock(nn.Module):\n\t    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n\t                 dropout, temb_channels=512):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n", "        out_channels = in_channels if out_channels is None else out_channels\n\t        self.out_channels = out_channels\n\t        self.use_conv_shortcut = conv_shortcut\n\t        self.norm1 = Normalize(in_channels)\n\t        self.conv1 = torch.nn.Conv2d(in_channels,\n\t                                     out_channels,\n\t                                     kernel_size=3,\n\t                                     stride=1,\n\t                                     padding=1)\n\t        if temb_channels > 0:\n", "            self.temb_proj = torch.nn.Linear(temb_channels,\n\t                                             out_channels)\n\t        self.norm2 = Normalize(out_channels)\n\t        self.dropout = torch.nn.Dropout(dropout)\n\t        self.conv2 = torch.nn.Conv2d(out_channels,\n\t                                     out_channels,\n\t                                     kernel_size=3,\n\t                                     stride=1,\n\t                                     padding=1)\n\t        if self.in_channels != self.out_channels:\n", "            if self.use_conv_shortcut:\n\t                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n\t                                                     out_channels,\n\t                                                     kernel_size=3,\n\t                                                     stride=1,\n\t                                                     padding=1)\n\t            else:\n\t                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n\t                                                    out_channels,\n\t                                                    kernel_size=1,\n", "                                                    stride=1,\n\t                                                    padding=0)\n\t    def forward(self, x, temb):\n\t        h = x\n\t        h = self.norm1(h)\n\t        h = nonlinearity(h)\n\t        h = self.conv1(h)\n\t        if temb is not None:\n\t            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n\t        h = self.norm2(h)\n", "        h = nonlinearity(h)\n\t        h = self.dropout(h)\n\t        h = self.conv2(h)\n\t        if self.in_channels != self.out_channels:\n\t            if self.use_conv_shortcut:\n\t                x = self.conv_shortcut(x)\n\t            else:\n\t                x = self.nin_shortcut(x)\n\t        return x+h\n\tclass AttnBlock(nn.Module):\n", "    def __init__(self, in_channels):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.norm = Normalize(in_channels)\n\t        self.q = torch.nn.Conv2d(in_channels,\n\t                                 in_channels,\n\t                                 kernel_size=1,\n\t                                 stride=1,\n\t                                 padding=0)\n\t        self.k = torch.nn.Conv2d(in_channels,\n", "                                 in_channels,\n\t                                 kernel_size=1,\n\t                                 stride=1,\n\t                                 padding=0)\n\t        self.v = torch.nn.Conv2d(in_channels,\n\t                                 in_channels,\n\t                                 kernel_size=1,\n\t                                 stride=1,\n\t                                 padding=0)\n\t        self.proj_out = torch.nn.Conv2d(in_channels,\n", "                                        in_channels,\n\t                                        kernel_size=1,\n\t                                        stride=1,\n\t                                        padding=0)\n\t    def forward(self, x):\n\t        h_ = x\n\t        h_ = self.norm(h_)\n\t        q = self.q(h_)\n\t        k = self.k(h_)\n\t        v = self.v(h_)\n", "        # compute attention\n\t        b,c,h,w = q.shape\n\t        q = q.reshape(b,c,h*w)\n\t        q = q.permute(0,2,1)   # b,hw,c\n\t        k = k.reshape(b,c,h*w) # b,c,hw\n\t        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n\t        w_ = w_ * (int(c)**(-0.5))\n\t        w_ = torch.nn.functional.softmax(w_, dim=2)\n\t        # attend to values\n\t        v = v.reshape(b,c,h*w)\n", "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n\t        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n\t        h_ = h_.reshape(b,c,h,w)\n\t        h_ = self.proj_out(h_)\n\t        return x+h_\n\t@models.register('img-encoder')\n\tclass ImgEncoder(nn.Module):\n\t    def __init__(self, *, ch, ch_mult=(1,2,4,8), num_res_blocks,\n\t                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n\t                 resolution, z_channels, double_z=True, use_attn=True, key='img', **ignore_kwargs):\n", "        super().__init__()\n\t        self.ch = ch\n\t        self.temb_ch = 0\n\t        self.num_resolutions = len(ch_mult)\n\t        self.num_res_blocks = num_res_blocks\n\t        self.resolution = resolution\n\t        self.in_channels = in_channels\n\t        self.use_attn = use_attn\n\t        # downsampling\n\t        self.conv_in = torch.nn.Conv2d(in_channels,\n", "                                       self.ch,\n\t                                       kernel_size=3,\n\t                                       stride=1,\n\t                                       padding=1)\n\t        curr_res = resolution\n\t        in_ch_mult = (1,)+tuple(ch_mult)\n\t        self.down = nn.ModuleList()\n\t        for i_level in range(self.num_resolutions):\n\t            block = nn.ModuleList()\n\t            attn = nn.ModuleList()\n", "            block_in = ch*in_ch_mult[i_level]\n\t            block_out = ch*ch_mult[i_level]\n\t            for i_block in range(self.num_res_blocks):\n\t                block.append(ResnetBlock(in_channels=block_in,\n\t                                         out_channels=block_out,\n\t                                         temb_channels=self.temb_ch,\n\t                                         dropout=dropout))\n\t                block_in = block_out\n\t                if curr_res in attn_resolutions:\n\t                    attn.append(AttnBlock(block_in))\n", "            down = nn.Module()\n\t            down.block = block\n\t            down.attn = attn\n\t            if i_level != self.num_resolutions-1:\n\t                down.downsample = Downsample(block_in, resamp_with_conv)\n\t                curr_res = curr_res // 2\n\t            self.down.append(down)\n\t        # middle\n\t        self.mid = nn.Module()\n\t        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n", "                                       out_channels=block_in,\n\t                                       temb_channels=self.temb_ch,\n\t                                       dropout=dropout)\n\t        if self.use_attn:\n\t            self.mid.attn_1 = AttnBlock(block_in)\n\t        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n\t                                       out_channels=block_in,\n\t                                       temb_channels=self.temb_ch,\n\t                                       dropout=dropout)\n\t        # end\n", "        self.norm_out = Normalize(block_in)\n\t        self.conv_out = torch.nn.Conv2d(block_in,\n\t                                        2*z_channels if double_z else z_channels,\n\t                                        kernel_size=3,\n\t                                        stride=1,\n\t                                        padding=1)\n\t        self.key = key\n\t    def forward(self, x):\n\t        if self.key is not None:\n\t            x = x[self.key]\n", "        # assert x.shape[2] == x.shape[3] == self.resolution, \"{}, {}, {}\".format(x.shape[2], x.shape[3], self.resolution)\n\t        # timestep embedding\n\t        temb = None\n\t        # # downsampling\n\t        # hs = [self.conv_in(x)]\n\t        # for i_level in range(self.num_resolutions):\n\t        #     for i_block in range(self.num_res_blocks):\n\t        #         h = self.down[i_level].block[i_block](hs[-1], temb)\n\t        #         if len(self.down[i_level].attn) > 0:\n\t        #             h = self.down[i_level].attn[i_block](h)\n", "        #         hs.append(h)\n\t        #     if i_level != self.num_resolutions-1:\n\t        #         hs.append(self.down[i_level].downsample(hs[-1]))\n\t        # # middle\n\t        # h = hs[-1]\n\t        # downsampling\n\t        h = self.conv_in(x)\n\t        for i_level in range(self.num_resolutions):\n\t            for i_block in range(self.num_res_blocks):\n\t                h = self.down[i_level].block[i_block](h, temb)\n", "                if len(self.down[i_level].attn) > 0:\n\t                    h = self.down[i_level].attn[i_block](h)\n\t            if i_level != self.num_resolutions-1:\n\t                h = self.down[i_level].downsample(h)\n\t        # middle\n\t        h = self.mid.block_1(h, temb)\n\t        if self.use_attn:\n\t            h = self.mid.attn_1(h)\n\t        h = self.mid.block_2(h, temb)\n\t        # end\n", "        h = self.norm_out(h)\n\t        h = nonlinearity(h)\n\t        h = self.conv_out(h)\n\t        # output: b x channel x 1 x 1 \n\t        h = h.squeeze()\n\t        return h\n\t@models.register('img-decoder')\n\tclass ImgDecoder(nn.Module):\n\t    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n\t                 attn_resolutions, dropout=0.0, resamp_with_conv=True, # in_channels,\n", "                 resolution, z_channels, give_pre_end=False, **ignorekwargs):\n\t        super().__init__()\n\t        self.ch = ch\n\t        self.temb_ch = 0\n\t        self.num_resolutions = len(ch_mult)\n\t        self.num_res_blocks = num_res_blocks\n\t        self.resolution = resolution\n\t        # self.in_channels = in_channels\n\t        self.give_pre_end = give_pre_end\n\t        # compute in_ch_mult, block_in and curr_res at lowest res\n", "        in_ch_mult = (1,)+tuple(ch_mult)\n\t        block_in = ch*ch_mult[self.num_resolutions-1]\n\t        curr_res = resolution // 2**(self.num_resolutions-1)\n\t        self.z_shape = (1,z_channels,curr_res,curr_res)\n\t        print(\"Working with z of shape {} = {} dimensions.\".format(\n\t            self.z_shape, np.prod(self.z_shape)))\n\t        # z to block_in\n\t        self.conv_in = torch.nn.Conv2d(z_channels,\n\t                                       block_in,\n\t                                       kernel_size=3,\n", "                                       stride=1,\n\t                                       padding=1)\n\t        # middle\n\t        self.mid = nn.Module()\n\t        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n\t                                       out_channels=block_in,\n\t                                       temb_channels=self.temb_ch,\n\t                                       dropout=dropout)\n\t        self.mid.attn_1 = AttnBlock(block_in)\n\t        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n", "                                       out_channels=block_in,\n\t                                       temb_channels=self.temb_ch,\n\t                                       dropout=dropout)\n\t        # upsampling\n\t        self.up = nn.ModuleList()\n\t        for i_level in reversed(range(self.num_resolutions)):\n\t            block = nn.ModuleList()\n\t            attn = nn.ModuleList()\n\t            block_out = ch*ch_mult[i_level]\n\t            for i_block in range(self.num_res_blocks+1):\n", "                block.append(ResnetBlock(in_channels=block_in,\n\t                                         out_channels=block_out,\n\t                                         temb_channels=self.temb_ch,\n\t                                         dropout=dropout))\n\t                block_in = block_out\n\t                if curr_res in attn_resolutions:\n\t                    attn.append(AttnBlock(block_in))\n\t            up = nn.Module()\n\t            up.block = block\n\t            up.attn = attn\n", "            if i_level != 0:\n\t                up.upsample = Upsample(block_in, resamp_with_conv)\n\t                curr_res = curr_res * 2\n\t            self.up.insert(0, up) # prepend to get consistent order\n\t        # end\n\t        self.norm_out = Normalize(block_in)\n\t        self.conv_out = torch.nn.Conv2d(block_in,\n\t                                        out_ch,\n\t                                        kernel_size=3,\n\t                                        stride=1,\n", "                                        padding=1)\n\t    def forward(self, z):\n\t        #assert z.shape[1:] == self.z_shape[1:]\n\t        self.last_z_shape = z.shape\n\t        # timestep embedding\n\t        temb = None\n\t        # z to block_in\n\t        h = self.conv_in(z)\n\t        # middle\n\t        h = self.mid.block_1(h, temb)\n", "        h = self.mid.attn_1(h)\n\t        h = self.mid.block_2(h, temb)\n\t        # upsampling\n\t        for i_level in reversed(range(self.num_resolutions)):\n\t            for i_block in range(self.num_res_blocks+1):\n\t                h = self.up[i_level].block[i_block](h, temb)\n\t                if len(self.up[i_level].attn) > 0:\n\t                    h = self.up[i_level].attn[i_block](h)\n\t            if i_level != 0:\n\t                h = self.up[i_level].upsample(h)\n", "        # end\n\t        if self.give_pre_end:\n\t            return h\n\t        h = self.norm_out(h)\n\t        h = nonlinearity(h)\n\t        h = self.conv_out(h)\n\t        return h\n\t@models.register('light-img-encoder')\n\tclass LightImgEncoder(nn.Module):\n\t    def __init__(self, *, ch, ch_mult=(1,2,4,8), num_res_blocks,\n", "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n\t                 resolution, z_channels, double_z=True, use_attn=True, key='img', **ignore_kwargs):\n\t        super().__init__()\n\t        self.ch = ch\n\t        self.temb_ch = 0\n\t        self.num_resolutions = len(ch_mult)\n\t        self.num_res_blocks = num_res_blocks\n\t        self.resolution = resolution\n\t        self.in_channels = in_channels\n\t        self.use_attn = use_attn\n", "        # downsampling\n\t        self.conv_in = torch.nn.Conv2d(in_channels,\n\t                                       self.ch,\n\t                                       kernel_size=3,\n\t                                       stride=1,\n\t                                       padding=1)\n\t        curr_res = resolution\n\t        in_ch_mult = (1,)+tuple(ch_mult)\n\t        self.down = nn.ModuleList()\n\t        for i_level in range(self.num_resolutions):\n", "            block = nn.ModuleList()\n\t            attn = nn.ModuleList()\n\t            block_in = ch*in_ch_mult[i_level]\n\t            block_out = ch*ch_mult[i_level]\n\t            for i_block in range(self.num_res_blocks):\n\t                block.append(ResnetBlock(in_channels=block_in,\n\t                                         out_channels=block_out,\n\t                                         temb_channels=self.temb_ch,\n\t                                         dropout=dropout))\n\t                block_in = block_out\n", "                if curr_res in attn_resolutions:\n\t                    attn.append(AttnBlock(block_in))\n\t            down = nn.Module()\n\t            down.block = block\n\t            down.attn = attn\n\t            if i_level != self.num_resolutions-1:\n\t                down.downsample = Downsample(block_in, resamp_with_conv)\n\t                curr_res = curr_res // 2\n\t            self.down.append(down)\n\t        # middle\n", "        # self.mid = nn.Module()\n\t        # self.mid.block_1 = ResnetBlock(in_channels=block_in,\n\t        #                                out_channels=block_in,\n\t        #                                temb_channels=self.temb_ch,\n\t        #                                dropout=dropout)\n\t        # if self.use_attn:\n\t        #     self.mid.attn_1 = AttnBlock(block_in)\n\t        # self.mid.block_2 = ResnetBlock(in_channels=block_in,\n\t        #                                out_channels=block_in,\n\t        #                                temb_channels=self.temb_ch,\n", "        #                                dropout=dropout)\n\t        # end\n\t        # self.norm_out = Normalize(block_in)\n\t        # self.conv_out = torch.nn.Conv2d(block_in,\n\t        #                                 2*z_channels if double_z else z_channels,\n\t        #                                 kernel_size=3,\n\t        #                                 stride=1,\n\t        #                                 padding=1)\n\t        self.key = key\n\t    def forward(self, x):\n", "        if self.key is not None:\n\t            x = x[self.key]\n\t        # assert x.shape[2] == x.shape[3] == self.resolution, \"{}, {}, {}\".format(x.shape[2], x.shape[3], self.resolution)\n\t        # timestep embedding\n\t        temb = None\n\t        # # downsampling\n\t        # hs = [self.conv_in(x)]\n\t        # for i_level in range(self.num_resolutions):\n\t        #     for i_block in range(self.num_res_blocks):\n\t        #         h = self.down[i_level].block[i_block](hs[-1], temb)\n", "        #         if len(self.down[i_level].attn) > 0:\n\t        #             h = self.down[i_level].attn[i_block](h)\n\t        #         hs.append(h)\n\t        #     if i_level != self.num_resolutions-1:\n\t        #         hs.append(self.down[i_level].downsample(hs[-1]))\n\t        # # middle\n\t        # h = hs[-1]\n\t        # downsampling\n\t        h = self.conv_in(x)\n\t        for i_level in range(self.num_resolutions):\n", "            for i_block in range(self.num_res_blocks):\n\t                h = self.down[i_level].block[i_block](h, temb)\n\t                if len(self.down[i_level].attn) > 0:\n\t                    h = self.down[i_level].attn[i_block](h)\n\t            if i_level != self.num_resolutions-1:\n\t                h = self.down[i_level].downsample(h)\n\t        # middle\n\t        # h = self.mid.block_1(h, temb)\n\t        # if self.use_attn:\n\t        #     h = self.mid.attn_1(h)\n", "        # h = self.mid.block_2(h, temb)\n\t        # end\n\t        # h = self.norm_out(h)\n\t        # h = nonlinearity(h)\n\t        # h = self.conv_out(h)\n\t        # output: b x channel x 1 x 1 \n\t        h = h.squeeze()\n\t        return h\n\t@models.register('light-img-decoder')\n\tclass LightImgDecoder(nn.Module):\n", "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n\t                 attn_resolutions, dropout=0.0, resamp_with_conv=True, # in_channels,\n\t                 resolution, z_channels, give_pre_end=False, **ignorekwargs):\n\t        super().__init__()\n\t        self.ch = ch\n\t        self.temb_ch = 0\n\t        self.num_resolutions = len(ch_mult)\n\t        self.num_res_blocks = num_res_blocks\n\t        self.resolution = resolution\n\t        # self.in_channels = in_channels\n", "        self.give_pre_end = give_pre_end\n\t        # compute in_ch_mult, block_in and curr_res at lowest res\n\t        in_ch_mult = (1,)+tuple(ch_mult)\n\t        block_in = ch*ch_mult[self.num_resolutions-1]\n\t        curr_res = resolution // 2**(self.num_resolutions-1)\n\t        self.z_shape = (1,z_channels,curr_res,curr_res)\n\t        print(\"Working with z of shape {} = {} dimensions.\".format(\n\t            self.z_shape, np.prod(self.z_shape)))\n\t        # z to block_in\n\t        self.conv_in = torch.nn.Conv2d(z_channels,\n", "                                       block_in,\n\t                                       kernel_size=3,\n\t                                       stride=1,\n\t                                       padding=1)\n\t        # middle\n\t        # self.mid = nn.Module()\n\t        # self.mid.block_1 = ResnetBlock(in_channels=block_in,\n\t        #                                out_channels=block_in,\n\t        #                                temb_channels=self.temb_ch,\n\t        #                                dropout=dropout)\n", "        # self.mid.attn_1 = AttnBlock(block_in)\n\t        # self.mid.block_2 = ResnetBlock(in_channels=block_in,\n\t        #                                out_channels=block_in,\n\t        #                                temb_channels=self.temb_ch,\n\t        #                                dropout=dropout)\n\t        # upsampling\n\t        self.up = nn.ModuleList()\n\t        for i_level in reversed(range(self.num_resolutions)):\n\t            block = nn.ModuleList()\n\t            attn = nn.ModuleList()\n", "            block_out = ch*ch_mult[i_level]\n\t            for i_block in range(self.num_res_blocks+1):\n\t                block.append(ResnetBlock(in_channels=block_in,\n\t                                         out_channels=block_out,\n\t                                         temb_channels=self.temb_ch,\n\t                                         dropout=dropout))\n\t                block_in = block_out\n\t                if curr_res in attn_resolutions:\n\t                    attn.append(AttnBlock(block_in))\n\t            up = nn.Module()\n", "            up.block = block\n\t            up.attn = attn\n\t            if i_level != 0:\n\t                up.upsample = Upsample(block_in, resamp_with_conv)\n\t                curr_res = curr_res * 2\n\t            self.up.insert(0, up) # prepend to get consistent order\n\t        # end\n\t        self.norm_out = Normalize(block_in)\n\t        self.conv_out = torch.nn.Conv2d(block_in,\n\t                                        out_ch,\n", "                                        kernel_size=3,\n\t                                        stride=1,\n\t                                        padding=1)\n\t    def forward(self, z):\n\t        #assert z.shape[1:] == self.z_shape[1:]\n\t        self.last_z_shape = z.shape\n\t        # timestep embedding\n\t        temb = None\n\t        # z to block_in\n\t        h = self.conv_in(z)\n", "        # middle\n\t        # h = self.mid.block_1(h, temb)\n\t        # h = self.mid.attn_1(h)\n\t        # h = self.mid.block_2(h, temb)\n\t        # upsampling\n\t        for i_level in reversed(range(self.num_resolutions)):\n\t            for i_block in range(self.num_res_blocks+1):\n\t                h = self.up[i_level].block[i_block](h, temb)\n\t                if len(self.up[i_level].attn) > 0:\n\t                    h = self.up[i_level].attn[i_block](h)\n", "            if i_level != 0:\n\t                h = self.up[i_level].upsample(h)\n\t        # end\n\t        if self.give_pre_end:\n\t            return h\n\t        h = self.norm_out(h)\n\t        h = nonlinearity(h)\n\t        h = self.conv_out(h)\n\t        return h"]}
{"filename": "models/base.py", "chunked_list": ["import os\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport models\n\tfrom einops import rearrange, reduce, repeat, parse_shape\n\timport pydiffvg\n\tfrom .gutils import render_bezier_path, path_d_from_control_points, render_sdf\n\tfrom .mlp import MLP\n", "from objprint import op\n\timport svgwrite\n\tfrom PIL import Image\n\tfrom abc import abstractmethod\n\tclass SVGRender:\n\t    def __init__(self):\n\t        self._colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n\t    @abstractmethod\n\t    def render_paths_sdf(self, curves, xy, use_occ, **kwargs):\n\t        pass\n", "    @abstractmethod\n\t    def render_paths_diffvg(self, curves, sidelength, **kwargs):\n\t        pass\n\t    @abstractmethod\n\t    def write_paths_to_svg(self, path_tensor, filename):\n\t        pass\n\tclass DualChannel(SVGRender):\n\t    def __init__(self):\n\t        super().__init__()\n\t    def render_paths_sdf(self, curves, xy, use_occ=True, **kwargs):\n", "        # dis: b p nxy\n\t        dis, inner = self.dis_cal(curves, xy, return_mask=True)\n\t        b, p, nxy = dis.shape\n\t        pos_prim = torch.zeros((b, p, nxy), dtype=torch.bool, device=dis.device)\n\t        pos_prim[:, :p//2, :] = True\n\t        point_in_pos_prim = torch.logical_and(pos_prim, inner)\n\t        point_out_neg_prim = torch.logical_and(torch.logical_not(pos_prim), torch.logical_not(inner))\n\t        clip_dis = dis.clone()\n\t        clip_dis[point_in_pos_prim] = 0\n\t        clip_dis[point_out_neg_prim] = 0\n", "        clip_dis = rearrange(clip_dis, 'b (ch hp) nxy -> b ch hp nxy', ch=2)\n\t        clip_dis = reduce(clip_dis, 'b ch hp nxy -> b hp nxy', reduction='max')\n\t        clip_dis = reduce(clip_dis, 'b hp nxy -> b nxy', reduction='min')\n\t        out = {\n\t            'dis': clip_dis,\n\t            'curves': curves,\n\t        }\n\t        if use_occ:\n\t            occ_dis = dis.clone()\n\t            occ_dis[point_in_pos_prim] *= -1\n", "            occ_dis[point_out_neg_prim] *= -1\n\t            occ_dis = rearrange(occ_dis, 'b (ch hp) nxy -> b ch hp nxy', ch=2) # channel: pos/neg\n\t            alias_warmup = kwargs['kernel']\n\t            occ = F.sigmoid(occ_dis) * 2 - 1 \n\t            # kernel size: 4\n\t            occ = render_sdf(occ, max(alias_warmup, 4/(self.sidelength)))\n\t            occ = reduce(occ, 'b ch hp nxy -> b hp nxy', reduction='max')\n\t            occ = reduce(occ, 'b hp nxy -> b nxy', reduction='min')\n\t            out.update({\n\t                'occ': occ,\n", "            })\n\t        return out\n\t    def render_paths_diffvg(self, curves, sidelength=256):\n\t        out = {\n\t            'curves': curves,\n\t        }\n\t        render = pydiffvg.RenderFunction.apply\n\t        b, p, c, pts = curves.shape\n\t        h, w = sidelength, sidelength\n\t        background = torch.ones([h, w, 4]).to(curves.device)\n", "        # curves: b p c pts\n\t        num_control_points = torch.tensor([pts / 2 - 2] * c)\n\t        all_points = rearrange(curves, 'b p c (n d) -> b p c n d', d=2)[:, :, :, :-1, :]\n\t        all_points = rearrange(all_points, 'b p c n d -> b p (c n) d')\n\t        assert(h == w)\n\t        all_points = (all_points + 1) / 2 * h\n\t        imgs_from_diffvg = []\n\t        for i in range(b):\n\t            parts = []\n\t            for j in range(p//2):\n", "                shapes = []\n\t                shape_groups = []\n\t                path_pos = pydiffvg.Path(num_control_points=num_control_points, points=all_points[i, j], is_closed=True)\n\t                shapes.append(path_pos)\n\t                path_group = pydiffvg.ShapeGroup(\n\t                    shape_ids=torch.tensor([len(shapes) - 1]),\n\t                    fill_color=torch.tensor([0.0, 0.0, 0.0, 1.0]))\n\t                shape_groups.append(path_group)\n\t                path_neg = pydiffvg.Path(num_control_points=num_control_points, points=all_points[i, j + p//2], is_closed=True)\n\t                shapes.append(path_neg)\n", "                path_group = pydiffvg.ShapeGroup(\n\t                    shape_ids=torch.tensor([len(shapes) - 1]),\n\t                    fill_color=torch.tensor([1.0, 1.0, 1.0, 1.0]))\n\t                shape_groups.append(path_group)\n\t                scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n\t                    w, h, shapes, shape_groups)\n\t                rendered_img = render(w, # width\n\t                            h, # height\n\t                            3,   # num_samples_x\n\t                            3,   # num_samples_y\n", "                            42,   # seed\n\t                            background,\n\t                            *scene_args)\n\t                parts.append(rendered_img[:, :, 0].transpose(0, 1))\n\t            parts = rearrange(parts, 'hp h w -> hp h w')\n\t            img_from_diffvg = reduce(parts, 'hp h w -> h w', 'min')\n\t            imgs_from_diffvg.append(img_from_diffvg)\n\t        imgs_from_diffvg = rearrange(imgs_from_diffvg, 'b h w -> b () h w')\n\t        out.update({\n\t            'rendered': imgs_from_diffvg,\n", "        })\n\t        return out\n\t    def write_paths_to_svg(self, path_tensor, filename, xy_flip=True):\n\t        if isinstance(path_tensor, torch.Tensor):\n\t            path_tensor = path_tensor.detach().cpu().numpy()\n\t        n_paths, n_curves, n_cp = path_tensor.shape\n\t        n_cp = n_cp // 2\n\t        n_pos_path = n_paths // 2\n\t        sl = 256\n\t        canvas = svgwrite.Drawing(filename=filename, debug=True)\n", "        canvas.viewbox(0, 0, sl, sl)\n\t        path_tensor = (path_tensor + 1) / 2 * sl\n\t        canvas_rect = canvas.rect(insert=(0, 0), size=(sl, sl), fill='white')\n\t        for i in range(n_pos_path):\n\t            path_d = path_d_from_control_points(path_tensor[i], xy_flip=xy_flip)\n\t            mask_d = path_d_from_control_points(path_tensor[i + n_pos_path], xy_flip=xy_flip)\n\t            mask = canvas.mask(id=f'mask{i}')\n\t            mask.add(canvas_rect)\n\t            mask.add(canvas.path(d=mask_d, fill='black'))\n\t            canvas.defs.add(mask)\n", "            path = canvas.path(   \n\t                d=path_d, \n\t                fill=self._colors[i%len(self._colors)],\n\t                fill_opacity='0.3',\n\t                mask=f'url(#mask{i})',\n\t            )\n\t            canvas.add(path)\n\t        canvas.save()\n\t        return canvas\n\tclass SingleChannel(SVGRender):\n", "    def __init__(self):\n\t        super().__init__()\n\t    def render_paths_sdf(self, curves, xy, use_occ=True, **kwargs):\n\t        # dis: b p nxy\n\t        dis, inner = self.dis_cal(curves, xy, return_mask=True)\n\t        dis[inner] *= -1\n\t        dis = reduce(dis, 'b p nxy -> b nxy', reduction='min')\n\t        out = {\n\t            'dis': dis,\n\t            'curves': curves,\n", "        }\n\t        if use_occ:\n\t            alias_warmup = kwargs['kernel']\n\t            occ = dis.clone()\n\t            occ = F.sigmoid(occ) * 2 - 1 \n\t            # kernel size: 4\n\t            occ = render_sdf(occ, max(alias_warmup, 4/(self.sidelength)))\n\t            out.update({\n\t                'occ': occ,\n\t            })\n", "            return out\n\t    def render_paths_diffvg(self, curves, sidelength=256):\n\t        render = pydiffvg.RenderFunction.apply\n\t        b, p, c, pts = curves.shape\n\t        h, w = sidelength, sidelength\n\t        background = torch.ones([h, w, 4]).to(curves.device)\n\t        # curves: b p c pts\n\t        num_control_points = torch.tensor([pts / 2 - 2] * c)\n\t        all_points = rearrange(curves, 'b p c (n d) -> b p c n d', d=2)[:, :, :, :-1, :]\n\t        all_points = rearrange(all_points, 'b p c n d -> b p (c n) d')\n", "        assert(h == w)\n\t        all_points = (all_points + 1) / 2 * h\n\t        imgs_from_diffvg = []\n\t        for i in range(b):\n\t            shapes = []\n\t            shape_groups = []\n\t            for j in range(p):\n\t                points = all_points[i, j]\n\t                path = pydiffvg.Path(num_control_points=num_control_points, points=points, is_closed=True)\n\t                shapes.append(path)\n", "                path_group = pydiffvg.ShapeGroup(\n\t                    shape_ids=torch.tensor([len(shapes) - 1]),\n\t                    fill_color=torch.tensor([0.0, 0.0, 0.0, 1.0]))\n\t                shape_groups.append(path_group)\n\t            scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n\t                w, h, shapes, shape_groups)\n\t            rendered_img = render(w, # width\n\t                        h, # height\n\t                        2,   # num_samples_x\n\t                        2,   # num_samples_y\n", "                        42,   # seed\n\t                        background,\n\t                        *scene_args)\n\t            imgs_from_diffvg.append(rendered_img[:, :, 0].transpose(0, 1))\n\t        imgs_from_diffvg = rearrange(imgs_from_diffvg, 'b h w -> b () h w')\n\t        return {\n\t            'rendered': imgs_from_diffvg,\n\t        }\n\t    def write_paths_to_svg(self, path_tensor, filename):\n\t        if isinstance(path_tensor, torch.Tensor):\n", "            path_tensor = path_tensor.detach().cpu().numpy()\n\t        n_paths, n_curves, n_cp = path_tensor.shape\n\t        n_cp = n_cp // 2\n\t        sl = 256\n\t        canvas = svgwrite.Drawing(filename=filename, debug=True)\n\t        canvas.viewbox(0, 0, sl, sl)\n\t        path_tensor = (path_tensor + 1) / 2 * sl\n\t        for i in range(n_paths):\n\t            path_d = path_d_from_control_points(path_tensor[i])\n\t            path = canvas.path(   \n", "                d=path_d, \n\t                fill=self._colors[i%len(self._colors)],\n\t                fill_opacity='0.3',\n\t            )\n\t            canvas.add(path)\n\t        canvas.save()\n\t        return canvas\n"]}
{"filename": "models/mlp.py", "chunked_list": ["import torch.nn as nn\n\timport models\n\t@models.register('mlp')\n\tclass MLP(nn.Module):\n\t    def __init__(self, layers, bias=True, activate='leaky_relu', slope = 0.01, activate_last=False, **kwargs):\n\t        super().__init__()\n\t        self.layers = layers\n\t        assert len(layers) > 1\n\t        assert activate in ['leaky_relu', 'relu', 'sigmoid', 'tanh', 'gelu']\n\t        self.in_dim = layers[0]\n", "        self.out_dim = layers[-1]\n\t        model = []\n\t        for i in range(len(layers) - 1):\n\t            model.append(nn.Linear(layers[i], layers[i + 1], bias=bias))\n\t            if not activate_last and i < len(layers) - 2:\n\t                if activate == 'leaky_relu':\n\t                    model.append(nn.LeakyReLU(negative_slope=slope, inplace=True))\n\t                elif activate == 'relu':\n\t                    model.append(nn.ReLU(inplace=True))\n\t                elif activate == 'sigmoid':\n", "                    model.append(nn.Sigmoid())\n\t                elif activate == 'tanh':\n\t                    model.append(nn.Tanh())\n\t                elif activate == 'gelu':\n\t                    model.append(nn.GELU())\n\t                else:\n\t                    raise NotImplementedError('not implemented activation')\n\t        self.model = nn.Sequential(*model)\n\t        def init_weights(m):\n\t            if type(m) == nn.Linear:\n", "                nn.init.xavier_uniform_(m.weight)\n\t                if m.bias is not None:\n\t                    nn.init.constant_(m.bias, 0)\n\t        self.model.apply(init_weights)\n\t    def forward(self, x):\n\t        shape = x.shape[:-1]\n\t        x = self.model(x.view(-1, x.shape[-1]))\n\t        return x.view(*shape, -1)\n"]}
{"filename": "models/transformer.py", "chunked_list": ["from re import I\n\timport torch\n\tfrom torch import nn\n\tfrom einops import rearrange, repeat\n\tfrom einops.layers.torch import Rearrange\n\timport models\n\t# helpers\n\tdef pair(t):\n\t    return t if isinstance(t, tuple) else (t, t)\n\tclass PreNorm(nn.Module):\n", "    def __init__(self, dim, fn, norm=\"layer\"):\n\t        super().__init__()\n\t        if norm == \"layer\":\n\t            self.norm = nn.LayerNorm(dim)\n\t        elif norm == \"batch\":\n\t            self.norm = nn.BatchNorm1d(dim)\n\t        elif norm == \"none\":\n\t            self.norm = nn.Identity()\n\t        else:\n\t            raise NotImplementedError(\"unsupported norm\", norm)\n", "        self.fn = fn\n\t        self.norm_name = norm\n\t    def forward(self, x, **kwargs):\n\t        if self.norm_name == \"batch\":\n\t            return self.fn(self.norm(x.transpose(1,2)).transpose(1,2), **kwargs)\n\t        else:\n\t            return self.fn(self.norm(x), **kwargs)\n\tclass ResConn(nn.Module):\n\t    def __init__(self, fn):\n\t        super().__init__()\n", "        self.fn = fn\n\t    def forward(self, x, **kwargs):\n\t        return x + self.fn(x, **kwargs)\n\tclass PostNorm(nn.Module):\n\t    def __init__(self, dim, fn, norm=\"layer\"):\n\t        super().__init__()\n\t        if norm == \"layer\":\n\t            self.norm = nn.LayerNorm(dim)\n\t        elif norm == \"batch\":\n\t            self.norm = nn.BatchNorm1d(dim)\n", "        elif norm == \"none\":\n\t            self.norm = nn.Identity()\n\t        else:\n\t            raise NotImplementedError(\"unsupported norm\", norm)\n\t        self.fn = fn\n\t        self.norm_name = norm\n\t    def forward(self, x, **kwargs):\n\t        if self.norm_name == \"batch\":\n\t            return self.norm(self.fn(x, **kwargs).transpose(1,2)).transpose(1,2)\n\t        else:\n", "            return self.norm(self.fn(x, **kwargs))\n\tclass FeedForward(nn.Module):\n\t    def __init__(self, dim, hidden_dim, dropout = 0.):\n\t        super().__init__()\n\t        self.net = nn.Sequential(\n\t            nn.Linear(dim, hidden_dim),\n\t            nn.GELU(),\n\t            nn.Dropout(dropout),\n\t            nn.Linear(hidden_dim, dim),\n\t            nn.Dropout(dropout)\n", "        )\n\t    def forward(self, x):\n\t        return self.net(x)\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n\t        super().__init__()\n\t        inner_dim = dim_head *  heads\n\t        project_out = not (heads == 1 and dim_head == dim)\n\t        self.heads = heads\n\t        self.scale = dim_head ** -0.5\n", "        self.attend = nn.Softmax(dim = -1)\n\t        self.dropout = nn.Dropout(dropout)\n\t        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n\t        self.to_out = nn.Sequential(\n\t            nn.Linear(inner_dim, dim),\n\t            nn.Dropout(dropout)\n\t        ) if project_out else nn.Identity()\n\t    def forward(self, x, mask=None):\n\t        # mask: b x n\n\t        qkv = self.to_qkv(x).chunk(3, dim = -1)\n", "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n\t        if mask is None:\n\t            # dots: b x h x n x n\n\t            dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\t        else:\n\t            b, n = mask.shape\n\t            h = q.shape[1]\n\t            attn_mask = mask.view(b, 1, 1, n).expand(-1, h, n, -1)\n\t            new_attn_mask = torch.zeros_like(attn_mask, dtype=q.dtype)\n\t            new_attn_mask.masked_fill_(attn_mask, float(\"-inf\"))\n", "            attn_mask = new_attn_mask\n\t            dots = attn_mask + torch.matmul(q, k.transpose(-1, -2)) * self.scale\n\t        attn = self.attend(dots)\n\t        attn = self.dropout(attn)\n\t        out = torch.matmul(attn, v)\n\t        out = rearrange(out, 'b h n d -> b n (h d)')\n\t        return self.to_out(out)\n\t@models.register('tfm')\n\tclass Transformer(nn.Module):\n\t    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0., norm=\"layer\"):\n", "        super().__init__()\n\t        self.layers = nn.ModuleList([])\n\t        for _ in range(depth):\n\t            self.layers.append(nn.ModuleList([\n\t                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout), norm=norm),\n\t                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout), norm=norm)\n\t            ]))\n\t    def forward(self, x, mask=None):\n\t        for attn, ff in self.layers:\n\t            x = attn(x, mask=mask) + x\n", "            x = ff(x) + x\n\t        return x\n"]}
{"filename": "models/__init__.py", "chunked_list": ["from .tools import register, make, make_lr_scheduler, freeze, load_submodule\n\tfrom . import bezier_sdf\n\tfrom . import base\n\tfrom . import decoder\n\tfrom . import gutils\n\tfrom . import recon_system\n\tfrom . import transformer\n\tfrom . import gen_system\n"]}
{"filename": "models/tools.py", "chunked_list": ["import os\n\timport copy\n\timport torch\n\tmodels = {}\n\tdef register(name):\n\t    def decorator(cls):\n\t        models[name] = cls\n\t        return cls\n\t    return decorator\n\tdef freeze(model):\n", "    for param in model.parameters():\n\t        param.requires_grad = False\n\tdef make(model_spec, args=None, load_sd=False):\n\t    if args is not None:\n\t        model_args = copy.deepcopy(model_spec['args'])\n\t        model_args.update(args)\n\t    else:\n\t        model_args = model_spec['args']\n\t    model = models[model_spec['name']](**model_args)\n\t    if load_sd:\n", "        model.load_state_dict(model_spec['sd'], strict=False)\n\t    return model\n\tdef make_lr_scheduler(optim, scheduler_spec):\n\t    if scheduler_spec is None:\n\t        return None\n\t    auto_create = [\n\t        'CosineAnnealingLR',\n\t        'ExponentialLR',\n\t        'CosineAnnealingWarmRestarts',\n\t        # 'LinearLR',\n", "        'MultiStepLR',\n\t        'ReduceLROnPlateau',\n\t        'StepLR',\n\t    ]\n\t    name = scheduler_spec['name']\n\t    if name in auto_create:\n\t        LRCLASS = getattr(torch.optim.lr_scheduler, name)\n\t        lr_scheduler = LRCLASS(optim, **scheduler_spec['args'])\n\t        return lr_scheduler\n\t    elif name == 'LambdaLR':\n", "        args = copy.deepcopy(scheduler_spec['args'])\n\t        lr_lambda = make_lambda(args['lr_lambda'])\n\t        args['lr_lambda'] = lr_lambda\n\t        lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optim, **args)\n\t        return lr_scheduler\n\t    else:\n\t        raise NotImplementedError\n\tdef warmup_lr(start, end=1.0, total_iters=5, exp_gamma=1):\n\t    def _warmup(epoch):\n\t        if epoch + 1 >= total_iters:\n", "            return end * exp_gamma ** (epoch + 1 - total_iters)\n\t        else:\n\t            return epoch / (total_iters - 1) * (end - start) + start \n\t    return _warmup\n\tdef make_lambda(lr_lambda):\n\t    FUNC = {\n\t        'warmup_lr': warmup_lr,\n\t    }[lr_lambda['name']]\n\t    return FUNC(**lr_lambda['args'])\n\tdef load_submodule(module_instance, ckpt, key):\n", "    submodule_state_dict = {k[len(key) + 1:]: v for k, v in ckpt['model']['sd'].items() if k.startswith(key + '.')}\n\t    module_instance.load_state_dict(submodule_state_dict, strict=True)\n"]}
{"filename": "models/gutils.py", "chunked_list": ["import numpy as np\n\timport cairosvg\n\timport torch\n\timport math\n\tdef surface_to_npim(surface):\n\t    \"\"\" Transforms a Cairo surface into a numpy array. \"\"\"\n\t    im = +np.frombuffer(surface.get_data(), np.uint8)\n\t    H,W = surface.get_height(), surface.get_width()\n\t    im.shape = (H,W,4) # for RGBA\n\t    return im\n", "def svg_to_npim(svg_bytestring, w, h):\n\t    \"\"\" Renders a svg bytestring as a RGB image in a numpy array \"\"\"\n\t    tree = cairosvg.parser.Tree(bytestring=svg_bytestring)\n\t    surf = cairosvg.surface.PNGSurface(tree,None,50,output_width=w, output_height=h).cairo\n\t    return surface_to_npim(surf)\n\tdef render_bezier_path(curves, sidelength):\n\t    '''\n\t    curves: c (pts*2)\n\t    '''\n\t    curves = (curves + 1) * sidelength / 2\n", "    d = path_d_from_control_points(curves, xy_flip=False)\n\t    svg_string = f'''<?xml version=\"1.0\" ?>\n\t<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"200\" version=\"1.1\" viewBox=\"0 0 {sidelength} {sidelength}\" width=\"200\">\n\t    <defs/>\n\t    <path d=\"{d}\" fill=\"black\" stroke=\"none\"/>\n\t</svg>'''\n\t    im = svg_to_npim(svg_string.encode('utf-8'), sidelength, sidelength)\n\t    return im[:, :, 3]\n\tdef render_multi_bezier_paths(paths, sidelength):\n\t    '''\n", "    paths: a list: [c (pts*2)]\n\t    '''\n\t    d = []\n\t    for curve in paths:\n\t        curve = (curve + 1) * sidelength / 2\n\t        dstr = path_d_from_control_points(curve, xy_flip=False)\n\t        d.append(f'''<path d=\"{dstr}\" fill=\"black\" stroke=\"none\"/>''')\n\t    svg_string = f'''<?xml version=\"1.0\" ?>\n\t<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"200\" version=\"1.1\" viewBox=\"0 0 {sidelength} {sidelength}\" width=\"200\">\n\t    <defs/>''' +  '\\n'.join(d) + '</svg>'\n", "    im = svg_to_npim(svg_string.encode('utf-8'), sidelength, sidelength)\n\t    return im[:, :, 3]\n\tdef render_bezier_paths(paths, sidelength):\n\t    '''\n\t    render in the same path tag\n\t    paths: a list:  [c (pts*2)]\n\t    '''\n\t    d = []\n\t    for curve in paths:\n\t        curve = (curve + 1) * sidelength / 2\n", "        d.append(path_d_from_control_points(curve, xy_flip=False))\n\t    d = ' '.join(d)\n\t    svg_string = f'''<?xml version=\"1.0\" ?>\n\t<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" baseProfile=\"full\" height=\"200\" version=\"1.1\" viewBox=\"0 0 {sidelength} {sidelength}\" width=\"200\">\n\t    <defs/>\n\t    <path d=\"{d}\" fill=\"black\" stroke=\"none\"/>\n\t</svg>'''\n\t    im = svg_to_npim(svg_string.encode('utf-8'), sidelength, sidelength)\n\t    return im[:, :, 3]\n\tdef path_d_from_control_points(cp, xy_flip=True):\n", "    if isinstance(cp, torch.Tensor):\n\t        cp = cp.detach().cpu().numpy()\n\t    # cp: n_curves (cps 2)\n\t    n, n_cp = cp.shape\n\t    n_cp = n_cp // 2\n\t    assert(n_cp == 3 or n_cp == 4)\n\t    cc = 'C' if n_cp == 4 else 'Q'\n\t    d = []\n\t    for i in range(n):\n\t        if i == 0:\n", "            d += ['M']\n\t            if xy_flip:\n\t                d += list(map(str, [cp[i, 1], cp[i, 0]]))\n\t            else:\n\t                d += list(map(str, [cp[i, 0], cp[i, 1]]))\n\t        d += [cc]\n\t        if n_cp == 4:\n\t            if xy_flip:\n\t                d += list(map(str, [cp[i, 3], cp[i, 2], cp[i, 5], cp[i, 4], cp[i, 7], cp[i, 6]]))\n\t            else:\n", "                d += list(map(str, cp[i, 2:8]))\n\t        else:\n\t            if xy_flip:\n\t                d += list(map(str, [cp[i, 3], cp[i, 2], cp[i, 5], cp[i, 4]]))\n\t            else:\n\t                d += list(map(str, cp[i, 2:6]))\n\t    d += ['Z']\n\t    d_str = ' '.join(d)\n\t    return d_str\n\tdef antialias_kernel(r):\n", "    r = -r\n\t    output = (0.5 + 0.25 * (torch.pow(r, 3) - 3 * r))\n\t    #   output = -0.5*r + 0.5\n\t    return output\n\tdef render_sdf(sdf, resolution):\n\t    normalization = resolution  # 0.70710678*2/resolution\n\t    normalized_sdf = sdf / normalization\n\t    clamped_sdf = torch.clamp(normalized_sdf, min=-1, max=1)\n\t    opacity = antialias_kernel(clamped_sdf)  # multiply by color here\n\t    return opacity\n", "def gradient(y, x, create_graph=True, allow_unused=False):\n\t    return torch.autograd.grad(y, [x], create_graph=create_graph, grad_outputs=torch.ones_like(y), allow_unused=allow_unused)[0]\n\tclass SolveCubicNumpy(torch.autograd.Function):\n\t    @staticmethod\n\t    def forward(ctx, inp):\n\t        ctx.set_materialize_grads(False)\n\t        assert(inp.shape[1] == 4)\n\t        inp_np = inp.cpu().numpy()\n\t        n = inp_np.shape[0]\n\t        roots = np.zeros([n, 3], dtype=inp_np.dtype)\n", "        def find_root(i):\n\t            r = np.roots(inp_np[i])\n\t            # r.real[np.abs(r.imag)<1e-5] \n\t            r = r.real[np.isreal(r)]\n\t            roots[i, :] = r\n\t        for i in range(n):\n\t            find_root(i)\n\t        roots = torch.from_numpy(roots).type_as(inp)\n\t        ctx.save_for_backward(inp, roots)\n\t        return roots\n", "    @staticmethod\n\t    def backward(ctx, grad_output):\n\t        if grad_output is None:\n\t            return None\n\t        inp, roots = ctx.saved_tensors\n\t        a = inp[:, 0:1]\n\t        b = inp[:, 1:2]\n\t        c = inp[:, 2:3]\n\t        grad_inp = torch.zeros_like(inp)\n\t        for i in range(3):\n", "            t = roots[:, i:(i+1)]\n\t            dp_dt = t * (3 * a * t + 2 * b) + c\n\t            dp_dd = torch.ones_like(dp_dt)\n\t            dp_dc = t\n\t            dp_db = dp_dc * t\n\t            dp_da = dp_db * t\n\t            dp_dC = torch.cat([dp_da, dp_db, dp_dc, dp_dd], dim=-1)\n\t            dt_dC = - torch.sign(dp_dt) / torch.clamp(torch.abs(dp_dt), min=1e-5) * dp_dC \n\t            grad_inp += dt_dC * grad_output[:, i:(i + 1)]\n\t        return grad_inp\n", "def rdot(a, b): # row-wise dot\n\t    return torch.sum(a * b, dim=-1).unsqueeze(-1)\n\tdef vec_norm(a):\n\t    # in pytorch 1.6.0\n\t    return a.norm(dim=-1, keepdim=True)\n\tdef sd_bezier(A, B, C, p):\n\t    # def rdot(a, b): # row-wise dot\n\t    #     return torch.sum(a * b, dim=-1).unsqueeze(-1)\n\t    # s = abs(torch.sign(B * 2.0 - A - C))\n\t    # B = (B + 1e-4) * (1 - s) + s * B\n", "    # a = B - A\n\t    # b = A - B * 2.0 + C\n\t    # c = a * 2.0\n\t    # d = A - p\n\t    # k = torch.cat([3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b),rdot(d,a)], dim=-1) / torch.clamp(rdot(b,b), min=1e-4)     \n\t    # t = torch.clamp(solve_cubic(k), 0.0, 1.0)\n\t    # t = t.unsqueeze(1)\n\t    # # n x 2 x 3\n\t    # vec = A.unsqueeze(-1) + (c.unsqueeze(-1) + b.unsqueeze(-1) * t) * t - p.unsqueeze(-1)\n\t    # dis = torch.min(torch.linalg.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n", "    # return dis\n\t    return _sd_bezier(A, B, C, p)\n\tdef _sd_bezier(A, B, C, p):\n\t    def rdot(a, b): # row-wise dot\n\t        return torch.sum(a * b, dim=-1).unsqueeze(-1)\n\t    s = abs(torch.sign(B * 2.0 - A - C))\n\t    B = (B + 1e-4) * (1 - s) + s * B\n\t    a = B - A\n\t    b = A - B * 2.0 + C\n\t    c = a * 2.0\n", "    d = A - p\n\t    bdb = torch.clamp(rdot(b, b), min=1e-4)\n\t    # k = torch.cat([3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b),rdot(d,a)], dim=-1) / torch.clamp(rdot(b,b), min=1e-4)     \n\t    t = torch.clamp(solve_cubic(3.*rdot(a, b) / bdb, (2.*rdot(a,a)+rdot(d,b))/bdb, rdot(d,a)/bdb), 0.0, 1.0)\n\t    t = t.unsqueeze(1)\n\t    # n x 2 x 3\n\t    vec = A.unsqueeze(-1) + (c.unsqueeze(-1) + b.unsqueeze(-1) * t) * t - p.unsqueeze(-1)\n\t    if hasattr(torch, 'linalg'):\n\t        dis = torch.min(torch.linalg.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n\t    else:\n", "        dis = torch.min(torch.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n\t    return dis\n\tdef _sd_bezier_np(A, B, C, p):\n\t    solve_cubic_np = SolveCubicNumpy.apply\n\t    s = abs(torch.sign(B * 2.0 - A - C))\n\t    B = (B + 1e-4) * (1 - s) + s * B\n\t    a = B - A\n\t    b = A - B * 2.0 + C\n\t    c = a * 2.0\n\t    d = A - p\n", "    # k = torch.cat([3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b),rdot(d,a)], dim=-1) / torch.clamp(rdot(b,b), min=1e-4)     \n\t    k = torch.cat([rdot(b,b), 3.*rdot(a,b), 2.*rdot(a,a)+rdot(d,b), rdot(d,a)], dim=-1)\n\t    t = torch.clamp(solve_cubic_np(k), 0.0, 1.0)\n\t    t = t.unsqueeze(1)\n\t    # n x 2 x 3\n\t    vec = A.unsqueeze(-1) + (c.unsqueeze(-1) + b.unsqueeze(-1) * t) * t - p.unsqueeze(-1)\n\t    dis = torch.min(torch.linalg.norm(vec, dim=1), dim=-1, keepdim=True)[0]\n\t    return dis\n\t# copy from https://members.loria.fr/SHornus/quadratic-arc-length.html\n\tdef bezier_length(a, b, c):\n", "    '''\n\t    a, b, c: n x 2\n\t    '''\n\t    A = a + c - 2 * b\n\t    B = b - a\n\t    C = a\n\t    F = A + B\n\t    A_norm = vec_norm(A)\n\t    B_norm = vec_norm(B)\n\t    F_norm = vec_norm(F)\n", "    AB_dot = rdot(A, B)\n\t    AF_dot = rdot(A, F)\n\t    A_norm_clamp = torch.clamp(A_norm, min=1e-8)\n\t    l = (F_norm * AF_dot - B_norm * AB_dot) / A_norm_clamp.pow(2) + \\\n\t        (A_norm.pow(2) * B_norm.pow(2) - AB_dot.pow(2)) / A_norm_clamp.pow(3) * \\\n\t        (torch.log(torch.clamp(A_norm * F_norm + AF_dot, min=1e-8)) - torch.log(torch.clamp(A_norm * B_norm + AB_dot, min=1e-8)))\n\t    return l\n\t# copied from https://www.shadertoy.com/view/ltXSDB by Inigo Quilez\n\tdef solve_cubic(a, b, c):\n\t    '''\n", "    abc: n x 3\n\t    '''\n\t    # a = abc[:, 0:1]\n\t    # b = abc[:, 1:2]\n\t    # c = abc[:, 2:3]\n\t    p = b - a*a / 3.\n\t    p3 = torch.pow(p, 3)\n\t    q = a * (2.0*a*a - 9.0*b) / 27.0 + c\n\t    d = q*q + 4.0*p3 / 27.0\n\t    offset = -a / 3.0\n", "    d_mask = (d >= 0)\n\t    z = torch.sqrt(torch.clamp(d[d_mask], min=1e-10)).unsqueeze(-1)\n\t    x = (torch.cat([z, -z], dim=-1) - q[d_mask].unsqueeze(-1)) / 2.0\n\t    uv = torch.sign(x) * torch.pow(torch.clamp(torch.abs(x), min=1e-10), 1.0/3)\n\t    root1 = (offset[d_mask].unsqueeze(1) + uv[:, 0:1] + uv[:, 1:2]).repeat(1, 3)\n\t    to_acos = torch.clamp(-torch.sqrt(-27.0 / torch.clamp(p3[~d_mask], max=-1e-8)) * q[~d_mask] / 2.0, -1 + 1e-4, 1 - 1e-4)\n\t    v = torch.acos(to_acos) / 3.0\n\t    m = torch.cos(v).unsqueeze(-1)\n\t    n = torch.sin(v).unsqueeze(-1) * math.sqrt(3)\n\t    root2 = torch.cat([m + m, -n - m, n - m], dim=-1) \\\n", "            * torch.sqrt(torch.clamp(-p[~d_mask].unsqueeze(-1) / 3.0, min=1e-10)) \\\n\t             + offset[~d_mask].unsqueeze(-1)\n\t    root = torch.zeros((a.shape[0], 3), device=a.device)\n\t    root[d_mask.repeat(1,3)] = root1.flatten()\n\t    root[~d_mask.repeat(1,3)] = root2.flatten()\n\t    return root\n\tdef solve_cubic_order2_zero(a_, b_, c_):\n\t    def cubic_root(p):\n\t        return p.sign() * p.abs().pow(1.0/3)\n\t    # ax^3 + bx + c = 0\n", "    # a is almost zero\n\t    a_zero_mask = a_.abs() < 1e-8\n\t    # a ~= 0\n\t    root = torch.zeros((a_.shape[0], 3), device=a_.device)\n\t    root[a_zero_mask.repeat(1,3)] = (- c_ / b_)[a_zero_mask].repeat(1, 3)\n\t    a = a_\n\t    c = b_\n\t    d = c_\n\t    A = -3*a*c\n\t    B = -9*a*d\n", "    C = c**2\n\t    delta = B**2 - 4*A*C\n\t    # delta > 0\n\t    d_mask = (delta > 0) & ~a_zero_mask\n\t    d_sqrt = torch.sqrt(delta[d_mask])\n\t    Y1 = 3*a[d_mask]*(-B[d_mask] - d_sqrt) / 2\n\t    Y2 = 3*a[d_mask]*(-B[d_mask] + d_sqrt) / 2\n\t    X1 = (-cubic_root(Y1) - cubic_root(Y2)) / (3*a[d_mask])\n\t    root[d_mask.repeat(1,3)] = X1.repeat(1, 3)\n\t    # delta <= 0\n", "    d_mask = (delta <= 0) & ~a_zero_mask\n\t    theta = torch.arccos(-3*a[d_mask]*B[d_mask]/2/torch.pow(A[d_mask], 1.5))\n\t    m = torch.sqrt(A[d_mask]) * torch.cos(theta / 3) / 3 / a[d_mask]\n\t    n = torch.sqrt(A[d_mask]) * torch.sin(theta / 3) / 3 / a[d_mask]\n\t    X1 = -2 * m\n\t    X2 = m + n\n\t    X3 = m - n\n\t    d_mask_s = d_mask.squeeze()\n\t    root[d_mask_s, 0] = X1\n\t    root[d_mask_s, 1] = X2\n", "    root[d_mask_s, 2] = X3\n\t    return root\n\tdef sd_parabola(params, p):\n\t    '''\n\t    params: n x 4, a, theta, x0, y0 \n\t    '''\n\t    a, theta, x0, y0 = params.split(1, dim=-1)\n\t    ct = torch.cos(theta)\n\t    st = torch.sin(theta)\n\t    x, y = p.split(1, dim=-1)\n", "    x_ = x * ct + y * st - x0 \n\t    y_ = -x * st + y * ct - y0 \n\t    sig = torch.sign(a * x_ ** 2 - y_)\n\t    # distance between (x_, y_) and y = a * x ** 2\n\t    # dis'(x) = 2a^2x^3 + (1-2an)x - m\n\t    # dis(x) = sqrt((x - m)^2 + (x^2 - n)^2)\n\t    # t = solve_cubic_order2_zero(2*a**2, 1-2*a*y_, -x_)\n\t    a_zero_mask = (a.abs() < 1e-5).squeeze(dim=-1)\n\t    t = torch.zeros((a.shape[0], 3), device=a.device)\n\t    t1 = (x_[a_zero_mask] / (1 - 2*a[a_zero_mask]*y_[a_zero_mask]))\n", "    t[a_zero_mask] = t1.repeat(1, 3)\n\t    t2 = solve_cubic(torch.zeros_like(a[~a_zero_mask]), (1-2*a[~a_zero_mask]*y_[~a_zero_mask])/2/a[~a_zero_mask]**2, -x_[~a_zero_mask]/2/a[~a_zero_mask]**2)\n\t    t[~a_zero_mask] = t2\n\t    dis = torch.min((t - x_) ** 2 + (a * t**2 - y_) ** 2, dim=-1)[0]\n\t    return sig, torch.sqrt(torch.clamp(dis, min=1e-10)).unsqueeze(-1)\n\tdef eval_parabola(params, p):\n\t    '''\n\t    params: n x 4, a, theta, x0, y0 \n\t    '''\n\t    a, theta, x0, y0 = params.split(1, dim=-1)\n", "    ct = torch.cos(theta)\n\t    st = torch.sin(theta)\n\t    x, y = p.split(1, dim=-1)\n\t    x_ = x * ct + y * st - x0 \n\t    y_ = -x * st + y * ct - y0 \n\t    # sig = torch.sign(a * x_ ** 2 - y_)\n\t    return a * x_ ** 2 - y_\n"]}
{"filename": "models/bezier_sdf.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport models\n\tfrom .mlp import MLP\n\tfrom .gutils import sd_bezier, render_bezier_path\n\tfrom einops import rearrange, reduce, repeat, parse_shape\n\timport svgwrite\n\tfrom .gutils import sd_parabola, eval_parabola\n\t@models.register('analytic-df')\n", "class AnalyticBezierDF(nn.Module):\n\t    def __init__(self, n_control_points):\n\t        super().__init__()\n\t        assert(n_control_points == 3)\n\t        self.n_control_points = n_control_points\n\t    def forward(self, xy, control_points):\n\t        assert(control_points.size(1) == 2 * self.n_control_points)\n\t        sd = sd_bezier(control_points[:, 0:2], control_points[:, 2:4], control_points[:, 4:6], xy)\n\t        return sd\n\t@models.register('batched-curve-to-dis')\n", "class DistanceCalculator(nn.Module):\n\t    def __init__(self, ds, ckpt=None, sidelength=256):\n\t        super().__init__()\n\t        if isinstance(ds, dict):\n\t            self.diff_sdf = models.make(ds)\n\t        else:\n\t            self.diff_sdf = ds\n\t        self.ckpt = ckpt\n\t        self.sidelength = sidelength\n\t    def init(self):\n", "        if self.ckpt is not None:\n\t            ckpt = torch.load(self.ckpt)\n\t            self.diff_sdf.load_state_dict(ckpt['model']['sd'], strict=True)\n\t            models.freeze(self.diff_sdf)\n\t            print('***** freeze diff_sdf *****')\n\t    def forward(self, curves, xy, return_mask=False):\n\t        b, p, c, pts = curves.shape\n\t        _, nxy, _ = xy.shape\n\t        curves_r = repeat(curves, 'b p c pts -> (b nxy p c) pts', nxy=nxy)\n\t        xy_r = repeat(xy, 'b nxy dxy -> (b nxy p c) dxy', p=p, c=c)\n", "        dis = self.diff_sdf(xy_r, curves_r)\n\t        dis = rearrange(dis, '(b nxy p c) () -> b nxy p c', b=b, nxy=nxy, p=p, c=c)\n\t        dis = reduce(dis, 'b nxy p c -> b p nxy', reduction='min')\n\t        if return_mask:\n\t            path_mask = np.zeros((b, p, self.sidelength, self.sidelength), dtype=np.bool) # inside is true\n\t            curves_np = curves.detach().cpu().numpy()\n\t            for i in range(b):\n\t                for j in range(p):\n\t                    path_mask[i, j] = render_bezier_path(curves_np[i, j], self.sidelength) > 128\n\t            path_mask = torch.tensor(path_mask).to(curves.device)\n", "            xy_grid = (xy + 1) / 2 * self.sidelength\n\t            xy_grid = xy_grid.long()\n\t            # xy_inside: b p nxy\n\t            xy_inside = torch.zeros_like(dis, dtype=torch.bool)\n\t            for i in range(b):\n\t                for j in range(p):\n\t                    xy_inside[i, j] = path_mask[i, j][xy_grid[i, :, 1], xy_grid[i, :, 0]]\n\t            return dis, xy_inside\n\t        else:\n\t            # dis: b p nxy\n", "            return dis\n"]}
{"filename": "models/recon_system.py", "chunked_list": ["import os\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport models\n\tfrom .mlp import MLP\n\tfrom einops import rearrange, reduce, repeat, parse_shape\n\tfrom .gutils import render_bezier_path, path_d_from_control_points, render_sdf\n\tfrom objprint import op\n", "import svgwrite\n\tfrom PIL import Image\n\tfrom .base import DualChannel, SingleChannel\n\t@models.register('z-to-curve')\n\tclass CurveDecoder(nn.Module):\n\t    def __init__(self, z_dim, n_points, n_curves, n_prims, hidden_dim, hidden_layers):\n\t        super().__init__()\n\t        self.z_dim = z_dim\n\t        self.n_points = n_points\n\t        self.n_curves = n_curves\n", "        self.n_prims = n_prims\n\t        self.hidden_dim = hidden_dim\n\t        self.hidden_layers = hidden_layers\n\t        out_dim = (n_points - 1) * 2 * n_curves * n_prims\n\t        self.layers = [z_dim] + [hidden_dim] * hidden_layers + [out_dim]\n\t        self.decoder = MLP(self.layers)\n\t    def forward(self, z):\n\t        # z: bs x z_dim\n\t        # x: bs x out_dim \n\t        x = self.decoder(z)\n", "        x = rearrange(x, 'b (p c pts) -> b p c pts', p=self.n_prims, c=self.n_curves, pts=(self.n_points - 1) * 2)\n\t        start = x[:, :, :, :2]\n\t        end = torch.roll(start, -1, dims=-2)\n\t        x = torch.cat([x, end], dim=-1)\n\t        x = x * 0.1\n\t        return x\n\t@models.register('occ-dual-channel')\n\t# each part is treated as pos subtract neg\n\tclass OccDualChannnel(nn.Module, DualChannel):\n\t    def __init__(self, encoder, decoder, dis_cal, img_decoder=None, detach_img_branch=False, submodule_config=None, sidelength=256, use_occ=False, occ_warmup=10):\n", "        nn.Module.__init__(self)\n\t        DualChannel.__init__(self)\n\t        self.encoder = models.make(encoder)\n\t        self.decoder = models.make(decoder)\n\t        self.img_decoder = models.make(img_decoder) if img_decoder is not None else None\n\t        self.dis_cal = models.make(dis_cal)\n\t        self.n_prims = self.decoder.n_prims\n\t        self.use_occ = use_occ\n\t        self.sidelength = sidelength\n\t        self._colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf']\n", "        self.occ_warmup = occ_warmup\n\t        self.detach_img_branch = detach_img_branch\n\t        self.submodule_config = submodule_config\n\t    def init(self):\n\t        self.dis_cal.init()\n\t        if self.submodule_config is not None:\n\t            for d in self.submodule_config:\n\t                ckpt_path = d['ckpt']\n\t                assert(os.path.exists(ckpt_path))\n\t                ckpt = torch.load(ckpt_path)\n", "                for key, name, freeze in zip(d['ckpt_key'], d['module_name'], d['freeze']):\n\t                    print('load', name, 'from', ckpt_path + '.' + key, 'freeze:', freeze)\n\t                    submodule = getattr(self, name)\n\t                    models.load_submodule(submodule, ckpt, key)\n\t                    if freeze:\n\t                        models.freeze(submodule)\n\t    def decode_image_from_latent_vector(self, z, **kwargs):\n\t        if self.img_decoder is None:\n\t            return {}\n\t        if self.detach_img_branch:\n", "            z = rearrange(z.detach(), 'b z -> b z 1 1')\n\t        else:\n\t            z = rearrange(z, 'b z -> b z 1 1')\n\t        img = self.img_decoder(z)\n\t        return {\n\t            'rec': img,\n\t        }\n\t    def forward(self, batch, **kwargs):\n\t        z = self.encoder(batch)\n\t        out = {\n", "            'z': z,\n\t        }\n\t        xy = batch['xy']\n\t        curves = self.decoder(z)\n\t        epoch = kwargs['epoch']\n\t        is_train = kwargs['train']\n\t        if is_train:\n\t            kernel = 1 - ((1-4/(self.sidelength))*epoch)/self.occ_warmup\n\t        else:\n\t            kernel = 0\n", "        res = self.render_paths_sdf(curves, xy, use_occ=True, kernel=kernel)\n\t        out.update(res)\n\t        res = self.decode_image_from_latent_vector(z, **kwargs)\n\t        out.update(res)\n\t        return out"]}
{"filename": "models/encoder.py", "chunked_list": ["from typing import Type, Any, Callable, Union, List, Optional\n\timport torch\n\timport torch.nn as nn\n\tfrom torch import Tensor\n\timport models\n\tdef conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n\t    \"\"\"3x3 convolution with padding\"\"\"\n\t    return nn.Conv2d(\n\t        in_planes,\n\t        out_planes,\n", "        kernel_size=3,\n\t        stride=stride,\n\t        padding=dilation,\n\t        groups=groups,\n\t        bias=False,\n\t        dilation=dilation,\n\t    )\n\tdef conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n\t    \"\"\"1x1 convolution\"\"\"\n\t    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n", "class BasicBlock(nn.Module):\n\t    def __init__(\n\t        self,\n\t        inplanes: int, planes: int,\n\t        stride: int = 1,\n\t        act: str = 'relu',\n\t        downsample: Optional[nn.Module] = None,\n\t        norm_layer: Optional[Callable[..., nn.Module]] = None,\n\t    ) -> None:\n\t        super().__init__()\n", "        if norm_layer is None:\n\t            norm_layer = nn.BatchNorm2d\n\t        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n\t        self.conv1 = conv3x3(inplanes, planes, stride)\n\t        self.bn1 = norm_layer(planes)\n\t        if act == 'relu':\n\t            self.relu = nn.ReLU(inplace=True)\n\t        elif act == 'leaky_relu':\n\t            self.relu = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n\t        else:\n", "            raise NotImplementedError('not implemented activation')\n\t        self.conv2 = conv3x3(planes, planes)\n\t        self.bn2 = norm_layer(planes)\n\t        self.downsample = downsample\n\t        self.stride = stride\n\t    def forward(self, x: Tensor) -> Tensor:\n\t        identity = x\n\t        out = self.conv1(x)\n\t        out = self.bn1(out)\n\t        out = self.relu(out)\n", "        out = self.conv2(out)\n\t        out = self.bn2(out)\n\t        if self.downsample is not None:\n\t            identity = self.downsample(x)\n\t        out += identity\n\t        out = self.relu(out)\n\t        return out\n\t@models.register('img64-to-z')\n\tclass ImageEncoder64(nn.Module):\n\t    def __init__(self, img_ef_dim, z_dim, key='img'):\n", "        super().__init__()\n\t        self.key = key\n\t        self.img_ef_dim = img_ef_dim\n\t        self.z_dim = z_dim\n\t        self._norm_layer = nn.BatchNorm2d\n\t        self.act = 'leaky_relu'\n\t        # 64 x 64\n\t        self.conv1 = nn.Conv2d(1, self.img_ef_dim, 7, stride=2, padding=3, bias=False)\n\t        self.bn1 = self._norm_layer(self.img_ef_dim)\n\t        self.relu = nn.LeakyReLU(inplace=True, negative_slope=0.01)\n", "        self.inplanes = self.img_ef_dim\n\t        # 32 x 32\n\t        self.layer1 = self._make_layer(self.img_ef_dim, 2, stride=1)\n\t        self.layer2 = self._make_layer(self.img_ef_dim * 2, 2, stride=2)\n\t        self.layer3 = self._make_layer(self.img_ef_dim * 4, 2, stride=2)\n\t        self.layer4 = self._make_layer(self.img_ef_dim * 8, 3, stride=2)\n\t        self.conv2 = nn.Conv2d(self.img_ef_dim * 8, self.img_ef_dim * 16, 4, stride=1, padding=0, bias=True)\n\t        self.fc = nn.Linear(self.img_ef_dim * 16, self.z_dim)\n\t        for m in self.modules():\n\t            if isinstance(m, nn.Conv2d):\n", "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n\t            elif isinstance(m, nn.BatchNorm2d):\n\t                nn.init.constant_(m.weight, 1)\n\t                nn.init.constant_(m.bias, 0)\n\t    def _make_layer(self, planes: int, blocks: int, stride: int = 1) -> nn.Sequential:\n\t        norm_layer = self._norm_layer\n\t        block = BasicBlock\n\t        downsample = None\n\t        if stride != 1 or self.inplanes != planes:\n\t            downsample = nn.Sequential(\n", "                conv1x1(self.inplanes, planes, stride),\n\t                norm_layer(planes),\n\t            )\n\t        layers = []\n\t        layers.append(block(self.inplanes, planes, stride, self.act, downsample, norm_layer))\n\t        self.inplanes = planes\n\t        for _ in range(1, blocks):\n\t            layers.append(block(self.inplanes, planes, act=self.act, norm_layer=norm_layer))\n\t        return nn.Sequential(*layers)\n\t    def forward(self, inp):\n", "        if self.key is None:\n\t            x = self.conv1(inp)\n\t        else:\n\t            x = self.conv1(inp[self.key])\n\t        x = self.bn1(x)\n\t        x = self.relu(x)\n\t        x = self.layer1(x)\n\t        x = self.layer2(x)\n\t        x = self.layer3(x)\n\t        x = self.layer4(x)\n", "        x = self.conv2(x)\n\t        x = torch.flatten(x, start_dim=1)\n\t        x = self.relu(x) # maybe remove\n\t        x = self.fc(x)\n\t        return x\n\t@models.register('embed')\n\tclass Embed(nn.Module):\n\t    def __init__(self, n_embed, z_dim):\n\t        super().__init__()\n\t        self.embed = nn.Embedding(n_embed, z_dim)\n", "        self.init_embed_weight()\n\t    def init_embed_weight(self):\n\t        weights = torch.ones_like(self.embed.weight, requires_grad=True).to(self.embed.weight.device)*0.5\n\t        self.embed.weight = torch.nn.Parameter(weights)\n\t    def forward(self, x):\n\t        return self.embed(x)\n\t@models.register('index-to-z')\n\tclass IndexEmbed(nn.Module):\n\t    def __init__(self, n_embed, z_dim):\n\t        super().__init__()\n", "        self.embed = nn.Embedding(n_embed, z_dim)\n\t    def forward(self, batch):\n\t        return self.embed(batch['index'])\n\t@models.register('family-char-to-z')\n\tclass FamilyCharEmbed(nn.Module):\n\t    def __init__(self, n_family, dim_family, n_char, dim_char):\n\t        super().__init__()\n\t        self.family_embed = nn.Embedding(n_family, dim_family)\n\t        self.char_embed = nn.Embedding(n_char, dim_char)\n\t    def forward(self, batch):\n", "        embed_fam = self.family_embed(batch['font_idx'])\n\t        embed_chr = self.char_embed(batch['char_idx'])\n\t        out = torch.cat([embed_fam, embed_chr], dim=-1)\n\t        return out\n"]}
{"filename": "models/gen_system.py", "chunked_list": ["import os\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import Dataset, DataLoader\n\timport models\n\tfrom .mlp import MLP\n\tfrom einops import rearrange, reduce, repeat, parse_shape\n\tfrom .base import DualChannel\n", "@models.register('vae-style-multi-ref-cnn')\n\tclass VAEStyleMultiRefCNN(nn.Module, DualChannel):\n\t    def __init__(self, img_encoder, tfm, n_char, decoder, z_dim, dis_cal, latent_encoder, img_decoder, sidelength, encode_type, submodule_config=None, train_latent=True, detach_img_branch=False, use_diffvg=False):\n\t        nn.Module.__init__(self)\n\t        DualChannel.__init__(self)\n\t        self.img_encoder = models.make(img_encoder)\n\t        self.encode_type = encode_type\n\t        self.tfm = models.make(tfm)\n\t        self.latent_encoder = models.make(latent_encoder) if latent_encoder is not None else None\n\t        self.mlp_head = MLP([z_dim, z_dim], bias=True, activate='gelu', activate_last=True)\n", "        self.fc_mu = nn.Linear(z_dim, z_dim)\n\t        self.fc_var = nn.Linear(z_dim, z_dim)\n\t        self.cls_token = nn.Embedding(n_char, z_dim)\n\t        self.decoder = models.make(decoder)\n\t        self.dis_cal = models.make(dis_cal)\n\t        self.img_decoder = models.make(img_decoder) if img_decoder is not None else None\n\t        self.merge = nn.Linear(z_dim * 2, z_dim)\n\t        self.submodule_config = submodule_config\n\t        self.n_prims = self.decoder.n_prims\n\t        self.sidelength = sidelength\n", "        self.train_latent = train_latent\n\t        self.detach_img_branch = detach_img_branch\n\t        self.use_diffvg = use_diffvg\n\t    def init(self):\n\t        self.dis_cal.init()\n\t        if self.submodule_config is not None:\n\t            for d in self.submodule_config:\n\t                ckpt_path = d['ckpt']\n\t                assert(os.path.exists(ckpt_path))\n\t                ckpt = torch.load(ckpt_path)\n", "                for key, name, freeze in zip(d['ckpt_key'], d['module_name'], d['freeze']):\n\t                    print('load', name, 'from', ckpt_path + '.' + key, 'freeze:', freeze)\n\t                    submodule = getattr(self, name)\n\t                    models.load_submodule(submodule, ckpt, key)\n\t                    if freeze:\n\t                        models.freeze(submodule)\n\t    def reparameterize(self, mu, log_var):\n\t        # mu, log_var: b x z_dim\n\t        std = torch.exp(0.5 * log_var)\n\t        eps = torch.randn_like(std)\n", "        return eps * std + mu\n\t    def ref_img_encode(self, batch):\n\t        if self.encode_type == 'cnn':\n\t            b, n = batch['refs'].shape[:2]\n\t            imgs = rearrange(batch['refs'], 'b n c h w -> (b n) c h w')\n\t            z_imgs = self.img_encoder(imgs)\n\t            z_imgs = rearrange(z_imgs, '(b n) z -> b n z', b=b)\n\t            mask = torch.arange(n).to(imgs.device).unsqueeze(0).expand(b, -1) >= batch['n_ref'].unsqueeze(1)\n\t            z_imgs = self.tfm(z_imgs, mask)\n\t            x = z_imgs.mean(dim=1) \n", "            x = self.mlp_head(x)\n\t            mu = self.fc_mu(x)\n\t            log_var = self.fc_var(x)\n\t            return mu, log_var\n\t        else:\n\t            raise NotImplementedError\n\t    def forward(self, batch, **kwargs):\n\t        mu, log_var = self.ref_img_encode(batch)\n\t        z_style = self.reparameterize(mu, log_var)\n\t        emb_char = self.cls_token(batch['tgt_char_idx'])\n", "        z = self.merge(torch.cat([z_style, emb_char], dim=-1))\n\t        # dis: b p nxy\n\t        out = {\n\t            'mu': mu,\n\t            'log_var': log_var,\n\t            'z': z,\n\t        }\n\t        is_train = kwargs['train']\n\t        if self.train_latent and is_train:\n\t            if self.latent_encoder is not None:\n", "                z_tgt = self.latent_encoder(batch).detach()\n\t                out.update({\n\t                    'z_detach': z_tgt\n\t                })\n\t        if not self.train_latent or not is_train:\n\t            xy = batch['xy']\n\t            curves = self.decoder(z)\n\t            if self.use_diffvg:\n\t                res = self.render_paths_diffvg(curves, sidelength=batch['img_origin_res'].size(-1))\n\t                out.update(res)\n", "            else:\n\t                res = self.render_paths_sdf(curves, xy, use_occ=True, kernel=4/self.sidelength)\n\t                out.update(res)\n\t            res = self.decode_image_from_latent_vector(z, **kwargs)\n\t            out.update(res)\n\t        return out\n\t    def decode_image_from_latent_vector(self, z, **kwargs):\n\t        if self.img_decoder is None:\n\t            return {}\n\t        if self.detach_img_branch:\n", "            z = rearrange(z.detach(), 'b z -> b z 1 1')\n\t        else:\n\t            z = rearrange(z, 'b z -> b z 1 1')\n\t        img = self.img_decoder(z)\n\t        return {\n\t            'rec': img,\n\t        }\n"]}
{"filename": "datasets/multi_ref_data.py", "chunked_list": ["import os\n\timport json\n\timport glob\n\tfrom PIL import Image\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import Dataset, random_split\n\timport torchvision\n", "import torchvision.transforms as transforms\n\tfrom torch.utils.data import DataLoader, random_split\n\tfrom einops import rearrange, reduce, repeat\n\timport datasets\n\timport utils\n\t@datasets.register('dvf-eval')\n\tclass DvfEval(Dataset):\n\t    def __init__(self, data_root,\n\t                       img_res=224,\n\t                       ref_list=list(range(26)),\n", "                       ratio=1,\n\t                       use_lowercase=True,\n\t                       valid_list=None):\n\t        self.data_root = data_root\n\t        if valid_list is not None:\n\t            if isinstance(valid_list, list):\n\t                font_list = sorted(valid_list)\n\t            else:\n\t                with open(valid_list) as f:\n\t                    font_list = sorted([line.strip() for line in f.readlines()])\n", "        else:\n\t            font_list = sorted(os.listdir(data_root))\n\t        n_fonts = len(font_list)\n\t        self.font_list = font_list[:(n_fonts // ratio)]\n\t        self.ref_list = ref_list\n\t        self.img_res = img_res\n\t        self.use_lowercase = use_lowercase\n\t    def __len__(self):\n\t        return len(self.font_list)\n\t    def make_tensor(self, t, dtype=torch.float32):\n", "        return torch.tensor(t, dtype=dtype)\n\t    def get_image(self, font, char):\n\t        ref_path = os.path.join(self.data_root, font, f'{char}_1024.png')\n\t        if self.use_lowercase:\n\t            cropped = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1280))\n\t            ref_origin = Image.new('L', (1280, 1280), 255)\n\t            ref_origin.paste(cropped, (128, 0, 1152, 1280))\n\t        else:\n\t            ref_origin = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1024))\n\t        ref_np = np.asarray(ref_origin.resize((self.img_res, self.img_res), resample=Image.BILINEAR))\n", "        ref_img = self.make_tensor(ref_np / 255.).view(1, self.img_res, self.img_res)\n\t        return ref_img\n\t    def __getitem__(self, idx):\n\t        n_ref = len(self.ref_list)\n\t        ref_list = self.make_tensor(self.ref_list, dtype=torch.long)\n\t        font_id = idx\n\t        font_name = self.font_list[font_id]\n\t        font_path = os.path.join(self.data_root, font_name)\n\t        ref_imgs = []\n\t        for char in ref_list:\n", "            ref_imgs.append(self.get_image(font_name, char))\n\t        ref_imgs = rearrange(ref_imgs, 'n 1 h w -> n 1 h w')\n\t        ret = {\n\t            'refs': ref_imgs,\n\t            'ref_char_idx': ref_list,\n\t            'n_ref': n_ref,\n\t            'font_idx': font_id,\n\t            'index': idx,\n\t            'font_path': font_path,\n\t            'font_name': font_name, \n", "        }\n\t        return ret\n\t@datasets.register('multi-ref-dvf-generation')\n\tclass MultiRefDvfGeneration(Dataset):\n\t    def __init__(self, data_root, \n\t                       img_res=224, \n\t                       coor_res=64, \n\t                       n_samples=4000, \n\t                       char_list=list(range(26)),\n\t                       val=False, \n", "                       sample_inside=False, \n\t                       signed=False,\n\t                       distance=False,\n\t                       full=False, \n\t                       occ=False, \n\t                       ratio=5,\n\t                       origin_res=False, # origin resolution\n\t                       include_lower_case=False,\n\t                       n_refs=[2,5],\n\t                       use_cache=True,\n", "                       length=None,\n\t                       valid_list=None):\n\t        self.data_root = data_root\n\t        if valid_list is not None:\n\t            with open(valid_list) as f:\n\t                font_list = sorted([line.strip() for line in f.readlines()])\n\t        else:\n\t            font_list = sorted(os.listdir(data_root))\n\t        if val:\n\t            print(len(font_list), 'in all for validating')\n", "        else:\n\t            print(len(font_list), 'in all for training')\n\t        n_fonts = len(font_list)\n\t        self.font_list = font_list[:(n_fonts // ratio)]\n\t        self.char_list = char_list\n\t        self.img_res = img_res\n\t        self.coor_res = coor_res\n\t        self.n_samples = n_samples\n\t        self.val = val\n\t        self.sample_inside = sample_inside\n", "        self.full = full\n\t        self.occ = occ\n\t        self.signed = signed\n\t        self.distance = distance\n\t        self.origin_res = origin_res\n\t        self.min_ref = n_refs[0]\n\t        self.max_ref = n_refs[1]\n\t        self.include_lower_case = include_lower_case\n\t        self.use_cache = use_cache\n\t        self.edge_and_sample = [None for i in range(len(self.font_list) * len(self.char_list))]\n", "        self.images_64 = [None for i in range(len(self.font_list) * len(self.char_list))]\n\t        self.length = length\n\t    def __len__(self):\n\t        return self.length or len(self.font_list) * len(self.char_list)\n\t    def make_tensor(self, t, dtype=torch.float32):\n\t        return torch.tensor(t, dtype=dtype)\n\t    def get_image(self, font_id, char_id):\n\t        index = font_id * len(self.char_list) + char_id\n\t        if self.images_64[index] is None:\n\t            ref_path = os.path.join(self.data_root, self.font_list[font_id], f'{self.char_list[char_id]}_1024.png')\n", "            if self.include_lower_case:\n\t                cropped = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1280))\n\t                ref_origin = Image.new('L', (1280, 1280), 255)\n\t                ref_origin.paste(cropped, (128, 0, 1152, 1280))\n\t            else:\n\t                ref_origin = Image.open(ref_path).convert('L').crop((0, 0, 1024, 1024))\n\t            ref_np = np.asarray(ref_origin.resize((self.img_res, self.img_res), resample=Image.BILINEAR))\n\t            ref_img = self.make_tensor(ref_np / 255.).view(1, self.img_res, self.img_res)\n\t            if self.use_cache:\n\t                self.images_64[index] = ref_img\n", "            else:\n\t                return ref_img\n\t        return self.images_64[index]\n\t    def __getitem__(self, idx):\n\t        if self.length is not None:\n\t            char_id = torch.randint(len(self.char_list), size=()).item()\n\t            font_id = torch.randint(len(self.font_list), size=()).item()\n\t        else:\n\t            char_id = idx % len(self.char_list)\n\t            font_id = idx // len(self.char_list)\n", "        img_path = os.path.join(self.data_root, self.font_list[font_id], f'{self.char_list[char_id]}_1024.png')\n\t        if not self.include_lower_case:\n\t            n_ref = torch.randint(low=self.min_ref, high=self.max_ref + 1, size=(1,)).item()\n\t            ref_char_idx = torch.randperm(26)[:self.max_ref]\n\t            ref_imgs = []\n\t            for i, ref_char_id in enumerate(ref_char_idx):\n\t                if i < n_ref:\n\t                    ref_imgs.append(self.get_image(font_id, ref_char_id.item()))\n\t                else:\n\t                    ref_imgs.append(0.5 * torch.ones((1, self.img_res, self.img_res), dtype=torch.float32))\n", "            ref_imgs = rearrange(ref_imgs, 'n 1 h w -> n 1 h w')\n\t        else:\n\t            n_ref = torch.randint(low=self.min_ref, high=self.max_ref + 1, size=(1,)).item()\n\t            ref_char_idx = torch.randperm(26)[:self.max_ref]\n\t            ref_char_idx = ref_char_idx.repeat(2,1).transpose(0,1)\n\t            ref_char_idx[:, 1] += 26\n\t            ref_char_idx = ref_char_idx.flatten()\n\t            ref_imgs = []\n\t            for i, ref_char_id in enumerate(ref_char_idx):\n\t                if i < n_ref * 2:\n", "                    ref_imgs.append(self.get_image(font_id, ref_char_id.item()))\n\t                else:\n\t                    ref_imgs.append(0.5 * torch.ones((1, self.img_res, self.img_res), dtype=torch.float32))\n\t            ref_imgs = rearrange(ref_imgs, 'n 1 h w -> n 1 h w')\n\t            n_ref = n_ref * 2\n\t        ret = {\n\t            'refs': ref_imgs,\n\t            'ref_char_idx': ref_char_idx,\n\t            'n_ref': n_ref,\n\t            'tgt_char_idx': char_id,\n", "            'font_idx': font_id,\n\t            'img_path': img_path,\n\t            'index': idx,\n\t        }\n\t        if self.origin_res:\n\t            ret['img_origin_res'] = self.get_image(font_id, char_id)\n\t        if self.full:\n\t            if self.include_lower_case:\n\t                cropped = Image.open(img_path).convert('L').crop((0, 0, 1024, 1280))\n\t                img_origin = Image.new('L', (1280, 1280), 255)\n", "                img_origin.paste(cropped, (128, 0, 1152, 1280))\n\t            else:\n\t                img_origin = Image.open(img_path).convert('L').crop((0, 0, 1024, 1024))\n\t            img_np = np.asarray(img_origin.resize((self.coor_res, self.coor_res), resample=Image.BICUBIC))\n\t            ret['full_img'] = self.make_tensor(img_np / 255.).view(1, self.coor_res, self.coor_res)\n\t        if self.val or self.n_samples == 0:\n\t            return ret\n\t        if self.edge_and_sample[idx] is None:\n\t            origin = Image.open(img_path).convert('L').crop((0, 0, 1024, 1024))\n\t            sdf_img = np.asarray(origin.resize((self.coor_res, self.coor_res), resample=Image.BICUBIC))\n", "            h, w = self.coor_res, self.coor_res\n\t            mask = sdf_img < 128 # in the glyph\n\t            edge = []\n\t            locs = np.where(mask)\n\t            def in_range(i, j, h, w):\n\t                return 0 <= i < h and 0 <= j < w\n\t            for i, j in zip(*locs):\n\t                at_edge = (in_range(i - 1, j, h, w) and not mask[i - 1, j]) or \\\n\t                        (in_range(i, j - 1, h, w) and not mask[i, j - 1]) or \\\n\t                        (in_range(i, j + 1, h, w) and not mask[i, j + 1]) or \\\n", "                        (in_range(i + 1, j, h, w) and not mask[i + 1, j]) \n\t                if at_edge:\n\t                    edge.append([2 * i / h - 1, 2 * j / w - 1])\n\t            edge = self.make_tensor(np.array(edge))\n\t            out_points = 2 * self.make_tensor(np.array(np.where(sdf_img >= 128)).T) / self.coor_res - 1\n\t            in_points = 2 * self.make_tensor(np.array(np.where(sdf_img < 128)).T) / self.coor_res - 1\n\t            self.edge_and_sample[idx] = (edge, out_points, in_points, sdf_img)\n\t        else:\n\t            edge, out_points, in_points, sdf_img = self.edge_and_sample[idx]\n\t        ne = edge.shape[0]\n", "        if self.sample_inside:\n\t            all_points = torch.cat([out_points, in_points], dim=0)\n\t            unif = torch.ones(all_points.shape[0])\n\t            indices = unif.multinomial(self.n_samples, replacement=self.n_samples > all_points.shape[0])\n\t            samples = all_points[indices]\n\t            ret['xy'] = samples\n\t            if self.distance:\n\t                samples_r = repeat(samples, 'ns d -> ns ne d', ne=ne)\n\t                edge_r = repeat(edge, 'ne d -> ns ne d', ns=self.n_samples)\n\t                dis = torch.pow(samples_r - edge_r, 2)\n", "                dis = reduce(dis, 'ns ne d -> ns ne', 'sum')\n\t                dis = reduce(dis, 'ns ne -> ns', 'min')\n\t                dis = torch.sqrt(dis)\n\t                if self.signed:\n\t                    dis[indices >= out_points.shape[0]] *= -1\n\t                else:\n\t                    dis[indices >= out_points.shape[0]] = 0\n\t                ret['dis'] = dis,\n\t        else:\n\t            unif = torch.ones(out_points.shape[0])\n", "            out_indices = unif.multinomial(self.n_samples, replacement=self.n_samples > out_points.shape[0])\n\t            samples = out_points[out_indices] # n_samples x 2\n\t            ret['xy'] = samples\n\t            if self.distance:\n\t                samples_r = repeat(samples, 'ns d -> ns ne d', ne=ne)\n\t                edge_r = repeat(edge, 'ne d -> ns ne d', ns=self.n_samples)\n\t                dis = torch.pow(samples_r - edge_r, 2)\n\t                dis = reduce(dis, 'ns ne d -> ns ne', 'sum')\n\t                dis = reduce(dis, 'ns ne -> ns', 'min')\n\t                dis = torch.sqrt(dis)\n", "                ret['dis'] = dis\n\t        if self.occ:\n\t            samples = ((ret['xy'] + 1) * self.coor_res / 2).long()\n\t            occ_gt = torch.tensor(sdf_img / 255.).view(self.coor_res, self.coor_res)\n\t            occ = occ_gt[samples[:, 0], samples[:, 1]]\n\t            ret.update({\n\t                'pixel': occ.float(),\n\t            })\n\t        return ret\n"]}
{"filename": "datasets/__init__.py", "chunked_list": ["from .tools import register, make\n\tfrom . import recon_data\n\tfrom . import multi_ref_data\n"]}
{"filename": "datasets/recon_data.py", "chunked_list": ["import os\n\timport json\n\timport glob\n\tfrom PIL import Image\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import Dataset, random_split\n\timport torchvision\n", "import torchvision.transforms as transforms\n\tfrom torch.utils.data import DataLoader, random_split\n\tfrom einops import rearrange, reduce, repeat\n\timport datasets\n\timport utils\n\t@datasets.register('deepvecfont-sdf')\n\tclass DeepvecfontSDF(Dataset):\n\t    def __init__(self, data_root, \n\t                       img_res=224, \n\t                       coor_res=64, \n", "                       n_samples=4000, \n\t                       char_list=list(range(26)),\n\t                       include_lower_case=False,\n\t                       val=False, \n\t                       sample_inside=False, \n\t                       signed_distance=False,\n\t                       full=False, \n\t                       occ=False, \n\t                       sample_dis=True,\n\t                       weight=False,\n", "                       repeat=1,\n\t                       ratio=5,\n\t                       use_cache=True,\n\t                       random_sample=False,\n\t                       dataset_length=None,\n\t                       valid_list=None):\n\t        self.data_root = data_root\n\t        self.repeat = repeat\n\t        if isinstance(valid_list, list):\n\t            font_list = sorted(valid_list)\n", "        elif isinstance(valid_list, str):\n\t            with open(valid_list) as f:\n\t                font_list = sorted([line.strip() for line in f.readlines()])\n\t        else:\n\t            font_list = sorted(os.listdir(data_root))\n\t        if val:\n\t            print(len(font_list), 'in all for validating')\n\t        else:\n\t            print(len(font_list), 'in all for training')\n\t        n_fonts = len(font_list)\n", "        self.font_list = font_list[:(n_fonts // ratio)]\n\t        self.char_list = char_list\n\t        self.edge_and_sample = [None for i in range(len(self.font_list) * len(self.char_list))]\n\t        self.img_res = img_res\n\t        self.coor_res = coor_res\n\t        self.n_samples = n_samples\n\t        self.val = val\n\t        self.sample_inside = sample_inside\n\t        self.full = full\n\t        self.occ = occ\n", "        self.weight = weight\n\t        self.signed_distance = signed_distance\n\t        self.sample_dis = sample_dis\n\t        self.include_lower_case = include_lower_case\n\t        self.cache = use_cache\n\t        self.random_sample = random_sample\n\t        self.dataset_length = dataset_length\n\t    def __len__(self):\n\t        # return len(self.font_list) * len(self.char_list) * self.repeat if not self.random_sample else self.dataset_length\n\t        return len(self.font_list) * len(self.char_list) * self.repeat if self.dataset_length is None else self.dataset_length\n", "    def make_tensor(self, t, dtype=torch.float32):\n\t        return torch.tensor(t, dtype=dtype)\n\t    def __getitem__(self, idx):\n\t        if not self.random_sample:\n\t            idx = idx % (len(self.font_list) * len(self.char_list))\n\t            char_id = idx % len(self.char_list)\n\t            font_id = idx // len(self.char_list)\n\t        else:\n\t            char_id = torch.randint(len(self.char_list), (1,)).item()\n\t            font_id = torch.randint(len(self.font_list), (1,)).item()\n", "        img_path = os.path.join(self.data_root, self.font_list[font_id], f'{self.char_list[char_id]}_1024.png')\n\t        if self.include_lower_case:\n\t            cropped = Image.open(img_path).convert('L').crop((0, 0, 1024, 1280))\n\t            origin = Image.new('L', (1280, 1280), 255)\n\t            origin.paste(cropped, (128, 0, 1152, 1280))\n\t        else:\n\t            origin = Image.open(img_path).convert('L').crop((0, 0, 1024, 1024))\n\t        img_np = np.asarray(origin.resize((self.img_res, self.img_res), resample=Image.BICUBIC))\n\t        img = self.make_tensor(img_np / 255.).view(1, self.img_res, self.img_res)\n\t        ret = {\n", "            'img_path': img_path,\n\t            'img': img,\n\t            'index': idx,\n\t            'font_idx': font_id,\n\t            'char_idx': char_id,\n\t            'font_name': self.font_list[font_id],\n\t            'char': self.char_list[char_id],\n\t        }\n\t        if self.edge_and_sample[idx] is None:\n\t            sdf_img = np.asarray(origin.resize((self.coor_res, self.coor_res), resample=Image.BICUBIC))\n", "            h, w = self.coor_res, self.coor_res\n\t            mask = sdf_img < 128 # in the glyph\n\t            edge = []\n\t            locs = np.where(mask)\n\t            def in_range(i, j, h, w):\n\t                return 0 <= i < h and 0 <= j < w\n\t            for i, j in zip(*locs):\n\t                at_edge = (in_range(i - 1, j, h, w) and not mask[i - 1, j]) or \\\n\t                        (in_range(i, j - 1, h, w) and not mask[i, j - 1]) or \\\n\t                        (in_range(i, j + 1, h, w) and not mask[i, j + 1]) or \\\n", "                        (in_range(i + 1, j, h, w) and not mask[i + 1, j]) \n\t                if at_edge:\n\t                    edge.append([2 * i / h - 1, 2 * j / w - 1])\n\t            edge = self.make_tensor(np.array(edge))\n\t            out_points = 2 * self.make_tensor(np.array(np.where(sdf_img >= 128)).T) / self.coor_res - 1\n\t            in_points = 2 * self.make_tensor(np.array(np.where(sdf_img < 128)).T) / self.coor_res - 1\n\t            if self.cache:\n\t                self.edge_and_sample[idx] = (edge, out_points, in_points, sdf_img)\n\t        else:\n\t            edge, out_points, in_points, sdf_img = self.edge_and_sample[idx]\n", "        if self.full:\n\t            ret['full_img'] = self.make_tensor(sdf_img / 255.).view(1, self.coor_res, self.coor_res)\n\t        if self.val or self.n_samples == 0:\n\t            return ret\n\t        ne = edge.shape[0]\n\t        if self.sample_inside:\n\t            all_points = torch.cat([out_points, in_points], dim=0)\n\t            unif = torch.ones(all_points.shape[0])\n\t            indices = unif.multinomial(self.n_samples, replacement=self.n_samples > all_points.shape[0])\n\t            samples = all_points[indices]\n", "        else:\n\t            unif = torch.ones(out_points.shape[0])\n\t            indices = unif.multinomial(self.n_samples, replacement=self.n_samples > out_points.shape[0])\n\t            samples = out_points[indices] # n_samples x 2\n\t        ret.update({\n\t            'xy': samples\n\t        })\n\t        if self.sample_dis:\n\t            samples_r = repeat(samples, 'ns d -> ns ne d', ne=ne)\n\t            edge_r = repeat(edge, 'ne d -> ns ne d', ns=self.n_samples)\n", "            dis = torch.pow(samples_r - edge_r, 2)\n\t            dis = reduce(dis, 'ns ne d -> ns ne', 'sum')\n\t            dis = reduce(dis, 'ns ne -> ns', 'min')\n\t            dis = torch.sqrt(dis)\n\t            if self.weight:\n\t                w = torch.exp(-dis)\n\t                ret.update({\n\t                    'weight': w\n\t                })\n\t            if self.sample_inside:\n", "                if self.signed_distance:\n\t                    dis[indices >= out_points.shape[0]] *= -1\n\t                else:\n\t                    dis[indices >= out_points.shape[0]] = 0\n\t            ret.update({\n\t                'dis': dis,\n\t            })\n\t        if self.occ:\n\t            samples = ((ret['xy'] + 1) * self.coor_res / 2).long()\n\t            occ_gt = torch.tensor(sdf_img / 255.).view(self.coor_res, self.coor_res)\n", "            occ = occ_gt[samples[:, 0], samples[:, 1]]\n\t            ret.update({\n\t                'pixel': occ.float(),\n\t            })\n\t        return ret"]}
{"filename": "datasets/tools.py", "chunked_list": ["import copy\n\tdatasets = {}\n\tdef register(name):\n\t    def decorator(cls):\n\t        datasets[name] = cls\n\t        return cls\n\t    return decorator\n\tdef make(dataset_spec, args=None):\n\t    if args is not None:\n\t        dataset_args = copy.deepcopy(dataset_spec['args'])\n", "        dataset_args.update(args)\n\t    else:\n\t        dataset_args = dataset_spec['args']\n\t    dataset = datasets[dataset_spec['name']](**dataset_args)\n\t    return dataset\n"]}
