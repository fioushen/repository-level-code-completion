{"filename": "eval_atrr.py", "chunked_list": ["\"\"\"\n\tMain script for evaluating the baseline method ATRR:\n\tPang, Tianyu, et al. \"Adversarial Training with Rectified Rejection.\" arXiv preprint arXiv:2105.14785 (2021).\n\thttps://arxiv.org/pdf/2105.14785.pdf\n\tBased on the official code from: https://github.com/P2333/Rectified-Rejection\n\t\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport numpy as np\n", "import torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n\timport models\n\timport json\n\timport math\n\tfrom attacks.objectives import *\n", "from attacks.mb_pgd_attack import *\n\tfrom utils.dataset import CustomDataset\n\tfrom utils.constants import *\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom autoattack import AutoAttack\n\tdef eval_outer_attack(attack_method,\n\t                      outer_attack_config, \n\t                      model, \n\t                      test_dataloader, \n", "                      num_classes,\n\t                      threshold, \n\t                      tempC, \n\t                      epsilon, \n\t                      n_samples=N_SAMPLES):\n\t    model.eval()\n\t    if attack_method == 'pgd':\n\t        objective = SelectiveUntargetedObjective(loss=utils.torch.atrr_accept_misclassify_loss)\n\t        attacker = MBATRRLinfPGDAttack(model,\n\t                                     objective,\n", "                                     tempC=tempC,\n\t                                     epsilon=epsilon,\n\t                                     **outer_attack_config)\n\t    elif attack_method == 'auto_attack':\n\t        attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n\t    elif attack_method == 'ATRROA':\n\t        objective = SelectiveUntargetedObjective(loss=utils.torch.atrr_targeted_loss)\n\t        attacker = MBATRRLinfPGDAttackMultitargeted(\n\t                                    model,\n\t                                    objective,\n", "                                    num_classes,\n\t                                    tempC=tempC,\n\t                                    epsilon=epsilon,\n\t                                    **outer_attack_config)\n\t    else:\n\t        raise KeyError(f'Not supported attack method {attack_method}')\n\t    adv_labels = None\n\t    adv_probs = None\n\t    adv_aux_probs = None\n\t    cnt = 0\n", "    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\t        # large perturbations\n\t        if attack_method == 'auto_attack':\n\t            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n\t        else:\n\t            adv_inputs = attacker.perturb(inputs, targets)\n\t        with torch.no_grad():\n", "            adv_logits, adv_aux_outputs = model(adv_inputs, return_aux=True)\n\t        con_pre, _ = torch.softmax(adv_logits * tempC, dim=1).max(1) # predicted label and confidence\n\t        adv_aux_outputs = adv_aux_outputs.sigmoid().squeeze()\n\t        evi_outputs = con_pre * adv_aux_outputs\n\t        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t        adv_aux_probs = utils.numpy.concatenate(adv_aux_probs, evi_outputs.detach().cpu().numpy())\n\t        cnt += inputs.shape[0]\n\t        if cnt >= n_samples:\n\t            break\n\t    # Error on adversarial inputs: accept and misclassify\n", "    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n\t    adv_error = np.logical_and(adv_preds != adv_labels, adv_aux_probs >= threshold)\n\t    return adv_error\n\tdef eval_inner_attack(attack_method,\n\t                      inner_attack_config, \n\t                      model, \n\t                      test_dataloader, \n\t                      threshold, \n\t                      tempC, \n\t                      epsilon, \n", "                      eps0_range, \n\t                      n_samples=N_SAMPLES):\n\t    model.eval()\n\t    inner_attack_results = {}\n\t    for epsilon_0 in eps0_range:\n\t        if attack_method == 'ATRRIA':\n\t            radius_objective = SelectiveUntargetedObjective(loss=utils.torch.atrr_reject_loss)\n\t            radius_attacker = MBATRRLinfPGDAttack(model,\n\t                                                radius_objective,\n\t                                                tempC=tempC,\n", "                                                epsilon=epsilon_0,\n\t                                                **inner_attack_config)\n\t        else:\n\t            raise KeyError(f'Not supported attack method {attack_method}')\n\t        cnt = 0\n\t        adv_probs_2 = None\n\t        adv_aux_probs_2 = None\n\t        for b, (inputs, targets) in enumerate(test_dataloader):\n\t            inputs = inputs.cuda()\n\t            targets = targets.cuda()\n", "            # small perturbations\n\t            if epsilon_0 > 0.:\n\t                adv_inputs = radius_attacker.perturb(inputs, targets)\n\t            else:\n\t                adv_inputs = inputs\n\t            with torch.no_grad():\n\t                adv_logits, adv_aux_outputs = model(adv_inputs, return_aux=True)\n\t            con_pre, _ = torch.softmax(adv_logits * tempC, dim=1).max(1) # predicted label and confidence\n\t            adv_aux_outputs = adv_aux_outputs.sigmoid().squeeze()\n\t            evi_outputs = con_pre * adv_aux_outputs\n", "            adv_probs_2 = utils.numpy.concatenate(adv_probs_2, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t            adv_aux_probs_2 = utils.numpy.concatenate(adv_aux_probs_2, evi_outputs.detach().cpu().numpy())\n\t            cnt += inputs.shape[0]\n\t            if cnt >= n_samples:\n\t                break\n\t        adv_error_2 = adv_aux_probs_2 < threshold # regard rejection as error\n\t        inner_attack_results[epsilon_0] = adv_error_2\n\t    return inner_attack_results\n\tdef eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n\t    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n", "    curve_x = []\n\t    curve_y = []\n\t    for epsilon_0 in inner_attack_results:\n\t        curve_x.append(epsilon_0/epsilon)\n\t        adv_error_2 = inner_attack_results[epsilon_0]\n\t        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n\t        curve_y.append(1 - final_adv_error)\n\t        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n\t    return np.array(curve_x), np.array(curve_y)\n\tdef eval(model, test_dataloader, tpr, tempC, logger):\n", "    model.eval()\n\t    clean_probs = None\n\t    clean_labels = None\n\t    clean_aux_probs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n\t            outputs, aux_outputs = model(inputs, return_aux=True)\n\t        con_pre, _ = torch.softmax(outputs * tempC, dim=1).max(1) # predicted label and confidence\n", "        aux_outputs = aux_outputs.sigmoid().squeeze()\n\t        evi_outputs = con_pre * aux_outputs\n\t        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n\t        clean_aux_probs = utils.numpy.concatenate(clean_aux_probs, evi_outputs.detach().cpu().numpy())\n\t        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\t    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n\t    val_probs = clean_probs[test_N:]\n\t    val_labels = clean_labels[test_N:]\n\t    val_aux_probs = clean_aux_probs[test_N:]\n\t    test_probs = clean_probs[:test_N]\n", "    test_labels = clean_labels[:test_N]\n\t    test_aux_probs = clean_aux_probs[:test_N]\n\t    # Find the confidence threshold using a validation set\n\t    val_preds = np.argmax(val_probs, axis=1)\n\t    val_errors = (val_preds != val_labels)\n\t    # sorted_val_aux_probs = np.sort(val_aux_probs)\n\t    sorted_val_aux_probs = np.sort(val_aux_probs[np.logical_not(val_errors)])  # sorted confidence of correctly classified\n\t    cutoff = math.floor(sorted_val_aux_probs.shape[0] * round((1. - tpr), 2))\n\t    thresh_detec = sorted_val_aux_probs[cutoff]\n\t    test_preds = np.argmax(test_probs, axis=1)\n", "    test_errors = (test_preds != test_labels)\n\t    test_acc = 1. - np.mean(test_errors)\n\t    # Clean accuracy within the accepted inputs\n\t    mask_accept = (test_aux_probs >= thresh_detec)\n\t    test_acc_with_detection = 1. - np.sum(test_errors & mask_accept) / np.sum(mask_accept)\n\t    rejection_rate = 1. - np.mean(mask_accept)\n\t    logger.info(f\"threshold: {thresh_detec:.4f}\")\n\t    logger.info(f\"clean accuracy: {test_acc:.2%}\")\n\t    logger.info(f\"clean accuracy with detection: {test_acc_with_detection:.2%}, clean rejection rate: {rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-rejection_rate)/(test_acc_with_detection+1-rejection_rate):.2%}\")\n\t    return thresh_detec\n", "def combine_outer_attack_results(combine_adv_error, curr_adv_error):\n\t    if combine_adv_error is None:\n\t        return curr_adv_error\n\t    else:\n\t        return curr_adv_error|combine_adv_error\n\tdef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n\t    if combine_inner_results is None:\n\t        return curr_inner_results\n\t    for eps0 in curr_inner_results:\n\t        combine_inner_results[eps0] |= curr_inner_results[eps0]\n", "    return combine_inner_results\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', type=int, default=0)\n\t    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n\t    parser.add_argument('--tempC', type=float, default=1.0, help=\"temperature parameter\")\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n", "    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n\t        config = json.load(config_file)\n\t    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    epsilon = config['epsilon']\n\t    batch_size = config['batch_size']\n\t    tempC = args.tempC\n", "    checkpoint_dir = args.checkpoint_dir\n\t    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n\t    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n\t        level=logging.DEBUG,\n\t        handlers=[\n", "            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n\t    logger.info(args)\n\t    logger.info(config)\n\t    N_class = N_CLASSES[dataset]\n\t    tpr = TPR_THRESHOLD[dataset]\n\t    if dataset == 'cifar10':\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n", "        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n", "    elif dataset == 'gtsrb':\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n", "        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'svhn':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n", "        iaa.ContrastNormalization((0.7, 1.3))]\n\t        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n", "        transform_test = transforms.ToTensor()\n\t        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'mnist':\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n", "        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n\t        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\t    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n\t    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNetTwoBranchDenseV1(N_class, resolution, out_dim=1, use_BN=True, along=True)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNetTwoBranchDenseV1(N_class, resolution, blocks=[3, 3, 3], out_dim=1, use_BN=True, along=True)\n", "    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNetTwoBranchDenseV1(N_class, resolution, depth=28, width=10, out_dim=1, use_BN=True, along=True)\n\t    else:\n\t        raise ValueError\n\t    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n\t    model.load_state_dict(checkpoint['model'])\n\t    model.cuda()\n\t    # evaluate clean and get threshold\n\t    threshold = eval(model, test_dataloader, tpr, tempC, logger)\n\t    final_outer_adv_error = None\n", "    final_inner_attack_results = None\n\t    # evaluate robustness with rejection under AutoAttack\n\t    outer_attack_config = None\n\t    outer_attack_method = 'auto_attack'\n\t    outer_file_name = f\"outer_{outer_attack_method}\"\n\t    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t    outer_file_name += \"_result.npy\"\n\t    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t    else:\n", "        outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                            outer_attack_config, \n\t                                              model, \n\t                                              test_dataloader, \n\t                                              N_class,\n\t                                              threshold, \n\t                                              tempC, \n\t                                              epsilon,  \n\t                                              n_samples=N_SAMPLES)\n\t    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n", "    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t    # evaluate robustness with rejection under multitargeted attack\n\t    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n\t    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n\t    outer_attack_method = 'ATRROA'\n\t    inner_attack_method = 'ATRRIA'\n\t    for base_lr in BASE_LR_RANGE[dataset]:\n\t        outer_attack_config.update({'base_lr': base_lr})\n\t        outer_file_name = f\"outer_{outer_attack_method}\"\n\t        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n", "        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n\t        outer_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t        else:\n\t            outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                                outer_attack_config, \n\t                                                  model, \n\t                                                  test_dataloader, \n", "                                                  N_class,\n\t                                                  threshold, \n\t                                                  tempC, \n\t                                                  epsilon, \n\t                                                  n_samples=N_SAMPLES)\n\t        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n\t        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t        inner_attack_config.update({'base_lr': base_lr})\n\t        inner_file_name = f\"inner_{inner_attack_method}\"\n\t        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n", "        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n\t        inner_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n\t            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n\t        else:\n\t            inner_attack_results = eval_inner_attack(inner_attack_method,\n\t                                                     inner_attack_config, \n\t                                                      model, \n\t                                                      test_dataloader, \n", "                                                      threshold, \n\t                                                      tempC, \n\t                                                      epsilon, \n\t                                                      eps0_range, \n\t                                                      n_samples=N_SAMPLES)\n\t        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n\t        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n\t        logger.info(f'{outer_file_name} saved!')\n\t        logger.info(f'{inner_file_name} saved!')\n\t        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n", "    # Evaluate robustness curve\n\t    logger.info('Final results under an ensemble of attacks:')\n\t    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n\t    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n\t    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "train_at.py", "chunked_list": ["import argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n", "import models\n\timport json\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom attacks.objectives import UntargetedObjective\n\tfrom attacks.mb_pgd_attack import MBLinfPGDAttack\n\tfrom utils.dataset import CustomDataset\n\tdef test(model, attacker, test_dataloader, N_class, max_batches, logger):\n\t    model.eval()\n\t    clean_losses = None\n", "    clean_accs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n\t            outputs = model(inputs)\n\t        clean_losses = utils.numpy.concatenate(clean_losses, utils.torch.classification_loss(outputs, targets, reduction='none').detach().cpu().numpy())\n\t        clean_accs = utils.numpy.concatenate(clean_accs, 1 - utils.torch.classification_error(outputs, targets, reduction='none').detach().cpu().numpy())\n\t    logger.info(\"clean_loss: {:.2f}, clean_acc: {:.2f}%\".format(np.mean(clean_losses), np.mean(clean_accs)*100))\n\t    adv_losses = None\n", "    adv_accs = None\n\t    adv_successes = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        if b >= max_batches:\n\t            break\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        # small perturbations\n\t        adv_inputs = attacker.perturb(inputs, targets)\n\t        with torch.no_grad():\n", "            logits = model(adv_inputs)\n\t        adv_losses = utils.numpy.concatenate(adv_losses, utils.torch.classification_loss(logits, targets, reduction='none').detach().cpu().numpy())\n\t        adv_accs = utils.numpy.concatenate(adv_accs, 1 - utils.torch.classification_error(logits, targets, reduction='none').detach().cpu().numpy())\n\t    logger.info(\"adv_loss: {:.2f}, adv_acc: {:.2f}%\".format(np.mean(adv_losses), np.mean(adv_accs)*100))\n\tdef lr_schedule(t, lr_max):\n\t    if t < 100:\n\t        return lr_max\n\t    elif t < 105:\n\t        return lr_max / 10.\n\t    else:\n", "        return lr_max / 100.\n\tdef train_robust(model, \n\t                train_dataloader,\n\t                attacker, \n\t                optimizer, \n\t                scheduler, \n\t                fraction,\n\t                N_class,\n\t                epoch,\n\t                lr_max,\n", "                print_freq,\n\t                logger):\n\t    num_training_iter = len(train_dataloader)\n\t    for b, (inputs, targets) in enumerate(train_dataloader):\n\t        if scheduler is None:\n\t            epoch_now = epoch + (b + 1) / len(train_dataloader)\n\t            lr = lr_schedule(epoch_now, lr_max)\n\t            optimizer.param_groups[0].update(lr=lr)\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n", "        split = int((1-fraction)*inputs.size(0))\n\t        # update fraction for correct loss computation\n\t        fraction = 1 - split / float(inputs.size(0))\n\t        clean_inputs = inputs[:split]\n\t        adv_inputs = inputs[split:]\n\t        clean_targets = targets[:split]\n\t        adv_targets = targets[split:]\n\t        adv_examples = attacker.perturb(adv_inputs, adv_targets)\n\t        if adv_inputs.shape[0] < inputs.shape[0]: # fraction is not 1\n\t            inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n", "        else:\n\t            inputs = adv_examples\n\t        model.train()\n\t        optimizer.zero_grad()\n\t        logits = model(inputs)\n\t        clean_logits = logits[:split]\n\t        adv_logits = logits[split:]\n\t        adv_loss = utils.torch.classification_loss(adv_logits, adv_targets)\n\t        adv_acc = 1 - utils.torch.classification_error(adv_logits, adv_targets)\n\t        if adv_inputs.shape[0] < inputs.shape[0]:\n", "            clean_loss = utils.torch.classification_loss(clean_logits, clean_targets)\n\t            # clean_error = utils.torch.classification_error(clean_logits, clean_targets)\n\t            loss = (1 - fraction) * clean_loss + fraction * adv_loss\n\t        else:\n\t            clean_loss = torch.zeros(1)\n\t            # clean_error = torch.zeros(1)\n\t            loss = adv_loss\n\t        loss.backward()\n\t        optimizer.step()\n\t        if scheduler is not None:\n", "            scheduler.step()\n\t        if (b+1) % print_freq == 0:\n\t            logger.info(\"Progress: {:d}/{:d}, adv_loss: {:.2f}, adv_acc: {:.2f}%\".format(b+1, \n\t                                                                                        num_training_iter, \n\t                                                                                        adv_loss.item(),\n\t                                                                                        adv_acc.item()*100,\n\t                                                                                        ))\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', type=int, default=0)\n", "    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n\t        config = json.load(config_file)\n", "    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    lr = config['lr']\n\t    optimizer_name = config['optimizer_name']\n\t    epsilon = config['epsilon']\n\t    eps_iter = config['eps_iter']\n\t    nb_iter = config['nb_iter']\n\t    fraction = config['fraction']\n\t    print_freq = config['print_freq']\n\t    checkpoint_freq = config['checkpoint_freq']\n", "    max_batches = config['max_batches']\n\t    nepoch = config['nepoch']\n\t    batch_size = config['batch_size']\n\t    output_dir = args.output_dir\n\t    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n", "        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n\t    logger.info(args)\n\t    logger.info(config)\n\t    if dataset == 'cifar10':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n", "        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n", "        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'gtsrb':\n\t        N_class = 43\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test.npz')\n\t        X_test = test_loaded['images']\n", "        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'svhn':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n", "        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n\t        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n", "            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n", "        eps_iter /= 255.\n\t    elif dataset == 'mnist':\n\t        N_class = 10\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n\t        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n", "    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n\t    else:\n\t        raise ValueError\n\t    model.cuda()\n", "    if optimizer_name == 'SGD-pp':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n\t        scheduler = None\n\t    elif optimizer_name == 'SGD':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\t        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n\t    else:\n\t        raise ValueError\n\t    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n\t    attacker = MBLinfPGDAttack(model, \n", "                                objective, \n\t                                epsilon=epsilon, \n\t                                max_iterations=nb_iter, \n\t                                base_lr=eps_iter, \n\t                                momentum=0.0, \n\t                                lr_factor=1.5, \n\t                                backtrack=False, \n\t                                rand_init_name=\"random+zero\",\n\t                                num_rand_init=1,\n\t                                clip_min=0.0,\n", "                                clip_max=1.0)\n\t    for epoch in range(nepoch):\n\t        logger.info(\"Epoch: {:d}\".format(epoch))\n\t        train_robust(model, \n\t                    train_dataloader,\n\t                    attacker,\n\t                    optimizer, \n\t                    scheduler, \n\t                    fraction,\n\t                    N_class,\n", "                    epoch,\n\t                    lr,\n\t                    print_freq,\n\t                    logger)\n\t        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\t        if (epoch+1) % checkpoint_freq == 0:\n\t            torch.save({\n\t                'epoch': epoch + 1,\n\t                'model': model.state_dict(),\n\t                'optimizer': optimizer.state_dict(),\n", "            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\t    torch.save({\n\t            'epoch': epoch + 1,\n\t            'model': model.state_dict(),\n\t            'optimizer': optimizer.state_dict(),\n\t        }, os.path.join(output_dir, 'classifier.pth.tar'))\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "train_ccat.py", "chunked_list": ["\"\"\"\n\tMain script for training the baseline method CCAT:\n\tStutz, David, Matthias Hein, and Bernt Schiele. \"Confidence-calibrated adversarial training: Generalizing to unseen attacks.\"\n\tInternational Conference on Machine Learning. PMLR, 2020.\n\tBased on the official code from: https://github.com/davidstutz/confidence-calibrated-adversarial-training\n\t\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport numpy as np\n", "import torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n\timport models\n\timport json\n\timport functools\n\tfrom imgaug import augmenters as iaa\n", "import utils.imgaug_lib\n\tfrom attacks.objectives import UntargetedObjective\n\tfrom attacks.mb_pgd_attack import MBLinfPGDAttack\n\tfrom utils.dataset import CustomDataset\n\tdef test(model, attacker, test_dataloader, max_batches, logger):\n\t    model.eval()\n\t    clean_losses = None\n\t    clean_accs = None\n\t    clean_confs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n", "        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n\t            outputs = model(inputs)\n\t        clean_losses = utils.numpy.concatenate(clean_losses, utils.torch.classification_loss(outputs, targets, reduction='none').detach().cpu().numpy())\n\t        clean_accs = utils.numpy.concatenate(clean_accs, 1 - utils.torch.classification_error(outputs, targets, reduction='none').detach().cpu().numpy())\n\t        clean_confs = utils.numpy.concatenate(clean_confs, torch.max(torch.nn.functional.softmax(outputs, dim=1), dim=1)[0].detach().cpu().numpy())\n\t    logger.info(\"clean_loss: {:.2f}, clean_acc: {:.2f}%, clean_conf: {:.4f}\".format(np.mean(clean_losses), np.mean(clean_accs)*100, np.mean(clean_confs)))\n\t    adv_losses = None\n\t    adv_accs = None\n", "    adv_confs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        if b >= max_batches:\n\t            break\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        # small perturbations\n\t        adv_inputs = attacker.perturb(inputs, targets)\n\t        with torch.no_grad():\n\t            logits = model(adv_inputs)\n", "        adv_losses = utils.numpy.concatenate(adv_losses, utils.torch.classification_loss(logits, targets, reduction='none').detach().cpu().numpy())\n\t        adv_accs = utils.numpy.concatenate(adv_accs, 1 - utils.torch.classification_error(logits, targets, reduction='none').detach().cpu().numpy())\n\t        adv_confs = utils.numpy.concatenate(adv_confs, torch.max(torch.nn.functional.softmax(logits, dim=1), dim=1)[0].detach().cpu().numpy())\n\t    logger.info(\"adv_loss: {:.2f}, adv_acc: {:.2f}%, adv_conf: {:.4f}\".format(np.mean(adv_losses), np.mean(adv_accs)*100, np.mean(adv_confs)))\n\tdef power_transition(perturbations, epsilon=0.3, gamma=1):\n\t    \"\"\"\n\t    Power transition rule.\n\t    :param perturbations: perturbations\n\t    :type perturbations: torch.autograd.Variable\n\t    :param norm: norm\n", "    :type norm: attacks.norms.Norm\n\t    :param epsilon: epsilon\n\t    :type epsilon: float\n\t    :param gamma: gamma\n\t    :type gamma: float\n\t    :return: gamma, norms\n\t    :rtype: torch.autograd.Variable, torch.autograd.Variable\n\t    \"\"\"\n\t    # returned value determines importance of uniform distribution:\n\t    # (1 - ret)*one_hot + ret*uniform\n", "    norms = torch.max(torch.abs(perturbations.view(perturbations.size()[0], -1)), dim=1)[0]\n\t    return 1 - torch.pow(1 - torch.min(torch.ones_like(norms), norms / epsilon), gamma), norms\n\tdef partial(f, *args, **kwargs):\n\t    \"\"\"\n\t    Create partial while preserving __name__ and __doc__.\n\t    :param f: function\n\t    :type f: callable\n\t    :param args: arguments\n\t    :type args: dict\n\t    :param kwargs: keyword arguments\n", "    :type kwargs: dict\n\t    :return: partial\n\t    :rtype: callable\n\t    \"\"\"\n\t    p = functools.partial(f, *args, **kwargs)\n\t    functools.update_wrapper(p, f)\n\t    return p\n\tdef train_robust(model, \n\t                train_dataloader,\n\t                attacker, \n", "                transition,\n\t                optimizer, \n\t                scheduler,\n\t                fraction,\n\t                N_class,\n\t                print_freq,\n\t                logger):\n\t    num_training_iter = len(train_dataloader)\n\t    for b, (inputs, targets) in enumerate(train_dataloader):\n\t        inputs = inputs.cuda()\n", "        targets = targets.cuda()\n\t        distributions = utils.torch.one_hot(targets, N_class)\n\t        split = int((1-fraction)*inputs.size(0))\n\t        # update fraction for correct loss computation\n\t        fraction = 1 - split / float(inputs.size(0))\n\t        clean_inputs = inputs[:split]\n\t        adv_inputs = inputs[split:]\n\t        clean_targets = targets[:split]\n\t        adv_targets = targets[split:]\n\t        clean_distributions = distributions[:split]\n", "        adv_distributions = distributions[split:]\n\t        adv_examples = attacker.perturb(adv_inputs, adv_targets)\n\t        if adv_inputs.shape[0] < inputs.shape[0]: # fraction is not 1\n\t            inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n\t        else:\n\t            inputs = adv_examples\n\t        gamma, adv_norms = transition(adv_examples - adv_inputs)\n\t        gamma = utils.torch.expand_as(gamma, adv_distributions)\n\t        adv_distributions = adv_distributions*(1 - gamma)\n\t        adv_distributions += gamma*torch.ones_like(adv_distributions)/N_class\n", "        model.train()\n\t        optimizer.zero_grad()\n\t        logits = model(inputs)\n\t        clean_logits = logits[:split]\n\t        adv_logits = logits[split:]\n\t        adv_loss = utils.torch.cross_entropy_divergence(adv_logits, adv_distributions)\n\t        adv_acc = 1 - utils.torch.classification_error(adv_logits, adv_targets)\n\t        adv_conf = torch.max(torch.nn.functional.softmax(adv_logits, dim=1), dim=1)[0]\n\t        if adv_inputs.shape[0] < inputs.shape[0]:\n\t            clean_loss = utils.torch.cross_entropy_divergence(clean_logits, clean_distributions)\n", "            clean_acc = 1 - utils.torch.classification_error(clean_logits, clean_targets)\n\t            clean_conf = torch.max(torch.nn.functional.softmax(clean_logits, dim=1), dim=1)[0]\n\t            loss = (1 - fraction) * clean_loss + fraction * adv_loss\n\t        else:\n\t            clean_loss = torch.zeros(1)\n\t            clean_acc = torch.ones(1)\n\t            loss = adv_loss\n\t        loss.backward()\n\t        optimizer.step()\n\t        scheduler.step()\n", "        if (b+1) % print_freq == 0:\n\t            logger.info(\"Progress: {:d}/{:d}, clean_loss: {:.2f}, adv_loss: {:.2f}, clean_acc: {:.2f}%, clean_conf: {:.4f}, adv_acc: {:.2f}%, adv_conf: {:.4f}\".format(b+1, \n\t                                                                                                                                num_training_iter, \n\t                                                                                                                                clean_loss.item(),\n\t                                                                                                                                adv_loss.item(),\n\t                                                                                                                                clean_acc.item()*100,\n\t                                                                                                                                clean_conf.mean().item(),\n\t                                                                                                                                adv_acc.item()*100,\n\t                                                                                                                                adv_conf.mean().item()\n\t                                                                                                                                ))\n", "def get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', type=int, default=0)\n\t    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n\t    # Set random seed\n", "    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n\t        config = json.load(config_file)\n\t    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    lr = config['lr']\n\t    optimizer_name = config['optimizer_name']\n\t    epsilon = config['epsilon']\n\t    gamma = config['gamma']\n\t    attack_base_lr = config['attack_base_lr']\n", "    max_iterations = config['max_iterations']\n\t    fraction = config['fraction']\n\t    print_freq = config['print_freq']\n\t    checkpoint_freq = config['checkpoint_freq']\n\t    max_batches = config['max_batches']\n\t    nepoch = config['nepoch']\n\t    batch_size = config['batch_size']\n\t    output_dir = args.output_dir\n\t    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n", "    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n\t        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n\t    logger.info(args)\n", "    logger.info(config)\n\t    if dataset == 'cifar10':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n", "        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'gtsrb':\n\t        N_class = 43\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n", "        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'svhn':\n", "        N_class = 10\n\t        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n\t        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n", "                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n", "        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'mnist':\n\t        N_class = 10\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n", "        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n\t    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n\t    else:\n\t        raise ValueError\n", "    model.cuda()\n\t    if optimizer_name == 'SGD':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\t        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n\t    else:\n\t        raise ValueError\n\t    objective = UntargetedObjective(loss=utils.torch.f7p_loss)\n\t    attacker = MBLinfPGDAttack(model, \n\t                                objective, \n\t                                epsilon=epsilon, \n", "                                max_iterations=max_iterations, \n\t                                base_lr=attack_base_lr, \n\t                                momentum=0.9, \n\t                                lr_factor=1.5, \n\t                                backtrack=True, \n\t                                rand_init_name=\"random+zero\",\n\t                                num_rand_init=1,\n\t                                clip_min=0.0,\n\t                                clip_max=1.0)\n\t    transition = partial(power_transition, epsilon=epsilon, gamma=gamma)\n", "    for epoch in range(nepoch):\n\t        logger.info(\"Epoch: {:d}\".format(epoch))\n\t        train_robust(model, \n\t                    train_dataloader,\n\t                    attacker,\n\t                    transition,\n\t                    optimizer, \n\t                    scheduler,\n\t                    fraction,\n\t                    N_class,\n", "                    print_freq,\n\t                    logger)\n\t        test(model, attacker, test_dataloader, max_batches, logger)\n\t        if (epoch+1) % checkpoint_freq == 0:\n\t            torch.save({\n\t                'epoch': epoch + 1,\n\t                'model': model.state_dict(),\n\t                'optimizer': optimizer.state_dict(),\n\t            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\t    torch.save({\n", "            'epoch': epoch + 1,\n\t            'model': model.state_dict(),\n\t            'optimizer': optimizer.state_dict(),\n\t        }, os.path.join(output_dir, 'classifier.pth.tar'))\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "train_rcd.py", "chunked_list": ["\"\"\"\n\tMain script for training the baseline method RCD:\n\tProvably robust classification of adversarial examples with detection, Fatemeh Sheikholeslami, Ali Lotfi Rezaabad,\n\tZico Kolter (https://openreview.net/pdf?id=sRA5rLNpmQc), ICLR 2021\n\t\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n", "from torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n\timport models\n\timport json\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom attacks.objectives import UntargetedObjective\n", "from attacks.mb_pgd_attack import MBRCDStratifiedLinfPGDAttack\n\tfrom utils.dataset import CustomDataset\n\tdef test(model, attacker, test_dataloader, N_class, max_batches, logger):\n\t    model.eval()\n\t    clean_accs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n\t            outputs = model(inputs)\n", "        clean_error = utils.torch.classification_error(outputs, targets, reduction='none')\n\t        clean_accs = utils.numpy.concatenate(clean_accs, 1 - clean_error.detach().cpu().numpy())\n\t    logger.info(\"clean acc: {:.2%}\".format(np.mean(clean_accs)))\n\t    adv_accs_1 = None\n\t    adv_accs_2 = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        if b >= max_batches:\n\t            break\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n", "        # Generate adversarial examples\n\t        adv_inputs = attacker.perturb(inputs, targets)\n\t        with torch.no_grad():\n\t            adv_logits = model(adv_inputs)\n\t        adv_split = int(attacker.fraction * adv_logits.size(0))\n\t        adv_logits_1 = adv_logits[:adv_split]\n\t        adv_targets_1 = targets[:adv_split]\n\t        adv_logits_2 = adv_logits[adv_split:]\n\t        adv_targets_2 = targets[adv_split:]\n\t        if attacker.fraction > 0.0:\n", "            adv_acc_1 = 1 - utils.torch.classification_error(adv_logits_1, adv_targets_1, reduction=\"none\")\n\t            adv_accs_1 = utils.numpy.concatenate(adv_accs_1, adv_acc_1.detach().cpu().numpy())\n\t        if attacker.fraction < 1.0:\n\t            adv_d_error_2 = utils.torch.classification_error(adv_logits_2, torch.ones_like(adv_targets_2) * N_class, reduction=\"none\")\n\t            adv_error_2 = utils.torch.classification_error(adv_logits_2, adv_targets_2, reduction=\"none\")\n\t            adv_acc_2 = 1 - torch.min(torch.stack([adv_error_2, adv_d_error_2], dim=1), dim=1)[0].float()\n\t            adv_accs_2 = utils.numpy.concatenate(adv_accs_2, adv_acc_2.detach().cpu().numpy())\n\t    if adv_accs_1 is None:\n\t        adv_accs_1 = np.ones(1)\n\t    if adv_accs_2 is None:\n", "        adv_accs_2 = np.ones(1)\n\t    logger.info(\"adv_acc_1: {:.2%}, adv_acc_2: {:.2%}\".format(np.mean(adv_accs_1), np.mean(adv_accs_2)))\n\tdef lr_schedule(t, lr_max):\n\t    if t < 100:\n\t        return lr_max\n\t    elif t < 105:\n\t        return lr_max / 10.\n\t    else:\n\t        return lr_max / 100.\n\tdef train_robust_detection(model,\n", "                           train_dataloader,\n\t                           attacker,\n\t                           optimizer,\n\t                           scheduler,\n\t                           fraction,\n\t                           lamb_1,\n\t                           lamb_2,\n\t                           epoch,\n\t                           lr_max,\n\t                           N_class,\n", "                           print_freq,\n\t                           logger):\n\t    num_training_iter = len(train_dataloader)\n\t    for b, (inputs, targets) in enumerate(train_dataloader):\n\t        if scheduler is None:\n\t            epoch_now = epoch + (b + 1) / num_training_iter\n\t            lr = lr_schedule(epoch_now, lr_max)\n\t            optimizer.param_groups[0].update(lr=lr)\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n", "        b_size = inputs.size(0)\n\t        split = int((1-fraction) * b_size)\n\t        # update fraction for correct loss computation\n\t        fraction = 1 - split / float(b_size)\n\t        clean_inputs = inputs[:split]\n\t        adv_inputs = inputs[split:]\n\t        clean_targets = targets[:split]\n\t        adv_targets = targets[split:]\n\t        double_adv_inputs = torch.cat((adv_inputs, clean_inputs), 0)\n\t        double_adv_targets = torch.cat((adv_targets, clean_targets), 0)\n", "        # Generate adversarial examples\n\t        adv_examples = attacker.perturb(double_adv_inputs, double_adv_targets)\n\t        if adv_inputs.shape[0] < b_size: # fraction is not 1\n\t            combined_inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n\t        else:\n\t            combined_inputs = adv_examples\n\t        model.train()\n\t        optimizer.zero_grad()\n\t        logits = model(combined_inputs)\n\t        clean_logits = logits[:split]\n", "        adv_logits = logits[split:]\n\t        adv_split = int(attacker.fraction * adv_logits.size(0))\n\t        if attacker.fraction > 0.0:\n\t            # robust loss\n\t            adv_logits_1 = adv_logits[:adv_split]\n\t            adv_targets_1 = double_adv_targets[:adv_split]\n\t            adv_loss_1 = utils.torch.classification_loss(adv_logits_1, adv_targets_1, reduction=\"mean\")\n\t            adv_acc_1 = 1 - utils.torch.classification_error(adv_logits_1, adv_targets_1, reduction=\"mean\")\n\t        else:\n\t            adv_loss_1 = torch.zeros(1)\n", "            adv_acc_1 = torch.ones(1)\n\t        if attacker.fraction < 1.0:\n\t            # robust abstain loss\n\t            adv_logits_2 = adv_logits[adv_split:]\n\t            adv_targets_2 = double_adv_targets[adv_split:]\n\t            adv_loss_2 = utils.torch.robust_abstain_loss(adv_logits_2, adv_targets_2, reduction=\"mean\")\n\t            adv_error_2 = utils.torch.classification_error(adv_logits_2, adv_targets_2, reduction=\"none\")\n\t            adv_d_error_2 = utils.torch.classification_error(adv_logits_2, torch.ones_like(adv_targets_2) * N_class, reduction=\"none\")\n\t            adv_acc_2 = 1 - torch.mean(torch.min(torch.stack([adv_error_2, adv_d_error_2], dim=1), dim=1)[0].float())\n\t        else:\n", "            adv_loss_2 = torch.zeros(1)\n\t            adv_acc_2 = torch.ones(1)\n\t        if adv_inputs.shape[0] < b_size:\n\t            # Loss on clean inputs\n\t            clean_loss = utils.torch.classification_loss(clean_logits, clean_targets, reduction=\"mean\")\n\t            clean_acc = 1 - utils.torch.classification_error(clean_logits, clean_targets, reduction=\"mean\")\n\t            # Combined loss\n\t            loss = fraction * (adv_loss_1 + lamb_1 * adv_loss_2) + (1 - fraction) * lamb_2 * clean_loss\n\t        else:\n\t            clean_acc = torch.ones(1)\n", "            loss = adv_loss_1 + lamb_1 * adv_loss_2\n\t        loss.backward()\n\t        optimizer.step()\n\t        if scheduler is not None:\n\t            scheduler.step()\n\t        if (b+1) % print_freq == 0:\n\t            logger.info(\"Progress: {:d}/{:d}, loss: {:.4f}, clean_acc: {:.2%}, adv_acc_1: {:.2%}, adv_acc_2: {:.2%}\".format(b+1,\n\t                                                                                                        num_training_iter,\n\t                                                                                                        loss.item(),\n\t                                                                                                        clean_acc.item(),\n", "                                                                                                        adv_acc_1.item(),\n\t                                                                                                        adv_acc_2.item()))\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', type=int, default=0)\n\t    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n", "    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n\t        config = json.load(config_file)\n\t    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    lr = config['lr']\n\t    optimizer_name = config['optimizer_name']\n\t    epsilon = config['epsilon']\n", "    eps_iter = config['eps_iter']\n\t    nb_iter = config['nb_iter']\n\t    print_freq = config['print_freq']\n\t    checkpoint_freq = config['checkpoint_freq']\n\t    max_batches = config['max_batches']\n\t    # fraction of clean inputs\n\t    fraction = config['fraction']\n\t    # adv_fraction: fraction of robust abstain examples\n\t    adv_fraction = config['adv_fraction']\n\t    lamb_1 = config['lamb_1']\n", "    lamb_2 = config['lamb_2']\n\t    nepoch = config['nepoch']\n\t    batch_size = config['batch_size']\n\t    output_dir = args.output_dir\n\t    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n", "        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n\t    logger.info(args)\n\t    logger.info(config)\n\t    if dataset == 'cifar10':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n", "        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n", "        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'gtsrb':\n\t        N_class = 43\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test.npz')\n\t        X_test = test_loaded['images']\n", "        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'svhn':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n", "        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n\t        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n", "            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n", "        eps_iter /= 255.\n\t    elif dataset == 'mnist':\n\t        N_class = 10\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n\t        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n", "    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class+1, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class+1, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNet(N_class+1, resolution, depth=28, width=10)\n\t    else:\n\t        raise ValueError\n\t    model.cuda()\n", "    if optimizer_name == 'SGD-pp':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n\t        scheduler = None\n\t    elif optimizer_name == 'SGD':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\t        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n\t    else:\n\t        raise ValueError\n\t    if fraction >= 1.0:\n\t        raise ValueError(\"Fraction of clean inputs should be less than 1\")\n", "    objective_1 = UntargetedObjective(loss=utils.torch.classification_loss)\n\t    objective_2 = UntargetedObjective(loss=utils.torch.robust_abstain_loss)\n\t    attacker = MBRCDStratifiedLinfPGDAttack(model,\n\t                                objective_1,\n\t                                objective_2,\n\t                                fraction=adv_fraction,\n\t                                epsilon=epsilon,\n\t                                max_iterations=nb_iter,\n\t                                base_lr=eps_iter,\n\t                                momentum=0.0,\n", "                                lr_factor=1.5,\n\t                                backtrack=False,\n\t                                rand_init_name=\"random+zero\",\n\t                                num_rand_init=1,\n\t                                clip_min=0.0,\n\t                                clip_max=1.0)\n\t    for epoch in range(nepoch):\n\t        logger.info(\"Epoch: {:d}\".format(epoch))\n\t        train_robust_detection(model,\n\t                            train_dataloader,\n", "                            attacker,\n\t                            optimizer,\n\t                            scheduler,\n\t                            fraction,\n\t                            lamb_1,\n\t                            lamb_2,\n\t                            epoch,\n\t                            lr,\n\t                            N_class,\n\t                            print_freq,\n", "                            logger)\n\t        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\t        if (epoch+1) % checkpoint_freq == 0:\n\t            torch.save({\n\t                'epoch': epoch + 1,\n\t                'model': model.state_dict(),\n\t                'optimizer': optimizer.state_dict(),\n\t            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\t    torch.save({\n\t            'epoch': epoch + 1,\n", "            'model': model.state_dict(),\n\t            'optimizer': optimizer.state_dict(),\n\t        }, os.path.join(output_dir, 'classifier.pth.tar'))\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "eval_cpr.py", "chunked_list": ["\"\"\"\n\tMain script for evaluating the proposed method CPR.\n\t\"\"\"\n\tfrom functools import partial\n\timport argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n\tfrom torch.utils.data import DataLoader\n", "from torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n\timport models\n\timport json\n\timport math\n\tfrom attacks.objectives import *\n\tfrom attacks.mb_pgd_attack import *\n\tfrom utils.dataset import CustomDataset\n", "from utils.constants import *\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom autoattack import AutoAttack\n\tclass ConsistentRejDefense(torch.nn.Module):\n\t    def __init__(self, model, defense_attack_params):\n\t        super(ConsistentRejDefense, self).__init__()\n\t        self.model = model\n\t        objective = UntargetedObjective(loss=utils.torch.classification_loss)\n\t        self.defense_attacker = MBLinfPGDAttack(model,\n", "                                    objective,\n\t                                    **defense_attack_params)\n\t    def transform(self, inputs):\n\t        with torch.no_grad():\n\t            logits = self.model(inputs)\n\t        pred_labels = torch.max(logits, axis=1)[1]\n\t        adv_inputs = self.defense_attacker.perturb(inputs, pred_labels)\n\t        return adv_inputs\n\t    def forward(self, inputs):\n\t        with torch.no_grad():\n", "            logits = self.model(inputs)\n\t        pred_labels = torch.max(logits, axis=1)[1]\n\t        adv_inputs = self.defense_attacker.perturb(inputs, pred_labels)\n\t        with torch.no_grad():\n\t            adv_logits = self.model(adv_inputs)\n\t        adv_pred_labels = torch.max(adv_logits, axis=1)[1]\n\t        adv_confs = (pred_labels==adv_pred_labels).float()\n\t        return logits, adv_confs\n\tdef eval_outer_attack(attack_method,\n\t                      outer_attack_config,\n", "                      model,\n\t                      test_dataloader,\n\t                      num_classes,\n\t                      threshold,\n\t                      epsilon,\n\t                      defense_attack_params,\n\t                      validation, \n\t                      total_samples,\n\t                      n_samples=N_SAMPLES):\n\t    model.eval()\n", "    defense = ConsistentRejDefense(model, defense_attack_params)\n\t    if attack_method == 'auto_attack':\n\t        attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n\t    elif attack_method == 'HCMOA':\n\t        objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n\t        attacker = MBConfLinfPGDAttackMultitargeted(model,\n\t                                                    objective,\n\t                                                    num_classes,\n\t                                                    epsilon=epsilon,\n\t                                                    **outer_attack_config)\n", "    elif attack_method == 'CHCMOA':\n\t        objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n\t        attacker = MBCONSRLinfPGDAttackMultitargeted(model,\n\t                                                defense,\n\t                                                objective,\n\t                                                num_classes,\n\t                                                epsilon=epsilon,\n\t                                                **outer_attack_config)\n\t    else:\n\t        raise KeyError(f'Not supported attack method {attack_method}')\n", "    adv_labels = None\n\t    adv_probs = None\n\t    adv_confs = None\n\t    cnt = 0\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        if validation and cnt < total_samples - n_samples:\n\t            # If validation: the last `n_samples` are used for evaluating the robustness with rejection\n\t            cnt += inputs.shape[0]\n", "            continue\n\t        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\t        # large perturbations\n\t        if attack_method == 'auto_attack':\n\t            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n\t        else:\n\t            adv_inputs = attacker.perturb(inputs, targets)\n\t        adv_logits, batch_adv_conf = defense(adv_inputs)\n\t        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t        adv_confs = utils.numpy.concatenate(adv_confs, batch_adv_conf.detach().cpu().numpy())\n", "        cnt += inputs.shape[0]\n\t        if (not validation) and cnt >= n_samples:\n\t            # If not validation: the first `n_samples` are used for evaluating the robustness with rejection\n\t            break\n\t    # Error on adversarial inputs: accept and misclassify\n\t    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n\t    adv_error = np.logical_and(adv_preds != adv_labels, adv_confs >= threshold)\n\t    return adv_error\n\tdef eval_inner_attack(attack_method,\n\t                      inner_attack_config,\n", "                      model,\n\t                      test_dataloader,\n\t                      num_classes,\n\t                      threshold,\n\t                      epsilon,\n\t                      eps0_range,\n\t                      defense_attack_params,\n\t                      validation, \n\t                      total_samples,\n\t                      n_samples=N_SAMPLES):\n", "    model.eval()\n\t    defense = ConsistentRejDefense(model, defense_attack_params)\n\t    inner_attack_results = {}\n\t    for epsilon_0 in eps0_range:\n\t        if attack_method == 'LCIA':\n\t            radius_objective = UntargetedObjective(loss=utils.torch.uniform_confidence_loss)\n\t        elif attack_method == 'CLCIA':\n\t            radius_objective = UntargetedObjective(loss=utils.torch.uniform_confidence_loss)\n\t        elif attack_method == 'PDIA':\n\t            radius_objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n", "        else:\n\t            raise KeyError(f'Not supported attack method {attack_method}')\n\t        if attack_method == 'CLCIA':\n\t            radius_attacker = MBCONSRLinfPGDAttack(model,\n\t                                                defense,\n\t                                                radius_objective,\n\t                                                epsilon=epsilon_0,\n\t                                                **inner_attack_config)\n\t        elif attack_method == 'PDIA':\n\t            radius_attacker = MBCONSRLinfPGDInnerAttackMultitargeted(model,\n", "                                                defense,\n\t                                                radius_objective,\n\t                                                num_classes=num_classes,\n\t                                                epsilon=epsilon_0,\n\t                                                **inner_attack_config)\n\t        else:\n\t            radius_attacker = MBConfLinfPGDAttack(model,\n\t                                                radius_objective,\n\t                                                epsilon=epsilon_0,\n\t                                                **inner_attack_config)\n", "        cnt = 0\n\t        adv_probs = None\n\t        adv_confs = None\n\t        for b, (inputs, targets) in enumerate(test_dataloader):\n\t            inputs = inputs.cuda()\n\t            targets = targets.cuda()\n\t            if validation and cnt < total_samples - n_samples:\n\t                cnt += inputs.shape[0]\n\t                continue\n\t            # small perturbations\n", "            if epsilon_0 > 0.:\n\t                adv_inputs = radius_attacker.perturb(inputs, targets)\n\t            else:\n\t                adv_inputs = inputs\n\t            adv_logits, batch_adv_conf = defense(adv_inputs)\n\t            adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t            adv_confs = utils.numpy.concatenate(adv_confs, batch_adv_conf.detach().cpu().numpy())\n\t            cnt += inputs.shape[0]\n\t            if (not validation) and cnt >= n_samples:\n\t                break\n", "        inner_attack_results[epsilon_0] = adv_confs < threshold\n\t    return inner_attack_results\n\tdef eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n\t    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n\t    curve_x = []\n\t    curve_y = []\n\t    for epsilon_0 in inner_attack_results:\n\t        curve_x.append(epsilon_0 / epsilon)\n\t        adv_error_2 = inner_attack_results[epsilon_0]\n\t        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n", "        curve_y.append(1 - final_adv_error)\n\t        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n\t    return np.array(curve_x), np.array(curve_y)\n\tdef eval(model, test_dataloader, tpr, defense_attack_params, logger):\n\t    model.eval()\n\t    defense = ConsistentRejDefense(model, defense_attack_params)\n\t    clean_probs = None\n\t    clean_confs = None\n\t    clean_labels = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n", "        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        outputs, batch_conf = defense(inputs)\n\t        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n\t        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\t        clean_confs = utils.numpy.concatenate(clean_confs, batch_conf.detach().cpu().numpy())\n\t    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n\t    val_probs = clean_probs[test_N:]\n\t    val_labels = clean_labels[test_N:]\n\t    val_confs = clean_confs[test_N:]\n", "    test_probs = clean_probs[:test_N]\n\t    test_labels = clean_labels[:test_N]\n\t    test_confs = clean_confs[:test_N]\n\t    # Find the confidence threshold using a validation set\n\t    val_preds = np.argmax(val_probs, axis=1)\n\t    val_errors = (val_preds != val_labels)\n\t    val_acc = 1. - np.mean(val_errors)\n\t    threshold = 0.5\n\t    val_mask_accept = (val_confs >= threshold)\n\t    val_acc_with_detection = 1. - np.sum(val_errors & val_mask_accept) / np.sum(val_mask_accept)\n", "    val_rejection_rate = 1. - np.mean(val_mask_accept)\n\t    logger.info(f\"threshold: {threshold:.4f}\")\n\t    logger.info(f\"clean val accuracy: {val_acc:.2%}\")\n\t    logger.info(f\"clean val accuracy with detection: {val_acc_with_detection:.2%}, clean val rejection rate: {val_rejection_rate:.2%}\")\n\t    test_preds = np.argmax(test_probs, axis=1)\n\t    test_errors = (test_preds != test_labels)\n\t    test_acc = 1. - np.mean(test_errors)\n\t    # Clean accuracy within the accepted inputs\n\t    test_mask_accept = (test_confs >= threshold)\n\t    test_acc_with_detection = 1. - np.sum(test_errors & test_mask_accept) / np.sum(test_mask_accept)\n", "    test_rejection_rate = 1. - np.mean(test_mask_accept)\n\t    logger.info(f\"clean test accuracy: {test_acc:.2%}\")\n\t    logger.info(f\"clean test accuracy with detection: {test_acc_with_detection:.2%}, clean test rejection rate: {test_rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-test_rejection_rate)/(test_acc_with_detection+1-test_rejection_rate):.2%}\")\n\t    return threshold\n\tdef combine_outer_attack_results(combine_adv_error, curr_adv_error):\n\t    if combine_adv_error is None:\n\t        return curr_adv_error\n\t    else:\n\t        return curr_adv_error | combine_adv_error\n\tdef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n", "    if combine_inner_results is None:\n\t        return curr_inner_results\n\t    for eps0 in curr_inner_results:\n\t        combine_inner_results[eps0] |= curr_inner_results[eps0]\n\t    return combine_inner_results\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='Evaluate the robustness with rejection of CCAT')\n\t    parser.add_argument('--seed', type=int, default=0)\n\t    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n", "    parser.add_argument('--validation', action='store_true', help='whether to use validation set to select hyper-parameters')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n\t        config = json.load(config_file)\n\t    dataset = config['dataset']\n", "    model_arch = config['model_arch']\n\t    epsilon = config['epsilon']\n\t    batch_size = config['batch_size']\n\t    checkpoint_dir = args.checkpoint_dir\n\t    validation = args.validation\n\t    N_class = N_CLASSES[dataset]\n\t    tpr = TPR_THRESHOLD[dataset]\n\t    if dataset == 'cifar10':\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n", "        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n", "        defense_attack_params = {'epsilon': 0.0055,\n\t                             'max_iterations': 10,\n\t                             'base_lr': 0.001,\n\t                             'momentum': 0.0,\n\t                             'lr_factor': 1.5,\n\t                             'backtrack': False,\n\t                             'rand_init_name': \"zero\",\n\t                             'num_rand_init': 1,\n\t                             'clip_min': 0.0,\n\t                             'clip_max': 1.0}\n", "    elif dataset == 'gtsrb':\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n", "        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        defense_attack_params = {'epsilon': 0.0055,\n\t                             'max_iterations': 10,\n\t                             'base_lr': 0.001,\n\t                             'momentum': 0.0,\n\t                             'lr_factor': 1.5,\n\t                             'backtrack': False,\n\t                             'rand_init_name': \"zero\",\n", "                             'num_rand_init': 1,\n\t                             'clip_min': 0.0,\n\t                             'clip_max': 1.0}\n\t    elif dataset == 'svhn':\n\t        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n", "        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n", "        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        defense_attack_params = {'epsilon': 0.0055,\n\t                             'max_iterations': 10,\n\t                             'base_lr': 0.001,\n\t                             'momentum': 0.0,\n\t                             'lr_factor': 1.5,\n", "                             'backtrack': False,\n\t                             'rand_init_name': \"zero\",\n\t                             'num_rand_init': 1,\n\t                             'clip_min': 0.0,\n\t                             'clip_max': 1.0}\n\t    elif dataset == 'mnist':\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n", "        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        defense_attack_params = {'epsilon': 0.1,\n\t                             'max_iterations': 20,\n\t                             'base_lr': 0.01,\n\t                             'momentum': 0.0,\n\t                             'lr_factor': 1.5,\n\t                             'backtrack': False,\n\t                             'rand_init_name': \"zero\",\n\t                             'num_rand_init': 1,\n\t                             'clip_min': 0.0,\n", "                             'clip_max': 1.0}\n\t    else:\n\t        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\t    output_dir = checkpoint_dir.replace('checkpoints/', 'results/consistent_rejection_v3/')\n\t    defense_epsilon = defense_attack_params['epsilon']\n\t    defense_iter = defense_attack_params['max_iterations']\n\t    if defense_iter==10:\n\t        output_dir = os.path.join(output_dir, f'defense_eps_{defense_epsilon}')\n\t    else:\n\t        output_dir = os.path.join(output_dir, f'defense_eps_{defense_epsilon}_iter_{defense_iter}')\n", "    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n\t        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n\t            logging.StreamHandler()\n", "        ])\n\t    logger.info(args)\n\t    logger.info(config)\n\t    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n\t    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n", "        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n\t    else:\n\t        raise ValueError\n\t    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n\t    model.load_state_dict(checkpoint['model'])\n\t    model.cuda()\n\t    # evaluate clean and get threshold\n\t    threshold = eval(model, test_dataloader, tpr,\n\t                    defense_attack_params=defense_attack_params,\n\t                    logger=logger)\n", "    n_test = len(test_dataset)\n\t    final_outer_adv_error = None\n\t    final_inner_attack_results = None\n\t    # evaluate robustness with rejection under AutoAttack\n\t    outer_attack_config = None\n\t    outer_attack_method = 'auto_attack'\n\t    outer_file_name = f\"outer_{outer_attack_method}\"\n\t    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t    outer_file_name += '_val_' if validation else ''\n\t    outer_file_name += \"_result.npy\"\n", "    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t    else:\n\t        outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                            outer_attack_config,\n\t                                            model,\n\t                                            test_dataloader,\n\t                                            N_class,\n\t                                            threshold,\n\t                                            epsilon,\n", "                                            defense_attack_params=defense_attack_params,\n\t                                            validation=validation, \n\t                                            total_samples=n_test,\n\t                                            n_samples=N_SAMPLES)\n\t    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n\t    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t    # evaluate robustness with rejection under BPDA multitargeted attack\n\t    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n\t    outer_attack_method = 'CHCMOA'\n\t    for base_lr in BASE_LR_RANGE[dataset]:\n", "        outer_attack_config.update({'base_lr': base_lr})\n\t        outer_file_name = f\"outer_{outer_attack_method}\"\n\t        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n\t        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n\t        outer_file_name += '_val_' if validation else ''\n\t        outer_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t        else:\n", "            outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                                outer_attack_config,\n\t                                                model,\n\t                                                test_dataloader,\n\t                                                N_class,\n\t                                                threshold,\n\t                                                epsilon,\n\t                                                defense_attack_params=defense_attack_params,\n\t                                                validation=validation, \n\t                                                total_samples=n_test,\n", "                                                n_samples=N_SAMPLES)\n\t        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n\t        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t        logger.info(f'{outer_file_name} saved!')\n\t    # evaluate robustness with rejection under BPDA inner attack\n\t    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n\t    inner_attack_method = 'CLCIA'\n\t    for base_lr in BASE_LR_RANGE[dataset]:\n\t        inner_attack_config.update({'base_lr': base_lr})\n\t        inner_file_name = f\"inner_{inner_attack_method}\"\n", "        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n\t        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n\t        inner_file_name += '_val_' if validation else ''\n\t        inner_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n\t            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n\t        else:\n\t            inner_attack_results = eval_inner_attack(inner_attack_method,\n\t                                                    inner_attack_config,\n", "                                                    model,\n\t                                                    test_dataloader,\n\t                                                    N_class,\n\t                                                    threshold,\n\t                                                    epsilon,\n\t                                                    eps0_range,\n\t                                                    defense_attack_params=defense_attack_params,\n\t                                                    validation=validation, \n\t                                                    total_samples=n_test,\n\t                                                    n_samples=N_SAMPLES)\n", "        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n\t        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n\t        logger.info(f'{inner_file_name} saved!')\n\t    # evaluate robustness with rejection under BPDA inner multitargeted attack\n\t    inner_attack_config = CONFIG_MULTITARGET_ATTACK_INNER\n\t    inner_attack_method = 'PDIA'\n\t    for base_lr in BASE_LR_RANGE[dataset]:\n\t        inner_attack_config.update({'base_lr': base_lr})\n\t        inner_file_name = f\"inner_{inner_attack_method}\"\n\t        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n", "        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n\t        inner_file_name += '_val_' if validation else ''\n\t        inner_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n\t            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n\t        else:\n\t            inner_attack_results = eval_inner_attack(inner_attack_method,\n\t                                                    inner_attack_config,\n\t                                                    model,\n", "                                                    test_dataloader,\n\t                                                    N_class,\n\t                                                    threshold,\n\t                                                    epsilon,\n\t                                                    eps0_range,\n\t                                                    defense_attack_params=defense_attack_params,\n\t                                                    validation=validation, \n\t                                                    total_samples=n_test,\n\t                                                    n_samples=N_SAMPLES)\n\t        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n", "        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n\t        logger.info(f'{inner_file_name} saved!')\n\t    # evaluate robustness with rejection under multitargeted attack\n\t    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n\t    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n\t    outer_attack_method = 'HCMOA'\n\t    inner_attack_method = 'LCIA'\n\t    for base_lr in BASE_LR_RANGE[dataset]:\n\t        outer_attack_config.update({'base_lr': base_lr})\n\t        outer_file_name = f\"outer_{outer_attack_method}\"\n", "        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n\t        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n\t        outer_file_name += '_val_' if validation else ''\n\t        outer_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t        else:\n\t            outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                                outer_attack_config,\n", "                                                model,\n\t                                                test_dataloader,\n\t                                                N_class,\n\t                                                threshold,\n\t                                                epsilon,\n\t                                                defense_attack_params=defense_attack_params,\n\t                                                validation=validation, \n\t                                                total_samples=n_test,\n\t                                                n_samples=N_SAMPLES)\n\t        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n", "        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t        inner_attack_config.update({'base_lr': base_lr})\n\t        inner_file_name = f\"inner_{inner_attack_method}\"\n\t        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n\t        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n\t        inner_file_name += '_val_' if validation else ''\n\t        inner_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n\t            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n", "        else:\n\t            inner_attack_results = eval_inner_attack(inner_attack_method,\n\t                                                    inner_attack_config,\n\t                                                    model,\n\t                                                    test_dataloader,\n\t                                                    N_class,\n\t                                                    threshold,\n\t                                                    epsilon,\n\t                                                    eps0_range,\n\t                                                    defense_attack_params=defense_attack_params,\n", "                                                    validation=validation, \n\t                                                    total_samples=n_test,\n\t                                                    n_samples=N_SAMPLES)\n\t        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n\t        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n\t        logger.info(f'{outer_file_name} saved!')\n\t        logger.info(f'{inner_file_name} saved!')\n\t        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n\t    # Evaluate robustness curve\n\t    logger.info('Final results under an ensemble of attacks:')\n", "    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n\t    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n\t    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "eval_rcd.py", "chunked_list": ["\"\"\"\n\tMain script for evaluating the baseline method RCD:\n\tProvably robust classification of adversarial examples with detection, Fatemeh Sheikholeslami, Ali Lotfi Rezaabad,\n\tZico Kolter (https://openreview.net/pdf?id=sRA5rLNpmQc), ICLR 2021\n\t\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n", "from torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n\timport models\n\timport json\n\tfrom attacks.objectives import *\n\tfrom attacks.mb_pgd_attack import *\n\tfrom utils.dataset import CustomDataset\n", "from utils.constants import *\n\tfrom autoattack import AutoAttack\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom functools import partial\n\tdef get_classifier_output(inputs, model, num_classes):\n\t    return model(inputs)[:, :num_classes]\n\tdef eval_outer_attack(attack_method,\n\t                      outer_attack_config, \n\t                      model, \n", "                      test_dataloader, \n\t                      num_classes,\n\t                      epsilon, \n\t                      validation, \n\t                      total_samples, \n\t                      n_samples=N_SAMPLES):\n\t    model.eval()\n\t    if attack_method == 'pgd':\n\t        objective = UntargetedObjective(loss=utils.torch.rcd_accept_misclassify_loss)\n\t        attacker = MBConfLinfPGDAttack(model,\n", "                                     objective,\n\t                                     epsilon=epsilon,\n\t                                     **outer_attack_config)\n\t    elif attack_method == 'auto_attack':\n\t        attacker = AutoAttack(partial(get_classifier_output, model=model, num_classes=num_classes), \n\t                              norm='Linf', \n\t                              eps=epsilon, \n\t                              version='standard', \n\t                              verbose=False)\n\t    elif attack_method == 'RCDOA':\n", "        objective = UntargetedObjective(loss=utils.torch.rcd_targeted_loss)\n\t        attacker = MBRCDLinfPGDAttackMultitargeted(\n\t                                    model,\n\t                                    objective,\n\t                                    num_classes,\n\t                                    epsilon=epsilon,\n\t                                    **outer_attack_config)\n\t    else:\n\t        raise KeyError(f'Not supported attack method {attack_method}')\n\t    adv_labels = None\n", "    adv_probs = None\n\t    cnt = 0\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        if validation and cnt < total_samples - n_samples:\n\t            # If validation: the last `n_samples` are used for evaluating the robustness with rejection\n\t            cnt += inputs.shape[0]\n\t            continue\n\t        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n", "        # large perturbations\n\t        if attack_method == 'auto_attack':\n\t            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n\t        else:\n\t            adv_inputs = attacker.perturb(inputs, targets)\n\t        with torch.no_grad():\n\t            adv_logits = model(adv_inputs)\n\t        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t        cnt += inputs.shape[0]\n\t        if (not validation) and cnt >= n_samples:\n", "            # If not validation: the first `n_samples` are used for evaluating the robustness with rejection\n\t            break\n\t    # Error on adversarial inputs: accept and misclassify\n\t    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n\t    adv_error = np.logical_and(adv_preds != adv_labels, adv_preds != (np.ones_like(adv_labels) * num_classes))\n\t    return adv_error\n\tdef eval_inner_attack(attack_method,\n\t                      inner_attack_config, \n\t                      model, \n\t                      test_dataloader, \n", "                      num_classes,\n\t                      epsilon, \n\t                      eps0_range, \n\t                      validation, \n\t                      total_samples, \n\t                      n_samples=N_SAMPLES):\n\t    model.eval()\n\t    inner_attack_results = {}\n\t    for epsilon_0 in eps0_range:\n\t        if attack_method == 'RCDIA':\n", "            radius_objective = UntargetedObjective(loss=utils.torch.rcd_reject_loss)\n\t            radius_attacker = MBConfLinfPGDAttack(model,\n\t                                                radius_objective,\n\t                                                epsilon=epsilon_0,\n\t                                                **inner_attack_config)\n\t        else:\n\t            raise KeyError(f'Not supported attack method {attack_method}')\n\t        cnt = 0\n\t        adv_probs_2 = None\n\t        for b, (inputs, targets) in enumerate(test_dataloader):\n", "            inputs = inputs.cuda()\n\t            targets = targets.cuda()\n\t            if validation and cnt < total_samples - n_samples:\n\t                cnt += inputs.shape[0]\n\t                continue\n\t            # small perturbations\n\t            if epsilon_0 > 0.:\n\t                adv_inputs = radius_attacker.perturb(inputs, targets)\n\t            else:\n\t                adv_inputs = inputs\n", "            with torch.no_grad():\n\t                adv_logits = model(adv_inputs)\n\t            adv_probs_2 = utils.numpy.concatenate(adv_probs_2, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t            cnt += inputs.shape[0]\n\t            if (not validation) and cnt >= n_samples:\n\t                break\n\t        adv_preds_2 = np.argmax(adv_probs_2, axis=1)   # prediction confidence\n\t        adv_error_2 = (adv_preds_2 == num_classes) # regard rejection as error\n\t        inner_attack_results[epsilon_0] = adv_error_2\n\t    return inner_attack_results\n", "def eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n\t    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n\t    curve_x = []\n\t    curve_y = []\n\t    for epsilon_0 in inner_attack_results:\n\t        curve_x.append(epsilon_0/epsilon)\n\t        adv_error_2 = inner_attack_results[epsilon_0]\n\t        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n\t        curve_y.append(1 - final_adv_error)\n\t        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n", "    return np.array(curve_x), np.array(curve_y)\n\tdef eval(model, test_dataloader, N_class, logger):\n\t    model.eval()\n\t    clean_probs = None\n\t    clean_labels = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n\t            outputs = model(inputs)\n", "        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n\t        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\t    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n\t    val_probs = clean_probs[test_N:]\n\t    val_labels = clean_labels[test_N:]\n\t    test_probs = clean_probs[:test_N]\n\t    test_labels = clean_labels[:test_N]\n\t    test_preds = np.argmax(test_probs, axis=1)\n\t    test_errors = (test_preds != test_labels)\n\t    test_acc = 1. - np.mean(test_errors)\n", "    # Clean accuracy within the accepted inputs\n\t    mask_accept = (test_preds != (np.ones_like(test_labels) * N_class))\n\t    test_acc_with_detection = 1. - np.sum(test_errors & mask_accept) / np.sum(mask_accept)\n\t    rejection_rate = 1. - np.mean(mask_accept)\n\t    logger.info(f\"clean accuracy: {test_acc:.2%}\")\n\t    logger.info(f\"clean accuracy with detection: {test_acc_with_detection:.2%}, clean rejection rate: {rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-rejection_rate)/(test_acc_with_detection+1-rejection_rate):.2%}\")\n\tdef combine_outer_attack_results(combine_adv_error, curr_adv_error):\n\t    if combine_adv_error is None:\n\t        return curr_adv_error\n\t    else:\n", "        return curr_adv_error|combine_adv_error\n\tdef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n\t    if combine_inner_results is None:\n\t        return curr_inner_results\n\t    for eps0 in curr_inner_results:\n\t        combine_inner_results[eps0] |= curr_inner_results[eps0]\n\t    return combine_inner_results\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', type=int, default=0)\n", "    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n\t    parser.add_argument('--validation', action='store_true', help='whether to use validation set to select hyper-parameters')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n", "        config = json.load(config_file)\n\t    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    epsilon = config['epsilon']\n\t    batch_size = config['batch_size']\n\t    checkpoint_dir = args.checkpoint_dir\n\t    validation = args.validation\n\t    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n\t    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n", "    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n\t        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n\t    logger.info(args)\n", "    logger.info(config)\n\t    N_class = N_CLASSES[dataset]\n\t    if dataset == 'cifar10':\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n", "        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'gtsrb':\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n", "        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'svhn':\n\t        N_class = 10\n", "        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n\t        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n", "                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n", "        epsilon /= 255.\n\t    elif dataset == 'mnist':\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n\t        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\t    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n", "    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class+1, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class+1, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNet(N_class+1, resolution, depth=28, width=10)\n\t    else:\n\t        raise ValueError\n\t    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n", "    model.load_state_dict(checkpoint['model'])\n\t    model.cuda()\n\t    # evaluate clean and get threshold\n\t    eval(model, test_dataloader, N_class, logger)\n\t    # evaluate robustness\n\t    n_test = len(test_dataset)\n\t    final_outer_adv_error = None\n\t    final_inner_attack_results = None\n\t    # evaluate robustness with rejection under AutoAttack\n\t    outer_attack_config = None\n", "    outer_attack_method = 'auto_attack'\n\t    outer_file_name = f\"outer_{outer_attack_method}\"\n\t    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t    outer_file_name += '_val_' if validation else ''\n\t    outer_file_name += \"_result.npy\"\n\t    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t    else:\n\t        outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                            outer_attack_config, \n", "                                          model, \n\t                                          test_dataloader, \n\t                                          N_class,\n\t                                          epsilon, \n\t                                          validation, \n\t                                          total_samples=n_test, \n\t                                          n_samples=N_SAMPLES)\n\t    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n\t    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t    # evaluate robustness with rejection under multitargeted attack\n", "    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n\t    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n\t    outer_attack_method = 'RCDOA'\n\t    inner_attack_method = 'RCDIA'\n\t    for base_lr in BASE_LR_RANGE[dataset]:\n\t        outer_attack_config.update({'base_lr': base_lr})\n\t        outer_file_name = f\"outer_{outer_attack_method}\"\n\t        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n\t        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n", "        outer_file_name += '_val_' if validation else ''\n\t        outer_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t        else:\n\t            outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                                outer_attack_config, \n\t                                              model, \n\t                                              test_dataloader, \n\t                                              N_class,\n", "                                              epsilon, \n\t                                              validation, \n\t                                              total_samples=n_test, \n\t                                              n_samples=N_SAMPLES)\n\t        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n\t        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t        inner_attack_config.update({'base_lr': base_lr})\n\t        inner_file_name = f\"inner_{inner_attack_method}\"\n\t        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n\t        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n", "        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n\t        inner_file_name += '_val_' if validation else ''\n\t        inner_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n\t            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n\t        else:\n\t            inner_attack_results = eval_inner_attack(inner_attack_method,\n\t                                                     inner_attack_config, \n\t                                                      model, \n\t                                                      test_dataloader, \n", "                                                      N_class,\n\t                                                      epsilon, \n\t                                                      eps0_range, \n\t                                                      validation, \n\t                                                      total_samples=n_test, \n\t                                                      n_samples=N_SAMPLES)\n\t        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n\t        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n\t        logger.info(f'{outer_file_name} saved!')\n\t        logger.info(f'{inner_file_name} saved!')\n", "        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n\t    # Evaluate robustness curve\n\t    logger.info('Final results under an ensemble of attacks:')\n\t    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n\t    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n\t    if validation:\n\t        np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_val_result.npy\"), result_dict)\n\t    else:\n\t        np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "train_trades.py", "chunked_list": ["import argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n\timport torch.optim as optim\n\tfrom torch.utils.data import DataLoader, TensorDataset\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n", "import utils.numpy\n\timport models\n\timport json\n\tfrom attacks.objectives import TargetedObjective, UntargetedObjective\n\tfrom attacks.mb_pgd_attack import MBLinfPGDAttack\n\timport utils.dataset\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom utils.dataset import CustomDataset\n\timport utils.trades\n", "def test(model, attacker, test_dataloader, N_class, max_batches, logger):\n\t    model.eval()\n\t    clean_losses = None\n\t    clean_accs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n\t            outputs = model(inputs)\n\t        clean_losses = utils.numpy.concatenate(clean_losses, utils.torch.classification_loss(outputs, targets, reduction='none').detach().cpu().numpy())\n", "        clean_accs = utils.numpy.concatenate(clean_accs, 1 - utils.torch.classification_error(outputs, targets, reduction='none').detach().cpu().numpy())\n\t    logger.info(\"clean_loss: {:.2f}, clean_acc: {:.2f}%\".format(np.mean(clean_losses), np.mean(clean_accs)*100))\n\t    adv_losses = None\n\t    adv_accs = None\n\t    adv_successes = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        if b >= max_batches:\n\t            break\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n", "        # small perturbations\n\t        adv_inputs = attacker.perturb(inputs, targets)\n\t        with torch.no_grad():\n\t            logits = model(adv_inputs)\n\t        adv_losses = utils.numpy.concatenate(adv_losses, utils.torch.classification_loss(logits, targets, reduction='none').detach().cpu().numpy())\n\t        adv_accs = utils.numpy.concatenate(adv_accs, 1 - utils.torch.classification_error(logits, targets, reduction='none').detach().cpu().numpy())\n\t    logger.info(\"adv_loss: {:.2f}, adv_acc: {:.2f}%\".format(np.mean(adv_losses), np.mean(adv_accs)*100))\n\tdef adjust_learning_rate(dataset, optimizer, epoch, lr_max):\n\t    \"\"\"decrease the learning rate\"\"\"\n\t    lr = lr_max\n", "    if dataset=='cifar10':\n\t        if epoch >= 75:\n\t            lr = lr_max * 0.1\n\t        if epoch >= 90:\n\t            lr = lr_max * 0.01\n\t        if epoch >= 100:\n\t            lr = lr_max * 0.001\n\t    if dataset=='svhn':\n\t        if epoch >= 150:\n\t            lr = lr_max * 0.1\n", "        if epoch >= 180:\n\t            lr = lr_max * 0.01\n\t        if epoch >= 200:\n\t            lr = lr_max * 0.001\n\t    elif dataset=='gtsrb':\n\t        if epoch >= 40:\n\t            lr = lr_max * 0.1\n\t        if epoch >= 45:\n\t            lr = lr_max * 0.01\n\t    elif dataset=='mnist':\n", "        if epoch >= 55:\n\t            lr = lr_max * 0.1\n\t        if epoch >= 75:\n\t            lr = lr_max * 0.01\n\t        if epoch >= 90:\n\t            lr = lr_max * 0.001\n\t    else:\n\t        raise ValueError\n\t    for param_group in optimizer.param_groups:\n\t        param_group['lr'] = lr\n", "def train_trades(dataset,\n\t                model,\n\t                train_dataloader,\n\t                adv_configs,\n\t                optimizer,\n\t                scheduler,\n\t                N_class,\n\t                epoch,\n\t                lr_max,\n\t                print_freq,\n", "                logger):\n\t    num_training_iter = len(train_dataloader)\n\t    model.train()\n\t    for b, (inputs, targets) in enumerate(train_dataloader):\n\t        if scheduler is None:\n\t            adjust_learning_rate(dataset, optimizer, epoch, lr_max)\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        optimizer.zero_grad()\n\t        # calculate robust loss\n", "        loss = utils.trades.trades_loss(model=model,\n\t                           x_natural=inputs,\n\t                           y=targets,\n\t                           optimizer=optimizer,\n\t                           step_size=adv_configs['step_size'],\n\t                           epsilon=adv_configs['epsilon'],\n\t                           perturb_steps=adv_configs['num_steps'],\n\t                           beta=adv_configs['beta'])\n\t        loss.backward()\n\t        optimizer.step()\n", "        if scheduler is not None:\n\t            scheduler.step()\n\t        if (b+1) % print_freq == 0:\n\t            logger.info(\"Progress: {:d}/{:d}, adv_loss: {:.2f}\".format(b+1,\n\t                                                                       num_training_iter,\n\t                                                                       loss.item()))\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', type=int, default=0)\n\t    parser.add_argument('--config-file', type=str, required=True, help='config file')\n", "    parser.add_argument('--output-dir', type=str, required=True, help='output dir')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n\t        config = json.load(config_file)\n\t    dataset = config['dataset']\n", "    model_arch = config['model_arch']\n\t    lr = config['lr']\n\t    optimizer_name = config['optimizer_name']\n\t    epsilon = config['epsilon']\n\t    eps_iter = config['eps_iter']\n\t    nb_iter = config['nb_iter']\n\t    beta = config['beta']\n\t    fraction = config['fraction']\n\t    print_freq = config['print_freq']\n\t    checkpoint_freq = config['checkpoint_freq']\n", "    max_batches = config['max_batches']\n\t    nepoch = config['nepoch']\n\t    batch_size = config['batch_size']\n\t    output_dir = args.output_dir\n\t    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n", "        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n\t    logger.info(args)\n\t    logger.info(config)\n\t    if dataset == 'cifar10':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n", "        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n", "        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'cifar100':\n\t        N_class = 100\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n", "        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR100('./datasets/cifar100', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR100('./datasets/cifar100', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'gtsrb':\n\t        N_class = 43\n\t        resolution = (3, 32, 32)\n", "        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n", "        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'svhn':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n", "        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n", "        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'mnist':\n\t        N_class = 10\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n", "        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n", "    else:\n\t        raise ValueError\n\t    model.cuda()\n\t    if optimizer_name == 'SGD':\n\t        if dataset in ['cifar10', 'gtsrb', 'svhn']:\n\t            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=2e-4)\n\t        elif dataset == 'mnist':\n\t            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n\t        else:\n\t            raise ValueError\n", "        scheduler = None\n\t    else:\n\t        raise ValueError\n\t    adv_configs = {'step_size': eps_iter,\n\t                  'epsilon': epsilon,\n\t                  'num_steps': nb_iter,\n\t                  'beta': beta}\n\t    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n\t    attacker = MBLinfPGDAttack(model,\n\t                                objective,\n", "                                epsilon=epsilon,\n\t                                max_iterations=nb_iter,\n\t                                base_lr=eps_iter,\n\t                                momentum=0.0,\n\t                                lr_factor=1.5,\n\t                                backtrack=False,\n\t                                rand_init_name=\"random+zero\",\n\t                                num_rand_init=1,\n\t                                clip_min=0.0,\n\t                                clip_max=1.0)\n", "    for epoch in range(nepoch):\n\t        logger.info(\"Epoch: {:d}\".format(epoch))\n\t        train_trades(dataset,\n\t                    model,\n\t                    train_dataloader,\n\t                    adv_configs,\n\t                    optimizer,\n\t                    scheduler,\n\t                    N_class,\n\t                    epoch,\n", "                    lr,\n\t                    print_freq,\n\t                    logger)\n\t        test(model, attacker, test_dataloader, N_class, max_batches, logger)\n\t        if (epoch+1) % checkpoint_freq == 0:\n\t            torch.save({\n\t                'epoch': epoch + 1,\n\t                'model': model.state_dict(),\n\t                'optimizer': optimizer.state_dict(),\n\t            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n", "    torch.save({\n\t            'epoch': epoch + 1,\n\t            'model': model.state_dict(),\n\t            'optimizer': optimizer.state_dict(),\n\t        }, os.path.join(output_dir, 'classifier.pth.tar'))\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "eval_ccat.py", "chunked_list": ["\"\"\"\n\tMain script for evaluating the baseline method CCAT:\n\tStutz, David, Matthias Hein, and Bernt Schiele. \"Confidence-calibrated adversarial training: Generalizing to unseen attacks.\"\n\tInternational Conference on Machine Learning. PMLR, 2020.\n\tBased on the official code from: https://github.com/davidstutz/confidence-calibrated-adversarial-training\n\tAlso used for evaluating the baseline method Adversarial Training + Rejection.\n\t\"\"\"\n\tfrom functools import partial\n\timport argparse\n\timport logging\n", "import os\n\timport numpy as np\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n\timport models\n\timport json\n", "import math\n\tfrom attacks.objectives import *\n\tfrom attacks.mb_pgd_attack import *\n\tfrom utils.dataset import CustomDataset\n\tfrom utils.constants import *\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom autoattack import AutoAttack\n\tdef eval_outer_attack(attack_method,\n\t                      outer_attack_config, \n", "                      model, \n\t                      test_dataloader, \n\t                      num_classes,\n\t                      threshold, \n\t                      epsilon, \n\t                      n_samples=N_SAMPLES):\n\t    model.eval()\n\t    if attack_method == 'auto_attack':\n\t        attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n\t    elif attack_method == 'HCMOA':\n", "        objective = UntargetedObjective(loss=utils.torch.ccat_targeted_loss)\n\t        attacker = MBConfLinfPGDAttackMultitargeted(model,\n\t                                                    objective,\n\t                                                    num_classes,\n\t                                                    epsilon=epsilon,\n\t                                                    **outer_attack_config)\n\t    else:\n\t        raise KeyError(f'Not supported attack method {attack_method}')\n\t    adv_labels = None\n\t    adv_probs = None\n", "    cnt = 0\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\t        # large perturbations\n\t        if attack_method == 'auto_attack':\n\t            adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n\t        else:\n\t            adv_inputs = attacker.perturb(inputs, targets)\n", "        with torch.no_grad():\n\t            adv_logits = model(adv_inputs)\n\t        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t        cnt += inputs.shape[0]\n\t        if cnt >= n_samples:\n\t            break\n\t    # Error on adversarial inputs: accept and misclassify\n\t    adv_confs = np.max(adv_probs, axis=1)   # prediction confidence\n\t    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n\t    adv_error = np.logical_and(adv_preds != adv_labels, adv_confs >= threshold)\n", "    return adv_error\n\tdef eval_inner_attack(attack_method,\n\t                      inner_attack_config, \n\t                      model, \n\t                      test_dataloader, \n\t                      threshold, \n\t                      epsilon, \n\t                      eps0_range, \n\t                      n_samples=N_SAMPLES):\n\t    model.eval()\n", "    inner_attack_results = {}\n\t    for epsilon_0 in eps0_range:\n\t        if attack_method == 'LCIA':\n\t            radius_objective = UntargetedObjective(loss=utils.torch.uniform_confidence_loss)\n\t        else:\n\t            raise KeyError(f'Not supported attack method {attack_method}')\n\t        radius_attacker = MBConfLinfPGDAttack(model,\n\t                                              radius_objective,\n\t                                              epsilon=epsilon_0,\n\t                                              **inner_attack_config)\n", "        cnt = 0\n\t        adv_probs_2 = None\n\t        for b, (inputs, targets) in enumerate(test_dataloader):\n\t            inputs = inputs.cuda()\n\t            targets = targets.cuda()\n\t            # small perturbations\n\t            if epsilon_0 > 0.:\n\t                adv_inputs = radius_attacker.perturb(inputs, targets)\n\t            else:\n\t                adv_inputs = inputs\n", "            with torch.no_grad():\n\t                adv_logits = model(adv_inputs)\n\t            adv_probs_2 = utils.numpy.concatenate(adv_probs_2, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t            cnt += inputs.shape[0]\n\t            if cnt >= n_samples:\n\t                break\n\t        adv_confs_2 = np.max(adv_probs_2, axis=1)   # prediction confidence\n\t        adv_error_2 = adv_confs_2 < threshold   # regard rejection as error\n\t        inner_attack_results[epsilon_0] = adv_error_2\n\t    return inner_attack_results\n", "def eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger):\n\t    logger.info(f\"robustness with reject: {1-np.mean(outer_adv_error):.2%}\")\n\t    curve_x = []\n\t    curve_y = []\n\t    for epsilon_0 in inner_attack_results:\n\t        curve_x.append(epsilon_0 / epsilon)\n\t        adv_error_2 = inner_attack_results[epsilon_0]\n\t        final_adv_error = np.mean(np.logical_or(outer_adv_error, adv_error_2))\n\t        curve_y.append(1 - final_adv_error)\n\t        logger.info(f\"eps0: {epsilon_0}, rejection rate: {np.mean(adv_error_2):.2%}, robustness:{1-final_adv_error:.2%}\")\n", "    return np.array(curve_x), np.array(curve_y)\n\tdef eval(model, test_dataloader, tpr, logger):\n\t    model.eval()\n\t    clean_probs = None\n\t    clean_labels = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n\t            outputs = model(inputs)\n", "        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n\t        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\t    test_N = int(clean_probs.shape[0] * (1 - VAL_RATIO))\n\t    val_probs = clean_probs[test_N:]\n\t    val_labels = clean_labels[test_N:]\n\t    test_probs = clean_probs[:test_N]\n\t    test_labels = clean_labels[:test_N]\n\t    # Find the confidence threshold using a validation set\n\t    val_confs = np.max(val_probs, axis=1)\n\t    val_preds = np.argmax(val_probs, axis=1)\n", "    val_errors = (val_preds != val_labels)\n\t    # sorted_val_confs = np.sort(val_confs)\n\t    sorted_val_confs = np.sort(val_confs[np.logical_not(val_errors)])   # sorted confidence of correctly classified\n\t    cutoff = math.floor(sorted_val_confs.shape[0] * round((1. - tpr), 2))\n\t    threshold = sorted_val_confs[cutoff]\n\t    test_confs = np.max(test_probs, axis=1)\n\t    test_preds = np.argmax(test_probs, axis=1)\n\t    test_errors = (test_preds != test_labels)\n\t    test_acc = 1. - np.mean(test_errors)\n\t    # Clean accuracy within the accepted inputs\n", "    mask_accept = (test_confs >= threshold)\n\t    test_acc_with_detection = 1. - np.sum(test_errors & mask_accept) / np.sum(mask_accept)\n\t    rejection_rate = 1. - np.mean(mask_accept)\n\t    logger.info(f\"threshold: {threshold:.4f}\")\n\t    logger.info(f\"clean accuracy: {test_acc:.2%}\")\n\t    logger.info(f\"clean accuracy with detection: {test_acc_with_detection:.2%}, clean rejection rate: {rejection_rate:.2%}, F1 score: {2*test_acc_with_detection*(1-rejection_rate)/(test_acc_with_detection+1-rejection_rate):.2%}\")\n\t    return threshold\n\tdef combine_outer_attack_results(combine_adv_error, curr_adv_error):\n\t    if combine_adv_error is None:\n\t        return curr_adv_error\n", "    else:\n\t        return curr_adv_error | combine_adv_error\n\tdef combine_inner_attack_results(combine_inner_results, curr_inner_results):\n\t    if combine_inner_results is None:\n\t        return curr_inner_results\n\t    for eps0 in curr_inner_results:\n\t        combine_inner_results[eps0] |= curr_inner_results[eps0]\n\t    return combine_inner_results\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='Evaluate the robustness with rejection of CCAT')\n", "    parser.add_argument('--seed', type=int, default=0)\n\t    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n", "        config = json.load(config_file)\n\t    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    epsilon = config['epsilon']\n\t    batch_size = config['batch_size']\n\t    checkpoint_dir = args.checkpoint_dir\n\t    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n\t    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n", "    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n\t        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n\t    logger.info(args)\n\t    logger.info(config)\n", "    N_class = N_CLASSES[dataset]\n\t    tpr = TPR_THRESHOLD[dataset]\n\t    if dataset == 'cifar10':\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n", "        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'gtsrb':\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n", "        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'svhn':\n\t        N_class = 10\n", "        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n\t        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n", "                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n", "        epsilon /= 255.\n\t    elif dataset == 'mnist':\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n\t        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\t    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n", "    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n\t    else:\n\t        raise ValueError\n\t    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n", "    model.load_state_dict(checkpoint['model'])\n\t    model.cuda()\n\t    # evaluate clean and get threshold\n\t    threshold = eval(model, test_dataloader, tpr, logger)\n\t    final_outer_adv_error = None\n\t    final_inner_attack_results = None\n\t    # evaluate robustness with rejection under AutoAttack\n\t    outer_attack_config = None\n\t    outer_attack_method = 'auto_attack'\n\t    outer_file_name = f\"outer_{outer_attack_method}\"\n", "    outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t    outer_file_name += \"_result.npy\"\n\t    if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t        outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t    else:\n\t        outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                            outer_attack_config, \n\t                                            model,\n\t                                            test_dataloader,\n\t                                            N_class,\n", "                                            threshold,\n\t                                            epsilon,\n\t                                            n_samples=N_SAMPLES)\n\t    final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n\t    np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n\t    # evaluate robustness with rejection under multitargeted attack\n\t    outer_attack_config = CONFIG_MULTITARGET_ATTACK_OUTER\n\t    inner_attack_config = CONFIG_PGD_ATTACK_INNER\n\t    outer_attack_method = 'HCMOA'\n\t    inner_attack_method = 'LCIA'\n", "    for base_lr in BASE_LR_RANGE[dataset]:\n\t        outer_attack_config.update({'base_lr': base_lr})\n\t        outer_file_name = f\"outer_{outer_attack_method}\"\n\t        outer_file_name += '_bt' if outer_attack_config['backtrack'] else ''\n\t        outer_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        outer_file_name += f\"_lr_{outer_attack_config['base_lr']}\"\n\t        outer_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, outer_file_name)):\n\t            outer_adv_error = np.load(os.path.join(output_dir, outer_file_name), allow_pickle=True)\n\t        else:\n", "            outer_adv_error = eval_outer_attack(outer_attack_method,\n\t                                                outer_attack_config, \n\t                                                model,\n\t                                                test_dataloader,\n\t                                                N_class,\n\t                                                threshold,\n\t                                                epsilon,\n\t                                                n_samples=N_SAMPLES)\n\t        final_outer_adv_error = combine_outer_attack_results(final_outer_adv_error, outer_adv_error)\n\t        np.save(os.path.join(output_dir, outer_file_name), outer_adv_error)\n", "        inner_attack_config.update({'base_lr': base_lr})\n\t        inner_file_name = f\"inner_{inner_attack_method}\"\n\t        inner_file_name += '_bt' if inner_attack_config['backtrack'] else ''\n\t        inner_file_name += f\"_eps_{int(epsilon*255):d}\"\n\t        inner_file_name += f\"_lr_{inner_attack_config['base_lr']}\"\n\t        inner_file_name += \"_result.npy\"\n\t        if os.path.exists(os.path.join(output_dir, inner_file_name)):\n\t            inner_attack_results = np.load(os.path.join(output_dir, inner_file_name), allow_pickle=True).item()\n\t        else:\n\t            inner_attack_results = eval_inner_attack(inner_attack_method,\n", "                                                    inner_attack_config,\n\t                                                    model,\n\t                                                    test_dataloader,\n\t                                                    threshold,\n\t                                                    epsilon,\n\t                                                    eps0_range,\n\t                                                    n_samples=N_SAMPLES)\n\t        final_inner_attack_results = combine_inner_attack_results(final_inner_attack_results, inner_attack_results)\n\t        np.save(os.path.join(output_dir, inner_file_name), inner_attack_results)\n\t        logger.info(f'{outer_file_name} saved!')\n", "        logger.info(f'{inner_file_name} saved!')\n\t        eval_robustness_curve(epsilon, outer_adv_error, inner_attack_results, logger=logger)\n\t    # Evaluate robustness curve\n\t    logger.info('Final results under an ensemble of attacks:')\n\t    curve_x, curve_y = eval_robustness_curve(epsilon, final_outer_adv_error, final_inner_attack_results, logger=logger)\n\t    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n\t    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_result.npy\"), result_dict)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "train_atrr.py", "chunked_list": ["\"\"\"\n\tMain script for training the baseline method ATRR:\n\tPang, Tianyu, et al. \"Adversarial Training with Rectified Rejection.\" arXiv preprint arXiv:2105.14785 (2021).\n\thttps://arxiv.org/pdf/2105.14785.pdf\n\tBased on the official code from: https://github.com/P2333/Rectified-Rejection\n\t\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport numpy as np\n", "import torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n\timport utils.numpy\n\timport models\n\timport json\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n", "from attacks.objectives import UntargetedObjective\n\tfrom attacks.mb_pgd_attack import *\n\tfrom utils.dataset import CustomDataset\n\tdef test(model, attacker, test_dataloader, max_batches, logger):\n\t    model.eval()\n\t    clean_accs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n", "            outputs = model(inputs)\n\t        clean_error = utils.torch.classification_error(outputs, targets, reduction='none')\n\t        clean_accs = utils.numpy.concatenate(clean_accs, 1 - clean_error.detach().cpu().numpy())\n\t    logger.info(\"clean acc: {:.2%}\".format(np.mean(clean_accs)))\n\t    adv_accs = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        if b >= max_batches:\n\t            break\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n", "        # Generate adversarial examples\n\t        adv_inputs = attacker.perturb(inputs, targets)\n\t        # Indicator for large perturbations\n\t        with torch.no_grad():\n\t            adv_logits = model(adv_inputs)\n\t        adv_error = utils.torch.classification_error(adv_logits, targets, reduction=\"none\")\n\t        adv_accs = utils.numpy.concatenate(adv_accs, 1 - adv_error.detach().cpu().numpy())\n\t    logger.info(\"adv_acc: {:.2%}\".format(np.mean(adv_accs)))\n\tdef lr_schedule(t, lr_max):\n\t    if t < 100:\n", "        return lr_max\n\t    elif t < 105:\n\t        return lr_max / 10.\n\t    else:\n\t        return lr_max / 100.\n\tdef train_robust_detection(model,\n\t                           train_dataloader,\n\t                           attacker,\n\t                           optimizer,\n\t                           scheduler,\n", "                           fraction,\n\t                           lamb,\n\t                           tempC, \n\t                           tempC_trueonly,\n\t                           epoch,\n\t                           lr_max,\n\t                           print_freq,\n\t                           logger):\n\t    num_training_iter = len(train_dataloader)\n\t    BCEcriterion = nn.BCELoss(reduction='mean')\n", "    for b, (inputs, targets) in enumerate(train_dataloader):\n\t        if scheduler is None:\n\t            epoch_now = epoch + (b + 1) / num_training_iter\n\t            lr = lr_schedule(epoch_now, lr_max)\n\t            optimizer.param_groups[0].update(lr=lr)\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        b_size = inputs.size(0)\n\t        split = int((1-fraction) * b_size)\n\t        # update fraction for correct loss computation\n", "        fraction = 1 - split / float(b_size)\n\t        clean_inputs = inputs[:split]\n\t        adv_inputs = inputs[split:]\n\t        clean_targets = targets[:split]\n\t        adv_targets = targets[split:]\n\t        # Generate adversarial examples\n\t        adv_examples = attacker.perturb(adv_inputs, adv_targets)\n\t        if adv_inputs.shape[0] < b_size: # fraction is not 1\n\t            combined_inputs = torch.cat((clean_inputs, adv_examples), dim=0)\n\t        else:\n", "            combined_inputs = adv_examples\n\t        model.train()\n\t        optimizer.zero_grad()\n\t        # `d_outputs` should correspond to the probability of rejection\n\t        logits, aux_outputs = model(combined_inputs, return_aux=True)\n\t        clean_logits = logits[:split]\n\t        clean_aux_outputs = aux_outputs[:split]\n\t        adv_logits = logits[split:]\n\t        adv_aux_outputs = aux_outputs[split:]\n\t        # robust loss\n", "        adv_loss = utils.torch.classification_loss(adv_logits, adv_targets, reduction=\"mean\")\n\t        adv_acc = 1 - utils.torch.classification_error(adv_logits, adv_targets, reduction=\"mean\")\n\t        # aux loss for detection\n\t        robust_output_s = torch.softmax(adv_logits * tempC, dim=1)\n\t        robust_con_pre, robust_con_label = robust_output_s.max(1) # predicted label and confidence\n\t        robust_output_s_ = torch.softmax(adv_logits * tempC_trueonly, dim=1)\n\t        robust_con_y = robust_output_s_[torch.tensor(range(adv_inputs.size(0))), adv_targets].detach() # predicted prob on the true label y\n\t        correct_index = torch.where(adv_logits.max(1)[1] == adv_targets)[0]\n\t        robust_con_pre[correct_index] = robust_con_pre[correct_index].detach()\n\t        robust_output_aux = adv_aux_outputs.sigmoid().squeeze() # bs, Calibration function A \\in [0,1]\n", "        robust_detector = robust_con_pre * robust_output_aux\n\t        aux_loss = BCEcriterion(robust_detector, robust_con_y)\n\t        if adv_inputs.shape[0] < b_size:\n\t            # Loss on clean inputs\n\t            clean_loss = utils.torch.classification_loss(clean_logits, clean_targets, reduction=\"mean\")\n\t            clean_acc = 1 - utils.torch.classification_error(clean_logits, clean_targets, reduction=\"mean\")\n\t            # Combined loss\n\t            loss = (1 - fraction) * clean_loss + fraction * (adv_loss + lamb * aux_loss)\n\t        else:\n\t            clean_acc = torch.ones(1)\n", "            loss = adv_loss + lamb * aux_loss\n\t        loss.backward()\n\t        optimizer.step()\n\t        if scheduler is not None:\n\t            scheduler.step()\n\t        if (b+1) % print_freq == 0:\n\t            if adv_inputs.shape[0] < b_size:\n\t                logger.info(\"Progress: {:d}/{:d}, loss: {:.4f}, clean_acc: {:.2%}, adv_acc: {:.2%}\".format(b+1,\n\t                                                                                                        num_training_iter,\n\t                                                                                                        loss.item(),\n", "                                                                                                        clean_acc.item(),\n\t                                                                                                        adv_acc.item()))\n\t            else:\n\t                logger.info(\"Progress: {:d}/{:d}, loss: {:.4f}, adv_acc: {:.2%}\".format(b+1,\n\t                                                                                        num_training_iter,\n\t                                                                                        loss.item(),\n\t                                                                                        adv_acc.item()))\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', '-s', type=int, default=0)\n", "    parser.add_argument('--config-file', '-c', type=str, required=True, help='config file')\n\t    parser.add_argument('--output-dir', '-o', type=str, required=True, help='output dir')\n\t    parser.add_argument('--pretrain-model-dir', help='dir to load pretrained models', type=str, default='')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n\t    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n", "        config = json.load(config_file)\n\t    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    lr = config['lr']\n\t    optimizer_name = config['optimizer_name']\n\t    epsilon = config['epsilon']\n\t    eps_iter = config['eps_iter']\n\t    nb_iter = config['nb_iter']\n\t    print_freq = config['print_freq']\n\t    checkpoint_freq = config['checkpoint_freq']\n", "    max_batches = config['max_batches']\n\t    # fraction of clean inputs\n\t    fraction = config['fraction']\n\t    lamb = config['lamb']\n\t    tempC = config['tempC']\n\t    tempC_trueonly = config['tempC_trueonly']\n\t    nepoch = config['nepoch']\n\t    batch_size = config['batch_size']\n\t    output_dir = args.output_dir\n\t    if not os.path.exists(output_dir):\n", "        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n\t        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'train_output.log')),\n\t            logging.StreamHandler()\n\t        ])\n", "    logger.info(args)\n\t    logger.info(config)\n\t    if dataset == 'cifar10':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n\t        ])\n", "        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'gtsrb':\n\t        N_class = 43\n\t        resolution = (3, 32, 32)\n", "        train_loaded = np.load('datasets/gtsrb/train.npz')\n\t        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n", "        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'svhn':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n", "        transform_train = transforms.Compose([\n\t            np.asarray,\n\t            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n", "        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t        eps_iter /= 255.\n\t    elif dataset == 'mnist':\n\t        N_class = 10\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n", "        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n\t        raise ValueError(\"Dataset '{}' is not supported.\".format(dataset))\n\t    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNetTwoBranchDenseV1(N_class, resolution, out_dim=1, use_BN=True, along=True)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNetTwoBranchDenseV1(N_class, resolution, blocks=[3, 3, 3], out_dim=1, use_BN=True, along=True)\n", "    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNetTwoBranchDenseV1(N_class, resolution, depth=28, width=10, out_dim=1, use_BN=True, along=True)\n\t    else:\n\t        raise ValueError\n\t    model.cuda()\n\t    if optimizer_name == 'SGD-pp':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n\t        scheduler = None\n\t    elif optimizer_name == 'SGD':\n\t        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n", "        scheduler = utils.torch.get_exponential_scheduler(optimizer, batches_per_epoch=len(train_dataloader), gamma=0.95)\n\t    else:\n\t        raise ValueError\n\t    objective = UntargetedObjective(loss=utils.torch.classification_loss)\n\t    attacker = MBLinfPGDAttack(model, \n\t                                objective, \n\t                                epsilon=epsilon, \n\t                                max_iterations=nb_iter, \n\t                                base_lr=eps_iter, \n\t                                momentum=0.0, \n", "                                lr_factor=1.5, \n\t                                backtrack=False, \n\t                                rand_init_name=\"random+zero\",\n\t                                num_rand_init=1,\n\t                                clip_min=0.0,\n\t                                clip_max=1.0)\n\t    for epoch in range(nepoch):\n\t        logger.info(\"Epoch: {:d}\".format(epoch))\n\t        train_robust_detection(model,\n\t                            train_dataloader,\n", "                            attacker,\n\t                            optimizer,\n\t                            scheduler,\n\t                            fraction,\n\t                            lamb,\n\t                            tempC, \n\t                            tempC_trueonly,\n\t                            epoch,\n\t                            lr,\n\t                            print_freq,\n", "                            logger)\n\t        test(model, attacker, test_dataloader, max_batches, logger)\n\t        if (epoch+1) % checkpoint_freq == 0:\n\t            torch.save({\n\t                'epoch': epoch + 1,\n\t                'model': model.state_dict(),\n\t                'optimizer': optimizer.state_dict(),\n\t            }, os.path.join(output_dir, 'checkpoint_{:d}.pth'.format(epoch+1)))\n\t    torch.save({\n\t            'epoch': epoch + 1,\n", "            'model': model.state_dict(),\n\t            'optimizer': optimizer.state_dict(),\n\t        }, os.path.join(output_dir, 'classifier.pth.tar'))\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "eval_at.py", "chunked_list": ["\"\"\"Evaluation of standard adversarial training without rejection.\"\"\"\n\timport argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision import transforms\n\tfrom torchvision import datasets\n\timport utils.torch\n", "import utils.numpy\n\timport models\n\timport json\n\tfrom attacks.objectives import UntargetedObjective\n\tfrom attacks.mb_pgd_attack import MBConfLinfPGDAttack\n\tfrom utils.dataset import CustomDataset\n\tfrom utils.constants import *\n\tfrom imgaug import augmenters as iaa\n\timport utils.imgaug_lib\n\tfrom autoattack import AutoAttack\n", "def eval_robustness_curve(model, test_dataloader, epsilon, eps0_range, logger=None, n_samples=N_SAMPLES):\n\t    model.eval()\n\t    # Use AutoAttack\n\t    attacker = AutoAttack(model, norm='Linf', eps=epsilon, version='standard', verbose=False)\n\t    adv_labels = None\n\t    adv_probs = None\n\t    cnt = 0\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n", "        adv_labels = utils.numpy.concatenate(adv_labels, targets.detach().cpu().numpy())\n\t        # large perturbations\n\t        adv_inputs = attacker.run_standard_evaluation(inputs, targets)\n\t        with torch.no_grad():\n\t            adv_logits = model(adv_inputs)\n\t        adv_probs = utils.numpy.concatenate(adv_probs, torch.softmax(adv_logits, dim=1).detach().cpu().numpy())\n\t        cnt += inputs.shape[0]\n\t        if cnt >= n_samples:\n\t            break\n\t    # Error on adversarial inputs: accept and misclassify\n", "    adv_confs = np.max(adv_probs, axis=1)   # prediction confidence\n\t    adv_preds = np.argmax(adv_probs, axis=1)    # predicted class\n\t    adv_error = adv_preds != adv_labels\n\t    logger.info(f\"robustness with reject: {1-np.mean(adv_error):.2%}\")\n\t    curve_x = []\n\t    curve_y = []\n\t    for epsilon_0 in eps0_range:\n\t        curve_x.append(epsilon_0/epsilon)\n\t        final_adv_error = np.mean(adv_error)\n\t        curve_y.append(1 - final_adv_error)\n", "    return np.array(curve_x), np.array(curve_y)\n\tdef eval(model, test_dataloader, logger):\n\t    # Accuracy on clean inputs\n\t    model.eval()\n\t    clean_probs = None\n\t    clean_labels = None\n\t    for b, (inputs, targets) in enumerate(test_dataloader):\n\t        inputs = inputs.cuda()\n\t        targets = targets.cuda()\n\t        with torch.no_grad():\n", "            outputs = model(inputs)\n\t        clean_probs = utils.numpy.concatenate(clean_probs, torch.softmax(outputs, dim=1).detach().cpu().numpy())\n\t        clean_labels = utils.numpy.concatenate(clean_labels, targets.detach().cpu().numpy())\n\t    N = clean_probs.shape[0]\n\t    test_N = int(N * (1 - VAL_RATIO))\n\t    test_probs = clean_probs[:test_N]\n\t    test_labels = clean_labels[:test_N]\n\t    test_preds = np.argmax(test_probs, axis=1)\n\t    test_errors = (test_preds != test_labels)\n\t    test_acc = 1 - np.mean(test_errors)\n", "    logger.info(f\"clean accuracy: {test_acc:.2%}, F1 score: {test_acc*2/(test_acc+1)}\")\n\tdef get_args():\n\t    parser = argparse.ArgumentParser(description='train robust model with detection')\n\t    parser.add_argument('--seed', type=int, default=0)\n\t    parser.add_argument('--config-file', type=str, required=True, help='config file')\n\t    parser.add_argument('--checkpoint-dir', type=str, required=True, help='checkpoint dir')\n\t    # args parse\n\t    return parser.parse_args()\n\tdef main():\n\t    args = get_args()\n", "    # Set random seed\n\t    utils.torch.set_seed(args.seed)\n\t    with open(args.config_file) as config_file:\n\t        config = json.load(config_file)\n\t    dataset = config['dataset']\n\t    model_arch = config['model_arch']\n\t    epsilon = config['epsilon']\n\t    batch_size = config['batch_size']\n\t    checkpoint_dir = args.checkpoint_dir\n\t    output_dir = checkpoint_dir.replace('checkpoints/', 'results/')\n", "    if not os.path.exists(output_dir):\n\t        os.makedirs(output_dir)\n\t    logger = logging.getLogger(__name__)\n\t    logging.basicConfig(\n\t        format='[%(asctime)s] - %(message)s',\n\t        datefmt='%Y/%m/%d %H:%M:%S',\n\t        level=logging.DEBUG,\n\t        handlers=[\n\t            logging.FileHandler(os.path.join(output_dir, 'eval_output.log')),\n\t            logging.StreamHandler()\n", "        ])\n\t    logger.info(args)\n\t    logger.info(config)\n\t    N_class = N_CLASSES[dataset]\n\t    if dataset == 'cifar10':\n\t        resolution = (3, 32, 32)\n\t        transform_train = transforms.Compose([\n\t        transforms.RandomCrop(32, padding=4),\n\t        transforms.RandomHorizontalFlip(),\n\t        transforms.ToTensor(),\n", "        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = datasets.CIFAR10('./datasets/cifar10', train=True, download=True, transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.CIFAR10('./datasets/cifar10', train=False, transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'gtsrb':\n\t        resolution = (3, 32, 32)\n\t        train_loaded = np.load('datasets/gtsrb/train.npz')\n", "        X_train = train_loaded['images']\n\t        y_train = train_loaded['labels']\n\t        test_loaded = np.load('datasets/gtsrb/test_selected.npz')\n\t        X_test = test_loaded['images']\n\t        y_test = test_loaded['labels']\n\t        train_dataset = CustomDataset(X_train, y_train, transform=transforms.ToTensor())\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = CustomDataset(X_test, y_test, transform=transforms.ToTensor())\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n", "    elif dataset == 'svhn':\n\t        N_class = 10\n\t        resolution = (3, 32, 32)\n\t        augmenters = [iaa.CropAndPad(\n\t            percent=(0, 0.2),\n\t            pad_mode='edge',\n\t        ),\n\t        iaa.ContrastNormalization((0.7, 1.3))]\n\t        transform_train = transforms.Compose([\n\t            np.asarray,\n", "            iaa.Sequential([\n\t                iaa.SomeOf(max(1, len(augmenters) // 2), augmenters),\n\t                utils.imgaug_lib.Clip(),\n\t            ]).augment_image,\n\t            np.copy,\n\t            transforms.ToTensor(),\n\t        ])\n\t        transform_test = transforms.ToTensor()\n\t        train_dataset = utils.dataset.SVHNTrainSet(transform=transform_train)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n", "        test_dataset = utils.dataset.SVHNTestSet(transform=transform_test)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t        epsilon /= 255.\n\t    elif dataset == 'mnist':\n\t        resolution = (1, 28, 28)\n\t        train_dataset = datasets.MNIST(root='./datasets/mnist', train=True, transform=transforms.ToTensor(), download=True)\n\t        train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n\t        test_dataset = datasets.MNIST(root='./datasets/mnist', train=False, transform=transforms.ToTensor(), download=True)\n\t        test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n\t    else:\n", "        raise ValueError(\"Invalid or unsupported dataset '{}'\".format(dataset))\n\t    eps0_range = np.array(ALPHA_LIST, dtype=np.float32) * epsilon\n\t    # Model Setup\n\t    if model_arch == \"lenet\":\n\t        model = models.FixedLeNet(N_class, resolution)\n\t    elif model_arch == \"resnet20\":\n\t        model = models.ResNet(N_class, resolution, blocks=[3, 3, 3])\n\t    elif model_arch == \"wideresnet\":\n\t        model = models.WideResNet(N_class, resolution, depth=28, width=10)\n\t    else:\n", "        raise ValueError\n\t    checkpoint = torch.load(os.path.join(checkpoint_dir, \"classifier.pth.tar\"))\n\t    model.load_state_dict(checkpoint['model'])\n\t    model.cuda()\n\t    eval(model, test_dataloader, logger)\n\t    curve_x, curve_y = eval_robustness_curve(model, test_dataloader, epsilon,  eps0_range,\n\t                                                            logger=logger, n_samples=N_SAMPLES)\n\t    result_dict = {\"curve_x\": curve_x, \"curve_y\": curve_y}\n\t    np.save(os.path.join(output_dir, f\"rob_curve_eps_{int(epsilon*255):d}_at_result.npy\"), result_dict)\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "attacks/objectives.py", "chunked_list": ["from __future__ import print_function\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport utils.torch\n\tclass TargetedObjective(nn.Module):\n\t    \"\"\"\n\t    Targeted objective based on a loss, e.g., the cross-entropy loss.\n\t    \"\"\"\n\t    def __init__(self, loss=utils.torch.classification_loss):\n", "        \"\"\"\n\t        Constructor.\n\t        :param loss: loss function to use\n\t        :type loss: callable\n\t        \"\"\"\n\t        super(TargetedObjective, self).__init__()\n\t        self.loss = loss\n\t        \"\"\" (callable) Loss. \"\"\"\n\t    def forward(self, logits, target_classes, perturbations=None):\n\t        \"\"\"\n", "        Objective function.\n\t        :param logits: logit output of the network\n\t        :type logits: torch.autograd.Variable\n\t        :param perturbations: perturbations\n\t        :type perturbations: torch.autograd.Variable or None\n\t        :return: error\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n\t        if perturbations is not None:\n\t            return -self.loss(logits, target_classes, perturbations=perturbations, reduction='none')\n", "        else:\n\t            return -self.loss(logits, target_classes, reduction='none')\n\tclass UntargetedObjective(nn.Module):\n\t    \"\"\"\n\t    Untargeted loss based objective, e.g., cross-entropy loss.\n\t    \"\"\"\n\t    def __init__(self, loss=utils.torch.classification_loss):\n\t        \"\"\"\n\t        Constructor.\n\t        :param loss: loss function to use\n", "        :type loss: callable\n\t        \"\"\"\n\t        super(UntargetedObjective, self).__init__()\n\t        self.loss = loss\n\t        \"\"\" (callable) Loss. \"\"\"\n\t    def forward(self, logits, true_classes, perturbations=None):\n\t        \"\"\"\n\t        Objective function.\n\t        :param logits: logit output of the network\n\t        :type logits: torch.autograd.Variable\n", "        :param perturbations: perturbations\n\t        :type perturbations: torch.autograd.Variable or None\n\t        :return: error\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n\t        # assert self.loss is not None\n\t        if perturbations is not None:\n\t            return self.loss(logits, true_classes, perturbations=perturbations, reduction='none')\n\t        else:\n\t            return self.loss(logits, true_classes, reduction='none')\n", "class SelectiveUntargetedObjective(UntargetedObjective):\n\t    \"\"\"\n\t    Untargeted loss based objective for selective classifier, e.g., cross-entropy loss.\n\t    \"\"\"\n\t    def __init__(self, loss):\n\t        \"\"\"\n\t        Constructor.\n\t        :param loss: loss function to use\n\t        :type loss: callable\n\t        \"\"\"\n", "        super(SelectiveUntargetedObjective, self).__init__(loss)\n\t    def forward(self, logits, d_logits, true_classes, perturbations=None):\n\t        \"\"\"\n\t        Objective function.\n\t        :param logits: logit output of the network\n\t        :type logits: torch.autograd.Variable\n\t        :param perturbations: perturbations\n\t        :type perturbations: torch.autograd.Variable or None\n\t        :return: error\n\t        :rtype: torch.autograd.Variable\n", "        \"\"\"\n\t        # assert self.loss is not None\n\t        if perturbations is not None:\n\t            return self.loss(logits, d_logits, true_classes, perturbations=perturbations, reduction='none')\n\t        else:\n\t            return self.loss(logits, d_logits, true_classes, reduction='none')\n"]}
{"filename": "attacks/mb_pgd_attack.py", "chunked_list": ["from __future__ import print_function\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport random\n\timport utils.torch\n\timport numpy as np\n\tclass MBBPDAAttack:\n\t    def __init__(self, \n\t                model, \n", "                defense,\n\t                objective, \n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n", "                clip_min=0.0,\n\t                clip_max=1.0):\n\t        self.model = model\n\t        self.defense = defense\n\t        self.objective = objective\n\t        self.epsilon = epsilon\n\t        self.max_iterations = max_iterations\n\t        self.base_lr = base_lr\n\t        self.momentum = momentum\n\t        self.lr_factor = lr_factor\n", "        self.backtrack = backtrack\n\t        self.rand_init_name = rand_init_name\n\t        self.num_rand_init = num_rand_init\n\t        self.clip_min = clip_min\n\t        self.clip_max = clip_max\n\t    def random_init(self, delta, x, random_name=\"random\"):\n\t        if random_name == 'random':\n\t            delta.data.normal_()\n\t            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n\t            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n", "            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n\t        elif random_name == 'zero':\n\t            delta.data.zero_()\n\t        else:\n\t            raise ValueError\n\t        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\t    def get_loss(self, adv_x, y):\n\t        outputs = self.model(adv_x)\n\t        loss = self.objective(outputs, y)\n\t        return loss\n", "    def perturb_once(self, x, y):\n\t        delta = torch.zeros_like(x)\n\t        batch_size = x.shape[0]\n\t        global_gradients = torch.zeros_like(delta)\n\t        adv_x = x + delta\n\t        t_adv_x = self.defense.transform(adv_x)\n\t        loss = self.get_loss(t_adv_x, y)\n\t        success_errors = loss.data.clone()\n\t        success_perturbs = delta.data.clone()\n\t        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n", "        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\t        if self.rand_init_name == 'random+zero':\n\t            random_name = random.choice([\"random\", \"zero\"])\n\t            self.random_init(delta, x, random_name)\n\t        else:\n\t            self.random_init(delta, x, self.rand_init_name)\n\t        for ii in range(self.max_iterations):\n\t            adv_x = x + delta\n\t            t_adv_x = self.defense.transform(adv_x)\n\t            t_adv_x = nn.Parameter(t_adv_x)\n", "            t_adv_x.requires_grad_()\n\t            loss = self.get_loss(t_adv_x, y)\n\t            cond = loss.data > success_errors\n\t            success_errors[cond] = loss.data[cond]\n\t            success_perturbs[cond] = delta.data[cond]\n\t            loss.mean().backward()\n\t            grad = t_adv_x.grad.data\n\t            # normalize and add momentum.\n\t            grad.data = torch.sign(grad.data)\n\t            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n", "            if self.backtrack:\n\t                next_perturbs = delta + torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\t                next_perturbs.data = torch.clamp(next_perturbs.data, min=-self.epsilon, max=self.epsilon)\n\t                next_perturbs.data = torch.clamp(x.data + next_perturbs.data, min=self.clip_min, max=self.clip_max) - x.data\n\t                with torch.no_grad():\n\t                    next_adv_x = x + next_perturbs\n\t                    t_next_adv_x = self.defense.transform(next_adv_x)\n\t                    next_error = self.get_loss(t_next_adv_x, y)\n\t                # Update learning rate if requested.\n\t                for b in range(batch_size):\n", "                    if next_error[b].item() >= loss.data[b]:\n\t                        delta[b].data += lrs[b]*global_gradients[b].data\n\t                    else:\n\t                        lrs[b] = max(lrs[b] / self.lr_factor, 1e-20)\n\t            else:\n\t                delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\t            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n\t            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\t        adv_x = x + delta\n\t        t_adv_x = self.defense.transform(adv_x)\n", "        loss = self.get_loss(t_adv_x, y)\n\t        cond = loss.data > success_errors\n\t        success_errors[cond] = loss.data[cond]\n\t        success_perturbs[cond] = delta.data[cond]\n\t        return success_errors, success_perturbs\n\t    def perturb(self, x, y):\n\t        \"\"\"\n\t        Given examples (x, y), returns their adversarial counterparts with\n\t        an attack length of eps.\n\t        :param x: input tensor.\n", "        :param y: label tensor.\n\t                  - if None and self.targeted=False, compute y as predicted\n\t                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n\t        \"\"\"\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = None\n", "        worst_perb = None\n\t        for k in range(self.num_rand_init):\n\t            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            if worst_loss is None:\n\t                worst_loss = curr_worst_loss\n\t                worst_perb = curr_worst_perb\n\t            else:\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n", "        return x + worst_perb\n\tclass MBConfBPDAAttack(MBBPDAAttack):\n\t    def __init__(self, \n\t                model, \n\t                defense,\n\t                objective, \n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n", "                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model, \n\t                defense,\n\t                objective, \n\t                epsilon, \n", "                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t    def perturb(self, x, y):\n", "        \"\"\"\n\t        Given examples (x, y), returns their adversarial counterparts with\n\t        an attack length of eps.\n\t        :param x: input tensor.\n\t        :param y: label tensor.\n\t                  - if None and self.targeted=False, compute y as predicted\n\t                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n\t        \"\"\"\n", "        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        self.rand_init_name = \"zero\"\n\t        worst_loss, worst_perb = self.perturb_once(x, y)\n\t        self.rand_init_name = \"random\"\n\t        for k in range(self.num_rand_init):\n\t            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            cond = curr_worst_loss > worst_loss\n\t            worst_loss[cond] = curr_worst_loss[cond]\n", "            worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBConfBPDAAttackMultitargeted(MBConfBPDAAttack):\n\t    def __init__(self, \n\t                model, \n\t                defense,\n\t                objective, \n\t                num_classes,\n\t                epsilon=0.3, \n\t                max_iterations=100, \n", "                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n\t                defense,\n", "                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n", "                clip_max)\n\t        self.num_classes = num_classes\n\t    def perturb(self, x, y):\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = torch.empty(x.size()[0], device=x.device)\n\t        worst_loss[:] = -np.inf\n\t        worst_perb = torch.zeros_like(x)\n\t        for y_delta in range(1, self.num_classes):\n", "            for k in range(self.num_rand_init):\n\t                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass BPDAAttack:\n\t    def __init__(self, model=None, defense=None, device=None, epsilon=None, learning_rate=0.5,\n\t                 max_iterations=100, clip_min=0, clip_max=1):\n\t        self.model = model\n", "        self.epsilon = epsilon\n\t        self.loss_fn = nn.CrossEntropyLoss(reduction='sum')\n\t        self.defense = defense\n\t        self.clip_min = clip_min\n\t        self.clip_max = clip_max\n\t        self.LEARNING_RATE = learning_rate\n\t        self.MAX_ITERATIONS = max_iterations\n\t        self.device = device\n\t    def generate(self, x, y):\n\t        \"\"\"\n", "        Given examples (X_nat, y), returns their adversarial\n\t        counterparts with an attack length of epsilon.\n\t        \"\"\"\n\t        adv = x.detach().clone()\n\t        lower = np.clip(x.detach().cpu().numpy() - self.epsilon, self.clip_min, self.clip_max)\n\t        upper = np.clip(x.detach().cpu().numpy() + self.epsilon, self.clip_min, self.clip_max)\n\t        for i in range(self.MAX_ITERATIONS):\n\t            adv_purified = self.defense(adv)\n\t            adv_purified.requires_grad_()\n\t            adv_purified.retain_grad()\n", "            scores = self.model(adv_purified)\n\t            loss = self.loss_fn(scores, y)\n\t            loss.backward()\n\t            grad_sign = adv_purified.grad.data.sign()\n\t            # early stop, only for batch_size = 1\n\t            # p = torch.argmax(F.softmax(scores), 1)\n\t            # if y != p:\n\t            #     break\n\t            adv += self.LEARNING_RATE * grad_sign\n\t            adv_img = np.clip(adv.detach().cpu().numpy(), lower, upper)\n", "            adv = torch.Tensor(adv_img).to(self.device)\n\t        return adv\n\tclass RandomAttack:\n\t    def __init__(self, \n\t                model, \n\t                objective, \n\t                epsilon=0.3, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n", "                clip_max=1.0):\n\t        self.model = model\n\t        self.objective = objective\n\t        self.epsilon = epsilon\n\t        self.rand_init_name = rand_init_name\n\t        self.num_rand_init = num_rand_init\n\t        self.clip_min = clip_min\n\t        self.clip_max = clip_max\n\t    def random_init(self, delta, x, random_name=\"random\"):\n\t        if random_name == 'random':\n", "            delta.data.normal_()\n\t            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n\t            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n\t            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n\t        elif random_name == 'zero':\n\t            delta.data.zero_()\n\t        else:\n\t            raise ValueError\n\t        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\t    def get_loss(self, x, delta, y):\n", "        adv_x = x + delta\n\t        outputs = self.model(adv_x)\n\t        loss = self.objective(outputs, y)\n\t        return loss\n\t    def perturb_once(self, x, y):\n\t        delta = torch.zeros_like(x)\n\t        batch_size = x.shape[0]\n\t        global_gradients = torch.zeros_like(delta)\n\t        loss = self.get_loss(x, delta, y)\n\t        success_errors = loss.data.clone()\n", "        success_perturbs = delta.data.clone()\n\t        self.random_init(delta, x, self.rand_init_name)\n\t        loss = self.get_loss(x, delta, y)\n\t        cond = loss.data > success_errors\n\t        success_errors[cond] = loss.data[cond]\n\t        success_perturbs[cond] = delta.data[cond]\n\t        return success_errors, success_perturbs\n\t    def perturb(self, x, y):\n\t        \"\"\"\n\t        Given examples (x, y), returns their adversarial counterparts with\n", "        an attack length of eps.\n\t        :param x: input tensor.\n\t        :param y: label tensor.\n\t                  - if None and self.targeted=False, compute y as predicted\n\t                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n\t        \"\"\"\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n", "        y = y.detach().clone().cuda()\n\t        worst_loss = None\n\t        worst_perb = None\n\t        for k in range(self.num_rand_init):\n\t            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            if worst_loss is None:\n\t                worst_loss = curr_worst_loss\n\t                worst_perb = curr_worst_perb\n\t            else:\n\t                cond = curr_worst_loss > worst_loss\n", "                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBLinfPGDAttack:\n\t    def __init__(self, \n\t                model, \n\t                objective, \n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n", "                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        self.model = model\n\t        self.objective = objective\n\t        self.epsilon = epsilon\n", "        self.max_iterations = max_iterations\n\t        self.base_lr = base_lr\n\t        self.momentum = momentum\n\t        self.lr_factor = lr_factor\n\t        self.backtrack = backtrack\n\t        self.rand_init_name = rand_init_name\n\t        self.num_rand_init = num_rand_init\n\t        self.clip_min = clip_min\n\t        self.clip_max = clip_max\n\t    def random_init(self, delta, x, random_name=\"random\"):\n", "        if random_name == 'random':\n\t            delta.data.normal_()\n\t            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n\t            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n\t            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n\t        elif random_name == 'zero':\n\t            delta.data.zero_()\n\t        else:\n\t            raise ValueError\n\t        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n", "    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        outputs = self.model(adv_x)\n\t        loss = self.objective(outputs, y)\n\t        return loss\n\t    def perturb_once(self, x, y):\n\t        delta = torch.zeros_like(x)\n\t        batch_size = x.shape[0]\n\t        global_gradients = torch.zeros_like(delta)\n\t        loss = self.get_loss(x, delta, y)\n", "        success_errors = loss.data.clone()\n\t        success_perturbs = delta.data.clone()\n\t        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n\t        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\t        if self.rand_init_name == 'random+zero':\n\t            random_name = random.choice([\"random\", \"zero\"])\n\t            self.random_init(delta, x, random_name)\n\t        else:\n\t            self.random_init(delta, x, self.rand_init_name)\n\t        delta = nn.Parameter(delta)\n", "        delta.requires_grad_()\n\t        for ii in range(self.max_iterations):\n\t            loss = self.get_loss(x, delta, y)\n\t            cond = loss.data > success_errors\n\t            success_errors[cond] = loss.data[cond]\n\t            success_perturbs[cond] = delta.data[cond]\n\t            loss.mean().backward()\n\t            grad = delta.grad.data\n\t            # normalize and add momentum.\n\t            grad.data = torch.sign(grad.data)\n", "            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\t            if self.backtrack:\n\t                next_perturbs = delta + torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\t                next_perturbs.data = torch.clamp(next_perturbs.data, min=-self.epsilon, max=self.epsilon)\n\t                next_perturbs.data = torch.clamp(x.data + next_perturbs.data, min=self.clip_min, max=self.clip_max) - x.data\n\t                with torch.no_grad():\n\t                    next_error = self.get_loss(x, next_perturbs, y)\n\t                # Update learning rate if requested.\n\t                for b in range(batch_size):\n\t                    if next_error[b].item() >= loss.data[b]:\n", "                        delta[b].data += lrs[b]*global_gradients[b].data\n\t                    else:\n\t                        lrs[b] = max(lrs[b] / self.lr_factor, 1e-20)\n\t            else:\n\t                delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\t            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n\t            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\t            delta.grad.data.zero_()\n\t        loss = self.get_loss(x, delta, y)\n\t        cond = loss.data > success_errors\n", "        success_errors[cond] = loss.data[cond]\n\t        success_perturbs[cond] = delta.data[cond]\n\t        return success_errors, success_perturbs\n\t    def perturb(self, x, y):\n\t        \"\"\"\n\t        Given examples (x, y), returns their adversarial counterparts with\n\t        an attack length of eps.\n\t        :param x: input tensor.\n\t        :param y: label tensor.\n\t                  - if None and self.targeted=False, compute y as predicted\n", "                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n\t        \"\"\"\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = None\n\t        worst_perb = None\n\t        for k in range(self.num_rand_init):\n", "            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            if worst_loss is None:\n\t                worst_loss = curr_worst_loss\n\t                worst_perb = curr_worst_perb\n\t            else:\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBConfLinfPGDAttack(MBLinfPGDAttack):\n", "    def __init__(self, \n\t                model, \n\t                objective, \n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n", "                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model, \n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n", "                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t    def perturb(self, x, y):\n\t        \"\"\"\n\t        Given examples (x, y), returns their adversarial counterparts with\n\t        an attack length of eps.\n\t        :param x: input tensor.\n", "        :param y: label tensor.\n\t                  - if None and self.targeted=False, compute y as predicted\n\t                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n\t        \"\"\"\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        self.rand_init_name = \"zero\"\n", "        worst_loss, worst_perb = self.perturb_once(x, y)\n\t        self.rand_init_name = \"random\"\n\t        for k in range(self.num_rand_init):\n\t            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            cond = curr_worst_loss > worst_loss\n\t            worst_loss[cond] = curr_worst_loss[cond]\n\t            worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBSATRLinfPGDAttack(MBConfLinfPGDAttack):\n\t    def __init__(self, \n", "                model, \n\t                objective, \n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n", "                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n", "                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        outputs, d_outputs = self.model(adv_x, return_d=True)\n\t        loss = self.objective(outputs, d_outputs, y)\n\t        return loss\n\tclass MBATRRLinfPGDAttack(MBConfLinfPGDAttack):\n", "    def __init__(self, \n\t                model, \n\t                objective, \n\t                tempC=1.0,\n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n", "                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n", "                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t        self.tempC = tempC\n\t    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        outputs, aux_outputs = self.model(adv_x, return_aux=True)\n", "        # con_pre, _ = torch.softmax(outputs * self.tempC, dim=1).max(dim=1, keepdim=True) # predicted label and confidence\n\t        aux_outputs = aux_outputs.sigmoid()\n\t        # evi_outputs = con_pre * aux_outputs\n\t        loss = self.objective(outputs, aux_outputs, y)\n\t        return loss\n\tclass MBSATRStratifiedLinfPGDAttack(MBLinfPGDAttack):\n\t    \"\"\"\n\t    Stratified PGD Attack for the proposed method SATR\n\t    :param predict: forward pass function.\n\t    :param loss_fn: loss function.\n", "    :param eps: maximum distortion.\n\t    :param nb_iter: number of iterations.\n\t    :param eps_iter: attack step size.\n\t    :param rand_init: (optional bool) random initialization.\n\t    :param clip_min: mininum value per input dimension.\n\t    :param clip_max: maximum value per input dimension.\n\t    :param targeted: if the attack is targeted.\n\t    \"\"\"\n\t    def __init__(\n\t                self, \n", "                model, \n\t                objective, \n\t                rr_objective,\n\t                fraction,\n\t                epsilon=0.3, \n\t                epsilon_0 = 0.1,\n\t                max_iterations=100, \n\t                outer_base_lr=0.1, \n\t                inner_base_lr=0.1, \n\t                momentum=0.9, \n", "                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                multitargeted=False,\n\t                num_classes=10,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model, \n\t                objective, \n", "                epsilon, \n\t                max_iterations, \n\t                outer_base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n", "        self.epsilon_0 = epsilon_0\n\t        self.fraction = fraction\n\t        self.rr_objective = rr_objective\n\t        self.multitargeted = multitargeted\n\t        self.num_classes = num_classes\n\t        self.outer_base_lr = outer_base_lr\n\t        self.inner_base_lr = inner_base_lr\n\t    def get_split(self, size_inp):\n\t        return int(self.fraction * size_inp)\n\t    def random_init(self, delta, x, random_name=\"random\"):\n", "        if random_name == 'random':\n\t            size_inp = delta.size(0)\n\t            delta.data.normal_()\n\t            u = torch.zeros(size_inp).uniform_(0, 1).cuda()\n\t            linf_norm = u / torch.max(delta.abs().view(size_inp, -1), dim=1)[0]\n\t            linf_norm = linf_norm.view(size_inp, 1, 1, 1)\n\t            split = self.get_split(size_inp)\n\t            if self.fraction > 0.0:\n\t                delta.data[:split] = self.epsilon * delta.data[:split] * linf_norm.data[:split]\n\t            if self.fraction < 1.0:\n", "                delta.data[split:] = self.epsilon_0 * delta.data[split:] * linf_norm.data[split:]\n\t        elif random_name == 'zero':\n\t            delta.data.zero_()\n\t        else:\n\t            raise ValueError\n\t        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n\t    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        outputs, d_outputs = self.model(adv_x, return_d=True)\n\t        # Adversarial inputs are created from a fraction of inputs from a batch. Similar to 50% adversarial training\n", "        split = self.get_split(x.size(0))\n\t        if self.fraction == 1.0:\n\t            targets = y[:split]\n\t            adv_outputs = outputs[:split]\n\t            adv_d_outputs = d_outputs[:split]\n\t            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets)\n\t            return loss_1\n\t        elif self.fraction == 0.0:\n\t            targets_2 = y[split:]\n\t            adv_outputs_2 = outputs[split:]\n", "            adv_d_outputs_2 = d_outputs[split:]\n\t            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2)\n\t            return loss_2\n\t        else:\n\t            targets = y[:split]\n\t            adv_outputs = outputs[:split]\n\t            adv_d_outputs = d_outputs[:split]\n\t            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets)\n\t            targets_2 = y[split:]\n\t            adv_outputs_2 = outputs[split:]\n", "            adv_d_outputs_2 = d_outputs[split:]\n\t            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2)\n\t            return torch.cat((loss_1, loss_2), dim=0)\n\t    def perturb_once(self, x, y):\n\t        split = self.get_split(x.size(0))\n\t        delta = torch.zeros_like(x)\n\t        batch_size = x.shape[0]\n\t        global_gradients = torch.zeros_like(delta)\n\t        loss = self.get_loss(x, delta, y)\n\t        success_errors = loss.data.clone()\n", "        success_perturbs = delta.data.clone()\n\t        outer_lrs = (torch.ones_like(success_errors[:split]).float() * self.outer_base_lr).cuda()\n\t        inner_lrs = (torch.ones_like(success_errors[split:]).float() * self.inner_base_lr).cuda()\n\t        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\t        if self.rand_init_name == 'random+zero':\n\t            random_name = random.choice([\"random\", \"zero\"])\n\t            self.random_init(delta, x, random_name)\n\t        else:\n\t            self.random_init(delta, x, self.rand_init_name)\n\t        delta = nn.Parameter(delta)\n", "        delta.requires_grad_()\n\t        for ii in range(self.max_iterations):\n\t            loss = self.get_loss(x, delta, y)\n\t            cond = loss.data > success_errors\n\t            success_errors[cond] = loss.data[cond]\n\t            success_perturbs[cond] = delta.data[cond]\n\t            loss.mean().backward()\n\t            grad = delta.grad.data\n\t            # normalize and add momentum.\n\t            grad.data = torch.sign(grad.data)\n", "            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\t            delta.data[:split] += torch.mul(utils.torch.expand_as(outer_lrs, global_gradients[:split]), global_gradients[:split])\n\t            delta.data[split:] += torch.mul(utils.torch.expand_as(inner_lrs, global_gradients[split:]), global_gradients[split:])\n\t            if self.fraction > 0.0:\n\t                delta.data[:split] = torch.clamp(delta.data[:split], min=-self.epsilon, max=self.epsilon)\n\t            if self.fraction < 1.0:\n\t                delta.data[split:] = torch.clamp(delta.data[split:], min=-self.epsilon_0, max=self.epsilon_0)\n\t            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\t            delta.grad.data.zero_()\n\t        loss = self.get_loss(x, delta, y)\n", "        cond = loss.data > success_errors\n\t        success_errors[cond] = loss.data[cond]\n\t        success_perturbs[cond] = delta.data[cond]\n\t        return success_errors, success_perturbs\n\t    def perturb(self, x, y):\n\t        \"\"\"\n\t        Given examples (x, y), returns their adversarial counterparts with\n\t        an attack length of eps.\n\t        :param x: input tensor.\n\t        :param y: label tensor.\n", "                  - if None and self.targeted=False, compute y as predicted\n\t                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n\t        \"\"\"\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        if self.multitargeted:\n\t            split = self.get_split(x.size(0))\n", "            y_delta = np.random.randint(1, self.num_classes, size=split)\n\t            y[:split] = (y[:split] + y_delta) % self.num_classes\n\t        worst_loss = None\n\t        worst_perb = None\n\t        for k in range(self.num_rand_init):\n\t            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            if worst_loss is None:\n\t                worst_loss = curr_worst_loss\n\t                worst_perb = curr_worst_perb\n\t            else:\n", "                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBRCDStratifiedLinfPGDAttack(MBLinfPGDAttack):\n\t    \"\"\"\n\t    Stratified PGD Attack for the RCD baseline\n\t    :param predict: forward pass function.\n\t    :param loss_fn: loss function.\n\t    :param eps: maximum distortion.\n", "    :param nb_iter: number of iterations.\n\t    :param eps_iter: attack step size.\n\t    :param rand_init: (optional bool) random initialization.\n\t    :param clip_min: mininum value per input dimension.\n\t    :param clip_max: maximum value per input dimension.\n\t    :param targeted: if the attack is targeted.\n\t    \"\"\"\n\t    def __init__(\n\t                self, \n\t                model, \n", "                objective_1, \n\t                objective_2,\n\t                fraction,\n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n", "                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model, \n\t                None, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n", "                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t        self.fraction = fraction\n\t        self.objective_1 = objective_1\n\t        self.objective_2 = objective_2\n\t    def get_split(self, size_inp):\n\t        return int(self.fraction * size_inp)\n", "    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        outputs = self.model(adv_x)\n\t        # Adversarial inputs are created from a fraction of inputs from a batch. Similar to 50% adversarial training\n\t        split = self.get_split(x.size(0))\n\t        if self.fraction == 1.0 or self.fraction == 0.0:\n\t            raise ValueError\n\t        else:\n\t            targets_1 = y[:split]\n\t            adv_outputs_1 = outputs[:split]\n", "            loss_1 = self.objective_1(adv_outputs_1, targets_1)\n\t            targets_2 = y[split:]\n\t            adv_outputs_2 = outputs[split:]\n\t            loss_2 = self.objective_2(adv_outputs_2, targets_2)\n\t            return torch.cat((loss_1, loss_2), dim=0)\n\tclass MBSATRLinfPGDAttackMultitargeted(MBSATRLinfPGDAttack):\n\t    def __init__(self, \n\t                model, \n\t                objective, \n\t                num_classes,\n", "                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n", "        super().__init__(model,\n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n", "                clip_min,\n\t                clip_max)\n\t        self.num_classes = num_classes\n\t    def perturb(self, x, y):\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = torch.empty(x.size()[0], device=x.device)\n\t        worst_loss[:] = -np.inf\n\t        worst_perb = torch.zeros_like(x)\n", "        for y_delta in range(1, self.num_classes):\n\t            for k in range(self.num_rand_init):\n\t                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBRCDLinfPGDAttackMultitargeted(MBConfLinfPGDAttack):\n\t    def __init__(self, \n\t                model, \n", "                objective, \n\t                num_classes,\n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n", "                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n", "                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t        self.num_classes = num_classes\n\t    def perturb(self, x, y):\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = torch.empty(x.size()[0], device=x.device)\n", "        worst_loss[:] = -np.inf\n\t        worst_perb = torch.zeros_like(x)\n\t        for y_delta in range(1, self.num_classes):\n\t            for k in range(self.num_rand_init):\n\t                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBATRRLinfPGDAttackMultitargeted(MBATRRLinfPGDAttack):\n", "    def __init__(self, \n\t                model, \n\t                objective, \n\t                num_classes,\n\t                tempC=1.0,\n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n", "                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n\t                objective, \n\t                tempC, \n\t                epsilon, \n\t                max_iterations, \n", "                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t        self.tempC = tempC\n\t        self.num_classes = num_classes\n", "    def perturb(self, x, y):\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = torch.empty(x.size()[0], device=x.device)\n\t        worst_loss[:] = -np.inf\n\t        worst_perb = torch.zeros_like(x)\n\t        for y_delta in range(1, self.num_classes):\n\t            for k in range(self.num_rand_init):\n\t                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n", "                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBConfLinfPGDAttackMultitargeted(MBConfLinfPGDAttack):\n\t    def __init__(self, \n\t                model, \n\t                objective, \n\t                num_classes,\n\t                epsilon=0.3, \n", "                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n", "                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n", "                clip_max)\n\t        self.num_classes = num_classes\n\t    def perturb(self, x, y):\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = torch.empty(x.size()[0], device=x.device)\n\t        worst_loss[:] = -np.inf\n\t        worst_perb = torch.zeros_like(x)\n\t        for y_delta in range(1, self.num_classes):\n", "            for k in range(self.num_rand_init):\n\t                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBCONSRLinfPGDAttackMultitargeted(MBConfLinfPGDAttack):\n\t    def __init__(self, \n\t                model, \n\t                defense,\n", "                objective, \n\t                num_classes,\n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n", "                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n", "                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t        self.num_classes = num_classes\n\t        self.defense = defense\n\t    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        t_adv_x = self.defense.transform(adv_x)\n\t        perb = t_adv_x.data-adv_x.detach().data\n", "        t_adv_x = x + delta + perb\n\t        outputs = self.model(adv_x)\n\t        t_outputs = self.model(t_adv_x)\n\t        loss = self.objective(outputs, y) + self.objective(t_outputs, y)\n\t        return loss\n\t    def perturb(self, x, y):\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = torch.empty(x.size()[0], device=x.device)\n", "        worst_loss[:] = -np.inf\n\t        worst_perb = torch.zeros_like(x)\n\t        for y_delta in range(1, self.num_classes):\n\t            for k in range(self.num_rand_init):\n\t                curr_worst_loss, curr_worst_perb = self.perturb_once(x, (y + y_delta) % self.num_classes)\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass MBCONSRLinfPGDAttack(MBConfLinfPGDAttack):\n", "    def __init__(self, \n\t                model, \n\t                defense,\n\t                objective, \n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n", "                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model, \n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n", "                lr_factor, \n\t                backtrack, \n\t                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t        self.defense = defense\n\t    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        t_adv_x = self.defense.transform(adv_x)\n", "        perb = t_adv_x.data-adv_x.detach().data\n\t        t_adv_x = x + delta + perb\n\t        outputs = self.model(adv_x)\n\t        t_outputs = self.model(t_adv_x)\n\t        loss = self.objective(outputs, y) + self.objective(t_outputs, y)\n\t        return loss\n\tclass MBCONSRLinfPGDInnerAttackMultitargeted(MBConfLinfPGDAttack):\n\t    def __init__(self, \n\t                model, \n\t                defense,\n", "                objective, \n\t                num_classes,\n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                lr_factor=1.5, \n\t                backtrack=True, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n", "                clip_min=0.0,\n\t                clip_max=1.0):\n\t        super().__init__(model,\n\t                objective, \n\t                epsilon, \n\t                max_iterations, \n\t                base_lr, \n\t                momentum, \n\t                lr_factor, \n\t                backtrack, \n", "                rand_init_name,\n\t                num_rand_init,\n\t                clip_min,\n\t                clip_max)\n\t        self.num_classes = num_classes\n\t        self.defense = defense\n\t    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        t_adv_x = self.defense.transform(adv_x)\n\t        perb = t_adv_x.data-adv_x.detach().data\n", "        t_adv_x = x + delta + perb\n\t        outputs = self.model(adv_x)\n\t        t_outputs = self.model(t_adv_x)\n\t        loss = self.objective(outputs, y) - self.objective(t_outputs, y)\n\t        return loss\n\t    def perturb(self, x, y):\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = torch.empty(x.size()[0], device=x.device)\n", "        worst_loss[:] = -np.inf\n\t        worst_perb = torch.zeros_like(x)\n\t        for target_y in range(self.num_classes):\n\t            for k in range(self.num_rand_init):\n\t                curr_worst_loss, curr_worst_perb = self.perturb_once(x, target_y)\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n"]}
{"filename": "attacks/pgd_attack.py", "chunked_list": ["from __future__ import print_function\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport random\n\timport utils.torch\n\timport numpy as np\n\tclass LinfPGDAttack:\n\t    def __init__(self, \n\t                model, \n", "                objective, \n\t                epsilon=0.3, \n\t                max_iterations=100, \n\t                base_lr=0.1, \n\t                momentum=0.9, \n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        self.model = model\n", "        self.objective = objective\n\t        self.epsilon = epsilon\n\t        self.max_iterations = max_iterations\n\t        self.base_lr = base_lr\n\t        self.momentum = momentum\n\t        self.rand_init_name = rand_init_name\n\t        self.num_rand_init = num_rand_init\n\t        self.clip_min = clip_min\n\t        self.clip_max = clip_max\n\t    def random_init(self, delta, x, random_name=\"random\"):\n", "        if random_name == 'random':\n\t            delta.data.normal_()\n\t            u = torch.zeros(delta.size(0)).uniform_(0, 1).cuda()\n\t            linf_norm = u / torch.max(delta.abs().view(delta.size(0), -1), dim=1)[0]\n\t            delta.data = self.epsilon * delta.data * linf_norm.view(delta.size(0), 1, 1, 1).data\n\t        elif random_name == 'zero':\n\t            delta.data.zero_()\n\t        else:\n\t            raise ValueError\n\t        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n", "    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        outputs, d_outputs = self.model(adv_x, return_d=True)\n\t        loss = self.objective(outputs, d_outputs, y, perturbations=delta)\n\t        return loss\n\t    def perturb_once(self, x, y):\n\t        delta = torch.zeros_like(x)\n\t        batch_size = x.shape[0]\n\t        global_gradients = torch.zeros_like(delta)\n\t        loss = self.get_loss(x, delta, y)\n", "        success_errors = loss.data.clone()\n\t        success_perturbs = delta.data.clone()\n\t        lrs = (torch.ones_like(success_errors).float() * self.base_lr).cuda()\n\t        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\t        if self.rand_init_name == 'random+zero':\n\t            random_name = random.choice([\"random\", \"zero\"])\n\t            self.random_init(delta, x, random_name)\n\t        else:\n\t            self.random_init(delta, x, self.rand_init_name)\n\t        delta = nn.Parameter(delta)\n", "        delta.requires_grad_()\n\t        for ii in range(self.max_iterations):\n\t            loss = self.get_loss(x, delta, y)\n\t            cond = loss.data > success_errors\n\t            success_errors[cond] = loss.data[cond]\n\t            success_perturbs[cond] = delta.data[cond]\n\t            loss.mean().backward()\n\t            grad = delta.grad.data\n\t            # normalize and add momentum.\n\t            grad.data = torch.sign(grad.data)\n", "            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\t            delta.data += torch.mul(utils.torch.expand_as(lrs, global_gradients), global_gradients)\n\t            delta.data = torch.clamp(delta.data, min=-self.epsilon, max=self.epsilon)\n\t            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\t            delta.grad.data.zero_()\n\t        loss = self.get_loss(x, delta, y)\n\t        cond = loss.data > success_errors\n\t        success_errors[cond] = loss.data[cond]\n\t        success_perturbs[cond] = delta.data[cond]\n\t        return success_errors, success_perturbs\n", "    def perturb(self, x, y):\n\t        \"\"\"\n\t        Given examples (x, y), returns their adversarial counterparts with\n\t        an attack length of eps.\n\t        :param x: input tensor.\n\t        :param y: label tensor.\n\t                  - if None and self.targeted=False, compute y as predicted\n\t                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n", "        \"\"\"\n\t        self.model.eval()\n\t        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = None\n\t        worst_perb = None\n\t        for k in range(self.num_rand_init):\n\t            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            if worst_loss is None:\n\t                worst_loss = curr_worst_loss\n", "                worst_perb = curr_worst_perb\n\t            else:\n\t                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n\tclass SATRStratifiedLinfPGDAttack:\n\t    \"\"\"\n\t    Stratified PGD Attack for the proposed method SATR\n\t    :param predict: forward pass function.\n", "    :param loss_fn: loss function.\n\t    :param eps: maximum distortion.\n\t    :param nb_iter: number of iterations.\n\t    :param eps_iter: attack step size.\n\t    :param rand_init: (optional bool) random initialization.\n\t    :param clip_min: mininum value per input dimension.\n\t    :param clip_max: maximum value per input dimension.\n\t    :param targeted: if the attack is targeted.\n\t    \"\"\"\n\t    def __init__(\n", "                self, \n\t                model, \n\t                objective, \n\t                rr_objective,\n\t                fraction,\n\t                epsilon=0.3, \n\t                epsilon_0 = 0.1,\n\t                max_iterations=100, \n\t                outer_base_lr=0.1, \n\t                inner_base_lr=0.1, \n", "                momentum=0.9,\n\t                rand_init_name=\"random\",\n\t                num_rand_init=1,\n\t                num_classes=10,\n\t                clip_min=0.0,\n\t                clip_max=1.0):\n\t        self.model = model\n\t        self.objective = objective\n\t        self.epsilon = epsilon\n\t        self.max_iterations = max_iterations\n", "        self.momentum = momentum\n\t        self.rand_init_name = rand_init_name\n\t        self.num_rand_init = num_rand_init\n\t        self.clip_min = clip_min\n\t        self.clip_max = clip_max\n\t        self.epsilon_0 = epsilon_0\n\t        self.fraction = fraction\n\t        self.rr_objective = rr_objective\n\t        self.num_classes = num_classes\n\t        self.outer_base_lr = outer_base_lr\n", "        self.inner_base_lr = inner_base_lr\n\t    def get_split(self, size_inp):\n\t        return int(self.fraction * size_inp)\n\t    def random_init(self, delta, x, random_name=\"random\"):\n\t        if random_name == 'random':\n\t            size_inp = delta.size(0)\n\t            delta.data.normal_()\n\t            u = torch.zeros(size_inp).uniform_(0, 1).cuda()\n\t            linf_norm = u / torch.max(delta.abs().view(size_inp, -1), dim=1)[0]\n\t            linf_norm = linf_norm.view(size_inp, 1, 1, 1)\n", "            split = self.get_split(size_inp)\n\t            if self.fraction > 0.0:\n\t                delta.data[:split] = self.epsilon * delta.data[:split] * linf_norm.data[:split]\n\t            if self.fraction < 1.0:\n\t                delta.data[split:] = self.epsilon_0 * delta.data[split:] * linf_norm.data[split:]\n\t        elif random_name == 'zero':\n\t            delta.data.zero_()\n\t        else:\n\t            raise ValueError\n\t        delta.data = (torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data)\n", "    def get_loss(self, x, delta, y):\n\t        adv_x = x + delta\n\t        outputs, d_outputs = self.model(adv_x, return_d=True)\n\t        # Adversarial inputs are created from a fraction of inputs from a batch. Similar to 50% adversarial training\n\t        split = self.get_split(x.size(0))\n\t        if self.fraction == 1.0:\n\t            targets = y[:split]\n\t            adv_outputs = outputs[:split]\n\t            adv_d_outputs = d_outputs[:split]\n\t            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets, perturbations=delta)\n", "            return loss_1\n\t        elif self.fraction == 0.0:\n\t            targets_2 = y[split:]\n\t            adv_outputs_2 = outputs[split:]\n\t            adv_d_outputs_2 = d_outputs[split:]\n\t            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2, perturbations=delta)\n\t            return loss_2\n\t        else:\n\t            targets = y[:split]\n\t            adv_outputs = outputs[:split]\n", "            adv_d_outputs = d_outputs[:split]\n\t            loss_1 = self.rr_objective(adv_outputs, adv_d_outputs, targets, perturbations=delta)\n\t            targets_2 = y[split:]\n\t            adv_outputs_2 = outputs[split:]\n\t            adv_d_outputs_2 = d_outputs[split:]\n\t            loss_2 = self.objective(adv_outputs_2, adv_d_outputs_2, targets_2, perturbations=delta)\n\t            return torch.cat((loss_1, loss_2), dim=0)\n\t    def perturb_once(self, x, y):\n\t        split = self.get_split(x.size(0))\n\t        delta = torch.zeros_like(x)\n", "        batch_size = x.shape[0]\n\t        global_gradients = torch.zeros_like(delta)\n\t        loss = self.get_loss(x, delta, y)\n\t        success_errors = loss.data.clone()\n\t        success_perturbs = delta.data.clone()\n\t        outer_lrs = (torch.ones_like(success_errors[:split]).float() * self.outer_base_lr).cuda()\n\t        inner_lrs = (torch.ones_like(success_errors[split:]).float() * self.inner_base_lr).cuda()\n\t        \"\"\" (numpy.ndarray) Holds per element learning rates. \"\"\"\n\t        if self.rand_init_name == 'random+zero':\n\t            random_name = random.choice([\"random\", \"zero\"])\n", "            self.random_init(delta, x, random_name)\n\t        else:\n\t            self.random_init(delta, x, self.rand_init_name)\n\t        delta = nn.Parameter(delta)\n\t        delta.requires_grad_()\n\t        for ii in range(self.max_iterations):\n\t            loss = self.get_loss(x, delta, y)\n\t            cond = loss.data > success_errors\n\t            success_errors[cond] = loss.data[cond]\n\t            success_perturbs[cond] = delta.data[cond]\n", "            loss.mean().backward()\n\t            grad = delta.grad.data\n\t            # normalize and add momentum.\n\t            grad.data = torch.sign(grad.data)\n\t            global_gradients.data = self.momentum*global_gradients.data + (1 - self.momentum)*grad.data\n\t            delta.data[:split] += torch.mul(utils.torch.expand_as(outer_lrs, global_gradients[:split]), global_gradients[:split])\n\t            delta.data[split:] += torch.mul(utils.torch.expand_as(inner_lrs, global_gradients[split:]), global_gradients[split:])\n\t            if self.fraction > 0.0:\n\t                delta.data[:split] = torch.clamp(delta.data[:split], min=-self.epsilon, max=self.epsilon)\n\t            if self.fraction < 1.0:\n", "                delta.data[split:] = torch.clamp(delta.data[split:], min=-self.epsilon_0, max=self.epsilon_0)\n\t            delta.data = torch.clamp(x.data + delta.data, min=self.clip_min, max=self.clip_max) - x.data\n\t            delta.grad.data.zero_()\n\t        loss = self.get_loss(x, delta, y)\n\t        cond = loss.data > success_errors\n\t        success_errors[cond] = loss.data[cond]\n\t        success_perturbs[cond] = delta.data[cond]\n\t        return success_errors, success_perturbs\n\t    def perturb(self, x, y):\n\t        \"\"\"\n", "        Given examples (x, y), returns their adversarial counterparts with\n\t        an attack length of eps.\n\t        :param x: input tensor.\n\t        :param y: label tensor.\n\t                  - if None and self.targeted=False, compute y as predicted\n\t                    labels.\n\t                  - if self.targeted=True, then y must be the targeted labels.\n\t        :return: tensor containing perturbed inputs.\n\t        \"\"\"\n\t        self.model.eval()\n", "        x = x.detach().clone().cuda()\n\t        y = y.detach().clone().cuda()\n\t        worst_loss = None\n\t        worst_perb = None\n\t        for k in range(self.num_rand_init):\n\t            curr_worst_loss, curr_worst_perb = self.perturb_once(x, y)\n\t            if worst_loss is None:\n\t                worst_loss = curr_worst_loss\n\t                worst_perb = curr_worst_perb\n\t            else:\n", "                cond = curr_worst_loss > worst_loss\n\t                worst_loss[cond] = curr_worst_loss[cond]\n\t                worst_perb[cond] = curr_worst_perb[cond]\n\t        return x + worst_perb\n"]}
{"filename": "utils/imgaug_lib.py", "chunked_list": ["from imgaug import augmenters as iaa\n\tfrom imgaug import dtypes as iadt\n\timport numpy\n\tclass Clip(iaa.Augmenter):\n\t    \"\"\"\n\t    Clip augmenter.\n\t    \"\"\"\n\t    def __init__(self, min=0, max=1, name=None, deterministic=False, random_state=None):\n\t        super(Clip, self).__init__(name=name, deterministic=deterministic, random_state=random_state)\n\t        self.min = min\n", "        \"\"\" (float) Minimum.\"\"\"\n\t        self.max = max\n\t        \"\"\" (float) Maximum. \"\"\"\n\t    def _augment_images(self, images, random_state, parents, hooks):\n\t        iadt.gate_dtypes(images, allowed=[\"float32\"], disallowed=[\n\t            \"bool\", \"uint8\", \"uint16\",\n\t            \"int8\", \"int16\", \"float16\",\n\t            \"uint32\", \"uint64\", \"uint128\",\n\t            \"uint256\", \"int32\", \"int64\",\n\t            \"int128\", \"int256\", \"float64\",\n", "            \"float96\", \"float128\", \"float256\"\n\t        ], augmenter=self)\n\t        converted_images = numpy.clip(images, self.min, self.max)\n\t        return converted_images\n\t    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n\t        return heatmaps\n\t    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n\t        return keypoints_on_images\n\t    def get_parameters(self):\n\t        return []\n", "class Image(iaa.Augmenter):\n\t    \"\"\"\n\t    Image augmenter.\n\t    \"\"\"\n\t    def __init__(self, name=None, deterministic=False, random_state=None):\n\t        super(Image, self).__init__(name=name, deterministic=deterministic, random_state=random_state)\n\t    def _augment_images(self, images, random_state, parents, hooks):\n\t        iadt.gate_dtypes(images, allowed=[\"float32\"], disallowed=[\n\t            \"bool\", \"uint8\", \"uint16\",\n\t            \"int8\", \"int16\", \"float16\",\n", "            \"uint32\", \"uint64\", \"uint128\",\n\t            \"uint256\", \"int32\", \"int64\",\n\t            \"int128\", \"int256\", \"float64\",\n\t            \"float96\", \"float128\", \"float256\"\n\t        ], augmenter=self)\n\t        converted_images = (images * 255).astype(numpy.uint8)\n\t        return converted_images\n\t    def _augment_heatmaps(self, heatmaps, random_state, parents, hooks):\n\t        return heatmaps\n\t    def _augment_keypoints(self, keypoints_on_images, random_state, parents, hooks):\n", "        return keypoints_on_images\n\t    def get_parameters(self):\n\t        return []"]}
{"filename": "utils/lib.py", "chunked_list": ["\"\"\"\n\tSome I/O utilities.\n\t\"\"\"\n\timport os\n\timport json\n\timport numpy as np\n\timport importlib\n\timport pickle\n\timport gc\n\timport socket\n", "import functools\n\t# See https://github.com/h5py/h5py/issues/961\n\timport warnings\n\twarnings.simplefilter(action='ignore', category=FutureWarning)\n\timport h5py\n\twarnings.resetwarnings()\n\t# https://stackoverflow.com/questions/40845304/runtimewarning-numpy-dtype-size-changed-may-indicate-binary-incompatibility\n\twarnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n\twarnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n\tdef write_hdf5(filepath, tensors, keys='tensor'):\n", "    \"\"\"\n\t    Write a simple tensor, i.e. numpy array ,to HDF5.\n\t    :param filepath: path to file to write\n\t    :type filepath: str\n\t    :param tensors: tensor to write\n\t    :type tensors: numpy.ndarray or [numpy.ndarray]\n\t    :param keys: key to use for tensor\n\t    :type keys: str or [str]\n\t    \"\"\"\n\t    #opened_hdf5() # To be sure as there were some weird opening errors.\n", "    assert type(tensors) == np.ndarray or isinstance(tensors, list)\n\t    if isinstance(tensors, list) or isinstance(keys, list):\n\t        assert isinstance(tensors, list) and isinstance(keys, list)\n\t        assert len(tensors) == len(keys)\n\t    if not isinstance(tensors, list):\n\t        tensors = [tensors]\n\t    if not isinstance(keys, list):\n\t        keys = [keys]\n\t    makedir(os.path.dirname(filepath))\n\t    # Problem that during experiments, too many h5df files are open!\n", "    # https://stackoverflow.com/questions/29863342/close-an-open-h5py-data-file\n\t    with h5py.File(filepath, 'w') as h5f:\n\t        for i in range(len(tensors)):\n\t            tensor = tensors[i]\n\t            key = keys[i]\n\t            chunks = list(tensor.shape)\n\t            if len(chunks) > 2:\n\t                chunks[2] = 1\n\t                if len(chunks) > 3:\n\t                    chunks[3] = 1\n", "                    if len(chunks) > 4:\n\t                        chunks[4] = 1\n\t            h5f.create_dataset(key, data=tensor, chunks=tuple(chunks), compression='gzip')\n\t        h5f.close()\n\t        return\n\tdef read_hdf5(filepath, key='tensor', efficient=False):\n\t    \"\"\"\n\t    Read a tensor, i.e. numpy array, from HDF5.\n\t    :param filepath: path to file to read\n\t    :type filepath: str\n", "    :param key: key to read\n\t    :type key: str\n\t    :param efficient: effienct reaidng\n\t    :type efficient: bool\n\t    :return: tensor\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    #opened_hdf5() # To be sure as there were some weird opening errors.\n\t    assert os.path.exists(filepath), 'file %s not found' % filepath\n\t    if efficient:\n", "        h5f = h5py.File(filepath, 'r')\n\t        assert key in [key for key in h5f.keys()], 'key %s does not exist in %s with keys %s' % (key, filepath, ', '.join(h5f.keys()))\n\t        return h5f[key]\n\t    else:\n\t        with h5py.File(filepath, 'r') as h5f:\n\t            assert key in [key for key in h5f.keys()], 'key %s does not exist in %s with keys %s' % (key, filepath, ', '.join(h5f.keys()))\n\t            return h5f[key][()]\n\tdef check_hdf5(filepath, key='tensor'):\n\t    \"\"\"\n\t    Check a file without loading data.\n", "    :param filepath: path to file to read\n\t    :type filepath: str\n\t    :param key: key to read\n\t    :type key: str\n\t    :return: can be loaded or not\n\t    :rtype: bool\n\t    \"\"\"\n\t    opened_hdf5()  # To be sure as there were some weird opening errors.\n\t    assert os.path.exists(filepath), 'file %s not found' % filepath\n\t    try:\n", "        with h5py.File(filepath, 'r') as h5f:\n\t            assert key in [key for key in h5f.keys()], 'key %s does not exist in %s' % (key, filepath)\n\t            tensor = h5f.get('tensor')\n\t            # That's it ...\n\t            return True\n\t    except:\n\t        return False\n\tdef opened_hdf5():\n\t    \"\"\"\n\t    Close all open HDF5 files and report number of closed files.\n", "    :return: number of closed files\n\t    :rtype: int\n\t    \"\"\"\n\t    opened = 0\n\t    for obj in gc.get_objects():  # Browse through ALL objects\n\t        try:\n\t            # is instance check may also fail!\n\t            if isinstance(obj, h5py.File):  # Just HDF5 files\n\t                obj.close()\n\t                opened += 1\n", "        except:\n\t            pass  # Was already closed\n\t    return opened\n\tdef write_pickle(file, mixed):\n\t    \"\"\"\n\t    Write a variable to pickle.\n\t    :param file: path to file to write\n\t    :type file: str\n\t    :return: mixed\n\t    :rtype: mixed\n", "    \"\"\"\n\t    makedir(os.path.dirname(file))\n\t    handle = open(file, 'wb')\n\t    pickle.dump(mixed, handle)\n\t    handle.close()\n\tdef read_pickle(file):\n\t    \"\"\"\n\t    Read pickle file.\n\t    :param file: path to file to read\n\t    :type file: str\n", "    :return: mixed\n\t    :rtype: mixed\n\t    \"\"\"\n\t    assert os.path.exists(file), 'file %s not found' % file\n\t    handle = open(file, 'rb')\n\t    results = pickle.load(handle)\n\t    handle.close()\n\t    return results\n\tdef read_json(file):\n\t    \"\"\"\n", "    Read a JSON file.\n\t    :param file: path to file to read\n\t    :type file: str\n\t    :return: parsed JSON as dict\n\t    :rtype: dict\n\t    \"\"\"\n\t    assert os.path.exists(file), 'file %s not found' % file\n\t    with open(file, 'r') as fp:\n\t        return json.load(fp)\n\tdef write_json(file, data):\n", "    \"\"\"\n\t    Read a JSON file.\n\t    :param file: path to file to read\n\t    :type file: str\n\t    :param data: data to write\n\t    :type data: mixed\n\t    :return: parsed JSON as dict\n\t    :rtype: dict\n\t    \"\"\"\n\t    makedir(os.path.dirname(file))\n", "    with open(file, 'w') as fp:\n\t        json.dump(data, fp)\n\tdef makedir(dir):\n\t    \"\"\"\n\t    Creates directory if it does not exist.\n\t    :param dir: directory path\n\t    :type dir: str\n\t    \"\"\"\n\t    if dir and not os.path.exists(dir):\n\t        os.makedirs(dir)\n", "def remove(filepath):\n\t    \"\"\"\n\t    Remove a file.\n\t    :param filepath: path to file\n\t    :type filepath: str\n\t    \"\"\"\n\t    if os.path.isfile(filepath) and os.path.exists(filepath):\n\t        os.unlink(filepath)\n\tdef get_class(module_name, class_name):\n\t    \"\"\"\n", "    See https://stackoverflow.com/questions/1176136/convert-string-to-python-class-object?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa.\n\t    :param module_name: module holding class\n\t    :type module_name: str\n\t    :param class_name: class name\n\t    :type class_name: str\n\t    :return: class or False\n\t    \"\"\"\n\t    # load the module, will raise ImportError if module cannot be loaded\n\t    try:\n\t        m = importlib.import_module(module_name)\n", "    except ImportError as e:\n\t        log('%s' % e, LogLevel.ERROR)\n\t        return False\n\t    # get the class, will raise AttributeError if class cannot be found\n\t    try:\n\t        c = getattr(m, class_name)\n\t    except AttributeError as e:\n\t        log('%s' % e, LogLevel.ERROR)\n\t        return False\n\t    return c\n", "def hostname():\n\t    \"\"\"\n\t    Get hostname.\n\t    :return: hostname\n\t    :rtype: str\n\t    \"\"\"\n\t    return socket.gethostname()\n\tdef pid():\n\t    \"\"\"\n\t    PID.\n", "    :return: PID\n\t    :rtype: int\n\t    \"\"\"\n\t    return os.getpid()\n\tdef partial(f, *args, **kwargs):\n\t    \"\"\"\n\t    Create partial while preserving __name__ and __doc__.\n\t    :param f: function\n\t    :type f: callable\n\t    :param args: arguments\n", "    :type args: dict\n\t    :param kwargs: keyword arguments\n\t    :type kwargs: dict\n\t    :return: partial\n\t    :rtype: callable\n\t    \"\"\"\n\t    p = functools.partial(f, *args, **kwargs)\n\t    functools.update_wrapper(p, f)\n\t    return p"]}
{"filename": "utils/torch.py", "chunked_list": ["import torch\n\timport numpy\n\timport scipy.ndimage\n\timport math\n\tfrom . import numpy as cnumpy\n\timport random\n\tSMALL_VALUE = 1e-8\n\tdef set_seed(seed):\n\t    \"\"\"Sets seed\"\"\"\n\t    if torch.cuda.is_available():\n", "        torch.cuda.manual_seed(seed)\n\t    torch.manual_seed(seed)\n\t    numpy.random.seed(seed)\n\t    random.seed(seed)\n\t    torch.backends.cudnn.benchmark = False\n\t    torch.backends.cudnn.deterministic = True\n\tdef memory():\n\t    \"\"\"\n\t    Get memory usage.\n\t    :return: memory usage\n", "    :rtype: str\n\t    \"\"\"\n\t    index = torch.cuda.current_device()\n\t    # results are in bytes\n\t    # Decimal\n\t    # Value \tMetric\n\t    # 1000 \tkB \tkilobyte\n\t    # 1000^2 \tMB \tmegabyte\n\t    # 1000^3 \tGB \tgigabyte\n\t    # 1000^4 \tTB \tterabyte\n", "    # 1000^5 \tPB \tpetabyte\n\t    # 1000^6 \tEB \texabyte\n\t    # 1000^7 \tZB \tzettabyte\n\t    # 1000^8 \tYB \tyottabyte\n\t    # Binary\n\t    # Value \tIEC \tJEDEC\n\t    # 1024 \tKiB \tkibibyte \tKB \tkilobyte\n\t    # 1024^2 \tMiB \tmebibyte \tMB \tmegabyte\n\t    # 1024^3 \tGiB \tgibibyte \tGB \tgigabyte\n\t    # 1024^4 \tTiB \ttebibyte \t-\n", "    # 1024^5 \tPiB \tpebibyte \t-\n\t    # 1024^6 \tEiB \texbibyte \t-\n\t    # 1024^7 \tZiB \tzebibyte \t-\n\t    # 1024^8 \tYiB \tyobibyte \t-\n\t    return '%g/%gMiB' % (\n\t        BMiB(torch.cuda.memory_allocated(index) + torch.cuda.memory_cached(index)),\n\t        BMiB(torch.cuda.max_memory_allocated(index) + torch.cuda.max_memory_cached(index)),\n\t    )\n\tdef is_cuda(mixed):\n\t    \"\"\"\n", "    Check if model/tensor is on CUDA.\n\t    :param mixed: model or tensor\n\t    :type mixed: torch.nn.Module or torch.autograd.Variable or torch.Tensor\n\t    :return: on cuda\n\t    :rtype: bool\n\t    \"\"\"\n\t    assert isinstance(mixed, torch.nn.Module) or isinstance(mixed, torch.autograd.Variable) \\\n\t        or isinstance(mixed, torch.Tensor), 'mixed has to be torch.nn.Module, torch.autograd.Variable or torch.Tensor'\n\t    is_cuda = False\n\t    if isinstance(mixed, torch.nn.Module):\n", "        is_cuda = True\n\t        for parameters in list(mixed.parameters()):\n\t            is_cuda = is_cuda and parameters.is_cuda\n\t    if isinstance(mixed, torch.autograd.Variable):\n\t        is_cuda = mixed.is_cuda\n\t    if isinstance(mixed, torch.Tensor):\n\t        is_cuda = mixed.is_cuda\n\t    return is_cuda\n\tdef estimate_size(mixed):\n\t    \"\"\"\n", "    Estimate tensor size.\n\t    :param tensor: tensor or model\n\t    :type tensor: numpy.ndarray, torch.tensor, torch.autograd.Variable or torch.nn.Module\n\t    :return: size in bits\n\t    :rtype: int\n\t    \"\"\"\n\t    # PyTorch types:\n\t    # Data type \tdtype \tCPU tensor \tGPU tensor\n\t    # 32-bit floating point \ttorch.float32 or torch.float \ttorch.FloatTensor \ttorch.cuda.FloatTensor\n\t    # 64-bit floating point \ttorch.float64 or torch.double \ttorch.DoubleTensor \ttorch.cuda.DoubleTensor\n", "    # 16-bit floating point \ttorch.float16 or torch.half \ttorch.HalfTensor \ttorch.cuda.HalfTensor\n\t    # 8-bit integer (unsigned) \ttorch.uint8 \ttorch.ByteTensor \ttorch.cuda.ByteTensor\n\t    # 8-bit integer (signed) \ttorch.int8 \ttorch.CharTensor \ttorch.cuda.CharTensor\n\t    # 16-bit integer (signed) \ttorch.int16 or torch.short \ttorch.ShortTensor \ttorch.cuda.ShortTensor\n\t    # 32-bit integer (signed) \ttorch.int32 or torch.int \ttorch.IntTensor \ttorch.cuda.IntTensor\n\t    # 64-bit integer (signed) \ttorch.int64 or torch.long \ttorch.LongTensor \ttorch.cuda.LongTensor\n\t    # Numpy types:\n\t    # Data type \tDescription\n\t    # bool_ \tBoolean (True or False) stored as a byte\n\t    # int_ \tDefault integer type (same as C long; normally either int64 or int32)\n", "    # intc \tIdentical to C int (normally int32 or int64)\n\t    # intp \tInteger used for indexing (same as C ssize_t; normally either int32 or int64)\n\t    # int8 \tByte (-128 to 127)\n\t    # int16 \tInteger (-32768 to 32767)\n\t    # int32 \tInteger (-2147483648 to 2147483647)\n\t    # int64 \tInteger (-9223372036854775808 to 9223372036854775807)\n\t    # uint8 \tUnsigned integer (0 to 255)\n\t    # uint16 \tUnsigned integer (0 to 65535)\n\t    # uint32 \tUnsigned integer (0 to 4294967295)\n\t    # uint64 \tUnsigned integer (0 to 18446744073709551615)\n", "    # float_ \tShorthand for float64.\n\t    # float16 \tHalf precision float: sign bit, 5 bits exponent, 10 bits mantissa\n\t    # float32 \tSingle precision float: sign bit, 8 bits exponent, 23 bits mantissa\n\t    # float64 \tDouble precision float: sign bit, 11 bits exponent, 52 bits mantissa\n\t    # complex_ \tShorthand for complex128.\n\t    # complex64 \tComplex number, represented by two 32-bit floats (real and imaginary components)\n\t    # complex128 \tComplex number, represented by two 64-bit floats (real and imaginary components)\n\t    types8 = [\n\t        torch.uint8, torch.int8,\n\t        numpy.int8, numpy.uint8, numpy.bool_,\n", "    ]\n\t    types16 = [\n\t        torch.float16, torch.half,\n\t        torch.int16, torch.short,\n\t        numpy.int16, numpy.uint16, numpy.float16,\n\t    ]\n\t    types32 = [\n\t        torch.float32, torch.float,\n\t        torch.int32, torch.int,\n\t        numpy.int32, numpy.uint32, numpy.float32,\n", "    ]\n\t    types64 = [\n\t        torch.float64, torch.double,\n\t        torch.int64, torch.long,\n\t        numpy.int64, numpy.uint64, numpy.float64, numpy.complex64,\n\t        numpy.int_, numpy.float_\n\t    ]\n\t    types128 = [\n\t        numpy.complex_, numpy.complex128\n\t    ]\n", "    if isinstance(mixed, torch.nn.Module):\n\t        size = 0\n\t        modules = mixed.modules()\n\t        for module in modules:\n\t            for parameters in list(module.parameters()):\n\t                size += estimate_size(parameters)\n\t        return size\n\t    if isinstance(mixed, (torch.Tensor, numpy.ndarray)):\n\t        if mixed.dtype in types128:\n\t            bits = 128\n", "        elif mixed.dtype in types64:\n\t            bits = 64\n\t        elif mixed.dtype in types32:\n\t            bits = 32\n\t        elif mixed.dtype in types16:\n\t            bits = 16\n\t        elif mixed.dtype in types8:\n\t            bits = 8\n\t        else:\n\t            assert False, 'could not identify torch.Tensor or numpy.ndarray type %s' % mixed.type()\n", "        size = numpy.prod(mixed.shape)\n\t        return size*bits\n\t    elif isinstance(mixed, torch.autograd.Variable):\n\t        return estimate_size(mixed.data)\n\t    else:\n\t        assert False, 'unsupported tensor size for estimating size, either numpy.ndarray, torch.tensor or torch.autograd.Variable'\n\tdef bits2MiB(bits):\n\t    \"\"\"\n\t    Convert bits to MiB.\n\t    :param bits: number of bits\n", "    :type bits: int\n\t    :return: MiB\n\t    :rtype: float\n\t    \"\"\"\n\t    return bits/(8*1024*1024)\n\tdef bits2MB(bits):\n\t    \"\"\"\n\t    Convert bits to MB.\n\t    :param bits: number of bits\n\t    :type bits: int\n", "    :return: MiB\n\t    :rtype: float\n\t    \"\"\"\n\t    return bits/(8*1000*1000)\n\tdef bytes2MiB(bytes):\n\t    \"\"\"\n\t    Convert bytes to MiB.\n\t    :param bytes: number of bytes\n\t    :type bytes: int\n\t    :return: MiB\n", "    :rtype: float\n\t    \"\"\"\n\t    return bytes/(1024*1024)\n\tdef bytes2MB(bytes):\n\t    \"\"\"\n\t    Convert bytes to MB.\n\t    :param bytes: number of bytes\n\t    :type bytes: int\n\t    :return: MiB\n\t    :rtype: float\n", "    \"\"\"\n\t    return bytes/(1000*1000)\n\tbMiB = bits2MiB\n\tBMiB = bytes2MiB\n\tbMB = bits2MB\n\tBMB = bytes2MB\n\tdef binary_labels(classes):\n\t    \"\"\"\n\t    Convert 0,1 labels to -1,1 labels.\n\t    :param classes: classes as B x 1\n", "    :type classes: torch.autograd.Variable or torch.Tensor\n\t    \"\"\"\n\t    classes[classes == 0] = -1\n\t    return classes\n\tdef one_hot(classes, C):\n\t    \"\"\"\n\t    Convert class labels to one-hot vectors.\n\t    :param classes: classes as B x 1\n\t    :type classes: torch.autograd.Variable or torch.Tensor\n\t    :param C: number of classes\n", "    :type C: int\n\t    :return: one hot vector as B x C\n\t    :rtype: torch.autograd.Variable or torch.Tensor\n\t    \"\"\"\n\t    assert isinstance(classes, torch.autograd.Variable) or isinstance(classes, torch.Tensor), 'classes needs to be torch.autograd.Variable or torch.Tensor'\n\t    assert len(classes.size()) == 2 or len(classes.size()) == 1, 'classes needs to have rank 2 or 1'\n\t    assert C > 0\n\t    if len(classes.size()) < 2:\n\t        classes = classes.view(-1, 1)\n\t    one_hot = torch.Tensor(classes.size(0), C)\n", "    if is_cuda(classes):\n\t         one_hot = one_hot.cuda()\n\t    if isinstance(classes, torch.autograd.Variable):\n\t        one_hot = torch.autograd.Variable(one_hot)\n\t    one_hot.zero_()\n\t    one_hot.scatter_(1, classes, 1)\n\t    return one_hot\n\tdef project_ball(tensor, epsilon=1, ord=2):\n\t    \"\"\"\n\t    Compute the orthogonal projection of the input tensor (as vector) onto the L_ord epsilon-ball.\n", "    **Assumes the first dimension to be batch dimension, which is preserved.**\n\t    :param tensor: variable or tensor\n\t    :type tensor: torch.autograd.Variable or torch.Tensor\n\t    :param epsilon: radius of ball.\n\t    :type epsilon: float\n\t    :param ord: order of norm\n\t    :type ord: int\n\t    :return: projected vector\n\t    :rtype: torch.autograd.Variable or torch.Tensor\n\t    \"\"\"\n", "    assert isinstance(tensor, torch.Tensor) or isinstance(tensor, torch.autograd.Variable), 'given tensor should be torch.Tensor or torch.autograd.Variable'\n\t    if ord == 0:\n\t        assert epsilon >= 0\n\t        sorted, _ = torch.sort(tensor.view(tensor.size()[0], -1), dim=1)\n\t        k = int(math.ceil(epsilon))\n\t        assert k > 0\n\t        thresholds = sorted[:, -k]\n\t        mask = (tensor >= expand_as(thresholds, tensor)).type(tensor.type())\n\t        tensor *= mask\n\t    elif ord == 1:\n", "        # ! Does not allow differentiation obviously!\n\t        cuda = is_cuda(tensor)\n\t        array = tensor.detach().cpu().numpy()\n\t        array = cnumpy.project_ball(array, epsilon=epsilon, ord=ord)\n\t        tensor = torch.from_numpy(array)\n\t        if cuda:\n\t            tensor = tensor.cuda()\n\t    elif ord == 2:\n\t        size = tensor.size()\n\t        flattened_size = numpy.prod(numpy.array(size[1:]))\n", "        tensor = tensor.view(-1, flattened_size)\n\t        clamped = torch.clamp(epsilon/torch.norm(tensor, 2, dim=1), max=1)\n\t        clamped = clamped.view(-1, 1)\n\t        tensor = tensor * clamped\n\t        if len(size) == 4:\n\t            tensor = tensor.view(-1, size[1], size[2], size[3])\n\t        elif len(size) == 2:\n\t            tensor = tensor.view(-1, size[1])\n\t    elif ord == float('inf'):\n\t        tensor = torch.clamp(tensor, min=-epsilon, max=epsilon)\n", "    else:\n\t        raise NotImplementedError()\n\t    return tensor\n\tdef project_sphere(tensor, epsilon=1, ord=2):\n\t    \"\"\"\n\t    Compute the orthogonal projection of the input tensor (as vector) onto the L_ord epsilon-ball.\n\t    **Assumes the first dimension to be batch dimension, which is preserved.**\n\t    :param tensor: variable or tensor\n\t    :type tensor: torch.autograd.Variable or torch.Tensor\n\t    :param epsilon: radius of ball.\n", "    :type epsilon: float\n\t    :param ord: order of norm\n\t    :type ord: int\n\t    :return: projected vector\n\t    :rtype: torch.autograd.Variable or torch.Tensor\n\t    \"\"\"\n\t    assert isinstance(tensor, torch.Tensor) or isinstance(tensor, torch.autograd.Variable), 'given tensor should be torch.Tensor or torch.autograd.Variable'\n\t    size = tensor.size()\n\t    flattened_size = numpy.prod(numpy.array(size[1:]))\n\t    tensor = tensor.view(-1, flattened_size)\n", "    tensor = tensor/torch.norm(tensor, dim=1, ord=ord).view(-1, 1)\n\t    tensor *= epsilon\n\t    if len(size) == 4:\n\t        tensor = tensor.view(-1, size[1], size[2], size[3])\n\t    elif len(size) == 2:\n\t        tensor = tensor.view(-1, size[1])\n\t    return tensor\n\tdef tensor_or_value(mixed):\n\t    \"\"\"\n\t    Get tensor or single value.\n", "    :param mixed: variable, tensor or value\n\t    :type mixed: mixed\n\t    :return: tensor or value\n\t    :rtype: torch.Tensor or value\n\t    \"\"\"\n\t    if isinstance(mixed, torch.Tensor):\n\t        if mixed.numel() > 1:\n\t            return mixed\n\t        else:\n\t            return mixed.item()\n", "    elif isinstance(mixed, torch.autograd.Variable):\n\t        return tensor_or_value(mixed.cpu().data)\n\t    else:\n\t        return mixed\n\tdef as_variable(mixed, cuda=False, requires_grad=False):\n\t    \"\"\"\n\t    Get a tensor or numpy array as variable.\n\t    :param mixed: input tensor\n\t    :type mixed: torch.Tensor or numpy.ndarray\n\t    :param device: gpu or not\n", "    :type device: bool\n\t    :param requires_grad: gradients\n\t    :type requires_grad: bool\n\t    :return: variable\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    assert isinstance(mixed, numpy.ndarray) or isinstance(mixed, torch.Tensor), 'input needs to be numpy.ndarray or torch.Tensor'\n\t    if isinstance(mixed, numpy.ndarray):\n\t        mixed = torch.from_numpy(mixed)\n\t    if cuda:\n", "        mixed = mixed.cuda()\n\t    return torch.autograd.Variable(mixed, requires_grad)\n\tdef tile(a, dim, n_tile):\n\t    \"\"\"\n\t    Numpy-like tiling in torch.\n\t    https://discuss.pytorch.org/t/how-to-tile-a-tensor/13853/2\n\t    :param a: tensor\n\t    :type a: torch.Tensor or torch.autograd.Variable\n\t    :param dim: dimension to tile\n\t    :type dim: int\n", "    :param n_tile: number of tiles\n\t    :type n_tile: int\n\t    :return: tiled tensor\n\t    :rtype: torch.Tensor or torch.autograd.Variable\n\t    \"\"\"\n\t    init_dim = a.size(dim)\n\t    repeat_idx = [1] * a.dim()\n\t    repeat_idx[dim] = n_tile\n\t    a = a.repeat(*(repeat_idx))\n\t    order_index = torch.LongTensor(numpy.concatenate([init_dim * numpy.arange(n_tile) + i for i in range(init_dim)]))\n", "    if is_cuda(a):\n\t        order_index = order_index.cuda()\n\t    return torch.index_select(a, dim, order_index)\n\tdef expand_as(tensor, tensor_as):\n\t    \"\"\"\n\t    Expands the tensor using view to allow broadcasting.\n\t    :param tensor: input tensor\n\t    :type tensor: torch.Tensor or torch.autograd.Variable\n\t    :param tensor_as: reference tensor\n\t    :type tensor_as: torch.Tensor or torch.autograd.Variable\n", "    :return: tensor expanded with singelton dimensions as tensor_as\n\t    :rtype: torch.Tensor or torch.autograd.Variable\n\t    \"\"\"\n\t    view = list(tensor.size())\n\t    for i in range(len(tensor.size()), len(tensor_as.size())):\n\t        view.append(1)\n\t    return tensor.view(view)\n\tdef get_exponential_scheduler(optimizer, batches_per_epoch, gamma=0.97):\n\t    \"\"\"\n\t    Get exponential scheduler.\n", "    Note that the resulting optimizer's step function is called after each batch!\n\t    :param optimizer: optimizer\n\t    :type optimizer: torch.optim.Optimizer\n\t    :param batches_per_epoch: number of batches per epoch\n\t    :type batches_per_epoch: int\n\t    :param gamma: gamma\n\t    :type gamma: float\n\t    :return: scheduler\n\t    :rtype: torch.optim.lr_scheduler.LRScheduler\n\t    \"\"\"\n", "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: gamma ** math.floor(epoch/batches_per_epoch)])\n\tdef classification_error(logits, targets, reduction='mean'):\n\t    \"\"\"\n\t    Accuracy.\n\t    :param logits: predicted classes\n\t    :type logits: torch.autograd.Variable\n\t    :param targets: target classes\n\t    :type targets: torch.autograd.Variable\n\t    :param reduce: reduce to number or keep per element\n\t    :type reduce: bool\n", "    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    if logits.size()[1] > 1:\n\t        # softmax transformation is not needed since only the argmax index is used for calculating accuracy\n\t        probs = logits\n\t        # probs = torch.nn.functional.softmax(logits, dim=1)\n\t    else:\n\t        probs = torch.sigmoid(logits)\n\t    return prob_classification_error(probs, targets, reduction=reduction)\n", "def prob_classification_error(probs, targets, reduction='mean'):\n\t    \"\"\"\n\t    Accuracy.\n\t    :param logits: predicted classes\n\t    :type logits: torch.autograd.Variable\n\t    :param targets: target classes\n\t    :type targets: torch.autograd.Variable\n\t    :param reduce: reduce to number or keep per element\n\t    :type reduce: bool\n\t    :return: error\n", "    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    # assert probs.size()[0] == targets.size()[0]\n\t    # assert len(list(targets.size())) == 1\n\t    # assert len(list(probs.size())) == 2\n\t    if probs.size()[1] > 1:\n\t        values, indices = torch.max(probs, dim=1)\n\t    else:\n\t        # Threshold is assumed to be at 0.5. Prediction = 1 if probability >= 0.5; else 0\n\t        indices = torch.round(probs).view(-1)\n", "    errors = torch.clamp(torch.abs(indices.long() - targets.long()), max=1)\n\t    if reduction == 'mean':\n\t        return torch.mean(errors.float())\n\t    elif reduction == 'sum':\n\t        return torch.sum(errors.float())\n\t    else:\n\t        return errors\n\tdef negat_log_proba(probs, reduction=\"none\"):\n\t    # Stable calculation of `-log(p)`\n\t    return torch.nn.functional.binary_cross_entropy(torch.clamp(probs, min=0.0, max=1.0),\n", "                                                    torch.ones_like(probs).cuda(), reduction=reduction)\n\tdef negat_log_one_minus_proba(probs, reduction=\"none\"):\n\t    # Stable calculation of `-log(1 - p)`\n\t    return torch.nn.functional.binary_cross_entropy(torch.clamp(probs, min=0.0, max=1.0),\n\t                                                    torch.zeros_like(probs).cuda(), reduction=reduction)\n\tdef reduce_loss(loss, reduction):\n\t    if reduction == \"none\":\n\t        return loss\n\t    elif reduction == \"mean\":\n\t        return torch.mean(loss)\n", "    elif reduction == \"sum\":\n\t        return torch.sum(loss)\n\t    else:\n\t        raise ValueError(\"Invalid value '{}' for reduction\".format(reduction))\n\tdef rcd_accept_misclassify_loss_bak(logits, targets, reduction='mean'):\n\t    # Attack loss function: -log(h_y(x) + h_{k+1}(x))\n\t    prob_outputs = torch.nn.functional.softmax(logits, dim=1)\n\t    total_class = logits.size(1)\n\t    masked_prob_outputs = prob_outputs * (one_hot(targets, total_class) +\n\t                                          one_hot(torch.ones_like(targets) * (total_class - 1), total_class))\n", "    loss = negat_log_proba(torch.sum(masked_prob_outputs, dim=1))\n\t    return reduce_loss(loss, reduction)\n\tdef rcd_accept_misclassify_loss(logits, targets, reduction='mean'):\n\t    # Attack loss function: -log[1 - max_{j \\notin {y, k+1}} h_j(x)]\n\t    N, K = logits.size()\n\t    prob = torch.nn.functional.softmax(logits, dim=1)\n\t    # `(K-1, K-2)` array where row `i` of the array will have the value `i` omitted\n\t    indices_temp = numpy.array([[j for j in range(K-1) if j != i] for i in range(K-1)])\n\t    targets_np = targets.detach().cpu().numpy().astype(numpy.int)\n\t    indices = torch.tensor(indices_temp[targets_np, :])\n", "    masked_prob = prob[torch.unsqueeze(torch.arange(N), 1), indices]\n\t    max_prob = torch.max(masked_prob, dim=1)[0]\n\t    loss = -negat_log_proba(max_prob)\n\t    return reduce_loss(loss, reduction)\n\tdef rcd_reject_loss(logits, targets, reduction='mean'):\n\t    prob_outputs = torch.nn.functional.softmax(logits, dim=1)\n\t    loss = -negat_log_proba(prob_outputs[:, -1])\n\t    # total_class = logits.size(1)  # k + 1\n\t    # masked_prob_outputs = prob_outputs * one_hot(torch.ones_like(targets) * (total_class - 1), total_class)\n\t    # loss = -torch.log(1 - torch.sum(masked_prob_outputs, dim=1) + SMALL_VALUE)\n", "    return reduce_loss(loss, reduction)\n\tdef robust_detection_loss(logits, targets, reduction='mean'):\n\t    N, K = logits.size()\n\t    prob = torch.nn.functional.softmax(logits, dim=1)\n\t    # `(K-1, K-2)` array where row `i` of the array will have the value `i` omitted\n\t    indices_temp = numpy.array([[j for j in range(K-1) if j != i] for i in range(K-1)])\n\t    targets_np = targets.detach().cpu().numpy().astype(numpy.int)\n\t    indices = torch.tensor(indices_temp[targets_np, :])\n\t    masked_prob = prob[torch.unsqueeze(torch.arange(N), 1), indices]\n\t    max_prob = torch.max(masked_prob, dim=1)[0]\n", "    return reduce_loss(max_prob, reduction)\n\tdef uniform_confidence_loss(logits, targets, reduction='mean', scaling_factor=100.0):\n\t    # Log-sum-exp based attack objective for confidence based rejection methods\n\t    loss = torch.logsumexp(logits, dim=1) - (1. / scaling_factor) * torch.logsumexp(scaling_factor * logits, dim=1)\n\t    return reduce_loss(loss, reduction)\n\tdef uniform_confidence_loss_v2(logits, targets, reduction='mean', scaling_factor=100.0):\n\t    # Log-sum-exp based attack objective for confidence based rejection methods\n\t    # Use the loss below to directly minimize the maximum logit\n\t    loss = (-1. / scaling_factor) * torch.logsumexp(scaling_factor * logits, dim=1)\n\t    return reduce_loss(loss, reduction)\n", "def hinge_confidence_loss(logits, targets, reduction='mean', scaling_factor=100.0, thresh_reject=1.0):\n\t    # Hinge loss based attack objective for confidence based rejection methods\n\t    preds = torch.softmax(logits, axis=1)\n\t    loss = torch.nn.functional.relu(thresh_reject -\n\t                                    (1. / scaling_factor) * torch.logsumexp(scaling_factor * preds, dim=1))\n\t    return reduce_loss(loss, reduction)\n\tdef softmax_square_loss(logits, targets, reduction='mean'):\n\t    # Used as attack objective for confidence based rejection methods CCAT and adversarial training. Maximizing the\n\t    # squared-softmax loss pushes the predictions close to uniform over all classes\n\t    softmax_output = torch.softmax(logits, axis=1)\n", "    loss = negat_log_proba(torch.sum(softmax_output * softmax_output, axis=1))\n\t    return reduce_loss(loss, reduction)\n\tdef entropy_confidence_loss(logits, targets, reduction='mean'):\n\t    # Used as attack objective for confidence based rejection methods CCAT and adversarial training. Maximizing the\n\t    # entropy loss pushes the predictions close to uniform over all classes\n\t    proba = torch.softmax(logits, axis=1)\n\t    loss = torch.sum(proba * negat_log_proba(proba), axis=1)\n\t    return reduce_loss(loss, reduction)\n\tdef high_conf_misclassify_loss(logits, true_classes, reduction='mean'):\n\t    prob_outputs = torch.nn.functional.softmax(logits, dim=1)\n", "    masked_prob_outputs = prob_outputs * (1 - one_hot(true_classes, prob_outputs.size(1)))\n\t    # maximum prob. of a class different from the true class\n\t    loss = -negat_log_proba(torch.max(masked_prob_outputs, dim=1)[0])\n\t    return reduce_loss(loss, reduction)\n\tdef f7p_loss(logits, true_classes, reduction='mean'):\n\t    # Used as attack objective for confidence based rejection methods CCAT and adversarial training\n\t    if logits.size(1) > 1:\n\t        current_probabilities = torch.nn.functional.softmax(logits, dim=1)\n\t        current_probabilities = current_probabilities * (1 - one_hot(true_classes, current_probabilities.size(1)))\n\t        loss = torch.max(current_probabilities, dim=1)[0]   # maximum prob. of a class different from the true class\n", "    else:\n\t        # binary\n\t        prob = torch.nn.functional.sigmoid(logits.view(-1))\n\t        loss = true_classes.float() * (1. - prob) + (1. - true_classes.float()) * prob\n\t    return reduce_loss(loss, reduction)\n\tdef cw_loss(logits, true_classes, target_classes, reduction='mean'):\n\t    \"\"\"\n\t    Loss.\n\t    :param logits: predicted classes\n\t    :type logits: torch.autograd.Variable\n", "    :param targets: target classes\n\t    :type targets: torch.autograd.Variable\n\t    :param reduction: reduction type\n\t    :type reduction: str\n\t    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    assert logits.size()[0] == true_classes.size()[0]\n\t    assert len(list(true_classes.size())) == 1  # or (len(list(targets.size())) == 2 and targets.size(1) == 1)\n\t    assert len(list(target_classes.size())) == 1\n", "    assert len(list(logits.size())) == 2\n\t    if logits.size()[1] > 1:\n\t        u = torch.arange(logits.shape[0])\n\t        loss = -(logits[u, true_classes] - torch.max((1 - one_hot(true_classes, logits.size(1))) * logits, dim=1)[0])\n\t    else:\n\t        raise ValueError\n\t    return reduce_loss(loss, reduction)\n\tdef conf_cls_loss(logits, d_probs, targets, reduction='mean'):\n\t    # Cross entropy loss for correct classification: -log f_y(x)\n\t    return classification_loss(logits, targets, reduction=reduction)\n", "'''\n\tNotation:\n\t`h_{\\bot}(x)` is the predicted probability of rejection and `h_y(x)` is the predicted probability of class `y`.\n\t'''\n\tdef conf_accept_loss(logits, d_probs, targets, reduction='mean'):\n\t    # Cross entropy loss for accept: log[h_{\\bot}(x)]\n\t    return -negat_log_proba(d_probs.view(-1), reduction)\n\tdef conf_accept_cls_loss(logits, d_probs, targets, reduction='mean'):\n\t    # Cross entropy loss for accept and correctly classify: -log[h_y(x) * (1 - h_{\\bot}(x))]\n\t    return classification_loss(logits, targets, reduction=reduction) + negat_log_one_minus_proba(d_probs.view(-1), reduction)\n", "def conf_prob_loss(logits, d_probs, targets, reduction='mean'):\n\t    # loss function: -log[(1 - h_{\\bot}(x)) h_y(x) + h_{\\bot}(x)]\n\t    probs = torch.softmax(logits, dim=1)\n\t    combine_probs = probs * (1 - d_probs) + d_probs\n\t    return torch.nn.functional.nll_loss(-1. * negat_log_proba(combine_probs), targets, reduction=reduction)\n\t'''\n\tDifferent versions of the attack loss function in the larger epsilon-ball for the proposed method SATR are named \n\t`conf_prob_loss_*` and defined below.\n\t'''\n\tdef rcd_targeted_loss(logits, targets, reduction='mean'):\n", "    batch_size, num_classes = logits.size()\n\t    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n\t    return reduce_loss(log_probs[torch.arange(batch_size), targets], reduction)\n\tdef atrr_targeted_loss(logits, aux_probs, targets, reduction='mean'):\n\t    batch_size, num_classes = logits.size()\n\t    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n\t    combined_probs = log_probs[torch.arange(batch_size), targets] - negat_log_proba(aux_probs.view(-1))\n\t    return reduce_loss(combined_probs, reduction)\n\tdef satr_targeted_loss(logits, d_probs, targets, reduction='mean'):\n\t    # A more numerically stable implementation:\n", "    # Calculates the log-softmax function directly instead of softmax followed by log.\n\t    # Takes the sum of the log-probabilities, rather than product followed by log.\n\t    batch_size, num_classes = logits.size()\n\t    combined_probs = torch.nn.functional.log_softmax(logits, dim=1) - negat_log_proba(1. - d_probs)\n\t    return reduce_loss(combined_probs[torch.arange(batch_size), targets], reduction)\n\tdef ccat_targeted_loss(logits, targets, reduction='mean'):\n\t    batch_size, num_classes = logits.size()\n\t    log_probs = torch.nn.functional.log_softmax(logits, dim=1)\n\t    return reduce_loss(log_probs[torch.arange(batch_size), targets], reduction)\n\tdef conf_prob_loss_A1(logits, d_probs, targets, reduction='mean'):\n", "    # Attack loss function: -log[(1 - h_{\\bot}(x))(1 - max_{j \\noteq y} h_j(x)) + h_{\\bot}(x)]\n\t    d_probs = d_probs.view(-1)\n\t    probs = torch.softmax(logits, dim=1)\n\t    masked_probs = probs * (1 - one_hot(targets, probs.size(1)))\n\t    combine_probs = torch.max(masked_probs, dim=1)[0] * (1-d_probs)\n\t    return -negat_log_proba(combine_probs, reduction)\n\tdef conf_prob_loss_A2(logits, d_probs, targets, reduction='mean'):\n\t    # Attack loss function: -log[(1 - h_{\\bot}(x)) h_y(x) + h_{\\bot}(x)]\n\t    return conf_prob_loss(logits, d_probs, targets, reduction)\n\tdef conf_prob_loss_A3(logits, d_probs, targets, reduction='mean'):\n", "    # Minimum of the cross-entropy loss of classification and the binary cross-entropy loss of rejection.\n\t    # min{-log(h_y(x)), -log(h_{\\bot}(x))}\n\t    loss_1 = classification_loss(logits, targets, reduction='none') \n\t    loss_2 = negat_log_proba(d_probs.view(-1))\n\t    loss = torch.min(torch.stack([loss_1, loss_2], dim=1), dim=1)[0]\n\t    return reduce_loss(loss, reduction)\n\tdef conf_prob_loss_A4(logits, d_probs, targets, reduction='mean'):\n\t    # Sum of the cross-entropy loss of classification and the binary cross-entropy loss of rejection.\n\t    # -log(h_y(x)) - log(h_{\\bot}(x))\n\t    loss_1 = classification_loss(logits, targets, reduction='none') \n", "    loss_2 = negat_log_proba(d_probs.view(-1))\n\t    loss = loss_1 + loss_2\n\t    return reduce_loss(loss, reduction)\n\tdef atrr_reject_loss(logits, aux_probs, targets, reduction='mean', scaling_factor=100.0):\n\t    # softmax_output = torch.softmax(logits, axis=1)\n\t    # K = logits.size(1)\n\t    # l2_norm_prob = (torch.sum(softmax_output * softmax_output, axis=1) - 1 / K) * (K / (K - 1))\n\t    max_conf_loss = torch.logsumexp(logits, dim=1) - (1. / scaling_factor) * torch.logsumexp(scaling_factor * logits, dim=1)\n\t    loss = negat_log_proba(aux_probs.view(-1)) + max_conf_loss\n\t    return reduce_loss(loss, reduction)\n", "def atrr_accept_misclassify_loss(logits, aux_probs, targets, reduction='mean'):\n\t    current_prob = torch.nn.functional.softmax(logits, dim=1)\n\t    current_prob = current_prob * (1 - one_hot(targets, current_prob.size(1)))\n\t    mis_conf = torch.max(current_prob, dim=1)[0]\n\t    loss = -negat_log_proba(mis_conf * aux_probs.view(-1))\n\t    return reduce_loss(loss, reduction)\n\tdef robust_abstain_loss(logits, targets, reduction='mean'):\n\t    N, K = logits.size()\n\t    loss_1 = torch.nn.functional.cross_entropy(logits[:, :K-1], targets, reduction='none')\n\t    # `(K, K-1)` array where row `i` of the array will have the value `i` omitted\n", "    indices_temp = numpy.array([[j for j in range(K) if j != i] for i in range(K)])\n\t    targets_np = targets.detach().cpu().numpy().astype(numpy.int)\n\t    indices = torch.tensor(indices_temp[targets_np, :])\n\t    '''\n\t    indices = torch.tensor(\n\t        [[j for j in range(K) if j != targets[i].item()] for i in range(N)]\n\t    )\n\t    '''\n\t    # Class `K-2` corresponds to rejection for array of size `K-1`\n\t    loss_2 = torch.nn.functional.cross_entropy(logits[torch.unsqueeze(torch.arange(N), 1), indices],\n", "                                               torch.ones_like(targets) * (K - 2), reduction='none')\n\t    loss = torch.min(torch.stack([loss_1, loss_2], dim=1), dim=1)[0]\n\t    return reduce_loss(loss, reduction)\n\tdef classification_loss(logits, targets, reduction='mean'):\n\t    \"\"\"\n\t    Calculates either the multi-class or binary cross-entropy loss.\n\t    :param logits: predicted classes\n\t    :type logits: torch.autograd.Variable\n\t    :param targets: target classes\n\t    :type targets: torch.autograd.Variable\n", "    :param reduction: reduction type\n\t    :type reduction: str\n\t    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    # assert logits.size()[0] == targets.size()[0]\n\t    # assert len(list(targets.size())) == 1  # or (len(list(targets.size())) == 2 and targets.size(1) == 1)\n\t    # assert len(list(logits.size())) == 2\n\t    if logits.size()[1] > 1:\n\t        return torch.nn.functional.cross_entropy(logits, targets, reduction=reduction)\n", "    else:\n\t        # probability 1 is class 1\n\t        # probability 0 is class 0\n\t        return torch.nn.functional.binary_cross_entropy(torch.sigmoid(logits).view(-1), targets.float(), reduction=reduction)\n\tdef step_function_perturbation(perturb, epsilon_0, alpha=1e-4, norm_type='inf', smooth_approx=False, temperature=0.01):\n\t    \"\"\"\n\t    Step function applied to the perturbation norm. By default, it computes the exact step function which is not\n\t    differentiable. If a smooth approximation based on the sigmoid function is needed, set `smooth_approx=True` and set\n\t    the `temperature` to a suitably small value.\n\t    :param perturb: Torch Tensor with the perturbation. Can be a tensor of shape `(b, d1, ...)`, where `b` is the batch\n", "                    size and the rest are dimensions. Can also be a single vector of shape `[d]`.\n\t    :param epsilon_0: Radius of the smaller perturbation ball - a small positive value.\n\t    :param alpha: Small negative offset for the step function. The step function's value is `-alpha` when the\n\t                  perturbation norm is less than `epsilon_0`.\n\t    :param norm_type: Type of norm: 'inf' for infinity norm, '1', '2' etc for the other types of norm.\n\t    :param smooth_approx: Set to True to get a sigmoid-based approximation of the step function.\n\t    :param temperature: small non-negative value that controls  the steepness of the sigmoid approximation.\n\t    :returns: tensor of function values computed for each element in the batch. Has shape `[b]`.\n\t    \"\"\"\n\t    assert isinstance(perturb, (torch.Tensor, torch.autograd.Variable)), (\"Input 'perturb' should be of type \"\n", "                                                                          \"torch.Tensor or torch.autograd.Variable\")\n\t    s = perturb.shape\n\t    dim = 1\n\t    if len(s) > 2:\n\t        perturb = perturb.view(s[0], -1)    # flatten into a vector\n\t    elif len(s) == 1:\n\t        # single vector\n\t        dim = None\n\t    if norm_type == 'inf':\n\t        norm_type = float('inf')\n", "    elif not isinstance(norm_type, (int, float)):\n\t        # example: norm_type = '2'\n\t        norm_type = int(norm_type)\n\t    norm_val = torch.linalg.vector_norm(perturb, ord=norm_type, dim=dim)\n\t    if not smooth_approx:\n\t        return torch.where(norm_val <= epsilon_0, -1. * alpha, 1.)\n\t    else:\n\t        return torch.sigmoid((1. / temperature) * (norm_val - epsilon_0)) - alpha\n\tdef ramp_function_perturbation(perturb, epsilon_0, epsilon, alpha=1e-4, norm_type='inf'):\n\t    \"\"\"\n", "    Ramp function applied to the perturbation norm as defined in the paper.\n\t    :param perturb: Torch Tensor with the perturbation. Can be a tensor of shape `(b, d1, ...)`, where `b` is the batch\n\t                    size and the rest are dimensions. Can also be a single vector of shape `(d)`.\n\t    :param epsilon_0: Radius of the smaller perturbation ball - a small positive value.\n\t    :param epsilon: Radius of the larger perturbation ball. Should be >= `epsilon_0`.\n\t    :param alpha: Small negative offset for the step function. The step function's value is `-alpha` when the\n\t                  perturbation norm is less than `epsilon_0`.\n\t    :param norm_type: Type of norm: 'inf' for infinity norm, '1', '2' etc for the other types of norm.\n\t    :returns: tensor of function values computed for each element in the batch. Has shape `[b]`.\n\t    \"\"\"\n", "    assert isinstance(perturb, (torch.Tensor, torch.autograd.Variable)), (\"Input 'perturb' should be of type \"\n\t                                                                          \"torch.Tensor or torch.autograd.Variable\")\n\t    assert epsilon >= epsilon_0, \"Value of 'epsilon' cannot be smaller than 'epsilon_0'\"\n\t    s = perturb.shape\n\t    dim = 1\n\t    if len(s) > 2:\n\t        perturb = perturb.view(s[0], -1)    # flatten into a vector\n\t    elif len(s) == 1:\n\t        # single vector\n\t        dim = None\n", "    if norm_type == 'inf':\n\t        norm_type = float('inf')\n\t    elif not isinstance(norm_type, (int, float)):\n\t        # example: norm_type = '2'\n\t        norm_type = int(norm_type)\n\t    norm_val = torch.linalg.vector_norm(perturb, ord=norm_type, dim=dim)\n\t    temp = torch.maximum(norm_val - epsilon_0, torch.zeros_like(norm_val))\n\t    return ((1. + alpha) / (epsilon - epsilon_0)) * temp - alpha\n\tdef max_p_loss(logits, targets=None, reduction='mean'):\n\t    \"\"\"\n", "    Loss.\n\t    :param logits: predicted classes\n\t    :type logits: torch.autograd.Variable\n\t    :param targets: target classes\n\t    :type targets: torch.autograd.Variable\n\t    :param reduction: reduction type\n\t    :type reduction: str\n\t    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n", "    max_log = torch.max(torch.nn.functional.softmax(logits, dim=1), dim=1)[0]\n\t    if reduction == 'mean':\n\t        return torch.mean(max_log)\n\t    elif reduction == 'sum':\n\t        return torch.sum(max_log)\n\t    else:\n\t        return max_log\n\tdef max_log_loss(logits, targets=None, reduction='mean'):\n\t    \"\"\"\n\t    Loss.\n", "    :param logits: predicted classes\n\t    :type logits: torch.autograd.Variable\n\t    :param targets: target classes\n\t    :type targets: torch.autograd.Variable\n\t    :param reduction: reduction type\n\t    :type reduction: str\n\t    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    max_log = torch.max(torch.nn.functional.log_softmax(logits, dim=1), dim=1)[0]\n", "    if reduction == 'mean':\n\t        return torch.mean(max_log)\n\t    elif reduction == 'sum':\n\t        return torch.sum(max_log)\n\t    else:\n\t        return max_log\n\tdef cross_entropy_divergence(logits, targets, reduction='mean'):\n\t    \"\"\"\n\t    Loss.\n\t    :param logits: predicted logits\n", "    :type logits: torch.autograd.Variable\n\t    :param targets: target distributions\n\t    :type targets: torch.autograd.Variable\n\t    :param reduction: reduction type\n\t    :type reduction: str\n\t    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    assert len(list(logits.size())) == len(list(targets.size()))\n\t    assert logits.size()[0] == targets.size()[0]\n", "    assert logits.size()[1] == targets.size()[1]\n\t    assert logits.size()[1] > 1\n\t    divergences = torch.sum(- targets * torch.nn.functional.log_softmax(logits, dim=1), dim=1)\n\t    if reduction == 'mean':\n\t        return torch.mean(divergences)\n\t    elif reduction == 'sum':\n\t        return torch.sum(divergences)\n\t    else:\n\t        return divergences\n\tdef bhattacharyya_coefficient(logits, targets):\n", "    \"\"\"\n\t    Loss.\n\t    :param logits: predicted logits\n\t    :type logits: torch.autograd.Variable\n\t    :param targets: target distributions\n\t    :type targets: torch.autograd.Variable\n\t    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    assert len(list(logits.size())) == len(list(targets.size()))\n", "    assert logits.size()[0] == targets.size()[0]\n\t    assert logits.size()[1] == targets.size()[1]\n\t    assert logits.size()[1] > 1\n\t    # http://www.cse.yorku.ca/~kosta/CompVis_Notes/bhattacharyya.pdf\n\t    # torch.sqrt not differentiable at zero\n\t    return torch.clamp(torch.sum(torch.sqrt(torch.nn.functional.softmax(logits, dim=1) * targets + SMALL_VALUE), dim=1), min=0, max=1)\n\tdef bhattacharyya_divergence(logits, targets, reduction='mean'):\n\t    \"\"\"\n\t    Loss.\n\t    :param logits: predicted logits\n", "    :type logits: torch.autograd.Variable\n\t    :param targets: target distributions\n\t    :type targets: torch.autograd.Variable\n\t    :param reduction: reduction type\n\t    :type reduction: str\n\t    :return: error\n\t    :rtype: torch.autograd.Variable\n\t    \"\"\"\n\t    divergences = - 2*torch.log(bhattacharyya_coefficient(logits, targets))\n\t    if reduction == 'mean':\n", "        return torch.mean(divergences)\n\t    elif reduction == 'sum':\n\t        return torch.sum(divergences)\n\t    else:\n\t        return divergences\n\tdef linear_transition(perturbations, norm, epsilon=0.3, gamma=1):\n\t    \"\"\"\n\t    Linear transition rule.\n\t    :param perturbations: perturbations\n\t    :type perturbations: torch.autograd.Variable\n", "    :param norm: norm\n\t    :type norm: attacks.norms.Norm\n\t    :param epsilon: epsilon\n\t    :type epsilon: float\n\t    :param gamma: gamma\n\t    :type gamma: float\n\t    :return: gamma, norms\n\t    :rtype: torch.autograd.Variable, torch.autograd.Variable\n\t    \"\"\"\n\t    norms = norm(perturbations)\n", "    return torch.min(torch.ones_like(norms), gamma * norms / epsilon), norms\n\tdef power_transition(perturbations, norm, epsilon=0.3, gamma=1):\n\t    \"\"\"\n\t    Power transition rule.\n\t    :param perturbations: perturbations\n\t    :type perturbations: torch.autograd.Variable\n\t    :param norm: norm\n\t    :type norm: attacks.norms.Norm\n\t    :param epsilon: epsilon\n\t    :type epsilon: float\n", "    :param gamma: gamma\n\t    :type gamma: float\n\t    :return: gamma, norms\n\t    :rtype: torch.autograd.Variable, torch.autograd.Variable\n\t    \"\"\"\n\t    # returned value determines importance of uniform distribution:\n\t    # (1 - ret)*one_hot + ret*uniform\n\t    norms = norm(perturbations)\n\t    return 1 - torch.pow(1 - torch.min(torch.ones_like(norms), norms / epsilon), gamma), norms\n\tdef exponential_transition(perturbations, norm, epsilon=0.3, gamma=1):\n", "    \"\"\"\n\t    Exponential transition rule.\n\t    :param perturbations: perturbations\n\t    :type perturbations: torch.autograd.Variable\n\t    :param norm: norm\n\t    :type norm: attacks.norms.Norm\n\t    :param epsilon: epsilon\n\t    :type epsilon: float\n\t    :param gamma: gamma\n\t    :type gamma: float\n", "    :return: gamma, norms\n\t    :rtype: torch.autograd.Variable, torch.autograd.Variable\n\t    \"\"\"\n\t    norms = norm(perturbations)\n\t    return 1 - torch.exp(-gamma * norms), norms\n\tclass View(torch.nn.Module):\n\t    \"\"\"\n\t    Simple view layer.\n\t    \"\"\"\n\t    def __init__(self, *args):\n", "        \"\"\"\n\t        Constructor.\n\t        :param args: shape\n\t        :type args: [int]\n\t        \"\"\"\n\t        super(View, self).__init__()\n\t        self.shape = args\n\t    def forward(self, input):\n\t        \"\"\"\n\t        Forward pass.\n", "        :param input: input\n\t        :type input: torch.autograd.Variable\n\t        :return: output\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n\t        return input.view(self.shape)\n\tclass Flatten(torch.nn.Module):\n\t    \"\"\"\n\t    Flatten module.\n\t    \"\"\"\n", "    def forward(self, input):\n\t        \"\"\"\n\t        Forward pass.\n\t        :param input: input\n\t        :type input: torch.autograd.Variable\n\t        :return: output\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n\t        return input.view(input.shape[0], -1)\n\tclass Clamp(torch.nn.Module):\n", "    \"\"\"\n\t    Wrapper for clamp.\n\t    \"\"\"\n\t    def __init__(self, min=0, max=1):\n\t        \"\"\"\n\t        Constructor.\n\t        \"\"\"\n\t        super(Clamp, self).__init__()\n\t        self.min = min\n\t        \"\"\" (float) Min value. \"\"\"\n", "        self.max = max\n\t        \"\"\" (float) Max value. \"\"\"\n\t    def forward(self, input):\n\t        \"\"\"\n\t        Forward pass.\n\t        :param input: input\n\t        :type input: torch.autograd.Variable\n\t        :return: output\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n", "        return torch.clamp(torch.clamp(input, min=self.min), max=self.max)\n\tclass Scale(torch.nn.Module):\n\t    \"\"\"\n\t    Simply scaling layer, mainly to allow simple saving and loading.\n\t    \"\"\"\n\t    def __init__(self, shape):\n\t        \"\"\"\n\t        Constructor.\n\t        :param shape: shape\n\t        :type shape: [int]\n", "        \"\"\"\n\t        super(Scale, self).__init__()\n\t        self.weight = torch.nn.Parameter(torch.zeros(shape)) # min\n\t        self.bias = torch.nn.Parameter(torch.ones(shape)) # max\n\t    def forward(self, input):\n\t        \"\"\"\n\t        Forward pass.\n\t        :param input: input\n\t        :type input: torch.autograd.Variable\n\t        :return: output\n", "        :rtype: torch.autograd.Variable\n\t        \"\"\"\n\t        return expand_as(self.weight, input) + torch.mul(expand_as(self.bias, input) - expand_as(self.weight, input), input)\n\tclass Entropy(torch.nn.Module):\n\t    \"\"\"\n\t    Entropy computation based on logits.\n\t    \"\"\"\n\t    def __init__(self):\n\t        \"\"\"\n\t        Constructor.\n", "        \"\"\"\n\t        super(Entropy, self).__init__()\n\t    def forward(self, input):\n\t        \"\"\"\n\t        Forward pass.\n\t        :param input: input\n\t        :type input: torch.autograd.Variable\n\t        :return: output\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n", "        return -1.*torch.sum(torch.nn.functional.softmax(input, dim=1) * torch.nn.functional.log_softmax(input, dim=1))\n\tclass Normalize(torch.nn.Module):\n\t    \"\"\"\n\t    Normalization layer to be learned.\n\t    \"\"\"\n\t    def __init__(self, n_channels):\n\t        \"\"\"\n\t        Constructor.\n\t        :param n_channels: number of channels\n\t        :type n_channels: int\n", "        \"\"\"\n\t        super(Normalize, self).__init__()\n\t        # nn.Parameter is a special kind of Tensor, that will get\n\t        # automatically registered as Module's parameter once it's assigned\n\t        # as an attribute. Parameters and buffers need to be registered, or\n\t        # they won't appear in .parameters() (doesn't apply to buffers), and\n\t        # won't be converted when e.g. .cuda() is called. You can use\n\t        # .register_buffer() to register buffers.\n\t        # nn.Parameters require gradients by default.\n\t        self.weight = torch.nn.Parameter(torch.ones(n_channels))\n", "        self.bias = torch.nn.Parameter(torch.zeros(n_channels))\n\t    def forward(self, input):\n\t        \"\"\"\n\t        Forward pass.\n\t        :param input: input\n\t        :type input: torch.autograd.Variable\n\t        :return: output\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n\t        return (input - self.bias.view(1, -1, 1, 1))/self.weight.view(1, -1, 1, 1)\n", "class GaussianLayer(torch.nn.Module):\n\t    \"\"\"\n\t    Gaussian convolution.\n\t    See https://pytorch.org/docs/stable/nn.html.\n\t    \"\"\"\n\t    def __init__(self, sigma=3, channels=3):\n\t        \"\"\"\n\t        \"\"\"\n\t        super(GaussianLayer, self).__init__()\n\t        self.sigma = sigma\n", "        \"\"\" (float) Sigma. \"\"\"\n\t        padding = math.ceil(self.sigma)\n\t        kernel = 2*padding + 1\n\t        self.seq = torch.nn.Sequential(\n\t            torch.nn.ReflectionPad2d((padding, padding, padding, padding)),\n\t            torch.nn.Conv2d(channels, channels, kernel, stride=1, padding=0, bias=None, groups=channels)\n\t        )\n\t        n = numpy.zeros((kernel, kernel))\n\t        n[padding, padding] = 1\n\t        k = scipy.ndimage.gaussian_filter(n, sigma=self.sigma)\n", "        for name, f in self.named_parameters():\n\t            f.data.copy_(torch.from_numpy(k))\n\t    def forward(self, input):\n\t        \"\"\"\n\t        Forward pass.\n\t        :param input: input\n\t        :type input: torch.autograd.Variable\n\t        :return: output\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n", "        return self.seq(input)\n"]}
{"filename": "utils/trades.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom torch.autograd import Variable\n\timport torch.optim as optim\n\tdef squared_l2_norm(x):\n\t    flattened = x.view(x.unsqueeze(0).shape[0], -1)\n\t    return (flattened ** 2).sum(1)\n\tdef l2_norm(x):\n\t    return squared_l2_norm(x).sqrt()\n", "def trades_loss(model,\n\t                x_natural,\n\t                y,\n\t                optimizer,\n\t                step_size=0.003,\n\t                epsilon=0.031,\n\t                perturb_steps=10,\n\t                beta=1.0,\n\t                distance='l_inf'):\n\t    # define KL-loss\n", "    criterion_kl = nn.KLDivLoss(size_average=False)\n\t    model.eval()\n\t    batch_size = len(x_natural)\n\t    # generate adversarial example\n\t    x_adv = x_natural.detach() + 0.001 * torch.randn(x_natural.shape).cuda().detach()\n\t    if distance == 'l_inf':\n\t        for _ in range(perturb_steps):\n\t            x_adv.requires_grad_()\n\t            with torch.enable_grad():\n\t                loss_kl = criterion_kl(F.log_softmax(model(x_adv), dim=1),\n", "                                       F.softmax(model(x_natural), dim=1))\n\t            grad = torch.autograd.grad(loss_kl, [x_adv])[0]\n\t            x_adv = x_adv.detach() + step_size * torch.sign(grad.detach())\n\t            x_adv = torch.min(torch.max(x_adv, x_natural - epsilon), x_natural + epsilon)\n\t            x_adv = torch.clamp(x_adv, 0.0, 1.0)\n\t    elif distance == 'l_2':\n\t        delta = 0.001 * torch.randn(x_natural.shape).cuda().detach()\n\t        delta = Variable(delta.data, requires_grad=True)\n\t        # Setup optimizers\n\t        optimizer_delta = optim.SGD([delta], lr=epsilon / perturb_steps * 2)\n", "        for _ in range(perturb_steps):\n\t            adv = x_natural + delta\n\t            # optimize\n\t            optimizer_delta.zero_grad()\n\t            with torch.enable_grad():\n\t                loss = (-1) * criterion_kl(F.log_softmax(model(adv), dim=1),\n\t                                           F.softmax(model(x_natural), dim=1))\n\t            loss.backward()\n\t            # renorming gradient\n\t            grad_norms = delta.grad.view(batch_size, -1).norm(p=2, dim=1)\n", "            delta.grad.div_(grad_norms.view(-1, 1, 1, 1))\n\t            # avoid nan or inf if gradient is 0\n\t            if (grad_norms == 0).any():\n\t                delta.grad[grad_norms == 0] = torch.randn_like(delta.grad[grad_norms == 0])\n\t            optimizer_delta.step()\n\t            # projection\n\t            delta.data.add_(x_natural)\n\t            delta.data.clamp_(0, 1).sub_(x_natural)\n\t            delta.data.renorm_(p=2, dim=0, maxnorm=epsilon)\n\t        x_adv = Variable(x_natural + delta, requires_grad=False)\n", "    else:\n\t        x_adv = torch.clamp(x_adv, 0.0, 1.0)\n\t    model.train()\n\t    x_adv = Variable(torch.clamp(x_adv, 0.0, 1.0), requires_grad=False)\n\t    # zero gradient\n\t    optimizer.zero_grad()\n\t    # calculate robust loss\n\t    logits = model(x_natural)\n\t    loss_natural = F.cross_entropy(logits, y)\n\t    loss_robust = (1.0 / batch_size) * criterion_kl(F.log_softmax(model(x_adv), dim=1),\n", "                                                    F.softmax(model(x_natural), dim=1))\n\t    loss = loss_natural + beta * loss_robust\n\t    return loss"]}
{"filename": "utils/dataset.py", "chunked_list": ["import os\n\timport torch\n\timport torch.utils.data # needs to be imported separately\n\timport utils.lib as utils\n\timport numpy\n\timport skimage.transform\n\tfrom PIL import Image\n\timport torch\n\t# base directory for data\n\tBASE_DATA = './datasets'\n", "# Common extension types used.\n\tTXT_EXT = '.txt'\n\tHDF5_EXT = '.h5'\n\tSTATE_EXT = '.pth.tar'\n\tLOG_EXT = '.log'\n\tPNG_EXT = '.png'\n\tPICKLE_EXT = '.pkl'\n\tTEX_EXT = '.tex'\n\tMAT_EXT = '.mat'\n\tGZIP_EXT = '.gz'\n", "# Naming conventions.\n\tdef data_file(name, ext=HDF5_EXT):\n\t    \"\"\"\n\t    Generate path to data file.\n\t    :param name: name of file\n\t    :type name: str\n\t    :param ext: extension (including period)\n\t    :type ext: str\n\t    :return: filepath\n\t    :rtype: str\n", "    \"\"\"\n\t    return os.path.join(BASE_DATA, name) + ext\n\tdef raw_svhn_train_file():\n\t    \"\"\"\n\t    Raw SVHN training directory.\n\t    :return: fielpath\n\t    :rtype: str\n\t    \"\"\"\n\t    return data_file('svhn/train_32x32', '.mat')\n\tdef raw_svhn_test_file():\n", "    \"\"\"\n\t    Raw SVHN training directory.\n\t    :return: fielpath\n\t    :rtype: str\n\t    \"\"\"\n\t    return data_file('svhn/test_32x32', '.mat')\n\tdef svhn_train_images_file():\n\t    \"\"\"\n\t    SVHN train images.\n\t    :return: filepath\n", "    :rtype: str\n\t    \"\"\"\n\t    return data_file('svhn/train_images', HDF5_EXT)\n\tdef svhn_test_images_file():\n\t    \"\"\"\n\t    SVHN test images.\n\t    :return: filepath\n\t    :rtype: str\n\t    \"\"\"\n\t    return data_file('svhn/test_images', HDF5_EXT)\n", "def svhn_train_labels_file():\n\t    \"\"\"\n\t    SVHN train labels.\n\t    :return: filepath\n\t    :rtype: str\n\t    \"\"\"\n\t    return data_file('svhn/train_labels', HDF5_EXT)\n\tdef svhn_test_labels_file():\n\t    \"\"\"\n\t    SVHN test labels.\n", "    :return: filepath\n\t    :rtype: str\n\t    \"\"\"\n\t    return data_file('svhn/test_labels', HDF5_EXT)\n\tclass CleanDataset(torch.utils.data.Dataset):\n\t    \"\"\"\n\t    General, clean dataset used for training, testing and attacking.\n\t    \"\"\"\n\t    def __init__(self, images, labels, indices=None, transform=None):\n\t        \"\"\"\n", "        Constructor.\n\t        :param images: images/inputs\n\t        :type images: str or numpy.ndarray\n\t        :param labels: labels\n\t        :type labels: str or numpy.ndarray\n\t        :param indices: indices\n\t        :type indices: numpy.ndarray\n\t        :param resize: resize in [channels, height, width\n\t        :type resize: resize\n\t        \"\"\"\n", "        self.images_file = None\n\t        \"\"\" (str) File images were loaded from. \"\"\"\n\t        self.labels_file = None\n\t        \"\"\" (str) File labels were loaded from. \"\"\"\n\t        if isinstance(images, str):\n\t            self.images_file = images\n\t            images = utils.read_hdf5(self.images_file)\n\t        if not images.dtype == numpy.float32:\n\t            images = images.astype(numpy.float32)\n\t        if isinstance(labels, str):\n", "            self.labels_file = labels\n\t            labels = utils.read_hdf5(self.labels_file)\n\t        labels = numpy.squeeze(labels)\n\t        if not labels.dtype == int:\n\t            labels = labels.astype(int)\n\t        assert isinstance(images, numpy.ndarray)\n\t        assert isinstance(labels, numpy.ndarray)\n\t        assert images.shape[0] == labels.shape[0]\n\t        if indices is None:\n\t            indices = range(images.shape[0])\n", "        assert numpy.min(indices) >= 0\n\t        assert numpy.max(indices) < images.shape[0]\n\t        self.images = images[indices]\n\t        # self.images = []\n\t        # for i in indices:\n\t        #     self.images.append(Image.fromarray((images[i] * 255).astype(numpy.uint8)))\n\t        \"\"\" (numpy.ndarray) Inputs. \"\"\"\n\t        self.labels = labels[indices]\n\t        \"\"\" (numpy.ndarray) Labels. \"\"\"\n\t        self.transform = transform\n", "        \"\"\" (numpy.ndarray) Possible attack targets. \"\"\"\n\t    def __getitem__(self, index):\n\t        assert index < len(self)\n\t        if self.transform is None:\n\t            return self.images[index], self.labels[index]\n\t        else:\n\t            return self.transform(self.images[index]), self.labels[index]\n\t    def __len__(self):\n\t        assert len(self.images) == self.labels.shape[0]\n\t        return len(self.images)\n", "    def __add__(self, other):\n\t        return torch.utils.data.ConcatDataset([self, other])\n\tclass SVHNTrainSet(CleanDataset):\n\t    def __init__(self, transform=None):\n\t        super(SVHNTrainSet, self).__init__(svhn_train_images_file(), svhn_train_labels_file(), None, transform)\n\tclass SVHNTestSet(CleanDataset):\n\t    def __init__(self, transform=None):\n\t        super(SVHNTestSet, self).__init__(svhn_test_images_file(), svhn_test_labels_file(), range(10000), transform)\n\tclass CustomDataset(torch.utils.data.Dataset):\n\t    def __init__(self, data, targets, transform=None):\n", "        self.data = data\n\t        self.targets = torch.LongTensor(targets)\n\t        self.transform = transform\n\t    def __getitem__(self, index):\n\t        x = self.data[index]\n\t        y = self.targets[index]\n\t        if self.transform:\n\t            x = self.transform(x)\n\t        return x, y\n\t    def __len__(self):\n", "        return len(self.data)"]}
{"filename": "utils/numpy.py", "chunked_list": ["import numpy\n\timport scipy.stats\n\timport math\n\tdef one_hot(array, N):\n\t    \"\"\"\n\t    Convert an array of numbers to an array of one-hot vectors.\n\t    :param array: classes to convert\n\t    :type array: numpy.ndarray\n\t    :param N: number of classes\n\t    :type N: int\n", "    :return: one-hot vectors\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    array = array.astype(int)\n\t    assert numpy.max(array) < N\n\t    assert numpy.min(array) >= 0\n\t    one_hot = numpy.zeros((array.shape[0], N))\n\t    one_hot[numpy.arange(array.shape[0]), array] = 1\n\t    return one_hot\n\tdef expand_as(array, array_as):\n", "    \"\"\"\n\t    Expands the tensor using view to allow broadcasting.\n\t    :param array: input tensor\n\t    :type array: numpy.ndarray\n\t    :param array_as: reference tensor\n\t    :type array_as: torch.Tensor or torch.autograd.Variable\n\t    :return: tensor expanded with singelton dimensions as tensor_as\n\t    :rtype: torch.Tensor or torch.autograd.Variable\n\t    \"\"\"\n\t    shape = list(array.shape)\n", "    for i in range(len(array.shape), len(array_as.shape)):\n\t        shape.append(1)\n\t    return array.reshape(shape)\n\tdef concatenate(array1, array2, axis=0):\n\t    \"\"\"\n\t    Basically a wrapper for numpy.concatenate, with the exception\n\t    that the array itself is returned if its None or evaluates to False.\n\t    :param array1: input array or None\n\t    :type array1: mixed\n\t    :param array2: input array\n", "    :type array2: numpy.ndarray\n\t    :param axis: axis to concatenate\n\t    :type axis: int\n\t    :return: concatenated array\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    assert isinstance(array2, numpy.ndarray)\n\t    if array1 is not None:\n\t        assert isinstance(array1, numpy.ndarray)\n\t        return numpy.concatenate((array1, array2), axis=axis)\n", "    else:\n\t        return array2\n\tdef exponential_norm(batch_size, dim, epsilon=1, ord=2):\n\t    \"\"\"\n\t    Sample vectors uniformly by norm and direction separately.\n\t    :param batch_size: how many vectors to sample\n\t    :type batch_size: int\n\t    :param dim: dimensionality of vectors\n\t    :type dim: int\n\t    :param epsilon: epsilon-ball\n", "    :type epsilon: float\n\t    :param ord: norm to use\n\t    :type ord: int\n\t    :return: batch_size x dim tensor\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    random = numpy.random.randn(batch_size, dim)\n\t    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n\t    random *= epsilon\n\t    truncated_normal = scipy.stats.truncexpon.rvs(1, loc=0, scale=0.9, size=(batch_size, 1))\n", "    random *= numpy.repeat(truncated_normal, axis=1, repeats=dim)\n\t    return random\n\tdef uniform_norm(batch_size, dim, epsilon=1, ord=2):\n\t    \"\"\"\n\t    Sample vectors uniformly by norm and direction separately.\n\t    :param batch_size: how many vectors to sample\n\t    :type batch_size: int\n\t    :param dim: dimensionality of vectors\n\t    :type dim: int\n\t    :param epsilon: epsilon-ball\n", "    :type epsilon: float\n\t    :param ord: norm to use\n\t    :type ord: int\n\t    :return: batch_size x dim tensor\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    random = numpy.random.randn(batch_size, dim)\n\t    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n\t    random *= epsilon\n\t    uniform = numpy.random.uniform(0, 1, (batch_size, 1))  # exponent is only difference!\n", "    random *= numpy.repeat(uniform, axis=1, repeats=dim)\n\t    return random\n\tdef uniform_ball(batch_size, dim, epsilon=1, ord=2):\n\t    \"\"\"\n\t    Sample vectors uniformly in the n-ball.\n\t    See Harman et al., On decompositional algorithms for uniform sampling from n-spheres and n-balls.\n\t    :param batch_size: how many vectors to sample\n\t    :type batch_size: int\n\t    :param dim: dimensionality of vectors\n\t    :type dim: int\n", "    :param epsilon: epsilon-ball\n\t    :type epsilon: float\n\t    :param ord: norm to use\n\t    :type ord: int\n\t    :return: batch_size x dim tensor\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    random = numpy.random.randn(batch_size, dim)\n\t    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n\t    random *= epsilon\n", "    uniform = numpy.random.uniform(0, 1, (batch_size, 1)) ** (1. / dim)\n\t    random *= numpy.repeat(uniform, axis=1, repeats=dim)\n\t    return random\n\tdef uniform_sphere(batch_size, dim, epsilon=1, ord=2):\n\t    \"\"\"\n\t    Sample vectors uniformly on the n-sphere.\n\t    See Harman et al., On decompositional algorithms for uniform sampling from n-spheres and n-balls.\n\t    :param batch_size: how many vectors to sample\n\t    :type batch_size: int\n\t    :param dim: dimensionality of vectors\n", "    :type dim: int\n\t    :param epsilon: epsilon-ball\n\t    :type epsilon: float\n\t    :param ord: norm to use\n\t    :type ord: int\n\t    :return: batch_size x dim tensor\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    random = numpy.random.randn(batch_size, dim)\n\t    random /= numpy.repeat(numpy.linalg.norm(random, ord=ord, axis=1).reshape(-1, 1), axis=1, repeats=dim)\n", "    random *= epsilon\n\t    return random\n\tdef truncated_normal(size, lower=-2, upper=2):\n\t    \"\"\"\n\t    Sample from truncated normal.\n\t    See https://stackoverflow.com/questions/18441779/how-to-specify-upper-and-lower-limits-when-using-numpy-random-normal.\n\t    :param size: size of vector\n\t    :type size: [int]\n\t    :param lower: lower bound\n\t    :type lower: float\n", "    :param upper: upper bound\n\t    :type upper: float\n\t    :return: batch_size x dim tensor\n\t    :rtype: numpy.ndarray\n\t    \"\"\"\n\t    return scipy.stats.truncnorm.rvs(lower, upper, size=size)\n\tdef project_simplex(v, s=1):\n\t    \"\"\"\n\t    Taken from https://gist.github.com/daien/1272551/edd95a6154106f8e28209a1c7964623ef8397246.\n\t    Compute the Euclidean projection on a positive simplex\n", "    Solves the optimisation problem (using the algorithm from [1]):\n\t        min_w 0.5 * || w - v ||_2^2 , s.t. \\sum_i w_i = s, w_i >= 0\n\t    Parameters\n\t    ----------\n\t    v: (n,) numpy array,\n\t       n-dimensional vector to project\n\t    s: int, optional, default: 1,\n\t       radius of the simplex\n\t    Returns\n\t    -------\n", "    w: (n,) numpy array,\n\t       Euclidean projection of v on the simplex\n\t    Notes\n\t    -----\n\t    The complexity of this algorithm is in O(n log(n)) as it involves sorting v.\n\t    Better alternatives exist for high-dimensional sparse vectors (cf. [1])\n\t    However, this implementation still easily scales to millions of dimensions.\n\t    References\n\t    ----------\n\t    [1] Efficient Projections onto the .1-Ball for Learning in High Dimensions\n", "        John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.\n\t        International Conference on Machine Learning (ICML 2008)\n\t        http://www.cs.berkeley.edu/~jduchi/projects/DuchiSiShCh08.pdf\n\t    \"\"\"\n\t    assert s > 0, \"Radius s must be strictly positive (%d <= 0)\" % s\n\t    n, = v.shape  # will raise ValueError if v is not 1-D\n\t    # check if we are already on the simplex\n\t    if v.sum() == s and numpy.alltrue(v >= 0):\n\t        # best projection: itself!\n\t        return v\n", "    # get the array of cumulative sums of a sorted (decreasing) copy of v\n\t    u = numpy.sort(v)[::-1]\n\t    cssv = numpy.cumsum(u)\n\t    # get the number of > 0 components of the optimal solution\n\t    rho = numpy.nonzero(u * numpy.arange(1, n+1) > (cssv - s))[0][-1]\n\t    # compute the Lagrange multiplier associated to the simplex constraint\n\t    theta = float(cssv[rho] - s) / rho\n\t    # compute the projection by thresholding v using theta\n\t    w = (v - theta).clip(min=0)\n\t    return w\n", "def projection_simplex_sort(v, z=1):\n\t    n_features = v.shape[0]\n\t    u = numpy.sort(v)[::-1]\n\t    cssv = numpy.cumsum(u) - z\n\t    ind = numpy.arange(n_features) + 1\n\t    cond = u - cssv / ind > 0\n\t    rho = ind[cond][-1]\n\t    theta = cssv[cond][-1] / float(rho)\n\t    w = numpy.maximum(v - theta, 0)\n\t    return w\n", "def projection_simplex_pivot(v, z=1, random_state=None):\n\t    rs = numpy.random.RandomState(random_state)\n\t    n_features = len(v)\n\t    U = numpy.arange(n_features)\n\t    s = 0\n\t    rho = 0\n\t    while len(U) > 0:\n\t        G = []\n\t        L = []\n\t        k = U[rs.randint(0, len(U))]\n", "        ds = v[k]\n\t        for j in U:\n\t            if v[j] >= v[k]:\n\t                if j != k:\n\t                    ds += v[j]\n\t                    G.append(j)\n\t            elif v[j] < v[k]:\n\t                L.append(j)\n\t        drho = len(G) + 1\n\t        if s + ds - (rho + drho) * v[k] < z:\n", "            s += ds\n\t            rho += drho\n\t            U = L\n\t        else:\n\t            U = G\n\t    theta = (s - z) / float(rho)\n\t    return numpy.maximum(v - theta, 0)\n\tdef projection_simplex_bisection(v, z=1, tau=0.0001, max_iter=1000):\n\t    lower = 0\n\t    upper = numpy.max(v)\n", "    current = numpy.inf\n\t    for it in xrange(max_iter):\n\t        if numpy.abs(current) / z < tau and current < 0:\n\t            break\n\t        theta = (upper + lower) / 2.0\n\t        w = numpy.maximum(v - theta, 0)\n\t        current = numpy.sum(w) - z\n\t        if current <= 0:\n\t            upper = theta\n\t        else:\n", "            lower = theta\n\t    return w\n\tdef project_ball(array, epsilon=1, ord=2):\n\t    \"\"\"\n\t    Compute the orthogonal projection of the input tensor (as vector) onto the L_ord epsilon-ball.\n\t    **Assumes the first dimension to be batch dimension, which is preserved.**\n\t    :param array: array\n\t    :type array: numpy.ndarray\n\t    :param epsilon: radius of ball.\n\t    :type epsilon: float\n", "    :param ord: order of norm\n\t    :type ord: int\n\t    :return: projected vector\n\t    :rtype: torch.autograd.Variable or torch.Tensor\n\t    \"\"\"\n\t    assert isinstance(array, numpy.ndarray), 'given tensor should be numpy.ndarray'\n\t    if ord == 0:\n\t        assert epsilon >= 1\n\t        size = array.shape\n\t        flattened_size = numpy.prod(numpy.array(size[1:]))\n", "        array = array.reshape(-1, flattened_size)\n\t        sorted = numpy.sort(array, axis=1)\n\t        k = int(math.ceil(epsilon))\n\t        thresholds = sorted[:, -k]\n\t        mask = (array >= expand_as(thresholds, array)).astype(float)\n\t        array *= mask\n\t    elif ord == 1:\n\t        size = array.shape\n\t        flattened_size = numpy.prod(numpy.array(size[1:]))\n\t        array = array.reshape(-1, flattened_size)\n", "        if False: #  Version 1\n\t            # https://github.com/ftramer/MultiRobustness/blob/master/pgd_attack.py\n\t            l1 = numpy.sum(numpy.abs(array), axis=1)\n\t            epsilons = numpy.ones((array.shape[0]))*epsilon\n\t            to_project = l1 > epsilons\n\t            if numpy.any(to_project):\n\t                n = numpy.sum(to_project)\n\t                d = array[to_project]#.reshape(n, -1)  # n * N (N=h*w*ch)\n\t                abs_d = numpy.abs(d)  # n * N\n\t                mu = -numpy.sort(-abs_d, axis=-1)  # n * N\n", "                cumsums = mu.cumsum(axis=-1)  # n * N\n\t                eps_d = epsilons[to_project]\n\t                js = 1.0 / numpy.arange(1, array.shape[1] + 1)\n\t                temp = mu - js * (cumsums - numpy.expand_dims(eps_d, -1))\n\t                rho = numpy.argmin(temp > 0, axis=-1)\n\t                theta = 1.0 / (1 + rho) * (cumsums[range(n), rho] - eps_d)\n\t                sgn = numpy.sign(d)\n\t                d = sgn * numpy.maximum(abs_d - numpy.expand_dims(theta, -1), 0)\n\t                array[to_project] = d.reshape(-1, array.shape[1])\n\t        if True: # Version 2\n", "            for i in range(array.shape[0]):\n\t                # compute the vector of absolute values\n\t                u = numpy.abs(array[i])\n\t                # check if v is already a solution\n\t                if u.sum() <= epsilon:\n\t                    # L1-norm is <= s\n\t                    continue\n\t                # v is not already a solution: optimum lies on the boundary (norm == s)\n\t                # project *u* on the simplex\n\t                #w = project_simplex(u, s=epsilon)\n", "                w = projection_simplex_sort(u, z=epsilon)\n\t                # compute the solution to the original problem on v\n\t                w *= numpy.sign(array[i])\n\t                array[i] = w\n\t        if len(size) == 4:\n\t            array = array.reshape(-1, size[1], size[2], size[3])\n\t        elif len(size) == 2:\n\t            array = array.reshape(-1, size[1])\n\t    elif ord == 2:\n\t        size = array.shape\n", "        flattened_size = numpy.prod(numpy.array(size[1:]))\n\t        array = array.reshape(-1, flattened_size)\n\t        clamped = numpy.clip(epsilon/numpy.linalg.norm(array, 2, axis=1), a_min=None, a_max=1)\n\t        clamped = clamped.reshape(-1, 1)\n\t        array = array * clamped\n\t        if len(size) == 4:\n\t            array = array.reshape(-1, size[1], size[2], size[3])\n\t        elif len(size) == 2:\n\t            array = array.reshape(-1, size[1])\n\t    elif ord == float('inf'):\n", "        array = numpy.clip(array, a_min=-epsilon, a_max=epsilon)\n\t    else:\n\t        raise NotImplementedError()\n\t    return array\n\tdef max_detector(probabilities):\n\t    return numpy.max(probabilities, axis=1)\n"]}
{"filename": "utils/constants.py", "chunked_list": ["BASE_LR_RANGE = {'mnist': [0.1, 0.01, 0.005],\n\t                 'cifar10': [0.01, 0.005, 0.001],\n\t                 'gtsrb': [0.01, 0.005, 0.001],\n\t                 'svhn': [0.01, 0.005, 0.001]}\n\t# Range of alpha values used in the definition of robust error with rejection\n\tALPHA_LIST = [0.0, 0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.5, 1.0]\n\tGAP_FACTOR = {\n\t    'mnist': 0.02,\n\t    'cifar10': 0.5/255,\n\t    'gtsrb': 0.5/255,\n", "    'svhn': 0.5/255\n\t}\n\t# Number of classes\n\tN_CLASSES = {\n\t    'mnist': 10,\n\t    'cifar10': 10,\n\t    'gtsrb': 43,\n\t    'svhn': 10\n\t}\n\t# Target true positive rate for selecting the rejection threshold\n", "TPR_THRESHOLD = {\n\t    'mnist': 0.99,\n\t    'cifar10': 0.95,\n\t    'gtsrb': 0.95,\n\t    'svhn': 0.95\n\t}\n\t# Fraction of samples used for validation\n\tVAL_RATIO = 0.1\n\t# Sample size used for calculating the robustness with rejection\n\tN_SAMPLES = 1000\n", "# Setting for the adaptive attacks\n\tCONFIG_PGDBT_ATTACK_OUTER = {\n\t    'max_iterations': 1000,\n\t    'base_lr': 0.001,\n\t    'momentum': 0.9,\n\t    'lr_factor': 1.1,\n\t    'backtrack': True,\n\t    'rand_init_name': 'random',\n\t    'num_rand_init': 10,\n\t    'clip_min': 0.0,\n", "    'clip_max': 1.0\n\t}\n\tCONFIG_PGD_ATTACK_INNER = {\n\t    'max_iterations': 200,\n\t    'base_lr': 0.1,\n\t    'momentum': 0.9,\n\t    'lr_factor': 1.25,\n\t    'backtrack': False,\n\t    'rand_init_name': 'random',\n\t    'num_rand_init': 5,\n", "    'clip_min': 0.0,\n\t    'clip_max': 1.0\n\t}\n\tCONFIG_MULTITARGET_ATTACK_OUTER = {\n\t    'max_iterations': 200,\n\t    'base_lr': 0.1,\n\t    'momentum': 0.9,\n\t    'lr_factor': 1.1,\n\t    'backtrack': False,\n\t    'rand_init_name': 'random',\n", "    'num_rand_init': 5,\n\t    'clip_min': 0.0,\n\t    'clip_max': 1.0\n\t}\n\tCONFIG_MULTITARGET_ATTACK_INNER = {\n\t     'max_iterations': 200,\n\t     'base_lr': 0.1,\n\t     'momentum': 0.9,\n\t     'lr_factor': 1.25,\n\t     'backtrack': False,\n", "     'rand_init_name': 'random',\n\t     'num_rand_init': 1,\n\t     'clip_min': 0.0,\n\t     'clip_max': 1.0\n\t}\n\t# Plot colors and markers\n\t# https://matplotlib.org/2.0.2/examples/color/named_colors.html\n\tCOLORS = ['r', 'b', 'c', 'orange', 'g', 'm', 'lawngreen', 'grey', 'hotpink', 'y', 'steelblue', 'tan',\n\t          'lightsalmon', 'navy', 'gold']\n\tMARKERS = ['o', '^', 'v', 's', '*', 'x', 'd', '>', '<', '1', 'h', 'P', '_', '2', '|', '3', '4']\n", "def lr_schedule(t, lr_max):\n\t    # Learning rate schedule\n\t    if t < 100:\n\t        return lr_max\n\t    elif t < 105:\n\t        return lr_max / 10.\n\t    else:\n\t        return lr_max / 100.\n"]}
{"filename": "models/preact_resnet_block.py", "chunked_list": ["\"\"\"\n\tPre-activation ResNet block in PyTorch.\n\tTaken from https://github.com/locuslab/robust_union/.\n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass PreActBlock(nn.Module):\n\t    \"\"\"\n\t    Pre-Activation ResNet block.\n", "    \"\"\"\n\t    expansion = 1\n\t    def __init__(self, in_planes, planes, stride=1):\n\t        \"\"\"\n\t        Constructor.\n\t        :param in_planes: number of input planes\n\t        :type in_planes: int\n\t        :param planes: output planes\n\t        :type planes: int\n\t        :param stride: stride for block\n", "        :type stride: int\n\t        \"\"\"\n\t        super(PreActBlock, self).__init__()\n\t        self.bn1 = nn.BatchNorm2d(in_planes)\n\t        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\t        self.bn2 = nn.BatchNorm2d(planes)\n\t        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\t        if stride != 1 or in_planes != self.expansion*planes:\n\t            self.shortcut = nn.Sequential(\n\t                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n", "            )\n\t    def forward(self, x):\n\t        out = F.relu(self.bn1(x))\n\t        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n\t        out = self.conv1(out)\n\t        out = self.conv2(F.relu(self.bn2(out)))\n\t        out += shortcut\n\t        return out"]}
{"filename": "models/mlp.py", "chunked_list": ["import torch\n\timport utils.torch\n\tfrom .classifier import Classifier\n\tfrom operator import mul\n\tfrom functools import reduce\n\tclass MLP(Classifier):\n\t    \"\"\"\n\t    MLP classifier.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), units=[64, 64, 64], activation=torch.nn.ReLU, normalization=torch.nn.BatchNorm1d, bias=True, **kwargs):\n", "        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param activation: activation function\n\t        :type activation: None or torch.nn.Module\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n", "        :param bias: whether to use bias\n\t        :type bias: bool\n\t        \"\"\"\n\t        super(MLP, self).__init__(N_class, resolution, **kwargs)\n\t        self.units = units\n\t        \"\"\" ([int]) Units. \"\"\"\n\t        self.activation = activation\n\t        \"\"\" (callable) activation\"\"\"\n\t        self.normalization = normalization\n\t        \"\"\" (callable) Normalization. \"\"\"\n", "        self.bias = bias\n\t        \"\"\" (bool) Bias. \"\"\"\n\t        gain = 1\n\t        if self.activation is not None:\n\t            gain = torch.nn.init.calculate_gain(self.activation().__class__.__name__.lower())\n\t        # not overwriting self.units!\n\t        units = [reduce(mul, self.resolution, 1)] + self.units\n\t        view = utils.torch.View(-1, units[0])\n\t        self.append_layer('view0', view)\n\t        for layer in range(1, len(units)):\n", "            in_features = units[layer - 1]\n\t            out_features = units[layer]\n\t            lin = torch.nn.Linear(in_features=in_features, out_features=out_features, bias=self.bias)\n\t            torch.nn.init.kaiming_normal_(lin.weight, gain)\n\t            if self.bias:\n\t                torch.nn.init.constant_(lin.bias, 0)\n\t            self.append_layer('lin%d' % layer, lin)\n\t            if self.activation:\n\t                act = self.activation()\n\t                self.append_layer('act%d' % layer, act)\n", "            if self.normalization is not None:\n\t                bn = self.normalization(out_features)\n\t                torch.nn.init.constant_(bn.weight, 1)\n\t                torch.nn.init.constant_(bn.bias, 0)\n\t                self.append_layer('bn%d' % layer, bn)\n\t        logits = torch.nn.Linear(units[-1], self._N_output)\n\t        torch.nn.init.kaiming_normal_(logits.weight, gain)\n\t        torch.nn.init.constant_(logits.bias, 0)\n\t        self.append_layer('logits', logits)\n"]}
{"filename": "models/preact_resnet_bottleneck.py", "chunked_list": ["\"\"\"\n\tPre-activation ResNet bottleneck block in PyTorch.\n\tTaken from https://github.com/locuslab/robust_union/.\n\t\"\"\"\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass PreActBottleneck(nn.Module):\n\t    \"\"\"\n\t    Pre-Activation ResNet bottleneck block.\n", "    \"\"\"\n\t    expansion = 4\n\t    def __init__(self, in_planes, planes, stride=1):\n\t        \"\"\"\n\t        Constructor.\n\t        :param in_planes: number of input planes\n\t        :type in_planes: int\n\t        :param planes: output planes\n\t        :type planes: int\n\t        :param stride: stride for block\n", "        :type stride: int\n\t        \"\"\"\n\t        super(PreActBottleneck, self).__init__()\n\t        self.bn1 = nn.BatchNorm2d(in_planes)\n\t        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n\t        self.bn2 = nn.BatchNorm2d(planes)\n\t        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\t        self.bn3 = nn.BatchNorm2d(planes)\n\t        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\t        if stride != 1 or in_planes != self.expansion*planes:\n", "            self.shortcut = nn.Sequential(\n\t                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n\t            )\n\t    def forward(self, x):\n\t        out = F.relu(self.bn1(x))\n\t        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n\t        out = self.conv1(out)\n\t        out = self.conv2(F.relu(self.bn2(out)))\n\t        out = self.conv3(F.relu(self.bn3(out)))\n\t        out += shortcut\n", "        return out"]}
{"filename": "models/wide_resnet_block.py", "chunked_list": ["\"\"\"\n\tWide ResNet block.\n\tTaken from https://github.com/meliketoy/wide-resnet.pytorch.\n\t\"\"\"\n\timport torch\n\timport numpy\n\tclass WideResNetBlock(torch.nn.Module):\n\t    \"\"\"\n\t    Wide ResNet block.\n\t    \"\"\"\n", "    def __init__(self, inplanes, planes, stride=1, dropout=0, normalization=True):\n\t        \"\"\"\n\t        Constructor.\n\t        :param inplanes: input channels\n\t        :type inplanes: int\n\t        :param planes: output channels\n\t        :type planes: int\n\t        :param stride: stride\n\t        :type stride: int\n\t        :param dropout: dropout rate\n", "        :type dropout: float\n\t        :param normalization: whether to use normalization\n\t        :type normalization: bool\n\t        \"\"\"\n\t        assert inplanes > 0\n\t        assert planes > 0\n\t        assert planes >= inplanes\n\t        assert stride >= 1\n\t        assert dropout >= 0 and dropout < 1\n\t        super(WideResNetBlock, self).__init__()\n", "        self.normalization = normalization\n\t        \"\"\" (bool) Normalization or not. \"\"\"\n\t        self.dropout = dropout\n\t        \"\"\" (float) Dropout factor. \"\"\"\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        if self.normalization:\n\t            self.bn1 = torch.nn.BatchNorm2d(inplanes)\n\t            torch.nn.init.constant_(self.bn1.weight, 1)\n\t            torch.nn.init.constant_(self.bn1.bias, 0)\n", "        self.relu1 = torch.nn.ReLU(inplace=self.inplace)\n\t        self.conv1 = torch.nn.Conv2d(inplanes, planes, kernel_size=3, padding=1, bias=True)\n\t        torch.nn.init.xavier_uniform_(self.conv1.weight, gain=numpy.sqrt(2))\n\t        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        torch.nn.init.constant_(self.conv1.bias, 0)\n\t        if self.dropout > 1e-3:\n\t            self.drop1 = torch.nn.Dropout(p=dropout)\n\t        if self.normalization:\n\t            self.bn2 = torch.nn.BatchNorm2d(planes)\n\t            torch.nn.init.constant_(self.bn2.weight, 1)\n", "            torch.nn.init.constant_(self.bn2.bias, 0)\n\t        self.relu2 = torch.nn.ReLU(inplace=self.inplace)\n\t        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=True)\n\t        torch.nn.init.xavier_uniform_(self.conv2.weight, gain=numpy.sqrt(2))\n\t        # torch.nn.init.kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')\n\t        torch.nn.init.constant_(self.conv2.bias, 0)\n\t        self.shortcut = torch.nn.Sequential()\n\t        if stride != 1 or inplanes != planes:\n\t            self.shortcut = torch.nn.Sequential(\n\t                torch.nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=True),\n", "            )\n\t    def forward(self, x):\n\t        if self.normalization:\n\t            out = self.bn1(x)\n\t            out = self.relu1(out)\n\t        else:\n\t            out = self.relu1(x)\n\t        out = self.conv1(out)\n\t        if self.dropout > 1e-3:\n\t            out = self.drop1(out)\n", "        if self.normalization:\n\t            out = self.bn2(out)\n\t        out = self.relu2(out)\n\t        out = self.conv2(out)\n\t        out += self.shortcut(x)\n\t        return out"]}
{"filename": "models/preact_resnet.py", "chunked_list": ["\"\"\"\n\tPre-activation ResNet in PyTorch.\n\tTaken from https://github.com/locuslab/robust_union/.\n\t\"\"\"\n\timport utils.torch\n\tfrom .classifier import Classifier\n\tfrom .preact_resnet_block import *\n\tfrom .preact_resnet_bottleneck import *\n\tclass PreActResNet(Classifier):\n\t    \"\"\"\n", "    More or less fixed pre-activation ResNet.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution, blocks, bottleneck=False, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes\n\t        :type N_class: int\n\t        :param resolution: resolution\n\t        :type resolution: [int]\n\t        :param blocks: number of layers per blocks, for exactly 4 blocks\n", "        :type blocks: [int] of length 4\n\t        :param bottleneck: whether to use bottleneck blocks\n\t        :type bottleneck: False\n\t        \"\"\"\n\t        super(PreActResNet, self).__init__(N_class, resolution, **kwargs)\n\t        assert len(blocks) == 4\n\t        self.in_planes = 64\n\t        self.blocks = blocks\n\t        \"\"\" ([int]) Blocks. \"\"\"\n\t        self.bottleneck = bottleneck\n", "        \"\"\" (bool) Inplace. \"\"\"\n\t        if self.bottleneck:\n\t            block = PreActBottleneck\n\t        else:\n\t            block = PreActBlock\n\t        self.append_layer('conv1', nn.Conv2d(resolution[0], 64, kernel_size=3, stride=1, padding=1, bias=False))\n\t        self.append_layer('layer1', self._make_layer(block, 64, blocks[0], stride=1))\n\t        self.append_layer('layer2', self._make_layer(block, 128, blocks[1], stride=2))\n\t        self.append_layer('layer3', self._make_layer(block, 256, blocks[2], stride=2))\n\t        self.append_layer('layer4', self._make_layer(block, 512, blocks[3], stride=2))\n", "        downsampled = 2*2*2\n\t        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n\t        self.append_layer('avgpool', pool)\n\t        view = utils.torch.View(-1, self.in_planes)\n\t        self.append_layer('view', view)\n\t        #out = F.avg_pool2d(out, 4)\n\t        #out = out.view(out.size(0), -1)\n\t        self.append_layer('linear', nn.Linear(512*block.expansion, N_class))\n\t    def _make_layer(self, block, planes, num_blocks, stride):\n\t        strides = [stride] + [1]*(num_blocks-1)\n", "        layers = []\n\t        for stride in strides:\n\t            layers.append(block(self.in_planes, planes, stride))\n\t            self.in_planes = planes * block.expansion\n\t        return nn.Sequential(*layers)"]}
{"filename": "models/wide_resnet.py", "chunked_list": ["\"\"\"\n\tWide ResNet.\n\tTaken from https://github.com/meliketoy/wide-resnet.pytorch.\n\t\"\"\"\n\timport numpy\n\timport torch\n\timport utils.torch\n\tfrom .classifier import Classifier\n\tfrom .wide_resnet_block import WideResNetBlock\n\timport torch.nn as nn\n", "class WideResNet(Classifier):\n\t    \"\"\"\n\t    Wide Res-Net.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n", "        :type resolution: int\n\t        :param depth: depth from which to calculate the blocks\n\t        :type depth: int\n\t        :param depth: width factor\n\t        :type depth: int\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        :param dropout: dropout rate\n", "        :type dropout: float\n\t        \"\"\"\n\t        super(WideResNet, self).__init__(N_class, resolution, **kwargs)\n\t        self.depth = depth\n\t        \"\"\" (int) Depth. \"\"\"\n\t        self.width = width\n\t        \"\"\" (int) Width. \"\"\"\n\t        self.channels = channels\n\t        \"\"\" (int) Channels. \"\"\"\n\t        self.dropout = dropout\n", "        \"\"\" (int) Dropout. \"\"\"\n\t        self.normalization = normalization\n\t        \"\"\" (callable) Normalization. \"\"\"\n\t        self.in_planes = channels\n\t        \"\"\" (int) Helper for channels. \"\"\"\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        assert (depth-4)%6 == 0, 'Wide-resnet depth should be 6n+4'\n\t        n = int((depth-4)/6)\n\t        k = width\n", "        planes = [self.channels, self.channels*k, 2*self.channels*k, 4*self.channels*k]\n\t        downsampled = 1\n\t        conv = torch.nn.Conv2d(resolution[0], planes[0], kernel_size=3, stride=1, padding=1, bias=True)\n\t        torch.nn.init.xavier_uniform_(conv.weight, gain=numpy.sqrt(2))\n\t        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        torch.nn.init.constant_(conv.bias, 0)\n\t        self.append_layer('conv0', conv)\n\t        block1 = self._wide_layer(WideResNetBlock, planes[1], n, stride=1)\n\t        self.append_layer('block1', block1)\n\t        block2 = self._wide_layer(WideResNetBlock, planes[2], n, stride=2)\n", "        downsampled *= 2\n\t        self.append_layer('block2', block2)\n\t        block3 = self._wide_layer(WideResNetBlock, planes[3], n, stride=2)\n\t        downsampled *= 2\n\t        self.append_layer('block3', block3)\n\t        if self.normalization:\n\t            bn = torch.nn.BatchNorm2d(planes[3], momentum=0.9)\n\t            torch.nn.init.constant_(bn.weight, 1)\n\t            torch.nn.init.constant_(bn.bias, 0)\n\t            self.append_layer('bn3', bn)\n", "        relu = torch.nn.ReLU(inplace=self.inplace)\n\t        self.append_layer('relu3', relu)\n\t        representation = planes[3]\n\t        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n\t        self.append_layer('avgpool', pool)\n\t        view = utils.torch.View(-1, representation)\n\t        self.append_layer('view', view)\n\t        gain = torch.nn.init.calculate_gain('relu')\n\t        logits = torch.nn.Linear(planes[3], self._N_output)\n\t        torch.nn.init.kaiming_normal_(logits.weight, gain)\n", "        torch.nn.init.constant_(logits.bias, 0)\n\t        self.append_layer('logits', logits)\n\t    def _wide_layer(self, block, planes, blocks, stride):\n\t        strides = [stride] + [1]*(blocks-1)\n\t        layers = []\n\t        for stride in strides:\n\t            layers.append(block(self.in_planes, planes, stride, self.dropout, self.normalization))\n\t            self.in_planes = planes\n\t        return torch.nn.Sequential(*layers)\n\tclass WideResNetTwoBranch(torch.nn.Module):\n", "    \"\"\"\n\t    Wide Res-Net.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n", "        :param depth: depth from which to calculate the blocks\n\t        :type depth: int\n\t        :param depth: width factor\n\t        :type depth: int\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        :param dropout: dropout rate\n\t        :type dropout: float\n", "        \"\"\"\n\t        super(WideResNetTwoBranch, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.resolution = resolution\n\t        self.depth = depth\n\t        \"\"\" (int) Depth. \"\"\"\n\t        self.width = width\n\t        \"\"\" (int) Width. \"\"\"\n\t        self.channels = channels\n\t        \"\"\" (int) Channels. \"\"\"\n", "        self.dropout = dropout\n\t        \"\"\" (int) Dropout. \"\"\"\n\t        self.normalization = normalization\n\t        \"\"\" (callable) Normalization. \"\"\"\n\t        self.in_planes = channels\n\t        \"\"\" (int) Helper for channels. \"\"\"\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        assert (depth-4)%6 == 0, 'Wide-resnet depth should be 6n+4'\n\t        self.feature_layers = nn.Sequential()\n", "        n = int((depth-4)/6)\n\t        k = width\n\t        planes = [self.channels, self.channels*k, 2*self.channels*k, 4*self.channels*k]\n\t        downsampled = 1\n\t        conv = torch.nn.Conv2d(resolution[0], planes[0], kernel_size=3, stride=1, padding=1, bias=True)\n\t        torch.nn.init.xavier_uniform_(conv.weight, gain=numpy.sqrt(2))\n\t        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        torch.nn.init.constant_(conv.bias, 0)\n\t        self.feature_layers.add_module('conv0', conv)\n\t        block1 = self._wide_layer(WideResNetBlock, planes[1], n, stride=1)\n", "        self.feature_layers.add_module('block1', block1)\n\t        block2 = self._wide_layer(WideResNetBlock, planes[2], n, stride=2)\n\t        downsampled *= 2\n\t        self.feature_layers.add_module('block2', block2)\n\t        block3 = self._wide_layer(WideResNetBlock, planes[3], n, stride=2)\n\t        downsampled *= 2\n\t        self.feature_layers.add_module('block3', block3)\n\t        if self.normalization:\n\t            bn = torch.nn.BatchNorm2d(planes[3], momentum=0.9)\n\t            torch.nn.init.constant_(bn.weight, 1)\n", "            torch.nn.init.constant_(bn.bias, 0)\n\t            self.feature_layers.add_module('bn3', bn)\n\t        relu = torch.nn.ReLU(inplace=self.inplace)\n\t        self.feature_layers.add_module('relu3', relu)\n\t        representation = planes[3]\n\t        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n\t        self.feature_layers.add_module('avgpool', pool)\n\t        view = utils.torch.View(-1, representation)\n\t        self.feature_layers.add_module('view', view)\n\t        self.classifier_layers = nn.Sequential()\n", "        gain = torch.nn.init.calculate_gain('relu')\n\t        logits = torch.nn.Linear(planes[3], self.N_class)\n\t        torch.nn.init.kaiming_normal_(logits.weight, gain)\n\t        torch.nn.init.constant_(logits.bias, 0)\n\t        self.classifier_layers.add_module('logits', logits)\n\t        self.dense_layers = nn.Sequential()\n\t        # # Shallow detector\n\t        # self.dense_layers.add_module(\"d0\", nn.Linear(representation, 256))\n\t        # self.dense_layers.add_module(\"d1\", nn.BatchNorm1d(256))\n\t        # self.dense_layers.add_module(\"d2\", nn.ReLU())\n", "        # self.dense_layers.add_module(\"d3\", nn.Linear(256, 1))\n\t        # Detector with more layers\n\t        self.dense_layers.add_module(\"d0\", nn.Linear(representation, 1024))\n\t        self.dense_layers.add_module(\"bn0\", nn.BatchNorm1d(1024))\n\t        self.dense_layers.add_module(\"rl0\", nn.ReLU())\n\t        for j in range(5):\n\t            self.dense_layers.add_module(f\"d{j + 1:d}\", nn.Linear(1024, 1024))\n\t            self.dense_layers.add_module(f\"rl{j + 1:d}\", nn.ReLU())\n\t        self.dense_layers.add_module(\"de\", nn.Linear(1024, 1))\n\t    def _wide_layer(self, block, planes, blocks, stride):\n", "        strides = [stride] + [1]*(blocks-1)\n\t        layers = []\n\t        for stride in strides:\n\t            layers.append(block(self.in_planes, planes, stride, self.dropout, self.normalization))\n\t            self.in_planes = planes\n\t        return torch.nn.Sequential(*layers)\n\t    def forward(self, x, return_d=False):\n\t        feature = self.feature_layers(x)\n\t        cls_output = self.classifier_layers(feature)\n\t        d_output = self.dense_layers(feature)\n", "        d_output = torch.sigmoid(d_output)\n\t        if return_d:\n\t            return cls_output, d_output\n\t        else:\n\t            return cls_output\n\tclass WideResNetTwoBranchDenseV1(torch.nn.Module):\n\t    \"\"\"\n\t    Wide Res-Net.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0, out_dim=10, use_BN=False, along=False, **kwargs):\n", "        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param depth: depth from which to calculate the blocks\n\t        :type depth: int\n\t        :param depth: width factor\n\t        :type depth: int\n", "        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        :param dropout: dropout rate\n\t        :type dropout: float\n\t        \"\"\"\n\t        super(WideResNetTwoBranchDenseV1, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.along = along\n", "        self.resolution = resolution\n\t        self.depth = depth\n\t        \"\"\" (int) Depth. \"\"\"\n\t        self.width = width\n\t        \"\"\" (int) Width. \"\"\"\n\t        self.channels = channels\n\t        \"\"\" (int) Channels. \"\"\"\n\t        self.dropout = dropout\n\t        \"\"\" (int) Dropout. \"\"\"\n\t        self.normalization = normalization\n", "        \"\"\" (callable) Normalization. \"\"\"\n\t        self.in_planes = channels\n\t        \"\"\" (int) Helper for channels. \"\"\"\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        assert (depth-4)%6 == 0, 'Wide-resnet depth should be 6n+4'\n\t        self.feature_layers = nn.Sequential()\n\t        n = int((depth-4)/6)\n\t        k = width\n\t        planes = [self.channels, self.channels*k, 2*self.channels*k, 4*self.channels*k]\n", "        downsampled = 1\n\t        conv = torch.nn.Conv2d(resolution[0], planes[0], kernel_size=3, stride=1, padding=1, bias=True)\n\t        torch.nn.init.xavier_uniform_(conv.weight, gain=numpy.sqrt(2))\n\t        # torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        torch.nn.init.constant_(conv.bias, 0)\n\t        self.feature_layers.add_module('conv0', conv)\n\t        block1 = self._wide_layer(WideResNetBlock, planes[1], n, stride=1)\n\t        self.feature_layers.add_module('block1', block1)\n\t        block2 = self._wide_layer(WideResNetBlock, planes[2], n, stride=2)\n\t        downsampled *= 2\n", "        self.feature_layers.add_module('block2', block2)\n\t        block3 = self._wide_layer(WideResNetBlock, planes[3], n, stride=2)\n\t        downsampled *= 2\n\t        self.feature_layers.add_module('block3', block3)\n\t        if self.normalization:\n\t            bn = torch.nn.BatchNorm2d(planes[3], momentum=0.9)\n\t            torch.nn.init.constant_(bn.weight, 1)\n\t            torch.nn.init.constant_(bn.bias, 0)\n\t            self.feature_layers.add_module('bn3', bn)\n\t        relu = torch.nn.ReLU(inplace=self.inplace)\n", "        self.feature_layers.add_module('relu3', relu)\n\t        representation = planes[3]\n\t        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n\t        self.feature_layers.add_module('avgpool', pool)\n\t        view = utils.torch.View(-1, representation)\n\t        self.feature_layers.add_module('view', view)\n\t        self.classifier_layers = nn.Sequential()\n\t        gain = torch.nn.init.calculate_gain('relu')\n\t        logits = torch.nn.Linear(planes[3], self.N_class)\n\t        torch.nn.init.kaiming_normal_(logits.weight, gain)\n", "        torch.nn.init.constant_(logits.bias, 0)\n\t        self.classifier_layers.add_module('logits', logits)\n\t        if use_BN:\n\t            self.dense_layers = nn.Sequential(\n\t                nn.Linear(representation, 256),\n\t                nn.BatchNorm1d(256),\n\t                nn.ReLU(),\n\t                nn.Linear(256, out_dim)\n\t                )\n\t        else:\n", "            self.dense_layers = nn.Sequential(\n\t                nn.Linear(representation, 256),\n\t                nn.ReLU(),\n\t                nn.Linear(256, out_dim)\n\t                )\n\t    def _wide_layer(self, block, planes, blocks, stride):\n\t        strides = [stride] + [1]*(blocks-1)\n\t        layers = []\n\t        for stride in strides:\n\t            layers.append(block(self.in_planes, planes, stride, self.dropout, self.normalization))\n", "            self.in_planes = planes\n\t        return torch.nn.Sequential(*layers)\n\t    def forward(self, x, return_aux=False):\n\t        feature = self.feature_layers(x)\n\t        cls_output = self.classifier_layers(feature)\n\t        if self.along:\n\t            evidence_return = self.dense_layers(feature)\n\t        else:\n\t            evidence_return = self.classifier_layers(feature) + self.dense_layers(feature)\n\t        if return_aux:\n", "            return cls_output, evidence_return\n\t        else:\n\t            return cls_output\n\tclass WideResNetConf(torch.nn.Module):\n\t    \"\"\"\n\t    Wide Res-Net.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0,\n\t                 conf_approx='network', temperature=0.01, **kwargs):\n\t        \"\"\"\n", "        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param depth: depth from which to calculate the blocks\n\t        :type depth: int\n\t        :param depth: width factor\n\t        :type depth: int\n\t        :param normalization: normalization to use\n", "        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        :param dropout: dropout rate\n\t        :type dropout: float\n\t        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n\t                           ['logsumexp', 'network'].\n\t        :type conf_approx: string\n\t        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n\t        :type temperature: float\n", "        \"\"\"\n\t        super(WideResNetConf, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.conf_approx = conf_approx\n\t        assert self.conf_approx in ('logsumexp', 'network'), \"Invalid input for 'conf_approx'\"\n\t        self.temperature = temperature\n\t        self.model = WideResNet(N_class, resolution, depth, width, normalization, channels, dropout)\n\t        if self.conf_approx == \"network\":\n\t            self.conf_net = nn.Sequential()\n\t            self.conf_net.add_module(\"d0\", nn.Linear(self.N_class, 1024))\n", "            self.conf_net.add_module(\"bn0\", nn.BatchNorm1d(1024))\n\t            self.conf_net.add_module(\"rl0\", nn.ReLU())\n\t            for k in range(5):\n\t                self.conf_net.add_module(f\"d{k+1:d}\", nn.Linear(1024, 1024))\n\t                self.conf_net.add_module(f\"rl{k+1:d}\", nn.ReLU())\n\t            self.conf_net.add_module(\"de\", nn.Linear(1024, 1))\n\t        else:\n\t            # Simple scale and shift of the confidence approximation score\n\t            self.conf_net = nn.Sequential()\n\t            self.conf_net.add_module(\"d0\", nn.Linear(1, 1))\n", "    def forward(self, x, return_d=False):\n\t        # classifier prediction logits\n\t        output = self.model(x)\n\t        # detector prediction\n\t        if self.conf_approx == 'logsumexp':\n\t            # `output` has the prediction logits\n\t            # `d_output` should correspond to the probability of rejection\n\t            '''\n\t            softmax_output = torch.nn.functional.softmax(output, dim=1)\n\t            K = softmax_output.size(1)\n", "            d_output = 1. - torch.clamp((self.temperature * torch.logsumexp((1 / self.temperature) * softmax_output, dim=1,\n\t                                                                             keepdims=True) - 1 / K) * (K / (K - 1)), min=0.0, max=1.0)\n\t            '''\n\t            max_logit = self.temperature * torch.logsumexp((1 / self.temperature) * output, 1, keepdims=True)\n\t            d_output = 1. - torch.sigmoid(self.conf_net(max_logit))\n\t        elif self.conf_approx == 'network':\n\t            d_output = torch.sigmoid(self.conf_net(output))\n\t        if return_d:\n\t            return output, d_output\n\t        else:\n", "            return output\n\tclass WideResNetEnsemble(torch.nn.Module):\n\t    \"\"\"\n\t    Wide Res-Net.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), depth=28, width=10, normalization=True, channels=16, dropout=0,\n\t                 conf_approx='network', temperature=0.01, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n", "        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param depth: depth from which to calculate the blocks\n\t        :type depth: int\n\t        :param depth: width factor\n\t        :type depth: int\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n", "        :type channels: int\n\t        :param dropout: dropout rate\n\t        :type dropout: float\n\t        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n\t                           ['logsumexp', 'network'].\n\t        :type conf_approx: string\n\t        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n\t        :type temperature: float\n\t        \"\"\"\n\t        super(WideResNetEnsemble, self).__init__(**kwargs)\n", "        self.N_class = N_class\n\t        # Classifier network C_1\n\t        self.classifier = WideResNet(N_class, resolution, depth, width, normalization, channels, dropout)\n\t        # Combined classifier and detector networks (C_0 and D_0). The trained classifier C_0 is used to initialize C_1,\n\t        # but is not used for the final prediction\n\t        self.classifier_with_reject = WideResNetConf(N_class,\n\t                                                     resolution,\n\t                                                     depth=depth,\n\t                                                     width=width,\n\t                                                     conf_approx=conf_approx,\n", "                                                     temperature=temperature)\n\t    def forward(self, x, return_d=False):\n\t        cls_output = self.classifier(x)\n\t        if return_d:\n\t            _, d_output = self.classifier_with_reject(x, return_d=True)\n\t            return cls_output, d_output\n\t        else:\n\t            return cls_output\n"]}
{"filename": "models/lenet.py", "chunked_list": ["import torch\n\timport utils.torch\n\tfrom .classifier import Classifier\n\tclass LeNet(Classifier):\n\t    \"\"\"\n\t    General LeNet classifier.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), channels=64, activation=torch.nn.ReLU, normalization=True, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n", "        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        :param units: units per layer\n\t        :type units: [int]\n\t        :param activation: activation function\n\t        :type activation: None or torch.nn.Module\n", "        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param bias: whether to use bias\n\t        :type bias: bool\n\t        \"\"\"\n\t        super(LeNet, self).__init__(N_class, resolution, **kwargs)\n\t        # the constructor parameters must be available as attributes for state to work\n\t        self.channels = channels\n\t        \"\"\" (int) Channels. \"\"\"\n\t        self.activation = activation\n", "        \"\"\" (callable) activation\"\"\"\n\t        self.normalization = normalization\n\t        \"\"\" (bool) Normalization. \"\"\"\n\t        layer = 0\n\t        layers = []\n\t        resolutions = []\n\t        gain = 1\n\t        if self.activation is not None:\n\t            gain = torch.nn.init.calculate_gain(self.activation().__class__.__name__.lower())\n\t        while True:\n", "            input_channels = self.resolution[0] if layer == 0 else layers[layer - 1]\n\t            output_channels = self.channels if layer == 0 else layers[layer - 1] * 2\n\t            conv = torch.nn.Conv2d(input_channels, output_channels, kernel_size=5, stride=1, padding=2)\n\t            #torch.nn.init.normal_(conv.weight, mean=0, std=0.1)\n\t            torch.nn.init.kaiming_normal_(conv.weight, gain)\n\t            torch.nn.init.constant_(conv.bias, 0.1)\n\t            self.append_layer('conv%d' % layer, conv)\n\t            if self.normalization:\n\t                bn = torch.nn.BatchNorm2d(output_channels)\n\t                torch.nn.init.constant_(bn.weight, 1)\n", "                torch.nn.init.constant_(bn.bias, 0)\n\t                self.append_layer('bn%d' % layer, bn)\n\t            if self.activation:\n\t                relu = self.activation()\n\t                self.append_layer('act%d' % layer, relu)\n\t            pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n\t            self.append_layer('pool%d' % layer, pool)\n\t            layers.append(output_channels)\n\t            resolutions.append([\n\t                self.resolution[1] // 2 if layer == 0 else resolutions[layer - 1][0] // 2,\n", "                self.resolution[2] // 2 if layer == 0 else resolutions[layer - 1][1] // 2,\n\t            ])\n\t            if resolutions[-1][0] // 2 < 3 or resolutions[-1][0] % 2 == 1 or resolutions[-1][1] // 2 < 3 or resolutions[-1][1] % 2 == 1:\n\t                break\n\t            layer += 1\n\t        representation = int(resolutions[-1][0] * resolutions[-1][1] * layers[-1])\n\t        view = utils.torch.View(-1, representation)\n\t        self.append_layer('view', view)\n\t        fc = torch.nn.Linear(representation, 1024)\n\t        self.append_layer('fc%d' % layer, fc)\n", "        if self.activation:\n\t            relu = self.activation()\n\t            self.append_layer('act%d' % layer, relu)\n\t        logits = torch.nn.Linear(1024, self._N_output)\n\t        #torch.nn.init.normal_(conv.weight, mean=0, std=0.1)\n\t        torch.nn.init.kaiming_normal_(logits.weight, gain)\n\t        torch.nn.init.constant_(logits.bias, 0.1)\n\t        self.append_layer('logits', logits)\n"]}
{"filename": "models/__init__.py", "chunked_list": ["\"\"\"\n\tVarious models. All models extend Classifier allowing to be easily saved an loaded using common.state.\n\t\"\"\"\n\tfrom .classifier import Classifier\n\tfrom .lenet import LeNet\n\tfrom .mlp import MLP\n\tfrom .resnet import ResNet, ResNetTwoBranch, ResNetConf, ResNetTwoBranchDenseV1, ResNetEnsemble\n\tfrom .wide_resnet import WideResNet, WideResNetTwoBranch, WideResNetConf, WideResNetTwoBranchDenseV1, WideResNetEnsemble\n\tfrom .fixed_lenet import FixedLeNet, FixedLeNetTwoBranch, FixedLeNetConf, FixedLeNetTwoBranchDenseV1, FixedLeNetEnsemble\n\tfrom .preact_resnet import PreActResNet\n"]}
{"filename": "models/resnet_block.py", "chunked_list": ["\"\"\"\n\tResNet block.\n\tTake from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py.\n\t\"\"\"\n\timport torch\n\timport utils.torch\n\tclass ResNetBlock(torch.nn.Module):\n\t    \"\"\"\n\t    ResNet block.\n\t    \"\"\"\n", "    def __init__(self, inplanes, planes, stride=1, downsample=None, normalization=True):\n\t        \"\"\"\n\t        Constructor.\n\t        :param inplanes: input channels\n\t        :type inplanes: int\n\t        :param planes: output channels\n\t        :type planes: int\n\t        :param stride: stride\n\t        :type stride: int\n\t        :param downsample: whether to downsample\n", "        :type downsample: bool\n\t        :param normalization: whether to use normalization\n\t        :type normalization: bool\n\t        \"\"\"\n\t        super(ResNetBlock, self).__init__()\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        self.conv1 = torch.nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\t        torch.nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        self.normalization = normalization\n", "        if self.normalization:\n\t            self.norm1 = torch.nn.BatchNorm2d(planes)\n\t            torch.nn.init.constant_(self.norm1.weight, 1)\n\t            torch.nn.init.constant_(self.norm1.bias, 0)\n\t        self.relu = torch.nn.ReLU(inplace=self.inplace)\n\t        self.conv2 = torch.nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\t        torch.nn.init.kaiming_normal_(self.conv2.weight, mode='fan_out', nonlinearity='relu')\n\t        if self.normalization:\n\t            self.norm2 = torch.nn.BatchNorm2d(planes)\n\t            torch.nn.init.constant_(self.norm2.weight, 1)\n", "            torch.nn.init.constant_(self.norm2.bias, 0)\n\t        self.downsample = downsample\n\t        self.stride = stride\n\t    def forward(self, x):\n\t        \"\"\"\n\t        Forward pass.\n\t        :param x: input\n\t        :type x: torch.autograd.Variable\n\t        :return: output\n\t        :rtype: torch.autograd.Variable\n", "        \"\"\"\n\t        out = self.conv1(x)\n\t        if self.normalization:\n\t            out = self.norm1(out)\n\t        out = self.relu(out)\n\t        out = self.conv2(out)\n\t        if self.normalization:\n\t            out = self.norm2(out)\n\t        if self.downsample is not None:\n\t            identity = self.downsample(x)\n", "        else:\n\t            identity = x\n\t        out += identity\n\t        out = self.relu(out)\n\t        return out"]}
{"filename": "models/fixed_lenet.py", "chunked_list": ["import torch\n\timport utils.torch\n\tfrom .classifier import Classifier\n\timport torch.nn as nn\n\tclass FixedLeNet(Classifier):\n\t    \"\"\"\n\t    Fixed LeNet architecture, working on MNIST architectures only.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 28, 28), **kwargs):\n\t        \"\"\"\n", "        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        \"\"\"\n\t        assert resolution[0] == 1\n\t        assert resolution[1] == 28\n\t        assert resolution[2] == 28\n\t        super(FixedLeNet, self).__init__(N_class, resolution, **kwargs)\n", "        self.append_layer('0', nn.Conv2d(resolution[0], 32, 5, padding=2))\n\t        self.append_layer('1', nn.ReLU())\n\t        self.append_layer('2', nn.MaxPool2d(2, 2))\n\t        self.append_layer('3', nn.Conv2d(32, 64, 5, padding=2))\n\t        self.append_layer('4', nn.ReLU())\n\t        self.append_layer('5', nn.MaxPool2d(2, 2))\n\t        self.append_layer('6', utils.torch.Flatten())\n\t        self.append_layer('7', nn.Linear(7 * 7 * 64, 1024))\n\t        self.append_layer('8', nn.ReLU())\n\t        self.append_layer('9', nn.Linear(1024, self.N_class))\n", "class FixedLeNetTwoBranch(torch.nn.Module):\n\t    \"\"\"\n\t    Fixed LeNet architecture, working on MNIST architectures only.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 28, 28), **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n", "        :type resolution: int\n\t        \"\"\"\n\t        assert resolution[0] == 1\n\t        assert resolution[1] == 28\n\t        assert resolution[2] == 28\n\t        super(FixedLeNetTwoBranch, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.feature_layers = nn.Sequential()\n\t        self.feature_layers.add_module(\"f0\", nn.Conv2d(resolution[0], 32, 5, padding=2))\n\t        self.feature_layers.add_module(\"f1\", nn.ReLU())\n", "        self.feature_layers.add_module(\"f2\", nn.MaxPool2d(2, 2))\n\t        self.feature_layers.add_module(\"f3\", nn.Conv2d(32, 64, 5, padding=2))\n\t        self.feature_layers.add_module(\"f4\", nn.ReLU())\n\t        self.feature_layers.add_module(\"f5\", nn.MaxPool2d(2, 2))\n\t        self.feature_layers.add_module(\"f6\", utils.torch.Flatten())\n\t        self.feature_layers.add_module(\"f7\", nn.Linear(7 * 7 * 64, 1024))\n\t        self.feature_layers.add_module(\"f8\", nn.ReLU())\n\t        self.classifier_layers = nn.Sequential()\n\t        self.classifier_layers.add_module(\"c0\", nn.Linear(1024, self.N_class))\n\t        # Shallow detector\n", "        # self.dense_layers = nn.Sequential()\n\t        # self.dense_layers.add_module(\"d0\", nn.Linear(1024, 256))\n\t        # self.dense_layers.add_module(\"d1\", nn.ReLU())\n\t        # self.dense_layers.add_module(\"d2\", nn.Linear(256, 1))\n\t        # Deeper detector\n\t        self.dense_layers = nn.Sequential()\n\t        self.dense_layers.add_module(\"d0\", nn.Linear(1024, 256))\n\t        self.dense_layers.add_module(\"d1\", nn.ReLU())\n\t        self.dense_layers.add_module(\"d2\", nn.Linear(256, 256))\n\t        self.dense_layers.add_module(\"d3\", nn.ReLU())\n", "        self.dense_layers.add_module(\"d4\", nn.Linear(256, 1))\n\t    def forward(self, x, return_d=False):\n\t        feature = self.feature_layers(x)\n\t        cls_output = self.classifier_layers(feature)\n\t        d_output = self.dense_layers(feature)\n\t        d_output = torch.sigmoid(d_output)\n\t        if return_d:\n\t            return cls_output, d_output\n\t        else:\n\t            return cls_output\n", "class FixedLeNetTwoBranchDenseV1(torch.nn.Module):\n\t    \"\"\"\n\t    Fixed LeNet architecture, working on MNIST architectures only.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 28, 28), out_dim=10, use_BN=False, along=False, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n", "        :type resolution: int\n\t        \"\"\"\n\t        assert resolution[0] == 1\n\t        assert resolution[1] == 28\n\t        assert resolution[2] == 28\n\t        super(FixedLeNetTwoBranchDenseV1, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.along = along\n\t        self.feature_layers = nn.Sequential()\n\t        self.feature_layers.add_module(\"f0\", nn.Conv2d(resolution[0], 32, 5, padding=2))\n", "        self.feature_layers.add_module(\"f1\", nn.ReLU())\n\t        self.feature_layers.add_module(\"f2\", nn.MaxPool2d(2, 2))\n\t        self.feature_layers.add_module(\"f3\", nn.Conv2d(32, 64, 5, padding=2))\n\t        self.feature_layers.add_module(\"f4\", nn.ReLU())\n\t        self.feature_layers.add_module(\"f5\", nn.MaxPool2d(2, 2))\n\t        self.feature_layers.add_module(\"f6\", utils.torch.Flatten())\n\t        self.feature_layers.add_module(\"f7\", nn.Linear(7 * 7 * 64, 1024))\n\t        self.feature_layers.add_module(\"f8\", nn.ReLU())\n\t        self.classifier_layers = nn.Sequential()\n\t        self.classifier_layers.add_module(\"c0\", nn.Linear(1024, self.N_class))\n", "        if use_BN:\n\t            self.dense_layers = nn.Sequential(\n\t                nn.Linear(1024, 256),\n\t                nn.BatchNorm1d(256),\n\t                nn.ReLU(),\n\t                nn.Linear(256, out_dim)\n\t                )\n\t        else:\n\t            self.dense_layers = nn.Sequential(\n\t                nn.Linear(1024, 256),\n", "                nn.ReLU(),\n\t                nn.Linear(256, out_dim)\n\t                )\n\t    def forward(self, x, return_aux=False):\n\t        feature = self.feature_layers(x)\n\t        cls_output = self.classifier_layers(feature)\n\t        if self.along:\n\t            evidence_return = self.dense_layers(feature)\n\t        else:\n\t            evidence_return = self.classifier_layers(feature) + self.dense_layers(feature)\n", "        if return_aux:\n\t            return cls_output, evidence_return\n\t        else:\n\t            return cls_output\n\tclass FixedLeNetConf(torch.nn.Module):\n\t    \"\"\"\n\t    Fixed LeNet architecture, working on MNIST architectures only.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 28, 28), conf_approx='network', temperature=0.01, **kwargs):\n\t        \"\"\"\n", "        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n\t                           ['logsumexp', 'network'].\n\t        :type conf_approx: string\n\t        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n\t        :type temperature: float\n", "        \"\"\"\n\t        assert resolution[0] == 1\n\t        assert resolution[1] == 28\n\t        assert resolution[2] == 28\n\t        super(FixedLeNetConf, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.conf_approx = conf_approx\n\t        assert self.conf_approx in ('logsumexp', 'network'), \"Invalid input for 'conf_approx'\"\n\t        self.temperature = temperature\n\t        self.model = FixedLeNet(N_class)\n", "        if self.conf_approx == \"network\":\n\t            self.conf_net = nn.Sequential()\n\t            self.conf_net.add_module(\"d0\", nn.Linear(self.N_class, 256))\n\t            self.conf_net.add_module(\"d1\", nn.ReLU())\n\t            self.conf_net.add_module(\"d2\", nn.Linear(256, 256))\n\t            self.conf_net.add_module(\"d3\", nn.ReLU())\n\t            self.conf_net.add_module(\"d4\", nn.Linear(256, 1))\n\t        else:\n\t            # Simple scale and shift of the confidence approximation score\n\t            self.conf_net = nn.Sequential()\n", "            self.conf_net.add_module(\"d0\", nn.Linear(1, 1))\n\t    def forward(self, x, return_d=False):\n\t        # classifier prediction logits\n\t        output = self.model(x)\n\t        # detector prediction\n\t        if self.conf_approx == 'logsumexp':\n\t            # `output` has the prediction logits\n\t            # `d_output` should correspond to the probability of rejection\n\t            '''\n\t            softmax_output = torch.nn.functional.softmax(output, dim=1)\n", "            K = softmax_output.size(1)\n\t            d_output = 1. - torch.clamp((self.temperature * torch.logsumexp((1 / self.temperature) * softmax_output, dim=1,\n\t                                                                            keepdims=True) - 1 / K) * (K / (K - 1)), min=0.0, max=1.0)\n\t            '''\n\t            max_logit = self.temperature * torch.logsumexp((1 / self.temperature) * output, 1, keepdims=True)\n\t            d_output = 1. - torch.sigmoid(self.conf_net(max_logit))\n\t        elif self.conf_approx == 'network':\n\t            d_output = torch.sigmoid(self.conf_net(output))\n\t        if return_d:\n\t            return output, d_output\n", "        else:\n\t            return output\n\tclass FixedLeNetEnsemble(torch.nn.Module):\n\t    \"\"\"\n\t    Fixed LeNet architecture, working on MNIST architectures only.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution, conf_approx='network', temperature=0.01, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n", "        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n\t                           ['logsumexp', 'network'].\n\t        :type conf_approx: string\n\t        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n\t        :type temperature: float\n\t        \"\"\"\n\t        assert resolution[0] == 1\n", "        assert resolution[1] == 28\n\t        assert resolution[2] == 28\n\t        super(FixedLeNetEnsemble, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        # Classifier network C_1\n\t        self.classifier = FixedLeNet(N_class, resolution)\n\t        # Combined classifier and detector networks (C_0 and D_0). The trained classifier C_0 is used to initialize C_1,\n\t        # but is not used for the final prediction\n\t        self.classifier_with_reject = FixedLeNetConf(N_class,\n\t                                                     resolution,\n", "                                                     conf_approx=conf_approx,\n\t                                                     temperature=temperature)\n\t    def forward(self, x, return_d=False):\n\t        cls_output = self.classifier(x)\n\t        if return_d:\n\t            _, d_output = self.classifier_with_reject(x, return_d=True)\n\t            return cls_output, d_output\n\t        else:\n\t            return cls_output\n"]}
{"filename": "models/classifier.py", "chunked_list": ["import torch\n\tfrom .resnet_block import ResNetBlock\n\tfrom .wide_resnet_block import WideResNetBlock\n\timport utils.torch\n\tclass Classifier(torch.nn.Module):\n\t    \"\"\"\n\t    Base classifier.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution, **kwargs):\n\t        \"\"\"\n", "        Initialize classifier.\n\t        The keyword arguments, resolution, number of classes and other architecture parameters\n\t        from subclasses are saved as attributes. This allows to easily save and load the model\n\t        using common.state without knowing the exact architecture in advance.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution\n\t        :type resolution: [int]\n\t        \"\"\"\n\t        super(Classifier, self).__init__()\n", "        assert N_class > 0, 'positive N_class expected'\n\t        assert len(resolution) <= 3\n\t        self.N_class = int(N_class)  # Having strange bug where torch complaints about numpy.in64 being passed to nn.Linear.\n\t        \"\"\" (int) Number of classes. \"\"\"\n\t        self.resolution = list(resolution)\n\t        \"\"\" ([int]) Resolution as (channels, height, width) \"\"\"\n\t        self.kwargs = kwargs\n\t        \"\"\" (dict) Kwargs. \"\"\"\n\t        self.include_clamp = self.kwargs_get('clamp', True)\n\t        \"\"\" (bool) Whether to apply input clamping. \"\"\"\n", "        self.include_whiten = self.kwargs_get('whiten', False)\n\t        \"\"\" (bool) Whether to apply input whitening/normalization. \"\"\"\n\t        self.include_scale = self.kwargs_get('scale', False)\n\t        \"\"\" (bool) Whether to apply input scaling. \"\"\"\n\t        # __ attributes are private, which is important for the State to work properly.\n\t        self.__layers = []\n\t        \"\"\" ([str]) Will hold layer names. \"\"\"\n\t        self._N_output = self.N_class if self.N_class > 2 else 1\n\t        \"\"\" (int) Number of outputs. \"\"\"\n\t        if self.include_clamp:\n", "            self.append_layer('clamp', utils.torch.Clamp())\n\t        assert not (self.include_whiten and self.include_scale)\n\t        if self.include_whiten:\n\t            # Note that the weight and bias needs to set manually corresponding to mean and std!\n\t            whiten = utils.torch.Normalize(resolution[0])\n\t            self.append_layer('whiten', whiten)\n\t        if self.include_scale:\n\t            # Note that the weight and bias needs to set manually!\n\t            scale = utils.torch.Scale(1)\n\t            scale.weight.data[0] = -1\n", "            scale.bias.data[0] = 1\n\t            self.append_layer('scale', scale)\n\t    def kwargs_get(self, key, default):\n\t        \"\"\"\n\t        Get argument if not None.\n\t        :param key: key\n\t        :type key: str\n\t        :param default: default value\n\t        :type default: mixed\n\t        :return: value\n", "        :rtype: mixed\n\t        \"\"\"\n\t        value = self.kwargs.get(key, default)\n\t        if value is None:\n\t            value = default\n\t        return value\n\t    def append_layer(self, name, layer):\n\t        \"\"\"\n\t        Add a layer.\n\t        :param name: layer name\n", "        :type name: str\n\t        :param layer: layer\n\t        :type layer: torch.nn.Module\n\t        \"\"\"\n\t        setattr(self, name, layer)\n\t        self.__layers.append(name)\n\t    def prepend_layer(self, name, layer):\n\t        \"\"\"\n\t        Add a layer.\n\t        :param name: layer name\n", "        :type name: str\n\t        :param layer: layer\n\t        :type layer: torch.nn.Module\n\t        \"\"\"\n\t        self.insert_layer(0, name, layer)\n\t    def insert_layer(self, index, name, layer):\n\t        \"\"\"\n\t        Add a layer.\n\t        :param index: index\n\t        :type index: int\n", "        :param name: layer name\n\t        :type name: str\n\t        :param layer: layer\n\t        :type layer: torch.nn.Module\n\t        \"\"\"\n\t        setattr(self, name, layer)\n\t        self.__layers.insert(index, name)\n\t    def forward(self, image, return_features=False):\n\t        \"\"\"\n\t        Forward pass, takes an image and outputs the predictions.\n", "        :param image: input image\n\t        :type image: torch.autograd.Variable\n\t        :param return_features: whether to also return representation layer\n\t        :type return_features: bool\n\t        :return: logits\n\t        :rtype: torch.autograd.Variable\n\t        \"\"\"\n\t        features = []\n\t        output = image\n\t        # separate loops for memory constraints\n", "        if return_features:\n\t            for name in self.__layers:\n\t                output = getattr(self, name)(output)\n\t                features.append(output)\n\t            return output, features\n\t        else:\n\t            for name in self.__layers:\n\t                output = getattr(self, name)(output)\n\t            return output\n\t    def layers(self):\n", "        \"\"\"\n\t        Get layer names.\n\t        :return: layer names\n\t        :rtype: [str]\n\t        \"\"\"\n\t        return self.__layers\n\t    def __str__(self):\n\t        \"\"\"\n\t        Print network.\n\t        \"\"\"\n", "        string = ''\n\t        for name in self.__layers:\n\t            string += '(' + name + ', ' + getattr(self, name).__class__.__name__ + ')\\n'\n\t            if isinstance(getattr(self, name), torch.nn.Sequential) or isinstance(getattr(self, name), ResNetBlock) or isinstance(getattr(self, name), WideResNetBlock):\n\t                for module in getattr(self, name).modules():\n\t                    string += '\\t(' + module.__class__.__name__ + ')\\n'\n\t        return string\n"]}
{"filename": "models/resnet.py", "chunked_list": ["\"\"\"\n\tResNet.\n\tTake from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py.\n\t\"\"\"\n\timport torch\n\timport utils.torch\n\tfrom .classifier import Classifier\n\tfrom .resnet_block import ResNetBlock\n\timport torch.nn as nn\n\tclass ResNet(Classifier):\n", "    \"\"\"\n\t    Simple classifier.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n", "        :param blocks: layers per block\n\t        :type blocks: [int]\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        \"\"\"\n\t        super(ResNet, self).__init__(N_class, resolution, **kwargs)\n\t        self.blocks = blocks\n\t        \"\"\" ([int]) Blocks. \"\"\"\n", "        self.channels = channels\n\t        \"\"\" (int) Channels. \"\"\"\n\t        self.normalization = normalization\n\t        \"\"\" (callable) Normalization. \"\"\"\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        conv1 = torch.nn.Conv2d(self.resolution[0], self.channels, kernel_size=3, stride=1, padding=1, bias=False)\n\t        torch.nn.init.kaiming_normal_(conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        self.append_layer('conv1', conv1)\n\t        if self.normalization:\n", "            norm1 = torch.nn.BatchNorm2d(self.channels)\n\t            torch.nn.init.constant_(norm1.weight, 1)\n\t            torch.nn.init.constant_(norm1.bias, 0)\n\t            self.append_layer('norm1', norm1)\n\t        relu = torch.nn.ReLU(inplace=self.inplace)\n\t        self.append_layer('relu1', relu)\n\t        downsampled = 1\n\t        for i in range(len(self.blocks)):\n\t            in_planes = (2 ** max(0, i - 1)) * self.channels\n\t            out_planes = (2 ** i) * self.channels\n", "            layers = self.blocks[i]\n\t            stride = 2 if i > 0 else 1\n\t            downsample = None\n\t            if stride != 1 or in_planes != out_planes:\n\t                conv = torch.nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\t                torch.nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='relu')\n\t                if self.normalization:\n\t                    bn = torch.nn.BatchNorm2d(out_planes)\n\t                    torch.nn.init.constant_(bn.weight, 1)\n\t                    torch.nn.init.constant_(bn.bias, 0)\n", "                    downsample = torch.nn.Sequential(*[conv, bn])\n\t                else:\n\t                    downsample = torch.nn.Sequential(*[conv])\n\t            sequence = []\n\t            sequence.append(ResNetBlock(in_planes, out_planes, stride=stride, downsample=downsample, normalization=self.normalization))\n\t            for _ in range(1, layers):\n\t                sequence.append(ResNetBlock(out_planes, out_planes, stride=1, downsample=None, normalization=self.normalization))\n\t            self.append_layer('block%d' % i, torch.nn.Sequential(*sequence))\n\t            downsampled *= stride\n\t        representation = out_planes\n", "        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n\t        self.append_layer('avgpool', pool)\n\t        view = utils.torch.View(-1, representation)\n\t        self.append_layer('view', view)\n\t        gain = torch.nn.init.calculate_gain('relu')\n\t        logits = torch.nn.Linear(representation, self._N_output)\n\t        torch.nn.init.kaiming_normal_(logits.weight, gain)\n\t        torch.nn.init.constant_(logits.bias, 0)\n\t        self.append_layer('logits', logits)\n\tclass ResNetTwoBranch(torch.nn.Module):\n", "    \"\"\"\n\t    Simple classifier.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n", "        :param blocks: layers per block\n\t        :type blocks: [int]\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        \"\"\"\n\t        super(ResNetTwoBranch, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.resolution = resolution\n", "        self.blocks = blocks\n\t        \"\"\" ([int]) Blocks. \"\"\"\n\t        self.channels = channels\n\t        \"\"\" (int) Channels. \"\"\"\n\t        self.normalization = normalization\n\t        \"\"\" (callable) Normalization. \"\"\"\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        self.feature_layers = nn.Sequential()\n\t        conv1 = torch.nn.Conv2d(self.resolution[0], self.channels, kernel_size=3, stride=1, padding=1, bias=False)\n", "        torch.nn.init.kaiming_normal_(conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        self.feature_layers.add_module('conv1', conv1)\n\t        if self.normalization:\n\t            norm1 = torch.nn.BatchNorm2d(self.channels)\n\t            torch.nn.init.constant_(norm1.weight, 1)\n\t            torch.nn.init.constant_(norm1.bias, 0)\n\t            self.feature_layers.add_module('norm1', norm1)\n\t        relu = torch.nn.ReLU(inplace=self.inplace)\n\t        self.feature_layers.add_module('relu1', relu)\n\t        downsampled = 1\n", "        for i in range(len(self.blocks)):\n\t            in_planes = (2 ** max(0, i - 1)) * self.channels\n\t            out_planes = (2 ** i) * self.channels\n\t            layers = self.blocks[i]\n\t            stride = 2 if i > 0 else 1\n\t            downsample = None\n\t            if stride != 1 or in_planes != out_planes:\n\t                conv = torch.nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\t                torch.nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='relu')\n\t                if self.normalization:\n", "                    bn = torch.nn.BatchNorm2d(out_planes)\n\t                    torch.nn.init.constant_(bn.weight, 1)\n\t                    torch.nn.init.constant_(bn.bias, 0)\n\t                    downsample = torch.nn.Sequential(*[conv, bn])\n\t                else:\n\t                    downsample = torch.nn.Sequential(*[conv])\n\t            sequence = []\n\t            sequence.append(ResNetBlock(in_planes, out_planes, stride=stride, downsample=downsample, normalization=self.normalization))\n\t            for _ in range(1, layers):\n\t                sequence.append(ResNetBlock(out_planes, out_planes, stride=1, downsample=None, normalization=self.normalization))\n", "            self.feature_layers.add_module('block%d' % i, torch.nn.Sequential(*sequence))\n\t            downsampled *= stride\n\t        representation = out_planes\n\t        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n\t        self.feature_layers.add_module('avgpool', pool)\n\t        view = utils.torch.View(-1, representation)\n\t        self.feature_layers.add_module('view', view)\n\t        self.classifier_layers = nn.Sequential()\n\t        gain = torch.nn.init.calculate_gain('relu')\n\t        logits = torch.nn.Linear(representation, self.N_class)\n", "        torch.nn.init.kaiming_normal_(logits.weight, gain)\n\t        torch.nn.init.constant_(logits.bias, 0)\n\t        self.classifier_layers.add_module('logits', logits)\n\t        self.dense_layers = nn.Sequential()\n\t        # # Shallow detector\n\t        # self.dense_layers.add_module(\"d0\", nn.Linear(representation, 256))\n\t        # self.dense_layers.add_module(\"d1\", nn.BatchNorm1d(256))\n\t        # self.dense_layers.add_module(\"d2\", nn.ReLU())\n\t        # self.dense_layers.add_module(\"d3\", nn.Linear(256, 1))\n\t        # Detector with more layers\n", "        self.dense_layers.add_module(\"d0\", nn.Linear(representation, 1024))\n\t        self.dense_layers.add_module(\"bn0\", nn.BatchNorm1d(1024))\n\t        self.dense_layers.add_module(\"rl0\", nn.ReLU())\n\t        for j in range(5):\n\t            self.dense_layers.add_module(f\"d{j + 1:d}\", nn.Linear(1024, 1024))\n\t            self.dense_layers.add_module(f\"rl{j + 1:d}\", nn.ReLU())\n\t        self.dense_layers.add_module(\"de\", nn.Linear(1024, 1))\n\t    def forward(self, x, return_d=False):\n\t        feature = self.feature_layers(x)\n\t        cls_output = self.classifier_layers(feature)\n", "        d_output = self.dense_layers(feature)\n\t        d_output = torch.sigmoid(d_output)\n\t        if return_d:\n\t            return cls_output, d_output\n\t        else:\n\t            return cls_output\n\tclass ResNetTwoBranchDenseV1(torch.nn.Module):\n\t    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64, out_dim=10, use_BN=False, along=False, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n", "        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param blocks: layers per block\n\t        :type blocks: [int]\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n", "        \"\"\"\n\t        super(ResNetTwoBranchDenseV1, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.along = along\n\t        self.resolution = resolution\n\t        self.blocks = blocks\n\t        \"\"\" ([int]) Blocks. \"\"\"\n\t        self.channels = channels\n\t        \"\"\" (int) Channels. \"\"\"\n\t        self.normalization = normalization\n", "        \"\"\" (callable) Normalization. \"\"\"\n\t        self.inplace = False\n\t        \"\"\" (bool) Inplace. \"\"\"\n\t        self.feature_layers = nn.Sequential()\n\t        conv1 = torch.nn.Conv2d(self.resolution[0], self.channels, kernel_size=3, stride=1, padding=1, bias=False)\n\t        torch.nn.init.kaiming_normal_(conv1.weight, mode='fan_out', nonlinearity='relu')\n\t        self.feature_layers.add_module('conv1', conv1)\n\t        if self.normalization:\n\t            norm1 = torch.nn.BatchNorm2d(self.channels)\n\t            torch.nn.init.constant_(norm1.weight, 1)\n", "            torch.nn.init.constant_(norm1.bias, 0)\n\t            self.feature_layers.add_module('norm1', norm1)\n\t        relu = torch.nn.ReLU(inplace=self.inplace)\n\t        self.feature_layers.add_module('relu1', relu)\n\t        downsampled = 1\n\t        for i in range(len(self.blocks)):\n\t            in_planes = (2 ** max(0, i - 1)) * self.channels\n\t            out_planes = (2 ** i) * self.channels\n\t            layers = self.blocks[i]\n\t            stride = 2 if i > 0 else 1\n", "            downsample = None\n\t            if stride != 1 or in_planes != out_planes:\n\t                conv = torch.nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\t                torch.nn.init.kaiming_normal_(conv.weight, mode='fan_out', nonlinearity='relu')\n\t                if self.normalization:\n\t                    bn = torch.nn.BatchNorm2d(out_planes)\n\t                    torch.nn.init.constant_(bn.weight, 1)\n\t                    torch.nn.init.constant_(bn.bias, 0)\n\t                    downsample = torch.nn.Sequential(*[conv, bn])\n\t                else:\n", "                    downsample = torch.nn.Sequential(*[conv])\n\t            sequence = []\n\t            sequence.append(ResNetBlock(in_planes, out_planes, stride=stride, downsample=downsample, normalization=self.normalization))\n\t            for _ in range(1, layers):\n\t                sequence.append(ResNetBlock(out_planes, out_planes, stride=1, downsample=None, normalization=self.normalization))\n\t            self.feature_layers.add_module('block%d' % i, torch.nn.Sequential(*sequence))\n\t            downsampled *= stride\n\t        representation = out_planes\n\t        pool = torch.nn.AvgPool2d((self.resolution[1] // downsampled, self.resolution[2] // downsampled), stride=1)\n\t        self.feature_layers.add_module('avgpool', pool)\n", "        view = utils.torch.View(-1, representation)\n\t        self.feature_layers.add_module('view', view)\n\t        self.classifier_layers = nn.Sequential()\n\t        gain = torch.nn.init.calculate_gain('relu')\n\t        logits = torch.nn.Linear(representation, self.N_class)\n\t        torch.nn.init.kaiming_normal_(logits.weight, gain)\n\t        torch.nn.init.constant_(logits.bias, 0)\n\t        self.classifier_layers.add_module('logits', logits)\n\t        if use_BN:\n\t            self.dense_layers = nn.Sequential(\n", "                nn.Linear(representation, 256),\n\t                nn.BatchNorm1d(256),\n\t                nn.ReLU(),\n\t                nn.Linear(256, out_dim)\n\t                )\n\t        else:\n\t            self.dense_layers = nn.Sequential(\n\t                nn.Linear(representation, 256),\n\t                nn.ReLU(),\n\t                nn.Linear(256, out_dim)\n", "                )\n\t    def forward(self, x, return_aux=False):\n\t        feature = self.feature_layers(x)\n\t        cls_output = self.classifier_layers(feature)\n\t        if self.along:\n\t            evidence_return = self.dense_layers(feature)\n\t        else:\n\t            evidence_return = self.classifier_layers(feature) + self.dense_layers(feature)\n\t        if return_aux:\n\t            return cls_output, evidence_return\n", "        else:\n\t            return cls_output\n\tclass ResNetConf(torch.nn.Module):\n\t    \"\"\"\n\t    Simple classifier.\n\t    \"\"\"\n\t    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64,\n\t                 conf_approx='network', temperature=0.01, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n", "        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param blocks: layers per block\n\t        :type blocks: [int]\n\t        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n", "        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n\t                           ['logsumexp', 'network'].\n\t        :type conf_approx: string\n\t        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n\t        :type temperature: float\n\t        \"\"\"\n\t        super(ResNetConf, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        self.conf_approx = conf_approx\n\t        assert self.conf_approx in ('logsumexp', 'network'), \"Invalid input for 'conf_approx'\"\n", "        self.temperature = temperature\n\t        self.model = ResNet(N_class, resolution, blocks, normalization, channels)\n\t        if self.conf_approx == \"network\":\n\t            self.conf_net = nn.Sequential()\n\t            self.conf_net.add_module(\"d0\", nn.Linear(self.N_class, 1024))\n\t            self.conf_net.add_module(\"bn0\", nn.BatchNorm1d(1024))\n\t            self.conf_net.add_module(\"rl0\", nn.ReLU())\n\t            for k in range(5):\n\t                self.conf_net.add_module(f\"d{k+1:d}\", nn.Linear(1024, 1024))\n\t                self.conf_net.add_module(f\"rl{k+1:d}\", nn.ReLU())\n", "            self.conf_net.add_module(\"de\", nn.Linear(1024, 1))\n\t        else:\n\t            # Simple scale and shift of the confidence approximation score\n\t            self.conf_net = nn.Sequential()\n\t            self.conf_net.add_module(\"d0\", nn.Linear(1, 1))\n\t    def forward(self, x, return_d=False):\n\t        # classifier prediction logits\n\t        output = self.model(x)\n\t        # detector prediction\n\t        if self.conf_approx == 'logsumexp':\n", "            # `output` has the prediction logits\n\t            # `d_output` should correspond to the probability of rejection\n\t            '''\n\t            softmax_output = torch.nn.functional.softmax(output, dim=1)\n\t            K = softmax_output.size(1)\n\t            d_output = 1. - torch.clamp((self.temperature * torch.logsumexp((1 / self.temperature) * softmax_output, dim=1,\n\t                                                                             keepdims=True) - 1 / K) * (K / (K - 1)), min=0.0, max=1.0)\n\t            '''\n\t            max_logit = self.temperature * torch.logsumexp((1 / self.temperature) * output, 1, keepdims=True)\n\t            d_output = 1. - torch.sigmoid(self.conf_net(max_logit))\n", "        elif self.conf_approx == 'network':\n\t            d_output = torch.sigmoid(self.conf_net(output))\n\t        if return_d:\n\t            return output, d_output\n\t        else:\n\t            return output\n\tclass ResNetEnsemble(torch.nn.Module):\n\t    \"\"\"\n\t    Simple classifier.\n\t    \"\"\"\n", "    def __init__(self, N_class, resolution=(1, 32, 32), blocks=[3, 3, 3], normalization=True, channels=64,\n\t                 conf_approx='network', temperature=0.01, **kwargs):\n\t        \"\"\"\n\t        Initialize classifier.\n\t        :param N_class: number of classes to classify\n\t        :type N_class: int\n\t        :param resolution: resolution (assumed to be square)\n\t        :type resolution: int\n\t        :param blocks: layers per block\n\t        :type blocks: [int]\n", "        :param normalization: normalization to use\n\t        :type normalization: None or torch.nn.Module\n\t        :param channels: channels to start with\n\t        :type channels: int\n\t        :param conf_approx: type of approximation used for the prediction confidence. Valid choices are\n\t                           ['logsumexp', 'network'].\n\t        :type conf_approx: string\n\t        :param temperature: temperature constant in (0, 1]. Small values can lead to better approximation of the confidence.\n\t        :type temperature: float\n\t        \"\"\"\n", "        super(ResNetEnsemble, self).__init__(**kwargs)\n\t        self.N_class = N_class\n\t        # Classifier network C_1\n\t        self.classifier = ResNet(N_class, resolution, blocks, normalization, channels)\n\t        # Combined classifier and detector networks (C_0 and D_0). The trained classifier C_0 is used to initialize C_1,\n\t        # but is not used for the final prediction\n\t        self.classifier_with_reject = ResNetConf(N_class,\n\t                                                 resolution,\n\t                                                 blocks=blocks,\n\t                                                 conf_approx=conf_approx,\n", "                                                 temperature=temperature)\n\t    def forward(self, x, return_d=False):\n\t        cls_output = self.classifier(x)\n\t        if return_d:\n\t            _, d_output = self.classifier_with_reject(x, return_d=True)\n\t            return cls_output, d_output\n\t        else:\n\t            return cls_output\n"]}
