{"filename": "src/train.py", "chunked_list": ["import click\n\timport warnings\n\timport torch as th\n\t# noinspection PyUnresolvedReferences\n\tfrom src.env.minigrid_envs import *\n\tfrom src.algo.ppo_model import PPOModel\n\tfrom src.algo.ppo_trainer import PPOTrainer\n\tfrom src.utils.configs import TrainingConfig\n\tfrom stable_baselines3.common.utils import set_random_seed\n\twarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n", "def train(config):\n\t    th.autograd.set_detect_anomaly(False)\n\t    th.set_default_dtype(th.float32)\n\t    th.backends.cudnn.benchmark = False\n\t    wrapper_class = config.get_wrapper_class()\n\t    venv = config.get_venv(wrapper_class)\n\t    callbacks = config.get_callbacks()\n\t    optimizer_class, optimizer_kwargs = config.get_optimizer()\n\t    activation_fn, cnn_activation_fn = config.get_activation_fn()\n\t    config.cast_enum_values()\n", "    policy_features_extractor_class, \\\n\t        features_extractor_common_kwargs, \\\n\t        model_cnn_features_extractor_class, \\\n\t        model_features_extractor_common_kwargs = \\\n\t        config.get_cnn_kwargs(cnn_activation_fn)\n\t    policy_kwargs = dict(\n\t        run_id=config.run_id,\n\t        n_envs=config.num_processes,\n\t        activation_fn=activation_fn,\n\t        learning_rate=config.learning_rate,\n", "        model_learning_rate=config.model_learning_rate,\n\t        policy_features_extractor_class=policy_features_extractor_class,\n\t        policy_features_extractor_kwargs=features_extractor_common_kwargs,\n\t        model_cnn_features_extractor_class=model_cnn_features_extractor_class,\n\t        model_cnn_features_extractor_kwargs=model_features_extractor_common_kwargs,\n\t        optimizer_class=optimizer_class,\n\t        optimizer_kwargs=optimizer_kwargs,\n\t        max_grad_norm=config.max_grad_norm,\n\t        model_features_dim=config.model_features_dim,\n\t        latents_dim=config.latents_dim,\n", "        model_latents_dim=config.model_latents_dim,\n\t        policy_mlp_norm=config.policy_mlp_norm,\n\t        model_mlp_norm=config.model_mlp_norm,\n\t        model_cnn_norm=config.model_cnn_norm,\n\t        model_mlp_layers=config.model_mlp_layers,\n\t        use_status_predictor=config.use_status_predictor,\n\t        gru_layers=config.gru_layers,\n\t        policy_mlp_layers=config.policy_mlp_layers,\n\t        policy_gru_norm=config.policy_gru_norm,\n\t        use_model_rnn=config.use_model_rnn,\n", "        model_gru_norm=config.model_gru_norm,\n\t        total_timesteps=config.total_steps,\n\t        n_steps=config.n_steps,\n\t        int_rew_source=config.int_rew_source,\n\t        icm_forward_loss_coef=config.icm_forward_loss_coef,\n\t        ngu_knn_k=config.ngu_knn_k,\n\t        ngu_dst_momentum=config.ngu_dst_momentum,\n\t        ngu_use_rnd=config.ngu_use_rnd,\n\t        rnd_err_norm=config.rnd_err_norm,\n\t        rnd_err_momentum=config.rnd_err_momentum,\n", "        rnd_use_policy_emb=config.rnd_use_policy_emb,\n\t        dsc_obs_queue_len=config.dsc_obs_queue_len,\n\t        log_dsc_verbose=config.log_dsc_verbose,\n\t    )\n\t    model = PPOTrainer(\n\t        policy=PPOModel,\n\t        env=venv,\n\t        seed=config.run_id,\n\t        run_id=config.run_id,\n\t        can_see_walls=config.can_see_walls,\n", "        image_noise_scale=config.image_noise_scale,\n\t        n_steps=config.n_steps,\n\t        n_epochs=config.n_epochs,\n\t        model_n_epochs=config.model_n_epochs,\n\t        learning_rate=config.learning_rate,\n\t        model_learning_rate=config.model_learning_rate,\n\t        gamma=config.gamma,\n\t        gae_lambda=config.gae_lambda,\n\t        ent_coef=config.ent_coef,\n\t        batch_size=config.batch_size,\n", "        pg_coef=config.pg_coef,\n\t        vf_coef=config.vf_coef,\n\t        max_grad_norm=config.max_grad_norm,\n\t        ext_rew_coef=config.ext_rew_coef,\n\t        int_rew_source=config.int_rew_source,\n\t        int_rew_coef=config.int_rew_coef,\n\t        int_rew_norm=config.int_rew_norm,\n\t        int_rew_momentum=config.int_rew_momentum,\n\t        int_rew_eps=config.int_rew_eps,\n\t        int_rew_clip=config.int_rew_clip,\n", "        adv_momentum=config.adv_momentum,\n\t        adv_norm=config.adv_norm,\n\t        adv_eps=config.adv_eps,\n\t        clip_range=config.clip_range,\n\t        clip_range_vf=config.clip_range_vf,\n\t        policy_kwargs=policy_kwargs,\n\t        env_source=config.env_source,\n\t        env_render=config.env_render,\n\t        fixed_seed=config.fixed_seed,\n\t        use_wandb=config.use_wandb,\n", "        local_logger=config.local_logger,\n\t        enable_plotting=config.enable_plotting,\n\t        plot_interval=config.plot_interval,\n\t        plot_colormap=config.plot_colormap,\n\t        log_explored_states=config.log_explored_states,\n\t        verbose=0,\n\t    )\n\t    if config.run_id == 0:\n\t        print('model.policy:', model.policy)\n\t    model.learn(\n", "        total_timesteps=config.total_steps,\n\t        callback=callbacks)\n\t@click.command()\n\t# Training params\n\t@click.option('--run_id', default=0, type=int, help='Index (and seed) of the current run')\n\t@click.option('--group_name', type=str, help='Group name (wandb option), leave blank if not logging with wandb')\n\t@click.option('--log_dir', default='./logs', type=str, help='Directory for saving training logs')\n\t@click.option('--total_steps', default=int(1e6), type=int, help='Total number of frames to run for training')\n\t@click.option('--features_dim', default=64, type=int, help='Number of neurons of a learned embedding (PPO)')\n\t@click.option('--model_features_dim', default=128, type=int,\n", "              help='Number of neurons of a learned embedding (dynamics model)')\n\t@click.option('--learning_rate', default=3e-4, type=float, help='Learning rate of PPO')\n\t@click.option('--model_learning_rate', default=3e-4, type=float, help='Learning rate of the dynamics model')\n\t@click.option('--num_processes', default=16, type=int, help='Number of training processes (workers)')\n\t@click.option('--batch_size', default=512, type=int, help='Batch size')\n\t@click.option('--n_steps', default=512, type=int, help='Number of steps to run for each process per update')\n\t# Env params\n\t@click.option('--env_source', default='minigrid', type=str, help='minigrid or procgen')\n\t@click.option('--game_name', default=\"DoorKey-8x8\", type=str, help='e.g. DoorKey-8x8, ninja, jumper')\n\t@click.option('--project_name', required=False, type=str, help='Where to store training logs (wandb option)')\n", "@click.option('--map_size', default=5, type=int, help='Size of the minigrid room')\n\t@click.option('--can_see_walls', default=1, type=int, help='Whether walls are visible to the agent')\n\t@click.option('--fully_obs', default=0, type=int, help='Whether the agent can receive full observations')\n\t@click.option('--image_noise_scale', default=0.0, type=float, help='Standard deviation of the Gaussian noise')\n\t@click.option('--procgen_mode', default='hard', type=str, help='Mode of ProcGen games (easy or hard)')\n\t@click.option('--procgen_num_threads', default=4, type=int, help='Number of parallel ProcGen threads')\n\t@click.option('--log_explored_states', default=0, type=int, help='Whether to log the number of explored states')\n\t@click.option('--fixed_seed', default=-1, type=int, help='Whether to use a fixed env seed (MiniGrid)')\n\t# Algo params\n\t@click.option('--n_epochs', default=4, type=int, help='Number of epochs to train policy and value nets')\n", "@click.option('--model_n_epochs', default=4, type=int, help='Number of epochs to train common_models')\n\t@click.option('--gamma', default=0.99, type=float, help='Discount factor')\n\t@click.option('--gae_lambda', default=0.95, type=float, help='GAE lambda')\n\t@click.option('--pg_coef', default=1.0, type=float, help='Coefficient of policy gradients')\n\t@click.option('--vf_coef', default=0.5, type=float, help='Coefficient of value function loss')\n\t@click.option('--ent_coef', default=0.01, type=float, help='Coefficient of policy entropy')\n\t@click.option('--max_grad_norm', default=0.5, type=float, help='Maximum norm of gradient')\n\t@click.option('--clip_range', default=0.2, type=float, help='PPO clip range of the policy network')\n\t@click.option('--clip_range_vf', default=-1, type=float,\n\t              help='PPO clip range of the value function (-1: disabled, >0: enabled)')\n", "@click.option('--adv_norm', default=2, type=int,\n\t              help='Normalized advantages by: [0] No normalization [1] Standardization per mini-batch [2] Standardization per rollout buffer [3] Standardization w.o. subtracting the mean per rollout buffer')\n\t@click.option('--adv_eps', default=1e-5, type=float, help='Epsilon for advantage normalization')\n\t@click.option('--adv_momentum', default=0.9, type=float, help='EMA smoothing factor for advantage normalization')\n\t# Reward params\n\t@click.option('--ext_rew_coef', default=1.0, type=float, help='Coefficient of extrinsic rewards')\n\t@click.option('--int_rew_coef', default=1e-2, type=float, help='Coefficient of intrinsic rewards (IRs)')\n\t@click.option('--int_rew_source', default='DEIR', type=str,\n\t              help='Source of IRs: [NoModel|DEIR|ICM|RND|NGU|NovelD|PlainDiscriminator|PlainInverse|PlainForward]')\n\t@click.option('--int_rew_norm', default=1, type=int,\n", "              help='Normalized IRs by: [0] No normalization [1] Standardization [2] Min-max normalization [3] Standardization w.o. subtracting the mean')\n\t@click.option('--int_rew_momentum', default=0.9, type=float,\n\t              help='EMA smoothing factor for IR normalization (-1: total average)')\n\t@click.option('--int_rew_eps', default=1e-5, type=float, help='Epsilon for IR normalization')\n\t@click.option('--int_rew_clip', default=-1, type=float, help='Clip IRs into [-X, X] when X>0')\n\t@click.option('--dsc_obs_queue_len', default=100000, type=int, help='Maximum length of observation queue (DEIR)')\n\t@click.option('--icm_forward_loss_coef', default=0.2, type=float, help='Coefficient of forward model losses (ICM)')\n\t@click.option('--ngu_knn_k', default=10, type=int, help='Search for K nearest neighbors (NGU)')\n\t@click.option('--ngu_use_rnd', default=1, type=int, help='Whether to enable lifelong IRs generated by RND (NGU)')\n\t@click.option('--ngu_dst_momentum', default=0.997, type=float,\n", "              help='EMA smoothing factor for averaging embedding distances (NGU)')\n\t@click.option('--rnd_use_policy_emb', default=1, type=int,\n\t              help='Whether to use the embeddings learned by policy/value nets as inputs (RND)')\n\t@click.option('--rnd_err_norm', default=1, type=int,\n\t              help='Normalized RND errors by: [0] No normalization [1] Standardization [2] Min-max normalization [3] Standardization w.o. subtracting the mean')\n\t@click.option('--rnd_err_momentum', default=-1, type=float,\n\t              help='EMA smoothing factor for RND error normalization (-1: total average)')\n\t# Network params\n\t@click.option('--use_model_rnn', default=1, type=int, help='Whether to enable RNNs for the dynamics model')\n\t@click.option('--latents_dim', default=256, type=int, help='Dimensions of latent features in policy/value nets\\' MLPs')\n", "@click.option('--model_latents_dim', default=256, type=int,\n\t              help='Dimensions of latent features in the dynamics model\\'s MLP')\n\t@click.option('--policy_cnn_type', default=0, type=int, help='CNN Structure ([0-2] from small to large)')\n\t@click.option('--policy_mlp_layers', default=1, type=int, help='Number of latent layers used in the policy\\'s MLP')\n\t@click.option('--policy_cnn_norm', default='BatchNorm', type=str, help='Normalization type for policy/value nets\\' CNN')\n\t@click.option('--policy_mlp_norm', default='BatchNorm', type=str, help='Normalization type for policy/value nets\\' MLP')\n\t@click.option('--policy_gru_norm', default='NoNorm', type=str, help='Normalization type for policy/value nets\\' GRU')\n\t@click.option('--model_cnn_type', default=0, type=int, help='CNN Structure ([0-2] from small to large)')\n\t@click.option('--model_mlp_layers', default=1, type=int, help='Number of latent layers used in the model\\'s MLP')\n\t@click.option('--model_cnn_norm', default='BatchNorm', type=str,\n", "              help='Normalization type for the dynamics model\\'s CNN')\n\t@click.option('--model_mlp_norm', default='BatchNorm', type=str,\n\t              help='Normalization type for the dynamics model\\'s MLP')\n\t@click.option('--model_gru_norm', default='NoNorm', type=str, help='Normalization type for the dynamics model\\'s GRU')\n\t@click.option('--activation_fn', default='relu', type=str, help='Activation function for non-CNN layers')\n\t@click.option('--cnn_activation_fn', default='relu', type=str, help='Activation function for CNN layers')\n\t@click.option('--gru_layers', default=1, type=int, help='Number of GRU layers in both the policy and the model')\n\t# Optimizer params\n\t@click.option('--optimizer', default='adam', type=str, help='Optimizer, adam or rmsprop')\n\t@click.option('--optim_eps', default=1e-5, type=float, help='Epsilon for optimizers')\n", "@click.option('--adam_beta1', default=0.9, type=float, help='Adam optimizer option')\n\t@click.option('--adam_beta2', default=0.999, type=float, help='Adam optimizer option')\n\t@click.option('--rmsprop_alpha', default=0.99, type=float, help='RMSProp optimizer option')\n\t@click.option('--rmsprop_momentum', default=0.0, type=float, help='RMSProp optimizer option')\n\t# Logging & Analysis options\n\t@click.option('--write_local_logs', default=1, type=int, help='Whether to output training logs locally')\n\t@click.option('--enable_plotting', default=0, type=int, help='Whether to generate plots for analysis')\n\t@click.option('--plot_interval', default=10, type=int, help='Interval of generating plots (iterations)')\n\t@click.option('--plot_colormap', default='Blues', type=str, help='Colormap of plots to generate')\n\t@click.option('--record_video', default=0, type=int, help='Whether to record video')\n", "@click.option('--rec_interval', default=10, type=int, help='Interval of two videos (iterations)')\n\t@click.option('--video_length', default=512, type=int, help='Length of the video (frames)')\n\t@click.option('--log_dsc_verbose', default=0, type=int, help='Whether to record the discriminator loss for each action')\n\t@click.option('--env_render', default=0, type=int, help='Whether to render games in human mode')\n\t@click.option('--use_status_predictor', default=0, type=int,\n\t              help='Whether to train status predictors for analysis (MiniGrid only)')\n\tdef main(\n\t    run_id, group_name, log_dir, total_steps, features_dim, model_features_dim, learning_rate, model_learning_rate,\n\t    num_processes, batch_size, n_steps, env_source, game_name, project_name, map_size, can_see_walls, fully_obs,\n\t    image_noise_scale, procgen_mode, procgen_num_threads, log_explored_states, fixed_seed, n_epochs, model_n_epochs,\n", "    gamma, gae_lambda, pg_coef, vf_coef, ent_coef, max_grad_norm, clip_range, clip_range_vf, adv_norm, adv_eps,\n\t    adv_momentum, ext_rew_coef, int_rew_coef, int_rew_source, int_rew_norm, int_rew_momentum, int_rew_eps, int_rew_clip,\n\t    dsc_obs_queue_len, icm_forward_loss_coef, ngu_knn_k, ngu_use_rnd, ngu_dst_momentum, rnd_use_policy_emb,\n\t    rnd_err_norm, rnd_err_momentum, use_model_rnn, latents_dim, model_latents_dim, policy_cnn_type, policy_mlp_layers,\n\t    policy_cnn_norm, policy_mlp_norm, policy_gru_norm, model_cnn_type, model_mlp_layers, model_cnn_norm, model_mlp_norm,\n\t    model_gru_norm, activation_fn, cnn_activation_fn, gru_layers, optimizer, optim_eps, adam_beta1, adam_beta2,\n\t    rmsprop_alpha, rmsprop_momentum, write_local_logs, enable_plotting, plot_interval, plot_colormap, record_video,\n\t    rec_interval, video_length, log_dsc_verbose, env_render, use_status_predictor\n\t):\n\t    set_random_seed(run_id, using_cuda=True)\n", "    args = locals().items()\n\t    config = TrainingConfig()\n\t    for k, v in args: setattr(config, k, v)\n\t    config.init_env_name(game_name, project_name)\n\t    config.init_meta_info()\n\t    config.init_logger()\n\t    config.init_values()\n\t    train(config)\n\t    config.close()\n\tif __name__ == '__main__':\n", "    main()\n"]}
{"filename": "src/env/minigrid_envs.py", "chunked_list": ["\"\"\"\n\tThis file is used to define and register custom MiniGrid environments.\n\tThe code was created based on the official implementation of gym-minigrid\n\t(version 1.0.3) and may differ from the latest version.\n\t\"\"\"\n\tfrom gym_minigrid.envs import DoorKeyEnv\n\tfrom gym_minigrid.minigrid import MiniGridEnv, Grid, Goal, Door, Key, Wall, COLOR_NAMES, DIR_TO_VEC, Ball, Box\n\tfrom gym_minigrid.register import register\n\tfrom gym_minigrid.roomgrid import RoomGrid\n\tclass CustomDoorKeyEnv(MiniGridEnv):\n", "    def __init__(self, size=8, agent_view_size=7, max_steps=None, disable_penalty=False):\n\t        super().__init__(\n\t            grid_size=size,\n\t            max_steps=10 * size * size if max_steps is None else max_steps,\n\t            agent_view_size=agent_view_size,\n\t        )\n\t        self.disable_penalty = disable_penalty\n\t    def _gen_grid(self, width, height):\n\t        # Create an empty grid\n\t        self.grid = Grid(width, height)\n", "        # Generate the surrounding walls\n\t        self.grid.wall_rect(0, 0, width, height)\n\t        # Place a goal in the bottom-right corner\n\t        self.put_obj(Goal(), width - 2, height - 2)\n\t        # Create a vertical splitting wall\n\t        splitIdx = self._rand_int(2, width - 2)\n\t        self.grid.vert_wall(splitIdx, 0)\n\t        # Place the agent at a random position and orientation\n\t        # on the left side of the splitting wall\n\t        self.place_agent(size=(splitIdx, height))\n", "        # Place a door in the wall\n\t        doorIdx = self._rand_int(1, width - 2)\n\t        self.put_obj(Door('yellow', is_locked=True), splitIdx, doorIdx)\n\t        # Place a yellow key on the left side\n\t        self.place_obj(\n\t            obj=Key('yellow'),\n\t            top=(0, 0),\n\t            size=(splitIdx, height)\n\t        )\n\t        self.mission = \"use the key to open the door and then get to the goal\"\n", "    def _reward(self):\n\t        if self.disable_penalty:\n\t            return 1\n\t        return 1 - 0.9 * (self.step_count / self.max_steps)\n\tclass DoorKeyEnv8x8ViewSize9x9(CustomDoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=8, agent_view_size=9)\n\tclass DoorKeyEnv8x8ViewSize5x5(CustomDoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=8, agent_view_size=5)\n", "class DoorKeyEnv8x8ViewSize3x3(CustomDoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=8, agent_view_size=3)\n\tclass DoorKeyEnv16x16ViewSize9x9(CustomDoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=16, agent_view_size=9)\n\tclass DoorKeyEnv16x16ViewSize5x5(CustomDoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=16, agent_view_size=5)\n\tclass DoorKeyEnv16x16ViewSize3x3(CustomDoorKeyEnv):\n", "    def __init__(self):\n\t        super().__init__(size=16, agent_view_size=3)\n\tclass DoorKeyEnv32x32ViewSize9x9(CustomDoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=32, agent_view_size=9)\n\tclass DoorKeyEnv32x32ViewSize5x5(CustomDoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=32, agent_view_size=5)\n\tclass DoorKeyEnv32x32ViewSize3x3(CustomDoorKeyEnv):\n\t    def __init__(self):\n", "        super().__init__(size=32, agent_view_size=3)\n\tclass DoorKeyEnv32x32(DoorKeyEnv):\n\t    def __init__(self):\n\t        super().__init__(size=32)\n\tregister(\n\t    id='MiniGrid-DoorKey-8x8-ViewSize-9x9-v0',\n\t    entry_point='src.env.minigrid_envs:DoorKeyEnv8x8ViewSize9x9'\n\t)\n\tregister(\n\t    id='MiniGrid-DoorKey-8x8-ViewSize-5x5-v0',\n", "    entry_point='src.env.minigrid_envs:DoorKeyEnv8x8ViewSize5x5'\n\t)\n\tregister(\n\t    id='MiniGrid-DoorKey-8x8-ViewSize-3x3-v0',\n\t    entry_point='src.env.minigrid_envs:DoorKeyEnv8x8ViewSize3x3'\n\t)\n\tregister(\n\t    id='MiniGrid-DoorKey-16x16-ViewSize-9x9-v0',\n\t    entry_point='src.env.minigrid_envs:DoorKeyEnv16x16ViewSize9x9'\n\t)\n", "register(\n\t    id='MiniGrid-DoorKey-16x16-ViewSize-5x5-v0',\n\t    entry_point='src.env.minigrid_envs:DoorKeyEnv16x16ViewSize5x5'\n\t)\n\tregister(\n\t    id='MiniGrid-DoorKey-32x32-v0',\n\t    entry_point='src.env.minigrid_envs:DoorKeyEnv32x32'\n\t)\n\tclass CustomKeyCorridor(RoomGrid):\n\t    def __init__(\n", "        self,\n\t        num_rows=3,\n\t        obj_type=\"ball\",\n\t        room_size=6,\n\t        seed=None,\n\t        agent_view_size=7,\n\t    ):\n\t        self.obj_type = obj_type\n\t        super().__init__(\n\t            room_size=room_size,\n", "            num_rows=num_rows,\n\t            max_steps=30*room_size**2,\n\t            seed=seed,\n\t            agent_view_size=agent_view_size,\n\t        )\n\t    def _gen_grid(self, width, height):\n\t        super()._gen_grid(width, height)\n\t        # Connect the middle column rooms into a hallway\n\t        for j in range(1, self.num_rows):\n\t            self.remove_wall(1, j, 3)\n", "        # Add a locked door on the bottom right\n\t        # Add an object behind the locked door\n\t        room_idx = self._rand_int(0, self.num_rows)\n\t        door, _ = self.add_door(2, room_idx, 2, locked=True)\n\t        obj, _ = self.add_object(2, room_idx, kind=self.obj_type)\n\t        # Add a key in a random room on the left side\n\t        self.add_object(0, self._rand_int(0, self.num_rows), 'key', door.color)\n\t        # Place the agent in the middle\n\t        self.place_agent(1, self.num_rows // 2)\n\t        # Make sure all rooms are accessible\n", "        self.connect_all()\n\t        self.obj = obj\n\t        self.mission = \"pick up the %s %s\" % (obj.color, obj.type)\n\t    def step(self, action):\n\t        obs, reward, done, info = super().step(action)\n\t        if action == self.actions.pickup:\n\t            if self.carrying and self.carrying == self.obj:\n\t                reward = self._reward()\n\t                done = True\n\t        return obs, reward, done, info\n", "class KeyCorridorS6R3V5(CustomKeyCorridor):\n\t    def __init__(self, seed=None):\n\t        super().__init__(\n\t            room_size=6,\n\t            num_rows=3,\n\t            seed=seed,\n\t            agent_view_size=5,\n\t        )\n\tclass KeyCorridorS6R3V3(CustomKeyCorridor):\n\t    def __init__(self, seed=None):\n", "        super().__init__(\n\t            room_size=6,\n\t            num_rows=3,\n\t            seed=seed,\n\t            agent_view_size=3,\n\t        )\n\tclass KeyCorridorS8R4(CustomKeyCorridor):\n\t    def __init__(self, seed=None):\n\t        super().__init__(\n\t            room_size=8,\n", "            num_rows=4,\n\t            seed=seed,\n\t            agent_view_size=7,\n\t        )\n\tclass KeyCorridorS10R5(CustomKeyCorridor):\n\t    def __init__(self, seed=None):\n\t        super().__init__(\n\t            room_size=10,\n\t            num_rows=5,\n\t            seed=seed,\n", "            agent_view_size=7,\n\t        )\n\tclass KeyCorridorS12R6(CustomKeyCorridor):\n\t    def __init__(self, seed=None):\n\t        super().__init__(\n\t            room_size=12,\n\t            num_rows=6,\n\t            seed=seed,\n\t            agent_view_size=7,\n\t        )\n", "register(\n\t    id='MiniGrid-KeyCorridorS6R3V5-v0',\n\t    entry_point='src.env.minigrid_envs:KeyCorridorS6R3V5'\n\t)\n\tregister(\n\t    id='MiniGrid-KeyCorridorS6R3V3-v0',\n\t    entry_point='src.env.minigrid_envs:KeyCorridorS6R3V3'\n\t)\n\tregister(\n\t    id='MiniGrid-KeyCorridorS8R4-v0',\n", "    entry_point='src.env.minigrid_envs:KeyCorridorS8R4'\n\t)\n\tregister(\n\t    id='MiniGrid-KeyCorridorS10R5-v0',\n\t    entry_point='src.env.minigrid_envs:KeyCorridorS10R5'\n\t)\n\tregister(\n\t    id='MiniGrid-KeyCorridorS12R6-v0',\n\t    entry_point='src.env.minigrid_envs:KeyCorridorS12R6'\n\t)\n", "class CustomFourRooms(MiniGridEnv):\n\t    def __init__(self, agent_pos=None, goal_pos=None, agent_view_size=5):\n\t        self._agent_default_pos = agent_pos\n\t        self._goal_default_pos = goal_pos\n\t        super().__init__(grid_size=19, max_steps=100, agent_view_size=agent_view_size)\n\t    def _gen_grid(self, width, height):\n\t        # Create the grid\n\t        self.grid = Grid(width, height)\n\t        # Generate the surrounding walls\n\t        self.grid.horz_wall(0, 0)\n", "        self.grid.horz_wall(0, height - 1)\n\t        self.grid.vert_wall(0, 0)\n\t        self.grid.vert_wall(width - 1, 0)\n\t        room_w = width // 2\n\t        room_h = height // 2\n\t        # For each row of rooms\n\t        for j in range(0, 2):\n\t            # For each column\n\t            for i in range(0, 2):\n\t                xL = i * room_w\n", "                yT = j * room_h\n\t                xR = xL + room_w\n\t                yB = yT + room_h\n\t                # Bottom wall and door\n\t                if i + 1 < 2:\n\t                    self.grid.vert_wall(xR, yT, room_h)\n\t                    pos = (xR, self._rand_int(yT + 1, yB))\n\t                    self.grid.set(*pos, None)\n\t                # Bottom wall and door\n\t                if j + 1 < 2:\n", "                    self.grid.horz_wall(xL, yB, room_w)\n\t                    pos = (self._rand_int(xL + 1, xR), yB)\n\t                    self.grid.set(*pos, None)\n\t        # Randomize the player start position and orientation\n\t        if self._agent_default_pos is not None:\n\t            self.agent_pos = self._agent_default_pos\n\t            self.grid.set(*self._agent_default_pos, None)\n\t            self.agent_dir = self._rand_int(0, 4)  # assuming random start direction\n\t        else:\n\t            self.place_agent()\n", "        if self._goal_default_pos is not None:\n\t            goal = Goal()\n\t            self.put_obj(goal, *self._goal_default_pos)\n\t            goal.init_pos, goal.cur_pos = self._goal_default_pos\n\t        else:\n\t            self.place_obj(Goal())\n\t        self.mission = 'Reach the goal'\n\t    def step(self, action):\n\t        obs, reward, done, info = MiniGridEnv.step(self, action)\n\t        return obs, reward, done, info\n", "class FourRoomsViewSize5x5(CustomFourRooms):\n\t    def __init__(self):\n\t        super().__init__(agent_view_size=5)\n\tclass FourRoomsViewSize3x3(CustomFourRooms):\n\t    def __init__(self):\n\t        super().__init__(agent_view_size=3)\n\tregister(\n\t    id=\"MiniGrid-FourRooms-ViewSize-5x5-v0\",\n\t    entry_point=\"src.env.minigrid_envs:FourRoomsViewSize5x5\"\n\t)\n", "register(\n\t    id=\"MiniGrid-FourRooms-ViewSize-3x3-v0\",\n\t    entry_point=\"src.env.minigrid_envs:FourRoomsViewSize3x3\"\n\t)\n\tclass MultiRoom:\n\t    def __init__(self,\n\t        top,\n\t        size,\n\t        entryDoorPos,\n\t        exitDoorPos\n", "    ):\n\t        self.top = top\n\t        self.size = size\n\t        self.entryDoorPos = entryDoorPos\n\t        self.exitDoorPos = exitDoorPos\n\tclass CustomMultiRoomEnv(MiniGridEnv):\n\t    def __init__(self,\n\t        minNumRooms,\n\t        maxNumRooms,\n\t        maxRoomSize=10,\n", "        agent_view_size=7,\n\t        grid_size=25,\n\t        max_steps=None,\n\t        disable_penalty=False,\n\t    ):\n\t        assert minNumRooms > 0\n\t        assert maxNumRooms >= minNumRooms\n\t        assert maxRoomSize >= 4\n\t        self.minNumRooms = minNumRooms\n\t        self.maxNumRooms = maxNumRooms\n", "        self.maxRoomSize = maxRoomSize\n\t        self.disable_penalty = disable_penalty\n\t        self.rooms = []\n\t        super(CustomMultiRoomEnv, self).__init__(\n\t            grid_size=grid_size,\n\t            max_steps=self.maxNumRooms * 20 if max_steps is None else max_steps,\n\t            agent_view_size=agent_view_size\n\t        )\n\t    def _gen_grid(self, width, height):\n\t        roomList = []\n", "        # Choose a random number of rooms to generate\n\t        numRooms = self._rand_int(self.minNumRooms, self.maxNumRooms+1)\n\t        while len(roomList) < numRooms:\n\t            curRoomList = []\n\t            entryDoorPos = (\n\t                self._rand_int(0, width - 2),\n\t                self._rand_int(0, width - 2)\n\t            )\n\t            # Recursively place the rooms\n\t            self._placeRoom(\n", "                numRooms,\n\t                roomList=curRoomList,\n\t                minSz=4,\n\t                maxSz=self.maxRoomSize,\n\t                entryDoorWall=2,\n\t                entryDoorPos=entryDoorPos\n\t            )\n\t            if len(curRoomList) > len(roomList):\n\t                roomList = curRoomList\n\t        # Store the list of rooms in this environment\n", "        assert len(roomList) > 0\n\t        self.rooms = roomList\n\t        # Create the grid\n\t        self.grid = Grid(width, height)\n\t        wall = Wall()\n\t        prevDoorColor = None\n\t        # For each room\n\t        for idx, room in enumerate(roomList):\n\t            topX, topY = room.top\n\t            sizeX, sizeY = room.size\n", "            # Draw the top and bottom walls\n\t            for i in range(0, sizeX):\n\t                self.grid.set(topX + i, topY, wall)\n\t                self.grid.set(topX + i, topY + sizeY - 1, wall)\n\t            # Draw the left and right walls\n\t            for j in range(0, sizeY):\n\t                self.grid.set(topX, topY + j, wall)\n\t                self.grid.set(topX + sizeX - 1, topY + j, wall)\n\t            # If this isn't the first room, place the entry door\n\t            if idx > 0:\n", "                # Pick a door color different from the previous one\n\t                doorColors = set(COLOR_NAMES)\n\t                if prevDoorColor:\n\t                    doorColors.remove(prevDoorColor)\n\t                # Note: the use of sorting here guarantees determinism,\n\t                # This is needed because Python's set is not deterministic\n\t                doorColor = self._rand_elem(sorted(doorColors))\n\t                entryDoor = Door(doorColor)\n\t                self.grid.set(*room.entryDoorPos, entryDoor)\n\t                prevDoorColor = doorColor\n", "                prevRoom = roomList[idx-1]\n\t                prevRoom.exitDoorPos = room.entryDoorPos\n\t        # Randomize the starting agent position and direction\n\t        self.place_agent(roomList[0].top, roomList[0].size)\n\t        # Place the final goal in the last room\n\t        self.goal_pos = self.place_obj(Goal(), roomList[-1].top, roomList[-1].size)\n\t        self.mission = 'traverse the rooms to get to the goal'\n\t    def _placeRoom(\n\t        self,\n\t        numLeft,\n", "        roomList,\n\t        minSz,\n\t        maxSz,\n\t        entryDoorWall,\n\t        entryDoorPos\n\t    ):\n\t        # Choose the room size randomly\n\t        sizeX = self._rand_int(minSz, maxSz+1)\n\t        sizeY = self._rand_int(minSz, maxSz+1)\n\t        # The first room will be at the door position\n", "        if len(roomList) == 0:\n\t            topX, topY = entryDoorPos\n\t        # Entry on the right\n\t        elif entryDoorWall == 0:\n\t            topX = entryDoorPos[0] - sizeX + 1\n\t            y = entryDoorPos[1]\n\t            topY = self._rand_int(y - sizeY + 2, y)\n\t        # Entry wall on the south\n\t        elif entryDoorWall == 1:\n\t            x = entryDoorPos[0]\n", "            topX = self._rand_int(x - sizeX + 2, x)\n\t            topY = entryDoorPos[1] - sizeY + 1\n\t        # Entry wall on the left\n\t        elif entryDoorWall == 2:\n\t            topX = entryDoorPos[0]\n\t            y = entryDoorPos[1]\n\t            topY = self._rand_int(y - sizeY + 2, y)\n\t        # Entry wall on the top\n\t        elif entryDoorWall == 3:\n\t            x = entryDoorPos[0]\n", "            topX = self._rand_int(x - sizeX + 2, x)\n\t            topY = entryDoorPos[1]\n\t        else:\n\t            assert False, entryDoorWall\n\t        # If the room is out of the grid, can't place a room here\n\t        if topX < 0 or topY < 0:\n\t            return False\n\t        if topX + sizeX > self.width or topY + sizeY >= self.height:\n\t            return False\n\t        # If the room intersects with previous rooms, can't place it here\n", "        for room in roomList[:-1]:\n\t            nonOverlap = \\\n\t                topX + sizeX < room.top[0] or \\\n\t                room.top[0] + room.size[0] <= topX or \\\n\t                topY + sizeY < room.top[1] or \\\n\t                room.top[1] + room.size[1] <= topY\n\t            if not nonOverlap:\n\t                return False\n\t        # Add this room to the list\n\t        roomList.append(MultiRoom(\n", "            (topX, topY),\n\t            (sizeX, sizeY),\n\t            entryDoorPos,\n\t            None\n\t        ))\n\t        # If this was the last room, stop\n\t        if numLeft == 1:\n\t            return True\n\t        # Try placing the next room\n\t        for i in range(0, 8):\n", "            # Pick which wall to place the out door on\n\t            wallSet = set((0, 1, 2, 3))\n\t            wallSet.remove(entryDoorWall)\n\t            exitDoorWall = self._rand_elem(sorted(wallSet))\n\t            nextEntryWall = (exitDoorWall + 2) % 4\n\t            # Pick the exit door position\n\t            # Exit on right wall\n\t            if exitDoorWall == 0:\n\t                exitDoorPos = (\n\t                    topX + sizeX - 1,\n", "                    topY + self._rand_int(1, sizeY - 1)\n\t                )\n\t            # Exit on south wall\n\t            elif exitDoorWall == 1:\n\t                exitDoorPos = (\n\t                    topX + self._rand_int(1, sizeX - 1),\n\t                    topY + sizeY - 1\n\t                )\n\t            # Exit on left wall\n\t            elif exitDoorWall == 2:\n", "                exitDoorPos = (\n\t                    topX,\n\t                    topY + self._rand_int(1, sizeY - 1)\n\t                )\n\t            # Exit on north wall\n\t            elif exitDoorWall == 3:\n\t                exitDoorPos = (\n\t                    topX + self._rand_int(1, sizeX - 1),\n\t                    topY\n\t                )\n", "            else:\n\t                assert False\n\t            # Recursively create the other rooms\n\t            success = self._placeRoom(\n\t                numLeft - 1,\n\t                roomList=roomList,\n\t                minSz=minSz,\n\t                maxSz=maxSz,\n\t                entryDoorWall=nextEntryWall,\n\t                entryDoorPos=exitDoorPos\n", "            )\n\t            if success:\n\t                break\n\t        return True\n\t    def _reward(self):\n\t        if self.disable_penalty:\n\t            return 1\n\t        return 1 - 0.9 * (self.step_count / self.max_steps)\n\tclass MultiRoomEnvN12(CustomMultiRoomEnv):\n\t    def __init__(self):\n", "        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=7,\n\t        )\n\tclass MultiRoomEnvN12V3(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n", "            agent_view_size=3,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps1k(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=7,\n\t            max_steps=1000,\n\t        )\n", "class MultiRoomEnvN12MaxSteps1kV3(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=3,\n\t            max_steps=1000,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps2k(CustomMultiRoomEnv):\n\t    def __init__(self):\n", "        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=7,\n\t            max_steps=2000,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps2kV3(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n", "            maxNumRooms=12,\n\t            agent_view_size=3,\n\t            max_steps=2000,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps3k(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=7,\n", "            max_steps=3000,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps3kV3(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=3,\n\t            max_steps=3000,\n\t        )\n", "class MultiRoomEnvN12MaxSteps500(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=7,\n\t            max_steps=500,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps500V3(CustomMultiRoomEnv):\n\t    def __init__(self):\n", "        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=3,\n\t            max_steps=500,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps600(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n", "            maxNumRooms=12,\n\t            agent_view_size=7,\n\t            max_steps=600,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps600V3(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=3,\n", "            max_steps=600,\n\t        )\n\tclass MultiRoomEnvN12MaxSteps800(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=7,\n\t            max_steps=800,\n\t        )\n", "class MultiRoomEnvN12MaxSteps800V3(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=12,\n\t            maxNumRooms=12,\n\t            agent_view_size=3,\n\t            max_steps=800,\n\t        )\n\tclass MultiRoomEnvN30MaxSteps1k(CustomMultiRoomEnv):\n\t    def __init__(self):\n", "        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=1000,\n\t        )\n\tclass MultiRoomEnvN30MaxSteps2k(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n", "            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=2000,\n\t        )\n\tclass MultiRoomEnvN30MaxSteps3k(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n", "            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=3000,\n\t        )\n\tclass MultiRoomEnvN30VS3MS1k(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n", "            grid_size=45,\n\t            agent_view_size=3,\n\t            max_steps=1000,\n\t        )\n\tclass MultiRoomEnvN30VS3MS2k(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n", "            agent_view_size=3,\n\t            max_steps=2000,\n\t        )\n\tclass MultiRoomEnvN30VS3MS3k(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=3,\n", "            max_steps=3000,\n\t        )\n\tclass MultiRoomEnvN30MS100NP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=100,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS300NP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=300,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS500NP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=500,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS1kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=1000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS2kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=2000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS3kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=3000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS4kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=4000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS5kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=5000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS6kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=6000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS7kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=7000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS8kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=8000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS9kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=9000,\n", "            disable_penalty=True,\n\t        )\n\tclass MultiRoomEnvN30MS10kNP(CustomMultiRoomEnv):\n\t    def __init__(self):\n\t        super().__init__(\n\t            minNumRooms=30,\n\t            maxNumRooms=30,\n\t            grid_size=45,\n\t            agent_view_size=7,\n\t            max_steps=10000,\n", "            disable_penalty=True,\n\t        )\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-ViewSize-3x3-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12V3'\n\t)\n", "register(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps1k-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps1k'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps1k-ViewSize-3x3-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps1kV3'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps2k-v0',\n", "    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps2k'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps2k-ViewSize-3x3-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps2kV3'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps3k-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps3k'\n\t)\n", "register(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps3k-ViewSize-3x3-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps3kV3'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps500-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps500'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps500-ViewSize-3x3-v0',\n", "    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps500V3'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps600-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps600'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps600-ViewSize-3x3-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps600V3'\n\t)\n", "register(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps800-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps800'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N12-MaxSteps800-ViewSize-3x3-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN12MaxSteps800V3'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps1k-v0',\n", "    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MaxSteps1k'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-ViewSize3x3-MaxSteps1k-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30VS3MS1k'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps2k-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MaxSteps2k'\n\t)\n", "register(\n\t    id='MiniGrid-MultiRoom-N30-ViewSize3x3-MaxSteps2k-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30VS3MS2k'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps3k-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MaxSteps3k'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-ViewSize3x3-MaxSteps3k-v0',\n", "    entry_point='src.env.minigrid_envs:MultiRoomEnvN30VS3MS3k'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps100-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS100NP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps300-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS300NP'\n\t)\n", "register(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps500-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS500NP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps1k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS1kNP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps2k-NoPenalty-v0',\n", "    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS2kNP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps3k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS3kNP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps4k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS4kNP'\n\t)\n", "register(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps5k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS5kNP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps6k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS6kNP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps7k-NoPenalty-v0',\n", "    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS7kNP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps8k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS8kNP'\n\t)\n\tregister(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps9k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS9kNP'\n\t)\n", "register(\n\t    id='MiniGrid-MultiRoom-N30-MaxSteps10k-NoPenalty-v0',\n\t    entry_point='src.env.minigrid_envs:MultiRoomEnvN30MS10kNP'\n\t)\n\tclass CustomObstructedMazeEnv(RoomGrid):\n\t    def __init__(self,\n\t                 num_rows,\n\t                 num_cols,\n\t                 num_rooms_visited,\n\t                 seed=None,\n", "                 agent_view_size=7,\n\t                 ):\n\t        room_size = 6\n\t        max_steps = 4 * num_rooms_visited * room_size ** 2\n\t        super().__init__(\n\t            room_size=room_size,\n\t            num_rows=num_rows,\n\t            num_cols=num_cols,\n\t            max_steps=max_steps,\n\t            seed=seed,\n", "            agent_view_size=agent_view_size,\n\t        )\n\t    def _gen_grid(self, width, height):\n\t        super()._gen_grid(width, height)\n\t        # Define all possible colors for doors\n\t        self.door_colors = self._rand_subset(COLOR_NAMES, len(COLOR_NAMES))\n\t        # Define the color of the ball to pick up\n\t        self.ball_to_find_color = COLOR_NAMES[0]\n\t        # Define the color of the balls that obstruct doors\n\t        self.blocking_ball_color = COLOR_NAMES[1]\n", "        # Define the color of boxes in which keys are hidden\n\t        self.box_color = COLOR_NAMES[2]\n\t        self.mission = \"pick up the %s ball\" % self.ball_to_find_color\n\t    def step(self, action):\n\t        obs, reward, done, info = super().step(action)\n\t        if action == self.actions.pickup:\n\t            if self.carrying and self.carrying == self.obj:\n\t                reward = self._reward()\n\t                done = True\n\t        return obs, reward, done, info\n", "    def add_door(self, i, j, door_idx=0, color=None, locked=False, key_in_box=False, blocked=False):\n\t        \"\"\"\n\t        Add a door. If the door must be locked, it also adds the key.\n\t        If the key must be hidden, it is put in a box. If the door must\n\t        be obstructed, it adds a ball in front of the door.\n\t        \"\"\"\n\t        door, door_pos = super().add_door(i, j, door_idx, color, locked=locked)\n\t        if blocked:\n\t            vec = DIR_TO_VEC[door_idx]\n\t            blocking_ball = Ball(self.blocking_ball_color) if blocked else None\n", "            self.grid.set(door_pos[0] - vec[0], door_pos[1] - vec[1], blocking_ball)\n\t        if locked:\n\t            obj = Key(door.color)\n\t            if key_in_box:\n\t                box = Box(self.box_color) if key_in_box else None\n\t                box.contains = obj\n\t                obj = box\n\t            self.place_in_room(i, j, obj)\n\t        return door, door_pos\n\tclass ObstructedMaze_Full_V3(CustomObstructedMazeEnv):\n", "    \"\"\"\n\t    A blue ball is hidden in one of the 4 corners of a 3x3 maze. Doors\n\t    are locked, doors are obstructed by a ball and keys are hidden in\n\t    boxes.\n\t    \"\"\"\n\t    def __init__(self, agent_room=(1, 1), key_in_box=True, blocked=True,\n\t                 num_quarters=4, num_rooms_visited=25, seed=None, agent_view_size=3):\n\t        self.agent_room = agent_room\n\t        self.key_in_box = key_in_box\n\t        self.blocked = blocked\n", "        self.num_quarters = num_quarters\n\t        super().__init__(\n\t            num_rows=3,\n\t            num_cols=3,\n\t            num_rooms_visited=num_rooms_visited,\n\t            seed=seed,\n\t            agent_view_size=agent_view_size,\n\t        )\n\t    def _gen_grid(self, width, height):\n\t        super()._gen_grid(width, height)\n", "        middle_room = (1, 1)\n\t        # Define positions of \"side rooms\" i.e. rooms that are neither\n\t        # corners nor the center.\n\t        side_rooms = [(2, 1), (1, 2), (0, 1), (1, 0)][:self.num_quarters]\n\t        for i in range(len(side_rooms)):\n\t            side_room = side_rooms[i]\n\t            # Add a door between the center room and the side room\n\t            self.add_door(*middle_room, door_idx=i, color=self.door_colors[i], locked=False)\n\t            for k in [-1, 1]:\n\t                # Add a door to each side of the side room\n", "                self.add_door(*side_room, locked=True,\n\t                              door_idx=(i+k)%4,\n\t                              color=self.door_colors[(i+k)%len(self.door_colors)],\n\t                              key_in_box=self.key_in_box,\n\t                              blocked=self.blocked)\n\t        corners = [(2, 0), (2, 2), (0, 2), (0, 0)][:self.num_quarters]\n\t        ball_room = self._rand_elem(corners)\n\t        self.obj, _ = self.add_object(*ball_room, \"ball\", color=self.ball_to_find_color)\n\t        self.place_agent(*self.agent_room)\n\tregister(\n", "    id=\"MiniGrid-ObstructedMaze-Full-V3-v0\",\n\t    entry_point=\"src.env.minigrid_envs:ObstructedMaze_Full_V3\"\n\t)"]}
{"filename": "src/env/subproc_vec_env.py", "chunked_list": ["import warnings\n\tfrom typing import Callable, List, Optional, Union, Sequence, Dict\n\timport gym\n\timport numpy as np\n\tfrom numpy import ndarray\n\tfrom stable_baselines3.common.vec_env import SubprocVecEnv, VecTransposeImage\n\tfrom stable_baselines3.common.vec_env.base_vec_env import tile_images, VecEnvStepReturn\n\tfrom stable_baselines3.common.vec_env.subproc_vec_env import _flatten_obs\n\tclass CustomSubprocVecEnv(SubprocVecEnv):\n\t    def __init__(self, \n", "                 env_fns: List[Callable[[], gym.Env]], \n\t                 start_method: Optional[str] = None):\n\t        super().__init__(env_fns, start_method)\n\t        self.can_see_walls = True\n\t        self.image_noise_scale = 0.0\n\t        self.image_rng = None  # to be initialized with run id in ppo_rollout.py\n\t    def set_seeds(self, seeds: List[int] = None) -> List[Union[None, int]]:\n\t        self.seeds = seeds\n\t        for idx, remote in enumerate(self.remotes):\n\t            remote.send((\"seed\", int(seeds[idx])))\n", "        return [remote.recv() for remote in self.remotes]\n\t    def get_seeds(self) -> List[Union[None, int]]:\n\t        return self.seeds\n\t    def send_reset(self, env_id: int) -> None:\n\t        self.remotes[env_id].send((\"reset\", None))\n\t    def invisibilize_obstacles(self, obs):\n\t        # Algorithm A5 in the Technical Appendix\n\t        # For MiniGrid envs only\n\t        obs = np.copy(obs)\n\t        for r in range(len(obs[0])):\n", "            for c in range(len(obs[0][r])):\n\t                # The color of Walls is grey\n\t                # See https://github.com/Farama-Foundation/gym-minigrid/blob/20384cfa59d7edb058e8dbd02e1e107afd1e245d/gym_minigrid/minigrid.py#L215-L223\n\t                # COLOR_TO_IDX['grey']: 5\n\t                if obs[1][r][c] == 5 and 0 <= obs[0][r][c] <= 2:\n\t                    obs[1][r][c] = 0\n\t                # OBJECT_TO_IDX[0,1,2]: 'unseen', 'empty', 'wall'\n\t                if 0 <= obs[0][r][c] <= 2:\n\t                    obs[0][r][c] = 0\n\t        return obs\n", "    def add_noise(self, obs):\n\t        # Algorithm A4 in the Technical Appendix\n\t        # Add noise to observations\n\t        obs = obs.astype(np.float64)\n\t        obs_noise = self.image_rng.normal(loc=0.0, scale=self.image_noise_scale, size=obs.shape)\n\t        return obs + obs_noise\n\t    def recv_obs(self, env_id: int) -> ndarray:\n\t        obs = VecTransposeImage.transpose_image(self.remotes[env_id].recv())\n\t        if not self.can_see_walls:\n\t            obs = self.invisibilize_obstacles(obs)\n", "        if self.image_noise_scale > 0:\n\t            obs = self.add_noise(obs)\n\t        return obs\n\t    def step_wait(self) -> VecEnvStepReturn:\n\t        results = [remote.recv() for remote in self.remotes]\n\t        self.waiting = False\n\t        obs_arr, rews, dones, infos = zip(*results)\n\t        obs_arr = _flatten_obs(obs_arr, self.observation_space).astype(np.float64)\n\t        for idx in range(len(obs_arr)):\n\t            if not self.can_see_walls:\n", "                obs_arr[idx] = self.invisibilize_obstacles(obs_arr[idx])\n\t            if self.image_noise_scale > 0:\n\t                obs_arr[idx] = self.add_noise(obs_arr[idx])\n\t        return obs_arr, np.stack(rews), np.stack(dones), infos\n\t    def get_first_image(self) -> Sequence[np.ndarray]:\n\t        for pipe in self.remotes[:1]:\n\t            # gather images from subprocesses\n\t            # `mode` will be taken into account later\n\t            pipe.send((\"render\", \"rgb_array\"))\n\t        imgs = [pipe.recv() for pipe in self.remotes[:1]]\n", "        return imgs\n\t    def render(self, mode: str = \"human\") -> Optional[np.ndarray]:\n\t        try:\n\t            # imgs = self.get_images()\n\t            imgs = self.get_first_image()\n\t        except NotImplementedError:\n\t            warnings.warn(f\"Render not defined for {self}\")\n\t            return\n\t        # Create a big image by tiling images from subprocesses\n\t        bigimg = tile_images(imgs[:1])\n", "        if mode == \"human\":\n\t            import cv2  # pytype:disable=import-error\n\t            cv2.imshow(\"vecenv\", bigimg[:, :, ::-1])\n\t            cv2.waitKey(1)\n\t        elif mode == \"rgb_array\":\n\t            return bigimg\n\t        else:\n\t            raise NotImplementedError(f\"Render mode {mode} is not supported by VecEnvs\")"]}
{"filename": "src/utils/configs.py", "chunked_list": ["import os\n\timport time\n\timport torch as th\n\timport wandb\n\tfrom torch import nn\n\tfrom gym_minigrid.wrappers import ImgObsWrapper, FullyObsWrapper, ReseedWrapper\n\tfrom procgen import ProcgenEnv\n\tfrom stable_baselines3.common.callbacks import CallbackList\n\tfrom stable_baselines3.common.env_util import make_vec_env\n\tfrom stable_baselines3.common.vec_env import VecMonitor\n", "from datetime import datetime\n\tfrom src.algo.common_models.cnns import BatchNormCnnFeaturesExtractor, LayerNormCnnFeaturesExtractor, \\\n\t    CnnFeaturesExtractor\n\tfrom src.env.subproc_vec_env import CustomSubprocVecEnv\n\tfrom src.utils.enum_types import EnvSrc, NormType, ModelType\n\tfrom wandb.integration.sb3 import WandbCallback\n\tfrom src.utils.loggers import LocalLogger\n\tfrom src.utils.video_recorder import VecVideoRecorder\n\tclass TrainingConfig():\n\t    def __init__(self):\n", "        self.dtype = th.float32\n\t        self.device = th.device('cuda' if th.cuda.is_available() else 'cpu')\n\t    def init_meta_info(self):\n\t        self.file_path = __file__\n\t        self.model_name = os.path.basename(__file__)\n\t        self.start_time = time.time()\n\t        self.start_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\t    def init_env_name(self, game_name, project_name):\n\t        env_name = game_name\n\t        self.env_source = EnvSrc.get_enum_env_src(self.env_source)\n", "        if self.env_source == EnvSrc.MiniGrid and not game_name.startswith('MiniGrid-'):\n\t            env_name = f'MiniGrid-{game_name}'\n\t            env_name += '-v0'\n\t        self.env_name = env_name\n\t        self.project_name = env_name if project_name is None else project_name\n\t    def init_logger(self):\n\t        if self.group_name is not None:\n\t            self.wandb_run = wandb.init(\n\t                name=f'run-id-{self.run_id}',\n\t                entity='abcde-project',  # your project name on wandb\n", "                project=self.project_name,\n\t                group=self.group_name,\n\t                settings=wandb.Settings(start_method=\"fork\"),\n\t                sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n\t                monitor_gym=True,  # auto-upload the videos of agents playing the game\n\t                save_code=True,  # optional\n\t            )\n\t            self.use_wandb = True\n\t        else:\n\t            self.use_wandb = False\n", "            self.wandb_run = None\n\t        self.log_dir = os.path.join(self.log_dir, self.env_name, self.start_datetime, str(self.run_id))\n\t        os.makedirs(self.log_dir, exist_ok=True)\n\t        if self.write_local_logs:\n\t            self.local_logger = LocalLogger(self.log_dir)\n\t            print(f'Writing local logs at {self.log_dir}')\n\t        else:\n\t            self.local_logger = None\n\t        print(f'Starting run {self.run_id}')\n\t    def init_values(self):\n", "        if self.clip_range_vf <= 0:\n\t            self.clip_range_vf = None\n\t    def close(self):\n\t        if self.wandb_run is not None:\n\t            self.wandb_run.finish()\n\t    def get_wrapper_class(self):\n\t        if self.env_source == EnvSrc.MiniGrid:\n\t            if self.fully_obs:\n\t                wrapper_class = lambda x: ImgObsWrapper(FullyObsWrapper(x))\n\t            else:\n", "                wrapper_class = lambda x: ImgObsWrapper(x)\n\t            if self.fixed_seed >= 0 and self.env_source == EnvSrc.MiniGrid:\n\t                assert not self.fully_obs\n\t                _seeds = [self.fixed_seed]\n\t                wrapper_class = lambda x: ImgObsWrapper(ReseedWrapper(x, seeds=_seeds))\n\t            return wrapper_class\n\t        return None\n\t    def get_venv(self, wrapper_class=None):\n\t        if self.env_source == EnvSrc.MiniGrid:\n\t            venv = make_vec_env(\n", "                self.env_name,\n\t                wrapper_class=wrapper_class,\n\t                vec_env_cls=CustomSubprocVecEnv,\n\t                n_envs=self.num_processes,\n\t                monitor_dir=self.log_dir,\n\t            )\n\t        elif self.env_source == EnvSrc.ProcGen:\n\t            venv = ProcgenEnv(\n\t                num_envs=self.num_processes,\n\t                env_name=self.env_name,\n", "                rand_seed=self.run_id,\n\t                num_threads=self.procgen_num_threads,\n\t                distribution_mode=self.procgen_mode,\n\t            )\n\t            venv = VecMonitor(venv=venv)\n\t        else:\n\t            raise NotImplementedError\n\t        if (self.record_video == 2) or \\\n\t                (self.record_video == 1 and self.run_id == 0):\n\t            _trigger = lambda x: x > 0 and x % (self.n_steps * self.rec_interval) == 0\n", "            venv = VecVideoRecorder(\n\t                venv,\n\t                os.path.join(self.log_dir, 'videos'),\n\t                record_video_trigger=_trigger,\n\t                video_length=self.video_length,\n\t            )\n\t        return venv\n\t    def get_callbacks(self):\n\t        if self.group_name is not None:\n\t            callbacks = CallbackList([\n", "                WandbCallback(\n\t                    gradient_save_freq=50,\n\t                    verbose=1,\n\t                )])\n\t        else:\n\t            callbacks = CallbackList([])\n\t        return callbacks\n\t    def get_optimizer(self):\n\t        if self.optimizer.lower() == 'adam':\n\t            optimizer_class = th.optim.Adam\n", "            optimizer_kwargs = dict(\n\t                eps=self.optim_eps,\n\t                betas=(self.adam_beta1, self.adam_beta2),\n\t            )\n\t        elif self.optimizer.lower() == 'rmsprop':\n\t            optimizer_class = th.optim.RMSprop\n\t            optimizer_kwargs = dict(\n\t                eps=self.optim_eps,\n\t                alpha=self.rmsprop_alpha,\n\t                momentum=self.rmsprop_momentum,\n", "            )\n\t        else:\n\t            raise NotImplementedError\n\t        return optimizer_class, optimizer_kwargs\n\t    def get_activation_fn(self):\n\t        if self.activation_fn.lower() == 'relu':\n\t            activation_fn = nn.ReLU\n\t        elif self.activation_fn.lower() == 'gelu':\n\t            activation_fn = nn.GELU\n\t        elif self.activation_fn.lower() == 'elu':\n", "            activation_fn = nn.ELU\n\t        else:\n\t            raise NotImplementedError\n\t        if self.cnn_activation_fn.lower() == 'relu':\n\t            cnn_activation_fn = nn.ReLU\n\t        elif self.cnn_activation_fn.lower() == 'gelu':\n\t            cnn_activation_fn = nn.GELU\n\t        elif self.cnn_activation_fn.lower() == 'elu':\n\t            cnn_activation_fn = nn.ELU\n\t        else:\n", "            raise NotImplementedError\n\t        return activation_fn, cnn_activation_fn\n\t    def cast_enum_values(self):\n\t        self.policy_cnn_norm = NormType.get_enum_norm_type(self.policy_cnn_norm)\n\t        self.policy_mlp_norm = NormType.get_enum_norm_type(self.policy_mlp_norm)\n\t        self.policy_gru_norm = NormType.get_enum_norm_type(self.policy_gru_norm)\n\t        self.model_cnn_norm = NormType.get_enum_norm_type(self.model_cnn_norm)\n\t        self.model_mlp_norm = NormType.get_enum_norm_type(self.model_mlp_norm)\n\t        self.model_gru_norm = NormType.get_enum_norm_type(self.model_gru_norm)\n\t        self.int_rew_source = ModelType.get_enum_model_type(self.int_rew_source)\n", "        if self.int_rew_source == ModelType.DEIR and not self.use_model_rnn:\n\t            print('\\nWARNING: Running DEIR without RNNs\\n')\n\t        if self.int_rew_source in [ModelType.DEIR, ModelType.PlainDiscriminator]:\n\t            assert self.n_steps * self.num_processes >= self.batch_size\n\t    def get_cnn_kwargs(self, cnn_activation_fn=nn.ReLU):\n\t        features_extractor_common_kwargs = dict(\n\t            features_dim=self.features_dim,\n\t            activation_fn=cnn_activation_fn,\n\t            model_type=self.policy_cnn_type,\n\t        )\n", "        model_features_extractor_common_kwargs = dict(\n\t            features_dim=self.model_features_dim,\n\t            activation_fn=cnn_activation_fn,\n\t            model_type=self.model_cnn_type,\n\t        )\n\t        if self.policy_cnn_norm == NormType.BatchNorm:\n\t            policy_features_extractor_class = BatchNormCnnFeaturesExtractor\n\t        elif self.policy_cnn_norm == NormType.LayerNorm:\n\t            policy_features_extractor_class = LayerNormCnnFeaturesExtractor\n\t        elif self.policy_cnn_norm == NormType.NoNorm:\n", "            policy_features_extractor_class = CnnFeaturesExtractor\n\t        else:\n\t            raise ValueError\n\t        if self.model_cnn_norm == NormType.BatchNorm:\n\t            model_cnn_features_extractor_class = BatchNormCnnFeaturesExtractor\n\t        elif self.model_cnn_norm == NormType.LayerNorm:\n\t            model_cnn_features_extractor_class = LayerNormCnnFeaturesExtractor\n\t        elif self.model_cnn_norm == NormType.NoNorm:\n\t            model_cnn_features_extractor_class = CnnFeaturesExtractor\n\t        else:\n", "            raise ValueError\n\t        return policy_features_extractor_class, \\\n\t            features_extractor_common_kwargs, \\\n\t            model_cnn_features_extractor_class, \\\n\t            model_features_extractor_common_kwargs\n"]}
{"filename": "src/utils/common_func.py", "chunked_list": ["import numpy as np\n\timport torch as th\n\tfrom torch import nn\n\tfrom typing import Optional, Callable\n\tfrom src.utils.enum_types import NormType, EnvSrc, ModelType\n\tdef bkdr_hash(inputs, seed=131, mask=0x7fffffff):\n\t    \"\"\"\n\t    A simple deterministic hashing algorithm by\n\t    Kernighan, Brian W., and Dennis M. Ritchie.\n\t    \"The C programming language.\" (2002).\n", "    \"\"\"\n\t    if isinstance(inputs, np.ndarray):\n\t        data = inputs.reshape(-1)\n\t    else:\n\t        data = inputs\n\t    res = 0\n\t    is_str = isinstance(data, str)\n\t    get_val = lambda x: ord(val) if is_str else val\n\t    for val in data:\n\t        int_val = get_val(val)\n", "        res = (res * seed) & mask\n\t        res = (res + int_val) & mask\n\t    return res\n\tdef normalize_rewards(norm_type, rewards, mean, std, eps=1e-5):\n\t    \"\"\"\n\t    Normalize the input rewards using a specified normalization method (norm_type).\n\t    [0] No normalization\n\t    [1] Standardization per mini-batch\n\t    [2] Standardization per rollout buffer\n\t    [3] Standardization without subtracting the average reward\n", "    \"\"\"\n\t    if norm_type <= 0:\n\t        return rewards\n\t    if norm_type == 1:\n\t        # Standardization\n\t        return (rewards - mean) / (std + eps)\n\t    if norm_type == 2:\n\t        # Min-max normalization\n\t        min_int_rew = np.min(rewards)\n\t        max_int_rew = np.max(rewards)\n", "        mean_int_rew = (max_int_rew + min_int_rew) / 2\n\t        return (rewards - mean_int_rew) / (max_int_rew - min_int_rew + eps)\n\t    if norm_type == 3:\n\t        # Standardization without subtracting the mean\n\t        return rewards / (std + eps)\n\tdef set_random_seed(self, seed: Optional[int] = None) -> None:\n\t    \"\"\"\n\t    From Stable Baslines 3.\n\t    Set the seed of the pseudo-random generators\n\t    (python, numpy, pytorch, gym, action_space)\n", "    \"\"\"\n\t    if seed is None:\n\t        return\n\t    set_random_seed(seed, using_cuda=self.device.type == th.device(\"cuda\").type)\n\t    self.action_space.seed(seed)\n\t    if self.env is not None:\n\t        seed_method = getattr(self.env, \"seed_method\", None)\n\t        if seed_method is not None:\n\t            self.env.seed(seed)\n\t    if self.eval_env is not None:\n", "        seed_method = getattr(self.eval_env, \"seed_method\", None)\n\t        if seed_method is not None:\n\t            self.eval_env.seed(seed)\n\tdef init_module_with_name(n: str, m: nn.Module, fn: Callable[['Module'], None] = None) -> nn.Module:\n\t    \"\"\"\n\t    Initialize the parameters of a neural network module using the hash of the module name\n\t    as a random seed. The purpose of this feature is to ensure that experimentally adding or\n\t    removing submodules does not affect the initialization of other modules.\n\t    \"\"\"\n\t    def _reset_parameters(module: nn.Module) -> None:\n", "        if hasattr(module, 'reset_parameters'):\n\t            module.reset_parameters()\n\t    has_child = False\n\t    for name, module in m.named_children():\n\t        has_child = True\n\t        init_module_with_name(n + '.' + name, module, fn)\n\t    if not has_child:\n\t        run_id = th.initial_seed()\n\t        hash_val = bkdr_hash(n)\n\t        hash_val = bkdr_hash([hash_val, run_id])\n", "        th.manual_seed(hash_val)\n\t        if fn is None:\n\t            _reset_parameters(m)\n\t        else:\n\t            fn(m)\n\t        th.manual_seed(run_id)\n\t    return m\n"]}
{"filename": "src/utils/running_mean_std.py", "chunked_list": ["import numpy as np\n\tclass RunningMeanStd(object):\n\t    \"\"\"\n\t    Implemented based on:\n\t    - https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Parallel_algorithm\n\t    - https://github.com/openai/random-network-distillation/blob/f75c0f1efa473d5109d487062fd8ed49ddce6634/mpi_util.py#L179-L214\n\t    - https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/running_mean_std.py\n\t    \"\"\"\n\t    def __init__(self, epsilon=1e-4, momentum=None, shape=()):\n\t        self.mean = np.zeros(shape, 'float64')\n", "        self.var = np.ones(shape, 'float64')\n\t        self.count = epsilon\n\t        self.eps = epsilon\n\t        self.momentum = momentum\n\t    def clear(self):\n\t        self.__init__(self.eps, self.momentum)\n\t    @staticmethod\n\t    def update_ema(old_data, new_data, momentum):\n\t        if old_data is None:\n\t            return new_data\n", "        return old_data * momentum + new_data * (1.0 - momentum)\n\t    def update(self, x):\n\t        batch_mean, batch_std, batch_count = np.mean(x, axis=0), np.std(x, axis=0), x.shape[0]\n\t        batch_var = np.square(batch_std)\n\t        if self.momentum is None or self.momentum < 0:\n\t            self.update_from_moments(batch_mean, batch_var, batch_count)\n\t        else:\n\t            self.mean = self.update_ema(self.mean, batch_mean, self.momentum)\n\t            new_var = np.mean(np.square(x - self.mean))\n\t            self.var = self.update_ema(self.var, new_var, self.momentum)\n", "            self.std = np.sqrt(self.var)\n\t    def update_from_moments(self, batch_mean, batch_var, batch_count):\n\t        delta = batch_mean - self.mean\n\t        tot_count = self.count + batch_count\n\t        new_mean = self.mean + delta * batch_count / tot_count\n\t        m_a = self.var * (self.count)\n\t        m_b = batch_var * (batch_count)\n\t        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / (self.count + batch_count)\n\t        new_var = M2 / (self.count + batch_count)\n\t        new_count = batch_count + self.count\n", "        self.mean = new_mean\n\t        self.var = new_var\n\t        self.std = np.sqrt(new_var)\n\t        self.count = new_count"]}
{"filename": "src/utils/loggers.py", "chunked_list": ["import os\n\timport numpy as np\n\tfrom typing import Any\n\tfrom torch import Tensor\n\tclass StatisticsLogger:\n\t    def __init__(self, mode: str = \"\"):\n\t        self.mode = mode\n\t        self.data = dict()\n\t    def _add(self, key: str, value: Any):\n\t        if value is None:\n", "            return\n\t        if isinstance(value, Tensor):\n\t            value = value.item()\n\t        if key not in self.data:\n\t            self.data[key] = list()\n\t        self.data[key].append(value)\n\t    def add(self, **kwargs):\n\t        for key, value in kwargs.items():\n\t            self._add(key, value)\n\t    def to_dict(self):\n", "        log_dict = dict()\n\t        for key, value in self.data.items():\n\t            log_dict.update({\n\t                f\"{self.mode}/{key}_mean\": np.mean(value)\n\t            })\n\t        return log_dict\n\tclass LocalLogger(object):\n\t    def __init__(self, path: str):\n\t        self.path = path\n\t        self.created = set()\n", "    def write(self, log_data: dict, log_type: str):\n\t        log_path = os.path.join(self.path, log_type + '.csv')\n\t        if log_type not in self.created:\n\t            log_file = open(log_path, 'w')\n\t            for col in log_data.keys():\n\t                log_file.write(col + ',')\n\t            log_file.write('\\n')\n\t            log_file.close()\n\t            self.created.add(log_type)\n\t        log_file = open(log_path, 'a')\n", "        for col in log_data.values():\n\t            log_file.write(f'{col},')\n\t        log_file.write('\\n')\n\t        log_file.close()\n"]}
{"filename": "src/utils/enum_types.py", "chunked_list": ["from enum import Enum\n\tfrom torch import nn\n\tclass ModelType(Enum):\n\t    NoModel = 0\n\t    ICM = 1  # Forward + Inverse\n\t    RND = 2\n\t    NGU = 3\n\t    NovelD = 4  # Inverse\n\t    DEIR = 5\n\t    PlainForward = 6\n", "    PlainInverse = 7\n\t    PlainDiscriminator = 8\n\t    @staticmethod\n\t    def get_enum_model_type(model_type):\n\t        if isinstance(model_type, ModelType):\n\t            return model_type\n\t        if isinstance(model_type, str):\n\t            model_type = model_type.strip().lower()\n\t            if model_type == \"icm\":\n\t                return ModelType.ICM\n", "            elif model_type == \"rnd\":\n\t                return ModelType.RND\n\t            elif model_type == \"ngu\":\n\t                return ModelType.NGU\n\t            elif model_type == \"noveld\":\n\t                return ModelType.NovelD\n\t            elif model_type == \"deir\":\n\t                return ModelType.DEIR\n\t            elif model_type == \"plainforward\":\n\t                return ModelType.PlainForward\n", "            elif model_type == \"plaininverse\":\n\t                return ModelType.PlainInverse\n\t            elif model_type == \"plaindiscriminator\":\n\t                return ModelType.PlainDiscriminator\n\t            else:\n\t                return ModelType.NoModel\n\t        raise ValueError\n\tclass NormType(Enum):\n\t    NoNorm = 0\n\t    BatchNorm = 1\n", "    LayerNorm = 2\n\t    @staticmethod\n\t    def get_enum_norm_type(norm_type):\n\t        if isinstance(norm_type, NormType):\n\t            return norm_type\n\t        if isinstance(norm_type, str):\n\t            norm_type = norm_type.strip().lower()\n\t            if norm_type == 'batchnorm':\n\t                return NormType.BatchNorm\n\t            if norm_type == 'layernorm':\n", "                return NormType.LayerNorm\n\t            if norm_type == 'nonorm':\n\t                return NormType.NoNorm\n\t        raise ValueError\n\t    @staticmethod\n\t    def get_norm_layer_1d(norm_type, fetures_dim, momentum=0.1):\n\t        norm_type = NormType.get_enum_norm_type(norm_type)\n\t        if norm_type == NormType.BatchNorm:\n\t            return nn.BatchNorm1d(fetures_dim, momentum=momentum)\n\t        if norm_type == NormType.LayerNorm:\n", "            return nn.LayerNorm(fetures_dim)\n\t        if norm_type == NormType.NoNorm:\n\t            return nn.Identity()\n\t        raise NotImplementedError\n\t    @staticmethod\n\t    def get_norm_layer_2d(norm_type, n_channels, n_size, momentum=0.1):\n\t        norm_type = NormType.get_enum_norm_type(norm_type)\n\t        if norm_type == NormType.BatchNorm:\n\t            return nn.BatchNorm2d(n_channels, momentum=momentum)\n\t        if norm_type == NormType.LayerNorm:\n", "            return nn.LayerNorm([n_channels, n_size, n_size])\n\t        if norm_type == NormType.NoNorm:\n\t            return nn.Identity()\n\t        raise NotImplementedError\n\tclass EnvSrc(Enum):\n\t    MiniGrid = 0\n\t    ProcGen = 1\n\t    @staticmethod\n\t    def get_enum_env_src(env_src):\n\t        if isinstance(env_src, EnvSrc):\n", "            return env_src\n\t        if isinstance(env_src, str):\n\t            env_src = env_src.strip().lower()\n\t            if env_src == 'minigrid':\n\t                return EnvSrc.MiniGrid\n\t            if env_src == 'procgen':\n\t                return EnvSrc.ProcGen\n\t        raise ValueError\n"]}
{"filename": "src/utils/video_recorder.py", "chunked_list": ["import os\n\tfrom typing import Callable\n\tfrom gym.wrappers.monitoring import video_recorder\n\tfrom stable_baselines3.common.vec_env.base_vec_env import VecEnv, VecEnvObs, VecEnvStepReturn, VecEnvWrapper\n\tfrom stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n\tfrom stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n\tclass VecVideoRecorder(VecEnvWrapper):\n\t    \"\"\"\n\t    Wraps a VecEnv or VecEnvWrapper object to record rendered image as mp4 video.\n\t    It requires ffmpeg or avconv to be installed on the machine.\n", "    :param venv:\n\t    :param video_folder: Where to save videos\n\t    :param record_video_trigger: Function that defines when to start recording.\n\t                                        The function takes the current number of step,\n\t                                        and returns whether we should start recording or not.\n\t    :param video_length:  Length of recorded videos\n\t    :param name_prefix: Prefix to the video name\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        venv: VecEnv,\n\t        video_folder: str,\n\t        record_video_trigger: Callable[[int], bool],\n\t        video_length: int = 200,\n\t        name_prefix: str = \"rl-video\",\n\t    ):\n\t        VecEnvWrapper.__init__(self, venv)\n\t        self.env = venv\n\t        # Temp variable to retrieve metadata\n\t        temp_env = venv\n", "        # Unwrap to retrieve metadata dict\n\t        # that will be used by gym recorder\n\t        while isinstance(temp_env, VecEnvWrapper):\n\t            temp_env = temp_env.venv\n\t        if self.env.metadata is None or \\\n\t                ('render.modes' not in self.env.metadata) or \\\n\t                len(self.env.metadata['render.modes']) == 0:\n\t            if isinstance(temp_env, DummyVecEnv) or isinstance(temp_env, SubprocVecEnv):\n\t                metadata = temp_env.get_attr(\"metadata\")[0]\n\t            else:\n", "                metadata = temp_env.metadata\n\t            self.env.metadata = metadata\n\t        self.record_video_trigger = record_video_trigger\n\t        self.video_recorder = None\n\t        self.video_folder = os.path.abspath(video_folder)\n\t        # Create output folder if needed\n\t        os.makedirs(self.video_folder, exist_ok=True)\n\t        self.name_prefix = name_prefix\n\t        self.step_id = 0\n\t        self.video_length = video_length\n", "        self.recording = False\n\t        self.recorded_frames = 0\n\t    def reset(self) -> VecEnvObs:\n\t        obs = self.venv.reset()\n\t        self.start_video_recorder()\n\t        return obs\n\t    def start_video_recorder(self) -> None:\n\t        self.close_video_recorder()\n\t        video_name = f\"{self.name_prefix}-step-{self.step_id}-to-step-{self.step_id + self.video_length}\"\n\t        base_path = os.path.join(self.video_folder, video_name)\n", "        self.video_recorder = video_recorder.VideoRecorder(\n\t            env=self.env, base_path=base_path, metadata={\"step_id\": self.step_id}\n\t        )\n\t        self.video_recorder.capture_frame()\n\t        self.recorded_frames = 1\n\t        self.recording = True\n\t    def _video_enabled(self) -> bool:\n\t        return self.record_video_trigger(self.step_id)\n\t    def step_wait(self) -> VecEnvStepReturn:\n\t        obs, rews, dones, infos = self.venv.step_wait()\n", "        self.step_id += 1\n\t        if self.recording:\n\t            self.video_recorder.capture_frame()\n\t            self.recorded_frames += 1\n\t            if self.recorded_frames > self.video_length:\n\t                print(f\"Saving video to {self.video_recorder.path}\")\n\t                self.close_video_recorder()\n\t        elif self._video_enabled():\n\t            self.start_video_recorder()\n\t        return obs, rews, dones, infos\n", "    def close_video_recorder(self) -> None:\n\t        if self.recording:\n\t            self.video_recorder.close()\n\t        self.recording = False\n\t        self.recorded_frames = 1\n\t    def close(self) -> None:\n\t        VecEnvWrapper.close(self)\n\t        self.close_video_recorder()\n\t    def __del__(self):\n\t        self.close()\n"]}
{"filename": "src/utils/env_utils.py", "chunked_list": ["import os\n\timport warnings\n\tfrom typing import Any, Callable, Dict, Optional, Type, Union, Sequence\n\timport multiprocessing as mp\n\timport envpool\n\timport gym\n\timport numpy as np\n\tfrom stable_baselines3.common.env_util import make_vec_env\n\tfrom stable_baselines3.common.monitor import Monitor\n\tfrom stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnv, VecEnvWrapper, VecMonitor\n", "from envpool.python.protocol import EnvPool\n\tfrom stable_baselines3.common.vec_env.base_vec_env import VecEnvObs, VecEnvStepReturn, tile_images\n\t# From Stable Baseline 3\n\t# https://github.com/DLR-RM/stable-baselines3/blob/18f4e3ace084a2fd3e0a3126613718945cf3e5b5/stable_baselines3/common/env_util.py\n\tfrom packaging import version\n\tis_legacy_gym = version.parse(gym.__version__) < version.parse(\"0.26.0\")\n\tclass EnvPoolVecAdapter(VecEnvWrapper):\n\t    \"\"\"\n\t    Convert EnvPool object to a Stable-Baselines3 (SB3) VecEnv.\n\t    :param venv: The envpool object.\n", "    \"\"\"\n\t    def __init__(self, venv: EnvPool):\n\t        # Retrieve the number of environments from the config\n\t        venv.num_envs = venv.spec.config.num_envs\n\t        super().__init__(venv=venv)\n\t        self.venv.obs = None\n\t    def step_async(self, actions: np.ndarray) -> None:\n\t        self.actions = actions\n\t    def reset(self) -> VecEnvObs:\n\t        if is_legacy_gym:\n", "            obs = self.venv.reset()\n\t        else:\n\t            obs = self.venv.reset()[0]\n\t        self.venv.obs = obs\n\t        return obs\n\t    def seed(self, seed: Optional[int] = None) -> None:\n\t        # You can only seed EnvPool env by calling envpool.make()\n\t        pass\n\t    def step_wait(self) -> VecEnvStepReturn:\n\t        if is_legacy_gym:\n", "            obs, rewards, dones, info_dict = self.venv.step(self.actions)\n\t        else:\n\t            obs, rewards, terms, truncs, info_dict = self.venv.step(self.actions)\n\t            dones = terms + truncs\n\t        infos = []\n\t        # Convert dict to list of dict\n\t        # and add terminal observation\n\t        for i in range(self.num_envs):\n\t            infos.append(\n\t                {\n", "                    key: info_dict[key][i]\n\t                    for key in info_dict.keys()\n\t                    if isinstance(info_dict[key], np.ndarray)\n\t                }\n\t            )\n\t            if dones[i]:\n\t                infos[i][\"terminal_observation\"] = obs[i]\n\t                if is_legacy_gym:\n\t                    obs[i] = self.venv.reset(np.array([i]))\n\t                else:\n", "                    obs[i] = self.venv.reset(np.array([i]))[0]\n\t        self.venv.obs = obs\n\t        return obs, rewards, dones, infos\n\t    def render(self, mode: str = \"human\") -> Optional[np.ndarray]:\n\t        if self.venv.obs is None:\n\t            return\n\t        try:\n\t            imgs = self.venv.obs\n\t        except NotImplementedError:\n\t            warnings.warn(f\"Render not defined for {self}\")\n", "            return\n\t        # Create a big image by tiling images from subprocesses\n\t        bigimg = tile_images(imgs[:1])\n\t        bigimg_size = bigimg.shape[-1]\n\t        bigimg = bigimg[-1].reshape(bigimg_size, bigimg_size)\n\t        # grayscale to fake-RGB\n\t        bigimg = np.stack((bigimg,) * 3, axis=-1)\n\t        if mode == \"human\":\n\t            import cv2  # pytype:disable=import-error\n\t            cv2.imshow(\"vecenv\", bigimg[:, :, ::-1])\n", "            cv2.waitKey(1)\n\t        elif mode == \"rgb_array\":\n\t            return bigimg\n\t        else:\n\t            raise NotImplementedError(f\"Render mode {mode} is not supported by VecEnvs\")\n"]}
{"filename": "src/algo/ppo_trainer.py", "chunked_list": ["import torch as th\n\timport wandb\n\timport warnings\n\tfrom gym import spaces\n\tfrom torch import Tensor\n\tfrom src.utils.loggers import StatisticsLogger, LocalLogger\n\tfrom src.algo.ppo_rollout import PPORollout\n\tfrom src.utils.enum_types import ModelType\n\tfrom stable_baselines3.common.base_class import BaseAlgorithm\n\tfrom stable_baselines3.common.policies import ActorCriticPolicy\n", "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n\tfrom stable_baselines3.common.utils import explained_variance, get_schedule_fn\n\tfrom torch.nn import functional as F\n\tfrom typing import Any, Dict, Optional, Type, Union\n\tclass PPOTrainer(PPORollout):\n\t    def __init__(\n\t        self,\n\t        policy: Union[str, Type[ActorCriticPolicy]],\n\t        env: Union[GymEnv, str],\n\t        run_id: int,\n", "        learning_rate: Union[float, Schedule] = 3e-4,\n\t        model_learning_rate: Union[float, Schedule] = 3e-4,\n\t        n_steps: int = 2048,\n\t        batch_size: Optional[int] = 64,\n\t        n_epochs: int = 4,\n\t        model_n_epochs: int = 4,\n\t        gamma: float = 0.99,\n\t        gae_lambda: float = 0.95,\n\t        clip_range: Union[float, Schedule] = 0.2,\n\t        clip_range_vf: Union[None, float, Schedule] = None,\n", "        ent_coef: float = 0.0,\n\t        pg_coef: float = 1.0,\n\t        vf_coef: float = 0.5,\n\t        max_grad_norm: float = 0.5,\n\t        use_sde: bool = False,\n\t        sde_sample_freq: int = -1,\n\t        target_kl: Optional[float] = None,\n\t        int_rew_source: ModelType = ModelType.DEIR,\n\t        int_rew_norm: int = 0,\n\t        int_rew_coef: float = 1e-3,\n", "        int_rew_momentum: float = 1,\n\t        int_rew_eps: float = 0.0,\n\t        adv_momentum: float = 0.0,\n\t        image_noise_scale: float = 0.0,\n\t        int_rew_clip: float = 0.0,\n\t        enable_plotting: int = 0,\n\t        can_see_walls: int = 1,\n\t        policy_kwargs: Optional[Dict[str, Any]] = None,\n\t        verbose: int = 0,\n\t        seed: Optional[int] = None,\n", "        device: Union[th.device, str] = \"auto\",\n\t        _init_setup_model: bool = True,\n\t        ext_rew_coef: float = 1.0,\n\t        adv_norm: int = 1,\n\t        adv_eps: float = 1e-8,\n\t        env_source: Optional[str] = None,\n\t        env_render: Optional[int] = None,\n\t        fixed_seed: Optional[int] = None,\n\t        plot_interval: int = 10,\n\t        plot_colormap: str = 'Blues',\n", "        log_explored_states: Optional[int] = None,\n\t        local_logger: Optional[LocalLogger] = None,\n\t        use_wandb: bool = False,\n\t    ):\n\t        super(PPOTrainer, self).__init__(\n\t            policy,\n\t            env,\n\t            run_id,\n\t            learning_rate=learning_rate,\n\t            n_steps=n_steps,\n", "            gamma=gamma,\n\t            gae_lambda=gae_lambda,\n\t            ent_coef=ent_coef,\n\t            pg_coef=pg_coef,\n\t            vf_coef=vf_coef,\n\t            int_rew_coef=int_rew_coef,\n\t            int_rew_norm=int_rew_norm,\n\t            int_rew_momentum=int_rew_momentum,\n\t            int_rew_eps=int_rew_eps,\n\t            int_rew_clip=int_rew_clip,\n", "            adv_momentum=adv_momentum,\n\t            image_noise_scale=image_noise_scale,\n\t            enable_plotting=enable_plotting,\n\t            can_see_walls=can_see_walls,\n\t            ext_rew_coef=ext_rew_coef,\n\t            adv_norm=adv_norm,\n\t            adv_eps=adv_eps,\n\t            max_grad_norm=max_grad_norm,\n\t            use_sde=use_sde,\n\t            sde_sample_freq=sde_sample_freq,\n", "            policy_kwargs=policy_kwargs,\n\t            verbose=verbose,\n\t            device=device,\n\t            seed=seed,\n\t            batch_size=batch_size,\n\t            int_rew_source=int_rew_source,\n\t            _init_setup_model=False,\n\t            env_source=env_source,\n\t            env_render=env_render,\n\t            fixed_seed=fixed_seed,\n", "            plot_interval=plot_interval,\n\t            plot_colormap=plot_colormap,\n\t            log_explored_states=log_explored_states,\n\t            local_logger=local_logger,\n\t            use_wandb=use_wandb,\n\t        )\n\t        # Sanity check, otherwise it will lead to noisy gradient and NaN\n\t        # because of the advantage normalization\n\t        assert (\n\t            batch_size > 1\n", "        ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n\t        if self.env is not None:\n\t            # Check that `n_steps * n_envs > 1` to avoid NaN\n\t            # when doing advantage normalization\n\t            buffer_size = self.env.num_envs * self.n_steps\n\t            assert (\n\t                buffer_size > 1\n\t            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n\t            # Check that the rollout buffer size is a multiple of the mini-batch size\n\t            untruncated_batches = buffer_size // batch_size\n", "            if buffer_size % batch_size > 0:\n\t                warnings.warn(\n\t                    f\"You have specified a mini-batch size of {batch_size},\"\n\t                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n\t                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n\t                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n\t                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n\t                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n\t                )\n\t        self.batch_size = batch_size\n", "        self.n_epochs = n_epochs\n\t        self.model_n_epochs = model_n_epochs\n\t        self.model_learning_rate = model_learning_rate\n\t        self.clip_range = clip_range\n\t        self.clip_range_vf = clip_range_vf\n\t        self.target_kl = target_kl\n\t        self.adv_norm = adv_norm\n\t        self.adv_eps = adv_eps\n\t        self.pg_loss_avg = None\n\t        self.ent_loss_avg = None\n", "        self.ent_coef_init = ent_coef\n\t        if _init_setup_model:\n\t            self._setup_model()\n\t    def _setup_model(self) -> None:\n\t        super(PPOTrainer, self)._setup_model()\n\t        # Initialize schedules for policy/value clipping\n\t        self.clip_range = get_schedule_fn(self.clip_range)\n\t        if self.clip_range_vf is not None:\n\t            if isinstance(self.clip_range_vf, (float, int)):\n\t                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive or `None`\"\n", "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n\t    def train_policy_and_models(self, clip_range, clip_range_vf) -> Tensor:\n\t        loss = None\n\t        continue_training = True\n\t        for epoch in range(max(self.n_epochs, self.model_n_epochs)):\n\t            for rollout_data in self.ppo_rollout_buffer.get(self.batch_size):\n\t                # Train intrinsic reward models\n\t                if epoch < self.model_n_epochs:\n\t                    if self.policy.int_rew_source != ModelType.NoModel:\n\t                        self.policy.int_rew_model.optimize(\n", "                            rollout_data=rollout_data,\n\t                            stats_logger=self.training_stats\n\t                        )\n\t                # Training for policy and value nets\n\t                if epoch < self.n_epochs:\n\t                    actions = rollout_data.actions\n\t                    if isinstance(self.action_space, spaces.Discrete):\n\t                        # Convert discrete action from float to long\n\t                        actions = rollout_data.actions.long().flatten()\n\t                    # Re-sample the noise matrix because the log_std has changed\n", "                    # if that line is commented (as in SAC)\n\t                    if self.use_sde:\n\t                        self.policy.reset_noise(self.batch_size)\n\t                    values, log_prob, entropy, memories = \\\n\t                        self.policy.evaluate_policy(\n\t                            rollout_data.observations,\n\t                            actions,\n\t                            rollout_data.last_policy_mems,\n\t                        )\n\t                    values = values.flatten()\n", "                    # Normalize advantage\n\t                    advantages = rollout_data.advantages\n\t                    # Normalize Advangages per mini-batch\n\t                    if self.adv_norm == 1:\n\t                        advantages = (advantages - advantages.mean()) / (advantages.std() + self.adv_eps)\n\t                    # ratio between old and new policy, should be one at the first iteration\n\t                    ratio = th.exp(log_prob - rollout_data.old_log_prob)\n\t                    # clipped surrogate loss\n\t                    policy_loss_1 = advantages * ratio\n\t                    policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n", "                    policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n\t                    # Logging\n\t                    clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n\t                    if self.clip_range_vf is None:\n\t                        # No clipping\n\t                        values_pred = values\n\t                    else:\n\t                        # Clip the different between old and new value\n\t                        # NOTE: this depends on the reward scaling\n\t                        values_pred = rollout_data.old_values + th.clamp(\n", "                            values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n\t                        )\n\t                    # Value loss using the TD(gae_lambda) target\n\t                    value_loss = F.mse_loss(rollout_data.returns, values_pred)\n\t                    # Entropy loss favor exploration\n\t                    if entropy is None:\n\t                        # Approximate entropy when no analytical form\n\t                        entropy_loss = -th.mean(-log_prob)\n\t                    else:\n\t                        entropy_loss = -th.mean(entropy)\n", "                    # Policy & Value Losses\n\t                    loss = self.pg_coef * policy_loss + \\\n\t                           self.ent_coef * entropy_loss + \\\n\t                           self.vf_coef * value_loss\n\t                    with th.no_grad():\n\t                        log_ratio = log_prob - rollout_data.old_log_prob\n\t                        approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n\t                    if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n\t                        continue_training = False\n\t                        if self.verbose >= 1:\n", "                            print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n\t                        break\n\t                    # Logging\n\t                    self.training_stats.add(\n\t                        policy_loss=policy_loss,\n\t                        value_loss=value_loss,\n\t                        entropy_loss=entropy_loss,\n\t                        adv=advantages.mean(),\n\t                        adv_std=advantages.std(),\n\t                        clip_fraction=clip_fraction,\n", "                        approx_kl_div=approx_kl_div,\n\t                    )\n\t                    # Optimization step\n\t                    self.policy.optimizer.zero_grad()\n\t                    loss.backward()\n\t                    th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n\t                    self.policy.optimizer.step()\n\t                    # END OF A TRAINING BATCH\n\t                    if not continue_training:\n\t                        break\n", "        return loss\n\t    def train(self) -> None:\n\t        # Update optimizer learning rate\n\t        self._update_learning_rate(self.policy.optimizer)\n\t        # Compute current clip range\n\t        clip_range = self.clip_range(self._current_progress_remaining)\n\t        # Optional: clip range for the value function\n\t        clip_range_vf = None\n\t        if self.clip_range_vf is not None:\n\t            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)\n", "        # Log training stats per each iteration\n\t        self.training_stats = StatisticsLogger(mode='train')\n\t        # Train PPO policy (+value function) and intrinsic reward models\n\t        ppo_loss = self.train_policy_and_models(clip_range, clip_range_vf)\n\t        # Update stats\n\t        self._n_updates += self.n_epochs\n\t        explained_var = explained_variance(self.ppo_rollout_buffer.values.flatten(), self.ppo_rollout_buffer.returns.flatten())\n\t        # Logging\n\t        log_data = {\n\t            \"time/total_timesteps\": self.num_timesteps,\n", "            \"train/loss\": ppo_loss.item(),\n\t            \"train/explained_variance\": explained_var,\n\t            \"train/n_updates\": self._n_updates,\n\t        }\n\t        if hasattr(self.policy, \"log_std\"):\n\t            log_data.update({\"train/std\": th.exp(self.policy.log_std).mean().item()})\n\t        # Update with other stats\n\t        log_data.update(self.training_stats.to_dict())\n\t        # Logging with wandb\n\t        if self.use_wandb:\n", "            wandb.log(log_data)\n\t        # Logging with local logger\n\t        if self.local_logger is not None:\n\t            self.local_logger.write(log_data, log_type='train')\n\t    def learn(\n\t        self,\n\t        total_timesteps: int,\n\t        callback: MaybeCallback = None,\n\t        log_interval: int = 1,\n\t        eval_env: Optional[GymEnv] = None,\n", "        eval_freq: int = -1,\n\t        n_eval_episodes: int = 5,\n\t        tb_log_name: str = \"CustomPPOAlgo\",\n\t        eval_log_path: Optional[str] = None,\n\t        reset_num_timesteps: bool = True,\n\t    ) -> BaseAlgorithm:\n\t        return super(PPOTrainer, self).learn(\n\t            total_timesteps=total_timesteps,\n\t            callback=callback,\n\t            log_interval=log_interval,\n", "            eval_env=eval_env,\n\t            eval_freq=eval_freq,\n\t            n_eval_episodes=n_eval_episodes,\n\t            tb_log_name=tb_log_name,\n\t            eval_log_path=eval_log_path,\n\t            reset_num_timesteps=reset_num_timesteps,\n\t        )\n"]}
{"filename": "src/algo/ppo_rollout.py", "chunked_list": ["import gym\n\timport numpy as np\n\timport time\n\timport torch as th\n\timport wandb\n\tfrom gym_minigrid.minigrid import Key, Door, Goal\n\tfrom matplotlib import pyplot as plt\n\tfrom src.algo.buffers.ppo_buffer import PPORolloutBuffer\n\tfrom src.utils.loggers import StatisticsLogger, LocalLogger\n\tfrom src.utils.common_func import set_random_seed\n", "from src.utils.enum_types import ModelType, EnvSrc\n\tfrom stable_baselines3.common.base_class import BaseAlgorithm\n\tfrom stable_baselines3.common.callbacks import BaseCallback\n\tfrom stable_baselines3.common.policies import ActorCriticPolicy, BasePolicy\n\tfrom stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n\tfrom stable_baselines3.common.utils import obs_as_tensor, safe_mean\n\tfrom stable_baselines3.common.vec_env import VecEnv\n\tfrom typing import Any, Dict, Optional, Tuple, Type, Union\n\tclass PPORollout(BaseAlgorithm):\n\t    def __init__(\n", "        self,\n\t        policy: Union[str, Type[ActorCriticPolicy]],\n\t        env: Union[GymEnv, str],\n\t        run_id: int,\n\t        learning_rate: Union[float, Schedule],\n\t        n_steps: int,\n\t        batch_size: int,\n\t        gamma: float,\n\t        gae_lambda: float,\n\t        ent_coef: float,\n", "        pg_coef: float,\n\t        vf_coef: float,\n\t        int_rew_source: ModelType,\n\t        int_rew_coef: float,\n\t        int_rew_norm : int,\n\t        int_rew_momentum: Optional[float],\n\t        int_rew_eps : float,\n\t        int_rew_clip : float,\n\t        adv_momentum : float,\n\t        image_noise_scale : float,\n", "        enable_plotting : int,\n\t        can_see_walls : int,\n\t        ext_rew_coef: float,\n\t        adv_norm: int,\n\t        adv_eps: float,\n\t        max_grad_norm: float,\n\t        use_sde: bool,\n\t        sde_sample_freq: int,\n\t        policy_base: Type[BasePolicy] = ActorCriticPolicy,\n\t        policy_kwargs: Optional[Dict[str, Any]] = None,\n", "        verbose: int = 0,\n\t        seed: Optional[int] = None,\n\t        device: Union[th.device, str] = \"auto\",\n\t        _init_setup_model: bool = True,\n\t        env_source: Optional[EnvSrc] = None,\n\t        env_render: Optional[int] = None,\n\t        fixed_seed: Optional[int] = None,\n\t        plot_interval: int = 10,\n\t        plot_colormap: str = \"Blues\",\n\t        log_explored_states: Optional[int] = None,\n", "        local_logger: Optional[LocalLogger] = None,\n\t        use_wandb: bool = False,\n\t    ):\n\t        super(PPORollout, self).__init__(\n\t            policy=policy,\n\t            env=env,\n\t            policy_base=policy_base,\n\t            learning_rate=learning_rate,\n\t            policy_kwargs=policy_kwargs,\n\t            verbose=verbose,\n", "            device=device,\n\t            use_sde=use_sde,\n\t            sde_sample_freq=sde_sample_freq,\n\t            support_multi_env=True,\n\t            seed=seed,\n\t        )\n\t        self.run_id = run_id\n\t        self.n_steps = n_steps\n\t        self.batch_size = batch_size\n\t        self.gamma = gamma\n", "        self.gae_lambda = gae_lambda\n\t        self.ent_coef = ent_coef\n\t        self.pg_coef = pg_coef\n\t        self.vf_coef = vf_coef\n\t        self.max_grad_norm = max_grad_norm\n\t        self.num_timesteps = 0\n\t        self.int_rew_source = int_rew_source\n\t        self.int_rew_coef = int_rew_coef\n\t        self.int_rew_norm = int_rew_norm\n\t        self.int_rew_eps = int_rew_eps\n", "        self.adv_momentum = adv_momentum\n\t        self.int_rew_clip = int_rew_clip\n\t        self.image_noise_scale = image_noise_scale\n\t        self.enable_plotting = enable_plotting\n\t        self.can_see_walls = can_see_walls\n\t        self.int_rew_momentum = int_rew_momentum\n\t        self.ext_rew_coef = ext_rew_coef\n\t        self.adv_norm = adv_norm\n\t        self.adv_eps = adv_eps\n\t        self.env_source = env_source\n", "        self.env_render = env_render\n\t        self.fixed_seed = fixed_seed\n\t        self.plot_interval = plot_interval\n\t        self.plot_colormap = plot_colormap\n\t        self.log_explored_states = log_explored_states\n\t        self.local_logger = local_logger\n\t        self.use_wandb = use_wandb\n\t        if _init_setup_model:\n\t            self._setup_model()\n\t    def _setup_model(self) -> None:\n", "        self._setup_lr_schedule()\n\t        set_random_seed(self.seed)\n\t        self.policy = self.policy_class(  # pytype:disable=not-instantiable\n\t            self.observation_space,\n\t            self.action_space,\n\t            self.lr_schedule,\n\t            use_sde=self.use_sde,\n\t            **self.policy_kwargs  # pytype:disable=not-instantiable\n\t        )\n\t        self.policy = self.policy.to(self.device)\n", "        self.ppo_rollout_buffer = PPORolloutBuffer(\n\t            self.n_steps,\n\t            self.observation_space,\n\t            self.action_space,\n\t            self.device,\n\t            gamma=self.gamma,\n\t            gae_lambda=self.gae_lambda,\n\t            n_envs=self.n_envs,\n\t            features_dim=self.policy.features_dim,\n\t            dim_policy_traj=self.policy.dim_policy_features,\n", "            dim_model_traj=self.policy.dim_model_features,\n\t            int_rew_coef=self.int_rew_coef,\n\t            ext_rew_coef=self.ext_rew_coef,\n\t            int_rew_norm=self.int_rew_norm,\n\t            int_rew_clip=self.int_rew_clip,\n\t            int_rew_eps=self.int_rew_eps,\n\t            adv_momentum=self.adv_momentum,\n\t            adv_norm=self.adv_norm,\n\t            adv_eps=self.adv_eps,\n\t            gru_layers=self.policy.gru_layers,\n", "            int_rew_momentum=self.int_rew_momentum,\n\t            use_status_predictor=self.policy.use_status_predictor,\n\t        )\n\t    def on_training_start(self):\n\t        if isinstance(self._last_obs, Dict):\n\t            self._last_obs = self._last_obs[\"rgb\"]\n\t        if self.env_source == EnvSrc.MiniGrid:\n\t            # Set advanced options for MiniGrid envs\n\t            self.env.can_see_walls = self.can_see_walls\n\t            self.env.image_noise_scale = self.image_noise_scale\n", "            self.env.image_rng = np.random.default_rng(seed=self.run_id + 1313)\n\t            # Initialize seeds for each MiniGrid env\n\t            np.random.seed(self.run_id)\n\t            seeds = np.random.rand(self.n_envs)\n\t            seeds = [int(s * 0x7fffffff) for s in seeds]\n\t            np.random.seed(self.run_id)\n\t            self.env.set_seeds(seeds)\n\t            self.env.waiting = True\n\t            for i in range(self.n_envs):\n\t                self.env.send_reset(env_id=i)\n", "            for i in range(self.n_envs):\n\t                self._last_obs[i] = self.env.recv_obs(env_id=i)\n\t            self.env.waiting = False\n\t            # Init variables for logging\n\t            self.width = self.env.get_attr('width')[0]\n\t            self.height = self.env.get_attr('height')[0]\n\t            self.global_visit_counts = np.zeros([self.width, self.height], dtype=np.int32)\n\t            self.global_reward_map_maxs = np.zeros([self.width, self.height], dtype=np.float64)\n\t            self.global_reward_map_sums = np.zeros([self.width, self.height], dtype=np.float64)\n\t            self.global_reward_map_nums = np.zeros([self.width, self.height], dtype=np.int32)\n", "            self.global_value_map_sums = np.zeros([self.width, self.height], dtype=np.float64)\n\t            self.global_value_map_nums = np.zeros([self.width, self.height], dtype=np.int32)\n\t        self.global_episode_rewards = np.zeros(self.n_envs, dtype=np.float32)\n\t        self.global_episode_intrinsic_rewards = np.zeros(self.n_envs, dtype=np.float32)\n\t        self.global_episode_unique_states = np.zeros(self.n_envs, dtype=np.float32)\n\t        self.global_episode_visited_states = [dict() for _ in range(self.n_envs)]\n\t        self.global_lifelong_unique_states = 0\n\t        self.global_lifelong_visited_states = dict()\n\t        self.global_episode_visited_positions = [dict() for _ in range(self.n_envs)]\n\t        self.global_episode_visited_pos_sum = np.zeros(self.n_envs, dtype=np.float32)\n", "        self.global_episode_steps = np.zeros(self.n_envs, dtype=np.int32)\n\t        if self.policy.use_status_predictor:\n\t            self.global_has_keys = np.zeros(self.n_envs, dtype=np.int32)\n\t            self.global_open_doors = np.zeros(self.n_envs, dtype=np.int32)\n\t            self.curr_key_status = np.zeros(self.n_envs, dtype=np.float32)\n\t            self.curr_door_status = np.zeros(self.n_envs, dtype=np.float32)\n\t            self.curr_target_dists = np.zeros((self.n_envs, 3), dtype=np.float32)\n\t        else:\n\t            self.global_has_keys = None\n\t            self.global_open_doors = None\n", "            self.curr_key_status = None\n\t            self.curr_door_status = None\n\t            self.curr_target_dists = None\n\t        self.episodic_obs_emb_history = [None for _ in range(self.n_envs)]\n\t        self.episodic_trj_emb_history = [None for _ in range(self.n_envs)]\n\t        if self.int_rew_source in [ModelType.DEIR, ModelType.PlainDiscriminator]:\n\t            self.policy.int_rew_model.init_obs_queue(self._last_obs)\n\t        def float_zeros(tensor_shape):\n\t            return th.zeros(tensor_shape, device=self.device, dtype=th.float32)\n\t        self._last_policy_mems = float_zeros([self.n_envs, self.policy.gru_layers, self.policy.dim_policy_features])\n", "        self._last_model_mems = float_zeros([self.n_envs, self.policy.gru_layers, self.policy.dim_model_features])\n\t    def init_on_rollout_start(self):\n\t        # Log statistics data per each rollout\n\t        self.rollout_stats = StatisticsLogger(mode='rollout')\n\t        self.rollout_done_episodes = 0\n\t        self.rollout_done_episode_steps = 0\n\t        self.rollout_sum_rewards = 0\n\t        self.rollout_episode_unique_states = 0\n\t        self.rollout_done_episode_unique_states = 0\n\t    def log_before_transition(self, values):\n", "        if self.env_source == EnvSrc.MiniGrid:\n\t            self._last_state_hash_vals = self.env.env_method('hash')\n\t        # Update Key and Door Status\n\t        agent_positions = None\n\t        if self.policy.use_status_predictor:\n\t            agent_positions = np.array(self.env.get_attr('agent_pos'))\n\t            agent_carryings = self.env.get_attr('carrying')\n\t            env_grids = self.env.get_attr('grid')\n\t            self.curr_door_pos = np.zeros((self.n_envs, 2), dtype=np.int32)\n\t            self.curr_key_pos = np.copy(agent_positions).reshape(self.n_envs, 2)\n", "            self.curr_goal_pos = np.zeros((self.n_envs, 2), dtype=np.int32)\n\t            for env_id in range(self.n_envs):\n\t                # The only possible carrying in DoorKey is the key\n\t                self.global_has_keys[env_id] = int(isinstance(agent_carryings[env_id], Key))\n\t                # Door, Key, Goal positions\n\t                for env_id, grid in enumerate(env_grids[env_id].grid):\n\t                    col = env_id % self.width\n\t                    row = env_id // self.width\n\t                    if isinstance(grid, Door):\n\t                        self.curr_door_pos[env_id] = np.array((col, row))\n", "                        self.global_open_doors[env_id] = int(grid.is_open)\n\t                    elif isinstance(grid, Key):\n\t                        self.curr_key_pos[env_id] = np.array((col, row))\n\t                    elif isinstance(grid, Goal):\n\t                        self.curr_goal_pos[env_id] = np.array((col, row))\n\t            self.curr_key_status = np.copy(self.global_has_keys)\n\t            self.curr_door_status = np.copy(self.global_open_doors)\n\t            self.rollout_stats.add(\n\t                key_status=np.mean(self.global_has_keys),\n\t                door_status=np.mean(self.global_open_doors),\n", "            )\n\t        # Update agent position and visit count\n\t        if self.policy.use_status_predictor or self.enable_plotting:\n\t            if agent_positions is None:\n\t                agent_positions = np.array(self.env.get_attr('agent_pos'))\n\t            for i in range(self.n_envs):\n\t                c, r = agent_positions[i]\n\t                self.global_visit_counts[r, c] += 1\n\t                self.global_value_map_sums[r, c] += values[i].item()\n\t                self.global_value_map_nums[r, c] += 1\n", "            # Current agent position\n\t            self.curr_agent_pos = np.copy(agent_positions)\n\t            # Define the target of position prediction loss\n\t            if self.policy.use_status_predictor:\n\t                # Manhattan Distance to the Door\n\t                key_dists = np.abs(self.curr_agent_pos - self.curr_key_pos)\n\t                key_dists = np.sum(key_dists, axis=1) / (self.width + self.height)\n\t                door_dists = np.abs(self.curr_agent_pos - self.curr_door_pos)\n\t                door_dists = np.sum(door_dists, axis=1) / (self.width + self.height)\n\t                goal_dists = np.abs(self.curr_agent_pos - self.curr_goal_pos)\n", "                goal_dists = np.sum(goal_dists, axis=1) / (self.width + self.height)\n\t                self.curr_target_dists = np.stack([key_dists, door_dists, goal_dists], axis=1)\n\t    def log_after_transition(self, rewards, intrinsic_rewards):\n\t        self.global_episode_rewards += rewards\n\t        self.global_episode_intrinsic_rewards += intrinsic_rewards\n\t        self.global_episode_steps += 1\n\t        # Logging episodic/lifelong visited states, reward map\n\t        if self.log_explored_states:\n\t            # 0 - Not to log\n\t            # 1 - Log both episodic and lifelong states\n", "            # 2 - Log episodic visited states only\n\t            if self.env_source == EnvSrc.MiniGrid:\n\t                agent_positions = np.array(self.env.get_attr('agent_pos'))\n\t                for env_id in range(self.n_envs):\n\t                    c, r = agent_positions[env_id]\n\t                    # count the visited positions\n\t                    pos = c * self.width + r\n\t                    pos_visit_count = self.global_episode_visited_positions[env_id]\n\t                    if pos not in pos_visit_count:\n\t                        pos_visit_count[pos] = 1\n", "                        self.global_episode_visited_pos_sum[env_id] += 1\n\t                    else:\n\t                        pos_visit_count[pos] += 1\n\t                    env_hash = self._last_state_hash_vals[env_id]\n\t                    if env_hash in self.global_episode_visited_states[env_id]:\n\t                        self.global_episode_visited_states[env_id][env_hash] += 1\n\t                    else:\n\t                        self.global_episode_visited_states[env_id][env_hash] = 1\n\t                        self.global_episode_unique_states[env_id] += 1\n\t                        self.rollout_episode_unique_states += 1\n", "                    if self.log_explored_states == 1:\n\t                        if env_hash in self.global_lifelong_visited_states:\n\t                            self.global_lifelong_visited_states[env_hash] += 1\n\t                        else:\n\t                            self.global_lifelong_visited_states[env_hash] = 1\n\t                            self.global_lifelong_unique_states += 1\n\t                    if self.enable_plotting:\n\t                        self.global_reward_map_maxs[r, c] = np.maximum(\n\t                            self.global_reward_map_maxs[r, c],\n\t                            intrinsic_rewards[env_id]\n", "                        )\n\t                        self.global_reward_map_sums[r, c] += intrinsic_rewards[env_id]\n\t                        self.global_reward_map_nums[r, c] += 1\n\t            elif self.env_source == EnvSrc.ProcGen:\n\t                for env_id in range(self.n_envs):\n\t                    # In Procgen games, the counted \"states\" are observations\n\t                    env_hash = tuple(self._last_obs[env_id].reshape(-1).tolist())\n\t                    if env_hash in self.global_episode_visited_states[env_id]:\n\t                        self.global_episode_visited_states[env_id][env_hash] += 1\n\t                    else:\n", "                        self.global_episode_visited_states[env_id][env_hash] = 1\n\t                        self.global_episode_unique_states[env_id] += 1\n\t                        self.rollout_episode_unique_states += 1\n\t                    if self.log_explored_states == 1:\n\t                        if env_hash in self.global_lifelong_visited_states:\n\t                            self.global_lifelong_visited_states[env_hash] += 1\n\t                        else:\n\t                            self.global_lifelong_visited_states[env_hash] = 1\n\t                            self.global_lifelong_unique_states += 1\n\t    def clear_on_episode_end(self, dones, policy_mems, model_mems):\n", "        for env_id in range(self.n_envs):\n\t            if dones[env_id]:\n\t                if policy_mems is not None: policy_mems[env_id] *= 0.0\n\t                if model_mems is not None: model_mems[env_id] *= 0.0\n\t                self.episodic_obs_emb_history[env_id] = None\n\t                self.episodic_trj_emb_history[env_id] = None\n\t                self.rollout_sum_rewards += self.global_episode_rewards[env_id]\n\t                self.rollout_done_episode_steps += self.global_episode_steps[env_id]\n\t                self.rollout_done_episode_unique_states += self.global_episode_unique_states[env_id]\n\t                self.rollout_done_episodes += 1\n", "                self.global_episode_rewards[env_id] = 0\n\t                self.global_episode_intrinsic_rewards[env_id] = 0\n\t                self.global_episode_unique_states[env_id] = 0\n\t                self.global_episode_visited_states[env_id] = dict()  # logging use\n\t                self.global_episode_visited_positions[env_id] = dict()  # logging use\n\t                self.global_episode_visited_pos_sum[env_id] = 0  # logging use\n\t                self.global_episode_steps[env_id] = 0\n\t                if self.policy.use_status_predictor:\n\t                    self.global_has_keys[env_id] = 0\n\t                    self.global_open_doors[env_id] = 0\n", "                    self.curr_key_status[env_id] = 0\n\t                    self.curr_door_status[env_id] = 0\n\t    def log_on_rollout_end(self, log_interval):\n\t        if log_interval is not None and self.iteration % log_interval == 0:\n\t            log_data = {\n\t                \"iterations\": self.iteration,\n\t                \"time/fps\": int(self.num_timesteps / (time.time() - self.start_time)),\n\t                \"time/time_elapsed\": int(time.time() - self.start_time),\n\t                \"time/total_timesteps\": self.num_timesteps,\n\t                \"rollout/ep_rew_mean\": self.rollout_sum_rewards / (self.rollout_done_episodes + 1e-8),\n", "                \"rollout/ep_len_mean\": self.rollout_done_episode_steps / (self.rollout_done_episodes + 1e-8),\n\t                # unique states / positions\n\t                \"rollout/ep_unique_states\": self.rollout_done_episode_unique_states / (\n\t                            self.rollout_done_episodes + 1e-8),\n\t                \"rollout/ll_unique_states\": self.global_lifelong_unique_states,\n\t                \"rollout/ep_unique_states_per_step\": self.rollout_episode_unique_states / (\n\t                            self.ppo_rollout_buffer.buffer_size * self.n_envs),\n\t                \"rollout/ll_unique_states_per_step\": self.global_lifelong_unique_states / self.num_timesteps,\n\t                # intrinsic rewards\n\t                \"rollout/int_rew_coef\": self.ppo_rollout_buffer.int_rew_coef,\n", "                \"rollout/int_rew_buffer_mean\": self.ppo_rollout_buffer.int_rew_mean,\n\t                \"rollout/int_rew_buffer_std\": self.ppo_rollout_buffer.int_rew_std,\n\t            }\n\t            if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n\t                log_data.update({\n\t                    \"rollout/ep_info_rew_mean\": safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n\t                    \"rollout/ep_info_len_mean\": safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]),\n\t                })\n\t            else:\n\t                log_data.update({\n", "                    \"rollout/ep_info_rew_mean\": 0.0,\n\t                })\n\t            if self.int_rew_coef > 0:\n\t                log_data.update({\n\t                    \"rollout/int_rew_mean\": np.mean(self.ppo_rollout_buffer.intrinsic_rewards),\n\t                    \"rollout/int_rew_std\": np.std(self.ppo_rollout_buffer.intrinsic_rewards),\n\t                    \"rollout/pos_int_rew_mean\": np.maximum(self.ppo_rollout_buffer.intrinsic_rewards, 0.0).mean(),\n\t                    \"rollout/neg_int_rew_mean\": np.minimum(self.ppo_rollout_buffer.intrinsic_rewards, 0.0).mean(),\n\t                })\n\t            if self.adv_norm > 0:\n", "                log_data.update({\n\t                    \"rollout/adv_mean\": np.mean(self.ppo_rollout_buffer.adv_mean),\n\t                    \"rollout/adv_std\": np.std(self.ppo_rollout_buffer.adv_std),\n\t                })\n\t            # Update with other stats\n\t            log_data.update(self.rollout_stats.to_dict())\n\t            # Logging with wandb\n\t            if self.use_wandb:\n\t                wandb.log(log_data)\n\t            # Logging with local logger\n", "            if self.local_logger is not None:\n\t                self.local_logger.write(log_data, log_type='rollout')\n\t        # Log visualized data\n\t        if self.enable_plotting and self.use_wandb:\n\t            if self.iteration > 0 and self.iteration % self.plot_interval == 0:\n\t                # Plot visit counts\n\t                _, ax = plt.subplots()\n\t                im = ax.imshow(\n\t                    self.global_visit_counts,\n\t                    cmap=self.plot_colormap,\n", "                    interpolation='nearest')\n\t                ax.figure.colorbar(im, ax=ax)\n\t                plt.title(f'visit counts (iters:{self.iteration}, steps:{self.num_timesteps})')\n\t                wandb.log({\"visit counts\": plt})\n\t                plt.close()\n\t                # Plot reward map\n\t                _, ax = plt.subplots()\n\t                im = ax.imshow(\n\t                    self.global_reward_map_sums / (self.global_reward_map_nums + 1e-8),\n\t                    cmap=self.plot_colormap,\n", "                    interpolation='nearest')\n\t                ax.figure.colorbar(im, ax=ax)\n\t                plt.title(f'reward map (iters:{self.iteration}, steps:{self.num_timesteps})')\n\t                wandb.log({\"reward map\": plt})\n\t                plt.close()\n\t                # Plot sum reward map\n\t                _, ax = plt.subplots()\n\t                im = ax.imshow(\n\t                    self.global_reward_map_sums,\n\t                    cmap=self.plot_colormap,\n", "                    interpolation='nearest')\n\t                ax.figure.colorbar(im, ax=ax)\n\t                plt.title(f'sum reward map (iters:{self.iteration}, steps:{self.num_timesteps})')\n\t                wandb.log({\"sum reward map\": plt})\n\t                plt.close()\n\t                # Plot max reward map\n\t                _, ax = plt.subplots()\n\t                im = ax.imshow(\n\t                    self.global_reward_map_maxs,\n\t                    cmap=self.plot_colormap,\n", "                    interpolation='nearest')\n\t                ax.figure.colorbar(im, ax=ax)\n\t                plt.title(f'max reward map (iters:{self.iteration}, steps:{self.num_timesteps})')\n\t                wandb.log({\"max reward map\": plt})\n\t                plt.close()\n\t                # Plot value map\n\t                _, ax = plt.subplots()\n\t                im = ax.imshow(\n\t                    self.global_value_map_sums / (self.global_value_map_nums + 1e-8),\n\t                    cmap=self.plot_colormap,\n", "                    interpolation='nearest')\n\t                ax.figure.colorbar(im, ax=ax)\n\t                plt.title(f'value map (iters:{self.iteration}, steps:{self.num_timesteps})')\n\t                wandb.log({\"value map\": plt})\n\t                plt.close()\n\t                # Clear counts\n\t                self.global_visit_counts = np.zeros([self.width, self.height], dtype=np.int32)\n\t                self.global_reward_map_maxs = np.zeros([self.width, self.height], dtype=np.float64)\n\t                self.global_reward_map_sums = np.zeros([self.width, self.height], dtype=np.float64)\n\t                self.global_reward_map_nums = np.zeros([self.width, self.height], dtype=np.int32)\n", "                self.global_value_map_sums = np.zeros([self.width, self.height], dtype=np.float64)\n\t                self.global_value_map_nums = np.zeros([self.width, self.height], dtype=np.int32)\n\t    def create_intrinsic_rewards(self, new_obs, actions, dones):\n\t        if self.int_rew_source == ModelType.NoModel:\n\t            intrinsic_rewards = np.zeros([self.n_envs], dtype=float)\n\t            model_mems = None\n\t            return intrinsic_rewards, model_mems\n\t        # Prepare input tensors for IR generation\n\t        with th.no_grad():\n\t            curr_obs_tensor = obs_as_tensor(self._last_obs, self.device)\n", "            next_obs_tensor = obs_as_tensor(new_obs, self.device)\n\t            curr_act_tensor = th.as_tensor(actions, dtype=th.int64, device=self.device)\n\t            done_tensor = th.as_tensor(dones, dtype=th.int64, device=self.device)\n\t            if self.policy.use_model_rnn:\n\t                last_model_mem_tensor = self._last_model_mems\n\t                if self.int_rew_source in [ModelType.RND, ModelType.NGU, ModelType.NovelD]:\n\t                    if self.policy.rnd_use_policy_emb:\n\t                        last_model_mem_tensor = self._last_policy_mems\n\t            else:\n\t                last_model_mem_tensor = None\n", "            if self.policy.use_status_predictor:\n\t                key_status_tensor = th.as_tensor(self.curr_key_status, dtype=th.int64, device=self.device)\n\t                door_status_tensor = th.as_tensor(self.curr_door_status, dtype=th.int64, device=self.device)\n\t                target_dists_tensor = th.as_tensor(self.curr_target_dists, dtype=th.float32, device=self.device)\n\t            else:\n\t                key_status_tensor = None\n\t                door_status_tensor = None\n\t                target_dists_tensor = None\n\t        # DEIR / Plain discriminator model\n\t        if self.int_rew_source in [ModelType.DEIR, ModelType.PlainDiscriminator]:\n", "            intrinsic_rewards, model_mems = self.policy.int_rew_model.get_intrinsic_rewards(\n\t                curr_obs=curr_obs_tensor,\n\t                next_obs=next_obs_tensor,\n\t                last_mems=last_model_mem_tensor,\n\t                obs_history=self.episodic_obs_emb_history,\n\t                trj_history=self.episodic_trj_emb_history,\n\t                plain_dsc=bool(self.int_rew_source == ModelType.PlainDiscriminator),\n\t            )\n\t            # Insert obs into the Discriminator's obs queue\n\t            # Algorithm A2 in the Technical Appendix\n", "            if self.int_rew_source in [ModelType.DEIR, ModelType.PlainDiscriminator]:\n\t                self.policy.int_rew_model.update_obs_queue(\n\t                    iteration=self.iteration,\n\t                    intrinsic_rewards=intrinsic_rewards,\n\t                    ir_mean=self.ppo_rollout_buffer.int_rew_stats.mean,\n\t                    new_obs=new_obs,\n\t                    stats_logger=self.rollout_stats\n\t                )\n\t        # Plain forward / inverse model\n\t        elif self.int_rew_source in [ModelType.PlainForward, ModelType.PlainInverse]:\n", "            intrinsic_rewards, model_mems = self.policy.int_rew_model.get_intrinsic_rewards(\n\t                curr_obs=curr_obs_tensor,\n\t                next_obs=next_obs_tensor,\n\t                last_mems=last_model_mem_tensor,\n\t                curr_act=curr_act_tensor,\n\t                curr_dones=done_tensor,\n\t                obs_history=self.episodic_obs_emb_history,\n\t                key_status=key_status_tensor,\n\t                door_status=door_status_tensor,\n\t                target_dists=target_dists_tensor,\n", "                stats_logger=self.rollout_stats\n\t            )\n\t        # ICM\n\t        elif self.int_rew_source == ModelType.ICM:\n\t            intrinsic_rewards, model_mems = self.policy.int_rew_model.get_intrinsic_rewards(\n\t                curr_obs=curr_obs_tensor,\n\t                next_obs=next_obs_tensor,\n\t                last_mems=last_model_mem_tensor,\n\t                curr_act=curr_act_tensor,\n\t                curr_dones=done_tensor,\n", "                stats_logger=self.rollout_stats\n\t            )\n\t        # RND\n\t        elif self.int_rew_source == ModelType.RND:\n\t            intrinsic_rewards, model_mems = self.policy.int_rew_model.get_intrinsic_rewards(\n\t                curr_obs=curr_obs_tensor,\n\t                last_mems=last_model_mem_tensor,\n\t                curr_dones=done_tensor,\n\t                stats_logger=self.rollout_stats\n\t            )\n", "        # NGU\n\t        elif self.int_rew_source == ModelType.NGU:\n\t            intrinsic_rewards, model_mems = self.policy.int_rew_model.get_intrinsic_rewards(\n\t                curr_obs=curr_obs_tensor,\n\t                next_obs=next_obs_tensor,\n\t                last_mems=last_model_mem_tensor,\n\t                curr_act=curr_act_tensor,\n\t                curr_dones=done_tensor,\n\t                obs_history=self.episodic_obs_emb_history,\n\t                stats_logger=self.rollout_stats\n", "            )\n\t        # NovelD\n\t        elif self.int_rew_source == ModelType.NovelD:\n\t            return self.policy.int_rew_model.get_intrinsic_rewards(\n\t                curr_obs=curr_obs_tensor,\n\t                next_obs=next_obs_tensor,\n\t                last_mems=last_model_mem_tensor,\n\t                curr_dones=done_tensor,\n\t                stats_logger=self.rollout_stats\n\t            )\n", "        else:\n\t            raise NotImplementedError\n\t        return intrinsic_rewards, model_mems\n\t    def collect_rollouts(\n\t        self,\n\t        env: VecEnv,\n\t        callback: BaseCallback,\n\t        ppo_rollout_buffer: PPORolloutBuffer,\n\t        n_rollout_steps: int,\n\t    ) -> bool:\n", "        assert self._last_obs is not None, \"No previous observation was provided\"\n\t        n_steps = 0\n\t        ppo_rollout_buffer.reset()\n\t        # Sample new weights for the state dependent exploration\n\t        if self.use_sde:\n\t            self.policy.reset_noise(env.num_envs)\n\t        callback.on_rollout_start()\n\t        self.init_on_rollout_start()\n\t        while n_steps < n_rollout_steps:\n\t            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n", "                # Sample a new noise matrix\n\t                self.policy.reset_noise(env.num_envs)\n\t            with th.no_grad():\n\t                # Convert to pytorch tensor or to TensorDict\n\t                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n\t                actions, values, log_probs, policy_mems = \\\n\t                    self.policy.forward(obs_tensor, self._last_policy_mems)\n\t                actions = actions.cpu().numpy()\n\t            # Rescale and perform action\n\t            clipped_actions = actions\n", "            # Clip the actions to avoid out of bound error\n\t            if isinstance(self.action_space, gym.spaces.Box):\n\t                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n\t            # Log before a transition\n\t            self.log_before_transition(values)\n\t            # Transition\n\t            new_obs, rewards, dones, infos = env.step(clipped_actions)\n\t            if isinstance(new_obs, Dict):\n\t                new_obs = new_obs[\"rgb\"]\n\t            if self.env_render:\n", "                env.render()\n\t            with th.no_grad():\n\t                # Compute value for the last timestep\n\t                new_obs_tensor = obs_as_tensor(new_obs, self.device)\n\t                _, new_values, _, _ = self.policy.forward(new_obs_tensor, policy_mems)\n\t            # IR Generation\n\t            intrinsic_rewards, model_mems = \\\n\t                self.create_intrinsic_rewards(new_obs, actions, dones)\n\t            # Log after the transition and IR generation\n\t            self.log_after_transition(rewards, intrinsic_rewards)\n", "            # Clear episodic memories when an episode ends\n\t            self.clear_on_episode_end(dones, policy_mems, model_mems)\n\t            # Update global stats\n\t            self.num_timesteps += self.n_envs\n\t            self._update_info_buffer(infos)\n\t            n_steps += 1\n\t            # Add to PPO buffer\n\t            if isinstance(self.action_space, gym.spaces.Discrete):\n\t                actions = actions.reshape(-1, 1)\n\t            ppo_rollout_buffer.add(\n", "                self._last_obs,\n\t                new_obs,\n\t                self._last_policy_mems,\n\t                self._last_model_mems,\n\t                actions,\n\t                rewards,\n\t                intrinsic_rewards,\n\t                self._last_episode_starts,\n\t                dones,\n\t                values,\n", "                log_probs,\n\t                self.curr_key_status,\n\t                self.curr_door_status,\n\t                self.curr_target_dists,\n\t            )\n\t            self._last_obs = new_obs\n\t            self._last_episode_starts = dones\n\t            if policy_mems is not None:\n\t                self._last_policy_mems = policy_mems.detach().clone()\n\t            if model_mems is not None:\n", "                self._last_model_mems = model_mems.detach().clone()\n\t        ppo_rollout_buffer.compute_intrinsic_rewards()\n\t        ppo_rollout_buffer.compute_returns_and_advantage(new_values, dones)\n\t        callback.on_rollout_end()\n\t        return True\n\t    def learn(\n\t        self,\n\t        total_timesteps: int,\n\t        callback: MaybeCallback = None,\n\t        log_interval: int = 1,\n", "        eval_env: Optional[GymEnv] = None,\n\t        eval_freq: int = -1,\n\t        n_eval_episodes: int = 5,\n\t        tb_log_name: str = \"CustomOnPolicyAlgorithm\",\n\t        eval_log_path: Optional[str] = None,\n\t        reset_num_timesteps: bool = True,\n\t    ) -> \"PPORollout\":\n\t        self.iteration = 0\n\t        total_timesteps, callback = self._setup_learn(\n\t            total_timesteps, eval_env, callback, eval_freq, n_eval_episodes, eval_log_path, reset_num_timesteps, tb_log_name\n", "        )\n\t        self.total_timesteps = total_timesteps\n\t        callback.on_training_start(locals(), globals())\n\t        self.on_training_start()\n\t        print('Collecting rollouts ...')\n\t        while self.num_timesteps < total_timesteps:\n\t            collect_start_time = time.time()\n\t            self.policy.eval()\n\t            continue_training = self.collect_rollouts(\n\t                self.env,\n", "                callback,\n\t                self.ppo_rollout_buffer,\n\t                n_rollout_steps=self.ppo_rollout_buffer.buffer_size)\n\t            self.policy.train()\n\t            collect_end_time = time.time()\n\t            # Uploading rollout infos\n\t            self.iteration += 1\n\t            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n\t            self.log_on_rollout_end(log_interval)\n\t            # Train the agent's policy and the IR model\n", "            if continue_training is False:\n\t                break\n\t            train_start_time = time.time()\n\t            self.train()\n\t            train_end_time = time.time()\n\t            # Print to the console\n\t            rews = [ep_info[\"r\"] for ep_info in self.ep_info_buffer]\n\t            rew_mean = 0.0 if len(rews) == 0 else np.mean(rews)\n\t            print(f'run: {self.run_id:2d}  '\n\t                  f'iters: {self.iteration}  '\n", "                  f'frames: {self.num_timesteps}  '\n\t                  f'rew: {rew_mean:.6f}  '\n\t                  f'rollout: {collect_end_time - collect_start_time:.3f} sec  '\n\t                  f'train: {train_end_time - train_start_time:.3f} sec')\n\t        callback.on_training_end()\n\t        return self\n"]}
{"filename": "src/algo/ppo_model.py", "chunked_list": ["import gym\n\timport numpy as np\n\tfrom src.algo.intrinsic_rewards.deir import DiscriminatorModel\n\tfrom src.algo.intrinsic_rewards.icm import ICMModel\n\tfrom src.algo.intrinsic_rewards.ngu import NGUModel\n\tfrom src.algo.intrinsic_rewards.noveld import NovelDModel\n\tfrom src.algo.intrinsic_rewards.plain_forward import PlainForwardModel\n\tfrom src.algo.intrinsic_rewards.plain_inverse import PlainInverseModel\n\tfrom src.algo.intrinsic_rewards.rnd import RNDModel\n\tfrom src.algo.common_models.gru_cell import CustomGRUCell\n", "from src.algo.common_models.mlps import *\n\tfrom src.utils.common_func import init_module_with_name\n\tfrom src.utils.enum_types import ModelType, NormType\n\tfrom stable_baselines3.common.policies import ActorCriticCnnPolicy\n\tfrom stable_baselines3.common.preprocessing import preprocess_obs\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom stable_baselines3.common.type_aliases import Schedule\n\tfrom torch.nn import GRUCell\n\tfrom typing import Dict, Any, List, Union\n\tclass PPOModel(ActorCriticCnnPolicy):\n", "    def __init__(\n\t        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        lr_schedule: Schedule,\n\t        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        learning_rate: float = 3e-4,\n\t        model_learning_rate: float = 3e-4,\n\t        run_id: int = 0,\n", "        n_envs: int = 0,\n\t        use_sde: bool = False,\n\t        log_std_init: float = 0.0,\n\t        full_std: bool = True,\n\t        sde_net_arch: Optional[List[int]] = None,\n\t        use_expln: bool = False,\n\t        squash_output: bool = False,\n\t        policy_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n\t        policy_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n", "        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_features_dim: int = 128,\n\t        latents_dim: int = 128,\n\t        model_latents_dim: int = 128,\n\t        policy_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n", "        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        policy_gru_norm: NormType = NormType.NoNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        use_status_predictor: int = 0,\n\t        gru_layers: int = 1,\n\t        policy_mlp_layers: int = 1,\n\t        total_timesteps: int = 0,\n\t        n_steps: int = 0,\n", "        int_rew_source: ModelType = ModelType.DEIR,\n\t        icm_forward_loss_coef: float = 0.2,\n\t        ngu_knn_k: int = 10,\n\t        ngu_dst_momentum: float = -1,\n\t        ngu_use_rnd: int = 0,\n\t        rnd_err_norm: int = 0,\n\t        rnd_err_momentum: float = -1,\n\t        rnd_use_policy_emb: int = 0,\n\t        dsc_obs_queue_len: int = 0,\n\t        log_dsc_verbose: int = 0,\n", "    ):\n\t        self.run_id = run_id\n\t        self.n_envs = n_envs\n\t        self.latents_dim = latents_dim\n\t        self.activation_fn = activation_fn\n\t        self.max_grad_norm = max_grad_norm\n\t        self.model_features_dim = model_features_dim\n\t        self.model_latents_dim = model_latents_dim\n\t        self.action_num = action_space.n\n\t        self.learning_rate = learning_rate\n", "        self.model_learning_rate = model_learning_rate\n\t        self.int_rew_source = int_rew_source\n\t        self.policy_mlp_norm = policy_mlp_norm\n\t        self.model_mlp_norm = model_mlp_norm\n\t        self.model_cnn_norm = model_cnn_norm\n\t        self.policy_gru_norm = policy_gru_norm\n\t        self.model_gru_norm = model_gru_norm\n\t        self.model_mlp_layers = model_mlp_layers\n\t        self.gru_layers = gru_layers\n\t        self.use_status_predictor = use_status_predictor\n", "        self.policy_mlp_layers = policy_mlp_layers\n\t        self.total_timesteps = total_timesteps\n\t        self.n_steps = n_steps\n\t        self.policy_gru_cell = GRUCell if self.policy_gru_norm == NormType.NoNorm else CustomGRUCell\n\t        self.model_gru_cell = GRUCell if self.model_gru_norm == NormType.NoNorm else CustomGRUCell\n\t        self.use_model_rnn = use_model_rnn\n\t        self.icm_forward_loss_coef = icm_forward_loss_coef\n\t        self.ngu_knn_k = ngu_knn_k\n\t        self.ngu_dst_momentum = ngu_dst_momentum\n\t        self.ngu_use_rnd = ngu_use_rnd\n", "        self.rnd_err_norm = rnd_err_norm\n\t        self.rnd_err_momentum = rnd_err_momentum\n\t        self.rnd_use_policy_emb = rnd_use_policy_emb\n\t        self.policy_features_extractor_class = policy_features_extractor_class\n\t        self.policy_features_extractor_kwargs = policy_features_extractor_kwargs\n\t        self.model_cnn_features_extractor_class = model_cnn_features_extractor_class\n\t        self.model_cnn_features_extractor_kwargs = model_cnn_features_extractor_kwargs\n\t        self.dsc_obs_queue_len = dsc_obs_queue_len\n\t        self.log_dsc_verbose = log_dsc_verbose\n\t        if isinstance(observation_space, gym.spaces.Dict):\n", "            observation_space = observation_space[\"rgb\"]\n\t        self.dim_policy_features = self.policy_features_extractor_kwargs['features_dim']\n\t        self.dim_model_features = self.model_features_dim\n\t        self.policy_mlp_common_kwargs = dict(\n\t            inputs_dim = self.dim_policy_features,\n\t            latents_dim = self.latents_dim,\n\t            activation_fn = self.activation_fn,\n\t            mlp_norm = self.policy_mlp_norm,\n\t            mlp_layers = self.policy_mlp_layers,\n\t        )\n", "        self.policy_rnn_kwargs = dict(\n\t            input_size=self.dim_policy_features,\n\t            hidden_size=self.dim_policy_features,\n\t        )\n\t        if self.policy_gru_norm != NormType.NoNorm:\n\t            self.policy_rnn_kwargs.update(dict(\n\t                norm_type=self.policy_gru_norm,\n\t            ))\n\t        super(ActorCriticCnnPolicy, self).__init__(\n\t            observation_space,\n", "            action_space,\n\t            lr_schedule,\n\t            net_arch,\n\t            activation_fn,\n\t            False,\n\t            use_sde,\n\t            log_std_init,\n\t            full_std,\n\t            sde_net_arch,\n\t            use_expln,\n", "            squash_output,\n\t            self.policy_features_extractor_class,\n\t            self.policy_features_extractor_kwargs,\n\t            normalize_images,\n\t            optimizer_class,\n\t            optimizer_kwargs,\n\t        )\n\t        self._init_modules()\n\t        self._init_optimizers()\n\t        # Build Intrinsic Reward Models\n", "        int_rew_model_kwargs = dict(\n\t            observation_space=self.observation_space,\n\t            action_space=self.action_space,\n\t            activation_fn=self.activation_fn,\n\t            optimizer_class=self.optimizer_class,\n\t            optimizer_kwargs=self.optimizer_kwargs,\n\t            max_grad_norm=self.max_grad_norm,\n\t            model_learning_rate=self.model_learning_rate,\n\t            model_cnn_features_extractor_class=self.model_cnn_features_extractor_class,\n\t            model_cnn_features_extractor_kwargs=self.model_cnn_features_extractor_kwargs,\n", "            model_features_dim=self.model_features_dim,\n\t            model_latents_dim=self.model_latents_dim,\n\t            model_mlp_norm=self.model_mlp_norm,\n\t            model_cnn_norm=self.model_cnn_norm,\n\t            model_gru_norm=self.model_gru_norm,\n\t            use_model_rnn=self.use_model_rnn,\n\t            model_mlp_layers=self.model_mlp_layers,\n\t            gru_layers=self.gru_layers,\n\t            use_status_predictor=self.use_status_predictor,\n\t        )\n", "        if self.int_rew_source in [ModelType.DEIR, ModelType.PlainDiscriminator]:\n\t            self.int_rew_model = DiscriminatorModel(\n\t                **int_rew_model_kwargs,\n\t                obs_rng=np.random.default_rng(seed=self.run_id + 131),\n\t                dsc_obs_queue_len=self.dsc_obs_queue_len,\n\t                log_dsc_verbose=self.log_dsc_verbose,\n\t            )\n\t        if self.int_rew_source == ModelType.ICM:\n\t            self.int_rew_model = ICMModel(\n\t                icm_forward_loss_coef=self.icm_forward_loss_coef,\n", "                **int_rew_model_kwargs,\n\t            )\n\t        if self.int_rew_source == ModelType.PlainForward:\n\t            self.int_rew_model = PlainForwardModel(\n\t                **int_rew_model_kwargs,\n\t            )\n\t        if self.int_rew_source == ModelType.PlainInverse:\n\t            self.int_rew_model = PlainInverseModel(\n\t                **int_rew_model_kwargs,\n\t            )\n", "        if self.int_rew_source == ModelType.RND:\n\t            self.int_rew_model = RNDModel(\n\t                **int_rew_model_kwargs,\n\t                rnd_err_norm=self.rnd_err_norm,\n\t                rnd_err_momentum=self.rnd_err_momentum,\n\t                rnd_use_policy_emb=self.rnd_use_policy_emb,\n\t                policy_cnn=self.features_extractor,\n\t                policy_rnns=self.policy_rnns,\n\t            )\n\t        if self.int_rew_source == ModelType.NGU:\n", "            self.int_rew_model = NGUModel(\n\t                **int_rew_model_kwargs,\n\t                ngu_knn_k=self.ngu_knn_k,\n\t                ngu_dst_momentum=self.ngu_dst_momentum,\n\t                ngu_use_rnd=self.ngu_use_rnd,\n\t                rnd_err_norm=self.rnd_err_norm,\n\t                rnd_err_momentum=self.rnd_err_momentum,\n\t                rnd_use_policy_emb=self.rnd_use_policy_emb,\n\t                policy_cnn=self.features_extractor,\n\t                policy_rnns=self.policy_rnns,\n", "            )\n\t        if self.int_rew_source == ModelType.NovelD:\n\t            self.int_rew_model = NovelDModel(\n\t                **int_rew_model_kwargs,\n\t                rnd_err_norm=self.rnd_err_norm,\n\t                rnd_err_momentum=self.rnd_err_momentum,\n\t                rnd_use_policy_emb=self.rnd_use_policy_emb,\n\t                policy_cnn=self.features_extractor,\n\t                policy_rnns=self.policy_rnns,\n\t            )\n", "    def _build_mlp_extractor(self) -> None:\n\t        self.mlp_extractor = PolicyValueOutputHeads(\n\t            **self.policy_mlp_common_kwargs\n\t        )\n\t    def _build(self, lr_schedule: Schedule) -> None:\n\t        super()._build(lr_schedule)\n\t        # Build RNNs\n\t        self.policy_rnns = []\n\t        for l in range(self.gru_layers):\n\t            name = f'policy_rnn_layer_{l}'\n", "            setattr(self, name, self.policy_gru_cell(**self.policy_rnn_kwargs))\n\t            self.policy_rnns.append(getattr(self, name))\n\t    def _init_modules(self) -> None:\n\t        nn.init.zeros_(self.action_net.weight)\n\t        nn.init.zeros_(self.action_net.bias)\n\t        nn.init.zeros_(self.value_net.weight)\n\t        nn.init.zeros_(self.value_net.bias)\n\t        module_names = {\n\t            self.features_extractor: 'features_extractor',\n\t            self.mlp_extractor: 'mlp_extractor',\n", "        }\n\t        for l in range(self.gru_layers):\n\t            name = f'policy_rnn_layer_{l}'\n\t            module = getattr(self, name)\n\t            module_names.update({module: name})\n\t        for module, name in module_names.items():\n\t            init_module_with_name(name, module)\n\t    def _init_optimizers(self) -> None:\n\t        self.optimizer = self.optimizer_class(\n\t            self.parameters(),\n", "            lr=self.learning_rate,\n\t            **self.optimizer_kwargs\n\t        )\n\t    def _get_rnn_embeddings(self, hiddens: Optional[Tensor], inputs: Tensor, modules: List[nn.Module]):\n\t        outputs = []\n\t        for i, module in enumerate(modules):\n\t            hidden_i = th.squeeze(hiddens[:, i, :])\n\t            output_i = module(inputs, hidden_i)\n\t            inputs = output_i\n\t            outputs.append(output_i)\n", "        outputs = th.stack(outputs, dim=1)\n\t        return outputs\n\t    def _get_latent(self, obs: Tensor, mem: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n\t        obs = preprocess_obs(obs, self.observation_space, normalize_images=self.normalize_images)\n\t        curr_features = self.features_extractor(obs)\n\t        memories = self._get_rnn_embeddings(mem, curr_features, self.policy_rnns)\n\t        features = th.squeeze(memories[:, -1, :])\n\t        latent_pi, latent_vf = self.mlp_extractor(features)\n\t        latent_sde = latent_pi\n\t        return latent_pi, latent_vf, latent_sde, memories\n", "    def forward(self, obs: Tensor, mem: Tensor, deterministic: bool = False) \\\n\t            -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n\t        latent_pi, latent_vf, latent_sde, memories = self._get_latent(obs, mem)\n\t        values = self.value_net(latent_vf)\n\t        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde=latent_sde)\n\t        actions = distribution.get_actions(deterministic=deterministic)\n\t        log_prob = distribution.log_prob(actions)\n\t        return actions, values, log_prob, memories\n\t    def evaluate_policy(self, obs: Tensor, act: Tensor, mem: Tensor) \\\n\t            -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n", "        latent_pi, latent_vf, latent_sde, memories = self._get_latent(obs, mem)\n\t        distribution = self._get_action_dist_from_latent(latent_pi, latent_sde)\n\t        log_prob = distribution.log_prob(act)\n\t        values = self.value_net(latent_vf)\n\t        return values, log_prob, distribution.entropy(), memories\n"]}
{"filename": "src/algo/buffers/ppo_buffer.py", "chunked_list": ["import numpy as np\n\timport torch as th\n\tfrom gym import spaces\n\tfrom gym.spaces import Dict\n\tfrom typing import Generator, Optional, Union\n\tfrom stable_baselines3.common.buffers import BaseBuffer\n\tfrom stable_baselines3.common.vec_env import VecNormalize\n\tfrom src.algo.buffers.type_aliases import RolloutBufferSamples\n\tfrom src.utils.common_func import normalize_rewards\n\tfrom src.utils.running_mean_std import RunningMeanStd\n", "class PPORolloutBuffer(BaseBuffer):\n\t    def __init__(\n\t        self,\n\t        buffer_size: int,\n\t        observation_space: spaces.Space,\n\t        action_space: spaces.Space,\n\t        device: Union[th.device, str] = \"cpu\",\n\t        gae_lambda: float = 1,\n\t        gamma: float = 0.99,\n\t        n_envs: int = 1,\n", "        features_dim: int = 0,\n\t        dim_policy_traj: int = 0,\n\t        dim_model_traj: int = 0,\n\t        int_rew_coef: float = 1.0,\n\t        ext_rew_coef: float = 1.0,\n\t        int_rew_norm: int = 0,\n\t        int_rew_clip: float = 0.0,\n\t        int_rew_eps: float = 1e-8,\n\t        adv_momentum: float = 0.0,\n\t        adv_norm: int = 0,\n", "        adv_eps: float = 1e-8,\n\t        gru_layers: int = 1,\n\t        int_rew_momentum: Optional[float] = None,\n\t        use_status_predictor: int = 0,\n\t    ):\n\t        if isinstance(observation_space, Dict):\n\t            observation_space = list(observation_space.values())[0]\n\t        super(PPORolloutBuffer, self)\\\n\t            .__init__(buffer_size, observation_space, action_space, device, n_envs=n_envs)\n\t        self.gae_lambda = gae_lambda\n", "        self.gamma = gamma\n\t        self.int_rew_coef = int_rew_coef\n\t        self.int_rew_norm = int_rew_norm\n\t        self.int_rew_clip = int_rew_clip\n\t        self.ext_rew_coef = ext_rew_coef\n\t        self.features_dim = features_dim\n\t        self.dim_policy_traj = dim_policy_traj\n\t        self.dim_model_traj = dim_model_traj\n\t        self.int_rew_eps = int_rew_eps\n\t        self.adv_momentum = adv_momentum\n", "        self.adv_mean = None\n\t        self.int_rew_mean = None\n\t        self.int_rew_std = None\n\t        self.ir_mean_buffer = []\n\t        self.ir_std_buffer = []\n\t        self.use_status_predictor = use_status_predictor\n\t        self.adv_norm = adv_norm\n\t        self.adv_eps = adv_eps\n\t        self.gru_layers = gru_layers\n\t        self.int_rew_momentum = int_rew_momentum\n", "        self.int_rew_stats = RunningMeanStd(momentum=self.int_rew_momentum)\n\t        self.advantage_stats = RunningMeanStd(momentum=self.adv_momentum)\n\t        self.generator_ready = False\n\t        self.reset()\n\t    def reset(self) -> None:\n\t        self.observations = np.zeros((self.buffer_size, self.n_envs) + self.obs_shape, dtype=np.float32)\n\t        self.new_observations = np.zeros((self.buffer_size, self.n_envs) + self.obs_shape, dtype=np.float32)\n\t        self.last_policy_mems = np.zeros((self.buffer_size, self.n_envs, self.gru_layers, self.dim_policy_traj), dtype=np.float32)\n\t        self.last_model_mems = np.zeros((self.buffer_size, self.n_envs, self.gru_layers, self.dim_model_traj), dtype=np.float32)\n\t        self.actions = np.zeros((self.buffer_size, self.n_envs, self.action_dim), dtype=np.float32)\n", "        self.rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        self.intrinsic_rewards = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        self.returns = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        self.episode_starts = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        self.episode_dones = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        self.values = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        self.log_probs = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        self.advantages = np.zeros((self.buffer_size, self.n_envs), dtype=np.float32)\n\t        if self.use_status_predictor:\n\t            self.curr_key_status = np.zeros((self.buffer_size, self.n_envs), dtype=np.int32)\n", "            self.curr_door_status = np.zeros((self.buffer_size, self.n_envs), dtype=np.int32)\n\t            self.curr_target_dists = np.zeros((self.buffer_size, self.n_envs, 3), dtype=np.float32)\n\t        self.generator_ready = False\n\t        super(PPORolloutBuffer, self).reset()\n\t    def compute_intrinsic_rewards(self) -> None:\n\t        # Normalize intrinsic rewards per rollout buffer\n\t        self.int_rew_stats.update(self.intrinsic_rewards.reshape(-1))\n\t        self.int_rew_mean = self.int_rew_stats.mean\n\t        self.int_rew_std = self.int_rew_stats.std\n\t        self.intrinsic_rewards = normalize_rewards(\n", "            norm_type=self.int_rew_norm,\n\t            rewards=self.intrinsic_rewards,\n\t            mean=self.int_rew_mean,\n\t            std=self.int_rew_std,\n\t            eps=self.int_rew_eps,\n\t        )\n\t        # Rescale by IR coef\n\t        self.intrinsic_rewards *= self.int_rew_coef\n\t        # Clip after normalization\n\t        if self.int_rew_clip > 0:\n", "            self.intrinsic_rewards = np.clip(self.intrinsic_rewards, -self.int_rew_clip, self.int_rew_clip)\n\t    def compute_returns_and_advantage(self, last_values: th.Tensor, dones: np.ndarray) -> None:\n\t        # Rescale extrinisc rewards\n\t        self.rewards *= self.ext_rew_coef\n\t        # Convert to numpy\n\t        last_values = last_values.clone().cpu().numpy().flatten()\n\t        last_gae_lam = 0\n\t        for step in reversed(range(self.buffer_size)):\n\t            if step == self.buffer_size - 1:\n\t                next_non_terminal = 1.0 - dones\n", "                next_values = last_values\n\t            else:\n\t                next_non_terminal = 1.0 - self.episode_starts[step + 1]\n\t                next_values = self.values[step + 1]\n\t            delta = self.rewards[step] + self.intrinsic_rewards[step] + \\\n\t                    self.gamma * next_values * next_non_terminal - self.values[step]\n\t            last_gae_lam = delta + self.gamma * self.gae_lambda * next_non_terminal * last_gae_lam\n\t            self.advantages[step] = last_gae_lam\n\t        # TD(lambda) estimator, see Github PR #375 or \"Telescoping in TD(lambda)\"\n\t        # in David Silver Lecture 4: https://www.youtube.com/watch?v=PnHCvfgC_ZA\n", "        self.returns = self.advantages + self.values\n\t        # Normalize advantages per rollout buffer\n\t        if self.adv_norm:\n\t            self.advantage_stats.update(self.advantages)\n\t            self.adv_mean = self.advantage_stats.mean\n\t            self.adv_std = self.advantage_stats.std\n\t            # Standardization\n\t            if self.adv_norm == 2:\n\t                self.advantages = (self.advantages - self.adv_mean) / (self.adv_std + self.adv_eps)\n\t            # Standardization without subtracting the mean value\n", "            if self.adv_norm == 3:\n\t                self.advantages = self.advantages / (self.adv_std + self.adv_eps)\n\t    def add(\n\t        self,\n\t        obs: np.ndarray,\n\t        new_obs: np.ndarray,\n\t        last_policy_mem: th.Tensor,\n\t        last_model_mem: th.Tensor,\n\t        action: np.ndarray,\n\t        reward: np.ndarray,\n", "        intrinsic_reward: np.ndarray,\n\t        episode_start: np.ndarray,\n\t        episode_done: np.ndarray,\n\t        value: th.Tensor,\n\t        log_prob: Optional[th.Tensor],\n\t        curr_key_status: Optional[np.ndarray],\n\t        curr_door_status: Optional[np.ndarray],\n\t        curr_target_dist: Optional[np.ndarray],\n\t    ) -> None:\n\t        if len(log_prob.shape) == 0:\n", "            # Reshape 0-d tensor to avoid error\n\t            log_prob = log_prob.reshape(-1, 1)\n\t        # Reshape needed when using multiple envs with discrete observations\n\t        # as numpy cannot broadcast (n_discrete,) to (n_discrete, 1)\n\t        if isinstance(self.observation_space, spaces.Discrete):\n\t            obs = obs.reshape((self.n_envs,) + self.obs_shape)\n\t        self.observations[self.pos] = np.array(obs).copy()\n\t        self.new_observations[self.pos] = np.array(new_obs).copy()\n\t        self.last_policy_mems[self.pos] = last_policy_mem.clone().cpu().numpy()\n\t        self.last_model_mems[self.pos] = last_model_mem.clone().cpu().numpy()\n", "        self.actions[self.pos] = np.array(action).copy()\n\t        self.rewards[self.pos] = np.array(reward).copy()\n\t        self.intrinsic_rewards[self.pos] = np.array(intrinsic_reward).copy()\n\t        self.episode_starts[self.pos] = np.array(episode_start).copy()\n\t        self.episode_dones[self.pos] = np.array(episode_done).copy()\n\t        self.values[self.pos] = value.clone().cpu().numpy().flatten()\n\t        self.log_probs[self.pos] = log_prob.clone().cpu().numpy()\n\t        if self.use_status_predictor:\n\t            self.curr_key_status[self.pos] = np.array(curr_key_status).copy()\n\t            self.curr_door_status[self.pos] = np.array(curr_door_status).copy()\n", "            self.curr_target_dists[self.pos] = np.array(curr_target_dist).copy()\n\t        self.pos += 1\n\t        if self.pos == self.buffer_size:\n\t            self.full = True\n\t    def prepare_data(self):\n\t        if not self.generator_ready:\n\t            _tensor_names = [\n\t                \"observations\",\n\t                \"new_observations\",\n\t                \"last_policy_mems\",\n", "                \"last_model_mems\",\n\t                \"episode_starts\",\n\t                \"episode_dones\",\n\t                \"actions\",\n\t                \"values\",\n\t                \"log_probs\",\n\t                \"advantages\",\n\t                \"returns\",\n\t            ]\n\t            if self.use_status_predictor:\n", "                _tensor_names += [\n\t                    \"curr_key_status\",\n\t                    \"curr_door_status\",\n\t                    \"curr_target_dists\",\n\t                ]\n\t            for tensor in _tensor_names:\n\t                self.__dict__[tensor] = self.swap_and_flatten(self.__dict__[tensor])\n\t            self.generator_ready = True\n\t    def get(self, batch_size: Optional[int] = None) -> Generator[RolloutBufferSamples, None, None]:\n\t        assert self.full, \"\"\n", "        self.prepare_data()\n\t        if batch_size is None:\n\t            batch_size = self.buffer_size * self.n_envs\n\t        indices = np.random.permutation(self.buffer_size * self.n_envs)\n\t        start_idx = 0\n\t        while start_idx < self.buffer_size * self.n_envs:\n\t            yield self._get_samples(indices[start_idx : start_idx + batch_size])\n\t            start_idx += batch_size\n\t    def _get_samples(self, batch_inds: np.ndarray, env: Optional[VecNormalize] = None) -> RolloutBufferSamples:\n\t        data = (\n", "            self.observations[batch_inds],\n\t            self.new_observations[batch_inds],\n\t            self.last_policy_mems[batch_inds],\n\t            self.last_model_mems[batch_inds],\n\t            self.episode_starts[batch_inds],\n\t            self.episode_dones[batch_inds],\n\t            self.actions[batch_inds],\n\t            self.values[batch_inds].flatten(),\n\t            self.log_probs[batch_inds].flatten(),\n\t            self.advantages[batch_inds].flatten(),\n", "            self.returns[batch_inds].flatten(),\n\t        )\n\t        if self.use_status_predictor:\n\t            data += (\n\t                self.curr_key_status[batch_inds].flatten(),\n\t                self.curr_door_status[batch_inds].flatten(),\n\t                self.curr_target_dists[batch_inds].flatten(),\n\t            )\n\t        samples = tuple(map(lambda x: self.to_torch(x, copy=False), data))\n\t        if not self.use_status_predictor:\n", "            samples += (None, None, None,)\n\t        return RolloutBufferSamples(*samples)\n"]}
{"filename": "src/algo/buffers/type_aliases.py", "chunked_list": ["from typing import NamedTuple, Optional\n\timport torch as th\n\tclass RolloutBufferSamples(NamedTuple):\n\t    observations: th.Tensor\n\t    new_observations: th.Tensor\n\t    last_policy_mems: th.Tensor\n\t    last_model_mems: th.Tensor\n\t    episode_starts: th.Tensor\n\t    episode_dones: th.Tensor\n\t    actions: th.Tensor\n", "    old_values: th.Tensor\n\t    old_log_prob: th.Tensor\n\t    advantages: th.Tensor\n\t    returns: th.Tensor\n\t    curr_key_status: Optional[th.Tensor]\n\t    curr_door_status: Optional[th.Tensor]\n\t    curr_target_dists: Optional[th.Tensor]\n"]}
{"filename": "src/algo/intrinsic_rewards/icm.py", "chunked_list": ["import gym\n\tfrom typing import Dict, Any\n\tfrom gym import spaces\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom src.algo.intrinsic_rewards.base_model import IntrinsicRewardBaseModel\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.enum_types import NormType\n\tfrom src.utils.loggers import StatisticsLogger\n\tclass ICMModel(IntrinsicRewardBaseModel):\n\t    def __init__(\n", "        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_learning_rate: float = 3e-4,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n", "        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        gru_layers: int = 1,\n\t        use_status_predictor: int = 0,\n", "        # Model-specific params\n\t        icm_forward_loss_coef: float = 0.2,\n\t    ):\n\t        super().__init__(observation_space, action_space, activation_fn, normalize_images,\n\t                         optimizer_class, optimizer_kwargs, max_grad_norm, model_learning_rate,\n\t                         model_cnn_features_extractor_class, model_cnn_features_extractor_kwargs,\n\t                         model_features_dim, model_latents_dim, model_mlp_norm,\n\t                         model_cnn_norm, model_gru_norm, use_model_rnn, model_mlp_layers,\n\t                         gru_layers, use_status_predictor)\n\t        self.icm_forward_loss_coef = icm_forward_loss_coef\n", "        self._build()\n\t        self._init_modules()\n\t        self._init_optimizers()\n\t    def _build(self) -> None:\n\t        # Build CNN and RNN\n\t        super()._build()\n\t        # Build MLP\n\t        self.model_mlp = ICMOutputHeads(\n\t            features_dim= self.model_features_dim,\n\t            latents_dim= self.model_latents_dim,\n", "            activation_fn = self.activation_fn,\n\t            action_num = self.action_num,\n\t            mlp_norm = self.model_mlp_norm,\n\t            mlp_layers = self.model_mlp_layers,\n\t        )\n\t    # Based on: https://github.com/pathak22/noreward-rl/blob/master/src/model.py\n\t    def forward(self,\n\t        curr_obs: Tensor, next_obs: Tensor, last_mems: Tensor,\n\t        curr_act: Tensor, curr_dones: Tensor,\n\t    ):\n", "        curr_cnn_embs = self._get_cnn_embeddings(curr_obs)\n\t        next_cnn_embs = self._get_cnn_embeddings(next_obs)\n\t        # Get RNN memories\n\t        if self.use_model_rnn:\n\t            curr_mems = self._get_rnn_embeddings(last_mems, curr_cnn_embs, self.model_rnns)\n\t            next_mems = self._get_rnn_embeddings(curr_mems, next_cnn_embs, self.model_rnns)\n\t            curr_rnn_embs = th.squeeze(curr_mems[:, -1, :])\n\t            next_rnn_embs = th.squeeze(next_mems[:, -1, :])\n\t            curr_embs = curr_rnn_embs\n\t            next_embs = next_rnn_embs\n", "        else:\n\t            curr_embs = curr_cnn_embs\n\t            next_embs = next_cnn_embs\n\t            curr_mems = None\n\t        curr_dones = curr_dones.view(-1)\n\t        n_samples = (1 - curr_dones).sum()\n\t        pred_embs, pred_act = self.model_mlp(curr_embs, next_embs, curr_act)\n\t        # Forward model\n\t        fwd_losses = 0.5 * F.mse_loss(pred_embs, next_embs, reduction='none') \\\n\t                         * self.model_features_dim  # eta (scaling factor)\n", "        fwd_losses = fwd_losses.mean(dim=1) * (1 - curr_dones)\n\t        fwd_loss = fwd_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        # Inverse model\n\t        inv_losses = F.cross_entropy(pred_act, curr_act, reduction='none') * (1 - curr_dones)\n\t        inv_loss = inv_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        return fwd_losses, fwd_loss, inv_loss, curr_mems\n\t    def get_intrinsic_rewards(self, curr_obs, next_obs, last_mems, curr_act, curr_dones, stats_logger):\n\t        with th.no_grad():\n\t            icm_fwd_losses, fwd_loss, inv_loss, _ = \\\n\t                self.forward(curr_obs, next_obs, last_mems, curr_act, curr_dones)\n", "        stats_logger.add(\n\t            inv_loss=inv_loss,\n\t            fwd_loss=fwd_loss,\n\t        )\n\t        int_rews = icm_fwd_losses.clone().cpu().numpy()\n\t        return int_rews, last_mems\n\t    def optimize(self, rollout_data, stats_logger):\n\t        actions = rollout_data.actions\n\t        if isinstance(self.action_space, spaces.Discrete):\n\t            actions = rollout_data.actions.long().flatten()\n", "        _, fwd_loss, inv_loss, _ = \\\n\t            self.forward(\n\t                rollout_data.observations,\n\t                rollout_data.new_observations,\n\t                rollout_data.last_model_mems,\n\t                actions,\n\t                rollout_data.episode_dones,\n\t            )\n\t        icm_loss = inv_loss * (1 - self.icm_forward_loss_coef) + \\\n\t                   fwd_loss * self.icm_forward_loss_coef\n", "        stats_logger.add(\n\t            icm_loss=icm_loss,\n\t            inv_loss=inv_loss,\n\t            fwd_loss=fwd_loss,\n\t        )\n\t        # Optimization\n\t        self.model_optimizer.zero_grad()\n\t        icm_loss.backward()\n\t        th.nn.utils.clip_grad_norm_(self.model_params, self.max_grad_norm)\n\t        self.model_optimizer.step()"]}
{"filename": "src/algo/intrinsic_rewards/deir.py", "chunked_list": ["\"\"\"\n\tThis file contains the main scripts of DEIR and plain discriminative models.\n\tWhen the latter is applied for intrinsic reward generation, the conditional\n\tmutual information term proposed in DEIR is disabled.\n\tFor readability, part of code used solely for logging and analysis is omitted.\n\t\"\"\"\n\timport gym\n\timport numpy as np\n\tfrom gym import spaces\n\tfrom numpy.random import Generator\n", "from typing import Dict, Any, List\n\tfrom stable_baselines3.common.preprocessing import get_obs_shape\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom stable_baselines3.common.utils import obs_as_tensor\n\tfrom src.algo.intrinsic_rewards.base_model import IntrinsicRewardBaseModel\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.enum_types import NormType\n\tclass DiscriminatorModel(IntrinsicRewardBaseModel):\n\t    def __init__(\n\t        self,\n", "        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_learning_rate: float = 3e-4,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n\t        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n", "        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        gru_layers: int = 1,\n\t        use_status_predictor: int = 0,\n\t        # Method-specific params\n", "        obs_rng: Optional[Generator] = None,\n\t        dsc_obs_queue_len: int = 0,\n\t        log_dsc_verbose: int = 0,\n\t    ):\n\t        super().__init__(observation_space, action_space, activation_fn, normalize_images,\n\t                         optimizer_class, optimizer_kwargs, max_grad_norm, model_learning_rate,\n\t                         model_cnn_features_extractor_class, model_cnn_features_extractor_kwargs,\n\t                         model_features_dim, model_latents_dim, model_mlp_norm,\n\t                         model_cnn_norm, model_gru_norm, use_model_rnn, model_mlp_layers,\n\t                         gru_layers, use_status_predictor)\n", "        self.obs_rng = obs_rng\n\t        self.dsc_obs_queue_len = dsc_obs_queue_len\n\t        self.log_dsc_verbose = log_dsc_verbose\n\t        self._init_obs_queue()\n\t        self._build()\n\t        self._init_modules()\n\t        self._init_optimizers()\n\t    def _build(self):\n\t        # Build CNN and RNN\n\t        super()._build()\n", "        # Build MLP\n\t        self.model_mlp = DiscriminatorOutputHeads(\n\t            inputs_dim= self.model_features_dim,\n\t            latents_dim= self.model_latents_dim,\n\t            activation_fn = self.activation_fn,\n\t            action_num = self.action_num,\n\t            mlp_norm=self.model_mlp_norm,\n\t            mlp_layers=self.model_mlp_layers,\n\t        )\n\t    def _get_fake_obs(self, curr_obs, next_obs):\n", "        \"\"\"\n\t        In order to prepare negative samples for the discriminative model's training,\n\t        this method randomly selects two fake observations from the observation queue\n\t        and returns the one that differs from the positive training sample. If both are\n\t        identical to a positive sample, then `obs_diff` is 0 and can be used as a signal\n\t        to invalidate that sample when calculating training losses.\n\t        \"\"\"\n\t        queue_len = min(self.obs_queue_filled, self.dsc_obs_queue_len)\n\t        batch_size = curr_obs.shape[0]\n\t        # Randomly select two fake observations from the queue\n", "        random_idx1 = self.obs_rng.integers(low=0, high=queue_len, size=batch_size, dtype=int)\n\t        random_idx2 = self.obs_rng.integers(low=0, high=queue_len, size=batch_size, dtype=int)\n\t        random_obs1 = obs_as_tensor(self.obs_queue[random_idx1], curr_obs.device)\n\t        random_obs2 = obs_as_tensor(self.obs_queue[random_idx2], curr_obs.device)\n\t        # `obs_diff{1,2}`: whether the ture observation at t+1 (`next_obs`)\n\t        #                  differs from the {1st,2nd} fake sample (`random_obs{1,2}`)\n\t        obs_diff1 = th.abs(next_obs - random_obs1).sum((1, 2, 3))\n\t        obs_diff2 = th.abs(next_obs - random_obs2).sum((1, 2, 3))\n\t        obs_diff1 = th.gt(obs_diff1, th.zeros_like(obs_diff1)).long().view(-1, 1, 1, 1)\n\t        obs_diff2 = th.gt(obs_diff2, th.zeros_like(obs_diff2)).long().view(-1, 1, 1, 1)\n", "        obs_diff = th.logical_or(obs_diff1, obs_diff2).long().view(-1, 1, 1, 1)\n\t        # return `random_obs1` when `next_obs` differs from `random_obs1`, otherwise `random_obs2`\n\t        rand_obs = random_obs1 * obs_diff1 + random_obs2 * (1 - obs_diff1)\n\t        return rand_obs, obs_diff\n\t    def _init_obs_queue(self):\n\t        self.obs_shape = get_obs_shape(self.observation_space)\n\t        self.obs_queue_filled = 0\n\t        self.obs_queue_pos = 0\n\t        self.obs_queue = np.zeros((self.dsc_obs_queue_len,) + self.obs_shape, dtype=float)\n\t    def _get_dsc_embeddings(self, curr_obs, next_obs, last_mems, device=None):\n", "        if not isinstance(curr_obs, Tensor):\n\t            curr_obs = obs_as_tensor(curr_obs, device)\n\t            next_obs = obs_as_tensor(next_obs, device)\n\t        # Get CNN embeddings\n\t        curr_cnn_embs = self._get_cnn_embeddings(curr_obs)\n\t        next_cnn_embs = self._get_cnn_embeddings(next_obs)\n\t        # If RNN enabled\n\t        if self.use_model_rnn:\n\t            curr_mems = self._get_rnn_embeddings(last_mems, curr_cnn_embs, self.model_rnns)\n\t            next_mems = self._get_rnn_embeddings(curr_mems, next_cnn_embs, self.model_rnns)\n", "            curr_rnn_embs = th.squeeze(curr_mems[:, -1, :])\n\t            next_rnn_embs = th.squeeze(next_mems[:, -1, :])\n\t            return curr_cnn_embs, next_cnn_embs, curr_rnn_embs, next_rnn_embs, curr_mems\n\t        # If RNN disabled\n\t        return curr_cnn_embs, next_cnn_embs, curr_cnn_embs, next_cnn_embs, None\n\t    def _get_training_losses(self,\n\t        curr_obs: Tensor, next_obs: Tensor, last_mems: Tensor,\n\t        curr_act: Tensor, curr_dones: Tensor,\n\t        obs_diff: Tensor, labels: Tensor,\n\t        key_status: Optional[Tensor],\n", "        door_status: Optional[Tensor],\n\t        target_dists: Optional[Tensor],\n\t    ):\n\t        # Count valid samples in a batch. A transition (o_t, a_t, o_t+1) is deemed invalid if:\n\t        # 1) an episode ends at t+1, or 2) the ture sample is identical to the fake sample selected at t+1\n\t        n_half_batch = curr_dones.shape[0] // 2\n\t        valid_pos_samples = (1 - curr_dones[n_half_batch:].view(-1)).long()\n\t        valid_neg_samples = th.logical_and(valid_pos_samples, obs_diff.view(-1)).long()\n\t        n_valid_pos_samples = valid_pos_samples.sum().long().item()\n\t        n_valid_neg_samples = valid_neg_samples.sum().long().item()\n", "        n_valid_samples = n_valid_pos_samples + n_valid_neg_samples\n\t        pos_loss_factor = 1 / n_valid_pos_samples if n_valid_pos_samples > 0 else 0.0\n\t        neg_loss_factor = 1 / n_valid_neg_samples if n_valid_neg_samples > 0 else 0.0\n\t        # Get discriminator embeddings\n\t        _, _, curr_embs, next_embs, _ = self._get_dsc_embeddings(curr_obs, next_obs, last_mems)\n\t        # Get likelihoods\n\t        likelihoods = self.model_mlp(curr_embs, next_embs, curr_act).view(-1)\n\t        likelihoods = th.sigmoid(likelihoods).view(-1)\n\t        # Discriminator loss\n\t        pos_dsc_losses = F.binary_cross_entropy(likelihoods[:n_half_batch], labels[:n_half_batch], reduction='none')\n", "        neg_dsc_losses = F.binary_cross_entropy(likelihoods[n_half_batch:], labels[n_half_batch:], reduction='none')\n\t        pos_dsc_loss = (pos_dsc_losses.view(-1) * valid_pos_samples).sum() * pos_loss_factor\n\t        neg_dsc_loss = (neg_dsc_losses.view(-1) * valid_neg_samples).sum() * neg_loss_factor\n\t        # Balance positive and negative samples\n\t        if 0 < n_valid_pos_samples < n_valid_neg_samples:\n\t            pos_dsc_loss *= n_valid_neg_samples / n_valid_pos_samples\n\t        if 0 < n_valid_neg_samples < n_valid_pos_samples:\n\t            neg_dsc_loss *= n_valid_pos_samples / n_valid_neg_samples\n\t        # Get discriminator loss\n\t        dsc_loss = (pos_dsc_loss + neg_dsc_loss) * 0.5\n", "        if self.log_dsc_verbose:\n\t            with th.no_grad():\n\t                pos_avg_likelihood = (likelihoods[:n_half_batch].view(-1) * valid_pos_samples).sum() * pos_loss_factor\n\t                neg_avg_likelihood = (likelihoods[n_half_batch:].view(-1) * valid_neg_samples).sum() * neg_loss_factor\n\t                avg_likelihood = (pos_avg_likelihood + neg_avg_likelihood) * 0.5\n\t                dsc_accuracy = 1 - th.abs(likelihoods - labels).sum() / likelihoods.shape[0]\n\t        else:\n\t            avg_likelihood, pos_avg_likelihood, neg_avg_likelihood, dsc_accuracy = None, None, None, None\n\t        if self.use_status_predictor:\n\t            key_loss, door_loss, pos_loss, key_dist, door_dist, goal_dist = \\\n", "                self._get_status_prediction_losses(curr_embs, key_status, door_status, target_dists)\n\t        else:\n\t            key_loss, door_loss, pos_loss, key_dist, door_dist, goal_dist = [self.constant_zero] * 6\n\t        return dsc_loss, pos_dsc_loss, neg_dsc_loss, \\\n\t               key_loss, door_loss, pos_loss, \\\n\t               key_dist, door_dist, goal_dist, \\\n\t               n_valid_samples, n_valid_pos_samples, n_valid_neg_samples, \\\n\t               avg_likelihood, pos_avg_likelihood, neg_avg_likelihood, dsc_accuracy\n\t    def _add_obs(self, obs):\n\t        \"\"\"\n", "        Add one new element into the observation queue.\n\t        \"\"\"\n\t        self.obs_queue[self.obs_queue_pos] = np.copy(obs)\n\t        self.obs_queue_filled += 1\n\t        self.obs_queue_pos += 1\n\t        self.obs_queue_pos %= self.dsc_obs_queue_len\n\t    def init_obs_queue(self, obs_arr):\n\t        \"\"\"\n\t        In order to ensure the observation queue is not empty on training start\n\t        by adding all observations received at time step 0.\n", "        \"\"\"\n\t        for obs in obs_arr:\n\t            self._add_obs(obs)\n\t    def update_obs_queue(self, iteration, intrinsic_rewards, ir_mean, new_obs, stats_logger):\n\t        \"\"\"\n\t        Update the observation queue after generating the intrinsic rewards for\n\t        the current RL rollout.\n\t        \"\"\"\n\t        for env_id in range(new_obs.shape[0]):\n\t            if iteration == 0 or intrinsic_rewards[env_id] >= ir_mean:\n", "                obs = new_obs[env_id]\n\t                self._add_obs(obs)\n\t                stats_logger.add(obs_insertions=1)\n\t            else:\n\t                stats_logger.add(obs_insertions=0)\n\t    def get_intrinsic_rewards(self,\n\t        curr_obs: Tensor, next_obs: Tensor, last_mems: Tensor,\n\t        obs_history: List, trj_history: List, plain_dsc: bool = False\n\t    ):\n\t        # Get observation and trajectory embeddings at t and t+1\n", "        with th.no_grad():\n\t            curr_cnn_embs, next_cnn_embs, curr_rnn_embs, next_rnn_embs, model_mems = \\\n\t                self._get_dsc_embeddings(curr_obs, next_obs, last_mems)\n\t        # Create IRs by the discriminator (Algorithm A1 in the Technical Appendix)\n\t        batch_size = curr_obs.shape[0]\n\t        int_rews = np.zeros(batch_size, dtype=np.float32)\n\t        for env_id in range(batch_size):\n\t            # Update the episodic history of observation embeddings\n\t            curr_obs_emb = curr_cnn_embs[env_id].view(1, -1)\n\t            next_obs_emb = next_cnn_embs[env_id].view(1, -1)\n", "            obs_embs = obs_history[env_id]\n\t            new_embs = [curr_obs_emb, next_obs_emb] if obs_embs is None else [obs_embs, next_obs_emb]\n\t            obs_embs = th.cat(new_embs, dim=0)\n\t            obs_history[env_id] = obs_embs\n\t            obs_dists = self.calc_euclidean_dists(obs_embs[:-1], obs_embs[-1])\n\t            # Update the episodic history of trajectory embeddings\n\t            if curr_rnn_embs is not None:\n\t                curr_trj_emb = curr_rnn_embs[env_id].view(1, -1)\n\t                next_trj_emb = next_rnn_embs[env_id].view(1, -1)\n\t                trj_embs = trj_history[env_id]\n", "                new_embs = [th.zeros_like(curr_trj_emb), curr_trj_emb, next_trj_emb] if trj_embs is None else [trj_embs, next_trj_emb]\n\t                trj_embs = th.cat(new_embs, dim=0)\n\t                trj_history[env_id] = trj_embs\n\t                trj_dists = self.calc_euclidean_dists(trj_embs[:-2], trj_embs[-2])\n\t            else:\n\t                trj_dists = th.ones_like(obs_dists)\n\t            # Generate intrinsic reward\n\t            if not plain_dsc:\n\t                # DEIR: Equation 4 in the main paper\n\t                deir_dists = th.pow(obs_dists, 2.0) / (trj_dists + 1e-6)\n", "                int_rews[env_id] += deir_dists.min().item()\n\t            else:\n\t                # Plain discriminator (DEIR without the MI term)\n\t                int_rews[env_id] += obs_dists.min().item()\n\t        return int_rews, model_mems\n\t    def optimize(self, rollout_data, stats_logger):\n\t        # Prepare input data\n\t        with th.no_grad():\n\t            actions = rollout_data.actions\n\t            if isinstance(self.action_space, spaces.Discrete):\n", "                actions = rollout_data.actions.long().flatten()\n\t            label_ones = th.ones_like(rollout_data.episode_dones)\n\t            label_zeros = th.zeros_like(rollout_data.episode_dones)\n\t            pred_labels = th.cat([label_ones, label_zeros], dim=0)\n\t            curr_obs = rollout_data.observations\n\t            next_obs = rollout_data.new_observations\n\t            fake_obs, obs_differences = self._get_fake_obs(curr_obs, next_obs)\n\t            key_status, door_status, target_dists = None, None, None\n\t            if self.use_status_predictor:\n\t                key_status = rollout_data.curr_key_status\n", "                door_status = rollout_data.curr_door_status\n\t                target_dists = rollout_data.curr_target_dists\n\t        # Get training losses & analysis metrics\n\t        dsc_loss, pos_dsc_loss, neg_dsc_loss, \\\n\t        key_loss, door_loss, pos_loss, \\\n\t        key_dist, door_dist, goal_dist, \\\n\t        n_valid_samples, n_valid_pos_samples, n_valid_neg_samples, \\\n\t        avg_likelihood, pos_avg_likelihood, neg_avg_likelihood, dsc_accuracy = \\\n\t            self._get_training_losses(\n\t                curr_obs.tile((2, 1, 1, 1)),\n", "                th.cat([next_obs, fake_obs], dim=0),\n\t                rollout_data.last_model_mems.tile((2, 1, 1)),\n\t                actions.tile(2),\n\t                rollout_data.episode_dones.tile((2, 1)).long().view(-1),\n\t                obs_differences,\n\t                pred_labels.float().view(-1),\n\t                key_status,\n\t                door_status,\n\t                target_dists\n\t            )\n", "        # Train the discriminator\n\t        self.model_optimizer.zero_grad()\n\t        dsc_loss.backward()\n\t        th.nn.utils.clip_grad_norm_(self.model_params, self.max_grad_norm)\n\t        self.model_optimizer.step()\n\t        # Train the status predictor(s) for analysis experiments\n\t        if self.use_status_predictor:\n\t            predictor_loss = key_loss + door_loss + pos_loss\n\t            self.predictor_optimizer.zero_grad()\n\t            predictor_loss.backward()\n", "            self.predictor_optimizer.step()\n\t        # Logging\n\t        stats_logger.add(\n\t            dsc_loss=dsc_loss,\n\t            pos_dsc_loss=pos_dsc_loss,\n\t            neg_dsc_loss=neg_dsc_loss,\n\t            avg_likelihood=avg_likelihood,\n\t            pos_avg_likelihood=pos_avg_likelihood,\n\t            neg_avg_likelihood=neg_avg_likelihood,\n\t            dsc_accuracy=dsc_accuracy,\n", "            key_loss=key_loss,\n\t            door_loss=door_loss,\n\t            pos_loss=pos_loss,\n\t            key_dist=key_dist,\n\t            door_dist=door_dist,\n\t            goal_dist=goal_dist,\n\t            n_valid_samples=n_valid_samples,\n\t            n_valid_pos_samples=n_valid_pos_samples,\n\t            n_valid_neg_samples=n_valid_neg_samples,\n\t        )\n"]}
{"filename": "src/algo/intrinsic_rewards/plain_forward.py", "chunked_list": ["import gym\n\tfrom typing import Dict, Any\n\timport numpy as np\n\tfrom gym import spaces\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom src.algo.intrinsic_rewards.base_model import IntrinsicRewardBaseModel\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.enum_types import NormType\n\tclass PlainForwardModel(IntrinsicRewardBaseModel):\n\t    def __init__(\n", "        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_learning_rate: float = 3e-4,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n", "        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        gru_layers: int = 1,\n\t        use_status_predictor: int = 0,\n", "    ):\n\t        super().__init__(observation_space, action_space, activation_fn, normalize_images,\n\t                         optimizer_class, optimizer_kwargs, max_grad_norm, model_learning_rate,\n\t                         model_cnn_features_extractor_class, model_cnn_features_extractor_kwargs,\n\t                         model_features_dim, model_latents_dim, model_mlp_norm,\n\t                         model_cnn_norm, model_gru_norm, use_model_rnn, model_mlp_layers,\n\t                         gru_layers, use_status_predictor)\n\t        self._build()\n\t        self._init_modules()\n\t        self._init_optimizers()\n", "    def _build(self) -> None:\n\t        # Build CNN and RNN\n\t        super()._build()\n\t        # Build MLP\n\t        self.model_mlp = ForwardModelOutputHeads(\n\t            feature_dim=self.model_features_dim,\n\t            latent_dim=self.model_latents_dim,\n\t            activation_fn=self.activation_fn,\n\t            action_num=self.action_num,\n\t            mlp_norm=self.model_mlp_norm,\n", "            mlp_layers=self.model_mlp_layers,\n\t        )\n\t    def forward(self,\n\t        curr_obs: Tensor, next_obs: Tensor, last_mems: Tensor,\n\t        curr_act: Tensor, curr_dones: Tensor,\n\t        curr_key_status: Optional[Tensor],\n\t        curr_door_status: Optional[Tensor],\n\t        curr_target_dists: Optional[Tensor],\n\t    ):\n\t        # CNN Extractor\n", "        curr_cnn_embs = self._get_cnn_embeddings(curr_obs)\n\t        next_cnn_embs = self._get_cnn_embeddings(next_obs)\n\t        if self.use_model_rnn:\n\t            curr_mems = self._get_rnn_embeddings(last_mems, curr_cnn_embs, self.model_rnns)\n\t            next_mems = self._get_rnn_embeddings(curr_mems, next_cnn_embs, self.model_rnns)\n\t            curr_rnn_embs = th.squeeze(curr_mems[:, -1, :])\n\t            next_rnn_embs = th.squeeze(next_mems[:, -1, :])\n\t            curr_embs = curr_rnn_embs\n\t            next_embs = next_rnn_embs\n\t        else:\n", "            curr_embs = curr_cnn_embs\n\t            next_embs = next_cnn_embs\n\t            curr_mems = None\n\t        # Forward model\n\t        pred_embs = self.model_mlp(curr_embs, curr_act)\n\t        # Intrinsic reward\n\t        curr_dones = curr_dones.view(-1)\n\t        n_samples = (1 - curr_dones).sum()\n\t        fwd_losses = F.mse_loss(pred_embs, next_embs, reduction='none')\n\t        fwd_losses = fwd_losses.mean(-1) * (1 - curr_dones)\n", "        fwd_loss = fwd_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        key_loss, door_loss, pos_loss, \\\n\t        key_dist, door_dist, goal_dist = \\\n\t            self._get_status_prediction_losses(\n\t                curr_embs, curr_key_status, curr_door_status, curr_target_dists\n\t            )\n\t        return fwd_loss, \\\n\t            key_loss, door_loss, pos_loss, \\\n\t            key_dist, door_dist, goal_dist, \\\n\t            curr_cnn_embs, next_cnn_embs, \\\n", "            curr_embs, next_embs, curr_mems\n\t    def get_intrinsic_rewards(self,\n\t        curr_obs, next_obs, last_mems, curr_act, curr_dones, obs_history,\n\t        key_status, door_status, target_dists, stats_logger\n\t    ):\n\t        with th.no_grad():\n\t            fwd_loss, \\\n\t            key_loss, door_loss, pos_loss, \\\n\t            key_dist, door_dist, goal_dist, \\\n\t            fwd_curr_cnn_embs, fwd_next_cnn_embs, \\\n", "            _, _, model_mems = \\\n\t                self.forward(\n\t                    curr_obs, next_obs, last_mems,\n\t                    curr_act, curr_dones,\n\t                    key_status, door_status, target_dists\n\t                )\n\t        batch_size = curr_obs.shape[0]\n\t        int_rews = np.zeros(batch_size, dtype=np.float32)\n\t        for env_id in range(batch_size):\n\t            # Update historical observation embeddings\n", "            curr_obs_emb = fwd_curr_cnn_embs[env_id].view(1, -1)\n\t            next_obs_emb = fwd_next_cnn_embs[env_id].view(1, -1)\n\t            obs_embs = obs_history[env_id]\n\t            new_embs = [curr_obs_emb, next_obs_emb] if obs_embs is None else [obs_embs, next_obs_emb]\n\t            obs_embs = th.cat(new_embs, dim=0)\n\t            obs_history[env_id] = obs_embs\n\t            # Generate intrinsic reward\n\t            obs_dists = self.calc_euclidean_dists(obs_embs[:-1], obs_embs[-1])\n\t            int_rews[env_id] += obs_dists.min().item()\n\t        # Logging\n", "        stats_logger.add(\n\t            fwd_loss=fwd_loss,\n\t            key_loss=key_loss,\n\t            door_loss=door_loss,\n\t            pos_loss=pos_loss,\n\t            key_dist=key_dist,\n\t            door_dist=door_dist,\n\t            goal_dist=goal_dist,\n\t        )\n\t        return int_rews, model_mems\n", "    def optimize(self, rollout_data, stats_logger):\n\t        actions = rollout_data.actions\n\t        if isinstance(self.action_space, spaces.Discrete):\n\t            actions = rollout_data.actions.long().flatten()\n\t        if self.use_status_predictor:\n\t            curr_key_status = rollout_data.curr_key_status\n\t            curr_door_status = rollout_data.curr_door_status\n\t            curr_target_dists = rollout_data.curr_target_dists\n\t        else:\n\t            curr_key_status = None\n", "            curr_door_status = None\n\t            curr_target_dists = None\n\t        fwd_loss, \\\n\t        key_loss, door_loss, pos_loss, \\\n\t        key_dist, door_dist, goal_dist, \\\n\t        _, _, _, _, _ = \\\n\t            self.forward(\n\t                rollout_data.observations,\n\t                rollout_data.new_observations,\n\t                rollout_data.last_model_mems,\n", "                actions,\n\t                rollout_data.episode_dones,\n\t                curr_key_status,\n\t                curr_door_status,\n\t                curr_target_dists,\n\t            )\n\t        forward_loss = fwd_loss\n\t        self.model_optimizer.zero_grad()\n\t        forward_loss.backward()\n\t        th.nn.utils.clip_grad_norm_(self.model_params, self.max_grad_norm)\n", "        self.model_optimizer.step()\n\t        if self.use_status_predictor:\n\t            predictor_loss = key_loss + door_loss + pos_loss\n\t            self.predictor_optimizer.zero_grad()\n\t            predictor_loss.backward()\n\t            self.predictor_optimizer.step()\n\t        stats_logger.add(\n\t            fwd_loss=fwd_loss,\n\t            key_loss=key_loss,\n\t            door_loss=door_loss,\n", "            pos_loss=pos_loss,\n\t            key_dist=key_dist,\n\t            door_dist=door_dist,\n\t            goal_dist=goal_dist,\n\t        )"]}
{"filename": "src/algo/intrinsic_rewards/plain_inverse.py", "chunked_list": ["import gym\n\tfrom typing import Dict, Any\n\timport numpy as np\n\tfrom gym import spaces\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom src.algo.intrinsic_rewards.base_model import IntrinsicRewardBaseModel\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.enum_types import NormType\n\tclass PlainInverseModel(IntrinsicRewardBaseModel):\n\t    def __init__(\n", "        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_learning_rate: float = 3e-4,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n", "        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        gru_layers: int = 1,\n\t        use_status_predictor: int = 0,\n", "    ):\n\t        super().__init__(observation_space, action_space, activation_fn, normalize_images,\n\t                         optimizer_class, optimizer_kwargs, max_grad_norm, model_learning_rate,\n\t                         model_cnn_features_extractor_class, model_cnn_features_extractor_kwargs,\n\t                         model_features_dim, model_latents_dim, model_mlp_norm,\n\t                         model_cnn_norm, model_gru_norm, use_model_rnn, model_mlp_layers,\n\t                         gru_layers, use_status_predictor)\n\t        self._build()\n\t        self._init_modules()\n\t        self._init_optimizers()\n", "    def _build(self) -> None:\n\t        # Build CNN and RNN\n\t        super()._build()\n\t        # Build MLP\n\t        self.model_mlp = InverseModelOutputHeads(\n\t            features_dim=self.model_features_dim,\n\t            latents_dim=self.model_latents_dim,\n\t            activation_fn=self.activation_fn,\n\t            action_num=self.action_num,\n\t            mlp_norm=self.model_mlp_norm,\n", "            mlp_layers=self.model_mlp_layers,\n\t        )\n\t    def forward(self,\n\t        curr_obs: Tensor, next_obs: Tensor, last_mems: Tensor,\n\t        curr_act: Tensor, curr_dones: Tensor,\n\t        curr_key_status: Optional[Tensor],\n\t        curr_door_status: Optional[Tensor],\n\t        curr_agent_pos: Optional[Tensor],\n\t    ):\n\t        # CNN Extractor\n", "        curr_cnn_embs = self._get_cnn_embeddings(curr_obs)\n\t        next_cnn_embs = self._get_cnn_embeddings(next_obs)\n\t        if self.use_model_rnn:\n\t            curr_mems = self._get_rnn_embeddings(last_mems, curr_cnn_embs, self.model_rnns)\n\t            next_mems = self._get_rnn_embeddings(curr_mems, next_cnn_embs, self.model_rnns)\n\t            curr_rnn_embs = th.squeeze(curr_mems[:, -1, :])\n\t            next_rnn_embs = th.squeeze(next_mems[:, -1, :])\n\t            curr_embs = curr_rnn_embs\n\t            next_embs = next_rnn_embs\n\t        else:\n", "            curr_embs = curr_cnn_embs\n\t            next_embs = next_cnn_embs\n\t            curr_mems = None\n\t        # Inverse model\n\t        pred_act = self.model_mlp(curr_embs, next_embs)\n\t        # Inverse loss\n\t        curr_dones = curr_dones.view(-1)\n\t        n_samples = (1 - curr_dones).sum()\n\t        inv_losses = F.cross_entropy(pred_act, curr_act, reduction='none') * (1 - curr_dones)\n\t        inv_loss = inv_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n", "        key_loss, door_loss, pos_loss, \\\n\t        key_dist, door_dist, goal_dist = \\\n\t        self._get_status_prediction_losses(\n\t            curr_embs, curr_key_status, curr_door_status, curr_agent_pos\n\t        )\n\t        return inv_loss, \\\n\t            key_loss, door_loss, pos_loss, \\\n\t            key_dist, door_dist, goal_dist, \\\n\t            curr_cnn_embs, next_cnn_embs, \\\n\t            curr_embs, next_embs, curr_mems\n", "    def get_intrinsic_rewards(self,\n\t        curr_obs, next_obs, last_mems, curr_act, curr_dones, obs_history,\n\t        key_status, door_status, target_dists, stats_logger\n\t    ):\n\t        with th.no_grad():\n\t            inv_loss, \\\n\t            key_loss, door_loss, pos_loss, \\\n\t            key_dist, door_dist, goal_dist, \\\n\t            inv_curr_cnn_embs, inv_next_cnn_embs, \\\n\t            _, _, model_mems = \\\n", "                self.forward(\n\t                    curr_obs, next_obs, last_mems,\n\t                    curr_act, curr_dones,\n\t                    key_status, door_status, target_dists\n\t                )\n\t        batch_size = curr_obs.shape[0]\n\t        int_rews = np.zeros(batch_size, dtype=np.float32)\n\t        for env_id in range(batch_size):\n\t            # Update historical observation embeddings\n\t            curr_obs_emb = inv_curr_cnn_embs[env_id].view(1, -1)\n", "            next_obs_emb = inv_next_cnn_embs[env_id].view(1, -1)\n\t            obs_embs = obs_history[env_id]\n\t            new_embs = [curr_obs_emb, next_obs_emb] if obs_embs is None else [obs_embs, next_obs_emb]\n\t            obs_embs = th.cat(new_embs, dim=0)\n\t            obs_history[env_id] = obs_embs\n\t            # Generate intrinsic reward\n\t            obs_dists = self.calc_euclidean_dists(obs_embs[:-1], obs_embs[-1])\n\t            int_rews[env_id] += obs_dists.min().item()\n\t        # Logging\n\t        stats_logger.add(\n", "            inv_loss=inv_loss,\n\t            key_loss=key_loss,\n\t            door_loss=door_loss,\n\t            pos_loss=pos_loss,\n\t            key_dist=key_dist,\n\t            door_dist=door_dist,\n\t            goal_dist=goal_dist,\n\t        )\n\t        return int_rews, model_mems\n\t    def optimize(self, rollout_data, stats_logger):\n", "        actions = rollout_data.actions\n\t        if isinstance(self.action_space, spaces.Discrete):\n\t            actions = rollout_data.actions.long().flatten()\n\t        if self.use_status_predictor:\n\t            curr_key_status = rollout_data.curr_key_status\n\t            curr_door_status = rollout_data.curr_door_status\n\t            curr_target_dists = rollout_data.curr_target_dists\n\t        else:\n\t            curr_key_status = None\n\t            curr_door_status = None\n", "            curr_target_dists = None\n\t        inv_loss, \\\n\t        key_loss, door_loss, pos_loss, \\\n\t        key_dist, door_dist, goal_dist, \\\n\t        _, _, _, _, _ = \\\n\t            self.forward(\n\t                rollout_data.observations,\n\t                rollout_data.new_observations,\n\t                rollout_data.last_model_mems,\n\t                actions,\n", "                rollout_data.episode_dones,\n\t                curr_key_status,\n\t                curr_door_status,\n\t                curr_target_dists,\n\t            )\n\t        stats_logger.add(\n\t            inv_loss=inv_loss,\n\t            key_loss=key_loss,\n\t            door_loss=door_loss,\n\t            pos_loss=pos_loss,\n", "            key_dist=key_dist,\n\t            door_dist=door_dist,\n\t            goal_dist=goal_dist,\n\t        )\n\t        inverse_loss = inv_loss\n\t        self.model_optimizer.zero_grad()\n\t        inverse_loss.backward()\n\t        th.nn.utils.clip_grad_norm_(self.model_params, self.max_grad_norm)\n\t        self.model_optimizer.step()\n\t        if self.use_status_predictor:\n", "            predictor_loss = key_loss + door_loss + pos_loss\n\t            self.predictor_optimizer.zero_grad()\n\t            predictor_loss.backward()\n\t            self.predictor_optimizer.step()\n"]}
{"filename": "src/algo/intrinsic_rewards/rnd.py", "chunked_list": ["import gym\n\tfrom typing import Dict, Any\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom src.algo.intrinsic_rewards.base_model import IntrinsicRewardBaseModel\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.common_func import normalize_rewards\n\tfrom src.utils.enum_types import NormType\n\tfrom src.utils.running_mean_std import RunningMeanStd\n\tclass RNDModel(IntrinsicRewardBaseModel):\n\t    def __init__(\n", "        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_learning_rate: float = 3e-4,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n", "        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        gru_layers: int = 1,\n\t        use_status_predictor: int = 0,\n", "        # Method-specific params\n\t        rnd_err_norm: int = 0,\n\t        rnd_err_momentum: float = -1.0,\n\t        rnd_use_policy_emb: int = 1,\n\t        policy_cnn: Type[nn.Module] = None,\n\t        policy_rnns: Type[nn.Module] = None,\n\t    ):\n\t        super().__init__(observation_space, action_space, activation_fn, normalize_images,\n\t                         optimizer_class, optimizer_kwargs, max_grad_norm, model_learning_rate,\n\t                         model_cnn_features_extractor_class, model_cnn_features_extractor_kwargs,\n", "                         model_features_dim, model_latents_dim, model_mlp_norm,\n\t                         model_cnn_norm, model_gru_norm, use_model_rnn, model_mlp_layers,\n\t                         gru_layers, use_status_predictor)\n\t        self.policy_cnn = policy_cnn\n\t        self.policy_rnns = policy_rnns\n\t        self.rnd_use_policy_emb = rnd_use_policy_emb\n\t        self.rnd_err_norm = rnd_err_norm\n\t        self.rnd_err_momentum = rnd_err_momentum\n\t        self.rnd_err_running_stats = RunningMeanStd(momentum=self.rnd_err_momentum)\n\t        self._build()\n", "        self._init_modules()\n\t        self._init_optimizers()\n\t    def _build(self) -> None:\n\t        # Build CNN and RNN\n\t        super()._build()\n\t        # Build MLP\n\t        self.model_mlp = RNDOutputHeads(\n\t            features_dim = self.model_features_dim,\n\t            latents_dim = self.model_latents_dim,\n\t            outputs_dim = self.model_latents_dim,\n", "            activation_fn = self.activation_fn,\n\t            mlp_norm = self.model_mlp_norm,\n\t            mlp_layers = self.model_mlp_layers,\n\t        )\n\t    def forward(self, curr_obs: Tensor, last_mems: Tensor, curr_dones: Optional[Tensor]):\n\t        curr_mlp_inputs, curr_mems = self._get_rnd_embeddings(curr_obs, last_mems)\n\t        tgt, prd = self.model_mlp(curr_mlp_inputs)\n\t        rnd_losses = F.mse_loss(prd, tgt.detach(), reduction='none').mean(-1)\n\t        curr_dones = curr_dones.view(-1)\n\t        n_samples = (1 - curr_dones).sum()\n", "        rnd_loss = rnd_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        return rnd_loss, rnd_losses, curr_mems\n\t    def get_intrinsic_rewards(self, curr_obs, last_mems, curr_dones, stats_logger):\n\t        with th.no_grad():\n\t            rnd_loss, rnd_losses, model_mems = \\\n\t                self.forward(curr_obs, last_mems, curr_dones)\n\t        rnd_rewards = rnd_losses.clone().cpu().numpy()\n\t        if self.rnd_err_norm > 0:\n\t            # Normalize RND error per step\n\t            self.rnd_err_running_stats.update(rnd_rewards)\n", "            rnd_rewards = normalize_rewards(\n\t                norm_type=self.rnd_err_norm,\n\t                rewards=rnd_rewards,\n\t                mean=self.rnd_err_running_stats.mean,\n\t                std=self.rnd_err_running_stats.std,\n\t            )\n\t        stats_logger.add(rnd_loss=rnd_loss)\n\t        return rnd_rewards, model_mems\n\t    def optimize(self, rollout_data, stats_logger):\n\t        rnd_loss, _, _ = \\\n", "            self.forward(\n\t                rollout_data.observations,\n\t                rollout_data.last_model_mems,\n\t                rollout_data.episode_dones,\n\t            )\n\t        stats_logger.add(rnd_loss=rnd_loss)\n\t        # Optimization\n\t        self.model_optimizer.zero_grad()\n\t        rnd_loss.backward()\n\t        th.nn.utils.clip_grad_norm_(self.model_params, self.max_grad_norm)\n", "        self.model_optimizer.step()"]}
{"filename": "src/algo/intrinsic_rewards/ngu.py", "chunked_list": ["\"\"\"\n\t[NOTICE] As specified in Appendix A.5 of our arXiv preprint, this implementation of \n\tNGU is NOT COMPLETELY IDENTICAL to the original implementation. It is because the \n\tlearning framework we adopted for training and comparing all exploration methods is \n\tmore lightweight than the originally proposed one. However, we confirmed that every \n\tstep of the original algorithm for intrinsic reward generation was followed in \n\tour reproduction (according to the original definition given in NGU’s paper).\n\t\"\"\"\n\timport gym\n\tfrom typing import Dict, Any\n", "import numpy as np\n\tfrom gym import spaces\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom src.algo.intrinsic_rewards.base_model import IntrinsicRewardBaseModel\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.common_func import normalize_rewards\n\tfrom src.utils.enum_types import NormType\n\tfrom src.utils.running_mean_std import RunningMeanStd\n\tclass NGUModel(IntrinsicRewardBaseModel):\n\t    def __init__(\n", "        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_learning_rate: float = 3e-4,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n", "        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        gru_layers: int = 1,\n\t        use_status_predictor: int = 0,\n", "        # Method-specific params\n\t        ngu_knn_k: int = 10,\n\t        ngu_dst_momentum: float = 0.997,\n\t        ngu_use_rnd: int = 1,\n\t        rnd_err_norm: int = 0,\n\t        rnd_err_momentum: float = -1,\n\t        rnd_use_policy_emb: int = 1,\n\t        policy_cnn: Type[nn.Module] = None,\n\t        policy_rnns: Type[nn.Module] = None,\n\t    ):\n", "        super().__init__(observation_space, action_space, activation_fn, normalize_images,\n\t                         optimizer_class, optimizer_kwargs, max_grad_norm, model_learning_rate,\n\t                         model_cnn_features_extractor_class, model_cnn_features_extractor_kwargs,\n\t                         model_features_dim, model_latents_dim, model_mlp_norm,\n\t                         model_cnn_norm, model_gru_norm, use_model_rnn, model_mlp_layers,\n\t                         gru_layers, use_status_predictor)\n\t        self.policy_cnn = policy_cnn\n\t        self.policy_rnns = policy_rnns\n\t        self.ngu_knn_k = ngu_knn_k\n\t        self.ngu_use_rnd = ngu_use_rnd\n", "        self.ngu_dst_momentum = ngu_dst_momentum\n\t        self.ngu_moving_avg_dists = RunningMeanStd(momentum=self.ngu_dst_momentum)\n\t        self.rnd_use_policy_emb = rnd_use_policy_emb\n\t        self.rnd_err_norm = rnd_err_norm\n\t        self.rnd_err_momentum = rnd_err_momentum\n\t        self.rnd_err_running_stats = RunningMeanStd(momentum=self.rnd_err_momentum)\n\t        self._build()\n\t        self._init_modules()\n\t        self._init_optimizers()\n\t    def _build(self) -> None:\n", "        # Build CNN and RNN\n\t        super()._build()\n\t        # Build MLP\n\t        self.model_mlp = NGUOutputHeads(\n\t            features_dim = self.model_features_dim,\n\t            latents_dim = self.model_latents_dim,\n\t            activation_fn = self.activation_fn,\n\t            action_num = self.action_num,\n\t            mlp_norm = self.model_mlp_norm,\n\t            mlp_layers = self.model_mlp_layers,\n", "            use_rnd=self.ngu_use_rnd,\n\t        )\n\t    def forward(self,\n\t        curr_obs: Tensor, next_obs: Tensor, last_mems: Tensor,\n\t        curr_act: Tensor, curr_dones: Tensor\n\t    ):\n\t        curr_cnn_embs = self._get_cnn_embeddings(curr_obs)\n\t        next_cnn_embs = self._get_cnn_embeddings(next_obs)\n\t        # Get RNN memories\n\t        if self.use_model_rnn:\n", "            curr_mems = self._get_rnn_embeddings(last_mems, curr_cnn_embs, self.model_rnns)\n\t            next_mems = self._get_rnn_embeddings(curr_mems, next_cnn_embs, self.model_rnns)\n\t            curr_rnn_embs = th.squeeze(curr_mems[:, -1, :])\n\t            next_rnn_embs = th.squeeze(next_mems[:, -1, :])\n\t            curr_embs = curr_rnn_embs\n\t            next_embs = next_rnn_embs\n\t        else:\n\t            curr_embs = curr_cnn_embs\n\t            next_embs = next_cnn_embs\n\t            curr_mems = None\n", "        pred_act = self.model_mlp.inverse_forward(curr_embs, next_embs)\n\t        # Inverse model\n\t        curr_dones = curr_dones.view(-1)\n\t        n_samples = (1 - curr_dones).sum()\n\t        inv_losses = F.cross_entropy(pred_act, curr_act, reduction='none') * (1 - curr_dones)\n\t        inv_loss = inv_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        # RND\n\t        rnd_losses, rnd_loss, _ = None, None, None\n\t        if self.ngu_use_rnd:\n\t            curr_rnd_embs, _ = self._get_rnd_embeddings(curr_obs, last_mems)\n", "            tgt_out, prd_out = self.model_mlp.rnd_forward(curr_rnd_embs)\n\t            rnd_losses = F.mse_loss(prd_out, tgt_out.detach(), reduction='none').mean(-1) * (1 - curr_dones)\n\t            rnd_loss = rnd_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        return inv_loss, curr_embs, next_embs, rnd_losses, rnd_loss, curr_mems\n\t    def get_intrinsic_rewards(self, curr_obs, next_obs, last_mems, curr_act, curr_dones, obs_history, stats_logger):\n\t        with th.no_grad():\n\t            inv_loss, ngu_curr_embs, ngu_next_embs, ngu_rnd_losses, ngu_rnd_loss, _ = \\\n\t                self.forward(curr_obs, next_obs, last_mems, curr_act, curr_dones)\n\t            stats_logger.add(inv_loss=inv_loss)\n\t            if ngu_rnd_losses is not None:\n", "                ngu_rnd_error = ngu_rnd_losses.clone().cpu().numpy()\n\t                if self.rnd_err_norm > 0:\n\t                    # Normalize RND error per step\n\t                    self.rnd_err_running_stats.update(ngu_rnd_error)\n\t                    ngu_rnd_error = normalize_rewards(\n\t                        norm_type=self.rnd_err_norm,\n\t                        rewards=ngu_rnd_error,\n\t                        mean=self.rnd_err_running_stats.mean,\n\t                        std=self.rnd_err_running_stats.std,\n\t                    )\n", "                ngu_lifelong_rewards = ngu_rnd_error + 1\n\t                stats_logger.add(rnd_loss=ngu_rnd_loss)\n\t        # Create IRs\n\t        batch_size = curr_obs.shape[0]\n\t        int_rews = np.zeros(batch_size, dtype=np.float32)\n\t        for env_id in range(batch_size):\n\t            # Update historical observation embeddings\n\t            curr_emb = ngu_curr_embs[env_id].view(1, -1)\n\t            next_emb = ngu_next_embs[env_id].view(1, -1)\n\t            obs_embs = obs_history[env_id]\n", "            new_embs = [curr_emb, next_emb] if obs_embs is None else [obs_embs, next_emb]\n\t            obs_embs = th.cat(new_embs, dim=0)\n\t            obs_history[env_id] = obs_embs\n\t            # Implemented based on the paper of NGU (Algorithm 1)\n\t            episodic_reward = 0.0\n\t            if obs_embs.shape[0] > 1:\n\t                # Compute the k-nearest neighbours of f (x_t) in M and store them in a list N_k\n\t                # - d is the Euclidean distance and\n\t                # - d^2_m is a running average of the squared Euclidean distance of the k-nearest neighbors.\n\t                knn_dists = self.calc_euclidean_dists(obs_embs[:-1], obs_embs[-1]) ** 2\n", "                knn_dists = knn_dists.clone().cpu().numpy()\n\t                knn_dists = np.sort(knn_dists)[:self.ngu_knn_k]\n\t                # Update the moving average d^2_m with the list of distances d_k\n\t                self.ngu_moving_avg_dists.update(knn_dists)\n\t                moving_avg_dist = self.ngu_moving_avg_dists.mean\n\t                # Normalize the distances d_k with the updated moving average d^2_m\n\t                normalized_dists = knn_dists / (moving_avg_dist + 1e-5)\n\t                # Cluster the normalized distances d_n\n\t                # i.e. they become 0 if too small and 0k is a list of k zeros\n\t                normalized_dists = np.maximum(normalized_dists - 0.008, np.zeros_like(knn_dists))\n", "                # Compute the Kernel values between the embedding f (x_t) and its neighbours N_k\n\t                kernel_values = 0.0001 / (normalized_dists + 0.0001)\n\t                # Compute the similarity between the embedding f (x_t) and its neighbours N_k\n\t                simlarity = np.sqrt(kernel_values.sum()) + 0.001\n\t                # Compute the episodic intrinsic reward at time t\n\t                if simlarity <= 8:\n\t                    episodic_reward += 1 / simlarity\n\t            if self.ngu_use_rnd and ngu_lifelong_rewards is not None:\n\t                L = 5.0  # L is a chosen maximum reward scaling (default: 5)\n\t                lifelong_reward = min(max(ngu_lifelong_rewards[env_id], 1.0), L)\n", "                int_rews[env_id] += episodic_reward * lifelong_reward\n\t            else:\n\t                int_rews[env_id] += episodic_reward\n\t        return int_rews, last_mems\n\t    def optimize(self, rollout_data, stats_logger):\n\t        actions = rollout_data.actions\n\t        if isinstance(self.action_space, spaces.Discrete):\n\t            actions = rollout_data.actions.long().flatten()\n\t        inv_loss, _, _, _, rnd_loss, _ = \\\n\t            self.forward(\n", "                rollout_data.observations,\n\t                rollout_data.new_observations,\n\t                rollout_data.last_model_mems,\n\t                actions,\n\t                rollout_data.episode_dones,\n\t            )\n\t        ngu_loss = inv_loss\n\t        stats_logger.add(inv_loss=inv_loss)\n\t        if self.ngu_use_rnd:\n\t            ngu_loss = ngu_loss + rnd_loss\n", "            stats_logger.add(rnd_loss=rnd_loss)\n\t        # Optimization\n\t        self.model_optimizer.zero_grad()\n\t        ngu_loss.backward()\n\t        th.nn.utils.clip_grad_norm_(self.model_params, self.max_grad_norm)\n\t        self.model_optimizer.step()\n"]}
{"filename": "src/algo/intrinsic_rewards/base_model.py", "chunked_list": ["import gym\n\timport numpy as np\n\tfrom torch.nn import GRUCell\n\tfrom typing import Dict, Any, List\n\tfrom stable_baselines3.common.preprocessing import preprocess_obs\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom src.algo.common_models.gru_cell import CustomGRUCell\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.enum_types import NormType\n\tfrom src.utils.common_func import init_module_with_name\n", "class IntrinsicRewardBaseModel(nn.Module):\n\t    def __init__(\n\t        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n", "        model_learning_rate: float = 3e-4,\n\t        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n\t        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n", "        gru_layers: int = 1,\n\t        use_status_predictor: int = 0,\n\t    ):\n\t        super().__init__()\n\t        if isinstance(observation_space, gym.spaces.Dict):\n\t            observation_space = observation_space[\"rgb\"]\n\t        self.observation_space = observation_space\n\t        self.normalize_images = normalize_images\n\t        self.action_space = action_space\n\t        self.action_num = action_space.n\n", "        self.max_grad_norm = max_grad_norm\n\t        self.model_features_dim = model_features_dim\n\t        self.model_latents_dim = model_latents_dim\n\t        self.model_learning_rate = model_learning_rate\n\t        self.model_mlp_norm = model_mlp_norm\n\t        self.model_cnn_norm = model_cnn_norm\n\t        self.model_gru_norm = model_gru_norm\n\t        self.model_mlp_layers = model_mlp_layers\n\t        self.gru_layers = gru_layers\n\t        self.use_status_predictor = use_status_predictor\n", "        self.model_gru_cell = GRUCell if self.model_gru_norm == NormType.NoNorm else CustomGRUCell\n\t        self.use_model_rnn = use_model_rnn\n\t        self.model_cnn_features_extractor_class = model_cnn_features_extractor_class\n\t        self.model_cnn_features_extractor_kwargs = model_cnn_features_extractor_kwargs\n\t        self.activation_fn = activation_fn\n\t        self.optimizer_class = optimizer_class\n\t        self.optimizer_kwargs = optimizer_kwargs\n\t        self.model_rnn_kwargs = dict(\n\t            input_size=self.model_features_dim,\n\t            hidden_size=self.model_features_dim,\n", "        )\n\t        if self.model_gru_norm != NormType.NoNorm:\n\t            self.model_rnn_kwargs.update(dict(\n\t                norm_type=self.model_gru_norm,\n\t            ))\n\t        self.constant_zero = th.zeros(1, dtype=th.float)\n\t        self.constant_one = th.ones(1, dtype=th.float)\n\t    def _build(self) -> None:\n\t        self.model_cnn_features_extractor_kwargs.update(dict(\n\t            features_dim=self.model_features_dim,\n", "        ))\n\t        self.model_cnn_extractor = \\\n\t            self.model_cnn_features_extractor_class(\n\t                self.observation_space,\n\t                **self.model_cnn_features_extractor_kwargs\n\t            )\n\t        # Build RNNs\n\t        self.model_rnns = []\n\t        if self.use_model_rnn:\n\t            for l in range(self.gru_layers):\n", "                name = f'model_rnn_layer_{l}'\n\t                setattr(self, name, self.model_gru_cell(**self.model_rnn_kwargs))\n\t                self.model_rnns.append(getattr(self, name))\n\t        # Build Key/Door status predictors\n\t        if self.use_status_predictor:\n\t            self.model_key_door_status_predictor = nn.Sequential(\n\t                nn.Dropout(p=0.1),\n\t                nn.Linear(self.model_features_dim, self.model_features_dim),\n\t                NormType.get_norm_layer_1d(self.model_mlp_norm, self.model_features_dim),\n\t                self.activation_fn(),\n", "                nn.Dropout(p=0.1),\n\t                nn.Linear(self.model_features_dim, 2),\n\t            )\n\t            self.model_agent_position_predictor = nn.Sequential(\n\t                nn.Dropout(p=0.1),\n\t                nn.Linear(self.model_features_dim, self.model_features_dim),\n\t                NormType.get_norm_layer_1d(self.model_mlp_norm, self.model_features_dim),\n\t                self.activation_fn(),\n\t                nn.Dropout(p=0.1),\n\t                nn.Linear(self.model_features_dim, 3),\n", "            )\n\t        else:\n\t            self.model_key_door_status_predictor = nn.Identity()\n\t            self.model_agent_position_predictor = nn.Identity()\n\t    def _init_modules(self) -> None:\n\t        assert hasattr(self, 'model_mlp'), \"Be sure to define the model's MLP first\"\n\t        module_names = {\n\t            self.model_cnn_extractor: 'model_cnn_extractor',\n\t            self.model_mlp: 'model_mlp',\n\t        }\n", "        if self.use_model_rnn:\n\t            for l in range(self.gru_layers):\n\t                name = f'model_rnn_layer_{l}'\n\t                module = getattr(self, name)\n\t                module_names.update({module: name})\n\t        if self.use_status_predictor:\n\t            module_names.update({\n\t                self.model_key_door_status_predictor: 'model_key_door_status_predictor',\n\t                self.model_agent_position_predictor: 'model_agent_position_predictor',\n\t            })\n", "        for module, name in module_names.items():\n\t            init_module_with_name(name, module)\n\t    def _init_optimizers(self) -> None:\n\t        param_dicts = dict(self.named_parameters(recurse=True)).items()\n\t        self.model_params = [\n\t            param for name, param in param_dicts\n\t            if (name.find('status_predictor') < 0 and name.find('position_predictor') < 0)\n\t        ]\n\t        self.model_optimizer = self.optimizer_class(self.model_params, lr=self.model_learning_rate, **self.optimizer_kwargs)\n\t        if self.use_status_predictor:\n", "            param_dicts = dict(self.named_parameters(recurse=True)).items()\n\t            self.status_predictor_params = [\n\t                param for name, param in param_dicts\n\t                    if name.find('status_predictor') >= 0 or name.find('position_predictor') >= 0\n\t            ]\n\t            self.predictor_optimizer = \\\n\t                self.optimizer_class(self.status_predictor_params, lr=self.model_learning_rate, **self.optimizer_kwargs)\n\t    def _get_rnn_embeddings(self, hiddens: Optional[Tensor], inputs: Tensor, modules: List[nn.Module]):\n\t        outputs = []\n\t        for i, module in enumerate(modules):\n", "            hidden_i = th.squeeze(hiddens[:, i, :])\n\t            output_i = module(inputs, hidden_i)\n\t            inputs = output_i\n\t            outputs.append(output_i)\n\t        outputs = th.stack(outputs, dim=1)\n\t        return outputs\n\t    def _get_cnn_embeddings(self, obs, module=None):\n\t        obs = preprocess_obs(obs, self.observation_space, normalize_images=self.normalize_images)\n\t        if module is None:\n\t            return self.model_cnn_extractor(obs)\n", "        return module(obs)\n\t    def _get_rnd_embeddings(self, obs, mems):\n\t        if self.rnd_use_policy_emb:\n\t            with th.no_grad():\n\t                cnn_embs = self._get_cnn_embeddings(obs, module=self.policy_cnn)\n\t                if self.use_model_rnn:\n\t                    gru_mems = self._get_rnn_embeddings(mems, cnn_embs, self.policy_rnns)\n\t                    rnn_embs = th.squeeze(gru_mems[:, -1, :])\n\t        else:\n\t            cnn_embs = self._get_cnn_embeddings(obs)\n", "            if self.use_model_rnn:\n\t                gru_mems = self._get_rnn_embeddings(mems, cnn_embs, self.model_rnns)\n\t                rnn_embs = th.squeeze(gru_mems[:, -1, :])\n\t        if self.use_model_rnn:\n\t            return rnn_embs, gru_mems\n\t        return cnn_embs, None\n\t    # Key, Door, Agent pos predictor\n\t    def _get_status_prediction_losses(self, embs, key_status, door_status, target_dists):\n\t        if self.use_status_predictor:\n\t            if key_status.shape[0] < embs.shape[0]:\n", "                embs = embs[:key_status.shape[0]]\n\t            # Key / Door status prediction\n\t            pred_status = self.model_key_door_status_predictor(embs.detach()).view(-1, 2)\n\t            pred_key_status = pred_status[:, 0]\n\t            pred_door_status = pred_status[:, 1]\n\t            pred_target_dists = self.model_agent_position_predictor(embs.detach()).view(-1, 3)\n\t            key_loss = F.binary_cross_entropy_with_logits(pred_key_status, key_status.float())\n\t            door_loss = F.binary_cross_entropy_with_logits(pred_door_status, door_status.float())\n\t            # Target distances prediction\n\t            pos_losses = F.mse_loss(pred_target_dists, target_dists.float().view(-1, 3), reduction='none')\n", "            key_dist = pos_losses[:, 0].mean()\n\t            door_dist = pos_losses[:, 1].mean()\n\t            goal_dist = pos_losses[:, 2].mean()\n\t            pos_loss = pos_losses.mean()\n\t        else:\n\t            key_loss = self.constant_zero\n\t            door_loss = self.constant_zero\n\t            pos_loss = self.constant_zero\n\t            key_dist = self.constant_zero\n\t            door_dist = self.constant_zero\n", "            goal_dist = self.constant_zero\n\t        return key_loss, door_loss, pos_loss, \\\n\t               key_dist, door_dist, goal_dist\n\t    @staticmethod\n\t    @th.jit.script\n\t    def calc_euclidean_dists(x : Tensor, y : Tensor):\n\t        \"\"\"\n\t        Calculate the Euclidean distances between two batches of embeddings.\n\t        Input shape: [n, d]\n\t        Return: ((x - y) ** 2).sum(dim=-1) ** 0.5\n", "        \"\"\"\n\t        features_dim = x.shape[-1]\n\t        x = x.view(1, -1, features_dim)\n\t        y = y.view(1, -1, features_dim)\n\t        return th.cdist(x, y, p = 2.0)[0]"]}
{"filename": "src/algo/intrinsic_rewards/noveld.py", "chunked_list": ["import gym\n\tfrom typing import Dict, Any\n\timport numpy as np\n\tfrom stable_baselines3.common.torch_layers import NatureCNN, BaseFeaturesExtractor\n\tfrom src.algo.intrinsic_rewards.base_model import IntrinsicRewardBaseModel\n\tfrom src.algo.common_models.mlps import *\n\tfrom src.utils.common_func import normalize_rewards\n\tfrom src.utils.enum_types import NormType\n\tfrom src.utils.running_mean_std import RunningMeanStd\n\tclass NovelDModel(IntrinsicRewardBaseModel):\n", "    def __init__(\n\t        self,\n\t        observation_space: gym.spaces.Space,\n\t        action_space: gym.spaces.Space,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        normalize_images: bool = True,\n\t        optimizer_class: Type[th.optim.Optimizer] = th.optim.Adam,\n\t        optimizer_kwargs: Optional[Dict[str, Any]] = None,\n\t        max_grad_norm: float = 0.5,\n\t        model_learning_rate: float = 3e-4,\n", "        model_cnn_features_extractor_class: Type[BaseFeaturesExtractor] = NatureCNN,\n\t        model_cnn_features_extractor_kwargs: Optional[Dict[str, Any]] = None,\n\t        model_features_dim: int = 256,\n\t        model_latents_dim: int = 256,\n\t        model_mlp_norm: NormType = NormType.BatchNorm,\n\t        model_cnn_norm: NormType = NormType.BatchNorm,\n\t        model_gru_norm: NormType = NormType.NoNorm,\n\t        use_model_rnn: int = 0,\n\t        model_mlp_layers: int = 1,\n\t        gru_layers: int = 1,\n", "        use_status_predictor: int = 0,\n\t        # Method-specific params\n\t        rnd_err_norm: int = 0,\n\t        rnd_err_momentum: float = -1.0,\n\t        rnd_use_policy_emb: int = 1,\n\t        policy_cnn: Type[nn.Module] = None,\n\t        policy_rnns: Type[nn.Module] = None,\n\t    ):\n\t        super().__init__(observation_space, action_space, activation_fn, normalize_images,\n\t                         optimizer_class, optimizer_kwargs, max_grad_norm, model_learning_rate,\n", "                         model_cnn_features_extractor_class, model_cnn_features_extractor_kwargs,\n\t                         model_features_dim, model_latents_dim, model_mlp_norm,\n\t                         model_cnn_norm, model_gru_norm, use_model_rnn, model_mlp_layers,\n\t                         gru_layers, use_status_predictor)\n\t        self.policy_cnn = policy_cnn\n\t        self.policy_rnns = policy_rnns\n\t        self.rnd_use_policy_emb = rnd_use_policy_emb\n\t        self.rnd_err_norm = rnd_err_norm\n\t        self.rnd_err_momentum = rnd_err_momentum\n\t        self.rnd_err_running_stats = RunningMeanStd(momentum=self.rnd_err_momentum)\n", "        self.noveld_visited_obs = dict()  # A dict of set\n\t        # The following constant values are from the original paper:\n\t        # Results show that α = 0.5 and β = 0 works the best (page 7)\n\t        self.noveld_alpha = 0.5\n\t        self.noveld_beta = 0.0\n\t        self._build()\n\t        self._init_modules()\n\t        self._init_optimizers()\n\t    def _build(self) -> None:\n\t        # Build CNN and RNN\n", "        super()._build()\n\t        # Build MLP\n\t        self.model_mlp = NovelDOutputHeads(\n\t            features_dim = self.model_features_dim,\n\t            latents_dim = self.model_latents_dim,\n\t            activation_fn = self.activation_fn,\n\t            mlp_norm = self.model_mlp_norm,\n\t            mlp_layers = self.model_mlp_layers,\n\t        )\n\t    def forward(self, curr_obs: Tensor, next_obs: Tensor, last_mems: Optional[Tensor], curr_dones: Tensor):\n", "        curr_embs, curr_mems = self._get_rnd_embeddings(curr_obs, last_mems)\n\t        next_embs, _ = self._get_rnd_embeddings(next_obs, curr_mems)\n\t        curr_tgt, curr_prd, next_tgt, next_prd = self.model_mlp(curr_embs, next_embs)\n\t        curr_dones = curr_dones.view(-1)\n\t        curr_rnd_losses = F.mse_loss(curr_prd, curr_tgt.detach(), reduction='none').mean(-1) * (1 - curr_dones)\n\t        next_rnd_losses = F.mse_loss(next_prd, next_tgt.detach(), reduction='none').mean(-1) * (1 - curr_dones)\n\t        n_samples = (1 - curr_dones).sum()\n\t        curr_rnd_loss = curr_rnd_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        next_rnd_loss = next_rnd_losses.sum() * (1 / n_samples if n_samples > 0 else 0.0)\n\t        rnd_loss = 0.5 * (curr_rnd_loss + next_rnd_loss)\n", "        return rnd_loss, curr_rnd_losses, next_rnd_losses, curr_mems\n\t    def get_intrinsic_rewards(self, curr_obs, next_obs, last_mems, curr_dones, stats_logger):\n\t        with th.no_grad():\n\t            noveld_rnd_loss, noveld_curr_rnd_losses, noveld_next_rnd_losses, model_mems = \\\n\t                self.forward(curr_obs, next_obs, last_mems, curr_dones)\n\t            noveld_curr_rnd_losses = noveld_curr_rnd_losses.clone().cpu().numpy()\n\t            noveld_next_rnd_losses = noveld_next_rnd_losses.clone().cpu().numpy()\n\t            if self.rnd_err_norm > 0:\n\t                # Normalize RND error per step\n\t                self.rnd_err_running_stats.update(noveld_next_rnd_losses)\n", "                noveld_curr_rnd_losses = normalize_rewards(\n\t                    norm_type=self.rnd_err_norm,\n\t                    rewards=noveld_curr_rnd_losses,\n\t                    mean=self.rnd_err_running_stats.mean,\n\t                    std=self.rnd_err_running_stats.std,\n\t                )\n\t                noveld_next_rnd_losses = normalize_rewards(\n\t                    norm_type=self.rnd_err_norm,\n\t                    rewards=noveld_next_rnd_losses,\n\t                    mean=self.rnd_err_running_stats.mean,\n", "                    std=self.rnd_err_running_stats.std,\n\t                )\n\t        stats_logger.add(rnd_loss=noveld_rnd_loss)\n\t        # Create IRs\n\t        batch_size = curr_obs.shape[0]\n\t        int_rews = np.zeros(batch_size, dtype=np.float32)\n\t        next_obs = next_obs.clone().cpu().numpy()\n\t        for env_id in range(batch_size):\n\t            curr_novelty = noveld_curr_rnd_losses[env_id]\n\t            next_novelty = noveld_next_rnd_losses[env_id]\n", "            novelty = max(next_novelty - curr_novelty * self.noveld_alpha, self.noveld_beta)\n\t            # Episodic Restriction on IRs\n\t            if env_id not in self.noveld_visited_obs:\n\t                self.noveld_visited_obs[env_id] = set()\n\t            if curr_dones[env_id]:\n\t                self.noveld_visited_obs[env_id].clear()\n\t            obs_hash = tuple(next_obs[env_id].reshape(-1).tolist())\n\t            if obs_hash in self.noveld_visited_obs[env_id]:\n\t                novelty *= 0.0\n\t            else:\n", "                self.noveld_visited_obs[env_id].add(obs_hash)\n\t            int_rews[env_id] += novelty\n\t        return int_rews, model_mems\n\t    def optimize(self, rollout_data, stats_logger):\n\t        rnd_loss, _, _, _ = \\\n\t            self.forward(\n\t                rollout_data.observations,\n\t                rollout_data.new_observations,\n\t                rollout_data.last_model_mems,\n\t                rollout_data.episode_dones,\n", "            )\n\t        noveld_loss = rnd_loss\n\t        stats_logger.add(rnd_loss=rnd_loss)\n\t        # Optimization\n\t        self.model_optimizer.zero_grad()\n\t        noveld_loss.backward()\n\t        th.nn.utils.clip_grad_norm_(self.model_params, self.max_grad_norm)\n\t        self.model_optimizer.step()\n"]}
{"filename": "src/algo/common_models/mlps.py", "chunked_list": ["import torch as th\n\tfrom torch import Tensor, nn\n\tfrom torch.nn import functional as F\n\tfrom typing import Type, Tuple, Optional\n\tfrom src.utils.enum_types import NormType\n\tclass PolicyValueOutputHeads(nn.Module):\n\t    def __init__(\n\t        self,\n\t        inputs_dim: int,\n\t        latents_dim: int = 128,\n", "        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        mlp_norm: NormType = NormType.NoNorm,\n\t        mlp_layers: int = 1,\n\t    ):\n\t        super(PolicyValueOutputHeads, self).__init__()\n\t        self.input_dim = inputs_dim\n\t        self.mlp_layers = mlp_layers\n\t        self.latent_dim_pi = latents_dim\n\t        self.latent_dim_vf = latents_dim\n\t        p_modules = [\n", "            nn.Linear(self.input_dim, self.latent_dim_pi),\n\t            NormType.get_norm_layer_1d(mlp_norm, self.latent_dim_pi),\n\t            activation_fn(),\n\t        ]\n\t        for _ in range(1, mlp_layers):\n\t            p_modules += [\n\t                nn.Linear(self.latent_dim_pi, self.latent_dim_pi),\n\t                NormType.get_norm_layer_1d(mlp_norm, self.latent_dim_pi),\n\t                activation_fn(),\n\t            ]\n", "        self.policy_latent_nn = nn.Sequential(*p_modules)\n\t        v_modules = [\n\t            nn.Linear(self.input_dim, self.latent_dim_vf),\n\t            NormType.get_norm_layer_1d(mlp_norm, self.latent_dim_vf),\n\t            activation_fn(),\n\t        ]\n\t        for _ in range(1, mlp_layers):\n\t            v_modules += [\n\t                nn.Linear(self.latent_dim_vf, self.latent_dim_vf),\n\t                NormType.get_norm_layer_1d(mlp_norm, self.latent_dim_vf),\n", "                activation_fn(),\n\t            ]\n\t        self.value_latent_nn = nn.Sequential(*v_modules)\n\t    def forward(self, features: Tensor):\n\t        return self.policy_latent_nn(features), self.value_latent_nn(features)\n\tclass ForwardModelOutputHeads(nn.Module):\n\t    def __init__(\n\t        self,\n\t        feature_dim: int,\n\t        latent_dim: int = 128,\n", "        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        action_num: int = 0,\n\t        mlp_norm: NormType = NormType.NoNorm,\n\t        mlp_layers: int = 1,\n\t    ):\n\t        super(ForwardModelOutputHeads, self).__init__()\n\t        self.action_num = action_num\n\t        modules = [\n\t            nn.Linear(feature_dim + action_num, latent_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, latent_dim),\n", "            activation_fn(),\n\t        ]\n\t        for _ in range(1, mlp_layers):\n\t            modules += [\n\t                nn.Linear(latent_dim, latent_dim),\n\t                NormType.get_norm_layer_1d(mlp_norm, latent_dim),\n\t                activation_fn(),\n\t            ]\n\t        modules.append(nn.Linear(latent_dim, feature_dim))\n\t        self.nn = nn.Sequential(*modules)\n", "    def forward(self, curr_emb: Tensor, curr_act: Tensor) -> Tensor:\n\t        one_hot_actions = F.one_hot(curr_act, num_classes=self.action_num)\n\t        inputs = th.cat([curr_emb, one_hot_actions], dim=1)\n\t        return self.nn(inputs)\n\tclass InverseModelOutputHeads(nn.Module):\n\t    def __init__(\n\t        self,\n\t        features_dim: int,\n\t        latents_dim: int = 128,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n", "        action_num: int = 0,\n\t        mlp_norm: NormType = NormType.NoNorm,\n\t        mlp_layers: int = 1,\n\t    ):\n\t        super(InverseModelOutputHeads, self).__init__()\n\t        modules = [\n\t            nn.Linear(features_dim * 2, latents_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, latents_dim),\n\t            activation_fn(),\n\t        ]\n", "        for _ in range(1, mlp_layers):\n\t            modules += [\n\t                nn.Linear(latents_dim, latents_dim),\n\t                NormType.get_norm_layer_1d(mlp_norm, latents_dim),\n\t                activation_fn(),\n\t            ]\n\t        modules.append(nn.Linear(latents_dim, action_num))\n\t        self.nn = nn.Sequential(*modules)\n\t    def forward(self, curr_emb: Tensor, next_emb: Tensor) -> Tensor:\n\t        inputs = th.cat([curr_emb, next_emb], dim=1)\n", "        return self.nn(inputs)\n\tclass ICMOutputHeads(nn.Module):\n\t    def __init__(\n\t        self,\n\t        features_dim: int,\n\t        latents_dim: int = 128,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        action_num: int = 0,\n\t        mlp_norm: NormType = NormType.NoNorm,\n\t        mlp_layers: int = 1,\n", "    ):\n\t        super(ICMOutputHeads, self).__init__()\n\t        self.icm_forward_model = ForwardModelOutputHeads(\n\t            features_dim, latents_dim, activation_fn, action_num,\n\t            mlp_norm, mlp_layers\n\t        )\n\t        self.icm_inverse_model = InverseModelOutputHeads(\n\t            features_dim, latents_dim, activation_fn, action_num,\n\t            mlp_norm, mlp_layers\n\t        )\n", "    def forward(self, curr_emb: Tensor, next_emb: Tensor, curr_act: Tensor) -> Tuple[Tensor, Tensor]:\n\t        return self.icm_forward_model(curr_emb, curr_act), \\\n\t               self.icm_inverse_model(curr_emb, next_emb)\n\tclass RNDOutputHeads(nn.Module):\n\t    def __init__(self,\n\t                 features_dim: int,\n\t                 latents_dim: int = 128,\n\t                 outputs_dim: int = 128,\n\t                 activation_fn: Type[nn.Module] = nn.ReLU,\n\t                 mlp_norm: NormType = NormType.NoNorm,\n", "                 mlp_layers: int = 1,\n\t    ):\n\t        super().__init__()\n\t        self.target = nn.Sequential(\n\t            nn.Linear(features_dim, latents_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, latents_dim),\n\t            activation_fn(),\n\t            nn.Linear(latents_dim, latents_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, latents_dim),\n\t            activation_fn(),\n", "            nn.Linear(latents_dim, outputs_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, outputs_dim),\n\t        )\n\t        self.predictor = nn.Sequential(\n\t            nn.Linear(features_dim, latents_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, latents_dim),\n\t            activation_fn(),\n\t            nn.Linear(latents_dim, outputs_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, outputs_dim),\n\t        )\n", "        for param in self.target.parameters():\n\t            param.requires_grad = False\n\t    def forward(self, emb: Tensor) -> Tuple[Tensor, Tensor]:\n\t        with th.no_grad():\n\t            target_outputs = self.target(emb)\n\t        predicted_outputs = self.predictor(emb)\n\t        return target_outputs, predicted_outputs\n\tclass NGUOutputHeads(nn.Module):\n\t    def __init__(\n\t        self,\n", "        features_dim: int,\n\t        latents_dim: int = 128,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        action_num: int = 0,\n\t        mlp_norm: NormType = NormType.NoNorm,\n\t        mlp_layers: int = 1,\n\t        use_rnd: int = 0,\n\t    ):\n\t        super(NGUOutputHeads, self).__init__()\n\t        self.use_rnd = use_rnd\n", "        if use_rnd:\n\t            self.ngu_rnd_model = RNDOutputHeads(\n\t                features_dim, latents_dim, latents_dim, activation_fn,\n\t                mlp_norm, mlp_layers\n\t            )\n\t        self.ngu_inverse_model = InverseModelOutputHeads(\n\t            features_dim, latents_dim, activation_fn, action_num,\n\t            mlp_norm, mlp_layers\n\t        )\n\t    def inverse_forward(self, curr_emb: Tensor, next_emb: Tensor) -> Tensor:\n", "        return self.ngu_inverse_model(curr_emb, next_emb)\n\t    def rnd_forward(self, curr_emb: Tensor) -> Tuple[Optional[Tensor], Optional[Tensor]]:\n\t        if self.use_rnd:\n\t            return self.ngu_rnd_model(curr_emb)\n\t        return None, None\n\tclass NovelDOutputHeads(nn.Module):\n\t    def __init__(\n\t            self,\n\t            features_dim: int,\n\t            latents_dim: int = 128,\n", "            activation_fn: Type[nn.Module] = nn.ReLU,\n\t            mlp_norm: NormType = NormType.NoNorm,\n\t            mlp_layers: int = 1,\n\t    ):\n\t        super(NovelDOutputHeads, self).__init__()\n\t        self.noveld_rnd_model = RNDOutputHeads(\n\t            features_dim, latents_dim, latents_dim, activation_fn,\n\t            mlp_norm, mlp_layers\n\t        )\n\t    def forward(self, curr_emb: Tensor, next_emb: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n", "        curr_tgt, curr_prd = self.noveld_rnd_model(curr_emb)\n\t        next_tgt, next_prd = self.noveld_rnd_model(next_emb)\n\t        return curr_tgt, curr_prd, next_tgt, next_prd\n\tclass DiscriminatorOutputHeads(nn.Module):\n\t    def __init__(\n\t        self,\n\t        inputs_dim: int,\n\t        latents_dim: int = 128,\n\t        activation_fn: Type[nn.Module] = nn.ReLU,\n\t        action_num: int = 0,\n", "        mlp_norm : NormType = NormType.NoNorm,\n\t        mlp_layers : int = 1,\n\t    ):\n\t        super(DiscriminatorOutputHeads, self).__init__()\n\t        self.action_num = action_num\n\t        modules = [\n\t            nn.Linear(inputs_dim * 2 + action_num, latents_dim),\n\t            NormType.get_norm_layer_1d(mlp_norm, latents_dim),\n\t            activation_fn(),\n\t        ]\n", "        for _ in range(1, mlp_layers):\n\t            modules += [\n\t                nn.Linear(latents_dim, latents_dim),\n\t                NormType.get_norm_layer_1d(mlp_norm, latents_dim),\n\t                activation_fn(),\n\t            ]\n\t        modules.append(nn.Linear(latents_dim, 1))\n\t        self.nn = nn.Sequential(*modules)\n\t    def forward(self, curr_emb: Tensor, next_emb: Tensor, curr_act: Tensor) -> Tensor:\n\t        one_hot_act = F.one_hot(curr_act, num_classes=self.action_num)\n", "        inputs = th.cat([curr_emb, next_emb, one_hot_act], dim=1)\n\t        return self.nn(inputs)\n"]}
{"filename": "src/algo/common_models/gru_cell.py", "chunked_list": ["import torch as th\n\tfrom torch import Tensor\n\tfrom torch.nn import RNNCellBase\n\tfrom typing import Optional\n\tfrom src.utils.enum_types import NormType\n\tclass CustomGRUCell(RNNCellBase):\n\t    def __init__(self,\n\t                 input_size: int,\n\t                 hidden_size: int,\n\t                 norm_type: NormType,\n", "                 bias: bool = True,\n\t                 device=None,\n\t                 dtype=None,\n\t                 ) -> None:\n\t        factory_kwargs = {'device': device, 'dtype': dtype}\n\t        super(CustomGRUCell, self).__init__(input_size, hidden_size, bias, num_chunks=3, **factory_kwargs)\n\t        self.norm_i = NormType.get_norm_layer_1d(norm_type, hidden_size * 3)\n\t        self.norm_h = NormType.get_norm_layer_1d(norm_type, hidden_size * 3)\n\t        self.norm_n = NormType.get_norm_layer_1d(norm_type, hidden_size)\n\t    def forward(self, input: Tensor, hx: Optional[Tensor] = None) -> Tensor:\n", "        if hx is None:\n\t            hx = th.zeros(input.size(0), self.hidden_size, dtype=input.dtype, device=input.device)\n\t        return self.gru_cell(\n\t            input, hx,\n\t            self.weight_ih, self.weight_hh,\n\t            self.bias_ih, self.bias_hh,\n\t        )\n\t    def gru_cell(self, inputs, hidden, w_ih, w_hh, b_ih, b_hh):\n\t        gi = self.norm_i(th.mm(inputs, w_ih.t()))\n\t        gh = self.norm_h(th.mm(hidden, w_hh.t()))\n", "        if self.bias:\n\t            gi = gi + b_ih\n\t            gh = gh + b_hh\n\t        i_r, i_i, i_n = gi.chunk(3, 1)\n\t        h_r, h_i, h_n = gh.chunk(3, 1)\n\t        resetgate = th.sigmoid(i_r + h_r)\n\t        inputgate = th.sigmoid(i_i + h_i)\n\t        newgate = th.tanh(self.norm_n(i_n + resetgate * h_n))\n\t        hy = newgate + inputgate * (hidden - newgate)\n\t        return hy"]}
{"filename": "src/algo/common_models/cnns.py", "chunked_list": ["import gym\n\timport torch as th\n\tfrom gym.spaces import Dict\n\tfrom torch import nn, Tensor\n\tfrom typing import Type\n\tfrom stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n\tfrom src.utils.enum_types import NormType\n\tclass CustomCnnFeaturesExtractor(BaseFeaturesExtractor):\n\t    def __init__(self,\n\t                 observation_space: gym.spaces.Box,\n", "                 features_dim: int,\n\t                 activation_fn: Type[nn.Module],\n\t                 model_type: int,\n\t    ):\n\t        if isinstance(observation_space, Dict):\n\t            observation_space = list(observation_space.values())[0]\n\t        super(CustomCnnFeaturesExtractor, self).\\\n\t            __init__(observation_space, features_dim)\n\t        n_input_channels = observation_space.shape[0]\n\t        self.n_input_channels = n_input_channels\n", "        self.n_input_size = observation_space.shape[1]\n\t        self.activation_fn = activation_fn()\n\t        self.model_type = model_type\n\t        n_flatten = None\n\t        if model_type == 0:\n\t            if self.n_input_size > 3:\n\t                self.cnn = nn.Sequential(\n\t                    NormType.get_norm_layer_2d(self.norm_type, n_input_channels, self.n_input_size),\n\t                    nn.Conv2d(n_input_channels, 32, (2, 2)),\n\t                    NormType.get_norm_layer_2d(self.norm_type, 32, self.n_input_size - 1),\n", "                    activation_fn(),\n\t                    nn.Conv2d(32, 64, (2, 2)),\n\t                    NormType.get_norm_layer_2d(self.norm_type, 64, self.n_input_size - 2),\n\t                    activation_fn(),\n\t                    nn.Conv2d(64, 64, (2, 2)),\n\t                    NormType.get_norm_layer_2d(self.norm_type, 64, self.n_input_size - 3),\n\t                    activation_fn(),\n\t                    nn.Flatten(),\n\t                )\n\t            else:\n", "                self.cnn = nn.Sequential(\n\t                    NormType.get_norm_layer_2d(self.norm_type, n_input_channels, self.n_input_size),\n\t                    nn.Conv2d(n_input_channels, 32, (2, 2), stride=1, padding=1),\n\t                    NormType.get_norm_layer_2d(self.norm_type, 32, self.n_input_size + 1),\n\t                    activation_fn(),\n\t                    nn.Conv2d(32, 64, (2, 2), stride=1, padding=0),\n\t                    NormType.get_norm_layer_2d(self.norm_type, 64, self.n_input_size),\n\t                    activation_fn(),\n\t                    nn.Conv2d(64, 64, (2, 2), stride=1, padding=0),\n\t                    NormType.get_norm_layer_2d(self.norm_type, 64, self.n_input_size - 1),\n", "                    activation_fn(),\n\t                    nn.Flatten(),\n\t                )\n\t        elif model_type == 1:\n\t            if self.n_input_size == 84: image_sizes = [20, 9, 6]\n\t            elif self.n_input_size == 64: image_sizes = [15, 6, 3]\n\t            else: raise NotImplementedError\n\t            # Smaller CNN for ProcGen games\n\t            self.cnn = nn.Sequential(\n\t                NormType.get_norm_layer_2d(self.norm_type, n_input_channels, self.n_input_size),\n", "                nn.Conv2d(in_channels=n_input_channels, out_channels=32, kernel_size=(8, 8), stride=4, padding=0),\n\t                NormType.get_norm_layer_2d(self.norm_type, 32, image_sizes[0]),\n\t                activation_fn(),\n\t                nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(4, 4), stride=2, padding=0),\n\t                NormType.get_norm_layer_2d(self.norm_type, 64, image_sizes[1]),\n\t                activation_fn(),\n\t                nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(4, 4), stride=1, padding=0),\n\t                NormType.get_norm_layer_2d(self.norm_type, 64, image_sizes[2]),\n\t                activation_fn(),\n\t                nn.Flatten(),\n", "            )\n\t        elif model_type == 2:\n\t            # IMPALA CNNs\n\t            layer_scale = 1\n\t            self.build_impala_cnn(\n\t                depths=[\n\t                    16 * layer_scale,\n\t                    32 * layer_scale,\n\t                    32 * layer_scale\n\t                ]\n", "            )\n\t            with th.no_grad():\n\t                samples = th.as_tensor(observation_space.sample()[None]).float()\n\t                for block in self.impala_conv_sequences:\n\t                    samples = block(samples)\n\t                samples = th.flatten(samples, start_dim=1)\n\t                n_flatten = samples.shape[1]\n\t            self.impala_flattern = nn.Sequential(\n\t                nn.Flatten(),\n\t                NormType.get_norm_layer_1d(self.norm_type, n_flatten),\n", "                activation_fn(),\n\t            )\n\t        else:\n\t            raise NotImplementedError\n\t        if n_flatten is None:\n\t            with th.no_grad():\n\t                sample = th.as_tensor(observation_space.sample()[None]).float()\n\t                n_flatten = self.cnn(sample).shape[1]\n\t        self.linear_layer = nn.Sequential(\n\t            nn.Linear(n_flatten, features_dim),\n", "            NormType.get_norm_layer_1d(self.norm_type, features_dim),\n\t            activation_fn(),\n\t        )\n\t    def build_impala_cnn(self, depths=None):\n\t        \"\"\"\n\t        Model used in the paper \"IMPALA: Scalable Distributed Deep-RL with\n\t        Importance Weighted Actor-Learner Architectures\" https://arxiv.org/abs/1802.01561\n\t        This function is created based on\n\t        https://github.com/openai/baselines/blob/master/baselines/common/models.py\n\t        \"\"\"\n", "        if depths is None:\n\t            depths = [16, 32, 32]\n\t        if self.n_input_size == 7:\n\t            # MiniGrid\n\t            image_sizes = [\n\t                [7, 4],\n\t                [4, 2],\n\t                [2, 1],\n\t            ]\n\t        elif self.n_input_size == 64:\n", "            # ProcGen\n\t            image_sizes = [\n\t                [64, 32],\n\t                [32, 16],\n\t                [16, 8],\n\t            ]\n\t        else:\n\t            raise NotImplementedError\n\t        self.impala_conv_sequences = []\n\t        for depth_i in range(len(depths)):\n", "            d_in = self.n_input_channels if depth_i == 0 else depths[depth_i - 1]\n\t            d_out = depths[depth_i]\n\t            module = nn.Sequential(\n\t                NormType.get_norm_layer_2d(self.norm_type, d_in, image_sizes[depth_i][0]),\n\t                nn.Conv2d(in_channels=d_in, out_channels=d_out, kernel_size=(3, 3), stride=1, padding=1),\n\t                nn.MaxPool2d(kernel_size=(3, 3), stride=2, padding=1),\n\t            )\n\t            mod_name = f'impala_blk_{depth_i}_prep'\n\t            setattr(self, mod_name, module)\n\t            self.impala_conv_sequences.append(getattr(self, mod_name))\n", "            for res_block_i in range(2):\n\t                res_blk_module = nn.Sequential(\n\t                    NormType.get_norm_layer_2d(self.norm_type, d_out, image_sizes[depth_i][1]),\n\t                    nn.ReLU(),\n\t                    nn.Conv2d(in_channels=d_out, out_channels=d_out, kernel_size=(3, 3), stride=1, padding=1),\n\t                    NormType.get_norm_layer_2d(self.norm_type, d_out, image_sizes[depth_i][1]),\n\t                    nn.ReLU(),\n\t                    nn.Conv2d(in_channels=d_out, out_channels=d_out, kernel_size=(3, 3), stride=1, padding=1),\n\t                )\n\t                res_blk_name = f'impala_blk_{depth_i}_res_{res_block_i}'\n", "                setattr(self, res_blk_name, res_blk_module)\n\t                self.impala_conv_sequences.append(getattr(self, res_blk_name))\n\t    def forward(self, observations: Tensor) -> Tensor:\n\t        if self.model_type == 2:\n\t            # IMPALA CNNs\n\t            outputs = observations.float() / 255.0\n\t            for block in self.impala_conv_sequences:\n\t                outputs = block(outputs)\n\t            outputs = self.impala_flattern(outputs)\n\t        else:\n", "            outputs = self.cnn(observations)\n\t        return self.linear_layer(outputs)\n\tclass CnnFeaturesExtractor(CustomCnnFeaturesExtractor):\n\t    def __init__(self, observation_space,\n\t                 features_dim: int = 256,\n\t                 activation_fn: Type[nn.Module] = nn.ReLU,\n\t                 model_type: int = 0):\n\t        # Specifying normalization type\n\t        self.norm_type = NormType.NoNorm\n\t        super().__init__(observation_space, features_dim,\n", "                         activation_fn, model_type)\n\tclass BatchNormCnnFeaturesExtractor(CustomCnnFeaturesExtractor):\n\t    def __init__(self, observation_space,\n\t                 features_dim: int = 256,\n\t                 activation_fn: Type[nn.Module] = nn.ReLU,\n\t                 model_type: int = 0):\n\t        # Specifying normalization type\n\t        self.norm_type = NormType.BatchNorm\n\t        super().__init__(observation_space, features_dim,\n\t                         activation_fn, model_type)\n", "class LayerNormCnnFeaturesExtractor(CustomCnnFeaturesExtractor):\n\t    def __init__(self, observation_space,\n\t                 features_dim: int = 256,\n\t                 activation_fn: Type[nn.Module] = nn.ReLU,\n\t                 model_type: int = 0):\n\t        # Specifying normalization type\n\t        self.norm_type = NormType.LayerNorm\n\t        super().__init__(observation_space, features_dim,\n\t                         activation_fn, model_type)\n"]}
