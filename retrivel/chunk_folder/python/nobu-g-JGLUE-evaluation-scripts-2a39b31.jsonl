{"filename": "scripts/gen_table.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\timport wandb\n\tfrom tabulate import tabulate\n\tfrom wandb.apis.public import Run, Sweep\n\tTASKS = {\n\t    \"marc_ja/accuracy\": \"MARC-ja/acc\",\n\t    \"jsts/pearson\": \"JSTS/pearson\",\n\t    \"jsts/spearman\": \"JSTS/spearman\",\n", "    \"jnli/accuracy\": \"JNLI/acc\",\n\t    \"jsquad/exact_match\": \"JSQuAD/EM\",\n\t    \"jsquad/f1\": \"JSQuAD/F1\",\n\t    \"jcqa/accuracy\": \"JComQA/acc\",\n\t}\n\tMODELS = {\n\t    \"roberta_base\": \"nlp-waseda/roberta-base-japanese\",\n\t    \"roberta_large\": \"nlp-waseda/roberta-large-japanese-seq512\",\n\t    \"deberta_base\": \"ku-nlp/deberta-v2-base-japanese\",\n\t    \"deberta_large\": \"ku-nlp/deberta-v2-large-japanese\",\n", "}\n\t@dataclass(frozen=True)\n\tclass RunSummary:\n\t    metric: float\n\t    lr: float\n\t    max_epochs: int\n\t    batch_size: int\n\tdef main():\n\t    api = wandb.Api()\n\t    name_to_sweep_path: dict[str, str] = {\n", "        line.split()[0]: line.split()[1] for line in Path(\"sweep_status.txt\").read_text().splitlines()\n\t    }\n\t    table: list[list[Optional[RunSummary]]] = []\n\t    for model in MODELS.keys():\n\t        items: list[Optional[RunSummary]] = []\n\t        for task in TASKS.keys():\n\t            task, metric_name = task.split(\"/\")\n\t            sweep: Sweep = api.sweep(name_to_sweep_path[f\"{task}-{model}\"])\n\t            if sweep.state == \"FINISHED\":\n\t                run: Optional[Run] = sweep.best_run()\n", "                assert run is not None\n\t                metric_name = \"valid/\" + metric_name\n\t                items.append(\n\t                    RunSummary(\n\t                        metric=run.summary[metric_name],\n\t                        lr=run.config[\"lr\"],\n\t                        max_epochs=run.config[\"max_epochs\"],\n\t                        batch_size=run.config[\"effective_batch_size\"],\n\t                    )\n\t                )\n", "            else:\n\t                items.append(None)\n\t        table.append(items)\n\t    print(\"Scores of best runs:\")\n\t    print(\n\t        tabulate(\n\t            [\n\t                [model] + [item.metric if item else \"-\" for item in items]\n\t                for model, items in zip(MODELS.values(), table)\n\t            ],\n", "            headers=[\"Model\"] + list(TASKS.values()),\n\t            tablefmt=\"github\",\n\t            floatfmt=\".3f\",\n\t            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n\t        )\n\t    )\n\t    print()\n\t    print(\"Learning rates of best runs:\")\n\t    print(\n\t        tabulate(\n", "            [[model] + [item.lr if item else \"-\" for item in items] for model, items in zip(MODELS.values(), table)],\n\t            headers=[\"Model\"] + list(TASKS.values()),\n\t            tablefmt=\"github\",\n\t            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n\t        )\n\t    )\n\t    print()\n\t    print(\"Training epochs of best runs:\")\n\t    print(\n\t        tabulate(\n", "            [\n\t                [model] + [item.max_epochs if item else \"-\" for item in items]\n\t                for model, items in zip(MODELS.values(), table)\n\t            ],\n\t            headers=[\"Model\"] + list(TASKS.values()),\n\t            tablefmt=\"github\",\n\t            colalign=[\"left\"] + [\"right\"] * len(TASKS),\n\t        )\n\t    )\n\tif __name__ == \"__main__\":\n", "    main()\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\n\timport sys\n\timport pytest\n\tfrom transformers import AutoTokenizer, PreTrainedTokenizerBase\n\tsys.path.append(os.path.join(os.path.dirname(__file__), \"../src\"))\n\t@pytest.fixture()\n\tdef tokenizer() -> PreTrainedTokenizerBase:\n\t    return AutoTokenizer.from_pretrained(\"ku-nlp/deberta-v2-tiny-japanese\")\n"]}
{"filename": "tests/metrics/test_squad.py", "chunked_list": ["import pytest\n\tfrom torchmetrics import SQuAD\n\tCASES = [\n\t    {\n\t        \"preds\": [\n\t            {\"prediction_text\": \"1976\", \"id\": \"000\"},\n\t        ],\n\t        \"target\": [{\"answers\": {\"answer_start\": [97], \"text\": [\"1976\"]}, \"id\": \"000\"}],\n\t        \"exact_match\": 1.0,\n\t        \"f1\": 1.0,  # precision: 1 / 1, recall: 1 / 1\n", "    },\n\t    {\n\t        \"preds\": [\n\t            {\"prediction_text\": \"2 時間 21 分\", \"id\": \"001\"},\n\t        ],\n\t        \"target\": [{\"answers\": {\"answer_start\": [10], \"text\": [\"2 時間\"]}, \"id\": \"001\"}],\n\t        \"exact_match\": 0.0,\n\t        \"f1\": 2 / 3,  # precision: 2 / 4, recall: 2 / 2\n\t    },\n\t    {\n", "        \"preds\": [\n\t            {\"prediction_text\": \"2 時間 21 分\", \"id\": \"001\"},\n\t        ],\n\t        \"target\": [{\"answers\": {\"answer_start\": [10, 10], \"text\": [\"2 時間\", \"2 時間 21 分\"]}, \"id\": \"001\"}],\n\t        \"exact_match\": 1.0,\n\t        \"f1\": 1.0,  # precision: 4 / 4, recall: 4 / 4\n\t    },\n\t    {\n\t        \"preds\": [\n\t            {\"prediction_text\": \"2 時間 21 分\", \"id\": \"001\"},\n", "        ],\n\t        \"target\": [{\"answers\": {\"answer_start\": [10, 12], \"text\": [\"2 時間\", \"時間 21 分\"]}, \"id\": \"001\"}],\n\t        \"exact_match\": 0.0,\n\t        \"f1\": 6 / 7,  # precision: 3 / 4, recall: 3 / 3\n\t    },\n\t    {\n\t        \"preds\": [\n\t            {\"prediction_text\": \"2 時 間 2 1 分\", \"id\": \"001\"},\n\t        ],\n\t        \"target\": [{\"answers\": {\"answer_start\": [10], \"text\": [\"2 時 間\"]}, \"id\": \"001\"}],\n", "        \"exact_match\": 0.0,\n\t        \"f1\": 2 / 3,  # precision: 3 / 6, recall: 3 / 3\n\t    },\n\t]\n\t@pytest.mark.parametrize(\"case\", CASES)\n\tdef test_jsquad(case: dict):\n\t    metric = SQuAD()\n\t    metrics = metric(case[\"preds\"], case[\"target\"])\n\t    assert metrics[\"exact_match\"].item() / 100.0 == pytest.approx(case[\"exact_match\"])\n\t    assert metrics[\"f1\"].item() / 100.0 == pytest.approx(case[\"f1\"])\n"]}
{"filename": "tests/datasets/test_jsquad.py", "chunked_list": ["from typing import Any\n\tfrom datasets import Dataset as HFDataset  # type: ignore[attr-defined]\n\tfrom datasets import load_dataset  # type: ignore[attr-defined]\n\tfrom omegaconf import DictConfig\n\tfrom transformers import PreTrainedTokenizerBase\n\tfrom datamodule.datasets.jsquad import JSQuADDataset\n\tdef test_init(tokenizer: PreTrainedTokenizerBase):\n\t    _ = JSQuADDataset(\"train\", tokenizer, max_seq_length=128, segmenter_kwargs=DictConfig({}), limit_examples=3)\n\tdef test_raw_examples():\n\t    dataset: HFDataset = load_dataset(\"shunk031/JGLUE\", name=\"JSQuAD\", split=\"validation\")\n", "    for example in dataset:\n\t        assert isinstance(example[\"id\"], str)\n\t        assert isinstance(example[\"title\"], str)\n\t        assert isinstance(example[\"context\"], str)\n\t        assert isinstance(example[\"question\"], str)\n\t        assert isinstance(example[\"answers\"], dict)\n\t        texts = example[\"answers\"][\"text\"]\n\t        answer_starts = example[\"answers\"][\"answer_start\"]\n\t        for text, answer_start in zip(texts, answer_starts):\n\t            assert example[\"context\"][answer_start:].startswith(text)\n", "        assert example[\"is_impossible\"] is False\n\tdef test_examples(tokenizer: PreTrainedTokenizerBase):\n\t    max_seq_length = 128\n\t    dataset = JSQuADDataset(\"validation\", tokenizer, max_seq_length, segmenter_kwargs=DictConfig({}), limit_examples=10)\n\t    for example in dataset.hf_dataset:\n\t        for answer in example[\"answers\"]:\n\t            if answer[\"answer_start\"] == -1:\n\t                continue\n\t            assert example[\"context\"][answer[\"answer_start\"] :].startswith(answer[\"text\"])\n\tdef test_getitem(tokenizer: PreTrainedTokenizerBase):\n", "    max_seq_length = 128\n\t    dataset = JSQuADDataset(\"train\", tokenizer, max_seq_length, segmenter_kwargs=DictConfig({}), limit_examples=3)\n\t    for i in range(len(dataset)):\n\t        feature = dataset[i]\n\t        assert len(feature.input_ids) == max_seq_length\n\t        assert len(feature.attention_mask) == max_seq_length\n\t        assert len(feature.token_type_ids) == max_seq_length\n\t        assert isinstance(feature.start_positions, int)\n\t        assert isinstance(feature.end_positions, int)\n\tdef test_features_0(tokenizer: PreTrainedTokenizerBase):\n", "    max_seq_length = 128\n\t    dataset = JSQuADDataset(\"validation\", tokenizer, max_seq_length, segmenter_kwargs=DictConfig({}), limit_examples=1)\n\t    example: dict[str, Any] = dict(\n\t        id=\"a10336p0q0\",\n\t        title=\"梅雨\",\n\t        context=\"梅雨 [SEP] 梅雨 （ つゆ 、 ばいう ） は 、 北海道 と 小笠原 諸島 を 除く 日本 、 朝鮮 半島 南部 、 中国 の 南部 から 長江 流域 に かけて の 沿海 部 、 および 台湾 など 、 東 アジア の 広範囲に おいて み られる 特有の 気象 現象 で 、 5 月 から 7 月 に かけて 来る 曇り や 雨 の 多い 期間 の こと 。 雨季 の 一種 である 。\",\n\t        question=\"日本 で 梅雨 が ない の は 北海道 と どこ か 。\",\n\t        answers=[\n\t            dict(text=\"小笠原 諸島\", answer_start=35),\n\t            dict(text=\"小笠原 諸島 を 除く 日本\", answer_start=35),\n", "            dict(text=\"小笠原 諸島\", answer_start=35),\n\t        ],\n\t        is_impossible=False,\n\t    )\n\t    features = dataset[0]\n\t    question_tokens: list[str] = tokenizer.tokenize(example[\"question\"])\n\t    context_tokens: list[str] = tokenizer.tokenize(example[\"context\"])\n\t    input_tokens = (\n\t        [tokenizer.cls_token] + question_tokens + [tokenizer.sep_token] + context_tokens + [tokenizer.sep_token]\n\t    )\n", "    padded_input_tokens = input_tokens + [tokenizer.pad_token] * (max_seq_length - len(input_tokens))\n\t    assert features.input_ids == tokenizer.convert_tokens_to_ids(padded_input_tokens)\n\t    assert features.attention_mask == [1] * len(input_tokens) + [0] * (max_seq_length - len(input_tokens))\n\t    assert features.token_type_ids == [0] * (len(question_tokens) + 2) + [1] * (len(context_tokens) + 1) + [0] * (\n\t        max_seq_length - len(input_tokens)\n\t    )\n\t    assert 0 <= features.start_positions <= features.end_positions < max_seq_length\n\t    answer_span = slice(features.start_positions, features.end_positions + 1)\n\t    tokenized_answer_text: str = tokenizer.decode(features.input_ids[answer_span])\n\t    answers: list[dict[str, Any]] = example[\"answers\"]\n", "    assert tokenized_answer_text == answers[0][\"text\"]\n"]}
{"filename": "src/train.py", "chunked_list": ["import logging\n\timport math\n\timport warnings\n\tfrom typing import Union\n\timport hydra\n\timport torch\n\timport transformers.utils.logging as hf_logging\n\timport wandb\n\tfrom lightning import Callback, LightningModule, Trainer, seed_everything\n\tfrom lightning.pytorch.loggers import Logger\n", "from lightning.pytorch.utilities.warnings import PossibleUserWarning\n\tfrom omegaconf import DictConfig, ListConfig\n\tfrom datamodule.datamodule import DataModule\n\thf_logging.set_verbosity(hf_logging.ERROR)\n\twarnings.filterwarnings(\n\t    \"ignore\",\n\t    message=r\"It is recommended to use .+ when logging on epoch level in distributed setting to accumulate the metric\"\n\t    r\" across devices\",\n\t    category=PossibleUserWarning,\n\t)\n", "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n\t@hydra.main(version_base=None, config_path=\"../configs\")\n\tdef main(cfg: DictConfig):\n\t    if isinstance(cfg.devices, str):\n\t        cfg.devices = list(map(int, cfg.devices.split(\",\"))) if \",\" in cfg.devices else int(cfg.devices)\n\t    if isinstance(cfg.max_batches_per_device, str):\n\t        cfg.max_batches_per_device = int(cfg.max_batches_per_device)\n\t    if isinstance(cfg.num_workers, str):\n\t        cfg.num_workers = int(cfg.num_workers)\n\t    cfg.seed = seed_everything(seed=cfg.seed, workers=True)\n", "    logger: Union[Logger, bool] = cfg.get(\"logger\", False) and hydra.utils.instantiate(cfg.get(\"logger\"))\n\t    callbacks: list[Callback] = list(map(hydra.utils.instantiate, cfg.get(\"callbacks\", {}).values()))\n\t    # Calculate gradient_accumulation_steps assuming DDP\n\t    num_devices: int = 1\n\t    if isinstance(cfg.devices, (list, ListConfig)):\n\t        num_devices = len(cfg.devices)\n\t    elif isinstance(cfg.devices, int):\n\t        num_devices = cfg.devices\n\t    cfg.trainer.accumulate_grad_batches = math.ceil(\n\t        cfg.effective_batch_size / (cfg.max_batches_per_device * num_devices)\n", "    )\n\t    batches_per_device = cfg.effective_batch_size // (num_devices * cfg.trainer.accumulate_grad_batches)\n\t    # if effective_batch_size % (accumulate_grad_batches * num_devices) != 0, then\n\t    # cause an error of at most accumulate_grad_batches * num_devices compared in effective_batch_size\n\t    # otherwise, no error\n\t    cfg.effective_batch_size = batches_per_device * num_devices * cfg.trainer.accumulate_grad_batches\n\t    cfg.datamodule.batch_size = batches_per_device\n\t    trainer: Trainer = hydra.utils.instantiate(\n\t        cfg.trainer,\n\t        logger=logger,\n", "        callbacks=callbacks,\n\t        devices=cfg.devices,\n\t    )\n\t    datamodule = DataModule(cfg=cfg.datamodule)\n\t    model: LightningModule = hydra.utils.instantiate(cfg.module.cls, hparams=cfg, _recursive_=False)\n\t    if cfg.compile is True:\n\t        model = torch.compile(model)\n\t    trainer.fit(model=model, datamodule=datamodule)\n\t    trainer.test(model=model, datamodule=datamodule, ckpt_path=\"best\" if not trainer.fast_dev_run else None)\n\t    wandb.finish()\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/test.py", "chunked_list": ["import logging\n\timport warnings\n\tfrom typing import Union\n\timport hydra\n\timport torch\n\timport transformers.utils.logging as hf_logging\n\tfrom lightning import Callback, LightningModule, Trainer\n\tfrom lightning.pytorch.loggers import Logger\n\tfrom lightning.pytorch.trainer.states import TrainerFn\n\tfrom lightning.pytorch.utilities.warnings import PossibleUserWarning\n", "from omegaconf import DictConfig, ListConfig, OmegaConf\n\tfrom datamodule.datamodule import DataModule\n\thf_logging.set_verbosity(hf_logging.ERROR)\n\twarnings.filterwarnings(\n\t    \"ignore\",\n\t    message=r\"It is recommended to use .+ when logging on epoch level in distributed setting to accumulate the metric\"\n\t    r\" across devices\",\n\t    category=PossibleUserWarning,\n\t)\n\tlogging.getLogger(\"torch\").setLevel(logging.WARNING)\n", "@hydra.main(version_base=None, config_path=\"../configs\", config_name=\"eval\")\n\tdef main(eval_cfg: DictConfig):\n\t    if isinstance(eval_cfg.devices, str):\n\t        eval_cfg.devices = (\n\t            list(map(int, eval_cfg.devices.split(\",\"))) if \",\" in eval_cfg.devices else int(eval_cfg.devices)\n\t        )\n\t    if isinstance(eval_cfg.max_batches_per_device, str):\n\t        eval_cfg.max_batches_per_device = int(eval_cfg.max_batches_per_device)\n\t    if isinstance(eval_cfg.num_workers, str):\n\t        eval_cfg.num_workers = int(eval_cfg.num_workers)\n", "    # Load saved model and configs\n\t    model: LightningModule = hydra.utils.call(eval_cfg.module.load_from_checkpoint, _recursive_=False)\n\t    if eval_cfg.compile is True:\n\t        model = torch.compile(model)\n\t    train_cfg: DictConfig = model.hparams\n\t    OmegaConf.set_struct(train_cfg, False)  # enable to add new key-value pairs\n\t    cfg = OmegaConf.merge(train_cfg, eval_cfg)\n\t    assert isinstance(cfg, DictConfig)\n\t    logger: Union[Logger, bool] = cfg.get(\"logger\", False) and hydra.utils.instantiate(cfg.get(\"logger\"))\n\t    callbacks: list[Callback] = list(map(hydra.utils.instantiate, cfg.get(\"callbacks\", {}).values()))\n", "    num_devices: int = 1\n\t    if isinstance(cfg.devices, (list, ListConfig)):\n\t        num_devices = len(cfg.devices)\n\t    elif isinstance(cfg.devices, int):\n\t        num_devices = cfg.devices\n\t    cfg.effective_batch_size = cfg.max_batches_per_device * num_devices\n\t    cfg.datamodule.batch_size = cfg.max_batches_per_device\n\t    trainer: Trainer = hydra.utils.instantiate(\n\t        cfg.trainer,\n\t        logger=logger,\n", "        callbacks=callbacks,\n\t        devices=cfg.devices,\n\t    )\n\t    datamodule = DataModule(cfg=cfg.datamodule)\n\t    datamodule.setup(stage=TrainerFn.TESTING)\n\t    if cfg.eval_set == \"test\":\n\t        dataloader = datamodule.test_dataloader()\n\t    elif cfg.eval_set == \"valid\":\n\t        dataloader = datamodule.val_dataloader()\n\t    else:\n", "        raise ValueError(f\"invalid eval_set: {cfg.eval_set}\")\n\t    trainer.test(model=model, dataloaders=dataloader)\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/metrics/jsquad.py", "chunked_list": ["# This file contains code adapted from transformers\n\t# (https://github.com/huggingface/transformers/blob/main/examples/flax/question-answering/utils_qa.py)\n\t# Copyright 2020 The HuggingFace Team All rights reserved.\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\tfrom typing import Any\n\timport numpy as np\n\timport torch\n\tfrom torchmetrics import Metric, SQuAD\n\tfrom datamodule.datasets import JSQuADDataset\n\tclass JSQuADMetric(Metric):\n", "    is_differentiable: bool = False\n\t    higher_is_better: bool = True\n\t    full_state_update: bool = False\n\t    def __init__(self) -> None:\n\t        super().__init__()\n\t        self.squad = SQuAD()\n\t    def update(\n\t        self,\n\t        example_ids: torch.Tensor,  # (b)\n\t        batch_start_logits: torch.Tensor,  # (b, seq)\n", "        batch_end_logits: torch.Tensor,  # (b, seq)\n\t        dataset: JSQuADDataset,\n\t    ) -> None:\n\t        preds = []\n\t        target = []\n\t        for example_id, start_logits, end_logits in zip(\n\t            example_ids.tolist(), batch_start_logits.tolist(), batch_end_logits.tolist()\n\t        ):\n\t            example = dataset.hf_dataset[example_id]\n\t            prediction_text: str = _postprocess_predictions(start_logits, end_logits, example)\n", "            preds.append(\n\t                {\n\t                    \"prediction_text\": self._postprocess_text(prediction_text),\n\t                    \"id\": example_id,\n\t                }\n\t            )\n\t            target.append(\n\t                {\n\t                    \"answers\": {\n\t                        \"text\": [self._postprocess_text(answer[\"text\"]) for answer in example[\"answers\"]],\n", "                        \"answer_start\": [answer[\"answer_start\"] for answer in example[\"answers\"]],\n\t                    },\n\t                    \"id\": example_id,\n\t                }\n\t            )\n\t        self.squad.update(preds, target)\n\t    def compute(self) -> dict[str, torch.Tensor]:\n\t        return {k: v / 100.0 for k, v in self.squad.compute().items()}\n\t    @staticmethod\n\t    def _postprocess_text(text: str) -> str:\n", "        \"\"\"句点を除去し，文字単位に分割\"\"\"\n\t        return \" \".join(text.replace(\" \", \"\").rstrip(\"。\"))\n\tdef _postprocess_predictions(\n\t    start_logits: list[float],\n\t    end_logits: list[float],\n\t    example: dict[str, Any],\n\t    n_best_size: int = 20,\n\t    max_answer_length: int = 30,\n\t) -> str:\n\t    \"\"\"\n", "    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n\t    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n\t    Args:\n\t        start_logits (:obj:`List[float]`):\n\t            The logits corresponding to the start of the span for each token.\n\t        end_logits (:obj:`List[float]`):\n\t            The logits corresponding to the end of the span for each token.\n\t        example: The processed dataset.\n\t        n_best_size (:obj:`int`, `optional`, defaults to 20):\n\t            The total number of n-best predictions to generate when looking for an answer.\n", "        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n\t            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n\t            are not conditioned on one another.\n\t    \"\"\"\n\t    prelim_predictions = []\n\t    # This is what will allow us to map some the positions in our logits to span of texts in the original\n\t    # context.\n\t    offset_mapping = example[\"offset_mapping\"]\n\t    # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n\t    # available in the current feature.\n", "    token_is_max_context = example.get(\"token_is_max_context\")\n\t    # Go through all possibilities for the `n_best_size` greater start and end logits.\n\t    start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n\t    end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n\t    for start_index in start_indexes:\n\t        for end_index in end_indexes:\n\t            # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n\t            # to part of the input_ids that are not in the context.\n\t            if (\n\t                start_index >= len(offset_mapping)\n", "                or end_index >= len(offset_mapping)\n\t                or offset_mapping[start_index] is None\n\t                or len(offset_mapping[start_index]) < 2\n\t                or offset_mapping[end_index] is None\n\t                or len(offset_mapping[end_index]) < 2\n\t            ):\n\t                continue\n\t            # Don't consider answers with a length that is either < 0 or > max_answer_length.\n\t            if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n\t                continue\n", "            # Don't consider answer that don't have the maximum context available (if such information is\n\t            # provided).\n\t            if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n\t                continue\n\t            prelim_predictions.append(\n\t                {\n\t                    \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n\t                    \"score\": start_logits[start_index] + end_logits[end_index],\n\t                    \"start_logit\": start_logits[start_index],\n\t                    \"end_logit\": end_logits[end_index],\n", "                }\n\t            )\n\t    # Only keep the best `n_best_size` predictions.\n\t    predictions: list[dict[str, Any]] = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n\t    # Use the offsets to gather the answer text in the original context.\n\t    context = example[\"context\"]\n\t    for pred in predictions:\n\t        offsets = pred.pop(\"offsets\")\n\t        pred[\"text\"] = context[offsets[0] : offsets[1]]\n\t    # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n", "    # failure.\n\t    if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n\t        predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n\t    # Compute the softmax of all scores (we do it with numpy to stay independent of torch/tf in this file, using\n\t    # the LogSumExp trick).\n\t    scores = np.array([pred.pop(\"score\") for pred in predictions])\n\t    exp_scores = np.exp(scores - np.max(scores))\n\t    probs = exp_scores / exp_scores.sum()\n\t    # Include the probabilities in our predictions.\n\t    for prob, pred in zip(probs, predictions):\n", "        pred[\"probability\"] = prob\n\t    return predictions[0][\"text\"]  # return top 1 prediction\n"]}
{"filename": "src/metrics/__init__.py", "chunked_list": ["from .jsquad import JSQuADMetric\n\t__all__ = [\"JSQuADMetric\"]\n"]}
{"filename": "src/datamodule/__init__.py", "chunked_list": []}
{"filename": "src/datamodule/datamodule.py", "chunked_list": ["from dataclasses import fields, is_dataclass\n\tfrom typing import Any, Optional, Union\n\timport hydra\n\timport lightning\n\timport torch\n\tfrom lightning.pytorch.trainer.states import TrainerFn\n\tfrom omegaconf import DictConfig\n\tfrom torch import Tensor\n\tfrom torch.utils.data import DataLoader, Dataset\n\tclass DataModule(lightning.LightningDataModule):\n", "    def __init__(self, cfg: DictConfig) -> None:\n\t        super().__init__()\n\t        self.cfg: DictConfig = cfg\n\t        self.batch_size: int = cfg.batch_size\n\t        self.num_workers: int = cfg.num_workers\n\t        self.train_dataset: Optional[Dataset] = None\n\t        self.valid_dataset: Optional[Dataset] = None\n\t        self.test_dataset: Optional[Dataset] = None\n\t    def prepare_data(self):\n\t        pass\n", "    def setup(self, stage: Optional[str] = None) -> None:\n\t        if stage == TrainerFn.FITTING:\n\t            self.train_dataset = hydra.utils.instantiate(self.cfg.train)\n\t        if stage in (TrainerFn.FITTING, TrainerFn.VALIDATING, TrainerFn.TESTING):\n\t            self.valid_dataset = hydra.utils.instantiate(self.cfg.valid)\n\t        if stage == TrainerFn.TESTING:\n\t            self.test_dataset = hydra.utils.instantiate(self.cfg.test)\n\t    def train_dataloader(self) -> DataLoader:\n\t        assert self.train_dataset is not None\n\t        return self._get_dataloader(dataset=self.train_dataset, shuffle=True)\n", "    def val_dataloader(self) -> DataLoader:\n\t        assert self.valid_dataset is not None\n\t        return self._get_dataloader(self.valid_dataset, shuffle=False)\n\t    def test_dataloader(self) -> DataLoader:\n\t        assert self.test_dataset is not None\n\t        return self._get_dataloader(self.test_dataset, shuffle=False)\n\t    def _get_dataloader(self, dataset: Dataset, shuffle: bool) -> DataLoader:\n\t        return DataLoader(\n\t            dataset=dataset,\n\t            batch_size=self.batch_size,\n", "            shuffle=shuffle,\n\t            num_workers=self.num_workers,\n\t            collate_fn=dataclass_data_collator,\n\t            pin_memory=True,\n\t        )\n\tdef dataclass_data_collator(features: list[Any]) -> dict[str, Union[Tensor, list[str]]]:\n\t    first: Any = features[0]\n\t    assert is_dataclass(first), \"Data must be a dataclass\"\n\t    batch: dict[str, Union[Tensor, list[str]]] = {}\n\t    for field in fields(first):\n", "        feats = [getattr(f, field.name) for f in features]\n\t        if \"text\" in field.name:\n\t            batch[field.name] = feats\n\t        else:\n\t            batch[field.name] = torch.as_tensor(feats)\n\t    return batch\n"]}
{"filename": "src/datamodule/datasets/jsquad.py", "chunked_list": ["import os\n\tfrom typing import Any, Optional\n\tfrom omegaconf import DictConfig\n\tfrom transformers import PreTrainedTokenizerBase\n\tfrom transformers.utils import PaddingStrategy\n\tfrom datamodule.datasets.base import BaseDataset\n\tfrom datamodule.datasets.util import QuestionAnsweringFeatures, batch_segment\n\tclass JSQuADDataset(BaseDataset[QuestionAnsweringFeatures]):\n\t    def __init__(\n\t        self,\n", "        split: str,\n\t        tokenizer: PreTrainedTokenizerBase,\n\t        max_seq_length: int,\n\t        segmenter_kwargs: DictConfig,\n\t        limit_examples: int = -1,\n\t    ) -> None:\n\t        super().__init__(\"JSQuAD\", split, tokenizer, max_seq_length, limit_examples)\n\t        self.hf_dataset = self.hf_dataset.map(\n\t            preprocess,\n\t            batched=True,\n", "            batch_size=100,\n\t            fn_kwargs=dict(segmenter_kwargs=segmenter_kwargs),\n\t            num_proc=os.cpu_count(),\n\t        ).map(\n\t            lambda x: self.tokenizer(\n\t                x[\"question\"],\n\t                x[\"context\"],\n\t                padding=PaddingStrategy.MAX_LENGTH,\n\t                truncation=\"only_second\",\n\t                max_length=self.max_seq_length,\n", "                return_offsets_mapping=True,\n\t            ),\n\t            batched=True,\n\t        )\n\t        # skip invalid examples for training\n\t        if self.split == \"train\":\n\t            self.hf_dataset = self.hf_dataset.filter(\n\t                lambda example: any(answer[\"answer_start\"] >= 0 for answer in example[\"answers\"])\n\t            )\n\t    def __getitem__(self, index: int) -> QuestionAnsweringFeatures:\n", "        example: dict[str, Any] = self.hf_dataset[index]\n\t        start_positions = end_positions = 0\n\t        for answer in example[\"answers\"]:\n\t            start_positions, end_positions = self._get_token_span(example, answer[\"text\"], answer[\"answer_start\"])\n\t            if start_positions > 0 or end_positions > 0:\n\t                break\n\t        return QuestionAnsweringFeatures(\n\t            example_ids=index,\n\t            input_ids=example[\"input_ids\"],\n\t            attention_mask=example[\"attention_mask\"],\n", "            token_type_ids=example[\"token_type_ids\"],\n\t            start_positions=start_positions,\n\t            end_positions=end_positions,\n\t        )\n\t    @staticmethod\n\t    def _get_token_span(example: dict[str, Any], answer_text: str, answer_start: int) -> tuple[int, int]:\n\t        \"\"\"スパンの位置について、文字単位からトークン単位に変換\"\"\"\n\t        # token_type_ids:\n\t        #   0: 1番目の入力(=`question`)のトークン or パディング\n\t        #   1: 2番目の入力(=`context`)のトークン\n", "        token_type_ids: list[int] = example[\"token_type_ids\"]\n\t        # トークンのインデックスと文字のスパンのマッピングを保持した変数\n\t        # \"京都 大学\" -> [\"[CLS]\", \"▁京都\", \"▁大学\", \"[SEP]\"] のように分割された場合、\n\t        #   [(0, 0), (0, 2), (2, 5), (0, 0)]\n\t        offset_mapping: list[tuple[int, int]] = example[\"offset_mapping\"]\n\t        context: str = example[\"context\"]\n\t        assert len(offset_mapping) == len(token_type_ids)\n\t        token_to_char_start_index = [x[0] for x in offset_mapping]\n\t        token_to_char_end_index = [x[1] for x in offset_mapping]\n\t        answer_end = answer_start + len(answer_text)\n", "        token_start_index = token_end_index = 0\n\t        for token_index, (token_type_id, char_start_index, char_end_index) in enumerate(\n\t            zip(token_type_ids, token_to_char_start_index, token_to_char_end_index)\n\t        ):\n\t            if token_type_id != 1 or char_start_index == char_end_index == 0:\n\t                continue\n\t            # 半角スペースが無視されていない時があるため、その場合はマッピングを1つずらす\n\t            if context[char_start_index] == \" \":\n\t                char_start_index += 1\n\t            if answer_start == char_start_index:\n", "                token_start_index = token_index\n\t            if answer_end == char_end_index:\n\t                token_end_index = token_index\n\t        return token_start_index, token_end_index\n\tdef preprocess(examples, segmenter_kwargs: dict[str, Any]) -> dict[str, Any]:\n\t    titles: list[str]\n\t    bodies: list[str]\n\t    titles, bodies = zip(*[context.split(\" [SEP] \") for context in examples[\"context\"]])\n\t    segmented_titles = batch_segment(titles, **segmenter_kwargs)\n\t    segmented_bodies = batch_segment(bodies, **segmenter_kwargs)\n", "    segmented_contexts = [f\"{title} [SEP] {body}\" for title, body in zip(segmented_titles, segmented_bodies)]\n\t    segmented_questions = batch_segment(examples[\"question\"], **segmenter_kwargs)\n\t    batch_answers: list[list[dict]] = []\n\t    for answers, segmented_context, title in zip(examples[\"answers\"], segmented_contexts, titles):\n\t        processed_answers: list[dict] = []\n\t        for answer_text, answer_start in zip(answers[\"text\"], answers[\"answer_start\"]):\n\t            segmented_answer_text, answer_start = find_segmented_answer(\n\t                segmented_context, answer_text, answer_start, len(title)\n\t            )\n\t            if answer_start is None:\n", "                processed_answers.append(dict(text=answer_text, answer_start=-1))\n\t                continue\n\t            assert segmented_answer_text is not None\n\t            processed_answers.append(dict(text=segmented_answer_text, answer_start=answer_start))\n\t        batch_answers.append(processed_answers)\n\t    return {\"context\": segmented_contexts, \"question\": segmented_questions, \"answers\": batch_answers}\n\tdef find_segmented_answer(\n\t    segmented_context: str, answer_text: str, answer_start: int, sep_index: int\n\t) -> tuple[Optional[str], Optional[int]]:\n\t    \"\"\"単語区切りされた context から単語区切りされた answer のスパンを探索\n", "    Args:\n\t        segmented_context: 単語区切りされた context\n\t        answer_text: answer の文字列\n\t        answer_start: answer の文字単位開始インデックス\n\t        sep_index: [SEP] の文字単位開始インデックス\n\t    Returns:\n\t        Optional[str]: 単語区切りされた answer（見つからなければ None）\n\t        Optional[int]: 単語区切りされた context における answer の文字単位開始インデックス（見つからなければ None）\n\t    \"\"\"\n\t    words = segmented_context.split(\" \")\n", "    char_to_word_index = {}\n\t    char_index = 0\n\t    for word_index, word in enumerate(words):\n\t        char_to_word_index[char_index] = word_index\n\t        # [SEP]だけ前後の半角スペースを考慮する必要があるため+2する\n\t        char_length = len(word) + 2 if word == \"[SEP]\" else len(word)\n\t        char_index += char_length\n\t    # 答えのスパンの開始位置が単語区切りに沿うかチェック\n\t    if answer_start in char_to_word_index:\n\t        word_index = char_to_word_index[answer_start]\n", "        buf = []\n\t        for word in words[word_index:]:\n\t            buf.append(word)\n\t            # 分かち書きしても答えのスパンが見つかる場合\n\t            if \"\".join(buf) == answer_text:\n\t                offset = 2 if answer_start >= sep_index else 0\n\t                return \" \".join(buf), answer_start + word_index - offset\n\t    return None, None\n"]}
{"filename": "src/datamodule/datasets/base.py", "chunked_list": ["from abc import ABC\n\tfrom typing import Generic, TypeVar\n\tfrom datasets import Dataset as HFDataset  # type: ignore[attr-defined]\n\tfrom datasets import load_dataset  # type: ignore[attr-defined]\n\tfrom torch.utils.data import Dataset\n\tfrom transformers import PreTrainedTokenizerBase\n\tFeatureType = TypeVar(\"FeatureType\")\n\tclass BaseDataset(Dataset[FeatureType], Generic[FeatureType], ABC):\n\t    def __init__(\n\t        self,\n", "        dataset_name: str,\n\t        split: str,\n\t        tokenizer: PreTrainedTokenizerBase,\n\t        max_seq_length: int,\n\t        limit_examples: int = -1,\n\t    ) -> None:\n\t        self.split: str = split\n\t        self.tokenizer: PreTrainedTokenizerBase = tokenizer\n\t        self.max_seq_length: int = max_seq_length\n\t        # NOTE: JGLUE does not provide test set.\n", "        if self.split == \"test\":\n\t            self.split = \"validation\"\n\t        # columns: id, title, context, question, answers, is_impossible\n\t        self.hf_dataset: HFDataset = load_dataset(\"shunk031/JGLUE\", name=dataset_name, split=self.split)\n\t        if limit_examples > 0:\n\t            self.hf_dataset = self.hf_dataset.select(range(limit_examples))\n\t    def __getitem__(self, index: int) -> FeatureType:\n\t        raise NotImplementedError\n\t    def __len__(self) -> int:\n\t        return len(self.hf_dataset)\n"]}
{"filename": "src/datamodule/datasets/jnli.py", "chunked_list": ["import os\n\tfrom typing import Any\n\tfrom omegaconf import DictConfig\n\tfrom transformers import PreTrainedTokenizerBase\n\tfrom transformers.utils import PaddingStrategy\n\tfrom datamodule.datasets.base import BaseDataset\n\tfrom datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n\tclass JNLIDataset(BaseDataset[SequenceClassificationFeatures]):\n\t    def __init__(\n\t        self,\n", "        split: str,\n\t        tokenizer: PreTrainedTokenizerBase,\n\t        max_seq_length: int,\n\t        segmenter_kwargs: DictConfig,\n\t        limit_examples: int = -1,\n\t    ) -> None:\n\t        super().__init__(\"JNLI\", split, tokenizer, max_seq_length, limit_examples)\n\t        self.hf_dataset = self.hf_dataset.map(\n\t            lambda x: {\n\t                \"segmented1\": batch_segment(x[\"sentence1\"], **segmenter_kwargs),  # type: ignore\n", "                \"segmented2\": batch_segment(x[\"sentence2\"], **segmenter_kwargs),  # type: ignore\n\t            },\n\t            batched=True,\n\t            batch_size=100,\n\t            num_proc=os.cpu_count(),\n\t        ).map(\n\t            lambda x: self.tokenizer(\n\t                x[\"segmented1\"],\n\t                x[\"segmented2\"],\n\t                padding=PaddingStrategy.MAX_LENGTH,\n", "                truncation=True,\n\t                max_length=self.max_seq_length,\n\t            ),\n\t            batched=True,\n\t        )\n\t    def __getitem__(self, index: int) -> SequenceClassificationFeatures:\n\t        example: dict[str, Any] = self.hf_dataset[index]\n\t        return SequenceClassificationFeatures(\n\t            input_ids=example[\"input_ids\"],\n\t            attention_mask=example[\"attention_mask\"],\n", "            token_type_ids=example[\"token_type_ids\"],\n\t            labels=example[\"label\"],  # 0: entailment, 1: contradiction, 2: neutral\n\t        )\n"]}
{"filename": "src/datamodule/datasets/__init__.py", "chunked_list": ["from .jcqa import JCommonsenseQADataset\n\tfrom .jnli import JNLIDataset\n\tfrom .jsquad import JSQuADDataset\n\tfrom .jsts import JSTSDataset\n\tfrom .marc_ja import MARCJaDataset\n\t__all__ = [\"MARCJaDataset\", \"JSTSDataset\", \"JNLIDataset\", \"JSQuADDataset\", \"JCommonsenseQADataset\"]\n"]}
{"filename": "src/datamodule/datasets/util.py", "chunked_list": ["from dataclasses import dataclass\n\tfrom typing import Optional\n\timport jaconv\n\timport MeCab\n\tfrom rhoknp import Jumanpp\n\t@dataclass(frozen=True)\n\tclass SequenceClassificationFeatures:\n\t    input_ids: list[int]\n\t    attention_mask: list[int]\n\t    token_type_ids: list[int]\n", "    labels: int\n\t@dataclass(frozen=True)\n\tclass MultipleChoiceFeatures:\n\t    input_ids: list[list[int]]\n\t    attention_mask: list[list[int]]\n\t    token_type_ids: list[list[int]]\n\t    labels: int\n\t@dataclass(frozen=True)\n\tclass QuestionAnsweringFeatures:\n\t    example_ids: int\n", "    input_ids: list[int]\n\t    attention_mask: list[int]\n\t    token_type_ids: list[int]\n\t    start_positions: int\n\t    end_positions: int\n\tdef batch_segment(\n\t    texts: list[str], analyzer: str = \"jumanpp\", h2z: bool = True, mecab_dic_dir: Optional[str] = None\n\t) -> list[str]:\n\t    segmenter = WordSegmenter(analyzer, h2z, mecab_dic_dir)\n\t    return [segmenter.get_segmented_string(text) for text in texts]\n", "class WordSegmenter:\n\t    def __init__(self, analyzer: str, h2z: bool, mecab_dic_dir: Optional[str] = None):\n\t        self._analyzer: str = analyzer\n\t        self._h2z: bool = h2z\n\t        if self._analyzer == \"jumanpp\":\n\t            self._jumanpp = Jumanpp()\n\t        elif self._analyzer == \"mecab\":\n\t            tagger_options = []\n\t            if mecab_dic_dir is not None:\n\t                tagger_options += f\"-d {mecab_dic_dir}\".split()\n", "            self._mecab = MeCab.Tagger(\" \".join(tagger_options))\n\t    def get_words(self, string: str) -> list[str]:\n\t        words: list[str] = []\n\t        if self._analyzer == \"jumanpp\":\n\t            sentence = self._jumanpp.apply_to_sentence(string)\n\t            for morpheme in sentence.morphemes:\n\t                words.append(morpheme.text)\n\t        elif self._analyzer == \"mecab\":\n\t            self._mecab.parse(\"\")\n\t            node = self._mecab.parseToNode(string)\n", "            while node:\n\t                word = node.surface\n\t                if node.feature.split(\",\")[0] != \"BOS/EOS\":\n\t                    words.append(word)\n\t                node = node.next\n\t        elif self._analyzer == \"char\":\n\t            for char in string:\n\t                words.append(char)\n\t        else:\n\t            NotImplementedError(f\"unknown analyzer: {self._analyzer}\")\n", "        return words\n\t    def get_segmented_string(self, string: str) -> str:\n\t        if self._h2z is True:\n\t            string = jaconv.h2z(string)\n\t        words = self.get_words(string)\n\t        return \" \".join(word for word in words)\n"]}
{"filename": "src/datamodule/datasets/jcqa.py", "chunked_list": ["import os\n\tfrom itertools import chain\n\tfrom typing import Any\n\tfrom omegaconf import DictConfig\n\tfrom transformers import PreTrainedTokenizerBase\n\tfrom transformers.utils import PaddingStrategy\n\tfrom datamodule.datasets.base import BaseDataset\n\tfrom datamodule.datasets.util import MultipleChoiceFeatures, batch_segment\n\tCHOICE_NAMES = [\"choice0\", \"choice1\", \"choice2\", \"choice3\", \"choice4\"]\n\tNUM_CHOICES = len(CHOICE_NAMES)\n", "class JCommonsenseQADataset(BaseDataset[MultipleChoiceFeatures]):\n\t    def __init__(\n\t        self,\n\t        split: str,\n\t        tokenizer: PreTrainedTokenizerBase,\n\t        max_seq_length: int,\n\t        segmenter_kwargs: DictConfig,\n\t        limit_examples: int = -1,\n\t    ) -> None:\n\t        super().__init__(\"JCommonsenseQA\", split, tokenizer, max_seq_length, limit_examples)\n", "        def preprocess_function(examples) -> dict[str, list[list[Any]]]:\n\t            # (example, 5)\n\t            first_sentences: list[list[str]] = [[question] * NUM_CHOICES for question in examples[\"question\"]]\n\t            second_sentences: list[list[str]] = [\n\t                [examples[name][i] for name in CHOICE_NAMES] for i in range(len(examples[\"question\"]))\n\t            ]\n\t            # Tokenize\n\t            tokenized_examples = self.tokenizer(\n\t                list(chain(*first_sentences)),\n\t                list(chain(*second_sentences)),\n", "                truncation=True,\n\t                max_length=self.max_seq_length,\n\t                padding=PaddingStrategy.MAX_LENGTH,\n\t            )\n\t            # Un-flatten\n\t            return {\n\t                k: [v[i : i + NUM_CHOICES] for i in range(0, len(v), NUM_CHOICES)]\n\t                for k, v in tokenized_examples.items()\n\t            }\n\t        self.hf_dataset = self.hf_dataset.map(\n", "            lambda x: {\n\t                key: batch_segment(x[key], **segmenter_kwargs) for key in [\"question\"] + CHOICE_NAMES  # type: ignore\n\t            },\n\t            batched=True,\n\t            batch_size=100,\n\t            num_proc=os.cpu_count(),\n\t        ).map(\n\t            preprocess_function,\n\t            batched=True,\n\t        )\n", "    def __getitem__(self, index: int) -> MultipleChoiceFeatures:\n\t        example: dict[str, Any] = self.hf_dataset[index]\n\t        return MultipleChoiceFeatures(\n\t            input_ids=example[\"input_ids\"],\n\t            attention_mask=example[\"attention_mask\"],\n\t            token_type_ids=example[\"token_type_ids\"],\n\t            labels=example[\"label\"],\n\t        )\n"]}
{"filename": "src/datamodule/datasets/jsts.py", "chunked_list": ["import os\n\tfrom typing import Any\n\tfrom omegaconf import DictConfig\n\tfrom transformers import PreTrainedTokenizerBase\n\tfrom transformers.utils import PaddingStrategy\n\tfrom datamodule.datasets.base import BaseDataset\n\tfrom datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n\tclass JSTSDataset(BaseDataset[SequenceClassificationFeatures]):\n\t    def __init__(\n\t        self,\n", "        split: str,\n\t        tokenizer: PreTrainedTokenizerBase,\n\t        max_seq_length: int,\n\t        segmenter_kwargs: DictConfig,\n\t        limit_examples: int = -1,\n\t    ) -> None:\n\t        super().__init__(\"JSTS\", split, tokenizer, max_seq_length, limit_examples)\n\t        self.hf_dataset = self.hf_dataset.map(\n\t            lambda x: {\n\t                \"segmented1\": batch_segment(x[\"sentence1\"], **segmenter_kwargs),  # type: ignore\n", "                \"segmented2\": batch_segment(x[\"sentence2\"], **segmenter_kwargs),  # type: ignore\n\t            },\n\t            batched=True,\n\t            batch_size=100,\n\t            num_proc=os.cpu_count(),\n\t        ).map(\n\t            lambda x: self.tokenizer(\n\t                x[\"segmented1\"],\n\t                x[\"segmented2\"],\n\t                padding=PaddingStrategy.MAX_LENGTH,\n", "                truncation=True,\n\t                max_length=self.max_seq_length,\n\t            ),\n\t            batched=True,\n\t        )\n\t    def __getitem__(self, index: int) -> SequenceClassificationFeatures:\n\t        example: dict[str, Any] = self.hf_dataset[index]\n\t        return SequenceClassificationFeatures(\n\t            input_ids=example[\"input_ids\"],\n\t            attention_mask=example[\"attention_mask\"],\n", "            token_type_ids=example[\"token_type_ids\"],\n\t            labels=example[\"label\"],\n\t        )\n"]}
{"filename": "src/datamodule/datasets/marc_ja.py", "chunked_list": ["import os\n\tfrom typing import Any\n\tfrom omegaconf import DictConfig\n\tfrom transformers import PreTrainedTokenizerBase\n\tfrom transformers.utils import PaddingStrategy\n\tfrom datamodule.datasets.base import BaseDataset\n\tfrom datamodule.datasets.util import SequenceClassificationFeatures, batch_segment\n\tclass MARCJaDataset(BaseDataset[SequenceClassificationFeatures]):\n\t    def __init__(\n\t        self,\n", "        split: str,\n\t        tokenizer: PreTrainedTokenizerBase,\n\t        max_seq_length: int,\n\t        segmenter_kwargs: DictConfig,\n\t        limit_examples: int = -1,\n\t    ) -> None:\n\t        super().__init__(\"MARC-ja\", split, tokenizer, max_seq_length, limit_examples)\n\t        self.hf_dataset = self.hf_dataset.map(\n\t            lambda x: {\"segmented\": batch_segment(x[\"sentence\"], **segmenter_kwargs)},  # type: ignore\n\t            batched=True,\n", "            batch_size=100,\n\t            num_proc=os.cpu_count(),\n\t        ).map(\n\t            lambda x: self.tokenizer(\n\t                x[\"segmented\"],\n\t                padding=PaddingStrategy.MAX_LENGTH,\n\t                truncation=True,\n\t                max_length=self.max_seq_length,\n\t            ),\n\t            batched=True,\n", "        )\n\t    def __getitem__(self, index: int) -> SequenceClassificationFeatures:\n\t        example: dict[str, Any] = self.hf_dataset[index]\n\t        return SequenceClassificationFeatures(\n\t            input_ids=example[\"input_ids\"],\n\t            attention_mask=example[\"attention_mask\"],\n\t            token_type_ids=example[\"token_type_ids\"],\n\t            labels=example[\"label\"],\n\t        )\n"]}
{"filename": "src/modules/jsquad.py", "chunked_list": ["import torch\n\tfrom omegaconf import DictConfig\n\tfrom transformers import AutoConfig, AutoModelForQuestionAnswering, PretrainedConfig, PreTrainedModel\n\tfrom transformers.modeling_outputs import QuestionAnsweringModelOutput\n\tfrom datamodule.datasets.jsquad import JSQuADDataset\n\tfrom metrics import JSQuADMetric\n\tfrom modules.base import BaseModule\n\tclass JSQuADModule(BaseModule):\n\t    MODEL_ARGS = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"start_positions\", \"end_positions\"]\n\t    def __init__(self, hparams: DictConfig) -> None:\n", "        super().__init__(hparams)\n\t        config: PretrainedConfig = AutoConfig.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            finetuning_task=\"JSQuAD\",\n\t        )\n\t        self.model: PreTrainedModel = AutoModelForQuestionAnswering.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            config=config,\n\t        )\n\t        self.metric = JSQuADMetric()\n", "    def forward(self, batch: dict[str, torch.Tensor]) -> QuestionAnsweringModelOutput:\n\t        return self.model(**{k: v for k, v in batch.items() if k in self.MODEL_ARGS})\n\t    def training_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> torch.Tensor:\n\t        out: QuestionAnsweringModelOutput = self(batch)\n\t        self.log(\"train/loss\", out.loss)\n\t        return out.loss\n\t    def validation_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n\t        out: QuestionAnsweringModelOutput = self(batch)\n\t        dataset: JSQuADDataset = self.trainer.val_dataloaders.dataset\n\t        self.metric.update(batch[\"example_ids\"], out.start_logits, out.end_logits, dataset)\n", "    def on_validation_epoch_end(self) -> None:\n\t        self.log_dict({f\"valid/{key}\": value for key, value in self.metric.compute().items()})\n\t        self.metric.reset()\n\t    def test_step(self, batch: dict[str, torch.Tensor], batch_idx: int) -> None:\n\t        out: QuestionAnsweringModelOutput = self(batch)\n\t        dataset: JSQuADDataset = self.trainer.test_dataloaders.dataset\n\t        self.metric.update(batch[\"example_ids\"], out.start_logits, out.end_logits, dataset)\n\t    def on_test_epoch_end(self) -> None:\n\t        self.log_dict({f\"test/{key}\": value for key, value in self.metric.compute().items()})\n\t        self.metric.reset()\n"]}
{"filename": "src/modules/base.py", "chunked_list": ["import copy\n\tfrom typing import Any\n\timport hydra\n\tfrom lightning import LightningModule\n\tfrom omegaconf import DictConfig, OmegaConf\n\tclass BaseModule(LightningModule):\n\t    def __init__(self, hparams: DictConfig) -> None:\n\t        super().__init__()\n\t        self.save_hyperparameters(hparams)\n\t    def configure_optimizers(self):\n", "        # Split weights in two groups, one with weight decay and the other not.\n\t        no_decay = (\"bias\", \"LayerNorm.weight\")\n\t        optimizer_grouped_parameters = [\n\t            {\n\t                \"params\": [\n\t                    p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay) and p.requires_grad\n\t                ],\n\t                \"weight_decay\": self.hparams.optimizer.weight_decay,\n\t                \"name\": \"decay\",\n\t            },\n", "            {\n\t                \"params\": [\n\t                    p for n, p in self.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad\n\t                ],\n\t                \"weight_decay\": 0.0,\n\t                \"name\": \"no_decay\",\n\t            },\n\t        ]\n\t        optimizer = hydra.utils.instantiate(\n\t            self.hparams.optimizer, params=optimizer_grouped_parameters, _convert_=\"partial\"\n", "        )\n\t        total_steps = self.trainer.estimated_stepping_batches\n\t        warmup_steps = self.hparams.warmup_steps or total_steps * self.hparams.warmup_ratio\n\t        if hasattr(self.hparams.scheduler, \"num_warmup_steps\"):\n\t            self.hparams.scheduler.num_warmup_steps = warmup_steps\n\t        if hasattr(self.hparams.scheduler, \"num_training_steps\"):\n\t            self.hparams.scheduler.num_training_steps = total_steps\n\t        lr_scheduler = hydra.utils.instantiate(self.hparams.scheduler, optimizer=optimizer)\n\t        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}}\n\t    def on_save_checkpoint(self, checkpoint: dict[str, Any]) -> None:\n", "        hparams: DictConfig = copy.deepcopy(checkpoint[\"hyper_parameters\"])\n\t        OmegaConf.set_struct(hparams, False)\n\t        checkpoint[\"hyper_parameters\"] = hparams\n"]}
{"filename": "src/modules/jnli.py", "chunked_list": ["from typing import Any\n\timport torch\n\tfrom omegaconf import DictConfig\n\tfrom torchmetrics.classification import MulticlassAccuracy\n\tfrom transformers import AutoConfig, AutoModelForSequenceClassification, PretrainedConfig, PreTrainedModel\n\tfrom transformers.modeling_outputs import SequenceClassifierOutput\n\tfrom modules.base import BaseModule\n\tclass JNLIModule(BaseModule):\n\t    def __init__(self, hparams: DictConfig) -> None:\n\t        super().__init__(hparams)\n", "        config: PretrainedConfig = AutoConfig.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            num_labels=3,\n\t            finetuning_task=\"JNLI\",\n\t        )\n\t        self.model: PreTrainedModel = AutoModelForSequenceClassification.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            config=config,\n\t        )\n\t        self.metric = MulticlassAccuracy(num_classes=3, average=\"micro\")\n", "    def forward(self, batch: dict[str, Any]) -> SequenceClassifierOutput:\n\t        return self.model(**batch)\n\t    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        self.log(\"train/loss\", out.loss)\n\t        return out.loss\n\t    def validation_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        preds = torch.argmax(out.logits, dim=1)  # (b)\n\t        self.metric.update(preds, batch[\"labels\"])\n", "    def on_validation_epoch_end(self) -> None:\n\t        self.log(\"valid/accuracy\", self.metric.compute())\n\t        self.metric.reset()\n\t    def test_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        preds = torch.argmax(out.logits, dim=1)  # (b)\n\t        self.metric.update(preds, batch[\"labels\"])\n\t    def on_test_epoch_end(self) -> None:\n\t        self.log(\"test/accuracy\", self.metric.compute())\n\t        self.metric.reset()\n"]}
{"filename": "src/modules/__init__.py", "chunked_list": ["from .jcqa import JCommonsenseQAModule\n\tfrom .jnli import JNLIModule\n\tfrom .jsquad import JSQuADModule\n\tfrom .jsts import JSTSModule\n\tfrom .marc_ja import MARCJaModule\n\t__all__ = [\"MARCJaModule\", \"JSTSModule\", \"JNLIModule\", \"JSQuADModule\", \"JCommonsenseQAModule\"]\n"]}
{"filename": "src/modules/jcqa.py", "chunked_list": ["from typing import Any\n\timport torch\n\tfrom omegaconf import DictConfig\n\tfrom torchmetrics.classification import MulticlassAccuracy\n\tfrom transformers import AutoConfig, AutoModelForMultipleChoice\n\tfrom transformers.modeling_outputs import MultipleChoiceModelOutput\n\tfrom datamodule.datasets.jcqa import NUM_CHOICES\n\tfrom modules.base import BaseModule\n\tclass JCommonsenseQAModule(BaseModule):\n\t    def __init__(self, hparams: DictConfig) -> None:\n", "        super().__init__(hparams)\n\t        config = AutoConfig.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            num_labels=NUM_CHOICES,\n\t            finetuning_task=\"JCommonsenseQA\",\n\t        )\n\t        self.model = AutoModelForMultipleChoice.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            config=config,\n\t        )\n", "        self.metric = MulticlassAccuracy(num_classes=NUM_CHOICES, average=\"micro\")\n\t    def forward(self, batch: dict[str, Any]) -> MultipleChoiceModelOutput:\n\t        return self.model(**batch)\n\t    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n\t        out: MultipleChoiceModelOutput = self(batch)\n\t        self.log(\"train/loss\", out.loss)\n\t        return out.loss\n\t    def validation_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: MultipleChoiceModelOutput = self(batch)\n\t        preds = torch.argmax(out.logits, dim=1)  # (b)\n", "        self.metric.update(preds, batch[\"labels\"])\n\t    def on_validation_epoch_end(self) -> None:\n\t        self.log(\"valid/accuracy\", self.metric.compute())\n\t        self.metric.reset()\n\t    def test_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: MultipleChoiceModelOutput = self(batch)\n\t        preds = torch.argmax(out.logits, dim=1)  # (b)\n\t        self.metric.update(preds, batch[\"labels\"])\n\t    def on_test_epoch_end(self) -> None:\n\t        self.log(\"test/accuracy\", self.metric.compute())\n", "        self.metric.reset()\n"]}
{"filename": "src/modules/jsts.py", "chunked_list": ["from typing import Any\n\timport torch\n\tfrom omegaconf import DictConfig\n\tfrom torchmetrics import MetricCollection, PearsonCorrCoef, SpearmanCorrCoef\n\tfrom transformers import AutoConfig, AutoModelForSequenceClassification\n\tfrom transformers.modeling_outputs import SequenceClassifierOutput\n\tfrom modules.base import BaseModule\n\tclass JSTSModule(BaseModule):\n\t    def __init__(self, hparams: DictConfig) -> None:\n\t        super().__init__(hparams)\n", "        config = AutoConfig.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            num_labels=1,\n\t            finetuning_task=\"JSTS\",\n\t        )\n\t        self.model = AutoModelForSequenceClassification.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            config=config,\n\t        )\n\t        self.metric = MetricCollection({\"spearman\": SpearmanCorrCoef(), \"pearson\": PearsonCorrCoef()})\n", "    def forward(self, batch: dict[str, Any]) -> SequenceClassifierOutput:\n\t        return self.model(**batch)\n\t    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        self.log(\"train/loss\", out.loss)\n\t        return out.loss\n\t    def validation_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        preds = torch.squeeze(out.logits, dim=-1)  # (b)\n\t        self.metric.update(preds, batch[\"labels\"])\n", "    def on_validation_epoch_end(self) -> None:\n\t        self.log_dict({f\"valid/{key}\": value for key, value in self.metric.compute().items()})\n\t        self.metric.reset()\n\t    def test_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        preds = torch.squeeze(out.logits, dim=-1)  # (b)\n\t        self.metric.update(preds, batch[\"labels\"])\n\t    def on_test_epoch_end(self) -> None:\n\t        self.log_dict({f\"test/{key}\": value for key, value in self.metric.compute().items()})\n\t        self.metric.reset()\n"]}
{"filename": "src/modules/marc_ja.py", "chunked_list": ["from typing import Any\n\timport torch\n\tfrom omegaconf import DictConfig\n\tfrom torchmetrics.classification import MulticlassAccuracy\n\tfrom transformers import AutoConfig, AutoModelForSequenceClassification\n\tfrom transformers.modeling_outputs import SequenceClassifierOutput\n\tfrom modules.base import BaseModule\n\tclass MARCJaModule(BaseModule):\n\t    def __init__(self, hparams: DictConfig) -> None:\n\t        super().__init__(hparams)\n", "        config = AutoConfig.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            num_labels=2,\n\t            finetuning_task=\"MARC-ja\",\n\t        )\n\t        self.model = AutoModelForSequenceClassification.from_pretrained(\n\t            hparams.model_name_or_path,\n\t            config=config,\n\t        )\n\t        self.metric = MulticlassAccuracy(num_classes=2, average=\"micro\")\n", "    def forward(self, batch: dict[str, Any]) -> SequenceClassifierOutput:\n\t        return self.model(**batch)\n\t    def training_step(self, batch: Any, batch_idx: int) -> torch.Tensor:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        self.log(\"train/loss\", out.loss)\n\t        return out.loss\n\t    def validation_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        preds = torch.argmax(out.logits, dim=1)  # (b)\n\t        self.metric.update(preds, batch[\"labels\"])\n", "    def on_validation_epoch_end(self) -> None:\n\t        self.log(\"valid/accuracy\", self.metric.compute())\n\t        self.metric.reset()\n\t    def test_step(self, batch: Any, batch_idx: int) -> None:\n\t        out: SequenceClassifierOutput = self(batch)\n\t        preds = torch.argmax(out.logits, dim=1)  # (b)\n\t        self.metric.update(preds, batch[\"labels\"])\n\t    def on_test_epoch_end(self) -> None:\n\t        self.log(\"test/accuracy\", self.metric.compute())\n\t        self.metric.reset()\n"]}
