{"filename": "src/extraction_benchmark/wceb.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport click\n\tfrom extraction_benchmark.cli import *\n\t@click.group()\n\tdef main():\n\t    \"\"\"\n\t    Web Content Extraction Benchmark.\n\t    Reproduction study of various main content extraction / boilerplate removal tools from\n", "    the scientific literature and the open source community.\n\t    \"\"\"\n\t    pass\n\tmain.add_command(complexity)\n\tmain.add_command(eval)\n\tmain.add_command(extract)\n\tmain.add_command(convert_datasets)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "src/extraction_benchmark/plt.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom matplotlib.pyplot import *\n\trcParams['figure.dpi'] = 200\n\trcParams['pdf.fonttype'] = 42\n\trcParams['font.sans-serif'] = ['Helvetica', 'Arial', 'DejaVu Sans']\n\trcParams['font.family'] = 'sans-serif'\n\tMEDIAN_BAR_COLOR = '#e68a38'\n\tERROR_BAR_COLOR = '#4d4d4d'\n", "rcParams['errorbar.capsize'] = 4\n\trcParams['boxplot.meanprops.color'] = 'pink'\n\trcParams['boxplot.flierprops.marker'] = '.'\n\t# Lighter version of tab10\n\trcParams['axes.prop_cycle'] = cycler(color=['#6caeda', '#ff993e', '#56b356'])\n"]}
{"filename": "src/extraction_benchmark/globals.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom extraction_benchmark.paths import *\n\tfrom extraction_benchmark.extractors import list_extractors\n\t_DATASET_FRIENDLY_NAME_MAP = {\n\t    'cetd': 'CETD',\n\t    'cleaneval': 'CleanEval',\n\t    'cleanportaleval': 'CleanPortalEval',\n\t    'dragnet': 'Dragnet',\n", "    'google-trends-2017': 'Google-Trends',\n\t    'l3s-gn1': 'L3S-GN1',\n\t    'readability': 'Readability',\n\t    'scrapinghub': 'Scrapinghub'\n\t}\n\tif os.path.isdir(DATASET_RAW_PATH):\n\t    DATASETS = {k: _DATASET_FRIENDLY_NAME_MAP.get(k, k) for k in os.listdir(DATASET_RAW_PATH)\n\t                if os.path.isdir(os.path.join(DATASET_RAW_PATH, k))}\n\telse:\n\t    DATASETS = {}\n", "_MODEL_FRIENDLY_NAME_MAP = dict(\n\t    ensemble_best='(Best only)',\n\t    ensemble_weighted='(Best weighted)',\n\t    ensemble_majority='(Majority all)',\n\t    bs4='BS4',\n\t    boilernet='BoilerNet',\n\t    boilerpipe='Boilerpipe',\n\t    bte='BTE',\n\t    dragnet='Dragnet',\n\t    extractnet='ExtractNet',\n", "    go_domdistiller='DOM Distiller',\n\t    goose3='Goose3',\n\t    justext='jusText',\n\t    lxml_cleaner='lxml Cleaner',\n\t    news_please='news-please',\n\t    newspaper3k='Newspaper3k',\n\t    readability='Readability',\n\t    resiliparse='Resiliparse',\n\t    trafilatura='Trafilatura',\n\t    web2text='Web2Text',\n", "    xpath_text='XPath Text',\n\t)\n\tMODELS = {k: _MODEL_FRIENDLY_NAME_MAP.get(k, k)\n\t          for k in list_extractors(names_only=True, include_ensembles=False)}\n\tMODELS_ALL = {k: _MODEL_FRIENDLY_NAME_MAP.get(k, k)\n\t              for k in list_extractors(names_only=True, include_ensembles=True)}\n\tMODELS_ENSEMBLE = [m for m in MODELS_ALL if m.startswith('ensemble_')]\n\tMODELS_BASELINE = ['bs4', 'html_text', 'inscriptis', 'lxml_cleaner', 'xpath_text']\n\tSCORES = [\n\t    'rouge',\n", "    'levenshtein'\n\t]\n\tCOMPLEXITIES = [\n\t    'low',\n\t    'medium',\n\t    'high'\n\t]\n"]}
{"filename": "src/extraction_benchmark/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n"]}
{"filename": "src/extraction_benchmark/eval.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom itertools import pairwise, product\n\timport math\n\tfrom multiprocessing import get_context\n\timport click\n\tfrom Levenshtein import ratio as levenshtein_ratio\n\timport pandas as pd\n\tfrom rouge_score import rouge_scorer, tokenizers\n", "from tqdm import tqdm\n\tfrom extraction_benchmark.globals import *\n\tfrom extraction_benchmark import plt\n\tfrom extraction_benchmark.util import jsonl_to_dict, read_jsonl, tokenize_ws\n\tclass Tokenizer(tokenizers.Tokenizer):\n\t    def tokenize(self, text):\n\t        return tokenize_ws(text)\n\tdef rouge_eval(key, model, dataset, target, pred):\n\t    rouge = rouge_scorer.RougeScorer(['rougeLsum'], use_stemmer=False, split_summaries=True, tokenizer=Tokenizer())\n\t    scores = []\n", "    score = rouge.score(target, pred)\n\t    for s in score:\n\t        t = dict()\n\t        t['hash_key'] = key\n\t        t['model'] = model\n\t        t['prec'] = score[s].precision\n\t        t['rec'] = score[s].recall\n\t        t['f1'] = score[s].fmeasure\n\t        t['scorer'] = s\n\t        t['dataset'] = dataset\n", "        if target.strip() == '':\n\t            t['rec'] = 1.0\n\t            if pred.strip() == '':\n\t                t['prec'] = 1.0\n\t                t['f1'] = 1.0\n\t        scores.append(t)\n\t    return scores\n\tdef levenshtein_eval(key, model, dataset, target, pred):\n\t    tokenizer = Tokenizer()\n\t    target = tokenizer.tokenize(target)\n", "    pred = tokenizer.tokenize(pred)\n\t    return [dict(\n\t        hash_key=key,\n\t        model=model,\n\t        dist=levenshtein_ratio(target, pred),\n\t        scorer='levenshtein',\n\t        dataset=dataset\n\t    )]\n\tdef _eval_expand_args(args):\n\t    scorer, model, dataset, answer_path, gt_path = args\n", "    if scorer == 'rouge':\n\t        scorer_func = rouge_eval\n\t    elif scorer == 'levenshtein':\n\t        scorer_func = levenshtein_eval\n\t    else:\n\t        raise ValueError('Illegal scorer')\n\t    ground_truth = jsonl_to_dict(gt_path)\n\t    df = pd.DataFrame()\n\t    for model_answer in read_jsonl(answer_path):\n\t        if model_answer['page_id'] not in ground_truth:\n", "            continue\n\t        target = ground_truth[model_answer['page_id']].get('plaintext') or ''\n\t        pred = model_answer.get('plaintext') or ''\n\t        df = pd.concat([df, pd.DataFrame(scorer_func(model_answer['page_id'], model, dataset, target, pred))])\n\t    store_path = os.path.join(METRICS_PATH, scorer, dataset)\n\t    os.makedirs(store_path, exist_ok=True)\n\t    df.to_csv(os.path.join(store_path, f'{scorer}_{model}.csv'), index=False)\n\tdef calculcate_scores(metrics, datasets, models, parallelism):\n\t    \"\"\"\n\t    Calculate performance scores for pages against the ground truth.\n", "    :param metrics: list of performance scores to calculate (``\"rouge\"`` or ``\"levenshtein\"``)\n\t    :param datasets: list of dataset names\n\t    :param models: list of models to evaluate\n\t    :param parallelism: number of parallel workers to run\n\t    \"\"\"\n\t    jobs = []\n\t    for ds in tqdm(datasets, desc='Loading extractions', leave=False):\n\t        ground_truth_path = os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl')\n\t        if not os.path.isfile(ground_truth_path):\n\t            continue\n", "        for model in models:\n\t            model_answer_path = os.path.join(MODEL_OUTPUTS_PATH, ds,  f'{model}.jsonl')\n\t            if os.path.isfile(model_answer_path):\n\t                jobs.extend([met, model, ds, model_answer_path, ground_truth_path] for met in metrics)\n\t    with get_context('spawn').Pool(processes=parallelism) as pool:\n\t        for _ in tqdm(pool.imap_unordered(_eval_expand_args, jobs),\n\t                      total=len(jobs), desc='Evaluating model answers'):\n\t            pass\n\tdef _layout_ax(ax, angle_xticks=True, hlines=True):\n\t    ax.set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n", "    if hlines:\n\t        for y in ax.get_yticks():\n\t            ax.axhline(y, linewidth=0.25, color='lightgrey', zorder=-1)\n\t    ax.spines['top'].set_visible(False)\n\t    ax.spines['right'].set_visible(False)\n\t    if angle_xticks:\n\t        ax.set_xticks(ax.get_xticks())\n\t        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n\tdef _map_model_label(label):\n\t    if label.get_text() in MODELS_ENSEMBLE:\n", "        label.set_color('#1767b0')\n\t    elif label.get_text() in MODELS_BASELINE:\n\t        label.set_color('gray')\n\t    label.set_text(MODELS_ALL.get(label.get_text(), label.get_text()))\n\t    return label\n\tdef _map_axis_tick_labels(axis):\n\t    ticklabels = [_map_model_label(t) for t in axis.get_ticklabels()]\n\t    axis.set_ticks(range(len(ticklabels)))\n\t    axis.set_ticklabels(ticklabels)\n\tdef _draw_performance_boxsubplot(ax, model_scores, xlabels, ylabel):\n", "    ax.boxplot(\n\t        model_scores,\n\t        positions=range(len(xlabels)),\n\t        labels=xlabels,\n\t        showfliers=False\n\t    )\n\t    _map_axis_tick_labels(ax.xaxis)\n\t    ax.set_ylabel(ylabel)\n\t    ax.set_ylim((-0.1, 1.1))\n\t    _layout_ax(ax)\n", "def _draw_performance_barsubplot(ax, model_scores, lower_err, upper_err, xlabels, ylabel):\n\t    ax.bar(\n\t        xlabels,\n\t        model_scores,\n\t        width=0.7,\n\t        yerr=(lower_err, upper_err),\n\t        error_kw=dict(lw=0.75, capthick=0.75, ecolor=plt.ERROR_BAR_COLOR),\n\t    )\n\t    _map_axis_tick_labels(ax.xaxis)\n\t    ax.set_ylabel(ylabel)\n", "    ax.set_ylim((0.0, 1.1))\n\t    ax.set_xlim((-0.7, len(xlabels) - 0.3))\n\t    _layout_ax(ax)\n\tdef _draw_performance_plot(plot_type, data, layout, suptitle, score_name):\n\t    fig, axs = plt.subplots(*layout, figsize=(9.5, 3.3 * len(data)))\n\t    if layout == (1, 1):\n\t        axs = [axs]\n\t    for ax, d in zip(axs, data):\n\t        if plot_type == 'box':\n\t            _draw_performance_boxsubplot(ax, *d)\n", "        else:\n\t            _draw_performance_barsubplot(ax, *d)\n\t    plt.suptitle(suptitle)\n\t    plt.tight_layout()\n\t    plt.savefig(os.path.join(METRICS_PATH, score_name, f'{score_name}_{plot_type}.pdf'))\n\t    plt.close()\n\tdef _sort_vectors(*vals, reverse=True):\n\t    \"\"\"Sort multiple vectors / lists by values in the first one.\"\"\"\n\t    return zip(*sorted(zip(*vals), key=lambda x: x[0], reverse=reverse))\n\tdef _write_agg_df_to_files(df, score_name, main_score_col, out_file_base):\n", "    out_df_max = df.groupby(['dataset']).max()\n\t    def _highlight_max_per_ds(s):\n\t        def _is_max(idx, val):\n\t            return val >= out_df_max[s.name][idx[1]]\n\t        return ['font-weight: bold' if _is_max(idx, val) else '' for idx, val in s.items()]\n\t    # Remap series to friendly names\n\t    def _remap_series_names(s):\n\t        s.index = pd.Index(data=[MODELS_ALL.get(n, n) for n in s.index.values], name='Model')\n\t        return s\n\t    os.makedirs(os.path.join(METRICS_PATH, score_name), exist_ok=True)\n", "    out_styler = df.style.apply(_highlight_max_per_ds).format(precision=3)\n\t    out_styler.to_excel(out_file_base + '.xlsx')\n\t    # Compile reduced versions of the table with global averages\n\t    for series in '_micro', '_macro':\n\t        out_df_reduced = df.loc[:, series, :].sort_values(f'mean_{main_score_col}', ascending=False)\n\t        out_df_reduced.name = 'Model'\n\t        if score_name == 'rouge':\n\t            out_df_reduced.columns = ['Mean Precision', 'Mean Recall', 'Mean F1',\n\t                                      'Median Precision', 'Median Recall', 'Median F1']\n\t        else:\n", "            out_df_reduced.columns = ['Mean Distance', 'Median Distance']\n\t        out_df_reduced = out_df_reduced.apply(_remap_series_names)\n\t        # XLSX\n\t        out_styler = out_df_reduced.style.highlight_max(props='font-weight: bold').format(precision=3)\n\t        out_styler.to_excel(out_file_base + f'{series}.xlsx', float_format='%.3f')\n\t        # LaTeX\n\t        if score_name == 'rouge':\n\t            out_df_reduced.columns = ['Mean Precision', 'Mean Recall', 'Mean $F_1$',\n\t                                      'Median Precision', 'Median Recall', 'Median $F_1$']\n\t        out_styler = out_df_reduced.style.highlight_max(props=r'bf:').format(precision=3)\n", "        out_styler.to_latex(os.path.join(out_file_base + f'{series}.tex'))\n\tdef _agg_model_at_complexity(complexity, in_df, score_name, score_cols, main_score_col):\n\t    models = sorted(in_df.index.unique('model'))\n\t    out_df = pd.DataFrame(columns=['model', 'dataset',\n\t                                   *[f'mean_{c}' for c in score_cols],\n\t                                   *[f'median_{c}' for c in score_cols]])\n\t    out_df.set_index(['model', 'dataset'], inplace=True)\n\t    model_main_scores = []\n\t    model_main_medians = []\n\t    model_main_means = []\n", "    model_main_lower_err = []\n\t    model_main_upper_err = []\n\t    for m in models:\n\t        model_df = in_df.loc[m, :, :].drop(columns=['scorer'])\n\t        model_ds_group = model_df.groupby('dataset')\n\t        mean_ds = model_ds_group.mean()\n\t        median_ds = model_ds_group.median()\n\t        ds_stats = pd.concat([mean_ds, median_ds], axis=1)\n\t        mean_micro = model_df.mean()\n\t        median_micro = model_df.median()\n", "        micro = pd.concat([mean_micro, median_micro]).to_frame('_micro').T\n\t        micro.index.name = 'dataset'\n\t        ds_stats = pd.concat([ds_stats, micro])\n\t        mean_macro = mean_ds.mean()\n\t        median_macro = median_ds.median()\n\t        macro = pd.concat([mean_macro, median_macro]).to_frame('_macro').T\n\t        macro.index.name = 'dataset'\n\t        ds_stats = pd.concat([ds_stats, macro])\n\t        ds_stats.columns = out_df.columns\n\t        ds_stats['model'] = m\n", "        ds_stats = ds_stats.reset_index().set_index(['model', 'dataset'])\n\t        out_df = pd.concat([out_df, ds_stats.round(3)])\n\t        model_main_scores.append(model_df[main_score_col])\n\t        model_main_medians.append(median_micro[main_score_col])\n\t        model_main_means.append(mean_micro[main_score_col])\n\t        model_main_lower_err.append(abs(mean_micro[main_score_col] - model_df[main_score_col].quantile(0.25)))\n\t        model_main_upper_err.append(abs(model_df[main_score_col].quantile(0.75) - mean_micro[main_score_col]))\n\t    file_suffix = f'_complexity_{complexity}' if complexity != 'all' else ''\n\t    _write_agg_df_to_files(out_df, score_name, main_score_col,\n\t                           os.path.join(METRICS_PATH, score_name, f'{score_name}{file_suffix}'))\n", "    _, main_scores, labels = _sort_vectors(model_main_medians, model_main_scores, models)\n\t    boxplot_data = [main_scores, labels, f'Complexity: {complexity.capitalize()}']\n\t    main_scores, low, high, labels = _sort_vectors(model_main_means, model_main_lower_err, model_main_upper_err, models)\n\t    barplot_data = [main_scores, low, high, labels, f'Complexity: {complexity.capitalize()}']\n\t    return boxplot_data, barplot_data\n\tdef _plot_score_histograms(title, score_df, out_file):\n\t    models = sorted(score_df.index.unique('model'), key=lambda m: score_df[m, :, :].median(), reverse=True)\n\t    cols = 4\n\t    rows = math.ceil(len(models) / cols)\n\t    fig, axs = plt.subplots(rows, cols, sharey=True, figsize=(2 * cols, 2 * rows))\n", "    for ax, m in zip(axs.flatten(), models):\n\t        ax.hist(\n\t            score_df[m, :, :],\n\t            bins=25\n\t        )\n\t        ax.axvline(score_df[m, :, :].median(), color=plt.MEDIAN_BAR_COLOR, linewidth=1)\n\t        ax.set_ylabel(m)\n\t        _map_model_label(ax.yaxis.get_label())\n\t        ax.set_xticks([0, 0.5, 1])\n\t        ax.set_yticklabels([])\n", "    # Hide empty plots\n\t    if len(models) % cols:\n\t        [ax.set_visible(False) for ax in axs[-1][len(models) % cols:].flatten()]\n\t    fig.suptitle(title)\n\t    plt.tight_layout()\n\t    plt.savefig(out_file)\n\t    plt.close()\n\tdef aggregate_scores(score_name, models, datasets, complexities):\n\t    \"\"\"\n\t    Aggregate evaluation statistics.\n", "    :param score_name: score to aggregated (``\"rouge\"`` or ``\"levenshtein\"``)\n\t    :param models: list of input model names\n\t    :param datasets: list of input dataset names\n\t    :param complexities: list of complexity classes to include\n\t    \"\"\"\n\t    score_in_path = os.path.join(METRICS_PATH, score_name)\n\t    if not os.path.isdir(score_in_path):\n\t        return\n\t    if score_name == 'rouge':\n\t        score_cols = ['prec', 'rec', 'f1']\n", "        main_score_col = 'f1'\n\t    else:\n\t        score_cols = ['dist']\n\t        main_score_col = 'dist'\n\t    comp_quant_path = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_quantiles.csv')\n\t    q = pd.read_csv(comp_quant_path, index_col=0)\n\t    compl_range = {'all': None}\n\t    compl_range.update({k: v for k, v in zip(COMPLEXITIES, pairwise([0, float(q.loc[0.25]), float(q.loc[0.75]), 1]))})\n\t    if score_name == 'rouge':\n\t        title_box = 'ROUGE-LSum Median $F_1$ Page Scores'\n", "        title_bar = 'ROUGE-LSum Mean $F_1$ Page Scores (Macro Average)'\n\t        title_hist = 'ROUGE-LSum $F_1$ Page Scores'\n\t    else:\n\t        title_hist = 'Normalized Levenshtein Distances'\n\t        title_box = 'Normalized Median Levenshtein Distances'\n\t        title_bar = 'Normalized Mean Levenshtein Distance (Macro Average)'\n\t    with click.progressbar(complexities, label=f'Aggregating \"{score_name}\" scores') as progress:\n\t        boxplot_data = []\n\t        barplot_data = []\n\t        for comp in progress:\n", "            score_df = pd.DataFrame()\n\t            for d, m in product(datasets, models):\n\t                p = os.path.join(score_in_path, d, f'{score_name}_{m}.csv')\n\t                if not os.path.isfile(p):\n\t                    continue\n\t                df = pd.read_csv(p, index_col=['model', 'dataset'])\n\t                if compl_range[comp] is not None:\n\t                    # Filter input dataframe to include only pages within chosen complexity range\n\t                    c = pd.read_csv(os.path.join(METRICS_COMPLEXITY_PATH, d, f'{d}_complexity.csv'),\n\t                                    index_col='hash_key')\n", "                    c = c[(c['complexity'] >= compl_range[comp][0]) & (c['complexity'] <= compl_range[comp][1])]\n\t                    df = df[df['hash_key'].isin(c.index)]\n\t                df.set_index(['hash_key'], append=True, inplace=True)\n\t                score_df = pd.concat([score_df, df])\n\t            hist_file_prefix = f'_complexity_{comp}' if comp != 'all' else ''\n\t            _plot_score_histograms(f'{title_hist} (Complexity: {comp.capitalize()})', score_df[main_score_col],\n\t                                   os.path.join(score_in_path, f'{score_name}{hist_file_prefix}_hist.pdf'))\n\t            box, bar = _agg_model_at_complexity(comp, score_df, score_name, score_cols, main_score_col)\n\t            boxplot_data.append(box)\n\t            barplot_data.append(bar)\n", "        _draw_performance_plot(\n\t            'box',\n\t            boxplot_data,\n\t            (len(boxplot_data), 1),\n\t            title_box,\n\t            score_name)\n\t        _draw_performance_plot(\n\t            'bar',\n\t            barplot_data,\n\t            (len(barplot_data), 1),\n", "            title_bar,\n\t            score_name)\n"]}
{"filename": "src/extraction_benchmark/util.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport json\n\timport re\n\tdef read_jsonl(file):\n\t    \"\"\"\n\t    Read JSONL file and return iterable of dicts.\n\t    :param file: input filename\n\t    :return: iterable of dicts\n", "    \"\"\"\n\t    with open(file, 'r') as f:\n\t        for line in f:\n\t            yield json.loads(line)\n\tdef jsonl_to_dict(file):\n\t    \"\"\"\n\t    Load a JSONL into a single dict with ``\"page_id\"`` as keys.\n\t    :param file: input file name\n\t    :return: assembled dict\n\t    \"\"\"\n", "    loaded = {}\n\t    for j in read_jsonl(file):\n\t        loaded[j['page_id']] = {k: v for k, v in j.items() if k != 'page_id'}\n\t    return loaded\n\t_TOKEN_RE_WS = re.compile(r'\\s+', flags=re.UNICODE | re.MULTILINE)\n\tdef tokenize_ws(text):\n\t    \"\"\"\n\t    Tokenize text by white space.\n\t    :param text: input text\n\t    :return: list of tokens\n", "    \"\"\"\n\t    text = text.strip()\n\t    if not text:\n\t        return []\n\t    return _TOKEN_RE_WS.split(text)\n\t_TOKEN_RE_WORDS = re.compile(r'\\w+', flags=re.UNICODE)\n\tdef tokenize_words(text):\n\t    \"\"\"\n\t    Tokenize text by extracting Unicode word tokens (skips any non-word tokens).\n\t    :param text: input text\n", "    :return: list of tokens\n\t    \"\"\"\n\t    return _TOKEN_RE_WORDS.findall(text)\n"]}
{"filename": "src/extraction_benchmark/extract.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom contextlib import redirect_stderr, redirect_stdout\n\tfrom functools import partial\n\timport io\n\tfrom itertools import product\n\timport json\n\timport logging\n\tfrom multiprocessing import get_context\n", "import os\n\tfrom typing import Any, Dict\n\timport warnings\n\timport click\n\tfrom extraction_benchmark.dataset_readers import read_datasets, read_raw_dataset\n\tfrom extraction_benchmark.extractors import extractors\n\tfrom extraction_benchmark.paths import *\n\tdef _dict_to_jsonl(filepath, lines_dict: Dict[str, Any]):\n\t    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\t    with open(filepath, 'w') as f:\n", "        for page_id in sorted(lines_dict):\n\t            json.dump({'page_id': page_id, **lines_dict[page_id]}, f, indent=None, ensure_ascii=False)\n\t            f.write('\\n')\n\tdef extract_ground_truth(datasets):\n\t    \"\"\"\n\t    Convert ground truth from raw dataset to JSON format.\n\t    :param datasets: list of input dataset\n\t    :return: set of page IDs that were extracted\n\t    \"\"\"\n\t    page_ids = set()\n", "    for ds in datasets:\n\t        with click.progressbar(read_raw_dataset(ds, True), label=f'Converting ground truth of {ds}') as ds_progress:\n\t            extracted = {k: v for k, v in ds_progress}\n\t            page_ids.update(extracted.keys())\n\t        _dict_to_jsonl(os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl'), extracted)\n\t    return page_ids\n\tdef extract_raw_html(datasets, page_id_whitelist=None):\n\t    \"\"\"\n\t    Convert HTML files from raw dataset to JSON format.\n\t    :param datasets: list of input dataset\n", "    :param page_id_whitelist: optional list of page IDs to include (if set, IDs not in this list will be skipped)\n\t    \"\"\"\n\t    if page_id_whitelist and type(page_id_whitelist) is not set:\n\t        page_id_whitelist = set(page_id_whitelist)\n\t    for ds in datasets:\n\t        out_dir = os.path.join(DATASET_COMBINED_HTML_PATH, ds)\n\t        os.makedirs(out_dir, exist_ok=True)\n\t        with click.progressbar(read_raw_dataset(ds, False), label=f'Converting HTML of {ds}') as ds_progress:\n\t            for page_id, val in ds_progress:\n\t                if page_id_whitelist and page_id not in page_id_whitelist:\n", "                    continue\n\t                if not val.get('html'):\n\t                    continue\n\t                with open(os.path.join(out_dir, page_id + '.html'), 'w') as f:\n\t                    f.write(val['html'])\n\tdef _extract_with_model_expand_args(args, skip_existing=False, verbose=False):\n\t    _extract_with_model(*args, skip_existing=skip_existing, verbose=verbose)\n\tdef _extract_with_model(model, dataset, skip_existing=False, verbose=False):\n\t    model, model_name = model\n\t    out_path = os.path.join(MODEL_OUTPUTS_PATH, dataset, model_name + '.jsonl')\n", "    logger = logging.getLogger('wceb-extract')\n\t    logger.setLevel(logging.INFO if verbose else logging.ERROR)\n\t    extracted = {}\n\t    if skip_existing and os.path.isfile(out_path):\n\t        with open(out_path, 'r') as f:\n\t            for line in f:\n\t                j = json.loads(line)\n\t                extracted[j['page_id']] = {k: v for k, v in j.items() if k != 'page_id'}\n\t    with warnings.catch_warnings():\n\t        warnings.simplefilter('ignore')\n", "        for file_hash, in_data in read_datasets([dataset], False):\n\t            if file_hash in extracted:\n\t                continue\n\t            out_data = dict(plaintext='', model=model_name)\n\t            try:\n\t                with redirect_stdout(io.StringIO()) as stdout, redirect_stderr(io.StringIO()) as stderr:\n\t                    out_data['plaintext'] = model(in_data['html'], page_id=file_hash) or ''\n\t                if stdout.getvalue():\n\t                    logger.info(stdout.getvalue().strip())\n\t                if stderr.getvalue():\n", "                    logger.warning(stderr.getvalue().strip())\n\t            except Exception as e:\n\t                logger.warning(f'Error in model {model_name} while extracting {dataset} ({file_hash}):')\n\t                logger.warning(str(e))\n\t            extracted[file_hash] = out_data\n\t    if not extracted:\n\t        return\n\t    _dict_to_jsonl(out_path, extracted)\n\tdef extract(models, datasets, skip_existing, parallelism, verbose=False):\n\t    \"\"\"\n", "    Extract datasets with the selected extraction models.\n\t    :param models: list of extraction model names (if ``ground_truth == False``)\n\t    :param datasets: list of dataset names under \"datasets/raw\"\n\t    :param skip_existing: skip models for which an answer file exists already\n\t    :param parallelism: number of parallel workers\n\t    :param verbose: log error information\n\t    \"\"\"\n\t    model = [(getattr(extractors, 'extract_' + m), m) for m in models]\n\t    jobs = list(product(model, datasets))\n\t    def item_show_func(j):\n", "        if j:\n\t            return f'Model: {j[0][1]}, Dataset: {j[1]}'\n\t    if parallelism == 1:\n\t        with click.progressbar(jobs, label='Running extrators', item_show_func=item_show_func) as progress:\n\t            for job in progress:\n\t                _extract_with_model_expand_args(job)\n\t        return\n\t    with get_context('spawn').Pool(processes=parallelism) as pool:\n\t        try:\n\t            with click.progressbar(pool.imap_unordered(partial(_extract_with_model_expand_args,\n", "                                                               skip_existing=skip_existing, verbose=verbose), jobs),\n\t                                   length=len(jobs), label='Running extrators') as progress:\n\t                for _ in progress:\n\t                    pass\n\t        except KeyboardInterrupt:\n\t            pool.terminate()\n\t    click.echo(f'Model outputs written to {MODEL_OUTPUTS_PATH}')\n"]}
{"filename": "src/extraction_benchmark/dataset_readers.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom abc import ABC, abstractmethod\n\timport errno\n\timport glob\n\timport gzip\n\timport hashlib\n\tfrom itertools import chain\n\timport json\n", "import os\n\timport re\n\tfrom typing import Any, Dict, Iterable, Optional, Tuple\n\tfrom resiliparse.parse import bytes_to_str, detect_encoding\n\tfrom resiliparse.parse.html import HTMLTree, NodeType\n\tfrom extraction_benchmark.globals import DATASETS\n\tfrom extraction_benchmark.paths import *\n\tclass DatasetReader(ABC):\n\t    \"\"\"Abstract dataset reader class.\"\"\"\n\t    def __init__(self, ground_truth):\n", "        \"\"\"\n\t        Initialize dataset reader.\n\t        :param ground_truth: whether the reader should return the raw HTML data or the ground truth.\n\t        \"\"\"\n\t        self.is_truth = ground_truth\n\t        self._iter = iter(self.read())\n\t    def __iter__(self):\n\t        return self\n\t    def __next__(self):\n\t        return next(self._iter)\n", "    def __len__(self):\n\t        return self.dataset_size()\n\t    @abstractmethod\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        \"\"\"\n\t        Return an iterable over the items in the dataset.\n\t        Returned items should be tuples with the case / page ID and the case / page data as a dict.\n\t        The dicts should contain at least an ``\"html\"`` or ``\"plaintext\"`` key, depending on whether\n\t        the iterated dataset is a raw HTML or a ground truth page.\n\t        \"\"\"\n", "        pass\n\t    @abstractmethod\n\t    def dataset_size(self) -> Optional[int]:\n\t        \"\"\"\n\t        Return size of dataset or ``None`` if size is unknown.\n\t        :return: size of dataset\n\t        \"\"\"\n\t        pass\n\t    @staticmethod\n\t    def _hash(data: bytes):\n", "        \"\"\"\n\t        Return SHA-256 hash of input bytes, which can be used as a an ID.\n\t        :param data: input bytes\n\t        :return: hash of bytes as hex string\n\t        \"\"\"\n\t        m = hashlib.sha256()\n\t        m.update(data)\n\t        return m.hexdigest()\n\t    @classmethod\n\t    def _file_hash(cls, file: str):\n", "        \"\"\"\n\t        Return SHA-256 hash of a file that can be used as a page ID.\n\t        :param file: input file name\n\t        :return: hash of the file as hex string\n\t        \"\"\"\n\t        with open(file, 'rb') as f:\n\t            return cls._hash(f.read())\n\t    def _build_dict(self, source_dataset, source_case, content, **kwargs) -> Dict[str, Any]:\n\t        \"\"\"\n\t        Helper method for creating a dict to return in :meth:`read`.\n", "        :param source_dataset: source dataset name\n\t        :param source_case: source case / file name\n\t        :param content: HTML or plaintext content\n\t        :param kwargs: other key / value pairs to include in the dict\n\t        :return: dict with requested data\n\t        \"\"\"\n\t        d = {\n\t            ('plaintext' if self.is_truth else 'html'): content,\n\t            **{k: v for k, v in kwargs.items() if v},\n\t            'source': [source_dataset, source_case] if source_case else [source_dataset]\n", "        }\n\t        return d\n\t    @staticmethod\n\t    def _read_file(path, fixed_encoding=None):\n\t        \"\"\"\n\t        Helper method for reading a file, detecting its encoding and returning the contents as UTF-8 string.\n\t        If the input file is GZip-compressed, it will be decompressed automatically.\n\t        :param path: file path\n\t        :param fixed_encoding: use this fixed encoding instead of trying to detect if from the file\n\t        :return: UTF-8 string of file contents\n", "        \"\"\"\n\t        with open(path, 'rb') as f:\n\t            file_bytes = f.read()\n\t            if path.endswith('.gz'):\n\t                file_bytes = gzip.decompress(file_bytes)\n\t            if fixed_encoding:\n\t                enc = fixed_encoding\n\t            else:\n\t                enc = detect_encoding(file_bytes, max_len=100000, html5_compatible=False) or 'utf-8'\n\t            return bytes_to_str(file_bytes, encoding=enc, fallback_encodings=['utf-8', 'cp1252'])\n", "class CleanEvalReader(DatasetReader):\n\t    def __init__(self, ground_truth):\n\t        super().__init__(ground_truth)\n\t        self.dataset_name = 'cleaneval'\n\t        self.dataset_path = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'orig')\n\t        self.dataset_path_truth = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'clean')\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        read_path = self.dataset_path_truth if self.is_truth else self.dataset_path\n\t        text_tag_re = re.compile(r'(?:^<text [^>]+>\\s*|\\s*</text>$)', flags=re.MULTILINE)\n\t        for file in os.listdir(read_path):\n", "            abs_path = os.path.join(read_path, file)\n\t            content = self._read_file(abs_path)\n\t            url = None\n\t            if self.is_truth:\n\t                url = re.search(r'^\\s*URL: (https?://.+)', content)\n\t                if url:\n\t                    url = url.group(1)\n\t                content = HTMLTree.parse(content).body.text\n\t                content = re.sub(r'\\n +', '\\n', content)\n\t                content = re.sub(r'^\\s*URL:[^\\n]+\\s*', '', content)    # Strip URL line\n", "            if self.is_truth:\n\t                abs_path = os.path.join(self.dataset_path, os.path.splitext(file)[0] + '.html')\n\t            else:\n\t                content = text_tag_re.sub('', content)\n\t            source = os.path.splitext(file)[0]\n\t            yield self._file_hash(abs_path), self._build_dict(self.dataset_name, source, content, url=url)\n\t    def dataset_size(self) -> Optional[int]:\n\t        return len(glob.glob(os.path.join(DATASET_RAW_PATH, self.dataset_name, 'clean', '*.txt')))\n\tclass CleanPortalEvalReader(CleanEvalReader):\n\t    def __init__(self, ground_truth):\n", "        super().__init__(ground_truth)\n\t        self.dataset_name = 'cleanportaleval'\n\t        self.dataset_path = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'input')\n\t        self.dataset_path_truth = os.path.join(DATASET_RAW_PATH, self.dataset_name, 'GoldStandard')\n\t    def dataset_size(self) -> Optional[int]:\n\t        return len(glob.glob(os.path.join(DATASET_RAW_PATH, self.dataset_name, 'GoldStandard', '*.txt')))\n\tclass DragnetReader(DatasetReader):\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        dataset_path = os.path.join(DATASET_RAW_PATH, 'dragnet', 'HTML')\n\t        dataset_path_truth = os.path.join(DATASET_RAW_PATH, 'dragnet', 'corrected', 'Corrected')\n", "        read_path = dataset_path_truth if self.is_truth else dataset_path\n\t        for file in os.listdir(read_path):\n\t            abs_path = os.path.join(read_path, file)\n\t            content = self._read_file(abs_path)\n\t            if self.is_truth:\n\t                file = os.path.splitext(os.path.splitext(file)[0])[0]\n\t                abs_path = os.path.join(dataset_path, file)\n\t            source = os.path.splitext(file)[0]\n\t            yield self._file_hash(abs_path), self._build_dict('dragnet', source, content)\n\t    def dataset_size(self) -> Optional[int]:\n", "        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'dragnet', 'corrected', 'Corrected', '*.txt')))\n\tclass CETDReader(DatasetReader):\n\t    def __init__(self, ground_truth):\n\t        super().__init__(ground_truth)\n\t        self.verticals = ['arstechnica', 'BBC', 'Chaos', 'nytimes', 'wiki', 'YAHOO!']\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        dataset_path = os.path.join(DATASET_RAW_PATH, 'cetd')\n\t        for vertical in self.verticals:\n\t            sub_path = os.path.join(dataset_path, vertical, 'gold' if self.is_truth else 'original')\n\t            for file in os.listdir(sub_path):\n", "                abs_path = os.path.join(sub_path, file)\n\t                content = self._read_file(abs_path)\n\t                if self.is_truth:\n\t                    abs_path = os.path.join(dataset_path, vertical, 'original', os.path.splitext(file)[0] + '.htm')\n\t                source = vertical + '_' + os.path.splitext(file)[0]\n\t                yield self._file_hash(abs_path), self._build_dict('cetd', source, content)\n\t    def dataset_size(self) -> Optional[int]:\n\t        return sum([len(glob.glob(os.path.join(DATASET_RAW_PATH, 'cetd', v, 'gold', '*.txt')))\n\t                    for v in self.verticals])\n\tclass ReadabilityReader(DatasetReader):\n", "    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        dataset_path = os.path.join(DATASET_RAW_PATH, 'readability', 'test-pages')\n\t        for case_dir in os.listdir(dataset_path):\n\t            sub_path = 'expected.html' if self.is_truth else 'source.html'\n\t            abs_path = os.path.join(dataset_path, os.path.join(case_dir, sub_path))\n\t            content = self._read_file(abs_path)\n\t            if self.is_truth:\n\t                content = HTMLTree.parse(content).body.text\n\t                abs_path = os.path.join(dataset_path, os.path.join(case_dir, 'source.html'))\n\t            yield self._file_hash(abs_path), self._build_dict('readability', case_dir, content)\n", "    def dataset_size(self) -> Optional[int]:\n\t        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'readability', 'test-pages', '*', 'expected.html')))\n\tclass ScrapingHubReader(DatasetReader):\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        dataset_path = os.path.join(DATASET_RAW_PATH, 'scrapinghub')\n\t        if self.is_truth:\n\t            truth_json = json.load(open(os.path.join(dataset_path, 'ground-truth.json'), 'r'))\n\t            for k, v in truth_json.items():\n\t                # Instead of using provided hash, re-calculate hash from original HTML file for consistency\n\t                with gzip.GzipFile(os.path.join(dataset_path, 'html', f'{k}.html.gz'), 'r') as f:\n", "                    file_hash = self._hash(f.read())\n\t                yield file_hash, self._build_dict('scrapinghub', k, v['articleBody'], url=v['url'])\n\t            return\n\t        dataset_path = os.path.join(dataset_path, 'html')\n\t        for file in os.listdir(dataset_path):\n\t            abs_path = os.path.join(dataset_path, file)\n\t            hash_id = os.path.splitext(os.path.splitext(file)[0])[0]\n\t            with gzip.GzipFile(abs_path, 'r') as f:\n\t                file_hash = self._hash(f.read())\n\t            yield file_hash, self._build_dict('scrapinghub', hash_id, self._read_file(abs_path))\n", "    def dataset_size(self) -> Optional[int]:\n\t        return len(json.load(open(os.path.join(DATASET_RAW_PATH, 'scrapinghub', 'ground-truth.json'), 'r')))\n\tclass L3SGN1Reader(DatasetReader):\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        dataset_path = os.path.join(DATASET_RAW_PATH, 'l3s-gn1', 'original')\n\t        dataset_path_truth = os.path.join(DATASET_RAW_PATH, 'l3s-gn1', 'annotated')\n\t        read_path = dataset_path_truth if self.is_truth else dataset_path\n\t        for file in os.listdir(read_path):\n\t            abs_path = os.path.join(read_path, file)\n\t            content = self._read_file(abs_path)\n", "            if self.is_truth:\n\t                abs_path = os.path.join(dataset_path, file)\n\t                content = self._extract_with_css_selector(content, '.x-nc-sel1, .x-nc-sel2, .x-nc-sel3')\n\t            source = os.path.splitext(file)[0]\n\t            yield self._file_hash(abs_path), self._build_dict('l3s-gn1', source, content)\n\t    def dataset_size(self) -> Optional[int]:\n\t        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'l3s-gn1', 'annotated', '*.html')))\n\t    @staticmethod\n\t    def _extract_with_css_selector(html, selector):\n\t        tree = HTMLTree.parse(html)\n", "        elements = tree.body.query_selector_all(selector)\n\t        content = ''\n\t        for e in elements:\n\t            if len(e.child_nodes) != 1 or e.first_child.type != NodeType.TEXT:\n\t                # Only count leaf nodes to avoid adding an element multiple times\n\t                continue\n\t            if e.parent.tag in ['address', 'article', 'aside', 'blockquote', 'canvas', 'dd', 'div', 'dl', 'dt',\n\t                                'fieldset',\n\t                                'figcaption', 'figure', 'footer', 'form', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'header',\n\t                                'hr', 'li', 'main', 'nav', 'noscript', 'ol', 'p', 'pre', 'section', 'table', 'tfoot',\n", "                                'ul', 'video']:\n\t                content += '\\n'\n\t            content += e.text.strip() + ' '\n\t        return content.strip()\n\tclass GoogleTrends2017Reader(L3SGN1Reader):\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        dataset_path = os.path.join(DATASET_RAW_PATH, 'google-trends-2017', 'raw_html')\n\t        dataset_path_truth = os.path.join(DATASET_RAW_PATH, 'google-trends-2017', 'prepared_html')\n\t        read_path = dataset_path_truth if self.is_truth else dataset_path\n\t        for file in os.listdir(read_path):\n", "            abs_path = os.path.join(read_path, file)\n\t            content = self._read_file(abs_path)\n\t            if self.is_truth:\n\t                abs_path = os.path.join(dataset_path, file)\n\t                content = self._extract_with_css_selector(content, '[__boilernet_label=\"1\"]')\n\t            source = os.path.splitext(file)[0]\n\t            yield self._file_hash(abs_path), self._build_dict('google-trends-2017', source, content)\n\t    def dataset_size(self) -> Optional[int]:\n\t        return len(glob.glob(os.path.join(DATASET_RAW_PATH, 'google-trends-2017', 'prepared_html', '*.html')))\n\tclass CombinedDatasetReader(DatasetReader):\n", "    def __init__(self, ground_truth, read_subsets=None):\n\t        super().__init__(ground_truth)\n\t        self.subsets = sorted(read_subsets) if read_subsets else sorted(DATASETS)\n\t    def read(self) -> Iterable[Tuple[str, Dict[str, Any]]]:\n\t        if self.is_truth:\n\t            for ds in self.subsets:\n\t                with open(os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl')) as f:\n\t                    for line in f:\n\t                        j = json.loads(line)\n\t                        yield j['page_id'], {k: v for k, v in j.items() if k != 'page_id'}\n", "            return\n\t        for ds in self.subsets:\n\t            for filename in glob.glob(os.path.join(DATASET_COMBINED_HTML_PATH, ds, '*.html')):\n\t                page_id = os.path.splitext(os.path.basename(filename))[0]\n\t                yield page_id, self._build_dict(ds, page_id, self._read_file(filename, 'utf-8'))\n\t    def dataset_size(self) -> Optional[int]:\n\t        # Count lines in all truth files\n\t        return sum(chain(*((1 for _ in open(\n\t            os.path.join(DATASET_COMBINED_TRUTH_PATH, f'{ds}.jsonl'), 'r')) for ds in self.subsets)))\n\tdef read_raw_dataset(dataset, ground_truth):\n", "    \"\"\"Read raw (unprocessed datasets).\"\"\"\n\t    match dataset:\n\t        case 'cetd':\n\t            return CETDReader(ground_truth)\n\t        case 'cleaneval':\n\t            return CleanEvalReader(ground_truth)\n\t        case 'cleanportaleval':\n\t            return CleanPortalEvalReader(ground_truth)\n\t        case 'dragnet':\n\t            return DragnetReader(ground_truth)\n", "        case 'google-trends-2017':\n\t            return GoogleTrends2017Reader(ground_truth)\n\t        case 'l3s-gn1':\n\t            return L3SGN1Reader(ground_truth)\n\t        case 'readability':\n\t            return ReadabilityReader(ground_truth)\n\t        case 'scrapinghub':\n\t            return ScrapingHubReader(ground_truth)\n\t        case _:\n\t            raise ValueError(f'Invalid dataset: {dataset}')\n", "def read_datasets(datasets: Iterable[str], ground_truth):\n\t    \"\"\"Read (subsets of) processed and combined datasets.\"\"\"\n\t    if not os.path.isdir(DATASET_COMBINED_PATH):\n\t        raise FileNotFoundError(errno.ENOENT, 'Combined dataset folder not found', DATASET_COMBINED_PATH)\n\t    return CombinedDatasetReader(ground_truth, datasets)\n"]}
{"filename": "src/extraction_benchmark/paths.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport os\n\tROOT_PATH = os.path.realpath(os.path.join(os.getcwd()))\n\tDATASET_PATH = os.path.join(ROOT_PATH, 'datasets')\n\tDATASET_RAW_PATH = os.path.join(DATASET_PATH, 'raw')\n\tDATASET_COMBINED_PATH = os.path.join(DATASET_PATH, 'combined')\n\tDATASET_COMBINED_TRUTH_PATH = os.path.join(DATASET_COMBINED_PATH, 'ground-truth')\n\tDATASET_COMBINED_HTML_PATH = os.path.join(DATASET_COMBINED_PATH, 'html')\n", "OUTPUTS_PATH = os.path.join(ROOT_PATH, 'outputs')\n\tHTML_FEATURES_PATH = os.path.join(OUTPUTS_PATH, 'html-features')\n\tMODEL_OUTPUTS_PATH = os.path.join(OUTPUTS_PATH, 'model-outputs')\n\tMETRICS_PATH = os.path.join(OUTPUTS_PATH, 'metrics-computed')\n\tMETRICS_AGG_PATH = os.path.join(METRICS_PATH, '_aggregated')\n\tMETRICS_COMPLEXITY_PATH = os.path.join(METRICS_PATH, '_complexity')\n\tTHIRD_PARTY_PATH = os.path.join(ROOT_PATH, 'third-party')\n"]}
{"filename": "src/extraction_benchmark/complexity.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport os.path\n\tfrom collections import defaultdict\n\tfrom multiprocessing import Pool\n\timport re\n\timport click\n\timport pandas as pd\n\timport numpy as np\n", "from sklearn.cluster import KMeans\n\tfrom sklearn.decomposition import PCA\n\tfrom sklearn.linear_model import LogisticRegression\n\tfrom sklearn.manifold import TSNE\n\tfrom sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef, precision_score, recall_score\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.preprocessing import StandardScaler\n\tfrom resiliparse.parse.html import HTMLTree\n\tfrom extraction_benchmark.dataset_readers import read_datasets\n\tfrom extraction_benchmark.globals import *\n", "from extraction_benchmark import plt\n\tfrom extraction_benchmark.util import tokenize_words\n\tdef calculate(datasets):\n\t    \"\"\"\n\t    Calculate page complexities for pages in the given datasets based on the ground truth.\n\t    :param datasets: list of dataset names\n\t    \"\"\"\n\t    complexity_total = pd.DataFrame(columns=['complexity'])\n\t    complexity_total.index.name = 'hash_key'\n\t    quantile_labels = [0.25, 0.33, 0.5, 0.66, 0.75]\n", "    os.makedirs(METRICS_COMPLEXITY_PATH, exist_ok=True)\n\t    with click.progressbar(datasets, label='Calculating page complexity scores') as ds_progress:\n\t        for ds in ds_progress:\n\t            tokens_truth = {}\n\t            tokens_src = {}\n\t            for h, truth in read_datasets([ds], True):\n\t                tokens_truth[h] = len(tokenize_words(truth['plaintext']))\n\t            for h, src in read_datasets([ds], False):\n\t                if h not in tokens_truth:\n\t                    continue\n", "                # Extract all text tokens except script / style\n\t                tree = HTMLTree.parse(src['html'])\n\t                for e in tree.body.query_selector_all('script, style'):\n\t                    e.decompose()\n\t                tokens_src[h] = len(tokenize_words(tree.body.text))\n\t            tokens_truth = pd.DataFrame.from_dict(tokens_truth, orient='index')\n\t            tokens_src = pd.DataFrame.from_dict(tokens_src, orient='index')\n\t            out_path_ds = os.path.join(METRICS_COMPLEXITY_PATH, ds)\n\t            os.makedirs(out_path_ds, exist_ok=True)\n\t            complexity = 1 - (tokens_truth / tokens_src).clip(lower=0, upper=1)\n", "            complexity.index.name = 'hash_key'\n\t            complexity.columns = ['complexity']\n\t            complexity.to_csv(os.path.join(out_path_ds, f'{ds}_complexity.csv'))\n\t            complexity['dataset'] = ds\n\t            quantiles = complexity['complexity'].quantile(quantile_labels)\n\t            quantiles.to_csv(os.path.join(out_path_ds, f'{ds}_complexity_quantiles.csv'))\n\t            complexity_total = pd.concat([complexity_total, complexity])\n\t    complexity_total.reset_index(inplace=True)\n\t    complexity_total.set_index(['hash_key', 'dataset'], inplace=True)\n\t    quantiles = complexity_total.quantile(quantile_labels)\n", "    quantiles.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity_quantiles.csv'))\n\t    complexity_total.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity.csv'))\n\t    click.echo(f'Complexity scores written to \"{METRICS_COMPLEXITY_PATH}\".')\n\t_WS_RE = re.compile(r'\\s+', flags=re.UNICODE | re.MULTILINE)\n\tdef extract_html_features(html):\n\t    tree = HTMLTree.parse(html)\n\t    for e in tree.body.query_selector_all('script, style, noscript'):\n\t        e.decompose()\n\t    text = _WS_RE.sub(' ', tree.body.text)\n\t    features = defaultdict(float)\n", "    all_tags = tree.body.query_selector_all('*')\n\t    n_tags = len(all_tags)\n\t    if n_tags != 0:\n\t        features['h1'] = len(tree.body.query_selector_all('h1')) / n_tags\n\t        features['h2'] = len(tree.body.query_selector_all('h2')) / n_tags\n\t        features['h3'] = len(tree.body.query_selector_all('h3')) / n_tags\n\t        features['h4'] = len(tree.body.query_selector_all('h4')) / n_tags\n\t        features['h5'] = len(tree.body.query_selector_all('h5')) / n_tags\n\t        features['h6'] = len(tree.body.query_selector_all('h6')) / n_tags\n\t        features['p'] = len(tree.body.query_selector_all('p')) / n_tags\n", "        features['ul'] = len(tree.body.query_selector_all('li')) / n_tags\n\t        features['table'] = len(tree.body.query_selector_all('table')) / n_tags\n\t        features['a'] = len(tree.body.query_selector_all('a')) / n_tags\n\t        features['div'] = len(tree.body.query_selector_all('div')) / n_tags\n\t        features['br'] = len(tree.body.query_selector_all('br')) / n_tags\n\t        features['strong'] = len(tree.body.query_selector_all('strong')) / n_tags\n\t        features['em'] = len(tree.body.query_selector_all('em')) / n_tags\n\t    features['html_to_non_html'] = n_tags / len(tokenize_words(text))\n\t    return features\n\tdef calculate_dataset_features(dataset):\n", "    df = pd.DataFrame()\n\t    for hash_key, data in read_datasets([dataset], False):\n\t        features = extract_html_features(data['html'])\n\t        s = pd.Series(features, name=hash_key)\n\t        df = pd.concat([df, s.to_frame().T])\n\t    df.index.name = 'hash_key'\n\t    out_dir = os.path.join(HTML_FEATURES_PATH, dataset)\n\t    os.makedirs(out_dir, exist_ok=True)\n\t    df.to_csv(os.path.join(out_dir, f'{dataset}_html_features.csv'))\n\tdef tsne_reduce_dim(X, n_components):\n", "    return TSNE(n_components=n_components,\n\t                learning_rate='auto',\n\t                init='random',\n\t                perplexity=30,\n\t                method='barnes_hut',\n\t                n_jobs=-1,\n\t                verbose=1).fit_transform(X)\n\tdef extract_page_features(dataset, parallelism):\n\t    with Pool(processes=parallelism) as pool:\n\t        with click.progressbar(pool.imap_unordered(calculate_dataset_features, dataset),\n", "                               length=len(dataset), label='Extracting dataset features') as progress:\n\t            for _ in progress:\n\t                pass\n\tdef _load_html_features(dataset):\n\t    df_features = pd.DataFrame()\n\t    df_complexity = pd.DataFrame()\n\t    with click.progressbar(dataset, label='Loading datasets') as progress:\n\t        for ds in progress:\n\t            df_tmp = pd.read_csv(os.path.join(HTML_FEATURES_PATH, ds, f'{ds}_html_features.csv'))\n\t            df_tmp['dataset'] = ds\n", "            df_features = pd.concat([df_features, df_tmp], ignore_index=True)\n\t            df_tmp = pd.read_csv(os.path.join(METRICS_COMPLEXITY_PATH, ds, f'{ds}_complexity.csv'))\n\t            df_tmp['dataset'] = ds\n\t            df_complexity = pd.concat([df_complexity, df_tmp], ignore_index=True)\n\t    df_features.set_index(['hash_key', 'dataset'], inplace=True)\n\t    df_complexity.set_index(['hash_key', 'dataset'], inplace=True)\n\t    return df_features.join(df_complexity, how='inner')\n\tdef _reduce_dim_2d(df, label_column):\n\t    click.echo('Reducing dimensionality to 2D for visualization...')\n\t    scaler = StandardScaler()\n", "    X = scaler.fit_transform(df)\n\t    X = tsne_reduce_dim(X, 2)\n\t    df_2d = pd.DataFrame(X, columns=['x', 'y'], index=df.index)\n\t    df_2d[label_column] = df[label_column]\n\t    df_2d['complexity'] = df['complexity']\n\t    return df_2d\n\tdef _binarize_complexity(values, quantile):\n\t    p = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_quantiles.csv')\n\t    if not os.path.isfile(p):\n\t        raise click.FileError(p, 'Please calculate page complexity quantiles first.')\n", "    quantiles = pd.read_csv(p, index_col=0)\n\t    return [int(x >= quantiles.loc[float(quantile)]['complexity']) for x in values]\n\tdef logistic_regression_classify(dataset, train_split_size, quantile):\n\t    df_features = _load_html_features(dataset)\n\t    df_features['complexity'] = _binarize_complexity(df_features['complexity'], quantile)\n\t    click.echo('Training classifier and predicting pages...')\n\t    idx_train, idx_test = train_test_split(df_features.index.values, train_size=train_split_size)\n\t    df_train = df_features[df_features.index.isin(idx_train)]\n\t    df_test = df_features[~df_features.index.isin(idx_train)]\n\t    scaler = StandardScaler()\n", "    X_train = scaler.fit_transform(df_train.drop(columns='complexity'))\n\t    X_test = scaler.fit_transform(df_test.drop(columns='complexity'))\n\t    y_pred = LogisticRegression().fit(X_train, df_train['complexity']).predict(X_test)\n\t    df_test = df_test.assign(logreg_label=y_pred)\n\t    df_test.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_classes.csv'))\n\t    click.echo(f'Classification written to \"{METRICS_COMPLEXITY_PATH}\"')\n\tdef kmeans_cluster(dataset, reduce_dim, n_clusters):\n\t    df_features = _load_html_features(dataset)\n\t    scaler = StandardScaler()\n\t    X = scaler.fit_transform(df_features.drop(columns='complexity'))\n", "    if reduce_dim:\n\t        X = PCA(n_components=reduce_dim).fit_transform(X)\n\t    click.echo('Clustering datapoints...')\n\t    labels = KMeans(n_clusters=n_clusters, max_iter=500, n_init=30).fit(X).labels_\n\t    # Ensure cluster labels are aligned with quantiles\n\t    if sum(labels[labels == 1]) < len(labels[labels == 0]):\n\t        labels = 1 - labels\n\t    df_features['kmeans_label'] = labels\n\t    df_features.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters.csv'))\n\t    click.echo(f'Clustering written to \"{METRICS_COMPLEXITY_PATH}\"')\n", "def _plot_scatter_axis(df, ax, label_col, title, labels):\n\t    for i, l in enumerate(labels):\n\t        filtered = df[df[label_col] == i]\n\t        ax.scatter(\n\t            x=filtered['x'],\n\t            y=filtered['y'],\n\t            s=4,\n\t            alpha=0.75,\n\t            label=l,\n\t        )\n", "    ax.legend(loc='lower right', fontsize='small', borderpad=0.4, shadow=False,\n\t              handlelength=0.5, handletextpad=0.5, edgecolor='none')\n\t    ax.set_title(title, fontsize='medium')\n\t    ax.spines['top'].set_visible(False)\n\t    ax.set_xticks(ticks=np.linspace(*ax.get_xlim(), 5), labels=[])\n\t    ax.set_yticks(ticks=np.linspace(*ax.get_ylim(), 5), labels=[])\n\t    ax.spines['right'].set_visible(False)\n\tdef visualize_clusters(quantile):\n\t    \"\"\"\n\t    Visualize clusters of HTML page features.\n", "    :param quantile: complexity quantile to align with cluster boundaries\n\t    \"\"\"\n\t    in_path = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters.csv')\n\t    if not os.path.isfile(in_path):\n\t        raise click.FileError(in_path, 'Please calculate page complexities first.')\n\t    df = pd.read_csv(in_path, index_col=['hash_key', 'dataset'])\n\t    df['complexity'] = _binarize_complexity(df['complexity'], quantile)\n\t    df_2d = _reduce_dim_2d(df, 'kmeans_label')\n\t    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(5, 2.5))\n\t    _plot_scatter_axis(df_2d, ax1, 'kmeans_label', '$k$-Means Clustering', ['Cluster 0', 'Cluster 1'])\n", "    _plot_scatter_axis(df_2d, ax2, 'complexity', 'Complexity Quantiles', ['Low', 'High'])\n\t    plt.tight_layout(pad=0.5)\n\t    plt.savefig(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters_2d.pdf'))\n\t    df_2d.to_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_clusters_2d.csv'))\n\t    click.echo(f'Plots written to \"{METRICS_COMPLEXITY_PATH}')\n\tdef visualize_classes():\n\t    \"\"\"\n\t    Visualize predicted classes of HTML page features.\n\t    \"\"\"\n\t    in_path = os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_classes.csv')\n", "    if not os.path.isfile(in_path):\n\t        raise click.FileError(in_path, 'Please calculate page complexities first.')\n\t    df = pd.read_csv(in_path, index_col=['hash_key', 'dataset'])\n\t    df_2d = _reduce_dim_2d(df, 'logreg_label')\n\t    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(5, 2.5))\n\t    _plot_scatter_axis(df_2d, ax1, 'logreg_label', 'Predicted Classes', ['Low', 'High'])\n\t    _plot_scatter_axis(df_2d, ax2, 'complexity', 'Complexity Quantiles', ['Low', 'High'])\n\t    plt.tight_layout(pad=0.5)\n\t    plt.savefig(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity_classes_2d.pdf'))\n\t    click.echo(f'Plots written to \"{METRICS_COMPLEXITY_PATH}\\n')\n", "    acc = accuracy_score(df_2d['complexity'], df_2d['logreg_label'])\n\t    mcc = matthews_corrcoef(df_2d['complexity'], df_2d['logreg_label'])\n\t    f1 = f1_score(df_2d['complexity'], df_2d['logreg_label'])\n\t    prec = precision_score(df_2d['complexity'], df_2d['logreg_label'])\n\t    rec = recall_score(df_2d['complexity'], df_2d['logreg_label'])\n\t    click.echo(f'MCC: {mcc:.3f}')\n\t    click.echo(f'Accuracy: {acc:.3f}')\n\t    click.echo(f'F1 Score: {f1:.3f}')\n\t    click.echo(f'Precision: {prec:.3f}')\n\t    click.echo(f'Recall: {rec:.3f}')\n", "def visualize_datasets(datasets, low_quantile='0.25', high_quantile='0.75'):\n\t    \"\"\"\n\t    Visualize median complexity of the datasets.\n\t    :param datasets: list of dataset names\n\t    :param low_quantile: low-complexity quantile threshold\n\t    :param high_quantile: high-complexity quantile threshold\n\t    \"\"\"\n\t    complexities = []\n\t    with click.progressbar(datasets, label='Loading datasets') as progress:\n\t        for ds in progress:\n", "            path = os.path.join(METRICS_COMPLEXITY_PATH, ds, f'{ds}_complexity.csv')\n\t            if not os.path.isfile(path):\n\t                continue\n\t            complexities.append(pd.read_csv(path)['complexity'])\n\t    # Sort by median\n\t    complexities, datasets = zip(*sorted(zip(complexities, datasets), key=lambda x: x[0].median(), reverse=True))\n\t    plt.figure(figsize=(5, 3))\n\t    plt.boxplot(\n\t        complexities,\n\t        positions=range(len(complexities)),\n", "        labels=[DATASETS[d] for d in datasets],\n\t    )\n\t    plt.ylabel('Page Complexity')\n\t    plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n\t    plt.gca().spines['top'].set_visible(False)\n\t    plt.gca().spines['right'].set_visible(False)\n\t    plt.ylim((-0.1, 1.1))\n\t    for y in plt.gca().get_yticks():\n\t        plt.axhline(y, linewidth=0.25, color='lightgrey', zorder=-1)\n\t    plt.tight_layout()\n", "    plt.savefig(os.path.join(METRICS_COMPLEXITY_PATH, f'complexity.pdf'))\n\t    complexity_quantiles = pd.read_csv(os.path.join(METRICS_COMPLEXITY_PATH, 'complexity_quantiles.csv'), index_col=0)\n\t    quantile_threshold_low = complexity_quantiles.loc[float(low_quantile)]['complexity']\n\t    quantile_threshold_high = complexity_quantiles.loc[float(high_quantile)]['complexity']\n\t    # Sort back into alphabetical order\n\t    complexities, datasets = zip(*sorted(zip(complexities, datasets), key=lambda x: x[1]))\n\t    click.echo('Dataset stats:')\n\t    click.echo('--------------')\n\t    for i, compl in enumerate(complexities):\n\t        low_count = compl[compl < quantile_threshold_low].count()\n", "        medium_count = compl[(compl >= quantile_threshold_low) & (compl < quantile_threshold_high)].count()\n\t        high_count = compl[compl >= quantile_threshold_high].count()\n\t        click.echo(f'{datasets[i]:<20} ', nl=False)\n\t        click.echo(f'pages: {compl.count():<8} ', nl=False)\n\t        click.echo(f'pages low: {low_count:<8} ', nl=False)\n\t        click.echo(f'pages medium: {medium_count:<8} ', nl=False)\n\t        click.echo(f'pages high: {high_count:<8} ', nl=False)\n\t        click.echo(f'median complexity: {compl.median():.2f}', nl=False)\n\t        click.echo()\n\t    click.echo()\n", "    click.echo(f'Complexity plots written to \"{METRICS_COMPLEXITY_PATH}\".')\n"]}
{"filename": "src/extraction_benchmark/extractors/bte.py", "chunked_list": ["#!/usr/bin/env python\n\t\"\"\"\n\tThis module implements Finn's BTE (Body Text Extraction) algorithm for\n\textracting the main body text of a web page and avoiding the surrounding\n\tirrelevant information. The description of the algorithm can be found\n\tin A. Finn, N. Kushmerick, and B. Smyth. Fact or Fiction: Content\n\tclassification for digital libraries. In DELOS Workshop: Personalisation\n\tand Recommender System in Digital Libraries, 2001.\n\tPython implementation by Jan Pomikalek <xpomikal@fi.muni.cz>\n\t\"\"\"\n", "import re\n\tdef html2text(html_text, preserve_par=False, preserve_head_list_par=False):\n\t    \"\"\"\n\t    Converts HTML to plain text with boilerplate removed.\n\t    If preserve_par is set to True, paragraph mark-up will be preserverd.\n\t    If preserve_head_list_par is set to True, paragraph mark-up will be\n\t    preserverd and headers and list items marked with <h> and <l> tags\n\t    respectively.\n\t    \"\"\"\n\t    cleaned_text = preclean(html_text)\n", "    tokens = tokenise(cleaned_text)\n\t    (start, end) = bte(tokens)\n\t    main_body = tokens[start:end+1]\n\t    cleaned_body = find_paragraphs(main_body, preserve_head_list_par)\n\t    # separate paragraphs with newlines\n\t    blocks = []\n\t    block = []\n\t    for token in cleaned_body:\n\t        if token_value(token) > 0:\n\t            block.append(token)\n", "        else:\n\t            if len(block) > 0:\n\t                blocks.append(\" \".join(block))\n\t                block = []\n\t            if preserve_par or preserve_head_list_par:\n\t                block.append(token)\n\t    if len(block) > 0:\n\t        blocks.append(\" \".join(block))\n\t    return \"\\n\".join(blocks)\n\tdef preclean(html_text):\n", "    \"\"\"\n\t    HTML preprocessing -- striping headers, scripts, styles; replacing HTML\n\t    entities.\n\t    \"\"\"\n\t    # strip all but body\n\t    cleaned_text = re.compile('^.*<body(\\s+[^>]*)?>', re.S | re.I\n\t            ).sub('', html_text)\n\t    cleaned_text = re.compile('</body>.*$', re.S | re.I\n\t            ).sub('', cleaned_text)\n\t    # strip scripts\n", "    cleaned_text = re.compile('<script(\\s+[^>]*)?>(.|\\s)*?</script>',\n\t            re.S | re.I).sub('<script></script>', cleaned_text)\n\t    # strip styles\n\t    cleaned_text = re.compile('<style(\\s+[^>]*)?>(.|\\s)*?</style>',\n\t            re.S | re.I).sub('<style></style>', cleaned_text)\n\t    # html entities\n\t    cleaned_text = html_entities(cleaned_text)\n\t    return cleaned_text\n\tdef html_entities(html_text):\n\t    \"Substitution of the most commonly used HTML entities.\"\n", "    html_text = re.sub('&quot;', '\"', html_text)\n\t    html_text = re.sub('&nbsp;', ' ', html_text)\n\t    html_text = re.sub('&#39;', \"'\", html_text)\n\t    return html_text\n\tdef tokenise(html_text):\n\t    \"\"\"\n\t    Tokenises HTML document to a sequence of HTML tags and strings of\n\t    non-whitespace characters (words).\n\t    \"\"\"\n\t    return [g1 for (g1, g2) in re.findall('(<([^>]|\\s)+>|[^\\s<]+)', html_text)]\n", "def bte(tokens):\n\t    \"\"\"\n\t    BTE algorithm. Expects a sequence of HTML tags and words as input parameter.\n\t    Outputs a pair of indices which indicate the beginning and end of the main\n\t    body.\n\t    \"\"\"\n\t    # find breakpoints\n\t    breakpoints = []\n\t    prev_value = None\n\t    sum_value = 0\n", "    for i in range(len(tokens)):\n\t        cur_value = token_value(tokens[i])\n\t        if prev_value and cur_value != prev_value:\n\t            breakpoints.append((i-1, sum_value))\n\t            sum_value = 0\n\t        sum_value+= cur_value\n\t        prev_value = cur_value\n\t    breakpoints.append((len(tokens)-1, sum_value))\n\t    # find breakpoints range which maximises the score\n\t    max_score = 0\n", "    max_start = 0\n\t    max_end   = 0\n\t    for i in range(len(breakpoints)):\n\t        score = breakpoints[i][1]\n\t        if score > max_score:\n\t            max_score = score\n\t            if i > 0: max_start = breakpoints[i-1][0]+1\n\t            else:     max_start = 0\n\t            max_end   = breakpoints[i][0]\n\t        for j in range(i+1, len(breakpoints)):\n", "            score+= breakpoints[j][1]\n\t            if score > max_score:\n\t                max_score = score\n\t                if i > 0: max_start = breakpoints[i-1][0]+1\n\t                else:     max_start = 0\n\t                max_end   = breakpoints[j][0]\n\t    return (max_start, max_end)\n\tdef token_value(token):\n\t    \"Returns -1 if the token is HTML tag, 1 otherwise (if word).\"\n\t    if token.startswith('<'):\n", "        return -1\n\t    else:\n\t        return 1\n\tdef find_paragraphs(tokens, tag_h_l=False):\n\t    \"\"\"\n\t    Marks paragraph blocks with <p>. If tag_h_l set to True, headers and\n\t    list items are also detected and marked with <h> and <l> respectively.\n\t    \"\"\"\n\t    PAR_FIND_TAGS = ['p', 'div', 'hr', 'blockquote', 'table']\n\t    PAR_REPLACE_TAG = '<p>'\n", "    HEADER_FIND_TAGS = ['h1', 'h2', 'h3']\n\t    HEADER_REPLACE_TAG = '<h>'\n\t    LIST_FIND_TAGS = ['li']\n\t    LIST_REPLACE_TAG = '<l>'\n\t    result = [PAR_REPLACE_TAG]\n\t    in_paragraph = False\n\t    for token in tokens:\n\t        if token_value(token) > 0:\n\t            result.append(token)\n\t            in_paragraph = True\n", "        else:\n\t            if not in_paragraph:\n\t                continue\n\t            m = re.search('^<([^\\s>]+)', token)\n\t            if not m:\n\t                continue\n\t            tag = m.group(1).lower()\n\t            if tag in PAR_FIND_TAGS:\n\t                result.append(PAR_REPLACE_TAG)\n\t                in_paragraph = False\n", "            if tag in HEADER_FIND_TAGS:\n\t                if tag_h_l:\n\t                    result.append(HEADER_REPLACE_TAG)\n\t                else:\n\t                    result.append(PAR_REPLACE_TAG)\n\t                in_paragraph = False\n\t            if tag in LIST_FIND_TAGS:\n\t                if tag_h_l:\n\t                    result.append(LIST_REPLACE_TAG)\n\t                else:\n", "                    result.append(PAR_REPLACE_TAG)\n\t                in_paragraph = False\n\t    return result\n"]}
{"filename": "src/extraction_benchmark/extractors/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .extractors import list_extractors\n"]}
{"filename": "src/extraction_benchmark/extractors/ensemble.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom collections import defaultdict\n\tfrom resiliparse.extract.html2text import extract_plain_text\n\tfrom resiliparse.parse.html import HTMLTree\n\tfrom extraction_benchmark.paths import *\n\tfrom extraction_benchmark.util import read_jsonl, tokenize_ws\n\t_MODEL_ANSWERS = defaultdict(dict)\n\t_ENSEMBLE_MODELS = {}\n", "def _load_model_answers(input_models):\n\t    for m in input_models:\n\t        if m in _MODEL_ANSWERS:\n\t            continue\n\t        for ds in os.listdir(MODEL_OUTPUTS_PATH):\n\t            in_file = os.path.join(MODEL_OUTPUTS_PATH, ds, m + '.jsonl')\n\t            if not os.path.isfile(in_file):\n\t                continue\n\t            for answer in read_jsonl(in_file):\n\t                _MODEL_ANSWERS[m][answer['page_id']] = ' '.join(tokenize_ws(answer['plaintext']))\n", "def pad_str_zero(s, n):\n\t    return ('\\0 ' * n) + s + (' \\0' * n)\n\tdef pad_str_space(s):\n\t    return ' ' + s + ' '\n\tdef extract_majority_vote(html, page_id, input_models, model_weights, vote_threshold, ngram_size=5):\n\t    _load_model_answers(input_models)\n\t    tree = HTMLTree.parse(html)\n\t    text = pad_str_zero(extract_plain_text(\n\t        tree, main_content=False, preserve_formatting=False, list_bullets=False,\n\t        links=False, alt_texts=False, noscript=False, form_fields=False), ngram_size - 1)\n", "    tokens = tokenize_ws(text)\n\t    token_votes = [0] * len(tokens)\n\t    for ti in range(ngram_size - 1, len(tokens) - ngram_size + 1):\n\t        ngram_str_l = pad_str_space(' '.join(tokens[ti - ngram_size + 1:ti + 1]))\n\t        ngram_str_r = pad_str_space(' '.join(tokens[ti:ti + ngram_size]))\n\t        for m, w in zip(input_models, model_weights):\n\t            answer = pad_str_zero(_MODEL_ANSWERS[m].get(page_id, ''), ngram_size)\n\t            if ngram_str_l in answer or ngram_str_r in answer:\n\t                token_votes[ti] += 1 * w\n\t            if token_votes[ti] >= vote_threshold:\n", "                break\n\t    # Strip padding (matters only if vote_threshold == 0, but still...)\n\t    tokens = tokens[ngram_size - 1:len(tokens) - ngram_size + 1]\n\t    token_votes = token_votes[ngram_size - 1:len(token_votes) - ngram_size + 1]\n\t    return ' '.join(t for t, v in zip(tokens, token_votes) if v >= vote_threshold)\n"]}
{"filename": "src/extraction_benchmark/extractors/extractors.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport re\n\tdef extract_bs4(html, **_):\n\t    from bs4 import BeautifulSoup\n\t    soup = BeautifulSoup(html, 'html.parser')\n\t    for e in soup(['script', 'style', 'noscript']):\n\t        e.decompose()\n\t    return soup.get_text(separator=' ', strip=True)\n", "def extract_boilerpipe(html, **_):\n\t    import boilerpipe.extract as boilerpipe\n\t    text = boilerpipe.Extractor(extractor='ArticleExtractor', html=html)\n\t    text = text.getText()\n\t    return str(text)\n\tdef extract_xpath_text(html, **_):\n\t    import lxml.html\n\t    root = lxml.html.fromstring(html)\n\t    text = ' '.join(root.xpath('//body[1]//*[not(name()=\"script\") and not(name()=\"style\")]/text()'))\n\t    text = re.sub(r'(\\s+\\n\\s*)', '\\n', text)\n", "    return re.sub(r'[ \\t]{2,}', ' ', text)\n\tdef extract_news_please(html, **_):\n\t    import newsplease\n\t    return newsplease.NewsPlease.from_html(html, url=None).maintext\n\tdef extract_readability(html, **_):\n\t    import readability, html_text\n\t    doc = readability.Document(html)\n\t    text = html_text.extract_text(doc.summary(html_partial=True))\n\t    return text\n\tdef extract_go_domdistiller(html, **_):\n", "    from extraction_benchmark.extractors import go_domdistiller\n\t    return go_domdistiller.extract(html)\n\tdef extract_inscriptis(html, **_):\n\t    import inscriptis\n\t    text = inscriptis.get_text(html)\n\t    return text\n\tdef extract_html_text(html, **_):\n\t    import html_text\n\t    return html_text.extract_text(html)\n\tdef extract_resiliparse(html, **_):\n", "    from resiliparse.extract import html2text\n\t    from resiliparse.parse.html import HTMLTree\n\t    return html2text.extract_plain_text(HTMLTree.parse(html),\n\t                                        preserve_formatting=True,\n\t                                        main_content=True,\n\t                                        list_bullets=False,\n\t                                        comments=False,\n\t                                        links=False,\n\t                                        alt_texts=False)\n\tdef extract_bte(html, **_):\n", "    from extraction_benchmark.extractors import bte\n\t    return bte.html2text(html)\n\tdef extract_trafilatura(html, **_):\n\t    import trafilatura\n\t    return trafilatura.extract(html, include_comments=False)\n\tdef extract_justext(html, **_):\n\t    import justext\n\t    article = ' '.join(\n\t        [p.text for p in justext.justext(html, justext.get_stoplist(\"English\"), 50, 200, 0.1, 0.2, 0.2, 200, True)\n\t         if not p.is_boilerplate])\n", "    return article\n\tdef extract_goose3(html, **_):\n\t    from goose3 import Goose, configuration\n\t    c = configuration.Configuration()\n\t    c.http_timeout = 5\n\t    with Goose(c) as g:\n\t        article = g.extract(raw_html=html)\n\t        return article.cleaned_text\n\tdef extract_lxml_cleaner(html, **_):\n\t    from bs4 import BeautifulSoup\n", "    from lxml.html.clean import Cleaner\n\t    tag_blacklist = [\n\t        # important\n\t        'aside', 'embed', 'footer', 'form', 'head', 'iframe', 'menu', 'object', 'script',\n\t        # other content\n\t        'applet', 'audio', 'canvas', 'figure', 'map', 'picture', 'svg', 'video',\n\t        # secondary\n\t        'area', 'blink', 'button', 'datalist', 'dialog',\n\t        'frame', 'frameset', 'fieldset', 'link', 'input', 'ins', 'label', 'legend',\n\t        'marquee', 'math', 'menuitem', 'nav', 'noscript', 'optgroup', 'option',\n", "        'output', 'param', 'progress', 'rp', 'rt', 'rtc', 'select', 'source',\n\t        'style', 'track', 'template', 'textarea', 'time', 'use',\n\t    ]\n\t    cleaner = Cleaner(\n\t        annoying_tags=False,  # True\n\t        comments=True,\n\t        embedded=False,  # True\n\t        forms=True,  # True\n\t        frames=True,  # True\n\t        javascript=True,\n", "        links=False,\n\t        meta=False,\n\t        page_structure=False,\n\t        processing_instructions=True,\n\t        remove_unknown_tags=False,\n\t        safe_attrs_only=False,\n\t        scripts=True,\n\t        style=False,\n\t        kill_tags=tag_blacklist\n\t    )\n", "    return BeautifulSoup(cleaner.clean_html(html), 'html.parser').get_text(separator=' ', strip=True)\n\tdef extract_boilernet(html, **_):\n\t    from extraction_benchmark.extractors import boilernet\n\t    return boilernet.extract(html)\n\tdef extract_web2text(html, **_):\n\t    from extraction_benchmark.extractors import web2text\n\t    return web2text.extract(html)\n\tdef extract_newspaper3k(html, **_):\n\t    import newspaper\n\t    article = newspaper.Article('')\n", "    article.set_html(html)\n\t    article.parse()\n\t    return article.text\n\tdef extract_dragnet(html, **_):\n\t    from dragnet import extract_content\n\t    return extract_content(html, encoding='utf8')\n\tdef extract_extractnet(html, **_):\n\t    from extractnet import Extractor\n\t    return Extractor().extract(html, encoding='utf8').get('content', '')\n\tdef _get_ensemble_model_list(best_only=False, weighted=False):\n", "    def _ls():\n\t        if best_only or weighted:\n\t            return [\n\t                (extract_goose3, 2 if weighted else 1),\n\t                (extract_readability, 2 if weighted else 1),\n\t                (extract_trafilatura, 2 if weighted else 1),\n\t                (extract_go_domdistiller, 1),\n\t                (extract_resiliparse, 1),\n\t                (extract_web2text, 1),\n\t                (extract_bte, 1),\n", "                (extract_justext, 1),\n\t                (extract_boilerpipe, 1),\n\t            ]\n\t        return [(m, 1) for m in list_extractors(names_only=False, include_ensembles=False)]\n\t    return zip(*[(m.__name__.replace('extract_', ''), w) for m, w in _ls()])\n\tdef extract_ensemble_majority(html, page_id):\n\t    from extraction_benchmark.extractors import ensemble\n\t    models, weights = _get_ensemble_model_list()\n\t    return ensemble.extract_majority_vote(html, page_id, models, weights, int(len(models) * .66))\n\tdef extract_ensemble_best(html, page_id):\n", "    from extraction_benchmark.extractors import ensemble\n\t    models, weights = _get_ensemble_model_list(best_only=True)\n\t    return ensemble.extract_majority_vote(html, page_id, models, weights, int(len(models) * .66))\n\tdef extract_ensemble_weighted(html, page_id):\n\t    from extraction_benchmark.extractors import ensemble\n\t    models, weights = _get_ensemble_model_list(best_only=True, weighted=True)\n\t    return ensemble.extract_majority_vote(html, page_id, models, weights, int(len(models) * .66))\n\tdef list_extractors(names_only=True, include_ensembles=False):\n\t    \"\"\"\n\t    Get a list of all supported extraction systems.\n", "    :param names_only: only return a list of strings (otherwise return extractor routines)\n\t    :param include_ensembles: include ensemble extractors in the list\n\t    :return: list of extractor names or functions\n\t    \"\"\"\n\t    return [(n.replace('extract_', '') if names_only else m) for n, m in globals().items()\n\t            if n.startswith('extract_') and (not n.startswith('extract_ensemble') or include_ensembles)]\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom collections import defaultdict\n\timport json\n\timport os\n\timport warnings\n\timport numpy as np\n\tfrom bs4 import BeautifulSoup\n\timport nltk\n", "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\timport tensorflow as tf\n\tfrom .net.preprocess import get_feature_vector, get_leaves, process\n\tBOILERNET_ROOT_PATH = os.path.dirname(os.path.abspath(__file__))\n\t_model = None\n\t_word_map = None\n\t_tag_map = None\n\tgpus = tf.config.experimental.list_physical_devices('GPU')\n\tfor gpu in gpus:\n\t    tf.config.experimental.set_memory_growth(gpu, True)\n", "def load_model():\n\t    global _model, _word_map, _tag_map\n\t    if not _model:\n\t        _model = tf.keras.models.load_model(os.path.join(BOILERNET_ROOT_PATH, 'model.h5'))\n\t        nltk.download('punkt', quiet=True)\n\t        with open(os.path.join(BOILERNET_ROOT_PATH, 'words.json')) as f:\n\t            _word_map = json.load(f)\n\t        with open(os.path.join(BOILERNET_ROOT_PATH, 'tags.json')) as f:\n\t            _tag_map = json.load(f)\n\t    return _model, _word_map, _tag_map\n", "def extract(html):\n\t    model, word_map, tag_map = load_model()\n\t    tags = defaultdict(int)\n\t    words = defaultdict(int)\n\t    with warnings.catch_warnings():\n\t        warnings.simplefilter(\"ignore\")\n\t        doc = BeautifulSoup(html, features='html5lib')\n\t    processed = process(doc, tags, words)\n\t    if not processed:\n\t        return ''\n", "    inputs = [get_feature_vector(w, t, word_map, tag_map) for w, t, _ in processed]\n\t    inputs = np.expand_dims(np.stack(inputs), 0)\n\t    predicted = np.around(model.predict(inputs, verbose=0))\n\t    main_content = ''\n\t    doc = BeautifulSoup(html, features='html5lib')\n\t    for i, (leaf, _, _) in enumerate(get_leaves(doc.find_all('html')[0])):\n\t        if predicted[0, i, 0]:\n\t            main_content += leaf + '\\n'\n\t    return main_content.strip()\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/train.py", "chunked_list": ["#! /usr/bin/python3\n\timport argparse\n\timport os\n\timport pickle\n\timport csv\n\timport math\n\timport tensorflow as tf\n\tfrom sklearn.utils import class_weight\n\tfrom .leaf_classifier import LeafClassifier\n\tdef get_dataset(dataset_file, batch_size, repeat=True):\n", "    def _read_example(example):\n\t        desc = {\n\t            'doc_feature_list': tf.io.VarLenFeature(tf.int64),\n\t            'doc_label_list': tf.io.VarLenFeature(tf.int64)\n\t        }\n\t        _, seq_features = tf.io.parse_single_sequence_example(example, sequence_features=desc)\n\t        return tf.sparse.to_dense(seq_features['doc_feature_list']), \\\n\t            tf.sparse.to_dense(seq_features['doc_label_list'])\n\t    buffer_size = 10 * batch_size\n\t    dataset = tf.data.TFRecordDataset([dataset_file]) \\\n", "        .map(_read_example, num_parallel_calls=4) \\\n\t        .prefetch(buffer_size) \\\n\t        .padded_batch(\n\t        batch_size=batch_size,\n\t        padded_shapes=([None, None], [None, 1]),\n\t        padding_values=(tf.constant(0, dtype=tf.int64), tf.constant(0, dtype=tf.int64))) \\\n\t        .shuffle(buffer_size=buffer_size)\n\t    if repeat:\n\t        return dataset.repeat()\n\t    return dataset\n", "def get_class_weights(train_set_file):\n\t    y_train = []\n\t    for _, y in get_dataset(train_set_file, 1, False):\n\t        y_train.extend(y.numpy().flatten())\n\t    return class_weight.compute_class_weight('balanced', [0, 1], y_train)\n\tdef main():\n\t    ap = argparse.ArgumentParser()\n\t    ap.add_argument('DATA_DIR', help='Directory of files produced by the preprocessing script')\n\t    ap.add_argument('-l', '--num_layers', type=int, default=2, help='The number of RNN layers')\n\t    ap.add_argument('-u', '--hidden_units', type=int, default=256,\n", "                    help='The number of hidden LSTM units')\n\t    ap.add_argument('-d', '--dropout', type=float, default=0.5, help='The dropout percentage')\n\t    ap.add_argument('-s', '--dense_size', type=int, default=256, help='Size of the dense layer')\n\t    ap.add_argument('-e', '--epochs', type=int, default=20, help='The number of epochs')\n\t    ap.add_argument('-b', '--batch_size', type=int, default=16, help='The batch size')\n\t    ap.add_argument('--interval', type=int, default=5,\n\t                    help='Calculate metrics and save the model after this many epochs')\n\t    ap.add_argument('--working_dir', default='train', help='Where to save checkpoints and logs')\n\t    args = ap.parse_args()\n\t    info_file = os.path.join(args.DATA_DIR, 'info.pkl')\n", "    with open(info_file, 'rb') as fp:\n\t        info = pickle.load(fp)\n\t        train_steps = math.ceil(info['num_train_examples'] / args.batch_size)\n\t    train_set_file = os.path.join(args.DATA_DIR, 'train.tfrecords')\n\t    train_dataset = get_dataset(train_set_file, args.batch_size)\n\t    dev_set_file = os.path.join(args.DATA_DIR, 'dev.tfrecords')\n\t    if os.path.isfile(dev_set_file):\n\t        dev_dataset = get_dataset(dev_set_file, 1, repeat=False)\n\t    else:\n\t        dev_dataset = None\n", "    test_set_file = os.path.join(args.DATA_DIR, 'test.tfrecords')\n\t    if os.path.isfile(test_set_file):\n\t        test_dataset = get_dataset(test_set_file, 1, repeat=False)\n\t    else:\n\t        test_dataset = None\n\t    class_weights = get_class_weights(train_set_file)\n\t    print('using class weights {}'.format(class_weights))\n\t    kwargs = {'input_size': info['num_words'] + info['num_tags'],\n\t              'hidden_size': args.hidden_units,\n\t              'num_layers': args.num_layers,\n", "              'dropout': args.dropout,\n\t              'dense_size': args.dense_size}\n\t    clf = LeafClassifier(**kwargs)\n\t    ckpt_dir = os.path.join(args.working_dir, 'ckpt')\n\t    log_file = os.path.join(args.working_dir, 'train.csv')\n\t    os.makedirs(ckpt_dir, exist_ok=True)\n\t    params_file = os.path.join(args.working_dir, 'params.csv')\n\t    print('writing {}...'.format(params_file))\n\t    with open(params_file, 'w') as fp:\n\t        writer = csv.writer(fp)\n", "        for arg in vars(args):\n\t            writer.writerow([arg, getattr(args, arg)])\n\t    clf.train(train_dataset, train_steps, args.epochs, log_file, ckpt_dir, class_weights,\n\t              dev_dataset, info.get('num_dev_examples'),\n\t              test_dataset, info.get('num_test_examples'),\n\t              args.interval)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/__init__.py", "chunked_list": []}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/preprocess.py", "chunked_list": ["#! /usr/bin/python3\n\timport os\n\timport argparse\n\timport pickle\n\timport json\n\tfrom collections import defaultdict\n\timport nltk\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom bs4 import BeautifulSoup, NavigableString\n", "from tqdm import tqdm\n\tfrom .misc import util\n\tdef get_leaves(node, tag_list=[], label=0):\n\t    \"\"\"Return all leaves (NavigableStrings) in a BS4 tree.\"\"\"\n\t    tag_list_new = tag_list + [node.name]\n\t    if node.has_attr('__boilernet_label'):\n\t        label = int(node['__boilernet_label'])\n\t    result = []\n\t    for c in node.children:\n\t        if isinstance(c, NavigableString):\n", "            # might be just whitespace\n\t            if c.string is not None and c.string.strip():\n\t                result.append((c, tag_list_new, label))\n\t        elif c.name not in util.TAGS_TO_IGNORE:\n\t            result.extend(get_leaves(c, tag_list_new, label))\n\t    return result\n\tdef get_leaf_representation(node, tag_list, label):\n\t    \"\"\"Return dicts of words and HTML tags that representat a leaf.\"\"\"\n\t    tags_dict = defaultdict(int)\n\t    for tag in tag_list:\n", "        tags_dict[tag] += 1\n\t    words_dict = defaultdict(int)\n\t    for word in nltk.word_tokenize(node.string):\n\t        words_dict[word.lower()] += 1\n\t    return dict(words_dict), dict(tags_dict), label\n\tdef process(doc, tags, words):\n\t    \"\"\"\n\t    Process \"doc\", updating the tag and word counts.\n\t    Return the document representation, the HTML tags and the words.\n\t    \"\"\"\n", "    result = []\n\t    for leaf, tag_list, is_content in get_leaves(doc.find_all('html')[0]):\n\t        leaf_representation = get_leaf_representation(leaf, tag_list, is_content)\n\t        result.append(leaf_representation)\n\t        words_dict, tags_dict, _ = leaf_representation\n\t        for word, count in words_dict.items():\n\t            words[word] += count\n\t        for tag, count in tags_dict.items():\n\t            tags[tag] += count\n\t    return result\n", "def parse(filenames):\n\t    \"\"\"\n\t    Read and parse all HTML files.\n\t    Return the parsed documents and a set of all words and HTML tags.\n\t    \"\"\"\n\t    result = {}\n\t    tags = defaultdict(int)\n\t    words = defaultdict(int)\n\t    for f in tqdm(filenames):\n\t        try:\n", "            with open(f, 'rb') as hfile:\n\t                doc = BeautifulSoup(hfile, features='html5lib')\n\t            basename = os.path.basename(f)\n\t            result[basename] = process(doc, tags, words)\n\t        except:\n\t            tqdm.write('error processing {}'.format(f))\n\t    return result, tags, words\n\tdef get_feature_vector(words_dict, tags_dict, word_map, tag_map):\n\t    \"\"\"Return a feature vector for an item to be classified.\"\"\"\n\t    vocab_vec = np.zeros(len(word_map), dtype='int32')\n", "    for word, num in words_dict.items():\n\t        # if the item is not in the map, use 0 (OOV word)\n\t        vocab_vec[word_map.get(word, 0)] = num\n\t    tags_vec = np.zeros(len(tag_map), dtype='int32')\n\t    for tag, num in tags_dict.items():\n\t        # if the tag is not in the map, use 0 (OOV tag)\n\t        tags_vec[tag_map.get(tag, 0)] = num\n\t    return np.concatenate([vocab_vec, tags_vec])\n\tdef get_vocabulary(d, num=None):\n\t    \"\"\"Return an integer map of the top-k vocabulary items and add <UNK>.\"\"\"\n", "    l = sorted(d.keys(), key=d.get, reverse=True)\n\t    if num is not None:\n\t        l = l[:num]\n\t    int_map = util.get_int_map(l, offset=1)\n\t    int_map['<UNK>'] = 0\n\t    return int_map\n\tdef get_doc_inputs(docs, word_map, tag_map):\n\t    \"\"\"Transform \"docs\" into the input format accepted by the classifier.\"\"\"\n\t    def _int64_feature(l):\n\t        \"\"\"Return an int64_list.\"\"\"\n", "        return tf.train.Feature(int64_list=tf.train.Int64List(value=l))\n\t    for doc in docs:\n\t        doc_features = []\n\t        doc_labels = []\n\t        for words_dict, tags_dict, label in doc:\n\t            feature_vector = get_feature_vector(words_dict, tags_dict, word_map, tag_map)\n\t            doc_features.append(_int64_feature(feature_vector))\n\t            doc_labels.append(_int64_feature([label]))\n\t        doc_feature_list = tf.train.FeatureList(feature=doc_features)\n\t        doc_label_list = tf.train.FeatureList(feature=doc_labels)\n", "        yield doc_feature_list, doc_label_list\n\tdef write_tfrecords(filename, dataset, word_map, tag_map):\n\t    \"\"\"Write the dataset to a .tfrecords file.\"\"\"\n\t    with tf.io.TFRecordWriter(filename) as writer:\n\t        for doc_feature_list, doc_label_list in get_doc_inputs(dataset, word_map, tag_map):\n\t            f = {'doc_feature_list': doc_feature_list, 'doc_label_list': doc_label_list}\n\t            feature_lists = tf.train.FeatureLists(feature_list=f)\n\t            example = tf.train.SequenceExample(feature_lists=feature_lists)\n\t            writer.write(example.SerializeToString())\n\tdef save(save_path, word_map, tag_map, train_set, dev_set=None, test_set=None):\n", "    \"\"\"Save the data.\"\"\"\n\t    os.makedirs(save_path, exist_ok=True)\n\t    with open(os.path.join(save_path, '../words.json'), 'w', encoding='utf-8') as fp:\n\t        json.dump(word_map, fp)\n\t    with open(os.path.join(save_path, '../tags.json'), 'w', encoding='utf-8') as fp:\n\t        json.dump(tag_map, fp)\n\t    info = {}\n\t    info['num_words'] = len(word_map)\n\t    info['num_tags'] = len(tag_map)\n\t    train_file = os.path.join(save_path, 'train.tfrecords')\n", "    print('writing {}...'.format(train_file))\n\t    write_tfrecords(train_file, train_set, word_map, tag_map)\n\t    info['num_train_examples'] = len(train_set)\n\t    if dev_set is not None:\n\t        dev_file = os.path.join(save_path, 'dev.tfrecords')\n\t        print('writing {}...'.format(dev_file))\n\t        write_tfrecords(dev_file, dev_set, word_map, tag_map)\n\t        info['num_dev_examples'] = len(dev_set)\n\t    if test_set is not None:\n\t        test_file = os.path.join(save_path, 'test.tfrecords')\n", "        print('writing {}...'.format(test_file))\n\t        write_tfrecords(test_file, test_set, word_map, tag_map)\n\t        info['num_test_examples'] = len(test_set)\n\t    info_file = os.path.join(save_path, 'info.pkl')\n\t    with open(info_file, 'wb') as fp:\n\t        pickle.dump(info, fp)\n\tdef read_file(f_name):\n\t    with open(f_name, encoding='utf-8') as fp:\n\t        for line in fp:\n\t            yield line.strip()\n", "def main():\n\t    ap = argparse.ArgumentParser()\n\t    ap.add_argument('DIRS', nargs='+', help='A list of directories containing the HTML files')\n\t    ap.add_argument('-s', '--split_dir', help='Directory that contains train-/dev-/testset split')\n\t    ap.add_argument('-w', '--num_words', type=int, help='Only use the top-k words')\n\t    ap.add_argument('-t', '--num_tags', type=int, help='Only use the top-l HTML tags')\n\t    ap.add_argument('--save', default='result', help='Where to save the results')\n\t    args = ap.parse_args()\n\t    # files required for tokenization\n\t    nltk.download('punkt')\n", "    filenames = []\n\t    for d in args.DIRS:\n\t        filenames.extend(util.get_filenames(d))\n\t    data, tags, words = parse(filenames)\n\t    tags = get_vocabulary(tags, args.num_tags)\n\t    words = get_vocabulary(words, args.num_words)\n\t    if args.split_dir:\n\t        train_set_file = os.path.join(args.split_dir, 'train_set.txt')\n\t        dev_set_file = os.path.join(args.split_dir, 'dev_set.txt')\n\t        test_set_file = os.path.join(args.split_dir, 'test_set.txt')\n", "        train_set = [data[basename] for basename in read_file(train_set_file)]\n\t        dev_set = [data[basename] for basename in read_file(dev_set_file)]\n\t        test_set = [data[basename] for basename in read_file(test_set_file)]\n\t    else:\n\t        train_set = data.values()\n\t        dev_set, test_set = None, None\n\t    save(args.save, words, tags, train_set, dev_set, test_set)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/leaf_classifier.py", "chunked_list": ["#! /usr/bin/python3\n\timport os\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom sklearn.metrics import precision_recall_fscore_support\n\tfrom tqdm import tqdm\n\tclass Metrics(tf.keras.callbacks.Callback):\n\t    \"\"\"Calculate metrics for a dev-/testset and add them to the logs.\"\"\"\n\t    def __init__(self, clf, data, steps, interval, prefix=''):\n\t        self.clf = clf\n", "        self.data = data\n\t        self.steps = steps\n\t        self.interval = interval\n\t        self.prefix = prefix\n\t    def on_epoch_end(self, epoch, logs):\n\t        if (epoch + 1) % self.interval == 0:\n\t            y_true, y_pred = self.clf.eval(self.data, self.steps, desc=self.prefix)\n\t            p, r, f, s = precision_recall_fscore_support(y_true, y_pred)\n\t        else:\n\t            p, r, f, s = np.nan, np.nan, np.nan, np.nan\n", "        logs_new = {'{}_precision'.format(self.prefix): p,\n\t                    '{}_recall'.format(self.prefix): r,\n\t                    '{}_f1'.format(self.prefix): f,\n\t                    '{}_support'.format(self.prefix): s}\n\t        logs.update(logs_new)\n\tclass Saver(tf.keras.callbacks.Callback):\n\t    \"\"\"Save the model.\"\"\"\n\t    def __init__(self, path, interval):\n\t        self.path = path\n\t        self.interval = interval\n", "    def on_epoch_end(self, epoch, logs):\n\t        if (epoch + 1) % self.interval == 0:\n\t            file_name = os.path.join(self.path, 'model.{:03d}.h5'.format(epoch))\n\t            self.model.save(file_name)\n\t# pylint: disable=E1101\n\tclass LeafClassifier(object):\n\t    \"\"\"This classifier assigns labels to sequences based on words and HTML tags.\"\"\"\n\t    def __init__(self, input_size, num_layers, hidden_size, dropout, dense_size):\n\t        \"\"\"Construct the network.\"\"\"\n\t        self.input_size = input_size\n", "        self.num_layers = num_layers\n\t        self.hidden_size = hidden_size\n\t        self.dropout = dropout\n\t        self.dense_size = dense_size\n\t        self.model = self._get_model()\n\t    def _get_model(self):\n\t        \"\"\"Return a keras model.\"\"\"\n\t        model = tf.keras.Sequential()\n\t        model.add(tf.keras.layers.InputLayer(input_shape=(None, self.input_size)))\n\t        model.add(tf.keras.layers.Dense(self.dense_size, activation='relu'))\n", "        model.add(tf.keras.layers.Masking(mask_value=0))\n\t        for _ in range(self.num_layers):\n\t            lstm = tf.keras.layers.LSTM(self.hidden_size, return_sequences=True)\n\t            model.add(tf.keras.layers.Bidirectional(lstm))\n\t        model.add(tf.keras.layers.Dropout(self.dropout))\n\t        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n\t        model.compile(loss='binary_crossentropy', optimizer='adam')\n\t        return model\n\t    def train(self, train_dataset, train_steps, epochs, log_file, ckpt, class_weight=(1, 1),\n\t              dev_dataset=None, dev_steps=None, test_dataset=None, test_steps=None, interval=1):\n", "        \"\"\"Train a number of input sequences.\"\"\"\n\t        callbacks = [Saver(ckpt, interval)]\n\t        if dev_dataset is not None:\n\t            callbacks.append(Metrics(self, dev_dataset, dev_steps, interval, 'dev'))\n\t        if test_dataset is not None:\n\t            callbacks.append(Metrics(self, test_dataset, test_steps, interval, 'test'))\n\t        callbacks.append(tf.keras.callbacks.CSVLogger(log_file))\n\t        self.model.fit(train_dataset, steps_per_epoch=train_steps, epochs=epochs,\n\t                       callbacks=callbacks, class_weight=class_weight)\n\t    def eval(self, dataset, steps, desc=None):\n", "        \"\"\"Evaluate the model on the test data and return the metrics.\"\"\"\n\t        y_true, y_pred = [], []\n\t        for b_x, b_y in tqdm(dataset, total=steps, desc=desc):\n\t            # somehow this cast is necessary\n\t            b_x = tf.dtypes.cast(b_x, 'float32')\n\t            y_true.extend(b_y.numpy().flatten())\n\t            y_pred.extend(np.around(self.model.predict_on_batch(b_x)).flatten())\n\t        return y_true, y_pred\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/misc/prepare_dataset.py", "chunked_list": ["import argparse\n\timport os\n\tfrom tqdm import tqdm\n\tfrom bs4 import BeautifulSoup, NavigableString, Comment\n\timport util\n\tdef process_bp(doc):\n\t    \"\"\"Process HTML files annotated by BoilerPipe. We ignore nodes labeled as \"comment\".\"\"\"\n\t    for node in doc.find_all(attrs={'__prediction': True}):\n\t        node['__boilernet_is_content'] = True\n\t        del node['__prediction']\n", "def process_other(doc):\n\t    \"\"\"Process manually annotated HTML files. We ignore nodes labeled as \"comment\".\"\"\"\n\t    for node in doc.find_all(attrs={'__label': True}):\n\t        if node['__label'] != 'comment':\n\t            node['__boilernet_is_content'] = True\n\t        del node['__label']\n\tdef process_gn1(doc):\n\t    \"\"\"Process HTML files from the GN1 dataset.\"\"\"\n\t    # the classes x-nc-sel0, x-nc-sel4 and x-nc-sel5 do not seem to indicate content\n\t    nodes = doc.find_all('span', class_='x-nc-sel1') \\\n", "            + doc.find_all('span', class_='x-nc-sel2') \\\n\t            + doc.find_all('span', class_='x-nc-sel3')\n\t    for node in nodes:\n\t        node['__boilernet_is_content'] = True\n\tdef remove_comments(doc):\n\t    \"\"\"Remove all comments from \"doc\".\"\"\"\n\t    for node in doc.find_all(text=lambda x: isinstance(x, Comment)):\n\t        node.extract()\n\tdef process(doc, dataset_function):\n\t    \"\"\"\n", "    Wrap each NavigableString in a <span> tag.\n\t    If the string is content, add a __boilernet_label attribute.\n\t    Remove all HTML comments from the document.\n\t    \"\"\"\n\t    remove_comments(doc)\n\t    dataset_function(doc)\n\t    for node, is_content in get_leaves(doc.find_all('html')[0]):\n\t        # if the parent node is already a span, we don't add another one\n\t        if node.parent.name == 'span':\n\t            span = node.parent\n", "        else:\n\t            span = doc.new_tag('span')\n\t            node.wrap(span)\n\t        if is_content:\n\t            span['__boilernet_label'] = 1\n\t        else:\n\t            span['__boilernet_label'] = 0\n\tdef get_leaves(node, is_content=False):\n\t    \"\"\"Return all leaves (NavigableStrings) in a BS4 tree.\"\"\"\n\t    if node.has_attr('__boilernet_is_content'):\n", "        is_content = True\n\t        del node['__boilernet_is_content']\n\t    result = []\n\t    for c in node.children:\n\t        if isinstance(c, NavigableString) and not isinstance(c, Comment):\n\t            # might be just whitespace\n\t            if c.string is not None and c.string.strip():\n\t                result.append((c, is_content))\n\t        elif c.name is not None:\n\t            if c.name.lower() in util.TAGS_TO_IGNORE:\n", "                # we remove these tags as they are ignored anyway and can make the file very large\n\t                c.extract()\n\t            else:\n\t                result.extend(get_leaves(c, is_content))\n\t    return result\n\tdef main():\n\t    dataset_functions = {'bp': process_bp, 'gn1': process_gn1, 'other': process_other}\n\t    ap = argparse.ArgumentParser()\n\t    ap.add_argument('INPUT', help='Input directory (html files)')\n\t    ap.add_argument('OUTPUT', help='Output directory')\n", "    ap.add_argument('DATASET', choices=dataset_functions.keys(), help='Dataset type')\n\t    ap.add_argument('--prefix', help='Add a prefix to the file names.')\n\t    args = ap.parse_args()\n\t    os.makedirs(args.OUTPUT, exist_ok=True)\n\t    for f in tqdm(util.get_filenames(args.INPUT, '.html')):\n\t        try:\n\t            with open(f, 'rb') as hfile:\n\t                doc = BeautifulSoup(hfile, features='html5lib')\n\t            # for some reason, parsing malformed HTML twice works better\n\t            doc2 = BeautifulSoup(doc.prettify(), features='html5lib')\n", "            process(doc2, dataset_functions[args.DATASET])\n\t            f_name = os.path.basename(f)\n\t            if args.prefix:\n\t                f_name = args.prefix + f_name\n\t            with open(os.path.join(args.OUTPUT, f_name), 'w', encoding='utf-8') as hfile:\n\t                hfile.write(doc2.prettify())\n\t        except:\n\t            tqdm.write('error processing {}'.format(f))\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/misc/__init__.py", "chunked_list": []}
{"filename": "src/extraction_benchmark/extractors/boilernet/net/misc/util.py", "chunked_list": ["#! /usr/bin/python3\n\timport random\n\timport os\n\t# everything that is a child of one of these tags is considered boilerplate and hence ignored by the\n\t# classifier\n\tTAGS_TO_IGNORE = {'head', 'iframe', 'script', 'meta', 'link', 'style', 'input', 'checkbox',\n\t                  'button', 'noscript'}\n\tdef get_int_map(items, offset=0):\n\t    \"\"\"Return a dict that maps each unique item in \"items\" to a unique int ID.\"\"\"\n\t    # we sort the items to guarantee that we get the same mapping every time for a fixed input\n", "    return {item: i + offset for i, item in enumerate(sorted(set(items)))}\n\tdef get_filenames(dir_path, filetype='.html'):\n\t    \"\"\"Return absolute paths to all files of a given type in a directory.\"\"\"\n\t    # join the dir path and the filename, then filter out directories\n\t    all_files = filter(os.path.isfile, map(lambda x: os.path.join(dir_path, x), os.listdir(dir_path)))\n\t    filtered_files = filter(lambda x: x.endswith(filetype), all_files)\n\t    return list(filtered_files)\n"]}
{"filename": "src/extraction_benchmark/extractors/go_domdistiller/__init__.py", "chunked_list": ["import os\n\timport subprocess\n\tfrom tempfile import TemporaryDirectory\n\tdef extract(html, **_):\n\t    cli_path = os.path.join(os.path.dirname(__file__), 'go_domdistiller_cli')\n\t    with TemporaryDirectory() as tmp_dir:\n\t        p = os.path.join(tmp_dir, 'go_domdistiller.html')\n\t        with open(p, 'w') as f:\n\t            f.write(html)\n\t        result = subprocess.run([cli_path, p], stdout=subprocess.PIPE)\n", "    return result.stdout.decode().strip()\n"]}
{"filename": "src/extraction_benchmark/extractors/web2text/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport errno\n\timport hashlib\n\timport os\n\timport subprocess\n\timport tempfile\n\tfrom extraction_benchmark.paths import THIRD_PARTY_PATH\n\tWEB2TEXT_BASEPATH = os.path.join(THIRD_PARTY_PATH, 'web2text')\n", "WEB2TEXT_PYTHONPATH = os.path.join(WEB2TEXT_BASEPATH, 'src', 'main', 'python')\n\tWEB2TEXT_VENV = os.path.join(WEB2TEXT_BASEPATH, 'venv')\n\tif not os.path.isdir(WEB2TEXT_PYTHONPATH):\n\t    raise FileNotFoundError(errno.ENOENT, WEB2TEXT_BASEPATH,\n\t                            'Could not find Web2Text under current working directory. Please ensure you have the '\n\t                            'submodule checked out and are running this from the repository\\'s root directory.')\n\tif not os.path.isdir(WEB2TEXT_VENV):\n\t    raise FileNotFoundError(errno.ENOENT, WEB2TEXT_VENV,\n\t                            'Could not find venv in Web2Text directory. '\n\t                            'Please follow README instructions to create one')\n", "def extract(html):\n\t    scala_cmd = ['scala', '-cp', os.path.join(THIRD_PARTY_PATH, 'web2text.jar')]\n\t    python_cmd = ['python', os.path.join(WEB2TEXT_PYTHONPATH, 'main.py')]\n\t    hash_id = hashlib.sha256(html.encode()).hexdigest()\n\t    proc_env = os.environ.copy()\n\t    proc_env['VIRTUAL_ENV'] = WEB2TEXT_VENV\n\t    proc_env['PATH'] = '{}/bin:{}'.format(proc_env['VIRTUAL_ENV'], proc_env['PATH'])\n\t    proc_env['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n\t    with tempfile.TemporaryDirectory() as tmp_dir:\n\t        file_base = os.path.join(tmp_dir, hash_id)\n", "        html_file = file_base + '.html'\n\t        features_file = file_base + '.features'\n\t        labels_file = file_base + '.labels'\n\t        text_file = file_base + '.txt'\n\t        open(html_file, 'w').write(html)\n\t        exit_code = subprocess.Popen(\n\t            scala_cmd + ['ch.ethz.dalab.web2text.ExtractPageFeatures', html_file, features_file],\n\t            env=proc_env,\n\t            stderr=subprocess.DEVNULL,\n\t            stdout=subprocess.DEVNULL\n", "        ).wait()\n\t        if exit_code != 0:\n\t            raise RuntimeError('Web2Text ExtractPageFeatures failed.')\n\t        exit_code = subprocess.Popen(\n\t            python_cmd + ['classify', features_file, labels_file],\n\t            env=proc_env,\n\t            stderr=subprocess.DEVNULL,\n\t            stdout=subprocess.DEVNULL\n\t        ).wait()\n\t        if exit_code != 0:\n", "            raise RuntimeError('Web2Text DOM node classification failed.')\n\t        exit_code = subprocess.Popen(\n\t            scala_cmd + ['ch.ethz.dalab.web2text.ApplyLabelsToPage', html_file, labels_file, text_file],\n\t            env=proc_env,\n\t            stderr=subprocess.DEVNULL,\n\t            stdout=subprocess.DEVNULL\n\t        ).wait()\n\t        if exit_code != 0:\n\t            raise RuntimeError('Web2Text ApplyLabelsToPage failed.')\n\t        return open(text_file, 'r').read()\n"]}
{"filename": "src/extraction_benchmark/cli/__init__.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\tfrom .complexity import complexity\n\tfrom .eval import eval\n\tfrom .extract import convert_datasets, extract\n"]}
{"filename": "src/extraction_benchmark/cli/eval.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport glob\n\timport click\n\tfrom extraction_benchmark.globals import *\n\t@click.group()\n\tdef eval():\n\t    \"\"\"\n\t    Evaluate model answers against the ground truth.\n", "    \"\"\"\n\t@eval.command()\n\t@click.argument('metric', type=click.Choice(['all', *SCORES]))\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\t@click.option('-m', '--model', type=click.Choice(['all', *MODELS_ALL]), default=['all'], multiple=True)\n\t@click.option('--eval-ensembles', help='Evaluate only ensembles', is_flag=True)\n\t@click.option('-p', '--parallelism', help='Number of threads to use', default=os.cpu_count())\n\tdef score(metric, dataset, model, eval_ensembles, parallelism):\n\t    \"\"\"\n\t    Calculate performance metrics on model answers.\n", "    \"\"\"\n\t    if 'all' in dataset:\n\t        dataset = sorted(DATASETS)\n\t    if 'all' in model:\n\t        model = sorted(MODELS_ALL)\n\t    if eval_ensembles:\n\t        model = sorted(m for m in MODELS_ALL if m.startswith('ensemble_'))\n\t    metric = sorted(SCORES) if metric == 'all' else [metric]\n\t    if not dataset:\n\t        click.echo('No datasets selected.', err=True)\n", "        return\n\t    if not model:\n\t        click.echo('No models selected.', err=True)\n\t        return\n\t    import nltk\n\t    try:\n\t        # Needed for Rouge\n\t        nltk.data.find('tokenizers/punkt')\n\t    except:\n\t        nltk.download('punkt')\n", "    from extraction_benchmark.eval import calculcate_scores\n\t    try:\n\t        calculcate_scores(metric, dataset, model, parallelism)\n\t    except FileNotFoundError as e:\n\t        click.FileError(e.filename,\n\t                        f'Make sure you have converted the raw datasets using \"convert-datasets\".')\n\t@eval.command()\n\t@click.argument('score', type=click.Choice(['all', *SCORES]))\n\t@click.option('-m', '--model', type=click.Choice(['all', *MODELS_ALL]), default=['all'], multiple=True)\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n", "@click.option('-x', '--exclude-dataset', type=click.Choice(DATASETS), default=[], multiple=True)\n\t@click.option('-c', '--complexity', type=click.Choice(['all', *COMPLEXITIES]), default=['all', 'low', 'high'],\n\t              required=True, multiple=True, show_default=True)\n\tdef aggregate(score, model, dataset, exclude_dataset, complexity):\n\t    \"\"\"\n\t    Aggregate calculated performance metrics.\n\t    \"\"\"\n\t    score = sorted(SCORES) if score == 'all' else [score]\n\t    if 'all' in model:\n\t        model = sorted(MODELS_ALL)\n", "    if 'all' in dataset:\n\t        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n\t    if not dataset:\n\t        click.echo('No datasets selected.', err=True)\n\t        return\n\t    if not model:\n\t        click.echo('No models selected.', err=True)\n\t        return\n\t    from extraction_benchmark.eval import aggregate_scores\n\t    try:\n", "        for s in score:\n\t            aggregate_scores(s, model, dataset, complexity)\n\t    except FileNotFoundError as e:\n\t        raise click.FileError(e.filename, 'Please calculate complexity scores first.')\n\t    click.echo(f'Aggregations written to \"{METRICS_PATH}\"')\n\t@eval.command()\n\tdef cythonize_rouge():\n\t    \"\"\"\n\t    Cythonize Rouge-Score module.\n\t    By cythonizing the Rouge-Score module, the slow scoring performance can be improved slightly.\n", "    \"\"\"\n\t    click.confirm('This will cythonize the rouge-score module. '\n\t                  'You will have to reinstall it to revert the changes. Continue?', abort=True)\n\t    import rouge_score\n\t    path = os.path.dirname(rouge_score.__file__)\n\t    py_files = glob.glob(os.path.join(path, '*.py'))\n\t    if not py_files:\n\t        click.echo('No Python files found in module. Has the module already been cythonized?', err=True)\n\t        return\n\t    for p in py_files:\n", "        os.rename(p, p + 'x')\n\t    from Cython.Build.Cythonize import main as cython_main\n\t    cython_main(['cythonize', '-3', '--inplace', *glob.glob(os.path.join(path, '*.pyx'))])\n"]}
{"filename": "src/extraction_benchmark/cli/extract.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport os\n\timport logging\n\timport click\n\tfrom extraction_benchmark.globals import *\n\t@click.command()\n\t@click.option('-m', '--model', type=click.Choice(['all', *MODELS_ALL]), default=['all'],\n\t              help='Extraction models (\"all\" does not include ensembles)', multiple=True)\n", "@click.option('--run-ensembles', is_flag=True, help='Run all ensembles (ignores --model)')\n\t@click.option('-e', '--exclude-model', type=click.Choice(MODELS_ALL), default=['web2text'], show_default=True,\n\t              help='Exclude models if \"all\" are selected.', multiple=True)\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\t@click.option('-x', '--exclude-dataset', type=click.Choice(DATASETS), default=[],\n\t              help='Exclude datasets if \"all\" are selected.', multiple=True)\n\t@click.option('-s', '--skip-existing', is_flag=True, help='Load existing answer and extract only new')\n\t@click.option('-p', '--parallelism', help='Number of threads to use', default=os.cpu_count())\n\t@click.option('-v', '--verbose', help='Verbose output', is_flag=True)\n\tdef extract(model, run_ensembles, exclude_model, dataset, exclude_dataset, skip_existing, parallelism, verbose):\n", "    \"\"\"\n\t    Run main content extractors on the datasets.\n\t    \"\"\"\n\t    if not os.path.isdir(DATASET_COMBINED_PATH):\n\t        raise click.UsageError('Combined dataset not found. '\n\t                               'Please create the converted dataset first using the \"convert-datasets\" command.')\n\t    if run_ensembles:\n\t        model = sorted(m for m in MODELS_ALL if m.startswith('ensemble_') and m not in exclude_model)\n\t    elif 'all' in model:\n\t        model = sorted(m for m in MODELS if m not in exclude_model)\n", "        click.confirm('This will run ALL models. Continue?', abort=True)\n\t    if not os.path.isdir(MODEL_OUTPUTS_PATH):\n\t        for m in model:\n\t            if m.startswith('ensemble_'):\n\t                raise click.UsageError('Model outputs need to be generated before ensemble can be run.')\n\t    if 'all' in dataset:\n\t        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n\t    if not dataset:\n\t        click.echo('No input datasets found.\\n'\n\t                   'Make sure that all datasets have been extracted correctly to a folder \"datasets/raw\" '\n", "                   'under the current working directory.', err=True)\n\t        return\n\t    if parallelism > 1 and ('web2text' in model or 'boilernet' in model):\n\t        click.echo('WARNING: Deep neural models selected. If you run into GPU memory issues, '\n\t                   'try running with --parallelism=1.', err=True)\n\t    from extraction_benchmark import extract\n\t    extract.extract(model, dataset, skip_existing, parallelism, verbose)\n\t@click.command()\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\t@click.option('-x', '--exclude-dataset', type=click.Choice(DATASETS), default=[], multiple=True)\n", "def convert_datasets(dataset, exclude_dataset):\n\t    \"\"\"\n\t    Combine raw datasets and convert them to a line-delimieted JSON format.\n\t    \"\"\"\n\t    if not os.path.isdir(DATASET_RAW_PATH):\n\t        raise click.UsageError('Raw datasets not found. '\n\t                               'Make sure that all datasets have been extracted correctly to a folder \"datasets/raw\" '\n\t                               'under the current working directory.')\n\t    if 'all' in dataset:\n\t        dataset = sorted(d for d in DATASETS if d not in exclude_dataset)\n", "    from extraction_benchmark import extract\n\t    page_ids = extract.extract_ground_truth(dataset)\n\t    extract.extract_raw_html(dataset, page_ids)\n"]}
{"filename": "src/extraction_benchmark/cli/complexity.py", "chunked_list": ["# Copyright 2023 Janek Bevendorff\n\t#\n\t# Licensed under the Apache License, Version 2.0 (the \"License\");\n\t# you may not use this file except in compliance with the License.\n\t# You may obtain a copy of the License at\n\t#\n\t#     http://www.apache.org/licenses/LICENSE-2.0\n\t#\n\t# Unless required by applicable law or agreed to in writing, software\n\t# distributed under the License is distributed on an \"AS IS\" BASIS,\n", "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n\t# See the License for the specific language governing permissions and\n\t# limitations under the License.\n\timport click\n\tfrom extraction_benchmark.globals import *\n\t@click.group()\n\tdef complexity():\n\t    \"\"\"\n\t    Calculate page extraction complexities.\n\t    \"\"\"\n", "    pass\n\t@complexity.command()\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\tdef calculate(dataset):\n\t    \"\"\"\n\t    Calculate page complexities.\n\t    Calculate page complexities for given datasets based on the ground truth.\n\t    \"\"\"\n\t    if 'all' in dataset:\n\t        dataset = sorted(DATASETS)\n", "    if not dataset:\n\t        click.echo('No input datasets found.\\n'\n\t                   'Make sure that all datasets have been extracted correctly to a folder \"datasets/raw\" '\n\t                   'under the current working directory.', err=False)\n\t        return\n\t    from extraction_benchmark.complexity import calculate\n\t    calculate(dataset)\n\t@complexity.command()\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\t@click.option('-l', '--low-quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']),\n", "              help='Low-complexity quantile threshold', default='0.25')\n\t@click.option('-h', '--high-quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']),\n\t              help='High-complexity quantile threshold', default='0.75')\n\tdef visualize(dataset, low_quantile, high_quantile):\n\t    \"\"\"\n\t    Visualize the median complexity of the datasets.\n\t    \"\"\"\n\t    if 'all' in dataset:\n\t        dataset = sorted(DATASETS)\n\t    from extraction_benchmark.complexity import visualize_datasets\n", "    visualize_datasets(dataset, low_quantile, high_quantile)\n\t@complexity.command()\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\t@click.option('-p', '--parallelism', help='Number of threads to use', default=os.cpu_count())\n\tdef extract_features(dataset, parallelism):\n\t    \"\"\"\n\t    Extract HTML features.\n\t    Extract HTML features from ground truth pages for complexity clustering.\n\t    \"\"\"\n\t    if 'all' in dataset:\n", "        dataset = sorted(DATASETS)\n\t    from extraction_benchmark.complexity import extract_page_features\n\t    extract_page_features(dataset, parallelism)\n\t@complexity.command()\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\t@click.option('-r', '--reduce-dim', type=int,\n\t              help='Reduce dimensionality before clustering (0 for no reduction)')\n\t@click.option('-c', '--clusters', type=int, default=2, help='Number of clusters')\n\tdef cluster(dataset, reduce_dim, clusters):\n\t    \"\"\"\n", "    Perform a k-means clustering.\n\t    Perform a k-means clustering of previously extract HTML features to estimate complexity.\n\t    \"\"\"\n\t    if 'all' in dataset:\n\t        dataset = sorted(DATASETS)\n\t    from extraction_benchmark.complexity import kmeans_cluster\n\t    try:\n\t        kmeans_cluster(dataset, reduce_dim, clusters)\n\t    except FileNotFoundError as e:\n\t        raise click.FileError(e.filename, 'Make sure HTML features have been calculated.')\n", "@complexity.command()\n\t@click.option('-q', '--quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']), default='0.33',\n\t              help='Quantile boundary')\n\tdef visualize_clusters(quantile):\n\t    \"\"\"\n\t    Visualize k-means clustering.\n\t    Visualize k-means clustering of HTML pages and align clusters with given complexity quantile.\n\t    \"\"\"\n\t    from extraction_benchmark.complexity import visualize_clusters\n\t    visualize_clusters(quantile)\n", "@complexity.command()\n\t@click.option('-d', '--dataset', type=click.Choice(['all', *DATASETS]), default=['all'], multiple=True)\n\t@click.option('-s', '--split-size', type=click.FloatRange(0.1, 0.9), default=0.25, help='Training split size')\n\t@click.option('-q', '--quantile', type=click.Choice(['0.25', '0.33', '0.5', '0.66', '0.75']), default='0.5',\n\t              help='Quantile boundary')\n\tdef classify(dataset, split_size, quantile):\n\t    \"\"\"\n\t    Train and evaluate a logistic regression classifier.\n\t    Train a logistic regression classifier on a split of the complexity scores and classify the remaining ones.\n\t    \"\"\"\n", "    if 'all' in dataset:\n\t        dataset = sorted(DATASETS)\n\t    from extraction_benchmark.complexity import logistic_regression_classify\n\t    try:\n\t        logistic_regression_classify(dataset, split_size, quantile)\n\t    except FileNotFoundError as e:\n\t        raise click.FileError(e.filename, 'Make sure HTML features have been calculated.')\n\t@complexity.command()\n\tdef visualize_classes():\n\t    \"\"\"\n", "    Visualize logistic regression classification.\n\t    \"\"\"\n\t    from extraction_benchmark.complexity import visualize_classes\n\t    visualize_classes()\n"]}
