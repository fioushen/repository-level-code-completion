{"filename": "src/__init__.py", "chunked_list": ["import json\n\timport os\n\tfrom pathlib import Path\n\timport sys\n\t# Get root path of the package, regardless of whether `dvc exp run --temp` is used or not.\n\tpackageRoot = Path(os.getcwd().split(\"/.dvc\", maxsplit=1)[0]) / \"src\"\n\t# Load environs that is tracked by git\n\tif (packageRoot / \"dist_environ.json\").exists():\n\t    print(\"Loading distributed environs...\", file=sys.stderr)\n\t    with open(str(packageRoot / \"dist_environ.json\"), \"r\") as f:\n", "        environDict = json.load(f)\n\t    for key, val in environDict.items():\n\t        if key not in os.environ:\n\t            os.environ[key] = str(val)\n\t        else:\n\t            print(f\"Use existing environ {key}: {os.environ[key]}\")\n\t# Load local environs that is not tracked by git\n\tif (packageRoot / \"__environ__.json\").exists():\n\t    print(\"Loading local environs...\", file=sys.stderr)\n\t    with open(str(packageRoot / \"__environ__.json\"), \"r\") as f:\n", "        environDict = json.load(f)\n\t    for key, val in environDict.items():\n\t        os.environ[key] = str(val)\n"]}
{"filename": "src/train_links.py", "chunked_list": ["import faulthandler\n\timport gc\n\timport os\n\timport signal\n\timport sys\n\timport time\n\timport traceback\n\tfrom pathlib import Path\n\tfrom typing import Final\n\timport pandas\n", "import torch\n\timport torch.multiprocessing as mp\n\timport tqdm\n\tfrom .distributed.AvgCluster import AvgCluster, AvgClusterServer\n\tfrom .dataloading.DataMgr import DataMgr\n\tfrom .pipelines.SessionMgr import EvalMetrics, EvalMetricsCalc, SessionMgr, write_stats\n\tfrom .distributed.MultiGPU import MultiGPUServer, MultiGPUTrainer\n\tfrom .pipelines.Initialization import parse_args\n\tfrom .pipelines.utils import get_peak_mem, vscode_debug\n\tdef prepare_trainer_data(create_formats=False):\n", "    '''\n\t    Prepare data for training on the trainer process.\n\t    Arguments\n\t    ---------\n\t    create_formats: bool\n\t        Whether to create all DGL data formats for the graph dataset.\n\t    Returns\n\t    -------\n\t    datamgr: DataMgr or AvgCluster\n\t        Data manager or distributed manager that manages the training graph.\n", "    distmgr: None or AvgCluster\n\t        Distributed manager for the graph dataset, if model aggregation training is used.\n\t    '''\n\t    if args.distributed is not None:\n\t        args.dataset_str = None\n\t        dist_params = {\n\t            (key[5:] if key.startswith(\"eval_\") else key): (eval(value) if key.startswith(\"eval_\") else value)\n\t            for key, value in args.distributed_params.items()\n\t        }\n\t        distmgr = AvgCluster(args.world_size, args.dist_is_master, \n", "            args.dataset, dataset_kwargs=args.dataset_params, **dist_params)\n\t        distmgr.sync_train_data()\n\t        datamgr = distmgr\n\t    else:\n\t        datamgr = DataMgr(args.dataset, args.to_undirected, args.use_self_loops, \n\t            **args.dataset_params)\n\t        datamgr.prepare_dataset()\n\t        distmgr = None\n\t        args.dataset_str = datamgr.dataset_str\n\t    if create_formats:\n", "        datamgr.create_formats_()\n\t    return datamgr, distmgr\n\tdef start_trainer(proc_id, args, datamgr, distmgr=None, device=None):\n\t    '''\n\t    Main function for the trainer process.\n\t    Also run validation and test in single GPU training\n\t    (i.e., if no distributed or MultiGPU training is used).\n\t    Arguments\n\t    ---------\n\t    proc_id: int\n", "        Process ID of for the trainer process in Multi-GPU training.\n\t    args: argparse.Namespace\n\t        Parsed arguments.\n\t    datamgr: DataMgr or AvgCluster\n\t        Data manager or distributed manager that manages the training graph.\n\t    distmgr: None or AvgCluster\n\t        Distributed manager for the graph dataset, if model aggregation training is used.\n\t    device: torch.device\n\t        Device to use for training.\n\t    '''\n", "    if device is None:\n\t        # check device\n\t        if args.use_gpu and torch.cuda.is_available():\n\t            if type(args.gpu) == list: # Multi-GPU training\n\t                dev_id = args.gpu[proc_id]\n\t                distmgr = MultiGPUTrainer(args, proc_id, dev_id)\n\t                device = distmgr.device\n\t                args.is_mgpu = True\n\t            else:\n\t                device = torch.device(\"cuda:{}\".format(args.gpu))\n", "                torch.cuda.set_device(args.gpu)\n\t                args.is_mgpu = False\n\t                args.mgpu_is_master = None\n\t        else:\n\t            args.use_gpu = False\n\t            device = torch.device(\"cpu\")\n\t        print(f\"Using device: {device}\")\n\t    valid_edge = datamgr.valid_edge\n\t    test_edge = datamgr.test_edge\n\t    in_dim = datamgr.in_dim\n", "    session = SessionMgr(args, datamgr, device, distmgr=distmgr)\n\t    session.create_train_data_loader(use_ddp=args.is_mgpu)\n\t    if valid_edge is not None and session.enable_eval:\n\t        session.create_evaluator(\"valid\")\n\t        session.is_eval_point()\n\t    if not args.is_mgpu:\n\t        model, optimizer = session.create_model()\n\t    else:\n\t        model, optimizer = session.create_model(\n\t            use_ddp=True, ddp_device=device)\n", "    # Load node features for MAG240M before timing begins\n\t    if datamgr.use_feat_loader:\n\t        _ = datamgr.node_features\n\t    # create training epoch\n\t    stats_dict: Final[dict] = session.create_stats_dict()\n\t    session.reset_timer()\n\t    postfix_dict_epoch = {\n\t        \"Model\": f\"{args.model} ({model.num_model_params})\",\n\t        \"PID\": os.getpid(),\n\t        \"SIGNAC\": args.signac_job.id if args.signac_job else None,\n", "        **args.tqdm_msg\n\t    }\n\t    postfix_dict_step: Final[dict] = dict()\n\t    session.postfix_dict_step = postfix_dict_step\n\t    # Training loop\n\t    session.epoch_progress = tqdm.tqdm(\n\t        range(1, args.n_epochs + 1), desc=\"Train Progress\",\n\t        postfix=postfix_dict_epoch, dynamic_ncols=True)\n\t    for epoch in session.epoch_progress:\n\t        step_progress = session.create_step_progress()\n", "        session.t_step = time.time() # Include time for data loader\n\t        for step, (_, pos_graph, neg_graph, blocks) in enumerate(session.dataloader, start=1):            \n\t            session.run_pre_train_hooks(epoch, step)\n\t            # Training - forward pass\n\t            pos_logits, neg_logits, loss = session.train_step(pos_graph, neg_graph, blocks)\n\t            # Step evaluation\n\t            train_loss = loss.item()\n\t            result_dict = {\n\t                EvalMetrics.EPOCH: epoch, \n\t                EvalMetrics.STEP: step, \n", "                EvalMetrics.LOSS: train_loss\n\t            }\n\t            result_dict.update(\n\t                EvalMetricsCalc.roc_auc_ap_score(pos_logits, neg_logits,\n\t                    metrics=[EvalMetrics.ROC_AUC]))\n\t            # Training - backward pass\n\t            optimizer.zero_grad()\n\t            loss.backward()\n\t            ## Gradient clipping\n\t            if ((epoch + step / session.step_per_epoch) >= args.grad_clip_since \n", "                and args.grad_clip_norm > 0):\n\t                grad_norm = torch.nn.utils.clip_grad.clip_grad_norm_(\n\t                    model.parameters(), args.grad_clip_norm\n\t                )\n\t                result_dict[EvalMetrics.GRAD_NORM] = grad_norm.item()\n\t            optimizer.step()\n\t            # Metrics recording\n\t            if args.use_gpu:\n\t                result_dict[EvalMetrics.GPU_VRAM] = session.max_gpu_vram\n\t            postfix_dict_step.update(\n", "                EvalMetricsCalc.format_str({\n\t                    EvalMetrics.GPU_VRAM: result_dict[EvalMetrics.GPU_VRAM]\n\t                }, as_dict=True)\n\t            )\n\t            session.t_train += time.time() - session.t_step\n\t            result_dict[EvalMetrics.TRAINING_TIME] = session.t_train\n\t            result_dict[EvalMetrics.TOTAL_TIME] = time.time() - session.t_start\n\t            result_str = EvalMetricsCalc.format_str(result_dict, rank=session.rank)\n\t            step_progress.write(\" | \".join(result_str).strip())\n\t            result_dict[EvalMetrics.ITER] = session.iter_count\n", "            result_dict[EvalMetrics.RAM] = get_peak_mem()\n\t            result_dict = {key.value: value for key, value in result_dict.items()}\n\t            stats_dict[\"train_stats\"].append(\n\t                pandas.Series(result_dict))\n\t            step_progress.set_postfix(postfix_dict_step)\n\t            if session.unit_step_progress:\n\t                step_progress.update(1)\n\t            else:\n\t                step_progress.update(pos_graph.num_edges())\n\t            stats_dict[\"metrics\"][\"train_stats\"] = result_dict\n", "            # Memory cleanup\n\t            model.cleanup_step()\n\t            del pos_graph, neg_graph, blocks, pos_logits, neg_logits, loss\n\t            gc.collect()\n\t            # Evaluation\n\t            if (valid_edge is not None) and session.is_eval_point():\n\t                model_at_best = session.run_evaluation(\"valid\")\n\t                if model_at_best:\n\t                    session.save_checkpoints(\"best\")\n\t                session.save_checkpoints(\"latest\")\n", "            else:\n\t                if step % args.save_every == 0:\n\t                    write_stats(args, stats_dict, train_stats=True, metrics=True)\n\t                if (args.use_gpu and args.clear_cache_every > 0 \n\t                    and step % args.clear_cache_every == 0):\n\t                    torch.cuda.empty_cache()\n\t            session.run_post_step_hooks()\n\t            session.t_step = time.time()\n\t            if session.is_stopping:\n\t                session.write(\"[Training] Stopping training...\")\n", "                break\n\t        session.close_step_progress()\n\t        if session.is_stopping:\n\t            break\n\t    write_stats(args, stats_dict, train_stats=True, valid_stats=True, metrics=True)\n\t    session.signac_mark_success()\n\t    if distmgr is not None:\n\t        distmgr.wait_cluster(session)\n\t    # Test on test set\n\t    if test_edge is not None and session.enable_eval:\n", "        session.clean_memory()\n\t        session.run_evaluation(\"test\")\n\t        session.clean_memory()\n\t        session.run_evaluation(\"best\")\n\t    # Keep this at the last of the code\n\t    if args.use_signac and distmgr is None:\n\t        args.result_path = args.result_path_final\n\t        write_stats(args, stats_dict, train_stats=True, valid_stats=True, metrics=True)\n\tdef start_server():\n\t    '''\n", "    Start the server for distributed training.\n\t    '''\n\t    # check device\n\t    if args.use_gpu and torch.cuda.is_available():\n\t        device = torch.device(\"cuda:{}\".format(args.gpu))\n\t        torch.cuda.set_device(args.gpu)\n\t    else:\n\t        args.use_gpu = False\n\t        device = torch.device(\"cpu\")\n\t    print(f\"Using device: {device}\")\n", "    datamgr = DataMgr(args.dataset, args.to_undirected, args.use_self_loops, **args.dataset_params)\n\t    datamgr.prepare_dataset()\n\t    args.dataset_str = datamgr.dataset_str\n\t    dist_params = {\n\t        (key[5:] if key.startswith(\"eval_\") else key): (eval(value) if key.startswith(\"eval_\") else value)\n\t        for key, value in args.distributed_params.items()\n\t    }\n\t    distmgr = AvgClusterServer(args.world_size, args.dist_is_master, datamgr, **dist_params)\n\t    distmgr.sync_train_data()\n\t    session = SessionMgr(args, datamgr, device, distmgr=distmgr)\n", "    if datamgr.valid_edge is not None:\n\t        session.create_evaluator(\"valid\")\n\t    session.create_model()\n\t    # create training epoch\n\t    stats_dict: Final[dict] = session.create_stats_dict()\n\t    distmgr.server_main(session)\n\tdef multigpu_start(proc_id, args, datamgr, distmgr=None):\n\t    '''\n\t    Start processes for Multi-GPU DDP training.\n\t    When `proc_id` is 0, it will start the server process;\n", "    otherwise, it will start the trainer process.\n\t    Arguments\n\t    ---------\n\t    proc_id: int\n\t        Process ID in Multi-GPU DDP training.\n\t    args: argparse.Namespace\n\t        Parsed arguments.\n\t    datamgr: DataMgr\n\t        Data manager.\n\t    distmgr: MultiGPUTrainer\n", "        MultiGPUTrainer for the trainer processes.\n\t    '''\n\t    if proc_id == 0:\n\t        faulthandler.enable()\n\t        # vscode_debug()\n\t    torch.cuda.manual_seed_all(args.random_seed)\n\t    datamgr.multigpu_postprocess()\n\t    if proc_id == 0:\n\t        dev_id = args.gpu[proc_id]\n\t        distmgr = MultiGPUServer(args, dev_id)\n", "        session = SessionMgr(args, datamgr, distmgr.device, distmgr=distmgr)\n\t        if datamgr.valid_edge is not None:\n\t            session.create_evaluator(\"valid\")\n\t        session.create_model(create_optimizer=False)\n\t        session.create_stats_dict()\n\t        distmgr.server_main(session)\n\t    else:\n\t        start_trainer(proc_id, args, datamgr, distmgr)\n\tif __name__ == '__main__':\n\t    faulthandler.enable()\n", "    # Parse arguments\n\t    args = parse_args()\n\t    if args.debug:\n\t        # Disable workers for dataloader in debug mode\n\t        args.num_workers = 0\n\t    if (args.distributed is None) or not args.dist_is_master:\n\t        if type(args.gpu) is list:\n\t            # Start Multi-GPU DDP training\n\t            print(\"[Session] Preparing training data...\")\n\t            datamgr, distmgr = prepare_trainer_data(create_formats=False)\n", "            shared_graph = datamgr.multigpu_preprocess()\n\t            print(\"[Session] Starting MultiGPU processes...\")\n\t            mp.start_processes(multigpu_start, args=(args, datamgr, distmgr), \n\t                nprocs=len(args.gpu), start_method=\"forkserver\")\n\t        else:\n\t            # Start single-GPU training and evaluation\n\t            torch.cuda.manual_seed_all(args.random_seed)\n\t            print(\"[Session] Starting trainer...\")\n\t            datamgr, distmgr = prepare_trainer_data()\n\t            start_trainer(None, args, datamgr, distmgr)\n", "    else:\n\t        # Start server side for distributed training\n\t        torch.cuda.manual_seed_all(args.random_seed)\n\t        print(\"[Session] Starting distributed server...\")\n\t        start_server()\n"]}
{"filename": "src/distributed/AvgCluster.py", "chunked_list": ["'''\n\tImplementation of distributed training with Time-based Model Aggregation (TMA)\n\tand randomized partitioning of the graph (SuperTMA / RandomTMA).\n\tSee paper \"Simplifying Distributed Neural Network Training on Massive Graphs: \n\tRandomized Partitions Improve Model Aggregation\" for details.\n\t'''\n\timport gc\n\timport os\n\timport pickle\n\timport time\n", "from collections import deque\n\tfrom datetime import timedelta\n\tfrom enum import IntEnum\n\tfrom functools import partial\n\tfrom pathlib import Path\n\tfrom typing import Final, List, Union\n\timport dgl\n\timport numpy as np\n\timport scipy.sparse as sp\n\timport torch\n", "import torch.distributed as dist\n\timport tqdm\n\tfrom ..dataloading.DataMgr import DataMgr, MAG240MDatasetRAM\n\tfrom ..pipelines.utils import dist_isend_obj, dist_recv_obj, parse_timedelta\n\tfrom ..samplers import ClusterGCN\n\tfrom ..samplers.ClusterGCN import ClusterGCNClusterSampler\n\tclass DistStatus(IntEnum):\n\t    '''\n\t    Status of the distributed training process.\n\t    '''\n", "    INITIALIZED = 0\n\t    RUNNING = 1\n\t    COMPLETED = 2\n\t    CLOSE_OK = 3\n\t    CLOSED = 4\n\tclass AvgOperator:\n\t    @classmethod\n\t    def initialize(cls, cluster, session):\n\t        '''\n\t        Initialize model parameters on trainers by broadcasting \n", "        the initialized parameters from server process (rank 0).\n\t        '''\n\t        avg_progress = tqdm.tqdm(\n\t            session.model.parameters(),\n\t            desc=\"Model Averaging:\",\n\t            postfix=dict(rank=cluster.rank, status=cluster.status,\n\t                         op=\"initialize\"),\n\t            dynamic_ncols=True\n\t        )\n\t        weights : torch.nn.parameter.Parameter\n", "        for weights in avg_progress:\n\t            dist.broadcast(weights.data, src=0)\n\t    @classmethod\n\t    def average(cls, cluster, session):\n\t        '''\n\t        Sync model parameters on trainers by averaging their parameters\n\t        and replace the parameters on the server process (rank 0).\n\t        '''\n\t        avg_progress = tqdm.tqdm(\n\t            session.model.parameters(),\n", "            desc=\"Model Averaging:\",\n\t            postfix=dict(rank=cluster.rank, status=cluster.status,\n\t                         op=\"average\"),\n\t            dynamic_ncols=True\n\t        )\n\t        weights : torch.nn.parameter.Parameter\n\t        for weights in avg_progress:\n\t            if cluster.is_master:\n\t                # Set weights from rank 0 to be zeros\n\t                weights.data.zero_()\n", "            dist.all_reduce(weights.data, dist.ReduceOp.SUM)\n\t            weights.data /= float(cluster.num_workers)\n\t    @classmethod\n\t    def soft_medoid(cls, cluster, session, metrics=\"-Loss\", \n\t                    medoid_T : int=1):\n\t        perf_dict_list = cluster.gather_object(cluster.perf_dict)\n\t        if cluster.is_master:\n\t            perf_list = np.array([\n\t                perf_dict_list[i][metrics[1:]]\n\t                for i in range(1, cluster.world_size)])\n", "            diff = np.abs(perf_list - perf_list[:, None])\n\t            coeff = np.exp(-diff.sum(0) / medoid_T)\n\t            coeff /= coeff.sum()\n\t            coeff = torch.tensor(coeff, dtype=torch.float32)\n\t        else:\n\t            coeff = torch.zeros(cluster.num_workers, dtype=torch.float32)\n\t        dist.broadcast(coeff, src=0)\n\t        avg_progress = tqdm.tqdm(\n\t            session.model.parameters(),\n\t            desc=\"Model Averaging:\",\n", "            postfix=dict(rank=cluster.rank, status=cluster.status, \n\t                         op=\"soft_medoid\", coeff=coeff),\n\t            dynamic_ncols=True\n\t        )\n\t        weights : torch.nn.parameter.Parameter\n\t        for weights in avg_progress:\n\t            if cluster.is_master:\n\t                # Set weights from rank 0 to be zeros\n\t                weights.data.zero_()\n\t            else:\n", "                weights.data = weights.data * coeff[cluster.rank - 1]\n\t            dist.all_reduce(weights.data, dist.ReduceOp.SUM)\n\t    @classmethod\n\t    def find_best_perf_id(cls, cluster, session, metrics=\"-Loss\"):\n\t        perf_dict_list = cluster.gather_object(cluster.perf_dict)\n\t        if cluster.is_master:\n\t            perf_list = np.array([\n\t                perf_dict_list[i][metrics[1:]]\n\t                for i in range(1, cluster.world_size)])\n\t            if metrics[0].startswith(\"-\"):\n", "                perf_list = - perf_list\n\t            best_perf_id = perf_list.argmax() + 1\n\t            best_perf_id = torch.tensor([best_perf_id], dtype=torch.int32)\n\t        else:\n\t            best_perf_id = torch.zeros([1], dtype=torch.int32)\n\t        dist.broadcast(best_perf_id, src=0)\n\t        return best_perf_id.item()\n\t    @classmethod\n\t    def select_best_perf(cls, cluster, session, metrics=\"-Loss\"):\n\t        best_perf_id = cls.find_best_perf_id(cluster, session, metrics)\n", "        avg_progress = tqdm.tqdm(\n\t            session.model.parameters(),\n\t            desc=\"Model Averaging:\",\n\t            postfix=dict(rank=cluster.rank, status=cluster.status,\n\t                         op=\"select_best_perf\", best_id=best_perf_id),\n\t            dynamic_ncols=True\n\t        )\n\t        weights : torch.nn.parameter.Parameter\n\t        for weights in avg_progress:\n\t            dist.broadcast(weights.data, src=best_perf_id)\n", "    @classmethod\n\t    def llcg(cls, cluster, session, corr_steps=2):\n\t        '''\n\t        Implementation of the global server correction step of LLCG.\n\t        See paper: Learn Locally, Correct Globally: A Scalable Framework for Training Graph Neural Networks\n\t        https://openreview.net/pdf?id=FndDxSz3LxQ\n\t        '''\n\t        cls.average(cluster, session)\n\t        # Server correction\n\t        if cluster.is_master:\n", "            with torch.enable_grad():\n\t                for step in tqdm.tqdm(range(corr_steps), desc=\"Server correction\"):\n\t                    try:\n\t                        _, pos_graph, neg_graph, blocks = next(session.dataloader_iter)\n\t                    except StopIteration:\n\t                        session.dataloader_iter = iter(session.dataloader)\n\t                        _, pos_graph, neg_graph, blocks = next(session.dataloader_iter)\n\t                    _, _, loss = session.train_step(pos_graph, neg_graph, blocks)\n\t                    session.optimizer.zero_grad()\n\t                    loss.backward()\n", "                    session.optimizer.step()\n\t            del _, pos_graph, neg_graph, blocks, loss\n\t            gc.collect()\n\t            torch.cuda.empty_cache()\n\t        weights : torch.nn.parameter.Parameter\n\t        avg_progress = tqdm.tqdm(\n\t            session.model.parameters(),\n\t            desc=\"Model Averaging:\",\n\t            postfix=dict(rank=cluster.rank, status=cluster.status,\n\t                         op=\"llcg\"),\n", "            dynamic_ncols=True\n\t        )\n\t        for weights in avg_progress:\n\t            dist.broadcast(weights.data, src=0)\n\tclass AvgCluster:\n\t    '''\n\t    Distributed manager on trainer side for TMA training.\n\t    This class also acts as a DataMgr on trainer side, since the training data\n\t    is distributed by the server, and there is no separate DataMgr on trainer side. \n\t    '''\n", "    def __init__(self, world_size: int, is_master: bool,\n\t                 dataset_name: str, dataset_kwargs=None,\n\t                 avg_operator=\"average\", **kwargs) -> None:\n\t        '''\n\t        Arguments:\n\t        ----------\n\t        world_size: int\n\t            Number of processes (server + trainer) in the cluster.\n\t        is_master: bool\n\t            Whether this process is the server process.\n", "        dataset_name: str\n\t            Name of the dataset to be used.\n\t        dataset_kwargs: dict\n\t            Keyword arguments for the dataset.\n\t        avg_operator: str\n\t            Name of the averaging operator to be used.\n\t            Must be a name of a method defined in `AvgOperator.{avg_operator}`.\n\t        kwargs: dict\n\t            Additional keyword arguments for the averaging operator.\n\t            See `AvgOperator.{avg_operator}` to check the required arguments.\n", "        '''\n\t        self.is_master = is_master\n\t        self.world_size = world_size\n\t        self.rank = dist.get_rank()\n\t        print(\"[Dist] Establishing distributed store...\")\n\t        self.dist_store = dist.TCPStore(\n\t            os.environ[\"MASTER_ADDR\"],\n\t            int(os.environ[\"TCPSTORE_PORT\"]),\n\t            world_size=world_size,\n\t            is_master=is_master,\n", "            timeout=timedelta(minutes=30)\n\t        )\n\t        print(\"[Dist] Distributed store established.\")\n\t        self.dist_store.set_timeout(timedelta(seconds=10))\n\t        self.status = DistStatus.INITIALIZED\n\t        self.running = False\n\t        self.dataset_name = dataset_name\n\t        if dataset_kwargs is None:\n\t            self.dataset_kwargs = dict()\n\t        else:\n", "            self.dataset_kwargs = dataset_kwargs\n\t        self.pgs_dict = dict()\n\t        self.perf_dict = dict()\n\t        self.enable_eval = False\n\t        # self.dist_store_rank = dist.PrefixStore(f\"{self.rank}_\", self.dist_store)\n\t        self.avg_operator = getattr(AvgOperator, avg_operator)\n\t        # Parse additional arguments for some aggregation operators\n\t        if self.avg_operator in [\n\t            AvgOperator.select_best_perf, AvgOperator.soft_medoid]:\n\t            if \"metrics\" in kwargs:\n", "                self.avg_operator = partial(\n\t                    self.avg_operator, metrics=kwargs[\"metrics\"])\n\t        if self.avg_operator in [AvgOperator.soft_medoid]:\n\t            if \"medoid_T\" in kwargs:\n\t                self.avg_operator = partial(\n\t                    self.avg_operator, medoid_T=kwargs[\"medoid_T\"])\n\t        if self.avg_operator in [AvgOperator.llcg]:\n\t            if \"corr_steps\" in kwargs:\n\t                self.avg_operator = partial(\n\t                    self.avg_operator, corr_steps=kwargs[\"corr_steps\"]\n", "                )\n\t        self.train_frac = self.dataset_kwargs.get(\"train_frac\", 1.0)\n\t        self.random_seed = self.dataset_kwargs.get(\"rnd_seed\", None)\n\t    @property\n\t    def status(self):\n\t        '''\n\t        Get the status of this process.\n\t        '''\n\t        return DistStatus(int(self.dist_store.get(f\"{self.rank}-status\")))\n\t    @status.setter\n", "    def status(self, status: DistStatus):\n\t        '''\n\t        Set the status of this process.\n\t        '''\n\t        self.dist_store.set(f\"{self.rank}-status\", str(status.value))\n\t    def gather_object(self, obj):\n\t        '''\n\t        Gather an object from all processes in the cluster.\n\t        Arguments:\n\t        ----------\n", "        obj: object\n\t            The object to be gathered.\n\t        Returns:\n\t        --------\n\t        obj_list: list\n\t            A list of objects gathered from all processes in the cluster.\n\t        '''\n\t        if self.is_master:\n\t            obj = None\n\t            obj_list = [None] * self.world_size\n", "        else:\n\t            obj_list = None\n\t        dist.gather_object(obj, obj_list)\n\t        return obj_list\n\t    @property\n\t    def is_averaging(self) -> bool:\n\t        '''\n\t        Indicate if this process should now enter model averaging.\n\t        '''\n\t        return bool(int(self.dist_store.get(\"is_averaging\")))\n", "    @is_averaging.setter\n\t    def is_averaging(self, value: bool):\n\t        '''\n\t        Set the flag indicating if this process should now enter model averaging.\n\t        '''\n\t        self.dist_store.set(\"is_averaging\", str(int(value)))\n\t    def model_averaging(self, session):\n\t        '''\n\t        Conduct model averaging.\n\t        '''\n", "        with torch.no_grad():\n\t            session.save_checkpoints(\"local\")\n\t            session.write(\n\t                f\"[Model Averaging] Entering model averaging as rank {self.rank}...\")\n\t            if self.status < DistStatus.RUNNING:\n\t                # Initialization phase: push server model weights to trainers\n\t                AvgOperator.initialize(self, session)\n\t                # Sync signac IDs\n\t                if session.args.use_signac:\n\t                    dist_signac_ids = [None] * self.world_size\n", "                    dist.all_gather_object(dist_signac_ids, session.args.signac_job.id)\n\t                    session.signac_job.doc.dist_signac_ids = dist_signac_ids\n\t                # Sync preprocessing time\n\t                if self.is_master:\n\t                    t_preprocess = [session.t_preprocess]\n\t                else:\n\t                    t_preprocess = [None]\n\t                dist.broadcast_object_list(t_preprocess)\n\t                session.reset_timer()\n\t                session.t_preprocess += t_preprocess[0]\n", "                # if not self.is_master:\n\t                #     session.add_timer(t_preprocess[0])\n\t            else: # Running phase\n\t                # Aggregate model weights through selected model operator\n\t                self.avg_operator(self, session)\n\t                # Update progress from each worker\n\t                pgs_dict_list = self.gather_object(self.pgs_dict)\n\t                if self.is_master:\n\t                    for worker_id in range(1, self.world_size):\n\t                        self.update_worker_progress(\n", "                            session, worker_id, pgs_dict_list[worker_id])\n\t            session.save_checkpoints(\"latest\")\n\t    def sync_train_data(self):\n\t        '''\n\t        Get the training data partition from the server process,\n\t        and side-load feature if necessary.\n\t        '''\n\t        sg, node_ids, partition_offset, clusters, train_adj_sg_obj, metaDict = dist_recv_obj(0)\n\t        self.train_graph = sg\n\t        self.train_edge_idx = sg.edges(\"eid\")\n", "        self.node_ids = node_ids # Node ID in the *original* graph\n\t        self.partition_offset = partition_offset\n\t        self.clusters = clusters\n\t        self.cluster_mapping_dict = {\n\t            cid: i for i, cid in enumerate(clusters)\n\t        }\n\t        if len(train_adj_sg_obj) == 4:\n\t            self.train_adj_sg = sp.csr_matrix(\n\t                train_adj_sg_obj[:3], shape=train_adj_sg_obj[-1]\n\t            )\n", "        else:\n\t            self.train_adj_sg = sp.csr_matrix(\n\t                (np.ones_like(train_adj_sg_obj[0]), *train_adj_sg_obj[:2]), \n\t                shape=train_adj_sg_obj[-1]\n\t            )\n\t        # Check if the new node ID are assigned following the order of the clusters\n\t        assert (self.train_graph.ndata[dgl.NID] == self.node_ids).all()\n\t        # New node ID should be torch.arange(self.train_graph.num_nodes())\n\t        if self.partition_offset is not None:\n\t            self.cluster_sampler = ClusterGCN.ClusterGCNEdgeSampler.create_from_mem(\n", "                self.partition_offset, torch.arange(self.train_graph.num_nodes())\n\t            )\n\t        self.n_relations = metaDict[\"n_relations\"]\n\t        self.n_nodetypes = metaDict[\"n_nodetypes\"]\n\t        self.valid_edge = None\n\t        self.test_edge = None\n\t        # Side-loading additional feature information that is not sent by the server\n\t        # for some large datasets (e.g. mag240m)\n\t        if \"feat\" in self.train_graph.ndata:\n\t            print('Using features in train_graph.ndata[\"feat\"]')\n", "            self.use_feat_loader = False\n\t            self.in_dim = self.train_graph.ndata[\"feat\"].shape[-1]\n\t            self.feat_dtype = self.train_graph.ndata[\"feat\"].dtype\n\t        elif self.dataset_name == \"mag240m\":\n\t            print('Using feature loader')\n\t            self.dataset = MAG240MDatasetRAM(\n\t                tmpfs_dir=self.dataset_kwargs.get(\"tmpfs_dir\", None))\n\t            self.use_feat_loader = True\n\t            self.node_features = self.dataset.paper_feat\n\t            self.in_dim = self.dataset.paper_feat.shape[-1]\n", "            if self.dataset.paper_feat.dtype == np.float16:\n\t                self.feat_dtype = torch.float16\n\t            else:\n\t                self.feat_dtype = torch.float32\n\t        else:\n\t            raise ValueError(\"Feature or feature loader is missing for trainer!\")\n\t    def feature_loader(self, first_block, device=None):\n\t        '''\n\t        Implement a feature loader as in `dataloading.DataMgr`.\n\t        '''\n", "        if device is None:\n\t            device = first_block.device\n\t        if self.dataset_name in ['mag240m']:\n\t            sg_nids = first_block.srcdata[dgl.NID].cpu()\n\t            nids = self.train_graph.ndata[dgl.NID][sg_nids]\n\t            feat = self.node_features[nids]\n\t            return torch.as_tensor(feat, device=device)\n\t        else:\n\t            return None\n\t    @property\n", "    def num_workers(self) -> int:\n\t        '''\n\t        Return the number of trainers (excluding server) in the distributed cluster.\n\t        '''\n\t        return self.world_size - 1\n\t    def run_pre_train_hooks(self, session):\n\t        '''\n\t        Run hooks before each training step to register current training progress.\n\t        '''\n\t        for name in [\"epoch\", \"step\", \"iter_count\", \"step_per_epoch\"]:\n", "            value = getattr(session, name)\n\t            self.pgs_dict[name] = int(value)\n\t            self.dist_store.set(f\"{self.rank}-{name}\", str(int(value)))\n\t        if not self.running: \n\t            # Initialize model parameters if training has not started\n\t            if session.args.use_signac:\n\t                self.dist_store.set(\n\t                    f\"{self.rank}-signac_id\", str(session.args.signac_job.id))\n\t            self.model_averaging(session)\n\t            self.status = DistStatus.RUNNING\n", "            self.running = True\n\t        else:\n\t            for name in [\"Loss\", \"AUC\"]:\n\t                self.perf_dict[name] = session.metrics[\"train_stats\"][name]\n\t            server_status = DistStatus(int(self.dist_store.get(f\"0-status\")))\n\t            if server_status == DistStatus.CLOSE_OK:\n\t                session.is_stopping = True\n\t    def run_post_step_hooks(self, session):\n\t        '''\n\t        Run hooks after each training step to enter model aggregation if needed.\n", "        '''\n\t        if self.is_averaging:\n\t            self.model_averaging(session)\n\t    def wait_cluster(self, session):\n\t        '''\n\t        Wait for other processes in the cluster to finish training.\n\t        '''\n\t        self.status = DistStatus.COMPLETED\n\t        while self.status != DistStatus.CLOSE_OK:\n\t            if self.is_averaging:\n", "                self.model_averaging(session)\n\t            time.sleep(0.1)\n\t        self.status = DistStatus.CLOSED\n\tclass AvgClusterServer(AvgCluster):\n\t    '''\n\t    Distributed manager on server side for TMA training.\n\t    '''\n\t    def __init__(self, world_size: int, is_master: bool,\n\t                 datamgr: DataMgr, num_parts: int, cache_path,\n\t                 sync_every: str, group_strategy=\"uniform-cluster\",\n", "                 avg_operator=\"average\", balance_ntypes=None, balance_edges=False,\n\t                 mode='k-way', drop_parts: List[int] = None, **kwargs) -> None:\n\t        '''\n\t        Initialize server side for TMA training and generate super-nodes (mini-clusters).\n\t        Arguments\n\t        ---------\n\t        world_size: int\n\t            Number of processes (server + trainer) in the cluster.\n\t        is_master: bool\n\t            Whether this process is the server process.\n", "        datamgr: DataMgr\n\t            Data manager for dataset. \n\t        num_parts: int\n\t            Number of supernodes (mini-clusters) to generate by METIS. \n\t            This has no effect if `group_strategy` is \"random-cluster\".\n\t        cache_path: str\n\t            Path to store the generated supernodes.\n\t        sync_every: str, format x minutes as \"{x}min\" or x hours \"{x}h\".\n\t            The frequency to aggregate model parameters.\n\t        group_strategy: str, choices=[\"uniform-cluster\", \"random-cluster\"]\n", "            The scheme to generate graph partition for each trainer.\n\t            - If \"uniform-cluster\", super-nodes (mini-clusters) will be first generated by METIS, and\n\t              then each trainer will be assigned a random subset of super-nodes (SuperTMA).\n\t              * When `num_parts` equal to the number of trainers, this is equivalent to PSGD-PA partition scheme.\n\t            - If \"random-cluster\", each trainer will be assigned a random subset of nodes (RandomTMA).\n\t        avg_operator: str\n\t            Name of the averaging operator to be used.\n\t            Must be a name of a method defined in `AvgOperator.{avg_operator}`.\n\t        drop_parts: List[int]\n\t            List of partition ID (not super-node ID) that will be dropped and not assigned to any trainer.\n", "            Used to mimic the case where some trainers are down.\n\t        kwargs: dict\n\t            Additional keyword arguments for the averaging operator.\n\t            See `AvgOperator.{avg_operator}` to check the required arguments.\n\t        For usages of `balance_ntypes`, `balance_edges` and `mode`, see \n\t        `samplers.ClusterGCN.ClusterGCNClusterSampler`.\n\t        '''\n\t        super().__init__(world_size, is_master,\n\t            datamgr.dataset_name,\n\t            dataset_kwargs=datamgr.dataset_kwargs,\n", "            avg_operator=avg_operator,\n\t            **kwargs\n\t        )\n\t        if avg_operator == \"llcg\": # not self.avg_operator here since it is a function\n\t            self.need_train_dataloader = True\n\t        else:\n\t            self.need_train_dataloader = False\n\t        self.datamgr = datamgr\n\t        self.train_graph = self.datamgr.train_graph\n\t        self.num_parts = num_parts\n", "        self.group_strategy = group_strategy.split(\":\")[0]\n\t        if len(group_strategy.split(\":\")) > 1:\n\t            self.group_random_seed = int(group_strategy.split(\":\", maxsplit=1)[-1])\n\t        else:\n\t            self.group_random_seed = None\n\t        self.sampler = ClusterGCNClusterSampler(\n\t            self.datamgr.train_graph,\n\t            self.num_parts, cache_path,\n\t            balance_ntypes=balance_ntypes,\n\t            balance_edges=balance_edges,\n", "            mode=mode,\n\t            random_seed=self.group_random_seed\n\t        )\n\t        if type(sync_every) is str:\n\t            self.sync_every = parse_timedelta(sync_every)\n\t        else:\n\t            self.sync_every = sync_every\n\t        self.is_averaging = False\n\t        self.is_saved_partitions = False\n\t        self.eval_thread = None\n", "        self.eval_model = None\n\t        self.enable_eval = True\n\t        self.drop_parts = (set(drop_parts) if drop_parts else [])\n\t    @property\n\t    def full_world_size(self):\n\t        return self.world_size + len(self.drop_parts)\n\t    @property\n\t    def partition_str(self):\n\t        '''\n\t        Get a string representation of the current partitioning strategy.\n", "        '''\n\t        part_str = f\"{self.full_world_size - 1}-{self.group_strategy}\"\n\t        if self.group_random_seed is not None:\n\t            part_str += f\":{self.group_random_seed}\"\n\t        return part_str\n\t    @property\n\t    def saved_partitions(self) -> List:\n\t        '''\n\t        Get the list of saved graph partition strings that indicate the partitions\n\t        that can be reused.\n", "        '''\n\t        if self.sampler.signac_job is None:\n\t            return []\n\t        else:\n\t            return self.sampler.signac_job.doc.get(\"saved_partitions\", [])\n\t    @saved_partitions.setter\n\t    def saved_partitions(self, value):\n\t        self.sampler.signac_job.doc.saved_partitions = value\n\t    def partition_iter(self):\n\t        '''\n", "        Get an iterator of graph partitions.\n\t        If partitions matching the current partition strategy have been saved,\n\t        then load them from disk to save time. Otherwise, generate new partitions.\n\t        Yield\n\t        -----\n\t        sg: DGLGraph\n\t            A partition of the training graph to be sent to a trainer.\n\t        node_ids: Tensor\n\t            Node IDs of the partition.\n\t        cluster_offset: Tensor\n", "            Indicate the start indices of each super-node (mini-cluster) in the `node_ids` tensor.\n\t        clusters_ids: Tensor\n\t            IDs of the super-nodes (mini-clusters) contained in the partition. When interpreted together with\n\t            `cluster_offset`, this can be used to map a node ID to its corresponding super-node ID.\n\t        '''\n\t        if self.partition_str in self.saved_partitions:\n\t            print(\"[Dist] Using saved partitions\")\n\t            self.is_saved_partitions = True\n\t            with self.sampler.signac_job:\n\t                for ind in range(1, self.full_world_size):\n", "                    if ind not in self.drop_parts:\n\t                        with open(f\"saved_partitions/{self.partition_str}-{ind}.pkl\", \"rb\") as f:\n\t                            yield pickle.load(f)\n\t        elif self.group_strategy == \"uniform-cluster\":\n\t            self.is_saved_partitions = False\n\t            group_list = self.sampler.group_clusters(self.full_world_size - 1, self.group_strategy)\n\t            for ind, clusters in enumerate(group_list, start=1):\n\t                if ind not in self.drop_parts:\n\t                    yield *self.sampler.sample(self.train_graph, clusters), clusters\n\t        elif self.group_strategy == \"random-nodes\":\n", "            self.is_saved_partitions = False\n\t            node_idx = self.train_graph.nodes()\n\t            if self.group_random_seed is not None:\n\t                rnd_state = torch.get_rng_state()\n\t            rand_exps = torch.rand(len(node_idx), dtype=torch.float16)\n\t            if self.group_random_seed is not None:\n\t                torch.set_rng_state(rnd_state)\n\t            step = 1.0 / (self.full_world_size - 1)\n\t            for i in range(self.full_world_size - 1):\n\t                if (i + 1) not in self.drop_parts:\n", "                    if i != self.full_world_size - 2:\n\t                        s_idx = node_idx[(rand_exps >= i * step) & (rand_exps < (i + 1) * step)]\n\t                    else:\n\t                        s_idx = node_idx[(rand_exps >= i * step)]\n\t                    sg = self.sampler.get_subgraph(self.train_graph, s_idx)\n\t                    yield sg, s_idx, [], []\n\t        else:\n\t            raise ValueError(f\"Unknown group strategy {self.group_strategy}\")\n\t    def sync_train_data(self):\n\t        '''\n", "        Partition and distribute training data to trainers.\n\t        '''\n\t        print(\"[Dist] Syncing training data...\")\n\t        print(\"Full graph:\", self.train_graph)\n\t        part_iter = self.partition_iter()\n\t        req_queue = deque()\n\t        for t_rank, (sg, node_ids, cluster_offset, clusters_ids) in enumerate(part_iter, start=1):\n\t            print(f\"Sending subgraph with {len(clusters_ids)} clusters to worker {t_rank}:\")\n\t            print(sg)\n\t            # Construct sparse matrix with src nodes as nodes in the subgraph and target nodes as \n", "            # nodes in the full graph in their original node IDs. \n\t            train_adj = self.train_graph.adj(scipy_fmt=\"csr\")\n\t            train_adj_sg : sp.csr_matrix = train_adj[node_ids, :]\n\t            if (train_adj_sg.data == 1).all():\n\t                train_adj_sg_obj = (\n\t                    train_adj_sg.indices, train_adj_sg.indptr, train_adj_sg.shape)\n\t            else:\n\t                train_adj_sg_obj = (\n\t                    train_adj_sg.data, train_adj_sg.indices, train_adj_sg.indptr, train_adj_sg.shape)\n\t            metaDict = dict(\n", "                n_relations = getattr(self.datamgr, \"n_relations\", 1),\n\t                n_nodetypes = getattr(self.datamgr, \"n_nodetypes\", 1)\n\t            )\n\t            size_req, byte_req = dist_isend_obj(\n\t                (sg, node_ids, cluster_offset, clusters_ids, train_adj_sg_obj, metaDict), t_rank)\n\t            req_queue.append((size_req, byte_req))\n\t            if (self.num_parts > 1 and (self.sampler.signac_job is not None) \n\t                and not self.is_saved_partitions and (len(self.drop_parts) == 0)):\n\t                with self.sampler.signac_job:\n\t                    Path(\"saved_partitions\").mkdir(parents=True, exist_ok=True)\n", "                with self.sampler.signac_job:\n\t                    with open(f\"saved_partitions/{self.partition_str}-{t_rank}.pkl\", \"wb\") as f:\n\t                        pickle.dump((sg, node_ids, cluster_offset, clusters_ids), f)\n\t            del sg, node_ids, cluster_offset, clusters_ids\n\t            gc.collect()\n\t        if (self.num_parts > 1 and (self.sampler.signac_job is not None) \n\t            and not self.is_saved_partitions and (len(self.drop_parts) == 0)):\n\t            print(f\"[Dist] Partitions saved to {self.sampler.signac_job.ws}/saved_partitions\")\n\t            self.saved_partitions = list(self.saved_partitions) + [self.partition_str]\n\t        while len(req_queue) > 0:\n", "            job = req_queue.popleft()\n\t            for req in job:\n\t                req.wait()\n\t    def get_worker_status(self, worker_id: int) -> int:\n\t        return int(self.dist_store.get(f\"{worker_id}-status\"))\n\t    def model_averaging(self, session, run_eval=True):\n\t        '''\n\t        Server side function to perform model averaging.\n\t        Arguments\n\t        ---------\n", "        session : SessionMgr\n\t            Session manager object.\n\t        run_eval : bool\n\t            Whether to run evaluation on validation set on aggregated model after model averaging.\n\t        '''\n\t        self.is_averaging = True\n\t        super().model_averaging(session)\n\t        self.is_averaging = False\n\t        self.t_averaging = time.time()\n\t        # Run evaluation\n", "        if run_eval:\n\t            if self.eval_model is None:\n\t                self.eval_model = session.create_model(\n\t                    create_optimizer=False, inplace=False)\n\t            elif session.model_at_best:\n\t                # Check if previous evaluation yields best performance\n\t                session.save_checkpoints(\"best\", model=self.eval_model)\n\t            if type(self.sync_every) is timedelta:\n\t                # Update current epoch count\n\t                self.next_sync_point = self.min_epoch_count\n", "            self.eval_model.load_state_dict(session.model.state_dict())\n\t            session.run_evaluation(\n\t                \"valid\",\n\t                alt_epoch=self.next_sync_point,\n\t                model=self.eval_model,\n\t                callback=lambda: self.poll_workers(session, run_eval=False)\n\t            )\n\t    @property\n\t    def min_epoch_count(self):\n\t        '''\n", "        Calculate the minimum epoch count among all trainers (i.e., progress of the slowest trainer) \n\t        based on information in `self.worker_dict`.\n\t        '''\n\t        epoch_count_list = [\n\t            self.worker_dict[worker_id][\"epoch\"] - 1\n\t            + self.worker_dict[worker_id][\"step\"]\n\t            / self.worker_dict[worker_id][\"step_per_epoch\"]\n\t            for worker_id in range(1, self.world_size)]\n\t        min_epoch_count = min(epoch_count_list)\n\t        return min_epoch_count\n", "    def poll_workers(self, session, run_eval=True):\n\t        '''\n\t        Poll progress of workers and trigger model averaging if necessary.\n\t        Arguments:\n\t        ----------\n\t        session : SessionMgr\n\t            Session manager object.\n\t        run_eval : bool\n\t            Whether to run evaluation on validation set on aggregated model after model averaging.\n\t        '''\n", "        worker_queue : Final[deque] = self.worker_queue\n\t        worker_dict : Final[dict] = self.worker_dict\n\t        while len(worker_queue) > 1:\n\t            worker_id = worker_queue.popleft()\n\t            if worker_id is None: # broadcast operations\n\t                min_epoch_count = self.min_epoch_count\n\t                if self.update_sync_point:\n\t                    # Calculate next sync point\n\t                    factor = min_epoch_count // self.sync_every\n\t                    base_next_sync_point = self.next_sync_point + self.sync_every\n", "                    if factor >= base_next_sync_point // self.sync_every:\n\t                        self.next_sync_point = factor * self.sync_every\n\t                    else:\n\t                        self.next_sync_point = base_next_sync_point\n\t                    session.write(\n\t                        f\"[Model Averaging] Next sync point at {self.next_sync_point}; \"\n\t                        f\"current min epoch at {min_epoch_count}\")\n\t                    self.update_sync_point = False\n\t                worker_status = [\n\t                    self.get_worker_status(worker_id) for worker_id in worker_queue]\n", "                if all((status == DistStatus.COMPLETED for status in worker_status)):\n\t                    if not run_eval:\n\t                        worker_queue.append(None)\n\t                        return\n\t                    worker_queue.append(None)\n\t                    self.model_averaging(session)\n\t                    worker_queue.pop()\n\t                    session.write(\"All workers have completed. Closing...\")\n\t                    for worker_id in worker_queue:\n\t                        # Inform each worker to close\n", "                        self.dist_store.set(f\"{worker_id}-status\", str(DistStatus.CLOSE_OK.value))\n\t                    while len(worker_queue) > 0:\n\t                        # Wait until each worker confirmed closing\n\t                        worker_id = worker_queue.popleft()\n\t                        status = self.get_worker_status(worker_id)\n\t                        while status != DistStatus.CLOSED:\n\t                            time.sleep(0.1)\n\t                            status = self.get_worker_status(worker_id)\n\t                        worker_dict[worker_id][\"bar\"].close()\n\t                        del worker_dict[worker_id][\"bar\"]\n", "                    # Exiting loop\n\t                elif all((status >= DistStatus.RUNNING for status in worker_status)):\n\t                    if time.time() - session.t_start > session.time_limit.total_seconds():\n\t                        ready_to_sync = True\n\t                        session.is_stopping = True\n\t                        self.status = DistStatus.CLOSE_OK\n\t                        self.next_sync_point = min_epoch_count\n\t                    elif type(self.sync_every) is not timedelta:\n\t                        ready_to_sync = np.zeros(len(worker_queue), dtype=bool)\n\t                        for i, worker_id in enumerate(worker_queue):\n", "                            if worker_status[i] == DistStatus.COMPLETED:\n\t                                ready_to_sync[i] = True\n\t                                continue\n\t                            sdict = worker_dict[worker_id]\n\t                            epoch_count = sdict[\"epoch\"] - 1\n\t                            epoch_count += sdict[\"step\"] / sdict[\"step_per_epoch\"]\n\t                            worker_dict[worker_id][\"bar\"].set_postfix(\n\t                                step=f'{sdict[\"step\"]}/{sdict[\"step_per_epoch\"]}',\n\t                                next_sync=self.next_sync_point,\n\t                                **session.postfix_dict_step\n", "                            )\n\t                            # iter_count = worker_dict[worker_id][\"iter_count\"]\n\t                            if epoch_count >= self.next_sync_point:\n\t                                ready_to_sync[i] = True\n\t                        ready_to_sync = ready_to_sync.all()\n\t                    else:\n\t                        ready_to_sync = ((time.time() - self.t_averaging)\n\t                                         > self.sync_every.total_seconds())\n\t                        self.next_sync_point = min_epoch_count\n\t                    if ready_to_sync:\n", "                        # Run model averaging\n\t                        worker_queue.append(None)\n\t                        self.model_averaging(session, run_eval=run_eval)\n\t                        if (type(self.sync_every) is not timedelta\n\t                            and self.next_sync_point < min_epoch_count):\n\t                            self.update_sync_point = True\n\t                        worker_queue.pop()\n\t                worker_queue.append(None)\n\t                return\n\t            else: # Update status from each worker\n", "                if type(self.sync_every) is not timedelta:\n\t                    # For deprecated step-based aggregation interval. \n\t                    # This has no effect when using time-based aggregation interval.\n\t                    worker_status = self.get_worker_status(worker_id)\n\t                    if worker_status >= DistStatus.RUNNING.value:\n\t                        # The following line of code may cause segmentation fault randomly\n\t                        pgs_dict = {\n\t                            name: int(self.dist_store.get(f\"{worker_id}-{name}\"))\n\t                            for name in [\"epoch\", \"step\", \"iter_count\", \"step_per_epoch\"]\n\t                        }\n", "                        self.update_worker_progress(session, worker_id, pgs_dict)\n\t                worker_queue.append(worker_id)\n\t    def update_worker_progress(self, session, worker_id: int, pgs_dict: dict):\n\t        '''\n\t        Update progress bar of each worker in the server process.\n\t        Arguments:\n\t        ----------\n\t        session: SessionMgr\n\t            Session manager object\n\t        worker_id: int\n", "            Rank of the distributed trainer\n\t        pgs_dict: dict\n\t            Dictionary containing progress information of the trainer\n\t        '''\n\t        epoch_delta = pgs_dict[\"epoch\"] - self.worker_dict[worker_id][\"epoch\"]\n\t        # worker_dict[worker_id][\"bar\"].write(str(epoch_delta))\n\t        self.worker_dict[worker_id][\"bar\"].update(\n\t            epoch_delta\n\t        )\n\t        self.worker_dict[worker_id][\"bar\"].set_postfix(\n", "            step=f'{pgs_dict[\"step\"]}/{pgs_dict[\"step_per_epoch\"]}',\n\t            next_sync=self.next_sync_point,\n\t            **session.postfix_dict_step\n\t        )\n\t        self.worker_dict[worker_id].update(pgs_dict)\n\t    def server_main(self, session):\n\t        '''\n\t        Main function for server process. \n\t        Arguments:\n\t        ----------\n", "        session: SessionMgr\n\t            Session manager object\n\t        '''\n\t        # session.t_preprocess = self.sampler.preprocess_time\n\t        session.reset_timer()\n\t        if self.need_train_dataloader:\n\t            print(\"[Dist] Creating train dataloader...\")\n\t            session.create_train_data_loader()\n\t            session.dataloader_iter = iter(session.dataloader)\n\t        # Calculate validation MFG\n", "        session.valid_evaluator.create_data_loader(session.model)\n\t        # Initial parameter sync\n\t        self.model_averaging(session, run_eval=False)\n\t        self.status = DistStatus.RUNNING\n\t        worker_queue = deque(range(1, self.world_size))\n\t        worker_dict = {\n\t            worker_id: dict(\n\t                bar=tqdm.tqdm(\n\t                    desc=f\"Worker {worker_id}\",\n\t                    total=session.args.n_epochs,\n", "                    position=i,\n\t                    dynamic_ncols=True\n\t                ),\n\t                epoch=0,\n\t                step=0,\n\t                iter_count=0,\n\t                step_per_epoch=float(\"inf\")\n\t            )\n\t            for i, worker_id in enumerate(worker_queue)\n\t        }\n", "        self.worker_queue = worker_queue\n\t        self.worker_dict = worker_dict\n\t        session.step_progress = worker_dict[1][\"bar\"]\n\t        session.postfix_dict_step = {\n\t            \"model\": f\"{session.args.model} ({session.model.num_model_params})\",\n\t            \"PID\": os.getpid(),\n\t            \"SIGNAC\": session.args.signac_job.id if session.args.signac_job else None,\n\t            **session.args.tqdm_msg\n\t        }\n\t        worker_queue.append(None)\n", "        if type(self.sync_every) is not timedelta:\n\t            self.next_sync_point = self.sync_every\n\t        else:\n\t            self.next_sync_point = None\n\t        self.update_sync_point = False\n\t        while len(worker_queue) > 1:\n\t            self.poll_workers(session)\n\t            time.sleep(0.1)\n\t        # Wait for current evaluation to complete\n\t        if self.eval_thread is not None:\n", "            self.eval_thread.join()\n\t        if session.model_at_best:\n\t            session.save_checkpoints(\"best\", model=self.eval_model)\n\t        # Evaluate on test set\n\t        if session.datamgr.test_edge is not None:\n\t            session.clean_memory()\n\t            session.run_evaluation(\"test\")\n\t            session.clean_memory()\n\t            session.run_evaluation(\"best\")\n\t        if session.args.signac_job:\n", "            session.args.signac_job.doc.worker_pgs = self.worker_dict\n\t        session.signac_mark_success()\n"]}
{"filename": "src/distributed/MultiGPU.py", "chunked_list": ["'''\n\tImplementation of MultiGPU training with synchronized SGD using DGL DDP functionality.\n\t'''\n\timport time\n\tfrom pathlib import Path\n\timport numpy as np\n\timport torch\n\timport torch.distributed as dist\n\tfrom ..pipelines.utils import parse_timedelta\n\tclass MultiGPUTrainer:\n", "    def __init__(self, args, proc_id:int, dev_id: int):\n\t        '''\n\t        Distributed manager on trainer side for MultiGPU training.\n\t        The trainer processes communicates with the server process through file system\n\t        (specifically, the signac job document of the server job),\n\t        since the server process only conducts evaluation and does not participate in the torch distributed \n\t        process group. As a result, signac must be used for MultiGPU training to work.\n\t        Arguments\n\t        ---------\n\t        args: argparse.Namespace\n", "            The parsed program arguments.\n\t        proc_id: int\n\t            Process ID of the trainer.\n\t        dev_id: int\n\t            The GPU device ID to use.\n\t        '''\n\t        dist_init_method = 'tcp://{master_ip}:{master_port}'.format(master_ip='127.0.0.1', master_port='12555')\n\t        torch.cuda.set_device(dev_id)\n\t        self.device = torch.device(\"cuda:{}\".format(dev_id))\n\t        self.gpu_list = args.gpu\n", "        self.proc_id = proc_id\n\t        self.pgs_dict = dict()\n\t        # Cannot be used together with distributed training\n\t        # Only the trainers are communicating through the process group\n\t        torch.distributed.init_process_group(\n\t            backend='nccl', init_method=dist_init_method, \n\t            world_size=self.trainer_size, rank=self.rank)\n\t        self.enable_eval = False\n\t        # Reopen signac job\n\t        spDict = args.signac_job.sp()\n", "        args.signac_job_server = args.project.open_job(spDict)\n\t        spDict[\"dist_params\"][\"rank\"] = proc_id\n\t        args.signac_job = args.project.open_job(spDict).init()\n\t        args.signac_job.reset()\n\t        args.result_path = Path(args.signac_job.ws)\n\t        self.signac_job_server = args.signac_job_server\n\t        self.signac_job = args.signac_job\n\t        while True:\n\t            if len(self.signac_job_server.doc.get(\"dist_signac_ids\", [])) == proc_id:\n\t                self.signac_job_server.doc.dist_signac_ids.append(self.signac_job.id)\n", "                break\n\t            else:\n\t                time.sleep(0.01)\n\t        self.signac_job.doc.dist_signac_ids = self.signac_job_server.doc.dist_signac_ids\n\t        # reset all random seeds\n\t        torch.manual_seed(args.random_seed)\n\t        torch.cuda.manual_seed_all(args.random_seed)\n\t        np.random.seed(args.random_seed)\n\t    @property\n\t    def used_gpu_list(self):\n", "        '''\n\t        Get the list of GPUs used by all trainer processes.\n\t        '''\n\t        return self.gpu_list[1:]\n\t    @property\n\t    def rank(self):\n\t        '''\n\t        Get the rank of the trainer process in the torch distributed process group.\n\t        '''\n\t        return self.proc_id - 1\n", "    @property\n\t    def is_eval(self):\n\t        '''\n\t        Get the evaluation signal from the server process.\n\t        '''\n\t        return self.signac_job_server.doc.get(\"is_eval\", False)\n\t    @is_eval.setter\n\t    def is_eval(self, value):\n\t        self.signac_job_server.doc.is_eval = value\n\t    @property\n", "    def train_started(self):\n\t        return self.signac_job_server.doc.get(\"train_started\", False)\n\t    @train_started.setter\n\t    def train_started(self, value):\n\t        '''\n\t        Inform the server process if training has started.\n\t        '''\n\t        self.signac_job_server.doc.train_started = value\n\t    def run_pre_train_hooks(self, session):\n\t        '''\n", "        Run hooks before each training step to check if the training should be stopped.\n\t        '''\n\t        if not self.train_started:\n\t            self.train_started = True\n\t        elif self.is_stopping:\n\t            print(f\"[Trainer {self.rank}] Stop signal detected.\")\n\t            session.is_stopping = True\n\t    def run_post_step_hooks(self, session):\n\t        '''\n\t        Run hooks after each training step to check if server is asking for model to be evaluated.\n", "        '''\n\t        if self.rank == 0:\n\t            if self.is_eval:\n\t                for name in [\"epoch\", \"step\", \"iter_count\", \"step_per_epoch\"]:\n\t                    value = getattr(session, name)\n\t                    self.pgs_dict[name] = int(value)\n\t                self.pgs_dict[\"epoch_count\"] = (self.pgs_dict[\"epoch\"] - 1\n\t                    + self.pgs_dict[\"step\"] / self.pgs_dict[\"step_per_epoch\"])\n\t                self.signac_job_server.doc.eval_pgs_dict = self.pgs_dict\n\t                with self.signac_job_server:\n", "                    session._save_checkpoints(\n\t                        None, session.model, None, checkpoint_path=\"eval.pth\")\n\t                session.write(f\"[Trainer] saved evaluation checkpoint.\")\n\t                self.is_eval = False\n\t    @property\n\t    def trainer_size(self):\n\t        '''\n\t        Get the size of the torch distributed process group.\n\t        '''\n\t        return len(self.gpu_list) - 1\n", "    @property\n\t    def world_size(self):\n\t        '''\n\t        Get the world size of the torch distributed process group.\n\t        '''\n\t        return len(self.gpu_list)\n\t    @property\n\t    def is_stopping(self):\n\t        '''\n\t        Get the stop signal from the server process.\n", "        '''\n\t        return self.signac_job_server.doc.get(\"is_stopping\", False)\n\t    @property\n\t    def ok_to_stop(self):\n\t        '''\n\t        Get the program exit signal from the server process.\n\t        '''\n\t        return self.signac_job_server.doc.get(\"ok_to_stop\", False)\n\t    def wait_cluster(self, session):\n\t        '''\n", "        Wait for all processes to complete and destroy the process group.\n\t        '''\n\t        while not self.ok_to_stop:\n\t            time.sleep(0.5)\n\t        print(f\"[Trainer {self.rank}] Destroying process group...\")\n\t        dist.destroy_process_group()\n\tclass MultiGPUServer:\n\t    def __init__(self, args, dev_id: int):\n\t        '''\n\t        Distributed manager on server side for MultiGPU training.\n", "        The trainer processes communicates with the server process through file system\n\t        (specifically, the signac job document of the server job),\n\t        since the server process only conducts evaluation and does not participate in the torch distributed \n\t        process group. As a result, signac must be used for MultiGPU training to work.\n\t        Arguments\n\t        ---------\n\t        args: argparse.Namespace\n\t            The parsed program arguments.\n\t        dev_id: int\n\t            The GPU device ID to use.\n", "        '''\n\t        torch.cuda.set_device(dev_id)\n\t        self.device = torch.device(\"cuda:{}\".format(dev_id))\n\t        self.signac_project = args.project\n\t        # Reopen signac job\n\t        spDict = args.signac_job.sp()\n\t        args.signac_job = args.project.open_job(spDict)\n\t        args.signac_job_server = args.signac_job\n\t        self.signac_job_server = args.signac_job_server\n\t        self.signac_job = args.signac_job\n", "        self.signac_job.doc.dist_signac_ids = [args.signac_job.id]\n\t        self.is_eval = False\n\t        self.sync_every = parse_timedelta(args.eval_every)\n\t        self.time_limit = parse_timedelta(args.time_limit)\n\t        self.enable_eval = True\n\t        # reset all random seeds\n\t        torch.manual_seed(args.random_seed)\n\t        torch.cuda.manual_seed_all(args.random_seed)\n\t        np.random.seed(args.random_seed)\n\t    @property\n", "    def train_started(self):\n\t        '''\n\t        Check if training has started by the trainer processes.\n\t        '''\n\t        return self.signac_job_server.doc.get(\"train_started\", False)\n\t    @property\n\t    def is_eval(self):\n\t        '''\n\t        Get the evaluation signal.\n\t        '''\n", "        return self.signac_job.doc.get(\"is_eval\", False)\n\t    @is_eval.setter\n\t    def is_eval(self, value):\n\t        '''\n\t        Set the evaluation signal from the server process.\n\t        '''\n\t        self.signac_job.doc.is_eval = value\n\t    @property\n\t    def is_stopping(self):\n\t        '''\n", "        Get the stop signal.\n\t        '''\n\t        return self.signac_job_server.doc.get(\"is_stopping\", False)\n\t    @is_stopping.setter\n\t    def is_stopping(self, value):\n\t        '''\n\t        Set the stop signal.\n\t        '''\n\t        self.signac_job.doc.is_stopping = value\n\t    @property\n", "    def ok_to_stop(self):\n\t        '''\n\t        Get the program exit signal from the server process.\n\t        '''\n\t        return self.signac_job_server.doc.get(\"ok_to_stop\", False)\n\t    @ok_to_stop.setter\n\t    def ok_to_stop(self, value):\n\t        '''\n\t        Set the program exit signal from the server process.\n\t        '''\n", "        self.signac_job.doc.ok_to_stop = value\n\t    @property\n\t    def used_gpu_list(self):\n\t        '''\n\t        Get the list of GPUs used by the server.\n\t        '''\n\t        return self.gpu_list[:1]\n\t    @property\n\t    def eval_pgs_dict(self):\n\t        return self.signac_job.doc.eval_pgs_dict\n", "    def time_training(self, session):\n\t        '''\n\t        Check if the time limit for training has been reached.\n\t        If so, set the stop signal.\n\t        '''\n\t        if not self.is_stopping and ((time.time() - session.t_start) > self.time_limit.total_seconds()):\n\t            self.is_stopping = True\n\t            self.is_eval = True\n\t    def trainer_signac_mark_success(self):\n\t        '''\n", "        Mark the signac job of the trainers as successful.\n\t        '''\n\t        for pid in self.signac_job.doc.dist_signac_ids[1:]:\n\t            tjob = self.signac_project.open_job(id=pid)\n\t            tjob.doc.success = True\n\t    def run_eval(self, session):\n\t        '''\n\t        Load the evaluation checkpoint stored by trainer and run evaluation.\n\t        '''\n\t        while self.is_eval:\n", "            time.sleep(0.1)\n\t        with self.signac_job:\n\t            checkpoint = torch.load(\"eval.pth\")\n\t            Path(\"eval.pth\").unlink()\n\t        self.t_eval = time.time()\n\t        filtered_state_dict = {\n\t            key.replace(\"module.\", \"\", 1): value \n\t            for key, value in checkpoint[\"model_state_dict\"].items()}\n\t        session.model.load_state_dict(filtered_state_dict)\n\t        session.save_checkpoints(\"latest\")\n", "        session.run_evaluation(\n\t            \"valid\", \n\t            alt_epoch=self.eval_pgs_dict[\"epoch_count\"],\n\t            callback=lambda: self.time_training(session)\n\t        )\n\t        if session.model_at_best:\n\t            session.save_checkpoints(\"best\")\n\t    def server_main(self, session):\n\t        '''\n\t        Main function of the server process.\n", "        Arugments:\n\t        ----------\n\t        session: SessionMgr\n\t            Session manager object\n\t        '''\n\t        session.valid_evaluator.create_data_loader(session.model)\n\t        self.is_stopping = False\n\t        while not self.train_started:\n\t            time.sleep(0.1)\n\t        session.reset_timer()\n", "        self.t_eval = time.time()\n\t        print(\"Server standby...\")\n\t        while not self.is_stopping:\n\t            self.time_training(session)\n\t            if (time.time() - self.t_eval) > self.sync_every.total_seconds():\n\t                self.is_eval = True\n\t                self.run_eval(session)\n\t            else:\n\t                time.sleep(0.1)\n\t        with self.signac_job:\n", "            if Path(\"eval.pth\").exists():\n\t                self.run_eval(session)\n\t        # Evaluate on test set\n\t        session.clean_memory()\n\t        session.run_evaluation(\"test\")\n\t        session.clean_memory()\n\t        session.run_evaluation(\"best\")\n\t        session.signac_mark_success()\n\t        self.trainer_signac_mark_success()\n\t        self.ok_to_stop = True\n"]}
{"filename": "src/distributed/__init__.py", "chunked_list": []}
{"filename": "src/samplers/ClusterGCN.py", "chunked_list": ["'''\n\tAdaption of the ClusterGCN sampler from DGL for link prediction.\n\t'''\n\timport torch\n\timport dgl\n\timport itertools\n\timport time\n\tfrom pathlib import Path\n\timport json\n\tfrom ..pipelines.utils import get_peak_mem\n", "import signac\n\timport numpy as np\n\tfrom dgl.dataloading.cluster_gcn import set_edge_lazy_features, set_node_lazy_features\n\tfrom dgl.dataloading.cluster_gcn import F\n\tclass ClusterGCNEdgeSampler(dgl.dataloading.ClusterGCNSampler):\n\t    '''\n\t    ClusterGCN sampler for link prediction.\n\t    '''\n\t    @classmethod\n\t    def create_sampler_from_mem(\n", "            cls, partition_offset, partition_node_ids,\n\t            prefetch_ndata=None, prefetch_edata=None, output_device=None\n\t        ) -> dgl.dataloading.ClusterGCNSampler:\n\t        '''\n\t        An alternative constructor for the *parent* class (ClusterGCNSampler) \n\t        that recreates a sampler using existing partition information, \n\t        instead of rerunning the partitioning algorithm from start.\n\t        For the usage of all arguments, see the constructor of `dgl.dataloading.ClusterGCNSampler`.\n\t        '''\n\t        super_self = dgl.dataloading.ClusterGCNSampler.__new__(\n", "            dgl.dataloading.ClusterGCNSampler)\n\t        super(dgl.dataloading.ClusterGCNSampler, super_self).__init__()\n\t        super_self.partition_offset = partition_offset\n\t        super_self.partition_node_ids = partition_node_ids\n\t        super_self.prefetch_ndata = prefetch_ndata or []\n\t        super_self.prefetch_edata = prefetch_edata or []\n\t        super_self.output_device = output_device\n\t        return super_self\n\t    @classmethod\n\t    def create_from_mem(\n", "            cls, partition_offset, partition_node_ids, negative_sampler=None,\n\t            prefetch_ndata=None, prefetch_edata=None, output_device=None\n\t        ):\n\t        '''\n\t        An alternative constructor that recreates an edge sampler using existing partition information, \n\t        instead of rerunning the partitioning algorithm from start.\n\t        Arguments\n\t        ---------\n\t        negative_sampler: `dgl.dataloading.negative_sampler._BaseNegativeSampler`\n\t            Negative sampler for link prediction.\n", "        For the usage of all other arguments, see the constructor of `dgl.dataloading.ClusterGCNSampler`.\n\t        '''\n\t        self = cls.create_sampler_from_mem(\n\t            partition_offset, partition_node_ids, \n\t            prefetch_ndata, prefetch_edata, output_device)\n\t        self.perf_dict = dict()\n\t        self.signac_job = None\n\t        self.cache_path = None\n\t        self.negative_sampler = negative_sampler\n\t        self.__class__ = cls\n", "        return self\n\t    def __init__(self, g, k, cache_path='ClusterGCN', negative_sampler=None,\n\t                 balance_ntypes=None, balance_edges=False, mode='k-way',\n\t                 prefetch_ndata=None, prefetch_edata=None, output_device=None):\n\t        '''\n\t        Arguments\n\t        ---------\n\t        negative_sampler: `dgl.dataloading.negative_sampler._BaseNegativeSampler`\n\t            Negative sampler for link prediction.\n\t        cache_path: `str`\n", "            Path to cache the graph partitions.\n\t        For the usage of all other arguments, see the constructor of `dgl.dataloading.ClusterGCNSampler`.\n\t        '''\n\t        metis_params_dict = dict(\n\t            k=k, balance_ntypes=balance_ntypes,\n\t            balance_edges=balance_edges,\n\t            mode=mode\n\t        )\n\t        if type(cache_path) is tuple and type(cache_path[0]) is signac.Project:\n\t            self.signac_job = self.get_signac_job(\n", "                cache_path[0], metis_params_dict, cache_path[1])\n\t            self.cache_path = Path(self.signac_job.ws)\n\t        else:\n\t            self.signac_job = None\n\t            self.cache_path = Path(cache_path)\n\t            self.cache_path.mkdir(parents=True, exist_ok=True)\n\t        print(f\"ClusterGCN: using cache path {self.cache_path}\")\n\t        self.cluster_file_path = self.cache_path / 'ClusterGCN.pkl'\n\t        self.perf_file_path = self.cache_path / 'ClusterGCN_perf.json'\n\t        if self.cluster_file_path.exists() and self.perf_file_path.exists():\n", "            self.loaded_from_cache = True\n\t        else:\n\t            self.loaded_from_cache = False\n\t        start_t = time.time()\n\t        super().__init__(g, cache_path=str(self.cluster_file_path),\n\t                         prefetch_ndata=prefetch_ndata,\n\t                         prefetch_edata=prefetch_edata,\n\t                         output_device=output_device, **metis_params_dict)\n\t        end_t = time.time()\n\t        self.perf_dict = dict()\n", "        if not self.loaded_from_cache:\n\t            self.perf_dict[\"Time\"] = end_t - start_t\n\t            self.perf_dict[\"RAM\"] = get_peak_mem()\n\t            with open(self.perf_file_path, \"w\") as f:\n\t                json.dump(self.perf_dict, f)\n\t            if self.signac_job:\n\t                self.signac_job.doc.success = True\n\t                self.signac_job.doc.metrics = self.perf_dict\n\t        else:\n\t            with open(self.perf_file_path, \"r\") as f:\n", "                self.perf_dict = json.load(f)\n\t        self.negative_sampler = negative_sampler\n\t    _build_neg_graph = dgl.dataloading.EdgePredictionSampler._build_neg_graph\n\t    def get_signac_job(self, project: signac.Project, params_dict: dict, \n\t                       dataset_str: str):\n\t        '''\n\t        Get the signac job for storing the graph partitioning results.\n\t        Arguments\n\t        ---------\n\t        project: `signac.Project`\n", "            The signac project.\n\t        params_dict: `dict`\n\t            The parameters for METIS.\n\t        dataset_str: `str`\n\t            The name of the dataset.\n\t        '''\n\t        sp_dict = dict(\n\t            type=\"preprocess\",\n\t            model=\"preprocess-ClusterGCN\",\n\t            model_params=params_dict,\n", "            dataset=dataset_str\n\t        )\n\t        return project.open_job(sp_dict).init()\n\t    def sample(self, g, partition_ids):\n\t        '''\n\t        Sample positive edges and negative edges for link prediction \n\t        given the partition IDs.\n\t        Arguments\n\t        ----------\n\t        g: `dgl.DGLGraph`\n", "            Graph to sample from.\n\t        partition_ids: `torch.Tensor`\n\t            IDs of the partitions for sampling.\n\t        Returns\n\t        -------\n\t        node_ids: `torch.Tensor`\n\t            Node IDs of the sampled subgraph.\n\t        subg: `dgl.DGLGraph`\n\t            The sampled positive subgraph.\n\t        neg_graph: `dgl.DGLGraph`\n", "            The sampled negative graph.\n\t        '''\n\t        subg = super().sample(g, partition_ids)  # type: dgl.DGLGraph\n\t        if self.negative_sampler is not None:\n\t            neg_graph = self._build_neg_graph(subg, subg.edges(form=\"eid\"))\n\t            # subg, neg_graph = dgl.transforms.compact_graphs(subg, neg_graph)\n\t            return subg.ndata[dgl.NID], subg, neg_graph\n\t        else:\n\t            return subg.ndata[dgl.NID], subg\n\t    @property\n", "    def preprocess_time(self):\n\t        '''\n\t        Get the time spent on graph partitioning.\n\t        '''\n\t        return self.perf_dict.get(\"Time\", 0.0)\n\t    @property\n\t    def preprocess_stats(self):\n\t        '''\n\t        Get the performance statistics of graph partitioning.\n\t        '''\n", "        return self.perf_dict.copy()\n\tclass ClusterGCNEdgeDataLoader:\n\t    def __init__(self, graph, num_parts, batch_size,\n\t                 cache_path='cluster_gcn.pkl',\n\t                 negative_sampler=None, \n\t                 train_frac_sample=(1.0, None), \n\t                 device=\"cpu\",\n\t                 _graph_sampler=None,\n\t                 sampler_kwargs=dict(),\n\t                 **loader_kwargs) -> None:\n", "        '''\n\t        Implementation for ClusterGCN sampler: \n\t        wrap ClusterGCNEdgeSampler to act as a dataloader for subgraphs in \n\t        link prediction training.\n\t        Arguments\n\t        ---------\n\t        graph: `dgl.DGLGraph`\n\t            The graph to sample from.\n\t        num_parts: int\n\t            number of total clusters to generate;\n", "        batch_size: int\n\t            number of clusters to include in each subgraph.\n\t        train_frac_sample: `tuple`\n\t            The fraction of training graph to sample, and the random seed.\n\t        device: `str`\n\t            The device to store the sampled graphs.\n\t        sampler_kwargs: `dict`\n\t            The keyword arguments for ClusterGCNEdgeSampler.\n\t        loader_kwargs:\n\t            Additional keyword arguments for DGL dataloader.\n", "        '''\n\t        self.graph = graph\n\t        self.num_parts = num_parts\n\t        self.batch_size = batch_size\n\t        self.negative_sampler = negative_sampler\n\t        if _graph_sampler is None:\n\t            self.graphsampler = ClusterGCNEdgeSampler(\n\t                self.graph, self.num_parts, cache_path,\n\t                negative_sampler=self.negative_sampler,\n\t                **sampler_kwargs\n", "            )\n\t        else:\n\t            self.graphsampler : ClusterGCNEdgeSampler = _graph_sampler\n\t            self.graphsampler.negative_sampler = negative_sampler\n\t            self.num_parts = len(self.graphsampler.partition_offset) - 1\n\t        # Downsample training set if needed\n\t        self.train_frac = train_frac_sample[0]\n\t        self.random_seed = train_frac_sample[1]\n\t        if self.train_frac < 1.0:\n\t            torch_rnd_state = torch.random.get_rng_state()\n", "            if self.random_seed is not None:\n\t                torch.random.manual_seed(self.random_seed)\n\t            num_clusters = np.round(self.train_frac * self.num_parts)\n\t            self.train_cluster_idx = torch.randperm(self.num_parts)[:num_clusters]\n\t            torch.random.set_rng_state(torch_rnd_state)\n\t            print(f\"Selected {len(self.train_cluster_idx)} out of {self.num_parts} clusters for training.\")\n\t        else:\n\t            self.train_cluster_idx = torch.arange(self.num_parts)\n\t        self.dataloader = dgl.dataloading.DataLoader(\n\t            self.graph, self.train_cluster_idx,\n", "            self.graphsampler, batch_size=self.batch_size,\n\t            device=device, **loader_kwargs\n\t        )\n\t    def __iter__(self):\n\t        if self.negative_sampler is not None:\n\t            for input_nodes, subg, neg_graph in self.dataloader:\n\t                # input_nodes, pos_graph, neg_graph, blocks\n\t                yield input_nodes, subg, neg_graph, itertools.repeat(subg)\n\t        else:\n\t            for input_nodes, subg in self.dataloader:\n", "                # input_nodes, pos_graph, neg_graph, blocks\n\t                yield input_nodes, subg, itertools.repeat(subg)\n\t    @property\n\t    def step_per_epoch(self):\n\t        '''\n\t        Get the number of training steps per epoch.\n\t        '''\n\t        return np.ceil(self.num_parts / self.batch_size)\n\t    @property\n\t    def preprocess_time(self):\n", "        '''\n\t        Get the time spent on graph partitioning.\n\t        '''\n\t        return self.graphsampler.preprocess_time\n\t    @property\n\t    def preprocess_stats(self):\n\t        '''\n\t        Get the performance statistics of graph partitioning.\n\t        '''\n\t        return self.graphsampler.preprocess_stats\n", "    @property\n\t    def cache_path(self):\n\t        '''\n\t        Get the cache path of the sampler.\n\t        '''\n\t        return self.graphsampler.cache_path\n\t    @property\n\t    def collate_fn(self):\n\t        return self.dataloader.collate_fn\n\t    @collate_fn.setter\n", "    def collate_fn(self, value):\n\t        self.dataloader.collate_fn = value\n\tclass ClusterGCNClusterSampler(ClusterGCNEdgeSampler):\n\t    def __init__(self, g, k, cache_path='ClusterGCN', negative_sampler=None, \n\t                 balance_ntypes=None, balance_edges=False, mode='k-way', \n\t                 prefetch_ndata=None, prefetch_edata=None, output_device=None, \n\t                 random_seed=None):\n\t        '''\n\t        Wrapper of ClusterGCNEdgeSampler to sample clusters instead of edges:\n\t        sample a group of mini-clusters to generate a subgraph.\n", "        Arguments\n\t        ---------\n\t        random_seed: `int`\n\t            Random seed for generating groups of clusters.\n\t        For all other arguments, see ClusterGCNEdgeSampler.\n\t        '''\n\t        super().__init__(g, k, cache_path, negative_sampler, balance_ntypes, \n\t                         balance_edges, mode, prefetch_ndata, prefetch_edata, output_device)\n\t        self.random_seed = random_seed\n\t    def sample(self, g, partition_ids):\n", "        '''\n\t        Generate a subgraph from a group of clusters.\n\t        Arguments\n\t        ---------\n\t        g: DGLGraph\n\t            The input graph.\n\t        partition_ids: torch.Tensor\n\t            The cluster IDs from which the subgraph is generated.\n\t        Returns\n\t        -------\n", "        sg: DGLGraph\n\t            The generated subgraph.\n\t        node_ids: torch.Tensor\n\t            The node IDs in the subgraph.\n\t        partition_offset: torch.Tensor\n\t            The offset of the start of each cluster in `node_ids`.\n\t        '''\n\t        node_ids = F.cat([\n\t            self.partition_node_ids[self.partition_offset[i]:self.partition_offset[i+1]]\n\t            for i in partition_ids], 0)\n", "        partition_size = np.array(\n\t            [self.partition_offset[i+1] - self.partition_offset[i]\n\t             for i in partition_ids]\n\t        )\n\t        partition_offset = np.insert(np.cumsum(partition_size), 0, 0)\n\t        sg = self.get_subgraph(g, node_ids)\n\t        return sg, node_ids, partition_offset\n\t    def get_subgraph(self, g, node_ids):\n\t        sg = g.subgraph(node_ids, relabel_nodes=True,\n\t                        output_device=self.output_device)\n", "        set_node_lazy_features(sg, self.prefetch_ndata)\n\t        set_edge_lazy_features(sg, self.prefetch_edata)\n\t        return sg\n\t    def group_clusters(self, num_groups, strategy=\"uniform-cluster\"):\n\t        '''\n\t        Group clusters into groups for generating subgraphs.\n\t        Arguments\n\t        ---------\n\t        num_groups: `int`\n\t            The number of groups to generate.\n", "        strategy: `str`\n\t            The grouping strategy. Currently only \"uniform-cluster\" is supported.\n\t        Returns\n\t        -------\n\t        group_list: `list`\n\t            A list of groups of clusters.\n\t        '''\n\t        rng = np.random.default_rng(seed=self.random_seed)\n\t        group_list = []\n\t        if strategy == \"uniform-cluster\":\n", "            cluster_per_group = float(self.num_clusters) / num_groups\n\t            cluster_list = np.arange(self.num_clusters)\n\t            rng.shuffle(cluster_list)\n\t            p = 0\n\t            for i in range(num_groups):\n\t                group_size = int(np.floor(cluster_per_group))\n\t                if rng.random() < cluster_per_group % 1:\n\t                    group_size += 1\n\t                if i != num_groups - 1:\n\t                    p_end = p + group_size\n", "                else:\n\t                    p_end = self.num_clusters\n\t                group_list.append(cluster_list[p:p_end])\n\t                p = p_end\n\t        else:\n\t            raise ValueError(f\"Unknown grouping strategy {strategy}\")\n\t        return group_list\n\t    @property\n\t    def num_clusters(self):\n\t        return len(self.partition_offset) - 1\n"]}
{"filename": "src/samplers/__init__.py", "chunked_list": []}
{"filename": "src/pipelines/Initialization.py", "chunked_list": ["'''\n\tInitialization of the pipeline:\n\t- Load parameters from params.yaml\n\t- Argument parsing\n\t- Signac initialization\n\t- Initialization of distributed training\n\t'''\n\timport argparse\n\timport datetime\n\timport json\n", "import os\n\timport pprint\n\timport shutil\n\timport sys\n\timport time\n\timport warnings\n\tfrom pathlib import Path\n\timport dgl\n\timport numpy as np\n\timport signac\n", "import torch\n\timport torch.distributed as dist\n\timport yaml\n\tfrom .utils import vscode_debug\n\tfrom .SessionMgr import EvalMetrics, ResultFiles\n\tdef parse_args():\n\t    '''\n\t    - Load arguments from params.yaml and parse arguments.\n\t    - Initialize of signac job and distributed training.\n\t    Returns:\n", "        args: Namespace object containing all arguments.\n\t    '''\n\t    parser = argparse.ArgumentParser(description='Variant Graph Auto Encoder')\n\t    # The arguments defined below are NOT comprehensive, \n\t    # The full list of arguments are defined in params.yaml\n\t    parser.add_argument(\"model\", type=str)\n\t    parser.add_argument('--dataset', type=str,\n\t                        default='ogbl-citation2', help='Dataset string.')\n\t    parser.add_argument('--lr', '--learning_rate', type=float,\n\t                        default=0.01, help='Initial learning rate.')\n", "    parser.add_argument('--grad_clip_norm', type=float, default=0.0)\n\t    parser.add_argument('--grad_clip_since', type=float, default=1.0)\n\t    parser.add_argument('--n_epochs', type=int, default=10,\n\t                        help='Number of epochs to train.')\n\t    parser.add_argument('--gpu', type=int, default=0, help='GPU id to use.')\n\t    parser.add_argument('--num_workers', type=int, default=2)\n\t    parser.add_argument('--use_self_loops', action=\"store_true\")\n\t    parser.add_argument('--no_training_neg', action=\"store_true\")\n\t    parser.add_argument('--batch_size', type=int,\n\t                        nargs=\"+\", default=[8192, 512])\n", "    parser.add_argument('--eval_every', type=float,\n\t                        nargs=\"+\", default=[0.5, 1])\n\t    parser.add_argument('--save_every', type=int, default=20)\n\t    parser.add_argument('--clear_cache_every', type=int, default=10)\n\t    parser.add_argument('--eval_metrics', type=str, nargs=\"+\", default=[\"MRR\"])\n\t    parser.add_argument('--result_path', type=str, default=\"results/{model}/\")\n\t    parser.add_argument('--random_seed', type=int, default=45342)\n\t    parser.add_argument('--use_gpu', type=bool, default=True)\n\t    parser.add_argument('--use_signac', type=bool, default=True)\n\t    parser.add_argument('--signac_override', action=\"store_true\")\n", "    parser.add_argument('--signac_gather_id', type=str, default=None)\n\t    parser.add_argument('--debug', action=\"store_true\")\n\t    args = parser.parse_args()\n\t    # Load parameters from params.yaml\n\t    with open(\"params.yaml\", \"r\") as fd:\n\t        params = yaml.safe_load(fd)\n\t    args.debug = params[\"debug\"]\n\t    args.signac_override = params[\"signac_override\"]\n\t    args.signac_gather_id = params.get(\"signac_gather_id\", None)\n\t    if args.debug:\n", "        vscode_debug()\n\t        breakpoint()\n\t        args.num_workers = 0\n\t    args.distributed = params[\"distributed\"][\"mode\"]\n\t    args.world_size = params[\"distributed\"][\"world_size\"]\n\t    args.dist_backend = params[\"distributed\"][\"backend\"]\n\t    if params[\"distributed\"][\"rank\"] is None:\n\t        args.dist_rank = int(os.environ[\"RANK\"])\n\t    else:\n\t        args.dist_rank = params[\"distributed\"][\"rank\"]\n", "    args.dist_gpu_list = params[\"distributed\"][\"gpu\"]\n\t    if args.distributed is not None:\n\t        dist_initialize_(args, params)\n\t    for key, val in params[\"train\"].items():\n\t        setattr(args, key, val)\n\t    for key, val in params[\"models\"][args.model].items():\n\t        setattr(args, key, val)\n\t    args.dataset_params = params[\"datasets\"][args.dataset]\n\t    for i in range(2):\n\t        if args.batch_size[i] is None:\n", "            args.batch_size[i] = args.dataset_params[\"batch_size\"][i]\n\t    common_args = set(params[\"models\"][args.model].keys()) & set(\n\t        params[\"train\"].keys())\n\t    if len(common_args) > 0:\n\t        warnings.warn(\n\t            f\"These training arguments are overwritten by {args.model} arguments: {common_args}\")\n\t    args.eval_clear_cache_every = params[\"models\"][args.model].get(\n\t        \"eval_clear_cache_every\", 0)\n\t    if args.model_params.get(\"n_layers\", None) is None:\n\t        args.model_params[\"n_layers\"] = len(args.model_params[\"hidden_dims\"])\n", "    args.tqdm_msg = params.get(\"tqdm_msg\", None)\n\t    if args.tqdm_msg is None:\n\t        args.tqdm_msg = dict()\n\t    if args.use_signac:\n\t        args.result_path_final = Path(\n\t            args.result_path.format(model=args.model))\n\t        args.result_path_final.mkdir(parents=True, exist_ok=True)\n\t        # Get project not sandboxed by DVC\n\t        args.rootpath = os.getcwd().split(\"/.dvc\", maxsplit=1)[0]\n\t        args.project = signac.get_project(args.rootpath)\n", "        spDict = dict(\n\t            model=args.model,\n\t            model_params=params[\"models\"][args.model],\n\t            train_params=params[\"train\"],\n\t            dataset_params=params[\"datasets\"][params[\"train\"][\"dataset\"]],\n\t            debug=args.debug,\n\t            pkg_version=dict(\n\t                torch=torch.__version__,\n\t                dgl=dgl.__version__\n\t            )\n", "        )\n\t        if args.distributed is not None: # Distributed training\n\t            spDict[\"dist_params\"] = {\n\t                \"mode\": args.distributed,\n\t                \"world_size\": params[\"distributed\"][\"world_size\"],\n\t                \"backend\": args.dist_backend,\n\t                \"rank\": args.dist_rank,\n\t                **args.distributed_params\n\t            }\n\t        elif type(args.gpu) == list: # Multi-GPU training\n", "            spDict[\"dist_params\"] = {\n\t                \"mode\": \"MultiGPU\",\n\t                \"world_size\": len(args.gpu),\n\t                \"rank\": 0\n\t            }\n\t        args.signac_job = args.project.open_job(spDict)\n\t        if args.signac_gather_id:\n\t            with open(args.signac_gather_id, \"a\") as f:\n\t                f.write(f\"{args.signac_job.id}\\n\")\n\t        # Check if a job with the same signac statepoint has succeeded before\n", "        # If so, skip or override the job based on the signac_override flag\n\t        dvc_pointer_file = (Path(args.project.workspace()) / f\".{args.signac_job.id}.dvc\")\n\t        has_success = None\n\t        if dvc_pointer_file.exists():\n\t            print(f\"Loading job information from {dvc_pointer_file}\")\n\t            with open(dvc_pointer_file, \"r\") as fd:\n\t                dvc_content = yaml.safe_load(fd)[\"outs\"]\n\t            if len(dvc_content) == 1:\n\t                for item in dvc_content:\n\t                    job_info = json.loads(item[\"desc\"])\n", "                    has_success = job_info[\"doc\"].get(\"success\", None)\n\t        if has_success is None:\n\t            args.signac_job.init()\n\t            has_success = args.signac_job.doc.get(\"success\", None)\n\t        if has_success:\n\t            if args.signac_override is True:\n\t                warnings.warn(\n\t                    f\"Job {args.signac_job.id} is already succeeded and will be overwritten in 10s!\")\n\t                time.sleep(10)\n\t                warnings.warn(\n", "                    f\"Reset job {args.signac_job.id}...\")\n\t                args.signac_job.reset()\n\t                if dvc_pointer_file.exists():\n\t                    dvc_pointer_file.unlink()\n\t            else:\n\t                print(\n\t                    f\"***{args.signac_job.id} has already succeeded. Using existing results.\")\n\t                file_list = list(ResultFiles)\n\t                for item in file_list:\n\t                    src_path = str(Path(args.signac_job.ws) / item.value)\n", "                    dst_path = str(args.result_path_final / item.value)\n\t                    shutil.copy2(src_path, dst_path)\n\t                sys.exit(0)\n\t        elif has_success == False:\n\t            if args.signac_override is None or args.signac_override == \"status_only\":\n\t                print(\n\t                    f\"***Job {args.signac_job.id} has failed!\")\n\t                sys.exit(1)\n\t            args.signac_job.reset()\n\t        elif args.signac_override == \"status_only\":\n", "            print(\n\t                f\"***Job {args.signac_job.id} does not exist!\")\n\t            sys.exit(1)\n\t        args.signac_job.init() \n\t        args.signac_job.doc.success = False\n\t        args.result_path = Path(args.signac_job.ws)\n\t        print(f\"Using signac workspace: {args.signac_job.id}\")\n\t    else:\n\t        args.result_path_final = None\n\t        args.result_path = Path(args.result_path.format(model=args.model))\n", "        args.result_path.mkdir(parents=True, exist_ok=True)\n\t    for i in range(len(args.eval_metrics)):\n\t        args.eval_metrics[i] = getattr(EvalMetrics, args.eval_metrics[i])\n\t    # Set all random seeds\n\t    torch.manual_seed(args.random_seed)\n\t    np.random.seed(args.random_seed)\n\t    print(\"Parameters:\")\n\t    pprint.pprint(vars(args))\n\t    return args\n\tdef dist_initialize_(args_, params_):\n", "    '''\n\t    Initialize distributed training and synchronize arguments\n\t    Args:\n\t        args_ (argparse.Namespace): \n\t            Namespace of arguments, will be modified in place.\n\t        params_ (dict): \n\t            Dictionary of parameters loaded from `params.yaml`,\n\t            will be modified in place.\n\t    '''\n\t    if args_.dist_rank >= args_.world_size:\n", "        raise ValueError(f\"Rank {args_.dist_rank} is too much for world size {args_.world_size}\")\n\t    print(f\"[Dist] Initializing distributed process group as rank {args_.dist_rank}...\")\n\t    dist.init_process_group(\n\t        args_.dist_backend, world_size=args_.world_size, rank=args_.dist_rank,\n\t        timeout=datetime.timedelta(minutes=45)\n\t    )\n\t    args_.dist_is_master = (dist.get_rank() == 0)\n\t    params_[\"train\"][\"gpu\"] = args_.dist_gpu_list\n\t    # Synchronize parameters\n\t    sync_list = [params_[\"train\"], params_[\"models\"], \n", "        params_[\"distributed\"][args_.distributed], params_[\"datasets\"]]\n\t    if args_.dist_is_master:\n\t        print(\"[Dist] Synchronizing parameters to clients...\")\n\t        dist.broadcast_object_list(sync_list)\n\t        args_.distributed_params = sync_list[2]\n\t        params_[\"train\"][\"gpu\"] = params_[\"train\"][\"gpu\"][dist.get_rank()]\n\t    else:\n\t        params_list = [None] * len(sync_list)\n\t        print(\"[Dist] Synchronizing parameters with server...\")\n\t        dist.broadcast_object_list(params_list)\n", "        for key, value in params_list[0].items():\n\t            params_[\"train\"][key] = value\n\t        params_[\"train\"][\"gpu\"] = params_[\"train\"][\"gpu\"][dist.get_rank()]\n\t        params_[\"models\"] = params_list[1]\n\t        args_.distributed_params = params_list[2]\n\t        params_[\"datasets\"] = sync_list[3]\n"]}
{"filename": "src/pipelines/SessionMgr.py", "chunked_list": ["'''\n\tCode for managing training and evaluation session.\n\t'''\n\timport gc\n\timport pprint\n\timport time\n\timport datetime\n\tfrom collections import deque\n\tfrom enum import Enum\n\tfrom multiprocessing import Process, Value\n", "from string import capwords\n\tfrom typing import Literal\n\timport dgl\n\timport numpy as np\n\timport pandas\n\timport torch\n\timport tqdm\n\timport yaml\n\tfrom .Evaluation import NodeBatchEvaluator, EvalMetrics, EvalMetricsCalc\n\tfrom ..dataloading.DataMgr import DataMgr\n", "from ..distributed.AvgCluster import AvgCluster\n\tfrom ..distributed.MultiGPU import MultiGPUTrainer\n\tfrom ..samplers import ClusterGCN\n\tfrom .utils import (CustomDistributedDataParallel, SharedMemCollateWrapper,\n\t                    get_gpu_usage, get_peak_mem, parse_timedelta, write_csv)\n\t# Correspond `nhood_sampler.name` in `params.yaml` to underlying sampler class\n\tnhoodSamplerDict = {\n\t    \"MultiLayerFullNeighborSampler\": dgl.dataloading.MultiLayerFullNeighborSampler,\n\t    \"MultiLayerNeighborSampler\": dgl.dataloading.MultiLayerNeighborSampler\n\t}\n", "# Correspond `nhood_sampler.name` in `params.yaml` to underlying dataloader class\n\tdataLoaderDict = {\n\t    \"ClusterGCNEdgeDataLoader\": ClusterGCN.ClusterGCNEdgeDataLoader\n\t}\n\t# Correspond `neg_sampler.name` in `params.yaml` to underlying negative sampler class\n\tnegSamplerDict = {\n\t    \"Uniform\": dgl.dataloading.negative_sampler.Uniform\n\t}\n\tclass SessionMgr:\n\t    '''\n", "    Manage training and evaluation session.\n\t    '''\n\t    def __init__(self, args, datamgr, device,\n\t                 stats_dict=dict(), distmgr=None) -> None:\n\t        '''\n\t        Parameters:\n\t        -----------\n\t        args: argparse.Namespace\n\t            Parsed arguments. \n\t        datamgr: DataMgr\n", "            Data manager.\n\t        device: torch.device\n\t            Device to use.\n\t        stats_dict: dict\n\t            Dictionary to store statistics.\n\t        distmgr: DistMgr\n\t            Distributed manager.\n\t        '''\n\t        self.args = args\n\t        self.stats_dict = stats_dict\n", "        self.datamgr = datamgr\n\t        self.distmgr = distmgr\n\t        self.device = device\n\t        self.dataloader = None\n\t        self.valid_evaluator = None\n\t        self.test_evaluator = None\n\t        self.epoch_progress : tqdm.tqdm = None\n\t        self.step_progress : tqdm.tqdm = None\n\t        self.postfix_dict_step = dict()\n\t        self.eval_every = None  # (step_list, iter_mod_list, iter_list)\n", "        self.iter_count = 0\n\t        self.epoch = None\n\t        self.step = None\n\t        self.t_start = time.time()\n\t        self.t_train = 0\n\t        self.t_step = time.time()\n\t        self.t_preprocess = 0\n\t        self.stats_dict = stats_dict\n\t        self.checkpoint_dict = dict()\n\t        self.model_at_best = None\n", "        self.start_gpu_daemon()\n\t        self._max_gpu_vram = 0.0\n\t        self.time_limit = parse_timedelta(args.time_limit)\n\t        print(f\"Training time is limited to {self.time_limit}\")\n\t        self.is_stopping = False\n\t    def __del__(self):\n\t        self.stop_gpu_daemon()\n\t    def start_gpu_daemon(self):\n\t        '''\n\t        Start GPU daemon to monitor GPU VRAM usage.\n", "        '''\n\t        self.__gpu_daemon_flag = Value(\"b\", 1)\n\t        self.__gpu_vram = Value(\"d\", 0.0)\n\t        if hasattr(self.distmgr, \"used_gpu_list\"):\n\t            glist = self.distmgr.used_gpu_list\n\t        else:\n\t            glist = self.args.gpu\n\t        if type(glist) is not list:\n\t            gpu_list = [glist]\n\t        else:\n", "            gpu_list = glist\n\t        self.gpu_daemon = Process(\n\t            target=get_gpu_usage, \n\t            args=(self.__gpu_daemon_flag, self.__gpu_vram, gpu_list),\n\t            daemon=True, name=\"GPU-monitor\")\n\t        self.gpu_daemon.start()\n\t    def stop_gpu_daemon(self):\n\t        '''\n\t        Stop GPU daemon.\n\t        '''\n", "        if self.gpu_daemon.is_alive():\n\t            self.__gpu_daemon_flag.value = 0\n\t            self.gpu_daemon.join()\n\t    def create_train_data_loader(self, use_ddp=False):\n\t        '''\n\t        Create train data loader.\n\t        Arguments:\n\t        ----------\n\t        use_ddp: bool, default False\n\t            Set to True if using MultiGPU training.\n", "        '''\n\t        args = self.args\n\t        train_sampler_args = args.nhood_sampler[0]\n\t        for s_args in (train_sampler_args, ):\n\t            if s_args[\"params\"].get(\"n_layers\", False) is None:\n\t                s_args[\"params\"][\"n_layers\"] = args.model_params[\"n_layers\"]\n\t        # Construct train data loader\n\t        negative_sampler = negSamplerDict[args.neg_sampler[\"name\"]](\n\t            **args.neg_sampler[\"params\"])\n\t        scope = locals()\n", "        train_sampler_args[\"params\"] = {\n\t            (key[5:] if key.startswith(\"eval_\") else key):\n\t            (eval(value, globals(), scope) if key.startswith(\"eval_\") else value)\n\t            for key, value in train_sampler_args[\"params\"].items()\n\t        }\n\t        if train_sampler_args[\"name\"] in nhoodSamplerDict:\n\t            train_nhood_sampler = nhoodSamplerDict[train_sampler_args[\"name\"]](\n\t                **train_sampler_args[\"params\"])\n\t            if isinstance(train_nhood_sampler, dgl.dataloading.BlockSampler):\n\t                train_nhood_sampler = dgl.dataloading.as_edge_prediction_sampler(\n", "                    train_nhood_sampler, negative_sampler=negative_sampler\n\t                )\n\t            # Downsample training set if needed\n\t            if self.datamgr.train_frac < 1.0 and type(self.distmgr) != MultiGPUTrainer:\n\t                print(f\"Sampling training edges using fraction {self.train_frac}...\")\n\t                self.datamgr.train_edge_idx = DataMgr.sample_idx(\n\t                    self.datamgr.train_edge_idx, self.datamgr.train_frac, \n\t                    self.datamgr.random_seed)\n\t            dataloader = dgl.dataloading.EdgeDataLoader(\n\t                self.datamgr.train_graph, self.datamgr.train_edge_idx,\n", "                train_nhood_sampler,\n\t                device=self.device, batch_size=args.batch_size[0],\n\t                shuffle=True, drop_last=False,\n\t                num_workers=args.num_workers,\n\t                use_uva=False, use_ddp=use_ddp,\n\t                ddp_seed=self.datamgr.random_seed\n\t            )\n\t        elif train_sampler_args[\"name\"] in dataLoaderDict:\n\t            dataLoaderCls = dataLoaderDict[train_sampler_args[\"name\"]]\n\t            dataloader_kwargs = dict(\n", "                negative_sampler=negative_sampler,\n\t                device=self.device, shuffle=True, drop_last=False,\n\t                num_workers=args.num_workers, use_uva=False,\n\t                use_ddp=use_ddp, **train_sampler_args[\"params\"]\n\t            )\n\t            if (dataLoaderCls == dataLoaderDict[\"ClusterGCNEdgeDataLoader\"]\n\t                and type(self.datamgr) is AvgCluster):\n\t                dataloader_kwargs[\"_graph_sampler\"] = self.datamgr.cluster_sampler\n\t                dataloader_kwargs[\"train_frac_sample\"] = (self.datamgr.train_frac, \n\t                    self.datamgr.random_seed)\n", "            dataloader = dataLoaderCls(\n\t                self.datamgr.train_graph,\n\t                **dataloader_kwargs\n\t            )\n\t        else:\n\t            raise ValueError(\n\t                f\"Unknown train sampler {train_sampler_args['name']}.\")\n\t        # Allow DDP dataloader to load graph from shared memory\n\t        if type(self.distmgr) == MultiGPUTrainer:\n\t            if self.datamgr.multigpu_train_graph_name is not None:\n", "                dataloader.collate_fn = SharedMemCollateWrapper(\n\t                    dataloader.collate_fn.sample_func,\n\t                    self.datamgr.multigpu_train_graph_name,\n\t                    self.datamgr.multigpu_shared_ndata,\n\t                    self.datamgr.multigpu_shared_edata\n\t                )\n\t        self.dataloader = dataloader\n\t        if hasattr(dataloader, \"step_per_epoch\"):\n\t            self.step_per_epoch = dataloader.step_per_epoch\n\t            self.unit_step_progress = True\n", "            if type(self.distmgr) == MultiGPUTrainer:\n\t                self.step_per_epoch = np.ceil(\n\t                    self.step_per_epoch / self.distmgr.trainer_size)\n\t        else:\n\t            if type(self.distmgr) == MultiGPUTrainer:\n\t                self.step_per_epoch = np.ceil(\n\t                    self.datamgr.train_edge_idx.shape[0] / args.batch_size[0] / self.distmgr.trainer_size)\n\t                self.unit_step_progress = True\n\t            else:\n\t                self.step_per_epoch = np.ceil(\n", "                    self.datamgr.train_edge_idx.shape[0] / args.batch_size[0])\n\t                self.unit_step_progress = False\n\t        self.t_preprocess += getattr(self.dataloader, \"preprocess_time\", 0.0)\n\t    def create_evaluator(self, target: Literal[\"valid\", \"test\"] = \"valid\"):\n\t        '''\n\t        Create evaluator for validation or test set.\n\t        '''\n\t        args = self.args\n\t        if len(args.nhood_sampler) > 1:\n\t            eval_sampler_args = args.nhood_sampler[1]\n", "        else:\n\t            eval_sampler_args = None\n\t        if eval_sampler_args is not None:\n\t            eval_nhood_sampler = nhoodSamplerDict[eval_sampler_args[\"name\"]](\n\t                **eval_sampler_args[\"params\"])\n\t        else:\n\t            eval_nhood_sampler = dgl.dataloading.MultiLayerFullNeighborSampler(1)\n\t        if target == \"valid\":\n\t            eval_edges = self.datamgr.valid_edge\n\t            save_embed = False\n", "            graph = self.datamgr.train_graph\n\t        elif target == \"test\":\n\t            eval_edges = self.datamgr.test_edge\n\t            save_embed = self.datamgr.save_embed\n\t            if self.datamgr.val_use_val_edges:\n\t                graph = self.datamgr.graph\n\t            else:\n\t                graph = self.datamgr.train_graph\n\t        evaluator = NodeBatchEvaluator(\n\t            self, graph, eval_edges,\n", "            batch_size=args.batch_size[1],\n\t            nhood_sampler=eval_nhood_sampler,\n\t            batch_device=self.device,\n\t            graph_device=None, shuffle=True, drop_last=False,\n\t            num_workers=0, save_embed=save_embed,\n\t            clear_cache_every=args.eval_clear_cache_every\n\t        )\n\t        if getattr(self.datamgr, \"use_feat_loader\", False):\n\t            evaluator.feature_loader = self.datamgr.feature_loader\n\t        if target == \"valid\":\n", "            self.valid_evaluator = evaluator\n\t        elif target == \"test\":\n\t            self.test_evaluator = evaluator\n\t        else:\n\t            raise ValueError(f\"Unknow target {target} for evaluator\")\n\t        return evaluator\n\t    def create_model(self, create_optimizer=True, inplace=True, use_ddp=False, \n\t                     ddp_device=None, ddp_proc_group=None):\n\t        '''\n\t        Create model and optimizer.\n", "        Arguments\n\t        ----------\n\t        create_optimizer : bool\n\t            Whether to create optimizer.\n\t        inplace : bool\n\t            Whether to modify the self.model and self.optimizer in place.\n\t        use_ddp : bool\n\t            Must be True if using MultiGPU training.\n\t        ddp_device : int\n\t            Device ID for DDP. Must be set if using MultiGPU training.\n", "        ddp_proc_group : torch.distributed.ProcessGroup\n\t            Process group for DDP. Must be set if using MultiGPU training.\n\t        '''\n\t        from ..models import modelDict\n\t        # create model\n\t        args = self.args\n\t        ModelClass = modelDict[args.model] # type: torch.nn.Module\n\t        model = ModelClass(\n\t            self.datamgr.in_dim, \n\t            device=self.device, \n", "            dtype=self.datamgr.feat_dtype,\n\t            n_relations=getattr(self.datamgr, \"n_relations\", 1),\n\t            n_nodetypes=getattr(self.datamgr, \"n_nodetypes\", 1),\n\t            **args.model_params) # type: BaseLinkEncoderDecoder\n\t        model = model.to(self.device)\n\t        print(\"Model created:\")\n\t        print(model)\n\t        if use_ddp:\n\t            model = CustomDistributedDataParallel(\n\t                model, device_ids=[ddp_device], output_device=ddp_device, \n", "                process_group=ddp_proc_group)\n\t        if inplace:\n\t            print('Total Parameters:', model.num_model_params)\n\t            self.model = model\n\t        if create_optimizer:\n\t            # create training component\n\t            optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n\t            if inplace:\n\t                self.optimizer = optimizer\n\t            return model, optimizer\n", "        else:\n\t            if inplace:\n\t                self.optimizer = None\n\t            return model\n\t    def create_stats_dict(self):\n\t        '''\n\t        Create dictionary for storing training and evaluation statistics.\n\t        '''\n\t        self.stats_dict = dict(\n\t            metrics=dict(\n", "                train_stats=dict(), valid_stats=dict(), \n\t                test_stats=dict(), \n\t                preprocess_stats=getattr(self.dataloader, \"preprocess_stats\", dict()),\n\t                best_stats={self.args.eval_metrics[0].value: None},\n\t                model_stats={\"Total Params\": self.model.num_model_params}),\n\t            train_stats=deque(),\n\t            valid_stats=deque()\n\t        )\n\t        return self.stats_dict\n\t    @property\n", "    def enable_eval(self):\n\t        '''\n\t        Check if evaluation should be enabled in this process.\n\t        '''\n\t        if hasattr(self.distmgr, \"enable_eval\"):\n\t            return self.distmgr.enable_eval\n\t        else:\n\t            return True\n\t    def is_eval_point(self, reset_eval_point=False):\n\t        '''\n", "        Check if evaluation should be performed at this time.\n\t        '''\n\t        if (self.distmgr is not None) and not self.enable_eval:\n\t            return False\n\t        if (self.eval_every is None) or reset_eval_point:\n\t            args = self.args\n\t            if type(args.eval_every) is str:\n\t                self.eval_every = parse_timedelta(args.eval_every)\n\t                self.t_eval = time.time()\n\t            else:\n", "                step_list = []\n\t                iter_mod_list = []\n\t                iter_list = []\n\t                for _, value in enumerate(args.eval_every):\n\t                    if type(value) is str:\n\t                        new_value_list = np.array(eval(value))\n\t                        new_value_list = np.ceil(\n\t                            self.step_per_epoch * new_value_list).astype(int)\n\t                        iter_list.extend(new_value_list)\n\t                    else:\n", "                        new_value = int(np.ceil(value * self.step_per_epoch))\n\t                        if value <= 1:\n\t                            step_list.append(new_value)\n\t                        else:\n\t                            iter_mod_list.append(new_value)\n\t                self.eval_every = [np.array(step_list), np.array(\n\t                    iter_mod_list), np.array(iter_list)]\n\t                print(\"Evaluation config:\")\n\t                pprint.pprint(self.eval_every)\n\t        if self.is_stopping:\n", "            return True\n\t        if type(self.eval_every) is datetime.timedelta:\n\t            if time.time() - self.t_eval > self.eval_every.total_seconds():\n\t                self.t_eval = time.time()\n\t                return True\n\t            else:\n\t                return False\n\t        if self.step is not None:\n\t            return (self.step in self.eval_every[0]\n\t                    or (np.mod(self.iter_count, self.eval_every[1]) == 0).any()\n", "                    or self.iter_count in self.eval_every[2])\n\t    def reset_timer(self):\n\t        '''\n\t        Reset timer for training.\n\t        '''\n\t        self.t_start = time.time()\n\t        self.t_train = 0\n\t        self.t_step = time.time()\n\t    def add_timer(self, delta:float):\n\t        '''\n", "        Add time to timer for training.\n\t        Arguments\n\t        ---------\n\t        delta : float\n\t            Time to add in seconds.\n\t        '''\n\t        self.t_start -= delta\n\t        self.t_train += delta\n\t    def run_pre_train_hooks(self, epoch, step, iter_count=None):\n\t        '''\n", "        Run pre-train hooks:\n\t        - Update epoch, step and iteration count.\n\t        - Run registered pre-train hooks in distributed manager.\n\t        - Check if training should be stopped.\n\t        Arguments\n\t        ---------\n\t        epoch : int\n\t            Current epoch count.\n\t        step : int\n\t            Current step count in the current epoch.\n", "        iter_count : int\n\t            Current iteration (step) count since the beginning of training.\n\t        '''\n\t        self.epoch = epoch\n\t        self.step = step\n\t        if iter_count is None:\n\t            self.iter_count += 1\n\t        else:\n\t            self.iter_count = iter_count\n\t        if self.distmgr:\n", "            self.distmgr.run_pre_train_hooks(self)\n\t        elif time.time() - self.t_start > self.time_limit.total_seconds():\n\t            self.is_stopping = True\n\t    def train_step(self, pos_graph, neg_graph, blocks):\n\t        '''\n\t        Run a training step on provided graph data.\n\t        Arguments\n\t        ---------\n\t        pos_graph : dgl.DGLGraph\n\t            Graph with edges as positive link prediction targets\n", "        neg_graph : dgl.DGLGraph\n\t            Graph with edges as negative link prediction targets\n\t        blocks : list of dgl.DGLGraph\n\t            List of graphs (or blocks) for message passing on each GNN layer.\n\t            The length of the list should be equal to the number of GNN layers.\n\t        '''\n\t        args = self.args\n\t        # Add edge types to negative graph for heterogeneous decoder\n\t        if self.model.hg_decoder and dgl.ETYPE not in neg_graph.edata:\n\t            if args.neg_sampler[\"name\"] in [\"PerSourceUniform\", \"Uniform\"]:\n", "                neg_graph.edata[dgl.ETYPE] = pos_graph.edata[dgl.ETYPE].repeat_interleave(\n\t                        args.neg_sampler[\"params\"][\"k\"]\n\t                    )\n\t            else:\n\t                etypes = pos_graph.edata[dgl.ETYPE]\n\t                rnd_ind = torch.randint(\n\t                    etypes.shape[0], (neg_graph.num_edges(),), \n\t                    device=neg_graph.device)\n\t                neg_graph.edata[dgl.ETYPE] = etypes[rnd_ind]\n\t        if args.no_training_neg:\n", "            remove_neg_edges_in_training_(self.train_graph, neg_graph, pos_graph)\n\t        if getattr(self.datamgr, \"use_feat_loader\", False):\n\t            x = self.datamgr.feature_loader(next(iter(blocks)))\n\t        else:\n\t            x = next(iter(blocks)).srcdata[\"feat\"]\n\t        neg_graph = dgl.remove_self_loop(neg_graph)\n\t        # Training - forward\n\t        self.model.train()\n\t        pos_logits, neg_logits = self.model.forward(blocks, x, pos_graph, neg_graph)\n\t        loss = self.model.loss_fn(pos_graph, neg_graph, pos_logits, neg_logits)\n", "        return pos_logits, neg_logits, loss\n\t    def get_save_embed_callback(self, target, model):\n\t        '''\n\t        Obtain a callback function for saving node embeddings.\n\t        The actual callback function to save embeddings should be defined in Data Manager.\n\t        Arguments:\n\t        ----------\n\t        target : str, \"test\", \"best\"\n\t            Name of the model parameter set to save embeddings from.\n\t            Used to name the saved embedding file.\n", "        model : nn.Module\n\t            Model to save embeddings from.\n\t        '''\n\t        def callback():\n\t            if self.test_evaluator.node_embedding is not None:\n\t                embedding = self.test_evaluator.node_embedding\n\t                graph = self.test_evaluator.graph\n\t                save_folder = self.args.result_path / \"embed\"\n\t                if self.datamgr.save_embed_callback is not None:\n\t                    self.datamgr.save_embed_callback(\n", "                        self, embedding, graph, model, target, save_folder)\n\t                    self.test_evaluator.node_embedding = None\n\t            return False\n\t        return callback\n\t    def run_evaluation(self, target: Literal[\"valid\", \"test\", \"best\"] = \"valid\", \n\t                       alt_epoch=None, model=None, callback=None):\n\t        '''\n\t        Run evaluation on validation or test set. \n\t        Arguments\n\t        ---------\n", "        target : str, \"valid\", \"test\", \"best\"\n\t            Set of data to evaluate on.\n\t            When \"best\", the model with the best validation MRR will be restored\n\t            and used to evaluate on test set.\n\t        alt_epoch : int\n\t            Epoch count to record in evaluation.\n\t            If None, the current epoch count recorded in the session will be used.\n\t        model : nn.Module\n\t            Model to evaluate.\n\t            If None, the model in the session will be used.\n", "        callback : function\n\t            Callback function to run during evaluation.\n\t        '''\n\t        args = self.args\n\t        if model is None:\n\t            model = self.model\n\t        self.stats_dict[\"metrics\"][\"preprocess_stats\"][\"Time\"] = self.t_preprocess\n\t        write_stats(args, self.stats_dict, train_stats=True, metrics=True)\n\t        if target == \"valid\":\n\t            self.write(\"[Evaluation] Running evaluation on validation set...\")\n", "            self.reset_peak_memory_stats()\n\t            result_dict = dict()\n\t            if self.epoch is not None and self.step is not None:\n\t                result_dict[EvalMetrics.EPOCH] = self.epoch\n\t                result_dict[EvalMetrics.STEP] = self.step\n\t                self.postfix_dict_step[\"Last Eval\"] = f\"{self.epoch}-{self.step}\"\n\t            result_dict[EvalMetrics.TRAINING_TIME] = self.t_train\n\t            result_dict[EvalMetrics.TOTAL_TIME] = time.time() - self.t_start\n\t            evaluator = self.valid_evaluator\n\t            eval_dict = evaluator(model, args.eval_metrics, callback=callback)\n", "            result_dict.update(eval_dict)\n\t            if args.use_gpu:\n\t                result_dict[EvalMetrics.GPU_VRAM] = self.max_gpu_vram\n\t            result_str = EvalMetricsCalc.format_str(result_dict)\n\t            result_str = \" | \".join(result_str).strip()\n\t            if alt_epoch:\n\t                result_dict[EvalMetrics.EPOCH] = alt_epoch\n\t                result_str = f\"{EvalMetrics.EPOCH.value} {alt_epoch:.2f} | {result_str}\"\n\t                self.postfix_dict_step[\"Last Eval\"] = alt_epoch\n\t            self.write(f\"[Validation results] {result_str}\")\n", "            if EvalMetrics.LOSS in self.metrics[\"train_stats\"]:\n\t                result_dict.update({\n\t                    EvalMetrics.LOSS: self.metrics[\"train_stats\"][EvalMetrics.LOSS.value]\n\t                })\n\t            result_dict[EvalMetrics.ITER] = self.iter_count\n\t            result_dict[EvalMetrics.RAM] = get_peak_mem()\n\t            self.postfix_dict_step.update(\n\t                EvalMetricsCalc.format_str(eval_dict, as_dict=True)\n\t            )\n\t            result_dict = {key.value: value for key, value in result_dict.items()}\n", "            self.stats_dict[\"valid_stats\"].append(\n\t                pandas.Series(result_dict))\n\t            self.stats_dict[\"metrics\"][\"valid_stats\"] = result_dict\n\t            # Update best stats\n\t            prev_best = self.stats_dict[\"metrics\"][\"best_stats\"][args.eval_metrics[0].value]\n\t            if prev_best is None or result_dict[args.eval_metrics[0].value] > prev_best:\n\t                self.stats_dict[\"metrics\"][\"best_stats\"] = result_dict\n\t                new_best = result_dict[args.eval_metrics[0].value]\n\t                self.postfix_dict_step[f\"Best {args.eval_metrics[0].value}\"] = new_best\n\t                self.model_at_best = True\n", "            else:\n\t                self.model_at_best = False\n\t            write_stats(args, self.stats_dict, valid_stats=True, metrics=True)\n\t        elif target == \"test\":\n\t            self.reset_peak_memory_stats()\n\t            self.write(\"[Test] Running evaluation on test set using latest model...\")\n\t            if self.test_evaluator is None:\n\t                self.create_evaluator(\"test\")\n\t            prev_stats = self.stats_dict[\"metrics\"][\"valid_stats\"].copy()\n\t            result_dict = self.test_evaluator(\n", "                model, args.eval_metrics, \n\t                callback=self.get_save_embed_callback(target, model)\n\t            )\n\t            if args.use_gpu:\n\t                result_dict[EvalMetrics.GPU_VRAM] = self.max_gpu_vram\n\t            result_dict[EvalMetrics.RAM] = get_peak_mem()\n\t            result_str = EvalMetricsCalc.format_str(result_dict)\n\t            result_str = \" | \".join(result_str).strip()\n\t            self.write(f\"[Test Results - Latest] {result_str}\")\n\t            result_dict = {key.value: value for key, value in result_dict.items()}\n", "            prev_stats.update(result_dict)\n\t            result_dict = prev_stats\n\t            self.stats_dict[\"metrics\"][\"test_stats\"][\"latest\"] = result_dict\n\t            pprint.pprint(result_dict)\n\t            write_stats(args, self.stats_dict, train_stats=False, valid_stats=False, metrics=True)\n\t        elif target == \"best\":\n\t            if self.model_at_best:\n\t                self.stats_dict[\"metrics\"][\"test_stats\"][\"best\"] = self.stats_dict[\"metrics\"][\"test_stats\"][\"latest\"]\n\t            else:\n\t                self.reset_peak_memory_stats()\n", "                self.write(\"[Test] Running evaluation on test set using the best model...\")\n\t                self.write(\"[Test] Loading checkpoint for the best model...\")\n\t                checkpoint = torch.load(self.checkpoint_dict[\"best\"])\n\t                model.load_state_dict(checkpoint[\"model_state_dict\"])\n\t                model.eval()\n\t                if self.test_evaluator is None:\n\t                    self.create_evaluator(\"test\")\n\t                prev_stats = self.stats_dict[\"metrics\"][\"best_stats\"].copy()\n\t                result_dict = self.test_evaluator(\n\t                    model, args.eval_metrics, \n", "                    callback=self.get_save_embed_callback(target, model)\n\t                )\n\t                if args.use_gpu:\n\t                    result_dict[EvalMetrics.GPU_VRAM] = self.max_gpu_vram\n\t                result_dict[EvalMetrics.RAM] = get_peak_mem()\n\t                result_str = EvalMetricsCalc.format_str(result_dict)\n\t                result_str = \" | \".join(result_str).strip()\n\t                print(f\"[Test Results - Best] {result_str}\")\n\t                result_dict = {key.value: value for key, value in result_dict.items()}\n\t                prev_stats.update(result_dict)\n", "                result_dict = prev_stats\n\t                self.stats_dict[\"metrics\"][\"test_stats\"][\"best\"] = result_dict\n\t                pprint.pprint(result_dict)\n\t            write_stats(args, self.stats_dict, train_stats=False, valid_stats=False, metrics=True)\n\t        self.reset_peak_memory_stats()\n\t        return self.model_at_best\n\t    def run_post_step_hooks(self):\n\t        '''\n\t        Run post step hooks registered by the distributed manager.\n\t        '''\n", "        if self.distmgr:\n\t            self.distmgr.run_post_step_hooks(self)\n\t    @staticmethod\n\t    def _save_checkpoints(args, model, optimizer, postfix=\"\", checkpoint_path=None, **kwargs):\n\t        '''\n\t        Utility function to save checkpoints to disk.\n\t        '''\n\t        if checkpoint_path is None:\n\t            if postfix != \"\":\n\t                postfix = \"-\" + postfix\n", "            checkpoint_file = \"checkpoint{}.pth\".format(postfix)\n\t            checkpoint_path = str(args.result_path / checkpoint_file)\n\t        filtered_kwargs = dict()\n\t        for key, value in kwargs.items():\n\t            if value is not None:\n\t                filtered_kwargs[key] = value\n\t        if optimizer is not None:\n\t            torch.save({\n\t                \"model_state_dict\": model.state_dict(),\n\t                \"optimizer_state_dict\": optimizer.state_dict(),\n", "                **filtered_kwargs\n\t            }, checkpoint_path)\n\t        else:\n\t            torch.save({\n\t                \"model_state_dict\": model.state_dict(),\n\t                **filtered_kwargs\n\t            }, checkpoint_path)\n\t        return checkpoint_path\n\t    def save_checkpoints(self, target: Literal[\"latest\", \"best\", \"local\"] = \"latest\", model=None):\n\t        '''\n", "        Save checkpoints to disk and store the its name and path in the checkpoint dictionary.\n\t        Arguments:\n\t        ----------\n\t        target: Literal[\"latest\", \"best\", \"local\"]\n\t            Name of the checkpoint to save.\n\t        model: torch.nn.Module\n\t            Model to save. If None, the model for current session will be saved.\n\t        '''\n\t        if model is None:\n\t            model = self.model\n", "        checkpoint_path = self._save_checkpoints(\n\t            self.args, model, self.optimizer, postfix=target, \n\t            epoch=self.epoch, step=self.step, iter=self.iter_count)\n\t        self.write(\n\t            f\"[Checkpoint] {capwords(target)} checkpoint saved to {checkpoint_path}\")\n\t        self.checkpoint_dict[target] = checkpoint_path\n\t        return checkpoint_path\n\t    def create_step_progress(self):\n\t        '''\n\t        Create a progress bar for the steps in the current epoch.\n", "        '''\n\t        if self.unit_step_progress:\n\t            self.step_progress = tqdm.tqdm(\n\t                total=self.step_per_epoch, \n\t                desc=\"Epoch Progress\", dynamic_ncols=True)\n\t        else:\n\t            self.step_progress = tqdm.tqdm(\n\t                total=len(self.datamgr.train_edge_idx), \n\t                desc=\"Epoch Progress\", dynamic_ncols=True)\n\t        return self.step_progress\n", "    def close_step_progress(self):\n\t        '''\n\t        Close the progress bar for the steps in the current epoch.\n\t        '''\n\t        self.step_progress.close()\n\t        self.step_progress = None\n\t    def clean_memory(self):\n\t        '''\n\t        Run garbage collection and empty GPU memory cache.\n\t        '''\n", "        gc.collect()\n\t        if self.args.use_gpu:\n\t            torch.cuda.empty_cache()\n\t    def write(self, string):\n\t        '''\n\t        Print a string to the console in a way that is compatible with the progress bars.\n\t        '''\n\t        if self.step_progress is not None:\n\t            self.step_progress.write(string)\n\t        elif self.epoch_progress is not None:\n", "            self.epoch_progress.write(string)\n\t        else:\n\t            print(string)\n\t    def signac_mark_success(self):\n\t        '''\n\t        Mark the current signac job as successful.\n\t        '''\n\t        if self.args.use_signac:\n\t            self.args.signac_job.doc.success = True\n\t    @property\n", "    def metrics(self):\n\t        return self.stats_dict[\"metrics\"]\n\t    @property\n\t    def signac_job(self):\n\t        return self.args.signac_job\n\t    @property\n\t    def gpu_vram(self):\n\t        '''\n\t        Get the latest recorded GPU VRAM usage.\n\t        '''\n", "        if self.gpu_daemon.is_alive():\n\t            return self.__gpu_vram.value\n\t        elif self.gpu_daemon is not None:\n\t            return float('nan')\n\t    @property\n\t    def max_gpu_vram(self):\n\t        '''\n\t        Update and get the maximum GPU VRAM usage ever recorded.\n\t        '''\n\t        current_gpu_varm = self.gpu_vram\n", "        if np.isnan(current_gpu_varm):\n\t            return current_gpu_varm\n\t        if self.gpu_vram > self._max_gpu_vram:\n\t            self._max_gpu_vram = current_gpu_varm\n\t        return self._max_gpu_vram\n\t    @max_gpu_vram.setter\n\t    def max_gpu_vram(self, value):\n\t        self._max_gpu_vram = value\n\t    def reset_peak_memory_stats(self):\n\t        '''\n", "        Reset the maximum GPU VRAM usage ever recorded.\n\t        '''\n\t        self.max_gpu_vram = 0.0\n\t        torch.cuda.reset_peak_memory_stats()\n\t    @property\n\t    def rank(self):\n\t        '''\n\t        Get the rank of the current process in distributed cluster from the distributed manager.\n\t        '''\n\t        if self.distmgr is None:\n", "            return None\n\t        else:\n\t            return self.distmgr.rank\n\tclass ResultFiles(Enum):\n\t    '''\n\t    Name of files that will be created to save the results.\n\t    '''\n\t    METRICS = \"metrics.yaml\"\n\t    TRAIN_STATS = \"train_stats.csv\"\n\t    VALID_STATS = \"valid_stats.csv\"\n", "def write_stats(args, stats_dict,\n\t                train_stats=False, valid_stats=False, metrics=False):\n\t    '''\n\t    Write the statistics to disk.\n\t    Arguments:\n\t    ----------\n\t    args: argparse.Namespace\n\t        Parsed arguments.\n\t    stats_dict: dict\n\t        Statistics dictionary of the session.\n", "    train_stats: bool\n\t        Whether to write the training statistics to disk.\n\t    valid_stats: bool\n\t        Whether to write the validation statistics to disk.\n\t    metrics: bool\n\t        Whether to write the metrics to disk.\n\t    '''\n\t    if metrics:\n\t        with open(args.result_path / ResultFiles.METRICS.value) as f:\n\t            yaml.dump(stats_dict[\"metrics\"], f)\n", "        if args.use_signac:\n\t            args.signac_job.doc.metrics = stats_dict[\"metrics\"]\n\t    if train_stats:\n\t        train_stats_df = pandas.DataFrame(\n\t            stats_dict[\"train_stats\"])\n\t        if EvalMetrics.ITER.value in train_stats_df.columns:\n\t            train_stats_df.set_index(EvalMetrics.ITER.value, inplace=True)\n\t        write_csv(train_stats_df, args.result_path /\n\t                  ResultFiles.TRAIN_STATS.value)\n\t    if valid_stats:\n", "        valid_stats_df = pandas.DataFrame(\n\t            stats_dict[\"valid_stats\"])\n\t        if EvalMetrics.ITER.value in valid_stats_df.columns:\n\t            valid_stats_df.set_index(EvalMetrics.ITER.value, inplace=True)\n\t        write_csv(valid_stats_df, args.result_path /\n\t                  ResultFiles.VALID_STATS.value)\n\tdef remove_neg_edges_in_training_(train_graph, neg_graph_, pos_graph=None):\n\t    '''\n\t    Remove negative edges that are also present in the training graph.\n\t    This is done in place to `neg_graph_`.\n", "    '''\n\t    nu = neg_graph_.edges()[0]\n\t    nv = neg_graph_.edges()[1]\n\t    try:\n\t        u = neg_graph_.ndata[dgl.NID][nu].to(\"cpu\")\n\t        v = neg_graph_.ndata[dgl.NID][nv].to(\"cpu\")\n\t    except KeyError:\n\t        u = pos_graph.ndata[dgl.NID][nu].to(\"cpu\")\n\t        v = pos_graph.ndata[dgl.NID][nv].to(\"cpu\")\n\t    neg_train_overlap_bool = train_graph.has_edges_between(u, v)\n", "    neg_train_overlap = neg_train_overlap_bool.sum().item()\n\t    if neg_train_overlap > 0:\n\t        uu = nu[neg_train_overlap_bool]\n\t        vv = nv[neg_train_overlap_bool]\n\t        eids = neg_graph_.edge_ids(uu, vv)\n\t        neg_graph_.remove_edges(eids)\n\t        del uu, vv, eids\n\t    del nu, nv, u, v, neg_train_overlap_bool, neg_train_overlap"]}
{"filename": "src/pipelines/Evaluation.py", "chunked_list": ["'''\n\tEvaluation of trained models.\n\t'''\n\timport gc\n\tfrom collections import defaultdict, deque\n\tfrom enum import Enum\n\tfrom functools import partial\n\tfrom typing import Tuple\n\timport dgl\n\timport numpy as np\n", "import torch\n\timport tqdm\n\tfrom sklearn.metrics import average_precision_score, roc_auc_score\n\tfrom ..models.BaseModules import BaseLinkEncoderDecoder\n\tfrom .utils import numpy2torch_dtype\n\tclass EvalMetrics(Enum):\n\t    '''\n\t    Enum for evaluation metrics and their string representations.\n\t    '''\n\t    ITER = \"Iter\"\n", "    EPOCH = \"Epoch\"\n\t    STEP = \"Step\"\n\t    LOSS = \"Loss\"\n\t    MRR = \"MRR\"\n\t    EID = \"EID\"\n\t    ACC = \"ACC\"\n\t    ROC_AUC = \"AUC\"\n\t    ROC_AP = \"AP\"\n\t    GRAD_NORM = \"Grad Norm\"\n\t    GPU_VRAM = \"GPU VRAM\"\n", "    RAM = \"RAM\"\n\t    TIME = \"Time\"\n\t    TRAINING_TIME = \"Total Training\"\n\t    TOTAL_TIME = \"Total Time\"\n\t    RANK = \"RANK\"\n\tclass EvalMetricsCalc:\n\t    '''\n\t    Calculation and presentation of evaluation metrics.\n\t    '''\n\t    # Format strings for printing each metric\n", "    format_dict = defaultdict(\n\t        lambda: \"{:.4f}\",\n\t        {\n\t            EvalMetrics.EPOCH: \"{:05d}\",\n\t            EvalMetrics.STEP: \"{:05d}\",\n\t            EvalMetrics.TIME: \"{:.4f} mins\",\n\t            EvalMetrics.TOTAL_TIME: \"{:>8.3f} mins\",\n\t            EvalMetrics.TRAINING_TIME: \"{:>8.3f} mins\",\n\t            EvalMetrics.GPU_VRAM: \"{:>6.0f} MB\",\n\t            EvalMetrics.RAM: \"{:>3.2f} GB\",\n", "            EvalMetrics.RANK: \"{:02d}\"\n\t        }\n\t    )\n\t    @staticmethod\n\t    def roc_auc_ap_score(pos_logits: torch.Tensor, neg_logits: torch.Tensor,\n\t                         metrics=[EvalMetrics.ROC_AUC, EvalMetrics.ROC_AP],\n\t                         prob_fn=torch.sigmoid) -> dict[EvalMetrics, float]:\n\t        '''\n\t        Calulate AUC and AP scores on ROC curve.\n\t        Arguments:\n", "        ----------\n\t        pos_logits: torch.Tensor\n\t            Logits of positive edge samples.\n\t        neg_logits: torch.Tensor\n\t            Logits of negative edge samples.\n\t        metrics: subset of [EvalMetrics.ROC_AUC, EvalMetrics.ROC_AP]\n\t            Metrics to calculate.\n\t        prob_fn: function\n\t            Function to convert logits to probabilities.\n\t        Returns:\n", "        --------\n\t        result_dict: dict[EvalMetrics, float]\n\t            Dictionary of calculated metrics.\n\t        '''\n\t        result_dict = {key: None for key in metrics}\n\t        with torch.no_grad():\n\t            pos_prob = prob_fn(pos_logits.squeeze(-1)).cpu().numpy()\n\t            neg_prob = prob_fn(neg_logits.squeeze(-1)).cpu().numpy()\n\t        preds_all = np.hstack([pos_prob, neg_prob])\n\t        labels_all = np.hstack(\n", "            [np.ones(len(pos_prob)), np.zeros(len(neg_prob))])\n\t        if EvalMetrics.ROC_AUC in result_dict:\n\t            result_dict[EvalMetrics.ROC_AUC] = float(\n\t                roc_auc_score(labels_all, preds_all))\n\t        if EvalMetrics.ROC_AP in result_dict:\n\t            result_dict[EvalMetrics.ROC_AP] = float(\n\t                average_precision_score(labels_all, preds_all))\n\t        return result_dict\n\t    @staticmethod\n\t    def rr_score(score_pos: torch.Tensor, score_neg_mat: torch.Tensor) -> torch.Tensor:\n", "        '''\n\t        Calculate reciprocal rank. \n\t        Arguments:\n\t        ----------\n\t        score_pos: torch.Tensor\n\t            Scores of positive edge samples.\n\t        score_neg_mat: torch.Tensor\n\t            Scores of negative edge samples.\n\t        Returns:\n\t        --------\n", "        reciprocal_rank: torch.Tensor\n\t        '''\n\t        score_neg_mat = score_neg_mat.sort(-1)[0]  # Sort in ascending order\n\t        rank = score_neg_mat.shape[1] + 1 - \\\n\t            torch.searchsorted(score_neg_mat, score_pos)\n\t        reciprocal_rank = rank.squeeze(-1).reciprocal()\n\t        return reciprocal_rank\n\t    @classmethod\n\t    def format_str(cls, resultDict: dict, formatDict=None, as_dict=False, rank=None):\n\t        '''\n", "        Format evaluation metrics into a string or dictionary.\n\t        Arguments:\n\t        ----------\n\t        resultDict: dict[EvalMetrics, float]\n\t            Dictionary of evaluation metrics.\n\t        formatDict: dict[EvalMetrics, str]\n\t            Dictionary of format strings for each metric.\n\t        as_dict: bool\n\t            If True, return a dictionary of metrics and their string representations.\n\t            If False, return a list of string representations of metrics.\n", "        rank: int\n\t            Rank in distributed cluster of the current process.\n\t        '''\n\t        if formatDict is None:\n\t            formatDict = cls.format_dict\n\t        _r_dict = resultDict.copy()\n\t        if rank is not None:\n\t            _r_dict[EvalMetrics.RANK] = rank\n\t        for key in (EvalMetrics.TIME, EvalMetrics.TRAINING_TIME, EvalMetrics.TOTAL_TIME):\n\t            if key in _r_dict:\n", "                _r_dict[key] /= 60 # Convert to minutes\n\t        if not as_dict:\n\t            return [f\"{key.value} {formatDict[key].format(value)}\"\n\t                    for key, value in _r_dict.items()]\n\t        else:\n\t            return {key.value: formatDict[key].format(value).strip()\n\t                    for key, value in _r_dict.items()}\n\tclass NodeBatchEvaluator:\n\t    def __init__(self, session, graph, eval_edge: dict,\n\t                 batch_size: int,\n", "                 nhood_sampler=dgl.dataloading.MultiLayerFullNeighborSampler(1),\n\t                 batch_device=None, graph_device=None, clear_cache_every=0, \n\t                 feature_loader=None, save_embed=False, **kwargs):\n\t        '''\n\t        Initialize evaluator.\n\t        Arguments:\n\t        ----------\n\t        session: SessionMgr\n\t            Session manager (see SessionMgr.py)\n\t        graph: dgl.DGLGraph\n", "            Graph to evaluate on.\n\t        eval_edge: dict[str, torch.Tensor]\n\t            Dictionary of evaluation edges.\n\t            See `dataloading.BaseDataMgr` for detailed format.\n\t        batch_size: int\n\t            Batch size during evaluation.\n\t        nhood_sampler: default dgl.dataloading.MultiLayerFullNeighborSampler(1)\n\t            Neighbor sampler to use during evaluation.\n\t            Default is to sample all neighbors.\n\t        batch_device: torch.device\n", "            Device to use for mini-batch evaluation.\n\t        graph_device: torch.device\n\t            Device that the graph is stored on.\n\t        clear_cache_every: int\n\t            Clear CUDA cache every this many batches.\n\t        feature_loader: function\n\t            Function to load node features.\n\t        save_embed: bool\n\t            If True, store all node embeddings generated by encoder to `self.node_embedding`.\n\t        kwargs: dict\n", "            Additional arguments to be passed to `dgl.dataloading.NodeDataLoader`\n\t        '''\n\t        self.session = session\n\t        self.graph = graph\n\t        self.save_embed = save_embed\n\t        self.eval_edge = eval_edge\n\t        if self.save_embed: # Include all nodes\n\t            self.eval_nodes = self.graph.nodes()\n\t        else:\n\t            self.eval_nodes = self.get_eval_nodes(self.eval_edge)\n", "        self.nhood_sampler = nhood_sampler\n\t        self.num_nodes = self.graph.number_of_nodes()\n\t        if graph_device is None:\n\t            graph_device = self.graph.device\n\t        self.graph_device = graph_device\n\t        self.batch_device = batch_device\n\t        self.batch_size = batch_size\n\t        self.clear_cache_every = clear_cache_every\n\t        self.eval_nodes_mfg = None\n\t        self.dataloader_list = None\n", "        self.dataloader_func = partial(\n\t            dgl.dataloading.NodeDataLoader, \n\t            graph=self.graph,\n\t            batch_size=batch_size,\n\t            graph_sampler=self.nhood_sampler, device=batch_device,\n\t            **kwargs\n\t        )\n\t        self.feature_loader = feature_loader\n\t        # self.ogb_evaluator = Evaluator(name='ogbl-citation2')\n\t        self.node_embedding = None\n", "    @property\n\t    def num_eval_edges(self):\n\t        '''\n\t        Get number of evaluation edges.\n\t        '''\n\t        return self.eval_edge[\"source_node\"].shape[0]\n\t    @property\n\t    def num_neg_samples(self):\n\t        '''\n\t        Get number of negative samples per evaluation edge.\n", "        '''\n\t        if self.eval_edge[\"target_node_neg\"] is not None:\n\t            return self.eval_edge[\"target_node_neg\"].shape[1]\n\t        else:\n\t            return self.eval_edge[\"target_node_neg_gen\"][2]\n\t    @staticmethod\n\t    def get_eval_nodes(eval_edge):\n\t        '''\n\t        Get a tensor of all node IDs that are involved in evaluation edges.\n\t        '''\n", "        if eval_edge[\"target_node_neg\"] is not None:\n\t            node_list = torch.concat([\n\t                eval_edge[\"source_node\"].view(-1), \n\t                eval_edge[\"target_node\"].view(-1), \n\t                eval_edge[\"target_node_neg\"].view(-1)]\n\t            )\n\t        else:\n\t            node_list = torch.concat([\n\t                eval_edge[\"source_node\"].view(-1), \n\t                eval_edge[\"target_node\"].view(-1), \n", "                torch.arange(*eval_edge[\"target_node_neg_gen\"][:2], \n\t                    dtype=numpy2torch_dtype(eval_edge[\"target_node_neg_gen\"][-2])\n\t                )]\n\t            )\n\t        return torch.unique(node_list)\n\t    @staticmethod\n\t    def idx_iter(iter_len: int, batch_size: int) -> Tuple[int, int]:\n\t        '''\n\t        Iterate over indices in batches.\n\t        Arguments:\n", "        ----------\n\t        iter_len: int\n\t            Length of the indices [0, iter_len - 1] to iterate over.\n\t        batch_size: int\n\t            Batch size.\n\t        '''\n\t        for ndx in range(0, iter_len, batch_size):\n\t            yield ndx, min(ndx + batch_size, iter_len)\n\t    def create_data_loader(self, model):\n\t        '''\n", "        Create data loader for evaluation.\n\t        Arguments:\n\t        ----------\n\t        model: BaseLinkEncoderDecoder\n\t            Model to evaluate.\n\t        '''\n\t        if self.eval_nodes_mfg is None:\n\t            self.eval_nodes_mfg = model.get_MFG_nodes(\n\t                self.graph, self.eval_nodes)\n\t        self.dataloader_list = [None] * len(self.eval_nodes_mfg)\n", "        for i in range(len(self.eval_nodes_mfg)):\n\t            self.dataloader_list[i] = self.dataloader_func(\n\t                indices=self.eval_nodes_mfg[i]\n\t            )\n\t    def __call__(self, model: BaseLinkEncoderDecoder, metrics=[EvalMetrics.MRR], callback=None) -> dict:\n\t        '''\n\t        Evaluate model.\n\t        Arguments:\n\t        ----------\n\t        model: BaseLinkEncoderDecoder\n", "            Model to evaluate.\n\t        metrics: list of EvalMetrics\n\t            List of metrics to evaluate.\n\t        callback: function\n\t            Callback function to call after running each batch of encoder and decoder.\n\t        '''\n\t        model.eval()\n\t        result_dict = {key: None for key in metrics}\n\t        with torch.no_grad():\n\t            ## Run encoder part of the model\n", "            postfix_dict = {\n\t                \"Layer\": None\n\t            }\n\t            b_neg_score_mat = None\n\t            if self.dataloader_list is None:\n\t                self.create_data_loader(model)\n\t            progress = tqdm.tqdm(\n\t                total=sum(map(len, self.eval_nodes_mfg)),\n\t                desc=\"Eval Encoder Progress\",\n\t                dynamic_ncols=True\n", "            )\n\t            encoder_queue = deque()\n\t            h_mat = None\n\t            h_mapping : torch.Tensor = None\n\t            for block_counter in range(max(model.num_conv_layers, 1)):\n\t                postfix_dict[\"Layer\"] = f\"{block_counter + 1}/{model.num_conv_layers}\"\n\t                layer_num_nodes = len(self.eval_nodes_mfg[block_counter])\n\t                postfix_dict[\"Num Nodes\"] = layer_num_nodes\n\t                prev_h_mat = h_mat\n\t                prev_h_mapping = h_mapping\n", "                h_mat = None\n\t                h_mat_ptr = 0\n\t                h_mapping = None\n\t                for step, (input_nodes, output_nodes, blocks) in enumerate(\n\t                    self.dataloader_list[block_counter]):\n\t                    if block_counter == 0:\n\t                        # First graph conv layer: create generator\n\t                        if self.feature_loader is not None:\n\t                            x = self.feature_loader(blocks[0])\n\t                        else:\n", "                            x = blocks[0].srcdata[\"feat\"]\n\t                        h_gen = model.encoder_generator(blocks[0], x)\n\t                        b_ptr, h = next(h_gen)  # Run conv layer\n\t                    else:\n\t                        # Use previously created generator for follow up layers\n\t                        idx = prev_h_mapping[input_nodes] - 1\n\t                        x = prev_h_mat[idx].to(blocks[0].device, non_blocking=True)\n\t                        if (idx < 0).any():\n\t                            raise KeyError(\n\t                                f\"Missing embeddings of {(idx < 0).sum()} nodes \" \n", "                                f\"from Layer {block_counter - 1}. \"\n\t                                f\"Please check `get_MFG_nodes` function of your model!\")\n\t                        gen_ptr, h_gen = encoder_queue.popleft()\n\t                        assert gen_ptr == block_counter\n\t                        b_ptr, h = h_gen.send((blocks[0], x))  # Run conv layer\n\t                    del x, blocks, input_nodes\n\t                    h = h.to(self.graph_device) # non_blocking=True here will cause performance issues.\n\t                    if b_ptr >= 0:  # more conv layer expected\n\t                        if model.num_conv_layers > 0:\n\t                            assert b_ptr - 1 == block_counter\n", "                        encoder_queue.append((b_ptr, h_gen))\n\t                    else:  # last conv layer reached\n\t                        assert abs(b_ptr) - 1 == block_counter\n\t                        h_gen.close()\n\t                    if h_mat is None:\n\t                        h_mat = torch.zeros(\n\t                            (layer_num_nodes, h.shape[1]), \n\t                            device=self.graph_device,\n\t                            dtype=h.dtype,\n\t                            pin_memory=True\n", "                        )\n\t                    if h_mapping is None:\n\t                        h_mapping = torch.zeros(\n\t                            self.num_nodes, dtype=output_nodes.dtype\n\t                        )\n\t                        # staring idx mapping from 1 so that 0 is reserved for nodes not calculated\n\t                    h_mat[h_mat_ptr: h_mat_ptr + h.shape[0]] = h\n\t                    h_mapping[output_nodes] = torch.arange(\n\t                        h_mat_ptr + 1, h_mat_ptr + h.shape[0] + 1, \n\t                        dtype=output_nodes.dtype)\n", "                    h_mat_ptr += h.shape[0]\n\t                    postfix_dict.update(\n\t                        EvalMetricsCalc.format_str({\n\t                            EvalMetrics.GPU_VRAM: self.session.max_gpu_vram\n\t                        }, as_dict=True)\n\t                    )\n\t                    progress.set_postfix(postfix_dict)\n\t                    progress.update(len(output_nodes))\n\t                    del output_nodes, h\n\t                    gc.collect()\n", "                    if (self.batch_device.type == \"cuda\"\n\t                        and self.clear_cache_every > 0\n\t                            and step % self.clear_cache_every == 0):\n\t                        torch.cuda.empty_cache()\n\t                    if callback is not None:\n\t                        callback()\n\t                # Clean unused generators for current layer\n\t                stop_clean = False\n\t                while not stop_clean and len(encoder_queue) > 0:\n\t                    if encoder_queue[0][0] == block_counter:\n", "                        _, h_gen = encoder_queue.popleft()\n\t                        h_gen.close()\n\t                    else:\n\t                        stop_clean = True\n\t            del prev_h_mat, prev_h_mapping\n\t            progress.close()\n\t            gc.collect()\n\t            if self.save_embed:\n\t                uidx = h_mapping[self.graph.nodes()] - 1\n\t                assert not (uidx < 0).any()\n", "                self.node_embedding = h_mat[uidx]\n\t            ## Run decoder part of the model\n\t            progress = tqdm.tqdm(\n\t                total=self.num_eval_edges,\n\t                desc=\"Eval Decoder Progress\",\n\t                dynamic_ncols=True\n\t            )\n\t            postfix_dict = dict()\n\t            batch_size = self.batch_size // self.num_neg_samples\n\t            if EvalMetrics.MRR in metrics:\n", "                reciprocal_rank = - torch.ones(self.num_eval_edges,\n\t                                               device=self.batch_device)\n\t            if self.eval_edge[\"target_node_neg\"] is None:\n\t                rng = np.random.default_rng(seed=self.eval_edge[\"target_node_neg_gen\"][-1])\n\t            if \"etypes\" in self.eval_edge:\n\t                etypes = self.eval_edge[\"etypes\"]\n\t                if len(etypes) == 1:\n\t                    etypes = etypes.expand(self.num_eval_edges)\n\t            else:\n\t                etypes = None\n", "            for step, (p, q) in enumerate(self.idx_iter(self.num_eval_edges, batch_size)):\n\t                uidx = h_mapping[self.eval_edge[\"source_node\"][p:q]] - 1\n\t                uh = h_mat[uidx].to(self.batch_device, non_blocking=True)\n\t                vidx = h_mapping[self.eval_edge[\"target_node\"][p:q]] - 1\n\t                vh = h_mat[vidx].to(self.batch_device, non_blocking=True)\n\t                if self.eval_edge[\"target_node_neg\"] is not None:\n\t                    vidx_neg = h_mapping[self.eval_edge[\"target_node_neg\"][p:q]] - 1\n\t                else: # Generate negative samples on the fly\n\t                    target_node_neg = rng.integers(\n\t                        *self.eval_edge[\"target_node_neg_gen\"][:2], \n", "                        size=(q-p, self.eval_edge[\"target_node_neg_gen\"][2]),\n\t                        dtype=self.eval_edge[\"target_node_neg_gen\"][3])\n\t                    vidx_neg = h_mapping[torch.as_tensor(target_node_neg)] - 1\n\t                vh_neg = h_mat[vidx_neg].to(self.batch_device, non_blocking=True)\n\t                assert not ((uidx < 0).any() or (vidx < 0).any() or (vidx_neg < 0).any())                \n\t                if etypes is not None:\n\t                    b_etypes = etypes[p:q].to(self.batch_device)\n\t                else:\n\t                    b_etypes = None\n\t                b_pos_score_mat = model.decoder_mat(uh, vh, b_etypes)\n", "                b_neg_score_mat = model.decoder_mat(uh[:, None, :], vh_neg, b_etypes)\n\t                if EvalMetrics.MRR in metrics:\n\t                    b_reciprocal_rank = EvalMetricsCalc.rr_score(\n\t                        b_pos_score_mat[:, None], b_neg_score_mat)\n\t                    reciprocal_rank[p:q] = b_reciprocal_rank\n\t                postfix_dict.update(\n\t                    EvalMetricsCalc.format_str({\n\t                        EvalMetrics.GPU_VRAM: self.session.max_gpu_vram\n\t                    }, as_dict=True)\n\t                )\n", "                progress.update(q - p)\n\t                progress.set_postfix(postfix_dict)\n\t                gc.collect()\n\t                if (self.batch_device.type == \"cuda\"\n\t                    and self.clear_cache_every > 0\n\t                        and step % self.clear_cache_every == 0):\n\t                    torch.cuda.empty_cache()\n\t                if step % 10 == 0 and callback is not None:\n\t                    if callback() == False:\n\t                        callback = None\n", "            progress.close()\n\t            # Calculate final metrics\n\t            if EvalMetrics.MRR in metrics:\n\t                assert not (reciprocal_rank < 0).any()\n\t                result_dict[EvalMetrics.MRR] = reciprocal_rank.mean().item()\n\t        return result_dict\n"]}
{"filename": "src/pipelines/__init__.py", "chunked_list": []}
{"filename": "src/pipelines/utils.py", "chunked_list": ["import contextlib\n\timport os\n\timport random\n\timport re\n\timport subprocess\n\timport time\n\tfrom datetime import timedelta\n\tfrom io import StringIO\n\tfrom multiprocessing import Process, Value\n\tfrom pathlib import Path\n", "from typing import Callable\n\timport dgl\n\timport numpy as np\n\timport pandas\n\timport torch\n\timport torch.distributed as dist\n\tfrom dgl.dataloading import CollateWrapper\n\tfrom torch.distributed.distributed_c10d import (_object_to_tensor,\n\t                                                _tensor_to_object)\n\tdef vscode_debug():\n", "    try:\n\t        import ptvsd\n\t        print(\"Waiting for debugger attach\")\n\t        ptvsd.enable_attach(address=('0.0.0.0', 5678), redirect_output=True)\n\t        ptvsd.wait_for_attach()\n\t    except KeyboardInterrupt:\n\t        pass\n\tdef write_csv(df: pandas.DataFrame, filename: str):\n\t    with open(filename) as f:\n\t        df.to_csv(f)\n", "def get_peak_mem():\n\t    ''' Get the peak memory size.\n\t        Adapted from https://github.com/dmlc/dgl/blob/480a4ae35c84c4497bfa901c25e6e6eca85b67eb/python/dgl/partition.py#L235\n\t    Returns\n\t    -------\n\t    float\n\t        The peak memory size in GB.\n\t    '''\n\t    if not os.path.exists('/proc/self/status'):\n\t        return 0.0\n", "    for line in open('/proc/self/status', 'r'):\n\t        if 'VmPeak' in line:\n\t            mem = re.findall(r'\\d+', line)[0]\n\t            return int(mem) / 1024 / 1024\n\t    return 0.0\n\tdef dist_send_obj(obj, dst):\n\t    byte_t, size_t = _object_to_tensor(obj)\n\t    print(f\"Size of tensor to send (GB): \"\n\t          f\"{byte_t.numel() * byte_t.element_size() / 1024 ** 3}\")\n\t    dist.send(size_t, dst, tag=0)\n", "    dist.send(byte_t, dst, tag=1)\n\tdef dist_isend_obj(obj, dst):\n\t    byte_t, size_t = _object_to_tensor(obj)\n\t    print(f\"Size of tensor to send (GB): \"\n\t          f\"{byte_t.numel() * byte_t.element_size() / 1024 ** 3}\")\n\t    size_req = dist.isend(size_t, dst, tag=0)\n\t    byte_req = dist.isend(byte_t, dst, tag=1)\n\t    return size_req, byte_req\n\tdef dist_recv_obj(src):\n\t    size_t = torch.zeros(1, dtype=torch.long)\n", "    dist.recv(size_t, src, tag=0)\n\t    byte_t = torch.empty(size_t.item(), dtype=torch.uint8)\n\t    dist.recv(byte_t, src, tag=1)\n\t    obj = _tensor_to_object(byte_t, size_t)\n\t    return obj\n\tdef get_gpu_usage(flag: Value, gpu_vram: Value, gpu_list: list, interval=0.5):\n\t    while flag.value > 0:\n\t        proc = subprocess.run(\n\t            [\"nvidia-smi\", \"--format=csv\", \n\t             \"--query-gpu=index,memory.total,memory.used,memory.free,\"\n", "             \"utilization.gpu,utilization.memory\"\n\t            ], stdout=subprocess.PIPE, text=True)\n\t        strio = StringIO(proc.stdout)\n\t        df = pandas.read_csv(strio, index_col=0)\n\t        df.replace(regex=r\"\\s*([\\d\\.]*)\\s*(MiB|%)\", value=r\"\\1\", \n\t            inplace=True)\n\t        df = df.astype(float)\n\t        gpu_vram.value = df.iloc[gpu_list, 1].sum()\n\t        time.sleep(interval + 0.5 * random.random())\n\tdef parse_timedelta(timedelta_str: str) -> timedelta:\n", "    timedelta_str = timedelta_str.strip()\n\t    if timedelta_str.endswith(\"h\"):\n\t        return timedelta(hours=float(timedelta_str[:-1]))\n\t    elif timedelta_str.endswith(\"min\"):\n\t        return timedelta(minutes=float(timedelta_str[:-3]))\n\t    else:\n\t        raise ValueError(f\"Incorrect time delta: {timedelta_str}\")\n\tdef numpy2torch_dtype(dtype: np.dtype):\n\t    return torch.as_tensor(np.array(0, dtype=dtype)).dtype\n\tclass CustomDistributedDataParallel(torch.nn.parallel.DistributedDataParallel):\n", "    def __getattr__(self, name):\n\t        if name == 'module':\n\t            return super().__getattr__('module')\n\t        else:\n\t            return getattr(self.module, name)\n\t# Make them classes to work with pickling in mp.spawn\n\tclass SharedMemCollateWrapper(CollateWrapper):\n\t    \"\"\"Wraps a collate function with :func:`remove_parent_storage_columns` for serializing\n\t    from PyTorch DataLoader workers.\n\t    \"\"\"\n", "    def __init__(self, sample_func, graph_name, \n\t                 ndata_dict : dict = None,\n\t                 edata_dict : dict = None):\n\t        self.sample_func = sample_func\n\t        self.g = None\n\t        self.graph_name = graph_name\n\t        self.ndata_dict = ndata_dict\n\t        self.edata_dict = edata_dict\n\t    def __call__(self, items):\n\t        if self.g is None:\n", "            self.g = dgl.hetero_from_shared_memory(self.graph_name)\n\t            if self.ndata_dict is not None:\n\t                for key, tensor in self.ndata_dict.items():\n\t                    self.g.ndata[key] = tensor\n\t            if self.edata_dict is not None:\n\t                for key, tensor in self.edata_dict.items():\n\t                    self.g.edata[key] = tensor\n\t        return super().__call__(items)\n"]}
{"filename": "src/dataloading/DataMgr.py", "chunked_list": ["import os\n\timport shutil\n\timport time\n\timport warnings\n\tfrom pathlib import Path\n\timport dgl\n\timport numpy as np\n\timport psutil\n\timport torch\n\timport tqdm\n", "from dgl.data import AsLinkPredDataset, RedditDataset\n\tfrom ogb.linkproppred import DglLinkPropPredDataset\n\tfrom ogb.lsc import MAG240MDataset\n\tfrom .BaseDataMgr import BaseDataMgr\n\t# Get the real root directory of the repo even when the code is running in a temporary dvc directory\n\trepoRoot = Path(os.getcwd().split(\"/.dvc\", maxsplit=1)[0])\n\tclass DataMgr(BaseDataMgr):\n\t    '''\n\t    Load and preprocess data for training and evaluation.\n\t    '''\n", "    def __init__(self, dataset_name: str, to_undirected=True, use_self_loops=False, **kwargs):\n\t        '''\n\t        Initialize the data manager; load the corresponding dataset.\n\t        Parameters:\n\t        -----------\n\t        dataset_name: str\n\t            Name of the dataset to load.\n\t            Currently supported: 'ogbl_citation2', 'reddit', 'mag240m'\n\t        to_undirected: bool, default True\n\t            Whether to convert the graph to undirected.\n", "        use_self_loops: bool, default False\n\t            Whether to explicitly preprocess the graph to add self-loops edges.\n\t            Even when self-loop edges are not explicitly added during preprocessing,\n\t            GNN models can still implicitly add self-loop edges for message passing.\n\t            Check argument of GNN models for details.\n\t        kwargs: dict\n\t            Additional keyword arguments specific to each dataset.\n\t            Keyword arguments defined under `dataset.{dataset_name}` in `params.yaml` are forwarded here.\n\t        '''\n\t        self.dataset_str = dataset_name\n", "        self.use_feat_loader = False\n\t        self.train_frac = kwargs.get(\"train_frac\", 1)\n\t        self.random_seed = kwargs.get(\"rnd_seed\", None)\n\t        if dataset_name == 'ogbl_citation2':\n\t            _name = dataset_name.replace(\"_\", \"-\")\n\t            self.dataset = DglLinkPropPredDataset(\n\t                name=_name, root=str(repoRoot/\"dataset\"))\n\t        elif dataset_name == \"reddit\":\n\t            self.dataset = RedditDataset(\n\t                self_loop=use_self_loops,\n", "                raw_dir=str(repoRoot / \"dataset\" / \"reddit\"),\n\t            )\n\t        elif dataset_name == \"mag240m\":\n\t            self.dataset = MAG240MDatasetRAM(\n\t                tmpfs_dir=kwargs.get(\"tmpfs_dir\", None),\n\t                root=str(repoRoot / \"dataset\"))\n\t            self.use_feat_loader = True\n\t            self.variant = kwargs[\"variant\"]\n\t            if self.variant == \"citation_only\":\n\t                print(\"Loading DGL graph...\")\n", "                self.variant_dir = Path(self.dataset.dir) / \"modified\" / \"citation_only\"\n\t                if to_undirected:\n\t                    glist, _ = dgl.load_graphs(str(self.variant_dir / \"train_g_bi.dgl\"))\n\t                    self.train_graph = glist[0]\n\t                    self.dataset_str += \"-citation_only-bi\"\n\t                else:\n\t                    glist, _ = dgl.load_graphs(str(self.variant_dir / \"train_g.dgl\"))\n\t                    self.train_graph = glist[0]\n\t                    self.dataset_str += \"-citation_only\"\n\t            else:\n", "                raise NotImplementedError\n\t            self.__node_features = None\n\t        else:\n\t            raise NotImplementedError\n\t        self.dataset_name = dataset_name\n\t        self.dataset_kwargs = kwargs\n\t        self.to_undirected = to_undirected\n\t        self.use_self_loops = use_self_loops\n\t        self.dataset_prepared = False\n\t        self.multigpu_prepared = False\n", "        self.multigpu_train_graph_name = None\n\t        self.multigpu_shared_ndata = None\n\t        self.multigpu_shared_edata = None\n\t        self.multigpu_graph_name = None\n\t        self.multigpu_g_ndata = None\n\t        self.multigpu_g_edata = None\n\t    @property\n\t    def val_use_val_edges(self):\n\t        '''\n\t        Whether to use validation and test edges for message passing during evaluation.\n", "        Default: False\n\t        '''\n\t        return getattr(self.dataset, \"val_use_val_edges\", False)\n\t    @property\n\t    def save_embed(self):\n\t        '''\n\t        Whether to save GNN embeddings of all nodes during model testing.\n\t        Default: False\n\t        '''\n\t        return getattr(self.dataset, \"save_embed\", False)\n", "    @property\n\t    def save_embed_callback(self):\n\t        '''\n\t        Get callback function for saving GNN embeddings of all nodes during model testing.\n\t        That function must be defined in the dataset class.\n\t        '''\n\t        return getattr(self.dataset, \"save_embed_callback\", None)\n\t    def prepare_dataset(self):\n\t        '''\n\t        Prepare the dataset for training and evaluation:\n", "        - Preprocess the graph\n\t        - Split the dataset into train/val/test sets\n\t        Returns: None\n\t        '''\n\t        if not self.dataset_prepared:\n\t            # build test set with 10% positive links\n\t            if self.dataset_name in ['ogbl_citation2']:\n\t                graph = self.dataset[0]\n\t                # Extract node features\n\t                # feats = graph.ndata.pop('feat').to(device)\n", "                in_dim = graph.ndata[\"feat\"].shape[-1]\n\t                split_edge = self.dataset.get_edge_split()\n\t                train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n\t                assert graph.num_edges() == len(train_edge[\"source_node\"])\n\t                if self.to_undirected:\n\t                    print(\"Converting graph to undirected graph...\")\n\t                    graph = dgl.to_bidirected(graph, copy_ndata=True)\n\t                if self.use_self_loops:\n\t                    if (train_edge[\"source_node\"] == train_edge[\"target_node\"]).any():\n\t                        warnings.warn(\"Self loops included in original training splits!\")\n", "                    train_graph = dgl.add_self_loop(graph)\n\t                    train_edge_idx = train_graph.edge_ids(train_edge[\"source_node\"], train_edge[\"target_node\"])\n\t                else:\n\t                    train_graph = graph # dgl graph object containing only training edges\n\t                    train_edge_idx = train_graph.edge_ids(train_edge[\"source_node\"], train_edge[\"target_node\"])\n\t                self.graph = graph\n\t                self.train_graph : dgl.DGLHeteroGraph = train_graph\n\t                self.train_edge = train_edge\n\t                self.train_edge_idx = train_edge_idx\n\t                self.valid_edge = valid_edge\n", "                self.test_edge = test_edge\n\t                self.in_dim = in_dim\n\t                self.feat_dtype = graph.ndata[\"feat\"].dtype\n\t            elif self.dataset_name in [\"reddit\"]:\n\t                self.prev_dataset = self.dataset\n\t                self.dataset = AsOGBLinkPredDataset(\n\t                    self.dataset,\n\t                    self.dataset_kwargs[\"split_ratio\"],\n\t                    self.dataset_kwargs[\"neg_ratio\"],\n\t                    save_dir=str(repoRoot / self.prev_dataset.save_dir)\n", "                )\n\t                if not self.dataset.has_cache():\n\t                    print(\"Saving generated dataset...\")\n\t                    self.dataset.save()\n\t                self.train_graph: dgl.DGLHeteroGraph = self.dataset.train_graph\n\t                self.in_dim = self.train_graph.ndata[\"feat\"].shape[-1]\n\t                self.feat_dtype = self.train_graph.ndata[\"feat\"].dtype\n\t                self.train_edge_idx = self.train_graph.edges(\"eid\")\n\t                self.valid_edge = dict(\n\t                    source_node=self.dataset.val_edges[0][0],\n", "                    target_node=self.dataset.val_edges[0][1],\n\t                    target_node_neg=self.dataset.val_edges[1][1]\n\t                )\n\t                self.test_edge = dict(\n\t                    source_node=self.dataset.test_edges[0][0],\n\t                    target_node=self.dataset.test_edges[0][1],\n\t                    target_node_neg=self.dataset.test_edges[1][1]\n\t                )\n\t            elif self.dataset_name in ['mag240m']:\n\t                if self.variant == \"citation_only\":\n", "                    self.in_dim = self.dataset.num_paper_features\n\t                    valid_edge = dict()\n\t                    test_edge = dict()\n\t                    print(\"Loading edge splits...\")\n\t                    with np.load(str(self.variant_dir / \"splits.npz\")) as data:\n\t                        key: str\n\t                        for key, value in data.items():\n\t                            set_name, var_name = key.split(\"/\", maxsplit=1)\n\t                            if set_name == \"valid\":\n\t                                valid_edge[var_name] = torch.tensor(value)\n", "                            elif set_name == \"test\":\n\t                                test_edge[var_name] = torch.tensor(value)\n\t                    self.valid_edge = valid_edge\n\t                    self.test_edge = test_edge\n\t                    if self.dataset.paper_feat.dtype == np.float16:\n\t                        self.feat_dtype = torch.float16\n\t                    else:\n\t                        self.feat_dtype = torch.float32\n\t                else:\n\t                    raise NotImplementedError\n", "                if self.use_self_loops:\n\t                    self.train_graph = dgl.add_self_loop(self.train_graph)\n\t                self.train_edge_idx = self.train_graph.edges(\"eid\")\n\t            self.dataset_prepared = True\n\t    def create_formats_(self):\n\t        '''\n\t        Explicitly create all formats of the DGL graph object.\n\t        '''\n\t        self.train_graph.create_formats_()\n\t    @property\n", "    def node_features(self):\n\t        '''\n\t        Provide node features from sources other than the ndata storage of the DGL graph object.\n\t        On its first call, this property will load the node features from the appropriate source. \n\t        '''\n\t        if self.dataset_name in ['mag240m']:\n\t            if self.__node_features is None:\n\t                self.__node_features = self.dataset.paper_feat\n\t            return self.__node_features\n\t        else:\n", "            return None\n\t    def feature_loader(self, first_block, device=None):\n\t        '''\n\t        Load node features from sources other than the ndata storage of the DGL graph object.\n\t        '''\n\t        if device is None:\n\t            device = first_block.device\n\t        if self.dataset_name in ['mag240m']:\n\t            nids = first_block.srcdata[dgl.NID].cpu()\n\t            feat = self.node_features[nids]\n", "            return torch.as_tensor(feat, device=device)\n\t        elif self.dataset_name.startswith(\"amazon\"):\n\t            return self.dataset.feature_loader(first_block, device)\n\t        else: \n\t            # Load features from ndata of the DGL graph\n\t            return None\n\t    @staticmethod\n\t    def sample_idx(idx: torch.Tensor, frac: float, rnd_seed=None):\n\t        '''\n\t        Sample a fraction of idx tensor.\n", "        The fraction of sampling is not guaranteed to be exactly `frac`,\n\t        but will be very close to it when the length of idx is large.\n\t        Parameters:\n\t        -----------\n\t        idx: torch.Tensor\n\t            The tensor to be sampled.\n\t        frac: float\n\t            The non-exact fraction of idx to be sampled.\n\t        rnd_seed: int\n\t            The random seed to be used for sampling.\n", "        '''\n\t        torch_rnd_state = torch.random.get_rng_state()\n\t        if rnd_seed is not None:\n\t            torch.random.manual_seed(rnd_seed)\n\t        rand_exps = torch.rand(len(idx), dtype=torch.float16)\n\t        s_idx = idx[rand_exps < frac]\n\t        torch.random.set_rng_state(torch_rnd_state)\n\t        return s_idx\n\t    def multigpu_preprocess(self):\n\t        '''\n", "        Preprocess the dataset for multi-GPU training.\n\t        DGL graph object will be moved to shared memory and temporarily unlinked from the attribute\n\t        to avoid being pickled.\n\t        '''\n\t        if (self.dataset_name in ['mag240m', 'reddit'] \n\t            or self.dataset_name.startswith('amazon')):\n\t            print(\"Moving DGL graph to shared memory...\")\n\t            # The graph object will be written to /dev/shm\n\t            self.multigpu_train_graph_name = f\"train_graph-{int(time.time())}\"\n\t            shared_graph = self.train_graph.shared_memory(self.multigpu_train_graph_name)\n", "            if len(self.train_graph.ndata) > 0:\n\t                self.multigpu_shared_ndata = {\n\t                    key: tensor.share_memory_() for key, tensor in \n\t                    self.train_graph.ndata.items()\n\t                }\n\t            if len(self.train_graph.edata) > 0:\n\t                self.multigpu_shared_edata = {\n\t                    key: tensor.share_memory_() for key, tensor in \n\t                    self.train_graph.edata.items()\n\t                }\n", "            self.train_graph = None\n\t            if self.val_use_val_edges:\n\t                self.multigpu_graph_name = f\"graph-{int(time.time())}\"\n\t                shared_graph = [shared_graph, None]\n\t                shared_graph[-1] = self.graph.shared_memory(self.multigpu_graph_name)\n\t                if len(self.graph.ndata) > 0:\n\t                    self.multigpu_g_ndata = {\n\t                        key: tensor.share_memory_() for key, tensor in \n\t                        self.graph.ndata.items()\n\t                    }\n", "                if len(self.graph.edata) > 0:\n\t                    self.multigpu_g_edata = {\n\t                        key: tensor.share_memory_() for key, tensor in \n\t                        self.graph.edata.items()\n\t                    }\n\t            self.graph = None\n\t            if self.train_frac < 1.0:\n\t                print(f\"Sampling training edges using fraction {self.train_frac}...\")\n\t                self.train_edge_idx = DataMgr.sample_idx(\n\t                    self.train_edge_idx, self.train_frac, self.random_seed)\n", "            self.train_edge_idx.share_memory_()\n\t            for key in [\"source_node\", \"target_node\", \"target_node_neg\"]:\n\t                if self.valid_edge[key] is not None:\n\t                    self.valid_edge[key].share_memory_()\n\t                if (self.test_edge is not None) and (self.test_edge[key] is not None):\n\t                    self.test_edge[key].share_memory_()\n\t            if self.dataset_name.startswith(\"amazon\"):\n\t                self.dataset.multigpu_preprocess()\n\t            self.multigpu_prepared = True\n\t            return shared_graph\n", "        elif self.dataset_name in ['ogbl_citation2']:\n\t            for key in [\"source_node\", \"target_node\", \"target_node_neg\"]:\n\t                self.valid_edge[key].share_memory_()\n\t                self.test_edge[key].share_memory_()\n\t    def multigpu_postprocess(self):\n\t        '''\n\t        Postprocess the dataset in each spawned process for multi-GPU training.\n\t        DGL graph object will be loaded from shared memory and re-attached to the attribute.\n\t        '''\n\t        if self.multigpu_prepared:\n", "            print(\"Loading DGL graph from shared memory...\")\n\t            # The following call does not incur additional memory usage \n\t            # unless the graph structure is modified afterwards; in this case, the graph \n\t            # in the shared memory will not be modified, but a new copy of the graph\n\t            # will be created. Attaching ndata / edata to the graph does not affect the \n\t            # the graph in the shared memory.\n\t            self.train_graph = dgl.hetero_from_shared_memory(self.multigpu_train_graph_name)\n\t            if self.multigpu_shared_ndata is not None:\n\t                for key, tensor in self.multigpu_shared_ndata.items():\n\t                    self.train_graph.ndata[key] = tensor\n", "            if self.multigpu_shared_edata is not None:\n\t                for key, tensor in self.multigpu_shared_edata.items():\n\t                    self.train_graph.edata[key] = tensor\n\t            if self.multigpu_graph_name is not None:\n\t                self.graph = dgl.hetero_from_shared_memory(self.multigpu_graph_name)\n\t                if self.multigpu_g_ndata is not None:\n\t                    for key, tensor in self.multigpu_g_ndata.items():\n\t                        self.graph.ndata[key] = tensor\n\t                if self.multigpu_g_edata is not None:\n\t                    for key, tensor in self.multigpu_g_edata.items():\n", "                        self.graph.edata[key] = tensor\n\t            self.multigpu_prepared = False\n\tclass MAG240MDatasetRAM(MAG240MDataset):\n\t    '''\n\t    Wrapper class for MAG240M dataset to load the paper feature file from RAM disk (tmpfs)\n\t    '''\n\t    def __init__(self, root: str = 'dataset', tmpfs_dir = None):\n\t        '''\n\t        Parameters:\n\t        -----------\n", "        tmpfs_dir: str\n\t            Path to the RAM disk (tmpfs) directory\n\t        For other parameters, see MAG240MDataset\n\t        '''\n\t        super().__init__(root)\n\t        self.tmpfs_dir = Path(tmpfs_dir)\n\t        self.using_tmpfs = False\n\t    @property\n\t    def paper_feat(self) -> np.ndarray:\n\t        '''\n", "        Load (and create) numpy memmap feature file in the RAM disk (tmpfs) \n\t        for mag240m dataset\n\t        '''\n\t        if self.tmpfs_dir is None:\n\t            return super().paper_feat\n\t        tmpfs_file = self.tmpfs_dir / 'mag240m_node_feat.npy'\n\t        if self.using_tmpfs or tmpfs_file.exists():\n\t            if self.using_tmpfs is False:\n\t                print(f\"Using feature file from tmpfs on {self.tmpfs_dir}\")\n\t            self.using_tmpfs = True\n", "            return np.load(str(tmpfs_file), mmap_mode=\"r\")\n\t        elif (Path(self.tmpfs_dir).exists()):\n\t            # Please make sure the path is in `tmpfs` filesystem\n\t            # since the code does not check.\n\t            feat_file = Path(self.dir) / 'processed' / 'paper' / 'node_feat.npy'\n\t            print(f\"Loading feature file into tmpfs on {self.tmpfs_dir}...\")\n\t            shutil.copy2(feat_file, tmpfs_file)\n\t            self.using_tmpfs = True\n\t            return np.load(str(tmpfs_file), mmap_mode=\"r\")\n\t        else:\n", "            return super().paper_feat\n\tclass AsOGBLinkPredDataset(AsLinkPredDataset):\n\t    '''\n\t    Wrapper class of dgl.data.AsLinkPredDataset to match OGB data format\n\t    '''\n\t    def process(self):\n\t        '''\n\t        Generate train/val/test edges for link prediction in OGB format\n\t        '''\n\t        if self.split_ratio is None:\n", "            return super().process()\n\t        else:\n\t            assert self.split_ratio is not None, \"Need to specify split_ratio\"\n\t            assert self.neg_ratio is not None, \"Need to specify neg_ratio\"\n\t            ratio = self.split_ratio\n\t            graph = self.dataset[0]\n\t            n = graph.num_edges()\n\t            src, dst = graph.edges()\n\t            src, dst = dgl.backend.asnumpy(src), dgl.backend.asnumpy(dst)\n\t            n_train, n_val, n_test = int(\n", "                n * ratio[0]), int(n * ratio[1]), int(n * ratio[2])\n\t            idx = np.random.permutation(n)\n\t            train_pos_idx = idx[:n_train]\n\t            val_pos_idx = idx[n_train:n_train+n_val]\n\t            test_pos_idx = idx[n_train+n_val:]\n\t            pos_val_src, pos_val_dst = (src[val_pos_idx], dst[val_pos_idx])\n\t            neg_val_dst = self.negative_sample(\n\t                graph, pos_val_src, self.neg_ratio)\n\t            pos_test_src, pos_test_dst = (src[test_pos_idx], dst[test_pos_idx])\n\t            neg_test_dst = self.negative_sample(\n", "                graph, pos_test_src, self.neg_ratio)\n\t            self._val_edges = (\n\t                (dgl.backend.tensor(pos_val_src), dgl.backend.tensor(pos_val_dst)),\n\t                (dgl.backend.tensor([]), dgl.backend.tensor(neg_val_dst))\n\t            )\n\t            self._test_edges = (\n\t                (dgl.backend.tensor(pos_test_src), dgl.backend.tensor(pos_test_dst)),\n\t                (dgl.backend.tensor([]), dgl.backend.tensor(neg_test_dst))\n\t            )\n\t            self._train_graph = dgl.convert.graph(\n", "                (src[train_pos_idx], dst[train_pos_idx]), num_nodes=self.num_nodes)\n\t            self._train_graph.ndata[\"feat\"] = graph.ndata[\"feat\"]\n\t    @classmethod\n\t    def negative_sample(cls, graph: dgl.DGLGraph, src: np.ndarray, \n\t            neg_ratio, relation=[], sample_rate=1.2, no_existing_edges=True,\n\t            rng=None, random_seed=None):\n\t        '''\n\t        Sample negative target nodes for source nodes for link prediction evaluation\n\t        Parameters:\n\t        -----------\n", "        graph: dgl.DGLGraph\n\t            Graph for performing negative sampling\n\t        src: np.ndarray\n\t            Source nodes for negative sampling\n\t        neg_ratio: int\n\t            Number of negative samples per source node\n\t        relation: list\n\t            List of edge types for negative sampling\n\t        sample_rate: float\n\t            Rate of excessive negative sampling\n", "            Excessive sampling is needed when no_existing_edges is True,\n\t            which means some sampled edges need to be filtered out if they have appeared in the graph.\n\t        no_existing_edges: bool\n\t            Whether to exclude existing edges in the graph from negative samples\n\t            Turning this off can significantly speed up negative sampling\n\t        rng: np.random.Generator, optional\n\t            Random number generator to use for negative sampling\n\t        random_seed: int, optional\n\t            Random seed to use for negative sampling\n\t        '''\n", "        if rng is None:\n\t            rng = np.random.default_rng(seed=random_seed)\n\t        if not no_existing_edges:\n\t            sample_rate = 1\n\t        sample_size_relax = int(np.ceil(sample_rate * neg_ratio))\n\t        if len(relation) == 0:\n\t            neg_dst_relax = rng.integers(0, graph.num_nodes(), \n\t                size=(src.shape[0], sample_size_relax), dtype=src.dtype)\n\t        elif len(relation) == 1:\n\t            dst_type = graph.to_canonical_etype(relation[0])[-1]\n", "            neg_dst_relax = rng.integers(0, graph.num_nodes(dst_type), \n\t                size=(src.shape[0], sample_size_relax), dtype=src.dtype)\n\t        else:\n\t            raise NotImplementedError(\n\t                \"Val edges with different relations are not supported.\")\n\t        if not no_existing_edges:\n\t            return neg_dst_relax\n\t        neg_dst = np.zeros((src.shape[0], neg_ratio), dtype=src.dtype)\n\t        for i, (src_id, dst_id) in tqdm.tqdm(\n\t            enumerate(zip(src, neg_dst_relax)),\n", "            desc=\"Neg sampling\",\n\t            total=src.shape[0],\n\t            dynamic_ncols=True\n\t        ):\n\t            dst_id = np.unique(dst_id)\n\t            # remove self loop\n\t            mask_self_loop = (src_id.item() == dst_id)\n\t            # remove existing edges\n\t            if len(relation) == 0:\n\t                has_edges = dgl.backend.asnumpy(\n", "                    graph.has_edges_between(src_id, dst_id))\n\t            elif len(relation) == 1:\n\t                has_edges = dgl.backend.asnumpy(\n\t                    graph.has_edges_between(src_id, dst_id, etype=relation[0]))\n\t            else:\n\t                raise NotImplementedError(\n\t                    \"Val edges with different relations are not supported.\")\n\t            mask = ~(np.logical_or(mask_self_loop, has_edges))\n\t            dst_id_final = dst_id[mask]\n\t            if dst_id_final.shape[0] >= neg_ratio:\n", "                neg_dst[i, :] = dst_id_final[:neg_ratio]\n\t            else:\n\t                # Try automatically resample with increased excessive sampling rate\n\t                neg_dst[i, :] = cls.negative_sample(\n\t                    graph, np.array([src_id.item()], neg_ratio, relation, \n\t                    sample_rate=1.5 * sample_rate), rng=rng)\n\t                # raise RuntimeError(\n\t                #     f\"Insufficient sample rate for negative sampling: {dst_id_final.shape[0]} / {self.neg_ratio}\")\n\t        return neg_dst\n"]}
{"filename": "src/dataloading/__init__.py", "chunked_list": []}
{"filename": "src/dataloading/BaseDataMgr.py", "chunked_list": ["from typing import Dict\n\timport dgl\n\timport torch\n\tclass BaseDataMgr:\n\t    '''\n\t    Abstract base class for data managers.\n\t    '''\n\t    def __init__(self) -> None:\n\t        '''\n\t        Variables that need to be attached to the object (self) in child classes:\n", "        '''\n\t        # dgl.DGLGraph that is input to GNN models\n\t        self.train_graph: dgl.DGLGraph = None\n\t        # DGL edge indices of the edges in the training set\n\t        self.train_edge_idx: torch.Tensor = None\n\t        # dictionary of edges in the validation and test sets.\n\t        # - format: dict(\n\t        #     source_node=tensor(source_node_indices), # size=num_eval_edges\n\t        #     target_node=tensor(target_node_indices), # size=num_eval_edges\n\t        #     target_node_neg=tensor(target_node_neg_indices) \n", "        #        # size=num_eval_edges * num_neg_samples_per_eval_edge\n\t        #   )\n\t        # - Optional if no validation / test is performed.\n\t        self.valid_edge: Dict[str, torch.Tensor] = None\n\t        self.test_edge: Dict[str, torch.Tensor] = None\n\t        # dimension of node features\n\t        self.in_dim: int = None\n\t        # dtype of node features.\n\t        self.feat_dtype: torch.dtype = None\n"]}
{"filename": "src/models/RGCN.py", "chunked_list": ["\"\"\"\n\tRGCN implemented with DGL\n\tReferences:\n\t- Modeling Relational Data with Graph Convolutional Networks\n\t- Paper: https://arxiv.org/abs/1703.06103\n\t- Code: https://github.com/MichSchli/RelationPrediction\n\t\"\"\"\n\tfrom .BaseModules import BaseLinkEncoderDecoder, TensorTypeCast\n\timport torch\n\timport torch.nn as nn\n", "import dgl\n\timport dgl.function as fn\n\tclass RelGraphConv(dgl.nn.RelGraphConv):\n\t    '''\n\t    Wrapper for dgl.nn.RelGraphConv to support edge normalization \n\t    and automatic loading of edge types.\n\t    '''\n\t    def __init__(self, in_feat, out_feat, num_rels,\n\t                 regularizer=None, num_bases=None,\n\t                 bias=True, activation=None,\n", "                 self_loop=True, dropout=0.0, \n\t                 layer_norm=False, edge_norm=None):\n\t        '''\n\t        Parameters:\n\t        -----------\n\t        edge_norm: \"right\" or None (default), optional\n\t            If \"right\", then the edge normalization is applied as in the original paper.\n\t            If None, then no edge normalization is applied.\n\t        For usage of other parameters, see `dgl.nn.RelGraphConv`.\n\t        '''\n", "        if regularizer is not None and num_bases is None:\n\t            num_bases = num_rels\n\t        super().__init__(in_feat, out_feat, num_rels, regularizer, num_bases, \n\t                         bias, activation, self_loop, dropout, layer_norm)\n\t        self.edge_norm = edge_norm\n\t        self.num_rels = num_rels\n\t    def get_edge_norm(self, g: dgl.DGLGraph):\n\t        '''\n\t        Get edge normalization weights for the graph.\n\t        '''\n", "        if self.edge_norm == \"right\":\n\t            with g.local_scope():\n\t                g.edata[\"etypes_onehot\"] = torch.nn.functional.one_hot(\n\t                    g.edata[dgl.ETYPE], num_classes=self.num_rels).type(dtype=torch.float32)\n\t                g.update_all(fn.copy_e(\"etypes_onehot\", \"m\"), fn.sum(\"m\", \"etypes_deg\"))\n\t                g.dstdata[\"etypes_deg\"].reciprocal_()\n\t                g.dstdata[\"etypes_deg\"][g.dstdata[\"etypes_deg\"].isinf()] = 0\n\t                g.apply_edges(fn.e_dot_v(\"etypes_onehot\", \"etypes_deg\", \"norm\"))\n\t                norm = g.edata[\"norm\"]\n\t        elif self.edge_norm is None:\n", "            norm = None\n\t        else:\n\t            raise ValueError(f\"Unknown option for RelGraphConv: edge_norm={self.edge_norm}\")\n\t        return norm\n\t    def forward(self, g, feat, *, presorted=False):\n\t        '''\n\t        Wrapper for `dgl.nn.RelGraphConv.forward` to support edge normalization\n\t        and automatic loading of edge types.\n\t        '''\n\t        etypes = g.edata[dgl.ETYPE]\n", "        norm = self.get_edge_norm(g)\n\t        return super().forward(g, feat, etypes, norm=norm, presorted=presorted)\n\tclass TypedLinear(dgl.nn.TypedLinear):\n\t    '''\n\t    Wrapper for dgl.nn.TypedLinear.\n\t    '''\n\t    def forward(self, h, ntypes):\n\t        return super().forward(h, ntypes, sorted_by_type=False)\n\tclass RGCN(BaseLinkEncoderDecoder):\n\t    def __init__(self, in_dim, n_layers, hidden_dims, n_relations, n_nodetypes, device, dtype=None,\n", "                 activation=nn.PReLU, dropout=0.5, weight_decay=5e-4, regularizer=None,\n\t                 n_bases=None, edge_norm='right', bias=False, norm=nn.LayerNorm, \n\t                 add_self_loops=True, decoder_type=None, decoder_n_layers=None, \n\t                 decoder_hidden_dims=[], decoder_out_dim=1, \n\t                 feat_encoder_dims=[]):\n\t        '''\n\t        Initialize link prediction model (encoder + decoder) with RGCN as encoder.\n\t        Parameters:\n\t        -----------\n\t        in_dim: int\n", "            Input feature dimension\n\t        n_layers: int\n\t            Number of graph convolution layers\n\t        hidden_dims: int or list[int]\n\t            Hidden dimension of each graph convolution layer\n\t            if int, then the same hidden dimension is used for all layers\n\t        n_relations: int\n\t            Number of relations for edges in the graph\n\t        n_nodetypes: int\n\t            Number of node types in the graph\n", "        device: torch.device\n\t            Device to run the model\n\t        dtype: torch.dtype\n\t            Data type of the model\n\t        activation : nn.Module or None, optional\n\t            Activation function for the model, by default nn.PReLU.\n\t        dropout : float, optional\n\t            Dropout rate for the model\n\t        weight_decay : float, optional\n\t            Weight for L2 regularization\n", "        norm : nn.Module or None, optional\n\t            Normalization function for the model, by default None\n\t        add_self_loops : bool, optional\n\t            Always add self-loops for each node during message passing, by default True\n\t        feat_encoder_dims: list[int], optional\n\t            Dimensions of the additional dense layers (`TypedLinear) for feature embedding \n\t            before graph convolution, by default [], which uses no such layer.\n\t        For usages of regularizer, n_bases, bias, edge_norm, see `RelGraphConv`.\n\t        For usages of decoder_type, decoder_n_layers, decoder_hidden_dims, decoder_out_dim, \n\t            see `BaseLinkEncoderDecoder`.\n", "        '''\n\t        if type(hidden_dims) is int:\n\t            hidden_dims = [hidden_dims] * n_layers\n\t        else:\n\t            assert len(hidden_dims) == n_layers\n\t        self.n_layers = n_layers\n\t        self.hidden_dims = hidden_dims\n\t        self.n_relations = n_relations\n\t        self.n_nodetypes = n_nodetypes\n\t        self.edge_norm = edge_norm\n", "        self.bias = bias\n\t        if type(norm) is str:\n\t            norm = eval(norm)\n\t        self.norm = norm\n\t        if type(activation) is str:\n\t            activation = eval(activation)\n\t        self.activation = activation\n\t        self.dropout = dropout\n\t        self.weight_decay = float(weight_decay)\n\t        self.regularizer = regularizer\n", "        self.n_bases = n_bases\n\t        self.add_self_loops = add_self_loops\n\t        self.feat_encoder_dims = feat_encoder_dims    \n\t        self.decoder_type = decoder_type\n\t        if self.decoder_type is None:\n\t            decoder_config = None\n\t        else:\n\t            decoder_config = dict(\n\t                type=self.decoder_type,\n\t                n_relations=self.n_relations\n", "            )\n\t        super().__init__(\n\t            decoder_n_layers=decoder_n_layers,\n\t            decoder_hidden_dims=decoder_hidden_dims, \n\t            decoder_out_dim=decoder_out_dim,\n\t            decoder_config=decoder_config,\n\t            norm=None, activation=self.activation, dropout=0,\n\t            device=device)\n\t        if len(self.feat_encoder_dims) > 0:\n\t            # Cast feat tensor type\n", "            self.layers.append(\n\t                TensorTypeCast(torch.float32)\n\t            )\n\t            _dims = [in_dim] + self.feat_encoder_dims\n\t            for i in range(1, len(_dims)):\n\t                self.layers.append(\n\t                    TypedLinear(\n\t                        _dims[i - 1], _dims[i], \n\t                        num_types=self.n_nodetypes\n\t                    )\n", "                )\n\t                self.ntype_layer_ind.add(len(self.layers) - 1)\n\t                # order: Conv -> Norm -> Activation -> Dropout\n\t                if self.norm is not None:\n\t                    self.layers.append(\n\t                        self.norm(_dims[i])\n\t                    )\n\t                if self.activation is not None:\n\t                    self.layers.append(\n\t                        self.activation()\n", "                    )\n\t                self.layers.append(\n\t                    nn.Dropout(self.dropout)\n\t                )\n\t            _dims = [self.feat_encoder_dims[-1]] + self.hidden_dims\n\t        else:\n\t            _dims = [in_dim] + self.hidden_dims\n\t        for i in range(1, len(_dims)):\n\t            self.layers.append(\n\t                RelGraphConv(\n", "                    _dims[i - 1], _dims[i], self.n_relations,\n\t                    regularizer=self.regularizer,\n\t                    num_bases=self.n_bases,\n\t                    bias=self.bias,\n\t                    self_loop=self.add_self_loops,\n\t                    edge_norm=self.edge_norm\n\t                )\n\t            )\n\t            self.graph_layer_ind.add(len(self.layers) - 1)\n\t            if i != len(_dims) - 1:\n", "                # order: Conv -> Norm -> Activation -> Dropout\n\t                if self.norm is not None:\n\t                    self.layers.append(\n\t                        self.norm(_dims[i])\n\t                    )\n\t                if self.activation is not None:\n\t                    self.layers.append(\n\t                        self.activation()\n\t                    )\n\t                self.layers.append(\n", "                    nn.Dropout(self.dropout)\n\t                )\n"]}
{"filename": "src/models/GraphSAGE.py", "chunked_list": ["\"\"\"\n\tGraphSAGE implemented with DGL\n\tReferences:\n\t- Inductive Representation Learning on Large Graphs\n\t- Paper: https://arxiv.org/abs/1706.02216\n\t- Code: https://github.com/williamleif/GraphSAGE\n\t\"\"\"\n\tfrom .BaseModules import BaseLinkEncoderDecoder, TensorTypeCast\n\timport torch\n\timport torch.nn as nn\n", "import dgl\n\tclass SAGE(BaseLinkEncoderDecoder):\n\t    def __init__(self, in_dim, n_layers, hidden_dims, n_relations, device, dtype=None,\n\t            n_nodetypes=1, activation=nn.PReLU, dropout=0.2, \n\t            aggr_type=\"mean\", bias=False, norm=nn.LayerNorm,\n\t            decoder_type=None, decoder_n_layers=None, decoder_hidden_dims=[], decoder_out_dim=1):\n\t        '''\n\t        Initialize link prediction model (encoder + decoder) with GraphSAGE as encoder.\n\t        Parameters:\n\t        -----------\n", "        in_dim: int\n\t            Input feature dimension\n\t        n_layers: int\n\t            Number of graph convolution layers\n\t        hidden_dims: int or list[int]\n\t            Hidden dimension of each graph convolution layer\n\t            if int, then the same hidden dimension is used for all layers\n\t        n_relations: int\n\t            Number of relations for edges in the graph\n\t        device: torch.device\n", "            Device to run the model\n\t        dtype: torch.dtype\n\t            Data type of the model\n\t        n_nodetypes: int, optional\n\t            Number of node types in the graph\n\t            Optional as GraphSAGE does not use node type information\n\t        activation : nn.Module or None, optional\n\t            Activation function for the model, by default nn.PReLU.\n\t        dropout : float, optional\n\t            Dropout rate for the model, by default 0.2\n", "        norm : nn.Module or None, optional\n\t            Normalization function for the model, by default None\n\t        For usages of aggr_type and bias, see `dgl.nn.SAGEConv`.\n\t        For usages of decoder_type, decoder_n_layers, decoder_hidden_dims, decoder_out_dim, \n\t            see `BaseLinkEncoderDecoder`.\n\t        '''\n\t        if type(hidden_dims) is int:\n\t            hidden_dims = [hidden_dims] * n_layers\n\t        else:\n\t            assert len(hidden_dims) == n_layers\n", "        self.n_layers = n_layers\n\t        self.hidden_dims = hidden_dims\n\t        self.n_relations = n_relations\n\t        self.aggr_type = aggr_type\n\t        self.bias = bias\n\t        if type(norm) is str:\n\t            norm = eval(norm)\n\t        self.norm = norm\n\t        if type(activation) is str:\n\t            activation = eval(activation)\n", "        self.activation = activation\n\t        self.dropout = dropout\n\t        self.decoder_type = decoder_type\n\t        if self.decoder_type is None:\n\t            decoder_config = None\n\t        else:\n\t            decoder_config = dict(\n\t                type=self.decoder_type,\n\t                n_relations=self.n_relations\n\t            )\n", "        super().__init__(\n\t            decoder_n_layers=decoder_n_layers,\n\t            decoder_hidden_dims=decoder_hidden_dims, \n\t            decoder_out_dim=decoder_out_dim,\n\t            decoder_config=decoder_config,\n\t            norm=None, activation=self.activation, dropout=0,\n\t            device=device)\n\t        self.init(in_dim, hidden_dims, n_layers)\n\t    def init(self, in_feats, n_hidden, n_layers):\n\t        _dims = [in_feats] + n_hidden\n", "        # Cast feat tensor type for mag240m dataset\n\t        self.layers.append(\n\t            TensorTypeCast(torch.float32)\n\t        )\n\t        for i in range(1, len(_dims)):\n\t            self.layers.append(\n\t                dgl.nn.SAGEConv(_dims[i - 1], _dims[i], self.aggr_type, \n\t                    bias=self.bias)\n\t            )\n\t            self.graph_layer_ind.add(len(self.layers) - 1)\n", "            if i != len(_dims) - 1:\n\t                # order: Conv -> Norm -> Activation -> Dropout\n\t                if self.norm is not None:\n\t                    self.layers.append(\n\t                        self.norm(_dims[i])\n\t                    )\n\t                if self.activation is not None:\n\t                    self.layers.append(\n\t                        self.activation()\n\t                    )\n", "                self.layers.append(\n\t                    nn.Dropout(self.dropout)\n\t                )\n"]}
{"filename": "src/models/GCN.py", "chunked_list": ["\"\"\"\n\tAdopted from https://github.com/dmlc/dgl/blob/master/examples/pytorch/gcn/gcn.py\n\tGCN implemented with DGL\n\tReferences:\n\t- Semi-Supervised Classification with Graph Convolutional Networks\n\t- Paper: https://arxiv.org/abs/1609.02907\n\t- Code: https://github.com/tkipf/gcn\n\t\"\"\"\n\tfrom .BaseModules import BaseLinkEncoderDecoder\n\timport torch\n", "import torch as th\n\timport torch.nn as nn\n\timport dgl.function as fn\n\tfrom dgl.nn.pytorch.conv import graphconv\n\tclass GCN(BaseLinkEncoderDecoder):\n\t    def __init__(self, in_dim, n_layers, hidden_dims, n_relations, device, dtype=None,\n\t        n_nodetypes=1, activation=nn.PReLU, dropout=0.5, weight_decay=5e-4,\n\t        edge_norm='both', bias=False, norm=nn.LayerNorm, add_self_loops=True,\n\t        decoder_type=None, decoder_n_layers=None, decoder_hidden_dims=[], decoder_out_dim=1):\n\t        '''\n", "        Initialize link prediction model (encoder + decoder) with GCN as encoder.\n\t        Parameters:\n\t        -----------\n\t        in_dim: int\n\t            Input feature dimension\n\t        n_layers: int\n\t            Number of graph convolution layers\n\t        hidden_dims: int or list[int]\n\t            Hidden dimension of each graph convolution layer\n\t            if int, then the same hidden dimension is used for all layers\n", "        n_relations: int\n\t            Number of relations for edges in the graph\n\t        device: torch.device\n\t            Device to run the model\n\t        dtype: torch.dtype\n\t            Data type of the model\n\t        n_nodetypes: int, optional\n\t            Number of node types in the graph\n\t            Optional as GCN does not use node type information\n\t        activation : nn.Module or None, optional\n", "            Activation function for the model, by default nn.PReLU.\n\t        dropout : float, optional\n\t            Dropout rate for the model, by default 0\n\t        weight_decay : float, optional\n\t            Weight for L2 regularization, by default 5e-4\n\t        norm : nn.Module or None, optional\n\t            Normalization function for the model, by default None\n\t        add_self_loops : bool, optional\n\t            Always add self-loops for each node during message passing, by default True\n\t        For usages of edge_norm and bias, see `dgl.nn.pytorch.conv.graphconv`.\n", "        For usages of decoder_type, decoder_n_layers, decoder_hidden_dims, decoder_out_dim, \n\t            see `BaseLinkEncoderDecoder`.\n\t        '''\n\t        if type(hidden_dims) is int:\n\t            hidden_dims = [hidden_dims] * n_layers\n\t        else:\n\t            assert len(hidden_dims) == n_layers\n\t        self.n_layers = n_layers\n\t        self.hidden_dims = hidden_dims\n\t        self.n_relations = n_relations\n", "        self.edge_norm = edge_norm\n\t        self.bias = bias\n\t        if type(norm) is str:\n\t            norm = eval(norm)\n\t        self.norm = norm\n\t        if type(activation) is str:\n\t            activation = eval(activation)\n\t        self.activation = activation\n\t        self.dropout = dropout\n\t        self.weight_decay = float(weight_decay)\n", "        self.add_self_loops = add_self_loops\n\t        self.decoder_type = decoder_type\n\t        if self.decoder_type is None:\n\t            decoder_config = None\n\t        else:\n\t            decoder_config = dict(\n\t                type=self.decoder_type,\n\t                n_relations=self.n_relations\n\t            )\n\t        super().__init__(\n", "            decoder_n_layers=decoder_n_layers,\n\t            decoder_hidden_dims=decoder_hidden_dims, \n\t            decoder_out_dim=decoder_out_dim,\n\t            decoder_config=decoder_config,\n\t            norm=None, activation=self.activation, dropout=0,\n\t            device=device)\n\t        _dims = [in_dim] + self.hidden_dims\n\t        for i in range(1, len(_dims)):\n\t            self.layers.append(\n\t                GraphConv(_dims[i - 1], _dims[i], \n", "                    norm=self.edge_norm, bias=self.bias, \n\t                    add_self_loops=add_self_loops)\n\t            )\n\t            self.graph_layer_ind.add(len(self.layers) - 1)\n\t            if i != len(_dims) - 1:\n\t                # order: Conv -> Norm -> Activation -> Dropout\n\t                if self.norm is not None:\n\t                    self.layers.append(\n\t                        self.norm(_dims[i])\n\t                    )\n", "                if self.activation is not None:\n\t                    self.layers.append(\n\t                        self.activation()\n\t                    )\n\t                self.layers.append(\n\t                    nn.Dropout(self.dropout)\n\t                )\n\t    def loss_fn(self, pos_graph, neg_graph, pos_logits, neg_logits):\n\t        '''\n\t        Training loss function that adds an L2 regularization term in addition to the \n", "        cross entropy loss.\n\t        Parameters:\n\t        ----------\n\t            pos_graph : dgl.DGLGraph\n\t                Graph with edges as positive link prediction targets\n\t            neg_graph : dgl.DGLGraph\n\t                Graph with edges as negative link prediction targets\n\t            pos_logits : torch.Tensor\n\t                Link prediction scores for edges in `pos_graph`\n\t            neg_logits : torch.Tensor\n", "                Link prediction scores for edges in `neg_graph`\n\t        Returns:\n\t        ----------\n\t            loss : torch.Tensor\n\t                Cross-entropy loss\n\t        '''\n\t        entropy_loss = super().loss_fn(pos_graph, neg_graph, pos_logits, neg_logits)\n\t        l2_loss = self.weight_decay * torch.linalg.norm(self.layers[0].weight, ord=\"fro\")\n\t        total_loss = entropy_loss + l2_loss\n\t        return total_loss\n", "class GraphConv(graphconv.GraphConv):\n\t    '''\n\t    Modified Graph Convolutional Layer based on DGL 0.7.2 implementation\n\t    to add option to always include self loops in message passing.\n\t    '''\n\t    def __init__(self, in_feats, out_feats, norm='both', \n\t                 weight=True, bias=True, activation=None, \n\t                 allow_zero_in_degree=False, add_self_loops=True):\n\t        '''\n\t        Parameters:\n", "        -----------\n\t            add_self_loops : bool, optional\n\t                Always include self-loops in the graph for message passing, by default True\n\t            For usages of all other parameters, see `dgl.nn.pytorch.conv.graphconv`.\n\t        '''\n\t        super().__init__(in_feats, out_feats, norm, weight, bias, activation, allow_zero_in_degree)\n\t        self.add_self_loops = add_self_loops\n\t    def forward(self, graph, feat, weight=None, edge_weight=None):\n\t        r\"\"\"\n\t        Parameters\n", "        ----------\n\t        graph : DGLGraph\n\t            The graph.\n\t        feat : torch.Tensor or pair of torch.Tensor\n\t            If a torch.Tensor is given, it represents the input feature of shape\n\t            :math:`(N, D_{in})`\n\t            where :math:`D_{in}` is size of input feature, :math:`N` is the number of nodes.\n\t            If a pair of torch.Tensor is given, which is the case for bipartite graph, the pair\n\t            must contain two tensors of shape :math:`(N_{in}, D_{in_{src}})` and\n\t            :math:`(N_{out}, D_{in_{dst}})`.\n", "        weight : torch.Tensor, optional\n\t            Optional external weight tensor.\n\t        edge_weight : torch.Tensor, optional\n\t            Optional tensor on the edge. If given, the convolution will weight\n\t            with regard to the message.\n\t        Returns\n\t        -------\n\t        torch.Tensor\n\t            The output feature\n\t        Raises\n", "        ------\n\t        DGLError\n\t            Case 1:\n\t            If there are 0-in-degree nodes in the input graph, it will raise DGLError\n\t            since no message will be passed to those nodes. This will cause invalid output.\n\t            The error can be ignored by setting ``allow_zero_in_degree`` parameter to ``True``.\n\t            Case 2:\n\t            External weight is provided while at the same time the module\n\t            has defined its own weight parameter.\n\t        Note\n", "        ----\n\t        * Input shape: :math:`(N, *, \\text{in_feats})` where * means any number of additional\n\t          dimensions, :math:`N` is the number of nodes.\n\t        * Output shape: :math:`(N, *, \\text{out_feats})` where all but the last dimension are\n\t          the same shape as the input.\n\t        * Weight shape: :math:`(\\text{in_feats}, \\text{out_feats})`.\n\t        \"\"\"\n\t        with graph.local_scope():\n\t            if not (self._allow_zero_in_degree or self.add_self_loops):\n\t                if (graph.in_degrees() == 0).any():\n", "                    raise graphconv.DGLError('There are 0-in-degree nodes in the graph, '\n\t                                   'output for those nodes will be invalid. '\n\t                                   'This is harmful for some applications, '\n\t                                   'causing silent performance regression. '\n\t                                   'Adding self-loop on the input graph by '\n\t                                   'calling `g = dgl.add_self_loop(g)` will resolve '\n\t                                   'the issue. Setting ``allow_zero_in_degree`` '\n\t                                   'to be `True` when constructing this module will '\n\t                                   'suppress the check and let the code run.')\n\t            aggregate_fn = fn.copy_src('h', 'm')\n", "            if edge_weight is not None:\n\t                assert edge_weight.shape[0] == graph.number_of_edges()\n\t                graph.edata['_edge_weight'] = edge_weight\n\t                aggregate_fn = fn.u_mul_e('h', '_edge_weight', 'm')\n\t            # (BarclayII) For RGCN on heterogeneous graphs we need to support GCN on bipartite.\n\t            feat_src, feat_dst = graphconv.expand_as_pair(feat, graph)\n\t            if self._norm in ['left', 'both']:\n\t                if not self.add_self_loops:\n\t                    degs = graph.out_degrees().float().clamp(min=1)\n\t                else:\n", "                    degs = graph.out_degrees().float()\n\t                    if graph.is_block:\n\t                        degs[:graph.num_dst_nodes()] += 1\n\t                    else:\n\t                        degs += 1\n\t                if self._norm == 'both':\n\t                    norm = th.pow(degs, -0.5)\n\t                else:\n\t                    norm = 1.0 / degs\n\t                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n", "                norm = th.reshape(norm, shp)\n\t                feat_src = feat_src * norm\n\t            if weight is not None:\n\t                if self.weight is not None:\n\t                    raise graphconv.DGLError('External weight is provided while at the same time the'\n\t                                   ' module has defined its own weight parameter. Please'\n\t                                   ' create the module with flag weight=False.')\n\t            else:\n\t                weight = self.weight\n\t            if self._in_feats > self._out_feats:\n", "                # mult W first to reduce the feature size for aggregation.\n\t                if weight is not None:\n\t                    feat_src = th.matmul(feat_src, weight)\n\t                graph.srcdata['h'] = feat_src\n\t                if self.add_self_loops:\n\t                    if graph.is_block:\n\t                        graph.dstdata['h'] = graph.srcdata['h'][:graph.num_dst_nodes()]\n\t                    else:\n\t                        graph.dstdata['h'] = graph.srcdata['h']\n\t                graph.update_all(aggregate_fn, fn.sum(msg='m', out='neigh'))\n", "                rst = graph.dstdata['neigh'] + graph.dstdata['h']\n\t            else:\n\t                # aggregate first then mult W\n\t                graph.srcdata['h'] = feat_src\n\t                if self.add_self_loops:\n\t                    if graph.is_block:\n\t                        graph.dstdata['h'] = graph.srcdata['h'][:graph.num_dst_nodes()]\n\t                    else:\n\t                        graph.dstdata['h'] = graph.srcdata['h']\n\t                graph.update_all(aggregate_fn, fn.sum(msg='m', out='neigh'))\n", "                rst = graph.dstdata['neigh'] + graph.dstdata['h']\n\t                if weight is not None:\n\t                    rst = th.matmul(rst, weight)\n\t            if self._norm in ['right', 'both']:\n\t                if not self.add_self_loops:\n\t                    degs = graph.in_degrees().float().clamp(min=1)\n\t                else:\n\t                    degs = graph.in_degrees().float() + 1\n\t                if self._norm == 'both':\n\t                    norm = th.pow(degs, -0.5)\n", "                else:\n\t                    norm = 1.0 / degs\n\t                shp = norm.shape + (1,) * (feat_dst.dim() - 1)\n\t                norm = th.reshape(norm, shp)\n\t                rst = rst * norm\n\t            if self.bias is not None:\n\t                rst = rst + self.bias\n\t            if self._activation is not None:\n\t                rst = self._activation(rst)\n\t            return rst\n"]}
{"filename": "src/models/MLP.py", "chunked_list": ["\"\"\"\n\tMLP implemented with DGL\n\t\"\"\"\n\tfrom .BaseModules import BaseLinkEncoderDecoder, TensorTypeCast\n\timport torch\n\timport torch.nn as nn\n\tclass MLP(BaseLinkEncoderDecoder):\n\t    def __init__(self, in_dim, n_layers, hidden_dims, device, dtype=torch.float32,\n\t        n_relations=1, n_nodetypes=1, activation=nn.PReLU, dropout=0.5, weight_decay=5e-4,\n\t        bias=False, norm=nn.LayerNorm,\n", "        decoder_n_layers=None, decoder_hidden_dims=[], decoder_out_dim=1):\n\t        '''\n\t        Initialize link prediction model (encoder + decoder) with graph-agnostic MLP as encoder.\n\t        Parameters:\n\t        -----------\n\t        in_dim: int\n\t            Input feature dimension\n\t        n_layers: int\n\t            Number of MLP layers\n\t        hidden_dims: int or list[int]\n", "            Hidden dimension of each MLP layer\n\t            if int, then the same hidden dimension is used for all layers\n\t        n_relations: int\n\t            Number of relations for edges in the graph\n\t            Optional as MLP does not use node type information\n\t        device: torch.device\n\t            Device to run the model\n\t        dtype: torch.dtype\n\t            Data type of the model\n\t        n_nodetypes: int, optional\n", "            Number of node types in the graph\n\t            Optional as MLP does not use node type information\n\t        activation : nn.Module or None, optional\n\t            Activation function for the model, by default nn.PReLU.\n\t        dropout : float, optional\n\t            Dropout rate for the model, by default 0.5\n\t        weight_decay : float, optional\n\t            Weight for L2 regularization, by default 5e-4\n\t        norm : nn.Module or None, optional\n\t            Normalization function for the model, by default None\n", "        For usages of bias, see `torch.nn.Linear`.\n\t        For usages of decoder_n_layers, decoder_hidden_dims, decoder_out_dim, \n\t            see `BaseLinkEncoderDecoder`.\n\t        '''\n\t        if type(hidden_dims) is int:\n\t            hidden_dims = [hidden_dims] * n_layers\n\t        else:\n\t            assert len(hidden_dims) == n_layers\n\t        self.n_layers = n_layers\n\t        self.hidden_dims = hidden_dims\n", "        self.bias = bias\n\t        if type(norm) is str:\n\t            norm = eval(norm)\n\t        self.norm = norm\n\t        if type(activation) is str:\n\t            activation = eval(activation)\n\t        self.activation = activation\n\t        self.dropout = dropout\n\t        self.weight_decay = float(weight_decay)\n\t        super().__init__(\n", "            decoder_n_layers=decoder_n_layers,\n\t            decoder_hidden_dims=decoder_hidden_dims, \n\t            decoder_out_dim=decoder_out_dim,\n\t            norm=None, activation=self.activation, dropout=0)\n\t        _dims = [in_dim] + self.hidden_dims\n\t        # Cast feat tensor type for mag240m dataset\n\t        self.layers.append(\n\t            TensorTypeCast(torch.float32)\n\t        )\n\t        for i in range(1, len(_dims)):\n", "            self.layers.append(\n\t                torch.nn.Linear(_dims[i - 1], _dims[i], \n\t                    bias=self.bias)\n\t            )\n\t            # For MLP, no graph layer exists\n\t            # self.graph_layer_ind.add(len(self.layers) - 1)\n\t            if i != len(_dims) - 1:\n\t                # order: Conv -> Norm -> Activation -> Dropout\n\t                if self.norm is not None:\n\t                    self.layers.append(\n", "                        self.norm(_dims[i])\n\t                    )\n\t                if self.activation is not None:\n\t                    self.layers.append(\n\t                        self.activation()\n\t                    )\n\t                self.layers.append(\n\t                    nn.Dropout(self.dropout)\n\t                )\n\t    def loss_fn(self, pos_graph, neg_graph, pos_logits, neg_logits):\n", "        '''\n\t        Training loss function that adds an L2 regularization term in addition to the \n\t        cross entropy loss.\n\t        Parameters:\n\t        ----------\n\t            pos_graph : dgl.DGLGraph\n\t                Graph with edges as positive link prediction targets\n\t            neg_graph : dgl.DGLGraph\n\t                Graph with edges as negative link prediction targets\n\t            pos_logits : torch.Tensor\n", "                Link prediction scores for edges in `pos_graph`\n\t            neg_logits : torch.Tensor\n\t                Link prediction scores for edges in `neg_graph`\n\t        Returns:\n\t        ----------\n\t            loss : torch.Tensor\n\t                Cross-entropy loss\n\t        '''\n\t        entropy_loss = super().loss_fn(pos_graph, neg_graph, pos_logits, neg_logits)\n\t        l2_loss = self.weight_decay * torch.linalg.norm(self.layers[1].weight, ord=\"fro\")\n", "        total_loss = entropy_loss + l2_loss\n\t        return total_loss"]}
{"filename": "src/models/BaseModules.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport dgl\n\timport dgl.function as fn\n\timport tqdm\n\tfrom collections import deque\n\timport time\n\tfrom functools import cached_property\n", "import warnings\n\tfrom .DistMult import DistMultDecoder\n\tclass BaseLinkEncoderDecoder(nn.Module):\n\t    '''\n\t    Base class for link prediction encoder and decoders. \n\t    To implement a complete model, inherit from this class and make sure that:\n\t    1. All encoder layers must be added to `self.layers` in sequential order.\n\t    2. Indices of all GNN layers must be added to `self.graph_layer_ind`.\n\t    3. Indices of all layers that require node type information must be added to `self.node_type_ind`.\n\t    4. Override `loss_fn` to consider additional loss terms.\n", "    5. `__init__` function must contain the following arguments to maintain compatibility with \n\t        the training pipeline:\n\t        - in_dim: int\n\t            Input feature dimension\n\t        - n_relations: int\n\t            Number of relations for edges in the graph\n\t        - n_nodetypes: int, optional\n\t            Number of node types in the graph\n\t        - device: torch.device\n\t            Device to run the model\n", "        - dtype: torch.dtype\n\t            Data type of the model\n\t        Other arguments defined in `models.{model_name}.model_params in `params.yaml` will be forwarded as kwargs.\n\t        See pipeline.SessionMgr.SessionMgr.create_model for more details.\n\t    6. If any GNN layer conducts message passing beyond 1-hop neighbors of each node (e.g. MixHop, H2GCN), \n\t       you will need to reimplement `get_MFG_nodes`.\n\t    '''\n\t    def __init__(self, decoder_config=None, decoder_n_layers=None, decoder_hidden_dims=[],\n\t                 decoder_out_dim=1, norm=None, activation=nn.PReLU, dropout=0, device=None) -> None:\n\t        '''\n", "        Initialize base class and decoder for link prediction models\n\t        Currently supported decoders include dot-product, MLP, and DistMult.\n\t        Parameters: \n\t        ----------\n\t        decoder_config : None or dict, optional\n\t            Specify the decoder type and its configurations:\n\t            - For MLP or dot-product decoder, use None (default, see also decoder_hidden_dims)\n\t            - For DistMult decoder, use {\"type\": \"DistMult\", \"n_relations\": int} and\n\t                set n_relations as the number of relations in the graph.\n\t        decoder_hidden_dims : int or list[int], optional\n", "            Hidden dimensions of the decoder model. \n\t            - If [] (default), a non-parameterized dot product decoder will be used. \n\t            - For MLP decoder: \n\t                * If the type is int, the number of layers must be specified in the decoder_n_layers, \n\t                    and each layer will have the same hidden dimension as specified.\n\t                * Use list[int] to specify both the number of layers and the hidden dimensions of each layer.\n\t            - For DistMult decoder, use list[int] with length 1 to specify the size of relation embeddings.\n\t        decoder_n_layers : int or None, optional\n\t            Number of layers in the decoder.\n\t            - If None, the number of layers will be inferred from the length of decoder_hidden_dims.\n", "            - Must be specified if decoder_hidden_dims is set as an int.\n\t        decoder_out_dim : int, optional\n\t            Output dimension of the decoder, by default 1 for link prediction. \n\t        norm : nn.Module or None, optional\n\t            Normalization function for the decoder, by default None\n\t        activation : nn.Module or None, optional\n\t            Activation function for the decoder, by default nn.PReLU.\n\t        dropout : float, optional\n\t            Dropout rate for the decoder, by default 0\n\t        device : torch.device, optional\n", "            Device for the decoder module to run on, by default None\n\t        '''\n\t        super().__init__()\n\t        self.graph_layer_ind = set()\n\t        self.ntype_layer_ind = set()\n\t        self.layers = nn.ModuleList()\n\t        if decoder_config is None:  # MLP decoder\n\t            if type(decoder_hidden_dims) is int:\n\t                decoder_hidden_dims = [\n\t                    decoder_hidden_dims] * (decoder_n_layers - 1)\n", "            elif decoder_n_layers is None:\n\t                decoder_n_layers = len(decoder_hidden_dims) + 1\n\t            else:\n\t                assert len(decoder_hidden_dims) == decoder_n_layers - 1\n\t            self.decoder_dims = decoder_hidden_dims\n\t            self.decoder_out_dim = decoder_out_dim\n\t            if len(decoder_hidden_dims) > 0:\n\t                self.decoder_type = None\n\t                self.hg_decoder = False\n\t                self.decoder_layers = nn.ModuleList()\n", "                _dims = [self.hidden_dims[-1], *\n\t                         self.decoder_dims, decoder_out_dim]\n\t                for i in range(1, len(_dims)):\n\t                    self.decoder_layers.append(\n\t                        nn.Linear(_dims[i - 1], _dims[i], bias=self.bias)\n\t                    )\n\t                    if i != len(_dims) - 1:\n\t                        if norm is not None:\n\t                            self.decoder_layers.append(\n\t                                self.norm(_dims[i])\n", "                            )\n\t                        if activation is not None:\n\t                            self.decoder_layers.append(\n\t                                activation()\n\t                            )\n\t                        self.decoder_layers.append(\n\t                            nn.Dropout(dropout)\n\t                        )\n\t        elif decoder_config[\"type\"] == \"DistMult\":\n\t            self.decoder_type = decoder_config[\"type\"]\n", "            self.hg_decoder = True\n\t            self.decoder_module = DistMultDecoder(\n\t                decoder_config[\"n_relations\"],\n\t                self.hidden_dims[-1],\n\t                device=device\n\t            )\n\t        else:\n\t            raise ValueError(f\"Unknown decoder config {decoder_config}\")\n\t    def encoder(self, blocks, x):\n\t        '''\n", "        Run encoder part of the model to generate node embeddings.\n\t        Parameters:\n\t        ----------\n\t            blocks : list of dgl.DGLGraph\n\t                List of graphs (or blocks) for message passing on each GNN layer.\n\t                The length of the list should be equal to the number of GNN layers.\n\t            x : torch.Tensor\n\t                Node features\n\t        Returns:\n\t        ----------\n", "            h : torch.Tensor\n\t                Node embeddings. \n\t        '''\n\t        h = x\n\t        blocks_queue: deque[dgl.DGLGraph] = deque(blocks)\n\t        for l, layer in enumerate(self.layers):\n\t            if l in self.graph_layer_ind: \n\t                # GNN layer\n\t                block = blocks_queue[0]\n\t                blocks_queue.popleft()\n", "                h = layer(block, h)\n\t            elif l in self.ntype_layer_ind: \n\t                # Layer that needs to know node types\n\t                h = layer(h, ntypes=blocks_queue[0].srcdata[dgl.NTYPE])\n\t            else: \n\t                # All other layers that do not consume graph information\n\t                h = layer(h)\n\t        if len(blocks_queue) != 0:\n\t            warnings.warn(\n\t                \"There are more blocks than GNN layers; please check sampler config.\")\n", "            time.sleep(5)\n\t        return h\n\t    def encoder_generator(self, block, h):\n\t        '''\n\t        A generator version of encoder that yields the node embeddings obtained after each GNN layer.\n\t        This function is used during evaluation to conduct layer-wise inference and \n\t        avoid extensive storage of the intermediate node embeddings, which is a generalization of the \n\t        `SAGE.inference` function in\n\t        https://github.com/dmlc/dgl/blob/master/examples/pytorch/graphsage/node_classification.py#L34\n\t        Parameters:\n", "        ----------\n\t            block : dgl.DGLGraph\n\t                Graph (or block) for message passing on the first GNN layer.\n\t            h : torch.Tensor\n\t                Node features for the first GNN layer.\n\t        Yields:\n\t        ----------\n\t            block_counter : int\n\t                Index of block (or GNN layer) to be consumed (or processed) next.\n\t                `block_counter < 0` indicates that all GNN layers of the encoder have been processed, \n", "                    and no value can be yielded from the generator.\n\t            h : torch.Tensor\n\t                Node embeddings obtained after the GNN layer indexed by `block_counter`.\n\t        generator.send(block, h): \n\t        ----------\n\t            block : dgl.DGLGraph\n\t                Graph (or block) for message passing for the next GNN layer\n\t                (indexed by yielded `block_counter`).\n\t            h : torch.Tensor\n\t                Node features for the next GNN layer.\n", "        '''\n\t        block_counter = 0\n\t        h_container = deque([h])\n\t        del h\n\t        for l, layer in enumerate(self.layers):\n\t            if l in self.ntype_layer_ind:  # Layer that needs to know node types\n\t                if block is None:  # Needs to fetch a new block from generator.send()\n\t                    h_container.append(None)\n\t                    # Yield block_counter and h_container[0] (h obtained from previous layer)\n\t                    # and receive new block and h for the next GNN layer\n", "                    block, h_container[0] = yield (block_counter, h_container.popleft())\n\t                h_container[0] = layer(\n\t                    h_container[0], ntypes=block.srcdata[dgl.NTYPE])\n\t                # Do not discard block until next GraphConv layer\n\t            elif l in self.graph_layer_ind:  # GNN layer\n\t                if block is None:  # Needs to fetch a new block from generator.send()\n\t                    h_container.append(None)\n\t                    # Yield block_counter and h_container[0] (h obtained from previous layer)\n\t                    # and receive new block and h for the next GNN layer\n\t                    block, h_container[0] = yield (block_counter, h_container.popleft())\n", "                h_container[0] = layer(block, h_container[0])\n\t                block = None  # discard used block\n\t                block_counter += 1\n\t            else:  # All other layers that do not consume graph information\n\t                h_container[0] = layer(h_container[0])\n\t        # Return final node embeddings\n\t        block = None\n\t        # negate block_counter to indicate the end of generator\n\t        yield -block_counter, h_container.popleft()\n\t    def decoder(self, z: torch.Tensor, graph: dgl.DGLGraph, neg_graph: dgl.DGLGraph = None):\n", "        '''\n\t        Get link prediction scores with node embeddings and DGL graphs.\n\t        Parameters:\n\t        ----------\n\t            z : torch.Tensor\n\t                Node embeddings\n\t            graph : dgl.DGLGraph\n\t                Graph with edges as (positive) link prediction targets\n\t            neg_graph : dgl.DGLGraph, optional\n\t                Graph with edges as negative link prediction targets\n", "        Returns: \n\t        ----------\n\t            score : torch.Tensor\n\t                Link prediction scores for edges in `graph`\n\t            neg_score : torch.Tensor, if neg_graph is not None\n\t                Link prediction scores for edges in `neg_graph`\n\t        '''\n\t        if self.decoder_type is None:\n\t            with graph.local_scope():\n\t                graph.ndata['h'] = z\n", "                if len(self.decoder_layers) > 0:\n\t                    graph.apply_edges(fn.u_mul_v('h', 'h', 'dot_prod'))\n\t                    h = graph.edata['dot_prod']\n\t                else:\n\t                    graph.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n\t                    score = graph.edata['score']\n\t            if len(self.decoder_layers) > 0:\n\t                for _, layer in enumerate(self.decoder_layers):\n\t                    h = layer(h)\n\t                    score = h\n", "            if neg_graph is not None:\n\t                neg_score = self.decoder(z, neg_graph)\n\t                return score, neg_score\n\t            else:\n\t                return score\n\t        else:  # DistMult\n\t            return self.decoder_module.decoder(z, graph, neg_graph)\n\t    def decoder_mat(self, uh, vh, etypes=None):\n\t        '''\n\t        Get link prediction scores from embeddings of source and destination nodes.\n", "        Parameters:\n\t        ----------\n\t            uh: torch.Tensor\n\t                Embeddings of source nodes\n\t            vh: torch.Tensor\n\t                Embeddings of destination nodes\n\t            etypes: torch.IntTensor, optional\n\t                Edge types in heterogeneous graphs\n\t        Returns: \n\t        ----------\n", "            score : torch.Tensor\n\t                Link prediction scores for edges\n\t        '''\n\t        if self.decoder_type is None:\n\t            h = uh * vh\n\t            if len(self.decoder_layers) > 0:\n\t                for _, layer in enumerate(self.decoder_layers):\n\t                    h = layer(h)\n\t                score = h.squeeze(-1)\n\t            else:\n", "                score = h.sum(-1)\n\t            return score\n\t        else:\n\t            return self.decoder_module.decoder_mat(uh, vh, etypes)\n\t    def forward(self, blocks, features, graph, neg_graph=None):\n\t        '''\n\t        Run forward pass (encoder + decoder) of the model. \n\t        Parameters:\n\t        ----------\n\t            blocks : list of dgl.DGLGraph\n", "                List of graphs (or blocks) for message passing on each GNN layer.\n\t                The length of the list should be equal to the number of GNN layers.\n\t            features : torch.Tensor\n\t                Node features\n\t            graph : dgl.DGLGraph\n\t                Graph with edges as (positive) link prediction targets\n\t            neg_graph : dgl.DGLGraph, optional\n\t                Graph with edges as negative link prediction targets\n\t        Returns: see function `decoder`.\n\t        '''\n", "        z = self.encoder(blocks, features)\n\t        adj_rec = self.decoder(z, graph, neg_graph)\n\t        return adj_rec\n\t    def loss_fn(self, pos_graph, neg_graph, pos_logits, neg_logits):\n\t        '''\n\t        Cross-entropy loss for link prediction scores.\n\t        To consider additional loss terms, override this function in child classes.\n\t        Parameters:\n\t        ----------\n\t            pos_graph : dgl.DGLGraph\n", "                Graph with edges as positive link prediction targets\n\t            neg_graph : dgl.DGLGraph\n\t                Graph with edges as negative link prediction targets\n\t            pos_logits : torch.Tensor\n\t                Link prediction scores for edges in `pos_graph`\n\t            neg_logits : torch.Tensor\n\t                Link prediction scores for edges in `neg_graph`\n\t        Returns:\n\t        ----------\n\t            loss : torch.Tensor\n", "                Cross-entropy loss\n\t        '''\n\t        score = torch.cat([pos_logits, neg_logits])\n\t        label = torch.cat(\n\t            [torch.ones_like(pos_logits), torch.zeros_like(neg_logits)])\n\t        num_non_pos_edges = float(neg_graph.num_edges())\n\t        pos_weight = torch.tensor(num_non_pos_edges / pos_graph.num_edges())\n\t        loss = F.binary_cross_entropy_with_logits(\n\t            score, label.float(), reduction=\"mean\", pos_weight=pos_weight)\n\t        return loss\n", "    def get_MFG_nodes(self, graph: dgl.DGLGraph, eval_nodes: np.ndarray,\n\t                      depth: int = None, include_inputs=False):\n\t        '''\n\t        Get nodes in the message flow graph (MFG) for layer-wise inference during evaluation.\n\t        This is used to determine which nodes to compute embeddings for in a layer-wise inference.\n\t        Parameters:\n\t        ----------\n\t            graph : dgl.DGLGraph\n\t                Graph used for message passing during evaluation. \n\t            eval_nodes : np.ndarray\n", "                ID of nodes to evaluate on that needs to be embedded.\n\t            depth : int, optional\n\t                Number of GNN layers to derive the MFG.\n\t                Default: None, which considers all GNN layers.\n\t            include_inputs : bool, optional\n\t                Whether to derive the MFG for the input layer.\n\t                Default: False, since node features are always accessible in the input layer.\n\t        '''\n\t        if depth is None:\n\t            depth = len(self.graph_layer_ind)\n", "            if not include_inputs:\n\t                depth -= 1\n\t        node_list = [None] * depth + [eval_nodes]\n\t        pgs = tqdm.tqdm(\n\t            reversed(range(depth)),\n\t            desc=\"Calculate MFG Nodes\",\n\t            dynamic_ncols=True,\n\t            total=depth)\n\t        for i in pgs:\n\t            pgs.set_postfix(num_nodes=len(node_list[i + 1]))\n", "            if len(node_list[i + 1]) == graph.num_nodes():\n\t                # All nodes have already been in the MFG\n\t                node_list[i] = node_list[i + 1]\n\t                continue\n\t            s_graph = dgl.sampling.sample_neighbors(\n\t                graph, node_list[i + 1], -1,\n\t                copy_ndata=False, copy_edata=False\n\t            )\n\t            u, _ = s_graph.edges()\n\t            del s_graph\n", "            nodes = torch.cat((u, node_list[i + 1]))\n\t            node_list[i] = nodes.unique()\n\t        pgs.close()\n\t        return node_list\n\t    def cleanup_step(self):\n\t        '''\n\t        Model specific clean up function, which is called after each training step. \n\t        Default behavior is do nothing; should be overridden in child classes if needed.\n\t        '''\n\t    @property\n", "    def num_conv_layers(self) -> int:\n\t        '''\n\t        Get number of GNN layers in the model.\n\t        '''\n\t        return len(self.graph_layer_ind)\n\t    @cached_property\n\t    def num_model_params(self) -> int:\n\t        '''\n\t        Get number of parameters in the model.\n\t        '''\n", "        return sum([p.nelement() for p in self.parameters()])\n\tclass TensorTypeCast(torch.nn.Module):\n\t    '''\n\t    Cast tensor to a different data type.\n\t    '''\n\t    def __init__(self, dtype):\n\t        '''\n\t        Parameters:\n\t        ----------\n\t            dtype : torch.dtype\n", "                Data type to cast to.\n\t        '''\n\t        super().__init__()\n\t        self.dst_dtype = dtype\n\t    def forward(self, h):\n\t        return h.type(dtype=self.dst_dtype)"]}
{"filename": "src/models/__init__.py", "chunked_list": ["from . import GraphSAGE, GCN, MLP, RGCN\n\t# Defines the corresponding model class for each model name used in `params.yaml`.\n\tmodelDict = {\n\t    \"GraphSAGE\": GraphSAGE.SAGE,\n\t    \"GCN\": GCN.GCN,\n\t    \"RGCN\": RGCN.RGCN,\n\t    \"ClusterGCN\": GCN.GCN,\n\t    \"ClusterSAGE\": GraphSAGE.SAGE,\n\t    \"MLP\": MLP.MLP\n\t}"]}
{"filename": "src/models/DistMult.py", "chunked_list": ["'''\n\tDistMult decoder implemented with DGL\n\tReferences:\n\t- Embedding Entities and Relations for Learning and Inference in Knowledge Bases\n\t- Paper: https://arxiv.org/abs/1412.6575\n\t'''\n\timport torch\n\timport torch.nn as nn\n\timport dgl\n\timport dgl.function as fn\n", "class DistMultDecoder(nn.Module):\n\t    def __init__(self, n_relations: int, hidden_dims: int, \n\t                 device, gamma=40.0, dtype=torch.float32) -> None:\n\t        '''\n\t        Initialize DistMult decoder.\n\t        Parameters:\n\t        -----------\n\t        n_relations: int\n\t            Number of relations for edges in the graph\n\t        hidden_dims: int\n", "            Dimension of DistMult embeddings\n\t        device: torch.device\n\t            Device to run the model\n\t        gamma: float, optional\n\t            Amplitude of the embedding initialization, by default 40.0\n\t        dtype: torch.dtype, optional\n\t            Data type of the model, by default torch.float32\n\t        '''\n\t        super().__init__()\n\t        self.n_relations = n_relations\n", "        self.hidden_dims = hidden_dims\n\t        self.gamma = gamma\n\t        self.device = device\n\t        self._w_relation = nn.Embedding(\n\t            self.n_relations, self.hidden_dims, \n\t            device=self.device, dtype=dtype)\n\t        self.trained_rels = torch.zeros(self.n_relations, \n\t            device=self.device, dtype=torch.float32)\n\t        emb_init = self.gamma / self.hidden_dims\n\t        nn.init.uniform_(self._w_relation.weight, -emb_init, emb_init)\n", "    def decoder(self, z, graph, neg_graph=None):\n\t        '''\n\t        Overrides `BaseLinkEncoderDecoder.decoder`.\n\t        '''\n\t        with graph.local_scope():\n\t            graph.ndata['h'] = z\n\t            graph.apply_edges(fn.u_mul_v('h', 'h', 'dot_prod'))\n\t            etypes = graph.edata[dgl.ETYPE]\n\t            check_mask = torch.zeros_like(etypes, dtype=torch.bool)\n\t            for etype_id in torch.arange(self.n_relations, device=self.device):\n", "                rel_embedding = self._w_relation(etype_id)\n\t                rel_mask = (etypes == etype_id)\n\t                graph.edata[\"dot_prod\"][rel_mask] *= rel_embedding\n\t                check_mask[rel_mask] = True\n\t                self.trained_rels[etype_id] += rel_mask.sum()\n\t            assert check_mask.all()\n\t            score = graph.edata[\"dot_prod\"].sum(-1, keepdims=True)\n\t        if neg_graph is not None:\n\t            neg_score = self.decoder(z, neg_graph)\n\t            return score, neg_score\n", "        else:\n\t            return score\n\t    def decoder_mat(self, uh, vh, etypes):\n\t        '''\n\t        Overrides `BaseLinkEncoderDecoder.decoder_mat`.\n\t        '''\n\t        h = uh * vh\n\t        check_mask = torch.zeros_like(etypes, dtype=torch.bool)\n\t        for etype_id in torch.arange(self.n_relations, device=self.device):\n\t            rel_embedding = self._w_relation(etype_id)\n", "            rel_mask = (etypes == etype_id)\n\t            h[rel_mask] *= rel_embedding\n\t            check_mask[rel_mask] = True\n\t        assert check_mask.all()\n\t        score = h.sum(-1)\n\t        return score\n"]}
