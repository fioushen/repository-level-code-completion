{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\tsetup(\n\t    name='nlpeer',\n\t    version='0.1',\n\t    author=\"Ubiquitous Knowledge Processing Lab\",\n\t    author_email=\"nils.dycke@tu-darmstadt.de\",\n\t    description=\"Code utilities for loading NLPeer data and for running experiments.\",\n\t    long_description=\"README.md\",\n\t    packages=find_packages()\n\t)"]}
{"filename": "nlpeer/__init__.py", "chunked_list": ["import json\n\timport logging\n\timport os\n\timport re\n\tfrom enum import Enum\n\tfrom os.path import join as pjoin\n\tfrom typing import Callable\n\timport numpy as np\n\tfrom intertext_graph import IntertextDocument\n\tfrom nlpeer.utils import list_dirs\n", "NTYPE_TITLE = \"title\"\n\tNTYPE_HEADING = \"heading\"\n\tNTYPE_PARAGRAPH = \"paragraph\"\n\tNTYPE_ABSTRACT = \"abstract\"\n\tNTYPE_LIST = \"list\"\n\tNTYPE_LIST_ITEM = \"list_item\"\n\tNYTPE_ELEMENT_REFERENCE = \"elem_reference\"\n\tNTYPE_BIB_REFERENCE = \"bib_reference\"\n\tNTYPE_HEADNOTE = \"headnote\"\n\tNTYPE_FOOTNOTE = \"footnote\"\n", "NTYPE_FIGURE = \"figure\"\n\tNTYPE_TABLE = \"table\"\n\tNTYPE_FORMULA = \"formula\"\n\tNTYPE_MEDIA = \"media\"\n\tNTYPE_BIB_ITEM = \"bib_item\"\n\tNTYPES = []\n\tNTYPES += [NTYPE_TITLE]\n\tNTYPES += [NTYPE_HEADING]\n\tNTYPES += [NTYPE_PARAGRAPH]\n\tNTYPES += [NTYPE_ABSTRACT]\n", "NTYPES += [NTYPE_LIST]\n\tNTYPES += [NTYPE_LIST_ITEM]\n\tNTYPES += [NYTPE_ELEMENT_REFERENCE]\n\tNTYPES += [NTYPE_BIB_REFERENCE]\n\tNTYPES += [NTYPE_HEADNOTE]\n\tNTYPES += [NTYPE_FOOTNOTE]\n\tNTYPES += [NTYPE_FIGURE]\n\tNTYPES += [NTYPE_TABLE]\n\tNTYPES += [NTYPE_FORMULA]\n\tNTYPES += [NTYPE_MEDIA]\n", "NTYPES += [NTYPE_BIB_ITEM]\n\tclass DATASETS(Enum):\n\t    ARR22 = \"ARR-22\"\n\t    F1000 = \"F1000\"\n\t    ACL17 = \"PeerRead-ACL2017\"\n\t    CONLL16 = \"PeerRead-CONLL2016\"\n\t    COLING20 = \"COLING2020\"\n\tclass PAPERFORMATS(Enum):\n\t    PDF = \".pdf\"\n\t    ITG = \".itg.json\"\n", "    GROBID = \".tei\"\n\t    XML = \".xml\"\n\t    TEX = \".tex\"\n\tclass ANNOTATION_TYPES(Enum):\n\t    ELINKS = \"elinks\", lambda pdir, ver: pjoin(pdir, f\"v{ver}\", \"elinks.json\")\n\t    ILINKS_RD = \"rd_ilinks\", lambda pdir, ver: pjoin(pdir, f\"v{ver}\", \"rd_ilinks.json\")\n\t    ELINKS_RD = \"rd_elinks\", lambda pdir, ver: pjoin(pdir, f\"v{ver}\", \"rd_elinks.json\")\n\t    DIFFS = \"diff\", lambda pdir, ver: pjoin(pdir, f\"diff_{ver}_{ver+1}.json\")\n\t    PRAG = \"rd_review_pragmatics\", lambda pdir, ver: pjoin(pdir, f\"v{ver}\", \"rd_review_pragmatics.json\")\n\tDATASET_PAPERFORMATS = {\n", "    DATASETS.ARR22: [PAPERFORMATS.PDF, PAPERFORMATS.ITG, PAPERFORMATS.GROBID],\n\t    DATASETS.F1000: [PAPERFORMATS.PDF, PAPERFORMATS.ITG, PAPERFORMATS.XML],\n\t    DATASETS.ACL17: [PAPERFORMATS.PDF, PAPERFORMATS.ITG, PAPERFORMATS.GROBID],\n\t    DATASETS.CONLL16: [PAPERFORMATS.PDF, PAPERFORMATS.ITG, PAPERFORMATS.GROBID],\n\t    DATASETS.COLING20: [PAPERFORMATS.PDF, PAPERFORMATS.ITG, PAPERFORMATS.GROBID, PAPERFORMATS.TEX]\n\t}\n\tdef arr_overall_to_score(oscore):\n\t    match = re.match(\"^\\d+\\.?\\d* ?=?\", oscore)\n\t    return float(oscore[0:match.span()[1] - 1].strip())\n\tdef f1000_overall_to_score(oscore):\n", "    if oscore == \"approve\":\n\t        return 2\n\t    elif oscore == \"approve-with-reservations\":\n\t        return 1\n\t    elif oscore == \"reject\":\n\t        return 0\n\t    else:\n\t        raise ValueError(\"Passed overall score does not exist in the F1000 dataset\")\n\tDATASET_REVIEW_OVERALL_SCALES = {\n\t    DATASETS.ARR22: (arr_overall_to_score, np.arange(1, 5 + 0.5, 0.5)),\n", "    DATASETS.F1000: (f1000_overall_to_score, np.arange(0, 3, 1)),\n\t    DATASETS.ACL17: (lambda x: int(x), np.arange(1, 5+1, 1)),\n\t    DATASETS.CONLL16: (lambda x: int(x), np.arange(1, 5+1, 1)),\n\t    DATASETS.COLING20: (lambda x: int(x), np.arange(1, 5+1, 1))\n\t}\n\tclass PaperReviewDataset:\n\t    \"\"\"\n\t    Dataset of papers and reviews indexed by paper IDs.\n\t    \"\"\"\n\t    datapath = None\n", "    _dataset_type = None\n\t    paper_ids = []\n\t    paper_dirs = {}\n\t    index = 0\n\t    _version = None\n\t    cache = None\n\t    def __init__(self, base_path: str,\n\t                 dataset: DATASETS,\n\t                 version: int,\n\t                 paper_format:PAPERFORMATS=PAPERFORMATS.ITG,\n", "                 hold_in_memory:bool=True,\n\t                 preload:bool=False):\n\t        # get path\n\t        datapath = pjoin(base_path, dataset.value, \"data\")\n\t        assert os.path.exists(datapath), f\"The passed dataset does not exist in the given directory {datapath}\"\n\t        self.datapath = datapath\n\t        self._dataset_type = dataset\n\t        self._version = version\n\t        # warn on memory\n\t        assert not preload or hold_in_memory, \"Invalid configuration. Can only preload if holding in memory\"\n", "        if preload:\n\t            logging.info(f\"You are loading {dataset.name} completely into memory. For large datasets this is \"\n\t                         f\"discouraged.\")\n\t        # setup loader\n\t        self._setup(hold_in_memory, preload, version, paper_format)\n\t    @property\n\t    def dataset_type(self):\n\t        return self._dataset_type\n\t    @dataset_type.setter\n\t    def dataset_type(self, dataset_type):\n", "        self._dataset_type = dataset_type\n\t    @property\n\t    def version(self):\n\t        return self._version\n\t    @version.setter\n\t    def version(self, version):\n\t        self._version = version\n\t    def ids(self):\n\t        return [pid for pid in self.paper_ids]\n\t    def file_path(self):\n", "        return os.path.sep.join(self.datapath.split(os.path.sep)[:-2])\n\t    @staticmethod\n\t    def _get_version_dir(pdir, version):\n\t        vdir = pjoin(pdir, f\"v{version}\")\n\t        return vdir if os.path.exists(vdir) else None\n\t    def load(self, paper_dir):\n\t        paper_id = paper_dir.split(os.path.sep)[-2]\n\t        # get metadata\n\t        meta_path = pjoin(paper_dir, \"meta.json\")\n\t        assert os.path.exists(meta_path), f\"Path to paper metadata does not exist: {meta_path}\"\n", "        with open(meta_path, \"r\") as fp:\n\t            paper_meta = json.load(fp)\n\t        # get doc\n\t        if self.paper_format == PAPERFORMATS.ITG:\n\t            itg_path = pjoin(paper_dir, f\"paper{self.paper_format.value}\")\n\t            assert os.path.exists(itg_path), f\"Path to the ITG version of the paper does not exist: {itg_path}\"\n\t            with open(itg_path, \"r\") as fp:\n\t                paper = IntertextDocument.load_json(fp)\n\t        else:\n\t            raise ValueError(f\"Paper format {self.paper_format} is currently not supported for loading!\")\n", "        # get reviews\n\t        review_path = pjoin(paper_dir, \"reviews.json\")\n\t        assert os.path.exists(review_path), f\"Path to the reviews of the paper do not exist: {review_path}\"\n\t        with open(review_path, \"r\") as fp:\n\t            reviews = json.load(fp)\n\t        return paper_id, paper_meta, paper, reviews\n\t    def _setup(self, hold_in_memory, preload, version, paper_format):\n\t        paper_dirs = map(lambda x: PaperReviewDataset._get_version_dir(x, version), list_dirs(self.datapath))\n\t        paper_dirs = filter(lambda x: x is not None, paper_dirs)\n\t        # fill management vars\n", "        self.paper_dirs = dict([(pd.split(os.path.sep)[-2], pd) for pd in paper_dirs])\n\t        self.paper_ids = list(self.paper_dirs.keys())\n\t        self.paper_format = paper_format\n\t        self.cache = None\n\t        self.index = 0\n\t        # if all data should be loaded at once and held in memory, do so\n\t        if hold_in_memory:\n\t            if preload:\n\t                self.cache = {pid: self.load(pd) for pid, pd in paper_dirs}\n\t            else:\n", "                self.cache = {}\n\t    def __len__(self):\n\t        return len(self.paper_ids)\n\t    def __getitem__(self, idx):\n\t        if type(idx) == str:\n\t            assert idx in self.paper_ids, \"Passed paper ID string is not part of this dataset\"\n\t            pid = idx\n\t        elif type(idx) == int:\n\t            assert 0 <= idx < len(self.paper_ids), f\"Passed paper index {idx} is out of range\"\n\t            pid = self.paper_ids[idx]\n", "        elif type(idx) == list:\n\t            return [self[lx] for lx in idx]\n\t        else:\n\t            raise TypeError(f\"The passed index type {type(idx)} is not supported for this dataset\")\n\t        # with cached data\n\t        if self.cache is not None:\n\t            if pid in self.cache:\n\t                return self.cache[pid]\n\t            else:\n\t                self.cache[pid] = self.load(self.paper_dirs[pid])\n", "                return self.cache[pid]\n\t        # regular loading\n\t        return self.load(self.paper_dirs[pid])\n\t    def __next__(self):\n\t        self.index += 1\n\t        if self.index-1 >= len(self):\n\t            raise StopIteration()\n\t        return self[self.index-1]\n\t    def __iter__(self):\n\t        while True:\n", "            try:\n\t                yield next(self)\n\t            except StopIteration:\n\t                break\n\t        self.index = 0\n\t    def filter_paperwise(self, paperwise_filter: Callable):\n\t        matched_paper_ids = [paper[0] for paper in self if paperwise_filter(paper)]\n\t        non_matched_paper_ids = [pid for pid in self.paper_ids if pid not in matched_paper_ids]\n\t        self.paper_ids = matched_paper_ids\n\t        for nm in non_matched_paper_ids:\n", "            del self.paper_dirs[nm]\n\t            if self.cache is not None and nm in self.cache:\n\t                del self.cache[nm]\n\t        # just to be sure, reset as the index is invalid as of now\n\t        self.index = 0\n\tclass ReviewPaperDataset(PaperReviewDataset):\n\t    \"\"\"\n\t    Dataset of papers and reviews indexed by review IDs.\n\t    \"\"\"\n\t    review_ids = []\n", "    paperwise_review_ids = {}\n\t    force_hold_in_memory = False\n\t    def __init__(self, base_path: str, dataset: DATASETS, version: int, paper_format:PAPERFORMATS=PAPERFORMATS.ITG, hold_in_memory:bool=False, preload:bool=False):\n\t        # hold data in memory by default, but add an own cleaning strategy while loading by index\n\t        self.force_hold_in_memory = hold_in_memory\n\t        super().__init__(base_path, dataset, version, paper_format, True, preload)\n\t    def _setup(self, hold_in_memory, preload, version, paper_format):\n\t        # load file pointers as in the paper review dataset\n\t        super()._setup(hold_in_memory, preload, version, paper_format)\n\t        assert all(\"%\" not in pid for pid in self.paper_ids), \"There exist malformed paper ids. % is not permitted\"\n", "        # index the reviews\n\t        self.review_ids = []\n\t        self.paperwise_review_ids = {}\n\t        for pid in self.paper_ids:\n\t            pdir = self.paper_dirs[pid]\n\t            rfile = pjoin(pdir, \"reviews.json\")\n\t            with open(rfile, \"r\") as rfs:\n\t                new_rids = [f\"{pid}%{str(i)}\" for i in range(len(json.load(rfs)))]\n\t                self.paperwise_review_ids[pid] = self.paperwise_review_ids.get(pid, []) \\\n\t                                                 + [len(self.review_ids) + rid for rid in range(len(new_rids))]\n", "                self.review_ids += new_rids\n\t    def ids(self):\n\t        return [pid for pid in self.review_ids]\n\t    @staticmethod\n\t    def _review_wise_sample(data, review_num):\n\t        paper_id, paper_meta, paper, reviews = data\n\t        return paper_id, paper_meta, paper, review_num, reviews[review_num]\n\t    def __len__(self):\n\t        # override by review ids' length\n\t        return len(self.review_ids)\n", "    def __getitem__(self, idx):\n\t        # override to fetch by index on reviews\n\t        if type(idx) == str:\n\t            assert idx in self.review_ids, \"Passed review ID string is not part of this dataset\"\n\t            assert idx.count(\"%\") == 1, \"Passed review ID string is malformed. % is only allowed as a separator.\"\n\t            rid = idx\n\t        elif type(idx) == int:\n\t            assert 0 <= idx < len(self.review_ids), f\"Passed review index {idx} is out of range\"\n\t            rid = self.review_ids[idx]\n\t        elif type(idx) == list:\n", "            return [self[lx] for lx in idx]\n\t        else:\n\t            raise TypeError(f\"The passed index type {type(idx)} is not supported for this dataset\")\n\t        # determine pid and review number\n\t        pid, rnum = rid.split(\"%\")[0], int(rid.split(\"%\")[1])\n\t        # with cached data (always the case)\n\t        if pid in self.cache:\n\t            res = self._review_wise_sample(self.cache[pid], rnum)\n\t        else:\n\t            self.cache[pid] = self.load(self.paper_dirs[pid])\n", "            res = self._review_wise_sample(self.cache[pid], rnum)\n\t        # cache cleaning strategy: discard loaded papers if the rnum is the last one\n\t        if not self.force_hold_in_memory and rnum == len(self.cache[pid][3]):\n\t            del self.cache[pid]\n\t        # return result\n\t        return res\n\t    def filter_paperwise(self, paperwise_filter: Callable):\n\t        # filter paper ids, such that paper_ids only old the still permitted ones\n\t        # cache is cleared, too\n\t        matched_paper_ids, non_matched_paper_ids = [], []\n", "        for pid in self.paper_ids:\n\t            paper = super().__getitem__(pid)\n\t            if paperwise_filter(paper):\n\t                matched_paper_ids += [pid]\n\t            else:\n\t                non_matched_paper_ids += [pid]\n\t        self.paper_ids = matched_paper_ids\n\t        for nm in non_matched_paper_ids:\n\t            del self.paper_dirs[nm]\n\t            if self.cache is not None and nm in self.cache:\n", "                del self.cache[nm]\n\t        # get review ids to remove\n\t        reviewids_to_remove = [r for pid, revs in self.paperwise_review_ids.items() if pid in non_matched_paper_ids\n\t                                 for r in revs ]\n\t        for pid in non_matched_paper_ids:\n\t            del self.paperwise_review_ids[pid]\n\t        for rid in reviewids_to_remove:\n\t            self.review_ids.remove(rid)\n\t        # just to make sure, in case someone is iterating over this (index would be wrong now)\n\t        self.index = 0\n", "class PaperReviewAnnotations:\n\t    \"\"\"\n\t    Extends a given paper review dataset by annotations according to the specified annotaiton type.\n\t    \"\"\"\n\t    cache = None\n\t    def __init__(self, annotation_type: ANNOTATION_TYPES, dataset: PaperReviewDataset):\n\t        base_path = dataset.file_path()\n\t        annopath = pjoin(base_path, dataset.dataset_type.value, \"annotations\")\n\t        assert os.path.exists(annopath), f\"The passed dataset does not exist in the given directory {annopath}\"\n\t        self.annopath = annopath\n", "        self._annotation_type = annotation_type\n\t        assert type(dataset) != ReviewPaperDataset, \"ReviewPaperDatasets not supported.\"\n\t        self.dataset = dataset\n\t        # setup loader\n\t        self._setup()\n\t        self.index = 0\n\t    def _setup(self):\n\t        version = self.dataset.version\n\t        anno_paper_dirs = list_dirs(self.annopath)\n\t        anno_paper_dirs = filter(lambda x: os.path.exists(self._annotation_type.value[1](x, version)), anno_paper_dirs)\n", "        # fill management vars\n\t        self.anno_paper_dirs = dict([(os.path.basename(pd), pd) for pd in anno_paper_dirs])\n\t        self.anno_pids = list(self.anno_paper_dirs.keys())\n\t        self._extract_annotations = lambda x: self._annotation_type.value[1](x, version)\n\t        self.cache = {}\n\t    def ids(self):\n\t        return self.anno_pids\n\t    def __len__(self):\n\t        return len(self.anno_paper_dirs)\n\t    def __getitem__(self, idx):\n", "        if type(idx) == str:\n\t            assert idx in self.anno_pids, \"Passed paper ID string is not part of this dataset\"\n\t            pid = idx\n\t        elif type(idx) == int:\n\t            assert 0 <= idx < len(self.anno_pids), f\"Passed paper index {idx} is out of range\"\n\t            pid = self.anno_pids[idx]\n\t        elif type(idx) == list:\n\t            return [self[lx] for lx in idx]\n\t        else:\n\t            raise TypeError(f\"The passed index type {type(idx)} is not supported for this dataset\")\n", "        d = self.dataset[pid]\n\t        # get annotation\n\t        if pid not in self.cache:\n\t            anno_path = self._extract_annotations(self.anno_paper_dirs[pid])\n\t            with open(anno_path, \"r\") as jf:\n\t                self.cache[pid] = json.load(jf)\n\t        return d, self.cache[pid]\n\t    def __next__(self):\n\t        self.index += 1\n\t        if self.index-1 >= len(self):\n", "            raise StopIteration()\n\t        return self[self.index-1]\n\t    def __iter__(self):\n\t        while True:\n\t            try:\n\t                yield next(self)\n\t            except StopIteration:\n\t                break\n\t        self.index = 0\n"]}
{"filename": "nlpeer/utils.py", "chunked_list": ["from os import walk, path\n\tdef list_files(dir_path, level=1):\n\t    for e in walk(dir_path):\n\t        level -= 1\n\t        if level > 0:\n\t            continue\n\t        files = [path.join(path.abspath(e[0]), f) for f in e[2]]\n\t        for f in files:\n\t            yield f\n\t        break\n", "def list_dirs(dir_path, level=1):\n\t    for e in walk(dir_path):\n\t        level -= 1\n\t        if level > 0:\n\t            continue\n\t        files = [path.join(path.abspath(e[0]), f) for f in e[1]]\n\t        for f in files:\n\t            yield f\n\t        break\n"]}
{"filename": "nlpeer/data/__init__.py", "chunked_list": ["import json\n\tfrom collections import Counter\n\tfrom typing import Callable, Collection, List\n\timport numpy as np\n\timport sklearn.utils\n\tfrom sklearn.model_selection import train_test_split\n\tfrom nlpeer import DATASETS, PaperReviewDataset, ReviewPaperDataset\n\tfrom nlpeer.utils import list_dirs\n\tdef filter_gte_x_reviews(dataset: PaperReviewDataset, x: int):\n\t    \"\"\"\n", "    Discards papers with less than x reviews.\n\t    :param dataset: dataset to be filtered\n\t    :param x: the threshold of min number of reviews\n\t    :return: void\n\t    \"\"\"\n\t    def gte0_reviews(paper_data):\n\t        paper_id, paper_meta, paper, reviews = paper_data\n\t        return len(reviews) >= x\n\t    dataset.filter_paperwise(gte0_reviews)\n\tdef load_splits_from_file(file_path: str, dataset_type: DATASETS = None):\n", "    \"\"\"\n\t    For a given dataset type and file path, loads the splits from file as lists of indexes.\n\t    :param file_path: path to the split file\n\t    :param dataset_type: type of the dataset to verify the split matches\n\t    :return: the list of splits\n\t    \"\"\"\n\t    with open(file_path, \"r\") as file:\n\t        split_data = json.load(file)\n\t    if dataset_type is not None:\n\t        assert split_data[\"dataset_type\"] == dataset_type.name, \\\n", "            f\"Mismatch of dataset types during loading. Expected {dataset_type}\"\n\t    return split_data[\"splits\"]\n\tdef store_splits_to_file(dataset: PaperReviewDataset, splits: Collection, out_path: str, random_gen):\n\t    \"\"\"\n\t    Stores the given splits for the given dataset to disk.\n\t    :param dataset: the dataset split by the given splits\n\t    :param splits: the list of split indexes\n\t    :param out_path: the output path to store the split\n\t    :param random_gen: random seed to store along\n\t    :return:\n", "    \"\"\"\n\t    jsplits = {\n\t        \"dataset_type\": dataset.dataset_type.name,\n\t        \"dataset\": dataset.__class__.__name__,\n\t        \"splits\": [\n\t            [[s, dataset.ids()[s]] for s in split] for split in splits\n\t        ],\n\t        \"random\": str(random_gen)\n\t    }\n\t    with open(out_path, \"w+\") as file:\n", "        json.dump(jsplits, file)\n\tdef paperwise_random_split(dataset: PaperReviewDataset, splits: List[float], random_seed: int):\n\t    \"\"\"\n\t    Splits the given dataset by papers with the given split proportions.\n\t    Automatically splits considering the distribution of reviews per paper for stratification.\n\t    :param dataset: dataset to be split\n\t    :param splits: the split proportions as a list of floats\n\t    :param random_seed: random seed for shuffling\n\t    :return: the splits\n\t    \"\"\"\n", "    # handle review paper datasets\n\t    if type(dataset) == ReviewPaperDataset:\n\t        reviews_per_paper = list(dataset.paperwise_review_ids.items())\n\t        def strat_criterion(rpp):\n\t            return len(rpp[1])\n\t        split_indices = paperwise_stratified_split(reviews_per_paper,\n\t                                                   splits,\n\t                                                   stratification_criterion=strat_criterion,\n\t                                                   random_seed=random_seed)\n\t        rid_splits = []\n", "        for split_i in split_indices:\n\t            covered_papers = [reviews_per_paper[i] for i in split_i]\n\t            rid_splits += [[ridx for pid, ridxs in covered_papers for ridx in ridxs]]\n\t        return rid_splits\n\t    # handle paper review datasets (split by paper proportions, that's it; ignore review proportions)\n\t    else:\n\t        # for consistency of randomness, we use the same method but without stratification\n\t        split_indices = paperwise_stratified_split(dataset, splits, stratification_criterion=None, random_seed=random_seed)\n\t        return [s.tolist() for s in split_indices]\n\tdef paperwise_stratified_split(dataset: Collection, splits: List[float], stratification_criterion: Callable, random_seed: int):\n", "    \"\"\"\n\t    Splits the given dataset by a stratification criterion given as function on papers.\n\t    :param dataset: the dataset to be split\n\t    :param splits: the split proportions\n\t    :param stratification_criterion: the callable stratification criterion mapping to discrete values\n\t    :param random_seed: random seed for shuffling\n\t    :return: the splits\n\t    \"\"\"\n\t    assert np.round(sum(splits), 1) == 1.0, f\"Split sizes need to add evenly to 1.0. Given sum: {sum(splits)}\"\n\t    # get \"class label\" for stratification\n", "    indices = np.arange(0, len(dataset))\n\t    if stratification_criterion is None:\n\t        strat_labels = np.array([\"nostrat\" for i in indices])\n\t    else:\n\t        strat_labels = np.array([stratification_criterion(s) for s in dataset])\n\t    # prep iterative splitting\n\t    out = []\n\t    to_split = (indices, strat_labels, 0)\n\t    split_further = None\n\t    # perform sequence of splits (until the last one, which is implied)\n", "    for original_split_size in splits[:-1]:\n\t        ixs, lbls, split_sofar = to_split\n\t        split_size = original_split_size / (1-split_sofar) if split_sofar > 0 else original_split_size\n\t        # lbl occurs only once: prohibited by scipy -- remove beforehand\n\t        single_label_instances = []\n\t        for l, c in Counter(lbls).items():\n\t            if c > 1:\n\t                continue\n\t            instances = lbls == l\n\t            single_label_instances += [ixs[instances][0]]\n", "            ixs = ixs[np.logical_not(instances)]\n\t            lbls = lbls[np.logical_not(instances)]\n\t        bi_split = train_test_split(ixs, test_size=split_size, stratify=lbls, random_state=random_seed)\n\t        split_further, split_done = bi_split[0], bi_split[1]\n\t        # add at random to one of the splits\n\t        if len(single_label_instances) > 0:\n\t            split_i = round(split_size*len(single_label_instances))\n\t            single_label_instances = sklearn.utils.shuffle(single_label_instances, random_state=random_seed)\n\t            split_further = np.append(split_further, single_label_instances[:split_i]).astype(int)\n\t            split_done = np.append(split_done, single_label_instances[split_i:]).astype(int)\n", "        out += [split_done]\n\t        to_split = (split_further, strat_labels[split_further], split_sofar + original_split_size)\n\t    # add left-over split\n\t    out += [split_further]\n\t    rout = []\n\t    for s in out:\n\t        rout += [sklearn.utils.shuffle(s)]\n\t    return rout\n"]}
{"filename": "nlpeer/data/create/__init__.py", "chunked_list": []}
{"filename": "nlpeer/data/create/clean.py", "chunked_list": ["import os\n\timport re\n\timport logging\n\tfrom io import BytesIO\n\timport PyPDF2\n\tfrom PyPDF2 import PdfFileReader, PdfFileWriter\n\tfrom PyPDF2.generic import ContentStream, TextStringObject, NameObject, NumberObject\n\tline_num_matcher = re.compile(\"^\\d{3,4}$\")\n\tdef clean_pdf_draft(pdf_path, with_meta_data=False):\n\t    line_accu = 0\n", "    def _is_line_number(operands):\n\t        global line_num_matcher\n\t        nonlocal line_accu\n\t        text = operands[0][0]\n\t        if not isinstance(text, TextStringObject):\n\t            return False\n\t        if not line_num_matcher.match(text):\n\t            return False\n\t        lnum = int(text)\n\t        is_incremental = lnum == line_accu + 1\n", "        is_first = lnum == 0 and line_accu == 0\n\t        if not is_incremental and not is_first:\n\t            return False\n\t        line_accu = lnum\n\t        return True\n\t    def _is_author_information(operands):\n\t        if len(operands) != 1:\n\t            return False\n\t        return \"\".join([str(o) for o in operands[0] if isinstance(o, TextStringObject)]) == \"AnonymousACLsubmission\"\n\t    def _parse_lines(lines):\n", "        res = []\n\t        for line in lines:\n\t            line_number = str(line[0][0][0])\n\t            contents = [lii for l in line[1] for li in l for lii in li]\n\t            parsed = \"\"\n\t            for c in contents:\n\t                if type(c) == TextStringObject:\n\t                    parsed += str(c)\n\t                elif type(c) == NumberObject and int(c) < 0:\n\t                    parsed += \" \"\n", "            res += [(line_number, parsed)]\n\t        return res\n\t    out_dir = pdf_path[:-len(pdf_path.split(os.path.sep)[-1])]\n\t    out_path = os.path.join(out_dir, \"clean.pdf\")\n\t    try:\n\t        pdf_reader = PdfFileReader(pdf_path)\n\t    except PyPDF2.errors.PdfReadError as err:\n\t        if \"PDF starts with\" in str(err):\n\t            logging.info(\"Invalid PDF header format -- seeking a point of entry\")\n\t            with open(pdf_path, \"rb\") as bf:\n", "                b = bf.read(1024)\n\t                index = b.find(b\"%PDF-\")\n\t                if index == -1:\n\t                    raise err\n\t            with open(pdf_path, \"rb\") as bf:\n\t                bf.seek(index)\n\t                bstream = BytesIO(bf.read())\n\t            pdf_reader = PdfFileReader(bstream)\n\t        else:\n\t            raise err\n", "    pdf_writer = PdfFileWriter()\n\t    pages = []\n\t    lines = []\n\t    for page in pdf_reader.pages:\n\t        content_object = page[\"/Contents\"].getObject()\n\t        content = ContentStream(content_object, pdf_reader)\n\t        line_content = []\n\t        page_content = []\n\t        # Loop over all pdf elements\n\t        # source https://gist.github.com/668/2c8f936697ded94394ff4a6ffa4ae87e\n", "        to_delete = []\n\t        op_i = 0\n\t        first_line = True\n\t        for operands, operator in content.operations:\n\t            # You might adapt this part depending on your PDF file\n\t            # > Below the operators inferred from https://pypdf2.readthedocs.io/en/latest/_modules/PyPDF2/_page.html\n\t            # > Check PDF iso standard for actual and full list\n\t            #\n\t            # Tf = text font\n\t            # Tfs = text font size\n", "            # Tc = character spacing\n\t            # Th = horizontal scaling\n\t            # Tl = leading\n\t            # Tmode = text rendering mode\n\t            # Trise = text rise\n\t            # Tw = word spacing\n\t            # Tj, ', \", TJ = Text showing [the actual text]\n\t            # T* = text positioning\n\t            # ...\n\t            if operator == b\"TJ\":\n", "                if _is_line_number(operands):\n\t                    to_delete += [op_i]\n\t                    # first line of the page -- get the last entry in line_content as a content\n\t                    if first_line:\n\t                        page_content += [(operands, [line_content[-1]] if len(line_content) > 0 else [])]\n\t                        first_line = False\n\t                    else:\n\t                        page_content += [(operands, line_content)]\n\t                    line_content = []\n\t                elif _is_author_information(operands):\n", "                    to_delete += [op_i]\n\t                else:\n\t                    line_content += [operands]\n\t            op_i +=1\n\t        # delete elements from PDF\n\t        acc = 0\n\t        for d in sorted(to_delete):\n\t            del content.operations[d - acc]\n\t            acc += 1\n\t        # todo try setting the rendered pane to the covered subset in ACL pdfs to guarantee excluding of weird fields\n", "        #page.mediabox.upper_right = (page.mediabox.right / 2, page.mediabox.top / 2)\n\t        # Set the modified content as content object on the page\n\t        page.__setitem__(NameObject('/Contents'), content)\n\t        # parse the lines\n\t        page_content = _parse_lines(page_content)\n\t        # store page (start and end line) and lines\n\t        if len(page_content) > 0:\n\t            pages += [(page_content[0], page_content[-1])]\n\t            lines += page_content\n\t        # Add the page to the output\n", "        pdf_writer.addPage(page)\n\t    meta = {}\n\t    meta[\"num_pages\"] = len(pages)\n\t    meta[\"num_lines\"] = len(lines)\n\t    meta[\"lines\"] = lines\n\t    meta[\"pages\"] = pages\n\t    references_line = [l for l in lines if l[1].strip() == \"References\"]\n\t    if len(references_line) > 0 and len(references_line[0]) > 0:\n\t        meta[\"bib_page_index\"] = next(i for i, p in enumerate(pages) if int(p[0][0]) <= int(references_line[0][0]) <= int(p[1][0]))\n\t    else:\n", "        meta[\"bib_page_index\"] = None\n\t    # override document info to a default value\n\t    pdf_writer.add_metadata({\n\t        \"author\": \"Anonymous\",\n\t        \"author_raw\": \"Anonymous\"\n\t    })\n\t    # write to file\n\t    with open(out_path, \"wb\") as output_file:\n\t        pdf_writer.write(output_file)\n\t    # default warning for validation\n", "    if line_accu < 500:\n\t        logging.info(f\"WARNING: Encountered a very low number of lines ({line_accu} < 500) in {output_file}.\")\n\t    return meta"]}
{"filename": "nlpeer/data/create/utils.py", "chunked_list": ["import logging\n\timport os\n\timport re\n\timport shutil\n\tfrom os.path import join as pjoin\n\timport spacy\n\tfrom spacy import Language\n\tfrom spacy.symbols import  ORTH\n\tfrom intertext_graph.itsentsplitter import SpacySplitter\n\tclass PDFProcessingError(BaseException):\n", "    def __init__(self, cause, ex=None):\n\t        self.cause = cause\n\t        self.underlying_exception = ex\n\tdef create_output_data_path(benchmark_dir_path, dataset_name):\n\t    out_path = pjoin(benchmark_dir_path, dataset_name)\n\t    if not os.path.exists(out_path):\n\t        logging.info(f\"Creating {out_path}\")\n\t        os.mkdir(out_path)\n\t    # create out path data\n\t    out_path = pjoin(out_path, \"data\")\n", "    if os.path.exists(out_path):\n\t        logging.info(f\"WARNING: Deleting old data directory {out_path}\")\n\t        shutil.rmtree(out_path)\n\t    os.mkdir(out_path)\n\t    return out_path\n\tdef sentence_split_review(review, splitter=None):\n\t    if splitter is None:\n\t        splitter = SpacySplitter(spacy.load('en_core_sci_sm'))\n\t    res = {}\n\t    for field, text in review[\"report\"].items():\n", "        if text is not None:\n\t            res[field] = splitter.split(text)\n\t    review[\"meta\"][\"sentences\"] = res\n\t    return review\n\tdef get_review_sentences(review):\n\t    res = {}\n\t    for field, sents in review[\"meta\"][\"sentences\"].items():\n\t        txt = review[\"report\"][field]\n\t        res[field] = [txt[s[0]:s[1]] for s in sents]\n\t    return res\n", "@Language.component(\"linebreak_component\")\n\tdef _split_on_special_token(doc):\n\t    for token in doc[:-1]:\n\t        if token.text == \"<br>\":\n\t            doc[token.i + 1].is_sent_start = True\n\t    return doc\n\tdef clean_and_split_review(review):\n\t    # setup special splitting\n\t    def augmented_clean(txt):\n\t        res = txt.strip()  # strip unnecessary whitespaces\n", "        res = re.sub(r\"\\n{2,}\", \" <br> \", res)  # clear line break\n\t        res = re.sub(r\" <br> [*\\-] \", \" <br> - \", res)  # replaced line break with an itemize\n\t        res = re.sub(r\"\\n[*\\-] \", \" <br> - \", res)  # non-replaced line break with an itemize\n\t        res = re.sub(r\"^\\* \", \"- \", res)\n\t        return res\n\t    # splitter\n\t    nlp = spacy.load('en_core_sci_sm', exclude=[\"ner\", \"tagger\", \"parser\", \"lemmatizer\"])\n\t    nlp.add_pipe(\"sentencizer\")\n\t    nlp.tokenizer.add_special_case(\"<br>\", [{ORTH: \"<br>\"}])\n\t    nlp.add_pipe(\"linebreak_component\", name=\"linebreaking\", first=True)\n", "    sentences = {}\n\t    report = {}\n\t    for field, text in review[\"report\"].items():\n\t        if text is not None:\n\t            cleaned = augmented_clean(text)\n\t            processed = nlp(cleaned)\n\t            # get sentences and replace the <br> parts by actual line breaks\n\t            new_text = [s.text for s in processed.sents]\n\t            for i, t in enumerate(new_text):\n\t                if t == \"<br>\" and i > 0:\n", "                    new_text[i-1] = new_text[i-1] + \"\\n\"\n\t            # update sentences to exclude <br> ones and replace any br left in a sentence\n\t            new_text = [t.replace(\"<br>\", \"\") for t in new_text if t != \"<br>\"]\n\t            # if line breaks do not occur at the beginning or ending of a sentence, they should be discarded\n\t            tmp = []\n\t            for t in new_text:\n\t                if len(t) > 1:\n\t                    e = t[0]+ t[1:-1].replace(\"\\n\", \" \") + t[-1]\n\t                else:\n\t                    e = t\n", "                if e[-1] not in [\" \", \"\\n\", \"\\t\"]:\n\t                    e += \" \"\n\t                tmp += [e]\n\t            new_text = tmp\n\t            report[field] = \"\".join(new_text)\n\t            sentences[field] = []\n\t            ix = 0\n\t            for s in new_text:\n\t                sentences[field] += [(ix, ix+len(s))]\n\t                ix += len(s)\n", "    review[\"meta\"][\"sentences\"] = sentences\n\t    review[\"report\"] = report\n\t    return review"]}
{"filename": "nlpeer/data/create/parse.py", "chunked_list": ["import copy\n\timport logging\n\tfrom copy import deepcopy\n\tfrom typing import Any, Optional, Tuple, List, Dict\n\tfrom xml.etree import ElementTree\n\timport os.path\n\tfrom grobid_client.grobid_client import GrobidClient, ServerUnavailableException\n\tfrom intertext_graph.itgraph import IntertextDocument, Node, Edge, Etype, SpanNode\n\tfrom intertext_graph.itsentsplitter import SentenceSplitter, IntertextSentenceSplitter, make_sentence_nodes\n\tfrom intertext_graph.parsers.f1000_xml_parser import F1000XMLParser\n", "from intertext_graph.parsers.itparser import IntertextParser\n\t#\n\t# Constants\n\t#\n\t# GROBID\n\tfrom lxml import etree\n\tfrom nlpeer import NTYPES, NTYPE_TITLE, NTYPE_HEADING, NTYPE_PARAGRAPH, NTYPE_ABSTRACT, NTYPE_LIST, NTYPE_LIST_ITEM, \\\n\t    NYTPE_ELEMENT_REFERENCE, NTYPE_BIB_REFERENCE, NTYPE_HEADNOTE, NTYPE_FOOTNOTE, NTYPE_FIGURE, NTYPE_TABLE, \\\n\t    NTYPE_FORMULA, NTYPE_MEDIA, NTYPE_BIB_ITEM\n\tGROBID_CONF = {}\n", "GROBID_HOST = os.environ.get(\"GROBID_HOST\")\n\tif GROBID_HOST:\n\t    GROBID_CONF[\"grobid_server\"] = GROBID_HOST\n\tGROBID_PORT = os.environ.get(\"GROBID_PORT\")\n\tif GROBID_PORT:\n\t    GROBID_CONF[\"grobid_port\"] = GROBID_PORT\n\t# todo: currently the bibliography is not parsed as part of the text, but as isolated nodes. Might make sense to add!\n\t#\n\t# Classes\n\t#\n", "class TEIXMLParser(IntertextParser):\n\t    \"\"\"\n\t    Parser to transform a TEI XML document into an IntertextDocument.\n\t    Author: Nils Dycke\n\t    Co-author: Jan-Micha Bodensohn (scaffolding and base paragraph parser)\n\t    \"\"\"\n\t    def __init__(self, xml_file_path: str):\n\t        \"\"\"\n\t        Initialize the TEIXMLParser for a particular paper.\n\t        :param xml_file_path: filepath of the TEI XML file (GROBID output)\n", "        \"\"\"\n\t        super(TEIXMLParser, self).__init__(xml_file_path)\n\t        self._xml_file_path: str = xml_file_path\n\t    def __call__(self) -> IntertextDocument:\n\t        \"\"\"\n\t        Parse the TEI XML document into an IntertextDocument.\n\t        :return: the IntertextDocument\n\t        \"\"\"\n\t        return self._parse_document()\n\t    @classmethod\n", "    def _batch_func(cls, path: Any) -> Any:\n\t        raise NotImplementedError  # TODO: implement this\n\t    @staticmethod\n\t    def _parse_section_content(section, prefix):\n\t        sub_graph = IntertextDocument([], [], prefix)\n\t        graph_refs = []\n\t        figures = {}\n\t        # add artificial root node\n\t        root = Node(\"root\", ntype=NTYPE_TITLE)\n\t        sub_graph.add_node(root)\n", "        predecessor = root\n\t        list_parent = None\n\t        for child in section:\n\t            # add paragraphs\n\t            if child.tag == f\"{prefix}p\":\n\t                paragraph = child\n\t                # get text\n\t                paragraph_text, children_ix = TEIXMLParser._flatten_xml_element_with_child_ix(paragraph)\n\t                TEIXMLParser._check_text(paragraph_text, \"paragraph\")\n\t                # get references in text\n", "                reference_ixs = [(t, pre_ix, pre_ix + contl) for (t, pre_ix, contl) in children_ix if\n\t                                 t.tag == f\"{prefix}ref\"]\n\t                # ACL layout specific --> lists start with bulletpoints\n\t                if paragraph_text[0] == \"\\u2022\":\n\t                    if list_parent is None:\n\t                        list_parent = TEIXMLParser._add_node(sub_graph, \"\", NTYPE_LIST, predecessor=predecessor,\n\t                                                             parent=root)\n\t                        predecessor = list_parent\n\t                    li = TEIXMLParser._add_node(sub_graph, paragraph_text, NTYPE_LIST_ITEM, predecessor=predecessor,\n\t                                                parent=list_parent)\n", "                    predecessor = li\n\t                else:\n\t                    if list_parent is not None:\n\t                        # no list type paragraph any more: erase existing list parent\n\t                        list_parent = None\n\t                    p = TEIXMLParser._add_node(sub_graph, paragraph_text, NTYPE_PARAGRAPH, predecessor=predecessor,\n\t                                               parent=root)\n\t                    predecessor = p\n\t                if len(reference_ixs) > 0:\n\t                    graph_refs += [(predecessor, reference_ixs)]\n", "            elif child.tag == f\"{prefix}figure\":\n\t                fxid, figure = TEIXMLParser._parse_figure(sub_graph, child, prefix, predecessor, root)\n\t                predecessor = figure\n\t                figures[fxid] = figure\n\t            elif child.tag == f\"{prefix}head\":\n\t                # head -- ignoring them\n\t                pass\n\t            elif child.tag == f\"{prefix}formula\":\n\t                # get text\n\t                formula_text = TEIXMLParser._flatten_xml_element(child)\n", "                TEIXMLParser._check_text(formula_text, \"formula\")\n\t                p = TEIXMLParser._add_node(sub_graph, formula_text, NTYPE_FORMULA, predecessor=predecessor, parent=root)\n\t                predecessor = p\n\t            else:\n\t                # add the rest\n\t                logging.info(f\"UNKNOWN TAG {section.tag} within section -- adding as flattend version.\")\n\t                paragraph_text, children_ix = TEIXMLParser._flatten_xml_element_with_child_ix(child)\n\t                TEIXMLParser._check_text(paragraph_text, \"paragraph\")\n\t                # get references in text\n\t                reference_ixs = [(t, pre_ix, pre_ix + contl) for (t, pre_ix, contl) in children_ix if\n", "                                 t.tag == f\"{prefix}ref\"]\n\t                p = TEIXMLParser._add_node(sub_graph, paragraph_text, NTYPE_PARAGRAPH, predecessor=predecessor,\n\t                                           parent=root)\n\t                predecessor = p\n\t                if len(reference_ixs) > 0:\n\t                    graph_refs += [(predecessor, reference_ixs)]\n\t        return sub_graph, predecessor, graph_refs, figures\n\t    def _parse_abstract(self, doc, abstract, prefix, predecessor, article_title):\n\t        abstract_title = self._add_node(doc, \"Abstract\", NTYPE_ABSTRACT, predecessor=predecessor, parent=article_title)\n\t        predecessor = abstract_title\n", "        if abstract is None:\n\t            return abstract_title, predecessor\n\t        content = \"\"\n\t        merge = False\n\t        for child in abstract:\n\t            # get text (including any possible children)\n\t            text = self._flatten_xml_element(child)\n\t            self._check_text(text, \"abstract paragraph\")\n\t            content += text\n\t            if child.tag != f\"{prefix}p\":\n", "                # if non-paragraph, treat as hick-up by GROBID and append to content of next node and force merge\n\t                logging.info(f\"Unexpected paragraph tag '{child.tag}' in the abstract! Treating as in-line text.\")\n\t                merge = True\n\t                content += \" \"\n\t            elif merge and predecessor.ntype == NTYPE_PARAGRAPH:\n\t                # if a previous element was erroneously inserted, merge this paragraph with the previous one\n\t                predecessor.content += \" \" + text\n\t                merge = False\n\t                content = \"\"\n\t            else:\n", "                # no merging required or cannot merge with previous node: simply add a new one with all content\n\t                predecessor = self._add_node(doc, content, NTYPE_PARAGRAPH, predecessor=predecessor,\n\t                                             parent=abstract_title)\n\t                merge = False\n\t                content = \"\"\n\t        # if there is still content left (no paragraph follows a hick-up)\n\t        if len(content) > 0:\n\t            predecessor = self._add_node(doc, content.strip(), NTYPE_PARAGRAPH, predecessor=predecessor,\n\t                                         parent=abstract_title)\n\t        return abstract_title, predecessor\n", "    @staticmethod\n\t    def _parse_figure(doc, figure, prefix, predecessor, parent):\n\t        head = figure.find(f\"{prefix}head\")\n\t        label = figure.find(f\"{prefix}label\")\n\t        xid = figure.get('{http://www.w3.org/XML/1998/namespace}id')\n\t        figDesc = figure.find(f\"{prefix}figDesc\")\n\t        if head is None:\n\t            return None, None\n\t        if \"type\" in figure.attrib and figure.attrib[\"type\"] == \"table\":\n\t            table = figure.find(f\"{prefix}table\")\n", "            table_node = TEIXMLParser._add_node(doc,\n\t                                                head.text if head.text is not None else \"\",\n\t                                                NTYPE_TABLE,\n\t                                                meta={\"label\": label.text if label is not None else None,\n\t                                                      \"id\": xid,\n\t                                                      \"caption\": figDesc.text if figDesc is not None else None},\n\t                                                predecessor=predecessor,\n\t                                                parent=parent)\n\t            predecessor = table_node\n\t            table_content = TEIXMLParser._add_node(doc,\n", "                                                   str(ElementTree.tostring(table)),\n\t                                                   NTYPE_MEDIA,\n\t                                                   meta={},\n\t                                                   predecessor=predecessor,\n\t                                                   parent=table_node)\n\t            predecessor = table_content\n\t        else:\n\t            graphic = figure.find(f\"{prefix}graphic\")\n\t            figure_node = TEIXMLParser._add_node(doc,\n\t                                                 head.text if head.text is not None else \"\",\n", "                                                 NTYPE_FIGURE,\n\t                                                 meta={\"label\": label.text if label is not None else None,\n\t                                                       \"id\": xid,\n\t                                                       \"caption\": figDesc.text if figDesc is not None else None},\n\t                                                 predecessor=predecessor,\n\t                                                 parent=parent)\n\t            predecessor = figure_node\n\t            figure_content = TEIXMLParser._add_node(doc,\n\t                                                    TEIXMLParser._flatten_xml_element(graphic) if graphic else \"\",\n\t                                                    NTYPE_MEDIA,\n", "                                                    meta={},\n\t                                                    predecessor=predecessor,\n\t                                                    parent=figure_node)\n\t            predecessor = figure_content\n\t        return xid, predecessor\n\t    @staticmethod\n\t    def _parse_bibitem(bib_item, prefix):\n\t        xid = bib_item.get('{http://www.w3.org/XML/1998/namespace}id')\n\t        publishing_info = bib_item.find(f\"{prefix}monogr\")\n\t        paper_info = bib_item.find(f\"{prefix}analytic\")\n", "        # parse publishing information\n\t        if publishing_info:\n\t            pub_title = publishing_info.find(f\"{prefix}title\")\n\t            pub_title = pub_title.text if pub_title is not None else None\n\t            pub = publishing_info.find(f\"{prefix}imprint/{prefix}publisher\")\n\t            pub = pub.text if pub is not None else None\n\t            pub_date = publishing_info.find(f\"{prefix}imprint/{prefix}date\")\n\t            pub_date = pub_date.attrib[\"when\"] if pub_date is not None and \"when\" in pub_date.attrib else None\n\t        else:\n\t            pub_title = None\n", "            pub = None\n\t            pub_date = None\n\t        # add paper information if present\n\t        if paper_info:\n\t            title = paper_info.find(f\"{prefix}title\")\n\t            title = title.text if title is not None else None\n\t            authors = paper_info.findall(f\"{prefix}author/{prefix}persName\")\n\t            author_names = []\n\t            for a in authors:\n\t                forename = a.find(f'{prefix}forename')\n", "                surname = a.find(f'{prefix}surname')\n\t                canonical_author = f\"{forename.text if forename is not None else ''} {surname.text if surname is not None else ''}\"\n\t                author_names += [canonical_author] if len(canonical_author) > 0 else []\n\t        else:\n\t            title = None\n\t            author_names = None\n\t        return xid, title, author_names, pub_title, pub, pub_date\n\t    @staticmethod\n\t    def _flatten_xml_element(element):\n\t        stack = [(element, -1, None)]\n", "        while True:\n\t            elem, pred, content = stack.pop(-1)\n\t            # visiting node the first time\n\t            if content is None:\n\t                content = elem.text if elem.text else \"\"\n\t                # revisit element after children\n\t                stack += [(elem, pred, content)]\n\t                # add children\n\t                stack += reversed([(child, len(stack) - 1, None) for child in elem])\n\t            else:\n", "                # revisiting the node (content is set)\n\t                suffix = elem.tail if elem.tail else \"\"\n\t                if pred >= 0:\n\t                    pre_elem, pre_pred, pre_content = stack[pred]\n\t                    stack[pred] = (pre_elem, pre_pred, pre_content + content + suffix)\n\t                else:\n\t                    # terminating at parent most element\n\t                    return content + suffix\n\t                # don't add to stack again\n\t        # should always terminate\n", "    @staticmethod\n\t    def _flatten_xml_element_with_child_ix(element):\n\t        # xml element, predecessor ix, pred. merged ixs, parsed content\n\t        stack = [(element, -1, [], None)]\n\t        while True:\n\t            elem, pred, mergedix, content = stack.pop(-1)\n\t            # visiting node the first time\n\t            if content is None:\n\t                content = elem.text if elem.text else \"\"\n\t                # revisit element after children\n", "                stack += [(elem, pred, mergedix, content)]\n\t                # add children\n\t                stack += reversed([(child, len(stack) - 1, [], None) for child in elem])\n\t            else:\n\t                # revisiting the node (content is set)\n\t                suffix = elem.tail if elem.tail else \"\"\n\t                if pred >= 0:\n\t                    pre_elem, pre_pred, pre_mergedix, pre_content = stack[pred]\n\t                    new_mergedix = pre_mergedix + \\\n\t                                   [(elem, len(pre_content), len(content))] + \\\n", "                                   [(t, prel + len(pre_content), contl) for (t, prel, contl) in mergedix]\n\t                    stack[pred] = (pre_elem, pre_pred, new_mergedix, pre_content + content + suffix)\n\t                else:\n\t                    # terminating at parent most element\n\t                    return content + suffix, mergedix\n\t                # don't add to stack again\n\t        # should always terminate\n\t    @staticmethod\n\t    def _add_node(doc, content, ntype, meta=None, predecessor=None, parent=None):\n\t        new_node = Node(\n", "            content=content,\n\t            ntype=ntype,\n\t            meta=meta\n\t        )\n\t        doc.add_node(new_node)\n\t        if parent is not None:\n\t            parent_edge = Edge(\n\t                src_node=parent,\n\t                tgt_node=new_node,\n\t                etype=Etype.PARENT\n", "            )\n\t            doc.add_edge(parent_edge)\n\t        if predecessor is not None:\n\t            next_edge = Edge(\n\t                src_node=predecessor,\n\t                tgt_node=new_node,\n\t                etype=Etype.NEXT\n\t            )\n\t            doc.add_edge(next_edge)\n\t        return new_node\n", "    @staticmethod\n\t    def _add_subtree(doc, subTree, lastSubTree, targetParent, targetPredecessor):\n\t        # add nodes\n\t        new_nodes = {}\n\t        for n in subTree.nodes:\n\t            new_n = TEIXMLParser._add_node(doc, n.content, n.ntype, n.meta)\n\t            new_nodes[n.ix] = new_n\n\t        # add edges\n\t        for e in subTree.edges:\n\t            new_e = Edge(\n", "                src_node=new_nodes[e.src_node.ix],\n\t                tgt_node=new_nodes[e.tgt_node.ix],\n\t                etype=e.etype\n\t            )\n\t            doc.add_edge(new_e)\n\t        # get pseudo root and replace parent edges\n\t        new_pseudo_root = new_nodes[subTree.root.ix]\n\t        for ce in new_pseudo_root.get_edges(Etype.PARENT, outgoing=True, incoming=False):\n\t            new_parent = Edge(\n\t                src_node=targetParent,\n", "                tgt_node=ce.tgt_node,\n\t                etype=Etype.PARENT\n\t            )\n\t            doc.add_edge(new_parent)\n\t            doc.remove_edge(ce)\n\t        # if no other nodes except root: skip the next part\n\t        new_pseudo_next_edges = new_pseudo_root.get_edges(Etype.NEXT, outgoing=True, incoming=False)\n\t        if len(new_pseudo_next_edges) > 0:\n\t            new_pseudo_next = new_pseudo_next_edges[0]\n\t            new_next = Edge(\n", "                src_node=targetPredecessor,\n\t                tgt_node=new_pseudo_next.tgt_node,\n\t                etype=Etype.NEXT\n\t            )\n\t            doc.add_edge(new_next)\n\t            doc.remove_edge(new_pseudo_next)\n\t        doc.remove_node(new_pseudo_root)\n\t        # output\n\t        start_subtree = targetParent\n\t        end_subtree = new_nodes[lastSubTree.ix] if len(new_pseudo_next_edges) > 0 else None\n", "        return start_subtree, end_subtree, new_nodes\n\t    def _parse_document(self) -> IntertextDocument:\n\t        \"\"\"\n\t        Parse the given TEI XML document.\n\t        :return: resulting IntertextDocument\n\t        \"\"\"\n\t        # create intertext document\n\t        prefix = self._xml_file_path.split(os.path.sep)[-1]\n\t        itg_doc = IntertextDocument(\n\t            nodes=[],\n", "            edges=[],\n\t            prefix=prefix\n\t        )\n\t        # the content of the document is completely derived from the TEI XML file\n\t        tree = ElementTree.parse(self._xml_file_path)\n\t        prefix = \"{http://www.tei-c.org/ns/1.0}\"\n\t        # create article title as root\n\t        title = tree.getroot().find(f\"{prefix}teiHeader/{prefix}fileDesc/{prefix}titleStmt/{prefix}title\").text\n\t        title = title if title is not None else \"\"\n\t        article_title_node = self._add_node(itg_doc, title, NTYPE_TITLE)\n", "        predecessor = article_title_node\n\t        #\n\t        # PARSE THE ABSTRACT\n\t        #\n\t        abstract = tree.getroot().find(f\"{prefix}teiHeader/{prefix}profileDesc/{prefix}abstract/{prefix}div\")\n\t        abstract_title_node, predecessor = self._parse_abstract(itg_doc, abstract, prefix, predecessor,\n\t                                                                article_title_node)\n\t        #\n\t        # PARSE BODY\n\t        #\n", "        body = tree.getroot().find(f\"{prefix}text/{prefix}body\")\n\t        body_refs = []\n\t        body_figs = {}\n\t        content_graph = []\n\t        last_section_title = []\n\t        for section in body.findall(f\"{prefix}div\"):\n\t            content, last_elem, refs, figures = self._parse_section_content(section, prefix)\n\t            head = section.find(f\"{prefix}head\")\n\t            if head is None:\n\t                logging.info(f\"Div without a heading in {self._xml_file_path}.\")\n", "            # empty section\n\t            if len(content.edges) == 0 and len(content.nodes) <= 1 and head is None:\n\t                logging.info(f\"Encountered empty section in {self._xml_file_path}.\")\n\t            content_graph += [(content, last_elem, refs, figures)]\n\t            # fixme currently erroneous head nodes (e.g. with text, but without number) are discarded entirely\n\t            if head is not None and \"n\" in head.attrib:\n\t                # pop current content\n\t                current_content, current_last, current_refs, current_figures = content_graph.pop(-1)\n\t                # add previous contents to the predecessor if existent, else create a dummy section first\n\t                if len(content_graph) > 0:\n", "                    if len(last_section_title) == 0:\n\t                        dummy_node = self._add_node(itg_doc, \"\", NTYPE_HEADING, {\"section\": \"1\"},\n\t                                                    predecessor=article_title_node, parent=article_title_node)\n\t                        last_section_title += [dummy_node]\n\t                        predecessor = dummy_node\n\t                    pred_parent = last_section_title[-1]\n\t                    for c, l, r, f in content_graph:\n\t                        sub_root, sub_last, node_map = self._add_subtree(itg_doc, c, l, pred_parent, predecessor)\n\t                        predecessor = sub_last if sub_last is not None else predecessor\n\t                        mapped_refs = [(node_map[n.ix], ref) for n, ref in r]\n", "                        body_refs += mapped_refs\n\t                        mapped_figs = {fxid: node_map[fig.ix] for fxid, fig in f.items()}\n\t                        body_figs.update(mapped_figs)\n\t                    # reset content stack -- added all previous contents\n\t                    content_graph = []\n\t                # get section name and number\n\t                section_name = head.text\n\t                section_n = head.attrib[\"n\"]\n\t                self._check_text(section_name, \"section title\")\n\t                # find parent node\n", "                section_parent_node = None\n\t                if len(last_section_title) == 0:\n\t                    # is first section\n\t                    section_parent_node = article_title_node\n\t                else:\n\t                    for st in last_section_title:\n\t                        st_n = st.meta[\"section\"]\n\t                        if self._is_child_section_count(section_n, st_n):\n\t                            section_parent_node = st\n\t                            break\n", "                    if section_parent_node is None:\n\t                        section_parent_node = article_title_node\n\t                # add new section title with content\n\t                section_title_node = self._add_node(itg_doc,\n\t                                                    section_name,\n\t                                                    NTYPE_HEADING,\n\t                                                    {\"section\": section_n},\n\t                                                    predecessor=predecessor,\n\t                                                    parent=section_parent_node)\n\t                predecessor = section_title_node\n", "                last_section_title += [section_title_node]\n\t                sub_root, sub_last, node_map = self._add_subtree(itg_doc, current_content, current_last,\n\t                                                                 section_title_node,\n\t                                                                 predecessor)\n\t                predecessor = sub_last if sub_last is not None else predecessor\n\t                mapped_refs = [(node_map[n.ix], r) for n, r in refs]\n\t                body_refs += mapped_refs\n\t                mapped_figs = {fxid: node_map[fig.ix] for fxid, fig in current_figures.items()}\n\t                body_figs.update(mapped_figs)\n\t        # add left-over content-graph elements\n", "        if len(content_graph) > 0:\n\t            if len(last_section_title) == 0:\n\t                dummy_node = self._add_node(itg_doc, \"\", NTYPE_HEADING, {\"section\": \"1\"},\n\t                                            predecessor=article_title_node, parent=article_title_node)\n\t                last_section_title += [dummy_node]\n\t                predecessor = dummy_node\n\t            pred_parent = last_section_title[-1]\n\t            for c, l, r, f in content_graph:\n\t                sub_root, sub_last, node_map = self._add_subtree(itg_doc, c, l, pred_parent, predecessor)\n\t                predecessor = sub_last if sub_last is not None else predecessor\n", "                mapped_refs = [(node_map[n.ix], ref) for n, ref in r]\n\t                body_refs += mapped_refs\n\t                mapped_figs = {fxid: node_map[fig.ix] for fxid, fig in f.items()}\n\t                body_figs.update(mapped_figs)\n\t        for figure in body.findall(f\"{prefix}figure\"):\n\t            fxid, figure_node = self._parse_figure(itg_doc, figure, prefix, predecessor, article_title_node)\n\t            predecessor = figure_node\n\t            body_figs[fxid] = figure_node\n\t        #\n\t        ## PARSE BACK MATTER\n", "        #\n\t        back = tree.getroot().find(f\"{prefix}text/{prefix}back\")\n\t        bibliography = {}\n\t        for bib_item in back.findall(f\"{prefix}div/{prefix}listBibl/{prefix}biblStruct\"):\n\t            xid, title, authors, pub_title, pub, pub_date = self._parse_bibitem(bib_item, prefix)\n\t            bib_node = self._add_node(itg_doc, f\"{', '.join(authors) if authors is not None else 'UNKNOWN'}, \"\n\t                                               f\"{title}, \"\n\t                                               f\"{pub_date if pub_date else ''}, \"\n\t                                               f\"{pub_title if pub_title else ''}, \"\n\t                                               f\"{pub if pub else ''}.\",\n", "                                      ntype=NTYPE_BIB_ITEM,\n\t                                      meta={\"xid\": xid,\n\t                                            \"authors\": authors,\n\t                                            \"title\": title,\n\t                                            \"pub_date\": pub_date,\n\t                                            \"pub_title\": pub_title,\n\t                                            \"pub\": pub})\n\t            bibliography[xid] = bib_node\n\t        #\n\t        ## ADD REFERENCES\n", "        #\n\t        for node, refs in body_refs:\n\t            for r in refs:\n\t                xml_elem, start, end = r\n\t                rtype = xml_elem.attrib[\"type\"]\n\t                # skip invalid references with missing target (for now)\n\t                if \"target\" not in xml_elem.attrib:\n\t                    continue\n\t                rtarget = xml_elem.attrib[\"target\"][1:]\n\t                # add span node\n", "                ref_node = SpanNode(\n\t                    ntype=NTYPE_BIB_REFERENCE if rtype == \"bibr\" else NYTPE_ELEMENT_REFERENCE,\n\t                    src_node=node,\n\t                    start=start,\n\t                    end=end,\n\t                    meta={\"from_xml_type\": rtype, \"from_xml_target\": rtarget}\n\t                )\n\t                itg_doc.add_node(ref_node)\n\t                # add link (where possible)\n\t                target_node = None\n", "                if rtype == \"bibr\" and rtarget in bibliography:\n\t                    target_node = bibliography[rtarget]\n\t                elif (rtype == \"figure\" or rtype == \"table\") and target_node in body_figs:\n\t                    target_node = body_figs[rtarget]\n\t                if target_node:\n\t                    link = Edge(ref_node, target_node, etype=Etype.LINK)\n\t                    itg_doc.add_edge(link)\n\t        return itg_doc\n\t    @staticmethod\n\t    def _check_text(text: str, element_name: str) -> str:\n", "        assert isinstance(text, str), f\"{element_name} is not a string, but a {type(text)}!\"\n\t        return text\n\t    @staticmethod\n\t    def _compare_section_counts(cntA: str, cntB: str) -> int:\n\t        nAs = cntA.split(\".\")\n\t        nBs = cntB.split(\".\")\n\t        for i, nA in enumerate(nAs):\n\t            if len(nBs) <= i:\n\t                return 1  # b higher level than a (a > b)\n\t            nB = nBs[i]\n", "            if int(nA) != int(nB):\n\t                return -1 if int(nA) < int(nB) else 1  # a earlier than b (a < b)\n\t        return 0 if len(nAs) == len(nBs) else -1  # a higher level than b (a < b), else equal\n\t    @staticmethod\n\t    def _is_child_section_count(cntA: str, cntB: str) -> bool:\n\t        return cntA.startswith(cntB)\n\tclass F1000XMLParserBM(F1000XMLParser):\n\t    \"\"\"\n\t    The F1000XMLParserBM is an extension of the F1000XMLParser provided by the ITG library. To standardize\n\t    the benchmark structure, we need to adapt the given parsing strategy.\n", "    We can translate some nodes post-hoc (assuming that the rough parsing structure is the same and semantics fit):\n\t        article-title -> title\n\t        title -> heading\n\t        p -> paragraph\n\t        abstract = abstract\n\t        supplementary_material -> paragraph\n\t        preformat -> paragraph\n\t        disp-quote -> paragraph\n\t        label && label.content.startswith(\"Figure\") -> figure\n\t            > fig                                   -> media\n", "        label && label.content.startswith(\"Table\") -> table\n\t            > table-wrap                           -> media\n\t        label && label.content.startswith(\"Algorithm\") -> figure\n\t            > boxed text\n\t        math -> formula\n\t        boxed-text -> figure #TODO\n\t        table-warp -> table #todo\n\t    We need to add or refactor other node types:\n\t        * \"ref\" is bib_item, but we parse more meta-information (i.e. authors etc.)\n\t        * lists and list-items are so-far not mapped.\n", "        * in-line references to figures and bibitems are not tagged as span nodes so far\n\t    Additionally, the parser delivers us meta-data on authors etc, which we should include in the benchmark. To keep\n\t    things clean, we will decouple this.\n\t    \"\"\"\n\t    def __init__(self, xml_file_path: str):\n\t        \"\"\"\n\t        Initialize the F1000XMLParserBM for a particular paper.\n\t        :param xml_file_path: filepath of the F1000 XML file\n\t        \"\"\"\n\t        super(F1000XMLParserBM, self).__init__(xml_file_path)\n", "        self._xml_file_path: str = xml_file_path\n\t        self.ntype_mapping = {\n\t            \"title\": NTYPE_HEADING,\n\t            \"article-title\": NTYPE_TITLE,\n\t            \"p\": NTYPE_PARAGRAPH,\n\t            \"supplementary-material\": NTYPE_PARAGRAPH,\n\t            \"preformat\": NTYPE_PARAGRAPH,\n\t            \"disp-quote\": NTYPE_PARAGRAPH,\n\t            \"list\": NTYPE_LIST,\n\t            \"list-item\": NTYPE_LIST_ITEM,\n", "            \"disp-formula\": NTYPE_FORMULA,\n\t            \"def-list\": NTYPE_LIST\n\t        }\n\t        self.complex_ntype_mapping = [\n\t            (F1000XMLParserBM._is_figure_ntype, F1000XMLParserBM._convert_to_figure_node),\n\t            (F1000XMLParserBM._is_table_ntype, F1000XMLParserBM._convert_to_table_node),\n\t            (F1000XMLParserBM._is_other_media_ntype, F1000XMLParserBM._convert_to_figure_node)\n\t        ]\n\t    @staticmethod\n\t    def _is_other_media_ntype(node):\n", "        return node.ntype == \"label\"\n\t    @staticmethod\n\t    def _is_figure_ntype(node):\n\t        return (node.ntype == \"label\" and\n\t                (node.content.lower().startswith(\"figure\") or\n\t                 node.content.lower().startswith(\"algorithm\") or\n\t                 node.content.lower().startswith(\"box\")) or\n\t                node.content.lower().startswith(\"listing\")) or \\\n\t               (node.ntype == \"fig\") or \\\n\t               (node.ntype == \"caption\") or \\\n", "               (node.ntype == \"boxed-text\")\n\t    @staticmethod\n\t    def _is_table_ntype(node):\n\t        return (node.ntype == \"label\" and node.content.lower().startswith(\"table\")) or \\\n\t               (\"table\" in node.ntype)\n\t    @staticmethod\n\t    def _convert_to_figure_node(node):\n\t        # adapt top node of type label or of type fig\n\t        node.ntype = NTYPE_FIGURE\n\t        node.meta = {\n", "            \"label\": node.content,\n\t            \"id\": node.meta[\"id\"] if \"id\" in node.meta else None,\n\t            \"caption\": node.meta[\"caption\"] if \"caption\" in node.meta else None\n\t        }\n\t        # get figure child node (\"the content\") from top node, if existent\n\t        fig_node = node.get_edges(Etype.PARENT, incoming=False, outgoing=True)\n\t        if len(fig_node) > 0:\n\t            fig_node = fig_node[0].tgt_node\n\t            # adapt children node of type fig\n\t            fig_node.ntype = NTYPE_MEDIA\n", "            fig_node.meta = {\n\t                \"id\": fig_node.meta[\"id\"] if \"id\" in fig_node.meta else None,\n\t                \"uri\": fig_node.meta[\"uri\"] if \"uri\" in fig_node.meta else None\n\t            }\n\t    @staticmethod\n\t    def _convert_to_table_node(node):\n\t        # adapt top node of type label\n\t        node.ntype = NTYPE_TABLE\n\t        node.meta = {\n\t            \"label\": node.content,\n", "            \"id\": node.meta[\"id\"] if \"id\" in node.meta else None,\n\t            \"caption\": node.meta[\"caption\"] if \"caption\" in node.meta else None\n\t        }\n\t        # get table child node from label node\n\t        tbl_node = node.get_edges(Etype.PARENT, incoming=False, outgoing=True)\n\t        if len(tbl_node) > 0:\n\t            tbl_node = tbl_node[0].tgt_node\n\t            # adapt children node of type table-wrap\n\t            tbl_node.ntype = NTYPE_MEDIA\n\t            tbl_node.meta = {\n", "                \"id\": tbl_node.meta[\"id\"] if \"id\" in tbl_node.meta else None,\n\t                \"uri\": tbl_node.meta[\"uri\"] if \"uri\" in tbl_node.meta else None\n\t            }\n\t    def __call__(self) -> IntertextDocument:\n\t        \"\"\"\n\t        Parse the F1000 XML document into an IntertextDocument.\n\t        :return: the IntertextDocument\n\t        \"\"\"\n\t        return self._parse_document()\n\t    def _parse_refs(self, ref_list: etree._Element) -> None:\n", "        for ref in ref_list:\n\t            target_xmlid = ref.attrib['id']\n\t            label = ref.find(\"label\")\n\t            label = label.text if label is not None else None\n\t            citation = ref.find(\"mixed-citation\")\n\t            title = citation.find(\"article-title\")\n\t            title = ''.join([e.strip() for e in title.itertext()]) if title is not None else None\n\t            pub_date = citation.find(\"year\")\n\t            pub_date = pub_date.text if pub_date is not None else None\n\t            if citation is not None:\n", "                pub_info = [i for i in citation.findall(\"*\") if i.tag not in [\"article-title\", \"person-group\", \"year\"]]\n\t                pub_text = \"\".join([\" \".join([t.strip() for t in i.itertext()]) for i in pub_info])\n\t            else:\n\t                pub_text = None\n\t            authors = citation.findall(\"person-group[@person-group-type='author']/name\")\n\t            author_list = []\n\t            for a in authors:\n\t                name_components = []\n\t                given = a.find(\"given-names\")\n\t                name_components += [given.text] if given is not None and given.text is not None else []\n", "                suffix = a.find(\"suffix\")\n\t                name_components += [suffix.text] if suffix is not None and suffix.text is not None else []\n\t                surname = a.find(\"surname\")\n\t                name_components += [surname.text] if surname is not None and surname.text is not None else []\n\t                author_list += [\" \".join(name_components)]\n\t            bib_item = Node(f\"{', '.join(author_list) if len(author_list) > 0 else 'UNKNOWN'}, \" +\n\t                            f\"{title if title is not None else 'UNKNOWN'}, \" +\n\t                            f\"{pub_date if pub_date is not None else ''}, \" +\n\t                            f\"{pub_text if pub_text is not None else ''}\",\n\t                            ntype=NTYPE_BIB_ITEM,\n", "                            meta={\"xid\": target_xmlid,\n\t                                  \"id\": label,\n\t                                  \"authors\": author_list,\n\t                                  \"title\": title,\n\t                                  \"pub_date\": pub_date,\n\t                                  \"pub\": pub_text})\n\t            self._xref_targets[target_xmlid] = bib_item\n\t    # copied from f1000 xml parser and adapted to support empty nodes\n\t    def _make_node(self, element: etree._Element, stringify: bool = False, meta: Dict[str, Any] = None) -> Optional[\n\t        Node]:\n", "        if stringify:\n\t            content = self._stringify(element)\n\t            if content:\n\t                content = self._parse_whitespace(content)\n\t            if content:\n\t                return super(F1000XMLParser, self)._make_node(content, element.tag, meta)\n\t        else:\n\t            content = \"\"\n\t            return super(F1000XMLParser, self)._make_node(content, element.tag, meta)\n\t    # copied from F1000 parser and adapted\n", "    def _split_element(cls, element: etree._Element, selector: str) -> List[etree._Element]:\n\t        \"\"\"Split an element before and after the selector.\"\"\"\n\t        node = deepcopy(element)\n\t        children = [c for c in node]\n\t        split_child = node.xpath(selector)[0]\n\t        split_child_i = children.index(split_child)\n\t        assert split_child_i != -1, \"something went wrong splitting an element. Invalid selector passed.\"\n\t        c_before = node[:max(split_child_i, 0)]\n\t        c_after = node[min(split_child_i + 1, len(children)):]\n\t        root_before = etree.Element(element.tag, element.attrib, element.nsmap)\n", "        root_before.text = element.text\n\t        for c in c_before:\n\t            root_before.append(c)\n\t        root_after = etree.Element(element.tag, element.attrib, element.nsmap)\n\t        root_after.tail = element.tail\n\t        for c in c_after:\n\t            root_after.append(c)\n\t        return [root_before, split_child, root_after]\n\t    # copied from f1000_xml_parser.py with minor adaptions\n\t    def _parse_element(self, element: etree._Element) -> Tuple[Optional[Node], List[etree._Element]]:\n", "        children = list(element)\n\t        if children:\n\t            # Parse nodes with children, i.e. subtrees\n\t            if element.tag == 'body':\n\t                return None, children\n\t            elif element.tag == 'abstract':\n\t                node = self._make_node(element, stringify=False)\n\t                node.content = \"Abstract\"\n\t            elif element.tag == 'sec':\n\t                # Move the title child to the root node of a section\n", "                title_element = element.xpath('title')\n\t                if title_element and title_element[0].text:\n\t                    meta = {'section': self._generate_sec_index(element)}\n\t                    if 'id' in element.attrib:\n\t                        meta['id'] = element.attrib['id']\n\t                    if 'sec-type' in element.attrib:\n\t                        meta['sec-type'] = element.attrib['sec-type']\n\t                    node = self._make_node(title_element[0], stringify=True, meta=meta)\n\t                    children.remove(title_element[0])\n\t                    # Keep track of potential xref targets\n", "                    if meta and 'id' in meta:\n\t                        self._xref_targets[meta['id']] = node\n\t                else:\n\t                    # Section has no title\n\t                    node = None\n\t            elif element.tag == 'list':\n\t                # Concatenate list items with new line\n\t                node = self._make_node(element, stringify=False)\n\t                # add children\n\t                children = element.xpath('list-item')\n", "            elif element.tag == \"list-item\":\n\t                node = self._make_node(element, stringify=True)\n\t                children = []\n\t            elif element.tag == \"def-list\":\n\t                node = self._make_node(element, stringify=False)\n\t                # add children\n\t                children = element.xpath(\"def-item\")\n\t            elif element.tag == \"def-item\":\n\t                t = element.xpath(\"term\")\n\t                d = element.xpath(\"def\")\n", "                if t and d:\n\t                    content = f\"{self._stringify(t[0])}: {self._stringify(d[0])}\"\n\t                    node = super(F1000XMLParser, self)._make_node(content, \"list-item\")\n\t                    children = []\n\t                else:\n\t                    return None, []\n\t            elif element.tag == 'p':\n\t                # Stringify paragraphs, drop all inline tags\n\t                tags = [e.tag for e in element]\n\t                if 'boxed-text' in tags:\n", "                    # Boxed text has to be processed before other nested types as is might contain these as children\n\t                    return None, self._elevate_element(element, 'boxed-text')\n\t                elif element.xpath(\"table-wrap\"):\n\t                    return None, self._split_element(element, \"table-wrap\")\n\t                elif element.xpath('preformat'):\n\t                    # Elevate immediate children but ignore nested inline tags\n\t                    return None, self._elevate_element(element, 'preformat')\n\t                elif 'list' in tags:\n\t                    # Split paragraph before and after an inline list\n\t                    # A human would probably read this as separate paragraphs\n", "                    return None, self._split_element(element, 'list')\n\t                elif \"disp-formula\" in tags:\n\t                    #if len(children) > 1: #todo verify\n\t                    #    return None, self._split_element(element, \"disp-formula\")\n\t                    #else:\n\t                    #    return self._make_node(children[0], stringify=True), []\n\t                    return None, self._split_element(element, \"disp-formula\")\n\t                meta = None\n\t                # Add metadata for xrefs which will later be parsed into edges\n\t                if 'xref' in tags:\n", "                    xrefs = self._collect_xrefs(element)\n\t                    if xrefs:\n\t                        meta = {'xrefs': xrefs}\n\t                # Drop inline xref\n\t                etree.strip_tags(element, 'xref')\n\t                node = self._make_node(element, stringify=True, meta=meta)\n\t                # Drop children\n\t                children = []\n\t            elif element.tag in ['fig', 'table-wrap', 'boxed-text']:\n\t                # Get optional meta data\n", "                meta = self._parse_node_meta(element)\n\t                label_element = element.xpath('label')\n\t                caption_element = element.xpath('caption')\n\t                if label_element:\n\t                    # Move the label child to the root node\n\t                    node = self._make_node(element.xpath('label')[0], stringify=True, meta=meta)\n\t                    # Remove the label tag and do another recursive call to parse element as an XML node\n\t                    etree.strip_elements(element, 'label', with_tail=False)\n\t                    children = [element]\n\t                elif caption_element:\n", "                    # Move the caption child to the root node\n\t                    node = self._make_node(element.xpath('caption')[0], stringify=True, meta=meta)\n\t                    # Remove the label tag and do another recursive call to parse element as an XML node\n\t                    etree.strip_elements(element, 'caption', with_tail=False)\n\t                    children = [element]\n\t                else:\n\t                    # Handle second recursive call or cases where there is no label\n\t                    node = self._make_xml_node(element, meta=meta)\n\t                    # Drop children\n\t                    children = []\n", "                # Keep track of potential xref targets\n\t                if meta and 'id' in meta:\n\t                    self._xref_targets[meta['id']] = node\n\t            elif 'formula' in element.tag:\n\t                meta = self._parse_node_meta(element)\n\t                node = self._make_node(element, stringify=True, meta=meta)\n\t                children = []\n\t            else:\n\t                node = self._make_xml_node(element)\n\t                # Drop children\n", "                children = []\n\t        else:\n\t            # Parse leaf nodes with potential inline tags\n\t            node = self._make_node(element, stringify=True)\n\t        return node, children\n\t    def _parse_document(self):\n\t        # prep\n\t        prefix = self._xml_file_path.split(os.path.sep)[-1]\n\t        # parse main document with the standard parser, but exchanged sub-routines\n\t        doc = self._parse(self._root, None, prefix)\n", "        if doc is None:\n\t            raise ValueError(f\"The passed F1000 XML {self._xml_file_path} could not be parsed. Body not found.\")\n\t        # replace old node types by new ones, where an easy type mapping suffices\n\t        nodes_by_ntype = [(newtype, [n for n in doc.nodes if n.ntype == oldtype]) for oldtype, newtype in\n\t                          self.ntype_mapping.items()]\n\t        for newtype, nodes in nodes_by_ntype:\n\t            for n in nodes:\n\t                n.ntype = newtype\n\t        # realize the more complex mappings on node level\n\t        for criterion, mapping in self.complex_ntype_mapping:\n", "            matching_nodes = filter(criterion, doc.nodes)\n\t            for mn in matching_nodes:\n\t                mapping(mn)\n\t        # verify validity\n\t        invalid_types = set([n.ntype for n in doc.nodes if n.ntype not in NTYPES])\n\t        if len(invalid_types) > 0:\n\t            logging.warning(\n\t                f\"Found invalid node types in the document {self._xml_file_path} after parsing: {str(invalid_types)}\")\n\t        # discard any invalid node types\n\t        to_remove = []\n", "        for n in doc.nodes:\n\t            if n in invalid_types:\n\t                to_remove += [n]\n\t        for n in to_remove:\n\t            doc.remove_node(n)\n\t        # add span nodes for citations and references\n\t        # todo currently full paragraphs are regarded as the source of a link, we want that to be span nodes\n\t        return doc\n\tclass F1000XMLParserMetadata(F1000XMLParser):\n\t    \"\"\"\n", "    The F1000XMLParserMetadata is simply a reduced version of the F1000XMLParser meant to extract\n\t    only metadata on the paper including reviews, change notes and metadata on the publication.\n\t    \"\"\"\n\t    def __init__(self, xml_file_path: str):\n\t        \"\"\"\n\t        Initialize the F1000XMLParserBM for a particular paper.\n\t        :param xml_file_path: filepath of the F1000 XML file\n\t        \"\"\"\n\t        super(F1000XMLParserMetadata, self).__init__(xml_file_path)\n\t        self._xml_file_path: str = xml_file_path\n", "    def __call__(self) -> IntertextDocument:\n\t        \"\"\"\n\t        Parse the F1000 XML document into an IntertextDocument.\n\t        :return: the IntertextDocument\n\t        \"\"\"\n\t        return self._parse_all_metadata()\n\t    def _parse_all_metadata(self):\n\t        # prep\n\t        prefix = self._xml_file_path.split(os.path.sep)[-1]\n\t        # use standard meta parser\n", "        self._parse_meta()\n\t        # add meta fields\n\t        meta_node = self._root.find('.//article-meta')\n\t        subs = meta_node.findall(\".//article-categories/subj-group/subject\")\n\t        self._meta[\"subjects\"] = [s.text for s in\n\t                                  subs[2:]]  # first always = article type, not subject; second always = articles\n\t        # add year\n\t        year = meta_node.find(\".//pub-date[@pub-type='epub']/year\")\n\t        self._meta[\"year\"] = year.text if year is not None else None #todo verify\n\t        # add license information\n", "        license_node = meta_node.find(\".//permissions\")\n\t        self._meta[\"license\"] = self._stringify(license_node)\n\t        # ** COPIED FROM PARENT CLASS __call__ method **\n\t        # ** START **\n\t        # Reviews\n\t        reviews = {}\n\t        for review in self._root.xpath('.//sub-article[@article-type=\"ref-report\"]'):\n\t            review_id = review.attrib['id']\n\t            license = review.find('.//license').attrib['{http://www.w3.org/1999/xlink}href']\n\t            recommendation = review.find('.//meta-value').text  # TODO: A bit dirty here\n", "            doi = review.find('.//front-stub/article-id[@pub-id-type=\"doi\"]').text\n\t            contributors = [\n\t                {\n\t                    'surname': contrib.find('.//name/surname').text,\n\t                    'given-names': contrib.find('.//name/given-names').text\n\t                } for contrib in review.find('.//front-stub/contrib-group')\n\t                if contrib.tag == 'contrib' and (contrib.find('.//name/surname') is not None)\n\t            ]\n\t            #todo get license\n\t            meta = {'review_id': review_id,\n", "                    'license': license,\n\t                    'recommendation': recommendation,\n\t                    'doi': doi,\n\t                    'contributors': contributors}\n\t            reviews[review_id] = self._parse(review, meta, review_id)\n\t        # Revision comment\n\t        version_changes = self._root.xpath('.//sec[@sec-type=\"version-changes\"]')\n\t        if version_changes:\n\t            # TODO: Check if any nodes need additional parsing\n\t            nodes, edges = self._parse_tree(version_changes[-1])\n", "            self._add_supplementary_edges(nodes, edges)\n\t            if prefix:\n\t                prefix = f'revision_{prefix}'\n\t            revision = IntertextDocument(nodes, edges, prefix or 'revision')\n\t        else:\n\t            revision = None\n\t        # ** END **\n\t        return self._meta, reviews, revision\n\tclass IntertextSentenceSplitterBM(IntertextSentenceSplitter):\n\t    def __init__(self, itg: IntertextDocument, splitter: SentenceSplitter = None,\n", "                 gold: Dict[str, List[Dict[str, str]]] = {}):\n\t        super().__init__(itg, splitter, gold)\n\t    def _get_sentences_from_itg(self) -> List[SpanNode]:\n\t        sentence_nodes = []\n\t        for node in self.itg.nodes:\n\t            if node.ntype in [NTYPE_PARAGRAPH, NTYPE_ABSTRACT, NTYPE_HEADING, NTYPE_TITLE, NTYPE_LIST_ITEM]:\n\t                boundaries = self.splitter.split(node.content)\n\t                new_sentence_nodes = make_sentence_nodes(\n\t                    node,\n\t                    boundaries\n", "                )\n\t                sentence_nodes += new_sentence_nodes\n\t        return sentence_nodes\n\tclass IntertextLayoutTagger:\n\t    def __init__(self, itg, position_information):\n\t        self.itg = itg\n\t        self.posinfo = position_information\n\t    def tag_text_with_position_information(self):\n\t        out = copy.deepcopy(self.itg)\n\t        out.meta.update({\n", "            \"position_tag_type\": \"from_draft\"\n\t        })\n\t        new_root = self.itg.root\n\t        line_nodes = []\n\t        lines_per_node = {}\n\t        for line in self.posinfo[\"lines\"]:\n\t            li, lt = line\n\t            lt_like = lt.lower().strip()\n\t            if lt.endswith(\"-\"):\n\t                lt_like = lt_like[:-1]\n", "            pix = next(i for i,p in enumerate(self.posinfo[\"pages\"]) if int(p[0][0]) <= int(li) <= int(p[1][0]))\n\t            for node in self.itg._unroll_graph(new_root):\n\t                if node.ntype in [NTYPE_PARAGRAPH, NTYPE_LIST_ITEM, NTYPE_TITLE, NTYPE_HEADING,\n\t                                  NTYPE_ABSTRACT, NTYPE_FORMULA, NTYPE_BIB_ITEM]:\n\t                    if lt_like in node.content.lower().strip():\n\t                        str_idx = node.content.lower().find(lt_like)\n\t                        new_root = node\n\t                        posi_node = SpanNode(ntype=\"line\",\n\t                                             src_node=node,\n\t                                             start=str_idx,\n", "                                             end=str_idx + len(lt_like),\n\t                                             meta={\"created_by\": \"IntertextLayoutTagger\", \"line\": li, \"page\": pix+1})\n\t                        posi_node.ix = f\"{node.ix}@{len(line_nodes)}_{li}\"\n\t                        line_nodes += [posi_node]\n\t                        for nn in self.itg.breadcrumbs(node, Etype.PARENT):\n\t                            lpn = lines_per_node.get(nn.ix, [])\n\t                            lines_per_node[nn.ix] = [min(lpn + [li]), max(lpn + [li])]\n\t                        break\n\t        layout_nodes = []\n\t        for n, lpn in lines_per_node.items():\n", "            pix0 = next(i for i, p in enumerate(self.posinfo[\"pages\"]) if int(p[0][0]) <= int(lpn[0]) <= int(p[1][0]))\n\t            pix1 = next(i for i, p in enumerate(self.posinfo[\"pages\"]) if int(p[0][0]) <= int(lpn[1]) <= int(p[1][0]))\n\t            node = self.itg.get_node_by_ix(n)\n\t            posi_node = SpanNode(ntype=\"line_range\",\n\t                                 src_node=node,\n\t                                 start=0,\n\t                                 end=len(node.content),\n\t                                 meta={\"created_by\": \"IntertextLayoutTagger\",\n\t                                       \"line_start\": lpn[0],\n\t                                       \"line_end\":lpn[1],\n", "                                       \"page_start\": pix0,\n\t                                       \"page_end\": pix1})\n\t            posi_node.ix = f\"{n}@{lpn[0]}-{lpn[1]}\"\n\t            layout_nodes += [posi_node]\n\t        # todo future: propagate the layout information to the parents until root\n\t        for n in line_nodes + layout_nodes:\n\t            out.add_node(n)\n\t            edge = out.get_edge_by_ix(f'{n.src_node.ix}_{n.ix}_link')\n\t            meta = {\"created_by\": \"IntertextLayoutTagger\"}\n\t            if edge.meta is None:\n", "                edge.meta = meta\n\t            else:\n\t                edge.meta.update(meta)\n\t        return out\n\t#\n\t## Functions\n\t#\n\tdef f1000xml_to_itg(xml_path):\n\t    assert os.path.exists(xml_path) and os.path.isfile(xml_path), \"provided file path to the XML does not exist. \" \\\n\t                                                                  \"Cannot parse.\"\n", "    parser = F1000XMLParserBM(xml_path)\n\t    itg_doc = parser()\n\t    return itg_doc\n\tdef pdf_to_tei(pdf_path, config=None):\n\t    assert os.path.exists(pdf_path) and os.path.isfile(pdf_path), \"provided file path to the PDF does not exist. \" \\\n\t                                                                  \"Cannot parse.\"\n\t    try:\n\t        client = GrobidClient(**GROBID_CONF)\n\t    except ServerUnavailableException as e:\n\t        print(\"GROBID server not available. ERROR during pdf parsing.\")\n", "        raise e\n\t    # use default config if none is provided\n\t    if config is None:\n\t        # no consolidation\n\t        config = {\n\t            \"generateIDs\": False,\n\t            \"consolidate_header\": False,\n\t            \"consolidate_citations\": False,\n\t            \"include_raw_citations\": False,\n\t            \"include_raw_affiliations\": False,\n", "            \"tei_coordinates\": False,\n\t            \"segment_sentences\": False\n\t        }\n\t    _, status, parsed = client.process_pdf(\"processFulltextDocument\",\n\t                                           pdf_path,\n\t                                           **config)\n\t    return status, parsed\n\tdef tei_to_itg(tei):\n\t    parser = TEIXMLParser(tei)\n\t    itg_doc = parser()\n", "    return itg_doc\n"]}
{"filename": "nlpeer/data/collect/coling20.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom os.path import join as pjoin\n\tfrom os.path import basename\n\timport xml.etree.ElementTree as ET\n\tfrom tqdm import tqdm\n\tfrom nlpeer.data.collect import retrieve_matched, fetch_camera_ready_mapping\n\tfrom nlpeer.data.datasets.parse import pdf_to_tei\n\tfrom nlpeer.data.datasets.utils import PDFProcessingError\n", "from nlpeer.data.utils import list_files\n\tdef load_coling2020_papers(path):\n\t    basepath = pjoin(path, \"COLING-2020/COLING2020_XY/COLING2020_XY\")\n\t    papers_path = pjoin(basepath, \"paper_pdf\")\n\t    reviews_path = pjoin(basepath, \"reviews\")\n\t    reviewed_paper_ids = [int(basename(f).split(\"_\")[0]) for f in list_files(reviews_path)]\n\t    papers = {int(basename(f).split(\".\")[0]): f for f in list_files(papers_path) if int(basename(f).split(\".\")[0]) in reviewed_paper_ids}\n\t    result = {}\n\t    for pid, p_path in tqdm(papers.items(), desc=\"Iterating over papers\"):\n\t        status, tei = pdf_to_tei(p_path)\n", "        if status != 200:\n\t            raise PDFProcessingError(\"GROBID Parsing Error\")\n\t        root = ET.fromstring(tei)\n\t        prefix = \"{http://www.tei-c.org/ns/1.0}\"\n\t        # create article title as root\n\t        title = root.find(f\"{prefix}teiHeader/{prefix}fileDesc/{prefix}titleStmt/{prefix}title\").text\n\t        title = title if title is not None else \"\"\n\t        abstract = root.find(f\"{prefix}teiHeader/{prefix}profileDesc/{prefix}abstract/{prefix}div\")\n\t        if abstract:\n\t            children = list(abstract)\n", "            if children:\n\t                abstract = ' '.join(abstract.itertext()).strip()\n\t            else:\n\t                abstract = abstract.text\n\t        else:\n\t            abstract = \"\"\n\t        result[pid] = {\n\t            \"title\": title,\n\t            \"abstract\": abstract\n\t        }\n", "    print(f\"Loaded {len(papers)} papers for matching\")\n\t    with open(pjoin(path, \"extracted_titles_and_abstracts.json\"), \"w+\") as f:\n\t        json.dump(result, f)\n\t    return result\n\tdef main(args):\n\t    if not os.path.isdir(args.data_path):\n\t        raise ValueError(f\"The passed data directory {args.data_path} does not exist\")\n\t    out_path = pjoin(args.data_path, \"COLING2020_camera_ready\")\n\t    # create output directories if necessary\n\t    if not os.path.exists(out_path):\n", "        os.mkdir(out_path)\n\t    if args.download_approved:\n\t        match_table = pjoin(out_path, \"COLING2020_approved.csv\")\n\t        if not os.path.exists(match_table):\n\t            raise ValueError(f\"The passed output directories do not contain an approved matching table. Make sure \"\n\t                             f\"that you manually verified the matches produced in the first step. Simply rename the \"\n\t                             f\"file afterwards -- the resulting file path should be {match_table}.\")\n\t        retrieve_matched(match_table, outfolder=out_path)\n\t    else:\n\t        papers = load_coling2020_papers(args.data_path)\n", "        fetch_camera_ready_mapping(papers, \"COLING2020\", out_path, \"external/acl-anthology/data\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description=\"Script for matching PeerRead's COLING2020 papers against\"\n\t                                                 \"the ACL anthology. You need to call this script from the top-most\"\n\t                                                 \"project directory as a working directory and have installed the\"\n\t                                                 \"ACL anthology package. As this requires manual evaluation of the \"\n\t                                                 \"matches, this is a two step script. Start the script without a\"\n\t                                                 \"download_approved parameter to generate a matching table and after\"\n\t                                                 \"approving the table (renaming it in <dataset>_approved.csv) you can\"\n\t                                                 \"run the script again with the download flag set to retrieve the\"\n", "                                                 \"matched PDFs from the ACL anthology.\")\n\t    parser.add_argument(\n\t        \"--data_path\", type=str, help=\"Path to the directory containing all datasets including a directory PeerRead \"\n\t                                      \"with the data to process \",\n\t                        required=True\n\t    )\n\t    parser.add_argument(\n\t        \"--download_approved\", default=False, required=False, type=bool, help=\"Downloads the matched PDFs from the \"\n\t                                                                              \"anthology based on the approved paper \"\n\t                                                                              \"tables in the output directories. You \"\n", "                                                                              \"need to run the command twice, \"\n\t                                                                              \"first without this flag and then with \"\n\t                                                                              \"this flag set. \"\n\t    )\n\t    args = parser.parse_args()\n\t    main(args)"]}
{"filename": "nlpeer/data/collect/__init__.py", "chunked_list": ["import heapq\n\timport json\n\timport os\n\timport time\n\timport urllib\n\timport pandas as pd\n\tfrom tqdm import tqdm\n\tif \"acl-anthology\" not in os.environ[\"PYTHONPATH\"]:\n\t    print(\"WARNING: The ACL anthology library might be missing from your Python path! Required for matching!\")\n\tfrom anthology import Anthology\n", "from nltk.translate.bleu_score import sentence_bleu\n\tfrom os.path import join as pjoin\n\tANTHOLOGY_DATA_PATH = os.environ.get(\"ACL_ANTHOLOGY_DATAPATH\")\n\tANTHOLOGY_DATA_MAP = {\n\t    \"ACL2017\": \"P17\",\n\t    \"CONLL2016\": \"K16\",\n\t    \"COLING2020\": \"2020.coling\"\n\t}\n\tdef in_accepted_venue(accepted_at, paper_id):\n\t    return paper_id.startswith(ANTHOLOGY_DATA_MAP[accepted_at])\n", "def fetch_camera_ready_mapping(papers, venue, outpath, anthology_path=None):\n\t    if anthology_path is None:\n\t        anthology_path = ANTHOLOGY_DATA_PATH\n\t    a = Anthology(anthology_path)\n\t    anthology_papers = filter(lambda x: any(v for v in ANTHOLOGY_DATA_MAP.values() if x[0].startswith(v)),\n\t                              a.papers.items())\n\t    anthology_papers = list(map(lambda p: (\n\t        p[0], p[1].get_title(\"text\"), p[1].get_abstract(), p[1].as_citeproc_json()[0][\"author\"], p[1].pdf),\n\t                                anthology_papers))\n\t    misses = {}\n", "    matches = {}\n\t    for pid, p in tqdm(papers.items()):\n\t        rel_papers = filter(lambda x: in_accepted_venue(venue, x[0]), anthology_papers)\n\t        top_k = []\n\t        max_k = 5\n\t        for ap in rel_papers:\n\t            apid, aptitle, apabs, apauthors, apurl = ap\n\t            if \"title\" in p and p[\"title\"] is not None:\n\t                title_score = match_title(p[\"title\"], aptitle)\n\t            else:\n", "                title_score = 0\n\t            if \"abstract\" in p and p[\"abstract\"] is not None:\n\t                abs_score = match_abstracts(p[\"abstract\"], apabs)\n\t            else:\n\t                abs_score = 0\n\t            perfect_matched = (title_score > 0.9 and abs_score > 0.9)\n\t            certainty = (title_score + abs_score) / 2\n\t            uncertainty = 1 - certainty  # lower uncertainty == better == more certain\n\t            entry = {\"anthology_id\": apid,\n\t                     \"url\": apurl,\n", "                     \"title\": aptitle,\n\t                     \"authors\": apauthors,\n\t                     \"tscore\": title_score,\n\t                     \"absscore\": abs_score\n\t                     }\n\t            if perfect_matched:\n\t                print(f\"Perfect match on {pid}\")\n\t                top_k = [(uncertainty, title_score, abs_score, entry[\"anthology_id\"], entry)]\n\t                break\n\t            elif uncertainty < 0.9:\n", "                if len(top_k) < max_k:\n\t                    try:\n\t                        heapq.heappush(top_k, (uncertainty, title_score, abs_score, entry[\"anthology_id\"], entry))\n\t                    except TypeError as e:\n\t                        # could not replace, because there is an element with the same score\n\t                        # ignore\n\t                        heapq.heapify(top_k)\n\t                elif top_k[0][0] > uncertainty:\n\t                    try:\n\t                        heapq.heapreplace(top_k, (uncertainty, title_score, abs_score, entry[\"anthology_id\"], entry))\n", "                    except TypeError as e:\n\t                        # could not replace, because there is an element with the same score\n\t                        # ignore\n\t                        heapq.heapify(top_k)\n\t        if len(top_k) == 0:\n\t            print(f\"Miss on {pid}\")\n\t            misses[pid] = {\n\t                \"title\": p[\"title\"] if \"title\" in p else None,\n\t                \"accepted\": venue\n\t            }\n", "        else:\n\t            matches[pid] = {\n\t                \"title\": p[\"title\"] if \"title\" in p else None,\n\t                \"accepted\": venue,\n\t                \"matches\": [e[4] for e in top_k]\n\t            }\n\t    with open(os.path.join(outpath, f\"{venue}_matching.json\"), \"w+\") as file:\n\t        json.dump(matches, file)\n\t    if len(misses) > 0:\n\t        with open(os.path.join(outpath, f\"{venue}_missed.json\"), \"w+\") as file:\n", "            json.dump(misses, file)\n\t    df = pd.DataFrame({\"sid\": list(papers.keys()),\n\t                       \"title\": [p[\"title\"] for p in papers.values()],\n\t                       \"match\": [(matches[pid][\"matches\"][0][\"url\"] if pid in matches else \"\") for pid in\n\t                                 papers],\n\t                       \"tscore\": [(matches[sid][\"matches\"][0][\"tscore\"] if sid in matches else \"\") for sid in\n\t                                  papers],\n\t                       \"absscore\": [(matches[sid][\"matches\"][0][\"absscore\"] if sid in matches else \"\") for sid in\n\t                                    papers],\n\t                       \"other_matches\": [(len(matches[sid][\"matches\"]) if sid in matches else \"\") for sid in\n", "                                         papers]\n\t                       })\n\t    df.to_csv(os.path.join(outpath, f\"{venue}_to_be_approved.csv\"))\n\tdef match_abstracts(absPR, absACL):\n\t    absPR = absPR.strip().lower()\n\t    absACL = absACL.strip().lower()\n\t    if absPR == absACL:\n\t        return 1\n\t    else:\n\t        return sentence_bleu([absPR.split(\" \")], absACL.split(\" \"), weights=(1 / 3, 1 / 3, 1 / 3))  # uses 3-grams\n", "def match_title(titlePR, titleACL):\n\t    titlePR = titlePR.strip().lower()\n\t    titleACL = titleACL.strip().lower()\n\t    if titlePR == titleACL:\n\t        return 1\n\t    else:\n\t        return sentence_bleu([titlePR.split(\" \")], titleACL.split(\" \"), weights=(2 / 3, 1 / 3))  # uses pairs\n\tdef aggregate_mappings(paths):\n\t    res = []\n\t    for p in paths:\n", "        df = pd.read_csv(p)\n\t        sid_to_match = df[[\"sid\", \"match\"]]\n\t        res += [sid_to_match]\n\t    res = pd.concat(res, ignore_index=True)\n\t    return list(res.transpose().to_dict().values())\n\tdef get_pdf(url, out):\n\t    for i in range(4):\n\t        try:\n\t            return urllib.request.urlretrieve(url, out)\n\t        except urllib.error.URLError as e:\n", "            print(e)\n\t            time.sleep(3)\n\t    print(f\">> Failed on {url}\")\n\tdef retrieve_matched(mapping_file, outfolder):\n\t    sid_to_aclurl = aggregate_mappings([mapping_file])\n\t    for e in tqdm(sid_to_aclurl, desc=\"Iterating over matching pairs\"):\n\t        sid = e[\"sid\"]\n\t        aclurl = e[\"match\"]\n\t        get_pdf(aclurl, pjoin(outfolder, f\"{sid}.pdf\"))\n\t    pd.DataFrame.from_records(sid_to_aclurl).to_csv(pjoin(outfolder, \"sid_to_url.csv\"))\n"]}
{"filename": "nlpeer/data/collect/peerread.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom nlpeer.data.collect import retrieve_matched, fetch_camera_ready_mapping\n\tfrom nlpeer.data.utils import list_files\n\tfrom os.path import join as pjoin\n\tdef load_peerread_papers(datapath):\n\t    papers = {}\n\t    for s in [\"train\", \"dev\", \"test\"]:\n\t        s_path = pjoin(datapath, s)\n", "        parsed_path = pjoin(s_path, \"reviews\")\n\t        for p in list_files(parsed_path):\n\t            pid = os.path.basename(p)[:-len('.json')]\n\t            with open(pjoin(parsed_path, f\"{pid}.json\"), \"r\") as jf:\n\t                reviews_for_paper = json.load(jf)\n\t            papers[pid] = {\"title\": reviews_for_paper[\"title\"], \"abstract\": reviews_for_paper[\"abstract\"]}\n\t    print(f\"Loaded {len(papers)} papers for matching\")\n\t    return papers\n\tdef main(args):\n\t    if not os.path.isdir(args.data_path):\n", "        raise ValueError(f\"The passed data directory {args.data_path} does not exist\")\n\t    if not os.path.isdir(pjoin(args.data_path, \"PeerRead\")):\n\t        raise ValueError(\n\t            f\"The passed data directory {args.data_path} does not contain the peer read corpus under PeerRead\")\n\t    acl17_path = pjoin(args.data_path, \"PeerRead\", \"data\", \"acl_2017\")\n\t    conll16_path = pjoin(args.data_path, \"PeerRead\", \"data\", \"conll_2016\")\n\t    if not os.path.exists(acl17_path) or not os.path.exists(conll16_path):\n\t        raise ValueError(\n\t            f\"The paths for the ACL17 and CONLL16 don't exist on these paths: {acl17_path}, {conll16_path}\")\n\t    acl17_out_path = pjoin(args.data_path, \"ACL2017_camera_ready\")\n", "    conll16_out_path = pjoin(args.data_path, \"CONLL2016_camera_ready\")\n\t    # create output directories if necessary\n\t    if not os.path.exists(acl17_out_path):\n\t        os.mkdir(acl17_out_path)\n\t    if not os.path.isdir(conll16_out_path):\n\t        os.mkdir(conll16_out_path)\n\t    if args.download_approved:\n\t        acl_match_table = pjoin(acl17_out_path, \"ACL2017_approved.csv\")\n\t        conll_match_table = pjoin(conll16_out_path, \"CONLL2016_approved.csv\")\n\t        if not os.path.exists(acl_match_table) or not os.path.exists(conll_match_table):\n", "            raise ValueError(f\"The passed output directories do not contain an approved matching table. Make sure \"\n\t                             f\"that you manually verified the matches produced in the first step. Simply rename the \"\n\t                             f\"file afterwards -- the resulting file paths should be {acl_match_table} and \"\n\t                             f\"{conll_match_table}\")\n\t        retrieve_matched(acl_match_table, outfolder=acl17_out_path)\n\t        retrieve_matched(conll_match_table, outfolder=conll16_path)\n\t    else:\n\t        acl17_papers = load_peerread_papers(acl17_path)\n\t        fetch_camera_ready_mapping(acl17_papers, \"ACL2017\", acl17_out_path, \"external/acl-anthology/data\")\n\t        conll16_papers = load_peerread_papers(conll16_path)\n", "        fetch_camera_ready_mapping(conll16_papers, \"CONLL16\", conll16_out_path, \"external/acl-anthology/data\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(description=\"Script for matching PeerRead's ACL2017 and CONLL16 papers against\"\n\t                                                 \"the ACL anthology. You need to call this script from the top-most\"\n\t                                                 \"project directory as a working directory and have installed the\"\n\t                                                 \"ACL anthology package. As this requires manual evaluation of the \"\n\t                                                 \"matches, this is a two step script. Start the script without a\"\n\t                                                 \"download_approved parameter to generate a matching table and after\"\n\t                                                 \"approving the table (renaming it in <dataset>_approved.csv) you can\"\n\t                                                 \"run the script again with the download flag set to retrieve the\"\n", "                                                 \"matched PDFs from the ACL anthology.\")\n\t    parser.add_argument(\n\t        \"--data_path\", type=str, help=\"Path to the directory containing all datasets including a directory PeerRead \"\n\t                                      \"with the data to process \"\n\t    )\n\t    parser.add_argument(\n\t        \"--download_approved\", default=False, required=False, type=bool, help=\"Downloads the matched PDFs from the \"\n\t                                                                              \"anthology based on the approved paper \"\n\t                                                                              \"tables in the output directories. You \"\n\t                                                                              \"need to run the command twice, \"\n", "                                                                              \"first without this flag and then with \"\n\t                                                                              \"this flag set. \"\n\t    )\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "nlpeer/data/annotate/explicit_links.py", "chunked_list": ["import argparse\n\timport json\n\timport logging\n\timport os\n\timport re\n\tfrom os.path import join as pjoin\n\tfrom typing import List, Tuple, Dict\n\timport fuzzysearch\n\timport pandas as pd\n\tfrom intertext_graph.itgraph import IntertextDocument, SpanNode\n", "from tqdm import tqdm\n\tfrom nlpeer import NTYPE_TITLE, NTYPE_HEADING, NTYPE_ABSTRACT, NTYPE_FIGURE, NTYPE_TABLE, NTYPE_BIB_ITEM, DATASETS\n\tfrom nlpeer.data.utils import list_dirs, list_files\n\tOUT_PATH = os.environ.get(\"OUT_PATH\")\n\tfrom external.f1000rd.analysis.exp_linker import replace_ordinal_string_with_number, \\\n\t    split_quote, strip_numbers_from_title\n\tdef store_collection_meta(meta, base_path):\n\t    if os.path.exists(pjoin(base_path, \"meta.json\")):\n\t        with open(pjoin(base_path,\"meta.json\"), \"r\") as f:\n\t            prev = json.load(f)\n", "    else:\n\t        prev = {}\n\t    prev[\"ELINKS\"] = meta\n\t    with open(pjoin(base_path, \"meta.json\"), \"w+\") as f:\n\t        json.dump(prev, f)\n\t# adapted from F1000RD/analysis/exp_linker\n\tdef load_static_patterns(path=None):\n\t    def_path = \"resources/exp_patterns.tsv\" if path is None else path\n\t    logging.info(f\"Loading static patterns from {def_path}\")\n\t    # static patterns\n", "    patterns_df = pd.read_csv(\n\t        def_path,\n\t        delimiter='\\t'\n\t    )\n\t    return [\n\t        (row.pattern, row.type)\n\t        for row in patterns_df.itertuples()\n\t    ]\n\t# adapted from F1000RD/analysis/exp_linker\n\tdef load_patterns(paper_doc: IntertextDocument):\n", "    # get headings\n\t    headings = []\n\t    for node in paper_doc.nodes:\n\t        if node.ntype == NTYPE_HEADING:\n\t            headings += [node.content]\n\t        elif node.ntype == NTYPE_ABSTRACT:\n\t            headings += [node.content]\n\t    # generate patterns from headings\n\t    hd_patterns = make_patterns_from_section_titles(headings)\n\t    return hd_patterns\n", "# adapted from F1000RD\n\tdef make_patterns_from_section_titles(section_titles: [str]):\n\t    \"\"\"\n\t    Make patterns from section titles\n\t    strip off numbering\n\t    \"\"\"\n\t    # Lambda functions that return regex patterns\n\t    TEMPLATES = [\n\t        lambda x: rf'^.{{,3}}(?P<ix>{x}).{{,3}}$', # Review headline with section title\n\t        lambda x: rf'\\bthe (?P<ix>{x})', # \"the results\"\n", "        lambda x: rf'^.{{,3}}(?P<ix>{x}) ?[-:;.]', # \"Results: bla bla\"\n\t        lambda x: rf'(in|under) (?P<ix>{x})', # \"in methods\"\n\t        lambda x: rf'\\b[\"\\'](?P<ix>{x})[\"\\']\\b'\n\t        # Section title in quotation marks of various kinds\n\t    ]\n\t    # Make pattern for each combination of section title and pattern template\n\t    patterns = []\n\t    for section_title in section_titles:\n\t        for template_func in TEMPLATES:\n\t            stripped_title = strip_numbers_from_title(re.escape(section_title.lower()))\n", "            pattern = template_func(stripped_title)\n\t            patterns.append((pattern, 'sec-name'))\n\t    return patterns\n\t# adapted from F1000RD/analysis/exp_linker\n\tdef find_pointers(txt: str, patterns: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n\t    out = []\n\t    for pat, cat in patterns:\n\t        res = re.finditer(pat, txt.lower())\n\t        for r in res:\n\t            value = r.group('ix')\n", "            value = replace_ordinal_string_with_number(\n\t                value\n\t            )\n\t            out += [(cat, value, r.regs[0])]\n\t    return out\n\t# extended F1000RD\n\tdef find_targets_for_sec_name(tgt_itg: IntertextDocument, match_text: str):\n\t    \"\"\"Handle \"sec-name\" matches\"\"\"\n\t    ret = []\n\t    META = {\n", "        'exp': ['sec-name']\n\t    }\n\t    # Handle title, abstract and regular section names separately, because\n\t    # they link to different ntypes\n\t    if match_text == 'title':\n\t        for tgt_node in tgt_itg.nodes:\n\t            if tgt_node.ntype == NTYPE_TITLE:\n\t                ret.append(tuple((META, tgt_node)))\n\t    elif match_text == 'abstract':\n\t        for tgt_node in tgt_itg.nodes:\n", "            if tgt_node.ntype == NTYPE_ABSTRACT:\n\t                ret.append(tuple((META, tgt_node)))\n\t    else:\n\t        for tgt_node in tgt_itg.nodes:\n\t            if tgt_node.ntype == NTYPE_HEADING:\n\t                if match_text in tgt_node.content.lower():\n\t                    ret.append(tuple((META, tgt_node)))\n\t    return ret\n\t#extended F1000RD\n\tdef find_targets_for_sec_ix(tgt_itg: IntertextDocument, match_text: str):\n", "    \"\"\"Handle section index matches\"\"\"\n\t    ret = []\n\t    META = {\n\t        'exp': ['sec-ix']\n\t    }\n\t    for tgt_node in tgt_itg.nodes:\n\t        if tgt_node.ntype == NTYPE_HEADING:\n\t            try:\n\t                if tgt_node.meta['section'] == match_text:\n\t                    ret.append(tuple((META, tgt_node)))\n", "            except KeyError:\n\t                continue\n\t    return ret\n\t# extended F1000RD\n\tdef find_targets_for_fig_ix(tgt_itg: IntertextDocument, match_text: str):\n\t    \"\"\"Handles figure index matches\"\"\"\n\t    ret = []\n\t    META = {\n\t        'exp': ['fig-ix']\n\t    }\n", "    for tgt_node in tgt_itg.nodes:\n\t        if tgt_node.ntype == NTYPE_FIGURE:\n\t            try:\n\t                if tgt_node.meta['label'] == f'{match_text}':\n\t                    ret.append(tuple((META, tgt_node)))\n\t            except KeyError:\n\t                continue\n\t    return ret\n\t# extended F1000RD\n\tdef find_targets_for_table_ix(tgt_itg: IntertextDocument,  match_text: str):\n", "    \"\"\"Handles table index matches\"\"\"\n\t    ret = []\n\t    META = {\n\t        'exp': ['table-ix']\n\t    }\n\t    for tgt_node in tgt_itg.nodes:\n\t        if tgt_node.ntype == NTYPE_TABLE:\n\t            try:\n\t                if tgt_node.meta['label'] == f'{match_text}':\n\t                    ret.append(tuple((META, tgt_node)))\n", "            except KeyError:\n\t                continue\n\t    return ret\n\t# from F1000RD\n\tdef find_targets_for_quote(tgt_itg: IntertextDocument, match_text: str):\n\t    \"\"\"Find targets for quotes\"\"\"\n\t    # Hard code the hyperparameters\n\t    # The maximum Levenshtein distance for two strings to be counted as a match\n\t    # The higher the value, the more hits are obtained (probably resulting in lower\n\t    # precision)\n", "    MAX_LEVENSHTEIN_DISTANCE = 2\n\t    # The maximum number of hits for a single quote\n\t    # This is to not bet trapped by very unspecific quotes / partial quotes such as \"and\",\n\t    # which have countless hits\n\t    # A higher number probably increases recall, but reduces precision\n\t    MAX_N_HITS = 2\n\t    ret = []\n\t    META={\n\t        'exp': ['quote']\n\t    }\n", "    # Split quote when it has gaps (\"bla [...] bla\")\n\t    partial_quotes = split_quote(match_text)\n\t    # For each split check if matches can be found\n\t    for tgt_node in tgt_itg.nodes:\n\t        if type(tgt_node) == SpanNode:\n\t            continue\n\t        partial_quote_idx = 0\n\t        content_start_pos = 0\n\t        match = True\n\t        # Go over each partial quote, look for matches\n", "        # If match found, for the next quote only look for matches after\n\t        # the previously found match\n\t        while (partial_quote_idx < len(partial_quotes)) & match:\n\t            partial_quote = partial_quotes[partial_quote_idx]\n\t            search_res = fuzzysearch.find_near_matches(\n\t                partial_quote,\n\t                tgt_node.content[content_start_pos:].lower(),\n\t                max_l_dist=MAX_LEVENSHTEIN_DISTANCE\n\t            )\n\t            if len(search_res) > 0:\n", "                top_res = sorted(search_res, key=lambda x: x.dist)[0]\n\t                content_start_pos = top_res.end + 1\n\t                partial_quote_idx += 1\n\t            else:\n\t                match = False\n\t        if match:\n\t            ret.append(tuple((META, tgt_node)))\n\t            # Check if more than MAX_N_HITS matches were found for the quote\n\t            # If yes, return no results\n\t            if len(ret) > MAX_N_HITS:\n", "                ret = []\n\t                break\n\t    return ret\n\tdef find_targets_for_ref_ix(tgt_itg: IntertextDocument,  match_text: str):\n\t    \"\"\"Find links to references\"\"\"\n\t    ret = []\n\t    META = {\n\t        'exp': ['ref-ix']\n\t    }\n\t    for tgt_node in tgt_itg.nodes:\n", "        if (tgt_node.ntype == NTYPE_BIB_ITEM):\n\t            try:\n\t                if tgt_node.meta['id'] == f'ref-{match_text}':\n\t                    ret.append(tuple((META, tgt_node)))\n\t            except KeyError:\n\t                continue\n\t    return ret\n\tdef find_targets_for_line(tgt_itg: IntertextDocument, match_text: str):\n\t    \"\"\" Find links for line references\"\"\"\n\t    ret = []\n", "    META = {\n\t        'exp': ['line']\n\t    }\n\t    for tgt_node in tgt_itg.nodes:\n\t        if type(tgt_node) == SpanNode and tgt_node.ntype == \"line\":\n\t            l = tgt_node.meta[\"line\"]\n\t            if int(l) == int(match_text):\n\t                ret += [(META, tgt_node)]\n\t                break  # perfect match, break\n\t        elif type(tgt_node) == SpanNode and tgt_node.ntype == \"line_range\":\n", "            ls, le = tgt_node.meta[\"line_start\"], tgt_node.meta[\"line_end\"]\n\t            if int(ls) <= int(match_text) <= int(le):\n\t                ret += [(META, tgt_node)]\n\t                # range match, continue looking for perfect matches\n\t    if len(ret) > 1:\n\t        line_node = [(m, t) for (m, t) in ret if \"line\" in t.meta]\n\t        if len(line_node) > 0:\n\t            return line_node\n\t        else:\n\t            best_range, best_match = int(ret[0][1].meta[\"line_end\"]) - int(ret[0][1].meta[\"line_start\"]), ret[0]\n", "            for m, t in ret[1:]:\n\t                w = int(t.meta[\"line_end\"]) - int(t.meta[\"line_start\"])\n\t                if w < best_range:\n\t                    best_range = w\n\t                    best_match = (m, t)\n\t            return [best_match]\n\t    else:\n\t        return ret\n\t# extended F1000RD\n\tdef find_anchors_for_match(doc, match):\n", "    mtype, mtxt = match\n\t    if mtype == 'sec-name':\n\t        return find_targets_for_sec_name(doc, mtxt)\n\t    elif mtype == 'sec-ix':\n\t        return find_targets_for_sec_ix(doc, mtxt)\n\t    elif mtype == 'fig-ix':\n\t        return find_targets_for_fig_ix(doc, mtxt)\n\t    elif mtype == 'table-ix':\n\t        return find_targets_for_table_ix(doc, mtxt)\n\t    elif mtype == 'quote':\n", "        return find_targets_for_quote(doc, mtxt)\n\t    elif mtype == 'ref-ix':\n\t        return find_targets_for_ref_ix(doc, mtxt)\n\t    elif mtype == \"line\":\n\t        return find_targets_for_line(doc, mtxt)\n\t    else:\n\t        return None\n\t# adapted from F1000RD/analysis/exp_linker\n\tdef find_anchors_in_paper(pointers, paper):\n\t    links = {}\n", "    for sec in pointers:\n\t        pointer_types = list(set([(t, txt) for (t, txt, span) in pointers[sec]]))\n\t        links_by_type = []\n\t        for match in pointer_types:\n\t            found_targets = find_anchors_for_match(paper, match)\n\t            if found_targets is None or len(found_targets) == 0:\n\t                links_by_type += [None]\n\t            else:\n\t                links_by_type += [(match, found_targets[0][0], found_targets[0][1])]\n\t        links[sec] = [[(pt,l) for pt, l in zip(pointer_types, links_by_type) if pt[0] == pointer[0] and pt[1] == pointer[1]][0][1] for pointer in pointers[sec]]\n", "    return links\n\tdef find_pointers_in_review(review, patterns):\n\t    report = review[\"report\"]\n\t    pointers = {}\n\t    for sec in report:\n\t        section = report[sec]\n\t        if section is None:\n\t            continue\n\t        pointers[sec] = find_pointers(section, patterns)\n\t    return pointers\n", "def create(dataset_type: DATASETS, bm_path=None):\n\t    # paths\n\t    bm_path = OUT_PATH if bm_path is None else bm_path\n\t    dataset_path = pjoin(bm_path, dataset_type.value, \"data\")\n\t    out_path =pjoin(bm_path, dataset_type.value, \"annotations\")\n\t    if not os.path.exists(out_path):\n\t        os.mkdir(out_path)\n\t    # meta info on dataset\n\t    collection_meta = {\n\t        \"dataset\": \"Benchmark Explicit Links by F1000RD REGEX Linker\",\n", "        \"annotation_types\": [\n\t            \"elinks\"\n\t        ]\n\t    }\n\t    store_collection_meta(collection_meta, out_path)\n\t    # load patterns\n\t    static_patterns = load_static_patterns()\n\t    # iterate papers and reviews\n\t    for ppath in tqdm(list_dirs(dataset_path), desc=\"iterating over papers\"):\n\t        pid = os.path.basename(ppath)\n", "        for vpath in list_dirs(ppath):\n\t            version = os.path.basename(vpath)\n\t            if \"reviews.json\" not in [os.path.basename(f) for f in list_files(vpath)]:\n\t                continue\n\t            # input\n\t            with open(pjoin(vpath, \"paper.itg.json\"), \"r\") as f:\n\t                pdoc = IntertextDocument.load_json(f)\n\t            with open(pjoin(vpath, \"reviews.json\"), \"r\") as f:\n\t                reviews = json.load(f)\n\t            if len(reviews) == 0:\n", "                continue\n\t            # do matching\n\t            dyn_patterns = load_patterns(pdoc)\n\t            links_per_rev = {}\n\t            pointers_per_rev = {}\n\t            for r in reviews:\n\t                pointers = find_pointers_in_review(r, static_patterns + dyn_patterns)\n\t                links = find_anchors_in_paper(pointers, pdoc)\n\t                pointers_per_rev[r[\"rid\"]], links_per_rev[r[\"rid\"]] = pointers, links\n\t            # output\n", "            output = {}\n\t            for rid in pointers_per_rev:\n\t                pointers, links = pointers_per_rev[rid], links_per_rev[rid]\n\t                output[rid] = {}\n\t                for sec in pointers:\n\t                    output[rid][sec] = []\n\t                    for p, l in zip(pointers[sec], links[sec]):\n\t                        ptype, ptxt, pspan = p\n\t                        if l is None:\n\t                            output[rid][sec] += [{\n", "                                \"type\": ptype,\n\t                                \"rev_span\": pspan,\n\t                                \"rev_text\": ptxt,\n\t                                \"paper_target\": None\n\t                            }]\n\t                        else:\n\t                            lmatch, lmeta, lnode = l\n\t                            output[rid][sec] += [{\n\t                                \"type\": ptype,\n\t                                \"rev_span\": pspan,\n", "                                \"rev_text\": ptxt,\n\t                                \"paper_target\": lnode.ix\n\t                            }]\n\t            links_out_path = pjoin(out_path, pid, version, \"elinks.json\")\n\t            if not os.path.exists(pjoin(out_path, pid)):\n\t                os.mkdir(pjoin(out_path, pid))\n\t            if not os.path.exists(pjoin(out_path, pid, version)):\n\t                os.mkdir(pjoin(out_path, pid, version))\n\t            with open(links_out_path, \"w+\") as f:\n\t                json.dump(output, f)\n", "def arg_parse():\n\t    parser = argparse.ArgumentParser(description=\"Creating the F1000 dataset within the benchmark\")\n\t    parser.add_argument(\n\t        \"--bm_path\", type=str, required=True, help=\"Path to the directory of the benchmark dataset\"\n\t    )\n\t    parser.add_argument(\n\t        \"--dataset_type\", type=str, required=False, choices=[d.name for d in DATASETS], default=None, help=\"Dataset type\"\n\t    )\n\t    return parser\n\tdef main(args):\n", "    logging.basicConfig(level=\"INFO\")\n\t    if args.dataset_type is None:\n\t        for d in DATASETS:\n\t            create(d, args.bm_path)\n\t    else:\n\t        create(DATASETS[args.dataset_type], args.bm_path)\n\tif __name__ == \"__main__\":\n\t    parser = arg_parse()\n\t    main(parser.parse_args())"]}
{"filename": "nlpeer/data/annotate/__init__.py", "chunked_list": []}
{"filename": "nlpeer/data/annotate/f1000rd.py", "chunked_list": ["import argparse\n\timport json\n\timport logging\n\timport os\n\timport re\n\tfrom os.path import join as pjoin\n\timport fuzzysearch\n\timport pandas as pd\n\tfrom intertext_graph.itgraph import IntertextDocument, SpanNode\n\tfrom nltk.translate.bleu_score import sentence_bleu\n", "from tqdm import tqdm\n\tfrom nlpeer import NTYPE_ABSTRACT\n\tfrom nlpeer.data.datasets.utils import get_review_sentences\n\tfrom nlpeer.data.utils import list_dirs, list_files\n\tDATA_PATH = os.environ.get(\"F1000_RD_PATH\")\n\tOUT_PATH = os.environ.get(\"OUT_PATH\")\n\tdef store_collection_meta(meta, base_path):\n\t    if os.path.exists(pjoin(base_path, \"meta.json\")):\n\t        with open(pjoin(base_path,\"meta.json\"), \"r\") as f:\n\t            prev = json.load(f)\n", "    else:\n\t        prev = {}\n\t    prev[\"F1000RD\"] = meta\n\t    with open(pjoin(base_path, \"meta.json\"), \"w+\") as f:\n\t        json.dump(prev, f)\n\tdef load_review_reports(path):\n\t    revs = {}\n\t    rid_map = {}\n\t    for paper_path in list_dirs(path):\n\t        pid = os.path.basename(paper_path)\n", "        revs[pid] = {}\n\t        for r in list_files(pjoin(paper_path, \"reviews\")):\n\t            with open(r, \"r\") as f:\n\t                report = IntertextDocument.load_json(f)\n\t            rid = os.path.basename(r)[:-len(\".json\")]\n\t            revs[pid][rid] = report\n\t            rid_map[rid] = pid\n\t    return revs, rid_map\n\tdef load_papers(path):\n\t    papers = {}\n", "    for paper_path in list_dirs(path):\n\t        pid = os.path.basename(paper_path)\n\t        with open(pjoin(paper_path, \"v1.json\"), \"r\") as f:\n\t            papers[pid] = IntertextDocument.load_json(f)\n\t    return papers\n\tdef match_and_update_review_sentences(review_rd:IntertextDocument, review_orig):\n\t    new_sentences = []\n\t    matched = {}\n\t    review_text = review_orig[\"report\"][\"main\"]\n\t    review_sentences = get_review_sentences(review_orig)[\"main\"]\n", "    review_sentences_cleaned = [re.sub(\"[^A-Za-z1-9 \\-]\", \"\", s.lower()).strip() for s in review_sentences]\n\t    unmatched = []\n\t    for snode in review_rd.nodes:\n\t        if type(snode) != SpanNode:\n\t            continue\n\t        sentence = snode.content\n\t        # perfect match -- add to matches, done\n\t        start_match, end_match = -1, -1\n\t        if sentence.lower() in review_text.lower():\n\t            start_match = review_text.lower().find(sentence.lower())\n", "            end_match = start_match + len(sentence)\n\t        else:\n\t            search_res = fuzzysearch.find_near_matches(\n\t                sentence.lower(),\n\t                review_text.lower(),\n\t                max_l_dist=2\n\t            )\n\t            if 0 < len(search_res) <= 3:\n\t                best = list(sorted(search_res, key= lambda x: x.dist))[0]\n\t                start_match = best.start\n", "                end_match = best.end\n\t        if start_match < 0:\n\t            unmatched += [(snode.start, snode.end, sentence)]\n\t            continue\n\t        new_sentences += [(start_match, end_match)]\n\t        matched[snode.ix] = len(new_sentences) - 1\n\t    review_orig[\"meta\"][\"sentences\"][\"main\"] = [[nsent[0], nsent[1]] for nsent in new_sentences]\n\t    return matched, unmatched\n\tdef annotate_review_sentences(sentence_mapping, label_map):\n\t    errors = []\n", "    annotations = {}\n\t    for six, label in label_map.items():\n\t        if six in sentence_mapping:\n\t            annotations[sentence_mapping[six]] = label\n\t        else:\n\t            errors += [six]\n\t    return annotations, errors\n\tdef match_and_update_paper_sentences(paper_rd: IntertextDocument, paper_orig:IntertextDocument):\n\t    def is_in_node(sentence, node):\n\t        return sentence.lower().strip() in node.content.lower()\n", "    def heuristic_match(sentence):\n\t        # todo\n\t        return None\n\t    # add sentence nodes from the F1000RD version\n\t    matched = {}\n\t    matched_nodes = {}\n\t    unmatched = []\n\t    for snode in paper_rd.nodes:\n\t        if type(snode) != SpanNode or snode.ntype != \"s\":\n\t            continue\n", "        sentence = snode.content\n\t        match = False\n\t        for cnode in paper_orig.nodes:\n\t            if type(cnode) == SpanNode:\n\t                continue\n\t            # account for listings, which are represented differently in F1000RD\n\t            if not is_in_node(sentence, cnode):\n\t                while sentence[0] == \"-\":\n\t                    sentence = sentence[1:]\n\t            if is_in_node(sentence, cnode):\n", "                start_orig = cnode.content.lower().find(sentence.lower())\n\t                matched[snode.ix] = (cnode.ix, start_orig, start_orig + len(sentence.strip().lower()))\n\t                matched_nodes[snode.src_node.ix] = cnode.ix\n\t                match = True\n\t                break\n\t        if not match:\n\t            # special case \"abstract\"\n\t            if sentence.lower().strip() == \"abstract\":\n\t                oanode, osnode = None, None\n\t                oanodes = [n for n in paper_orig.nodes if n.ntype == NTYPE_ABSTRACT]\n", "                if len(oanodes) > 0:\n\t                    oanode = oanodes[0]\n\t                    osnodes = [n for n in paper_orig.nodes if type(n) == SpanNode and n.ntype == \"s\" and n.src_node.ix == oanode.ix]\n\t                    if len(osnodes) > 0:\n\t                        osnode = osnodes[0]\n\t                if oanode is not None:\n\t                    matched[snode.ix] = (oanode.ix, osnode.start, osnode.end)\n\t                    matched_nodes[snode.src_node.ix] = oanode.ix\n\t            else:\n\t                best_match = heuristic_match(sentence)    # not implemented atm\n", "                if best_match is None:\n\t                    unmatched += [(snode.start, snode.end, sentence)]\n\t                else:\n\t                    matched[snode.ix] = (best_match[0].ix, best_match[1], best_match[2])\n\t                    matched_nodes[snode.src_node.ix] = best_match[0].ix\n\t    # extend node matching by exact texts\n\t    for cnode in paper_rd.nodes:\n\t        if type(cnode) == SpanNode or cnode.ix in matched_nodes:\n\t            continue\n\t        c = cnode.content.strip().lower()\n", "        for conode in paper_orig.nodes:\n\t            if type(conode) == SpanNode:\n\t                continue\n\t            if c == conode.content.strip().lower():\n\t                matched_nodes[cnode.ix] = conode.ix\n\t    # delete previous setence nodes\n\t    to_delete = []\n\t    for n in paper_orig.nodes:\n\t        if type(n) == SpanNode and n.ntype == \"s\":\n\t            to_delete += [n]\n", "    for n in to_delete:\n\t        paper_orig.remove_node(n)\n\t    match_map = {}\n\t    for rd_ix in matched:\n\t        orig_ix, orig_start, orig_end = matched[rd_ix]\n\t        sentNode = SpanNode(ntype=\"s\",\n\t                            src_node=paper_orig.get_node_by_ix(orig_ix),\n\t                            start=orig_start,\n\t                            end=orig_end,\n\t                            meta=paper_rd.get_node_by_ix(rd_ix).meta)\n", "        #todo update ix to match pattern of pid_ver_nodenum@sentnum\n\t        paper_orig.add_node(sentNode)\n\t        match_map[rd_ix] = sentNode.ix\n\t    return match_map, matched_nodes, unmatched\n\tdef create(in_path=None, out_path=None):\n\t    # paths\n\t    in_path = DATA_PATH if in_path is None else in_path\n\t    out_path = OUT_PATH if out_path is None else out_path\n\t    out_f1000_path = pjoin(out_path, \"F1000\", \"data\")\n\t    out_annotations_path = pjoin(out_path, \"F1000\", \"annotations\")\n", "    assert out_path is not None and in_path is not None, \"Cannot create F1000RD dataset. In and/or out paths are \" \\\n\t                                                         \"missing! \"\n\t    logging.info(f\"Loading data from {in_path}\")\n\t    logging.info(f\"Storing data at {out_path}\")\n\t    if not os.path.exists(out_annotations_path):\n\t        os.mkdir(out_annotations_path)\n\t    # paths\n\t    prag_path = pjoin(in_path, \"data\", \"simple\", \"prag.csv\")\n\t    exp_links_path = pjoin(in_path, \"data\", \"simple\", \"exp_links.csv\")\n\t    imp_links_path = pjoin(in_path, \"data\", \"simple\", \"imp_links.csv\")\n", "    itgs_path = pjoin(in_path, \"data\", \"itg\")\n\t    # meta info on dataset\n\t    collection_meta = {\n\t        \"dataset_timestamp\": os.path.getmtime(itgs_path),\n\t        \"origin_dataset\": \"F1000RD\",\n\t        \"annotation_types\": [\n\t            \"rd_review_pragmatics\",\n\t            \"rd_elinks\",\n\t            \"rd_ilinks\"\n\t        ]\n", "    }\n\t    with open(pjoin(in_path, \"LICENSE.txt\"), \"r\") as f:\n\t        collection_meta[\"license\"] = f.read().strip()\n\t    store_collection_meta(collection_meta, out_annotations_path)\n\t    # load review reports per paper and papers\n\t    pid_to_reviews, rid_map = load_review_reports(itgs_path)\n\t    pid_to_papers = load_papers(itgs_path)\n\t    # copying review annotations\n\t    logging.info(\"Adding annotations from F1000RD to F1000/annotations\")\n\t    prags = pd.read_csv(prag_path, header=0)\n", "    elinks = pd.read_csv(exp_links_path, header=0)\n\t    ilinks = pd.read_csv(imp_links_path, header=0)\n\t    match_error = []\n\t    anno_error = []\n\t    prag_annotations, elink_annotations, ilink_annotations = {}, {}, {}\n\t    for rid in tqdm(rid_map, desc=\"Iterating over reviews\"):\n\t        pid = rid_map[rid]\n\t        rd_review = pid_to_reviews[pid][rid]\n\t        rd_paper = pid_to_papers[pid]\n\t        # get reference review from F1000 to transfer annotations\n", "        orig_reviews_path = pjoin(out_f1000_path, pid, \"v1\", \"reviews.json\")\n\t        with open(orig_reviews_path, \"r\") as f:\n\t            orig_reviews = json.load(f)\n\t        orig_review = next(r for r in orig_reviews if r[\"rid\"] == rid)\n\t        # update sentences in the review\n\t        rev_matched_sents, rev_unmatched_sents = match_and_update_review_sentences(rd_review, orig_review)\n\t        if len(rev_unmatched_sents) > 0:\n\t            match_error += [(\"review\", rid, r) for r in rev_unmatched_sents]\n\t        # store changed review sentences\n\t        with open(orig_reviews_path, \"w+\") as f:\n", "            json.dump(orig_reviews, f)\n\t        # get reference paper from F1000 to transfer annotations\n\t        orig_paper_path = pjoin(out_f1000_path, pid, \"v1\", \"paper.itg.json\")\n\t        with open(orig_paper_path, \"r\") as f:\n\t            orig_paper = IntertextDocument.load_json(f)\n\t        # update paper sentences\n\t        paper_matched_sents, matched_nodes, paper_unmatched_sents = match_and_update_paper_sentences(rd_paper, orig_paper)\n\t        if len(paper_unmatched_sents) > 0:\n\t            match_error += [(\"paper\", pid, r) for r in paper_unmatched_sents]\n\t        paper_matched_sents.update(matched_nodes)\n", "        # store changed paper sentences\n\t        with open(orig_paper_path, \"w+\")as f:\n\t            orig_paper.save_json(f)\n\t        # pragmatics\n\t        pmap = {}\n\t        for _, pentry in prags[prags.review_id == rid].iterrows():\n\t            pmap[pentry[\"review_sentence_id\"]] = pentry[\"prag\"]\n\t        pannos, errs = annotate_review_sentences(rev_matched_sents, pmap)\n\t        anno_error += [(\"prag\", pid, rid, e) for e in errs]\n\t        if pid not in prag_annotations:\n", "            prag_annotations[pid] = {}\n\t        if rid not in prag_annotations[pid]:\n\t            prag_annotations[pid][rid] = pannos\n\t        # explicit links\n\t        errs = explicit_link_annotations(elink_annotations, elinks, paper_matched_sents, pid, rev_matched_sents, rid, orig_review)\n\t        anno_error += [(\"elink\", pid, rid, e) for e in errs]\n\t        # implicit links\n\t        errs = implicit_link_annotations(ilink_annotations, ilinks, paper_matched_sents, pid,\n\t                                 rev_matched_sents, rid)\n\t        anno_error += [(\"ilink\", pid, rid, e) for e in errs]\n", "    # ignore boilerplate not matched errors\n\t    anno_error = [e for e in anno_error if not(e[0] == \"prag\" and e[-1].endswith(\"_0@0\"))]\n\t    # write to annotations\n\t    for o, a in [(\"rd_review_pragmatics\", prag_annotations),\n\t                 (\"rd_elinks\", elink_annotations),\n\t                 (\"rd_ilinks\", ilink_annotations)]:\n\t        for pid in a:\n\t            if len(a[pid]) == 0:\n\t                continue\n\t            if not os.path.exists(pjoin(out_annotations_path, pid)):\n", "                os.mkdir(pjoin(out_annotations_path, pid))\n\t            if not os.path.exists(pjoin(out_annotations_path, pid, \"v1\")):\n\t                os.mkdir(pjoin(out_annotations_path, pid, \"v1\"))\n\t            p_path = pjoin(out_annotations_path, pid, \"v1\", f\"{o}.json\")\n\t            with open(p_path, \"w+\") as f:\n\t                json.dump(a[pid], f)\n\t    # log errors\n\t    logging.info(\"Matching Errors\")\n\t    for e in match_error:\n\t        if e[2][2].strip().startswith(\"Reviewer response for version\"):\n", "            continue\n\t        logging.info(f\"Failed to match for {e[0]} {e[1]} with start= {e[2][0]}; end= {e[2][1]} and sentence: {e[2][2]}\")\n\t    logging.warning(\"Annotation Transfer Errors\")\n\t    for e in anno_error:\n\t        logging.warning(f\"Failed to transfer annotations: {e}\")\n\tdef implicit_link_annotations(ilink_annotations, ilinks, paper_matched_sents, pid, rev_matched_sents,\n\t                             rid):\n\t    errors = []\n\t    rows = list(ilinks[ilinks.review_id == rid].iterrows())\n\t    if len(rows) > 0:\n", "        if pid not in ilink_annotations:\n\t            ilink_annotations[pid] = {}\n\t        if rid not in ilink_annotations[pid]:\n\t            ilink_annotations[pid][rid] = {}\n\t        for _, entry in rows:\n\t            new_rsent_id = rev_matched_sents[entry[\"review_sentence_id\"]] if entry[\n\t                                                                                 \"review_sentence_id\"] in rev_matched_sents else None\n\t            new_psent_id = paper_matched_sents[entry[\"paper_sentence_id\"]] if entry[\n\t                                                                                  \"paper_sentence_id\"] in paper_matched_sents else None\n\t            imp_a = entry[\"imp_a\"] if pd.notna(entry[\"imp_a\"]) else 0\n", "            imp_b = entry[\"imp_b\"] if pd.notna(entry[\"imp_b\"]) else 0\n\t            linked = int(imp_a) + int(imp_b)\n\t            if linked > 0 and (new_psent_id is None or new_rsent_id is None):\n\t                errors += [(entry[\"review_sentence_id\"], entry[\"paper_sentence_id\"])]\n\t                continue\n\t            if linked > 0:\n\t                ilink_annotations[pid][rid][new_rsent_id] = (new_psent_id, linked)\n\t    return errors\n\tdef explicit_link_annotations(elink_annotations, elinks, paper_matched_sents, pid, rev_matched_sents, rid, review):\n\t    errors = []\n", "    rows = list(elinks[elinks.review_id == rid].iterrows())\n\t    if len(rows) > 0:\n\t        if pid not in elink_annotations:\n\t            elink_annotations[pid] = {}\n\t        if rid not in elink_annotations[pid]:\n\t            elink_annotations[pid][rid] = {\"main\": []}\n\t        for _, entry in rows:\n\t            new_rsent_id = rev_matched_sents[entry[\"review_sentence_id\"]] if entry[\n\t                                                                                 \"review_sentence_id\"] in rev_matched_sents else None\n\t            new_psent_id = paper_matched_sents[entry[\"paper_sentence_id\"]] if entry[\n", "                                                                                  \"paper_sentence_id\"] in paper_matched_sents else None\n\t            if new_psent_id is None or new_rsent_id is None:\n\t                errors += [(entry[\"type\"], entry[\"review_sentence_id\"], entry[\"paper_sentence_id\"], entry[\"review_text\"], entry[\"paper_text\"])]\n\t                continue\n\t            sent_span = review[\"meta\"][\"sentences\"][\"main\"][int(new_rsent_id)]\n\t            sent = review[\"report\"][\"main\"][sent_span[0]: sent_span[1]]\n\t            elink_annotations[pid][rid][\"main\"] += [{\"type\": entry[\"type\"], \"rev_span\": sent_span, \"rev_text\": sent}]\n\t    return errors\n\tdef arg_parse():\n\t    parser = argparse.ArgumentParser(description=\"Creating the F1000 dataset within the benchmark\")\n", "    parser.add_argument(\n\t        \"--data_path\", type=str, help=\"Path to the directory of the F1000 dataset.\"\n\t    )\n\t    parser.add_argument(\n\t        \"--output_directory\", type=str, help=\"Path to the top directory of the benchmark dataset.\"\n\t    )\n\t    return parser\n\tdef main(args):\n\t    logging.basicConfig(level=\"INFO\")\n\t    create(args.data_path, args.output_directory)\n", "if __name__ == \"__main__\":\n\t    parser = arg_parse()\n\t    main(parser.parse_args())"]}
{"filename": "nlpeer/tasks/__init__.py", "chunked_list": ["import random\n\tfrom typing import Tuple, List\n\timport numpy as np\n\timport sklearn\n\timport sklearn.metrics\n\timport spacecutter.losses\n\timport torch\n\timport torchmetrics\n\tfrom intertext_graph import Node, Etype, IntertextDocument, SpanNode\n\tfrom torch.nn import CrossEntropyLoss, NLLLoss\n", "from torch.optim import AdamW\n\tfrom torch.utils.data import Dataset\n\tfrom nlpeer.data import paperwise_stratified_split\n\tfrom nlpeer import NTYPE_TITLE, NTYPE_HEADING, NTYPE_PARAGRAPH, NTYPE_ABSTRACT, DATASETS, ANNOTATION_TYPES, \\\n\t    DATASET_REVIEW_OVERALL_SCALES, PaperReviewDataset, ReviewPaperDataset, PaperReviewAnnotations\n\tdef get_optimizer(name):\n\t    if name == \"adam\":\n\t        return AdamW\n\t    else:\n\t        raise ValueError(\"Unknown optimizer\")\n", "def get_loss_function(name, **kwargs):\n\t    name = name.lower()\n\t    if name == \"mse\":\n\t        return torch.nn.MSELoss()\n\t    elif name == \"mrse\":\n\t        return lambda x, y: torch.sqrt(torch.nn.MSELoss()(x, y)) #fixme\n\t    elif name == \"cll\": # cumulative link loss\n\t        ## https://people.csail.mit.edu/jrennie/papers/ijcai05-preference.pdf\n\t        ## https://fa.bianp.net/blog/2013/loss-functions-for-ordinal-regression/\n\t        ## https://www.ethanrosenthal.com/2018/12/06/spacecutter-ordinal-regression/\n", "        return spacecutter.losses.CumulativeLinkLoss()\n\t    elif name == \"ce\":\n\t        return CrossEntropyLoss()\n\t    elif name == \"nll\":\n\t        return NLLLoss()\n\t    elif name == \"nll-weighted\":\n\t        return NLLLoss(weight=torch.tensor([0.4, 0.6]))\n\t    elif name == \"acc_reg_3\":\n\t        def acc_at_reg_intervals_3(x, y):\n\t            acc = torchmetrics.Accuracy().cuda()\n", "            x2 = torch.add(torch.round(torch.multiply(x, 2)), 1).to(torch.int)\n\t            y2 = torch.add(torch.round(torch.multiply(y, 2)), 1).to(torch.int)\n\t            return acc(x2, y2)\n\t        return acc_at_reg_intervals_3\n\t    elif name == \"acc\":\n\t        return torchmetrics.Accuracy()\n\t    elif name == \"f1-micro\":\n\t        return lambda x, y: sklearn.metrics.f1_score(x, y, average=\"micro\")\n\t    elif name == \"f1-macro\":\n\t        return lambda x, y: sklearn.metrics.f1_score(x, y, average=\"macro\")\n", "    elif name == \"f1-macro-rounded\":\n\t        sc = kwargs[\"score_range\"]\n\t        def metric(x, y):\n\t            print(\"x......\", x.tolist())\n\t            print(\"y.......\", y.tolist())\n\t            xh = histogram([(s-min(sc)) / (max(sc) - min(sc)) for s in sc], x.tolist())\n\t            yh = histogram([(s-min(sc)) / (max(sc) - min(sc)) for s in sc], y.tolist())\n\t            print(\"xh\", xh)\n\t            print(\"yh\", yh)\n\t            tbucketed = [int(k) for k in xh[-1]]\n", "            pbucketed = [int(k) for k in yh[-1]]\n\t            return sklearn.metrics.f1_score(tbucketed, pbucketed, average=\"macro\")\n\t        return metric\n\t    else:\n\t        raise ValueError(f\"Unknown loss function {name}\")\n\tdef histogram(score_range, scores):\n\t    buckets = score_range\n\t    # create buckets\n\t    bucket_names, counts, full = [], [], []\n\t    for i, left in enumerate(buckets):\n", "        right = buckets[i + 1] if i < len(buckets) - 1 else np.infty\n\t        bucket_names += [f\"{left}-{right}\"]\n\t        counts += [0]\n\t    # fill in  scores\n\t    for s in scores:\n\t        # round to left most\n\t        if s < buckets[0]:\n\t            s = buckets[0]\n\t        for i, left in enumerate(buckets):\n\t            right = buckets[i + 1] if i < len(buckets) - 1 else np.infty\n", "            if left <= s < right:\n\t                counts[i] += 1\n\t                full += [left]\n\t    return bucket_names, counts, full\n\tdef merge_configs(config, default_config):\n\t    to_expand = [(config, default_config)]\n\t    while len(to_expand) > 0:\n\t        c, oc = to_expand.pop(0)\n\t        for k, v in oc.items():\n\t            if k not in c:\n", "                c[k] = v\n\t            elif type(c[k]) == dict:\n\t                to_expand += [(c[k], v)]\n\t            # else ignore oc config, use the conf you already have\n\tdef join_review_fields(review_report):\n\t    with_main = \"main\" in review_report and review_report[\"main\"] is not None\n\t    return \"\\n\".join(f\"{fname.title()}\\n\"+ ftext.replace('\\n', '') + f\"\\n\" for (fname, ftext) in review_report.items() if fname != \"main\") \\\n\t           + (review_report[\"main\"].replace(\"\\n\", \"\") + \"\\n\" if with_main else \"\")\n\tclass ReviewScorePredictionDataset(Dataset):\n\t    \"\"\"\n", "    Task:\n\t        Review x paper (abs) -> review score\n\t    \"\"\"\n\t    def __init__(self, dataset: ReviewPaperDataset, transform=None, target_transform=None):\n\t        self.data = dataset\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n\t    def ids(self):\n\t        return self.data.ids()\n\t    def __len__(self):\n", "        return len(self.data)\n\t    def __getitem__(self, idx):\n\t        if idx >= len(self):\n\t            raise IndexError()\n\t        paper_id, paper_meta, paper, review_num, review = self.data[idx]\n\t        extract_oscore = DATASET_REVIEW_OVERALL_SCALES[self.data.dataset_type][0]\n\t        oscore = extract_oscore(review[\"scores\"][\"overall\"])\n\t        if self.target_transform:\n\t            oscore = self.target_transform(oscore)\n\t        sample = {\n", "            \"pid\": paper_id,\n\t            \"abstract\": paper_meta[\"abstract\"],\n\t            \"paper\": paper,\n\t            \"rid\": review_num,\n\t            \"review\": join_review_fields(review[\"report\"]),\n\t        }\n\t        if self.transform:\n\t            sample = self.transform(sample)\n\t        return sample, np.float32(oscore)\n\t    def to_dict(self, idxs:list=None):\n", "        entries = list(range(len(self))) if idxs is None else idxs\n\t        sam0, _ = self[entries[0]]\n\t        fields = list(sam0.keys())\n\t        df = {f: [] for f in fields + [\"oscore\"]}\n\t        for i in entries:\n\t            sample, score = self[i]\n\t            for f in fields:\n\t                df[f] += [sample[f]]\n\t            df[\"oscore\"] += [score]\n\t        return df\n", "def abstract_with_review_only(sep_token=\"<s>\", truncate_paper=None, truncate_review=None):\n\t    assert truncate_paper is None or 0 <= truncate_paper, \"None or int >= 0 expected for paper truncation\"\n\t    assert truncate_review is None or 0 <= truncate_review, \"None or int >= 0 expected for review truncation\"\n\t    def get_abs(sample):\n\t        if truncate_paper is None:\n\t            abs = sample[\"abstract\"]\n\t        else:\n\t            abs = sample[\"abstract\"][:min(len(sample[\"abstract\"]), truncate_paper)]\n\t        if truncate_review is None:\n\t            rev = sample[\"review\"]\n", "        else:\n\t            rev = sample[\"review\"][:min(len(sample[\"review\"]), truncate_review)]\n\t        return {\"txt\": rev + sep_token + abs}\n\t    return get_abs\n\tclass PragmaticLabelingDataset(Dataset):\n\t    \"\"\"\n\t    Task:\n\t        Review sentece -> review score\n\t    \"\"\"\n\t    def __init__(self, dataset: PaperReviewDataset, transform=None, target_transform=None):\n", "        self.data = dataset\n\t        assert self.data.dataset_type in [DATASETS.ARR22, DATASETS.F1000], \\\n\t            \"Only ARR3Y and F1000 are supported for loading a pragmatic labeling dataset\"\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n\t        self._setup()\n\t    def _setup(self):\n\t        if self.data.dataset_type == DATASETS.ARR22:\n\t            def get_sentences_with_pragmatics(sample):\n\t                paper_id, paper_meta, paper, reviews = sample\n", "                out = {}\n\t                for review in reviews:\n\t                    rout = []\n\t                    for f, l in [(\"paper_summary\", \"neutral\"), (\"summary_of_strengths\", \"strength\"),\n\t                                 (\"summary_of_weaknesses\", \"weakness\"), (\"comments,_suggestions_and_typos\", \"request\")]:\n\t                        txt = review[\"report\"][f]\n\t                        if txt is None:\n\t                            continue\n\t                        sent_spans = review[\"meta\"][\"sentences\"][f]\n\t                        sentences = [txt[s[0]:s[1]].strip() for s in sent_spans]\n", "                        # discard too short sentences (likely erroneous splitting and without content)\n\t                        sentences = [s for s in sentences if len(s) > 5]\n\t                        rout += [(l, s) for s in sentences]\n\t                    out[review[\"rid\"]] = rout\n\t                return out\n\t            paper_wise_sents = {sid: get_sentences_with_pragmatics(self.data[sid]) for sid in self.data.ids()}\n\t            # discard too short sentences (bad splitting, noisy examples)\n\t            paper_wise_sents = {sid: {rid: [(i[0], i[1].strip().replace(\"\\n\", \" \")) for i in s if len(i[1]) > 6]\n\t                                      for rid, s in sents.items()}\n\t                                for sid, sents in paper_wise_sents.items()}\n", "        elif self.data.dataset_type == DATASETS.F1000:\n\t            self.annos = PaperReviewAnnotations(annotation_type=ANNOTATION_TYPES.PRAG,\n\t                                                dataset=self.data)\n\t            def f1000rd_to_labelset(lbl):\n\t                if lbl == \"Strength\":\n\t                    return \"strength\"\n\t                elif lbl == \"Weakness\":\n\t                    return \"weakness\"\n\t                elif lbl == \"Todo\":\n\t                    return \"request\"\n", "                elif lbl in [\"Structure\", \"Recap\", \"Other\"]:\n\t                    return \"neutral\"\n\t                else:\n\t                    raise ValueError(f\"Passed label {lbl} is not part of the F1000RD label set!\")\n\t            def get_sentences_with_pragmatics(sample):\n\t                anno = sample[1]\n\t                paper_id, paper_meta, paper, reviews = sample[0]\n\t                out = {}\n\t                for review in reviews:\n\t                    if review[\"rid\"] not in anno:\n", "                        continue\n\t                    txt = review[\"report\"][\"main\"]\n\t                    sent_spans = review[\"meta\"][\"sentences\"][\"main\"]\n\t                    sentences = [txt[s[0]:s[1]].strip() for s in sent_spans]\n\t                    out[review[\"rid\"]] = [(f1000rd_to_labelset(a), sentences[int(i)]) for i, a in\n\t                                          anno[review[\"rid\"]].items()]\n\t                return out\n\t            paper_wise_sents = {p[0][0]: get_sentences_with_pragmatics(p) for p in self.annos}\n\t        review_wise_sents = {f\"{sid}%{k}\": v for sid, sents in paper_wise_sents.items() for k, v in sents.items()}\n\t        covered_reviews = list(review_wise_sents.keys())\n", "        self.sentences = {f\"{rid}_{i}\": s for rid in covered_reviews for i, s in enumerate(review_wise_sents[rid])}\n\t        self.sentence_ids = list(self.sentences.keys())\n\t    def ids(self):\n\t        return self.sentence_ids\n\t    def __len__(self):\n\t        return len(self.sentence_ids)\n\t    def __getitem__(self, idx):\n\t        if idx >= len(self):\n\t            raise IndexError(f\"Index {idx} out of range for this dataset\")\n\t        sid = self.sentence_ids[idx]\n", "        label, sentence = self.sentences[sid]\n\t        if self.target_transform:\n\t            label = self.target_transform(label)\n\t        pid, ridi = tuple(sid.split(\"%\"))\n\t        rid = tuple(ridi.split(\"_\"))[0]\n\t        paper_id, paper_meta, paper, reviews = self.data[pid]\n\t        review = next(r for r in reviews if r[\"rid\"] == rid)\n\t        sample = {\n\t            \"pid\": paper_id,\n\t            \"rid\": rid,\n", "            \"review\": join_review_fields(review[\"report\"]),\n\t            \"sentence\": sentence\n\t        }\n\t        if self.transform:\n\t            sample = self.transform(sample)\n\t        return sample, label\n\t    def to_dict(self, idxs:list=None):\n\t        entries = list(range(len(self))) if idxs is None else idxs\n\t        sam0, _ = self[entries[0]]\n\t        fields = list(sam0.keys())\n", "        df = {f: [] for f in fields + [\"label\"]}\n\t        for i in entries:\n\t            sample, lbl = self[i]\n\t            for f in fields:\n\t                df[f] += [sample[f]]\n\t            df[\"label\"] += [lbl]\n\t        return df\n\tdef random_split_pragmatic_labeling_dataset(dataset: PragmaticLabelingDataset, splits: list, random_seed: int = None):\n\t    ids = dataset.ids()\n\t    shuffled_ids = sklearn.utils.shuffle(ids, random_state=random_seed)\n", "    split_idx = [list(idx) for idx in paperwise_stratified_split(shuffled_ids, splits, None, random_seed)]\n\t    split_ids = [list(np.array(ids)[idx]) for idx in split_idx]\n\t    return split_idx, split_ids\n\tdef review_sentence_no_context():\n\t    def transform(sample):\n\t        return {\"txt\": sample[\"sentence\"].strip()}\n\t    return transform\n\tdef get_class_map_pragmatics():\n\t    labels = [\"strength\", \"weakness\", \"request\", \"neutral\"]\n\t    return {\n", "        l: i for i, l in enumerate(labels)\n\t    }\n\tclass SkimmingDataset(Dataset):\n\t    \"\"\"\n\t    Task:\n\t        For each paper paragraph, determine whether it was referenced in a review\n\t    \"\"\"\n\t    def __init__(self, dataset: PaperReviewDataset,\n\t                 transform=None,\n\t                 target_transform=None,\n", "                 selected_types=None,\n\t                 sampling:str=\"random\",\n\t                 sample_size:int=5):\n\t        self.data = dataset\n\t        self.transform = transform\n\t        self.target_transform = target_transform\n\t        self.samples = None\n\t        self.sample_ids = None\n\t        self.selected_types = selected_types\n\t        self.sampling = sampling\n", "        self.sample_size = sample_size\n\t        self._setup()\n\t    @classmethod\n\t    def _get_linked_paper_nodes(cls, el_sample):\n\t        paper_data, elinks = el_sample\n\t        paper_id, paper_meta, paper, reviews, = paper_data\n\t        res = []\n\t        for rid in elinks:\n\t            for els in elinks[rid].values():\n\t                for el in els:\n", "                    anchor = el[\"paper_target\"]\n\t                    ltype = el[\"type\"]\n\t                    if anchor is not None:\n\t                        res += [(anchor, ltype)]\n\t        return res, paper\n\t    def _sample(self, positive, negative, doc) -> Tuple[List[List[Node]], List[List[Node]], List[str]]:\n\t        # we cannot deal with no negatives or positives -- we always need both in the mix\n\t        if len(positive) == 0 or len(negative) == 0:\n\t            return [], [], []\n\t        if self.sampling == \"random\":\n", "            negative = [n for n in negative if len(n.content) > 10] # quality assure paragraphs\n\t            paras = positive + negative\n\t            positive_sampled, negative_sampled, sampled_ids = [], [], []\n\t            for n in positive:\n\t                # make random choices until you include at least one negative\n\t                while True:\n\t                    random_paragraphs = random.choices([p for p in paras if p != n], k=self.sample_size-1)\n\t                    if len(set(random_paragraphs).intersection(set(negative))) > 0:\n\t                        break\n\t                positive_sampled += [[n] + [p for p in random_paragraphs if p in positive]]\n", "                negative_sampled += [[p for p in random_paragraphs if p in negative]]\n\t                sampled_ids += [n.ix]\n\t        elif self.sampling == \"close\":\n\t            paras = positive + negative\n\t            positive_sampled, negative_sampled, sampled_ids = [], [], []\n\t            for n in positive:\n\t                close_paragraphs = list(sorted(paras, key=lambda x: doc.tree_distance(n, x, Etype.NEXT)))\n\t                close_paragraphs = [p for p in close_paragraphs if p != n]\n\t                top_close = close_paragraphs[:min(len(close_paragraphs), self.sample_size-1)]\n\t                positive_sampled += [[n] + [p for p in top_close if p in positive]]\n", "                negative_sampled += [[p for p in top_close if p in negative]]\n\t                sampled_ids += [n.ix]\n\t        elif self.sampling == \"full\":\n\t            positive_sampled = [positive for p in positive]\n\t            negative_sampled = [negative for p in positive]\n\t            sampled_ids = [p.ix for p in positive]\n\t        elif self.sampling == \"none\":\n\t            positive_sampled = [positive]\n\t            negative_sampled = [negative]\n\t            sampled_ids = [\"all\"]\n", "        else:\n\t            raise NotImplementedError(f\"Given sampling strategy {self.sampling} does not exist.\")\n\t        return positive_sampled, negative_sampled, sampled_ids\n\t    def _sample_pos_neg_paragraphs(self, anchors, pdoc: IntertextDocument):\n\t        # all paragraphs\n\t        paras = [n for n in pdoc.nodes if n.ntype == NTYPE_PARAGRAPH]\n\t        # for each anchor, search for the matching paragraph node and add this to the positives\n\t        pos = []\n\t        for a, t in filter(lambda x: self.selected_types is None or x[1] in self.selected_types, anchors):\n\t            n = pdoc.get_node_by_ix(a)\n", "            if type(n) == SpanNode:\n\t                n = n.src_node\n\t            for p in pdoc.breadcrumbs(n, Etype.PARENT):\n\t                if p in paras:\n\t                    pos += [p]\n\t                    break\n\t        # for now: don't reflect frequency of references\n\t        pos = list(set(pos))\n\t        # pick only \"relevant paragraphs\" of size > 10\n\t        neg = [n for n in paras if n not in pos]\n", "        # sampling strategy\n\t        return self._sample(pos, neg, pdoc)\n\t    def _setup(self):\n\t        self.elink_annos = PaperReviewAnnotations(annotation_type=ANNOTATION_TYPES.ELINKS,\n\t                                                  dataset=self.data)\n\t        elinks_per_paper = {p[0][0]: self._get_linked_paper_nodes(p) for p in self.elink_annos}\n\t        batches = {}\n\t        for pid, sample in elinks_per_paper.items():\n\t            anchors, pdoc = sample\n\t            pos, neg, ix = self._sample_pos_neg_paragraphs(anchors, pdoc)\n", "            for i, ix in enumerate(ix):\n\t                batches[f\"{pid}%{ix}\"] = pos[i], neg[i]\n\t        self.samples = batches\n\t        self.sample_ids = list(batches.keys())\n\t    def ids(self):\n\t        return self.sample_ids\n\t    def __len__(self):\n\t        return len(self.sample_ids)\n\t    def __getitem__(self, idx):\n\t        if idx >= len(self):\n", "            raise IndexError(f\"Index {idx} out of range for this dataset\")\n\t        sid = self.sample_ids[idx]\n\t        positives, negatives = self.samples[sid]\n\t        if self.target_transform:\n\t            plabel = self.target_transform(\"positive\")\n\t            nlabel = self.target_transform(\"negative\")\n\t        else:\n\t            plabel = \"positive\"\n\t            nlabel = \"negative\"\n\t        if self.transform:\n", "            positives = [self.transform(p) for p in positives]\n\t            negatives = [self.transform(n) for n in negatives]\n\t        return positives, negatives, [plabel for p in positives], [nlabel for n in negatives]\n\t    def to_dict(self, idxs: list = None):\n\t        entries = list(range(len(self))) if idxs is None else idxs\n\t        pos0, neg0, pos0_lbls, neg0_lbls = self[entries[0]]\n\t        fields = list(pos0[0].keys())\n\t        df = {\"positives\": [],\n\t              \"negatives\": []}\n\t        for i in entries:\n", "            positives, negatives, pos_labels, neg_labels = self[i]\n\t            # create nested data frame for positives and negatives and fill as usual\n\t            pos_samples = {f: [] for f in fields + [\"label\"]}\n\t            neg_samples = {f: [] for f in fields + [\"label\"]}\n\t            for f in fields:\n\t                pos_samples[f] += [p[f] for p in positives]\n\t                neg_samples[f] += [p[f] for p in negatives]\n\t            neg_samples[\"label\"] += neg_labels\n\t            pos_samples[\"label\"] += pos_labels\n\t            # append to overall dataframe-style dict\n", "            df[\"positives\"] += [pos_samples]\n\t            df[\"negatives\"] += [neg_samples]\n\t        return df\n\tdef get_paragraph_text(with_struct=False, with_meta=False):\n\t    def get_text(paragraph):\n\t        res = [paragraph.content]\n\t        if with_struct or with_meta:\n\t            parent = None\n\t            title = None\n\t            depth = 0\n", "            edges = [e for e in paragraph.incoming_edges if e.etype == Etype.PARENT]\n\t            while len(edges) > 0:\n\t                edge = edges.pop()\n\t                depth += 1\n\t                if edge.src_node.ntype in [NTYPE_HEADING, NTYPE_ABSTRACT] and parent is None:\n\t                    parent = edge.src_node\n\t                elif edge.src_node.ntype in [NTYPE_TITLE]:\n\t                    title = edge.src_node\n\t                edges += [e for e in edge.src_node.incoming_edges if e.etype == Etype.PARENT]\n\t                if title is not None and parent is not None:\n", "                    break\n\t            if with_meta:\n\t                res = [str(depth)] + res if depth > 0 else res\n\t            res = ([(parent.meta[\"section\"] if parent.meta and \"section\" in parent.meta else \"\") + \"; \" + parent.content] + res) if parent is not None else res\n\t            res = ([title.content] + res) if title is not None else res\n\t        return {\n\t            \"txt\": \"<s>\".join(res)\n\t        }\n\t    return get_text\n\tdef get_class_map_skimming():\n", "    labels = [\"negative\", \"positive\"]\n\t    return {\n\t        l: i for i, l in enumerate(labels)\n\t    }\n"]}
{"filename": "nlpeer/tasks/review_score/train.py", "chunked_list": ["import argparse\n\timport os\n\timport time\n\timport uuid\n\tfrom random import random\n\tfrom pytorch_lightning import Trainer, seed_everything\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\tfrom pytorch_lightning.loggers import WandbLogger\n\timport torch\n\timport wandb\n", "from nlpeer.data import DATASETS\n\tfrom nlpeer.tasks import merge_configs\n\tfrom nlpeer.tasks.review_score.data import ReviewScorePredictionDataModule\n\tOUT_PATH = None\n\tBENCHMARK_PATH = None\n\tDEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n\tdef get_default_config():\n\t    global BENCHMARK_PATH, DEVICE\n\t    assert BENCHMARK_PATH is not None and os.path.exists(BENCHMARK_PATH),\\\n\t        f\"Provided benchmark path is None or does not exist: {BENCHMARK_PATH}\"\n", "    return {\n\t        \"data_loader\": {\n\t            \"num_workers\": 8,\n\t            \"shuffle\": True\n\t        },\n\t        \"dataset\": {\n\t            \"benchmark_path\": BENCHMARK_PATH\n\t        },\n\t        \"machine\": {\n\t            \"device\": DEVICE\n", "        }\n\t    }\n\tdef setup(config):\n\t    # get module and transforms\n\t    if config[\"model\"][\"type\"] in [\"roberta\", \"biobert\", \"scibert\"]:\n\t        from nlpeer.tasks.review_score.models.TransformerBased import from_config\n\t        module = from_config(config)\n\t    elif config[\"model\"][\"type\"].startswith(\"baseline\"):\n\t        from nlpeer.tasks.review_score.models.baseline import from_config\n\t        module = from_config(config)\n", "    else:\n\t        raise ValueError(f\"The provided model type {config['model']['type']} is not supported\")\n\t    input_transform, target_transform = module.get_prepare_input(DATASETS[config[\"dataset\"][\"type\"]])\n\t    tokenizer = module.get_tokenizer()\n\t    # load data module\n\t    data_module = ReviewScorePredictionDataModule(benchmark_path=config[\"dataset\"][\"benchmark_path\"],\n\t                                                  dataset_type=DATASETS[config[\"dataset\"][\"type\"]],\n\t                                                  in_transform=input_transform,\n\t                                                  target_transform=target_transform,\n\t                                                  tokenizer=tokenizer,\n", "                                                  filter=config[\"dataset\"][\"filter\"] if \"filter\" in config[\"dataset\"] else None,\n\t                                                  data_loader_config=config[\"data_loader\"],\n\t                                                  paper_version=config[\"dataset\"][\"paper_version\"])\n\t    return module, data_module\n\tdef train(model, data_module, params, logger=None, debug=False):\n\t    global OUT_PATH\n\t    print(f\"RUN = {wandb.run.name}\")\n\t    wandb.log({\"config\": params})\n\t    chkp_dir = os.path.join(OUT_PATH, f\"checkpoints/{params['dataset']['type']}/{params['model']['type']}\")\n\t    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n", "                                          mode=\"min\",\n\t                                          dirpath=chkp_dir,\n\t                                          filename=\"{epoch}-{val_loss}-\" + str(uuid.uuid4()),\n\t                                          save_top_k=1,\n\t                                          #every_n_train_steps=10\n\t                                          )\n\t    early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n\t                                        mode=\"min\",\n\t                                        patience=8,\n\t                                        min_delta=0.001)\n", "    trainer = Trainer(logger=logger,\n\t                      log_every_n_steps=1,\n\t                      limit_train_batches=0.1 if debug else 1.0,\n\t                      #devices=1,\n\t                      max_epochs=params[\"train\"][\"epochs\"],\n\t                      accelerator=params[\"machine\"][\"device\"],\n\t                      callbacks=[checkpoint_callback, early_stop_callback])\n\t    # fit the model\n\t    trainer.fit(model, data_module)\n\t    # run final validation step\n", "    res = trainer.validate(model.load_from_checkpoint(checkpoint_callback.best_model_path), data_module)\n\t    print(res)\n\t    # output best model path\n\t    wandb.log({\"best_model\": checkpoint_callback.best_model_path})\n\t    print(f\"best_model = {checkpoint_callback.best_model_path}\")\n\tdef run(config, debug=False, project=None):\n\t    global OUT_PATH\n\t    dconfig = get_default_config()\n\t    merge_configs(config, dconfig)\n\t    # set seed and log\n", "    seed = int(time.time()) % 100000\n\t    config[\"random_seed\"] = seed\n\t    seed_everything(seed)\n\t    # actual training\n\t    model, data = setup(config)\n\t    train(model, data, config, WandbLogger(dir=os.path.join(OUT_PATH, \"logs\"), project=project), debug)\n\tdef main(args):\n\t    global BENCHMARK_PATH, OUT_PATH\n\t    BENCHMARK_PATH = args.benchmark_path\n\t    OUT_PATH = args.store_results\n", "    assert os.path.exists(BENCHMARK_PATH) and os.path.exists(OUT_PATH),\\\n\t        f\"Benchmark or out path do not exist. Check {BENCHMARK_PATH} and {OUT_PATH} again.\"\n\t    # use default config (basically for debugging)\n\t    dconf = {\n\t        \"dataset\": {\n\t            \"type\": args.dataset,\n\t            \"paper_version\": 1\n\t        },\n\t        \"model\": {\n\t            \"type\": args.model\n", "        },\n\t        \"train\": {\n\t            \"epochs\": 20,\n\t            \"train_loss\": \"mse\",\n\t            \"dev_loss\": \"mse\",\n\t            \"optimizer\": \"adam\",\n\t            \"learning_rate\": 2e-5 if args.lr is None else args.lr,\n\t            \"epsilon\": 1e-8\n\t        },\n\t        \"data_loader\": {\n", "            \"batch_size\": 16 if args.batch_size is None else args.batch_size\n\t        }\n\t    }\n\t    if args.downsample:\n\t        def drop_random(pid):\n\t            dec = random()\n\t            return dec <= args.downsample\n\t        dconf[\"dataset\"][\"filter\"] = drop_random\n\t    print(\"Using default config...\" + str(dconf))\n\t    runs = args.repeat if args.repeat else 1\n", "    for i in range(runs):\n\t        run(dconf, debug= args.debug, project=args.project if args.project is not None else \"RSP_train\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_path\", required=True, type=str, help=\"Path to the benchmark dir\")\n\t    parser.add_argument(\"--store_results\", required=True, type=str, help=\"Path for logs + results\")\n\t    parser.add_argument(\"--dataset\", required=False, choices=[d.name for d in DATASETS], help=\"Dataset name if no sweep provided\")\n\t    parser.add_argument(\"--model\", required=False, type=str, help=\"model name if no sweep provided\")\n\t    parser.add_argument(\"--lr\", required=False, type=float, help=\"learning rate (opt)\")\n\t    parser.add_argument(\"--downsample\", required=False, type=float, help=\"downsample ratio, if present\")\n", "    parser.add_argument(\"--batch_size\", required=False, type=int, help=\"batch size (opt)\")\n\t    parser.add_argument(\"--repeat\", required=False, type=int, default=1, help=\"Number of repetitions\")\n\t    parser.add_argument(\"--debug\", required=False, type=bool, default=False, help=\"Number of repetitions\")\n\t    parser.add_argument(\"--project\", required=False, type=str, help=\"Project name in WANDB\")\n\t    args = parser.parse_args()\n\t    assert args.project is not None and args.repeat is not None, \"Project name required for running a wandb sweep\"\n\t    assert args.dataset is not None and args.model is not None,\\\n\t        \"Dataset type required if not loading from sweep config\"\n\t    main(args)\n"]}
{"filename": "nlpeer/tasks/review_score/evaluate.py", "chunked_list": ["import argparse\n\timport simplejson\n\timport json\n\timport os\n\tfrom copy import copy\n\tfrom typing import Callable\n\timport plotly.graph_objects as go\n\tfrom os.path import join as pjoin\n\timport numpy as np\n\timport scipy.stats\n", "from scipy.stats import entropy\n\timport sklearn\n\timport torch\n\timport wandb\n\tfrom pytorch_lightning import Trainer\n\tfrom pytorch_lightning.loggers import WandbLogger\n\timport nlpeer.tasks\n\tfrom nlpeer import DATASETS, PAPERFORMATS, DATASET_REVIEW_OVERALL_SCALES, ReviewPaperDataset\n\tfrom nlpeer.data.utils import list_files\n\tfrom nlpeer.tasks import histogram\n", "from nlpeer.tasks.review_score import train\n\tfrom nlpeer.tasks.review_score.data import ReviewScorePredictionDataModule\n\tOUT_PATH = None\n\tBENCHMARK_PATH = None\n\tDEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n\tdef get_default_config():\n\t    global BENCHMARK_PATH, DEVICE\n\t    assert BENCHMARK_PATH is not None and os.path.exists(BENCHMARK_PATH), \\\n\t        f\"Provided benchmark path is None or does not exist: {BENCHMARK_PATH}\"\n\t    return {\n", "        \"data_loader\": {\n\t            \"num_workers\": 0,\n\t            \"shuffle\": False\n\t        },\n\t        \"dataset\": {\n\t            \"benchmark_path\": BENCHMARK_PATH,\n\t            \"splits_from_file\": True\n\t        },\n\t        \"machine\": {\n\t            \"device\": DEVICE\n", "        }\n\t    }\n\t# 1. Diversity\n\tdef relative_diversity(score_range, true_scores, predicted_scores, log=False):\n\t    true_histogram = histogram(score_range, true_scores)\n\t    pred_histogram = histogram(score_range, predicted_scores)\n\t    kldiv = entropy(pred_histogram[1], qk=true_histogram[1], base=2)\n\t    if log:\n\t        t1 = wandb.Table(data=[[bucket, cnt] for bucket, cnt in zip(true_histogram[0], true_histogram[1])],\n\t                         columns=[\"score\", \"cnt\"])\n", "        t2 = wandb.Table(data=[[bucket, cnt] for bucket, cnt in zip(pred_histogram[0], pred_histogram[1])],\n\t                         columns=[\"score\", \"cnt\"])\n\t        fig = go.Figure()\n\t        fig.add_trace(\n\t            go.Bar(x=list(true_histogram[0]), y=true_histogram[1], name=\"True Scores\"))\n\t        fig.add_trace(\n\t            go.Bar(x=list(pred_histogram[0]), y=pred_histogram[1], name=\"Pred Scores\"))\n\t        wandb.log({f\"diversity_hist\": fig})\n\t        t3 = wandb.Table(data=[[p] for p in predicted_scores],\n\t                         columns=[\"scores\"])\n", "        wandb.log({f\"diversity_fine_hist\": wandb.plot.histogram(t3, \"scores\", title=\"Predicted score dist\")})\n\t        wandb.log({\"diversity_true\": t1})\n\t        wandb.log({\"diversity_model\": t2})\n\t        wandb.log({\"kl_divergence\": kldiv})\n\t    return {\n\t        \"kldiv\": float(kldiv),\n\t        \"true_histogram\": true_histogram,\n\t        \"pred_histogram\": pred_histogram,\n\t        \"true_mean\": float(np.mean(true_histogram[2])),\n\t        \"true_std\": float(np.std(true_histogram[2])),\n", "        \"pred_mean_rounded\": float(np.mean(pred_histogram[2])),\n\t        \"pred_std_rounded\": float(np.std(pred_histogram[2])),\n\t        \"pred_mean\": float(np.mean(predicted_scores)),\n\t        \"pred_std\": float(np.std(predicted_scores)),\n\t    }\n\t# 2. MRSE plus rounded classification loss\n\tdef error(score_range, true_scores, predicted_scores, log=False):\n\t    # mrse\n\t    mrse = sklearn.metrics.mean_squared_error(true_scores, predicted_scores)\n\t    # r2\n", "    r2 = sklearn.metrics.r2_score(true_scores, predicted_scores)\n\t    # rounded classification loss\n\t    tbucketed = [int(x) for x in histogram(score_range, true_scores)[-1]]\n\t    pbucketed = [int(x) for x in histogram(score_range, predicted_scores)[-1]]\n\t    print(\"true\", tbucketed)\n\t    print(\"predicted\", pbucketed)\n\t    f1_micro = sklearn.metrics.f1_score(tbucketed, pbucketed, average=\"micro\")\n\t    f1_macro = sklearn.metrics.f1_score(tbucketed, pbucketed, average=\"macro\")\n\t    # cll = train.get_loss_function(\"CLL\")(torch.Tensor(pbucketed).reshape(), torch.Tensor(tbucketed).flatten())\n\t    if log:\n", "        wandb.log({\"mrse\": mrse})\n\t        wandb.log({\"r^2\": r2})\n\t        wandb.log({\"f1_micro_rounded\": f1_micro})\n\t        wandb.log({\"f1_macro_rounded\": f1_macro})\n\t        # wandb.log({\"cumulative_link_loss\": cll})\n\t    return {\n\t        \"mrse\": float(mrse),\n\t        \"r^2\": float(r2),\n\t        \"f1_micro\": float(f1_micro),\n\t        \"f1_macro\": float(f1_macro)\n", "    }\n\t# 3. Fairness by paper-type (long vs. short; dataset vs. method; F1000 categories)\n\tdef relative_fairness(true_scores, predicted_scores, original_dataset, index_map, paper_criterion: Callable,\n\t                      criterion_name: str, log=False):\n\t    crit_per_sample = np.array([paper_criterion(original_dataset[index_map[i]]) for i in range(len(true_scores))])\n\t    crits = set(crit_per_sample)\n\t    samples_per_crit = {c: crit_per_sample == c for c in crits}\n\t    true_scores_per_crit = {c: np.array(true_scores)[samples_per_crit[c]] for c in crits}\n\t    predicted_scores_per_crit = {c: np.array(predicted_scores)[samples_per_crit[c]] for c in crits}\n\t    if len(crits) <= 1:\n", "        print(\"Fairness not applicable -- only one class present!\")\n\t        wandb.log({f\"fairness_{criterion_name}\": \"not applicable, only one class present\"})\n\t        return {}, {}\n\t    # \"human fairness\"\n\t    h, m, = {}, {}\n\t    # h[\"pearson\"] = scipy.stats.pearsonr(true_scores, crit_per_sample)con\n\t    # h[\"spearman\"] = scipy.stats.spearmanr(true_scores, crit_per_sample)\n\t    # h[\"kendalltau\"] = scipy.stats.kendalltau(true_scores, crit_per_sample)\n\t    # h[\"point_biserial\"] = scipy.stats.pointbiserialr(np.array([predicted_scores_per_crit[s] for s in crit_per_sample]), true_scores)\n\t    annova = scipy.stats.f_oneway(*[true_scores_per_crit[c] for c in crits])\n", "    h[\"human_annova\"] = {\"stat\": float(annova.statistic), \"pvalue\": float(annova.pvalue)}\n\t    # \"model fairness\"\n\t    # m[\"pearson\"] = scipy.stats.pearsonr(predicted_scores, crit_per_sample)\n\t    # m[\"spearman\"] = scipy.stats.spearmanr(predicted_scores, crit_per_sample)\n\t    # m[\"kendalltau\"] = scipy.stats.kendalltau(predicted_scores, crit_per_sample)\n\t    # m[\"point_biserial\"] = scipy.stats.pointbiserialr(predicted_scores, true_scores)\n\t    annova = scipy.stats.f_oneway(*[predicted_scores_per_crit[c] for c in crits])\n\t    m[\"model_annova\"] = {\"stat\": float(annova.statistic), \"pvalue\": float(annova.pvalue)}\n\t    if log:\n\t        scores_per_crit = [(\"true\", true_scores_per_crit), (\"pred\", predicted_scores_per_crit)]\n", "        means = {}\n\t        stds = {}\n\t        for t, spc in scores_per_crit:\n\t            means[t] = []\n\t            stds[t] = []\n\t            for crit in crits:\n\t                means[t] += [np.mean(spc[crit])]\n\t                stds[t] += [np.std(spc[crit])]\n\t        fig = go.Figure()\n\t        fig.add_trace(\n", "            go.Bar(x=list(crits), y=means[\"true\"], error_y=dict(type=\"data\", array=stds[\"true\"]), name=\"True Scores\"))\n\t        fig.add_trace(\n\t            go.Bar(x=list(crits), y=means[\"pred\"], error_y=dict(type=\"data\", array=stds[\"pred\"]), name=\"Pred Scores\"))\n\t        wandb.log({f\"fairness_{criterion_name}\": fig})\n\t        wandb.log({\"human_fairness\": h, \"model_fairness\": m})\n\t    return h, m\n\tdef get_criterion(criterion_name, dataset_type):\n\t    if criterion_name == \"paper_type\":\n\t        if dataset_type == DATASETS.F1000:\n\t            def f1000_paper_type(p):\n", "                return p[1][\"atype\"]\n\t            return f1000_paper_type\n\t        elif dataset_type in [DATASETS.ARR22, DATASETS.ACL17, DATASETS.CONLL16]:\n\t            def staracl_paper_type(p):\n\t                if \"bib_page_index\" not in p[1]:\n\t                    return \"short\"\n\t                bindex = p[1][\"bib_page_index\"]\n\t                if bindex is None or bindex < 8:\n\t                    return \"short\"\n\t                else:\n", "                    return \"long\"\n\t            return staracl_paper_type\n\t        elif dataset_type == DATASETS.COLING20:\n\t            def coling_paper_type(p):\n\t                t = p[1][\"type\"]\n\t                return \"long\" if t.lower() == \"long paper\" else \"short\"\n\t            return coling_paper_type\n\t    else:\n\t        raise ValueError(f\"No such criterion exists: {criterion_name}\")\n\tdef setup(config):\n", "    assert \"load_path\" in config[\"model\"], \"Provide a 'load_path' attribute in the config.model to load a checkpoint\"\n\t    if config[\"model\"][\"type\"] in [\"roberta\", \"biobert\", \"scibert\"]:\n\t        from nlpeer.tasks.review_score.models.TransformerBased import from_config\n\t        module = from_config(config)\n\t    elif config[\"model\"][\"type\"].startswith(\"baseline\"):\n\t        from nlpeer.tasks.review_score.models.baseline import from_config\n\t        module = from_config(config)\n\t    else:\n\t        raise ValueError(\"The provided model type is not supported\")\n\t    input_transform, target_transform = module.get_prepare_input(DATASETS[config[\"dataset\"][\"type\"]])\n", "    tokenizer = module.get_tokenizer()\n\t    # prepare data (loading splits from disk)\n\t    data_module = ReviewScorePredictionDataModule(benchmark_path=config[\"dataset\"][\"benchmark_path\"],\n\t                                                  dataset_type=DATASETS[config[\"dataset\"][\"type\"]],\n\t                                                  in_transform=input_transform,\n\t                                                  target_transform=target_transform,\n\t                                                  tokenizer=tokenizer,\n\t                                                  data_loader_config=config[\"data_loader\"],\n\t                                                  paper_version=config[\"dataset\"][\"paper_version\"])\n\t    original_data = ReviewPaperDataset(config[\"dataset\"][\"benchmark_path\"],\n", "                                       DATASETS[config[\"dataset\"][\"type\"]],\n\t                                       config[\"dataset\"][\"paper_version\"],\n\t                                       PAPERFORMATS.ITG)\n\t    # mapping between split indexes and original dataset (trivial in this case)\n\t    imap = {i: i for i in range(len(original_data))}\n\t    return module, data_module, original_data, imap\n\tdef get_predictions(model, data_module, params, logger, debug=False):\n\t    trainer = Trainer(logger=logger,\n\t                      log_every_n_steps=1,\n\t                      limit_predict_batches=0.1 if debug else 1.0,\n", "                      accelerator=params[\"machine\"][\"device\"])\n\t    return trainer.predict(model, data_module)\n\tdef run(config, debug=False):\n\t    global OUT_PATH\n\t    cname = f\"{config['dataset']['type']}_{config['model']['type']}\"\n\t    with wandb.init(dir=os.path.join(OUT_PATH, \"logs\"), config=config, project=config[\"project\"], name=cname):\n\t        dconfig = get_default_config()\n\t        tasks.merge_configs(config, dconfig)\n\t        wandb_logger = WandbLogger(dir=os.path.join(OUT_PATH, \"logs\"), config=config)\n\t        print(f\"Starting evaluation with config {config}\")\n", "        # get model checkpoints and config paths\n\t        model_load_path = pjoin(config[\"model\"][\"load_path\"], config[\"model\"][\"type\"])\n\t        print(model_load_path)\n\t        mconf_path = pjoin(model_load_path, \"config.json\") if os.path.exists(\n\t            pjoin(model_load_path, \"config.json\")) else None\n\t        checkpoints = [c for c in list_files(model_load_path) if os.path.basename(c) != \"config.json\"]\n\t        assert len(checkpoints) > 0, f\"Model load path {model_load_path} directory lacks checkpoints: {checkpoints}\"\n\t        # load the model config\n\t        mconf = copy(config)\n\t        accu_res = evaluation(checkpoints, config, mconf, wandb_logger, debug)\n", "        mean_res = compute_mean_stats(accu_res)\n\t        wandb.log({\"mean_res\": mean_res})\n\t        mean_res[\"accu\"] = accu_res\n\t        def np_encoder(object):\n\t            if isinstance(object, np.generic):\n\t                return object.item()\n\t            else:\n\t                return None\n\t        with open(pjoin(OUT_PATH, f\"rsp_eval_{config['dataset']['type']}_{config['model']['type']}.json\"), \"w+\") as f:\n\t            simplejson.dump(mean_res, f, indent=4, ignore_nan=True, default=np_encoder)\n", "def compute_mean_stats(accu_res):\n\t    mean_res = {}\n\t    for s in accu_res:\n\t        for t in s:\n\t            if type(s[t]) != dict:\n\t                continue\n\t            if t not in mean_res:\n\t                mean_res[t] = {}\n\t            for k, v in s[t].items():\n\t                try:\n", "                    vf = float(v)\n\t                except (ValueError, TypeError):\n\t                    continue\n\t                if k not in mean_res[t]:\n\t                    mean_res[t][k] = []\n\t                mean_res[t][k] += [vf]\n\t    for t in mean_res:\n\t        for k, v in mean_res[t].items():\n\t            mean_res[t][k] = {\"mean\": float(np.mean(v)), \"median\": float(np.median(v)), \"std\": float(np.std(v))}\n\t    return mean_res\n", "def evaluation(checkpoints, config, model_config, wandb_logger, debug=False):\n\t    accu_res = []\n\t    for cpath in checkpoints:\n\t        # define setup config\n\t        model_config[\"model\"][\"load_path\"] = cpath\n\t        model_config[\"dataset\"] = config[\"dataset\"]\n\t        model_config[\"data_loader\"] = config[\"data_loader\"]\n\t        model_config[\"machine\"] = config[\"machine\"]\n\t        # prep evaluation\n\t        model, data_module, original_dataset, imap = setup(model_config)\n", "        score_range = DATASET_REVIEW_OVERALL_SCALES[original_dataset.dataset_type][1]\n\t        predictions = get_predictions(model, data_module, config, wandb_logger, debug)\n\t        labels = torch.cat([x[\"labels\"] for x in predictions]).detach().cpu().tolist()\n\t        predictions = torch.cat([x[\"predictions\"] for x in predictions]).detach().cpu().tolist()\n\t        # assuming normalization of outputs\n\t        predictions = [(i * (np.max(score_range) - np.min(score_range))) + np.min(score_range) for i in\n\t                       predictions]\n\t        labels = [(i * (np.max(score_range) - np.min(score_range))) + np.min(score_range) for i in\n\t                       labels]\n\t        # evaluate\n", "        eval_conf = config[\"evaluation\"]\n\t        res = {}\n\t        if \"diversity\" in eval_conf and eval_conf[\"diversity\"]:\n\t            res[\"diversity\"] = relative_diversity(score_range, labels, predictions, True)\n\t        if \"error\" in eval_conf and eval_conf[\"error\"]:\n\t            res[\"error\"] = error(score_range, labels, predictions, True)\n\t        if \"fairness\" in eval_conf and eval_conf[\"fairness\"] is not None:\n\t            criterion_name = eval_conf[\"fairness\"][\"criterion\"]\n\t            paper_criterion = get_criterion(criterion_name, original_dataset.dataset_type)\n\t            res[\"fairness\"] = relative_fairness(labels,\n", "                                                predictions,\n\t                                                original_dataset,\n\t                                                imap,\n\t                                                paper_criterion,\n\t                                                criterion_name,\n\t                                                True)\n\t        res[\"predictions\"] = predictions\n\t        accu_res += [res]\n\t    return accu_res\n\tdef main(args):\n", "    global BENCHMARK_PATH, OUT_PATH\n\t    BENCHMARK_PATH = args.benchmark_dir\n\t    OUT_PATH = args.store_results\n\t    assert os.path.exists(BENCHMARK_PATH) and os.path.exists(OUT_PATH), \\\n\t        f\"Benchmark or out path do not exist. Check {BENCHMARK_PATH} and {OUT_PATH} again.\"\n\t    config = {\n\t        \"name\": f\"Evaluation\",\n\t        \"project\": args.project,\n\t        \"random_seed\": {\n\t            \"value\": 29491\n", "        },\n\t        \"model\": {\n\t            \"type\": args.model_type,\n\t            \"load_path\": pjoin(args.chkp_dir, f\"{DATASETS[args.dataset].name}\"),\n\t            \"normalized_output\": False   #not pretty\n\t        },\n\t        \"data_loader\": {\n\t            \"batch_size\": 8\n\t        },\n\t        \"dataset\": {\n", "            \"type\": DATASETS[args.dataset].name,\n\t            \"paper_version\": args.paper_version\n\t        },\n\t        \"evaluation\": {\n\t            \"diversity\": True,\n\t            \"error\": True,\n\t            \"fairness\": {\n\t                \"criterion\": \"paper_type\"\n\t            }\n\t        }\n", "    }\n\t    run(config, debug= args.debug)\n\t    # wandb.agent(args.sweep_id.strip(), function=run, project=args.project, count=10)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_dir\", required=True, type=str, help=\"Path to the benchmark dir\")\n\t    parser.add_argument(\"--project\", required=True, type=str, help=\"Project name in WANDB\")\n\t    parser.add_argument(\"--store_results\", required=True, type=str, help=\"Path for logs + results\")\n\t    parser.add_argument(\"--chkp_dir\", required=True, type=str, help=\"Path to load the checkpoints from\")\n\t    parser.add_argument(\"--dataset\", required=True, choices=[d.name for d in DATASETS], help=\"Name of the dataset\")\n", "    parser.add_argument(\"--model_type\", required=True, type=str, help=\"Model type e.g. biobert\")\n\t    parser.add_argument(\"--paper_version\", required=False, default=1, type=int, help=\"Version of the paper\")\n\t    parser.add_argument(\"--debug\", required=False, default=False, type=bool, help=\"Turn on debugging\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "nlpeer/tasks/review_score/__init__.py", "chunked_list": []}
{"filename": "nlpeer/tasks/review_score/data.py", "chunked_list": ["import argparse\n\timport os\n\tfrom copy import copy\n\tfrom os.path import join as pjoin\n\timport sklearn\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader\n\tfrom datasets import Dataset, DatasetDict\n\tfrom nlpeer.data import filter_gte_x_reviews, load_splits_from_file, \\\n\t    store_splits_to_file, paperwise_random_split\n", "from nlpeer import DATASETS, PAPERFORMATS, ReviewPaperDataset\n\tfrom nlpeer.tasks import ReviewScorePredictionDataset\n\tclass ReviewScorePredictionDataModule(LightningDataModule):\n\t    def __init__(self, benchmark_path: str,\n\t                 dataset_type: DATASETS,\n\t                 in_transform,\n\t                 target_transform,\n\t                 tokenizer,\n\t                 data_loader_config,\n\t                 paper_version=1,\n", "                 filter=None,\n\t                 paper_format=PAPERFORMATS.ITG):\n\t        super().__init__()\n\t        self.base_path = benchmark_path\n\t        self.dataset_type = dataset_type\n\t        self.paper_version = paper_version\n\t        self.paper_format = paper_format\n\t        self.in_transform = in_transform\n\t        self.target_transform = target_transform\n\t        self.tokenizer = tokenizer\n", "        self.splits = self._load_splits_idx()\n\t        self.dataset = None\n\t        self.filter = filter\n\t        self.data_loader_config = data_loader_config\n\t    def _load_splits_idx(self):\n\t        fp = pjoin(self.base_path, self.dataset_type.value, \"splits\", \"rsp_split.json\")\n\t        assert os.path.exists(fp) and os.path.isfile(fp), \\\n\t            f\"Cannot setup ReviewScorePrediction splits, as {fp} does not exist.\"\n\t        return load_splits_from_file(fp, self.dataset_type)\n\t    def setup(self, stage: str | None) -> None:\n", "        '''called one each GPU separately - stage defines if we are at fit or test step'''\n\t        # load all data\n\t        full_dataset = ReviewPaperDataset(self.base_path, self.dataset_type, self.paper_version, self.paper_format)\n\t        rsp_data = ReviewScorePredictionDataset(full_dataset,\n\t                                                transform=self.in_transform,\n\t                                                target_transform=self.target_transform)\n\t        # assign each review a split by its rid loaded from disk\n\t        split_ixs = []\n\t        for s in self.splits:\n\t            split_ixs += [[rsp_data.ids().index(rid) for i, rid in s]]\n", "        # discard train samples\n\t        if self.filter:\n\t            split_ixs[0] = [s for s in split_ixs[0] if self.filter(s)]\n\t        print(f\"{len(split_ixs[0])} Training samples loaded\")\n\t        self.dataset = DatasetDict({\n\t            \"train\": Dataset.from_dict(rsp_data.to_dict(split_ixs[0])),\n\t            \"dev\": Dataset.from_dict(rsp_data.to_dict(split_ixs[1])),\n\t            \"test\": Dataset.from_dict(rsp_data.to_dict(split_ixs[2])),\n\t        })\n\t        for split in self.dataset.keys():\n", "            self.dataset[split] = self.dataset[split].map(\n\t                self.convert_to_features,\n\t                batched=True,\n\t                remove_columns=[\"txt\", \"oscore\"]\n\t            )\n\t            self.dataset[split].set_format(type=\"torch\")\n\t    def train_dataloader(self):\n\t        \"\"\"returns training dataloader\"\"\"\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = True\n", "        return DataLoader(self.dataset[\"train\"], **dl_config)\n\t    def val_dataloader(self):\n\t        \"\"\"returns validation dataloader\"\"\"\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"dev\"], **dl_config)\n\t    def test_dataloader(self):\n\t        \"\"\"returns test dataloader\"\"\"\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = False\n", "        return DataLoader(self.dataset[\"test\"], **dl_config)\n\t    def predict_dataloader(self):\n\t        \"\"\"returns test dataloader\"\"\"\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"test\"], **dl_config)\n\t    def convert_to_features(self, sample_batch, indices=None):\n\t        labels = sample_batch[\"oscore\"]\n\t        input_raw = sample_batch[\"txt\"]\n\t        features = self.tokenizer(input_raw)\n", "        features[\"labels\"] = labels\n\t        return features\n\tdef create_and_store_splits(full_data: ReviewPaperDataset,\n\t                            out_dir,\n\t                            splits: list[float],\n\t                            random_gen: int = None):\n\t    splits = paperwise_random_split(full_data, splits, random_gen)\n\t    out_path = os.path.join(out_dir, \"rsp_split.json\")\n\t    store_splits_to_file(full_data, splits, out_path, random_gen)\n\t    return out_path\n", "def prepare_dataset_splits(benchmark_path, paper_version, splits, random_gen, datasets=None):\n\t    if datasets is None:\n\t        datasets = [d for d in DATASETS]\n\t    out_files = []\n\t    for d in datasets:\n\t        full_dataset = ReviewPaperDataset(benchmark_path,\n\t                                          d,\n\t                                          paper_version,\n\t                                          PAPERFORMATS.ITG)\n\t        # filter by >= 1 review per paper\n", "        filter_gte_x_reviews(full_dataset, 1)\n\t        out_path = os.path.join(benchmark_path, d.value, \"splits\")\n\t        if not os.path.exists(out_path):\n\t            os.mkdir(out_path)\n\t        out_files += [create_and_store_splits(full_dataset, out_path, splits, random_gen)]\n\t    return out_files\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_dir\", required=True, help=\"Path to the benchmark directory\")\n\t    parser.add_argument(\"--paper_version\", required=True, help=\"Which paper version\", type=int)\n", "    parser.add_argument(\"--random_seed\", required=True, help=\"Random seed to generate random splits\", type=int)\n\t    parser.add_argument(\"--datasets\", nargs=\"*\", required=False, help=\"list of datasets, if applicable\", type=str)\n\t    args = parser.parse_args()\n\t    prepare_dataset_splits(args.benchmark_dir,\n\t                           args.paper_version,\n\t                           [0.7, 0.1, 0.2],\n\t                           args.random_seed,\n\t                           [DATASETS[d] for d in args.datasets])"]}
{"filename": "nlpeer/tasks/review_score/models/baseline.py", "chunked_list": ["import argparse\n\timport os\n\tfrom collections import Counter\n\tfrom typing import Any\n\timport numpy as np\n\timport torch\n\timport wandb\n\timport pytorch_lightning as pl\n\tfrom nlpeer import DATASETS, DATASET_REVIEW_OVERALL_SCALES\n\tfrom nlpeer.tasks.review_score.data import load_dataset_splits\n", "class LitBaselineModule(pl.LightningModule):\n\t    def __init__(self, score):\n\t        super().__init__()\n\t        self.default_score = score\n\t    def forward(self, inputs) -> Any:\n\t        return {\"predictions\": torch.tensor([self.default_score for i in range(inputs[\"labels\"].shape[0])]),\n\t                \"labels\": inputs[\"labels\"]}\n\t    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = None):\n\t        return self(batch)\n\t    def get_tokenizer(self):\n", "        return lambda lot: {\"nocontent\": [[l] for l in lot]}\n\t    def get_prepare_input(self, dataset_type: DATASETS):\n\t        input_transform = lambda x: {\"txt\": \"\"}\n\t        target_transform = None\n\t        return input_transform, target_transform\n\tdef from_config(config):\n\t    assert config[\"model\"][\"type\"].startswith(\"baseline\"),\\\n\t        \"Loading config for a baseline model, but passed a non-baseline config!\"\n\t    # module\n\t    model_params = config[\"model\"]\n", "    if \"load_path\" in model_params:\n\t        with open(model_params[\"load_path\"], \"r\") as file:\n\t            score = float(file.read().strip())\n\t    else:\n\t        score = model_params[\"score\"]\n\t    module = LitBaselineModule(score)\n\t    # use tokenizer and transforms that do nothing, but bring the data into the right format\n\t    tokenizer = lambda lot: {\"nocontent\": [[l] for l in lot]}\n\t    input_transform = lambda x: [1]\n\t    target_transform = None\n", "    return module\n\tdef compute_baselines(config):\n\t    train, dev, _ = load_dataset_splits(config[\"dataset\"][\"benchmark_path\"],\n\t                                               config[\"dataset\"][\"paper_version\"],\n\t                                               config[\"dataset\"][\"type\"],\n\t                                               None,\n\t                                               None,\n\t                                               None)\n\t    rev_scores = [train[i][1] for i in range(len(train))]\n\t    rev_scores += [dev[i][1] for i in range(len(dev))]\n", "    # avg. review score\n\t    avg_score = np.mean(rev_scores)\n\t    # avg. rounded review score\n\t    scale = DATASET_REVIEW_OVERALL_SCALES[config[\"dataset\"][\"type\"]][1]\n\t    avg_rounded_score = scale[np.argmin(np.abs(scale - avg_score))]\n\t    # majority\n\t    maj_score = Counter(rev_scores).most_common(1)[0][0]\n\t    return {\n\t        \"average_score\": avg_score,\n\t        \"average_rounded_score\": avg_rounded_score,\n", "        \"majority_score\": maj_score\n\t    }\n\tdef main(args):\n\t    if args.dataset == \"ALL\":\n\t        datasets = list(DATASETS)\n\t    else:\n\t        datasets = [DATASETS[args.dataset]]\n\t    with wandb.init(project=args.project):\n\t        for d in datasets:\n\t            config = {\n", "                \"dataset\":{\n\t                    \"benchmark_path\": args.benchmark_dir,\n\t                    \"paper_version\": args.paper_version,\n\t                    \"type\": d\n\t                }\n\t            }\n\t            baselines = compute_baselines(config)\n\t            tbl = wandb.Table(data=[[k, v] for k,v in baselines.items()], columns=[\"type\", \"value\"])\n\t            wandb.log({f\"baseline_{d.name}\": tbl})\n\t            for k, v in baselines.items():\n", "                if not os.path.exists(os.path.join(args.out_dir, d.name)):\n\t                    os.mkdir(os.path.join(args.out_dir, d.name))\n\t                if not os.path.exists(os.path.join(args.out_dir, d.name, f\"baseline_{k}\")):\n\t                    os.mkdir(os.path.join(args.out_dir, d.name, f\"baseline_{k}\"))\n\t                with open(os.path.join(args.out_dir, d.name, f\"baseline_{k}\", \"score.txt\"), \"w+\") as f:\n\t                    f.write(str(v))\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_dir\", required=True, type=str, help=\"Path to the benchmark dir\")\n\t    parser.add_argument(\"--out_dir\", required=True, type=str, help=\"Path to store output\")\n", "    parser.add_argument(\"--project\", required=True, type=str, help=\"Project associated with the sweep\")\n\t    parser.add_argument(\"--dataset\", required=True, choices=[d.name for d in DATASETS] + [\"ALL\"], help=\"Name of the dataset\")\n\t    parser.add_argument(\"--paper_version\", required=False, default=1, type=int, help=\"Version of the paper\")\n\t    args = parser.parse_args()\n\t    main(args)"]}
{"filename": "nlpeer/tasks/review_score/models/__init__.py", "chunked_list": []}
{"filename": "nlpeer/tasks/review_score/models/TransformerBased.py", "chunked_list": ["from typing import Callable, Optional\n\timport numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom pytorch_lightning.utilities.types import STEP_OUTPUT\n\tfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup, \\\n\t    AutoConfig\n\t# model\n\tfrom nlpeer.data import DATASETS\n\tfrom nlpeer import DATASET_REVIEW_OVERALL_SCALES\n", "from nlpeer.tasks import get_optimizer, get_loss_function, abstract_with_review_only\n\tclass LitTransformerRspModule(pl.LightningModule):\n\t    def __init__(self,\n\t                 model_type,\n\t                 train_loss,\n\t                 dev_loss,\n\t                 optimizer,\n\t                 weight_decay: float = 0.1,\n\t                 warmup_ratio: int = 0.06,\n\t                 **kwargs):\n", "        super().__init__()\n\t        # save hyperparameters\n\t        self.save_hyperparameters()\n\t        # optimizer args\n\t        self.optimizer = optimizer\n\t        # define model\n\t        self.config = AutoConfig.from_pretrained(model_type, num_labels=1)\n\t        self.model = AutoModelForSequenceClassification.from_pretrained(model_type, config=self.config)\n\t        self.model_type = model_type\n\t        # use provided losses\n", "        self.train_loss = train_loss\n\t        self.dev_loss = dev_loss\n\t    def forward(self, **inputs):\n\t        return self.model(**inputs)\n\t    def training_step(self, batch, batch_idx):\n\t        out = self(**batch)\n\t        loss, logits = out[0], out[1]\n\t        targets = batch[\"labels\"]\n\t        pred = logits.squeeze() if logits.size()[0] > 1 else logits\n\t        tloss = self.train_loss(pred, targets)\n", "        self.log(\"train_loss\", tloss)\n\t        return loss\n\t    def validation_step(self, batch, batch_idx) -> Optional[STEP_OUTPUT]:\n\t        out = self(**batch)\n\t        loss, logits = out[0], out[1]\n\t        targets = batch[\"labels\"].to(torch.float32)\n\t        pred = logits.squeeze() if logits.size()[0] > 1 else logits\n\t        vloss = self.dev_loss(pred, targets)\n\t        self.log(\"val_loss\", vloss)\n\t        return loss\n", "    def configure_optimizers(self):\n\t        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\t        optimizer_grouped_parameters = [\n\t            {\n\t                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n\t                \"weight_decay\": self.hparams.weight_decay,\n\t            },\n\t            {\n\t                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n\t                \"weight_decay\": 0.0,\n", "            },\n\t        ]\n\t        optimizer = self.optimizer(optimizer_grouped_parameters, lr=self.hparams.lr, eps=self.hparams.eps)\n\t        scheduler = get_linear_schedule_with_warmup(\n\t            optimizer,\n\t            num_warmup_steps=self.hparams.warmup_ratio * self.trainer.estimated_stepping_batches,\n\t            num_training_steps=self.trainer.estimated_stepping_batches,\n\t        )\n\t        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n\t        return [optimizer], [scheduler]\n", "    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = None):\n\t        out = self(**batch)\n\t        loss, logits = out[0], out[1]\n\t        return {\"loss\": loss,\n\t                \"predictions\": logits.flatten(),\n\t                \"labels\": batch[\"labels\"].to(torch.float32)}\n\t    def on_predict_epoch_end(self, results):\n\t        preds = torch.cat([x['predictions'] for x in results[0]]).detach().cpu().numpy()\n\t        labels = torch.cat([x['labels'] for x in results[0]]).detach().cpu().numpy()\n\t        return {\"predictions\": preds, \"labels\": labels}\n", "    def get_tokenizer(self) -> Callable:\n\t        tokenizer = AutoTokenizer.from_pretrained(self.model_type)\n\t        def tokenize(dataset):\n\t            return tokenizer(dataset, padding=\"max_length\", truncation=True, max_length=512)\n\t        return tokenize\n\t    def get_prepare_input(self, dataset_type: DATASETS) -> tuple[Callable, Callable or None]:\n\t        input_transform = abstract_with_review_only()\n\t        # target normalize\n\t        review_scale = DATASET_REVIEW_OVERALL_SCALES[dataset_type][1]\n\t        def normalize_overall_score(s):\n", "            min_val = np.min(review_scale)\n\t            max_val = np.max(review_scale)\n\t            return (s - min_val) / float(max_val - min_val)\n\t        target_transform = normalize_overall_score\n\t        return input_transform, target_transform\n\tdef from_config(config):\n\t    # load from config\n\t    if config and \"model\" in config and \"train\" in config and \"load_path\" not in config[\"model\"]:\n\t        model_params = config[\"model\"]\n\t        if model_params[\"type\"] in [\"biobert\", \"roberta\", \"scibert\"]: #aliases\n", "            if model_params[\"type\"] == \"roberta\":\n\t                mtype = \"roberta-base\"\n\t            elif model_params[\"type\"] == \"biobert\":\n\t                mtype = \"dmis-lab/biobert-v1.1\"\n\t            elif model_params[\"type\"] == \"scibert\":\n\t                mtype = \"allenai/scibert_scivocab_uncased\"\n\t        else:\n\t            mtype = model_params[\"type\"]\n\t        return LitTransformerRspModule(model_type=mtype,\n\t                                       train_loss=get_loss_function(config[\"train\"][\"train_loss\"]),\n", "                                       dev_loss=get_loss_function(config[\"train\"][\"dev_loss\"], **config[\"train\"][\"dev_loss_kwargs\"] if \"dev_loss_kwargs\" in config[\"train\"] else {}),\n\t                                       optimizer=get_optimizer(config[\"train\"][\"optimizer\"]),\n\t                                       lr=config[\"train\"][\"learning_rate\"],\n\t                                       eps=config[\"train\"][\"epsilon\"])\n\t    # from disk\n\t    elif config and \"model\" in config and \"load_path\" in config[\"model\"]:\n\t        return LitTransformerRspModule.load_from_checkpoint(config[\"model\"][\"load_path\"])\n\t    else:\n\t        raise ValueError(\"Malformed config. Requires training regime and/or a model path to load from\")"]}
{"filename": "nlpeer/tasks/skimming/train.py", "chunked_list": ["import argparse\n\timport os\n\timport time\n\timport uuid\n\tfrom pytorch_lightning import Trainer, seed_everything\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\tfrom pytorch_lightning.loggers import WandbLogger\n\timport torch\n\timport wandb\n\tfrom nlpeer.data import DATASETS\n", "from nlpeer.tasks.skimming.data import SkimmingDataModule\n\tOUT_PATH = None\n\tBENCHMARK_PATH = None\n\tDEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n\tdef get_default_config():\n\t    global BENCHMARK_PATH, DEVICE\n\t    assert BENCHMARK_PATH is not None and os.path.exists(BENCHMARK_PATH), \\\n\t        f\"Provided benchmark path is None or does not exist: {BENCHMARK_PATH}\"\n\t    return {\n\t        \"data_loader\": {\n", "            \"num_workers\": 8,\n\t            \"shuffle\": True\n\t        },\n\t        \"dataset\": {\n\t            \"benchmark_path\": BENCHMARK_PATH\n\t        },\n\t        \"machine\": {\n\t            \"device\": DEVICE\n\t        }\n\t    }\n", "def merge_configs(config, default_config):\n\t    to_expand = [(config, default_config)]\n\t    while len(to_expand) > 0:\n\t        c, oc = to_expand.pop(0)\n\t        for k, v in oc.items():\n\t            if k not in c:\n\t                c[k] = v\n\t            elif type(c[k]) == dict:\n\t                to_expand += [(c[k], v)]\n\t            # else ignore oc config, use the conf you already have\n", "def setup(config, debug=False):\n\t    # get module and transforms\n\t    if config[\"model\"][\"type\"] in [\"roberta\", \"biobert\", \"scibert\"]:\n\t        from nlpeer.tasks.skimming.models.TransformerBased import from_config\n\t        module = from_config(config)\n\t    elif config[\"model\"][\"type\"].startswith(\"baseline\"):\n\t        # from tasks.stance_detection.models.baseline import from_config\n\t        # module = from_config(config)\n\t        raise NotImplementedError()\n\t    else:\n", "        raise ValueError(f\"The provided model type {config['model']['type']} is not supported\")\n\t    input_transform, target_transform = module.get_prepare_input()\n\t    tokenizer = module.get_tokenizer()\n\t    if debug:\n\t        config[\"data_loader\"][\"num_workers\"] = 0\n\t    # load data module\n\t    data_module = SkimmingDataModule(benchmark_path=config[\"dataset\"][\"benchmark_path\"],\n\t                                          dataset_type=DATASETS[config[\"dataset\"][\"type\"]],\n\t                                          in_transform=input_transform,\n\t                                          target_transform=target_transform,\n", "                                          tokenizer=tokenizer,\n\t                                          sampling_strategy=\"random\",\n\t                                          sampling_size=5, # tood load from params\n\t                                          data_loader_config=config[\"data_loader\"],\n\t                                          paper_version=config[\"dataset\"][\"paper_version\"])\n\t    return module, data_module\n\tdef train(model, data_module, params, logger=None, debug=False):\n\t    global OUT_PATH\n\t    print(f\"RUN = {wandb.run.name}\")\n\t    chkp_dir = os.path.join(OUT_PATH, f\"checkpoints/{params['dataset']['type']}/{params['model']['type']}\")\n", "    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n\t                                          mode=\"max\",\n\t                                          dirpath=chkp_dir,\n\t                                          filename=\"{epoch}-{val_loss}-\" + str(uuid.uuid4()),\n\t                                          save_top_k=1,\n\t                                          # every_n_train_steps=10\n\t                                          )\n\t    early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n\t                                        mode=\"max\",\n\t                                        patience=8,\n", "                                        min_delta=0.005)\n\t    trainer = Trainer(logger=logger,\n\t                      log_every_n_steps=1,\n\t                      limit_train_batches=0.05 if debug else 1.0,\n\t                      # devices=1,\n\t                      max_epochs=params[\"train\"][\"epochs\"],\n\t                      accelerator=params[\"machine\"][\"device\"],\n\t                      callbacks=[checkpoint_callback, early_stop_callback])\n\t    # fit the model\n\t    trainer.fit(model, data_module)\n", "    # output best model path\n\t    wandb.log({\"best_model\": checkpoint_callback.best_model_path})\n\t    print(f\"best_model = {checkpoint_callback.best_model_path}\")\n\tdef run(config, debug=False, project=None):\n\t    global OUT_PATH\n\t    dconfig = get_default_config()\n\t    merge_configs(config, dconfig)\n\t    # set seed and log\n\t    seed = int(time.time()) % 100000\n\t    config[\"random_seed\"] = seed\n", "    seed_everything(seed)\n\t    # actual training\n\t    model, data = setup(config, debug)\n\t    train(model, data, config, WandbLogger(dir=os.path.join(OUT_PATH, \"logs\"), project=project), debug)\n\tdef main(args):\n\t    global BENCHMARK_PATH, OUT_PATH\n\t    BENCHMARK_PATH = args.benchmark_path\n\t    OUT_PATH = args.store_results\n\t    assert os.path.exists(BENCHMARK_PATH) and os.path.exists(OUT_PATH), \\\n\t        f\"Benchmark or out path do not exist. Check {BENCHMARK_PATH} and {OUT_PATH} again.\"\n", "    # use default config (basically for debugging)\n\t    dconf = {\n\t        \"dataset\": {\n\t            \"type\": args.dataset,\n\t            \"paper_version\": 1\n\t        },\n\t        \"model\": {\n\t            \"type\": args.model\n\t        },\n\t        \"train\": {\n", "            \"epochs\": 20,\n\t            \"train_loss\": \"ce\",\n\t            \"dev_loss\": \"acc\",\n\t            \"optimizer\": \"adam\",\n\t            \"learning_rate\": 2e-5 if args.lr is None else args.lr,\n\t            \"epsilon\": 1e-8,\n\t            \"structure_labels\": False if args.structure_labels is None else args.structure_labels\n\t        },\n\t        \"data_loader\": {\n\t            \"batch_size\": 2 if args.batch_size is None else args.batch_size\n", "        }\n\t    }\n\t    runs = args.repeat if args.repeat else 1\n\t    for i in range(runs):\n\t        run(dconf, debug=args.debug, project=args.project if args.project is not None else \"Skimming_train\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_path\", required=True, type=str, help=\"Path to the benchmark dir\")\n\t    parser.add_argument(\"--store_results\", required=True, type=str, help=\"Path for logs + results\")\n\t    parser.add_argument(\"--dataset\", required=False, choices=[d.name for d in DATASETS],\n", "                        help=\"Dataset name if no sweep provided\")\n\t    parser.add_argument(\"--model\", required=False, type=str, help=\"model name if no sweep provided\")\n\t    parser.add_argument(\"--lr\", required=False, type=float, help=\"learning rate (opt)\")\n\t    parser.add_argument(\"--structure_labels\", required=False, type=bool, help=\"True, if to use structure labels as input\")\n\t    parser.add_argument(\"--batch_size\", required=False, type=int, help=\"batch size (opt)\")\n\t    parser.add_argument(\"--repeat\", required=False, type=int, default=1, help=\"Number of repetitions\")\n\t    parser.add_argument(\"--debug\", required=False, type=bool, default=False, help=\"Number of repetitions\")\n\t    parser.add_argument(\"--project\", required=False, type=str, help=\"Project name in WANDB\")\n\t    args = parser.parse_args()\n\t    assert args.project is not None and args.repeat is not None, \"Project name required for running a wandb sweep\"\n", "    assert args.dataset is not None and args.model is not None, \\\n\t        \"Dataset type required if not loading from sweep config\"\n\t    main(args)\n"]}
{"filename": "nlpeer/tasks/skimming/evaluate.py", "chunked_list": ["import argparse\n\timport simplejson\n\timport json\n\timport os\n\tfrom copy import copy\n\tfrom typing import Callable\n\tfrom collections import Counter\n\timport plotly.graph_objects as go\n\tfrom os.path import join as pjoin\n\timport numpy as np\n", "import scipy.stats\n\timport sklearn\n\timport torch\n\timport wandb\n\tfrom pytorch_lightning import Trainer\n\tfrom pytorch_lightning.loggers import WandbLogger\n\timport nlpeer.tasks\n\tfrom nlpeer import DATASETS\n\tfrom nlpeer.tasks import get_class_map_skimming\n\tfrom nlpeer.data.utils import list_files\n", "from nlpeer.tasks.skimming.data import SkimmingDataModule\n\tfrom nlpeer.tasks.stance_detection.data import StanceDetectionDataModule\n\tOUT_PATH = None\n\tBENCHMARK_PATH = None\n\tDEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n\tdef get_default_config():\n\t    global BENCHMARK_PATH, DEVICE\n\t    assert BENCHMARK_PATH is not None and os.path.exists(BENCHMARK_PATH), \\\n\t        f\"Provided benchmark path is None or does not exist: {BENCHMARK_PATH}\"\n\t    return {\n", "        \"data_loader\": {\n\t            \"num_workers\": 0,\n\t            \"shuffle\": False\n\t        },\n\t        \"dataset\": {\n\t            \"benchmark_path\": BENCHMARK_PATH,\n\t            \"splits_from_file\": True\n\t        },\n\t        \"machine\": {\n\t            \"device\": DEVICE\n", "        }\n\t    }\n\t# 1. F1-score + accuracy + confusion matrix\n\tdef classification_error(true_scores, predicted_scores, log=False):\n\t    majority_score = get_class_map_skimming()[\"negative\"]\n\t    f1_accu, prec_accu, rec_accu, acc_accu = [], [], [], []\n\t    base_f1_accu, base_prec_accu, base_rec_accu, base_acc_accu = [], [], [], []\n\t    for sample_t, sample_p in zip(true_scores, predicted_scores):\n\t        f1_accu += [sklearn.metrics.f1_score(sample_t, sample_p, pos_label=get_class_map_skimming()[\"positive\"])]\n\t        prec_accu += [sklearn.metrics.precision_score(sample_t, sample_p, pos_label=get_class_map_skimming()[\"positive\"])]\n", "        rec_accu += [sklearn.metrics.recall_score(sample_t, sample_p, pos_label=get_class_map_skimming()[\"positive\"])]\n\t        acc_accu += [sklearn.metrics.accuracy_score(sample_t, sample_p)]\n\t        majority_baseline = [majority_score for s in sample_t]\n\t        base_f1_accu += [sklearn.metrics.f1_score(sample_t, sample_p, pos_label=get_class_map_skimming()[\"positive\"])]\n\t        base_prec_accu += [sklearn.metrics.precision_score(sample_t, sample_p, pos_label=get_class_map_skimming()[\"positive\"])]\n\t        base_rec_accu += [sklearn.metrics.recall_score(sample_t, sample_p, pos_label=get_class_map_skimming()[\"positive\"])]\n\t        base_acc_accu += [sklearn.metrics.accuracy_score(sample_t, majority_baseline)]\n\t    if log:\n\t        wandb.log({\"f1\": {\"mean\": np.mean(f1_accu), \"std\": np.std(f1_accu)}})\n\t        wandb.log({\"prec\": {\"mean\": np.mean(prec_accu), \"std\": np.std(prec_accu)}})\n", "        wandb.log({\"rec\": {\"mean\": np.mean(rec_accu), \"std\": np.std(rec_accu)}})\n\t        wandb.log({\"accuracy\": {\"mean\": np.mean(acc_accu), \"std\": np.std(acc_accu)}})\n\t        wandb.log({\"labels\": get_class_map_skimming()})\n\t        wandb.log(\n\t            {\"baseline\": {\"f1\": {\"mean\": np.mean(base_f1_accu), \"std\": np.std(base_f1_accu)},\n\t                          \"prec\": {\"mean\": np.mean(base_prec_accu), \"std\": np.std(base_prec_accu)},\n\t                          \"rec\": {\"mean\": np.mean(base_rec_accu), \"std\": np.std(base_rec_accu)},\n\t                          \"accuracy\": {\"mean\": np.mean(base_acc_accu), \"std\": np.std(base_acc_accu)}}})\n\t    return {\n\t        \"f1\": {\"mean\": float(np.mean(f1_accu)), \"std\": float(np.std(f1_accu))},\n", "        \"prec\": {\"mean\": float(np.mean(prec_accu)), \"std\": float(np.std(prec_accu))},\n\t        \"rec\": {\"mean\": float(np.mean(rec_accu)), \"std\": float(np.std(rec_accu))},\n\t        \"accuracy\": {\"mean\": float(np.mean(acc_accu)), \"std\": float(np.std(acc_accu))},\n\t        \"base_f1\": {\"mean\": float(np.mean(base_f1_accu)), \"std\": float(np.std(base_f1_accu))},\n\t        \"base_prec\": {\"mean\": float(np.mean(base_prec_accu)), \"std\": float(np.std(base_prec_accu))},\n\t        \"base_rec\": {\"mean\": float(np.mean(base_rec_accu)), \"std\": float(np.std(base_rec_accu))},\n\t        \"base_accuracy\": {\"mean\": float(np.mean(base_acc_accu)), \"std\": float(np.std(base_acc_accu))},\n\t    }\n\tdef rank_measures(true_scores, estimated_scores, ks:list):\n\t    accu = []\n", "    auroc_accu = []\n\t    aupr_accu = []\n\t    mrr = []\n\t    for true_s, estimated_s in zip(true_scores, estimated_scores):\n\t        ranking = list(sorted(zip(true_s, estimated_s), key=lambda x: x[1], reverse=True))\n\t        measures_at_k = {}\n\t        for k in ks:\n\t            if k > len(ranking):\n\t                measures_at_k[f\"precision@{k}\"] = np.nan\n\t                measures_at_k[f\"recall@{k}\"] = np.nan\n", "            measures_at_k[f\"precision@{k}\"] = sklearn.metrics.precision_score([r[0] for r in ranking[:k]], [get_class_map_skimming()[\"positive\"] for r in ranking[:k]], pos_label=get_class_map_skimming()[\"positive\"])\n\t            measures_at_k[f\"recall@{k}\"] = sklearn.metrics.recall_score([r[0] for r in ranking[:k]], [get_class_map_skimming()[\"positive\"] for r in ranking[:k]], pos_label=get_class_map_skimming()[\"positive\"])\n\t        auroc = sklearn.metrics.roc_auc_score(true_s, estimated_s)\n\t        auroc_accu += [auroc]\n\t        pr_curve = sklearn.metrics.precision_recall_curve(true_s, estimated_s)\n\t        au_pr_curve = sklearn.metrics.auc(pr_curve[1], pr_curve[0])\n\t        aupr_accu += [au_pr_curve]\n\t        accu += [measures_at_k]\n\t        mrr += [1 / (1+float(next(i for i, r in enumerate(ranking) if r[0] == get_class_map_skimming()[\"positive\"])))]\n\t    result_at_k = {}\n", "    mean_prec = {f\"mean_precision@{k}\": float(np.nanmean([m[f\"precision@{k}\"] for m in accu])) for k in ks}\n\t    std_prec = {f\"std_precision@{k}\": float(np.nanstd([m[f\"precision@{k}\"] for m in accu])) for k in ks}\n\t    result_at_k.update(mean_prec)\n\t    result_at_k.update(std_prec)\n\t    mean_rec = {f\"mean_recall@{k}\": float(np.nanmean([m[f\"recall@{k}\"] for m in accu])) for k in ks}\n\t    std_rec = {f\"std_recall@{k}\": float(np.nanstd([m[f\"recall@{k}\"] for m in accu])) for k in ks}\n\t    result_at_k.update(mean_rec)\n\t    result_at_k.update(std_rec)\n\t    result_auroc = {}\n\t    result_auroc[\"mean_auroc\"] = float(np.nanmean(auroc_accu))\n", "    result_auroc[\"std_auroc\"] = float(np.nanstd(auroc_accu))\n\t    result_aupr = {}\n\t    result_aupr[\"mean_aupr\"] = float(np.nanmean(aupr_accu))\n\t    result_aupr[\"std_aupr\"] = float(np.nanstd(aupr_accu))\n\t    result_mrr = {\"mean_reciprocal_rank\": float(np.mean(mrr)), \"std_reciprocal_rank\": float(np.std(mrr))}\n\t    return result_at_k, result_auroc, result_aupr, result_mrr\n\t# 2. Ranking error\n\tdef ranking_error(true_relevance, estimated_relevance, log=False):\n\t    precision_recall, auroc, aupr, mrr = rank_measures(true_relevance, estimated_relevance, ks=[2, 3, 4, 5, 6, 8])\n\t    random_baseline = [[float(x) for x in np.random.random(len(x)).tolist()] for x in true_relevance]\n", "    base_pr, base_auroc, base_aupr, base_mrr = rank_measures(true_relevance, random_baseline, ks=[2, 3, 4, 5, 6, 8])\n\t    if log:\n\t        wandb.log({\"precision_recall_at_k\": precision_recall})\n\t        wandb.log({\"AUROC\": auroc})\n\t        wandb.log({\"AU_PR_Curve\": aupr})\n\t        wandb.log({\"MRR\": mrr})\n\t    return {\n\t        \"precision_recall_at_k\": precision_recall,\n\t        \"auroc\": auroc,\n\t        \"au_pr_curve\": aupr,\n", "        \"mrr\": mrr,\n\t        \"baseline\": {\n\t            \"precision_recall_at_k\": base_pr,\n\t            \"auroc\": base_auroc,\n\t            \"au_pr_curve\": base_aupr,\n\t            \"mrr\": base_mrr,\n\t        }\n\t    }\n\tdef setup(config):\n\t    assert \"load_path\" in config[\"model\"], \"Provide a 'load_path' attribute in the config.model to load a checkpoint\"\n", "    if config[\"model\"][\"type\"] in [\"roberta\", \"biobert\", \"scibert\"]:\n\t        from nlpeer.tasks.skimming.models.TransformerBased import from_config\n\t        module = from_config(config)\n\t    elif config[\"model\"][\"type\"] in [\"baseline\"]:\n\t        from nlpeer.tasks.skimming.models.LengthBaseline import from_config\n\t        module = from_config(config)\n\t    else:\n\t        raise ValueError(\"The provided model type is not supported\")\n\t    input_transform, target_transform = module.get_prepare_input()\n\t    tokenizer = module.get_tokenizer()\n", "    # prepare data (loading splits from disk)\n\t    data_module_main = SkimmingDataModule(benchmark_path=config[\"dataset\"][\"benchmark_path\"],\n\t                                          dataset_type=DATASETS[config[\"dataset\"][\"type\"]],\n\t                                          in_transform=input_transform,\n\t                                          target_transform=target_transform,\n\t                                          tokenizer=tokenizer,\n\t                                          sampling_strategy=\"random\", #todo load from params\n\t                                          sampling_size=5, # tood load from params\n\t                                          data_loader_config=config[\"data_loader\"],\n\t                                          paper_version=config[\"dataset\"][\"paper_version\"])\n", "    return module, data_module_main, (input_transform, target_transform, tokenizer)\n\tdef get_predictions(model, data_module, params, logger, debug=False):\n\t    trainer = Trainer(logger=logger,\n\t                      log_every_n_steps=1,\n\t                      devices=1,\n\t                      limit_predict_batches= 0.4 if debug else 1.0,\n\t                      accelerator=params[\"machine\"][\"device\"])\n\t    # accuracy as a loss/performance metric is stored with device information -- discard for predictions.\n\t    model.dev_loss = None\n\t    model.train_loss = None\n", "    model.to(\"cuda\" if params[\"machine\"][\"device\"] == \"gpu\" else params[\"machine\"][\"device\"])\n\t    return trainer.predict(model, data_module)\n\tdef run(config, debug=False):\n\t    global OUT_PATH\n\t    cname = f\"{config['dataset']['type']}_{config['model']['type']}\"\n\t    with wandb.init(dir=os.path.join(OUT_PATH, \"logs\"), config=config, project=config[\"project\"], name=cname):\n\t        dconfig = get_default_config()\n\t        if debug:\n\t            dconfig[\"data_loader\"][\"num_workers\"] = 0\n\t        tasks.merge_configs(config, dconfig)\n", "        wandb_logger = WandbLogger(dir=os.path.join(OUT_PATH, \"logs\"), config=config)\n\t        print(f\"Starting evaluation with config {config}\")\n\t        # get model checkpoints and config paths\n\t        model_load_path = pjoin(config[\"model\"][\"load_path\"], config[\"model\"][\"type\"])\n\t        mconf_path = pjoin(model_load_path, \"config.json\") if os.path.exists(\n\t            pjoin(model_load_path, \"config.json\")) else None\n\t        checkpoints = [c for c in list_files(model_load_path) if os.path.basename(c) != \"config.json\"]\n\t        if len(checkpoints) == 0:\n\t            print(f\"WARNING: Model load path directory lacks checkpoints: {checkpoints}\")\n\t        if config[\"model\"][\"type\"] in [\"baseline\"]:\n", "            checkpoints = [f\"baseline-{config['model']['thresh']}\"]\n\t        # load the model config\n\t        mconf = copy(config)\n\t        print(\"Evaluation...\")\n\t        accu_res = evaluation(checkpoints, config, mconf, wandb_logger, debug)\n\t        mean_res = compute_mean_stats(accu_res)\n\t        wandb.log({\"mean_res\": mean_res})\n\t        mean_res[\"accu\"] = accu_res\n\t        def np_encoder(object):\n\t            if isinstance(object, np.generic):\n", "                return object.item()\n\t            else:\n\t                return None\n\t        print(\"Storing results\")\n\t        with open(pjoin(OUT_PATH, f\"skimming_eval_{config['dataset']['type']}_{config['model']['type']}.json\"),\n\t                  \"w+\") as f:\n\t            simplejson.dump(mean_res, f, indent=4, ignore_nan=True, default=np_encoder)\n\tdef compute_mean_stats(accu_res):\n\t    mean_res = {}\n\t    for s in accu_res:\n", "        for t in s:\n\t            if type(s[t]) != dict:\n\t                continue\n\t            if t not in mean_res:\n\t                mean_res[t] = {}\n\t            for k, v in s[t].items():\n\t                try:\n\t                    vf = float(v)\n\t                except (ValueError, TypeError):\n\t                    continue\n", "                if k not in mean_res[t]:\n\t                    mean_res[t][k] = []\n\t                mean_res[t][k] += [vf]\n\t    for t in mean_res:\n\t        for k, v in mean_res[t].items():\n\t            mean_res[t][k] = {\"mean\": float(np.mean(v)), \"median\": float(np.median(v)), \"std\": float(np.std(v))}\n\t    return mean_res\n\tdef compute_nested_mean_stats(accu_res):\n\t    mean_res = {}\n\t    for s in accu_res:\n", "        measures = [([], list(s.items()))]\n\t        while len(measures) > 0:\n\t            prefix, items = measures.pop()\n\t            for key, val in items:\n\t                if type(val) == dict:\n\t                    measures += [(prefix + [key], list(val.items()))]\n\t                elif type(val) not in [str, list]:\n\t                    cur_position = mean_res\n\t                    for p in prefix:\n\t                        if p not in cur_position:\n", "                            cur_position[p] = {}\n\t                        cur_position = cur_position[p]\n\t                    if key not in cur_position:\n\t                        cur_position[key] = [val]\n\t                    else:\n\t                        cur_position[key] += [val]\n\t                else:\n\t                    raise ValueError(f\"Not supported type {type(val)} found. Cannot aggregate.\")\n\t    pointers = [([], list(mean_res.items()))]\n\t    while len(pointers) > 0:\n", "        prefix, items = pointers.pop()\n\t        for key, val in items:\n\t            if type(val) == dict:\n\t                pointers += [(prefix + [key], list(val.items()))]\n\t            else:\n\t                cur_position = mean_res\n\t                for p in prefix:\n\t                    if p not in cur_position:\n\t                        cur_position[p] = {}\n\t                    cur_position = cur_position[p]\n", "                measurements = val[:]\n\t                cur_position[key] = np.mean(measurements), np.median(measurements), np.std(measurements)\n\t    return mean_res\n\tdef evaluation(checkpoints, config, model_config, wandb_logger, debug=False):\n\t    accu_res = []\n\t    for cpath in checkpoints:\n\t        # define setup config\n\t        model_config[\"model\"][\"load_path\"] = cpath\n\t        model_config[\"dataset\"] = config[\"dataset\"]\n\t        model_config[\"data_loader\"] = config[\"data_loader\"]\n", "        model_config[\"machine\"] = config[\"machine\"]\n\t        # prep evaluation\n\t        model, data_module_main, transforms = setup(model_config)\n\t        predictions = get_predictions(model, data_module_main, config, wandb_logger, debug)\n\t        labels, estimates, pred_labels, rankings = [], [] , [], []\n\t        for batch in predictions:\n\t            es = [sample.detach().cpu().tolist() for sample in batch[\"logits\"]]\n\t            labels += [sample.detach().cpu().tolist() for sample in batch[\"labels\"]]\n\t            estimates += es\n\t            pred_labels += [sample.detach().cpu().tolist() for sample in batch[\"predictions\"]]\n", "            rankings += [[s[get_class_map_skimming()[\"positive\"]] for s in e] for e in es]\n\t        print(\"labels\", labels)\n\t        print(\"predicted\", pred_labels)\n\t        print(\"rankings\", rankings)\n\t        print(\"estimates\", estimates)\n\t        # evaluate\n\t        eval_conf = config[\"evaluation\"]\n\t        res = {}\n\t        if \"error\" in eval_conf and eval_conf[\"error\"]:\n\t            res[\"error\"] = classification_error(labels, pred_labels, True)\n", "        if \"ranking_error\" in eval_conf and eval_conf[\"ranking_error\"]:\n\t            res[\"ranking_error\"] = ranking_error(labels, rankings, True)\n\t        res[\"predicted_ranking\"] = rankings\n\t        res[\"true_labels\"] = labels\n\t        accu_res += [res]\n\t    return accu_res\n\tdef main(args):\n\t    global BENCHMARK_PATH, OUT_PATH\n\t    BENCHMARK_PATH = args.benchmark_dir\n\t    OUT_PATH = args.store_results\n", "    assert os.path.exists(BENCHMARK_PATH) and os.path.exists(OUT_PATH), \\\n\t        f\"Benchmark or out path do not exist. Check {BENCHMARK_PATH} and {OUT_PATH} again.\"\n\t    config = {\n\t        \"name\": f\"Evaluation\",\n\t        \"project\": args.project,\n\t        \"random_seed\": {\n\t            \"value\": 6295629\n\t        },\n\t        \"model\": {\n\t            \"type\": args.model_type,\n", "            \"load_path\": pjoin(args.chkp_dir, f\"{DATASETS[args.dataset].name}\")\n\t        },\n\t        \"data_loader\": {\n\t            \"batch_size\": 2\n\t        },\n\t        \"dataset\": {\n\t            \"type\": DATASETS[args.dataset].name,\n\t            \"paper_version\": args.paper_version\n\t        },\n\t        \"evaluation\": {\n", "            \"error\": True,\n\t            \"ranking_error\": True,\n\t            \"domain_ransfer\": False\n\t        }\n\t    }\n\t    run(config, debug=args.debug)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_dir\", required=True, type=str, help=\"Path to the benchmark dir\")\n\t    parser.add_argument(\"--project\", required=True, type=str, help=\"Project name in WANDB\")\n", "    parser.add_argument(\"--store_results\", required=True, type=str, help=\"Path for logs + results\")\n\t    parser.add_argument(\"--debug\", required=False, default=False, type=bool, help=\"Set true for debugging\")\n\t    parser.add_argument(\"--chkp_dir\", required=True, type=str, help=\"Path to load the checkpoints from\")\n\t    parser.add_argument(\"--dataset\", required=True, choices=[d.name for d in DATASETS], help=\"Name of the dataset\")\n\t    parser.add_argument(\"--model_type\", required=True, type=str, help=\"Model type e.g. biobert\")\n\t    parser.add_argument(\"--paper_version\", required=False, default=1, type=int, help=\"Version of the paper\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "nlpeer/tasks/skimming/__init__.py", "chunked_list": []}
{"filename": "nlpeer/tasks/skimming/data.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom collections import Counter\n\tfrom copy import copy\n\tfrom os.path import join as pjoin\n\tfrom datasets import DatasetDict, Dataset\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader\n\tfrom nlpeer.data import paperwise_stratified_split\n", "from nlpeer import DATASETS, PAPERFORMATS, PaperReviewDataset\n\tfrom nlpeer.tasks import SkimmingDataset\n\tclass SkimmingDataModule(LightningDataModule):\n\t    def __init__(self, benchmark_path: str,\n\t                 dataset_type: DATASETS,\n\t                 in_transform,\n\t                 target_transform,\n\t                 tokenizer,\n\t                 data_loader_config,\n\t                 sampling_strategy: str = \"random\",\n", "                 sampling_size: int = 5,\n\t                 paper_version=1,\n\t                 paper_format=PAPERFORMATS.ITG):\n\t        super().__init__()\n\t        self.base_path = benchmark_path\n\t        self.dataset_type = dataset_type\n\t        self.paper_version = paper_version\n\t        self.paper_format = paper_format\n\t        self.in_transform = in_transform\n\t        self.target_transform = target_transform\n", "        self.tokenizer = tokenizer\n\t        self.sampling_strategy = sampling_strategy\n\t        self.sampling_size = sampling_size\n\t        self.splits = self._load_splits_idx()\n\t        self.full_data = None\n\t        self.train_data = None\n\t        self.dev_data = None\n\t        self.test_data = None\n\t        self.data_loader_config = data_loader_config\n\t    def _load_splits_idx(self):\n", "        fp = pjoin(self.base_path, self.dataset_type.value, \"splits\", \"skimming_split.json\")\n\t        assert os.path.exists(fp) and os.path.isfile(\n\t            fp), f\"Cannot setup Actionability splits, as {fp} does not exist.\"\n\t        with open(fp, \"r\") as file:\n\t            split_data = json.load(file)\n\t        return split_data[\"splits\"]\n\t    def setup(self, stage: str | None) -> None:\n\t        '''called one each GPU separately - stage defines if we are at fit or test step'''\n\t        full_dataset = PaperReviewDataset(self.base_path,\n\t                                          self.dataset_type,\n", "                                          self.paper_version,\n\t                                          self.paper_format,\n\t                                          hold_in_memory=True)\n\t        sk_dataset = SkimmingDataset(full_dataset,\n\t                                     self.in_transform,\n\t                                     self.target_transform,\n\t                                     selected_types=[\"quote\", \"line\", \"sec-ix\", \"sec-name\"],\n\t                                     sampling=self.sampling_strategy,\n\t                                     sample_size=self.sampling_size)\n\t        # \"none\" sampling strategy for testing\n", "        sk_dataset_test = SkimmingDataset(full_dataset,\n\t                                          self.in_transform,\n\t                                          self.target_transform,\n\t                                          selected_types=[\"quote\", \"line\", \"sec-ix\", \"sec-name\"],\n\t                                          sampling=\"none\")\n\t        # load splits\n\t        split_ixs = []\n\t        for s in self.splits[:2]:\n\t            split_ixs += [[sk_dataset.ids().index(rid) for i, rid in s]]\n\t        # load test split by pids (instead of pid%node_id (of positive nodes))\n", "        test_pids = set([rid.split(\"%\")[0] for i, rid in self.splits[2]])\n\t        split_ixs += [[sk_dataset_test.ids().index(pid+\"%all\") for pid in test_pids]]\n\t        self.dataset = DatasetDict({\n\t            \"train\": Dataset.from_dict(sk_dataset.to_dict(split_ixs[0])),\n\t            \"dev\": Dataset.from_dict(sk_dataset.to_dict(split_ixs[1])),\n\t            \"test\": Dataset.from_dict(sk_dataset_test.to_dict(split_ixs[2])),\n\t        })\n\t        for split in self.dataset.keys():\n\t            print(f\"Setting up {split}\")\n\t            self.dataset[split] = self.dataset[split].map(\n", "                self.convert_to_features,\n\t                batched=False\n\t            )\n\t            self.dataset[split].set_format(type=\"torch\")\n\t        print(\"Dataset initialized\")\n\t    @staticmethod\n\t    def _collate_batches(batch):\n\t        result = {\"positives\": [], \"negatives\": []}\n\t        for t in result:\n\t            for s in batch:\n", "                result[t] += [s[t]]\n\t        return result\n\t    def train_dataloader(self):\n\t        '''returns training dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = True\n\t        return DataLoader(self.dataset[\"train\"], **dl_config, collate_fn=self._collate_batches)\n\t    def val_dataloader(self):\n\t        '''returns validation dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n", "        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"dev\"], **dl_config, collate_fn=self._collate_batches)\n\t    def test_dataloader(self):\n\t        '''returns test dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"test\"], **dl_config, collate_fn=self._collate_batches)\n\t    def predict_dataloader(self):\n\t        '''returns test dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n", "        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"test\"], **dl_config, collate_fn=self._collate_batches)\n\t    def convert_to_features(self, sample, indices=None):\n\t        # here we get: sample_batch[\"positives\"] -> list of dicts (likewise for negatives)\n\t        # iterate over positives and negatives\n\t        # now we get the relevant features (txt, label) from each of the entries and apply the tokenizer\n\t        # we output a dict of {\"positives\": [mapped feats], \"negatives\": [...]}\n\t        features = {\"positives\": None, \"negatives\": None}\n\t        for t in features:\n\t            items = sample[t]\n", "            input_raw = items[\"txt\"]\n\t            labels = items[\"label\"]\n\t            features[t] = self.tokenizer(input_raw)\n\t            features[t][\"labels\"] = labels\n\t        return features\n\tdef store_splits(dataset, splits, random_gen, out_path):\n\t    jsplits = {\n\t        \"dataset_type\": dataset.data.dataset_type.name,\n\t        \"dataset\": dataset.__class__.__name__,\n\t        \"splits\": [\n", "            [[int(s), dataset.ids()[s]] for s in split] for split in splits\n\t        ],\n\t        \"random\": str(random_gen)\n\t    }\n\t    with open(out_path, \"w+\") as f:\n\t        json.dump(jsplits, f)\n\tdef create_and_store_splits(full_data: SkimmingDataset,\n\t                            out_dir,\n\t                            splits: list[float],\n\t                            random_gen: int = None):\n", "    # covered papers\n\t    pid_samples = [i.split(\"%\")[0] for i in full_data.ids()]\n\t    pids = list(set(pid_samples))\n\t    hist = Counter(pid_samples)\n\t    max = hist.most_common()[0][1]\n\t    min = hist.most_common()[-1][1]\n\t    bucket_num = 5\n\t    bucket_step = (max - min) // (bucket_num + 1)\n\t    bucket_step = bucket_step if bucket_step > 0 else 1\n\t    def bucketed_samples_per_pid(pid):\n", "        s = hist[pid] - min\n\t        return s // bucket_step\n\t    splitted_pids_ix = paperwise_stratified_split(pids, splits, bucketed_samples_per_pid, random_gen)\n\t    splitted_pids = [[pids[i] for i in split] for split in splitted_pids_ix]\n\t    split_ix, split_ids = [], []\n\t    for spids in splitted_pids:\n\t        ix_of_split = []\n\t        id_of_split = []\n\t        for pid in spids:\n\t            matching_samples = [(i, iid) for i, iid in enumerate(full_data.ids()) if iid.split(\"%\")[0] == pid]\n", "            ix_of_split += [ix for ix, iid in matching_samples]\n\t            id_of_split += [iid for ix, iid in matching_samples]\n\t        split_ix += [ix_of_split]\n\t        split_ids += [id_of_split]\n\t    out_path = os.path.join(out_dir, \"skimming_split.json\")\n\t    store_splits(full_data, split_ix, random_gen, out_path)\n\t    return out_path\n\tdef prepare_dataset_splits(benchmark_path, paper_version, splits, random_gen, datasets=None):\n\t    if datasets is None:\n\t        datasets = [d for d in DATASETS]\n", "    out_files = []\n\t    for d in datasets:\n\t        full_dataset = PaperReviewDataset(benchmark_path,\n\t                                          d,\n\t                                          paper_version,\n\t                                          PAPERFORMATS.ITG)\n\t        sk_dataset = SkimmingDataset(full_dataset,\n\t                                     selected_types=[\"quote\", \"line\", \"sec-ix\", \"sec-name\"],\n\t                                     sampling=\"full\")\n\t        out_path = os.path.join(benchmark_path, d.value, \"splits\")\n", "        if not os.path.exists(out_path):\n\t            os.mkdir(out_path)\n\t        out_files += [create_and_store_splits(sk_dataset, out_path, splits, random_gen)]\n\t    return out_files\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_dir\", required=True, help=\"Path to the benchmark directory\")\n\t    parser.add_argument(\"--paper_version\", required=True, help=\"Which paper version\", type=int)\n\t    parser.add_argument(\"--random_seed\", required=True, help=\"Random seed to generate random splits\", type=int)\n\t    parser.add_argument(\"--datasets\", nargs=\"*\", required=False, help=\"list of datasets, if applicable\", type=str)\n", "    args = parser.parse_args()\n\t    prepare_dataset_splits(args.benchmark_dir,\n\t                           args.paper_version,\n\t                           [0.7, 0.1, 0.2],\n\t                           args.random_seed,\n\t                           [DATASETS[d] for d in\n\t                            (args.datasets if args.datasets is not None else [d.name for d in DATASETS])])\n"]}
{"filename": "nlpeer/tasks/skimming/models/LengthBaseline.py", "chunked_list": ["import argparse\n\timport logging\n\tfrom typing import Callable, Optional\n\timport numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\timport wandb\n\tfrom pytorch_lightning.utilities.types import STEP_OUTPUT\n\tfrom transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, \\\n\t    get_linear_schedule_with_warmup\n", "# model\n\tfrom nlpeer import DATASETS, DATASET_REVIEW_OVERALL_SCALES\n\tfrom nlpeer.tasks import get_optimizer, get_loss_function, get_paragraph_text, get_class_map_skimming\n\tclass LitBaselineSkimmingModule(pl.LightningModule):\n\t    def __init__(self,\n\t                 **kwargs):\n\t        super().__init__()\n\t        # save hyperparameters\n\t        self.save_hyperparameters()\n\t    def forward(self, inputs):\n", "        return torch.Tensor([(len(i)) for i in inputs[\"txt\"]])\n\t    def training_step(self, batch, batch_idx):\n\t        return None\n\t    def validation_step(self, batch, batch_idx) -> Optional[STEP_OUTPUT]:\n\t        positives = batch[\"positives\"]\n\t        negatives = batch[\"negatives\"]\n\t        batch_size = len(positives)\n\t        total_vloss = None\n\t        for i in range(batch_size):\n\t            out_p = self(positives[i])\n", "            targets_p = positives[i][\"labels\"].to(torch.int32)\n\t            out_n = self(negatives[i])\n\t            targets_n = negatives[i][\"labels\"].to(torch.int32)\n\t            vloss = self.dev_loss(torch.cat((out_p, out_n), 0).flatten(), torch.cat((targets_p, targets_n), 0).flatten())\n\t            total_vloss = total_vloss + vloss if total_vloss is not None else vloss\n\t        self.log(\"val_loss\", total_vloss / float(batch_size))\n\t        return total_vloss / float(batch_size)\n\t    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = None):\n\t        positives = batch[\"positives\"]\n\t        negatives = batch[\"negatives\"]\n", "        batch_size = len(positives)\n\t        targets, predictions, logits = [],[], []\n\t        for i in range(batch_size):\n\t            out_p = self(positives[i])\n\t            targets_p = positives[i][\"labels\"].to(torch.int32)\n\t            out_n = self(negatives[i])\n\t            targets_n = negatives[i][\"labels\"].to(torch.int32)\n\t            targets += [torch.concat((targets_p.flatten(), targets_n.flatten()), 0)]\n\t            predictions += [torch.concat((torch.Tensor([(1 if p > self.thresh else 0) for p in out_p]).flatten(), torch.Tensor([(1 if p > self.thresh else 0) for p in out_n]).flatten()), 0)]\n\t            logits += [torch.concat((torch.Tensor([[0, x] for x in out_p.flatten()]),\n", "                                     torch.Tensor([[0, x] for x in out_n.flatten()])), 0)]\n\t        return {\"predictions\": predictions, \"labels\": targets, \"logits\": logits}\n\t    def on_predict_epoch_end(self, results):\n\t        preds = [[y.detach().cpu().numpy() for y in x[\"predictions\"]] for x in results[0]]\n\t        labels = [[y.detach().cpu().numpy() for y in x[\"labels\"]] for x in results[0]]\n\t        logits = [[y.detach().cpu().numpy() for y in x[\"logits\"]] for x in results[0]]\n\t        return {\"predictions\": preds, \"labels\": labels, \"logits\": logits}\n\t    def get_tokenizer(self) -> Callable:\n\t        def tokenize(dataset):\n\t            return {\"txt\": dataset}\n", "        return tokenize\n\t    def get_prepare_input(self) -> tuple[Callable, Callable or None]:\n\t        input_transform = get_paragraph_text()\n\t        target_transform = lambda x: get_class_map_skimming()[x]\n\t        return input_transform, target_transform\n\tdef from_config(config):\n\t    return LitBaselineSkimmingModule()"]}
{"filename": "nlpeer/tasks/skimming/models/__init__.py", "chunked_list": []}
{"filename": "nlpeer/tasks/skimming/models/TransformerBased.py", "chunked_list": ["import argparse\n\timport logging\n\tfrom typing import Callable, Optional\n\timport numpy as np\n\timport pytorch_lightning as pl\n\timport torch\n\timport wandb\n\tfrom pytorch_lightning.utilities.types import STEP_OUTPUT\n\tfrom transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, \\\n\t    get_linear_schedule_with_warmup\n", "# model\n\tfrom nlpeer.data import DATASETS\n\tfrom nlpeer import DATASET_REVIEW_OVERALL_SCALES\n\tfrom nlpeer.tasks import get_optimizer, get_loss_function, get_paragraph_text, get_class_map_skimming\n\tclass LitTransformerSkimmingModule(pl.LightningModule):\n\t    def __init__(self,\n\t                 model_type,\n\t                 train_loss,\n\t                 dev_loss,\n\t                 optimizer,\n", "                 num_labels,\n\t                 learning_mode=\"pointwise\",\n\t                 structure_labels=False,\n\t                 weight_decay: float = 0.1,\n\t                 warmup_ratio: int = 0.06,\n\t                 **kwargs):\n\t        super().__init__()\n\t        # save hyperparameters\n\t        self.save_hyperparameters()\n\t        # optimizer args\n", "        self.optimizer = optimizer\n\t        # define model\n\t        self.config = AutoConfig.from_pretrained(model_type, num_labels=num_labels)\n\t        self.model = AutoModelForSequenceClassification.from_pretrained(model_type, config=self.config)\n\t        self.model_type = model_type\n\t        self.structure_labels = structure_labels\n\t        # use provided losses\n\t        self.train_loss = train_loss\n\t        self.dev_loss = dev_loss\n\t    def forward(self, **inputs):\n", "        return self.model(**inputs)\n\t    def training_step(self, batch, batch_idx):\n\t        positives = batch[\"positives\"]\n\t        negatives = batch[\"negatives\"]\n\t        batch_size = len(positives)\n\t        total_tloss = None\n\t        for i in range(batch_size):\n\t            out_p = self(attention_mask=positives[i][\"attention_mask\"], input_ids=positives[i][\"input_ids\"],\n\t                         labels=positives[i][\"labels\"])\n\t            loss_p, logits_p = out_p[0], out_p[1]\n", "            targets_p = positives[i][\"labels\"]\n\t            out_n = self(attention_mask=negatives[i][\"attention_mask\"], input_ids=negatives[i][\"input_ids\"],\n\t                         labels=negatives[i][\"labels\"])\n\t            loss_n, logits_n = out_n[0], out_n[1]\n\t            targets_n = negatives[i][\"labels\"]\n\t            tloss = self.train_loss(torch.cat((logits_p, logits_n), 0), torch.cat((targets_p, targets_n), 0))\n\t            total_tloss = total_tloss + tloss if total_tloss is not None else tloss\n\t        self.log(\"train_loss\", total_tloss)\n\t        return total_tloss\n\t    def validation_step(self, batch, batch_idx) -> Optional[STEP_OUTPUT]:\n", "        positives = batch[\"positives\"]\n\t        negatives = batch[\"negatives\"]\n\t        batch_size = len(positives)\n\t        total_vloss = None\n\t        for i in range(batch_size):\n\t            out_p = self(attention_mask=positives[i][\"attention_mask\"], input_ids=positives[i][\"input_ids\"], labels=positives[i][\"labels\"])\n\t            loss_p, logits_p = out_p[0], out_p[1]\n\t            predictions_p = torch.argmax(logits_p, axis=1).to(torch.int32)\n\t            targets_p = positives[i][\"labels\"].to(torch.int32)\n\t            out_n = self(attention_mask=negatives[i][\"attention_mask\"], input_ids=negatives[i][\"input_ids\"], labels=negatives[i][\"labels\"])\n", "            loss_n, logits_n = out_n[0], out_n[1]\n\t            predictions_n = torch.argmax(logits_n, axis=1).to(torch.int32)\n\t            targets_n = negatives[i][\"labels\"].to(torch.int32)\n\t            vloss = self.dev_loss(torch.cat((predictions_p, predictions_n), 0).flatten(), torch.cat((targets_p, targets_n), 0).flatten())\n\t            total_vloss = total_vloss + vloss if total_vloss is not None else vloss\n\t        self.log(\"val_loss\", total_vloss / float(batch_size))\n\t        return total_vloss / float(batch_size)\n\t    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = None):\n\t        positives = batch[\"positives\"]\n\t        negatives = batch[\"negatives\"]\n", "        batch_size = len(positives)\n\t        softmax = torch.nn.Softmax(dim=1)\n\t        targets = []\n\t        predictions = []\n\t        logits = []\n\t        for i in range(batch_size):\n\t            out_p = self(attention_mask=positives[i][\"attention_mask\"], input_ids=positives[i][\"input_ids\"],\n\t                         labels=positives[i][\"labels\"])\n\t            loss_p, logits_p = out_p[0], out_p[1]\n\t            predictions_p = torch.argmax(logits_p, axis=1).to(torch.int32)\n", "            targets_p = positives[i][\"labels\"].to(torch.int32)\n\t            out_n = self(attention_mask=negatives[i][\"attention_mask\"], input_ids=negatives[i][\"input_ids\"],\n\t                         labels=negatives[i][\"labels\"])\n\t            loss_n, logits_n = out_n[0], out_n[1]\n\t            predictions_n = torch.argmax(logits_n, axis=1).to(torch.int32)\n\t            targets_n = negatives[i][\"labels\"].to(torch.int32)\n\t            targets += [torch.concat((targets_p.flatten(), targets_n.flatten()), 0)]\n\t            predictions += [torch.concat((predictions_p.flatten(), predictions_n.flatten()), 0)]\n\t            logits += [torch.concat((softmax(logits_p), softmax(logits_n)), 0)]\n\t        return {\"predictions\": predictions, \"logits\": logits, \"labels\": targets}\n", "    def on_predict_epoch_end(self, results):\n\t        preds = [[y.detach().cpu().numpy() for y in x[\"predictions\"]] for x in results[0]]\n\t        labels = [[y.detach().cpu().numpy() for y in x[\"labels\"]] for x in results[0]]\n\t        logits = [[y.detach().cpu().numpy() for y in x[\"logits\"]]for x in results[0]]\n\t        return {\"predictions\": preds, \"labels\": labels, \"logits\": logits}\n\t    def configure_optimizers(self):\n\t        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\t        optimizer_grouped_parameters = [\n\t            {\n\t                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n", "                \"weight_decay\": self.hparams.weight_decay,\n\t            },\n\t            {\n\t                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n\t                \"weight_decay\": 0.0,\n\t            },\n\t        ]\n\t        optimizer = self.optimizer(optimizer_grouped_parameters, lr=self.hparams.lr, eps=self.hparams.eps)\n\t        scheduler = get_linear_schedule_with_warmup(\n\t            optimizer,\n", "            num_warmup_steps=self.hparams.warmup_ratio * self.trainer.estimated_stepping_batches,\n\t            num_training_steps=self.trainer.estimated_stepping_batches,\n\t        )\n\t        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n\t        return [optimizer], [scheduler]\n\t    def get_tokenizer(self) -> Callable:\n\t        tokenizer = AutoTokenizer.from_pretrained(self.model_type, use_fast=True)\n\t        def tokenize(dataset):\n\t            return tokenizer(dataset, padding=\"max_length\", truncation=True, max_length=512)\n\t        return tokenize\n", "    def get_prepare_input(self) -> tuple[Callable, Callable or None]:\n\t        input_transform = get_paragraph_text(self.structure_labels)\n\t        target_transform = lambda x: get_class_map_skimming()[x]\n\t        return input_transform, target_transform\n\tdef from_config(config):\n\t    # load from config\n\t    if config and \"model\" in config and \"train\" in config and \"load_path\" not in config[\"model\"]:\n\t        model_params = config[\"model\"]\n\t        if model_params[\"type\"] in [\"biobert\", \"roberta\", \"scibert\"]:  # aliases\n\t            if model_params[\"type\"] == \"roberta\":\n", "                mtype = \"roberta-base\"\n\t            elif model_params[\"type\"] == \"biobert\":\n\t                mtype = \"dmis-lab/biobert-v1.1\"\n\t            elif model_params[\"type\"] == \"scibert\":\n\t                mtype = \"allenai/scibert_scivocab_uncased\"\n\t        else:\n\t            mtype = model_params[\"type\"]\n\t        return LitTransformerSkimmingModule(model_type=mtype,\n\t                                                 train_loss=get_loss_function(config[\"train\"][\"train_loss\"]),\n\t                                                 dev_loss=get_loss_function(config[\"train\"][\"dev_loss\"],\n", "                                                                            **config[\"train\"][\n\t                                                                                \"dev_loss_kwargs\"] if \"dev_loss_kwargs\" in\n\t                                                                                                      config[\n\t                                                                                                          \"train\"] else {}),\n\t                                                 optimizer=get_optimizer(config[\"train\"][\"optimizer\"]),\n\t                                                 lr=config[\"train\"][\"learning_rate\"],\n\t                                                 eps=config[\"train\"][\"epsilon\"],\n\t                                                 structure_labels=config[\"train\"][\"structure_labels\"],\n\t                                                 num_labels=len(get_class_map_skimming()))\n\t    # from disk\n", "    elif config and \"model\" in config and \"load_path\" in config[\"model\"]:\n\t        return LitTransformerSkimmingModule.load_from_checkpoint(config[\"model\"][\"load_path\"])\n\t    else:\n\t        raise ValueError(\"Malformed config. Requires training regime and/or a model path to load from\")\n"]}
{"filename": "nlpeer/tasks/pragmatic_labeling/train.py", "chunked_list": ["import argparse\n\timport os\n\timport time\n\timport uuid\n\tfrom pytorch_lightning import Trainer, seed_everything\n\tfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\tfrom pytorch_lightning.loggers import WandbLogger\n\timport torch\n\timport wandb\n\tfrom nlpeer.data import DATASETS\n", "from nlpeer.tasks.pragmatic_labeling.data import PragmaticLabelingDataModule\n\tOUT_PATH = None\n\tBENCHMARK_PATH = None\n\tDEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n\tdef get_default_config():\n\t    global BENCHMARK_PATH, DEVICE\n\t    assert BENCHMARK_PATH is not None and os.path.exists(BENCHMARK_PATH), \\\n\t        f\"Provided benchmark path is None or does not exist: {BENCHMARK_PATH}\"\n\t    return {\n\t        \"data_loader\": {\n", "            \"num_workers\": 8,\n\t            \"shuffle\": True\n\t        },\n\t        \"dataset\": {\n\t            \"benchmark_path\": BENCHMARK_PATH\n\t        },\n\t        \"machine\": {\n\t            \"device\": DEVICE\n\t        }\n\t    }\n", "def merge_configs(config, default_config):\n\t    to_expand = [(config, default_config)]\n\t    while len(to_expand) > 0:\n\t        c, oc = to_expand.pop(0)\n\t        for k, v in oc.items():\n\t            if k not in c:\n\t                c[k] = v\n\t            elif type(c[k]) == dict:\n\t                to_expand += [(c[k], v)]\n\t            # else ignore oc config, use the conf you already have\n", "def setup(config, debug=False):\n\t    # get module and transforms\n\t    if config[\"model\"][\"type\"] in [\"roberta\", \"biobert\", \"scibert\"]:\n\t        from nlpeer.tasks.pragmatic_labeling.models.TransformerBased import from_config\n\t        module = from_config(config)\n\t    elif config[\"model\"][\"type\"].startswith(\"baseline\"):\n\t        # from tasks.stance_detection.models.baseline import from_config\n\t        # module = from_config(config)\n\t        raise NotImplementedError()\n\t    else:\n", "        raise ValueError(f\"The provided model type {config['model']['type']} is not supported\")\n\t    input_transform, target_transform = module.get_prepare_input()\n\t    tokenizer = module.get_tokenizer()\n\t    if debug:\n\t        config[\"data_loader\"][\"num_workers\"] = 0\n\t    # load data module\n\t    data_module = PragmaticLabelingDataModule(benchmark_path=config[\"dataset\"][\"benchmark_path\"],\n\t                                            dataset_type=DATASETS[config[\"dataset\"][\"type\"]],\n\t                                            in_transform=input_transform,\n\t                                            target_transform=target_transform,\n", "                                            tokenizer=tokenizer,\n\t                                            data_loader_config=config[\"data_loader\"],\n\t                                            paper_version=config[\"dataset\"][\"paper_version\"])\n\t    data_module.setup(\"fit\")\n\t    return module, data_module\n\tdef train(model, data_module, params, logger=None, debug=False):\n\t    global OUT_PATH\n\t    print(f\"RUN = {wandb.run.name}\")\n\t    chkp_dir = os.path.join(OUT_PATH, f\"checkpoints/{params['dataset']['type']}/{params['model']['type']}\")\n\t    checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\",\n", "                                          mode=\"max\",\n\t                                          dirpath=chkp_dir,\n\t                                          filename=\"{epoch}-{val_loss}-\" + str(uuid.uuid4()),\n\t                                          save_top_k=1,\n\t                                          # every_n_train_steps=10\n\t                                          )\n\t    early_stop_callback = EarlyStopping(monitor=\"val_loss\",\n\t                                        mode=\"max\",\n\t                                        patience=8,\n\t                                        min_delta=0.001)\n", "    trainer = Trainer(logger=logger,\n\t                      log_every_n_steps=1,\n\t                      limit_train_batches=0.1 if debug else 1.0,\n\t                      # devices=1,\n\t                      max_epochs=params[\"train\"][\"epochs\"],\n\t                      accelerator=params[\"machine\"][\"device\"],\n\t                      callbacks=[checkpoint_callback, early_stop_callback])\n\t    # fit the model\n\t    trainer.fit(model, data_module)\n\t    # output best model path\n", "    wandb.log({\"best_model\": checkpoint_callback.best_model_path})\n\t    print(f\"best_model = {checkpoint_callback.best_model_path}\")\n\tdef run(config, debug=False, project=None):\n\t    global OUT_PATH\n\t    dconfig = get_default_config()\n\t    merge_configs(config, dconfig)\n\t    # set seed and log\n\t    seed = int(time.time()) % 100000\n\t    config[\"random_seed\"] = seed\n\t    seed_everything(seed)\n", "    # actual training\n\t    model, data = setup(config, debug)\n\t    train(model, data, config, WandbLogger(dir=os.path.join(OUT_PATH, \"logs\"), project=project), debug)\n\tdef main(args):\n\t    global BENCHMARK_PATH, OUT_PATH\n\t    BENCHMARK_PATH = args.benchmark_path\n\t    OUT_PATH = args.store_results\n\t    assert os.path.exists(BENCHMARK_PATH) and os.path.exists(OUT_PATH), \\\n\t        f\"Benchmark or out path do not exist. Check {BENCHMARK_PATH} and {OUT_PATH} again.\"\n\t    # use default config (basically for debugging)\n", "    dconf = {\n\t        \"dataset\": {\n\t            \"type\": args.dataset,\n\t            \"paper_version\": 1\n\t        },\n\t        \"model\": {\n\t            \"type\": args.model\n\t        },\n\t        \"train\": {\n\t            \"epochs\": 20,\n", "            \"train_loss\": \"nll\",\n\t            \"dev_loss\": \"acc\",\n\t            \"optimizer\": \"adam\",\n\t            \"learning_rate\": 2e-5 if args.lr is None else args.lr,\n\t            \"epsilon\": 1e-8\n\t        },\n\t        \"data_loader\": {\n\t            \"batch_size\": 16 if args.batch_size is None else args.batch_size\n\t        }\n\t    }\n", "    runs = args.repeat if args.repeat else 1\n\t    for i in range(runs):\n\t        run(dconf, debug=args.debug, project=args.project if args.project is not None else \"RSP_train\")\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_path\", required=True, type=str, help=\"Path to the benchmark dir\")\n\t    parser.add_argument(\"--store_results\", required=True, type=str, help=\"Path for logs + results\")\n\t    parser.add_argument(\"--dataset\", required=False, choices=[d.name for d in DATASETS],\n\t                        help=\"Dataset name if no sweep provided\")\n\t    parser.add_argument(\"--model\", required=False, type=str, help=\"model name if no sweep provided\")\n", "    parser.add_argument(\"--lr\", required=False, type=float, help=\"learning rate (opt)\")\n\t    parser.add_argument(\"--batch_size\", required=False, type=int, help=\"batch size (opt)\")\n\t    parser.add_argument(\"--repeat\", required=False, type=int, default=1, help=\"Number of repetitions\")\n\t    parser.add_argument(\"--debug\", required=False, type=bool, default=False, help=\"Number of repetitions\")\n\t    parser.add_argument(\"--project\", required=False, type=str, help=\"Project name in WANDB\")\n\t    args = parser.parse_args()\n\t    assert args.project is not None and args.repeat is not None, \"Project name required for running a wandb sweep\"\n\t    assert args.dataset is not None and args.model is not None, \\\n\t        \"Dataset type required if not loading from sweep config\"\n\t    main(args)\n"]}
{"filename": "nlpeer/tasks/pragmatic_labeling/evaluate.py", "chunked_list": ["import argparse\n\tfrom collections import Counter\n\timport simplejson\n\timport json\n\timport os\n\tfrom copy import copy\n\tfrom typing import Callable\n\timport plotly.graph_objects as go\n\tfrom os.path import join as pjoin\n\timport numpy as np\n", "import scipy.stats\n\timport sklearn\n\timport torch\n\timport wandb\n\tfrom pytorch_lightning import Trainer\n\tfrom pytorch_lightning.loggers import WandbLogger\n\timport tasks\n\tfrom nlpeer import DATASETS\n\tfrom nlpeer.tasks import get_class_map_pragmatics\n\tfrom nlpeer.data.utils import list_files\n", "from nlpeer.tasks.pragmatic_labeling.data import PragmaticLabelingDataModule\n\tOUT_PATH = None\n\tBENCHMARK_PATH = None\n\tDEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n\tdef get_default_config():\n\t    global BENCHMARK_PATH, DEVICE\n\t    assert BENCHMARK_PATH is not None and os.path.exists(BENCHMARK_PATH), \\\n\t        f\"Provided benchmark path is None or does not exist: {BENCHMARK_PATH}\"\n\t    return {\n\t        \"data_loader\": {\n", "            \"num_workers\": 0,\n\t            \"shuffle\": False\n\t        },\n\t        \"dataset\": {\n\t            \"benchmark_path\": BENCHMARK_PATH,\n\t            \"splits_from_file\": True\n\t        },\n\t        \"machine\": {\n\t            \"device\": DEVICE\n\t        }\n", "    }\n\t# 1. F1-score + accuracy + confusion matrix\n\tdef error(true_scores, predicted_scores, log=False):\n\t    f1_micro = tasks.get_loss_function(\"F1-micro\")(predicted_scores, true_scores)\n\t    f1_macro = tasks.get_loss_function(\"F1-macro\")(predicted_scores, true_scores)\n\t    accuracy = sklearn.metrics.accuracy_score(predicted_scores, true_scores)\n\t    confusion_matrix = sklearn.metrics.confusion_matrix(true_scores, predicted_scores, labels=list(get_class_map_pragmatics().values()))\n\t    majority_score = Counter(true_scores).most_common(1)[0][0]\n\t    majority_baseline = [majority_score for s in true_scores]\n\t    f1_micro_baseline = sklearn.metrics.f1_score(true_scores, majority_baseline, average=\"micro\")\n", "    f1_macro_baseline = sklearn.metrics.f1_score(true_scores, majority_baseline, average=\"macro\")\n\t    accuracy_baseline = sklearn.metrics.accuracy_score(true_scores, majority_baseline)\n\t    if log:\n\t        wandb.log({\"f1_micro\": f1_micro})\n\t        wandb.log({\"f1_macro\": f1_macro})\n\t        wandb.log({\"accuracy\": accuracy})\n\t        wandb.log({\n\t            \"confusion_matrix\":\n\t                       wandb.Table(data=confusion_matrix, columns=list(get_class_map_pragmatics().keys()))\n\t            })\n", "        wandb.log({\"labels\": get_class_map_pragmatics()})\n\t        wandb.log({\"baseline\": {\"f1_micro\": f1_micro_baseline, \"f1_macro\": f1_macro_baseline, \"accuracy\": accuracy_baseline}})\n\t    return {\n\t        \"accuracy\": float(accuracy),\n\t        \"f1_micro\": float(f1_micro),\n\t        \"f1_macro\": float(f1_macro),\n\t        \"confusion_matrix\": str(confusion_matrix),\n\t        \"labels\": get_class_map_pragmatics(),\n\t        \"baseline_f1_micro\": float(f1_micro_baseline),\n\t        \"baseline_f1_macro\": float(f1_macro_baseline),\n", "        \"baseline_accuracy\": float(accuracy_baseline)\n\t    }\n\tdef setup(config):\n\t    assert \"load_path\" in config[\"model\"], \"Provide a 'load_path' attribute in the config.model to load a checkpoint\"\n\t    if config[\"model\"][\"type\"] in [\"roberta\", \"biobert\", \"scibert\"]:\n\t        from nlpeer.tasks.pragmatic_labeling.models.TransformerBased import from_config\n\t        module = from_config(config)\n\t    else:\n\t        raise ValueError(\"The provided model type is not supported\")\n\t    input_transform, target_transform = module.get_prepare_input()\n", "    tokenizer = module.get_tokenizer()\n\t    # prepare data (loading splits from disk)\n\t    data_module_main = PragmaticLabelingDataModule(benchmark_path=config[\"dataset\"][\"benchmark_path\"],\n\t                                                 dataset_type=DATASETS[config[\"dataset\"][\"type\"]],\n\t                                                 in_transform=input_transform,\n\t                                                 target_transform=target_transform,\n\t                                                 tokenizer=tokenizer,\n\t                                                 data_loader_config=config[\"data_loader\"],\n\t                                                 paper_version=config[\"dataset\"][\"paper_version\"])\n\t    return module, data_module_main, (input_transform, target_transform, tokenizer)\n", "def resetup(config, transforms):\n\t    input_transform, target_transform, tokenizer = transforms\n\t    other = DATASETS.F1000 if config[\"dataset\"][\"type\"] == DATASETS.ARR22.name else DATASETS.ARR22\n\t    data_module_secondary = PragmaticLabelingDataModule(benchmark_path=config[\"dataset\"][\"benchmark_path\"],\n\t                                                      dataset_type=other,\n\t                                                      in_transform=input_transform,\n\t                                                      target_transform=target_transform,\n\t                                                      tokenizer=tokenizer,\n\t                                                      data_loader_config=config[\"data_loader\"],\n\t                                                      paper_version=config[\"dataset\"][\"paper_version\"])\n", "    return data_module_secondary\n\tdef get_predictions(model, data_module, params, logger, debug=False):\n\t    trainer = Trainer(logger=logger,\n\t                      log_every_n_steps=1,\n\t                      devices=1,\n\t                      limit_predict_batches= 0.1 if debug else 1.0,\n\t                      accelerator=params[\"machine\"][\"device\"])\n\t    # accuracy as a loss/performance metric is stored with device information -- discard for predictions.\n\t    model.dev_loss = None\n\t    model.train_loss = None\n", "    model.to(\"cuda\" if params[\"machine\"][\"device\"] == \"gpu\" else params[\"machine\"][\"device\"])\n\t    return trainer.predict(model, data_module)\n\tdef run(config, debug=False):\n\t    global OUT_PATH\n\t    cname = f\"{config['dataset']['type']}_{config['model']['type']}\"\n\t    with wandb.init(dir=os.path.join(OUT_PATH, \"logs\"), config=config, project=config[\"project\"], name=cname):\n\t        dconfig = get_default_config()\n\t        tasks.merge_configs(config, dconfig)\n\t        wandb_logger = WandbLogger(dir=os.path.join(OUT_PATH, \"logs\"), config=config)\n\t        print(f\"Starting evaluation with config {config}\")\n", "        # get model checkpoints and config paths\n\t        model_load_path = pjoin(config[\"model\"][\"load_path\"], config[\"model\"][\"type\"])\n\t        mconf_path = pjoin(model_load_path, \"config.json\") if os.path.exists(\n\t            pjoin(model_load_path, \"config.json\")) else None\n\t        checkpoints = [c for c in list_files(model_load_path) if os.path.basename(c) != \"config.json\"]\n\t        assert len(checkpoints) > 0, f\"Model load path directory lacks checkpoints: {checkpoints}\"\n\t        # load the model config\n\t        mconf = copy(config)\n\t        print(\"Evaluation...\")\n\t        accu_res = evaluation(checkpoints, config, mconf, wandb_logger, debug)\n", "        mean_res = compute_mean_stats(accu_res)\n\t        wandb.log({\"mean_res\": mean_res})\n\t        mean_res[\"accu\"] = accu_res\n\t        def np_encoder(object):\n\t            if isinstance(object, np.generic):\n\t                return object.item()\n\t            else:\n\t                return None\n\t        print(\"Storing results\")\n\t        with open(pjoin(OUT_PATH, f\"stance_eval_{config['dataset']['type']}_{config['model']['type']}.json\"),\n", "                  \"w+\") as f:\n\t            simplejson.dump(mean_res, f, indent=4, ignore_nan=True, default=np_encoder)\n\tdef compute_mean_stats(accu_res):\n\t    mean_res = {}\n\t    for s in accu_res:\n\t        for t in s:\n\t            if type(s[t]) != dict:\n\t                continue\n\t            if t not in mean_res:\n\t                mean_res[t] = {}\n", "            for k, v in s[t].items():\n\t                try:\n\t                    vf = float(v)\n\t                except (ValueError, TypeError):\n\t                    continue\n\t                if k not in mean_res[t]:\n\t                    mean_res[t][k] = []\n\t                mean_res[t][k] += [vf]\n\t    for t in mean_res:\n\t        for k, v in mean_res[t].items():\n", "            mean_res[t][k] = {\"mean\": float(np.mean(v)), \"median\": float(np.median(v)), \"std\": float(np.std(v))}\n\t    return mean_res\n\tdef evaluation(checkpoints, config, model_config, wandb_logger, debug=False):\n\t    accu_res = []\n\t    for cpath in checkpoints:\n\t        # define setup config\n\t        model_config[\"model\"][\"load_path\"] = cpath\n\t        model_config[\"dataset\"] = config[\"dataset\"]\n\t        model_config[\"data_loader\"] = config[\"data_loader\"]\n\t        model_config[\"machine\"] = config[\"machine\"]\n", "        # prep evaluation\n\t        model, data_module_main, transforms = setup(model_config)\n\t        predictions = get_predictions(model, data_module_main, config, wandb_logger, debug)\n\t        labels = torch.cat([x[\"labels\"] for x in predictions]).detach().cpu().tolist()\n\t        predictions = torch.cat([x[\"predictions\"] for x in predictions]).detach().cpu().tolist()\n\t        print(predictions)\n\t        print(labels)\n\t        # evaluate\n\t        eval_conf = config[\"evaluation\"]\n\t        res = {}\n", "        if \"error\" in eval_conf and eval_conf[\"error\"]:\n\t            res[\"error\"] = error(labels, predictions, True)\n\t        if \"domain_ransfer\" in eval_conf and eval_conf[\"domain_ransfer\"]:\n\t            data_module_sec = resetup(config, transforms)\n\t            predictions2 = get_predictions(model, data_module_sec, config, wandb_logger, debug)\n\t            labels2 = torch.cat([x[\"labels\"] for x in predictions2]).detach().cpu().tolist()\n\t            predictions2 = torch.cat([x[\"predictions\"] for x in predictions2]).detach().cpu().tolist()\n\t            print(predictions2)\n\t            print(labels2)\n\t            res[f\"transfer_from_{config['dataset']['type']}_to_{data_module_sec.dataset_type.name}\"] = error(labels2, predictions2, True)\n", "        accu_res += [res]\n\t    return accu_res\n\tdef main(args):\n\t    global BENCHMARK_PATH, OUT_PATH\n\t    BENCHMARK_PATH = args.benchmark_dir\n\t    OUT_PATH = args.store_results\n\t    assert os.path.exists(BENCHMARK_PATH) and os.path.exists(OUT_PATH), \\\n\t        f\"Benchmark or out path do not exist. Check {BENCHMARK_PATH} and {OUT_PATH} again.\"\n\t    config = {\n\t        \"name\": f\"Evaluation\",\n", "        \"project\": args.project,\n\t        \"random_seed\": {\n\t            \"value\": 29491\n\t        },\n\t        \"model\": {\n\t            \"type\": args.model_type,\n\t            \"load_path\": pjoin(args.chkp_dir, f\"{DATASETS[args.dataset].name}\")\n\t        },\n\t        \"data_loader\": {\n\t            \"batch_size\": 15\n", "        },\n\t        \"dataset\": {\n\t            \"type\": DATASETS[args.dataset].name,\n\t            \"paper_version\": args.paper_version\n\t        },\n\t        \"evaluation\": {\n\t            \"error\": True,\n\t            \"domain_ransfer\": True\n\t        }\n\t    }\n", "    run(config, debug=args.debug)\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_dir\", required=True, type=str, help=\"Path to the benchmark dir\")\n\t    parser.add_argument(\"--project\", required=True, type=str, help=\"Project name in WANDB\")\n\t    parser.add_argument(\"--store_results\", required=True, type=str, help=\"Path for logs + results\")\n\t    parser.add_argument(\"--debug\", required=False, default=False, type=bool, help=\"Set true for debugging\")\n\t    parser.add_argument(\"--chkp_dir\", required=True, type=str, help=\"Path to load the checkpoints from\")\n\t    parser.add_argument(\"--dataset\", required=True, choices=[d.name for d in DATASETS], help=\"Name of the dataset\")\n\t    parser.add_argument(\"--model_type\", required=True, type=str, help=\"Model type e.g. biobert\")\n", "    parser.add_argument(\"--paper_version\", required=False, default=1, type=int, help=\"Version of the paper\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "nlpeer/tasks/pragmatic_labeling/__init__.py", "chunked_list": []}
{"filename": "nlpeer/tasks/pragmatic_labeling/data.py", "chunked_list": ["import argparse\n\timport json\n\timport os\n\tfrom copy import copy\n\tfrom os.path import join as pjoin\n\tfrom datasets import DatasetDict, Dataset\n\tfrom pytorch_lightning import LightningDataModule\n\tfrom torch.utils.data import DataLoader\n\tfrom nlpeer import DATASETS, PAPERFORMATS, PaperReviewDataset\n\tfrom nlpeer.tasks import PragmaticLabelingDataset, random_split_pragmatic_labeling_dataset\n", "class PragmaticLabelingDataModule(LightningDataModule):\n\t    def __init__(self, benchmark_path: str,\n\t                 dataset_type: DATASETS,\n\t                 in_transform,\n\t                 target_transform,\n\t                 tokenizer,\n\t                 data_loader_config,\n\t                 paper_version=1,\n\t                 paper_format=PAPERFORMATS.ITG):\n\t        super().__init__()\n", "        self.base_path = benchmark_path\n\t        self.dataset_type = dataset_type\n\t        self.paper_version = paper_version\n\t        self.paper_format = paper_format\n\t        self.in_transform = in_transform\n\t        self.target_transform = target_transform\n\t        self.tokenizer = tokenizer\n\t        self.splits = self._load_splits_idx()\n\t        self.full_data = None\n\t        self.train_data = None\n", "        self.dev_data = None\n\t        self.test_data = None\n\t        self.data_loader_config = data_loader_config\n\t    def _load_splits_idx(self):\n\t        fp = pjoin(self.base_path, self.dataset_type.value, \"splits\", \"prag_split.json\")\n\t        assert os.path.exists(fp) and os.path.isfile(\n\t            fp), f\"Cannot setup Pragmatic Labeling splits, as {fp} does not exist.\"\n\t        with open(fp, \"r\") as file:\n\t            split_data = json.load(file)\n\t        return split_data[\"splits\"]\n", "    def setup(self, stage: str | None) -> None:\n\t        '''called one each GPU separately - stage defines if we are at fit or test step'''\n\t        full_dataset = PaperReviewDataset(self.base_path,\n\t                                          self.dataset_type,\n\t                                          self.paper_version,\n\t                                          self.paper_format,\n\t                                          hold_in_memory=True)\n\t        sd_dataset = PragmaticLabelingDataset(full_dataset, self.in_transform, self.target_transform)\n\t        split_ixs = []\n\t        for s in self.splits:\n", "            split_ixs += [[sd_dataset.ids().index(rid) for i, rid in s]]\n\t        self.dataset = DatasetDict({\n\t            \"train\": Dataset.from_dict(sd_dataset.to_dict(split_ixs[0])),\n\t            \"dev\": Dataset.from_dict(sd_dataset.to_dict(split_ixs[1])),\n\t            \"test\": Dataset.from_dict(sd_dataset.to_dict(split_ixs[2])),\n\t        })\n\t        for split in self.dataset.keys():\n\t            print(f\"Setting up {split}\")\n\t            self.dataset[split] = self.dataset[split].map(\n\t                self.convert_to_features,\n", "                batched=True,\n\t                remove_columns=[\"txt\", \"label\"]\n\t            )\n\t            self.dataset[split].set_format(type=\"torch\")\n\t        print(\"Dataset initialized\")\n\t    def train_dataloader(self):\n\t        '''returns training dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = True\n\t        return DataLoader(self.dataset[\"train\"], **dl_config)\n", "    def val_dataloader(self):\n\t        '''returns validation dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"dev\"], **dl_config)\n\t    def test_dataloader(self):\n\t        '''returns test dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"test\"], **dl_config)\n", "    def predict_dataloader(self):\n\t        '''returns test dataloader'''\n\t        dl_config = copy(self.data_loader_config)\n\t        dl_config[\"shuffle\"] = False\n\t        return DataLoader(self.dataset[\"test\"], **dl_config)\n\t    def convert_to_features(self, sample_batch, indices=None):\n\t        labels = sample_batch[\"label\"]\n\t        input_raw = sample_batch[\"txt\"]\n\t        features = self.tokenizer(input_raw)\n\t        features[\"labels\"] = labels\n", "        return features\n\tdef store_splits(dataset, splits, random_gen, out_path):\n\t    jsplits = {\n\t        \"dataset_type\": dataset.data.dataset_type.name,\n\t        \"dataset\": dataset.__class__.__name__,\n\t        \"splits\": [\n\t            [[int(s), dataset.ids()[s]] for s in split] for split in splits\n\t        ],\n\t        \"random\": str(random_gen)\n\t    }\n", "    with open(out_path, \"w+\") as f:\n\t        json.dump(jsplits, f)\n\tdef create_and_store_splits(full_data: PragmaticLabelingDataset,\n\t                            out_dir,\n\t                            splits: list[float],\n\t                            random_gen: int = None):\n\t    split_ix, split_ids = random_split_pragmatic_labeling_dataset(full_data, splits, random_gen)\n\t    out_path = os.path.join(out_dir, \"prag_split.json\")\n\t    store_splits(full_data, split_ix, random_gen, out_path)\n\t    return out_path\n", "def prepare_dataset_splits(benchmark_path, paper_version, splits, random_gen, datasets=None):\n\t    if datasets is None:\n\t        datasets = [d for d in DATASETS]\n\t    out_files = []\n\t    for d in datasets:\n\t        full_dataset = PaperReviewDataset(benchmark_path,\n\t                                          d,\n\t                                          paper_version,\n\t                                          PAPERFORMATS.ITG)\n\t        sd_dataset = PragmaticLabelingDataset(full_dataset)\n", "        out_path = os.path.join(benchmark_path, d.value, \"splits\")\n\t        if not os.path.exists(out_path):\n\t            os.mkdir(out_path)\n\t        out_files += [create_and_store_splits(sd_dataset, out_path, splits, random_gen)]\n\t    return out_files\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\"--benchmark_dir\", required=True, help=\"Path to the benchmark directory\")\n\t    parser.add_argument(\"--paper_version\", required=True, help=\"Which paper version\", type=int)\n\t    parser.add_argument(\"--random_seed\", required=True, help=\"Random seed to generate random splits\", type=int)\n", "    parser.add_argument(\"--datasets\", nargs=\"*\", required=False, help=\"list of datasets, if applicable\", type=str)\n\t    args = parser.parse_args()\n\t    assert set(DATASETS[d] for d in args.datasets).issubset({DATASETS.ARR22, DATASETS.F1000}), \\\n\t        \"Only ARR22 and F1000 supported for stance classification and hence splitting\"\n\t    prepare_dataset_splits(args.benchmark_dir,\n\t                           args.paper_version,\n\t                           [0.7, 0.1, 0.2],\n\t                           args.random_seed,\n\t                           [DATASETS[d] for d in args.datasets])\n"]}
{"filename": "nlpeer/tasks/pragmatic_labeling/models/__init__.py", "chunked_list": []}
{"filename": "nlpeer/tasks/pragmatic_labeling/models/TransformerBased.py", "chunked_list": ["from typing import Callable, Optional\n\timport pytorch_lightning as pl\n\timport torch\n\tfrom pytorch_lightning.utilities.types import STEP_OUTPUT\n\tfrom transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, \\\n\t    get_linear_schedule_with_warmup\n\t# model\n\tfrom nlpeer.tasks import get_optimizer, get_loss_function, review_sentence_no_context, get_class_map_pragmatics\n\tclass LitTransformerPragmaticLabelingModule(pl.LightningModule):\n\t    def __init__(self,\n", "                 model_type,\n\t                 train_loss,\n\t                 dev_loss,\n\t                 optimizer,\n\t                 num_labels,\n\t                 weight_decay: float = 0.1,\n\t                 warmup_ratio: int = 0.06,\n\t                 **kwargs):\n\t        super().__init__()\n\t        # save hyperparameters\n", "        self.save_hyperparameters()\n\t        # optimizer args\n\t        self.optimizer = optimizer\n\t        # define model\n\t        self.config = AutoConfig.from_pretrained(model_type, num_labels=num_labels)\n\t        self.model = AutoModelForSequenceClassification.from_pretrained(model_type, config=self.config)\n\t        self.model_type = model_type\n\t        # use provided losses\n\t        self.train_loss = train_loss\n\t        self.dev_loss = dev_loss\n", "    def forward(self, **inputs):\n\t        return self.model(**inputs)\n\t    def training_step(self, batch, batch_idx):\n\t        out = self(**batch)\n\t        loss, logits = out[0], out[1]\n\t        targets = batch[\"labels\"]\n\t        tloss = self.train_loss(logits, targets)\n\t        self.log(\"train_loss\", tloss)\n\t        return loss\n\t    def validation_step(self, batch, batch_idx) -> Optional[STEP_OUTPUT]:\n", "        out = self(**batch)\n\t        loss, logits = out[0], out[1]\n\t        targets = batch[\"labels\"].to(torch.int32)\n\t        predictions = torch.argmax(logits, axis=1).to(torch.int32)\n\t        vloss = self.dev_loss(predictions, targets)\n\t        self.log(\"val_loss\", vloss)\n\t        return vloss\n\t    def configure_optimizers(self):\n\t        no_decay = [\"bias\", \"LayerNorm.weight\"]\n\t        optimizer_grouped_parameters = [\n", "            {\n\t                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n\t                \"weight_decay\": self.hparams.weight_decay,\n\t            },\n\t            {\n\t                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n\t                \"weight_decay\": 0.0,\n\t            },\n\t        ]\n\t        optimizer = self.optimizer(optimizer_grouped_parameters, lr=self.hparams.lr, eps=self.hparams.eps)\n", "        scheduler = get_linear_schedule_with_warmup(\n\t            optimizer,\n\t            num_warmup_steps=self.hparams.warmup_ratio * self.trainer.estimated_stepping_batches,\n\t            num_training_steps=self.trainer.estimated_stepping_batches,\n\t        )\n\t        scheduler = {\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}\n\t        return [optimizer], [scheduler]\n\t    def predict_step(self, batch, batch_idx: int, dataloader_idx: int = None):\n\t        out = self(**batch)\n\t        loss, logits = out[0], out[1]\n", "        targets = batch[\"labels\"].to(torch.int32)\n\t        predictions = torch.argmax(logits, axis=1).to(torch.int32)\n\t        return {\"loss\": loss, \"predictions\": predictions, \"logits\": logits, \"labels\": targets}\n\t    def on_predict_epoch_end(self, results):\n\t        preds = torch.cat([x['predictions'] for x in results[0]]).detach().cpu().numpy()\n\t        labels = torch.cat([x['labels'] for x in results[0]]).detach().cpu().numpy()\n\t        logits = torch.cat([x['logits'] for x in results[0]]).detach().cpu().numpy()\n\t        return {\"predictions\": preds, \"labels\": labels, \"logits\": logits}\n\t    def get_tokenizer(self) -> Callable:\n\t        tokenizer = AutoTokenizer.from_pretrained(self.model_type, use_fast=True)\n", "        def tokenize(dataset):\n\t            return tokenizer(dataset, padding=\"max_length\", truncation=True, max_length=512)\n\t        return tokenize\n\t    def get_prepare_input(self) -> tuple[Callable, Callable or None]:\n\t        input_transform = review_sentence_no_context()\n\t        target_transform = lambda x: get_class_map_pragmatics()[x]\n\t        return input_transform, target_transform\n\tdef from_config(config):\n\t    # load from config\n\t    if config and \"model\" in config and \"train\" in config and \"load_path\" not in config[\"model\"]:\n", "        model_params = config[\"model\"]\n\t        if model_params[\"type\"] in [\"biobert\", \"roberta\", \"scibert\"]:  # aliases\n\t            if model_params[\"type\"] == \"roberta\":\n\t                mtype = \"roberta-base\"\n\t            elif model_params[\"type\"] == \"biobert\":\n\t                mtype = \"dmis-lab/biobert-v1.1\"\n\t            elif model_params[\"type\"] == \"scibert\":\n\t                mtype = \"allenai/scibert_scivocab_uncased\"\n\t        else:\n\t            mtype = model_params[\"type\"]\n", "        return LitTransformerPragmaticLabelingModule(model_type=mtype,\n\t                                                     train_loss=get_loss_function(config[\"train\"][\"train_loss\"]),\n\t                                                     dev_loss=get_loss_function(config[\"train\"][\"dev_loss\"], **config[\"train\"][\n\t                                              \"dev_loss_kwargs\"] if \"dev_loss_kwargs\" in config[\"train\"] else {}),\n\t                                                     optimizer=get_optimizer(config[\"train\"][\"optimizer\"]),\n\t                                                     lr=config[\"train\"][\"learning_rate\"],\n\t                                                     eps=config[\"train\"][\"epsilon\"],\n\t                                                     num_labels=len(get_class_map_pragmatics()))\n\t    # from disk\n\t    elif config and \"model\" in config and \"load_path\" in config[\"model\"]:\n", "        return LitTransformerPragmaticLabelingModule.load_from_checkpoint(config[\"model\"][\"load_path\"])\n\t    else:\n\t        raise ValueError(\"Malformed config. Requires training regime and/or a model path to load from\")"]}
