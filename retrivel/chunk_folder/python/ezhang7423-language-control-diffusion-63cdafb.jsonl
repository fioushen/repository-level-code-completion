{"filename": "src/lcd/__main__.py", "chunked_list": ["import importlib.util\n\timport os\n\tfrom pathlib import Path\n\timport typer\n\tfrom loguru import logger\n\tfrom lcd.utils.setup import abspath  # for monkeypatches\n\t# Define the directory path containing the Python modules to import\n\tapps_dir = abspath() / \"apps\"\n\tapp: typer.Typer = typer.Typer(name=\"lcd\", no_args_is_help=True, pretty_exceptions_show_locals=False)\n\tdef main():\n", "    \"\"\"The master entrypoint to lcd.\n\t    :param app: The app to run.\n\t    :type app: str\n\t    \"\"\"\n\t    command_kwargs = dict(\n\t        context_settings={\n\t            \"allow_extra_args\": True,\n\t            \"ignore_unknown_options\": True,\n\t        },\n\t        # no_args_is_help=True\n", "    )\n\t    # Iterate over all files in the directory\n\t    for filename in os.listdir(apps_dir):\n\t        # Check if the file is a Python module\n\t        if filename.endswith(\".py\"):\n\t            # Construct the module name from the filename\n\t            module_name = filename[:-3]\n\t            # Import the module using importlib\n\t            module_spec = importlib.util.spec_from_file_location(\n\t                module_name, os.path.join(apps_dir, filename)\n", "            )\n\t            module = importlib.util.module_from_spec(module_spec)\n\t            module_spec.loader.exec_module(module)\n\t            # Add the module to the dictionary\n\t            if hasattr(module, \"_app_\"):\n\t                module_app = module._app_\n\t                app.add_typer(\n\t                    module_app,\n\t                    name=module_name,\n\t                    **command_kwargs\n", "                    # add docs later by modulestring\n\t                )\n\t            else:\n\t                try:\n\t                    app.command(name=module_name, **command_kwargs)(module.main)\n\t                except AttributeError as e:\n\t                    logger.info(\n\t                        f\"{app} doesn't have a main function. Please check that {os.path.join(apps_dir, module_name)}.py defines main()\"\n\t                    )\n\t    app()\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "src/lcd/__init__.py", "chunked_list": ["# type: ignore[attr-defined]\n\t\"\"\"Code for efficiently scaling through space, time, and tasks\"\"\"\n\t\"\"\"\n\tMonkey Patching\n\t\"\"\"\n\t# monkey patch loguru to default with colors\n\timport loguru\n\tfrom lcd.utils.setup import abspath\n\tloguru.logger = loguru.logger.opt(colors=True)\n\tdef remove_shm_from_resource_tracker():\n", "    \"\"\"\n\t    Monkey patch multiprocessing.resource_tracker so SharedMemory won't be tracked\n\t    More details at: https://bugs.python.org/issue38119\n\t    \"\"\"\n\t    # pylint: disable=protected-access, import-outside-toplevel\n\t    # Ignore linting errors in this bug workaround hack\n\t    from multiprocessing import resource_tracker\n\t    def fix_register(name, rtype):\n\t        if rtype == \"shared_memory\":\n\t            return None\n", "        return resource_tracker._resource_tracker.register(name, rtype)\n\t    resource_tracker.register = fix_register\n\t    def fix_unregister(name, rtype):\n\t        if rtype == \"shared_memory\":\n\t            return None\n\t        return resource_tracker._resource_tracker.unregister(name, rtype)\n\t    resource_tracker.unregister = fix_unregister\n\t    if \"shared_memory\" in resource_tracker._CLEANUP_FUNCS:\n\t        del resource_tracker._CLEANUP_FUNCS[\"shared_memory\"]\n\t# More details at: https://bugs.python.org/issue38119\n", "remove_shm_from_resource_tracker()\n\timport sys\n\tfrom importlib import metadata as importlib_metadata\n\t\"\"\"\n\tGlobal variables\n\t\"\"\"\n\tfrom pathlib import Path\n\t# * Feel free to change this path if your data is stored somewhere else\n\t# DATA_PATH = (abspath() / \"../../submodules/hulc-data\").resolve()\n\tDATA_PATH = (abspath() / \"../../submodules/hulc-data\").resolve()\n", "HULC_PATH = (abspath() / \"../../submodules/hulc-baseline\").resolve()\n\tREPO_PATH = (abspath() / \"../../\").resolve()\n\t# Check these paths exist\n\tassert DATA_PATH.exists(), f\"{DATA_PATH=} does not exist\"\n\tassert HULC_PATH.exists(), f\"{HULC_PATH=} does not exist\"\n\tassert REPO_PATH.exists(), f\"{REPO_PATH=} does not exist\"\n\t\"\"\"\n\tVersioning\n\t\"\"\"\n\tdef get_version() -> str:\n", "    try:\n\t        return importlib_metadata.version(__name__)\n\t    except importlib_metadata.PackageNotFoundError:  # pragma: no cover\n\t        return \"unknown\"\n\tversion: str = get_version()\n"]}
{"filename": "src/lcd/utils/git_utils.py", "chunked_list": ["import os\n\timport pdb\n\timport git\n\tPROJECT_PATH = os.path.dirname(os.path.realpath(os.path.join(__file__, \"..\", \"..\")))\n\tdef get_repo(path=PROJECT_PATH, search_parent_directories=True):\n\t    repo = git.Repo(path, search_parent_directories=search_parent_directories)\n\t    return repo\n\tdef get_git_rev(*args, **kwargs):\n\t    try:\n\t        repo = get_repo(*args, **kwargs)\n", "        if repo.head.is_detached:\n\t            git_rev = repo.head.object.name_rev\n\t        else:\n\t            git_rev = repo.active_branch.commit.name_rev\n\t    except:\n\t        git_rev = None\n\t    return git_rev\n\tdef git_diff(*args, **kwargs):\n\t    repo = get_repo(*args, **kwargs)\n\t    diff = repo.git.diff()\n", "    return diff\n\tdef save_git_diff(savepath, *args, **kwargs):\n\t    diff = git_diff(*args, **kwargs)\n\t    with open(savepath, \"w\") as f:\n\t        f.write(diff)\n\tif __name__ == \"__main__\":\n\t    git_rev = get_git_rev()\n\t    print(git_rev)\n\t    save_git_diff(\"diff_test.txt\")\n"]}
{"filename": "src/lcd/utils/setup.py", "chunked_list": ["import datetime\n\timport importlib\n\timport inspect\n\timport os\n\timport pdb\n\timport random\n\tfrom pathlib import Path\n\timport numpy as np\n\timport torch\n\tfrom tap import Tap\n", "from .git_utils import get_git_rev, save_git_diff\n\tfrom .serialization import mkdir\n\tdef set_seed(seed):\n\t    random.seed(seed)\n\t    np.random.seed(seed)\n\t    torch.manual_seed(seed)\n\t    torch.cuda.manual_seed_all(seed)\n\tdef abspath():\n\t    # https://stackoverflow.com/questions/16771894/python-nameerror-global-name-file-is-not-defined\n\t    # https://docs.python.org/3/library/inspect.html#inspect.FrameInfo\n", "    # return os.path.dirname(inspect.stack()[1][1]) # type: ignore\n\t    # return os.path.dirname(getsourcefile(lambda:0)) # type: ignore\n\t    return Path(inspect.getsourcefile(inspect.stack()[1][0])).parent  # type: ignore\n\tdef watch(args_to_watch):\n\t    def _fn(args):\n\t        exp_name = []\n\t        for key, label in args_to_watch:\n\t            if not hasattr(args, key):\n\t                continue\n\t            val = getattr(args, key)\n", "            if type(val) == dict:\n\t                val = \"_\".join(f\"{k}-{v}\" for k, v in val.items())\n\t            exp_name.append(f\"{label}{val}\")\n\t        exp_name = \"_\".join(exp_name)\n\t        exp_name = exp_name.replace(\"/_\", \"/\")\n\t        exp_name = exp_name.replace(\"(\", \"\").replace(\")\", \"\")\n\t        exp_name = exp_name.replace(\", \", \"-\")\n\t        return exp_name\n\t    return _fn\n\tdef lazy_fstring(template, args):\n", "    ## https://stackoverflow.com/a/53671539\n\t    return eval(f\"f'{template}'\")\n\tclass Parser(Tap):\n\t    def save(self):\n\t        fullpath = os.path.join(self.savepath, \"args.json\")\n\t        print(f\"[ utils/setup ] Saved args to {fullpath}\")\n\t        super().save(fullpath, skip_unpicklable=True)\n\t    def parse_args(self, experiment=None):\n\t        args = super().parse_args(known_only=True)\n\t        ## if not loading from a config script, skip the result of the setup\n", "        if not hasattr(args, \"config\"):\n\t            return args\n\t        args = self.read_config(args, experiment)\n\t        self.add_extras(args)\n\t        self.eval_fstrings(args)\n\t        self.set_seed(args)\n\t        self.get_commit(args)\n\t        self.set_loadbase(args)\n\t        self.generate_exp_name(args)\n\t        self.mkdir(args)\n", "        self.save_diff(args)\n\t        return args\n\t    def read_config(self, args, experiment):\n\t        \"\"\"\n\t        Load parameters from config file\n\t        \"\"\"\n\t        print(f\"[ utils/setup ] Reading config: {args.config}\")\n\t        module = importlib.import_module(args.config)\n\t        params = getattr(module, \"base\")[experiment]\n\t        self._dict = {}\n", "        for key, val in params.items():\n\t            setattr(args, key, val)\n\t            self._dict[key] = val\n\t        return args\n\t    def add_extras(self, args):\n\t        \"\"\"\n\t        Override config parameters with command-line arguments\n\t        \"\"\"\n\t        extras = args.extra_args\n\t        if not len(extras):\n", "            return\n\t        print(f\"[ utils/setup ] Found extras: {extras}\")\n\t        assert (\n\t            len(extras) % 2 == 0\n\t        ), f\"Found odd number ({len(extras)}) of extras: {extras}\"\n\t        for i in range(0, len(extras), 2):\n\t            key = extras[i].replace(\"--\", \"\")\n\t            val = extras[i + 1]\n\t            assert hasattr(\n\t                args, key\n", "            ), f\"[ utils/setup ] {key} not found in config: {args.config}\"\n\t            old_val = getattr(args, key)\n\t            old_type = type(old_val)\n\t            print(f\"[ utils/setup ] Overriding config | {key} : {old_val} --> {val}\")\n\t            if val == \"None\":\n\t                val = None\n\t            elif val == \"latest\":\n\t                val = \"latest\"\n\t            elif old_type in [bool, type(None)]:\n\t                try:\n", "                    val = eval(val)\n\t                except:\n\t                    print(\n\t                        f\"[ utils/setup ] Warning: could not parse {val} (old: {old_val}, {old_type}), using str\"\n\t                    )\n\t            else:\n\t                val = old_type(val)\n\t            setattr(args, key, val)\n\t            self._dict[key] = val\n\t    def eval_fstrings(self, args):\n", "        for key, old in self._dict.items():\n\t            if type(old) is str and old[:2] == \"f:\":\n\t                val = old.replace(\"{\", \"{args.\").replace(\"f:\", \"\")\n\t                new = lazy_fstring(val, args)\n\t                print(f\"[ utils/setup ] Lazy fstring | {key} : {old} --> {new}\")\n\t                setattr(self, key, new)\n\t                self._dict[key] = new\n\t    def set_seed(self, args):\n\t        if not hasattr(args, \"seed\") or args.seed is None:\n\t            return\n", "        print(f\"[ utils/setup ] Setting seed: {args.seed}\")\n\t        set_seed(args.seed)\n\t    def set_loadbase(self, args):\n\t        if hasattr(args, \"loadbase\") and args.loadbase is None:\n\t            print(f\"[ utils/setup ] Setting loadbase: {args.logbase}\")\n\t            args.loadbase = args.logbase\n\t    def generate_exp_name(self, args):\n\t        if not \"exp_name\" in dir(args):\n\t            return\n\t        exp_name = getattr(args, \"exp_name\")\n", "        if callable(exp_name):\n\t            exp_name_string = exp_name(args)\n\t            print(f\"[ utils/setup ] Setting exp_name to: {exp_name_string}\")\n\t            setattr(args, \"exp_name\", exp_name_string)\n\t            self._dict[\"exp_name\"] = exp_name_string\n\t    def mkdir(self, args):\n\t        from lcd import REPO_PATH\n\t        if \"logbase\" in dir(args) and \"exp_name\" in dir(args):\n\t            datestr = datetime.datetime.now().strftime(\"%m-%d_%H:%M:%S\")\n\t            args.savepath = os.path.join(\n", "                str(REPO_PATH), args.logbase, args.exp_name, datestr\n\t            )\n\t            self._dict[\"savepath\"] = args.savepath\n\t            if \"suffix\" in dir(args):\n\t                args.savepath = os.path.join(args.savepath, args.suffix)\n\t            if mkdir(args.savepath):\n\t                print(f\"[ utils/setup ] Made savepath: {args.savepath}\")\n\t            self.savepath = args.savepath\n\t            self.save()\n\t    def get_commit(self, args):\n", "        args.commit = get_git_rev()\n\t    def save_diff(self, args):\n\t        try:\n\t            save_git_diff(os.path.join(args.savepath, \"diff.txt\"))\n\t        except:\n\t            print(\"[ utils/setup ] WARNING: did not save git diff\")\n"]}
{"filename": "src/lcd/utils/serialization.py", "chunked_list": ["import glob\n\timport os\n\timport pdb\n\timport pickle\n\tfrom collections import namedtuple\n\timport torch\n\tDiffusion = namedtuple(\n\t    \"Diffusion\", \"dataset renderer model diffusion ema trainer epoch\"\n\t)\n\tdef mkdir(savepath):\n", "    \"\"\"\n\t    returns `True` iff `savepath` is created\n\t    \"\"\"\n\t    if not os.path.exists(savepath):\n\t        os.makedirs(savepath)\n\t        return True\n\t    else:\n\t        return False\n\tdef get_latest_epoch(loadpath):\n\t    states = glob.glob1(os.path.join(*loadpath), \"state_*\")\n", "    latest_epoch = -1\n\t    for state in states:\n\t        epoch = int(state.replace(\"state_\", \"\").replace(\".pt\", \"\"))\n\t        latest_epoch = max(epoch, latest_epoch)\n\t    return latest_epoch\n\tdef load_config(*loadpath):\n\t    loadpath: str = os.path.join(*loadpath)\n\t    config = pickle.load(open(loadpath, \"rb\"))\n\t    print(f\"[ utils/serialization ] Loaded config from {loadpath}\")\n\t    print(config)\n", "    return config\n\tdef load_diffusion(*loadpath, epoch=\"latest\", device=\"cuda:0\", seed=None):\n\t    dataset_config = load_config(*loadpath, \"dataset_config.pkl\")\n\t    render_config = load_config(*loadpath, \"render_config.pkl\")\n\t    model_config = load_config(*loadpath, \"model_config.pkl\")\n\t    diffusion_config = load_config(*loadpath, \"diffusion_config.pkl\")\n\t    trainer_config = load_config(*loadpath, \"trainer_config.pkl\")\n\t    trainer_config._dict[\"results_folder\"] = os.path.join(*loadpath)\n\t    dataset = dataset_config(seed=seed)\n\t    renderer = render_config()\n", "    model = model_config()\n\t    diffusion = diffusion_config(model)\n\t    trainer = trainer_config(diffusion, dataset, renderer)\n\t    if epoch == \"latest\":\n\t        epoch = get_latest_epoch(loadpath)\n\t    print(f\"\\n[ utils/serialization ] Loading model epoch: {epoch}\\n\")\n\t    trainer.load(epoch)\n\t    return Diffusion(\n\t        dataset, renderer, model, diffusion, trainer.ema_model, trainer, epoch\n\t    )\n", "def check_compatibility(experiment_1, experiment_2):\n\t    \"\"\"\n\t    returns True if `experiment_1 and `experiment_2` have\n\t    the same normalizers and number of diffusion steps\n\t    \"\"\"\n\t    normalizers_1 = experiment_1.dataset.normalizer.get_field_normalizers()\n\t    normalizers_2 = experiment_2.dataset.normalizer.get_field_normalizers()\n\t    for key in normalizers_1:\n\t        norm_1 = type(normalizers_1[key])\n\t        norm_2 = type(normalizers_2[key])\n", "        assert (\n\t            norm_1 == norm_2\n\t        ), f\"Normalizers should be identical, found {norm_1} and {norm_2} for field {key}\"\n\t    n_steps_1 = experiment_1.diffusion.n_timesteps\n\t    n_steps_2 = experiment_2.diffusion.n_timesteps\n\t    assert n_steps_1 == n_steps_2, (\n\t        \"Number of timesteps should match between diffusion experiments, \"\n\t        f\"found {n_steps_1} and {n_steps_2}\"\n\t    )\n"]}
{"filename": "src/lcd/utils/config.py", "chunked_list": ["import collections\n\timport importlib\n\timport os\n\timport pickle\n\tdef import_class(_class):\n\t    if type(_class) is not str:\n\t        return _class\n\t    ## 'diffusion' on standard installs\n\t    repo_name = __name__.split(\".\")[0]\n\t    ## eg, 'utils'\n", "    module_name = \".\".join(_class.split(\".\")[:-1])\n\t    ## eg, 'Renderer'\n\t    class_name = _class.split(\".\")[-1]\n\t    ## eg, 'diffusion.utils'\n\t    module = importlib.import_module(f\"{repo_name}.{module_name}\")\n\t    ## eg, diffusion.utils.Renderer\n\t    _class = getattr(module, class_name)\n\t    print(f\"[ utils/config ] Imported {repo_name}.{module_name}:{class_name}\")\n\t    return _class\n\tclass AttriDict(dict):\n", "    \"\"\"\n\t    A dict which is accessible via attribute dot notation\n\t    https://stackoverflow.com/a/41514848\n\t    https://stackoverflow.com/a/14620633\n\t    \"\"\"\n\t    DICT_RESERVED_KEYS = list(vars(dict).keys())\n\t    def __init__(self, *args, **kwargs):\n\t        \"\"\"\n\t        :param args: multiple dicts ({}, {}, ..)\n\t        :param kwargs: arbitrary keys='value'\n", "        \"\"\"\n\t        super().__init__(*args, **kwargs)\n\t        self.__dict__ = self\n\t    def __getattr__(self, attr):\n\t        if attr not in AttriDict.DICT_RESERVED_KEYS:\n\t            return self.get(attr)\n\t        return getattr(self, attr)\n\t    def __setattr__(self, key, value):\n\t        if key == \"__dict__\":\n\t            super().__setattr__(key, value)\n", "            return\n\t        if key in AttriDict.DICT_RESERVED_KEYS:\n\t            raise AttributeError(\"You cannot set a reserved name as attribute\")\n\t        self.__setitem__(key, value)\n\t    def __copy__(self):\n\t        return self.__class__(self)\n\t    def copy(self):\n\t        return self.__copy__()\n\tclass Config(collections.abc.Mapping):  # type: ignore\n\t    def __init__(self, _class, verbose=True, savepath=None, device=None, **kwargs):\n", "        self._class = import_class(_class)\n\t        self._device = device\n\t        self._dict = {}\n\t        for key, val in kwargs.items():\n\t            self._dict[key] = val\n\t        if verbose:\n\t            print(self)\n\t        if savepath is not None:\n\t            savepath = os.path.join(*savepath) if type(savepath) is tuple else savepath\n\t            pickle.dump(self, open(savepath, \"wb\"))\n", "            print(f\"[ utils/config ] Saved config to: {savepath}\\n\")\n\t    def __repr__(self):\n\t        string = f\"\\n[utils/config ] Config: {self._class}\\n\"\n\t        for key in sorted(self._dict.keys()):\n\t            val = self._dict[key]\n\t            string += f\"    {key}: {val}\\n\"\n\t        return string\n\t    def __iter__(self):\n\t        return iter(self._dict)\n\t    def __getitem__(self, item):\n", "        return self._dict[item]\n\t    def __len__(self):\n\t        return len(self._dict)\n\t    def __getattr__(self, attr):\n\t        if attr == \"_dict\" and \"_dict\" not in vars(self):\n\t            self._dict = {}\n\t            return self._dict\n\t        try:\n\t            return self._dict[attr]\n\t        except KeyError:\n", "            raise AttributeError(attr)\n\t    def __call__(self, *args, **kwargs):\n\t        instance = self._class(*args, **kwargs, **self._dict)\n\t        if self._device:\n\t            instance = instance.to(self._device)\n\t        return instance\n"]}
{"filename": "src/lcd/utils/__init__.py", "chunked_list": ["from .arrays import *\n\tfrom .config import *\n\tfrom .serialization import *\n\tfrom .setup import *\n\tfrom .training import *\n"]}
{"filename": "src/lcd/utils/eval.py", "chunked_list": ["# originally taken from https://github.com/lukashermann/hulc/blob/fb14d5461ae54f919d52c0c30131b38f806ef8db/hulc/evaluation/evaluate_policy.py and adapted for hierarchical imitation learning and data collection\n\timport gc\n\timport json\n\timport os\n\timport sys\n\timport time\n\tfrom collections import Counter\n\tfrom datetime import datetime\n\tfrom pathlib import Path\n\timport hydra\n", "import numpy as np\n\timport torch\n\tfrom hulc.evaluation.utils import get_env_state_for_initial_condition, join_vis_lang\n\tfrom loguru import logger\n\tfrom omegaconf import OmegaConf\n\tfrom tqdm import tqdm\n\timport lcd\n\tfrom lcd import DATA_PATH, HULC_PATH, REPO_PATH\n\tfrom lcd.utils.serialization import load_config\n\tclass DiffusionModelWrapper(torch.nn.Module):\n", "    def __init__(\n\t        self, device=\"cpu\", model_path__epoch=(None, None), model__args=(None, None)\n\t    ) -> None:\n\t        \"\"\"\n\t        Wrapper for the LCD diffusion model to use during evaluation.\n\t        Can either take in model directly through model__args or load from the filesystem with model_path__epoch\n\t        \"\"\"\n\t        super().__init__()\n\t        model_path, epoch, model, args = (*model_path__epoch, *model__args)\n\t        assert model_path or model, \"Need to specify either model_path or model\"\n", "        if model_path:\n\t            sys.modules[\"diffuser\"] = lcd\n\t            model_config = load_config(model_path, \"model_config.pkl\")\n\t            diffusion_config = load_config(model_path, \"diffusion_config.pkl\")\n\t            init_hulc_goal_state_dicts = torch.load(\n\t                os.path.join(model_path, f\"state_{epoch}.pt\"), map_location=\"cpu\"\n\t            )\n\t            model = model_config()\n\t            diffusion = diffusion_config(model)\n\t            diffusion.load_state_dict(init_hulc_goal_state_dicts[\"ema\"])\n", "            self.model = diffusion\n\t            self.args = json.load(open(f\"{model_path}/args.json\"))\n\t        else:\n\t            self.model = model\n\t            self.args = args\n\t        self.model.to(device)\n\t    def forward(self, cond, inpaint):\n\t        samples = self.model.conditional_sample(\n\t            cond, horizon=1, inpaint={0: inpaint}\n\t        ).trajectories\n", "        return samples[:, :, :32].squeeze(dim=1)\n\tdef get_sequences(args, regenerate=False):\n\t    if regenerate:\n\t        from hulc.evaluation.multistep_sequences import get_sequences\n\t        # this takes a few minutes\n\t        return get_sequences(args.num_sequences)\n\t    else:\n\t        return torch.load(DATA_PATH / \"default_1000_sequences.pt\")[: args.num_sequences]\n\tdef count_success(results):\n\t    count = Counter(results)\n", "    step_success = []\n\t    for i in range(1, 6):\n\t        n_success = sum(count[j] for j in reversed(range(i, 6)))\n\t        sr = n_success / len(results)\n\t        step_success.append(sr)\n\t    return step_success\n\tdef get_log_dir(log_dir):\n\t    log_dir = Path(log_dir) if log_dir is not None else REPO_PATH / \"results\"\n\t    if not log_dir.exists():\n\t        log_dir.mkdir()\n", "    print(f\"logging to {log_dir}\")\n\t    return log_dir\n\tdef print_and_save(results, args, histories, model_id):\n\t    def get_task_success_rate(results, sequences):\n\t        cnt_success = Counter()\n\t        cnt_fail = Counter()\n\t        for result, (_, sequence) in zip(results, sequences):\n\t            for successful_tasks in sequence[:result]:\n\t                cnt_success[successful_tasks] += 1\n\t            if result < len(sequence):\n", "                failed_task = sequence[result]\n\t                cnt_fail[failed_task] += 1\n\t        total = cnt_success + cnt_fail\n\t        task_info = {}\n\t        for task in total:\n\t            task_info[task] = {\"success\": cnt_success[task], \"total\": total[task]}\n\t            print(\n\t                f\"{task}: {cnt_success[task]} / {total[task]} |  SR: {cnt_success[task] / total[task] * 100:.1f}%\"\n\t            )\n\t        return task_info\n", "    log_dir = get_log_dir(args.log_dir)\n\t    sequences = get_sequences(args)\n\t    if args.generate:\n\t        Path(log_dir / \"rollouts\").mkdir(\n\t            exist_ok=True, parents=True\n\t        )  # create rollouts folder if it doesn't exist\n\t        torch.save(\n\t            histories,\n\t            log_dir\n\t            / f\"rollouts/{model_id}_gen_history_{datetime.now().strftime('%d-%m-%Y-%H:%M:%S')}.pt\",\n", "        )\n\t    current_data = {}\n\t    # epoch = checkpoint.stem\n\t    print(f\"Results for Model {model_id}:\")\n\t    avg_seq_len = np.mean(results)\n\t    chain_sr = {i + 1: sr for i, sr in enumerate(count_success(results))}\n\t    print(f\"Average successful sequence length: {avg_seq_len}\")\n\t    print(\"Success rates for i instructions in a row:\")\n\t    for i, sr in chain_sr.items():\n\t        print(f\"{i}: {sr * 100:.1f}%\")\n", "    data = {\n\t        \"avg_seq_len\": avg_seq_len,\n\t        \"chain_sr\": chain_sr,\n\t        \"task_info\": get_task_success_rate(results, sequences),\n\t    }\n\t    current_data[model_id] = data\n\t    previous_data = {}\n\t    try:\n\t        with open(log_dir / \"results.json\") as file:\n\t            previous_data = json.load(file)\n", "    except FileNotFoundError:\n\t        pass\n\t    json_data = {**previous_data, **current_data}\n\t    with open(log_dir / \"results.json\", \"w\") as file:\n\t        json.dump(json_data, file)\n\t    return data\n\tdef evaluate_policy(state, args):\n\t    conf_dir = HULC_PATH / \"conf\"\n\t    task_cfg = OmegaConf.load(\n\t        conf_dir / \"callbacks/rollout/tasks/new_playtable_tasks.yaml\"\n", "    )\n\t    task_oracle = hydra.utils.instantiate(task_cfg)\n\t    val_annotations = OmegaConf.load(\n\t        conf_dir / \"annotations/new_playtable_validation.yaml\"\n\t    )\n\t    eval_sequences = get_sequences(args)\n\t    if not args.debug:\n\t        eval_sequences = tqdm(eval_sequences, position=0, leave=True)\n\t    results = []\n\t    histories = []\n", "    for initial_state, eval_sequence in eval_sequences:\n\t        robot_obs, scene_obs = get_env_state_for_initial_condition(initial_state)\n\t        state.env.reset(robot_obs=robot_obs, scene_obs=scene_obs)\n\t        success_counter = 0\n\t        history = []\n\t        if args.debug:\n\t            time.sleep(1)\n\t            print()\n\t            print()\n\t            print(f\"Evaluating sequence: {' -> '.join(eval_sequence)}\")\n", "            print(\"Subtask: \", end=\"\")\n\t        for subtask in eval_sequence:\n\t            success, info = rollout(\n\t                state.env,\n\t                state.model,\n\t                task_oracle,\n\t                args,\n\t                subtask,\n\t                state.lang_embeddings,\n\t                val_annotations,\n", "            )\n\t            if success:\n\t                success_counter += 1\n\t                history.append(info)\n\t            else:\n\t                break\n\t        histories.append(history)\n\t        results.append(success_counter)\n\t        if not args.debug:\n\t            eval_sequences.set_description(\n", "                \" \".join(\n\t                    [\n\t                        f\"{i + 1}/5 : {v * 100:.1f}% |\"\n\t                        for i, v in enumerate(count_success(results))\n\t                    ]\n\t                )\n\t                + \"|\"\n\t            )\n\t    return results, histories\n\tdef rollout(env, model, task_oracle, args, subtask, lang_embeddings, val_annotations):\n", "    if args.debug:\n\t        print(f\"{subtask} \", end=\"\")\n\t        time.sleep(0.5)\n\t    if args.generate:\n\t        states = []\n\t        actions = []\n\t        scene_info = []\n\t    obs = env.get_obs()\n\t    # get lang annotation for subtask\n\t    lang_annotation = val_annotations[subtask][0]\n", "    if args.dm is not None:\n\t        model.replan_freq = args.subgoal_interval\n\t        current_state = model.get_pp_plan_vision(obs, obs)[-1]\n\t        lang_goal = lang_embeddings[lang_annotation].to(args.device)\n\t        goal = {\"lang\": args.dm(lang_goal[None], current_state)}\n\t    else:\n\t        goal = lang_embeddings.get_lang_goal(lang_annotation)\n\t        plan, latent_goal = model.get_pp_plan_lang(obs, goal)\n\t    model.reset()\n\t    start_info = current_info = env.get_info()\n", "    success_step = 0\n\t    success = False\n\t    for step in range(args.ep_len):\n\t        if (\n\t            success_step and step == success_step + 4\n\t        ):  # record four states past the successful state\n\t            break\n\t        if args.dm is not None:\n\t            if not (step % args.subgoal_interval):\n\t                current_state = (\n", "                    model.visual_goal(\n\t                        model.perceptual_encoder(obs[\"rgb_obs\"], {}, None)\n\t                    )\n\t                    .squeeze()\n\t                    .cpu()\n\t                )\n\t                goal = {\"lang\": args.dm(lang_goal[None], current_state)}\n\t        action = model.step(obs, goal, direct=args.dm is not None)\n\t        if args.generate:\n\t            states.append(\n", "                model.visual_goal(model.perceptual_encoder(obs[\"rgb_obs\"], {}, None))\n\t                .squeeze()\n\t                .cpu()\n\t            )\n\t            scene_info.append(current_info)\n\t            actions.append(action.squeeze().cpu())\n\t        obs, _, _, current_info = env.step(action)\n\t        if args.debug:\n\t            img = env.render(mode=\"rgb_array\")\n\t            join_vis_lang(img, lang_annotation)\n", "            # time.sleep(0.1)\n\t        # check if current step solves a task\n\t        current_task_info = task_oracle.get_task_info_for_set(\n\t            start_info, current_info, {subtask}\n\t        )\n\t        if len(current_task_info) > 0 and not success:\n\t            if args.debug:\n\t                logger.info(\"<green>Success</green>\")\n\t            success = True\n\t            success_step = step\n", "    if args.debug and not success:\n\t        logger.info(\"<red>Fail</red>\")\n\t    if not args.generate:\n\t        return success, {}\n\t    else:\n\t        gc.collect()\n\t        torch.cuda.empty_cache()\n\t        return success, {\n\t            \"states\": torch.stack(states).cpu().detach(),\n\t            \"actions\": torch.stack(actions).cpu().detach(),\n", "            \"goal_lang\": goal[\"lang\"].cpu().detach().squeeze(),\n\t            \"goal_task\": subtask,\n\t            \"scene_info\": scene_info,\n\t        }\n"]}
{"filename": "src/lcd/utils/timer.py", "chunked_list": ["import time\n\tclass Timer:\n\t    def __init__(self):\n\t        self._start = time.time()\n\t    def __call__(self, reset=True):\n\t        now = time.time()\n\t        diff = now - self._start\n\t        if reset:\n\t            self._start = now\n\t        return diff\n"]}
{"filename": "src/lcd/utils/training.py", "chunked_list": ["import copy\n\timport os\n\timport pdb\n\timport einops\n\timport numpy as np\n\timport torch\n\timport tqdm\n\timport wandb\n\tfrom lcd.datasets.sequence import Batch\n\tfrom .arrays import batch_to_device\n", "from .timer import Timer\n\tdef cycle(dl):\n\t    while True:\n\t        yield from dl\n\tdef wlog(*args, **kwargs):\n\t    if wandb.run is not None:\n\t        wandb.log(*args, **kwargs)\n\tclass EMA:\n\t    \"\"\"\n\t    empirical moving average\n", "    \"\"\"\n\t    def __init__(self, beta):\n\t        super().__init__()\n\t        self.beta = beta\n\t    def update_model_average(self, ma_model, current_model):\n\t        for current_params, ma_params in zip(\n\t            current_model.parameters(), ma_model.parameters()\n\t        ):\n\t            old_weight, up_weight = ma_params.data, current_params.data\n\t            ma_params.data = self.update_average(old_weight, up_weight)\n", "    def update_average(self, old, new):\n\t        if old is None:\n\t            return new\n\t        return old * self.beta + (1 - self.beta) * new\n\tclass Trainer:\n\t    def __init__(\n\t        self,\n\t        diffusion_model,\n\t        dataset,\n\t        ema_decay=0.995,\n", "        train_batch_size=32,\n\t        train_lr=2e-5,\n\t        gradient_accumulate_every=2,\n\t        step_start_ema=2000,\n\t        update_ema_every=10,\n\t        log_freq=100,\n\t        sample_freq=1000,\n\t        save_freq=1000,\n\t        label_freq=100000,\n\t        save_parallel=False,\n", "        results_folder=\"./results\",\n\t        n_reference=8,\n\t        bucket=None,\n\t    ):\n\t        super().__init__()\n\t        self.model = diffusion_model\n\t        self.ema = EMA(ema_decay)\n\t        self.ema_model = copy.deepcopy(self.model)\n\t        self.update_ema_every = update_ema_every\n\t        self.step_start_ema = step_start_ema\n", "        self.log_freq = log_freq\n\t        self.sample_freq = sample_freq\n\t        self.save_freq = save_freq\n\t        self.label_freq = label_freq\n\t        self.save_parallel = save_parallel\n\t        self.batch_size = train_batch_size\n\t        self.gradient_accumulate_every = gradient_accumulate_every\n\t        self.dataset = dataset\n\t        self.dataloader = cycle(\n\t            torch.utils.data.DataLoader(\n", "                self.dataset,\n\t                batch_size=train_batch_size,\n\t                num_workers=1,\n\t                shuffle=True,\n\t                pin_memory=True,\n\t            )\n\t        )\n\t        self.dataloader_vis = cycle(\n\t            torch.utils.data.DataLoader(\n\t                self.dataset, batch_size=1, num_workers=0, shuffle=True, pin_memory=True\n", "            )\n\t        )\n\t        self.optimizer = torch.optim.Adam(diffusion_model.parameters(), lr=train_lr)\n\t        self.logdir = results_folder\n\t        self.bucket = bucket\n\t        self.n_reference = n_reference\n\t        self.reset_parameters()\n\t        self.step = 0\n\t    def reset_parameters(self):\n\t        self.ema_model.load_state_dict(self.model.state_dict())\n", "    def step_ema(self):\n\t        if self.step < self.step_start_ema:\n\t            self.reset_parameters()\n\t            return\n\t        self.ema.update_model_average(self.ema_model, self.model)\n\t    # -----------------------------------------------------------------------------#\n\t    # ------------------------------------ api ------------------------------------#\n\t    # -----------------------------------------------------------------------------#\n\t    def train(self, n_train_steps):\n\t        timer = Timer()\n", "        for step in tqdm.tqdm(range(n_train_steps)):\n\t            for i in range(self.gradient_accumulate_every):\n\t                batch = next(self.dataloader)\n\t                batch = batch_to_device(batch)\n\t                if isinstance(batch, Batch):\n\t                    loss, infos = self.model.loss(*batch)\n\t                else:\n\t                    loss, infos = self.model.loss(*batch[0], **batch[1])\n\t                loss = loss / self.gradient_accumulate_every\n\t                loss.backward()\n", "            wlog({\"loss\": loss, **infos})\n\t            self.optimizer.step()\n\t            self.optimizer.zero_grad()\n\t            if self.step % self.update_ema_every == 0:\n\t                self.step_ema()\n\t            if self.step % self.save_freq == 0:\n\t                self.save(self.step)\n\t            if self.step % self.log_freq == 0:\n\t                infos_str = \" | \".join(\n\t                    [f\"{key}: {val:8.4f}\" for key, val in infos.items()]\n", "                )\n\t                print(\n\t                    f\"{self.step}: {loss:8.4f} | {infos_str} | t: {timer():8.4f}\",\n\t                    flush=True,\n\t                )\n\t            if self.sample_freq and self.step % self.sample_freq == 0:\n\t                samples = self.ema_model.conditional_sample(\n\t                    batch[0][1], horizon=1, **batch[1]\n\t                ).trajectories\n\t                l1 = (samples[:, :, :32] - batch[0][0][:, :, :32]).abs()\n", "                wlog({\"generation_mae_mean\": l1.mean(), \"generation_mae_std\": l1.std()})\n\t            self.step += 1\n\t    def save(self, epoch):\n\t        \"\"\"\n\t        saves model and ema to disk;\n\t        syncs to storage bucket if a bucket is specified\n\t        \"\"\"\n\t        data = {\n\t            \"step\": self.step,\n\t            \"model\": self.model.state_dict(),\n", "            \"ema\": self.ema_model.state_dict(),\n\t        }\n\t        savepath = os.path.join(self.logdir, f\"state_{epoch}.pt\")\n\t        torch.save(data, savepath)\n\t        torch.save(self.ema_model, os.path.join(self.logdir, f\"model_{epoch}.pt\"))\n\t        print(f\"[ utils/training ] Saved model to {savepath}\", flush=True)\n\t    def load(self, epoch):\n\t        \"\"\"\n\t        loads model and ema from disk\n\t        \"\"\"\n", "        loadpath = os.path.join(self.logdir, f\"state_{epoch}.pt\")\n\t        data = torch.load(loadpath)\n\t        self.step = data[\"step\"]\n\t        self.model.load_state_dict(data[\"model\"])\n\t        self.ema_model.load_state_dict(data[\"ema\"])\n"]}
{"filename": "src/lcd/utils/arrays.py", "chunked_list": ["import collections\n\timport pdb\n\timport numpy as np\n\timport torch\n\tDTYPE = torch.float\n\tDEVICE = \"cuda:0\"\n\t# -----------------------------------------------------------------------------#\n\t# ------------------------------ numpy <--> torch -----------------------------#\n\t# -----------------------------------------------------------------------------#\n\tdef to_np(x):\n", "    if torch.is_tensor(x):\n\t        x = x.detach().cpu().numpy()\n\t    return x\n\tdef to_torch(x, dtype=None, device=None):\n\t    dtype = dtype or DTYPE\n\t    device = device or DEVICE\n\t    if type(x) is dict:\n\t        return {k: to_torch(v, dtype, device) for k, v in x.items()}\n\t    elif torch.is_tensor(x):\n\t        return x.to(device).type(dtype)\n", "    return torch.tensor(x, dtype=dtype, device=device)\n\tdef to_device(x, device=DEVICE):\n\t    if torch.is_tensor(x):\n\t        return x.to(device)\n\t    elif type(x) is dict:\n\t        return {k: to_device(v, device) for k, v in x.items()}\n\t    elif isinstance(x, tuple):\n\t        return tuple(to_device(v, device) for v in x)\n\t    elif isinstance(x, list):\n\t        return [to_device(v, device) for v in x]\n", "    else:\n\t        raise RuntimeError(f\"Unrecognized type in `to_device`: {type(x)}\")\n\tdef apply_dict(fn, d, *args, **kwargs):\n\t    return {k: fn(v, *args, **kwargs) for k, v in d.items()}\n\tdef set_device(device):\n\t    if \"cuda\" in device:\n\t        torch.set_default_tensor_type(torch.cuda.FloatTensor)  # type: ignore\n\tdef batch_to_device(batch, device=\"cuda:0\"):\n\t    vals = [to_device(getattr(batch, field), device) for field in batch._fields]\n\t    return type(batch)(*vals)\n", "def _to_str(num):\n\t    if num >= 1e6:\n\t        return f\"{(num/1e6):.2f} M\"\n\t    else:\n\t        return f\"{(num/1e3):.2f} k\"\n\t# -----------------------------------------------------------------------------#\n\t# ----------------------------- parameter counting ----------------------------#\n\t# -----------------------------------------------------------------------------#\n\tdef param_to_module(param):\n\t    module_name = param[::-1].split(\".\", maxsplit=1)[-1][::-1]\n", "    return module_name\n\tdef report_parameters(model, topk=10):\n\t    counts = {k: p.numel() for k, p in model.named_parameters()}\n\t    n_parameters = sum(counts.values())\n\t    print(f\"[ utils/arrays ] Total parameters: {_to_str(n_parameters)}\")\n\t    modules = dict(model.named_modules())\n\t    sorted_keys = sorted(counts, key=lambda x: -counts[x])  # type: ignore\n\t    max_length = max([len(k) for k in sorted_keys])\n\t    for i in range(topk):\n\t        key = sorted_keys[i]\n", "        count = counts[key]\n\t        module = param_to_module(key)\n\t        print(\" \" * 8, f\"{key:10}: {_to_str(count)} | {modules[module]}\")\n\t    remaining_parameters = sum([counts[k] for k in sorted_keys[topk:]])\n\t    print(\n\t        \" \" * 8,\n\t        f\"... and {len(counts)-topk} others accounting for {_to_str(remaining_parameters)} parameters\",\n\t    )\n\t    return n_parameters\n"]}
{"filename": "src/lcd/scripts/diffuser.py", "chunked_list": ["import os\n\timport torch\n\timport lcd.utils as utils\n\timport wandb\n\tfrom lcd import DATA_PATH\n\tfrom lcd.apps import rollout\n\tfrom lcd.datasets.sequence import Batch\n\tfrom lcd.utils.arrays import batch_to_device\n\tfrom lcd.utils.training import cycle\n\tscript_dir = os.path.dirname(os.path.realpath(__file__))\n", "# -----------------------------------------------------------------------------#\n\t# ----------------------------------- setup -----------------------------------#\n\t# -----------------------------------------------------------------------------#\n\tclass Parser(utils.Parser):\n\t    config: str = \"lcd.config.calvin\"\n\targs = Parser().parse_args(\"diffusion\")\n\targs.dim_mults = tuple(int(i) for i in args.dim_mults)\n\t# -----------------------------------------------------------------------------#\n\t# ---------------------------------- dataset ----------------------------------#\n\t# -----------------------------------------------------------------------------#\n", "dataset_config = utils.Config(\n\t    args.loader,\n\t    savepath=(args.savepath, \"dataset_config.pkl\"),\n\t    horizon=args.horizon,\n\t    normalizer=args.normalizer,\n\t    preprocess_fns=args.preprocess_fns,\n\t    use_padding=args.use_padding,\n\t    max_path_length=args.max_path_length,\n\t    frame_offset=args.frame_offset,\n\t    lang_embeds=DATA_PATH / \"t5-v1_1-xxl_embeddings.pt\",\n", "    task_to_ann=DATA_PATH / \"annotations.json\",\n\t    buf=DATA_PATH / f\"hulc-trajectories/{args.seed}_all_trajectories.pt\",\n\t    observation_dim=args.observation_dim,\n\t    action_dim=args.action_dim,\n\t)\n\tdataset = dataset_config()\n\tobservation_dim = dataset.observation_dim\n\taction_dim = dataset.action_dim\n\t# -----------------------------------------------------------------------------#\n\t# ------------------------------ model & trainer ------------------------------#\n", "# -----------------------------------------------------------------------------#\n\tmodel_config = utils.Config(\n\t    args.model,\n\t    savepath=(args.savepath, \"model_config.pkl\"),\n\t    horizon=args.horizon,\n\t    transition_dim=observation_dim + action_dim,\n\t    cond_dim=observation_dim,\n\t    dim_mults=args.dim_mults,\n\t    dim=args.model_dim,\n\t    attention=args.attention,\n", "    device=args.device,\n\t    downsample=args.downsample,\n\t)\n\tdiffusion_config = utils.Config(\n\t    args.diffusion,\n\t    savepath=(args.savepath, \"diffusion_config.pkl\"),\n\t    horizon=args.horizon,\n\t    observation_dim=observation_dim,\n\t    action_dim=action_dim,\n\t    n_timesteps=args.n_diffusion_steps,\n", "    loss_type=args.loss_type,\n\t    clip_denoised=args.clip_denoised,\n\t    predict_epsilon=args.predict_epsilon,\n\t    ## loss weighting\n\t    action_weight=args.action_weight,\n\t    loss_weights=args.loss_weights,\n\t    loss_discount=args.loss_discount,\n\t    device=args.device,\n\t)\n\ttrainer_config = utils.Config(\n", "    utils.Trainer,\n\t    savepath=(args.savepath, \"trainer_config.pkl\"),\n\t    train_batch_size=args.batch_size,\n\t    train_lr=args.learning_rate,\n\t    gradient_accumulate_every=args.gradient_accumulate_every,\n\t    ema_decay=args.ema_decay,\n\t    sample_freq=args.sample_freq,\n\t    save_freq=args.save_freq,\n\t    label_freq=int(args.n_train_steps // args.n_saves),\n\t    save_parallel=args.save_parallel,\n", "    results_folder=args.savepath,\n\t    bucket=args.bucket,\n\t    n_reference=args.n_reference,\n\t)\n\t# -----------------------------------------------------------------------------#\n\t# -------------------------------- instantiate --------------------------------#\n\t# -----------------------------------------------------------------------------#\n\tmodel = model_config()\n\tdiffusion = diffusion_config(model)\n\ttrainer = trainer_config(diffusion, dataset)\n", "# -----------------------------------------------------------------------------#\n\t# ------------------------ test forward & backward pass -----------------------#\n\t# -----------------------------------------------------------------------------#\n\tutils.report_parameters(model)\n\tprint(\"Testing forward...\", end=\" \", flush=True)\n\tdataloader = cycle(torch.utils.data.DataLoader(dataset, batch_size=1, num_workers=0))\n\tbatch = batch_to_device(next(dataloader))\n\tif isinstance(batch, Batch):\n\t    loss, infos = diffusion.loss(*batch)\n\telse:\n", "    loss, infos = diffusion.loss(*batch[0], **batch[1])\n\tloss.backward()\n\tprint(\"✓\")\n\t# -----------------------------------------------------------------------------#\n\t# --------------------------------- main loop ---------------------------------#\n\t# -----------------------------------------------------------------------------#\n\tn_epochs = int(args.n_train_steps // args.n_steps_per_epoch)\n\tif args.wandb:\n\t    wandb.init(\n\t        project=\"vanilla-diffuser\",\n", "        entity=\"lang-diffusion\",\n\t        name=f\"hulc-{args.wandb_name}\",\n\t        config=vars(args),\n\t    )\n\tdef eval_model(num_evals, epoch=0):\n\t    rollout.main(seed=args.seed, num_sequences=num_evals)\n\t    dm_args = args.as_dict()\n\t    dm_args[\"epoch\"] = epoch\n\t    rollout.lcd(dm__args=(diffusion, dm_args))\n\tprint(\"Testing evaluation...\", end=\" \", flush=True)\n", "eval_model(\n\t    num_evals=2,\n\t)\n\tprint(\"✓\")\n\tfor i in range(n_epochs):\n\t    print(f\"Epoch {i} / {n_epochs} | {args.savepath}\")\n\t    trainer.train(n_train_steps=args.n_steps_per_epoch)\n\t    if not (i % args.eval_freq):\n\t        eval_model(\n\t            num_evals=args.n_evals_per_epoch,\n", "            epoch=args.n_steps_per_epoch * (i + 1),\n\t        )\n\teval_model(\n\t    num_evals=1000,\n\t    epoch=args.n_steps_per_epoch * (i + 1),\n\t)\n"]}
{"filename": "src/lcd/scripts/generation/embeddings.py", "chunked_list": ["from typing import List\n\timport json\n\timport torch\n\tfrom einops import rearrange\n\tfrom transformers import T5EncoderModel, T5Tokenizer\n\tDEFAULT_T5_NAME = \"google/t5-v1_1-base\"\n\tT5_CONFIGS = {}\n\tMAX_LENGTH = 256\n\t# taken from https://github.com/lucidrains/imagen-pytorch/blob/35f24ea102ab1d71da7df3c8a650c4fe712d2a9c/imagen_pytorch/t5.py#L107\n\tdef t5_encode_text(texts: List[str], name=DEFAULT_T5_NAME, return_attn_mask=False):\n", "    token_ids, attn_mask = t5_tokenize(texts, name=name)\n\t    encoded_text = t5_encode_tokenized_text(token_ids, attn_mask=attn_mask, name=name)\n\t    if return_attn_mask:\n\t        attn_mask = attn_mask.bool()\n\t        return encoded_text, attn_mask\n\t    return encoded_text\n\tdef exists(val):\n\t    return val is not None\n\tdef get_tokenizer(name):\n\t    tokenizer = T5Tokenizer.from_pretrained(name, model_max_length=MAX_LENGTH)\n", "    return tokenizer\n\tdef get_model(name):\n\t    model = T5EncoderModel.from_pretrained(name)\n\t    return model\n\tdef default(val, d):\n\t    if exists(val):\n\t        return val\n\t    return d() if callable(d) else d\n\tdef get_model_and_tokenizer(name):\n\t    global T5_CONFIGS\n", "    if name not in T5_CONFIGS:\n\t        T5_CONFIGS[name] = dict()\n\t    if \"model\" not in T5_CONFIGS[name]:\n\t        T5_CONFIGS[name][\"model\"] = get_model(name)\n\t    if \"tokenizer\" not in T5_CONFIGS[name]:\n\t        T5_CONFIGS[name][\"tokenizer\"] = get_tokenizer(name)\n\t    return T5_CONFIGS[name][\"model\"], T5_CONFIGS[name][\"tokenizer\"]\n\tdef t5_tokenize(texts: List[str], name=DEFAULT_T5_NAME):\n\t    t5, tokenizer = get_model_and_tokenizer(name)\n\t    if torch.cuda.is_available():\n", "        t5 = t5.cuda()\n\t    device = next(t5.parameters()).device\n\t    encoded = tokenizer.batch_encode_plus(\n\t        texts,\n\t        return_tensors=\"pt\",\n\t        padding=\"longest\",\n\t        max_length=MAX_LENGTH,\n\t        truncation=True,\n\t    )\n\t    input_ids = encoded.input_ids.to(device)\n", "    attn_mask = encoded.attention_mask.to(device)\n\t    return input_ids, attn_mask\n\tdef t5_encode_tokenized_text(\n\t    token_ids, attn_mask=None, pad_id=None, name=DEFAULT_T5_NAME\n\t):\n\t    assert exists(attn_mask) or exists(pad_id)\n\t    t5, _ = get_model_and_tokenizer(name)\n\t    attn_mask = default(attn_mask, lambda: (token_ids != pad_id).long())\n\t    t5.eval()\n\t    with torch.no_grad():\n", "        output = t5(input_ids=token_ids, attention_mask=attn_mask)\n\t        encoded_text = output.last_hidden_state.detach()\n\t    attn_mask = attn_mask.bool()\n\t    encoded_text = encoded_text.masked_fill(\n\t        ~rearrange(attn_mask, \"... -> ... 1\"), 0.0\n\t    )  # just force all embeddings that is padding to be equal to 0.\n\t    return encoded_text\n\tNAME = \"t5-v1_1-xxl\"\n\tanns = sum(json.load(open(\"/data2/eddie/calvin/annotations.json\")).values(), [])\n\tembeds = {}\n", "embeddings = t5_encode_text(anns, name=f\"google/{NAME}\")\n\tfor a, e in zip(anns, embeddings):\n\t    embeds[a] = e.cpu()\n\ttorch.save(embeds, f\"{NAME}_embeddings.pt\")\n"]}
{"filename": "src/lcd/scripts/generation/dataset.py", "chunked_list": ["# %%\n\timport os\n\tfrom os.path import join as j\n\timport torch\n\tp = \"/home/ubuntu/vanilla-hulc-larel-baseline/evaluation\"\n\tfiles = [j(p, f) for f in os.listdir(p) if f.startswith(\"gen\")]\n\tos.chdir(p)\n\t# %%\n\t\"\"\"\n\t{seq_length: [ traj... ]}\n", "traj = [subtask, subtask...] # total of seq_length subtasks\n\tsubtask: dict_keys(['states', 'actions', 'goal_image', 'goal_lang', 'goal_task'])\n\tsubtask.states = [ { dict_keys(['rgb_obs', 'robot_obs', 'depth_obs', 'robot_obs_raw', 'scene_obs']) } ...] # rgb is normalized already. \n\tepth_obs is empty\n\tsubtask.actions = tensor ([len(subtask), 1, 1, 7])\n\tsubtask.goal_image = { dict_keys(['rgb_obs', 'robot_obs', 'depth_obs', 'robot_obs_raw', 'scene_obs']) }\n\tsubtask.goal_task = 'rotate_pink_block_right'\n\tsubtask.goal_lang = ?\n\t\"\"\"\n\t# s = torch.load(j(p, 'gen_history_HULC_D_D_02-01-2023-09:17:46.pt'), map_location='cpu')\n", "# len(s[1][0][0])\n\t\"\"\"\n\tYou can generate each key state with +- some random epsilon for each index\n\tYou can use this \"embeddings = np.load(Path(val_dataset_path) / lang_folder / \"embeddings.npy\", allow_pickle=True).item()\" for embedding\n\t\"\"\"\n\tfrom collections import defaultdict\n\tfrom pathlib import Path\n\timport numpy as np\n\timport yaml\n\tfrom hulc.models.hulc import Hulc\n", "def gen_annotations_embeddings():\n\t    n = np.load(\n\t        \"/home/ubuntu/vanilla-hulc-icml-baseline/data/task_D_D/training/lang_paraphrase-MiniLM-L3-v2/auto_lang_ann.npy\",\n\t        allow_pickle=True,\n\t    ).item()\n\t    ann, tasks, emb = n[\"language\"].values()\n\t    annotations = defaultdict(list)\n\t    embeddings = defaultdict(list)\n\t    for i in range(len(ann)):\n\t        if ann[i] not in annotations[tasks[i]]:\n", "            annotations[tasks[i]].append(ann[i])\n\t            embeddings[tasks[i]].append(emb[i])\n\t    return annotations, embeddings\n\tannotations, embeddings = gen_annotations_embeddings()\n\t# model = Hulc.load_from_checkpoint(Path(\"/home/eddie/git/lad/vhulc/checkpoints/HULC_D_D/saved_models/HULC_D_D.ckpt\"))\n\t# model.freeze()\n\t# %%\n\timport random\n\timport tqdm\n\tdef sample(task):\n", "    anns = annotations[task]\n\t    idx = random.randint(0, len(anns) - 1)\n\t    return anns[idx], embeddings[task][idx][0]\n\t# def gen_goal(states):\n\t#     emb = model.perceptual_encoder(aggregate(states), {}, None)\n\t#     return model.visual_goal(emb.squeeze(dim=1))\n\tdef aggregate(states):\n\t    return {\n\t        \"rgb_static\": torch.concat([v[\"rgb_obs\"][\"rgb_static\"] for v in states]),\n\t        \"rgb_gripper\": torch.concat([v[\"rgb_obs\"][\"rgb_gripper\"] for v in states]),\n", "    }\n\t# %%\n\t# ret = defaultdict(list)\n\t# for f in tqdm.tqdm(files):\n\t#     f = torch.load(f, map_location=\"cpu\")\n\t#     for seq_length in f:\n\t#         for traj in f[seq_length]:\n\t#             for subtask in traj:\n\t#                 goal_space_states = gen_goal(subtask[\"states\"])\n\t#                 ret[\"states\"].append(goal_space_states)\n", "#                 ret[\"goal_task\"].append(subtask[\"goal_task\"])\n\t#                 ann, lang = sample(subtask[\"goal_task\"])\n\t#                 ret[\"goal_lang\"].append(lang)\n\t#                 ret[\"goal_ann\"].append(ann)\n\t#     inter = {}\n\t#     inter[\"states\"] = np.array(ret[\"states\"], dtype=object)\n\t#     inter[\"goal_lang\"] = torch.tensor(np.array(ret[\"goal_lang\"]))\n\t#     inter[\"goal_task\"] = np.array(ret[\"goal_task\"])\n\t#     inter[\"goal_ann\"] = np.array(ret[\"goal_ann\"])\n\t#     torch.save(inter, \"all_trajectories_all_states.pt\")\n", "# ? AWS FROM SCRATCH\n\t# %%\n\tfrom collections import defaultdict\n\timport tqdm\n\tfor seed in [12, 13, 42]:\n\t    seed = str(seed)\n\t    p = \"/home/ubuntu/vanilla-hulc-larel-baseline/evaluation\"\n\t    files = [j(p, f) for f in os.listdir(p) if f.startswith(f\"TG{seed}\")]\n\t    os.chdir(p)\n\t    ret = defaultdict(list)\n", "    for f in tqdm.tqdm(files):\n\t        try:\n\t            f = torch.load(f, map_location=\"cpu\")\n\t            for traj in f:\n\t                for subtask in traj:\n\t                    # goal_space_states = gen_goal(subtask[\"states\"])\n\t                    ret[\"states\"].append(subtask[\"states\"])\n\t                    ret[\"goal_task\"].append(subtask[\"goal_task\"])\n\t                    ann, lang = sample(subtask[\"goal_task\"])\n\t                    ret[\"goal_lang\"].append(lang)\n", "                    ret[\"goal_ann\"].append(ann)\n\t            inter = {}\n\t            inter[\"states\"] = np.array(ret[\"states\"], dtype=object)\n\t            inter[\"goal_lang\"] = torch.tensor(np.array(ret[\"goal_lang\"]))\n\t            inter[\"goal_task\"] = np.array(ret[\"goal_task\"])\n\t            inter[\"goal_ann\"] = np.array(ret[\"goal_ann\"])\n\t        except Exception as e:\n\t            print(\"*\" * 50)\n\t            print(f)\n\t            print(e)\n", "            print(\"*\" * 50)\n\t        torch.save(inter, f\"{seed}_all_trajectories.pt\")\n"]}
{"filename": "src/lcd/scripts/generation/task_orderings.py", "chunked_list": ["from collections import defaultdict\n\timport torch\n\tfrom hulc.evaluation.multistep_sequences import get_sequences\n\tseq = get_sequences(num_sequences=10000, num_workers=64)\n\tfirst_task = [s[1][0] for s in seq]\n\tindices = defaultdict(list)\n\tfor i, s in enumerate(seq):\n\t    indices[s[1][0]].append(i)\n\tfirst_indices = list(indices.keys())\n\tfor i, s in enumerate(seq):\n", "    if s[1][1] not in first_indices:\n\t        indices[s[1][1]].append(i)\n\t    if s[1][2] == \"unstack_block\":\n\t        indices[s[1][2]].append(i)\n\tinfo = (indices, seq)\n\ttorch.save(info, \"(indices,seq).pt\")\n"]}
{"filename": "src/lcd/config/calvin.py", "chunked_list": ["from lcd.utils import watch\n\t# ------------------------ base ------------------------#\n\t## automatically make experiment names for planning\n\t## by labelling folders with these args\n\targs_to_watch = [\n\t    (\"prefix\", \"\"),\n\t    # ('horizon', 'H'),\n\t    (\"n_diffusion_steps\", \"T\"),\n\t    ## value kwargs\n\t    (\"seed\", \"S\"),\n", "]\n\tlogbase = \"logs\"\n\tbase = {\n\t    \"diffusion\": {\n\t        ## model\n\t        \"model\": \"models.TemporalUnet\",\n\t        \"diffusion\": \"models.GaussianDiffusion\",\n\t        \"horizon\": 4,  # 1,\n\t        \"n_diffusion_steps\": 20,\n\t        \"action_weight\": 100,\n", "        \"loss_weights\": None,\n\t        \"loss_discount\": 1,\n\t        \"predict_epsilon\": False,\n\t        \"dim_mults\": (1, 4, 8),  # (1, 2, 4, 8),\n\t        \"attention\": True,  # False\n\t        \"renderer\": \"utils.MuJoCoRenderer\",\n\t        \"downsample\": False,  # True\n\t        \"model_dim\": 64,  # 128,\n\t        ## dataset\n\t        \"loader\": \"datasets.HulcDataset\",  # 'datasets.HulcDataset',\n", "        \"frame_offset\": 0,\n\t        \"normalizer\": \"GaussianNormalizer\",\n\t        \"preprocess_fns\": [],\n\t        \"clip_denoised\": False,\n\t        \"use_padding\": True,\n\t        \"max_path_length\": 1000,\n\t        \"observation_dim\": 32,\n\t        \"action_dim\": 32,\n\t        ## serialization\n\t        \"logbase\": logbase,\n", "        \"prefix\": \"diffusion/defaults\",\n\t        \"exp_name\": watch(args_to_watch),\n\t        ## training\n\t        \"wandb\": False,  # true\n\t        \"wandb_name\": \"default\",\n\t        \"wandb_project\": \"language-control-diffusion\",\n\t        # \"wandb_entity\": \"<REPLACE WITH YOUR WANDB ORG>\",  #!! Change me\n\t        \"wandb_entity\": \"lang-diffusion\",  #!! Change me\n\t        \"n_steps_per_epoch\": 10000,\n\t        \"n_evals_per_epoch\": 10,\n", "        \"eval_freq\": 10,\n\t        \"loss_type\": \"l2\",\n\t        \"n_train_steps\": 3e5,\n\t        \"batch_size\": 512,\n\t        \"learning_rate\": 2e-4,\n\t        \"gradient_accumulate_every\": 1,\n\t        \"ema_decay\": 0.995,\n\t        \"save_freq\": 10000,\n\t        \"sample_freq\": 1000,\n\t        \"n_saves\": 5,\n", "        \"save_parallel\": False,\n\t        \"n_reference\": 8,\n\t        \"bucket\": None,\n\t        \"device\": \"cuda\",\n\t        \"seed\": 0,\n\t    }\n\t}\n"]}
{"filename": "src/lcd/models/diffusion.py", "chunked_list": ["import pdb\n\tfrom collections import namedtuple\n\timport numpy as np\n\timport torch\n\tfrom torch import nn\n\timport lcd.utils as utils\n\tfrom .helpers import Losses, apply_conditioning, cosine_beta_schedule, extract\n\tSample = namedtuple(\"Sample\", \"trajectories values chains\")\n\t@torch.no_grad()\n\tdef default_sample_fn(model, x, cond, t, **kwargs):\n", "    model_mean, _, model_log_variance = model.p_mean_variance(\n\t        x=x, cond=cond, t=t, **kwargs\n\t    )\n\t    model_std = torch.exp(0.5 * model_log_variance)\n\t    # no noise when t == 0\n\t    noise = torch.randn_like(x)\n\t    noise[t == 0] = 0\n\t    values = torch.zeros(len(x), device=x.device)\n\t    return model_mean + model_std * noise, values\n\tdef make_timesteps(batch_size, i, device):\n", "    t = torch.full((batch_size,), i, device=device, dtype=torch.long)\n\t    return t\n\tclass GaussianDiffusion(nn.Module):\n\t    def __init__(\n\t        self,\n\t        model,\n\t        horizon,\n\t        observation_dim,\n\t        action_dim,\n\t        n_timesteps=1000,\n", "        loss_type=\"l1\",\n\t        clip_denoised=False,\n\t        predict_epsilon=True,\n\t        action_weight=1.0,\n\t        loss_discount=1.0,\n\t        loss_weights=None,\n\t        classifier_drop_probability=-1.0,  # 0.1\n\t    ):\n\t        super().__init__()\n\t        self.horizon = horizon\n", "        self.observation_dim = observation_dim\n\t        self.action_dim = action_dim\n\t        self.transition_dim = observation_dim + action_dim\n\t        self.model = model\n\t        self.classifier_drop_probability = classifier_drop_probability\n\t        betas = cosine_beta_schedule(n_timesteps)\n\t        alphas: torch.Tensor = 1.0 - betas\n\t        alphas_cumprod = torch.cumprod(alphas, dim=0)\n\t        alphas_cumprod_prev = torch.cat([torch.ones(1), alphas_cumprod[:-1]])\n\t        self.n_timesteps = int(n_timesteps)\n", "        self.clip_denoised = clip_denoised\n\t        self.predict_epsilon = predict_epsilon\n\t        self.register_buffer(\"betas\", betas)\n\t        self.register_buffer(\"alphas_cumprod\", alphas_cumprod)\n\t        self.register_buffer(\"alphas_cumprod_prev\", alphas_cumprod_prev)\n\t        # calculations for diffusion q(x_t | x_{t-1}) and others\n\t        self.register_buffer(\"sqrt_alphas_cumprod\", torch.sqrt(alphas_cumprod))\n\t        self.register_buffer(\n\t            \"sqrt_one_minus_alphas_cumprod\", torch.sqrt(1.0 - alphas_cumprod)\n\t        )\n", "        self.register_buffer(\n\t            \"log_one_minus_alphas_cumprod\", torch.log(1.0 - alphas_cumprod)\n\t        )\n\t        self.register_buffer(\n\t            \"sqrt_recip_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod)\n\t        )\n\t        self.register_buffer(\n\t            \"sqrt_recipm1_alphas_cumprod\", torch.sqrt(1.0 / alphas_cumprod - 1)\n\t        )\n\t        # calculations for posterior q(x_{t-1} | x_t, x_0)\n", "        posterior_variance = (\n\t            betas * (1.0 - alphas_cumprod_prev) / (1.0 - alphas_cumprod)\n\t        )\n\t        self.register_buffer(\"posterior_variance\", posterior_variance)\n\t        ## log calculation clipped because the posterior variance\n\t        ## is 0 at the beginning of the diffusion chain\n\t        self.register_buffer(\n\t            \"posterior_log_variance_clipped\",\n\t            torch.log(torch.clamp(posterior_variance, min=1e-20)),\n\t        )\n", "        self.register_buffer(\n\t            \"posterior_mean_coef1\",\n\t            betas * np.sqrt(alphas_cumprod_prev) / (1.0 - alphas_cumprod),\n\t        )\n\t        self.register_buffer(\n\t            \"posterior_mean_coef2\",\n\t            (1.0 - alphas_cumprod_prev) * np.sqrt(alphas) / (1.0 - alphas_cumprod),\n\t        )\n\t        ## get loss coefficients and initialize objective\n\t        loss_weights = self.get_loss_weights(action_weight, loss_discount, loss_weights)\n", "        self.loss_fn = Losses[loss_type](loss_weights, self.action_dim)\n\t    def get_loss_weights(self, action_weight, discount, weights_dict):\n\t        \"\"\"\n\t        sets loss coefficients for trajectory\n\t        action_weight   : float\n\t            coefficient on first action loss\n\t        discount   : float\n\t            multiplies t^th timestep of trajectory loss by discount**t\n\t        weights_dict    : dict\n\t            { i: c } multiplies dimension i of observation loss by c\n", "        \"\"\"\n\t        self.action_weight = action_weight\n\t        dim_weights = torch.ones(self.transition_dim, dtype=torch.float32)\n\t        ## set loss coefficients for dimensions of observation\n\t        if weights_dict is None:\n\t            weights_dict = {}\n\t        for ind, w in weights_dict.items():\n\t            dim_weights[self.action_dim + ind] *= w\n\t        ## decay loss with trajectory timestep: discount**t\n\t        discounts = discount ** torch.arange(self.horizon, dtype=torch.float)\n", "        discounts = discounts / discounts.mean()\n\t        loss_weights = torch.einsum(\"h,t->ht\", discounts, dim_weights)\n\t        ## manually set a0 weight\n\t        loss_weights[0, : self.action_dim] = action_weight\n\t        return loss_weights\n\t    # ------------------------------------------ sampling ------------------------------------------#\n\t    def predict_noise_from_start(self, x_t, t, x0):\n\t        \"\"\"\n\t        if self.predict_epsilon, model output is (scaled) noise;\n\t        otherwise, model predicts x0 directly\n", "        \"\"\"\n\t        if self.predict_epsilon:\n\t            return x0\n\t        else:\n\t            return (\n\t                extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - x0\n\t            ) / extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\t    def predict_start_from_noise(self, x_t, t, noise):\n\t        \"\"\"\n\t        if self.predict_epsilon, model output is (scaled) noise;\n", "        otherwise, model predicts x0 directly\n\t        \"\"\"\n\t        if self.predict_epsilon:\n\t            return (\n\t                extract(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t\n\t                - extract(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n\t            )\n\t        else:\n\t            return noise\n\t    def q_posterior(self, x_start, x_t, t):\n", "        posterior_mean = (\n\t            extract(self.posterior_mean_coef1, t, x_t.shape) * x_start\n\t            + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n\t        )\n\t        posterior_variance = extract(self.posterior_variance, t, x_t.shape)\n\t        posterior_log_variance_clipped = extract(\n\t            self.posterior_log_variance_clipped, t, x_t.shape\n\t        )\n\t        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\t    def p_mean_variance(self, x, cond, t):\n", "        x_recon = self.predict_start_from_noise(x, t=t, noise=self.model(x, cond, t))\n\t        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(\n\t            x_start=x_recon, x_t=x, t=t\n\t        )\n\t        return model_mean, posterior_variance, posterior_log_variance\n\t    @torch.no_grad()\n\t    def p_sample_loop(\n\t        self,\n\t        shape,\n\t        cond,\n", "        prior=None,\n\t        inpaint=None,\n\t        verbose=False,\n\t        return_chain=False,\n\t        sample_fn=default_sample_fn,\n\t        **sample_kwargs,\n\t    ):\n\t        device = self.betas.device\n\t        batch_size = shape[0]\n\t        if prior is None:\n", "            x = torch.randn(shape, device=device)  # type: ignore\n\t        else:\n\t            x = prior\n\t        if inpaint is not None:\n\t            x = apply_conditioning(x, inpaint, self.action_dim)\n\t        chain = [x]\n\t        for i in reversed(range(0, self.n_timesteps)):\n\t            t = make_timesteps(batch_size, i, device)\n\t            x, values = sample_fn(self, x, cond, t, **sample_kwargs)\n\t            if inpaint is not None:\n", "                x = apply_conditioning(x, inpaint, self.action_dim)\n\t            if return_chain:\n\t                chain.append(x)\n\t        # x, values = sort_by_values(x, values)\n\t        if return_chain:\n\t            chain = torch.stack(chain, dim=1)  # type: ignore\n\t        return Sample(x, values, chain)\n\t    @torch.no_grad()\n\t    def ddim_sample(\n\t        self, shape, cond, prior=None, inpaint=None, return_chain=False, **sample_kwargs\n", "    ):\n\t        batch_size, device, total_timesteps, sampling_timesteps, eta = (\n\t            shape[0],\n\t            self.betas.device,\n\t            self.n_timesteps,\n\t            10,\n\t            0,\n\t        )\n\t        times = torch.linspace(\n\t            -1, total_timesteps - 1, steps=sampling_timesteps + 1\n", "        )  # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n\t        times = list(reversed(times.int().tolist()))\n\t        time_pairs = list(\n\t            zip(times[:-1], times[1:])\n\t        )  # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n\t        x = torch.randn(shape, device=device)  # type: ignore\n\t        if prior is not None:\n\t            x += 0.02 * prior\n\t        if inpaint is not None:\n\t            x = apply_conditioning(x, inpaint, self.action_dim)\n", "        x_start = None\n\t        chain = []\n\t        for time, time_next in time_pairs:\n\t            t = make_timesteps(batch_size, time, device)\n\t            t_next = make_timesteps(batch_size, time_next, device)\n\t            model_out = self.model(x, cond, t)\n\t            x_start = self.predict_start_from_noise(x, t=t, noise=model_out)\n\t            pred_noise = self.predict_noise_from_start(x, t=t, x0=model_out)\n\t            if time_next < 0:\n\t                x = x_start\n", "                continue\n\t            alpha = extract(self.alphas_cumprod, t, x.shape)\n\t            alpha_next = extract(self.alphas_cumprod, t_next, x.shape)\n\t            sigma = (\n\t                eta * ((1 - alpha / alpha_next) * (1 - alpha_next) / (1 - alpha)).sqrt()\n\t            )\n\t            c = (1 - alpha_next - sigma**2).sqrt()\n\t            noise = torch.randn_like(x)\n\t            x = x_start * alpha_next.sqrt() + c * pred_noise + sigma * noise\n\t            if inpaint is not None:\n", "                x = apply_conditioning(x, inpaint, self.action_dim)\n\t            if return_chain:\n\t                chain.append(x)\n\t        if return_chain:\n\t            chain = torch.stack(chain, dim=1)  # type: ignore\n\t        if inpaint is not None:\n\t            x = apply_conditioning(x, inpaint, self.action_dim)\n\t        return Sample(x, None, chain)\n\t    @torch.no_grad()\n\t    def conditional_sample(self, cond, horizon=None, **sample_kwargs):\n", "        \"\"\"\n\t        conditions : [ (time, state), ... ]\n\t        \"\"\"\n\t        batch_size = len(cond)\n\t        horizon = horizon or self.horizon\n\t        shape = (batch_size, horizon, self.transition_dim)\n\t        if sample_kwargs.get(\"ddim\"):\n\t            return self.ddim_sample(shape, cond, **sample_kwargs)\n\t        return self.p_sample_loop(shape, cond, **sample_kwargs)\n\t    # ------------------------------------------ training ------------------------------------------#\n", "    def q_sample(self, x_start, t, noise=None):\n\t        if noise is None:\n\t            noise = torch.randn_like(x_start)\n\t        sample = (\n\t            extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start\n\t            + extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise\n\t        )\n\t        return sample\n\t    def p_losses(self, x_start, cond, t, inpaint=None, **model_kwargs):\n\t        noise = torch.randn_like(x_start)\n", "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise).to(x_start)\n\t        if inpaint is not None:\n\t            x_noisy = apply_conditioning(x_noisy, inpaint, self.action_dim)\n\t        if np.random.uniform() < self.classifier_drop_probability:\n\t            cond = torch.zeros_like(cond)\n\t        x_recon = self.model(x_noisy, cond, t, **model_kwargs)\n\t        if inpaint is not None:\n\t            x_recon = apply_conditioning(x_recon, inpaint, self.action_dim)\n\t        assert noise.shape == x_recon.shape\n\t        if self.predict_epsilon:\n", "            loss, info = self.loss_fn(x_recon, noise)\n\t        else:\n\t            loss, info = self.loss_fn(x_recon, x_start)\n\t        return loss, info\n\t    def loss(self, x, cond, inpaint=None, **model_kwargs):\n\t        batch_size = len(x)\n\t        t = torch.randint(0, self.n_timesteps, (batch_size,), device=x.device).long()\n\t        return self.p_losses(x, cond, t, inpaint=inpaint, **model_kwargs)\n\t    def forward(self, *args, **kwargs):\n\t        return self.conditional_sample(*args, **kwargs)\n"]}
{"filename": "src/lcd/models/temporal.py", "chunked_list": ["import pdb\n\timport einops\n\timport torch\n\timport torch.nn as nn\n\tfrom einops.layers.torch import Rearrange\n\tfrom .helpers import (\n\t    Conv1dBlock,\n\t    CrossAttention,\n\t    Downsample1d,\n\t    PreNorm,\n", "    Residual,\n\t    SinusoidalPosEmb,\n\t    Upsample1d,\n\t)\n\tclass ResidualTemporalBlock(nn.Module):\n\t    def __init__(self, inp_channels, out_channels, embed_dim, horizon, kernel_size=5):\n\t        super().__init__()\n\t        self.blocks = nn.ModuleList(\n\t            [\n\t                Conv1dBlock(inp_channels, out_channels, kernel_size),\n", "                Conv1dBlock(out_channels, out_channels, kernel_size),\n\t            ]\n\t        )\n\t        self.time_mlp = nn.Sequential(\n\t            nn.Mish(),\n\t            nn.Linear(embed_dim, out_channels),\n\t            Rearrange(\"batch t -> batch t 1\"),\n\t        )\n\t        self.residual_conv = (\n\t            nn.Conv1d(inp_channels, out_channels, 1)\n", "            if inp_channels != out_channels\n\t            else nn.Identity()\n\t        )\n\t    def forward(self, x, t):\n\t        \"\"\"\n\t        x : [ batch_size x inp_channels x horizon ]\n\t        t : [ batch_size x embed_dim ]\n\t        returns:\n\t        out : [ batch_size x out_channels x horizon ]\n\t        \"\"\"\n", "        out = self.blocks[0](x) + self.time_mlp(t)\n\t        out = self.blocks[1](out)\n\t        return out + self.residual_conv(x)\n\tclass TemporalUnet(nn.Module):\n\t    def __init__(\n\t        self,\n\t        horizon,\n\t        transition_dim,\n\t        cond_dim,\n\t        dim=32,\n", "        dim_mults=(1, 2, 4, 8),\n\t        attention=False,\n\t        downsample=True,\n\t    ):\n\t        super().__init__()\n\t        dims = [transition_dim, *map(lambda m: dim * m, dim_mults)]  # type: ignore\n\t        in_out = list(zip(dims[:-1], dims[1:]))\n\t        print(f\"[ models/temporal ] Channel dimensions: {in_out}\")\n\t        time_dim = dim\n\t        self.time_mlp = nn.Sequential(\n", "            SinusoidalPosEmb(dim),\n\t            nn.Linear(dim, dim * 4),\n\t            nn.Mish(),\n\t            nn.Linear(dim * 4, dim),\n\t        )\n\t        self.downs = nn.ModuleList([])\n\t        self.ups = nn.ModuleList([])\n\t        num_resolutions = len(in_out)\n\t        print(in_out)\n\t        for ind, (dim_in, dim_out) in enumerate(in_out):\n", "            is_last = ind >= (num_resolutions - 1)\n\t            self.downs.append(\n\t                nn.ModuleList(\n\t                    [\n\t                        ResidualTemporalBlock(\n\t                            dim_in, dim_out, embed_dim=time_dim, horizon=horizon\n\t                        ),\n\t                        ResidualTemporalBlock(\n\t                            dim_out, dim_out, embed_dim=time_dim, horizon=horizon\n\t                        ),\n", "                        CrossAttention(dim_out, cross_attention_dim=4096)\n\t                        if attention\n\t                        else nn.Identity(),\n\t                        Downsample1d(dim_out)\n\t                        if not is_last and downsample\n\t                        else nn.Identity(),\n\t                    ]\n\t                )\n\t            )\n\t            if not is_last and downsample:\n", "                horizon = horizon // 2\n\t        mid_dim = dims[-1]\n\t        self.mid_block1 = ResidualTemporalBlock(\n\t            mid_dim, mid_dim, embed_dim=time_dim, horizon=horizon\n\t        )\n\t        self.mid_attn = (\n\t            CrossAttention(mid_dim, cross_attention_dim=4096)\n\t            if attention\n\t            else nn.Identity()\n\t        )\n", "        self.mid_block2 = ResidualTemporalBlock(\n\t            mid_dim, mid_dim, embed_dim=time_dim, horizon=horizon\n\t        )\n\t        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n\t            is_last = ind >= (num_resolutions - 1)\n\t            self.ups.append(\n\t                nn.ModuleList(\n\t                    [\n\t                        ResidualTemporalBlock(\n\t                            dim_out * 2, dim_in, embed_dim=time_dim, horizon=horizon\n", "                        ),\n\t                        ResidualTemporalBlock(\n\t                            dim_in, dim_in, embed_dim=time_dim, horizon=horizon\n\t                        ),\n\t                        CrossAttention(dim_in, cross_attention_dim=4096)\n\t                        if attention\n\t                        else nn.Identity(),\n\t                        Upsample1d(dim_in)\n\t                        if not is_last and downsample\n\t                        else nn.Identity(),\n", "                    ]\n\t                )\n\t            )\n\t            if not is_last and downsample:\n\t                horizon = horizon * 2\n\t        self.final_conv = nn.Sequential(\n\t            Conv1dBlock(dim, dim, kernel_size=5), nn.Conv1d(dim, transition_dim, 1)\n\t        )\n\t    def forward(self, x, cond, time):\n\t        \"\"\"\n", "        x : [ batch x horizon x transition ]\n\t        \"\"\"\n\t        x = einops.rearrange(x, \"b h t -> b t h\")\n\t        t = self.time_mlp(time)\n\t        h = []\n\t        for resnet, resnet2, attn, downsample in self.downs:\n\t            x = resnet(x, t)\n\t            x = resnet2(x, t)\n\t            x = attn(x, cond)\n\t            h.append(x)\n", "            x = downsample(x)\n\t        x = self.mid_block1(x, t)\n\t        x = self.mid_attn(x, cond)\n\t        x = self.mid_block2(x, t)\n\t        for resnet, resnet2, attn, upsample in self.ups:\n\t            x = torch.cat((x, h.pop()), dim=1)\n\t            x = resnet(x, t)\n\t            x = resnet2(x, t)\n\t            x = attn(x, cond)\n\t            x = upsample(x)\n", "        x = self.final_conv(x)\n\t        x = einops.rearrange(x, \"b t h -> b h t\")\n\t        return x\n"]}
{"filename": "src/lcd/models/__init__.py", "chunked_list": ["from .diffusion import GaussianDiffusion\n\tfrom .temporal import TemporalUnet\n"]}
{"filename": "src/lcd/models/helpers.py", "chunked_list": ["import math\n\timport pdb\n\timport einops\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom einops.layers.torch import Rearrange\n\timport lcd.utils as utils\n\t# -----------------------------------------------------------------------------#\n", "# ---------------------------------- modules ----------------------------------#\n\t# -----------------------------------------------------------------------------#\n\tclass SinusoidalPosEmb(nn.Module):\n\t    def __init__(self, dim):\n\t        super().__init__()\n\t        self.dim = dim\n\t    def forward(self, x):\n\t        device = x.device\n\t        half_dim = self.dim // 2\n\t        emb = math.log(10000) / (half_dim - 1)\n", "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n\t        emb = x[:, None] * emb[None, :]\n\t        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n\t        return emb\n\tclass Downsample1d(nn.Module):\n\t    def __init__(self, dim):\n\t        super().__init__()\n\t        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)\n\t    def forward(self, x):\n\t        return self.conv(x)\n", "class Upsample1d(nn.Module):\n\t    def __init__(self, dim):\n\t        super().__init__()\n\t        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)\n\t    def forward(self, x):\n\t        return self.conv(x)\n\tclass Conv1dBlock(nn.Module):\n\t    \"\"\"\n\t    Conv1d --> GroupNorm --> Mish\n\t    \"\"\"\n", "    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):\n\t        super().__init__()\n\t        self.block = nn.Sequential(\n\t            nn.Conv1d(\n\t                inp_channels, out_channels, kernel_size, padding=kernel_size // 2\n\t            ),\n\t            Rearrange(\"batch channels horizon -> batch channels 1 horizon\"),\n\t            nn.GroupNorm(n_groups, out_channels),\n\t            Rearrange(\"batch channels 1 horizon -> batch channels horizon\"),\n\t            nn.Mish(),\n", "        )\n\t    def forward(self, x):\n\t        return self.block(x)\n\t# -----------------------------------------------------------------------------#\n\t# --------------------------------- attention ---------------------------------#\n\t# -----------------------------------------------------------------------------#\n\tclass Residual(nn.Module):\n\t    def __init__(self, fn):\n\t        super().__init__()\n\t        self.fn = fn\n", "    def forward(self, x, *args, **kwargs):\n\t        return self.fn(x, *args, **kwargs) + x\n\tclass LayerNorm(nn.Module):\n\t    def __init__(self, dim, eps=1e-5):\n\t        super().__init__()\n\t        self.eps = eps\n\t        self.g = nn.Parameter(torch.ones(1, dim, 1))\n\t        self.b = nn.Parameter(torch.zeros(1, dim, 1))\n\t    def forward(self, x):\n\t        var = torch.var(x, dim=1, unbiased=False, keepdim=True)\n", "        mean = torch.mean(x, dim=1, keepdim=True)\n\t        return (x - mean) / (var + self.eps).sqrt() * self.g + self.b\n\tclass PreNorm(nn.Module):\n\t    def __init__(self, dim, fn):\n\t        super().__init__()\n\t        self.fn = fn\n\t        self.norm = LayerNorm(dim)\n\t    def forward(self, x):\n\t        x = self.norm(x)\n\t        return self.fn(x)\n", "class CrossAttention(nn.Module):  # replace with regular cross attention\n\t    def __init__(\n\t        self,\n\t        query_dim,\n\t        cross_attention_dim=None,\n\t        heads=4,\n\t        dim_head=32,\n\t        bias=False,\n\t        dropout=0,\n\t    ):\n", "        super().__init__()\n\t        cross_attention_dim = (\n\t            cross_attention_dim if cross_attention_dim is not None else query_dim\n\t        )\n\t        self.scale = dim_head**-0.5\n\t        self.heads = heads\n\t        self.query_dim = query_dim\n\t        hidden_dim = dim_head * heads\n\t        self.norm = LayerNorm(query_dim)\n\t        self.to_q = nn.Linear(query_dim, hidden_dim, bias=bias)\n", "        self.to_k = nn.Linear(cross_attention_dim, hidden_dim, bias=bias)\n\t        self.to_v = nn.Linear(cross_attention_dim, hidden_dim, bias=bias)\n\t        self.to_out = nn.Conv1d(hidden_dim, query_dim, 1)\n\t        self.dropout = nn.Dropout(p=dropout) if dropout else nn.Identity()\n\t    def forward(self, x, context=None):\n\t        og_x = x\n\t        x = self.norm(x)\n\t        x = x.permute(0, 2, 1)\n\t        q = self.to_q(x)\n\t        context = context if context is not None else x\n", "        k = self.to_k(context)\n\t        v = self.to_v(context)\n\t        q, k, v = map(lambda t: einops.rearrange(t.permute(0, 2, 1), \"b (h c) d -> b h c d\", h=self.heads), (q, k, v))  # type: ignore\n\t        # b = batch\n\t        # h = heads\n\t        # c = channels\n\t        # d = number of queries\n\t        # e = number of key/values\n\t        qk = (torch.einsum(\"b h c d, b h c e -> b h d e\", q, k) * self.scale).softmax(\n\t            dim=-1\n", "        )\n\t        out = torch.einsum(\"b h d e, b h c e -> b h c d\", qk, v)\n\t        out = einops.rearrange(out, \"b h c d -> b (h c) d\")\n\t        return self.dropout(self.to_out(out) + og_x)\n\t# -----------------------------------------------------------------------------#\n\t# ---------------------------------- sampling ---------------------------------#\n\t# -----------------------------------------------------------------------------#\n\tdef extract(a, t, x_shape):\n\t    b, *_ = t.shape\n\t    out = a.gather(-1, t)\n", "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n\tdef cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):\n\t    \"\"\"\n\t    cosine schedule\n\t    as proposed in https://openreview.net/forum?id=-NEXDKk8gZ\n\t    \"\"\"\n\t    steps = timesteps + 1\n\t    x = np.linspace(0, steps, steps)\n\t    alphas_cumprod = np.cos(((x / steps) + s) / (1 + s) * np.pi * 0.5) ** 2\n\t    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n", "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n\t    betas_clipped = np.clip(betas, a_min=0, a_max=0.999)\n\t    return torch.tensor(betas_clipped, dtype=dtype)\n\tdef apply_conditioning(x, conditions, action_dim):\n\t    for t, val in conditions.items():\n\t        x[:, t, action_dim:] = val.clone()\n\t    return x\n\t# -----------------------------------------------------------------------------#\n\t# ---------------------------------- losses -----------------------------------#\n\t# -----------------------------------------------------------------------------#\n", "class WeightedLoss(nn.Module):\n\t    def __init__(self, weights, action_dim):\n\t        super().__init__()\n\t        self.register_buffer(\"weights\", weights)\n\t        self.action_dim = action_dim\n\t    def forward(self, pred, targ):\n\t        \"\"\"\n\t        pred, targ : tensor\n\t            [ batch_size x horizon x transition_dim ]\n\t        \"\"\"\n", "        loss = self._loss(pred, targ)\n\t        weighted_loss = (loss * self.weights).mean()\n\t        a0_loss = (loss[:, 0, : self.action_dim] / self.weights[0, : self.action_dim]).mean()  # type: ignore\n\t        return weighted_loss / self.weights[0, 0].detach(), {\"a0_loss\": a0_loss}\n\t    def _loss(self, pred, targ):\n\t        return NotImplementedError\n\tclass WeightedL1(WeightedLoss):\n\t    def _loss(self, pred, targ):\n\t        return torch.abs(pred - targ)\n\tclass WeightedL2(WeightedLoss):\n", "    def _loss(self, pred, targ):\n\t        return F.mse_loss(pred, targ, reduction=\"none\")\n\tLosses = {\"l1\": WeightedL1, \"l2\": WeightedL2}\n"]}
{"filename": "src/lcd/datasets/sequence.py", "chunked_list": ["import json\n\timport random\n\tfrom collections import namedtuple\n\timport numpy as np\n\timport torch\n\tBatch = namedtuple(\"Batch\", \"trajectories conditions\")\n\tKwargsBatch = namedtuple(\"KwargsBatch\", \"batch kwargs\")\n\tValueBatch = namedtuple(\"ValueBatch\", \"trajectories conditions values\")\n\tdef cut_last(tensor):\n\t    return tensor[:-1, ...]\n", "def cut_first(tensor):\n\t    return tensor[1:, ...]\n\tdef stack_next(tensor):\n\t    return torch.concat((cut_first(tensor), cut_last(tensor)), dim=1)\n\tclass HulcDataset(torch.utils.data.Dataset):\n\t    \"Characterizes a dataset for PyTorch\"\n\t    def __init__(\n\t        self,\n\t        *args,\n\t        buf,\n", "        task_to_ann,\n\t        lang_embeds,\n\t        frame_offset=0,\n\t        horizon=4,\n\t        clip_stride=16,\n\t        **kwargs,\n\t    ):\n\t        print(f\"{buf=}\")\n\t        self.clip_stride = clip_stride\n\t        self.frame_offset = frame_offset\n", "        self.horizon = horizon\n\t        print(\"loading dataset...\")\n\t        buf = torch.load(buf, map_location=\"cpu\")\n\t        print(\"done loading!\")\n\t        buf[\"states\"] = np.array([i[-100:] for i in buf[\"states\"]])\n\t        lens = np.array([len(f) for f in buf[\"states\"]])\n\t        valid_indices = np.argwhere(lens >= 17).squeeze()\n\t        self.buf = {\n\t            \"states\": buf[\"states\"][valid_indices],\n\t            \"goal_lang\": np.array(buf[\"goal_lang\"])[valid_indices],\n", "            \"goal_task\": np.array(buf[\"goal_task\"])[valid_indices],\n\t        }\n\t        self.task_to_ann = json.load(open(task_to_ann))\n\t        self.lang_embeds = torch.load(lang_embeds)\n\t        self.observation_dim = kwargs.get(\"observation_dim\")\n\t        self.action_dim = kwargs.get(\"action_dim\")\n\t    def __len__(self):\n\t        return len(self.buf[\"states\"])\n\t    def __getitem__(self, index):\n\t        obs_traj = self.buf[\"states\"][index]\n", "        idx = random.randint(0, len(obs_traj) - self.clip_stride - 1)\n\t        offset = random.randint(-self.frame_offset, self.frame_offset)\n\t        indices = (\n\t            idx + torch.arange(self.horizon + 1) * self.clip_stride + offset\n\t        ).clamp(max=len(obs_traj) - 1)\n\t        traj = stack_next(obs_traj[indices])\n\t        lang = self.lang_embeds[\n\t            random.choice(self.task_to_ann[self.buf[\"goal_task\"][index]])\n\t        ]\n\t        return KwargsBatch(\n", "            Batch(traj, lang), {\"inpaint\": {0: traj[0, self.action_dim :]}}\n\t        )\n"]}
{"filename": "src/lcd/datasets/__init__.py", "chunked_list": ["from .sequence import HulcDataset\n"]}
{"filename": "src/lcd/apps/rollout.py", "chunked_list": ["from pathlib import Path\n\timport torch\n\timport typer\n\tfrom hulc.evaluation.utils import get_default_model_and_env\n\tfrom loguru import logger\n\tfrom lcd import DATA_PATH, HULC_PATH\n\tfrom lcd.utils.config import AttriDict\n\tfrom lcd.utils.eval import DiffusionModelWrapper, evaluate_policy, print_and_save\n\t_app_ = app = typer.Typer(name=\"lcd\")\n\targs = AttriDict()\n", "state = AttriDict()\n\tdef eval_pipeline():\n\t    logger.debug(f\"{args=}\")\n\t    results, histories = evaluate_policy(state, args)\n\t    train_folder=args.train_folder.replace('/', '-').replace('\\\\','')\n\t    model_id = f\"train_folder={train_folder}_{args.seed=}\"\n\t    if args.diffusion_path is not None:\n\t        model_id += f\"_{args.diffusion_path=}_{args.diffusion_epoch=}\"\n\t    print_and_save(results, args, histories, model_id)\n\t@app.callback()\n", "def main(\n\t    dataset_path: str = str(HULC_PATH / \"dataset/task_D_D\"),\n\t    train_folder: str = str(DATA_PATH / \"hulc-baselines-30\"),\n\t    seed: int = 12,\n\t    debug: bool = False,\n\t    log_dir: str = None,\n\t    device: int = 0,\n\t    num_sequences: int = 1000,\n\t):\n\t    \"\"\"\n", "    Rollout in the environment for evaluation or dataset collection\n\t    \"\"\"\n\t    ep_len = 360\n\t    args.update(locals())\n\t    # *******\n\t    (\n\t        state.model,\n\t        state.env,\n\t        _,\n\t        state.lang_embeddings,\n", "    ) = get_default_model_and_env(\n\t        args.train_folder,\n\t        args.dataset_path,\n\t        Path(args.train_folder) / f\"saved_models/seed={args.seed}.ckpt\",\n\t        env=None,\n\t        lang_embeddings=None,\n\t        device_id=args.device,\n\t    )\n\t@app.command()\n\tdef lcd(\n", "    diffusion_path: str = DATA_PATH / \"lcd-seeds/seed-12\",\n\t    diffusion_epoch: str = 250_000,\n\t    subgoal_interval: int = 16,\n\t    dm__args: str = None,\n\t):\n\t    \"\"\"\n\t    Rollout with LCD\n\t    \"\"\"\n\t    args.update(locals())\n\t    # *******\n", "    if dm__args:\n\t        args.dm = DiffusionModelWrapper(\n\t            device=f\"cuda:{args.device}\",\n\t            model__args=dm__args,\n\t        )\n\t        args.diffusion_path = dm__args[1][\"savepath\"]\n\t        args.diffusion_epoch = dm__args[1][\"epoch\"]\n\t    else:\n\t        args.dm = DiffusionModelWrapper(\n\t            device=f\"cuda:{args.device}\",\n", "            model_path__epoch=(args.diffusion_path, args.diffusion_epoch),\n\t        )\n\t    state.lang_embeddings = torch.load(DATA_PATH / \"t5-v1_1-xxl_embeddings.pt\")\n\t    eval_pipeline()\n\t@app.command()\n\tdef hulc():\n\t    \"\"\"\n\t    Rollout with HULC\n\t    \"\"\"\n\t    eval_pipeline()\n", "@app.command()\n\tdef generate():\n\t    \"\"\"\n\t    Generate an on-policy dataset for training a high level policy (e.g. LCD)\n\t    \"\"\"\n\t    args.generate = True\n\t    eval_pipeline()\n"]}
{"filename": "src/lcd/apps/train_lcd.py", "chunked_list": ["import os\n\timport sys\n\tfrom pathlib import Path\n\timport typer\n\tfrom loguru import logger\n\tfrom lcd import HULC_PATH, REPO_PATH\n\tpy = sys.executable\n\tdef main(\n\t    ctx: typer.Context,\n\t):\n", "    \"\"\"Train the original hulc model\"\"\"\n\t    if ctx.args:\n\t        args = \" \".join(ctx.args)\n\t    else:\n\t        args = f\" --seed 12 --wandb True\"\n\t    cmd = f\"{py} {REPO_PATH / 'src/lcd/scripts/diffuser.py'} {args}\"\n\t    logger.info(f\"Running: \\n{cmd}\")\n\t    os.system(cmd)\n"]}
{"filename": "src/lcd/apps/train_hulc.py", "chunked_list": ["import os\n\timport sys\n\tfrom pathlib import Path\n\timport typer\n\tfrom loguru import logger\n\tfrom lcd import HULC_PATH\n\tpy = sys.executable\n\tdef cache_shm_dataset(dataset):\n\t    if not os.system(f\"\"\"tmux new-session -d -s calvin_cache_dataset\"\"\"):\n\t        # only continue if the tmux session wasn't created before\n", "        os.system(f\"\"\"tmux send-keys -t calvin_cache_dataset 'cd {HULC_PATH}' Enter\"\"\")\n\t        os.system(\n\t            f\"\"\"tmux send-keys -t calvin_cache_dataset '{py} hulc/training.py datamodule.root_data_dir={dataset} model=dummy logger=tb_logger trainer.gpus=1' Enter\"\"\"\n\t        )\n\t        logger.info(\n\t            \"View dataset caching progress with: tmux a -t calvin_cache_dataset\"\n\t        )\n\t        logger.info(\n\t            \"Please wait until the dataset has completely finished loading, then run this command again. This tmux session should remain running indefinitely in the background, effectively as a daemon.\"\n\t        )\n", "        exit(0)\n\tdef main(\n\t    ctx: typer.Context,\n\t):\n\t    \"\"\"Train the original hulc model\"\"\"\n\t    if ctx.args:\n\t        args = \" \".join(ctx.args)\n\t    else:\n\t        args = f\" trainer.gpus=-1 datamodule.root_data_dir={HULC_PATH / 'dataset/task_D_D'} seed=12\"\n\t    # parse dataset\n", "    dataset = None\n\t    for arg in args.split(\" \"):\n\t        split_arg = arg.split(\"=\")\n\t        if len(split_arg) == 2 and split_arg[0] == \"datamodule.root_data_dir\":\n\t            dataset = split_arg[1]\n\t    if dataset is None:\n\t        logger.error(\n\t            \"Must specify the dataset directory in the args with datamodule.root_data_dir=/path/to/data\"\n\t        )\n\t        raise ValueError\n", "    # cache shm dataset\n\t    cache_shm_dataset(dataset)\n\t    cmd = f\"{py} {HULC_PATH / 'hulc/training.py'} {args}\"\n\t    logger.info(f\"Running: \\n{cmd}\")\n\t    os.system(cmd)\n"]}
