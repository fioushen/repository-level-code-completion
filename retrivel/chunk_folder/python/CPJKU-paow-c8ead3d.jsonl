{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\timport io\n\timport os\n\tfrom setuptools import find_packages, setup\n\t# Package meta-data.\n\tNAME = 'paow'\n\tDESCRIPTION = 'Symbolic music generation'\n\tKEYWORDS = 'generation midi score'\n\tURL = \"https://github.com/cpjku/paow\"\n", "AUTHOR = 'Silvan Peter'\n\tREQUIRES_PYTHON = '>=3.7'\n\tVERSION = '0.0.1'\n\t# What packages are required for this module to be executed?\n\tREQUIRED = [\n\t    'numpy',\n\t    'partitura>=1.1.0'\n\t    ]\n\there = os.path.abspath(os.path.dirname(__file__))\n\ttry:\n", "    with io.open(os.path.join(here, 'README.md'), encoding='utf-8') as f:\n\t        long_description = '\\n' + f.read()\n\texcept FileNotFoundError:\n\t    long_description = DESCRIPTION\n\t# Load the package's __version__.py module as a dictionary.\n\tabout = {}\n\tif not VERSION:\n\t    with open(os.path.join(here, NAME, '__version__.py')) as f:\n\t        exec(f.read(), about)\n\telse:\n", "    about['__version__'] = VERSION\n\t# Where the magic happens:\n\tsetup(\n\t    name=NAME,\n\t    version=about['__version__'],\n\t    description=DESCRIPTION,\n\t    long_description=long_description,\n\t    long_description_content_type='text/markdown',\n\t    keywords=KEYWORDS,\n\t    author=AUTHOR,\n", "    url=URL,\n\t    python_requires=REQUIRES_PYTHON,\n\t    packages=find_packages(exclude=('tests',)),\n\t        package_data={\n\t        \"paow\": [\n\t            \"assets/dummy.txt\",\n\t        ]\n\t        },\n\t    install_requires=REQUIRED,\n\t    extras_require={},\n", "    include_package_data=True,\n\t    license=\"Apache 2.0\",\n\t    classifiers=[\n\t        # Trove classifiers\n\t        # Full list: https://pypi.python.org/pypi?%3Aaction=list_classifiers\n\t        \"Programming Language :: Python\",\n\t        \"Programming Language :: Python :: 3\",\n\t    ],\n\t)\n"]}
{"filename": "paow/__init__.py", "chunked_list": ["#!/usr/bin/python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThe top level of the package.\n\t\"\"\"\n\timport pkg_resources\n\tEXAMPLE = pkg_resources.resource_filename(\"paow\", \n\t                                          \"assets/dummy.txt\")\n\tfrom .utils import Sequencer, MidiRouter\n\tfrom .evolutionary import Optimizer\n", "__all__ = [\n\t    \"Sequencer\",\n\t    \"MidiRouter\",\n\t    \"Optimizer\",\n\t]\n"]}
{"filename": "paow/23_05_31/test_pedal_thirds.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis script generates a MIDI file with octave-doubled arpeggios made of stacked major and minor thirds.\n\tWhether the next third is major or minor as well as the root pitch is randomly sampled.\n\tThe tempo starts at ~0.25 notes/sec and over the course of one hour is increased to 16 notes/sec.\n\tThe sustain pedal is controlled by sinusoid curves with varying frequency constants.\n\tEnjoy!\n\t\"\"\"\n\tfrom mido import Message, MidiFile, MidiTrack\n", "import numpy as np\n\tmid = MidiFile()\n\ttrack = MidiTrack()\n\tmid.tracks.append(track)\n\t# some sinusoids with different frequency constants for pedal controls\n\tmidi_pedal_maps = []\n\tfactors = np.arange(1,11)\n\tfor n in range(1,11):\n\t    midi_pedal_maps.append(\n\t    np.round((np.sin(2*np.pi*np.arange(0,128)/(128/n*4))+1)/2*127).astype(int)\n", "    )\n\t# tempos: 100 tempos from whole notes at 60 bpm to sixteenth notes at 240 bpm\n\t# => 0.25 notes/sec - 16 notes/sec\n\t# MIDI default parts per quarter is 480, MIDI default tempo is 120 bpm -> 960 ticks/sec\n\ttempos_in_notes_per_sec = np.linspace(0.25,16,100)\n\ttempos_in_ticks_per_note = 960 / tempos_in_notes_per_sec\n\t# 128 notes per tempo gives a duration of ~1h\n\ttotal_duration_in_hours = (1/tempos_in_notes_per_sec * 128).sum()/3600 # 1.0089\n\t# get nice integer times divisible by two\n\ttempos_in_ticks_per_note_int = np.round(tempos_in_ticks_per_note / 2).astype(int) \n", "# creating the notes\n\tfor l in range(100):\n\t    # lowest MIDI pitch: 21\n\t    root = np.random.randint(21, 40)\n\t    pitch = root\n\t    local_duration_halfed = tempos_in_ticks_per_note_int[l] \n\t    for k in range(128):\n\t        #track.append(Message('program_change', program=12, time=0))      \n\t        track.append(Message('note_on', note=pitch, velocity=44, time=local_duration_halfed))\n\t        track.append(Message('note_on', note=pitch+12, velocity=44, time=0))\n", "        track.append(Message('note_off', note=pitch, velocity=0, time=local_duration_halfed))\n\t        track.append(Message('note_off', note=pitch+12, velocity=0, time=0))\n\t        track.append(Message(\"control_change\",\n\t                            control=64,\n\t                            value=midi_pedal_maps[l%10][k]))\n\t        # highest MIDI pitch: 108\n\t        pitch = (pitch + np.random.randint(3,5) - root )%58 + root\n\tmid.save('test_pedal_thirds_hour.mid')"]}
{"filename": "paow/23_07_12/sample_continous.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis experiment outpaints the super mario theme for an hour, \n\tgradually forgetting where it started.\n\tEnjoy!\n\t\"\"\"\n\timport time\n\timport numpy as np\n\tfrom note_seq import note_sequence_to_midi_file, midi_file_to_note_sequence\n", "from hparams import get_sampler_hparams\n\tfrom utils import get_sampler, load_model\n\tfrom utils.sampler_utils import get_samples, np_to_ns, ns_to_np\n\tdef sample_cont(sampler, H):\n\t    x_T = None\n\t    batch = 1\n\t    if H.piece:\n\t        ns = midi_file_to_note_sequence(H.piece)\n\t        bars = min(64, int(max([n.end_time for n in ns.notes]) // 2))\n\t        npy = ns_to_np(ns, bars, 'melody').outputs[0]\n", "        x_T = np.zeros((batch, H.NOTES, 3), dtype=int)\n\t        x_T[:, :] = H.codebook_size\n\t        x_T[:, :npy.shape[0], 0] = npy[:, 0]\n\t        if npy.shape[1] == 3:\n\t            x_T[:, :npy.shape[0], 1] = npy[:, 1]\n\t            x_T[:, :npy.shape[0], 2] = npy[:, 2]\n\t    n_samples = 0\n\t    sampler.sampling_batch_size = batch\n\t    piece = None\n\t    while n_samples < H.n_samples:\n", "        sa = get_samples(sampler, H.sample_steps, x_T)\n\t        if piece is None:\n\t            piece = sa\n\t        else:\n\t            piece = np.append(piece, sa[:, sa.shape[1] // 2:], axis=1)\n\t        x_T = np.zeros((batch, H.NOTES, 3), dtype=int)\n\t        x_T[:, :] = H.codebook_size\n\t        x_T[:, :sa.shape[1] // 2] = sa[:, sa.shape[1] // 2:]\n\t        ns = np_to_ns(piece)\n\t        for _ in ns:\n", "            n_samples += 1\n\t        print(f'{n_samples}/{H.n_samples}')\n\t    for n in ns:\n\t        note_sequence_to_midi_file(n, f'data/out/conti{time.time()}.mid')\n\tif __name__ == '__main__':\n\t    H = get_sampler_hparams('sample')\n\t    H.sample_schedule = \"rand\"\n\t    sampler = get_sampler(H).cuda()\n\t    sampler = load_model(\n\t                sampler, f'{H.sampler}_ema', H.load_step, H.load_dir)\n", "    sample_cont(sampler, H)"]}
{"filename": "paow/23_07_05/run.py", "chunked_list": ["import numpy as np\n\timport mido\n\timport random\n\timport time\n\timport os\n\timport audiofile\n\tfrom audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift, AirAbsorption, ApplyImpulseResponse, TimeMask, GainTransition\n\timport zipfile\n\timport wave\n\timport pyaudio\n", "import threading\n\tfrom scipy import signal\n\tfrom scipy.interpolate import interp1d\n\tfrom pedalboard import Reverb, Gain, Chorus, Convolution, Compressor, Phaser, LadderFilter, Pedalboard\n\tgrammar = {\n\t    \"Pattern\" : [[\"P1\", \"A1\", \"D1\", \"W1\"], [\"P2\", \"A2\", \"D2\", \"W2\"], [\"P3\", \"A3\", \"D3\", \"W3\"], [\"P4\", \"A4\", \"D4\", \"W4\"]],\n\t    \"P1\": np.array([67, 75, 71, 78, 77, 88]),\n\t    \"P2\": np.array([27, 98, 62, 55, 71, 63]),\n\t    \"P4\": np.array([27, 98, 62, 55, 71, 63]),\n\t    \"P3\": np.array([55, 62, 69, 76, 81, 82, 88]),\n", "    \"P4\": np.array([27, 100]),\n\t    \"A1\": np.array([30, 30, 30, 30, 30, 30]),\n\t    \"A2\": np.array([20, 20, 20, 20, 20, 20]),\n\t    \"A3\": np.array([20, 20, 20, 20, 20, 20, 20]),\n\t    \"A4\": np.array([20, 20]),\n\t    \"D1\": np.array([300, 300, 300, 300, 300, 300]),\n\t    \"D2\": np.array([4000, 4000, 3000, 3000, 3000, 3000]),\n\t    \"D3\": np.array([3000, 3000, 3000, 3000, 3000, 3000, 3000]),\n\t    \"D4\": np.array([3000, 3000]),\n\t    \"W1\": np.array([800, 800, 800, 800, 800, 800]),\n", "    \"W2\": np.array([0, 5000, 0, 0, 0, 3000]),\n\t    \"W3\": np.array([100, 100, 100, 100, 100, 100, 3000]),\n\t    \"W4\": np.array([0, 3000])\n\t}\n\tdef audio_grammar(main_path):\n\t    path1 = os.path.join(main_path, \"T1\")\n\t    path2 = os.path.join(main_path, \"T2\")\n\t    path3 = os.path.join(main_path, \"T3\")\n\t    one = list(map(lambda x: os.path.join(path1, x), os.listdir(path1)))\n\t    two = list(map(lambda x: os.path.join(path2, x), os.listdir(path2)))\n", "    three = list(map(lambda x: os.path.join(path3, x), os.listdir(path3)))\n\t    grammar = {\n\t        \"Type\": [\"T1\", \"T2\", \"T3\"],\n\t        \"T1\": one,\n\t        \"T2\": two,\n\t        \"T3\": three\n\t    }\n\t    return grammar\n\tdef perturbation(p, a, d, w):\n\t    new_p = np.array(p) + np.random.randint(-1, 2, len(p))\n", "    new_a = np.array(a) + np.random.randint(-10, 10, len(a))\n\t    add_mask = np.zeros(len(a))\n\t    ind_a = random.choice(np.arange(len(a)))\n\t    add_mask[ind_a] = 10\n\t    new_a = new_a + add_mask\n\t    new_d = np.array(d) + np.random.randint(-1000, 1000, len(d))\n\t    new_w = np.array(w) + np.random.randint(-300, 300, len(d))\n\t    # Filter out negative values\n\t    new_d[new_d < 0] = 0\n\t    new_w[new_w < 0] = 0\n", "    return new_p, new_a, new_d, new_w\n\tdef audio_param_perturbation(parameters):\n\t    \"\"\"Perturb the parameters of the audio file\"\"\"\n\t    new_parameters = {}\n\t    for key, value in parameters.items():\n\t        if key == \"min_amplitude\":\n\t            # sample from a gaussian distribution\n\t            new_min_amplitude = np.random.normal(value, 0.1)\n\t            new_parameters[key] = new_min_amplitude if new_min_amplitude > 0 else 0.001\n\t            new_parameters[\"max_amplitude\"] = new_parameters[\"min_amplitude\"] + 0.1\n", "        if key == \"min_rate\":\n\t            new_min_rate = np.random.normal(value, 1.)\n\t            new_parameters[key] = new_min_rate if new_min_rate > 0.1 else 0.5\n\t            max_rate = np.random.normal(value, 1.)\n\t            max_rate = max_rate if max_rate > 0 else 0\n\t            new_parameters[\"max_rate\"] = new_parameters[\"min_rate\"] + max_rate\n\t        if key == \"min_semitones\":\n\t            new_parameters[key] = -np.random.randint(0, 12)\n\t        if key == \"max_semitones\":\n\t            new_parameters[key] = np.random.randint(0, 12)\n", "        if key == \"min_fraction\":\n\t            new_parameters[key] = - np.random.uniform(0., 1.)\n\t        if key == \"max_fraction\":\n\t            new_parameters[key] = np.random.uniform(0., 1.)\n\t        if key == \"p\":\n\t            new_parameters[key] = np.random.uniform(0., 1.)\n\t        if key == \"volume\":\n\t            new_parameters[key] = np.random.uniform(0.5, 1.)\n\t    return new_parameters\n\tdef generate_grammar(grammar, memory, mem_length=10):\n", "    \"\"\"Generate a pattern from the grammar\"\"\"\n\t    if len(memory) == 0:\n\t        # first pattern\n\t        pattern = random.choice(grammar[\"Pattern\"])\n\t    else:\n\t        # weight more recent patterns\n\t        weights = np.arange(len(memory) + len(grammar[\"Pattern\"]))\n\t        pattern = random.choices(memory+grammar[\"Pattern\"], weights=weights)[0]\n\t    p, a, d, w = pattern\n\t    # Replace Letters\n", "    ########\n\t    p, a, d, w = grammar[p], grammar[a], grammar[d], grammar[w]\n\t    # add to memory\n\t    memory.append(pattern)\n\t    # last element remove from memory if it exceeds size\n\t    if len(memory) > mem_length:\n\t        memory.pop(-1)\n\t    return perturbation(p, a, d, w), memory\n\tdef generate_audio_grammar(grammar, effects, memory=[], mem_length=10, mode=1):\n\t    \"\"\"Generate a pattern from the grammar\"\"\"\n", "    if len(memory) == 0:\n\t        # first pattern\n\t        pattern = random.choice(grammar[\"Type\"])\n\t    else:\n\t        # weight more recent patterns\n\t        weights = np.arange(len(memory) + len(grammar[\"Type\"]))\n\t        pattern = random.choices(memory+grammar[\"Type\"], weights=weights)[0]\n\t    # add to memory\n\t    memory.append(pattern)\n\t    # last element remove from memory if it exceeds size\n", "    if len(memory) > mem_length:\n\t        memory.pop(-1)\n\t    audio_clip = random.choice(grammar[pattern])\n\t    parameters = {\n\t        \"min_amplitude\": 0.001,\n\t        \"max_amplitude\": 0.015,\n\t        \"min_rate\": 0.8,\n\t        \"max_rate\": 1.2,\n\t        \"min_semitones\": -4,\n\t        \"max_semitones\": 4,\n", "        \"min_fraction\": -0.5,\n\t        \"max_fraction\": 0.5,\n\t        \"p\": 0.5,\n\t        \"volume\": 1.0\n\t    }\n\t    # return audio_clip\n\t    parameters = audio_param_perturbation(parameters)\n\t    path = effects.apply_effect(audio_clip, parameters, mode)\n\t    return path, memory\n\tdef generate_and_send_midi(music_grammar, port_name, generation_length=3600, mem_length=10, test=False):\n", "    \"\"\"Generate a midi message and send it to the port\n\t    Parameters\n\t    ----------\n\t    music_grammar : dict\n\t        The grammar for the music\n\t    port_name : mido port name\n\t        The port to send the midi message\n\t    generation_length : int\n\t        The length of the generation in seconds\n\t    mem_length : int\n", "        The length of the memory\n\t    \"\"\"\n\t    # Initialize port\n\t    port = mido.open_output(port_name) if not test else None\n\t    # Hold down the pedal\n\t    msg = mido.Message('control_change', control=64, value=127)\n\t    port.send(msg)\n\t    start_time = time.time()\n\t    memory = list()\n\t    while time.time() - start_time < generation_length:\n", "        try:\n\t            (p, a, d, w), memory = generate_grammar(music_grammar, memory, mem_length)\n\t            # Generate midi message\n\t            ########\n\t            # Send midi message\n\t            for i in range(len(p)):\n\t                if test:\n\t                    print(\"Note: {}, Velocity: {}, Duration: {}\".format(p[i], a[i], d[i]))\n\t                    continue\n\t                msg = mido.Message('note_on', note=int(p[i]), velocity=int(a[i]), time=0)\n", "                port.send(msg)\n\t                print(msg)\n\t                msg = mido.Message('note_off', note=int(p[i]), velocity=int(a[i]), time=int(d[i]))\n\t                port.send(msg)\n\t                # Wait for the next note\n\t                time.sleep(w[i] / 1000)\n\t            # Release the pedal\n\t            if random.random() < 0.1:\n\t                msg = mido.Message('control_change', control=64, value=127)\n\t                port.send(msg)\n", "            # Wait for the next generation\n\t            ########\n\t            time.sleep(random.randint(1, 50) * 0.1)\n\t        except KeyboardInterrupt:\n\t            if not test:\n\t                # Release the pedal\n\t                msg = mido.Message('control_change', control=64, value=0)\n\t                port.send(msg)\n\t                port.close()\n\t            raise ValueError(\"Interrupted by user\")\n", "    # Close port\n\t    if not test:\n\t        # Release the pedal\n\t        msg = mido.Message('control_change', control=64, value=0)\n\t        port.send(msg)\n\t        port.close()\n\tclass EffectClass:\n\t    def __init__(self, sample_rate=44100):\n\t        self.sample_rate = sample_rate\n\t    def apply_effect(self, audio_path, params, mode):\n", "        signal, sampling_rate = audiofile.read(audio_path)\n\t        # reverse signal\n\t        signal = np.flip(signal) if random.gauss(mu=0, sigma=1) > 0.6 else signal\n\t        # normalize signal\n\t        # signal = signal/signal.max()\n\t        effect = self.select_effect(params)\n\t        out = effect(signal, sampling_rate)\n\t        # adjust volume\n\t        out = out*params[\"volume\"]\n\t        # do a fade in and fade out\n", "        fade_in = np.linspace(0, 1, 10000)\n\t        fade_out = np.linspace(1, 0, 10000)\n\t        out[:10000] = out[:10000]*fade_in\n\t        out[-10000:] = out[-10000:]*fade_out\n\t        fp = os.path.join(os.path.dirname(__file__), \"temp-{}.wav\".format(mode))\n\t        audiofile.write(fp, out, sampling_rate)\n\t        return fp\n\t    def select_effect(self, params):\n\t        effect = Compose([\n\t            # AddGaussianNoise(min_amplitude=params[\"min_amplitude\"], max_amplitude=params[\"max_amplitude\"], p=params[\"p\"]),\n", "            TimeStretch(min_rate=params[\"min_rate\"], max_rate=params[\"max_rate\"], p=params[\"p\"]),\n\t            PitchShift(min_semitones=params[\"min_semitones\"], max_semitones=params[\"max_semitones\"], p=params[\"p\"]),\n\t            Shift(min_fraction=params[\"min_fraction\"], max_fraction=params[\"max_fraction\"], p=params[\"p\"])\n\t        ])\n\t        return effect\n\tdef random_wave(length, sr=48000, base_freq=60, change_every=1, step=50):\n\t    change_every = length // change_every\n\t    x_samples = np.arange(0, change_every*step, step)\n\t    freq_samples = (np.random.random(x_samples.shape)*2*step - step) + base_freq\n\t    x = np.arange(length * sr) / sr\n", "    dx = 2*np.pi*x\n\t    interpolation = interp1d(x_samples, freq_samples, kind='quadratic')\n\t    freq = interpolation(x)\n\t    x_plot = freq*dx  # Cumsum freq * change in x\n\t    y = np.sin(x_plot)\n\t    return y\n\tclass GrammarGeneration:\n\t    def __init__(self, output_midi_port=\"iM/ONE 1\", generation_length=3600, mem_length=10):\n\t        self.generation_length = generation_length\n\t        self.mem_length = mem_length\n", "        self.output_midi_port = output_midi_port\n\t        # Download audio files\n\t        save_path = self.download_files()\n\t        # Initialize Background Audio\n\t        self.background_audio_path = self.generate_background_audio(save_path)\n\t        # Initialize Audio grammar\n\t        self.audio_grammar = audio_grammar(save_path)\n\t        # Initialize thread for midi generation\n\t        self.midi_thread = threading.Thread(target=generate_and_send_midi, args=(grammar, output_midi_port, generation_length, 10, False))\n\t        # Initialize effects\n", "        self.effects = EffectClass()\n\t        # Start midi generation\n\t        self.midi_thread.start()\n\t        # Start audio generation\n\t        self.start_audio_generation()\n\t    def download_files(self):\n\t        \"\"\"\n\t        Download audio files from dropbox\n\t        Returns:\n\t            save_path: path to the downloaded files\n", "        \"\"\"\n\t        url = \"https://www.dropbox.com/sh/xqrkq4ka8a5tvxa/AAA7j8JYvyECOfsPRdNc4Fsea?dl=1\"\n\t        if not os.path.exists(os.path.join(os.path.dirname(__file__), \"audio_files\")):\n\t            import requests\n\t            save_path = os.path.join(os.path.dirname(__file__), \"audio_files\", \"audio_files.zip\")\n\t            # create directory if it doesn't exist'\n\t            if not os.path.exists(os.path.dirname(save_path)):\n\t                os.makedirs(os.path.dirname(save_path))\n\t            r = requests.get(url, stream=True)\n\t            with open(save_path, 'wb') as fd:\n", "                for chunk in r.iter_content(chunk_size=128):\n\t                    fd.write(chunk)\n\t            # unzip files\n\t            with zipfile.ZipFile(save_path, 'r') as zip_ref:\n\t                zip_ref.extractall(os.path.join(os.path.dirname(__file__), \"audio_files\"))\n\t        return os.path.join(os.path.dirname(__file__), \"audio_files\")\n\t    def generate_background_audio(self, save_path, sr=48000):\n\t        # Fixed background audio time in seconds (5 minutes)\n\t        fixed_backround_audio_time = 3*60\n\t        fixed_backround_audio_time = fixed_backround_audio_time if fixed_backround_audio_time < self.generation_length else self.generation_length\n", "        # Initialize numpy array with gaussian noise\n\t        x = np.random.normal(0, 1, fixed_backround_audio_time * sr)\n\t        # Filter out values lower than 0.9 to create random impulse responses\n\t        x[x < 2.5] = 0\n\t        # create a wave of random frequency that changes every second\n\t        waves = np.array([\n\t            random_wave(fixed_backround_audio_time, sr=sr, base_freq=i, step=i//5)\n\t            for i in np.random.randint(40, 1500, 5)\n\t        ])\n\t        # Create filter of sawtooth impulse response\n", "        b = np.linspace(1, 0, sr//20, endpoint=False)\n\t        s = signal.lfilter(b, [1], x)\n\t        # add all together\n\t        waves = np.sum(waves, axis=0)\n\t        # normalize\n\t        waves = waves / np.max(np.abs(waves))\n\t        s = waves * s\n\t        # Normalize s between -1 and 1\n\t        s = s / np.max(np.abs(s))\n\t        # Apply other audio effects\n", "        effects = Compose([\n\t            # AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.01, p=0.5),\n\t            # ApplyImpulseResponse(, p=0.5),\n\t            # AirAbsorption(min_temperature=10, max_temperature=20, p=0.2),\n\t            GainTransition(min_gain_in_db=-20, max_gain_in_db=0, p=0.5),\n\t            TimeMask(min_band_part=0.01, max_band_part=0.1, fade=True, p=0.8),\n\t        ])\n\t        # Apply effects\n\t        s = effects(s, sr)\n\t        # Normalize s between -1 and 1\n", "        s = s / np.max(np.abs(s))\n\t        # Do a Fade-in of 1s\n\t        ramp = np.linspace(0., 1., sr)\n\t        s[:sr] = s[:sr] * ramp\n\t        # Scale audio to 0.5\n\t        board = Pedalboard([\n\t            # Compressor(threshold_db=-50, ratio=25),\n\t            # Gain(gain_db=30),\n\t            Chorus(),\n\t            Phaser(),\n", "            Reverb(room_size=0.5),\n\t        ])\n\t        s = board(s, sr)\n\t        s = s / np.max(np.abs(s))\n\t        s = s * 0.01\n\t        # Save audio file\n\t        audiofile.write(os.path.join(save_path, \"background.wav\"), s, sr)\n\t        ApplyImpulseResponse(os.path.join(save_path, \"background.wav\"), p=0.5)\n\t        return os.path.join(save_path, \"background.wav\")\n\t    def start_audio_generation(self):\n", "        CHUNK = 1024\n\t        wf = wave.open(self.background_audio_path, 'rb')\n\t        p = pyaudio.PyAudio()\n\t        background_stream = p.open(\n\t            format=p.get_format_from_width(wf.getsampwidth()),\n\t            channels=wf.getnchannels(),\n\t            rate=wf.getframerate(),\n\t            output=True,\n\t        )\n\t        audio_stream = p.open(\n", "            format=p.get_format_from_width(wf.getsampwidth()), channels=1, rate=48000, output=True)\n\t        audio_stream_p = p.open(\n\t            format=p.get_format_from_width(wf.getsampwidth()), channels=1, rate=48000, output=True\n\t        )\n\t        background_data = wf.readframes(CHUNK)\n\t        running_audio_data = b''\n\t        running_audio_data_p = b''\n\t        start_time = time.time()\n\t        audio_memory = []\n\t        while time.time() - start_time < self.generation_length:\n", "            # If background audio is finished, rewind\n\t            # Background audio is 5 minutes long to reduce memory and cpu usage\n\t            if background_data == b'':\n\t                wf.rewind()\n\t                background_data = wf.readframes(CHUNK)\n\t            background_stream.write(background_data)\n\t            background_data = wf.readframes(CHUNK)\n\t            if running_audio_data == b'':\n\t                audio_path, audio_memory = generate_audio_grammar(self.audio_grammar, self.effects, audio_memory, self.mem_length, mode=1)\n\t                wf_audio = wave.open(audio_path, 'rb')\n", "                if random.randint(0, 100) < 80:\n\t                    print(\"Playing audio clip.\")\n\t                    running_audio_data = wf_audio.readframes(CHUNK)\n\t            else:\n\t                audio_stream.write(running_audio_data)\n\t                running_audio_data = wf_audio.readframes(CHUNK)\n\t            if running_audio_data_p == b'':\n\t                audio_path, audio_memory = generate_audio_grammar(self.audio_grammar, self.effects, audio_memory, self.mem_length, mode=2)\n\t                wf_audio_p = wave.open(audio_path, 'rb')\n\t                if random.randint(0, 100) < 80:\n", "                    print(\"Playing audio clip.\")\n\t                    running_audio_data_p = wf_audio_p.readframes(CHUNK)\n\t            else:\n\t                audio_stream.write(running_audio_data_p)\n\t                running_audio_data_p = wf_audio_p.readframes(CHUNK)\n\t        background_stream.stop_stream()\n\t        audio_stream.stop_stream()\n\t        audio_stream_p.stop_stream()\n\t        background_stream.close()\n\t        audio_stream.close()\n", "        audio_stream_p.close()\n\t        p.terminate()\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    args = argparse.ArgumentParser()\n\t    args.add_argument(\"--generation_length\", type=int, default=2200, help=\"Length of the generated audio in seconds\")\n\t    args.add_argument(\"--mem_length\", type=int, default=10, help=\"Length of the memory of the audio and midi generation\")\n\t    args.add_argument(\"--output_midi_port\", type=str, default=\"iM/ONE 1\", help=\"Name of the midi port to send the generated midi to\")\n\t    args = args.parse_args()\n\t    GrammarGeneration(generation_length=args.generation_length, mem_length=args.mem_length,\n", "                      output_midi_port=args.output_midi_port)\n"]}
{"filename": "paow/23_08_30/merge_midis.py", "chunked_list": ["import os\n\timport partitura as pt\n\timport numpy as np\n\tMidi_path = r\"PATH0\"\n\tOut_path = r\"PATH1\"\n\tcur_time = 0\n\tnote_array = []\n\tfor l in range(10):\n\t    mid = pt.load_performance_midi(os.path.join(Midi_path, 'other_cutsample_{}_basic_pitch.mid'.format(l)))\n\t    print(l, mid.note_array()[0],mid.note_array()[-1] )\n", "    for note in mid[0].notes:\n\t        note['note_on'] += cur_time\n\t        note['note_off'] += cur_time\n\t        note['sound_off'] += cur_time\n\t        note_array.append(tuple([note['midi_pitch'], note['note_on'], note['note_off'] - note['note_on'], note['velocity']]))\n\t    cur_time += 60*5\n\tperformed_part = pt.performance.PerformedPart.from_note_array(np.array(note_array,dtype=[(\"pitch\", \"i4\"),\n\t           (\"onset_sec\", \"f4\"),\n\t           (\"duration_sec\", \"f4\"),\n\t           (\"velocity\", \"i4\")\n", "           ]))\n\tpt.save_performance_midi(performed_part, os.path.join(Out_path, \"other.mid\"))"]}
{"filename": "paow/23_08_30/cut_audio.py", "chunked_list": ["import numpy as np\n\timport os\n\timport librosa\n\timport soundfile as sf\n\tsegment_length=60*5\n\tin_path = r\"PATH0\"\n\tout_path = r\"PATH1\"\n\t# LOAD\n\tsignal = librosa.load(in_path)\n\tsr = signal[1]\n", "sig_len = signal[0].shape[0]\n\tsnippets = int(np.ceil(sig_len / (sr*segment_length)))\n\tfor i in range(snippets):\n\t    sf.write(os.path.join(out_path, 'other_cutsample_{}.wav'.format(i)),\n\t             signal[0][(i)*sr*segment_length:(i+1)*sr*segment_length] , sr)"]}
{"filename": "paow/23_06_28/main.py", "chunked_list": ["from model import PopMusicTransformer\n\timport os\n\tos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\timport random \n\tdef main():\n\t    # declare model\n\t    model = PopMusicTransformer(\n\t        checkpoint='checkpoint',\n\t        is_training=False)\n\t    # generate from scratch\n", "    for i in range(500):\n\t        #randomly choose a tempreture between 1 and 2\n\t        temp = random.uniform(1, 2)\n\t        model.generate(\n\t            n_target_bar=16,\n\t            temperature=temp,\n\t            topk=5,\n\t            output_path='./2nd_round/gen_from_26_3_0_temp{}_{}.midi'.format(temp, i),\n\t            prompt=None)\n\t    # close model\n", "    model.close()\n\tif __name__ == '__main__':\n\t    main()"]}
{"filename": "paow/23_06_28/merge_midis.py", "chunked_list": ["import os\n\timport partitura as pt\n\timport numpy as np\n\tMidi_address = 'MIDIs/'\n\tdir = os.listdir(Midi_address)\n\tcur_time = 0\n\tnote_array = []\n\tfor file in dir:\n\t    mid = pt.load_performance_midi(Midi_address+file)\n\t    for note in mid[0].notes:\n", "        note['note_on'] += cur_time\n\t        note['note_off'] += cur_time\n\t        note['sound_off'] += cur_time\n\t        note_array.append(tuple([note['midi_pitch'], note['note_on'], note['note_off'] - note['note_on'], note['velocity']]))\n\t    cur_time = mid[0].notes[-1]['note_off'] + 3\n\tperformed_part = pt.performance.PerformedPart.from_note_array(np.array(note_array,dtype=[(\"pitch\", \"i4\"),\n\t           (\"onset_sec\", \"f4\"),\n\t           (\"duration_sec\", \"f4\"),\n\t           (\"velocity\", \"i4\")\n\t           ]))\n", "pt.save_performance_midi(performed_part, 'wannabe_mozert.mid')"]}
{"filename": "paow/utils/partitura_utils.py", "chunked_list": ["import partitura as pt\n\timport numpy as np\n\tdef addnote(midipitch, part, voice, start, end, idx):\n\t    \"\"\"\n\t    adds a single note by midiptich to a part\n\t    \"\"\"\n\t    offset = midipitch%12\n\t    octave = int(midipitch-offset)/12\n\t    name = [(\"C\",0),\n\t            (\"C\",1),\n", "            (\"D\",0),\n\t            (\"D\",1),\n\t            (\"E\",0),\n\t            (\"F\",0),\n\t            (\"F\",1),\n\t            (\"G\",0),\n\t            (\"G\",1),\n\t            (\"A\",0),\n\t            (\"A\",1),\n\t            (\"B\",0)]\n", "    # print( id, start, end, offset)\n\t    step, alter = name[int(offset)]\n\t    part.add(pt.score.Note(id='n{}'.format(idx), step=step, \n\t                        octave=int(octave), alter=alter, voice=voice), \n\t                        start=start, end=end)\n\tdef partFromProgression(prog, \n\t                        quarter_duration = 4,\n\t                        rhythm = None):\n\t    part = pt.score.Part('P0', 'part from progression', quarter_duration=quarter_duration)\n\t    if rhythm is None:\n", "        rhythm = [(i*quarter_duration, (i+1)*quarter_duration) for i in range(len(prog.chords))]\n\t    for i, c in enumerate(prog.chords):\n\t        for j, pitch in enumerate(c.pitches):\n\t            addnote(pitch, part, j, rhythm[i][0], rhythm[i][1], str(j)+str(i))\n\t    return part\n\tdef parttimefromrekorder(na, \n\t                         quarter_duration = 4,\n\t                         quarters = 8,\n\t                         num_frames = 8,\n\t                         rhythm = None):\n", "    positions = quarter_duration * quarters\n\t    sec_per_div = 10/quarters/quarter_duration\n\t    if rhythm is None:\n\t        interval = positions//num_frames\n\t        rhythm = [(i*interval, (i+1)*interval) for i in range(num_frames)]\n\t    frames = list()\n\t    for k in range(num_frames):\n\t        frames.append(na[\"pitch\"][np.logical_and(\n\t            na[\"onset_sec\"] < rhythm[k][1]*sec_per_div, \n\t            na[\"onset_sec\"] >= rhythm[k][0]*sec_per_div)])\n", "    # time_per_div = (num_frames/10) / quarter_duration\n\t    na[\"onset_sec\"] = np.round(na[\"onset_sec\"]/sec_per_div)\n\t    na[\"duration_sec\"] = np.round(na[\"duration_sec\"]/sec_per_div)\n\t    na[\"duration_sec\"][na[\"duration_sec\"] < 1] = 1   \n\t    return na, frames\n\tdef addmelody2part(part, na, quarter_duration = 4):\n\t    for j, note in enumerate(na):\n\t            addnote(note[\"pitch\"], part, j, \n\t                    note[\"onset_sec\"], \n\t                    note[\"duration_sec\"]+note[\"onset_sec\"], \n", "                    str(j)+\"_melody\")\n\tdef progression_and_melody_to_part(progression, \n\t                                   melody,\n\t                                   quarter_duration = 4,\n\t                                   rhythm = None):\n\t    \"\"\"\n\t    converts a progression and a melody to a partitura part\n\t    \"\"\"\n\t    part = partFromProgression(progression,\n\t                               quarter_duration = quarter_duration,\n", "                               rhythm = rhythm\n\t                               )\n\t    na, _ = parttimefromrekorder(melody,\n\t                                 quarter_duration = quarter_duration,\n\t                                 num_frames = len(progression.chords)\n\t                                )\n\t    addmelody2part(part, na)\n\t    return part"]}
{"filename": "paow/utils/rhythm.py", "chunked_list": ["import numpy as np\n\tdef euclidean(cycle = 16, pulses = 4, offset= 0):\n\t    \"\"\"\n\t    compute an Euclidean rhythm\n\t    \"\"\"\n\t    rhythm = []\n\t    # pulses = how_many_notes-1\n\t    bucket = cycle-pulses\n\t    timepoint = 0\n\t    for step in range(cycle):\n", "        # print(bucket, rhythm) \n\t        bucket = bucket + pulses\n\t        if (bucket >= cycle):\n\t            bucket = bucket - cycle\n\t            rhythm.append(timepoint)\n\t        timepoint += 1\n\t    rhythm_array = np.array(rhythm)\n\t    rhythm_array = (rhythm_array + offset) % cycle\n\t    rhythm_array = np.sort( rhythm_array)\n\t    return rhythm_array\n", "if __name__ == \"__main__\":\n\t    a = euclidean()"]}
{"filename": "paow/utils/pitch.py", "chunked_list": ["import numpy as np\n\tfrom itertools import product\n\timport string\n\timport random\n\tdef randomword(length):\n\t    \"\"\"\n\t    a random character generator\n\t    \"\"\"\n\t    letters = string.ascii_lowercase\n\t    return ''.join(random.choice(letters) for i in range(length))\n", "def cycle_distance(u,v):\n\t    \"\"\"\n\t    pitch class distance between two values u and v\n\t    \"\"\"\n\t    a = np.sqrt(((u%12-v%12)%12)**2)\n\t    b = np.sqrt(((v%12-u%12)%12)**2)\n\t    return np.min([a, b])\n\tdef chordDistance(c1, c2):\n\t    \"\"\"\n\t    compute the minimal pitch distance between to chords\n", "    when every note of the second chord\n\t    can be transposed by up to -1 / +1 octaves.\n\t    assumes input chords as pitch-ordered np.arrays. \n\t    Parameters\n\t    ----------\n\t    c1 : np.array\n\t        first chord\n\t    c2 : np.array\n\t        second chord\n\t    Returns \n", "    -------\n\t    best_adds : np.array\n\t        the best transposition for each note of the second chord\n\t    best_total : int\n\t        the total distance between the two chords\n\t    \"\"\"\n\t    l = min(c1.shape[0],c2.shape[0])\n\t    c1 = c1[:l]\n\t    c2 = c2[:l]\n\t    local_adds = dict()\n", "    for comb in product(np.arange(-1,1), repeat =  l):\n\t        c=np.array(comb)\n\t        new_c2 = c2 + 12* c\n\t        total = np.sum(np.abs(new_c2-c1))\n\t        local_adds[total] = c\n\t    best_total = np.min(list(local_adds.keys()))\n\t    best_adds = local_adds[best_total]\n\t    return  best_adds, best_total\n\tclass Chord:\n\t    \"\"\"\n", "    the Chord class representing a chord made of 3-4 notes of a scale.\n\t    For diatonic scales the intervals between notes are thirds.\n\t    Parameters\n\t    ----------\n\t    offset : int\n\t        the MIDI pitch of scale degree 0\n\t    scale : np.array\n\t        the scale from which the chord is built\n\t    how_many : int\n\t        the number of notes in the chord\n", "    root_id : int\n\t        the index of the root in the scale\n\t    \"\"\"\n\t    def __init__(self, \n\t                 offset = 48, #np.random.randint(48,60)-12, \n\t                 scale = np.array([0,2,4,5,7,9,11]),\n\t                 how_many = None,\n\t                 root_id = None\n\t                ):\n\t        self.offset = offset\n", "        self.scale = scale\n\t        if root_id is None:\n\t            self.root_id = np.random.randint(self.scale.shape[0])\n\t        else:\n\t            self.root_id = root_id\n\t        self.root = self.offset + self.scale[self.root_id]\n\t        if how_many is None:\n\t            self.how_many = np.random.randint(3,5)\n\t        else:\n\t            self.how_many = np.clip(how_many,3,4)\n", "        if self.how_many == 3:\n\t            self.jumps = [2,2,3]\n\t        elif self.how_many == 4:\n\t            self.jumps = [2,2,2,1]\n\t        self.jumps_cs = np.cumsum([0]+ self.jumps) \n\t        self.inversion = 0\n\t        self.inversion_jumps = None\n\t        self.inversion_jumps_cs = None\n\t        self.all_ids = None\n\t        self.pitches = None\n", "        self.pitch_classes = None\n\t        self.pitch_classes_relative = None\n\t        self.repitch = [0,0]\n\t        self.compute_pitch()\n\t        self.name = self.get_name()\n\t    def get_name(self):\n\t        \"\"\"\n\t        rudimentary naming convention for chords\n\t        no difference between maj7 and dominant 7.\n\t        suffix for inversions and chromatic alterations.\n", "        \"\"\"\n\t        root_names = [\"C\",\"C#\",\"D\",\"D#\",\"E\",\"F\",\"F#\",\"G\",\"G#\",\"A\",\"A#\",\"B\"]\n\t        root_name = root_names[self.root%12]\n\t        first_jump =  cycle_distance(self.scale[(self.root_id+self.jumps[0])%self.scale.shape[0]],\n\t                                        self.scale[self.root_id])\n\t        if first_jump == 3:\n\t            mod = \"m\"\n\t        elif first_jump == 4:\n\t            mod = \"M\"\n\t        else:\n", "            mod = \"?\"\n\t        if self.how_many == 3:\n\t            typ = \"\"\n\t        elif self.how_many == 4:\n\t            typ = \"7\"\n\t        repitch_names = {1:\"#\", -1:\"b\"}\n\t        if self.repitch != [0,0]:\n\t            chromatic = \"_\" + str(self.repitch[0]) +repitch_names[self.repitch[1]]\n\t        else:\n\t            chromatic = \"\"\n", "        name = root_name + mod + typ + \"_\" + str(self.inversion) + chromatic\n\t        return name\n\t    def compute_pitch(self):\n\t        self.inversion_jumps = self.jumps[self.inversion:]+self.jumps[:self.inversion]\n\t        self.inversion_jumps_cs = np.cumsum([0]+ self.inversion_jumps) \n\t        self.all_ids = np.array([(self.root_id+self.jumps_cs[self.inversion])%self.scale.shape[0]\n\t                                 +self.inversion_jumps_cs[n] for n in range(self.how_many)])\n\t        self.pitches = self.offset + np.concatenate((self.scale, self.scale+12))[self.all_ids]\n\t        self.pitches[self.repitch[0]] += self.repitch[1]\n\t        self.pitch_classes = self.pitches%12\n", "        self.pitch_classes_relative = (self.pitches-self.offset)%12\n\t        self.name = self.get_name()\n\t    def invert(self, n):\n\t        n %= self.how_many\n\t        self.inversion = n\n\t        self.compute_pitch()\n\t    def add_repitch(self, idx, mod):\n\t        mod = np.clip(mod,-1,1)\n\t        idx %= self.how_many\n\t        self.repitch = [idx, mod]\n", "        self.compute_pitch()\n\tclass Progression:\n\t    \"\"\"\n\t    the Progression class representing a sequence of chords\n\t    \"\"\"\n\t    def __init__(self,\n\t                 number_of_chords = 8, \n\t                 chords = None):\n\t        if chords is not None:\n\t            self.chords = chords\n", "        else:\n\t            self.chords = [Chord() for c in range(number_of_chords)]\n\t        self.number_of_chords = number_of_chords\n\t        self.id = randomword(10)\n\t    def copy(self):\n\t        return Progression(\n\t                number_of_chords = self.number_of_chords,\n\t                chords= self.chords\n\t                )\n\t    def join(self, \n", "             another,\n\t             idx = None):\n\t        \"\"\"\n\t        create two new progressions by joining two existing ones\n\t        where the split of chords is defined by an index array\n\t        \"\"\"\n\t        if another.number_of_chords != self.number_of_chords:\n\t            raise ValueError(\"progressions must have the same number of chords\")\n\t        if idx is None:\n\t            idx = np.unique(np.random.randint(0,self.number_of_chords,self.number_of_chords//2))\n", "        new_progression = Progression(number_of_chords = self.number_of_chords)\n\t        new_another_progression = Progression(number_of_chords = self.number_of_chords)\n\t        for k in range(self.number_of_chords):\n\t            if k in idx:\n\t                new_progression.chords[k] = self.chords[k]\n\t                new_another_progression.chords[k] = another.chords[k]\n\t            else:\n\t                new_progression.chords[k] = another.chords[k]\n\t                new_another_progression.chords[k] = self.chords[k]\n\t        return new_progression, new_another_progression\n", "if __name__ == \"__main__\":\n\t    a = Chord(how_many = 3, root_id = 0)\n"]}
{"filename": "paow/utils/__init__.py", "chunked_list": ["#!/usr/bin/python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis module contains utility functions for the paow package.\n\t\"\"\"\n\tfrom .midi import (MidiRouter, Sequencer, MidiInputThread)\n\tfrom .rhythm import (euclidean)\n\tfrom .pitch import (Chord, Progression, cycle_distance, chordDistance)\n\tfrom .partitura_utils import (progression_and_melody_to_part, parttimefromrekorder, partFromProgression)"]}
{"filename": "paow/utils/midi.py", "chunked_list": ["import partitura as pt  \n\timport mido\n\timport numpy as np\n\timport time\n\timport threading\n\timport multiprocessing\n\tfrom collections import defaultdict\n\timport partitura as pt\n\timport winsound\n\tclass MidiInputThread(threading.Thread):\n", "    def __init__(\n\t        self,\n\t        port,\n\t        queue = list(),\n\t        init_time=None,\n\t    ):\n\t        threading.Thread.__init__(self)\n\t        self.midi_in = port\n\t        self.init_time = init_time\n\t        self.duration = 10\n", "        self.listen = False\n\t        self.queue = queue\n\t        self.beep_interval = self.duration / 8\n\t    def run(self):\n\t        self.start_listening()\n\t        self.queue = list()\n\t        print(self.current_time)\n\t        for msg in self.midi_in.iter_pending():\n\t            continue\n\t        c_time = self.current_time\n", "        while c_time < 2.49:\n\t            if self.beep_interval is not None:\n\t                if c_time % self.beep_interval < 0.01 and c_time < 2.0:\n\t                    winsound.Beep(1000, 50)\n\t            c_time = self.current_time\n\t        while c_time < self.duration + 2.5:\n\t            msg = self.midi_in.poll()\n\t            if msg is not None:\n\t                self.queue.append((msg, c_time-2.5))\n\t                print(msg, c_time-2.5)\n", "            if self.beep_interval is not None:\n\t                if c_time % self.beep_interval < 0.01:\n\t                    winsound.Beep(262, 50)\n\t            c_time = self.current_time\n\t        self.stop_listening()\n\t        return self.queue\n\t    @property\n\t    def current_time(self):\n\t        \"\"\"\n\t        Get current time since starting to listen\n", "        \"\"\"\n\t        return time.perf_counter() - self.init_time\n\t    def start_listening(self):\n\t        \"\"\"\n\t        Start listening to midi input (open input port and\n\t        get starting time)\n\t        \"\"\"\n\t        print(\"start listening\")\n\t        self.listen = True\n\t        if self.init_time is None:\n", "            self.init_time = time.perf_counter()\n\t    def stop_listening(self):\n\t        \"\"\"\n\t        Stop listening to MIDI input\n\t        \"\"\"\n\t        print(\"stop listening\")\n\t        # break while loop in self.run\n\t        self.listen = False\n\t        # reset init time\n\t        self.init_time = None\n", "def midi_msg_to_pt_note(msg):\n\t    return msg\n\tdef pt_note_to_midi_msg(pt_note):\n\t    return pt_note\n\tclass MidiRouter(object):\n\t    \"\"\"\n\t    This is a class handling MIDI I/O.\n\t    It takes (partial) strings for port names as inputs\n\t    and searches for a fitting port.\n\t    The reason this is set up in this way is that\n", "    different OS tend to name/index MIDI ports differently.\n\t    Use an instance if this class (and *only* this instance)\n\t    to handle everything related to port opening, closing,\n\t    finding, and panic. Expecially Windows is very finicky\n\t    about MIDI ports and it'll likely break if ports are\n\t    handled separately.\n\t    This class can be used to:\n\t    - create a midirouter = MidiRouter(**kwargs) with\n\t    a number of (partial) port names or fluidsynths\n\t    - poll a specific port: e.g.\n", "    midirouter.input_port.poll()\n\t    - send on a specific port: e.g.\n\t    midirouter.output_port.send(msg)\n\t    - open all set ports: midirouter.open_ports()\n\t    - close all set ports: midirouter.close_ports()\n\t    - panic reset all ports: midirouter.panic()\n\t    - get the full name of the used midi ports: e.g.\n\t    midirouter.input_port_name\n\t    (DON'T use this name to open, close, etc with it,\n\t    use the midirouter functions instead)\n", "    Args:\n\t        inport_name (string):\n\t            a (partial) string for the input name.\n\t        outport_name (string):\n\t            a (partial) string for the output name.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        inport_name=None,\n\t        outport_name=None\n", "    ):\n\t        self.available_input_ports = mido.get_input_names()\n\t        print(\"Available inputs MIDI for mido\", self.available_input_ports)\n\t        self.available_output_ports = mido.get_output_names()\n\t        print(\"Available outputs MIDI for mido\", self.available_output_ports)\n\t        self.input_port_names = {}\n\t        self.output_port_names = {}\n\t        self.open_ports_list = []\n\t        # the MIDI port name the accompanion listens at (port name)\n\t        self.input_port_name = self.proper_port_name(\n", "            inport_name, True\n\t        )\n\t        # the MIDI port names / Instrument the accompanion is sent\n\t        self.output_port_name = self.proper_port_name(\n\t            outport_name, False\n\t        )\n\t        self.open_ports()\n\t        self.input_port = self.assign_ports_by_name(\n\t            self.input_port_name, input=True\n\t        )\n", "        self.output_port = self.assign_ports_by_name(\n\t            self.output_port_name, input=False\n\t        )\n\t    def proper_port_name(self, try_name, input=True):\n\t        if isinstance(try_name, str):\n\t            if input:\n\t                possible_names = [\n\t                    (i, name)\n\t                    for i, name in enumerate(self.available_input_ports)\n\t                    if try_name in name\n", "                ]\n\t            else:\n\t                possible_names = [\n\t                    (i, name)\n\t                    for i, name in enumerate(self.available_output_ports)\n\t                    if try_name in name\n\t                ]\n\t            if len(possible_names) == 1:\n\t                print(\n\t                    \"port name found for trial name: \",\n", "                    try_name,\n\t                    \"the port is set to: \",\n\t                    possible_names[0],\n\t                )\n\t                if input:\n\t                    self.input_port_names[possible_names[0][1]] = None\n\t                else:\n\t                    self.output_port_names[possible_names[0][1]] = None\n\t                return possible_names[0]\n\t            elif len(possible_names) < 1:\n", "                print(\"no port names found for trial name: \", try_name)\n\t                return None\n\t            elif len(possible_names) > 1:\n\t                print(\" many port names found for trial name: \", try_name)\n\t                if input:\n\t                    self.input_port_names[possible_names[0][1]] = None\n\t                else:\n\t                    self.output_port_names[possible_names[0][1]] = None\n\t                return possible_names[0]\n\t                # return None\n", "        elif isinstance(try_name, int):\n\t            if input:\n\t                try:\n\t                    possible_name = (try_name, self.available_input_ports[try_name])\n\t                    self.input_port_names[possible_name[1]] = None\n\t                    return possible_name\n\t                except ValueError:\n\t                    raise ValueError(f\"no input port found for index: {try_name}\")\n\t            else:\n\t                try:\n", "                    possible_name = (try_name, self.available_output_ports[try_name])\n\t                    self.output_port_names[possible_name[1]] = None\n\t                    return possible_name\n\t                except ValueError:\n\t                    raise ValueError(f\"no output port found for index: {try_name}\")\n\t        else:\n\t            return None\n\t    def open_ports_by_name(self, try_name, input=True):\n\t        if try_name is not None:\n\t            if input:\n", "                port = mido.open_input(try_name)\n\t            else:\n\t                port = mido.open_output(try_name)\n\t                # Adding eventual key release.\n\t                port.reset()\n\t            self.open_ports_list.append(port)\n\t            return port\n\t        else:\n\t            return try_name\n\t    def open_ports(self):\n", "        for port_name in self.input_port_names.keys():\n\t            if self.input_port_names[port_name] is None:\n\t                port = self.open_ports_by_name(port_name, input=True)\n\t                self.input_port_names[port_name] = port\n\t        for port_name in self.output_port_names.keys():\n\t            if self.output_port_names[port_name] is None:\n\t                port = self.open_ports_by_name(port_name, input=False)\n\t                self.output_port_names[port_name] = port\n\t    def close_ports(self):\n\t        for port in self.open_ports_list:\n", "            port.close()\n\t        self.open_ports_list = []\n\t        for port_name in self.output_port_names.keys():\n\t            self.output_port_names[port_name] = None\n\t        for port_name in self.input_port_names.keys():\n\t            self.input_port_names[port_name] = None\n\t    def panic(self):\n\t        for port in self.open_ports_list:\n\t            port.panic()\n\t    def assign_ports_by_name(self, try_name, input=True):\n", "        if try_name is not None:\n\t            if input:\n\t                return self.input_port_names[try_name[1]]\n\t            else:\n\t                return self.output_port_names[try_name[1]]\n\t        else:\n\t            return None\n\tdef play_midi_from_score(score=None, \n\t                         midirouter=None, \n\t                         quarter_duration=1, \n", "                         default_velocity=60,\n\t                         print_messages=False):\n\t    \"\"\"\n\t    Play a score using a midirouter\n\t    \"\"\"\n\t    if midirouter is None:\n\t        print(\"No midirouter provided\")\n\t        return\n\t    if score is None:\n\t        print(\"No score provided\")\n", "        return\n\t    note_array = score.note_array()\n\t    onset_sec = note_array[\"onset_quarter\"] * quarter_duration\n\t    offset_sec = note_array[\"duration_quarter\"] * quarter_duration + onset_sec\n\t    noteon_messages = np.array([(\"note_on\", p, onset_sec[i]) for i, p in enumerate(note_array[\"pitch\"])], dtype=[(\"msg\", \"<U10\"), (\"pitch\", int), (\"time\", float)])\n\t    noteoff_messages = np.array([(\"note_off\", p, offset_sec[i]) for i, p in enumerate(note_array[\"pitch\"])], dtype=[(\"msg\", \"<U10\"), (\"pitch\", int), (\"time\", float)])\n\t    messages = np.concatenate([noteon_messages, noteoff_messages])\n\t    messages = np.sort(messages, order=\"time\")\n\t    timediff = np.diff(messages[\"time\"])\n\t    output_port = midirouter.output_port\n", "    for i, msg in enumerate(messages):\n\t        if i == 0:\n\t            pass\n\t        else:\n\t            time.sleep(timediff[i-1])\n\t        m = mido.Message(msg[\"msg\"], note=msg[\"pitch\"], velocity=default_velocity)\n\t        output_port.send(m)\n\t        if print_messages:\n\t            print(m)\n\tclass Sequencer(multiprocessing.Process):\n", "    def __init__(\n\t        self,\n\t        outport_name=\"seq\",\n\t        queue=None,\n\t        *args, \n\t        **kwargs):\n\t        super(Sequencer, self).__init__(*args, **kwargs)\n\t        self.queue = queue\n\t        self.outport_name = outport_name\n\t        self.router = None\n", "        # sequencer variables\n\t        self.playhead = 0\n\t        self.playing = False\n\t        self.start_time = 0\n\t        self.tempo = 120#120\n\t        self.quarter_per_ns = self.tempo / ( 60 * 1e9)\n\t        self.next_time = 0\n\t        # music variables\n\t        self.default_velocity = 60\n\t        self.loop_start_quarter = 0\n", "        self.loop_end_quarter = 8\n\t        self.looped_notes = None\n\t        self.onset_quarter = None\n\t        self.offset_quarter = None\n\t        self.messages = None\n\t        self.message_times = None\n\t    def up(self, args):\n\t        self.queue.put(args)\n\t    def update_part(self, part):\n\t        pass\n", "        note_array = part.note_array()\n\t        mask_lower = note_array[\"onset_quarter\"] >= self.loop_start_quarter           \n\t        mask_upper = note_array[\"onset_quarter\"] < self.loop_end_quarter\n\t        mask = np.all((mask_lower, mask_upper), axis=0)\n\t        self.looped_notes = note_array[mask]\n\t        self.onset_quarter = note_array[mask][\"onset_quarter\"]\n\t        self.offset_quarter = np.clip(note_array[mask][\"duration_quarter\"] + self.onset_quarter, \n\t                                      self.loop_start_quarter, \n\t                                      self.loop_end_quarter-0.1)\n\t        self.messages = defaultdict(list)\n", "        self.message_times = np.array([])\n\t        for i, note in enumerate(self.looped_notes):\n\t            on = mido.Message('note_on', \n\t                         note=note[\"pitch\"], \n\t                         velocity=self.default_velocity, \n\t                         time=0)\n\t            off = mido.Message('note_off',\n\t                            note=note[\"pitch\"],\n\t                            velocity=0,\n\t                            time=0)\n", "            self.messages[self.onset_quarter[i]].append(on)\n\t            self.messages[self.offset_quarter[i]].append(off)\n\t        self.message_times = np.sort(np.array(list(self.messages.keys())))\n\t        self.next_time = self.message_times[0]\n\t        print(self.message_times)\n\t    def run(self):\n\t        self.start_time = time.perf_counter_ns()\n\t        self.router = MidiRouter(outport_name = self.outport_name)\n\t        self.playing = True\n\t        print(\"Sequencer started\")        \n", "        while self.playing:\n\t            try: \n\t                args = self.queue.get_nowait()\n\t                # print(args.note_array())\n\t                self.update_part(args)\n\t            except:\n\t                pass\n\t            current_time = time.perf_counter_ns()\n\t            elapsed_time = current_time - self.start_time\n\t            elapsed_quarter = elapsed_time * self.quarter_per_ns\n", "            self.playhead = elapsed_quarter % (self.loop_end_quarter - self.loop_start_quarter)\n\t            if self.playhead >= self.next_time - 0.02 and \\\n\t                self.playhead < self.next_time + 0.1 and \\\n\t                self.messages is not None:\n\t                for msg in self.messages[self.next_time]:\n\t                    self.router.output_port.send(msg)\n\t                # this_time = self.next_time\n\t                idx, = np.where(self.message_times == self.next_time)\n\t                i = idx[0]\n\t                self.next_time = self.message_times[(i+1)%len(self.message_times)]\n", "                # print(\"sent\", msg, i, self.next_time, self.playhead, self.message_times)\n\t                # time.sleep((self.next_time - this_time) / 2* self.quarter_per_ns * 1e9)\n\t            else:\n\t                time.sleep(0.02)\n\t    # def stop(self):\n\t    #     self.playing = False\n\t    #     self.join()\n\tdef addnote(midipitch, part, \n\t            voice = 1, \n\t            start = 0, \n", "            end = 1, \n\t            idx = 0):\n\t    \"\"\"\n\t    adds a single note by midipitch to a part\n\t    \"\"\"\n\t    step, alter, octave = pt.utils.music.midi_pitch_to_pitch_spelling(midipitch)\n\t    part.add(pt.score.Note(id='n{}'.format(idx), step=step, \n\t                        octave=int(octave), alter=alter, voice=voice, staff=int(voice)),\n\t                        start=start, end=end)\n\tif __name__ == \"__main__\":\n", "    part = pt.load_musicxml(pt.EXAMPLE_MUSICXML)[0]\n\t    queue =multiprocessing.Queue()\n\t    s = Sequencer(queue=queue,\n\t                  outport_name=\"iM/ONE\")\n\t    s.start()\n\t    time.sleep(2)\n\t    s.up(part)\n\t    # addnote(97, part, start=3, end=4, idx=100)\n\t    # s.up(part)\n\t    # time.sleep(4)\n", "    # s.terminate()\n\t    # s.join()\n"]}
{"filename": "paow/23_06_21/transcribe.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis experiment replays automatically transcribed solo piano performances.\n\tThis script contains the transcription functionality.\n\tIt is *not* standalone, but copied from the ATEPP repository:\n\thttps://github.com/BetsyTang/ATEPP/\n\tand specifically the inference part of the transcription model:\n\thttps://github.com/BetsyTang/ATEPP/blob/master/piano_transcription-master/pytorch/inference.py\n\tClone this repository to create your own transcriptions.\n", "Enjoy!\n\t\"\"\"\n\timport os\n\timport sys\n\tsys.path.insert(1, os.path.join(sys.path[0], '../utils'))\n\timport glob\n\timport numpy as np\n\timport argparse\n\timport h5py\n\timport math\n", "import time\n\timport librosa\n\timport logging\n\timport matplotlib.pyplot as plt\n\timport torch\n\tfrom utilities import (create_folder, get_filename, RegressionPostProcessor, \n\t    OnsetsFramesPostProcessor, write_events_to_midi, load_audio)\n\tfrom models import Note_pedal, Regress_onset_offset_frame_velocity_CRNN\n\tfrom pytorch_utils import move_data_to_device, forward\n\timport config\n", "class PianoTranscription(object):\n\t    def __init__(self, model_type, checkpoint_path=None, \n\t        segment_samples=16000*10, device=torch.device('cuda'), \n\t        post_processor_type='regression'):\n\t        \"\"\"Class for transcribing piano solo recording.\n\t        Args:\n\t          model_type: str\n\t          checkpoint_path: str\n\t          segment_samples: int\n\t          device: 'cuda' | 'cpu'\n", "        \"\"\"\n\t        if 'cuda' in str(device) and torch.cuda.is_available():\n\t            self.device = 'cuda'\n\t        else:\n\t            self.device = 'cpu'\n\t        self.segment_samples = segment_samples\n\t        self.post_processor_type = post_processor_type\n\t        self.frames_per_second = config.frames_per_second\n\t        self.classes_num = config.classes_num\n\t        self.onset_threshold = 0.3\n", "        self.offset_threshod = 0.3\n\t        self.frame_threshold = 0.1\n\t        self.pedal_offset_threshold = 0.2\n\t        # Build model\n\t        Model = eval(model_type)\n\t        self.model = Model(frames_per_second=self.frames_per_second, \n\t            classes_num=self.classes_num)\n\t        # Load model\n\t        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n\t        self.model.load_state_dict(checkpoint['model'], strict=False)\n", "        # Parallel\n\t        if 'cuda' in str(self.device):\n\t            self.model.to(self.device)\n\t            print('GPU number: {}'.format(torch.cuda.device_count()))\n\t            self.model = torch.nn.DataParallel(self.model)\n\t        else:\n\t            print('Using CPU.')\n\t    def transcribe(self, audio, midi_path):\n\t        \"\"\"Transcribe an audio recording.\n\t        Args:\n", "          audio: (audio_samples,)\n\t          midi_path: str, path to write out the transcribed MIDI.\n\t        Returns:\n\t          transcribed_dict, dict: {'output_dict':, ..., 'est_note_events': ..., \n\t            'est_pedal_events': ...}\n\t        \"\"\"\n\t        audio = audio[None, :]  # (1, audio_samples)\n\t        # Pad audio to be evenly divided by segment_samples\n\t        audio_len = audio.shape[1]\n\t        pad_len = int(np.ceil(audio_len / self.segment_samples)) \\\n", "            * self.segment_samples - audio_len\n\t        audio = np.concatenate((audio, np.zeros((1, pad_len))), axis=1)\n\t        # Enframe to segments\n\t        segments = self.enframe(audio, self.segment_samples)\n\t        \"\"\"(N, segment_samples)\"\"\"\n\t        # Forward\n\t        output_dict = forward(self.model, segments, batch_size=1)\n\t        \"\"\"{'reg_onset_output': (N, segment_frames, classes_num), ...}\"\"\"\n\t        # Deframe to original length\n\t        for key in output_dict.keys():\n", "            output_dict[key] = self.deframe(output_dict[key])[0 : audio_len]\n\t        \"\"\"output_dict: {\n\t          'reg_onset_output': (segment_frames, classes_num), \n\t          'reg_offset_output': (segment_frames, classes_num), \n\t          'frame_output': (segment_frames, classes_num), \n\t          'velocity_output': (segment_frames, classes_num), \n\t          'reg_pedal_onset_output': (segment_frames, 1), \n\t          'reg_pedal_offset_output': (segment_frames, 1), \n\t          'pedal_frame_output': (segment_frames, 1)}\"\"\"\n\t        # Post processor\n", "        if self.post_processor_type == 'regression':\n\t            \"\"\"Proposed high-resolution regression post processing algorithm.\"\"\"\n\t            post_processor = RegressionPostProcessor(self.frames_per_second, \n\t                classes_num=self.classes_num, onset_threshold=self.onset_threshold, \n\t                offset_threshold=self.offset_threshod, \n\t                frame_threshold=self.frame_threshold, \n\t                pedal_offset_threshold=self.pedal_offset_threshold)\n\t        elif self.post_processor_type == 'onsets_frames':\n\t            \"\"\"Google's onsets and frames post processing algorithm. Only used \n\t            for comparison.\"\"\"\n", "            post_processor = OnsetsFramesPostProcessor(self.frames_per_second, \n\t                self.classes_num)\n\t        # Post process output_dict to MIDI events\n\t        (est_note_events, est_pedal_events) = \\\n\t            post_processor.output_dict_to_midi_events(output_dict)\n\t        # Write MIDI events to file\n\t        if midi_path:\n\t            write_events_to_midi(start_time=0, note_events=est_note_events, \n\t                pedal_events=est_pedal_events, midi_path=midi_path)\n\t            print('Write out to {}'.format(midi_path))\n", "        transcribed_dict = {\n\t            'output_dict': output_dict, \n\t            'est_note_events': est_note_events,\n\t            'est_pedal_events': est_pedal_events}\n\t        return transcribed_dict\n\t    def enframe(self, x, segment_samples):\n\t        \"\"\"Enframe long sequence to short segments.\n\t        Args:\n\t          x: (1, audio_samples)\n\t          segment_samples: int\n", "        Returns:\n\t          batch: (N, segment_samples)\n\t        \"\"\"\n\t        assert x.shape[1] % segment_samples == 0\n\t        batch = []\n\t        pointer = 0\n\t        while pointer + segment_samples <= x.shape[1]:\n\t            batch.append(x[:, pointer : pointer + segment_samples])\n\t            pointer += segment_samples // 2\n\t        batch = np.concatenate(batch, axis=0)\n", "        return batch\n\t    def deframe(self, x):\n\t        \"\"\"Deframe predicted segments to original sequence.\n\t        Args:\n\t          x: (N, segment_frames, classes_num)\n\t        Returns:\n\t          y: (audio_frames, classes_num)\n\t        \"\"\"\n\t        if x.shape[0] == 1:\n\t            return x[0]\n", "        else:\n\t            x = x[:, 0 : -1, :]\n\t            \"\"\"Remove an extra frame in the end of each segment caused by the\n\t            'center=True' argument when calculating spectrogram.\"\"\"\n\t            (N, segment_samples, classes_num) = x.shape\n\t            assert segment_samples % 4 == 0\n\t            y = []\n\t            y.append(x[0, 0 : int(segment_samples * 0.75)])\n\t            for i in range(1, N - 1):\n\t                y.append(x[i, int(segment_samples * 0.25) : int(segment_samples * 0.75)])\n", "            y.append(x[-1, int(segment_samples * 0.25) :])\n\t            y = np.concatenate(y, axis=0)\n\t            return y\n\tdef inference(args):\n\t    \"\"\"Inference template.\n\t    Args:\n\t      model_type: str\n\t      checkpoint_path: str\n\t      post_processor_type: 'regression' | 'onsets_frames'. High-resolution \n\t        system should use 'regression'. 'onsets_frames' is only used to compare\n", "        with Googl's onsets and frames system.\n\t      audio_path: str\n\t      audio_paths: list\n\t      cuda: bool\n\t    \"\"\"\n\t    # Arugments & parameters\n\t    model_type = args.model_type\n\t    checkpoint_path = args.checkpoint_path\n\t    post_processor_type = args.post_processor_type\n\t    device = 'cuda' if args.cuda and torch.cuda.is_available() else 'cpu'\n", "    audio_path = args.audio_path\n\t    sample_rate = config.sample_rate\n\t    segment_samples = sample_rate * 10  \n\t    \"\"\"Split audio to multiple 10-second segments for inference\"\"\"\n\t    # Paths\n\t    midi_path = 'results/{}.mid'.format(get_filename(audio_path))\n\t    create_folder(os.path.dirname(midi_path))\n\t    # Transcriptor\n\t    transcriptor = PianoTranscription(model_type, device=device, \n\t        checkpoint_path=checkpoint_path, segment_samples=segment_samples, \n", "        post_processor_type=post_processor_type)\n\t    # Load audio\n\t    (audio, _) = load_audio(audio_path, sr=sample_rate, mono=True)\n\t    # Transcribe and write out to MIDI file\n\t    transcribe_time = time.time()\n\t    transcribed_dict = transcriptor.transcribe(audio, midi_path)\n\t    print('Transcribe time: {:.3f} s'.format(time.time() - transcribe_time))\n\t    # Visualize for debug\n\t    plot = False\n\t    if plot:\n", "        output_dict = transcribed_dict['output_dict']\n\t        fig, axs = plt.subplots(5, 1, figsize=(15, 8), sharex=True)\n\t        mel = librosa.feature.melspectrogram(audio, sr=16000, n_fft=2048, hop_length=160, n_mels=229, fmin=30, fmax=8000)\n\t        axs[0].matshow(np.log(mel), origin='lower', aspect='auto', cmap='jet')\n\t        axs[1].matshow(output_dict['frame_output'].T, origin='lower', aspect='auto', cmap='jet')\n\t        axs[2].matshow(output_dict['reg_onset_output'].T, origin='lower', aspect='auto', cmap='jet')\n\t        axs[3].matshow(output_dict['reg_offset_output'].T, origin='lower', aspect='auto', cmap='jet')\n\t        axs[4].plot(output_dict['pedal_frame_output'])\n\t        axs[0].set_xlim(0, len(output_dict['frame_output']))\n\t        axs[4].set_xlabel('Frames')\n", "        axs[0].set_title('Log mel spectrogram')\n\t        axs[1].set_title('frame_output')\n\t        axs[2].set_title('reg_onset_output')\n\t        axs[3].set_title('reg_offset_output')\n\t        axs[4].set_title('pedal_frame_output')\n\t        plt.tight_layout(0, .05, 0)\n\t        fig_path = '_zz.pdf'.format(get_filename(audio_path))\n\t        plt.savefig(fig_path)\n\t        print('Plot to {}'.format(fig_path))\n\tif __name__ == '__main__':\n", "    parser = argparse.ArgumentParser(description='')\n\t    parser.add_argument('--model_type', type=str, required=True)\n\t    parser.add_argument('--checkpoint_path', type=str, required=True)\n\t    parser.add_argument('--post_processor_type', type=str, default='regression', choices=['onsets_frames', 'regression'])\n\t    parser.add_argument('--audio_path', type=str, required=True)\n\t    parser.add_argument('--cuda', action='store_true', default=False)\n\t    args = parser.parse_args()\n\t    inference(args)"]}
{"filename": "paow/23_07_26/run.py", "chunked_list": ["import numpy as np\n\timport mido\n\timport random\n\timport time\n\tdef perturbation(p, a, d, w):\n\t    new_p = np.array(p) + np.random.randint(-1, 2, len(p))\n\t    new_a = np.array(a) + np.random.randint(-10, 10, len(a))\n\t    add_mask = np.zeros(len(a))\n\t    ind_a = random.choice(np.arange(len(a)))\n\t    add_mask[ind_a] = 10\n", "    new_a = new_a + add_mask\n\t    new_d = np.array(d) + np.random.randint(-1000, 1000, len(d))\n\t    new_w = np.array(w) + np.random.randint(-300, 300, len(d))\n\t    # Filter out negative values\n\t    new_d[new_d < 0] = 0\n\t    new_w[new_w < 0] = 0\n\t    return new_p, new_a, new_d, new_w\n\tdef generate_grammar(grammar, memory, mem_length=10):\n\t    \"\"\"Generate a pattern from the grammar\"\"\"\n\t    if len(memory) == 0:\n", "        # first pattern\n\t        pattern = random.choice(grammar[\"Pattern\"])\n\t    else:\n\t        # weight more recent patterns\n\t        weights = np.arange(len(memory) + len(grammar[\"Pattern\"]))\n\t        pattern = random.choices(memory+grammar[\"Pattern\"], weights=weights)[0]\n\t    p, a, d, w = pattern\n\t    # Replace Letters\n\t    ########\n\t    p, a, d, w = grammar[p], grammar[a], grammar[d], grammar[w]\n", "    # add to memory\n\t    memory.append(pattern)\n\t    # last element remove from memory if it exceeds size\n\t    if len(memory) > mem_length:\n\t        memory.pop(-1)\n\t    return perturbation(p, a, d, w), memory\n\tdef generate_and_send_midi(music_grammar, port_name, generation_length=3600, mem_length=10, test=False):\n\t    \"\"\"Generate a midi message and send it to the port\n\t    Parameters\n\t    ----------\n", "    music_grammar : dict\n\t        The grammar for the music\n\t    port_name : mido port name\n\t        The port to send the midi message\n\t    generation_length : int\n\t        The length of the generation in seconds\n\t    mem_length : int\n\t        The length of the memory\n\t    \"\"\"\n\t    # Initialize port\n", "    port = mido.open_output(port_name) if not test else None\n\t    # Hold down the pedal\n\t    msg = mido.Message('control_change', control=64, value=127)\n\t    port.send(msg)\n\t    start_time = time.time()\n\t    memory = list()\n\t    while time.time() - start_time < generation_length:\n\t        try:\n\t            (p, a, d, w), memory = generate_grammar(music_grammar, memory, mem_length)\n\t            # Generate midi message\n", "            ########\n\t            # With a small probability, release the pedal\n\t            if random.random() < 0.05:\n\t                # Hold down the pedal (sometimes it releases if no message is sent)\n\t                msg = mido.Message('control_change', control=64, value=0)\n\t                port.send(msg)\n\t            # Send midi message\n\t            for i in range(len(p)):\n\t                if test:\n\t                    print(\"Note: {}, Velocity: {}, Duration: {}\".format(p[i], a[i], d[i]))\n", "                    continue\n\t                msg = mido.Message('note_on', note=int(p[i]), velocity=int(a[i]), time=0)\n\t                port.send(msg)\n\t                print(msg)\n\t                msg = mido.Message('note_off', note=int(p[i]), velocity=int(a[i]), time=int(d[i]))\n\t                port.send(msg)\n\t                # Wait for the next note\n\t                time.sleep(w[i] / 1000)\n\t            if random.random() < 0.2:\n\t                # Hold down the pedal (sometimes it releases if no message is sent)\n", "                msg = mido.Message('control_change', control=64, value=127)\n\t                port.send(msg)\n\t            # Wait for the next generation\n\t            ########\n\t            time.sleep(random.randint(1, 50) * 0.2)\n\t        except KeyboardInterrupt:\n\t            if not test:\n\t                # Release the pedal\n\t                msg = mido.Message('control_change', control=64, value=0)\n\t                port.send(msg)\n", "                port.close()\n\t            raise ValueError(\"Interrupted by user\")\n\t    # Close port\n\t    if not test:\n\t        # Release the pedal\n\t        msg = mido.Message('control_change', control=64, value=0)\n\t        port.send(msg)\n\t        port.close()\n\tclass GrammarGeneration:\n\t    def __init__(self, output_midi_port=\"iM/ONE 1\", generation_length=3600, mem_length=10):\n", "        self.generation_length = generation_length\n\t        self.mem_length = mem_length\n\t        self.output_midi_port = output_midi_port\n\t        # Define grammar\n\t        self.grammar = self.define_grammar()\n\t        # Initialize thread for midi generation\n\t        self.start_midi_generation()\n\t    def start_midi_generation(self):\n\t        generate_and_send_midi(\n\t            music_grammar=self.grammar,\n", "            port_name=self.output_midi_port,\n\t            generation_length=self.generation_length,\n\t            mem_length=self.mem_length,\n\t        )\n\t    def define_grammar(self):\n\t        return {\n\t            \"Pattern\": [[f\"P{i}\", f\"A{i}\", f\"D{i}\", f\"W{i}\"] for i in range(7)],\n\t            \"P0\": np.array([60, 64, 67, 71, 74, 77, 80]),\n\t            \"P1\": np.array([72, 42, 73, 41, 76, 36, 75, 37]),\n\t            \"P2\": np.array([27, 107, 62, 55, 71, 63]),\n", "            \"P3\": np.array([35 + i*7 for i in range(10)]),\n\t            \"P4\": np.array([25, 108]),\n\t            \"P5\": np.array([27, 104, 60, 60, 60, 60]),\n\t            \"P6\": np.array([100 - i*5 for i in range(12)]),\n\t            \"A0\": np.array([random.randint(10, 45) for i in range(7)]),\n\t            \"A1\": np.array([25, 20, 25, 20, 35, 30, 25, 20]),\n\t            \"A2\": np.array([20, 30, 20, 20, 20, 20]),\n\t            \"A3\": np.array([random.randint(10, 40) for i in range(10)]),\n\t            \"A4\": np.array([20, 40]),\n\t            \"A5\": np.array([40, 40, 15, 15, 15, 15]),\n", "            \"A6\": np.array([random.randint(10, 40) for i in range(12)]),\n\t            \"D0\": np.array([2000 for i in range(7)]),\n\t            \"D1\": np.array([500, 500, 500, 500, 1000, 1000, 2000, 1000]),\n\t            \"D2\": np.array([4000, 4000, 3000, 3000, 3000, 3000]),\n\t            \"D3\": np.array([250 for i in range(10)]),\n\t            \"D4\": np.array([3000, 3000]),\n\t            \"D5\": np.array([4000, 4000, 100, 100, 100, 100]),\n\t            \"D6\": np.array([250 for i in range(12)]),\n\t            \"W0\": np.array([0 for i in range(6)] + [1000]),\n\t            \"W1\": np.array([0, 500, 0, 500, 0, 1000, 0, 2000]),\n", "            \"W2\": np.array([0, 5000, 0, 0, 0, 3000]),\n\t            \"W3\": np.array([300 for i in range(9)] + [1000]),\n\t            \"W4\": np.array([0, 3000]),\n\t            \"W5\": np.array([0, 0, 250, 80, 300, 0]),\n\t            \"W6\": np.array([300 for i in range(11)] + [1000]),\n\t        }\n\tif __name__ == \"__main__\":\n\t    import argparse\n\t    args = argparse.ArgumentParser()\n\t    args.add_argument(\"--generation_length\", type=int, default=3600, help=\"Length of the generated audio in seconds\")\n", "    args.add_argument(\"--mem_length\", type=int, default=5, help=\"Length of the memory of the audio and midi generation\")\n\t    args.add_argument(\"--output_midi_port\", type=str, default=\"iM/ONE 1\", help=\"Name of the midi port to send the generated midi to\")\n\t    args = args.parse_args()\n\t    GrammarGeneration(generation_length=args.generation_length, mem_length=args.mem_length,\n\t                      output_midi_port=args.output_midi_port)\n"]}
{"filename": "paow/23_08_02/all_notes.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis script generates a MIDI file with increasing numbers of simultaneous notes at increasing tempos.\n\tSetting the \"MIDI IN Delay\" of the Disklavier to on make the grand piano shut down when too many simultaneous notes arrive.\n\tWe turn it off for this experiment, which means the notes are handled as fast as possible and for many simultaneous notes, strange rhythms emerge. \n\tThe MIDI heard in the stream is a concatenation of two calls to this script with different settings.\n\tEnjoy!\n\t\"\"\"\n\tfrom mido import Message, MidiFile, MidiTrack\n", "import numpy as np\n\tmid = MidiFile()\n\ttrack = MidiTrack()\n\tmid.tracks.append(track)\n\t# some sinusoids with different frequency constants for pedal controls\n\tmidi_pedal_maps = []\n\tfactors = np.arange(1,11)\n\tfor n in range(1,11):\n\t    midi_pedal_maps.append(\n\t    np.round((np.sin(2*np.pi*np.arange(0,128)/(128/n*4))+1)/2*127).astype(int)\n", "    )\n\t# tempos: 100 tempos from whole notes at 60 bpm to sixteenth notes at 240 bpm\n\t# => 0.25 notes/sec - 16 notes/sec\n\t# MIDI default parts per quarter is 480, MIDI default tempo is 120 bpm -> 960 ticks/sec\n\ttempos_in_notes_per_sec = np.linspace(0.25,16,100)\n\ttempos_in_ticks_per_note = 960 / tempos_in_notes_per_sec\n\t# 128 notes per tempo gives a duration of ~1h\n\ttotal_duration_in_hours = (1/tempos_in_notes_per_sec * 128).sum()/3600 # 1.0089\n\t# get nice integer times divisible by two\n\ttempos_in_ticks_per_note_int = np.round(tempos_in_ticks_per_note / 2).astype(int) \n", "# creating the notes\n\tfor l in range(100):\n\t    # lowest MIDI pitch: 21\n\t    order = np.random.permutation(np.arange(21,21+88))\n\t    local_duration_halfed = tempos_in_ticks_per_note_int[l] \n\t    for k in range(44): # 44 -> at most half of the keys will be pressed \n\t        track.append(Message('note_on', note=order[0], velocity=np.random.randint(20,30), time=local_duration_halfed))\n\t        for pitch in order[1:k+1]:  \n\t            track.append(Message('note_on', note=pitch, velocity=np.random.randint(20,30), time=0))\n\t        track.append(Message('note_off', note=order[0], velocity=0, time=local_duration_halfed))\n", "        for pitch in order[1:k+1]:\n\t            track.append(Message('note_off', note=pitch, velocity=0, time=0))\n\t        # use sinusoidal pedal messages\n\t        # track.append(Message(\"control_change\",\n\t        #                     control=64,\n\t        #                     value=midi_pedal_maps[l%10][k]))\n\tmid.save('half_all_notes_no_pedal.mid')"]}
{"filename": "paow/23_06_07/carve_black_midi_blocks.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis experiment carves three well-known pieces from blocks of black midi.\n\tEnjoy!\n\t\"\"\"\n\timport partitura as pt\n\timport numpy as np\n\timport numpy.lib.recfunctions as rfn\n\tfrom partitura.musicanalysis.performance_codec import to_matched_score\n", "# get an original performance\n\tperformance, alignment, score = pt.load_match(\"MYSTERY.match\", create_score=True)\n\tm_score, _ = to_matched_score(score, performance, alignment)\n\tptime_to_stime_map, stime_to_ptime_map = pt.musicanalysis.performance_codec.get_time_maps_from_alignment(performance, score, alignment)\n\t# parameters\n\tdtypes=[('onset_sec', '<f4'), ('duration_sec', '<f4'), ('pitch', '<i4'), ('velocity', '<i4')]\n\twindow_width = 10\n\ttime_bins = 100\n\tdur_bins = 100\n\tdur_max = 1.0\n", "temperature = 1 \n\t# custom softmax with temperature and clipping\n\tdef softmax(array, temp=1):\n\t    return np.exp(np.clip(array/temp, 0,100))/np.exp(np.clip(array/temp, 0,100)).sum()\n\t# sample ids from a histogram-based distribution\n\tdef sample(histogram, number_of_samples, temp):\n\t    return np.random.choice(np.arange(len(histogram)),\n\t                            size = number_of_samples, \n\t                            replace = True,\n\t                            p = softmax(histogram, temp = temp))\n", "# sample from specific values, quantized to bins\n\tdef sample_from_values(values, v_min, v_max, bins, number_of_samples, temp):\n\t    values_normalized = np.clip((values - v_min)/(v_max-v_min), 0 , 1)*bins\n\t    histogram = np.zeros(bins+1)\n\t    for val in values_normalized.astype(int):\n\t        histogram[val] += 1\n\t    new_value_idx = sample(histogram, number_of_samples, temp)\n\t    new_values = v_min + new_value_idx/bins * ((v_max-v_min))\n\t    return new_values\n\t# save the performance with note noise\n", "def save_current_stack(nas, filename, performance):\n\t    full_note_array = rfn.stack_arrays(nas).data\n\t    pitch_sort_idx = np.argsort(full_note_array[\"pitch\"])\n\t    full_note_array = full_note_array[pitch_sort_idx]\n\t    onset_sort_idx = np.argsort(full_note_array[\"onset_sec\"], kind=\"mergesort\")\n\t    full_note_array = full_note_array[onset_sort_idx]\n\t    ppart = pt.performance.PerformedPart.from_note_array(full_note_array)\n\t    ppart.controls = performance[0].controls\n\t    pt.save_performance_midi(ppart, filename)\n\tdur_score = (m_score[\"onset\"]+m_score[\"duration\"]).max() - m_score[\"onset\"].min()\n", "dur_perf = (m_score[\"p_onset\"]+m_score[\"p_duration\"]).max() - m_score[\"p_onset\"].min()\n\twindows = np.ceil(dur_score/window_width).astype(int)\n\tnas = [performance.note_array()[['onset_sec','duration_sec',\"pitch\",\"velocity\"]][:-1],\n\t       performance.note_array()[['onset_sec','duration_sec',\"pitch\",\"velocity\"]][-1:]]\n\tsave_current_stack(nas, \"orig.mid\", performance)\n\t# factors for number of notes and temperature parameters\n\tnotes_factor = [np.linspace(1.0,0.75, windows),\n\t                np.linspace(2.0,1.25, windows),\n\t                np.linspace(4.0,2.0, windows)]\n\ttemperature_factor = [np.linspace(1.0,0.05, windows),\n", "                      np.linspace(4.0,1.0, windows),\n\t                      np.linspace(5.0,1.0, windows)]\n\t# create three versions with increasing noise\n\tfor k in range(3):\n\t    for no, ws in enumerate(np.arange(m_score[\"onset\"].min(),(m_score[\"onset\"]+m_score[\"duration\"]).max(), window_width)):\n\t        mask = np.all((m_score[\"onset\"] >= ws, m_score[\"onset\"] < ws+ window_width), axis=0)\n\t        number_of_notes = int(mask.sum() * notes_factor[k][no])\n\t        temperature = temperature_factor[k][no]\n\t        # sample onsets\n\t        if number_of_notes > 0:\n", "            p_min = stime_to_ptime_map(ws)\n\t            p_max = stime_to_ptime_map(ws+window_width)\n\t            new_onsets = sample_from_values(m_score[\"p_onset\"][mask], p_min, p_max, time_bins, number_of_notes, temperature)\n\t            # sample durations\n\t            new_durations = sample_from_values(m_score[\"p_duration\"][mask], 0.001, dur_max, dur_bins, number_of_notes, temperature)\n\t            # sample pitches\n\t            # piano keyboard distribution 88 notes from 21 to 108\n\t            new_pitches = sample_from_values(m_score[\"pitch\"][mask], 21, 108, 87, number_of_notes, temperature).astype(int)\n\t            # sample velocities\n\t            new_velocities = sample_from_values(m_score[\"velocity\"][mask], 0, 127, 127, number_of_notes, temperature).astype(int)\n", "            new_note_array = np.array([(o,d,p,v) for o,d,p,v in zip(new_onsets, new_durations, new_pitches, new_velocities)], dtype = dtypes)\n\t            nas.append(new_note_array)\n\t    save_current_stack(nas, \"noise{0}.mid\".format(k), performance)"]}
{"filename": "paow/23_08_09/the_limits_of_the_age_of_reproduction.py", "chunked_list": ["#!/usr/bin/env python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis script contains a technical exercise investigating the limits of\n\treproduction of the Disklavier.\n\tFirst, each note is played at different MIDI velocities. The audio for\n\tthis part will be analyzed to create a dictionary of sounds of the Disklavier\n\tfor later experiments.\n\tAfterwards, we explore the speed limits of the hammer actuators of the\n\tDisklavier, by playing repeated notes with increasing speed and velocity.\n", "This experiment will be useful to assess how fast can the piano play, and\n\twhether there are reproduction differences at different MIDI velocities.\n\t\"\"\"\n\timport numpy as np\n\timport partitura as pt\n\tfrom partitura.performance import PerformedPart, Performance\n\t# MIDI velocities to test\n\tMIDI_VELOCITIES = np.clip(np.arange(0, 140, 10), a_min=1, a_max=127)\n\t# MIDI note numbers of the piano keys\n\tPIANO_KEYS = np.arange(21, 109)\n", "# Number of repetitions per second\n\tREPETITIONS = np.arange(1, 16)\n\tdef test_note_velocities(\n\t    midi_velocities: np.ndarray = MIDI_VELOCITIES,\n\t    piano_keys: np.ndarray = PIANO_KEYS,\n\t) -> np.ndarray:\n\t    \"\"\"\n\t    Generate a note array to test the MIDI velocity of the Disklavier.\n\t    Parameters\n\t    ----------\n", "    midi_velocities: np.ndarray\n\t        An array specifying the MIDI velocities to test.\n\t    piano_keys: np.ndarray\n\t       MIDI pitch of the piano keys to include in the test.\n\t    Returns\n\t    -------\n\t    note_array: np.ndarray\n\t        A structured array with note information. This array follows the\n\t        structure of the note arrays of `PerformedPart` objects.\n\t    \"\"\"\n", "    # Number of notes\n\t    n_onsets = len(piano_keys) * len(midi_velocities)\n\t    # Initialize array\n\t    note_array = np.zeros(\n\t        n_onsets,\n\t        dtype=[\n\t            (\"pitch\", \"i4\"),\n\t            (\"onset_sec\", \"f4\"),\n\t            (\"duration_sec\", \"f4\"),\n\t            (\"velocity\", \"i4\"),\n", "        ],\n\t    )\n\t    note_array[\"pitch\"] = np.repeat(piano_keys, len(midi_velocities))\n\t    note_array[\"velocity\"] = np.tile(midi_velocities, len(piano_keys))\n\t    # onsets will be every second\n\t    note_array[\"onset_sec\"] = np.arange(n_onsets)\n\t    # durations will be 0.5 seconds\n\t    note_array[\"duration_sec\"] = np.ones(n_onsets) * 0.5\n\t    return note_array\n\tdef test_repetitions(\n", "    repetitions: np.ndarray = REPETITIONS,\n\t    pitch: int = 60,\n\t    velocity: int = 60,\n\t) -> np.ndarray:\n\t    \"\"\"\n\t    Generate a note array to test the repetition speed of the Disklavier.\n\t    Parameters\n\t    ----------\n\t    repetitions: np.ndarray\n\t        An array specifying the number of repetitions per second of a note.\n", "    pitch: int\n\t        MIDI pitch of the note.\n\t    velocity: int\n\t        MIDI velocity of the note\n\t    Returns\n\t    -------\n\t    note_array: np.ndarray\n\t        A structured array with note information. This array follows the\n\t        structure of the note arrays of `PerformedPart` objects.\n\t    \"\"\"\n", "    n_onsets = sum(repetitions)\n\t    # Initialize array\n\t    note_array = np.zeros(\n\t        n_onsets,\n\t        dtype=[\n\t            (\"pitch\", \"i4\"),\n\t            (\"onset_sec\", \"f4\"),\n\t            (\"duration_sec\", \"f4\"),\n\t            (\"velocity\", \"i4\"),\n\t        ],\n", "    )\n\t    start = 0\n\t    for i, r in enumerate(repetitions):\n\t        sl = slice(start, start + r)\n\t        # set pitch\n\t        note_array[\"pitch\"][sl] = pitch\n\t        onsets = np.linspace(0, 1, r, False)\n\t        # set onsets\n\t        note_array[\"onset_sec\"][sl] = (\n\t            onsets + note_array[\"onset_sec\"][start - 1] + (2 * (i != 0))\n", "        )\n\t        # set duration (as half of the inter onset interval\n\t        note_array[\"duration_sec\"][sl] = np.ones(r) * 0.5 / r\n\t        # set velocity\n\t        note_array[\"velocity\"][sl] = np.ones(r) * velocity\n\t        start += r\n\t    return note_array\n\tdef get_duration(note_arrays: list) -> float:\n\t    \"\"\"\n\t    Get duration of the last array note array in the list.\n", "    Parameters\n\t    ----------\n\t    note_arrays: list\n\t        A list of structured note arrays with note information.\n\t    Returns\n\t    -------\n\t    end_time : float\n\t        Duration of the music in the note array in seconds.\n\t    \"\"\"\n\t    end_time = (\n", "        note_arrays[-1][\"onset_sec\"] + note_arrays[-1][\"duration_sec\"]\n\t    ).max() - note_arrays[-1][\"onset_sec\"].min()\n\t    return end_time\n\tdef main() -> None:\n\t    \"\"\"\n\t    Generate a MIDI file that explores the reproduction limits of\n\t    the Disklavier.\n\t    \"\"\"\n\t    # Initialize note arrays with test of MIDI velocities\n\t    note_arrays = [test_note_velocities()]\n", "    na_start = get_duration(note_arrays) + 3\n\t    diminished_chord = np.array([0, 3, 6, 9])\n\t    pitch = np.hstack(\n\t        ([21] + [diminished_chord + (i * 12) + 24 for i in range(7)] + [108])\n\t    )\n\t    for p in pitch:\n\t        for vel in np.arange(10, 130, 25):\n\t            note_arrays.append(test_repetitions(pitch=p, velocity=vel))\n\t            note_arrays[-1][\"onset_sec\"] += na_start\n\t            na_start += get_duration(note_arrays) + 3\n", "    # Concatenate all Note arrays\n\t    note_array = np.hstack(note_arrays)\n\t    # Create a PerformedPart\n\t    ppart = PerformedPart.from_note_array(note_array)\n\t    # Create a Performance\n\t    performance = Performance(ppart)\n\t    # Export the Performance to a MIDI file\n\t    pt.save_performance_midi(\n\t        performance,\n\t        out=\"limits_of_the_age_of_reproduction.mid\",\n", "    )\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "paow/evolutionary/__init__.py", "chunked_list": ["#!/usr/bin/python\n\t# -*- coding: utf-8 -*-\n\t\"\"\"\n\tThis module contains evolutionary algorithms for music generation.\n\t\"\"\"\n\tfrom .chord_for_melody import Optimizer, Optimizer2"]}
{"filename": "paow/evolutionary/chord_for_melody.py", "chunked_list": ["import numpy as np\n\tfrom paow.utils import Chord, Progression, euclidean\n\tfrom paow.utils import cycle_distance, chordDistance, parttimefromrekorder\n\tclass Optimizer:\n\t    def __init__(self):\n\t        pass    \n\t    def modify(self, population):\n\t        # add an accidental\n\t        subpop3 = np.random.choice(population, 30)\n\t        for element in subpop3:\n", "            cidx = np.random.randint(len(element.chords))\n\t            nidx = np.random.randint(4)\n\t            mod = np.random.choice([-1,1])\n\t            new_element = element.copy()\n\t            new_element.chords[cidx].add_repitch(nidx,mod)\n\t            population.append(new_element)\n\t        # invert a chord\n\t        subpop4 = np.random.choice(population, 30)\n\t        for element in subpop4:\n\t            cidx = np.random.randint(len(element.chords))\n", "            nidx = np.random.randint(4)\n\t            new_element =  element.copy()\n\t            new_element.chords[cidx].invert(nidx)\n\t            population.append(new_element)\n\t        # join some elements\n\t        subpop1 = np.random.choice(population, 30)\n\t        subpop2 = np.random.choice(population, 30)\n\t        for element0, element1 in zip(subpop1, subpop2):\n\t            elnew1, elnew2 = element0.join(element1)\n\t            population.append(elnew1)\n", "            population.append(elnew2)\n\t        return population\n\t    def fitness(self, progression, melody_windows):\n\t        # the lower the fitness score the better\n\t        fit = 0\n\t        for c0,c1 in zip(progression.chords[:-1], progression.chords[1:]):\n\t            _, dist = chordDistance(c0.pitches, c1.pitches)\n\t            # penalize big leaps between pitches of adjacent chords\n\t            fit += dist \n\t            # penalize big leaps between scale tonic of adjacent chords\n", "            fit += abs(c0.offset - c1.offset)\n\t            # penalize small leaps between root of adjacent chords\n\t            fit += abs(5.0 - cycle_distance(c0.root,c1.root))\n\t            #penalize sticking on a root\n\t            if c0.root_id == c1.root_id:\n\t                fit += 30\n\t        for i, c0 in enumerate(progression.chords):\n\t            for note in melody_windows[i]:\n\t                fit += 3 * np.min([cycle_distance(note, pit) for pit in c0.pitches] ) \n\t                # print(fit, melody_windows[i], c0.pitches)\n", "        # add a small random number for hashing\n\t        fit += np.random.rand(1)[0]\n\t        return fit \n\t    def select(self, population, number, melody_windows):\n\t        pop = {ele.id:ele for ele in population}\n\t        fitness_dict = {self.fitness(ele, melody_windows):ele.id for ele in population}\n\t        sorted_fitness = list(fitness_dict.keys())\n\t        sorted_fitness.sort()\n\t        # print(sorted_fitness)\n\t        # print([(len(pop[fitness_dict[k]].chords), k) for k in sorted_fitness[:50]])\n", "        new_pop = [pop[fitness_dict[k]] for k in sorted_fitness[:number]]\n\t        return new_pop, sorted_fitness\n\t    def run(self,\n\t            epochs = 10,\n\t            population_size = 100,\n\t            population_replacement = 0.3,\n\t            new_population = True,\n\t            melody = None,\n\t            number_of_chords = 8,\n\t            quarter_duration = 4,\n", "            quarters = 8):\n\t        # find best euclidean rhythm\n\t        melody_onsets = [note[\"onset_sec\"] for note in melody]\n\t        melody_onsets_int = np.round(np.array(melody_onsets) / (10.0/quarters) * quarter_duration)\n\t        # use up to eight notes\n\t        number_of_notes = np.min((len(melody_onsets), number_of_chords))\n\t        spaced_idx = np.round(np.linspace(0, len(melody_onsets_int) - 1, number_of_notes)).astype(int)\n\t        melody_onsets_int = melody_onsets_int[spaced_idx]\n\t        positions = quarter_duration * quarters # 8 quarters, 1.25 sec\n\t        dists = dict()\n", "        for k in range(positions):\n\t            euc = euclidean(cycle = positions, pulses = number_of_notes, offset= k)\n\t            dist = np.sum(np.abs(euc - melody_onsets_int))\n\t            euc[0] = 0\n\t            euc_shifted = list(np.roll(euc, -1))[:number_of_notes] + [positions]\n\t            dists[dist] = [(on, off) for on, off in zip(euc, euc_shifted)]\n\t        min_dist = np.min(list(dists.keys()))\n\t        rhythm = dists[min_dist]\n\t        na, frames = parttimefromrekorder(melody,\n\t                                 quarter_duration = quarter_duration,\n", "                                 num_frames = number_of_notes,\n\t                                 rhythm = rhythm)\n\t        print(na, frames)\n\t        if new_population:\n\t            population = [Progression(number_of_chords= number_of_notes) for po in range(population_size)]    \n\t        else:\n\t            population = self.population\n\t        for epoch in range(epochs): \n\t            population = self.modify(population)\n\t            population, sorted_fitness = self.select(population, int(population_replacement * population_size), frames) \n", "            print(f\"Epoch {epoch} best fitness: {sorted_fitness[0]:.4f}\")\n\t            population += [Progression(number_of_chords= number_of_notes) \n\t                           for po in range(population_size - len(population))]\n\t        self.population = population\n\t        return self.population, rhythm\n\tclass Optimizer2:\n\t    def __init__(self):\n\t        pass    \n\t    def modify(self, population):\n\t        # add an accidental\n", "        subpop3 = np.random.choice(population, 10)\n\t        for element in subpop3:\n\t            cidx = np.random.randint(len(element.chords))\n\t            nidx = np.random.randint(4)\n\t            mod = np.random.choice([-1,1])\n\t            new_element = element.copy()\n\t            new_element.chords[cidx].add_repitch(nidx,mod)\n\t            population.append(new_element)\n\t        # invert a chord\n\t        subpop4 = np.random.choice(population, 10)\n", "        for element in subpop4:\n\t            cidx = np.random.randint(len(element.chords))\n\t            nidx = np.random.randint(4)\n\t            new_element =  element.copy()\n\t            new_element.chords[cidx].invert(nidx)\n\t            population.append(new_element)\n\t        # change root of a chord\n\t        subpop4 = np.random.choice(population, 10)\n\t        for element in subpop4:\n\t            cidx = np.random.randint(len(element.chords))\n", "            nidx = np.random.randint(7)\n\t            new_element =  element.copy()\n\t            new_element.chords[cidx].root_id = nidx\n\t            new_element.chords[cidx].compute_pitch()\n\t            population.append(new_element)\n\t        # # join some elements\n\t        # subpop1 = np.random.choice(population, 30)\n\t        # subpop2 = np.random.choice(population, 30)\n\t        # for element0, element1 in zip(subpop1, subpop2):\n\t        #     elnew1, elnew2 = element0.join(element1)\n", "        #     population.append(elnew1)\n\t        #     population.append(elnew2)\n\t        return population\n\t    def fitness(self, progression, melody_windows):\n\t        # the lower the fitness score the better\n\t        fit = 0\n\t        for c0,c1 in zip(progression.chords[:-1], progression.chords[1:]):\n\t            _, dist = chordDistance(c0.pitches, c1.pitches)\n\t            # penalize big leaps between pitches of adjacent chords\n\t            fit += dist \n", "            # penalize big leaps between scale tonic of adjacent chords\n\t            fit += abs(c0.offset - c1.offset)\n\t            # penalize small leaps between root of adjacent chords\n\t            fit += abs(5.0 - cycle_distance(c0.root,c1.root))\n\t            #penalize sticking on a root\n\t            if c0.root_id == c1.root_id:\n\t                fit += 30\n\t        for i, c0 in enumerate(progression.chords):\n\t            for note in melody_windows[i]:\n\t                fit += 3 * np.min([cycle_distance(note, pit) for pit in c0.pitches] ) \n", "                # print(fit, melody_windows[i], c0.pitches)\n\t        # add a small random number for hashing\n\t        fit += np.random.rand(1)[0]\n\t        return fit \n\t    def select(self, population, number, melody_windows):\n\t        pop = {ele.id:ele for ele in population}\n\t        fitness_dict = {self.fitness(ele, melody_windows):ele.id for ele in population}\n\t        sorted_fitness = list(fitness_dict.keys())\n\t        sorted_fitness.sort()\n\t        # print(sorted_fitness)\n", "        # print([(len(pop[fitness_dict[k]].chords), k) for k in sorted_fitness[:50]])\n\t        new_pop = [pop[fitness_dict[k]] for k in sorted_fitness[:number]]\n\t        return new_pop, sorted_fitness\n\t    def run(self,\n\t            epochs = 10,\n\t            population_size = 100,\n\t            population_replacement = 0.3,\n\t            new_population = True,\n\t            melody = None,\n\t            number_of_chords = 8,\n", "            quarter_duration = 4,\n\t            quarters = 8):\n\t        # find best euclidean rhythm\n\t        melody_onsets = [note[\"onset_sec\"] for note in melody]\n\t        melody_onsets_int = np.round(np.array(melody_onsets) / (10.0/quarters) * quarter_duration)\n\t        # use up to eight notes\n\t        number_of_notes = np.min((len(melody_onsets), number_of_chords))\n\t        spaced_idx = np.round(np.linspace(0, len(melody_onsets_int) - 1, number_of_notes)).astype(int)\n\t        melody_onsets_int = melody_onsets_int[spaced_idx]\n\t        positions = quarter_duration * quarters # 8 quarters, 1.25 sec\n", "        dists = dict()\n\t        for k in range(positions):\n\t            euc = euclidean(cycle = positions, pulses = number_of_notes, offset= k)\n\t            dist = np.sum(np.abs(euc - melody_onsets_int))\n\t            euc[0] = 0\n\t            euc_shifted = list(np.roll(euc, -1))[:number_of_notes-1] + [positions]\n\t            dists[dist] = [(on, off) for on, off in zip(euc, euc_shifted)]\n\t        min_dist = np.min(list(dists.keys()))\n\t        rhythm = dists[min_dist]\n\t        na, frames = parttimefromrekorder(melody,\n", "                                 quarter_duration = quarter_duration,\n\t                                 num_frames = number_of_notes,\n\t                                 rhythm = rhythm)\n\t        print(na, frames)\n\t        if new_population:\n\t            population = [Progression(number_of_chords= number_of_notes) for po in range(population_size)]    \n\t        else:\n\t            population = self.population\n\t        for epoch in range(epochs): \n\t            population = self.modify(population)\n", "            population, sorted_fitness = self.select(population, int(population_replacement * population_size), frames) \n\t            print(f\"Epoch {epoch} best fitness: {sorted_fitness[0]:.4f}\")\n\t            population += [Progression(number_of_chords= number_of_notes) \n\t                           for po in range(population_size - len(population))]\n\t        self.population = population\n\t        return self.population, rhythm\n\tif __name__ == \"__main__\":\n\t    pass\n\t    # fields = [\n\t    # (\"onset_sec\", \"f4\"),\n", "    # (\"duration_sec\", \"f4\"),\n\t    # (\"pitch\", \"i4\"),\n\t    # (\"velocity\", \"i4\"),\n\t    # ]\n\t    # rows = [\n\t    # (0.933,1.712,48,100),\n\t    # (7.176,1.885,51,100),\n\t    # (2.685,1.777,53,100),\n\t    # (4.464,2.71,59,100),\n\t    # ]\n", "    # note_array = np.array(rows, dtype=fields)\n\t    # exp = Optimizer()\n\t    # p = exp.run(melody=note_array)"]}
{"filename": "paow/evolutionary/test_evol_loivy.py", "chunked_list": ["from paow.evolutionary import Optimizer, Optimizer2\n\tfrom paow.utils import partFromProgression, Sequencer, MidiRouter, MidiInputThread\n\timport numpy as np\n\timport multiprocessing\n\tfrom collections import defaultdict\n\tdef show(p):\n\t    for c in p[0].chords:\n\t        print(c.pitches)\n\tdef note2note_array(notes):\n\t    fields = [\n", "    (\"onset_sec\", \"f4\"),\n\t    (\"duration_sec\", \"f4\"),\n\t    (\"pitch\", \"i4\"),\n\t    (\"velocity\", \"i4\"),\n\t    ]\n\t    notes_list = list()\n\t    sounding_notes = {}\n\t    for note in notes:\n\t        msg = note[0]\n\t        time = note[1]\n", "        note_on = msg.type == \"note_on\"\n\t        note_off = msg.type == \"note_off\"\n\t        if note_on or note_off:\n\t            note = msg.note\n\t            if note_on and msg.velocity > 0:\n\t                # save the onset time and velocity\n\t                sounding_notes[note] = time\n\t            elif note_off or msg.velocity == 0:\n\t                if note not in sounding_notes:\n\t                    continue\n", "                else:\n\t                    onset = sounding_notes[note]\n\t                    duration = time - onset\n\t                    notes_list.append((onset, duration, note, msg.velocity))\n\t                    del sounding_notes[note]\n\t    notes_array = np.array(notes_list, dtype=fields)\n\t    return notes_array\n\tdef rec(l):\n\t    notes = l.run()\n\t    na = note2note_array(notes)\n", "    return na\n\tdef recompute(note_array = None, e = 10):\n\t    if note_array is None:\n\t        fields = [\n\t            (\"onset_sec\", \"f4\"),\n\t            (\"duration_sec\", \"f4\"),\n\t            (\"pitch\", \"i4\"),\n\t            (\"velocity\", \"i4\"),\n\t            ]\n\t        rows = [\n", "            (0.933,1.712,48,100),\n\t            (7.176,1.885,51,100),\n\t            (2.685,1.777,53,100),\n\t            (4.464,2.71,59,100),\n\t        ]\n\t        note_array = np.array(rows, dtype=fields)\n\t    exp = Optimizer2()\n\t    p, r = exp.run(melody=note_array, epochs = e)\n\t    part = partFromProgression(p[0],quarter_duration = 4,rhythm = r)\n\t    return part, r\n", "def st(s = None, re = False):\n\t    if s is None:\n\t        queue = multiprocessing.Queue()\n\t        s = Sequencer(queue=queue,outport_name=\"seq\")\n\t        s.start()\n\t    else:\n\t        s.terminate()\n\t        s.join()\n\t        if re:\n\t            queue =multiprocessing.Queue()\n", "            s = Sequencer(queue=queue,outport_name=\"seq\")\n\t            s.start()\n\t    return s\n\tif __name__ == \"__main__\":\n\t    mr = MidiRouter(inport_name=\"inp\")\n\t    l = MidiInputThread(port = mr.input_port)\n\t    s = st()\n\t    # part, r = recompute(note_array)\n\t    # note_array = rec(l)\n\t    # s.up(part)\n", "    # s.start()\n\t    # s.terminate()\n\t    # s.join()\n"]}
{"filename": "paow/23_08_23/random_pseq_markov_file.py", "chunked_list": ["import numpy as np\n\tdef euclidean(cycle = 16, \n\t              pulses = 4, \n\t              offset= 0):\n\t    \"\"\"\n\t    compute an Euclidean rhythm\n\t    \"\"\"\n\t    rhythm = []\n\t    bucket = cycle-pulses\n\t    for step in range(cycle):\n", "        bucket = bucket + pulses\n\t        if (bucket >= cycle):\n\t            bucket = bucket - cycle\n\t            rhythm.append(step)\n\t    rhythm_array = np.array(rhythm)\n\t    rhythm_array = (rhythm_array + offset) % cycle\n\t    rhythm_array = np.sort( rhythm_array)\n\t    return rhythm_array\n\tdef pbinds(file,\n\t           how_many_seqs=2,\n", "           euc_seq_min=16,\n\t           euc_seq_max=16,\n\t           pulses_min= 4,\n\t           pulses_max = 6,\n\t           loop_len = 3.0):\n\t    file.write(\"// start a server and a MIDIClient and define a MIDIOut with the variable name m.\\n\")\n\t    file.write(\"(\\n\")\n\t    file.write(\"var markovmatrix;\\n\")\n\t    file.write(\"var no_of_seqs = {};\\n\".format(how_many_seqs*3-2))\n\t    file.write(\"var currentstate = {}.rand;\\n\".format(how_many_seqs*3-2))\n", "    first = list()\n\t    second = list()\n\t    third = list()\n\t    for pseq_id in range(how_many_seqs*3):\n\t        if pseq_id %3 == 0:\n\t            first.append(\"\\\\x{0}\".format(pseq_id))\n\t            seq_len = np.random.randint(euc_seq_min, euc_seq_max+1)\n\t            pulses = np.random.randint(pulses_min, np.min((pulses_max+1, seq_len)))\n\t            pitch_plus = 0\n\t        elif pseq_id %3 == 1:\n", "            second.append(\"\\\\x{0}\".format(pseq_id))\n\t            seq_len = np.random.randint(euc_seq_min, euc_seq_max+1)\n\t            pulses = np.random.randint(pulses_min, np.min((pulses_max+1, seq_len)))\n\t            pitch_plus = 0\n\t        else:\n\t            third.append(\"\\\\x{0}\".format(pseq_id))\n\t            # seq_len = np.random.randint(euc_seq_min, euc_seq_max+1)\n\t            pulses = np.max((1, np.floor(pulses/2).astype(int)))\n\t            pitch_plus = -8\n\t        pulse_onsets = euclidean(seq_len,pulses, np.random.randint(2))\n", "        pseq_array = [\"Rest(0)\"]*seq_len\n\t        for po in pulse_onsets:\n\t            if pitch_plus == 0:\n\t                pseq_array[po] = \"[{1:d},{0:d}]\".format(2+pseq_id%7, 2+pseq_id%7+ 14)\n\t            else:\n\t                pseq_array[po] = \"[{1:d},{0:d}]\".format(pseq_id%2 + pitch_plus, pseq_id%2 + pitch_plus-7)\n\t        pseq_string = \"Pseq([\"+ \",\".join(pseq_array)+\"],1)\"\n\t        file.write('Pdef(\\\\x{0}, {{ Pbind(\\\\ctranspose, -1, \\\\degree, {1}, \\\\dur,{3}/{2} , \\\\amp, Pseq([0.3,0.31,0.29],inf))}});\\n'.format(pseq_id,pseq_string,seq_len, loop_len))\n\t    first =  [val for val in first for _ in (0, 1, 3)][2:]\n\t    second =  [val for val in second for _ in (0, 1, 3)][1:-1]\n", "    third =  [val for val in third for _ in (0, 1, 3)][0:-2]\n\t    file.write(\"{inf.do{\")\n\t    file.write(\"(Pdef([\" + \",\".join(first)+ \"].at(currentstate)) <> (type: \\midi, midiout: m)).play;\\n\")\n\t    file.write(\"(Pdef([\" + \",\".join(second)+ \"].at(currentstate)) <> (type: \\midi, midiout: m)).play;\\n\")\n\t    file.write(\"(Pdef([\" + \",\".join(third)+ \"].at(currentstate)) <> (type: \\midi, midiout: m)).play;\\n\")\n\t    file.write(\"currentstate = (currentstate-1..currentstate+3).wchoose([0.3,18,3,1,0.3].normalizeSum);\\n\")\n\t    file.write(\"currentstate.postln;\\n\")\n\t    file.write(\"if ( currentstate>(no_of_seqs - 1),   {currentstate = 0; }, {  } );\\n\")\n\t    file.write(\"if ( currentstate<0,   {currentstate = 0; }, {  } );\\n\")\n\t    file.write(\"{}.wait;\\n\".format(seq_len))\n", "    file.write(\"};}.fork;\\n\")\n\t    file.write(\")\\n\")\n\tif __name__ == \"__main__\":\n\t    with open(\"markov.scd\", \"w\") as f:\n\t        pbinds(f, 31, 16, 24, 6, 18, 3.4)\n"]}
{"filename": "paow/23_06_14/render_script.py", "chunked_list": ["#!/usr/bin/env python\n\t\"\"\"\n\tThis program generates an expressive performance of an input music score\n\t(in any format supported by the Partitura package) using a (trained)\n\tpredictive model.\n\t\"\"\"\n\timport argparse\n\timport json\n\timport logging\n\timport os\n", "import sys\n\timport warnings\n\tfrom typing import Iterable, Dict, List, Tuple\n\timport numpy as np\n\timport torch\n\tfrom basismixer import TOY_MODEL_CONFIG\n\tfrom basismixer.performance_codec import get_performance_codec\n\tfrom basismixer.predictive_models import FullPredictiveModel, construct_model\n\tfrom basismixer.utils import (\n\t    DEFAULT_VALUES,\n", "    RENDER_CONFIG,\n\t    get_all_output_names,\n\t    get_default_values,\n\t    load_score,\n\t    post_process_predictions,\n\t    sanitize_performed_part,\n\t)\n\tfrom partitura import save_match, save_performance_midi\n\tfrom partitura.musicanalysis import make_note_feats\n\tfrom partitura.score import merge_parts, remove_grace_notes, Part\n", "logging.basicConfig(level=logging.INFO)\n\twarnings.filterwarnings(\"ignore\")\n\tLOGGER = logging.getLogger(__name__)\n\tLOGGER.addHandler(logging.StreamHandler(sys.stdout))\n\tdef load_model(\n\t    model_config: Iterable,\n\t    default_values: Dict = DEFAULT_VALUES,\n\t) -> Tuple[FullPredictiveModel, List[str]]:\n\t    \"\"\"\n\t    Load a saved model\n", "    Parameters\n\t    ----------\n\t    model_config : iterable\n\t        A list of tuples each containing the filename of the config file\n\t        of the model and the filename of the parameters of the model.\n\t    default_values : dict\n\t        Default values for the parameters not included in the trained\n\t        model(s) (e.g., the models might just be predicting velocity, and\n\t        the rest of the parameters (beat_period, timing, etc.)\n\t        will be set to these parameters.\n", "    Returns\n\t    -------\n\t    full_model : basismixer.predictive_models.FullPredictiveModel\n\t       An instance of a FullPredictiveModel for generating expressive\n\t       performances.\n\t    output_names : list\n\t       List of the expressive parameters included in the models loaded\n\t       from `model_config`.\n\t    \"\"\"\n\t    # Load the models\n", "    models = []\n\t    for con, par in model_config:\n\t        # Load config file\n\t        model_config = json.load(open(con))\n\t        # Load the parameters\n\t        params = torch.load(\n\t            par,\n\t            map_location=torch.device(\"cpu\"),\n\t        )[\"state_dict\"]\n\t        # Construct the model according to its configuration and\n", "        # set the parameters\n\t        model = construct_model(\n\t            model_config,\n\t            params,\n\t            device=torch.device(\"cpu\"),\n\t        )\n\t        # append the model to the list of models\n\t        models.append(model)\n\t    # Get all output names (expressive parameters) predicted by the models\n\t    output_names = list(\n", "        set(\n\t            [\n\t                name\n\t                for out_name in [m.output_names for m in models]\n\t                for name in out_name\n\t            ]\n\t        )\n\t    )\n\t    # Get the name of the inputs (basis functions) included in the models\n\t    input_names = list(\n", "        set(\n\t            [\n\t                name\n\t                for in_name in [m.input_names for m in models]\n\t                for name in in_name\n\t            ]\n\t        )\n\t    )\n\t    input_names.sort()\n\t    output_names.sort()\n", "    # Get the appropriate list of expressive parameters for generating\n\t    # a performance\n\t    all_output_names = get_all_output_names(output_names)\n\t    # Get the default values for those parameters not predicted by the\n\t    # loaded models\n\t    def_values = get_default_values(default_values, all_output_names)\n\t    # Construct the full model\n\t    full_model = FullPredictiveModel(\n\t        models=models,\n\t        input_names=input_names,\n", "        output_names=all_output_names,\n\t        default_values=def_values,\n\t    )\n\t    # Set of expressive parameters not included in the predicted models\n\t    not_in_model_names = set(all_output_names).difference(output_names)\n\t    LOGGER.info(\n\t        \"Trained models include the following parameters:\\n\"\n\t        + \"\\n\".join(output_names)\n\t        + \"\\n\\nThe following parameters will use default values:\\n\"\n\t        + \"\\n\"\n", "        + \"\\n\".join(\n\t            [\n\t                \"{0}:{1:.2f}\".format(k, default_values[k])\n\t                for k in not_in_model_names\n\t            ]\n\t        )\n\t    )\n\t    return full_model, output_names\n\tdef compute_basis_from_score(\n\t    score_fn: str,\n", "    input_names: List[str],\n\t) -> Tuple[np.ndarray, Part]:\n\t    \"\"\"\n\t    Load a score and extract input score features through the basis functions\n\t    Parameters\n\t    ----------\n\t    score_fn : str\n\t        Filename of the score. Must be in one of the formats supported\n\t        by partitura.\n\t    input_names : list\n", "        List of input basis functions to extract score features.\n\t        See `basismixer.basisfunctions` for the full list of functions\n\t        supported\n\t    Returns\n\t    -------\n\t    basis : np.ndarray\n\t        A 2D array with dimensions (n_notes, n_basis_functions), where n_notes\n\t        is the number of notes in the score and\n\t        n_basis_functions == len(input_names) is the number of basis functions.\n\t        Each i-th row in this array corresponds to the basis functions\n", "        evaluated for the i-th note in the score. The order of the notes is\n\t        the same as in `part.notes_tied` (see below).\n\t    part : partitura.score.Part\n\t        A `Part` object representing the score.\n\t    \"\"\"\n\t    # Load score\n\t    part = merge_parts(load_score(score_fn))\n\t    # Delete grace notes\n\t    remove_grace_notes(part)\n\t    # Compute basis functions\n", "    _basis, bf_names = make_note_feats(\n\t        part, list(set([bf.split(\".\")[0] for bf in input_names]))\n\t    )\n\t    basis = np.zeros((len(_basis), len(input_names)))\n\t    for i, n in enumerate(input_names):\n\t        try:\n\t            ix = bf_names.index(n)\n\t        except ValueError:\n\t            continue\n\t        basis[:, i] = _basis[:, ix]\n", "    return basis, part\n\tdef predict(\n\t    model_config: Iterable,\n\t    score_fn: str,\n\t    default_values: dict = DEFAULT_VALUES,\n\t) -> Tuple[np.ndarray, FullPredictiveModel, Part]:\n\t    \"\"\"\n\t    Main method for predicting a performance.\n\t    Parameters\n\t    ----------\n", "     model_config : iterable\n\t        A list of tuples each containing the filename of the config\n\t        file of the model and the filename of the parameters of the model.\n\t    score_fn : str\n\t        Filename of the score. Must be in one of the formats supported\n\t        by partitura.\n\t    default_values : dict\n\t        Default values for the parameters not included in the trained\n\t        model(s) (e.g., the models might just be predicting velocity,\n\t        and the rest of the parameters (beat_period, timing, etc.)\n", "        will be set to these parameters.\n\t    Returns\n\t    -------\n\t    predictions : structured array\n\t        Numpy structured array containing the predictions by the model.\n\t        The fields are the names of the parameters specified by the `model`.\n\t    model : basismixer.predictive_models.FullPredictiveModel\n\t       An instance of a FullPredictiveModel for generating expressive\n\t       performances.\n\t    part : partitura.score.Part\n", "        A `Part` object representing the score.\n\t    \"\"\"\n\t    # Load predictive model\n\t    model, predicted_parameter_names = load_model(\n\t        model_config, default_values=DEFAULT_VALUES\n\t    )\n\t    # Compute score representation\n\t    basis, part = compute_basis_from_score(score_fn, model.input_names)\n\t    # Score positions for each note in the score\n\t    score_onsets = part.beat_map([n.start.t for n in part.notes_tied])\n", "    # make predictions\n\t    predictions = model.predict(basis, score_onsets)\n\t    return predictions, model, part\n\tif __name__ == \"__main__\":\n\t    parser = argparse.ArgumentParser(\n\t        \"Render a piano performance of a given score file \"\n\t        \"and save it as a MIDI file\"\n\t    )\n\t    parser.add_argument(\n\t        \"score_fn\",\n", "        help=(\n\t            \"Score file (MusicXML, MIDI or formats supported by MuseScore 3)\"\n\t        ),\n\t    )\n\t    parser.add_argument(\"midi_fn\", help=\"Output MIDI file\")\n\t    parser.add_argument(\n\t        \"--model-config\",\n\t        \"-c\",\n\t        help=(\n\t            \"JSON file specifying the configuration and parameters of the\"\n", "            \" model\"\n\t        ),\n\t    )\n\t    parser.add_argument(\n\t        \"--default-values\",\n\t        \"-d\",\n\t        help=(\n\t            \"JSON file specifying the default values for rendering the\"\n\t            \" performance\"\n\t        ),\n", "        default=None,\n\t    )\n\t    parser.add_argument(\n\t        \"--render-config\",\n\t        \"-r\",\n\t        help=(\n\t            \"JSON file specifying the configuration for post-processing the\"\n\t            \" generated performance\"\n\t        ),\n\t        default=None,\n", "    )\n\t    parser.add_argument(\n\t        \"--save-match\",\n\t        \"-m\",\n\t        help=\"Export Score-Performance alignment in Matchfile format\",\n\t        action=\"store_true\",\n\t        default=False,\n\t    )\n\t    args = parser.parse_args()\n\t    # Use toy model if no model is given\n", "    if args.model_config is None:\n\t        model_config = TOY_MODEL_CONFIG\n\t    else:\n\t        model_config = json.load(open(args.model_config))\n\t    # Use default config files if not given\n\t    if args.default_values is None:\n\t        default_values = DEFAULT_VALUES\n\t    else:\n\t        default_values = json.load(open(args.default_values))\n\t    if args.render_config is None:\n", "        render_config = RENDER_CONFIG\n\t    else:\n\t        render_config = json.load(open(args.render_config))\n\t    # Predict performance\n\t    preds, model, part = predict(\n\t        model_config=model_config,\n\t        score_fn=args.score_fn,\n\t        default_values=default_values,\n\t    )\n\t    # Post process predictions\n", "    post_process_predictions(preds, render_config)\n\t    # decode predictions\n\t    perf_codec = get_performance_codec(model.output_names)\n\t    predicted_ppart = perf_codec.decode(\n\t        part=part,\n\t        parameters=preds,\n\t        return_alignment=args.save_match,\n\t    )\n\t    if args.save_match:\n\t        predicted_ppart, alignment = predicted_ppart\n", "    # Sanitize part\n\t    sanitize_performed_part(predicted_ppart)\n\t    # Save MIDI file\n\t    save_performance_midi(predicted_ppart, args.midi_fn)\n\t    if args.save_match:\n\t        save_match(\n\t            alignment=alignment,\n\t            performance_data=predicted_ppart,\n\t            score_data=part,\n\t            out=args.midi_fn.replace(\".mid\", \".match\"),\n", "            piece=os.path.basename(args.score_fn),\n\t            performer=\"Basis Mixer\",\n\t        )\n"]}
{"filename": "paow/minimal_midi/test_all_notes.py", "chunked_list": ["import mido\n\timport time\n\tprint(mido.get_output_names())\n\toutput_name = mido.get_output_names()[-1]\n\to = mido.open_output(output_name)\n\tfor k in range(21,100):\n\t    o.send(mido.Message(\"note_on\", note = k, velocity = 30))\n\ttime.sleep(0.5)\n\tfor k in range(21,100):\n\t    o.send(mido.Message(\"note_off\", note = k, velocity = 0))\n", "time.sleep(0.5)\n\tfor k in range(21,77):\n\t    o.send(mido.Message(\"note_on\", note = k, velocity = 30))\n\ttime.sleep(0.5)\n\tfor k in range(21,77):\n\t    o.send(mido.Message(\"note_off\", note = k, velocity = 0))\n"]}
{"filename": "paow/minimal_midi/test_minimal.py", "chunked_list": ["import mido\n\timport time\n\tprint(mido.get_output_names())\n\tms = mido.Message(\"note_on\", note = 64, velocity = 54)\n\tme = mido.Message(\"note_off\", note = 64, velocity = 0)\n\toutput_name = mido.get_output_names()[-1]\n\to = mido.open_output(output_name)\n\tfor k in range(19):\n\t    o.send(ms)\n\t    time.sleep(0.5)\n", "    o.send(me)\n\t    time.sleep(0.1)\n"]}
{"filename": "paow/minimal_midi/test_midi_aerophone.py", "chunked_list": ["import mido\n\t\"\"\"\n\ttest the different control change message produced and accepted by the aerophone.\n\t\"\"\"\n\tprint(mido.get_output_names(), mido.get_input_names())\n\t# if oport is not None:\n\t#     oport.close()\n\toport = mido.open_output( 'AE-30 1')\n\tiport  = mido.open_input('AE-30 0')\n\tdef f(no, val):\n", "    oport.send(mido.Message(\"control_change\",\n\t                                channel=0,\n\t                                control=no, # control number k, can be mapped later as you like\n\t                                value=val))\n\tdef g():\n\t    for msg in iport:\n\t        if msg.is_cc():\n\t            # 2 = breath, 3 too?, 11, 9\n\t            # 4 button\n\t            if msg.control == 95:# in [2,3,9,11]: \n", "                print(msg)\n\tdef h():\n\t    iport.close()\n\t    oport.close()"]}
{"filename": "paow/minimal_midi/test_midi_aerophone2.py", "chunked_list": ["import mido\n\timport warnings\n\timport numpy as np\n\t\"\"\"\n\tforce notes from aerophone to s predefined scale.\n\t\"\"\"\n\tdef scalify(input_array, \n\t            offset = 0, \n\t            scale = np.array([0,0,2,3,3,5,5,7,7,9,10,10])):\n\t    \"\"\"\n", "    forces notes in an array to some scale\n\t    \"\"\"\n\t    output_array = (input_array-offset)-(input_array-offset)%12 + scale[(input_array-offset)%12]+ offset\n\t    return output_array\n\tdef f():\n\t    if oport is not None:\n\t        oport.close()\n\t    if iport is not None:\n\t        iport.close()\n\tdef g():\n", "    sounding_notes = {}\n\t    for msg in iport:\n\t        note_on = msg.type == \"note_on\"\n\t        note_off = msg.type == \"note_off\"\n\t        if (note_on or note_off):\n\t            # hash sounding note\n\t            pitch =  msg.note\n\t            # start note if it's a 'note on' event with velocity > 0\n\t            if note_on and msg.velocity > 0:\n\t                repitch = scalify(pitch-12)\n", "                # save the onset time and velocity\n\t                sounding_notes[pitch] = (repitch)\n\t                print(pitch, repitch)\n\t                oport.send(mido.Message(\"note_on\",\n\t                                channel=msg.channel,\n\t                                note=repitch,\n\t                                velocity=msg.velocity))\n\t            # end note if it's a 'note off' event or 'note on' with velocity 0\n\t            elif note_off or (note_on and msg.velocity == 0):\n\t                if pitch not in sounding_notes:\n", "                    warnings.warn(\"ignoring MIDI message %s\" % msg)\n\t                    continue\n\t                else:\n\t                    oport.send(mido.Message(\"note_off\",\n\t                                channel=msg.channel,\n\t                                note=sounding_notes[pitch],\n\t                                velocity=0))\n\t                    del sounding_notes[pitch]\n\tif __name__ == \"__main__\":\n\t    print(mido.get_output_names(), mido.get_input_names())\n", "    oport = mido.open_output( 'seq 2')\n\t    iport  = mido.open_input('3- AE-30 3')"]}
