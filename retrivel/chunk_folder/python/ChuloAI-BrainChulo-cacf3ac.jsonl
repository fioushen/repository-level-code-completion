{"filename": "download-model.py", "chunked_list": ["'''\n\tDownloads models from Hugging Face to models/model-name.\n\tSource: https://github.com/oobabooga/text-generation-webui\n\tExample:\n\tpython download-model.py facebook/opt-1.3b\n\t'''\n\timport argparse\n\timport base64\n\timport datetime\n\timport hashlib\n", "import json\n\timport re\n\timport sys\n\tfrom pathlib import Path\n\timport requests\n\timport tqdm\n\tfrom tqdm.contrib.concurrent import thread_map\n\tdef select_model_from_default_options():\n\t    models = {\n\t        \"Vicuna-7B-1.1-GPTQ-4bit-128g\": (\"TheBloke\", \"vicuna-7B-1.1-GPTQ-4bit-128g\", \"main\"),\n", "    }\n\t    choices = {}\n\t    print(\"Select the model that you want to download:\\n\")\n\t    for i, name in enumerate(models):\n\t        char = chr(ord('A') + i)\n\t        choices[char] = name\n\t        print(f\"{char}) {name}\")\n\t    char = chr(ord('A') + len(models))\n\t    print(f\"{char}) None of the above\")\n\t    print()\n", "    print(\"Input> \", end='')\n\t    choice = input()[0].strip().upper()\n\t    if choice == char:\n\t        print(\n\t            \"\"\"\\nThen type the name of your desired Hugging Face model in the format organization/name.\n\tExamples:\n\tfacebook/opt-1.3b\n\tEleutherAI/pythia-1.4b-deduped\n\t\"\"\")\n\t        print(\"Input> \", end='')\n", "        model = input()\n\t        branch = \"main\"\n\t    else:\n\t        arr = models[choices[choice]]\n\t        model = f\"{arr[0]}/{arr[1]}\"\n\t        branch = arr[2]\n\t    return model, branch\n\tdef sanitize_model_and_branch_names(model, branch):\n\t    if model[-1] == '/':\n\t        model = model[:-1]\n", "    if branch is None:\n\t        branch = \"main\"\n\t    else:\n\t        pattern = re.compile(r\"^[a-zA-Z0-9._-]+$\")\n\t        if not pattern.match(branch):\n\t            raise ValueError(\n\t                \"Invalid branch name. Only alphanumeric characters, period, underscore and dash are allowed.\")\n\t    return model, branch\n\tdef get_download_links_from_huggingface(model, branch, text_only=False):\n\t    base = \"https://huggingface.co\"\n", "    page = f\"/api/models/{model}/tree/{branch}?cursor=\"\n\t    cursor = b\"\"\n\t    links = []\n\t    sha256 = []\n\t    classifications = []\n\t    has_pytorch = False\n\t    has_pt = False\n\t    has_ggml = False\n\t    has_safetensors = False\n\t    is_lora = False\n", "    while True:\n\t        if len(cursor) > 0:\n\t            page = f\"/api/models/{model}/tree/{branch}?cursor=\"\n\t            content = requests.get(f\"{base}{page}{cursor.decode()}\").content\n\t        else:\n\t            page = f\"/api/models/{model}/tree/{branch}\"\n\t            content = requests.get(f\"{base}{page}\").content\n\t        dict = json.loads(content)\n\t        if len(dict) == 0:\n\t            break\n", "        for i in range(len(dict)):\n\t            fname = dict[i]['path']\n\t            if not is_lora and fname.endswith(\n\t                    ('adapter_config.json', 'adapter_model.bin')):\n\t                is_lora = True\n\t            is_pytorch = re.match(\"(pytorch|adapter)_model.*\\\\.bin\", fname)\n\t            is_safetensors = re.match(\".*\\\\.safetensors\", fname)\n\t            is_pt = re.match(\".*\\\\.pt\", fname)\n\t            is_ggml = re.match(\"ggml.*\\\\.bin\", fname)\n\t            is_tokenizer = re.match(\"tokenizer.*\\\\.model\", fname)\n", "            is_text = re.match(\".*\\\\.(txt|json|py|md)\", fname) or is_tokenizer\n\t            if any(\n\t                (is_pytorch,\n\t                 is_safetensors,\n\t                 is_pt,\n\t                 is_ggml,\n\t                 is_tokenizer,\n\t                 is_text)):\n\t                if 'lfs' in dict[i]:\n\t                    sha256.append([fname, dict[i]['lfs']['oid']])\n", "                if is_text:\n\t                    links.append(\n\t                        f\"https://huggingface.co/{model}/resolve/{branch}/{fname}\")\n\t                    classifications.append('text')\n\t                    continue\n\t                if not text_only:\n\t                    links.append(\n\t                        f\"https://huggingface.co/{model}/resolve/{branch}/{fname}\")\n\t                    if is_safetensors:\n\t                        has_safetensors = True\n", "                        classifications.append('safetensors')\n\t                    elif is_pytorch:\n\t                        has_pytorch = True\n\t                        classifications.append('pytorch')\n\t                    elif is_pt:\n\t                        has_pt = True\n\t                        classifications.append('pt')\n\t                    elif is_ggml:\n\t                        has_ggml = True\n\t                        classifications.append('ggml')\n", "        cursor = base64.b64encode(\n\t            f'{{\"file_name\":\"{dict[-1][\"path\"]}\"}}'.encode()) + b':50'\n\t        cursor = base64.b64encode(cursor)\n\t        cursor = cursor.replace(b'=', b'%3D')\n\t    # If both pytorch and safetensors are available, download safetensors only\n\t    if (has_pytorch or has_pt) and has_safetensors:\n\t        for i in range(len(classifications) - 1, -1, -1):\n\t            if classifications[i] in ['pytorch', 'pt']:\n\t                links.pop(i)\n\t    return links, sha256, is_lora\n", "def get_output_folder(model, branch, is_lora, base_folder=None):\n\t    if base_folder is None:\n\t        base_folder = 'models' if not is_lora else 'loras'\n\t    output_folder = f\"{'_'.join(model.split('/')[-2:])}\"\n\t    if branch != 'main':\n\t        output_folder += f'_{branch}'\n\t    output_folder = Path(base_folder) / output_folder\n\t    return output_folder\n\tdef get_single_file(url, output_folder, start_from_scratch=False):\n\t    filename = Path(url.rsplit('/', 1)[1])\n", "    output_path = output_folder / filename\n\t    if output_path.exists() and not start_from_scratch:\n\t        # Check if the file has already been downloaded completely\n\t        r = requests.get(url, stream=True)\n\t        total_size = int(r.headers.get('content-length', 0))\n\t        if output_path.stat().st_size >= total_size:\n\t            return\n\t        # Otherwise, resume the download from where it left off\n\t        headers = {'Range': f'bytes={output_path.stat().st_size}-'}\n\t        mode = 'ab'\n", "    else:\n\t        headers = {}\n\t        mode = 'wb'\n\t    r = requests.get(url, stream=True, headers=headers)\n\t    with open(output_path, mode) as f:\n\t        total_size = int(r.headers.get('content-length', 0))\n\t        block_size = 1024\n\t        with tqdm.tqdm(total=total_size, unit='iB', unit_scale=True, bar_format='{l_bar}{bar}| {n_fmt:6}/{total_fmt:6} {rate_fmt:6}') as t:\n\t            for data in r.iter_content(block_size):\n\t                t.update(len(data))\n", "                f.write(data)\n\tdef start_download_threads(\n\t        file_list,\n\t        output_folder,\n\t        start_from_scratch=False,\n\t        threads=1):\n\t    thread_map(\n\t        lambda url: get_single_file(\n\t            url,\n\t            output_folder,\n", "            start_from_scratch=start_from_scratch),\n\t        file_list,\n\t        max_workers=threads,\n\t        disable=True)\n\tdef download_model_files(\n\t        model,\n\t        branch,\n\t        links,\n\t        sha256,\n\t        output_folder,\n", "        start_from_scratch=False,\n\t        threads=1):\n\t    # Creating the folder and writing the metadata\n\t    if not output_folder.exists():\n\t        output_folder.mkdir()\n\t    with open(output_folder / 'huggingface-metadata.txt', 'w') as f:\n\t        f.write(f'url: https://huggingface.co/{model}\\n')\n\t        f.write(f'branch: {branch}\\n')\n\t        f.write(\n\t            f'download date: {str(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))}\\n')\n", "        sha256_str = ''\n\t        for i in range(len(sha256)):\n\t            sha256_str += f'    {sha256[i][1]} {sha256[i][0]}\\n'\n\t        if sha256_str != '':\n\t            f.write(f'sha256sum:\\n{sha256_str}')\n\t    # Downloading the files\n\t    print(f\"Downloading the model to {output_folder}\")\n\t    start_download_threads(\n\t        links,\n\t        output_folder,\n", "        start_from_scratch=start_from_scratch,\n\t        threads=threads)\n\tdef check_model_files(model, branch, links, sha256, output_folder):\n\t    # Validate the checksums\n\t    validated = True\n\t    for i in range(len(sha256)):\n\t        fpath = (output_folder / sha256[i][0])\n\t        if not fpath.exists():\n\t            print(f\"The following file is missing: {fpath}\")\n\t            validated = False\n", "            continue\n\t        with open(output_folder / sha256[i][0], \"rb\") as f:\n\t            bytes = f.read()\n\t            file_hash = hashlib.sha256(bytes).hexdigest()\n\t            if file_hash != sha256[i][1]:\n\t                print(f'Checksum failed: {sha256[i][0]}  {sha256[i][1]}')\n\t                validated = False\n\t            else:\n\t                print(f'Checksum validated: {sha256[i][0]}  {sha256[i][1]}')\n\t    if validated:\n", "        print('[+] Validated checksums of all model files!')\n\t    else:\n\t        print('[-] Invalid checksums. Rerun download-model.py with the --clean flag.')\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument('MODEL', type=str, default=None, nargs='?')\n\t    parser.add_argument('--branch', type=str, default='main',\n\t                        help='Name of the Git branch to download from.')\n\t    parser.add_argument('--threads', type=int, default=1,\n\t                        help='Number of files to download simultaneously.')\n", "    parser.add_argument('--text-only', action='store_true',\n\t                        help='Only download text files (txt/json).')\n\t    parser.add_argument('--output', type=str, default=None,\n\t                        help='The folder where the model should be saved.')\n\t    parser.add_argument('--clean', action='store_true',\n\t                        help='Does not resume the previous download.')\n\t    parser.add_argument('--check', action='store_true',\n\t                        help='Validates the checksums of model files.')\n\t    args = parser.parse_args()\n\t    branch = args.branch\n", "    model = args.MODEL\n\t    if model is None:\n\t        model, branch = select_model_from_default_options()\n\t    # Cleaning up the model/branch names\n\t    try:\n\t        model, branch = sanitize_model_and_branch_names(model, branch)\n\t    except ValueError as err_branch:\n\t        print(f\"Error: {err_branch}\")\n\t        sys.exit()\n\t    # Getting the download links from Hugging Face\n", "    links, sha256, is_lora = get_download_links_from_huggingface(\n\t        model, branch, text_only=args.text_only)\n\t    # Getting the output folder\n\t    output_folder = get_output_folder(\n\t        model, branch, is_lora, base_folder=args.output)\n\t    if args.check:\n\t        # Check previously downloaded files\n\t        check_model_files(model, branch, links, sha256, output_folder)\n\t    else:\n\t        # Download files\n", "        download_model_files(model, branch, links, sha256,\n\t                             output_folder, threads=args.threads)\n"]}
{"filename": "bootstrap_models.py", "chunked_list": ["import sys\n\timport os\n\timport subprocess\n\tDEFAULT_EMBEDDINGS_MODEL =\"sentence-transformers/all-MiniLM-L6-v2\"\n\tDEFAULT_MODEL = \"openlm-research/open_llama_3b\"\n\tMODEL_DIR = \"./models\"\n\tdef _download_if_not_exists(model):\n\t    models_dir = os.path.join(MODEL_DIR, model)\n\t    if os.path.exists(models_dir):\n\t        print(f\"Directory {models_dir} already exists! Skpping download!\")\n", "    else:\n\t        print(f\"Downloading model {model} to {models_dir}\")\n\t        print(\"Please note that if model is large this may take a while.\")\n\t        process = subprocess.run([\"python3\",  \"download-model.py\", model, \"--output\", MODEL_DIR], capture_output=True)\n\t        process.check_returncode()\n\tdef main(model, embeddings_model):\n\t    print(f\"\"\"Your choices:\n\tMODEL: {model}\n\tEMBEDDINGS MODEL: {embeddings_model}\n\t\"\"\")\n", "    try:\n\t        os.mkdir(MODEL_DIR)\n\t    except FileExistsError:\n\t        pass\n\t    _download_if_not_exists(embeddings_model)\n\t    _download_if_not_exists(model)\n\t    print(\"Success!\")\n\tif __name__ == \"__main__\":\n\t    model = None\n\t    embeddings = None\n", "    if len(sys.argv) > 2:\n\t        embeddings = sys.argv[2]\n\t    if len(sys.argv) > 1:\n\t        model = sys.argv[1]\n\t    if len(sys.argv) == 1:\n\t        print(\n\t            \"NOTE: You can change the default downloaded model by passing an additional argument:\"\n\t            + f\"{sys.argv[0]} [hugging-face-llm-model-name] [hugging-face-embeddings-model-name]\"\n\t        )\n\t    if not embeddings:\n", "        embeddings = DEFAULT_EMBEDDINGS_MODEL\n\t    if not model:\n\t        model = DEFAULT_MODEL\n\t    main(model, embeddings)"]}
{"filename": "app/settings.py", "chunked_list": ["import os\n\tfrom dotenv import load_dotenv\n\timport logging\n\tfrom langchain.embeddings import (\n\t    HuggingFaceEmbeddings,\n\t    HuggingFaceInstructEmbeddings,\n\t)\n\tclass CustomFormatter(logging.Formatter):\n\t    grey = \"\\x1b[38;20m\"\n\t    yellow = \"\\x1b[33;20m\"\n", "    red = \"\\x1b[31;20m\"\n\t    bold_red = \"\\x1b[31;1m\"\n\t    reset = \"\\x1b[0m\"\n\t    format = \"%(asctime)s - %(name)s - %(levelname)s - %(message)s (%(filename)s:%(lineno)d)\"\n\t    FORMATS = {\n\t        logging.DEBUG: grey + format + reset,\n\t        logging.INFO: grey + format + reset,\n\t        logging.WARNING: yellow + format + reset,\n\t        logging.ERROR: red + format + reset,\n\t        logging.CRITICAL: bold_red + format + reset,\n", "    }\n\t    def format(self, record):\n\t        log_fmt = self.FORMATS.get(record.levelno)\n\t        formatter = logging.Formatter(log_fmt)\n\t        return formatter.format(record)\n\tlogging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n\tlogger = logging.getLogger(__name__)\n\tlogger.setLevel(logging.DEBUG)\n\t# create console handler with a higher log level\n\tch = logging.StreamHandler()\n", "ch.setLevel(logging.DEBUG)\n\tch.setFormatter(CustomFormatter())\n\tlogger.addHandler(ch)\n\tclass Settings:\n\t    _instance = None\n\t    def __new__(cls):\n\t        if cls._instance is None:\n\t            cls._instance = super().__new__(cls)\n\t        return cls._instance\n\t    def __init__(self):\n", "        load_dotenv()\n\t        # Chat API - By default, we are assuming Oobabooga's Text Generation\n\t        # WebUI is running\n\t        self.root_path = os.getcwd()\n\t        if self.root_path.endswith(\"app\"):\n\t            self.root_path = self.root_path[:-4]\n\t        self.model_root_path = os.path.join(self.root_path, \"models\")\n\t        self.backend_root_path = os.path.join(self.root_path, \"app\")\n\t        # let's ensure the models and backend path is accurate when mounting docker volumes\n\t        if self.root_path.startswith(\"/code\"):\n", "            self.model_root_path = \"/models\"\n\t            self.backend_root_path = self.root_path\n\t        # Guidance new settings\n\t        self.test_file = os.getenv(\"TEST_FILE\", \"/data/uploads/the_trial.txt\")\n\t        self.embeddings_map = {\n\t            **{name: HuggingFaceInstructEmbeddings for name in [\"hkunlp/instructor-xl\", \"hkunlp/instructor-large\"]},\n\t            **{name: HuggingFaceEmbeddings for name in [\"all-MiniLM-L6-v2\", \"sentence-t5-xxl\"]},\n\t        }\n\t        self.persist_directory = os.getenv(\"PERSIST_DIRECTORY\", \"./persist_directory\")\n\t        self.embeddings_model = os.getenv(\"EMBEDDINGS_MODEL\", f\"{self.model_root_path}/all-MiniLM-L6-v2\")\n", "        self.chat_api_url = os.getenv(\"CHAT_API_URL\", \"http://0.0.0.0:5000/api/v1/generate\")\n\t        self.model_path = self.model_root_path + os.getenv(\"MODEL_PATH\")\n\t        self.guidance_reasoning_model_path = self.model_root_path  + os.getenv(\"GUIDANCE_REASONING_MODEL_PATH\")\n\t        self.guidance_extraction_model_path = self.model_root_path  + os.getenv(\"GUIDANCE_EXTRACTION_MODEL_PATH\")\n\t        # Where all data is stored\n\t        self.data_path = os.getenv(\"DATA_PATH\", f\"{self.backend_root_path}/data\")\n\t        # Where short-term memory is stored\n\t        self.memories_path = os.getenv(\"MEMORIES_PATH\", f\"{self.data_path}/memories\")\n\t        # Where uploads are saved\n\t        self.upload_path = os.getenv(\"UPLOAD_PATH\", f\"{self.data_path}/uploads\")\n", "        # Where conversation history is stored\n\t        self.conversation_history_path = os.getenv(\n\t            \"CONVERSATION_HISTORY_PATH\",\n\t            f\"{self.data_path}/conversation_history/\",\n\t        )\n\t        # Document store name\n\t        self.document_store_name = os.getenv(\"DOCUMENT_STORE_NAME\", \"brainchulo_docs\")\n\t        self.conversation_store_name = os.getenv(\"CONVERSATION_STORE_NAME\", \"brainchulo_convos\")\n\t        # Default objective - If we go objective-based, this is the default\n\t        self.default_objective = os.getenv(\"DEFAULT_OBJECTIVE\", \"Be a CEO.\")\n", "        # Database URL\n\t        self.database_url = os.getenv(\"DATABASE_URL\", \"sqlite:///data/brainchulo.db\")\n\t        self.andromeda_url = os.getenv(\"ANDROMEDA_URL\", \"http://0.0.0.0:9000\")\n\t        self.use_flow_agents = os.getenv(\"USE_FLOW_AGENTS\", \"false\") == \"true\"\n\tdef load_config():\n\t    return Settings()\n"]}
{"filename": "app/main.py", "chunked_list": ["import os\n\timport shutil\n\tfrom fastapi import FastAPI, Depends, UploadFile\n\tfrom fastapi.middleware.cors import CORSMiddleware\n\tfrom sqlmodel import SQLModel, create_engine, Session, desc\n\tfrom models.all import Conversation, Message, ConversationWithMessages\n\tfrom typing import List\n\tfrom settings import load_config, logger\n\tfrom plugins import load_plugins\n\tfrom alembic import command\n", "from alembic.config import Config\n\tfrom configparser import ConfigParser\n\tconfig = load_config()\n\tsqlite_database_url = config.database_url\n\tconnect_args = {\"check_same_thread\": False}\n\tengine = create_engine(sqlite_database_url, echo=True, connect_args=connect_args)\n\t# Introducing a new feature flag\n\t# So GuidanceLLaMAcpp can coexist with FlowAgents\n\tif config.use_flow_agents:\n\t    logger.info(\"Using (experimental) flow agents\")\n", "    from conversations.document_based_flow import DocumentBasedConversationFlowAgent\n\t    convo = DocumentBasedConversationFlowAgent()\n\telse:\n\t    logger.info(\"Using experimental Guidance LLaMA cpp implementation.\")\n\t    from conversations.document_based import DocumentBasedConversation\n\t    convo = DocumentBasedConversation()\n\tdef create_db_and_tables():\n\t    confparser = ConfigParser()\n\t    confparser.read(f\"{config.backend_root_path}/alembic.ini\")\n\t    confparser.set('alembic', 'script_location', f\"{config.backend_root_path}/migrations\")\n", "    confparser.set('alembic', 'prepend_sys_path', config.backend_root_path)\n\t    migrations_config_path = os.path.join(config.backend_root_path, \"generated_alembic.ini\")\n\t    with open(migrations_config_path, 'w') as config_file:\n\t        confparser.write(config_file)\n\t    migrations_config = Config(migrations_config_path)\n\t    command.upgrade(migrations_config, \"head\")\n\tdef get_session():\n\t    with Session(engine) as session:\n\t        yield session\n\tapp = FastAPI()\n", "# Load the plugins\n\tload_plugins(app=app)\n\torigins = [\n\t    \"http://127.0.0.1:5173\",\n\t    \"http://localhost:5173\",\n\t    \"http://0.0.0.0:5173\",\n\t]\n\tapp.add_middleware(\n\t    CORSMiddleware,\n\t    allow_origins=[\"*\"],\n", "    allow_credentials=True,\n\t    allow_methods=[\"*\"],\n\t    allow_headers=[\"*\"],\n\t)\n\t@app.on_event(\"startup\")\n\tdef on_startup():\n\t    create_db_and_tables()\n\t@app.post('/llm/query/', response_model=str)\n\tdef llm_query(*, query: str, session: Session = Depends(get_session)):\n\t    \"\"\"\n", "    Query the LLM\n\t    \"\"\"\n\t    return convo.predict(query, [])\n\t@app.post(\"/conversations\", response_model=Conversation)\n\tdef create_conversation(*, session: Session = Depends(get_session), conversation: Conversation):\n\t    \"\"\"\n\t    Create a new conversation.\n\t    \"\"\"\n\t    conversation = Conversation.from_orm(conversation)\n\t    session.add(conversation)\n", "    session.commit()\n\t    session.refresh(conversation)\n\t    print(str(conversation))\n\t    return conversation\n\t@app.put('/conversations/{conversation_id}', response_model=Conversation)\n\tdef update_conversation(*, session: Session = Depends(get_session), conversation_id: int, payload: dict):\n\t    \"\"\"\n\t    Update the title of a conversation.\n\t    \"\"\"\n\t    conversation = session.get(Conversation, conversation_id)\n", "    conversation.title = payload[\"title\"]\n\t    session.add(conversation)\n\t    session.commit()\n\t    session.refresh(conversation)\n\t    return conversation\n\t@app.delete(\"/conversations/{conversation_id}\")\n\tdef delete_conversation(*, session: Session = Depends(get_session), conversation_id: int):\n\t    \"\"\"\n\t    Delete a conversation.\n\t    \"\"\"\n", "    conversation = session.get(Conversation, conversation_id)\n\t    session.delete(conversation)\n\t    session.commit()\n\t    return conversation\n\t@app.get(\"/conversations\", response_model=List[Conversation])\n\tdef get_conversations(session: Session = Depends(get_session)):\n\t    \"\"\"\n\t    Get all conversations.\n\t    \"\"\"\n\t    return session.query(Conversation).order_by(desc(Conversation.id)).all()\n", "@app.get(\"/conversations/{conversation_id}\", response_model=ConversationWithMessages)\n\tdef get_conversation(conversation_id: int, session: Session = Depends(get_session)):\n\t    \"\"\"\n\t    Get a conversation by id.\n\t    \"\"\"\n\t    conversation = session.get(Conversation, conversation_id)\n\t    return conversation\n\t@app.post(\"/conversations/{conversation_id}/messages\", response_model=Message)\n\tdef create_message(*, session: Session = Depends(get_session), conversation_id: int, message: Message):\n\t    \"\"\"\n", "    Create a new message.\n\t    \"\"\"\n\t    message = Message.from_orm(message)\n\t    session.add(message)\n\t    session.commit()\n\t    session.refresh(message)\n\t    return message\n\t@app.post(\"/conversations/{conversation_id}/files\", response_model=dict)\n\tdef upload_file(*, conversation_id: int, file: UploadFile):\n\t    \"\"\"\n", "    Upload a file.\n\t    \"\"\"\n\t    try:\n\t        uploaded_file_name = file.filename\n\t        filepath = os.path.join(os.getcwd(), \"data\", config.upload_path, uploaded_file_name)\n\t        os.makedirs(os.path.dirname(filepath), mode=0o777, exist_ok=True)\n\t        with open(filepath, \"wb\") as f:\n\t            shutil.copyfileobj(file.file, f)\n\t        convo.load_document(filepath, conversation_id)\n\t        return {\"text\": f\"{uploaded_file_name} has been loaded into memory for this conversation.\"}\n", "    except Exception as e:\n\t        logger.error(f\"Error adding file to history: {e}\")\n\t        return f\"Error adding file to history: {e}\"\n\t@app.post('/llm/{conversation_id}/', response_model=str)\n\tdef llm(*, conversation_id: str, query: str, session: Session = Depends(get_session)):\n\t    \"\"\"\n\t    Query the LLM\n\t    \"\"\"\n\t    conversation_data = get_conversation(conversation_id, session)\n\t    history = conversation_data.messages\n", "    return convo.predict(query, conversation_id)\n\t    # we could also work from history only\n\t    # return convo.predict(query, history)\n\t@app.post(\"/conversations/{conversation_id}/messages/{message_id}/upvote\", response_model=Message)\n\tdef upvote_message(*, session: Session = Depends(get_session), conversation_id: int, message_id: int):\n\t    \"\"\"\n\t    Upvote a message.\n\t    \"\"\"\n\t    message = session.get(Message, message_id)\n\t    message.rating = 1\n", "    session.add(message)\n\t    session.commit()\n\t    session.refresh(message)\n\t    return message\n\t@app.post(\"/conversations/{conversation_id}/messages/{message_id}/downvote\", response_model=Message)\n\tdef downvote_message(*, session: Session = Depends(get_session), conversation_id: int, message_id: int):\n\t    \"\"\"\n\t    Downvote a message.\n\t    \"\"\"\n\t    message = session.get(Message, message_id)\n", "    message.rating = -1\n\t    session.add(message)\n\t    session.commit()\n\t    session.refresh(message)\n\t    return message\n\t@app.post(\"/conversations/{conversation_id}/messages/{message_id}/resetVote\", response_model=Message)\n\tdef reset_message_vote(*, session: Session = Depends(get_session), conversation_id: int, message_id: int):\n\t    \"\"\"\n\t    Reset a message vote.\n\t    \"\"\"\n", "    message = session.get(Message, message_id)\n\t    message.rating = 0\n\t    session.add(message)\n\t    session.commit()\n\t    session.refresh(message)\n\t    return message\n\t@app.post(\"/reset\", response_model=dict)\n\tdef reset_all():\n\t    \"\"\"\n\t    Reset the database.\n", "    \"\"\"\n\t    SQLModel.metadata.drop_all(engine)\n\t    print(\"Database has been reset.\")\n\t    SQLModel.metadata.create_all(engine)\n\t    return {\"text\": \"Database has been reset.\"}\n\tif __name__ == \"__main__\":\n\t    import uvicorn\n\t    uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=7865, reload=False)\n"]}
{"filename": "app/__init__.py", "chunked_list": []}
{"filename": "app/plugins/__init__.py", "chunked_list": ["import importlib\n\timport pkgutil\n\timport os\n\tdef load_plugins(app):\n\t    # Get the current directory\n\t    plugins_dir = os.path.dirname(__file__)\n\t    # Iterate over the files in the plugins directory\n\t    for _, plugin_name, is_package in pkgutil.iter_modules([plugins_dir]):\n\t        if is_package:\n\t            # Dynamically import the routes module from each plugin\n", "            plugin_module = importlib.import_module(f\"plugins.{plugin_name}.routes\")\n\t            # Get the router from the plugin's routes module\n\t            router = plugin_module.router\n\t            # Include the plugin's routes in the main FastAPI app\n\t            app.include_router(router, prefix=f\"/plugins/{plugin_name}\")\n"]}
{"filename": "app/plugins/sample_plugin/database.py", "chunked_list": ["from datetime import datetime\n\tfrom typing import Optional, List\n\tfrom sqlmodel import SQLModel, Field, Relationship\n\tfrom models.all import Conversation\n\tclass SamplePluginModelBase(SQLModel):\n\t    \"\"\"A base model for SamplePluginModel\"\"\"\n\t    # __tablename__: str = 'sample_plugin_model'\n\t    created_at: datetime = Field(default_factory=datetime.utcnow)\n\t    title: Optional[str]\n\t    conversation_id: Optional[int] = Field(default=None, foreign_key=\"conversation.id\")\n", "class SamplePluginModel(SamplePluginModelBase, table=True):\n\t    \"\"\"A model for SamplePlugin\"\"\"\n\t    id: Optional[int] = Field(default=None, primary_key=True)\n\tclass SamplePluginModelRead(SamplePluginModelBase):\n\t    \"\"\"A read model for SamplePlugin\"\"\"\n\t    id: int\n"]}
{"filename": "app/plugins/sample_plugin/__init__.py", "chunked_list": []}
{"filename": "app/plugins/sample_plugin/routes.py", "chunked_list": ["from fastapi import APIRouter\n\trouter = APIRouter()\n\t@router.get(\"/\")\n\tdef sample_plugin_route():\n\t    return {\"message\": \"Sample plugin route\"}\n\t# Additional routes and functionality for the sample plugin\n"]}
{"filename": "app/tools/base.py", "chunked_list": ["from pyparsing import abstractmethod\n\tfrom typing import Dict, List, Any\n\tclass ToolFactory():\n\t    \"\"\"Instantiates tools with a reference to the current conversation_id\"\"\"\n\t    def __init__(self, list_of_tool_classes: List[\"BaseTool\"]) -> None:\n\t        self._tool_class_references = list_of_tool_classes\n\t    def build_tools(self, conversation_id, context: Dict[str, Any]) -> Dict[str, \"BaseTool\"]:\n\t        resolved_tools = {}\n\t        for tool_class in self._tool_class_references:\n\t            tool = tool_class(conversation_id, context)\n", "            resolved_tools[tool.name] = tool\n\t        return resolved_tools\n\tclass BaseTool:\n\t    \"\"\"Base interface expected of tools\"\"\"\n\t    def __init__(self, conversation_id: str, name: str, tool_context: Dict[str, Any], required_context_keys: List[str]):\n\t        \"\"\"Stores a reference to the conversation ID.\n\t        Avoiding injecting expensive operations in the __init__ method of the subclasses.\n\t        If you need to do something expensive, use a Singleton or redesign this Tool interface.\n\t        The reason being is that these Tools are instantied **per processed message**, so the\n\t        constructor must be cheap to execute.\n", "        \"\"\"\n\t        self.conversation_id = conversation_id\n\t        self.name = name\n\t        self.tool_context = tool_context\n\t        self._validate_context_keys(required_context_keys, tool_context)\n\t    def _validate_context_keys(self, keys, context):\n\t        for key in keys:\n\t            if key not in context:\n\t                raise TypeError(f\"This instance of {self.__class__.__name__} requires variable {key} in context.\")\n\t    @abstractmethod\n", "    def short_description(self) -> str:\n\t        \"\"\"Returns a short description of the tool.\"\"\"\n\t        raise NotImplementedError()\n\t    @abstractmethod\n\t    def few_shot_examples(self) -> str:\n\t        \"\"\"Returns few \"\"\"\n\t        raise NotImplementedError()\n\t    @abstractmethod\n\t    def __call__(self, variables: Dict[str, str]) -> str:\n\t        \"\"\"Executes the tool\"\"\"\n", "        raise NotImplementedError()\n"]}
{"filename": "app/tools/conversation_memory.py", "chunked_list": ["from tools.base import BaseTool\n\tfrom tools.utils import _build_conversation_filter\n\tfrom typing import Dict, Any\n\tclass ConversationSearchTool(BaseTool):\n\t    def __init__(self, conversation_id: str, tool_context: Dict[str, Any], name: str = \"Conversation Search\"):\n\t        required_context_keys = [\"vector_store_convs\", \"k\"]\n\t        super().__init__(conversation_id, name, tool_context=tool_context, required_context_keys=required_context_keys)\n\t    def short_description(self) -> str:\n\t        return \"A tool to search in your memory previous conversations with this user.\"\n\t    def few_shot_examples(self) -> str:\n", "        return \"\"\"Question: What's your name?\n\tThought: I should search my name in the previous conversations.\n\tAction: Conversation Search\n\tAction Input:\n\tWhat's my name?\n\tObservation:\n\tUser: I'd like to give you a better name.\n\tBot: How would you like to call me?\n\tUser: I'd like to call you Joseph.\n\tBot: Alright, you may call me Joseph from now on.\n", "Thought: The user wants to call me Joseph.\n\tFinal Answer: As I recall from a previous conversation, you call me Joseph.\"\"\"\n\t    def __call__(self, search_input):\n\t        \"\"\"\n\t        Search for the given input in the vector store and return the top 10 most similar documents with their scores.\n\t        This function is used as a helper function for the SearchLongTermMemory tool\n\t        Args:\n\t          search_input (str): The input to search for in the vector store.\n\t        Returns:\n\t          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n", "        \"\"\"\n\t        k = self.tool_context[\"k\"]\n\t        filter_= _build_conversation_filter(conversation_id=self.conversation_id)\n\t        docs = self.tool_context[\"vector_store_convs\"].similarity_search_with_score(\n\t            search_input, k=5, filter=filter_\n\t        )\n\t        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]\n"]}
{"filename": "app/tools/document_memory.py", "chunked_list": ["from tools.base import BaseTool\n\tfrom tools.utils import _build_conversation_filter\n\tfrom typing import Dict, Any\n\tclass DocumentSearchTool(BaseTool):\n\t    def __init__(self, conversation_id: str, tool_context: Dict[str, Any], name: str = \"Document Search\"):\n\t        required_context_keys = [\"vector_store_docs\", \"k\"]\n\t        super().__init__(conversation_id, name, tool_context=tool_context, required_context_keys=required_context_keys)\n\t    def short_description(self) -> str:\n\t        return \"Useful for when you need to answer questions about documents that were uploaded by the user.\"\n\t    def few_shot_examples(self) -> str:\n", "        return \"\"\"Question: What's your name?\n\tThought: I should search my name in the documents.\n\tAction: Document Search\n\tAction Input:\n\tWhat's my name?\n\tObservation: You're an AI. You don't have a name.\n\tThought: I should answer that I don't have a name.\n\tFinal Answer: As an AI, I don't have a name, at least not in the human sense.\"\"\"\n\t    def __call__(self, search_input: Dict[str, str]) -> str:\n\t        \"\"\"Executes the tool\n", "        Search for the given input in the vector store and return the top k most similar documents with their scores.\n\t        This function is used as a helper function for the SearchLongTermMemory tool\n\t        Args:\n\t          search_input (str): The input to search for in the vector store.\n\t        Returns:\n\t          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n\t        \"\"\"\n\t        k = self.tool_context[\"k\"]\n\t        filter_= _build_conversation_filter(conversation_id=self.conversation_id)\n\t        docs = self.tool_context[\"vector_store_docs\"].similarity_search_with_score(\n", "            search_input, k=k, filter=filter_\n\t        )\n\t        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]\n"]}
{"filename": "app/tools/utils.py", "chunked_list": ["from typing import Optional\n\tdef _build_conversation_filter(conversation_id: Optional[str]):\n\t    if conversation_id is not None:\n\t        return {\"conversation_id\": conversation_id}\n\t    else:\n\t        return {}"]}
{"filename": "app/tools/web_access.py", "chunked_list": ["from settings import Settings\n\tfrom tools.base import BaseTool\n\tconfig = Settings.load_config()\n\tclass WebAccess(BaseTool):\n\t    url: str\n\t    def commands(self):\n\t        return {}\n"]}
{"filename": "app/agents/base.py", "chunked_list": ["# import the necessary libraries\n\tfrom abc import abstractmethod\n\tfrom colorama import Style, Fore\n\tfrom typing import List\n\tfrom andromeda_chain import AndromedaChain\n\tfrom tools.base import BaseTool\n\tdef color_print(msg, color):\n\t    print(color + Style.BRIGHT + msg  + Style.RESET_ALL, flush=True)\n\tclass BaseAgent:\n\t    \"\"\"Base Agent.\n", "    Nothing too exciting here.\n\t    \"\"\"\n\t    def __init__(self, andromeda: AndromedaChain, tools: List[BaseTool]):\n\t        self.andromeda = andromeda\n\t        self.tools = tools\n\t    @abstractmethod\n\t    def run(self, query: str) -> str:\n\t        raise NotImplementedError()\n\t    def do_tool(self, tool_name, act_input):\n\t        color_print(f\"Using tool: {tool_name}\", Fore.GREEN)\n", "        result = self.tools[tool_name](act_input)\n\t        color_print(f\"Tool result: {result}\", Fore.BLUE)\n\t        return result\n\t    def _build_tool_description_line(self, tool: BaseTool):\n\t        return f\"{tool.name}: {tool.short_description()}\"\n\t    def prepare_start_prompt(self, prompt_start_template):\n\t        tools = [item for _, item in self.tools.items()]\n\t        tools_descriptions = \"\\n\".join([self._build_tool_description_line(tool) for tool in tools])\n\t        action_list = str([tool.name for tool in tools]).replace('\"', \"\")\n\t        few_shot_examples = \"\\n\".join(\n", "            [\n\t                f\"Example {idx}:\\n{tool.few_shot_examples()}\"\n\t                for idx, tool in enumerate(tools)\n\t            ]\n\t        )\n\t        self.valid_tools = [tool.name for tool in tools]\n\t        self.prepared_prompt = prompt_start_template.format(\n\t            tools_descriptions=tools_descriptions,\n\t            action_list=action_list,\n\t            few_shot_examples=few_shot_examples\n", "        )\n"]}
{"filename": "app/agents/flow_cot.py", "chunked_list": ["from typing import Dict, Callable\n\tfrom andromeda_chain import AndromedaChain\n\tfrom agents.flow_based import BaseFlowAgent\n\tfrom flow.flow import Flow, PromptNode, ToolNode, ChoiceNode, StartNode\n\tfrom prompts.flow_guidance_cot import FlowChainOfThoughts, PROMPT_START_STRING\n\tclass ChainOfThoughtsFlowAgent(BaseFlowAgent):\n\t    def __init__(self, andromeda: AndromedaChain, tools: Dict[str, Callable[[str], str]]):\n\t        def execute_tool(variables):\n\t            action_name = variables[\"tool_name\"]\n\t            action_input = variables[\"act_input\"]\n", "            return self.do_tool(action_name, action_input)\n\t        start = StartNode(\"start\", FlowChainOfThoughts.flow_prompt_start, {\n\t            \"Action\": \"choose_action\",\n\t            \"Final Answer\": \"final_prompt\"\n\t        })\n\t        thought = PromptNode(\"thought\", FlowChainOfThoughts.thought_gen)\n\t        choose_action = PromptNode(\"choose_action\", FlowChainOfThoughts.choose_action)\n\t        perform_action = PromptNode(\"perform_action\", FlowChainOfThoughts.action_input)\n\t        execute_tool_node = ToolNode(\"execute_tool\", execute_tool)\n\t        decide = ChoiceNode(\"decide\", [\"thought\", \"final_prompt\"], max_decisions=3, force_exit_on=\"final_prompt\")\n", "        final = PromptNode(\"final_prompt\", FlowChainOfThoughts.final_prompt)\n\t        thought.set_next(choose_action)\n\t        choose_action.set_next(perform_action)\n\t        perform_action.set_next(execute_tool_node)\n\t        execute_tool_node.set_next(decide)\n\t        flow = Flow(\n\t            [start, thought, choose_action, perform_action, execute_tool_node, decide, final]\n\t        )\n\t        super().__init__(andromeda, tools, flow)\n\t        self.valid_answers = [\"Action\", \"Final Answer\"]\n", "        self.prepare_start_prompt(PROMPT_START_STRING)\n\t    def run(self, query: str) -> str:\n\t        if not self.prepared_prompt:\n\t            raise TypeError(\"Your inherinted BaseFlowAgent class must call 'prepare_start_prompt' in it's constructor.\")\n\t        return super().run(query, variables={\n\t            \"prompt_start\": self.prepared_prompt,\n\t            \"question\": query,\n\t            \"valid_tools\": self.valid_tools,\n\t            \"valid_answers\": self.valid_answers,\n\t        })\n"]}
{"filename": "app/agents/chain_of_thoughts.py", "chunked_list": ["from agents.base import BaseAgent\n\tfrom guidance_tooling.guidance_programs.tools import ingest_file\n\tfrom guidance_tooling.guidance_programs.tools import clean_text\n\tfrom langchain.llms import LlamaCpp\n\timport os\n\timport time\n\timport guidance\n\tfrom colorama import Fore, Style\n\tfrom langchain.chains import RetrievalQA\n\tfrom langchain.llms import LlamaCpp\n", "from prompt_templates.qa_agent import *\n\tfrom settings import load_config\n\timport re \n\tconfig = load_config()\n\tllm = None\n\tvalid_answers = ['Action', 'Final Answer']\n\tvalid_tools = [\"Check Question\", \"Google Search\"]\n\tTEST_FILE = os.getenv(\"TEST_FILE\")\n\tTEST_MODE = os.getenv(\"TEST_MODE\")\n\tETHICS = os.getenv(\"ETHICS\")\n", "QA_MODEL = os.getenv(\"MODEL_PATH\")\n\tmodel_path = config.model_path\n\tif ETHICS == \"ON\":\n\t    agent_template = QA_ETHICAL_AGENT\n\telse: \n\t    agent_template = QA_AGENT\n\tdef get_llm():\n\t    global llm\n\t    if llm is None:\n\t        print(\"Loading qa model...\")\n", "        model_path =QA_MODEL\n\t        model_n_ctx =1000\n\t        n_gpu_layers = 500\n\t        use_mlock = 0\n\t        n_batch = os.environ.get('N_BATCH') if os.environ.get('N_BATCH') != None else 512\n\t        callbacks = []\n\t        llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False,n_gpu_layers=n_gpu_layers, use_mlock=use_mlock,top_p=0.9, n_batch=n_batch)\n\t    return llm\n\tclass ChainOfThoughtsAgent(BaseAgent):\n\t    def __init__(self, guidance, llama_model, llama_model2):\n", "        self.guidance = guidance\n\t         # We first load the model in charge of reasoning along the guidance program\n\t        self.llama_model = llama_model\n\t        # We then load the model in charge of correctly identifying the data within the context and provide an answer\n\t        self.llama_model2 = llama_model2\n\t    def print_stage(self, stage_name, message):\n\t        print(Fore.CYAN + Style.BRIGHT + f\"Entering {stage_name} round\" + Style.RESET_ALL)\n\t        time.sleep(1)\n\t        print(Fore.GREEN + Style.BRIGHT + message + Style.RESET_ALL)\n\t    def searchQA(self, t):    \n", "        return self.checkQuestion(self.question, self.context)\n\t    def checkQuestion(self, question: str, context):\n\t        context = context\n\t        if TEST_MODE == \"ON\":\n\t            print(Fore.GREEN + Style.BRIGHT + \"No document loaded in conversation. Falling back on test file.\" + Style.RESET_ALL)\n\t            question = question.replace(\"Action Input: \", \"\")\n\t            qa = RetrievalQA.from_chain_type(llm=self.llm, chain_type=\"stuff\", retriever=self.retriever, return_source_documents=True)\n\t            answer_data = qa({\"query\": question})\n\t            if 'result' not in answer_data:\n\t                print(f\"\\033[1;31m{answer_data}\\033[0m\")\n", "                return \"Issue in retrieving the answer.\"\n\t            context_documents = answer_data['source_documents']\n\t            context = \" \".join([clean_text(doc.page_content) for doc in context_documents])\n\t            print(Fore.WHITE + Style.BRIGHT + \"Printing langchain context...\" + Style.RESET_ALL)\n\t            print(Fore.WHITE + Style.BRIGHT + context + Style.RESET_ALL)\n\t        return context\n\t    def ethics_check(self, question, ethics_prompt):\n\t        ethics_program = self.guidance(ethics_prompt)\n\t        return ethics_program(question=question)\n\t    def query_identification(self, question, conversation_prompt):\n", "        guidance.llm = self.llama_model\n\t        conversation_program = self.guidance(conversation_prompt) \n\t        return conversation_program(question=question)\n\t    def phatic_answer(self, question, history, phatic_prompt):\n\t        phatic_program = self.guidance(phatic_prompt)\n\t        return phatic_program(question=question, history=history)\n\t    def data_retrieval(self, question):\n\t        if self.llama_model2 is not None:\n\t            guidance.llm = self.llama_model2\n\t        referential_program = self.guidance(referential_prompt)\n", "        referential_round = referential_program(question=question, search=self.searchQA)\n\t        return referential_round\n\t    def answer_question(self, question, answer_prompt):\n\t        if self.llama_model2 is not None:\n\t            guidance.llm = self.llama_model2\n\t        answer_program = self.guidance(answer_prompt)\n\t        answer_round = answer_program(question=question, search=self.searchQA)\n\t        return answer_round[\"final_answer\"] \n\t    def run(self, query: str, context, history) -> str:\n\t        self.question = query \n", "        self.context = context\n\t        self.history = history\n\t        print(Fore.GREEN + Style.BRIGHT + \"Starting guidance agent...\" + Style.RESET_ALL)\n\t        conversation_round= self.query_identification(self.question , conversation_prompt)\n\t        if conversation_round[\"query_type\"] == \"Phatic\": \n\t            self.print_stage(\"answering\", \"User query identified as phatic\")\n\t            phatic_round = self.phatic_answer(self.question , history, phatic_prompt)\n\t            return phatic_round[\"phatic_answer\"]  \n\t        self.print_stage(\"data retrieval\", \"User query identified as referential\")\n\t        referential_round = self.data_retrieval(self.question )\n", "        if referential_round[\"answerable\"] == \"Yes\":\n\t            self.print_stage(\"answering\", \"Matching information found\")\n\t            return self.answer_question(self.question, answer_prompt)\n\t        else:\n\t            return \"I don't have enough information to answer.\"\n"]}
{"filename": "app/agents/__init__.py", "chunked_list": ["from agents.chain_of_thoughts import ChainOfThoughtsAgent\n\tfrom agents.flow_cot import ChainOfThoughtsFlowAgent"]}
{"filename": "app/agents/flow_based.py", "chunked_list": ["# import the necessary libraries\n\tfrom typing import Dict, Callable\n\tfrom agents.base import BaseAgent\n\tfrom colorama import Style\n\tfrom andromeda_chain import AndromedaChain\n\tfrom flow.flow import Flow\n\tdef color_print(msg, color):\n\t    print(color + Style.BRIGHT + msg  + Style.RESET_ALL)\n\tclass BaseFlowAgent(BaseAgent):\n\t    \"\"\"Base Flow Agent.\n", "    Implements a graph that the agents execute.\n\t    \"\"\"\n\t    def __init__(self, andromeda: AndromedaChain, tools: Dict[str, Callable[[str], str]], flow: Flow):\n\t        super().__init__(andromeda, tools)\n\t        self.flow = flow\n\t    def run(self, query: str, variables=None) -> str:\n\t        if not variables:\n\t            variables = {}\n\t        return self.flow.execute(self.andromeda, query, variables)"]}
{"filename": "app/prompts/guidance_choice.py", "chunked_list": ["from andromeda_chain.prompt import AndromedaPrompt\n\tCHOICE_PROMPT =  AndromedaPrompt(\n\t    name=\"choice_prompt\",\n\t    prompt_template = \"\"\"{{history}}\n\tYou must now choose an option out of the {{valid_choices}}.\n\tRemember that it must be coherent with your last thought.\n\t{{select 'choice' logprobs='logprobs' options=valid_choices}}: \"\"\",\n\t    input_vars=[\"history\", \"valid_choices\"],\n\t    output_vars=[\"choice\"],\n\t)"]}
{"filename": "app/prompts/guidance_check_question.py", "chunked_list": ["from andromeda_chain.prompt import AndromedaPrompt\n\tPROMPT_CHECK_QUESTION = AndromedaPrompt(\n\t    name=\"start-prompt\",\n\t    prompt_template=\"\"\"You MUST answer with 'yes' or 'no'. Given the following pieces of context, determine if there are any elements related to the question in the context.\n\tDon't forget you MUST answer with 'yes' or 'no'.\n\tContext: {{context}}\n\tQuestion: Are there any elements related to \"\"{{question}}\"\" in the context?\n\t{{select 'answer' options=['yes', 'no']}}\n\t\"\"\",\n\t    guidance_kwargs={},\n", "    input_vars=[\"context\", \"question\"],\n\t    output_vars=[\"answer\"],\n\t)\n"]}
{"filename": "app/prompts/__init__.py", "chunked_list": []}
{"filename": "app/prompts/flow_guidance_cot.py", "chunked_list": ["from andromeda_chain.prompt import AndromedaPrompt\n\tPROMPT_START_STRING = \"\"\"You're an AI assistant with access to tools.\n\tYou're nice and friendly, and try to answer questions to the best of your ability.\n\tYou have access to the following tools.\n\t{tools_descriptions}\n\tStrictly use the following format:\n\tQuestion: the input question you must answer\n\tThought: you should always think about what to do\n\tAction: the action to take, should be one of {action_list}\n\tAction Input: the input to the action, should be a question.\n", "Observation: the result of the action\n\t... (this Thought/Action/Action Input/Observation can repeat N times)\n\tThought: I now know the final answer\n\tFinal Answer: the final answer to the original input question\n\tWhen chatting with the user, you can search information using your tools.\n\t{few_shot_examples}\n\tNow your turn.\n\tQuestion:\"\"\"\n\tclass FlowChainOfThoughts:\n\t    choose_action = AndromedaPrompt(\n", "        name=\"cot_choose_action\",\n\t        prompt_template=\"\"\"{{history}}\n\tAction: {{select 'tool_name' options=valid_tools}}\"\"\",\n\t        guidance_kwargs={},\n\t        input_vars=[\"history\", \"valid_tools\"],\n\t        output_vars=[\"tool_name\"],\n\t    )\n\t    action_input = AndromedaPrompt(\n\t        name=\"cot_action_input\",\n\t        prompt_template=\"\"\"{{history}}{{gen 'act_input' stop='\\\\n'}}\"\"\",\n", "        guidance_kwargs={},\n\t        input_vars=[\"history\"],\n\t        output_vars=[\"act_input\"],\n\t    )\n\t    thought_gen = AndromedaPrompt(\n\t        name=\"cot_thought_gen\",\n\t        prompt_template=\"\"\"{{history}}\n\tObservation: {{observation}}\n\tThought: {{gen 'thought' stop='\\\\n'}}\"\"\",\n\t        guidance_kwargs={},\n", "        input_vars=[\"history\", \"observation\"],\n\t        output_vars=[\"thought\"],\n\t    )\n\t    final_prompt = AndromedaPrompt(\n\t        name=\"cot_final\",\n\t        prompt_template=\"\"\"{{history}}\n\tThought: I should now reply the user with what I thought and gathered.\n\tFinal Answer: {{gen 'final_answer' stop='\\\\n'}}\"\"\",\n\t        guidance_kwargs={},\n\t        input_vars=[\"history\"],\n", "        output_vars=[\"final_answer\"],\n\t    )\n\t    flow_prompt_start = AndromedaPrompt(\n\t        name=\"cot_flow_prompt_start\",\n\t        prompt_template=\"\"\"{{prompt_start}} {{question}}\n\tThink carefully about what you should do next. Take an action or provide a final answer to the user.\n\tThought: {{gen 'thought' stop='\\\\n'}}{{#block hidden=True}}\n\t{{select 'choice' logprobs='logprobs' options=valid_answers}}\n\t:{{/block}}\"\"\",\n\t        guidance_kwargs={},\n", "        input_vars=[\"prompt_start\", \"question\", \"valid_answers\"],\n\t        output_vars=[\"thought\", \"choice\"],\n\t    )\n"]}
{"filename": "app/models/all.py", "chunked_list": ["from datetime import datetime\n\tfrom typing import Optional, List\n\tfrom sqlmodel import SQLModel, Field, Relationship\n\tfrom importlib import import_module\n\tNAMING_CONVENTION = {\n\t    \"ix\": \"ix_%(column_0_label)s\",\n\t    \"uq\": \"uq_%(table_name)s_%(column_0_name)s\",\n\t    \"ck\": \"ck_%(table_name)s_%(constraint_name)s\",\n\t    \"fk\": \"fk_%(table_name)s_%(column_0_name)s_%(referred_table_name)s\",\n\t    \"pk\": \"pk_%(table_name)s\",\n", "}\n\tmetadata = SQLModel.metadata\n\tmetadata.naming_convention = NAMING_CONVENTION\n\tdef load_plugin_tables():\n\t    \"\"\"\n\t    Loads all plugins in the plugins directory that have a database.py file and merge their metadata.\n\t    :return: None\n\t    \"\"\"\n\t    import os\n\t    plugins_dir = os.path.dirname(__file__) + '/../plugins'\n", "    for plugin_name in os.listdir(plugins_dir):\n\t        if plugin_name.startswith('_'):\n\t            continue  # Skip hidden files\n\t        plugin_dir = os.path.join(plugins_dir, plugin_name)\n\t        if not os.path.exists(os.path.join(plugin_dir, 'database.py')):\n\t            continue  # Skip plugins without a database.py file\n\t        import_module(f'plugins.{plugin_name}.database')\n\tclass ConversationBase(SQLModel):\n\t    created_at: datetime = Field(default_factory=datetime.utcnow)\n\t    title: Optional[str]\n", "class Conversation(ConversationBase, table=True):\n\t    id: Optional[int] = Field(default=None, primary_key=True)\n\t    messages: List[\"Message\"] = Relationship(back_populates=\"conversation\")\n\tclass ConversationRead(ConversationBase):\n\t    id: int\n\tclass MessageBase(SQLModel):\n\t    text: str\n\t    is_user: bool\n\t    conversation_id: Optional[int] = Field(default=None, foreign_key=\"conversation.id\")\n\t    rating: int = 0 # -1, 0, 1\n", "    created_at: datetime = Field(default_factory=datetime.utcnow, nullable=False)\n\tclass Message(MessageBase, table=True):\n\t    id: Optional[int] = Field(default=None, primary_key=True)\n\t    conversation: Optional[Conversation] = Relationship(back_populates=\"messages\")\n\tclass MessageRead(MessageBase):\n\t    id: int\n\tclass ConversationWithMessages(ConversationRead):\n\t    messages: List[MessageRead] = []\n\tload_plugin_tables()"]}
{"filename": "app/prompt_templates/qa_agent.py", "chunked_list": ["QA_ETHICAL_AGENT=\"\"\"\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\tYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\n\tSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n\t{{~/system}}\n\t{{#block hidden=True}}\n\t{{~! The assistant either starts the conversation or not, depending on if the user's query is offensive or not}}\n\t{{#user~}}\n", "Utilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query: {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n\t{{~/user}}\n\t{{#assistant~}}\n\tObservation: Let's see if the query is offensive.\n\tDecision:{{#select 'offensive' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n\t{{~/assistant}}\n\t{{/block}}\n\t{{#if (equal offensive 'Yes')~}}\n\tFinal Answer: I'm sorry, but I can't answer that.\n\t{{else}}\n", "{{~! The assistant then classifies the user intent to decide whether he needs to enter qa mode}}\n\t{{#user~}}\n\tQuestion: {{question}}\n\tNow classify the intent behind this question:{{question}} Identify if this question is phatic (serving a social function) or referential (seeking information). Provide reasoning for your classification based on elements like the content, structure, or context of the user's input.\n\t{{~/user}}\n\t{{#assistant~}}\n\tThought: I need to evaluate the user's query and determine its intent - is {{question}} phatic or referential?\n\tDecision:{{#select 'query_type' logprobs='logprobs'}}Phatic{{or}}Referential{{/select}}\n\t{{~/assistant}}\n\t{{#if (equal query_type \"Phatic\")~}}\n", "Observation: The user's query is conversational. I need to answer as a helpful assistant while taking into account our chat history;\n\tChat history: {{history}}\n\tLatest user message: {{question}}\n\tThought: I need to stay in my role of a helpful assistant and make casual conversation.\n\tFinal Answer: {{gen 'phatic answer' temperature=0.7 max_tokens=50}}\n\t{{else}}\n\t{{#user~}}\n\tHere are the relevant documents from our database:{{set 'documents' (search question)}}\n\tUsing the concept of deixis, please evaluate if the answer to {{question}} is in :{{documents}}. Note that your response MUST contain either 'yes' or 'no'.\n\t{{~/user}}\n", "{{#assistant~}}\n\tThought: I need to determine if the answer to {{question}} is in: {{documents}}.\n\tDecision:{{#select 'answerable' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n\t{{#if (equal answerable \"Yes\")~}}\n\tObservation: I believe I can answer {{question}} based on the information contained in my analysis.\n\tThought: Now that I know that I can answer, I should provide the information to the user.\n\tFinal Answer: {{gen 'answer' temperature=0 max_tokens=100}}\n\t{{else}}\n\tThought: I don't think I can answer the question based on the information contained in the returned documents.\n\tFinal Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n", "{{/if}}\n\t{{~/assistant}}\n\t{{/if}}\n\t{{/if}}\n\t\"\"\"\n\tQA_AGENT= \"\"\"\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\tAnswer the following questions as best you can. You have access to the following tools:\n", "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n\t{{~/system}}\n\t{{#user~}}\n\tQuestion: {{question}}\n\t{{~/user}}\n\t{{#assistant~}}\n\tThought: Let's first check our database.\n\tAction: Check Question\n\tAction Input: {{question}}\n\t{{~/assistant}}\n", "{{~! The assistant then classifies the user intent to decide whether he needs to enter qa mode}}\n\t{{#user~}}\n\tQuestion: {{question}}\n\tNow classify the intent behind this question:{{question}} Identify if this question is phatic (serving a social function) or referential (seeking information). Provide reasoning for your classification based on elements like the content, structure, or context of the user's input.\n\t{{~/user}}\n\t{{#assistant~}}\n\tThought: I need to evaluate the user's query and determine its intent - is {{question}} phatic or referential?\n\tDecision:{{#select 'query_type' logprobs='logprobs'}}Phatic{{or}}Referential{{/select}}\n\t{{~/assistant}}\n\t{{#if (equal query_type \"Phatic\")~}}\n", "Observation: The user's query is conversational. I need to answer as a helpful assistant while taking into account our chat history;\n\tChat history: {{history}}\n\tLatest user message: {{question}}\n\tThought: I need to stay in my role of a helpful assistant and make casual conversation.\n\tFinal Answer: {{gen 'phatic answer' temperature=0.7 max_tokens=50}}\n\t{{else}}\n\t{{#user~}}\n\tHere are the relevant documents from our database:{{set 'documents' (search question)}}\n\tUsing the concept of deixis, please evaluate if the answer to {{question}} is in :{{documents}}. Note that your response MUST contain either 'yes' or 'no'.\n\t{{~/user}}\n", "{{#assistant~}}\n\tThought: I need to determine if the answer to {{question}} is in: {{documents}}.\n\tDecision:{{#select 'answerable' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n\t{{#if (equal answerable \"Yes\")~}}\n\tObservation: I believe I can answer {{question}} based on the information contained in my analysis.\n\tThought: Now that I know that I can answer, I should provide the information to the user.\n\tFinal Answer: {{gen 'answer' temperature=0 max_tokens=100}}\n\t{{else}}\n\tThought: I don't think I can answer the question based on the information contained in the returned documents.\n\tFinal Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n", "{{/if}}\n\t{{~/assistant}}\n\t{{/if}}\n\t\"\"\"\n\tETHICS_PROMPT= '''\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\tAnswer the following questions as best you can. You have access to the following tools:\n\tSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n", "{{~/system}}\n\t{{~! The assistant either starts the conversation or not, depending on if the user's query is offensive or not}}\n\t{{#user~}}\n\tUtilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n\t{{~/user}}\n\t{{#assistant~}}\n\tObservation: Let's see if the query is offensive.\n\tDecision:{{#select 'ethic' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n\t{{~/assistant}}\n\t'''\n", "ethics_prompt= '''\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\tYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\n\tSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n\t{{~/system}}\n\t{{~! The assistant either starts the conversation or not, depending on if the user's query is offensive or not}}\n\t{{#user~}}\n\tUtilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query: {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n", "{{~/user}}\n\t{{#assistant~}}\n\tObservation: Let's see if the query is inherently offensive.\n\tDecision:{{#select 'offensive' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n\t{{~/assistant}}\n\t'''\n\tconversation_prompt=\"\"\"\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n", "You are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\n\tSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n\t{{~/system}}\n\t{{~! The assistant then classifies the user intent to decide whether he needs to enter qa mode}}\n\t{{#user~}}\n\tQuestion: {{question}}\n\tNow classify the intent behind this question:{{question}} Identify if this question is phatic (serving a social function) or referential (seeking information). Provide reasoning for your classification based on elements like the content, structure, or context of the user's input.\n\t{{~/user}}\n\t{{#assistant~}}\n\tThought: I need to evaluate the user's query and determine its intent - is {{question}} phatic or referential?\n", "Decision:{{#select 'query_type' logprobs='logprobs'}}Phatic{{or}}Referential{{/select}}\n\t{{~/assistant}}\n\t\"\"\"\n\tphatic_prompt= '''\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\tYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\n\tSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n\t{{~/system}}\n", "Observation: The user's query is conversational. I need to answer him as a helpful assistant while taking into account our chat history;\n\tChat history: {{history}}\n\tLatest user message: {{question}}\n\tFinal Answer: {{gen 'phatic_answer' temperature=0 max_tokens=50}}'''\n\treferential_prompt = '''\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\tYou are a helpful assistant. Follow user queries as best you can. You have access to the following tools:\n\tSearch: Useful for when you need to answer questions about factual information. The input is the question to search relevant information.\n", "{{~/system}}\n\t{{#user~}}\n\tUsing the concept of deixis, please evaluate if the answer to {{question}} is in the documents retrieved from your database. Note that your response MUST contain either 'yes' or 'no'.\n\t{{~/user}}\n\t{{#assistant~}}\n\tObservation: I need to determine if the answer to {{question}}  is in:\n\t{{#each (search question)}}\n\t{{this.document_content}}\n\t{{/each}}\n\t{{#select 'answerable' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n", "{{~/assistant}}\n\t'''\n\tanswer_prompt = \"\"\"\n\t{{#system~}}\n\tBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\tYou are a helpful assistant. Answer the following questions as best you can. You have access to the following tools:\n\tSearch: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n\t{{~/system}}\n\t{{#user~}}\n", "Please answer {{question}} based on the information contained in:\n\t{{#each (search question)}}\n\t{{this.document_content}}\n\t{{/each}}\n\t{{~/user}}\n\t{{#assistant~}}\n\tThought: Now that I know that I can answer, I should provide the information to the user. \n\tFinal Answer: {{gen 'final_answer' temperature=0.7 max_tokens=50}}\n\t{{~/assistant}}\"\"\"\n"]}
{"filename": "app/flow/__init__.py", "chunked_list": []}
{"filename": "app/flow/flow.py", "chunked_list": ["from abc import abstractmethod\n\tfrom colorama import Fore\n\tfrom typing import Tuple, List, Union, Dict\n\tfrom andromeda_chain import AndromedaChain, AndromedaPrompt, AndromedaResponse\n\tfrom agents.base import color_print\n\tfrom prompts.guidance_choice import CHOICE_PROMPT\n\tfrom copy import deepcopy\n\tclass Node:\n\t    def __init__(self, name) -> None:\n\t        self.name = name\n", "    @abstractmethod\n\t    def run(self, variables) -> Union[AndromedaResponse, Dict[str, str]]:\n\t        raise NotImplementedError()\n\tclass HiddenNode(Node):\n\t    \"\"\"Classes of nodes that can be executed in the background,\n\t    without expanding the context history for the agent.\n\t    \"\"\"\n\tclass ChoiceNode(HiddenNode):\n\t    def __init__(self, name, choices: List[str], max_decisions, force_exit_on) -> None:\n\t        super().__init__(name)\n", "        self.choices = choices\n\t        self.max_decisions = max_decisions\n\t        self.force_exit_on=force_exit_on\n\t        self.decisions_made = 0\n\t    def run(self, chain: AndromedaChain, variables) -> str:        \n\t        if self.decisions_made >= self.max_decisions:\n\t            return self.force_exit_on\n\t        history = \"\"\n\t        if \"history\" in variables:\n\t            history = variables[\"history\"]\n", "        result = chain.run_guidance_prompt(\n\t            CHOICE_PROMPT,\n\t            input_vars={\n\t                \"history\": history,\n\t                \"valid_choices\": self.choices\n\t            }\n\t        )\n\t        self.decisions_made += 1\n\t        return result.result_vars[\"choice\"]\n\t    def set_next(self, next_):\n", "        self._next = next_\n\t    def next(self):\n\t        return self._next\n\tclass ToolNode(Node):\n\t    def __init__(self, name, tool_callback, variable_name = \"observation\") -> None:\n\t        super().__init__(name)\n\t        self.tool_callback = tool_callback\n\t        self.variable_name = variable_name\n\t    def run(self, variables) -> Dict[str, str]:        \n\t        return {self.variable_name: self.tool_callback(variables)}\n", "    def set_next(self, next_):\n\t        self._next = next_\n\t    def next(self):\n\t        return self._next\n\tclass PromptNode(Node):\n\t    def __init__(self, name, prompt: AndromedaPrompt) -> None:\n\t        super().__init__(name)\n\t        self.prompt = prompt\n\t        self._next = None\n\t    def run(self, chain: AndromedaChain, variables) -> AndromedaResponse:        \n", "        input_dict = {}\n\t        for var_ in self.prompt.input_vars:\n\t            value = variables[var_]\n\t            input_dict[var_] = value\n\t        return chain.run_guidance_prompt(\n\t            self.prompt,\n\t            input_vars=input_dict\n\t        )\n\t    def set_next(self, next_):\n\t        self._next = next_\n", "    def next(self):\n\t        return self._next\n\tclass StartNode(PromptNode):\n\t    def __init__(self, name, prompt: AndromedaPrompt, choices: List[str]) -> None:\n\t        super().__init__(name, prompt)\n\t        self.choices = choices\n\t    def run(self, chain: AndromedaChain, variables) -> Tuple[str, AndromedaResponse]:\n\t        response = super().run(chain, variables)\n\t        choice = response.result_vars.pop(\"choice\")\n\t        return self.choices[choice], response\n", "class Flow:\n\t    def __init__(self, nodes: List[PromptNode]) -> None:\n\t        assert len(nodes) > 0\n\t        self.nodes = nodes\n\t    def execute(self, chain, query: str, variables: Dict[str, str], return_key=\"final_answer\"):\n\t        node = self.nodes[0]\n\t        variables[\"query\"] = query\n\t        history = \"\"\n\t        while node:\n\t            color_print(f\"---> On node {node.name}\", Fore.RED)\n", "            debug_vars = deepcopy(variables)\n\t            if \"history\" in debug_vars:\n\t                debug_vars.pop(\"history\")\n\t            if \"prompt_start\" in debug_vars:\n\t                debug_vars.pop(\"prompt_start\")\n\t            if isinstance(node, StartNode):\n\t                color_print(f\"Executing start node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n\t                choice, result = node.run(chain, variables)\n\t                color_print(f\"Node result: {result.result_vars}\", Fore.GREEN)\n\t                history = result.expanded_generation\n", "                variables.update(result.result_vars)\n\t                variables[\"history\"] = history\n\t                node = _find_node_by_name(choice, self.nodes, node)\n\t                color_print(f\"Choice decided to jump to node {node.name}\", Fore.RED)\n\t            elif isinstance(node, PromptNode):\n\t                color_print(f\"Executing node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n\t                result = node.run(chain, variables)\n\t                color_print(f\"Node result: {result.result_vars}\", Fore.GREEN)\n\t                history = result.expanded_generation\n\t                # Merge contexts\n", "                variables.update(result.result_vars)\n\t                variables[\"history\"] = history\n\t                node = node.next()\n\t            elif isinstance(node, ToolNode):\n\t                color_print(f\"Executing tool node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n\t                tool_result = node.run(variables)\n\t                variables.update(tool_result)\n\t                node = node.next()\n\t            elif isinstance(node, ChoiceNode):\n\t                color_print(f\"Executing choice node {node.name} with variables: {debug_vars}\", Fore.YELLOW)\n", "                choice = node.run(chain, variables)\n\t                node = _find_node_by_name(choice, self.nodes, node)\n\t                color_print(f\"Choice decided to jump to node {node.name}\", Fore.RED)\n\t            else:\n\t                raise ValueError(f\"Invalid node class: {type(node)}\")\n\t            color_print(f\"History: {history}\", Fore.CYAN)\n\t        color_print(f\"Flow ended, returning variable '{return_key}'.\", Fore.GREEN)\n\t        return variables[return_key] \n\tdef _find_node_by_name(choice, nodes, current_node):\n\t    new_node = None\n", "    for n in nodes:\n\t        if n.name == choice:\n\t            new_node = n\n\t            break\n\t    if not new_node:\n\t        raise ValueError(f\"Choice {choice} led to limbo! Please choose the name of another node in the flow.\")\n\t    if new_node == current_node: \n\t        raise ValueError(f\"Choice {choice} led to an infinite loop on itself! Make sure choice node hop to itself.\")\n\t    return new_node"]}
{"filename": "app/migrations/env.py", "chunked_list": ["from logging.config import fileConfig\n\tfrom sqlalchemy import engine_from_config\n\tfrom sqlalchemy import pool\n\tfrom alembic import context\n\tfrom models import all\n\t# this is the Alembic Config object, which provides\n\t# access to the values within the .ini file in use.\n\tconfig = context.config\n\t# Interpret the config file for Python logging.\n\t# This line sets up loggers basically.\n", "if config.config_file_name is not None:\n\t    fileConfig(config.config_file_name)\n\t# add your model's MetaData object here\n\t# for 'autogenerate' support\n\t# from myapp import mymodel\n\t# target_metadata = mymodel.Base.metadata\n\ttarget_metadata = all.metadata\n\t# other values from the config, defined by the needs of env.py,\n\t# can be acquired:\n\t# my_important_option = config.get_main_option(\"my_important_option\")\n", "# ... etc.\n\tdef run_migrations_offline() -> None:\n\t    \"\"\"Run migrations in 'offline' mode.\n\t    This configures the context with just a URL\n\t    and not an Engine, though an Engine is acceptable\n\t    here as well.  By skipping the Engine creation\n\t    we don't even need a DBAPI to be available.\n\t    Calls to context.execute() here emit the given string to the\n\t    script output.\n\t    \"\"\"\n", "    url = config.get_main_option(\"sqlalchemy.url\")\n\t    context.configure(\n\t        url=url,\n\t        target_metadata=target_metadata,\n\t        literal_binds=True,\n\t        dialect_opts={\"paramstyle\": \"named\"},\n\t    )\n\t    with context.begin_transaction():\n\t        context.run_migrations()\n\tdef run_migrations_online() -> None:\n", "    \"\"\"Run migrations in 'online' mode.\n\t    In this scenario we need to create an Engine\n\t    and associate a connection with the context.\n\t    \"\"\"\n\t    connectable = engine_from_config(\n\t        config.get_section(config.config_ini_section, {}),\n\t        prefix=\"sqlalchemy.\",\n\t        poolclass=pool.NullPool,\n\t    )\n\t    with connectable.connect() as connection:\n", "        context.configure(\n\t            connection=connection, target_metadata=target_metadata, render_as_batch=True, user_module_prefix=\"sqlmodel.sql.sqltypes.\"\n\t        )\n\t        with context.begin_transaction():\n\t            context.run_migrations()\n\tif context.is_offline_mode():\n\t    run_migrations_offline()\n\telse:\n\t    run_migrations_online()\n"]}
{"filename": "app/migrations/versions/51a9f1ab4b8b_add_sample_plugin_demo_model.py", "chunked_list": ["\"\"\"Add Sample Plugin Demo Model\n\tRevision ID: 51a9f1ab4b8b\n\tRevises: 804b20c9599c\n\tCreate Date: 2023-06-04 21:30:37.777274\n\t\"\"\"\n\tfrom alembic import op\n\timport sqlalchemy as sa\n\timport sqlmodel.sql.sqltypes\n\t# revision identifiers, used by Alembic.\n\trevision = '51a9f1ab4b8b'\n", "down_revision = '804b20c9599c'\n\tbranch_labels = None\n\tdepends_on = None\n\tdef upgrade() -> None:\n\t    # ### commands auto generated by Alembic - please adjust! ###\n\t    op.create_table('samplepluginmodel',\n\t    sa.Column('created_at', sa.DateTime(), nullable=False),\n\t    sa.Column('title', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n\t    sa.Column('conversation_id', sa.Integer(), nullable=True),\n\t    sa.Column('id', sa.Integer(), nullable=False),\n", "    sa.ForeignKeyConstraint(['conversation_id'], ['conversation.id'], name=op.f('fk_samplepluginmodel_conversation_id_conversation')),\n\t    sa.PrimaryKeyConstraint('id', name=op.f('pk_samplepluginmodel'))\n\t    )\n\t    # ### end Alembic commands ###\n\tdef downgrade() -> None:\n\t    # ### commands auto generated by Alembic - please adjust! ###\n\t    op.drop_table('samplepluginmodel')\n\t    # ### end Alembic commands ###\n"]}
{"filename": "app/migrations/versions/804b20c9599c_initial.py", "chunked_list": ["\"\"\"initial\n\tRevision ID: 804b20c9599c\n\tRevises: \n\tCreate Date: 2023-05-31 20:06:36.912608\n\t\"\"\"\n\tfrom alembic import op\n\timport sqlalchemy as sa\n\timport sqlmodel.sql.sqltypes\n\t# revision identifiers, used by Alembic.\n\trevision = '804b20c9599c'\n", "down_revision = None\n\tbranch_labels = None\n\tdepends_on = None\n\tdef upgrade() -> None:\n\t    # ### commands auto generated by Alembic - please adjust! ###\n\t    op.create_table('conversation',\n\t    sa.Column('created_at', sa.DateTime(), nullable=False),\n\t    sa.Column('title', sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n\t    sa.Column('id', sa.Integer(), nullable=False),\n\t    sa.PrimaryKeyConstraint('id', name=op.f('pk_conversation'))\n", "    )\n\t    op.create_table('message',\n\t    sa.Column('text', sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n\t    sa.Column('is_user', sa.Boolean(), nullable=False),\n\t    sa.Column('conversation_id', sa.Integer(), nullable=True),\n\t    sa.Column('rating', sa.Integer(), nullable=False),\n\t    sa.Column('created_at', sa.DateTime(), nullable=False),\n\t    sa.Column('id', sa.Integer(), nullable=False),\n\t    sa.ForeignKeyConstraint(['conversation_id'], ['conversation.id'], name=op.f('fk_message_conversation_id_conversation')),\n\t    sa.PrimaryKeyConstraint('id', name=op.f('pk_message'))\n", "    )\n\t    # ### end Alembic commands ###\n\tdef downgrade() -> None:\n\t    # ### commands auto generated by Alembic - please adjust! ###\n\t    op.drop_table('message')\n\t    op.drop_table('conversation')\n\t    # ### end Alembic commands ###\n"]}
{"filename": "app/guidance_tooling/guidance_programs/tools.py", "chunked_list": ["from dotenv import load_dotenv\n\tfrom langchain.chains import RetrievalQA\n\tfrom langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n\tfrom langchain import HuggingFacePipeline\n\tfrom colorama import Fore, Style\n\timport re\n\tfrom langchain.vectorstores import Chroma\n\tfrom langchain.docstore.document import Document\n\tfrom langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n\timport os\n", "from langchain.llms import LlamaCpp\n\tload_dotenv()\n\tTEST_FILE = os.getenv(\"TEST_FILE\")\n\tEMBEDDINGS_MODEL = os.getenv(\"EMBEDDINGS_MODEL\")\n\tEMBEDDINGS_MAP = {\n\t    **{name: HuggingFaceInstructEmbeddings for name in [\"hkunlp/instructor-xl\", \"hkunlp/instructor-large\"]},\n\t    **{name: HuggingFaceEmbeddings for name in [\"all-MiniLM-L6-v2\", \"sentence-t5-xxl\"]}\n\t}\n\tmodel_type = os.environ.get('MODEL_TYPE')\n\tmodel_path = os.environ.get('MODEL_PATH')\n", "model_n_ctx =1000\n\ttarget_source_chunks = os.environ.get('TARGET_SOURCE_CHUNKS')\n\tn_gpu_layers = os.environ.get('N_GPU_LAYERS')\n\tuse_mlock = os.environ.get('USE_MLOCK')\n\tn_batch = os.environ.get('N_BATCH') if os.environ.get('N_BATCH') != None else 512\n\tcallbacks = []\n\tqa_prompt = \"\"\n\tCHROMA_SETTINGS = {}  # Set your Chroma settings here\n\tdef clean_text(text):\n\t    # Remove line breaksRetrievalQA\n", "    text = text.replace('\\n', ' ')\n\t    # Remove special characters\n\t    text = re.sub(r'[^\\w\\s]', '', text)\n\t    return text\n\tdef load_unstructured_document(document: str) -> list[Document]:\n\t    with open(document, 'r') as file:\n\t        text = file.read()\n\t    title = os.path.basename(document)\n\t    return [Document(page_content=text, metadata={\"title\": title})]\n\tdef split_documents(documents: list[Document], chunk_size: int = 250, chunk_overlap: int = 20) -> list[Document]:\n", "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n\t    return text_splitter.split_documents(documents)\n\tdef ingest_file(file_path):\n\t        # Load unstructured document\n\t        documents = load_unstructured_document(file_path)\n\t        # Split documents into chunks\n\t        documents = split_documents(documents, chunk_size=250, chunk_overlap=100)\n\t        # Determine the embedding model to use\n\t        EmbeddingsModel = EMBEDDINGS_MAP.get(EMBEDDINGS_MODEL)\n\t        if EmbeddingsModel is None:\n", "            raise ValueError(f\"Invalid embeddings model: {EMBEDDINGS_MODEL}\")\n\t        model_kwargs = {\"device\": \"cuda:0\"} if EmbeddingsModel == HuggingFaceInstructEmbeddings else {}\n\t        embedding = EmbeddingsModel(model_name=EMBEDDINGS_MODEL, model_kwargs=model_kwargs)\n\t        # Store embeddings from the chunked documents\n\t        vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n\t        retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n\t        print(file_path)\n\t        print(retriever)\n\t        return retriever\n\tdef load_tools():  \n", "    #llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False,n_gpu_layers=n_gpu_layers, use_mlock=use_mlock,top_p=0.9, n_batch=n_batch)\n\t    def ingest_file(file_path):\n\t        # Load unstructured document\n\t        documents = load_unstructured_document(file_path)\n\t        # Split documents into chunks\n\t        documents = split_documents(documents, chunk_size=120, chunk_overlap=20)\n\t        # Determine the embedding model to use\n\t        EmbeddingsModel = EMBEDDINGS_MAP.get(EMBEDDINGS_MODEL)\n\t        if EmbeddingsModel is None:\n\t            raise ValueError(f\"Invalid embeddings model: {EMBEDDINGS_MODEL}\")\n", "        model_kwargs = {\"device\": \"cuda:0\"} if EmbeddingsModel == HuggingFaceInstructEmbeddings else {}\n\t        embedding = EmbeddingsModel(model_name=EMBEDDINGS_MODEL, model_kwargs=model_kwargs)\n\t        # Store embeddings from the chunked documents\n\t        vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n\t        retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n\t        print(file_path)\n\t        print(retriever)\n\t        return retriever, file_path\n\t    file_path = TEST_FILE\n\t    retriever, title = ingest_file(file_path)\n", "    dict_tools = {\n\t        'File Ingestion': ingest_file,\n\t    }\n\t    return dict_tools\n"]}
{"filename": "app/memory/base.py", "chunked_list": ["from typing import Any, Iterable, List, Optional, Dict, Tuple, Type\n\tfrom pydantic import BaseModel\n\tfrom pyparsing import abstractmethod\n\tfrom langchain.docstore.document import Document\n\tfrom langchain.vectorstores import VectorStore\n\tclass BaseMemory(BaseModel):\n\t    collection_name: Optional[str]\n\t    vector_store: Optional[Type[VectorStore]]\n\t    def __init__(self, collection_name: str = \"default_collection\"):\n\t        # init super class\n", "        super().__init__()\n\t        self.collection_name = collection_name\n\t    def add_texts(\n\t        self,\n\t        texts: Iterable[str],\n\t        metadatas: Optional[List[dict]] = None,\n\t        ids: Optional[List[str]] = None,\n\t        **kwargs: Any,\n\t    ) -> List[str]:\n\t        response = self.vector_store.add_texts(texts, metadatas, ids, **kwargs)\n", "        self.vector_store._client.persist()\n\t        return response\n\t    def add_documents(self, documents: list[Document]):\n\t        texts = [doc.page_content for doc in documents]\n\t        metadatas = [doc.metadata for doc in documents]\n\t        return self.add_texts(texts, metadatas)\n\t    def similarity_search(\n\t        self,\n\t        query: str,\n\t        k: int = 4,\n", "        filter: Optional[Dict[str, str]] = None,\n\t        **kwargs: Any,\n\t    ) -> List[Document]:\n\t        return self.vector_store.similarity_search(query, k=k, **kwargs)\n\t    def similarity_search_by_vector(\n\t        self,\n\t        embedding: List[float],\n\t        k: int = 4,\n\t        filter: Optional[Dict[str, str]] = None,\n\t        **kwargs: Any,\n", "    ) -> List[Document]:\n\t        return self.vector_store.similarity_search_by_vector(\n\t            embedding, k=k, **kwargs)\n\t    def similarity_search_with_score(\n\t        self,\n\t        query: str,\n\t        k: int = 4,\n\t        filter: Optional[Dict[str, str]] = None,\n\t        **kwargs: Any,\n\t    ) -> List[Tuple[Document, float]]:\n", "        return self.vector_store.similarity_search_with_score(\n\t            query, k=k, **kwargs)\n\t    def max_marginal_relevance_search_by_vector(\n\t        self,\n\t        embedding: List[float],\n\t        k: int = 4,\n\t        fetch_k: int = 20,\n\t        lambda_mult: float = 0.5,\n\t        filter: Optional[Dict[str, str]] = None,\n\t        **kwargs: Any,\n", "    ) -> List[Document]:\n\t        return self.vector_store.max_marginal_relevance_search_by_vector(\n\t            embedding, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult, **kwargs)\n\t    def max_marginal_relevance_search(\n\t        self,\n\t        query: str,\n\t        k: int = 4,\n\t        fetch_k: int = 20,\n\t        lambda_mult: float = 0.5,\n\t        filter: Optional[Dict[str, str]] = None,\n", "        **kwargs: Any,\n\t    ) -> List[Document]:\n\t        return self.vector_store.max_marginal_relevance_search(\n\t            query, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult, **kwargs)\n\t    def delete_collection(self) -> None:\n\t        return self.vector_store.delete_collection()\n\t    def persist(self) -> None:\n\t        return self.vector_store.persist()\n\t    def update_document(self, document_id: str, document: Document) -> None:\n\t        return self.vector_store.update_document(document_id, document)\n", "    def get_store(self) -> Document:\n\t        return self.vector_store\n"]}
{"filename": "app/memory/chroma_memory.py", "chunked_list": ["import os\n\tfrom typing import Any, Iterable, List, Optional, Type\n\tfrom memory.base import BaseMemory\n\tfrom langchain import vectorstores\n\tfrom langchain.embeddings import (\n\t    HuggingFaceInstructEmbeddings,\n\t    HuggingFaceEmbeddings,\n\t)\n\tfrom langchain.docstore.document import Document\n\tfrom settings import load_config, logger\n", "config = load_config()\n\tclass Chroma(BaseMemory):\n\t    vector_store: Optional[Type[vectorstores.Chroma]]\n\t    collection_name: Optional[str]\n\t    def __init__(self, **kwargs: Any):\n\t        super().__init__(**kwargs)\n\t        embeddings_model_name = config.embeddings_model.split(\"/\")[-1]\n\t        EmbeddingsModel = config.embeddings_map.get(embeddings_model_name)\n\t        if EmbeddingsModel is None:\n\t            raise ValueError(f\"Invalid embeddings model: {config.embeddings_model}\")\n", "        kwargs = {\"model_name\": config.embeddings_model}\n\t        if EmbeddingsModel == HuggingFaceInstructEmbeddings:\n\t            kwargs[\"model_kwargs\"] = {\"device\": \"cuda\"}\n\t        embeddings = EmbeddingsModel(**kwargs)\n\t        persist_directory = os.path.join(os.getcwd(), \"data\", config.memories_path)\n\t        # Create directory if it doesn't exist\n\t        os.makedirs(persist_directory, exist_ok=True, mode=0o777)\n\t        self.vector_store = vectorstores.Chroma(\n\t            collection_name=self.collection_name,\n\t            embedding_function=embeddings,\n", "            persist_directory=persist_directory,\n\t        )\n\t        self.setup_index()\n\t    def setup_index(self):\n\t        collection = self.vector_store._client.get_collection(self.collection_name)\n\t        if len(collection.get()['ids']) < 6:\n\t            self.add_texts(\n\t                [\"Hello\", \"world!\", \"this\", \"is\", \"a\", \"test\"],\n\t                ids=[\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"],\n\t            )\n"]}
{"filename": "app/conversations/document_based.py", "chunked_list": ["from langchain.document_loaders import TextLoader\n\tfrom memory.chroma_memory import Chroma\n\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\tfrom colorama import Fore, Style\n\tfrom guidance_tooling.guidance_programs.tools import clean_text\n\tfrom andromeda_chain import AndromedaChain\n\tfrom agents import ChainOfThoughtsAgent\n\tfrom settings import logger, load_config\n\timport guidance \n\timport os\n", "config = load_config()\n\tdict_tools = None\n\tllama_model = None\n\tllama_model2 = None\n\tguidance_reasoning_model_path = config.guidance_reasoning_model_path\n\tguidance_extraction_model_path = config.guidance_extraction_model_path\n\tTEST_MODE = os.getenv(\"TEST_MODE\")\n\tGUIDANCE_MODEL = os.getenv(\"GUIDANCE_MODEL_PATH\")\n\tdef get_llama_model():\n\t    global llama_model\n", "    if llama_model is None:\n\t        print(\"Loading main guidance model...\")\n\t        llama_model = guidance.llms.LlamaCpp(\n\t            model = guidance_reasoning_model_path,\n\t            tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n\t            before_role = \"<|\",\n\t            after_role = \"|>\",\n\t            n_gpu_layers=300,\n\t            n_threads=12,\n\t            caching=False, )\n", "        print(\"Loading main guidance model...\")\n\t        guidance.llm = llama_model\n\t    return llama_model\n\tdef get_llama_model2():\n\t    global llama_model2\n\t    if llama_model2 is None and guidance_extraction_model_path is not None: \n\t        print(\"Loading guidance model...\")\n\t        llama_model2 = guidance.llms.LlamaCpp(\n\t            model = guidance_extraction_model_path,\n\t            tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n", "            before_role = \"<|\",\n\t            after_role = \"|>\",\n\t            n_gpu_layers=300,\n\t            n_threads=12,\n\t            caching=False, )\n\t        print(\"Loading second guidance model...\")\n\t    return llama_model2\n\tclass DocumentBasedConversation:\n\t    def __init__(self):\n\t        \"\"\"\n", "        Initializes an instance of the class. It sets up LLM, text splitter, vector store, prompt template, retriever,\n\t        conversation chain, tools, and conversation agent if USE_AGENT is True.\n\t        \"\"\"\n\t        self.text_splitter = RecursiveCharacterTextSplitter(\n\t            chunk_size=500, chunk_overlap=20, length_function=len)\n\t        self.llama_model = get_llama_model()\n\t        if llama_model2 is not None:\n\t            self.llama_model2 = get_llama_model2()\n\t        guidance.llm = self.llama_model\n\t        self.vector_store_docs = Chroma(collection_name=\"docs_collection\")\n", "        self.vector_store_convs = Chroma(collection_name=\"convos_collection\")\n\t        tools = {\n\t            \"Search Documents\": self.search_documents,\n\t            \"Search Conversations\": self.search_conversations,\n\t        }\n\t        self.andromeda = AndromedaChain(config.andromeda_url)\n\t        self.document_qa_agent = ChainOfThoughtsAgent(guidance, llama_model,llama_model2)\n\t    def load_document(self, document_path, conversation_id=None):\n\t        \"\"\"\n\t        Load a document from a file and add its contents to the vector store.\n", "        Args:\n\t          document_path: A string representing the path to the document file.\n\t        Returns:\n\t          None.\n\t        \"\"\"\n\t        text_loader = TextLoader(document_path, encoding=\"utf8\")\n\t        documents = text_loader.load()\n\t        documents = self.text_splitter.split_documents(documents)\n\t        if conversation_id is not None:\n\t            for doc in documents:\n", "                doc.metadata[\"conversation_id\"] = conversation_id\n\t        self.vector_store_docs.add_documents(documents)\n\t    def search_documents(self, search_input, conversation_id=None):\n\t        \"\"\"\n\t        Search for the given input in the vector store and return the top 10 most similar documents with their scores.\n\t        This function is used as a helper function for the SearchLongTermMemory tool\n\t        Args:\n\t          search_input (str): The input to search for in the vector store.\n\t        Returns:\n\t          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n", "        \"\"\"\n\t        logger.info(f\"Searching for: {search_input} in LTM\")\n\t        docs = self.vector_store_docs.similarity_search_with_score(\n\t            search_input, k=5, filter=filter\n\t        )\n\t        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]\n\t    def search_conversations(self, search_input, conversation_id=None):\n\t        \"\"\"\n\t        Search for the given input in the vector store and return the top 10 most similar documents with their scores.\n\t        This function is used as a helper function for the SearchLongTermMemory tool\n", "        Args:\n\t          search_input (str): The input to search for in the vector store.\n\t        Returns:\n\t          List[Tuple[str, float]]: A list of tuples containing the document text and their similarity score.\n\t        \"\"\"\n\t        if conversation_id is not None:\n\t            filter = {\"conversation_id\": conversation_id}\n\t        else:\n\t            filter = {}\n\t        logger.info(f\"Searching for: {search_input} in LTM\")\n", "        docs = self.vector_store_convs.similarity_search_with_score(\n\t            search_input, k=5, filter=filter\n\t        )\n\t        return [{\"document_content\": doc[0].page_content, \"similarity\": doc[1]} for doc in docs]\n\t    def predict(self, input, history):\n\t      global dict_tools\n\t      \"\"\"\n\t      Predicts a response based on the given input.\n\t      Args:\n\t        input (str): The input string to generate a response for.\n", "      Returns:\n\t        str: The generated response string.\n\t      Raises:\n\t        OutputParserException: If the response from the conversation agent could not be parsed.\n\t      \"\"\"\n\t      context = self.search_documents(input)\n\t      str_context = str(context)\n\t      print(Fore.GREEN + Style.BRIGHT + \"Printing vector search context...\" + Style.RESET_ALL)\n\t      print(Fore.GREEN + Style.BRIGHT + str_context + Style.RESET_ALL)\n\t      final_answer = self.document_qa_agent.run(input, context, history)\n", "      print(Fore.CYAN + Style.BRIGHT + \"Printing full thought process...\" + Style.RESET_ALL)\n\t      print(Fore.CYAN + Style.BRIGHT + str(final_answer) + Style.RESET_ALL)\n\t      if isinstance(final_answer, dict):\n\t          final_answer = {'answer': str(final_answer), 'function': str(final_answer['fn'])}\n\t      else:\n\t          # Handle the case when final_answer is not a dictionary.\n\t          final_answer = {'answer': str(final_answer)}\n\t          # Check if 'Final Answer:' key exists in the dictionary\n\t      if 'Final Answer:' in final_answer['answer']:\n\t          # Find the index of 'Final Answer:' and extract everything after it\n", "          answer_start_index = final_answer['answer'].index('Final Answer:') + len('Final Answer:')\n\t          final_answer_text = final_answer['answer'][answer_start_index:]\n\t          return final_answer_text.strip()\n\t      else:\n\t          return final_answer[\"answer\"]\n"]}
{"filename": "app/conversations/document_based_flow.py", "chunked_list": ["from langchain.document_loaders import TextLoader\n\tfrom memory.chroma_memory import Chroma\n\tfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\tfrom andromeda_chain import AndromedaChain\n\tfrom agents import ChainOfThoughtsFlowAgent\n\tfrom tools.base import ToolFactory\n\tfrom tools.document_memory import DocumentSearchTool\n\tfrom tools.conversation_memory import ConversationSearchTool\n\tfrom settings import load_config, logger\n\tconfig = load_config()\n", "class DocumentBasedConversationFlowAgent:\n\t    def __init__(self):\n\t        \"\"\"\n\t        Initializes an instance of the class. It sets up LLM, text splitter, vector store, prompt template, retriever,\n\t        conversation chain, tools, and conversation agent if USE_AGENT is True.\n\t        \"\"\"\n\t        self.text_splitter = RecursiveCharacterTextSplitter(\n\t            chunk_size=500, chunk_overlap=20, length_function=len)\n\t        self.vector_store_docs = Chroma(collection_name=\"docs_collection\")\n\t        self.vector_store_convs = Chroma(collection_name=\"convos_collection\")\n", "        self.tools = [DocumentSearchTool, ConversationSearchTool]\n\t        self.andromeda = AndromedaChain(config.andromeda_url)\n\t        self.active_agent_class = ChainOfThoughtsFlowAgent\n\t        self.tool_context = {\n\t            \"vector_store_docs\": self.vector_store_docs,\n\t            \"vector_store_convs\": self.vector_store_convs,\n\t            \"k\": 5,\n\t        }\n\t    def load_document(self, document_path, conversation_id=None):\n\t        \"\"\"\n", "        Load a document from a file and add its contents to the vector store.\n\t        Args:\n\t          document_path: A string representing the path to the document file.\n\t        Returns:\n\t          None.\n\t        \"\"\"\n\t        text_loader = TextLoader(document_path, encoding=\"utf8\")\n\t        documents = text_loader.load()\n\t        documents = self.text_splitter.split_documents(documents)\n\t        if conversation_id is not None:\n", "            for doc in documents:\n\t                doc.metadata[\"conversation_id\"] = conversation_id\n\t        self.vector_store_docs.add_documents(documents)\n\t    def predict(self, input: str, conversation_id: str):\n\t        \"\"\"\n\t        Predicts a response based on the given input.\n\t        Args:\n\t          input (str): The input string to generate a response for.\n\t        Returns:\n\t          str: The generated response string.\n", "        Raises:\n\t          OutputParserException: If the response from the conversation agent could not be parsed.\n\t        \"\"\"\n\t        logger.info(\"Defined tools: %s\", self.tools)\n\t        loaded_tools = ToolFactory(self.tools).build_tools(conversation_id, self.tool_context)\n\t        logger.info(\"Loaded tools: %s\", loaded_tools)\n\t        loaded_agent = self.active_agent_class(self.andromeda, loaded_tools)\n\t        final_answer = loaded_agent.run(input)\n\t        if isinstance(final_answer, dict):\n\t            final_answer = {'answer': str(final_answer), 'function': str(final_answer['fn'])}\n", "        else:\n\t            # Handle the case when final_answer is not a dictionary.\n\t            final_answer = {'answer': str(final_answer)}\n\t        return final_answer[\"answer\"]\n"]}
