{"filename": "demo.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tfrom typing import Tuple, List, Optional\n\timport re\n\timport time\n\tfrom kernel.logits_processor import (\n\t    LogitsProcessorList,\n\t    InvalidScoreLogitsProcessor,\n\t    TemperatureLogitsWarper,\n\t    TopPLogitsWarper,\n", "    TopKLogitsWarper\n\t)\n\tfrom kernel import ckernel\n\tKernel = ckernel.Kernel\n\t# from ckernel import Kernel\n\t# from kernel.ckernel import Kernel\n\tclass Model(nn.Module):\n\t    def __init__(self, engine_path: str, batch_size: int):\n\t        self.batch_size_ = batch_size\n\t        self.kernel = Kernel(engine_path, batch_size)\n", "        self.num_layers_ = 28\n\t        self.logits_processor = LogitsProcessorList()\n\t        self.logits_warper = LogitsProcessorList()\n\t    def chat(\n\t        self, \n\t        tokenizer,\n\t        query: str,\n\t        history = None,\n\t        max_length: int = 2048,\n\t        max_new_tokens: int = 1024,\n", "        num_beams=1,\n\t        do_sample=True,\n\t        top_p=0.7,\n\t        top_k=50,\n\t        temperature=1.0,\n\t        **kwargs\n\t    ):\n\t        # ÂàùÂßãÂåñ history\n\t        if history is None:\n\t            history = []\n", "        # ÂàùÂßãÂåñÂêéÂ§ÑÁêÜ\n\t        self.logits_processor = LogitsProcessorList()\n\t        self.logits_processor.append(InvalidScoreLogitsProcessor())\n\t        self.logits_warper = LogitsProcessorList()\n\t        self.logits_warper.append(TemperatureLogitsWarper(temperature))\n\t        self.logits_warper.append(TopPLogitsWarper(top_p))\n\t        self.logits_warper.append(TopKLogitsWarper(top_k))\n\t        # ÁªÑË£Öprompt\n\t        if not history:\n\t            prompt = query\n", "        else:\n\t            prompt = \"\"\n\t            for i, (old_query, response) in enumerate(history):\n\t                prompt += \"[Round {}]\\nÈóÆÔºö{}\\nÁ≠îÔºö{}\\n\".format(i, old_query, response)\n\t            prompt += \"[Round {}]\\nÈóÆÔºö{}\\nÁ≠îÔºö\".format(len(history), query)\n\t        # Á¨¨‰∏ÄÊ¨°Êé®ÁêÜ\n\t        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda().to(torch.int32)\n\t        ori_len = len(input_ids[0])\n\t        attention_mask, position_ids = self.pre_processing_step_1(tokenizer, input_ids)\n\t        input_tensors = [input_ids, position_ids, attention_mask]\n", "        outputs_1 = self.kernel.forward(input_tensors)\n\t        ori_input_ids = input_ids\n\t        ori_input_ids, input_tensors = self.post_processing_step(\n\t            ori_input_ids, input_tensors, outputs_1\n\t        )\n\t        # ÈáçÂ§çÊé®ÁêÜÁõ¥Âà∞Êù°‰ª∂ÁªàÊ≠¢\n\t        while len(ori_input_ids[0]) < max_length \\\n\t                and len(ori_input_ids[0]) - ori_len < max_new_tokens \\\n\t                and tokenizer.eos_token_id not in ori_input_ids[0]:\n\t            outputs_x = self.kernel.forward(input_tensors)\n", "            ori_input_ids, input_tensors = self.post_processing_step(\n\t                ori_input_ids, input_tensors, outputs_x)\n\t            # print(tokenizer.decode(ori_input_ids[0]))\n\t        # Â§ÑÁêÜÂõûÁ≠î\n\t        response = tokenizer.decode(ori_input_ids[0][ori_len:])\n\t        response = self.process_response(response)\n\t        history = history + [(query, response)]\n\t        return response, history\n\t    def stream_chat(\n\t        self, \n", "        tokenizer,\n\t        query: str,\n\t        history = None,\n\t        max_length: int = 2048,\n\t        max_new_tokens: int = 1024,\n\t        num_beams=1,\n\t        do_sample=True,\n\t        top_p=0.7,\n\t        top_k=50,\n\t        temperature=1.0,\n", "        **kwargs\n\t    ):\n\t        # ÂàùÂßãÂåñ history\n\t        if history is None:\n\t            history = []\n\t        # ÂàùÂßãÂåñÂêéÂ§ÑÁêÜ\n\t        self.logits_processor = LogitsProcessorList()\n\t        self.logits_processor.append(InvalidScoreLogitsProcessor())\n\t        self.logits_warper = LogitsProcessorList()\n\t        self.logits_warper.append(TemperatureLogitsWarper(temperature))\n", "        self.logits_warper.append(TopPLogitsWarper(top_p))\n\t        self.logits_warper.append(TopKLogitsWarper(top_k))\n\t        # ÁªÑË£Öprompt\n\t        if not history:\n\t            prompt = query\n\t        else:\n\t            prompt = \"\"\n\t            for i, (old_query, response) in enumerate(history):\n\t                prompt += \"[Round {}]\\nÈóÆÔºö{}\\nÁ≠îÔºö{}\\n\".format(i, old_query, response)\n\t            prompt += \"[Round {}]\\nÈóÆÔºö{}\\nÁ≠îÔºö\".format(len(history), query)\n", "        # Á¨¨‰∏ÄÊ¨°Êé®ÁêÜ\n\t        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda().to(torch.int32)\n\t        ori_len = len(input_ids[0])\n\t        attention_mask, position_ids = self.pre_processing_step_1(tokenizer, input_ids)\n\t        input_tensors = [input_ids, position_ids, attention_mask]\n\t        outputs_1 = self.kernel.forward(input_tensors)\n\t        ori_input_ids = input_ids\n\t        ori_input_ids, input_tensors = self.post_processing_step(\n\t            ori_input_ids, input_tensors, outputs_1\n\t        )\n", "        # ÈáçÂ§çÊé®ÁêÜÁõ¥Âà∞Êù°‰ª∂ÁªàÊ≠¢\n\t        while len(ori_input_ids[0]) < max_length \\\n\t                and len(ori_input_ids[0]) - ori_len < max_new_tokens \\\n\t                and tokenizer.eos_token_id not in ori_input_ids[0]:\n\t            outputs_x = self.kernel.forward(input_tensors)\n\t            ori_input_ids, input_tensors = self.post_processing_step(\n\t                ori_input_ids, input_tensors, outputs_x)\n\t            # Â§ÑÁêÜÂõûÁ≠î\n\t            response = tokenizer.decode(ori_input_ids[0][ori_len:])\n\t            response = self.process_response(response)\n", "            history = history + [(query, response)]\n\t            yield response, history\n\t    def pre_processing_step_1(self, tokenizer, input_ids: torch.Tensor):\n\t        BOS = tokenizer.bos_token_id\n\t        MASK = tokenizer.mask_token_id\n\t        gMASK = tokenizer.gmask_token_id\n\t        batch_size, seq_length = input_ids.shape\n\t        # ËæìÂÖ•Âº†ÈáèÊâ©Â±ïÂ∏∏Èáè\n\t        input_range = torch.arange(seq_length, dtype=torch.int32).repeat((batch_size, 1)).to(input_ids.device)\n\t        input_upper = torch.tril(torch.ones((batch_size, seq_length, seq_length), dtype=torch.int32)).to(\n", "            input_ids.device)\n\t        # Ëé∑Âèñ attention_mask\n\t        context_lengths = torch.argmax((input_ids == BOS).to(torch.int32), dim=1)\n\t        context_mask = (input_range + 1) <= context_lengths.unsqueeze(1)\n\t        padding_mask = context_mask.unsqueeze(1)\n\t        attention_mask = torch.logical_not(torch.logical_or(input_upper, padding_mask)).unsqueeze(1)\n\t        # Âà§Êñ≠MASK‰ΩçÁΩÆ\n\t        is_gmasks = (input_ids == gMASK).to(torch.int32)\n\t        is_masks = (input_ids == MASK).to(torch.int32)\n\t        use_gmasks = torch.sum(is_gmasks, dim=1) > 0\n", "        # Ëé∑Âèñ position_ids\n\t        mask_positions = torch.where(use_gmasks, torch.argmax(is_gmasks, dim=1), torch.argmax(is_masks, dim=1)).to(\n\t            torch.int32).unsqueeze(1)\n\t        position_ids_pre = torch.where(context_mask, input_range, mask_positions)\n\t        block_position_ids = torch.clamp(input_range - context_lengths.unsqueeze(1) + 1, min=0)\n\t        position_ids = torch.stack((position_ids_pre, block_position_ids), dim=1).to(torch.int32)\n\t        return attention_mask, position_ids\n\t    def post_processing_step(self,\n\t            ori_input_ids: torch.Tensor,\n\t            input_tensors: List[torch.Tensor],\n", "            output_tensors: List[torch.Tensor]\n\t        ):\n\t        logits = output_tensors[-1]\n\t        next_token_logits = logits[:, -1, :]\n\t        # ‰∏Ä‰∫õÂêéÂ§ÑÁêÜÈÄªËæë\n\t        next_token_scores = self.logits_processor(input_tensors[0], next_token_logits)\n\t        next_token_scores = self.logits_warper(input_tensors[0], next_token_scores)\n\t        # ÈááÊ†∑‰∏ã‰∏Ä‰∏™token\n\t        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)\n\t        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n", "        ori_input_ids = torch.cat((ori_input_ids, next_tokens[:, None]), dim=-1)\n\t        # ËæìÂá∫‰∏ã‰∏ÄËΩÆÁöÑ input_ids, position_ids, attention_mask\n\t        input_tensors[2] = input_tensors[2][..., -1:, -1:]\n\t        input_tensors[1] = torch.cat(\n\t            (\n\t                input_tensors[1][:, :-1, -1:],\n\t                input_tensors[1][:, -1:, -1:] + \\\n\t                    torch.tensor(1, dtype=input_tensors[1].dtype)\n\t            ), \n\t            dim=1\n", "        )\n\t        input_tensors[0] = ori_input_ids[:, -1:].to(torch.int32)\n\t        if len(input_tensors) == 3:\n\t            input_tensors.extend(output_tensors[:-1])\n\t        else:\n\t            # for i in range(len(output_tensors) - 1):\n\t            #     input_tensors[3 + i] = output_tensors[i]\n\t            input_tensors[3:] = output_tensors[:-1]\n\t        return ori_input_ids, input_tensors\n\t    def process_response(self, response):\n", "        response = response.strip()\n\t        # response = response.replace(\"[[ËÆ≠ÁªÉÊó∂Èó¥]]\", \"2023Âπ¥\")\n\t        punkts = [\n\t            [\",\", \"Ôºå\"],\n\t            [\"!\", \"ÔºÅ\"],\n\t            [\":\", \"Ôºö\"],\n\t            [\";\", \"Ôºõ\"],\n\t            [\"\\?\", \"Ôºü\"],\n\t        ]\n\t        for item in punkts:\n", "            response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n\t            response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n\t        return response\n\tif __name__ == \"__main__\":\n\t    from transformers import AutoTokenizer\n\t    import time\n\t    from tqdm import trange\n\t    tokenizer = AutoTokenizer.from_pretrained(\"chatglm_6b\", trust_remote_code=True)\n\t    model = Model(\"models/chatglm6b-bs1-18.5G.plan\", 1)\n\t    all_res = []\n", "    st = time.time()\n\t    for i in trange(10):\n\t        responses, history = model.chat(\n\t            tokenizer=tokenizer,\n\t            query=\"‰Ω†Â•Ω, ËØ∑Áî®pythonÂÜô‰∏Ä‰∏™ÈìæË°®„ÄÇ\"\n\t        )\n\t        all_res.append(responses)\n\t    et = time.time()\n\t    print(all_res)\n\t    tokens = tokenizer.encode(\"\".join(all_res), return_tensors=\"pt\")[0]\n", "    token_num = len(tokens)\n\t    speed = round(token_num / (et - st), 1)\n\t    print(\"speed: {} tokens/s\".format(speed))\n"]}
{"filename": "cli_demo.py", "chunked_list": ["import os\n\timport platform\n\timport signal\n\tfrom transformers import AutoTokenizer, AutoConfig\n\t# from stream_demo import ChatGLMForConditionalGeneration\n\tfrom demo import Model\n\timport readline\n\ttokenizer = AutoTokenizer.from_pretrained(\"chatglm_6b\", trust_remote_code=True)\n\tmodel = Model(\n\t    engine_path=\"models/chatglm6b-bs1-11.5G.plan\",\n", "    batch_size=1, \n\t)\n\tos_name = platform.system()\n\tclear_command = 'cls' if os_name == 'Windows' else 'clear'\n\tstop_stream = False\n\tdef build_prompt(history):\n\t    prompt = \"Ê¨¢Ëøé‰ΩøÁî® ChatGLM-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è\"\n\t    for query, response in history:\n\t        prompt += f\"\\n\\nÁî®Êà∑Ôºö{query}\"\n\t        prompt += f\"\\n\\nChatGLM-6BÔºö{response}\"\n", "    return prompt\n\tdef signal_handler(signal, frame):\n\t    global stop_stream\n\t    stop_stream = True\n\tdef main():\n\t    history = []\n\t    global stop_stream\n\t    print(\"Ê¨¢Ëøé‰ΩøÁî® ChatGLM-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è\")\n\t    while True:\n\t        query = input(\"\\nÁî®Êà∑Ôºö\")\n", "        if query.strip() == \"stop\":\n\t            break\n\t        if query.strip() == \"clear\":\n\t            history = []\n\t            os.system(clear_command)\n\t            print(\"Ê¨¢Ëøé‰ΩøÁî® ChatGLM-6B Ê®°ÂûãÔºåËæìÂÖ•ÂÜÖÂÆπÂç≥ÂèØËøõË°åÂØπËØùÔºåclear Ê∏ÖÁ©∫ÂØπËØùÂéÜÂè≤Ôºåstop ÁªàÊ≠¢Á®ãÂ∫è\")\n\t            continue\n\t        count = 0\n\t        for response, history in model.stream_chat(tokenizer, query, history=history):\n\t            if stop_stream:\n", "                stop_stream = False\n\t                break\n\t            else:\n\t                count += 1\n\t                if count % 8 == 0:\n\t                    os.system(clear_command)\n\t                    print(build_prompt(history), flush=True)\n\t                    signal.signal(signal.SIGINT, signal_handler)\n\t        os.system(clear_command)\n\t        print(build_prompt(history), flush=True)\n", "if __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "onnx_export/export_test_v2.py", "chunked_list": ["import os\n\t# from transformers import AutoTokenizer, AutoModel, AutoConfig\n\timport torch\n\timport sys\n\timport argparse\n\tfrom transformers.generation.utils import LogitsProcessorList\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\tfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\n", "from chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\n\tfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\n\tfrom onnx_export.utils import build_inputs\n\tfrom transformers.models.bloom import BloomOnnxConfig\n\tparser = argparse.ArgumentParser(description='export pytorch model to onnx')\n\tparser.add_argument(\n\t    '--data_type',\n\t    default=\"fp32\",\n\t    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n\t)\n", "args = parser.parse_args()\n\tif args.data_type == \"fp16\":\n\t    device = 'cuda'\n\telse:\n\t    device = 'cpu'\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tif not os.path.exists(output_dir):\n\t    os.makedirs(output_dir)\n\tonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\n\tif not os.path.exists(onnx_output_dir):\n", "    os.mkdir(onnx_output_dir)\n\tquery = \"ÊÉ≥Ë¶ÅÂá∫ÂõΩÁïôÂ≠¶ÔºåÂ∫îËØ•ÊÄé‰πàÂäûÔºü\"\n\thistory = [\n\t    (\n\t        \"‰Ω†Â•Ω\",\n\t        \"‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\",\n\t    )\n\t]\n\tmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\n\ttokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\n", "config = ChatGLMConfig.from_pretrained(model_dir)\n\t# config.num_layers = 1\n\tmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\n\tif device == \"cuda\":\n\t    model = model.half().cuda()\n\telse:\n\t    model = model.float().cpu()\n\tdevice = torch.device(device)\n\tmodel.eval()\n\t# input_tensors\n", "input_tensors = build_inputs(device, tokenizer, query, history)\n\t# --debug for chat --\n\t# response, history = model.chat(tokenizer, query, history)\n\t# print(\"res\", response)\n\tprint(\"=\" * 50)\n\tprint(\" ---forward first --- \")\n\toutputs = model.forward(\n\t    **input_tensors\n\t)\n\tprint(\" ---forward first with no attention_mask --- \")\n", "outputs2 = model.forward(\n\t    input_ids=input_tensors[\"input_ids\"],\n\t    position_ids=input_tensors[\"position_ids\"],\n\t    attention_mask=torch.tensor([], device=device)\n\t)\n\tdef compare_diff(outputs_1, outputs_2):\n\t    print(\"--- compare diff ---\")\n\t    max_diff = 0\n\t    logits_diff = (outputs_2[\"logits\"] - outputs_1[\"logits\"]).max().item()\n\t    if logits_diff > max_diff:\n", "        max_diff = logits_diff\n\t    print(\"logits diff is \", logits_diff)\n\t    past_key_values0 = outputs_1[\"past_key_values\"]\n\t    past_key_values1 = outputs_2[\"past_key_values\"]\n\t    for i in range(model.config.num_layers):\n\t        present_key_name = f\"present_key_values.{i}.key\"\n\t        present_value_name = f\"present_key_values.{i}.value\"\n\t        diff1 = (past_key_values0[i][0] - past_key_values1[i][0]).max().item()\n\t        diff2 = (past_key_values0[i][1] - past_key_values1[i][1]).max().item()\n\t        print(f\"{present_key_name} diff: \", diff1)\n", "        print(f\"{present_value_name} diff: \", diff2)\n\t        if diff1 > max_diff:\n\t            max_diff = diff1\n\t        if diff2 > max_diff:\n\t            max_diff = diff2\n\t    print(\"max diff is: \", max_diff)\n\tcompare_diff(outputs, outputs2)\n\tprint(\"=\" * 50)\n\tprint(\"=\" * 50)\n\tprint(\" ---forward second --- \")\n", "attention_mask = input_tensors[\"attention_mask\"]\n\tposition_ids = input_tensors[\"position_ids\"]\n\tpast_key_values = outputs[\"past_key_values\"]\n\t# copy from forward in second time\n\tinput_ids = torch.tensor([[30910]]).to(device)\n\tattention_mask = torch.cat(\n\t    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n\t)\n\tnew_position_id = position_ids[..., -1:].clone()\n\tnew_position_id += 1\n", "position_ids = torch.cat(\n\t    [position_ids, new_position_id], dim=-1\n\t)\n\t# copy from prepare_inputs_for_generation in modeling_chatglm.py\n\tposition_ids = position_ids[..., -1:]\n\tpast_key_values1 = outputs[\"past_key_values\"]\n\toutputs_3 = model.forward(\n\t    input_ids=input_ids,\n\t    attention_mask=attention_mask,\n\t    position_ids=position_ids,\n", "    past_key_values=past_key_values1\n\t)\n\tprint(\" ---forward second with no attention_mask --- \")\n\toutputs_4 = model.forward(\n\t    input_ids=input_ids,\n\t    position_ids=position_ids,\n\t    attention_mask=torch.tensor([], device=device),\n\t    past_key_values=past_key_values1\n\t)\n\tcompare_diff(outputs_3, outputs_4)\n", "print(\"=\" * 50)\n"]}
{"filename": "onnx_export/modeling_chatglm.py", "chunked_list": ["\"\"\" PyTorch ChatGLM model. \"\"\"\n\timport math\n\timport copy\n\timport warnings\n\timport re\n\timport sys\n\timport torch\n\timport torch.utils.checkpoint\n\timport torch.nn.functional as F\n\tfrom torch import nn\n", "from torch.nn import CrossEntropyLoss, LayerNorm\n\tfrom torch.nn.utils import skip_init\n\tfrom typing import Optional, Tuple, Union, List, Callable, Dict, Any\n\tfrom transformers.modeling_outputs import (\n\t    BaseModelOutputWithPast,\n\t    CausalLMOutputWithPast,\n\t)\n\tfrom transformers.modeling_utils import PreTrainedModel\n\tfrom transformers.utils import logging\n\tfrom transformers.generation.logits_process import LogitsProcessor\n", "from transformers.generation.utils import LogitsProcessorList, StoppingCriteriaList, GenerationConfig, ModelOutput\n\tfrom .configuration_chatglm import ChatGLMConfig\n\t# flags required to enable jit fusion kernels\n\tif sys.platform != 'darwin':\n\t    torch._C._jit_set_profiling_mode(False)\n\t    torch._C._jit_set_profiling_executor(False)\n\t    torch._C._jit_override_can_fuse_on_cpu(True)\n\t    torch._C._jit_override_can_fuse_on_gpu(True)\n\tlogger = logging.get_logger(__name__)\n\t_CHECKPOINT_FOR_DOC = \"THUDM/ChatGLM2-6B\"\n", "_CONFIG_FOR_DOC = \"ChatGLM6BConfig\"\n\tCHATGLM_6B_PRETRAINED_MODEL_ARCHIVE_LIST = [\n\t    \"THUDM/chatglm2-6b\",\n\t    # See all ChatGLM models at https://huggingface.co/models?filter=chatglm\n\t]\n\tdef default_init(cls, *args, **kwargs):\n\t    return cls(*args, **kwargs)\n\tclass InvalidScoreLogitsProcessor(LogitsProcessor):\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n\t        if torch.isnan(scores).any() or torch.isinf(scores).any():\n", "            scores.zero_()\n\t            scores[..., 5] = 5e4\n\t        return scores\n\tclass PrefixEncoder(torch.nn.Module):\n\t    \"\"\"\n\t    The torch.nn model to encode the prefix\n\t    Input shape: (batch-size, prefix-length)\n\t    Output shape: (batch-size, prefix-length, 2*layers*hidden)\n\t    \"\"\"\n\t    def __init__(self, config: ChatGLMConfig):\n", "        super().__init__()\n\t        self.prefix_projection = config.prefix_projection\n\t        if self.prefix_projection:\n\t            # Use a two-layer MLP to encode the prefix\n\t            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)\n\t            self.trans = torch.nn.Sequential(\n\t                torch.nn.Linear(config.hidden_size, config.hidden_size),\n\t                torch.nn.Tanh(),\n\t                torch.nn.Linear(config.hidden_size, config.num_layers * config.hidden_size * 2)\n\t            )\n", "        else:\n\t            self.embedding = torch.nn.Embedding(config.pre_seq_len,\n\t                                                config.num_layers * config.kv_channels * config.multi_query_group_num * 2)\n\t    def forward(self, prefix: torch.Tensor):\n\t        if self.prefix_projection:\n\t            prefix_tokens = self.embedding(prefix)\n\t            past_key_values = self.trans(prefix_tokens)\n\t        else:\n\t            past_key_values = self.embedding(prefix)\n\t        return past_key_values\n", "def split_tensor_along_last_dim(\n\t        tensor: torch.Tensor,\n\t        num_partitions: int,\n\t        contiguous_split_chunks: bool = False,\n\t) -> List[torch.Tensor]:\n\t    \"\"\"Split a tensor along its last dimension.\n\t    Arguments:\n\t        tensor: input tensor.\n\t        num_partitions: number of partitions to split the tensor\n\t        contiguous_split_chunks: If True, make each chunk contiguous\n", "                                 in memory.\n\t    Returns:\n\t        A list of Tensors\n\t    \"\"\"\n\t    # Get the size and dimension.\n\t    last_dim = tensor.dim() - 1\n\t    last_dim_size = tensor.size()[last_dim] // num_partitions\n\t    # Split.\n\t    tensor_list = torch.split(tensor, last_dim_size, dim=last_dim)\n\t    # Note: torch.split does not create contiguous tensors by default.\n", "    if contiguous_split_chunks:\n\t        return tuple(chunk.contiguous() for chunk in tensor_list)\n\t    return tensor_list\n\tclass RotaryEmbedding(nn.Module):\n\t    def __init__(self, dim, original_impl=False, device=None, dtype=None):\n\t        super().__init__()\n\t        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=device).to(dtype=dtype) / dim))\n\t        self.register_buffer(\"inv_freq\", inv_freq)\n\t        self.dim = dim\n\t        self.original_impl = original_impl\n", "    def forward_impl(\n\t            self, seq_len: int, n_elem: int, dtype: torch.dtype, device: torch.device, base: int = 10000\n\t    ):\n\t        \"\"\"Enhanced Transformer with Rotary Position Embedding.\n\t        Derived from: https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/labml_nn/\n\t        transformers/rope/__init__.py. MIT License:\n\t        https://github.com/labmlai/annotated_deep_learning_paper_implementations/blob/master/license.\n\t        \"\"\"\n\t        # $\\Theta = {\\theta_i = 10000^{\\frac{2(i-1)}{d}}, i \\in [1, 2, ..., \\frac{d}{2}]}$\n\t        theta = 1.0 / (base ** (torch.arange(0, n_elem, 2, dtype=dtype, device=device) / n_elem))\n", "        # Create position indexes `[0, 1, ..., seq_len - 1]`\n\t        seq_idx = torch.arange(seq_len, dtype=dtype, device=device)\n\t        # Calculate the product of position index and $\\theta_i$\n\t        idx_theta = torch.outer(seq_idx, theta).float()\n\t        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)], dim=-1)\n\t        # this is to mimic the behaviour of complex32, else we will get different results\n\t        if dtype in (torch.float16, torch.bfloat16, torch.int8):\n\t            cache = cache.bfloat16() if dtype == torch.bfloat16 else cache.half()\n\t        return cache\n\t    def forward(self, max_seq_len, offset=0):\n", "        return self.forward_impl(\n\t            max_seq_len, self.dim, dtype=self.inv_freq.dtype, device=self.inv_freq.device\n\t        )\n\t@torch.jit.script\n\tdef apply_rotary_pos_emb(x: torch.Tensor, rope_cache: torch.Tensor) -> torch.Tensor:\n\t    # x: [sq, b, np, hn]\n\t    sq, b, np, hn = x.size(0), x.size(1), x.size(2), x.size(3)\n\t    rot_dim = rope_cache.shape[-2] * 2\n\t    x, x_pass = x[..., :rot_dim], x[..., rot_dim:]\n\t    # truncate to support variable sizes\n", "    rope_cache = rope_cache[:sq]\n\t    xshaped = x.reshape(sq, -1, np, rot_dim // 2, 2)\n\t    rope_cache = rope_cache.view(sq, -1, 1, xshaped.size(3), 2)\n\t    x_out2 = torch.stack(\n\t        [\n\t            xshaped[..., 0] * rope_cache[..., 0] - xshaped[..., 1] * rope_cache[..., 1],\n\t            xshaped[..., 1] * rope_cache[..., 0] + xshaped[..., 0] * rope_cache[..., 1],\n\t        ],\n\t        -1,\n\t    )\n", "    x_out2 = x_out2.flatten(3)\n\t    return torch.cat((x_out2, x_pass), dim=-1)\n\tclass RMSNorm(torch.nn.Module):\n\t    def __init__(self, normalized_shape, eps=1e-5, device=None, dtype=None, **kwargs):\n\t        super().__init__()\n\t        self.weight = torch.nn.Parameter(torch.empty(normalized_shape, device=device, dtype=dtype))\n\t        self.eps = eps\n\t    def forward(self, hidden_states: torch.Tensor):\n\t        input_dtype = hidden_states.dtype\n\t        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n", "        hidden_states = hidden_states * torch.rsqrt(variance + self.eps)\n\t        return (self.weight * hidden_states).to(input_dtype)\n\tclass CoreAttention(torch.nn.Module):\n\t    def __init__(self, config: ChatGLMConfig, layer_number):\n\t        super(CoreAttention, self).__init__()\n\t        self.apply_query_key_layer_scaling = config.apply_query_key_layer_scaling\n\t        self.attention_softmax_in_fp32 = config.attention_softmax_in_fp32\n\t        if self.apply_query_key_layer_scaling:\n\t            self.attention_softmax_in_fp32 = True\n\t        self.layer_number = max(1, layer_number)\n", "        projection_size = config.kv_channels * config.num_attention_heads\n\t        # Per attention head and per partition values.\n\t        self.hidden_size_per_partition = projection_size\n\t        self.hidden_size_per_attention_head = projection_size // config.num_attention_heads\n\t        self.num_attention_heads_per_partition = config.num_attention_heads\n\t        coeff = None\n\t        self.norm_factor = math.sqrt(self.hidden_size_per_attention_head)\n\t        if self.apply_query_key_layer_scaling:\n\t            coeff = self.layer_number\n\t            self.norm_factor *= coeff\n", "        self.coeff = coeff\n\t        self.attention_dropout = torch.nn.Dropout(config.attention_dropout)\n\t    def forward(self, query_layer, key_layer, value_layer, attention_mask):\n\t        pytorch_major_version = int(torch.__version__.split('.')[0])\n\t        if pytorch_major_version >= 2:\n\t            query_layer, key_layer, value_layer = [k.permute(1, 2, 0, 3) for k in [query_layer, key_layer, value_layer]]\n\t            if attention_mask is None and query_layer.shape[2] == key_layer.shape[2]:\n\t                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n\t                                                                                 is_causal=True)\n\t            else:\n", "                if attention_mask is not None:\n\t                    attention_mask = ~attention_mask\n\t                context_layer = torch.nn.functional.scaled_dot_product_attention(query_layer, key_layer, value_layer,\n\t                                                                                 attention_mask)\n\t            context_layer = context_layer.permute(2, 0, 1, 3)\n\t            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n\t            context_layer = context_layer.reshape(*new_context_layer_shape)\n\t        else:\n\t            # Raw attention scores\n\t            # [b, np, sq, sk]\n", "            output_size = (query_layer.size(1), query_layer.size(2), query_layer.size(0), key_layer.size(0))\n\t            # [sq, b, np, hn] -> [sq, b * np, hn]\n\t            query_layer = query_layer.view(output_size[2], output_size[0] * output_size[1], -1)\n\t            # [sk, b, np, hn] -> [sk, b * np, hn]\n\t            key_layer = key_layer.view(output_size[3], output_size[0] * output_size[1], -1)\n\t            # preallocting input tensor: [b * np, sq, sk]\n\t            matmul_input_buffer = torch.empty(\n\t                output_size[0] * output_size[1], output_size[2], output_size[3], dtype=query_layer.dtype,\n\t                device=query_layer.device\n\t            )\n", "            # Raw attention scores. [b * np, sq, sk]\n\t            matmul_result = torch.baddbmm(\n\t                matmul_input_buffer,\n\t                query_layer.transpose(0, 1),  # [b * np, sq, hn]\n\t                key_layer.transpose(0, 1).transpose(1, 2),  # [b * np, hn, sk]\n\t                beta=0.0,\n\t                alpha=(1.0 / self.norm_factor),\n\t            )\n\t            # change view to [b, np, sq, sk]\n\t            attention_scores = matmul_result.view(*output_size)\n", "            # ===========================\n\t            # Attention probs and dropout\n\t            # ===========================\n\t            # attention scores and attention mask [b, np, sq, sk]\n\t            if self.attention_softmax_in_fp32:\n\t                attention_scores = attention_scores.float()\n\t            if self.coeff is not None:\n\t                attention_scores = attention_scores * self.coeff\n\t            if attention_mask is None and attention_scores.shape[2] == attention_scores.shape[3]:\n\t                attention_mask = torch.ones(output_size[0], 1, output_size[2], output_size[3],\n", "                                            device=attention_scores.device, dtype=torch.bool)\n\t                attention_mask.tril_()\n\t                attention_mask = ~attention_mask\n\t            if attention_mask is not None:\n\t                attention_scores = attention_scores.masked_fill(attention_mask, float(\"-inf\"))\n\t            attention_probs = F.softmax(attention_scores, dim=-1)\n\t            attention_probs = attention_probs.type_as(value_layer)\n\t            # This is actually dropping out entire tokens to attend to, which might\n\t            # seem a bit unusual, but is taken from the original Transformer paper.\n\t            attention_probs = self.attention_dropout(attention_probs)\n", "            # =========================\n\t            # Context layer. [sq, b, hp]\n\t            # =========================\n\t            # value_layer -> context layer.\n\t            # [sk, b, np, hn] --> [b, np, sq, hn]\n\t            # context layer shape: [b, np, sq, hn]\n\t            output_size = (value_layer.size(1), value_layer.size(2), query_layer.size(0), value_layer.size(3))\n\t            # change view [sk, b * np, hn]\n\t            value_layer = value_layer.view(value_layer.size(0), output_size[0] * output_size[1], -1)\n\t            # change view [b * np, sq, sk]\n", "            attention_probs = attention_probs.view(output_size[0] * output_size[1], output_size[2], -1)\n\t            # matmul: [b * np, sq, hn]\n\t            context_layer = torch.bmm(attention_probs, value_layer.transpose(0, 1))\n\t            # change view [b, np, sq, hn]\n\t            context_layer = context_layer.view(*output_size)\n\t            # [b, np, sq, hn] --> [sq, b, np, hn]\n\t            context_layer = context_layer.permute(2, 0, 1, 3).contiguous()\n\t            # [sq, b, np, hn] --> [sq, b, hp]\n\t            new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size_per_partition,)\n\t            context_layer = context_layer.view(*new_context_layer_shape)\n", "        return context_layer\n\tclass SelfAttention(torch.nn.Module):\n\t    \"\"\"Parallel self-attention layer abstract class.\n\t    Self-attention layer takes input with size [s, b, h]\n\t    and returns output of the same size.\n\t    \"\"\"\n\t    def __init__(self, config: ChatGLMConfig, layer_number, device=None):\n\t        super(SelfAttention, self).__init__()\n\t        self.layer_number = max(1, layer_number)\n\t        self.projection_size = config.kv_channels * config.num_attention_heads\n", "        # Per attention head and per partition values.\n\t        self.hidden_size_per_attention_head = self.projection_size // config.num_attention_heads\n\t        self.num_attention_heads_per_partition = config.num_attention_heads\n\t        self.multi_query_attention = config.multi_query_attention\n\t        self.qkv_hidden_size = 3 * self.projection_size\n\t        if self.multi_query_attention:\n\t            self.num_multi_query_groups_per_partition = config.multi_query_group_num\n\t            self.qkv_hidden_size = (\n\t                    self.projection_size + 2 * self.hidden_size_per_attention_head * config.multi_query_group_num\n\t            )\n", "        self.query_key_value = nn.Linear(config.hidden_size, self.qkv_hidden_size,\n\t                                         bias=config.add_bias_linear or config.add_qkv_bias,\n\t                                         device=device, **_config_to_kwargs(config)\n\t                                         )\n\t        self.core_attention = CoreAttention(config, self.layer_number)\n\t        # Output.\n\t        self.dense = nn.Linear(self.projection_size, config.hidden_size, bias=config.add_bias_linear,\n\t                               device=device, **_config_to_kwargs(config)\n\t                               )\n\t    def _allocate_memory(self, inference_max_sequence_len, batch_size, device=None, dtype=None):\n", "        if self.multi_query_attention:\n\t            num_attention_heads = self.num_multi_query_groups_per_partition\n\t        else:\n\t            num_attention_heads = self.num_attention_heads_per_partition\n\t        return torch.empty(\n\t            inference_max_sequence_len,\n\t            batch_size,\n\t            num_attention_heads,\n\t            self.hidden_size_per_attention_head,\n\t            dtype=dtype,\n", "            device=device,\n\t        )\n\t    def forward(\n\t            self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True\n\t    ):\n\t        # hidden_states: [sq, b, h]\n\t        # =================================================\n\t        # Pre-allocate memory for key-values for inference.\n\t        # =================================================\n\t        # =====================\n", "        # Query, Key, and Value\n\t        # =====================\n\t        # Attention heads [sq, b, h] --> [sq, b, (np * 3 * hn)]\n\t        mixed_x_layer = self.query_key_value(hidden_states)\n\t        if self.multi_query_attention:\n\t            (query_layer, key_layer, value_layer) = mixed_x_layer.split(\n\t                [\n\t                    self.num_attention_heads_per_partition * self.hidden_size_per_attention_head,\n\t                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,\n\t                    self.num_multi_query_groups_per_partition * self.hidden_size_per_attention_head,\n", "                ],\n\t                dim=-1,\n\t            )\n\t            query_layer = query_layer.view(\n\t                query_layer.size()[:-1] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)\n\t            )\n\t            key_layer = key_layer.view(\n\t                key_layer.size()[:-1] + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)\n\t            )\n\t            value_layer = value_layer.view(\n", "                value_layer.size()[:-1]\n\t                + (self.num_multi_query_groups_per_partition, self.hidden_size_per_attention_head)\n\t            )\n\t        else:\n\t            new_tensor_shape = mixed_x_layer.size()[:-1] + \\\n\t                               (self.num_attention_heads_per_partition,\n\t                                3 * self.hidden_size_per_attention_head)\n\t            mixed_x_layer = mixed_x_layer.view(*new_tensor_shape)\n\t            # [sq, b, np, 3 * hn] --> 3 [sq, b, np, hn]\n\t            (query_layer, key_layer, value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n", "        # apply relative positional encoding (rotary embedding)\n\t        if rotary_pos_emb is not None:\n\t            query_layer = apply_rotary_pos_emb(query_layer, rotary_pos_emb)\n\t            key_layer = apply_rotary_pos_emb(key_layer, rotary_pos_emb)\n\t        # adjust key and value for inference\n\t        if use_cache:\n\t            if kv_cache is not None:\n\t                cache_k, cache_v = kv_cache\n\t                key_layer = torch.cat((cache_k, key_layer), dim=0)\n\t                value_layer = torch.cat((cache_v, value_layer), dim=0)\n", "            kv_cache = (key_layer, value_layer)\n\t        else:\n\t            kv_cache = None\n\t        if self.multi_query_attention:\n\t            key_layer = key_layer.unsqueeze(-2)\n\t            key_layer = key_layer.expand(\n\t                -1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1\n\t            )\n\t            key_layer = key_layer.contiguous().view(\n\t                key_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)\n", "            )\n\t            value_layer = value_layer.unsqueeze(-2)\n\t            value_layer = value_layer.expand(\n\t                -1, -1, -1, self.num_attention_heads_per_partition // self.num_multi_query_groups_per_partition, -1\n\t            )\n\t            value_layer = value_layer.contiguous().view(\n\t                value_layer.size()[:2] + (self.num_attention_heads_per_partition, self.hidden_size_per_attention_head)\n\t            )\n\t        # ==================================\n\t        # core attention computation\n", "        # ==================================\n\t        context_layer = self.core_attention(query_layer, key_layer, value_layer, attention_mask)\n\t        # =================\n\t        # Output. [sq, b, h]\n\t        # =================\n\t        output = self.dense(context_layer)\n\t        return output, kv_cache\n\tdef _config_to_kwargs(args):\n\t    common_kwargs = {\n\t        \"dtype\": args.torch_dtype,\n", "    }\n\t    return common_kwargs\n\tclass MLP(torch.nn.Module):\n\t    \"\"\"MLP.\n\t    MLP will take the input with h hidden state, project it to 4*h\n\t    hidden dimension, perform nonlinear transformation, and project the\n\t    state back into h hidden dimension.\n\t    \"\"\"\n\t    def __init__(self, config: ChatGLMConfig, device=None):\n\t        super(MLP, self).__init__()\n", "        self.add_bias = config.add_bias_linear\n\t        # Project to 4h. If using swiglu double the output width, see https://arxiv.org/pdf/2002.05202.pdf\n\t        self.dense_h_to_4h = nn.Linear(\n\t            config.hidden_size,\n\t            config.ffn_hidden_size * 2,\n\t            bias=self.add_bias,\n\t            device=device,\n\t            **_config_to_kwargs(config)\n\t        )\n\t        def swiglu(x):\n", "            x = torch.chunk(x, 2, dim=-1)\n\t            return F.silu(x[0]) * x[1]\n\t        self.activation_func = swiglu\n\t        # Project back to h.\n\t        self.dense_4h_to_h = nn.Linear(\n\t            config.ffn_hidden_size,\n\t            config.hidden_size,\n\t            bias=self.add_bias,\n\t            device=device,\n\t            **_config_to_kwargs(config)\n", "        )\n\t    def forward(self, hidden_states):\n\t        # [s, b, 4hp]\n\t        intermediate_parallel = self.dense_h_to_4h(hidden_states)\n\t        intermediate_parallel = self.activation_func(intermediate_parallel)\n\t        # [s, b, h]\n\t        output = self.dense_4h_to_h(intermediate_parallel)\n\t        return output\n\tclass GLMBlock(torch.nn.Module):\n\t    \"\"\"A single transformer layer.\n", "    Transformer layer takes input with size [s, b, h] and returns an\n\t    output of the same size.\n\t    \"\"\"\n\t    def __init__(self, config: ChatGLMConfig, layer_number, device=None):\n\t        super(GLMBlock, self).__init__()\n\t        self.layer_number = layer_number\n\t        self.apply_residual_connection_post_layernorm = config.apply_residual_connection_post_layernorm\n\t        self.fp32_residual_connection = config.fp32_residual_connection\n\t        LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm\n\t        # Layernorm on the input data.\n", "        self.input_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n\t                                             dtype=config.torch_dtype)\n\t        # Self attention.\n\t        self.self_attention = SelfAttention(config, layer_number, device=device)\n\t        self.hidden_dropout = config.hidden_dropout\n\t        # Layernorm on the attention output\n\t        self.post_attention_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n\t                                                      dtype=config.torch_dtype)\n\t        # MLP\n\t        self.mlp = MLP(config, device=device)\n", "    def forward(\n\t            self, hidden_states, attention_mask, rotary_pos_emb, kv_cache=None, use_cache=True,\n\t    ):\n\t        # hidden_states: [s, b, h]\n\t        # Layer norm at the beginning of the transformer layer.\n\t        layernorm_output = self.input_layernorm(hidden_states)\n\t        # Self attention.\n\t        attention_output, kv_cache = self.self_attention(\n\t            layernorm_output,\n\t            attention_mask,\n", "            rotary_pos_emb,\n\t            kv_cache=kv_cache,\n\t            use_cache=use_cache\n\t        )\n\t        # Residual connection.\n\t        if self.apply_residual_connection_post_layernorm:\n\t            residual = layernorm_output\n\t        else:\n\t            residual = hidden_states\n\t        layernorm_input = torch.nn.functional.dropout(attention_output, p=self.hidden_dropout, training=self.training)\n", "        layernorm_input = residual + layernorm_input\n\t        # Layer norm post the self attention.\n\t        layernorm_output = self.post_attention_layernorm(layernorm_input)\n\t        # MLP.\n\t        mlp_output = self.mlp(layernorm_output)\n\t        # Second residual connection.\n\t        if self.apply_residual_connection_post_layernorm:\n\t            residual = layernorm_output\n\t        else:\n\t            residual = layernorm_input\n", "        output = torch.nn.functional.dropout(mlp_output, p=self.hidden_dropout, training=self.training)\n\t        output = residual + output\n\t        return output, kv_cache\n\tclass GLMTransformer(torch.nn.Module):\n\t    \"\"\"Transformer class.\"\"\"\n\t    def __init__(self, config: ChatGLMConfig, device=None):\n\t        super(GLMTransformer, self).__init__()\n\t        self.fp32_residual_connection = config.fp32_residual_connection\n\t        self.post_layer_norm = config.post_layer_norm\n\t        # Number of layers.\n", "        self.num_layers = config.num_layers\n\t        # Transformer layers.\n\t        def build_layer(layer_number):\n\t            return GLMBlock(config, layer_number, device=device)\n\t        self.layers = torch.nn.ModuleList([build_layer(i + 1) for i in range(self.num_layers)])\n\t        if self.post_layer_norm:\n\t            LayerNormFunc = RMSNorm if config.rmsnorm else LayerNorm\n\t            # Final layer norm before output.\n\t            self.final_layernorm = LayerNormFunc(config.hidden_size, eps=config.layernorm_epsilon, device=device,\n\t                                                 dtype=config.torch_dtype)\n", "        self.gradient_checkpointing = False\n\t    def _get_layer(self, layer_number):\n\t        return self.layers[layer_number]\n\t    def forward(\n\t            self, hidden_states, attention_mask, rotary_pos_emb, kv_caches=None,\n\t            use_cache: Optional[bool] = True,\n\t            output_hidden_states: Optional[bool] = False,\n\t    ):\n\t        if not kv_caches:\n\t            kv_caches = [None for _ in range(self.num_layers)]\n", "        presents = () if use_cache else None\n\t        if self.gradient_checkpointing and self.training:\n\t            if use_cache:\n\t                logger.warning_once(\n\t                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n\t                )\n\t                use_cache = False\n\t        all_self_attentions = None\n\t        all_hidden_states = () if output_hidden_states else None\n\t        for index in range(self.num_layers):\n", "            if output_hidden_states:\n\t                all_hidden_states = all_hidden_states + (hidden_states,)\n\t            layer = self._get_layer(index)\n\t            if self.gradient_checkpointing and self.training:\n\t                layer_ret = torch.utils.checkpoint.checkpoint(\n\t                    layer,\n\t                    hidden_states,\n\t                    attention_mask,\n\t                    rotary_pos_emb,\n\t                    kv_caches[index],\n", "                    use_cache\n\t                )\n\t            else:\n\t                layer_ret = layer(\n\t                    hidden_states,\n\t                    attention_mask,\n\t                    rotary_pos_emb,\n\t                    kv_cache=kv_caches[index],\n\t                    use_cache=use_cache\n\t                )\n", "            hidden_states, kv_cache = layer_ret\n\t            if use_cache:\n\t                presents = presents + (kv_cache,)\n\t        if output_hidden_states:\n\t            all_hidden_states = all_hidden_states + (hidden_states,)\n\t        # Final layer norm.\n\t        if self.post_layer_norm:\n\t            hidden_states = self.final_layernorm(hidden_states)\n\t        return hidden_states, presents, all_hidden_states, all_self_attentions\n\tclass ChatGLMPreTrainedModel(PreTrainedModel):\n", "    \"\"\"\n\t    An abstract class to handle weights initialization and\n\t    a simple interface for downloading and loading pretrained models.\n\t    \"\"\"\n\t    is_parallelizable = False\n\t    supports_gradient_checkpointing = True\n\t    config_class = ChatGLMConfig\n\t    base_model_prefix = \"transformer\"\n\t    _no_split_modules = [\"GLMBlock\"]\n\t    def _init_weights(self, module: nn.Module):\n", "        \"\"\"Initialize the weights.\"\"\"\n\t        return\n\t    def get_masks(self, input_ids, past_key_values, padding_mask=None):\n\t        batch_size, seq_length = input_ids.shape\n\t        full_attention_mask = torch.ones(batch_size, seq_length, seq_length, device=input_ids.device)\n\t        full_attention_mask.tril_()\n\t        past_length = 0\n\t        if past_key_values:\n\t            past_length = past_key_values[0][0].shape[0]\n\t        if past_length:\n", "            full_attention_mask = torch.cat((torch.ones(batch_size, seq_length, past_length,\n\t                                                        device=input_ids.device), full_attention_mask), dim=-1)\n\t        if padding_mask is not None:\n\t            full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)\n\t        if not past_length and padding_mask is not None:\n\t            full_attention_mask -= padding_mask.unsqueeze(-1) - 1\n\t        full_attention_mask = (full_attention_mask < 0.5).bool()\n\t        full_attention_mask.unsqueeze_(1)\n\t        return full_attention_mask\n\t    def get_position_ids(self, input_ids, device):\n", "        batch_size, seq_length = input_ids.shape\n\t        position_ids = torch.arange(seq_length, dtype=torch.long, device=device).unsqueeze(0).repeat(batch_size, 1)\n\t        return position_ids\n\t    def _set_gradient_checkpointing(self, module, value=False):\n\t        if isinstance(module, GLMTransformer):\n\t            module.gradient_checkpointing = value\n\tclass Embedding(torch.nn.Module):\n\t    \"\"\"Language model embeddings.\"\"\"\n\t    def __init__(self, config: ChatGLMConfig, device=None):\n\t        super(Embedding, self).__init__()\n", "        self.hidden_size = config.hidden_size\n\t        # Word embeddings (parallel).\n\t        self.word_embeddings = nn.Embedding(\n\t            config.padded_vocab_size,\n\t            self.hidden_size,\n\t            dtype=config.torch_dtype,\n\t            device=device\n\t        )\n\t        self.fp32_residual_connection = config.fp32_residual_connection\n\t    def forward(self, input_ids):\n", "        # Embeddings.\n\t        words_embeddings = self.word_embeddings(input_ids)\n\t        embeddings = words_embeddings\n\t        # Data format change to avoid explicit tranposes : [b s h] --> [s b h].\n\t        embeddings = embeddings.transpose(0, 1).contiguous()\n\t        # If the input flag for fp32 residual connection is set, convert for float.\n\t        if self.fp32_residual_connection:\n\t            embeddings = embeddings.float()\n\t        return embeddings\n\tclass ChatGLMModel(ChatGLMPreTrainedModel):\n", "    def __init__(self, config: ChatGLMConfig, device=None, empty_init=True):\n\t        super().__init__(config)\n\t        if empty_init:\n\t            init_method = skip_init\n\t        else:\n\t            init_method = default_init\n\t        init_kwargs = {}\n\t        if device is not None:\n\t            init_kwargs[\"device\"] = device\n\t        self.embedding = init_method(Embedding, config, **init_kwargs)\n", "        self.num_layers = config.num_layers\n\t        self.multi_query_group_num = config.multi_query_group_num\n\t        self.kv_channels = config.kv_channels\n\t        # Rotary positional embeddings\n\t        self.seq_length = config.seq_length\n\t        rotary_dim = (\n\t            config.hidden_size // config.num_attention_heads if config.kv_channels is None else config.kv_channels\n\t        )\n\t        self.rotary_pos_emb = RotaryEmbedding(rotary_dim // 2, original_impl=config.original_rope, device=device,\n\t                                              dtype=config.torch_dtype)\n", "        self.encoder = init_method(GLMTransformer, config, **init_kwargs)\n\t        self.output_layer = init_method(nn.Linear, config.hidden_size, config.padded_vocab_size, bias=False,\n\t                                        dtype=config.torch_dtype, **init_kwargs)\n\t        self.pre_seq_len = config.pre_seq_len\n\t        self.prefix_projection = config.prefix_projection\n\t        if self.pre_seq_len is not None:\n\t            for param in self.parameters():\n\t                param.requires_grad = False\n\t            self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n\t            self.prefix_encoder = PrefixEncoder(config)\n", "            self.dropout = torch.nn.Dropout(0.1)\n\t    def get_input_embeddings(self):\n\t        return self.embedding.word_embeddings\n\t    def get_prompt(self, batch_size, device, dtype=torch.half):\n\t        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(device)\n\t        past_key_values = self.prefix_encoder(prefix_tokens).type(dtype)\n\t        past_key_values = past_key_values.view(\n\t            batch_size,\n\t            self.pre_seq_len,\n\t            self.num_layers * 2,\n", "            self.multi_query_group_num,\n\t            self.kv_channels\n\t        )\n\t        # seq_len, b, nh, hidden_size\n\t        past_key_values = self.dropout(past_key_values)\n\t        past_key_values = past_key_values.permute([2, 1, 0, 3, 4]).split(2)\n\t        return past_key_values\n\t    def forward(\n\t            self,\n\t            input_ids,\n", "            position_ids: Optional[torch.Tensor] = None,\n\t            attention_mask: Optional[torch.BoolTensor] = None,\n\t            full_attention_mask: Optional[torch.BoolTensor] = None,\n\t            past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n\t            inputs_embeds: Optional[torch.Tensor] = None,\n\t            use_cache: Optional[bool] = None,\n\t            output_hidden_states: Optional[bool] = None,\n\t            return_dict: Optional[bool] = None,\n\t    ):\n\t        output_hidden_states = (\n", "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n\t        )\n\t        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\t        batch_size, seq_length = input_ids.shape\n\t        if inputs_embeds is None:\n\t            inputs_embeds = self.embedding(input_ids)\n\t        if full_attention_mask is None:\n\t            if (attention_mask is not None and not attention_mask.all()) or (past_key_values and seq_length != 1):\n\t                full_attention_mask = self.get_masks(input_ids, past_key_values, padding_mask=attention_mask)\n", "        # Rotary positional embeddings\n\t        rotary_pos_emb = self.rotary_pos_emb(self.seq_length)\n\t        if position_ids is not None:\n\t            rotary_pos_emb = rotary_pos_emb[position_ids]\n\t        else:\n\t            rotary_pos_emb = rotary_pos_emb[None, :seq_length]\n\t        rotary_pos_emb = rotary_pos_emb.transpose(0, 1).contiguous()\n\t        if past_key_values is None:\n\t            if self.pre_seq_len is not None:\n\t                past_key_values = self.get_prompt(batch_size=batch_size, device=input_ids.device,\n", "                                                  dtype=inputs_embeds.dtype)\n\t        # Run encoder.\n\t        hidden_states, presents, all_hidden_states, all_self_attentions = self.encoder(\n\t            inputs_embeds, full_attention_mask, rotary_pos_emb=rotary_pos_emb,\n\t            kv_caches=past_key_values, use_cache=use_cache, output_hidden_states=output_hidden_states\n\t        )\n\t        if not return_dict:\n\t            return tuple(v for v in [hidden_states, presents, all_hidden_states, all_self_attentions] if v is not None)\n\t        return BaseModelOutputWithPast(\n\t            last_hidden_state=hidden_states,\n", "            past_key_values=presents,\n\t            hidden_states=all_hidden_states,\n\t            attentions=all_self_attentions,\n\t        )\n\t    def quantize(self, weight_bit_width: int):\n\t        from .quantization import quantize\n\t        quantize(self.encoder, weight_bit_width)\n\t        return self\n\tclass ChatGLMForConditionalGeneration(ChatGLMPreTrainedModel):\n\t    def __init__(self, config: ChatGLMConfig, empty_init=True, device=None):\n", "        super().__init__(config)\n\t        self.max_sequence_length = config.max_length\n\t        self.transformer = ChatGLMModel(config, empty_init=empty_init, device=device)\n\t        self.config = config\n\t        self.quantized = False\n\t        if self.config.quantization_bit:\n\t            self.quantize(self.config.quantization_bit, empty_init=True)\n\t    def _update_model_kwargs_for_generation(\n\t            self,\n\t            outputs: ModelOutput,\n", "            model_kwargs: Dict[str, Any],\n\t            is_encoder_decoder: bool = False,\n\t            standardize_cache_format: bool = False,\n\t    ) -> Dict[str, Any]:\n\t        # update past_key_values\n\t        model_kwargs[\"past_key_values\"] = self._extract_past_from_model_output(\n\t            outputs, standardize_cache_format=standardize_cache_format\n\t        )\n\t        # update attention mask\n\t        if \"attention_mask\" in model_kwargs:\n", "            attention_mask = model_kwargs[\"attention_mask\"]\n\t            model_kwargs[\"attention_mask\"] = torch.cat(\n\t                [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n\t            )\n\t        # update position ids\n\t        if \"position_ids\" in model_kwargs:\n\t            position_ids = model_kwargs[\"position_ids\"]\n\t            new_position_id = position_ids[..., -1:].clone()\n\t            new_position_id += 1\n\t            model_kwargs[\"position_ids\"] = torch.cat(\n", "                [position_ids, new_position_id], dim=-1\n\t            )\n\t        model_kwargs[\"is_first_forward\"] = False\n\t        return model_kwargs\n\t    def prepare_inputs_for_generation(\n\t            self,\n\t            input_ids: torch.LongTensor,\n\t            past_key_values: Optional[torch.Tensor] = None,\n\t            attention_mask: Optional[torch.Tensor] = None,\n\t            position_ids: Optional[torch.Tensor] = None,\n", "            is_first_forward: bool = True,\n\t            **kwargs\n\t    ) -> dict:\n\t        # only last token for input_ids if past is not None\n\t        if position_ids is None:\n\t            position_ids = self.get_position_ids(input_ids, device=input_ids.device)\n\t        if not is_first_forward:\n\t            position_ids = position_ids[..., -1:]\n\t            input_ids = input_ids[:, -1:]\n\t        return {\n", "            \"input_ids\": input_ids,\n\t            \"past_key_values\": past_key_values,\n\t            \"position_ids\": position_ids,\n\t            \"attention_mask\": attention_mask,\n\t            \"return_last_logit\": True\n\t        }\n\t    def forward(\n\t            self,\n\t            input_ids: Optional[torch.Tensor] = None,\n\t            position_ids: Optional[torch.Tensor] = None,\n", "            past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n\t            inputs_embeds: Optional[torch.Tensor] = None,\n\t            labels: Optional[torch.Tensor] = None,\n\t            use_cache: Optional[bool] = None,\n\t            output_attentions: Optional[bool] = None,\n\t            output_hidden_states: Optional[bool] = None,\n\t            return_dict: Optional[bool] = None,\n\t            return_last_logit: Optional[bool] = False,\n\t    ):\n\t        attention_mask: Optional[torch.Tensor] = None\n", "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\t        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\t        transformer_outputs = self.transformer(\n\t            input_ids=input_ids,\n\t            position_ids=position_ids,\n\t            attention_mask=attention_mask,\n\t            past_key_values=past_key_values,\n\t            inputs_embeds=inputs_embeds,\n\t            use_cache=use_cache,\n\t            output_hidden_states=output_hidden_states,\n", "            return_dict=return_dict,\n\t        )\n\t        hidden_states = transformer_outputs[0]\n\t        if return_last_logit:\n\t            hidden_states = hidden_states[-1:]\n\t        lm_logits = self.transformer.output_layer(hidden_states)\n\t        lm_logits = lm_logits.transpose(0, 1).contiguous()\n\t        loss = None\n\t        if labels is not None:\n\t            lm_logits = lm_logits.to(torch.float32)\n", "            # Shift so that tokens < n predict n\n\t            shift_logits = lm_logits[..., :-1, :].contiguous()\n\t            shift_labels = labels[..., 1:].contiguous()\n\t            # Flatten the tokens\n\t            loss_fct = CrossEntropyLoss(ignore_index=-100)\n\t            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\t            lm_logits = lm_logits.to(hidden_states.dtype)\n\t            loss = loss.to(hidden_states.dtype)\n\t        if not return_dict:\n\t            output = (lm_logits,) + transformer_outputs[1:]\n", "            return ((loss,) + output) if loss is not None else output\n\t        return CausalLMOutputWithPast(\n\t            loss=loss,\n\t            logits=lm_logits,\n\t            past_key_values=transformer_outputs.past_key_values,\n\t            hidden_states=transformer_outputs.hidden_states,\n\t            attentions=transformer_outputs.attentions,\n\t        )\n\t    @staticmethod\n\t    def _reorder_cache(\n", "            past: Tuple[Tuple[torch.Tensor, torch.Tensor], ...], beam_idx: torch.LongTensor\n\t    ) -> Tuple[Tuple[torch.Tensor, torch.Tensor], ...]:\n\t        \"\"\"\n\t        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n\t        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n\t        beam_idx at every generation step.\n\t        Output shares the same memory storage as `past`.\n\t        \"\"\"\n\t        return tuple(\n\t            (\n", "                layer_past[0].index_select(1, beam_idx.to(layer_past[0].device)),\n\t                layer_past[1].index_select(1, beam_idx.to(layer_past[1].device)),\n\t            )\n\t            for layer_past in past\n\t        )\n\t    def process_response(self, response):\n\t        response = response.strip()\n\t        response = response.replace(\"[[ËÆ≠ÁªÉÊó∂Èó¥]]\", \"2023Âπ¥\")\n\t        return response\n\t    def build_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):\n", "        prompt = tokenizer.build_prompt(query, history=history)\n\t        inputs = tokenizer([prompt], return_tensors=\"pt\")\n\t        inputs = inputs.to(self.device)\n\t        return inputs\n\t    def build_stream_inputs(self, tokenizer, query: str, history: List[Tuple[str, str]] = None):\n\t        if history:\n\t            prompt = \"\\n\\n[Round {}]\\n\\nÈóÆÔºö{}\\n\\nÁ≠îÔºö\".format(len(history) + 1, query)\n\t            input_ids = tokenizer.encode(prompt, add_special_tokens=False)\n\t            input_ids = input_ids[1:]\n\t            inputs = tokenizer.batch_encode_plus([(input_ids, None)], return_tensors=\"pt\", add_special_tokens=False)\n", "        else:\n\t            prompt = \"[Round {}]\\n\\nÈóÆÔºö{}\\n\\nÁ≠îÔºö\".format(len(history) + 1, query)\n\t            inputs = tokenizer([prompt], return_tensors=\"pt\")\n\t        inputs = inputs.to(self.device)\n\t        return inputs\n\t    @torch.no_grad()\n\t    def chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, max_length: int = 8192, num_beams=1,\n\t             do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None, **kwargs):\n\t        if history is None:\n\t            history = []\n", "        if logits_processor is None:\n\t            logits_processor = LogitsProcessorList()\n\t        logits_processor.append(InvalidScoreLogitsProcessor())\n\t        gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams, \"do_sample\": do_sample, \"top_p\": top_p,\n\t                      \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n\t        inputs = self.build_inputs(tokenizer, query, history=history)\n\t        outputs = self.generate(**inputs, **gen_kwargs)\n\t        outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):]\n\t        response = tokenizer.decode(outputs)\n\t        response = self.process_response(response)\n", "        history = history + [(query, response)]\n\t        return response, history\n\t    @torch.no_grad()\n\t    def stream_chat(self, tokenizer, query: str, history: List[Tuple[str, str]] = None, past_key_values=None,\n\t                    max_length: int = 8192, do_sample=True, top_p=0.8, temperature=0.8, logits_processor=None,\n\t                    return_past_key_values=False, **kwargs):\n\t        if history is None:\n\t            history = []\n\t        if logits_processor is None:\n\t            logits_processor = LogitsProcessorList()\n", "        logits_processor.append(InvalidScoreLogitsProcessor())\n\t        gen_kwargs = {\"max_length\": max_length, \"do_sample\": do_sample, \"top_p\": top_p,\n\t                      \"temperature\": temperature, \"logits_processor\": logits_processor, **kwargs}\n\t        if past_key_values is None and not return_past_key_values:\n\t            inputs = self.build_inputs(tokenizer, query, history=history)\n\t        else:\n\t            inputs = self.build_stream_inputs(tokenizer, query, history=history)\n\t        if past_key_values is not None:\n\t            past_length = past_key_values[0][0].shape[0]\n\t            if self.transformer.pre_seq_len is not None:\n", "                past_length -= self.transformer.pre_seq_len\n\t            inputs.position_ids += past_length\n\t            attention_mask = inputs.attention_mask\n\t            attention_mask = torch.cat((attention_mask.new_ones(1, past_length), attention_mask), dim=1)\n\t            inputs['attention_mask'] = attention_mask\n\t        for outputs in self.stream_generate(**inputs, past_key_values=past_key_values,\n\t                                            return_past_key_values=return_past_key_values, **gen_kwargs):\n\t            if return_past_key_values:\n\t                outputs, past_key_values = outputs\n\t            outputs = outputs.tolist()[0][len(inputs[\"input_ids\"][0]):]\n", "            response = tokenizer.decode(outputs)\n\t            if response and response[-1] != \"ÔøΩ\":\n\t                response = self.process_response(response)\n\t                new_history = history + [(query, response)]\n\t                if return_past_key_values:\n\t                    yield response, new_history, past_key_values\n\t                else:\n\t                    yield response, new_history\n\t    @torch.no_grad()\n\t    def stream_generate(\n", "            self,\n\t            input_ids,\n\t            generation_config: Optional[GenerationConfig] = None,\n\t            logits_processor: Optional[LogitsProcessorList] = None,\n\t            stopping_criteria: Optional[StoppingCriteriaList] = None,\n\t            prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None,\n\t            return_past_key_values=False,\n\t            **kwargs,\n\t    ):\n\t        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n", "        if generation_config is None:\n\t            generation_config = self.generation_config\n\t        generation_config = copy.deepcopy(generation_config)\n\t        model_kwargs = generation_config.update(**kwargs)\n\t        bos_token_id, eos_token_id = generation_config.bos_token_id, generation_config.eos_token_id\n\t        if isinstance(eos_token_id, int):\n\t            eos_token_id = [eos_token_id]\n\t        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n\t        if has_default_max_length and generation_config.max_new_tokens is None:\n\t            warnings.warn(\n", "                f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n\t                \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n\t                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n\t                UserWarning,\n\t            )\n\t        elif generation_config.max_new_tokens is not None:\n\t            generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n\t            if not has_default_max_length:\n\t                logger.warn(\n\t                    f\"Both `max_new_tokens` (={generation_config.max_new_tokens}) and `max_length`(=\"\n", "                    f\"{generation_config.max_length}) seem to have been set. `max_new_tokens` will take precedence. \"\n\t                    \"Please refer to the documentation for more information. \"\n\t                    \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\",\n\t                    UserWarning,\n\t                )\n\t        if input_ids_seq_length >= generation_config.max_length:\n\t            input_ids_string = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n\t            logger.warning(\n\t                f\"Input length of {input_ids_string} is {input_ids_seq_length}, but `max_length` is set to\"\n\t                f\" {generation_config.max_length}. This can lead to unexpected behavior. You should consider\"\n", "                \" increasing `max_new_tokens`.\"\n\t            )\n\t        # 2. Set generation parameters if not already defined\n\t        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n\t        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n\t        logits_processor = self._get_logits_processor(\n\t            generation_config=generation_config,\n\t            input_ids_seq_length=input_ids_seq_length,\n\t            encoder_input_ids=input_ids,\n\t            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n", "            logits_processor=logits_processor,\n\t        )\n\t        stopping_criteria = self._get_stopping_criteria(\n\t            generation_config=generation_config, stopping_criteria=stopping_criteria\n\t        )\n\t        logits_warper = self._get_logits_warper(generation_config)\n\t        unfinished_sequences = input_ids.new(input_ids.shape[0]).fill_(1)\n\t        scores = None\n\t        while True:\n\t            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n", "            # forward pass to get next token\n\t            outputs = self(\n\t                **model_inputs,\n\t                return_dict=True,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t            )\n\t            next_token_logits = outputs.logits[:, -1, :]\n\t            # pre-process distribution\n\t            next_token_scores = logits_processor(input_ids, next_token_logits)\n", "            next_token_scores = logits_warper(input_ids, next_token_scores)\n\t            # sample\n\t            probs = nn.functional.softmax(next_token_scores, dim=-1)\n\t            if generation_config.do_sample:\n\t                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\t            else:\n\t                next_tokens = torch.argmax(probs, dim=-1)\n\t            # update generated ids, model inputs, and length for next step\n\t            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n\t            model_kwargs = self._update_model_kwargs_for_generation(\n", "                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n\t            )\n\t            unfinished_sequences = unfinished_sequences.mul((sum(next_tokens != i for i in eos_token_id)).long())\n\t            if return_past_key_values:\n\t                yield input_ids, outputs.past_key_values\n\t            else:\n\t                yield input_ids\n\t            # stop when each sentence is finished, or if we exceed the maximum length\n\t            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n\t                break\n", "    def quantize(self, bits: int, empty_init=False, device=None, **kwargs):\n\t        if bits == 0:\n\t            return\n\t        from .quantization import quantize\n\t        if self.quantized:\n\t            logger.info(\"Already quantized.\")\n\t            return self\n\t        self.quantized = True\n\t        self.config.quantization_bit = bits\n\t        self.transformer.encoder = quantize(self.transformer.encoder, bits, empty_init=empty_init, device=device,\n", "                                            **kwargs)\n\t        return self\n"]}
{"filename": "onnx_export/export_compare_data.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\timport argparse\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\tfrom onnx_export.utils import build_inputs\n\tfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\n\tfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\n", "from chatglm2_6b.configuration_chatglm import ChatGLMConfig\n\tparser = argparse.ArgumentParser(description='export pytorch model to onnx')\n\tparser.add_argument(\n\t    '--data_type',\n\t    default=\"fp32\",\n\t    help='use fp16/fp32 to export input/output, Defualt is fp32'\n\t)\n\targs = parser.parse_args()\n\tif args.data_type == \"fp16\":\n\t    device = 'cuda'\n", "else:\n\t    device = 'cpu'\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tif not os.path.exists(output_dir):\n\t    os.makedirs(output_dir)\n\t# save input tensor\n\tpt_input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n\tpt_input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\n\tpt_input_dict1 = dict()\n\tpt_input_dict2 = dict()\n", "# save output tensor\n\tpt_output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n\tpt_output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\n\tpt_output_dict1 = dict()\n\tpt_output_dict2 = dict()\n\tclass Container(torch.nn.Module):\n\t    def __init__(self, my_values):\n\t        super().__init__()\n\t        for key in my_values:\n\t            setattr(self, key, my_values[key])\n", "query = \"Êôö‰∏äÁù°‰∏çÁùÄÂ∫îËØ•ÊÄé‰πàÂäû\"\n\thistory = [\n\t    (\n\t        \"‰Ω†Â•Ω\",\n\t        \"‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\",\n\t    )\n\t]\n\tmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\n\ttokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\n\tinput_tensors = build_inputs(device, tokenizer, query, history)\n", "# model = ChatGLMForConditionalGeneration.from_pretrained(model_dir)\n\tconfig = ChatGLMConfig.from_pretrained(model_dir)\n\t# config.num_layers = 1\n\tmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\n\tif device == \"cuda\":\n\t    model = model.half().cuda()\n\telse:\n\t    model = model.float().cpu()\n\tmodel.eval()\n\t# debug this to get input2\n", "# model.chat(tokenizer=tokenizer, query=prompt)\n\t# test chat speed\n\t\"\"\"\n\tall_res = []\n\tprint(\"test chat speed for pytorch model, may cost a lot time\", )\n\ttest_text = \"‰Ω†Â•Ω, ËØ∑Áî®pythonÂÜô‰∏Ä‰∏™ÈìæË°®„ÄÇ\"\n\tst = time.time()\n\tfor i in trange(10):\n\t    responses, history = model.chat(tokenizer=tokenizer, query=test_text)\n\t    all_res.append(responses)\n", "et = time.time()\n\ttokens = tokenizer.encode(\"\".join(all_res), return_tensors=\"pt\")[0]\n\ttoken_num = len(tokens)\n\tspeed = round(token_num / (et - st), 1)\n\tprint(\"speed: {} tokens/s\".format(speed))\n\t\"\"\"\n\t# --- prepare data for input1 ---\n\tinput_ids1 = input_tensors[\"input_ids\"]\n\tposition_ids1 = input_tensors[\"position_ids\"]\n\t# save input1\n", "pt_input_dict1[\"input_ids\"] = input_ids1[:1].detach().cpu()\n\tpt_input_dict1[\"position_ids\"] = position_ids1[:1].detach()\n\tif args.data_type == \"fp16\":\n\t    dtype = torch.float16\n\telse:\n\t    dtype = torch.float32\n\toutput_dict1 = model.forward(\n\t    input_ids=input_ids1,\n\t    position_ids=position_ids1,\n\t)\n", "# save output1 logists\n\tpt_output_dict1[\"logits\"] = output_dict1[\"logits\"][:1].detach().cpu()\n\tpt_output_dict1[\"num_layers\"] = config.num_layers\n\tpast_key_values_1 = output_dict1[\"past_key_values\"]\n\tprint(\"one past_key_shape for input 1 is \", past_key_values_1[0][0].shape)\n\tprint(\"logits for input1 shape is \", output_dict1[\"logits\"].shape)\n\t# --- prepare data for input2 ---\n\t# copy from forward in second time\n\tinput_ids2 = torch.tensor([[30910]]).to(device)\n\t# copy from _update_model_kwargs_for_generation in modeling_chatglm.py\n", "new_position_id = position_ids1[..., -1:].clone()\n\tnew_position_id += 1\n\tposition_ids2 = torch.cat(\n\t    [position_ids1, new_position_id], dim=-1\n\t)\n\t# input_ids2 = torch.cat((input_ids2, input_ids2), dim=0)\n\t# position_ids2 = torch.cat((position_ids2, position_ids2), dim=0)\n\t# attention_mask2 = torch.cat((attention_mask2, attention_mask2), dim=0)\n\toutput_dict2 = model.forward(\n\t    input_ids=input_ids2,\n", "    position_ids=position_ids2,\n\t    past_key_values=past_key_values_1,\n\t)\n\tpast_key_values_2 = output_dict2[\"past_key_values\"]\n\tprint(\"one past_key_shape for input 2 is \", past_key_values_2[0][0].shape)\n\tprint(\"logits for input2 shape is \", output_dict2[\"logits\"].shape)\n\t# save input2\n\tpt_input_dict2[\"input_ids\"] = input_ids2[:1].detach().cpu()\n\tpt_input_dict2[\"position_ids\"] = position_ids2[:1].detach().cpu()\n\t# save logits2\n", "pt_output_dict2[\"logits\"] = output_dict2[\"logits\"][:1].detach().cpu()\n\tpt_output_dict2[\"num_layers\"] = config.num_layers\n\tfor layer_idx in range(model.config.num_layers):\n\t    # --- input key and value ---\n\t    past_key_name = f\"past_key_values.{layer_idx}.key\"\n\t    past_value_name = f\"past_key_values.{layer_idx}.value\"\n\t    # --- output key and value ---\n\t    present_key_name = f\"present_key_values.{layer_idx}.key\"\n\t    present_value_name = f\"present_key_values.{layer_idx}.value\"\n\t    # save output1 present_key_values \n", "    present_key = past_key_values_1[layer_idx][0][:,:1].detach().cpu()\n\t    present_value = past_key_values_1[layer_idx][1][:, :1].detach().cpu()\n\t    pt_output_dict1[present_key_name] = present_key\n\t    pt_output_dict1[present_value_name] = present_value\n\t    # save input2 past_key_values\n\t    # input2 past_key_values is same as output1 present_key_values\n\t    pt_input_dict2[past_key_name] = present_key\n\t    pt_input_dict2[past_value_name] = present_value\n\t    # save output2 present_key_values\n\t    present_key2 = past_key_values_2[layer_idx][0][:, :1].detach().cpu()\n", "    present_value2 = past_key_values_2[layer_idx][1][:, :1].detach().cpu()\n\t    pt_output_dict2[present_key_name] = present_key2\n\t    pt_output_dict2[present_value_name] = present_value2\n\t# save input1\n\tinput_container1 = torch.jit.script(Container(pt_input_dict1))\n\tinput_container1.save(pt_input_path1)\n\t# save output1\n\toutput1_container = torch.jit.script(Container(pt_output_dict1))\n\toutput1_container.save(pt_output_path1)\n\t# save input2\n", "input2_container = torch.jit.script(Container(pt_input_dict2))\n\tinput2_container.save(pt_input_path2)\n\t# save output2\n\toutput2_container = torch.jit.script(Container(pt_output_dict2))\n\toutput2_container.save(pt_output_path2)"]}
{"filename": "onnx_export/export_test_v3.py", "chunked_list": ["import os\n\t# from transformers import AutoTokenizer, AutoModel, AutoConfig\n\timport torch\n\timport sys\n\timport argparse\n\tfrom transformers.generation.utils import LogitsProcessorList\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\tfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\n", "from chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\n\tfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\n\tfrom onnx_export.utils import build_inputs\n\tfrom transformers.models.bloom import BloomOnnxConfig\n\tparser = argparse.ArgumentParser(description='export pytorch model to onnx')\n\tparser.add_argument(\n\t    '--data_type',\n\t    default=\"fp32\",\n\t    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n\t)\n", "args = parser.parse_args()\n\tif args.data_type == \"fp16\":\n\t    device = 'cuda'\n\telse:\n\t    device = 'cpu'\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tif not os.path.exists(output_dir):\n\t    os.makedirs(output_dir)\n\tonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\n\tif not os.path.exists(onnx_output_dir):\n", "    os.mkdir(onnx_output_dir)\n\tquery = \"ÊÉ≥Ë¶ÅÂá∫ÂõΩÁïôÂ≠¶ÔºåÂ∫îËØ•ÊÄé‰πàÂäûÔºü\"\n\thistory = [\n\t    (\n\t        \"‰Ω†Â•Ω\",\n\t        \"‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\",\n\t    )\n\t]\n\tmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\n\ttokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\n", "config = ChatGLMConfig.from_pretrained(model_dir)\n\t# config.num_layers = 1\n\tmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\n\tif device == \"cuda\":\n\t    model = model.half().cuda()\n\telse:\n\t    model = model.float().cpu()\n\tdevice = torch.device(device)\n\tmodel.eval()\n\t# input_tensors\n", "input_tensors = build_inputs(device, tokenizer, query, history)\n\t# --debug for chat --\n\t# response, history = model.chat(tokenizer, query, history)\n\t# print(\"res\", response)\n\tprint(\"=\" * 50)\n\tprint(\" ---forward first --- \")\n\toutputs = model.forward(\n\t    **input_tensors\n\t)\n\tprint(\" ---forward first with fake past_key_values --- \")\n", "input_ids = input_tensors[\"input_ids\"]\n\tbatch = input_ids.shape[0]\n\tpake_past_key_values = [\n\t    [\n\t        torch.zeros([0, batch, 2, 128], device=input_ids.device)\n\t        for _ in range(2)\n\t    ]\n\t    for _ in range(model.config.num_layers)\n\t]\n\toutputs2 = model.forward(\n", "    **input_tensors,\n\t    past_key_values=pake_past_key_values\n\t)\n\tdef compare_diff(outputs_1, outputs_2):\n\t    print(\"--- compare diff ---\")\n\t    max_diff = 0\n\t    logits_diff = (outputs_2[\"logits\"] - outputs_1[\"logits\"]).max().item()\n\t    if logits_diff > max_diff:\n\t        max_diff = logits_diff\n\t    print(\"logits diff is \", logits_diff)\n", "    past_key_values0 = outputs_1[\"past_key_values\"]\n\t    past_key_values1 = outputs_2[\"past_key_values\"]\n\t    for i in range(model.config.num_layers):\n\t        present_key_name = f\"present_key_values.{i}.key\"\n\t        present_value_name = f\"present_key_values.{i}.value\"\n\t        diff1 = (past_key_values0[i][0] - past_key_values1[i][0]).max().item()\n\t        diff2 = (past_key_values0[i][1] - past_key_values1[i][1]).max().item()\n\t        print(f\"{present_key_name} diff: \", diff1)\n\t        print(f\"{present_value_name} diff: \", diff2)\n\t        if diff1 > max_diff:\n", "            max_diff = diff1\n\t        if diff2 > max_diff:\n\t            max_diff = diff2\n\t    print(\"max diff is: \", max_diff)\n\tcompare_diff(outputs, outputs2)\n\tprint(\"=\" * 50)\n"]}
{"filename": "onnx_export/change_onnx2.py", "chunked_list": ["import onnx\n\timport os\n\t# Load the ONNX model\n\tmodel = onnx.load(\"onnx_output/chatglm_6b.onnx\")\n\tfor node in model.graph.node:\n\t  # if node.name in [\"If_92\", \"If_100\"]:\n\t  if node.op_type == \"If\":\n\t    # print(node)\n\t    attr_name = node.attribute[1].name\n\t    output1 = node.attribute[0].g.output[0]\n", "    output2 = node.attribute[1].g.output[0]\n\t    output_dim1 = output1.type.tensor_type.shape.dim\n\t    output_dim2 = output2.type.tensor_type.shape.dim\n\t    # print(len(output_dim1))\n\t    # print(len(output_dim2))\n\t    if len(output_dim1) != len(output_dim2):\n\t        print(\"======old node======\")\n\t        print(node)\n\t        attr_name = node.attribute[1].name\n\t        output_name = node.attribute[1].g.output[0].name\n", "        sub_graph_name = node.attribute[1].g.name\n\t        node.attribute[1].CopyFrom(node.attribute[0])\n\t        node.attribute[1].g.name = sub_graph_name\n\t        node.attribute[1].name = attr_name\n\t        node.attribute[1].g.output[0].name = output_name\n\t        node.attribute[1].g.node[-1].output[0] = output_name\n\t        node.attribute[1].g.node[0].output[0] = \"my_\" + node.attribute[1].g.node[0].output[0]\n\t        node.attribute[1].g.node[1].input[1] = \"my_\" + node.attribute[1].g.node[1].input[1]\n\t        for n in node.attribute[1].g.node:\n\t            n.name = \"my_\" + n.name\n", "        print(\"======new node======\")\n\t        print(node)\n\t# Create an external data directory to save tensors\n\texternal_data_dir = \"onnx_output2\"\n\tif not os.path.exists(external_data_dir):\n\t    os.makedirs(external_data_dir)\n\telse:\n\t    for file in os.listdir(external_data_dir):\n\t        os.remove(os.path.join(external_data_dir, file))\n\t# Save the model with external data\n", "# not support onnx memory > 2GB, to sovel it, see this url: https://github.com/onnx/onnx/issues/3275\n\tonnx.save(\n\t    model,\n\t    \"onnx_output2/chatglm_6b.onnx\",\n\t    save_as_external_data=True,\n\t    all_tensors_to_one_file=False\n\t)\n\tprint(\"onnx saved success\")"]}
{"filename": "onnx_export/utils.py", "chunked_list": ["import torch\n\tfrom typing import List, Tuple\n\tdef build_inputs(device, tokenizer, query: str,\n\t                 history: List[Tuple[str, str]] = None):\n\t    prompt = \"\"\n\t    for i, (old_query, response) in enumerate(history):\n\t        prompt += \"[Round {}]\\n\\nÈóÆÔºö{}\\n\\nÁ≠îÔºö{}\\n\\n\".format(i + 1, old_query,\n\t                                                            response)\n\t    prompt += \"[Round {}]\\n\\nÈóÆÔºö{}\\n\\nÁ≠îÔºö\".format(len(history) + 1, query)\n\t    inputs = tokenizer([prompt], return_tensors=\"pt\")\n", "    inputs = inputs.to(device)\n\t    return inputs\n"]}
{"filename": "onnx_export/run_onnx_cpu.py", "chunked_list": ["import onnxruntime as ort\n\timport torch\n\timport numpy as np\n\timport os\n\tfrom colored import fg, stylize\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\t# model_dir = os.path.join(project_dir, \"chatglm_6b\")\n\tonnx_path_with_cache = os.path.join(output_dir, \"onnx_output\", \"chatglm2_6b.onnx\")\n", "onnx_path_no_cache = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\n\tdef compare_value(pre_numpy: np.array, true_numpy: np.array):\n\t    assert pre_numpy.shape == true_numpy.shape\n\t    diff = np.abs(pre_numpy - true_numpy).max()\n\t    if diff > 5e-4:\n\t        print(stylize(f\"diff: {diff} is_pass: failed\", fg(\"red\")))\n\t    else:\n\t        print(stylize(f\"diff: {diff} is_pass: OK\", fg(\"green\")))\n\t    return diff\n\tdef run_cpu_onnx_inference(onnx_path, input_path: str, output_path):\n", "    \"\"\"\n\t    \"\"\"\n\t    # ndWtokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n\t    # providers = [(\"CUDAExecutionProvider\", {\"cudnn_conv_use_max_workspace\": '1'})]\n\t    providers = [\"CPUExecutionProvider\"]\n\t    sess_options = ort.SessionOptions()\n\t    # sess_options.optimized_model_filepath = new_onnx_path\n\t    # sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n\t    print(f\"loading onnx {onnx_path}, please wait...\")\n\t    session = ort.InferenceSession(\n", "        onnx_path, sess_options=sess_options, providers=providers\n\t    )\n\t    print(session.get_providers())\n\t    input_dict = torch.jit.load(input_path)\n\t    output_dict = torch.jit.load(output_path)\n\t    input_ids = input_dict.input_ids.data.cpu().numpy().astype(np.int64)\n\t    position_ids = input_dict.position_ids.data.cpu().numpy().astype(np.int64)\n\t    logits = output_dict.logits.data.cpu().numpy()\n\t    key = \"present_key_values.0.key\"\n\t    one_present_key = getattr(output_dict, key).data.cpu().numpy()\n", "    num_layers = getattr(output_dict, \"num_layers\")\n\t    io_binding = session.io_binding()\n\t    print(\"input number\", len(session.get_inputs()))\n\t    print(\"output number\", len(session.get_outputs()))\n\t    input_names = [_.name for _ in session.get_inputs()]\n\t    print(\"=================input names=================\")\n\t    print(input_names)\n\t    output_names = [_.name for _ in session.get_outputs()]\n\t    print(\"=================output names=================\")\n\t    print(output_names)\n", "    io_binding.bind_cpu_input(\n\t        \"input_ids\",\n\t        input_ids\n\t    )\n\t    io_binding.bind_cpu_input(\n\t        \"position_ids\",\n\t        position_ids\n\t    )\n\t    for layer_idx in range(num_layers):\n\t        input_names = [\n", "            f\"past_key_values.{layer_idx}.key\",\n\t            f\"past_key_values.{layer_idx}.value\"\n\t        ]\n\t        # inputs[input_names[0]] = past_key_values\n\t        # inputs[input_names[1]] = past_key_values\n\t        for name in input_names:\n\t            try:\n\t                past_key_values = getattr(input_dict, name).data.cpu().numpy()\n\t            except Exception:\n\t                past_key_values = np.zeros(\n", "                    [0, input_ids.shape[1], 2, 128],\n\t                    dtype=one_present_key.dtype\n\t                )\n\t            io_binding.bind_cpu_input(\n\t                name,\n\t                past_key_values\n\t            )\n\t        output_name = [\n\t            f\"present_key_values.{layer_idx}.key\",\n\t            f\"present_key_values.{layer_idx}.value\"\n", "        ]\n\t        for name in output_name:\n\t            io_binding.bind_output(\n\t                name,\n\t                device_type=\"cpu\",\n\t                device_id=0,\n\t                element_type=one_present_key.dtype,\n\t                shape=one_present_key.shape,\n\t            )\n\t    io_binding.bind_output(\n", "        name=\"logits\",\n\t        device_type=\"cpu\",\n\t        device_id=0,\n\t        element_type=logits.dtype,\n\t        shape=logits.shape,\n\t    )\n\t    # print(inputs)\n\t    session.run_with_iobinding(io_binding)\n\t    max_diff = 0\n\t    # compile logists\n", "    print('=' * 20)\n\t    print(\"compare logits\")\n\t    pred_outputs = io_binding.copy_outputs_to_cpu()\n\t    diff1 = compare_value(pred_outputs[-1], logits)\n\t    if diff1 > max_diff:\n\t        max_diff = diff1\n\t    # compile present_key_values\n\t    for i in range(num_layers):\n\t        key_name = f\"present_key_values.{i}.key\"\n\t        value_name = f\"present_key_values.{i}.value\"\n", "        print('=' * 20)\n\t        print(f\"compare {key_name}\")\n\t        # key_numpy = [key_name]\n\t        key_true = getattr(output_dict, key_name).data.cpu().numpy()\n\t        key_pred = pred_outputs[i * 2]\n\t        diff2 = compare_value(key_pred, key_true)\n\t        if diff2 > max_diff:\n\t            max_diff = diff2\n\t        print('=' * 20)\n\t        print(f\"compare {value_name}\")\n", "        value_pred = pred_outputs[i * 2 + 1]\n\t        value_true = getattr(output_dict, value_name).data.cpu().numpy()\n\t        diff3 = compare_value(value_pred, value_true)\n\t        if diff3 > max_diff:\n\t            max_diff = diff3\n\t    print('=' * 20)\n\t    print(f\"max diff: {max_diff}\")\n\tif __name__ == \"__main__\":\n\t    input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n\t    output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n", "    run_cpu_onnx_inference(onnx_path_with_cache, input_path1, output_path1)\n\t    print(\"\\n\")\n\t    input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\n\t    output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\n\t    run_cpu_onnx_inference(onnx_path_with_cache, input_path2, output_path2)"]}
{"filename": "onnx_export/export2onnx_v2_no_cache.py", "chunked_list": ["import os\n\t# from transformers import AutoTokenizer, AutoModel, AutoConfig\n\timport torch\n\timport sys\n\timport argparse\n\tfrom transformers.generation.utils import LogitsProcessorList\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\tfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\n", "from chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\n\tfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\n\tfrom onnx_export.utils import build_inputs\n\tfrom transformers.models.bloom import BloomOnnxConfig\n\tparser = argparse.ArgumentParser(description='export pytorch model to onnx')\n\tparser.add_argument(\n\t    '--data_type',\n\t    default=\"fp32\",\n\t    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n\t)\n", "args = parser.parse_args()\n\tif args.data_type == \"fp16\":\n\t    device = 'cuda'\n\telse:\n\t    device = 'cpu'\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tif not os.path.exists(output_dir):\n\t    os.makedirs(output_dir)\n\tonnx_output_dir = os.path.join(output_dir, \"onnx_output_no_cache\")\n\tif not os.path.exists(onnx_output_dir):\n", "    os.mkdir(onnx_output_dir)\n\telse:\n\t    for file in os.listdir(onnx_output_dir):\n\t        os.remove(os.path.join(onnx_output_dir, file))\n\tonnx_model_path = os.path.join(onnx_output_dir, \"chatglm2_6b.onnx\")\n\tquery = \"ÊÉ≥Ë¶ÅÂá∫ÂõΩÁïôÂ≠¶ÔºåÂ∫îËØ•ÊÄé‰πàÂäûÔºü\"\n\thistory = [\n\t    (\n\t        \"‰Ω†Â•Ω\",\n\t        \"‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\",\n", "    )\n\t]\n\tmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\n\ttokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\n\tconfig = ChatGLMConfig.from_pretrained(model_dir)\n\t# config.num_layers = 1\n\tmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\n\tif device == \"cuda\":\n\t    model = model.half().cuda()\n\telse:\n", "    model = model.float().cpu()\n\tdevice = torch.device(device)\n\tmodel.eval()\n\t# input_tensors\n\tinput_tensors = build_inputs(device, tokenizer, query, history)\n\tdel input_tensors[\"attention_mask\"]\n\t# --debug for chat --\n\t# response, history = model.chat(tokenizer, query, history)\n\t# print(\"res\", response)\n\tprint(\" ---forward first --- \")\n", "outputs = model.forward(\n\t    **input_tensors\n\t)\n\tprint(\"--- export onnx ---\")\n\t# ---prepare for onnx export ---\n\tinput_names = [\"input_ids\", 'position_ids', \"attention_mask\"]\n\toutput_names = [\"logits\"]\n\tdynamic_axes = {\n\t    'input_ids': {0: \"batch_size\", 1: \"sequence\"},\n\t    'position_ids': {0: \"batch_size\", 1: \"sequence\"},\n", "    \"logits\": {0: \"batch_size\", 1: \"sequence\"}\n\t}\n\tfor layer_idx in range(model.config.num_layers):\n\t    # --- input key and value ---\n\t    # --- output key and value ---\n\t    present_key_name = f\"present_key_values.{layer_idx}.key\"\n\t    present_value_name = f\"present_key_values.{layer_idx}.value\"\n\t    output_names += [present_key_name, present_value_name]\n\t    dynamic_axes.update({\n\t        present_key_name: {\n", "            0: \"past_sequence + 1\",\n\t            1: \"batch_size\"\n\t        },\n\t        present_value_name: {\n\t            0: \"past_sequence + 1\",\n\t            1: \"batch_size\"\n\t        }\n\t    })\n\twith torch.no_grad():\n\t    torch.onnx.export(\n", "        model,\n\t        args=(\n\t            input_tensors[\"input_ids\"],\n\t            input_tensors[\"position_ids\"],\n\t        ),\n\t        f=onnx_model_path,\n\t        opset_version=14,\n\t        input_names=input_names,\n\t        output_names=output_names,\n\t        dynamic_axes=dynamic_axes,\n", "        training=torch.onnx.TrainingMode.EVAL,\n\t    )"]}
{"filename": "onnx_export/run_onnx_trt.py", "chunked_list": ["import onnxruntime as ort\n\timport torch\n\timport numpy as np\n\timport os\n\tfrom colored import fg, stylize\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\t# model_dir = os.path.join(project_dir, \"chatglm_6b\")\n\tonnx_path = os.path.join(output_dir, \"onnx_output\", \"chatglm_6b.onnx\")\n", "new_onnx_dir = os.path.join(project_dir, \"output\", \"new_onnx_output\")\n\tif not os.path.exists(new_onnx_dir):\n\t    os.mkdir(new_onnx_dir)\n\tnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm_6b.onnx\")\n\tdef compare_value(pre_numpy: np.array, true_numpy: np.array):\n\t    assert pre_numpy.shape == true_numpy.shape\n\t    diff = np.abs(pre_numpy - true_numpy).max()\n\t    if diff > 1e-3:\n\t        print(stylize(f\"diff: {diff} is_pass: failed\", fg(\"red\")))\n\t    else:\n", "        print(stylize(f\"diff: {diff} is_pass: OK\", fg(\"green\")))\n\t    return diff\n\tdef run_cuda_onnx_inference(input_path: str, output_path):\n\t    providers = [\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\"]\n\t    sess_options = ort.SessionOptions()\n\t    sess_options.optimized_model_filepath = new_onnx_path\n\t    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n\t    session = ort.InferenceSession(\n\t        onnx_path, sess_options=sess_options, providers=providers\n\t    )\n", "    print(session.get_providers())\n\t    # cuda device id\n\t    device_id = 0\n\t    input_dict = torch.jit.load(input_path)\n\t    output_dict = torch.jit.load(output_path)\n\t    input_ids = input_dict.input_ids.data.cpu().numpy().astype(np.int64)\n\t    position_ids = input_dict.position_ids.data.cpu().numpy().astype(np.int64)\n\t    attention_mask = input_dict.attention_mask.data.cpu().numpy()\n\t    logits = output_dict.logits.data.cpu().numpy()\n\t    key = \"present_key_values.0.decorder.key\"\n", "    one_present_key = getattr(output_dict, key).data.cpu().numpy()\n\t    num_layers = getattr(output_dict, \"num_layers\")\n\t    io_binding = session.io_binding()\n\t    io_binding.bind_ortvalue_input(\n\t        \"input_ids\",\n\t        ort.OrtValue.ortvalue_from_numpy(input_ids, \"cuda\", device_id=device_id)\n\t    )\n\t    io_binding.bind_ortvalue_input(\n\t        \"position_ids\",\n\t        ort.OrtValue.ortvalue_from_numpy(position_ids, \"cuda\", device_id=device_id)\n", "    )\n\t    io_binding.bind_ortvalue_input(\n\t        \"attention_mask\",\n\t        ort.OrtValue.ortvalue_from_numpy(attention_mask, \"cuda\", device_id=device_id)\n\t    )\n\t    for layer_idx in range(num_layers):\n\t        input_names = [\n\t            f\"past_key_values.{layer_idx}.decorder.key\",\n\t            f\"past_key_values.{layer_idx}.decorder.value\"\n\t        ]\n", "        # inputs[input_names[0]] = past_key_values\n\t        # inputs[input_names[1]] = past_key_values\n\t        for name in input_names:\n\t            try:\n\t                past_key_values = getattr(input_dict, name).data.cpu().numpy()\n\t            except Exception:\n\t                past_key_values = np.zeros(\n\t                    [1, input_ids.shape[1], 32, 128],\n\t                    dtype=one_present_key.dtype\n\t                )\n", "            io_binding.bind_cpu_input(\n\t                name,\n\t                past_key_values\n\t            )\n\t            io_binding.bind_ortvalue_input(\n\t                name=name,\n\t                ortvalue=ort.OrtValue.ortvalue_from_numpy(\n\t                    past_key_values, \"cuda\", device_id=device_id\n\t                )\n\t            )\n", "        output_name = [\n\t            f\"present_key_values.{layer_idx}.decorder.key\",\n\t            f\"present_key_values.{layer_idx}.decorder.value\"\n\t        ]\n\t        for name in output_name:\n\t            output_value = np.zeros_like(\n\t                one_present_key,\n\t                dtype=one_present_key.dtype\n\t            )\n\t            io_binding.bind_ortvalue_output(\n", "                name=name,\n\t                ortvalue=ort.OrtValue.ortvalue_from_numpy(\n\t                    output_value, \"cuda\", device_id=device_id\n\t                )\n\t            )\n\t    logits_numpy = np.zeros_like(logits, dtype=logits.dtype)\n\t    io_binding.bind_ortvalue_output(\n\t        name=\"logits\",\n\t        ortvalue=ort.OrtValue.ortvalue_from_numpy(\n\t            logits_numpy\n", "        )\n\t    )\n\t    # print(inputs)\n\t    session.run_with_iobinding(io_binding)\n\t    # compile logists\n\t    print('=' * 20)\n\t    print(\"compare logits\")\n\t    pred_outputs = io_binding.copy_outputs_to_cpu()\n\t    compare_value(pred_outputs[-1], logits)\n\t    # compile present_key_values\n", "    for i in range(num_layers):\n\t        key_name = f\"present_key_values.{i}.decorder.key\"\n\t        value_name = f\"present_key_values.{i}.decorder.value\"\n\t        print('=' * 20)\n\t        print(f\"compare {key_name}\")\n\t        # key_numpy = [key_name]\n\t        key_true = getattr(output_dict, key_name).data.cpu().numpy()\n\t        key_pred = pred_outputs[i * 2]\n\t        compare_value(key_pred, key_true)\n\t        print('=' * 20)\n", "        print(f\"compare {value_name}\")\n\t        value_pred = pred_outputs[i * 2 + 1]\n\t        value_true = getattr(output_dict, value_name).data.cpu().numpy()\n\t        compare_value(value_pred, value_true)\n\tif __name__ == \"__main__\":\n\t    input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n\t    output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n\t    run_cuda_onnx_inference(input_path1, output_path1)\n\t    print(\"\\n\")\n\t    input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\n", "    output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\n\t    run_cuda_onnx_inference(input_path2, output_path2)\n"]}
{"filename": "onnx_export/export2onnx_v2.py", "chunked_list": ["import os\n\t# from transformers import AutoTokenizer, AutoModel, AutoConfig\n\timport torch\n\timport sys\n\timport argparse\n\tfrom transformers.generation.utils import LogitsProcessorList\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\tfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\n", "from chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\n\tfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\n\tfrom onnx_export.utils import build_inputs\n\tfrom transformers.models.bloom import BloomOnnxConfig\n\tparser = argparse.ArgumentParser(description='export pytorch model to onnx')\n\tparser.add_argument(\n\t    '--data_type',\n\t    default=\"fp32\",\n\t    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n\t)\n", "args = parser.parse_args()\n\tif args.data_type == \"fp16\":\n\t    device = 'cuda'\n\telse:\n\t    device = 'cpu'\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tif not os.path.exists(output_dir):\n\t    os.makedirs(output_dir)\n\tonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\n\tif not os.path.exists(onnx_output_dir):\n", "    os.mkdir(onnx_output_dir)\n\telse:\n\t    for file in os.listdir(onnx_output_dir):\n\t        os.remove(os.path.join(onnx_output_dir, file))\n\tonnx_model_path = os.path.join(onnx_output_dir, \"chatglm2_6b.onnx\")\n\tquery = \"ÊÉ≥Ë¶ÅÂá∫ÂõΩÁïôÂ≠¶ÔºåÂ∫îËØ•ÊÄé‰πàÂäûÔºü\"\n\thistory = [\n\t    (\n\t        \"‰Ω†Â•Ω\",\n\t        \"‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\",\n", "    )\n\t]\n\tmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\n\ttokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\n\tconfig = ChatGLMConfig.from_pretrained(model_dir)\n\t# config.num_layers = 1\n\tmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\n\tif device == \"cuda\":\n\t    model = model.half().cuda()\n\telse:\n", "    model = model.float().cpu()\n\tdevice = torch.device(device)\n\tmodel.eval()\n\t# input_tensors\n\tinput_tensors = build_inputs(device, tokenizer, query, history)\n\tdel input_tensors[\"attention_mask\"]\n\t# --debug for chat --\n\t# response, history = model.chat(tokenizer, query, history)\n\t# print(\"res\", response)\n\tprint(\" ---forward first --- \")\n", "outputs = model.forward(\n\t    **input_tensors\n\t)\n\tprint(\"--second forward ---\")\n\t# input_ids = input_tensors[\"input_ids\"]\n\tposition_ids = input_tensors[\"position_ids\"]\n\tpast_key_values = outputs[\"past_key_values\"]\n\t# copy from forward in second time\n\tnew_input_ids = torch.tensor([[30910]]).to(device)\n\tinput_ids = torch.cat([input_tensors[\"input_ids\"], new_input_ids], dim=-1)\n", "# copy from _update_model_kwargs_for_generation in modeling_chatglm.py\n\tnew_position_id = position_ids[..., -1:].clone()\n\tnew_position_id += 1\n\tposition_ids = torch.cat(\n\t    [position_ids, new_position_id], dim=-1\n\t)\n\t# copy from prepare_inputs_for_generation in modeling_chatglm.py\n\t# position_ids = position_ids[..., -1:]\n\t# input_ids = input_ids[..., -1:]\n\t# print shape\n", "print(\n\t    \"input_ids shape:\", input_ids.shape,\n\t    \"; type:\", input_ids.dtype\n\t)\n\tprint(\n\t    \"position_ids shape:\", position_ids.shape,\n\t    \"; type: \", input_ids.dtype\n\t)\n\tprint(\n\t    \"first forward one past_key_value shape: \", past_key_values[0][0].shape,\n", "    \"; type:\", past_key_values[0][0].dtype\n\t)\n\tprint(\"first forward logits shape: \", outputs[\"logits\"].shape)\n\toutputs2 = model.forward(\n\t    input_ids=input_ids,\n\t    position_ids=position_ids,\n\t    past_key_values=past_key_values\n\t)\n\tprint(\"--- export onnx ---\")\n\t# ---prepare for onnx export ---\n", "input_names = [\"input_ids\", 'position_ids']\n\toutput_names = [\"logits\"]\n\tdynamic_axes = {\n\t    'input_ids': {0: \"batch_size\", 1: \"sequence\"},\n\t    'position_ids': {0: \"batch_size\", 1: \"sequence\"},\n\t    \"logits\": {0: \"batch_size\", 1: \"sequence\"}\n\t}\n\tfor layer_idx in range(model.config.num_layers):\n\t    # --- input key and value ---\n\t    past_key_name = f\"past_key_values.{layer_idx}.key\"\n", "    past_value_name = f\"past_key_values.{layer_idx}.value\"\n\t    input_names += [past_key_name, past_value_name]\n\t    # --- output key and value ---\n\t    present_key_name = f\"present_key_values.{layer_idx}.key\"\n\t    present_value_name = f\"present_key_values.{layer_idx}.value\"\n\t    output_names += [present_key_name, present_value_name]\n\t    dynamic_axes.update({\n\t        past_key_name: {\n\t            0: \"past_sequence\",\n\t            1: \"batch_size\",\n", "        },\n\t        past_value_name: {\n\t            0: \"past_sequence\",\n\t            1: \"batch_size\",\n\t        },\n\t        present_key_name: {\n\t            0: \"past_sequence + sequence\",\n\t            1: \"batch_size\"\n\t        },\n\t        present_value_name: {\n", "            0: \"past_sequence + sequence\",\n\t            1: \"batch_size\"\n\t        }\n\t    })\n\twith torch.no_grad():\n\t    torch.onnx.export(\n\t        model,\n\t        args=(\n\t            input_ids,\n\t            position_ids,\n", "            past_key_values\n\t        ),\n\t        f=onnx_model_path,\n\t        opset_version=14,\n\t        input_names=input_names,\n\t        output_names=output_names,\n\t        dynamic_axes=dynamic_axes,\n\t        training=torch.onnx.TrainingMode.EVAL,\n\t    )"]}
{"filename": "onnx_export/export_test.py", "chunked_list": ["import os\n\t# from transformers import AutoTokenizer, AutoModel, AutoConfig\n\timport torch\n\timport sys\n\timport argparse\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\tfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\n\tfrom chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\n", "from chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\n\tfrom onnx_export.utils import build_inputs\n\tparser = argparse.ArgumentParser(description='export pytorch model to onnx')\n\tparser.add_argument(\n\t    '--data_type',\n\t    default=\"fp32\",\n\t    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n\t)\n\targs = parser.parse_args()\n\tif args.data_type == \"fp16\":\n", "    device = 'cuda'\n\telse:\n\t    device = 'cpu'\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tif not os.path.exists(output_dir):\n\t    os.makedirs(output_dir)\n\tonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\n\tif not os.path.exists(onnx_output_dir):\n\t    os.mkdir(onnx_output_dir)\n\tquery = \"ÊÉ≥Ë¶ÅÂá∫ÂõΩÁïôÂ≠¶ÔºåÂ∫îËØ•ÊÄé‰πàÂäûÔºü\"\n", "history = [\n\t    (\n\t        \"‰Ω†Â•Ω\",\n\t        \"‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\",\n\t    )\n\t]\n\tmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\n\ttokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\n\tconfig = ChatGLMConfig.from_pretrained(model_dir)\n\t# config.num_layers = 1\n", "model = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\n\tif device == \"cuda\":\n\t    model = model.half().cuda()\n\telse:\n\t    model = model.float().cpu()\n\tdevice = torch.device(device)\n\tmodel.eval()\n\t# input_tensors\n\tinput_tensors = build_inputs(device, tokenizer, query, history)\n\t# --debug for chat --\n", "# response, history = model.chat(tokenizer, query, history)\n\t# print(\"res\", response)\n\tprint(\"=\" * 50)\n\tprint(\" ---forward first --- \")\n\toutputs = model.forward(\n\t    **input_tensors\n\t)\n\tprint(\" ---forward first with no attention_mask --- \")\n\toutputs2 = model.forward(\n\t    input_ids=input_tensors[\"input_ids\"],\n", "    position_ids=input_tensors[\"position_ids\"],\n\t)\n\tdef compare_diff(outputs_1, outputs_2):\n\t    print(\"--- compare diff ---\")\n\t    max_diff = 0\n\t    logits_diff = (outputs_2[\"logits\"] - outputs_1[\"logits\"]).max().item()\n\t    if logits_diff > max_diff:\n\t        max_diff = logits_diff\n\t    print(\"logits diff is \", logits_diff)\n\t    past_key_values0 = outputs_1[\"past_key_values\"]\n", "    past_key_values1 = outputs_2[\"past_key_values\"]\n\t    for i in range(model.config.num_layers):\n\t        present_key_name = f\"present_key_values.{i}.key\"\n\t        present_value_name = f\"present_key_values.{i}.value\"\n\t        diff1 = (past_key_values0[i][0] - past_key_values1[i][0]).max().item()\n\t        diff2 = (past_key_values0[i][1] - past_key_values1[i][1]).max().item()\n\t        print(f\"{present_key_name} diff: \", diff1)\n\t        print(f\"{present_value_name} diff: \", diff2)\n\t        if diff1 > max_diff:\n\t            max_diff = diff1\n", "        if diff2 > max_diff:\n\t            max_diff = diff2\n\t    print(\"max diff is: \", max_diff)\n\tcompare_diff(outputs, outputs2)\n\tprint(\"=\" * 50)\n\tprint(\"=\" * 50)\n\tprint(\" ---forward second --- \")\n\tattention_mask = input_tensors[\"attention_mask\"]\n\tposition_ids = input_tensors[\"position_ids\"]\n\tpast_key_values = outputs[\"past_key_values\"]\n", "# copy from forward in second time\n\tinput_ids = torch.tensor([[30910]]).to(device)\n\tattention_mask = torch.cat(\n\t    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n\t)\n\tnew_position_id = position_ids[..., -1:].clone()\n\tnew_position_id += 1\n\tposition_ids = torch.cat(\n\t    [position_ids, new_position_id], dim=-1\n\t)\n", "# copy from prepare_inputs_for_generation in modeling_chatglm.py\n\tposition_ids = position_ids[..., -1:]\n\tpast_key_values1 = outputs[\"past_key_values\"]\n\toutputs_3 = model.forward(\n\t    input_ids=input_ids,\n\t    attention_mask=attention_mask,\n\t    position_ids=position_ids,\n\t    past_key_values=past_key_values1\n\t)\n\tprint(\" ---forward second with no attention_mask --- \")\n", "outputs_4 = model.forward(\n\t    input_ids=input_ids,\n\t    position_ids=position_ids,\n\t    past_key_values=past_key_values1\n\t)\n\tcompare_diff(outputs_3, outputs_4)\n\tprint(\"=\" * 50)\n"]}
{"filename": "onnx_export/run_onnx_cuda.py", "chunked_list": ["import onnxruntime as ort\n\timport torch\n\timport numpy as np\n\timport os\n\tfrom colored import fg, stylize\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\t# model_dir = os.path.join(project_dir, \"chatglm_6b\")\n\tonnx_path_with_cache = os.path.join(output_dir, \"onnx_output\", \"chatglm2_6b.onnx\")\n", "onnx_path_no_cache = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\n\tdef compare_value(pre_numpy: np.array, true_numpy: np.array):\n\t    assert pre_numpy.shape == true_numpy.shape\n\t    diff = np.abs(pre_numpy - true_numpy).max()\n\t    if diff > 1e-3:\n\t        print(stylize(f\"diff: {diff} is_pass: failed\", fg(\"red\")))\n\t    else:\n\t        print(stylize(f\"diff: {diff} is_pass: OK\", fg(\"green\")))\n\t    return diff\n\tdef run_cuda_onnx_inference(onnx_path, input_path: str, output_path):\n", "    providers = [(\"CUDAExecutionProvider\", {'enable_cuda_graph': False})]\n\t    sess_options = ort.SessionOptions()\n\t    session = ort.InferenceSession(\n\t        onnx_path, sess_options=sess_options, providers=providers\n\t    )\n\t    print(session.get_providers())\n\t    # cuda device id\n\t    device_id = 0\n\t    input_dict = torch.jit.load(input_path)\n\t    output_dict = torch.jit.load(output_path)\n", "    input_ids = input_dict.input_ids.data.cpu().numpy().astype(np.int64)\n\t    position_ids = input_dict.position_ids.data.cpu().numpy().astype(np.int64)\n\t    attention_mask = input_dict.attention_mask.data.cpu().numpy()\n\t    logits = output_dict.logits.data.cpu().numpy()\n\t    key = \"present_key_values.0.key\"\n\t    one_present_key = getattr(output_dict, key).data.cpu().numpy()\n\t    num_layers = getattr(output_dict, \"num_layers\")\n\t    io_binding = session.io_binding()\n\t    io_binding.bind_ortvalue_input(\n\t        \"input_ids\",\n", "        ort.OrtValue.ortvalue_from_numpy(input_ids, \"cuda\", device_id=device_id)\n\t    )\n\t    io_binding.bind_ortvalue_input(\n\t        \"position_ids\",\n\t        ort.OrtValue.ortvalue_from_numpy(position_ids, \"cuda\", device_id=device_id)\n\t    )\n\t    io_binding.bind_ortvalue_input(\n\t        \"attention_mask\",\n\t        ort.OrtValue.ortvalue_from_numpy(attention_mask, \"cuda\", device_id=device_id)\n\t    )\n", "    for layer_idx in range(num_layers):\n\t        input_names = [\n\t            f\"past_key_values.{layer_idx}.key\",\n\t            f\"past_key_values.{layer_idx}.value\"\n\t        ]\n\t        # inputs[input_names[0]] = past_key_values\n\t        # inputs[input_names[1]] = past_key_values\n\t        for name in input_names:\n\t            try:\n\t                past_key_values = getattr(input_dict, name).data.cpu().numpy()\n", "                io_binding.bind_ortvalue_input(\n\t                    name=name,\n\t                    ortvalue=ort.OrtValue.ortvalue_from_numpy(\n\t                        past_key_values, \"cuda\", device_id=device_id\n\t                    )\n\t                )\n\t            except Exception:\n\t                past_key_values = np.zeros(\n\t                    [1, input_ids.shape[1], 32, 128],\n\t                    dtype=one_present_key.dtype\n", "                )\n\t            # io_binding.bind_cpu_input(\n\t            #     name,\n\t            #     past_key_values\n\t            # )\n\t        output_name = [\n\t            f\"present_key_values.{layer_idx}.key\",\n\t            f\"present_key_values.{layer_idx}.value\"\n\t        ]\n\t        for name in output_name:\n", "            output_value = np.zeros_like(\n\t                one_present_key,\n\t                dtype=one_present_key.dtype\n\t            )\n\t            io_binding.bind_ortvalue_output(\n\t                name=name,\n\t                ortvalue=ort.OrtValue.ortvalue_from_numpy(\n\t                    output_value, \"cuda\", device_id=device_id\n\t                )\n\t            )\n", "    logits_numpy = np.zeros_like(logits, dtype=logits.dtype)\n\t    io_binding.bind_ortvalue_output(\n\t        name=\"logits\",\n\t        ortvalue=ort.OrtValue.ortvalue_from_numpy(\n\t            logits_numpy\n\t        )\n\t    )\n\t    # print(inputs)\n\t    session.run_with_iobinding(io_binding)\n\t    # compile logists\n", "    print('=' * 20)\n\t    print(\"compare logits\")\n\t    pred_outputs = io_binding.copy_outputs_to_cpu()\n\t    compare_value(pred_outputs[-1], logits)\n\t    # compile present_key_values\n\t    for i in range(num_layers):\n\t        key_name = f\"present_key_values.{i}.key\"\n\t        value_name = f\"present_key_values.{i}.value\"\n\t        print('=' * 20)\n\t        print(f\"compare {key_name}\")\n", "        # key_numpy = [key_name]\n\t        key_true = getattr(output_dict, key_name).data.cpu().numpy()\n\t        key_pred = pred_outputs[i * 2]\n\t        compare_value(key_pred, key_true)\n\t        print('=' * 20)\n\t        print(f\"compare {value_name}\")\n\t        value_pred = pred_outputs[i * 2 + 1]\n\t        value_true = getattr(output_dict, value_name).data.cpu().numpy()\n\t        compare_value(value_pred, value_true)\n\tif __name__ == \"__main__\":\n", "    input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n\t    output_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n\t    run_cuda_onnx_inference(onnx_path_with_cache, input_path1, output_path1)\n\t    print(\"\\n\")\n\t    input_path2 = os.path.join(output_dir, \"pt_input2.pt\")\n\t    output_path2 = os.path.join(output_dir, \"pt_output2.pt\")\n\t    run_cuda_onnx_inference(onnx_path_no_cache, input_path2, output_path2)\n"]}
{"filename": "onnx_export/export2onnx.py", "chunked_list": ["import os\n\t# from transformers import AutoTokenizer, AutoModel, AutoConfig\n\timport torch\n\timport sys\n\timport argparse\n\tfrom transformers.generation.utils import LogitsProcessorList\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\tfrom chatglm2_6b.configuration_chatglm import ChatGLMConfig\n", "from chatglm2_6b.modeling_chatglm import ChatGLMForConditionalGeneration\n\tfrom chatglm2_6b.tokenization_chatglm import ChatGLMTokenizer\n\tfrom onnx_export.utils import build_inputs\n\tfrom transformers.models.bloom import BloomOnnxConfig\n\tparser = argparse.ArgumentParser(description='export pytorch model to onnx')\n\tparser.add_argument(\n\t    '--data_type',\n\t    default=\"fp32\",\n\t    help='use fp16/fp32 to export onnx model. if use fp16, you need GPU memory > 24G, defualt is fp32'\n\t)\n", "args = parser.parse_args()\n\tif args.data_type == \"fp16\":\n\t    device = 'cuda'\n\telse:\n\t    device = 'cpu'\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tif not os.path.exists(output_dir):\n\t    os.makedirs(output_dir)\n\tonnx_output_dir = os.path.join(output_dir, \"onnx_output\")\n\tif not os.path.exists(onnx_output_dir):\n", "    os.mkdir(onnx_output_dir)\n\telse:\n\t    for file in os.listdir(onnx_output_dir):\n\t        os.remove(os.path.join(onnx_output_dir, file))\n\tonnx_model_path = os.path.join(onnx_output_dir, \"chatglm2_6b.onnx\")\n\tquery = \"ÊÉ≥Ë¶ÅÂá∫ÂõΩÁïôÂ≠¶ÔºåÂ∫îËØ•ÊÄé‰πàÂäûÔºü\"\n\thistory = [\n\t    (\n\t        \"‰Ω†Â•Ω\",\n\t        \"‰Ω†Â•Ωüëã!ÊàëÊòØ‰∫∫Â∑•Êô∫ËÉΩÂä©Êâã ChatGLM2-6B,ÂæàÈ´òÂÖ¥ËßÅÂà∞‰Ω†,Ê¨¢ËøéÈóÆÊàë‰ªª‰ΩïÈóÆÈ¢ò„ÄÇ\",\n", "    )\n\t]\n\tmodel_dir = os.path.join(project_dir, \"chatglm2_6b\")\n\ttokenizer = ChatGLMTokenizer.from_pretrained(model_dir)\n\tconfig = ChatGLMConfig.from_pretrained(model_dir)\n\t# config.num_layers = 1\n\tmodel = ChatGLMForConditionalGeneration.from_pretrained(model_dir, config=config)\n\tif device == \"cuda\":\n\t    model = model.half().cuda()\n\telse:\n", "    model = model.float().cpu()\n\tdevice = torch.device(device)\n\tmodel.eval()\n\t# input_tensors\n\tinput_tensors = build_inputs(device, tokenizer, query, history)\n\t# --debug for chat --\n\t# response, history = model.chat(tokenizer, query, history)\n\t# print(\"res\", response)\n\tprint(\" ---forward first --- \")\n\toutputs = model.forward(\n", "    **input_tensors\n\t)\n\tprint(\"--second forward ---\")\n\t# input_ids = input_tensors[\"input_ids\"]\n\tattention_mask = input_tensors[\"attention_mask\"]\n\tposition_ids = input_tensors[\"position_ids\"]\n\tpast_key_values = outputs[\"past_key_values\"]\n\t# copy from forward in second time\n\tinput_ids = torch.tensor([[30910]]).to(device)\n\t# copy from _update_model_kwargs_for_generation in modeling_chatglm.py\n", "attention_mask = torch.cat(\n\t    [attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1\n\t)\n\tnew_position_id = position_ids[..., -1:].clone()\n\tnew_position_id += 1\n\tposition_ids = torch.cat(\n\t    [position_ids, new_position_id], dim=-1\n\t)\n\t# copy from prepare_inputs_for_generation in modeling_chatglm.py\n\tposition_ids = position_ids[..., -1:]\n", "# print shape\n\tprint(\n\t    \"input_ids shape:\", input_ids.shape,\n\t    \"; type:\", input_ids.dtype\n\t)\n\tprint(\n\t    \"position_ids shape:\", position_ids.shape,\n\t    \"; type: \", input_ids.dtype\n\t)\n\tprint(\n", "    \"attention_mask shape:\", attention_mask.shape,\n\t    \"; type: \", attention_mask.dtype\n\t)\n\tprint(\n\t    \"one past_key_value shape: \", past_key_values[0][0].shape,\n\t    \"; type:\", past_key_values[0][0].dtype\n\t)\n\tprint(\"logits shape: \", outputs[\"logits\"].shape)\n\toutputs2 = model.forward(\n\t    input_ids=input_ids,\n", "    attention_mask=attention_mask,\n\t    position_ids=position_ids,\n\t    past_key_values=past_key_values\n\t)\n\tprint(\"--- export onnx ---\")\n\t# ---prepare for onnx export ---\n\tinput_names = [\"input_ids\", 'position_ids', \"attention_mask\"]\n\toutput_names = [\"logits\"]\n\tdynamic_axes = {\n\t    'input_ids': {0: \"batch_size\", 1: \"sequence\"},\n", "    'position_ids': {0: \"batch_size\", 1: \"sequence\"},\n\t    \"attention_mask\": {0: \"batch_size\", 1: \"past_sequence + sequence\"},\n\t    \"logits\": {0: \"batch_size\", 1: \"sequence\"}\n\t}\n\tfor layer_idx in range(model.config.num_layers):\n\t    # --- input key and value ---\n\t    past_key_name = f\"past_key_values.{layer_idx}.key\"\n\t    past_value_name = f\"past_key_values.{layer_idx}.value\"\n\t    input_names += [past_key_name, past_value_name]\n\t    # --- output key and value ---\n", "    present_key_name = f\"present_key_values.{layer_idx}.key\"\n\t    present_value_name = f\"present_key_values.{layer_idx}.value\"\n\t    output_names += [present_key_name, present_value_name]\n\t    dynamic_axes.update({\n\t        past_key_name: {\n\t            0: \"past_sequence\",\n\t            1: \"batch_size\",\n\t        },\n\t        past_value_name: {\n\t            0: \"past_sequence\",\n", "            1: \"batch_size\",\n\t        },\n\t        present_key_name: {\n\t            0: \"past_sequence + 1\",\n\t            1: \"batch_size\"\n\t        },\n\t        present_value_name: {\n\t            0: \"past_sequence + 1\",\n\t            1: \"batch_size\"\n\t        }\n", "    })\n\twith torch.no_grad():\n\t    torch.onnx.export(\n\t        model,\n\t        args=(\n\t            input_ids,\n\t            position_ids,\n\t            attention_mask, \n\t            past_key_values\n\t        ),\n", "        f=onnx_model_path,\n\t        opset_version=14,\n\t        input_names=input_names,\n\t        output_names=output_names,\n\t        dynamic_axes=dynamic_axes,\n\t        training=torch.onnx.TrainingMode.EVAL,\n\t    )"]}
{"filename": "kernel/ckernel.py", "chunked_list": ["import os\n\timport torch\n\timport pybind11\n\tfrom torch.utils import cpp_extension\n\tfrom torch.utils.cpp_extension import load\n\tinput_dirs = cpp_extension.include_paths()\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tcustom_include_dir = os.path.join(project_dir, \"include\")\n\tcuda_include_dir = \"/usr/local/cuda/include\"\n", "input_dirs += [custom_include_dir, cuda_include_dir]\n\ttorch_dir = os.path.dirname(torch.__path__[0])\n\ttorch_lib_dir = os.path.join(torch_dir, \"lib\")\n\tlibrary_dirs = [\"/usr/local/cuda/lib64\", torch_lib_dir]\n\tlibraries = [\"nvinfer\"]\n\ttorch_lib_dir = os.path.join(torch.__path__[0], \"lib\")\n\tlibrary_dirs.append(torch_lib_dir)\n\tlibraries.extend([\"torch\", \"torch_cuda\", \"torch_cpu\", \"c10\", \"cudart\", \"c10_cuda\"])\n\tlibrary_dirs.append(pybind11.get_include(False))\n\textra_link_args = [\n", "    \"-std=c++17\",\n\t    \"-L/usr/local/cuda/lib64\",\n\t    f\"-L{torch_lib_dir}\",\n\t  ]\n\textra_cuda_cflags = [\n\t    \"-std=c++17\",\n\t    \"-L/usr/local/cuda/lib64\",\n\t    f\"-L{torch_lib_dir}\",\n\t    \"-lnvinfer\"\n\t]\n", "sources=['kernel.cpp', \"bind.cpp\"]\n\tsources = [os.path.join(now_dir, s) for s in sources]\n\tckernel = load(\n\t  name='ckernel',\n\t  sources=sources,\n\t  extra_include_paths=input_dirs,\n\t  extra_cflags=extra_link_args,\n\t  extra_cuda_cflags=extra_cuda_cflags,\n\t  extra_ldflags=extra_cuda_cflags,\n\t  with_cuda=True,\n", "  verbose=False\n\t)\n\t# print(ckernel)\n\t# print(ckernel.Kernel)\n\t# print(ckernel.Kernel.forward)\n"]}
{"filename": "kernel/logits_processor.py", "chunked_list": ["import inspect\n\timport torch\n\tclass LogitsProcessorList(list):\n\t    def __call__(self, input_ids, scores, **kwargs):\n\t        for processor in self:\n\t            function_args = inspect.signature(processor.__call__).parameters\n\t            if len(function_args) > 2:\n\t                if not all(arg in kwargs for arg in list(function_args.keys())[2:]):\n\t                    raise ValueError(\n\t                        f\"Make sure that all the required parameters: {list(function_args.keys())} for \"\n", "                        f\"{processor.__class__} are passed to the logits processor.\"\n\t                    )\n\t                scores = processor(input_ids, scores, **kwargs)\n\t            else:\n\t                scores = processor(input_ids, scores)\n\t        return scores\n\tclass LogitsWarper:\n\t    def __call__(self, input_ids, scores):\n\t        raise NotImplementedError(\n\t            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n", "        )\n\tclass TemperatureLogitsWarper(LogitsWarper):\n\t    def __init__(self, temperature: float):\n\t        if not isinstance(temperature, float) or not (temperature > 0):\n\t            raise ValueError(f\"`temperature` has to be a strictly positive float, but is {temperature}\")\n\t        self.temperature = temperature\n\t    def __call__(self, input_ids: torch.Tensor, scores: torch.Tensor) -> torch.FloatTensor:\n\t        scores = scores / self.temperature\n\t        return scores\n\tclass TopPLogitsWarper(LogitsWarper):\n", "    def __init__(self, top_p: float, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n\t        top_p = float(top_p)\n\t        if top_p < 0 or top_p > 1.0:\n\t            raise ValueError(f\"`top_p` has to be a float > 0 and < 1, but is {top_p}\")\n\t        self.top_p = top_p\n\t        self.filter_value = filter_value\n\t        self.min_tokens_to_keep = min_tokens_to_keep\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n\t        sorted_logits, sorted_indices = torch.sort(scores, descending=False)\n\t        cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n", "        # Remove tokens with cumulative top_p above the threshold (token with 0 are kept)\n\t        sorted_indices_to_remove = cumulative_probs <= (1 - self.top_p)\n\t        if self.min_tokens_to_keep > 1:\n\t            # Keep at least min_tokens_to_keep\n\t            sorted_indices_to_remove[..., -self.min_tokens_to_keep :] = 0\n\t        # scatter sorted tensors to original indexing\n\t        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n\t        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n\t        return scores\n\tclass TopKLogitsWarper(LogitsWarper):\n", "    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n\t        if not isinstance(top_k, int) or top_k <= 0:\n\t            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n\t        self.top_k = max(top_k, min_tokens_to_keep)\n\t        self.filter_value = filter_value\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n\t        top_k = min(self.top_k, scores.size(-1))  # Safety check\n\t        # Remove all tokens with a probability less than the last token of the top-k\n\t        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n\t        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n", "        return scores\n\tclass LogitsProcessor:\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n\t        \"\"\"Torch method for processing logits.\"\"\"\n\t        raise NotImplementedError(\n\t            f\"{self.__class__} is an abstract class. Only classes inheriting this class can be called.\"\n\t        )\n\tclass InvalidScoreLogitsProcessor(LogitsProcessor):\n\t    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n\t        if torch.isnan(scores).any() or torch.isinf(scores).any():\n", "            scores.zero_()\n\t            scores[..., 5] = 5e4\n\t        return scores\n"]}
{"filename": "kernel/setup.py", "chunked_list": ["import os\n\timport torch\n\timport pybind11\n\tfrom setuptools import setup, Extension\n\tfrom torch.utils import cpp_extension\n\tfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\tinput_dirs = cpp_extension.include_paths()\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tcustom_include_dir = os.path.join(project_dir, \"include\")\n", "cuda_include_dir = \"/usr/local/cuda/include\"\n\tinput_dirs += [custom_include_dir, cuda_include_dir]\n\ttorch_dir = os.path.dirname(torch.__path__[0])\n\ttorch_lib_dir = os.path.join(torch_dir, \"lib\")\n\tlibrary_dirs = [\"/usr/local/cuda/lib64\", torch_lib_dir]\n\tlibraries = [\"nvinfer\"]\n\ttorch_lib_dir = os.path.join(torch.__path__[0], \"lib\")\n\tlibrary_dirs.append(torch_lib_dir)\n\tlibraries.extend([\"torch\", \"torch_cuda\", \"torch_cpu\", \"c10\", \"cudart\", \"c10_cuda\"])\n\tlibrary_dirs.append(pybind11.get_include(False))\n", "extra_link_args = [\"-D_GLIBCXX_USE_CXX11_ABI=1\", \"-std=c++17\"]\n\t# extra_link_args = [\"-D_GLIBCXX_USE_CXX11_ABI=0\"]\n\t#   \"-L/usr/local/cuda/lib64\", \"-lnvinfer\", f\"-L{torch_lib_dir}\"]\n\tsetup(name='ckernel',\n\t      version=\"0.0.1\",\n\t      ext_modules=[\n\t        CUDAExtension(\n\t          name='ckernel',\n\t          sources=['kernel.cpp', \"bind.cpp\"],\n\t          include_dirs=input_dirs,\n", "          language=\"c++\",\n\t          extra_link_args=extra_link_args,\n\t          libraries=libraries,\n\t          library_dirs=library_dirs\n\t        )\n\t      ],\n\t      cmdclass={'kernel': BuildExtension}\n\t    )"]}
{"filename": "kernel/pykernel_no_past_old.py", "chunked_list": ["import os\n\timport sys\n\timport os.path\n\timport torch\n\timport tensorrt as trt\n\tfrom colored import stylize, fg\n\tfrom typing import List, Tuple\n\timport pycuda.driver as cuda\n\timport pycuda.autoinit\n\tclass MyLogger(trt.ILogger):\n", "    def __init__(self):\n\t        trt.ILogger.__init__(self)\n\t    def log(self, severity, msg):\n\t        if severity == trt.ILogger.ERROR:\n\t            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.WARNING:\n\t            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # ÈªÑËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INTERNAL_ERROR:\n\t            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INFO:\n", "            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # ÁªøËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.VERBOSE:\n\t            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # ËìùËâ≤Â≠ó‰Ωì\n\t        else:\n\t            print(\"[UNKNOWN] \" + msg)\n\tclass KernelNoPast:\n\t    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n\t        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n\t        self.batch_size_ = batch_size\n\t        self.n_input_ = 2\n", "        self.n_output_ = num_layers * 2 + 1\n\t        self.n_total_ = self.n_input_ + self.n_output_\n\t        self.tensor_names_ = []\n\t        # self.logger_ = trt.Logger(trt.Logger.INFO)\n\t        self.logger_ = MyLogger()\n\t        self.runtime_ = trt.Runtime(self.logger_)\n\t        # load engine\n\t        with open(engine_path, \"rb\") as f:\n\t            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n\t        # verify io number\n", "        self.verify_io_number()\n\t        # init stream and context\n\t        self.stream_ = cuda.Stream()\n\t        self.context_ = self.engine_.create_execution_context()\n\t        print(stylize(\"init context and stream OK\", fg(\"green\")))\n\t        # self.context_.set_optimization_profile_async(0, self.stream_.handle)\n\t    #def __del__(self):\n\t    #    pass\n\t    def verify_io_number(self):\n\t        n_io = self.engine_.num_io_tensors\n", "        if n_io != self.n_total_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of IO tensors is not correct, \" +\n\t                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n\t                fg(\"red\")))\n\t        n_input = 0\n\t        n_output = 0\n\t        for i in range(n_io):\n\t            name = self.engine_.get_tensor_name(i)\n\t            self.tensor_names_.append(name)\n", "            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n\t                n_input += 1\n\t            else:\n\t                n_output += 1\n\t        if n_input != self.n_input_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of input tensors is not correct, \" +\n\t                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n\t                fg(\"red\")))\n\t        if n_output != self.n_output_:\n", "            raise RuntimeError(stylize(\n\t                \"Number of output tensors is not correct, \" +\n\t                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n\t                fg(\"red\")))\n\t        n_profile = self.engine_.num_optimization_profiles\n\t        if n_profile != 1:\n\t            raise RuntimeError(stylize(\n\t                \"Number of optimization profiles is not correct, \" +\n\t                f\"must be 1, but you have {n_profile} profiles\",\n\t                fg(\"red\")))\n", "        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\t    def set_input_shape(self, seq_len: int):\n\t        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n\t        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n\t    def get_tensor_size(self):\n\t        shape_list = []\n\t        data_type_list = []\n\t        for i in range(self.n_total_):\n\t            tensor_name = self.tensor_names_[i]\n\t            shape = self.context_.get_tensor_shape(tensor_name)\n", "            shape_list.append(shape)\n\t            data_type = self.engine_.get_tensor_dtype(tensor_name)\n\t            data_type_list.append(data_type)\n\t        return shape_list, data_type_list\n\t    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n\t        assert len(input_tensors) == 2, \\\n\t            print(\"this number of input tensor must be 2\")\n\t        seq_len = input_tensors[0].size(1)\n\t        device = input_tensors[0].device\n\t        self.set_input_shape(seq_len)\n", "        shape_list, data_type_list = self.get_tensor_size()\n\t        # --- prepare for output --- #\n\t        output_tensors = []\n\t        for i in range(self.n_input_, self.n_total_):\n\t            if data_type_list[i] == trt.DataType.FLOAT:\n\t                torch_type = torch.float32\n\t            elif data_type_list[i] == trt.DataType.HALF:\n\t                torch_type = torch.float16\n\t            elif data_type_list[i] == trt.DataType.INT32:\n\t                torch_type = torch.int32\n", "            elif data_type_list[i] == trt.DataType.INT8:\n\t                torch_type = torch.int8\n\t            else:\n\t                raise Exception(f\"not support type {data_type_list[i]}\")\n\t            tensor = torch.empty(\n\t                size=tuple(shape_list[i]), dtype=torch_type, device=device\n\t            )\n\t            output_tensors.append(tensor)\n\t        # === run inference with V3 ===\n\t        for i in range(self.n_input_):\n", "            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                input_tensors[i].data_ptr()\n\t            )\n\t        for i in range(self.n_input_, self.n_total_):\n\t            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                output_tensors[i - self.n_input_].data_ptr()\n\t            )\n\t        self.context_.execute_async_v3(stream_handle=self.stream_.handle)\n", "        # cuda.synchronize()\n\t        # cuda.streams.synchronize(self.stream_)\n\t        return output_tensors\n\tif __name__ == \"__main__\":\n\t    now_dir = os.path.dirname(os.path.abspath(__file__))\n\t    project_dir = os.path.dirname(now_dir)\n\t    model_dir = os.path.join(project_dir, \"models\")\n\t    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_no_cache.plan\")\n\t    kernel = KernelNoPast(engine_path1)\n\t    input_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n", "    position_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n\t    input_tensors1 = (input_ids, position_ids)\n\t    output_tensors1 = kernel.forward(input_tensors1)\n\t    print(\"first shape\", output_tensors1[0].shape)\n\t    print(\"last shape\", output_tensors1[-1].shape)\n"]}
{"filename": "kernel/pykernel_no_past_new.py", "chunked_list": ["import os\n\timport sys\n\timport os.path\n\timport torch\n\timport tensorrt as trt\n\tfrom colored import stylize, fg\n\tfrom typing import List, Tuple\n\tfrom cuda import cudart\n\tclass MyLogger(trt.ILogger):\n\t    def __init__(self):\n", "        trt.ILogger.__init__(self)\n\t    def log(self, severity, msg):\n\t        if severity == trt.ILogger.ERROR:\n\t            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.WARNING:\n\t            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # ÈªÑËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INTERNAL_ERROR:\n\t            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INFO:\n\t            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # ÁªøËâ≤Â≠ó‰Ωì\n", "        elif severity == trt.ILogger.VERBOSE:\n\t            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # ËìùËâ≤Â≠ó‰Ωì\n\t        else:\n\t            print(\"[UNKNOWN] \" + msg)\n\tdef gpu_check(error: cudart.cudaError_t):\n\t    if error != cudart.cudaError_t.cudaSuccess:\n\t        error_name = cudart.cudaGetErrorName(error)\n\t        error_info = cudart.cudaGetErrorString(error)\n\t        print(stylize(f\"ERROR [{error_name}]: {error_info}\", fg(\"red\")))\n\t        raise Exception(f\"ERROR [{error_name}]: {error_info}\")\n", "class KernelNoPast:\n\t    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n\t        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n\t        self.batch_size_ = batch_size\n\t        self.n_input_ = 2 + num_layers * 2\n\t        self.n_output_ = num_layers * 2 + 1\n\t        self.n_total_ = self.n_input_ + self.n_output_\n\t        self.tensor_names_ = []\n\t        # self.logger_ = trt.Logger(trt.Logger.INFO)\n\t        self.logger_ = MyLogger()\n", "        self.runtime_ = trt.Runtime(self.logger_)\n\t        # load engine\n\t        with open(engine_path, \"rb\") as f:\n\t            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n\t        # verify io number\n\t        self.verify_io_number()\n\t        # init stream and context\n\t        error_log, self.stream_ = cudart.cudaStreamCreate()\n\t        gpu_check(error_log)\n\t        self.context_ = self.engine_.create_execution_context()\n", "        print(stylize(\"init context and stream OK\", fg(\"green\")))\n\t        self.context_.set_optimization_profile_async(0, self.stream_)\n\t    def __del__(self):\n\t        cudart.cudaStreamDestroy(self.stream_)\n\t    def verify_io_number(self):\n\t        n_io = self.engine_.num_io_tensors\n\t        if n_io != self.n_total_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of IO tensors is not correct, \" +\n\t                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n", "                fg(\"red\")))\n\t        n_input = 0\n\t        n_output = 0\n\t        for i in range(n_io):\n\t            name = self.engine_.get_tensor_name(i)\n\t            self.tensor_names_.append(name)\n\t            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n\t                n_input += 1\n\t            else:\n\t                n_output += 1\n", "        if n_input != self.n_input_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of input tensors is not correct, \" +\n\t                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n\t                fg(\"red\")))\n\t        if n_output != self.n_output_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of output tensors is not correct, \" +\n\t                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n\t                fg(\"red\")))\n", "        n_profile = self.engine_.num_optimization_profiles\n\t        if n_profile != 1:\n\t            raise RuntimeError(stylize(\n\t                \"Number of optimization profiles is not correct, \" +\n\t                f\"must be 1, but you have {n_profile} profiles\",\n\t                fg(\"red\")))\n\t        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\t    def set_input_shape(self, seq_len: int):\n\t        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n\t        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n", "        for i in range(2, self.n_input_):\n\t            self.context_.set_input_shape(\n\t                self.tensor_names_[i],\n\t                (0, self.batch_size_, 2, 128)\n\t            )\n\t    def get_tensor_size(self):\n\t        shape_list = []\n\t        data_type_list = []\n\t        for i in range(self.n_total_):\n\t            tensor_name = self.tensor_names_[i]\n", "            shape = self.context_.get_tensor_shape(tensor_name)\n\t            shape_list.append(shape)\n\t            data_type = self.engine_.get_tensor_dtype(tensor_name)\n\t            data_type_list.append(data_type)\n\t        return shape_list, data_type_list\n\t    @staticmethod \n\t    def get_data_type(raw_data_type: trt.DataType):\n\t        if raw_data_type == trt.DataType.FLOAT:\n\t            torch_type = torch.float32\n\t        elif raw_data_type == trt.DataType.HALF:\n", "            torch_type = torch.float16\n\t        elif raw_data_type == trt.DataType.INT32:\n\t            torch_type = torch.int32\n\t        elif raw_data_type == trt.DataType.INT8:\n\t            torch_type = torch.int8\n\t        else:\n\t            raise Exception(f\"not support type {raw_data_type}\")\n\t        return torch_type\n\t    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n\t        seq_len = input_tensors[0].size(1)\n", "        device = input_tensors[0].device\n\t        self.set_input_shape(seq_len)\n\t        shape_list, data_type_list = self.get_tensor_size()\n\t        # --- prepare for output --- #\n\t        output_tensors = []\n\t        for i in range(self.n_input_, self.n_total_):\n\t            torch_type = self.get_data_type(data_type_list[i]) \n\t            tensor = torch.zeros(\n\t                size=tuple(shape_list[i]), dtype=torch_type, device=device\n\t            )\n", "            output_tensors.append(tensor)\n\t        # === run inference with V3 ===\n\t        for i in range(2):\n\t            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                input_tensors[i].data_ptr()\n\t            )\n\t        torch_type = self.get_data_type(data_type_list[2])\n\t        sample_tensor = torch.zeros([1], dtype=torch_type, device=device)\n\t        for i in range(2, self.n_input_):\n", "            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                sample_tensor.data_ptr()\n\t            )\n\t        for i in range(self.n_input_, self.n_total_):\n\t            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                output_tensors[i - self.n_input_].data_ptr()\n\t            )\n\t        self.context_.execute_async_v3(stream_handle=self.stream_)\n", "        (error_info,) = cudart.cudaDeviceSynchronize()\n\t        gpu_check(error_info)\n\t        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n\t        gpu_check(error_info)\n\t        return output_tensors\n\tif __name__ == \"__main__\":\n\t    now_dir = os.path.dirname(os.path.abspath(__file__))\n\t    project_dir = os.path.dirname(now_dir)\n\t    model_dir = os.path.join(project_dir, \"models\")\n\t    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n", "    kernel = KernelNoPast(engine_path1)\n\t    input_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n\t    position_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n\t    input_tensors1 = (input_ids, position_ids)\n\t    output_tensors1 = kernel.forward(input_tensors1)\n\t    print(\"first shape\", output_tensors1[0].shape)\n\t    print(\"last shape\", output_tensors1[-1].shape)\n"]}
{"filename": "kernel/__init__.py", "chunked_list": ["# from .ckernel import Kernel\n\t# from .logits_processor import *\n\t# from .ckernel import ckernel"]}
{"filename": "kernel/pykernel.py", "chunked_list": ["import tensorrt as trt\n\timport pycuda.driver as cuda\n\timport pycuda.autoinit\n\timport torch\n\timport torch.nn as nn\n\tfrom typing import Tuple, List, Optional\n\timport re\n\tfrom .logits_processor import *\n\tclass Kernel(nn.Module):\n\t    def __init__(self, engine_path: str, batch_size: int):\n", "        self.logger_ = trt.Logger(trt.Logger.INFO)\n\t        self.runtime_ = trt.Runtime(self.logger_)\n\t        self.engine_ = None\n\t        self.context_1_ = None\n\t        self.context_2_ = None\n\t        self.stream_1_ = cuda.Stream()\n\t        self.stream_2_ = cuda.Stream()\n\t        self.batch_size_ = batch_size\n\t        self.n_total_ = 116\n\t        self.n_input_ = 59\n", "        self.n_output_ = 57\n\t        self.tensor_names_ = []\n\t        self.num_layers_ = 28\n\t        self.logits_processor = None\n\t        self.logits_warper = None\n\t        self.load_engine(engine_path)\n\t    def __del__(self):\n\t        pass\n\t    def load_engine(self, engine_path: str):\n\t        # Load TensorRT Engine\n", "        with open(engine_path, 'rb') as f:\n\t            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n\t        self.context_1_ = self.engine.create_execution_context()\n\t        self.context_1_.set_optimization_profile_async(0, self.stream_1_.handle)\n\t        self.context_2_ = self.engine.create_execution_context()\n\t        self.context_2_.set_optimization_profile_async(1, self.stream_2_.handle)\n\t    def vertify_io_number(self):\n\t        pass\n\t    def init_execute_context(self):\n\t        pass\n", "    def forward(self,\n\t                input_ids: Optional[torch.Tensor] = None,\n\t                position_ids: Optional[torch.Tensor] = None,\n\t                attention_mask: Optional[torch.Tensor] = None,\n\t                past_key_values: Optional[Tuple[torch.FloatTensor]] = None):\n\t        if past_key_values is None:\n\t            outputs = self.inference_step_1(input_ids, attention_mask, position_ids)\n\t            return outputs\n\t        else:\n\t            outputs = self.inference_step_x(input_ids, input_ids, attention_mask, position_ids, past_key_values)\n", "            return outputs\n\t    def set_input_for_context1(self, seq_length):\n\t        pass\n\t    def set_input_for_context2(self, past_seq_length):\n\t        pass\n\t    def run_gpu_inference(\n\t            self,\n\t            input_ids,\n\t            position_ids,\n\t            attention_mask,\n", "            past_key_values,\n\t            bytes_list,\n\t            type_bytes_list,\n\t            context,\n\t            stream\n\t    ):\n\t        pass\n\t    def chat(\n\t            self,\n\t            tokenizer,\n", "            query: str,\n\t            history: List[Tuple[str, str]] = None,\n\t            max_length: int = 2048,\n\t            max_new_tokens: int = 40,\n\t            num_beams=1,\n\t            do_sample=True,\n\t            top_p=0.7,\n\t            top_k=50,\n\t            temperature=1.0,\n\t            **kwargs\n", "    ):\n\t        # ÂàùÂßãÂåñ history\n\t        if history is None:\n\t            history = []\n\t        # ÂàùÂßãÂåñÂêéÂ§ÑÁêÜ\n\t        self.logits_processor = LogitsProcessorList()\n\t        self.logits_processor.append(InvalidScoreLogitsProcessor())\n\t        self.logits_warper = LogitsProcessorList()\n\t        self.logits_warper.append(TemperatureLogitsWarper(temperature))\n\t        self.logits_warper.append(TopPLogitsWarper(top_p))\n", "        self.logits_warper.append(TopKLogitsWarper(top_k))\n\t        # ÁªÑË£Öprompt\n\t        if not history:\n\t            prompt = query\n\t        else:\n\t            prompt = \"\"\n\t            for i, (old_query, response) in enumerate(history):\n\t                prompt += \"[Round {}]\\nÈóÆÔºö{}\\nÁ≠îÔºö{}\\n\".format(i, old_query, response)\n\t            prompt += \"[Round {}]\\nÈóÆÔºö{}\\nÁ≠îÔºö\".format(len(history), query)\n\t        # Á¨¨‰∏ÄÊ¨°Êé®ÁêÜ\n", "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").cuda().to(torch.int32)\n\t        ori_len = len(input_ids[0])\n\t        attention_mask, position_ids = self.pre_processing_step_1(tokenizer, input_ids)\n\t        outputs_1 = self.inference_step_1(input_ids, attention_mask, position_ids)\n\t        ori_input_ids, input_ids, attention_mask, position_ids, past_key_values = self.post_processing_step_1(\n\t            outputs_1, input_ids, attention_mask, position_ids)\n\t        # ÈáçÂ§çÊé®ÁêÜÁõ¥Âà∞Êù°‰ª∂ÁªàÊ≠¢\n\t        while len(ori_input_ids[0]) < max_length \\\n\t                and len(ori_input_ids[0]) - ori_len < max_new_tokens \\\n\t                and tokenizer.eos_token_id not in ori_input_ids[0]:\n", "            outputs_x = self.inference_step_x(ori_input_ids, input_ids, attention_mask, position_ids, past_key_values)\n\t            ori_input_ids, input_ids, attention_mask, position_ids, past_key_values = self.post_processing_step_x(\n\t                outputs_x, ori_input_ids, input_ids, attention_mask, position_ids)\n\t            # print(tokenizer.decode(ori_input_ids[0]))\n\t        # Â§ÑÁêÜÂõûÁ≠î\n\t        response = tokenizer.decode(ori_input_ids[0][ori_len:])\n\t        response = self.process_response(response)\n\t        history = history + [(query, response)]\n\t        return response, history\n\t    def pre_processing_step_1(self, tokenizer, input_ids: torch.Tensor):\n", "        BOS = tokenizer.bos_token_id\n\t        MASK = tokenizer.mask_token_id\n\t        gMASK = tokenizer.gmask_token_id\n\t        batch_size, seq_length = input_ids.shape\n\t        # ËæìÂÖ•Âº†ÈáèÊâ©Â±ïÂ∏∏Èáè\n\t        input_range = torch.arange(seq_length, dtype=torch.int32).repeat((batch_size, 1)).to(input_ids.device)\n\t        input_upper = torch.tril(torch.ones((batch_size, seq_length, seq_length), dtype=torch.int32)).to(\n\t            input_ids.device)\n\t        # Ëé∑Âèñ attention_mask\n\t        context_lengths = torch.argmax((input_ids == BOS).to(torch.int32), dim=1)\n", "        context_mask = (input_range + 1) <= context_lengths.unsqueeze(1)\n\t        padding_mask = context_mask.unsqueeze(1)\n\t        attention_mask = torch.logical_not(torch.logical_or(input_upper, padding_mask)).unsqueeze(1)\n\t        # Âà§Êñ≠MASK‰ΩçÁΩÆ\n\t        is_gmasks = (input_ids == gMASK).to(torch.int32)\n\t        is_masks = (input_ids == MASK).to(torch.int32)\n\t        use_gmasks = torch.sum(is_gmasks, dim=1) > 0\n\t        # Ëé∑Âèñ position_ids\n\t        mask_positions = torch.where(use_gmasks, torch.argmax(is_gmasks, dim=1), torch.argmax(is_masks, dim=1)).to(\n\t            torch.int32).unsqueeze(1)\n", "        position_ids_pre = torch.where(context_mask, input_range, mask_positions)\n\t        block_position_ids = torch.clamp(input_range - context_lengths.unsqueeze(1) + 1, min=0)\n\t        position_ids = torch.stack((position_ids_pre, block_position_ids), dim=1).to(torch.int32)\n\t        return attention_mask, position_ids\n\t    def inference_step_1(self,\n\t                         input_ids: torch.Tensor,\n\t                         attention_mask: torch.Tensor,\n\t                         position_ids: torch.Tensor):\n\t        seq_len = input_ids.size(1)\n\t        bindings = []\n", "        outputs_1 = {}\n\t        for binding in self.engine_:\n\t            if binding.endswith(\"[profile 1]\"):\n\t                continue\n\t            if self.engine_.binding_is_input(binding):\n\t                if binding == 'input_ids':\n\t                    tensor = input_ids\n\t                elif binding == 'attention_mask':\n\t                    tensor = attention_mask\n\t                elif binding == 'position_ids':\n", "                    tensor = position_ids\n\t                elif binding.startswith(\"past\"):\n\t                    tensor = torch.empty(size=(0, 1, 32, 128), dtype=torch.float16, device=torch.device('cuda'))\n\t                else:\n\t                    assert 0\n\t                self.context_1_.set_input_shape(binding, tuple(tensor.shape))\n\t            else:\n\t                if binding == \"logits\":\n\t                    tensor = torch.empty(size=(1, seq_len, 130528), dtype=torch.float16, device=torch.device('cuda'))\n\t                elif binding.startswith(\"present\"):\n", "                    tensor = torch.empty(size=(seq_len, 1, 32, 128), dtype=torch.float16, device=torch.device('cuda'))\n\t                else:\n\t                    assert 0\n\t                outputs_1[binding] = tensor\n\t            bindings.append(tensor.data_ptr())\n\t        assert self.context_1_.execute_async_v2(bindings, torch.cuda.current_stream().cuda_stream)\n\t        return outputs_1\n\t    def post_processing_step_1(self,\n\t                               outputs_1,\n\t                               input_ids: torch.Tensor,\n", "                               attention_mask: torch.Tensor,\n\t                               position_ids: torch.Tensor):\n\t        logits = outputs_1['logits']\n\t        next_token_logits = logits[:, -1, :]\n\t        # ‰∏Ä‰∫õÂêéÂ§ÑÁêÜÈÄªËæë\n\t        next_token_scores = self.logits_processor(input_ids, next_token_logits)\n\t        next_token_scores = self.logits_warper(input_ids, next_token_scores)\n\t        # ÈááÊ†∑‰∏ã‰∏Ä‰∏™token\n\t        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)\n\t        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n", "        ori_input_ids = torch.cat((input_ids, next_tokens[:, None]), dim=-1)\n\t        # ËæìÂá∫‰∏ã‰∏ÄËΩÆÁöÑ input_ids, position_ids, attention_mask\n\t        attention_mask = attention_mask[..., -1:, -1:]\n\t        position_ids = torch.cat(\n\t            (position_ids[:, :-1, -1:], position_ids[:, -1:, -1:] + torch.tensor(1, dtype=position_ids.dtype)), dim=1)\n\t        input_ids = ori_input_ids[:, -1:].to(torch.int32)\n\t        past_key_values = ()\n\t        for i in range(self.num_layers_):\n\t            past_key = outputs_1[f'present_key_values.{i}.decorder.key']\n\t            past_value = outputs_1[f'present_key_values.{i}.decorder.value']\n", "            past_key_values += ((past_key, past_value),)\n\t        return ori_input_ids, input_ids, attention_mask, position_ids, past_key_values\n\t    def inference_step_x(self,\n\t                         ori_input_ids: torch.Tensor,\n\t                         input_ids: torch.Tensor,\n\t                         attention_mask: torch.Tensor,\n\t                         position_ids: torch.Tensor,\n\t                         past_key_values: Tuple[torch.Tensor]):\n\t        seq_len = ori_input_ids.size(1)\n\t        bindings = []\n", "        outputs = {}\n\t        for binding in self.engine_:\n\t            if not binding.endswith(\"[profile 1]\"):\n\t                bindings.append(0)\n\t                continue\n\t            if self.engine_.binding_is_input(binding):\n\t                if binding == 'input_ids [profile 1]':\n\t                    tensor = input_ids\n\t                elif binding == 'attention_mask [profile 1]':\n\t                    tensor = attention_mask\n", "                elif binding == 'position_ids [profile 1]':\n\t                    tensor = position_ids\n\t                elif binding.startswith(\"past\"):\n\t                    layer_id = int(binding.split('.')[1])\n\t                    tuple_id = 0 if '.key' in binding else 1\n\t                    tensor = past_key_values[layer_id][tuple_id]\n\t                else:\n\t                    assert 0\n\t                self.context_2_.set_input_shape(binding[:-12], tuple(tensor.shape))\n\t            else:\n", "                if binding == \"logits [profile 1]\":\n\t                    tensor = torch.empty(size=(1, 1, 130528), dtype=torch.float16, device=input_ids.device)\n\t                elif binding.startswith(\"present\"):\n\t                    tensor = torch.empty(size=(seq_len, 1, 32, 128), dtype=torch.float16, device=input_ids.device)\n\t                else:\n\t                    assert 0\n\t                outputs[binding] = tensor\n\t            bindings.append(tensor.data_ptr())\n\t        assert self.context_2_.execute_async_v2(bindings, torch.cuda.current_stream().cuda_stream)\n\t        return outputs\n", "    def post_processing_step_x(self,\n\t                               outputs_x,\n\t                               ori_input_ids: torch.Tensor,\n\t                               input_ids: torch.Tensor,\n\t                               attention_mask: torch.Tensor,\n\t                               position_ids: torch.Tensor):\n\t        logits = outputs_x['logits [profile 1]']\n\t        next_token_logits = logits[:, -1, :]\n\t        # ‰∏Ä‰∫õÂêéÂ§ÑÁêÜÈÄªËæë\n\t        next_token_scores = self.logits_processor(input_ids, next_token_logits)\n", "        next_token_scores = self.logits_warper(input_ids, next_token_scores)\n\t        # ÈááÊ†∑‰∏ã‰∏Ä‰∏™token\n\t        probs = torch.nn.functional.softmax(next_token_scores, dim=-1)\n\t        next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\t        ori_input_ids = torch.cat((ori_input_ids, next_tokens[:, None]), dim=-1)\n\t        # ËæìÂá∫‰∏ã‰∏ÄËΩÆÁöÑ input_ids, position_ids, attention_mask\n\t        attention_mask = attention_mask[..., -1:, -1:]\n\t        position_ids = torch.cat(\n\t            (position_ids[:, :-1, -1:], position_ids[:, -1:, -1:] + torch.tensor(1, dtype=position_ids.dtype)), dim=1)\n\t        input_ids = ori_input_ids[:, -1:].to(torch.int32)\n", "        past_key_values = ()\n\t        for i in range(self.num_layers_):\n\t            past_key = outputs_x[f'present_key_values.{i}.decorder.key [profile 1]']\n\t            past_value = outputs_x[f'present_key_values.{i}.decorder.value [profile 1]']\n\t            past_key_values += ((past_key, past_value),)\n\t        return ori_input_ids, input_ids, attention_mask, position_ids, past_key_values\n\t    def process_response(self, response):\n\t        response = response.strip()\n\t        # response = response.replace(\"[[ËÆ≠ÁªÉÊó∂Èó¥]]\", \"2023Âπ¥\")\n\t        punkts = [\n", "            [\",\", \"Ôºå\"],\n\t            [\"!\", \"ÔºÅ\"],\n\t            [\":\", \"Ôºö\"],\n\t            [\";\", \"Ôºõ\"],\n\t            [\"\\?\", \"Ôºü\"],\n\t        ]\n\t        for item in punkts:\n\t            response = re.sub(r\"([\\u4e00-\\u9fff])%s\" % item[0], r\"\\1%s\" % item[1], response)\n\t            response = re.sub(r\"%s([\\u4e00-\\u9fff])\" % item[0], r\"%s\\1\" % item[1], response)\n\t        return response\n", "if __name__ == \"__main__\":\n\t    kernel = Kernel(\"../models/chatglm6b-bs1-12.5G.plan\", 1)"]}
{"filename": "kernel/pykernel_with_past.py", "chunked_list": ["import os\n\timport sys\n\timport os.path\n\timport torch\n\timport tensorrt as trt\n\tfrom colored import stylize, fg\n\tfrom typing import List, Tuple\n\tfrom cuda import cudart\n\tclass MyLogger(trt.ILogger):\n\t    def __init__(self):\n", "        trt.ILogger.__init__(self)\n\t    def log(self, severity, msg):\n\t        if severity == trt.ILogger.ERROR:\n\t            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.WARNING:\n\t            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # ÈªÑËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INTERNAL_ERROR:\n\t            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INFO:\n\t            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # ÁªøËâ≤Â≠ó‰Ωì\n", "        elif severity == trt.ILogger.VERBOSE:\n\t            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # ËìùËâ≤Â≠ó‰Ωì\n\t        else:\n\t            print(\"[UNKNOWN] \" + msg)\n\tdef gpu_check(error: cudart.cudaError_t):\n\t    if error != cudart.cudaError_t.cudaSuccess:\n\t        error_name = cudart.cudaGetErrorName(error)\n\t        error_info = cudart.cudaGetErrorString(error)\n\t        print(stylize(f\"ERROR [{error_name}]: {error_info}\", fg(\"red\")))\n\t        raise Exception(f\"ERROR [{error_name}]: {error_info}\")\n", "class KernelWithPast:\n\t    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n\t        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n\t        self.batch_size_ = batch_size\n\t        self.n_input_ = 2 + num_layers * 2\n\t        self.n_output_ = num_layers * 2 + 1\n\t        self.num_layers = num_layers\n\t        self.n_total_ = self.n_input_ + self.n_output_\n\t        self.tensor_names_ = []\n\t        # self.logger_ = trt.Logger(trt.Logger.INFO)\n", "        self.logger_ = MyLogger()\n\t        self.runtime_ = trt.Runtime(self.logger_)\n\t        # load engine\n\t        with open(engine_path, \"rb\") as f:\n\t            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n\t        # verify io number\n\t        self.verify_io_number()\n\t        # init stream and context\n\t        error_log, self.stream_ = cudart.cudaStreamCreate()\n\t        gpu_check(error_log)\n", "        self.context_ = self.engine_.create_execution_context()\n\t        print(stylize(\"init context and stream OK\", fg(\"green\")))\n\t        self.context_.set_optimization_profile_async(0, self.stream_)\n\t    def __del__(self):\n\t        cudart.cudaStreamDestroy(self.stream_)\n\t    def verify_io_number(self):\n\t        n_io = self.engine_.num_io_tensors\n\t        if n_io != self.n_total_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of IO tensors is not correct, \" +\n", "                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n\t                fg(\"red\")))\n\t        n_input = 0\n\t        n_output = 0\n\t        for i in range(n_io):\n\t            name = self.engine_.get_tensor_name(i)\n\t            self.tensor_names_.append(name)\n\t            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n\t                n_input += 1\n\t            else:\n", "                n_output += 1\n\t        if n_input != self.n_input_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of input tensors is not correct, \" +\n\t                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n\t                fg(\"red\")))\n\t        if n_output != self.n_output_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of output tensors is not correct, \" +\n\t                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n", "                fg(\"red\")))\n\t        n_profile = self.engine_.num_optimization_profiles\n\t        if n_profile != 1:\n\t            raise RuntimeError(stylize(\n\t                \"Number of optimization profiles is not correct, \" +\n\t                f\"must be 1, but you have {n_profile} profiles\",\n\t                fg(\"red\")))\n\t        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\t    def set_input_shape(self, seq_len: int):\n\t        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, 1))\n", "        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, 1))\n\t        for i in range(2, self.n_input_):\n\t            self.context_.set_input_shape(\n\t                self.tensor_names_[i],\n\t                (seq_len, self.batch_size_, 2, 128)\n\t            )\n\t    def get_tensor_size(self):\n\t        shape_list = []\n\t        data_type_list = []\n\t        for i in range(self.n_total_):\n", "            tensor_name = self.tensor_names_[i]\n\t            shape = self.context_.get_tensor_shape(tensor_name)\n\t            shape_list.append(shape)\n\t            data_type = self.engine_.get_tensor_dtype(tensor_name)\n\t            data_type_list.append(data_type)\n\t        return shape_list, data_type_list\n\t    def forward(self, input_tensors: Tuple[torch.Tensor, ...]):\n\t        assert len(input_tensors) == self.n_input_, \\\n\t            print(f\"this number of input tensor must be {self.n_input_}\")\n\t        device = input_tensors[0].device\n", "        past_seq_len = input_tensors[2].shape[0]\n\t        self.set_input_shape(past_seq_len)\n\t        shape_list, data_type_list = self.get_tensor_size()\n\t        # --- prepare for output --- #\n\t        output_tensors = []\n\t        for i in range(self.n_input_, self.n_total_):\n\t            if data_type_list[i] == trt.DataType.FLOAT:\n\t                torch_type = torch.float32\n\t            elif data_type_list[i] == trt.DataType.HALF:\n\t                torch_type = torch.float16\n", "            elif data_type_list[i] == trt.DataType.INT32:\n\t                torch_type = torch.int32\n\t            elif data_type_list[i] == trt.DataType.INT8:\n\t                torch_type = torch.int8\n\t            else:\n\t                raise Exception(f\"not support type {data_type_list[i]}\")\n\t            tensor = torch.zeros(\n\t                size=tuple(shape_list[i]), dtype=torch_type, device=device\n\t            )\n\t            output_tensors.append(tensor)\n", "        # === run inference with V3 ===\n\t        for i in range(self.n_input_):\n\t            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                input_tensors[i].data_ptr()\n\t            )\n\t        for i in range(self.n_input_, self.n_total_):\n\t            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                output_tensors[i - self.n_input_].data_ptr()\n", "            )\n\t        self.context_.execute_async_v3(stream_handle=self.stream_)\n\t        (error_info,) = cudart.cudaDeviceSynchronize()\n\t        gpu_check(error_info)\n\t        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n\t        gpu_check(error_info)\n\t        return output_tensors\n\tif __name__ == \"__main__\":\n\t    now_dir = os.path.dirname(os.path.abspath(__file__))\n\t    project_dir = os.path.dirname(now_dir)\n", "    model_dir = os.path.join(project_dir, \"models\")\n\t    num_layers = 28\n\t    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n\t    kernel = KernelWithPast(engine_path1)\n\t    input_ids = torch.ones([1, 1], dtype=torch.int64).cuda()\n\t    position_ids = torch.ones([1, 1], dtype=torch.int64).cuda()\n\t    input_tensors1 = [input_ids, position_ids]\n\t    device1 = torch.device(\"cuda:0\")\n\t    for _ in range(num_layers):\n\t        for _ in range(2):\n", "            on_key_value = torch.rand([65, 1, 2, 128]).to(device1)\n\t            input_tensors1.append(on_key_value)\n\t    output_tensors1 = kernel.forward(tuple(input_tensors1))\n\t    print(\"first shape\", output_tensors1[0].shape)\n\t    print(\"last shape\", output_tensors1[-1].shape)\n"]}
{"filename": "kernel/pykernel_no_past.py", "chunked_list": ["import os\n\timport sys\n\timport os.path\n\timport torch\n\timport tensorrt as trt\n\tfrom colored import stylize, fg\n\tfrom typing import List, Tuple\n\tfrom cuda import cudart\n\tclass MyLogger(trt.ILogger):\n\t    def __init__(self):\n", "        trt.ILogger.__init__(self)\n\t    def log(self, severity, msg):\n\t        if severity == trt.ILogger.ERROR:\n\t            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.WARNING:\n\t            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # ÈªÑËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INTERNAL_ERROR:\n\t            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.ILogger.INFO:\n\t            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # ÁªøËâ≤Â≠ó‰Ωì\n", "        elif severity == trt.ILogger.VERBOSE:\n\t            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # ËìùËâ≤Â≠ó‰Ωì\n\t        else:\n\t            print(\"[UNKNOWN] \" + msg)\n\tdef gpu_check(error: cudart.cudaError_t):\n\t    if error != cudart.cudaError_t.cudaSuccess:\n\t        error_name = cudart.cudaGetErrorName(error)\n\t        error_info = cudart.cudaGetErrorString(error)\n\t        print(stylize(f\"ERROR [{error_name}]: {error_info}\", fg(\"red\")))\n\t        raise Exception(f\"ERROR [{error_name}]: {error_info}\")\n", "class KernelNoPast:\n\t    def __init__(self, engine_path: str, batch_size: int = 1, num_layers: int = 28):\n\t        assert os.path.exists(engine_path), print(f\"{engine_path} not exists.\")\n\t        self.batch_size_ = batch_size\n\t        self.n_input_ = 2\n\t        self.n_output_ = num_layers * 2 + 1\n\t        self.n_total_ = self.n_input_ + self.n_output_\n\t        self.tensor_names_ = []\n\t        # self.logger_ = trt.Logger(trt.Logger.INFO)\n\t        self.logger_ = MyLogger()\n", "        self.runtime_ = trt.Runtime(self.logger_)\n\t        # load engine\n\t        with open(engine_path, \"rb\") as f:\n\t            self.engine_ = self.runtime_.deserialize_cuda_engine(f.read())\n\t        # verify io number\n\t        self.verify_io_number()\n\t        # init stream and context\n\t        error_log, self.stream_ = cudart.cudaStreamCreate()\n\t        gpu_check(error_log)\n\t        self.context_ = self.engine_.create_execution_context()\n", "        print(stylize(\"init context and stream OK\", fg(\"green\")))\n\t        self.context_.set_optimization_profile_async(0, self.stream_)\n\t    def __del__(self):\n\t        cudart.cudaStreamDestroy(self.stream_)\n\t    def verify_io_number(self):\n\t        n_io = self.engine_.num_io_tensors\n\t        if n_io != self.n_total_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of IO tensors is not correct, \" +\n\t                f\"must be {self.n_total_}, but you have {n_io} tensors\",\n", "                fg(\"red\")))\n\t        n_input = 0\n\t        n_output = 0\n\t        for i in range(n_io):\n\t            name = self.engine_.get_tensor_name(i)\n\t            self.tensor_names_.append(name)\n\t            if self.engine_.get_tensor_mode(name) == trt.TensorIOMode.INPUT:\n\t                n_input += 1\n\t            else:\n\t                n_output += 1\n", "        if n_input != self.n_input_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of input tensors is not correct, \" +\n\t                f\"must be {self.n_input_}, but you have {n_input} tensors\",\n\t                fg(\"red\")))\n\t        if n_output != self.n_output_:\n\t            raise RuntimeError(stylize(\n\t                \"Number of output tensors is not correct, \" +\n\t                f\"must be {self.n_output_}, but you have {n_output} tensors\",\n\t                fg(\"red\")))\n", "        n_profile = self.engine_.num_optimization_profiles\n\t        if n_profile != 1:\n\t            raise RuntimeError(stylize(\n\t                \"Number of optimization profiles is not correct, \" +\n\t                f\"must be 1, but you have {n_profile} profiles\",\n\t                fg(\"red\")))\n\t        print(stylize(f\"number of profile: {n_profile}\", fg(\"green\")))\n\t    def set_input_shape(self, seq_len: int):\n\t        self.context_.set_input_shape(\"input_ids\", (self.batch_size_, seq_len))\n\t        self.context_.set_input_shape(\"position_ids\", (self.batch_size_, seq_len))\n", "    def get_tensor_size(self):\n\t        shape_list = []\n\t        data_type_list = []\n\t        for i in range(self.n_total_):\n\t            tensor_name = self.tensor_names_[i]\n\t            shape = self.context_.get_tensor_shape(tensor_name)\n\t            shape_list.append(shape)\n\t            data_type = self.engine_.get_tensor_dtype(tensor_name)\n\t            data_type_list.append(data_type)\n\t        return shape_list, data_type_list\n", "    def forward(self, input_tensors: Tuple[torch.Tensor, torch.Tensor]):\n\t        assert len(input_tensors) == self.n_input_, \\\n\t            print(f\"this number of input tensor must be {self.n_input_}\")\n\t        seq_len = input_tensors[0].size(1)\n\t        device = input_tensors[0].device\n\t        self.set_input_shape(seq_len)\n\t        shape_list, data_type_list = self.get_tensor_size()\n\t        # --- prepare for output --- #\n\t        output_tensors = []\n\t        for i in range(self.n_input_, self.n_total_):\n", "            if data_type_list[i] == trt.DataType.FLOAT:\n\t                torch_type = torch.float32\n\t            elif data_type_list[i] == trt.DataType.HALF:\n\t                torch_type = torch.float16\n\t            elif data_type_list[i] == trt.DataType.INT32:\n\t                torch_type = torch.int32\n\t            elif data_type_list[i] == trt.DataType.INT8:\n\t                torch_type = torch.int8\n\t            else:\n\t                raise Exception(f\"not support type {data_type_list[i]}\")\n", "            tensor = torch.zeros(\n\t                size=tuple(shape_list[i]), dtype=torch_type, device=device\n\t            )\n\t            output_tensors.append(tensor)\n\t        # === run inference with V3 ===\n\t        for i in range(self.n_input_):\n\t            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                input_tensors[i].data_ptr()\n\t            )\n", "        for i in range(self.n_input_, self.n_total_):\n\t            self.context_.set_tensor_address(\n\t                self.tensor_names_[i],\n\t                output_tensors[i - self.n_input_].data_ptr()\n\t            )\n\t        self.context_.execute_async_v3(stream_handle=self.stream_)\n\t        (error_info,) = cudart.cudaDeviceSynchronize()\n\t        gpu_check(error_info)\n\t        (error_info,) = cudart.cudaStreamSynchronize(self.stream_)\n\t        gpu_check(error_info)\n", "        return output_tensors\n\tif __name__ == \"__main__\":\n\t    now_dir = os.path.dirname(os.path.abspath(__file__))\n\t    project_dir = os.path.dirname(now_dir)\n\t    model_dir = os.path.join(project_dir, \"models\")\n\t    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_no_cache.plan\")\n\t    kernel = KernelNoPast(engine_path1)\n\t    input_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n\t    position_ids = torch.ones([1, 4], dtype=torch.int64).cuda()\n\t    input_tensors1 = (input_ids, position_ids)\n", "    output_tensors1 = kernel.forward(input_tensors1)\n\t    print(\"first shape\", output_tensors1[0].shape)\n\t    print(\"last shape\", output_tensors1[-1].shape)\n"]}
{"filename": "tensorrt_export/onnx_trt_compare_runing.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n\t# code gen with onnx_trt_compare.sh\n\t# but i edit the code to make it more readable\n\timport tensorrt as trt\n\timport os\n\timport numpy as np\n\tfrom polygraphy.logger import G_LOGGER\n\tG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\n\tfrom polygraphy import constants\n", "import onnx\n\tfrom polygraphy.backend.common import BytesFromPath\n\tfrom polygraphy.backend.onnx import BytesFromOnnx, ModifyOutputs as ModifyOnnxOutputs, OnnxFromPath\n\tfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\n\tfrom polygraphy.backend.trt import EngineFromBytes, TrtRunner\n\tfrom polygraphy.common import TensorMetadata\n\tfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\n\tfrom polygraphy.exception import PolygraphyException\n\t# --dir info--\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n", "project_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\n\tnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\n\tmodel_dir = os.path.join(project_dir, \"models\")\n\ttrt_model_path = os.path.join(model_dir, \"model-no-cache-FP32-MarkAll.plan\")\n\tnum_layers = 1\n\t# Data Loader\n\tdata_loader = DataLoader(\n\t    input_metadata=TensorMetadata()\n", "    .add('input_ids', dtype=np.int32, shape=(1, 512), min_shape=None, max_shape=None)\n\t    .add('position_ids', dtype=np.int32, shape=(1, 512), min_shape=None, max_shape=None)\n\t)\n\tprint(\"load onnx\")\n\t# build_onnxrt_session = SessionFromOnnx(new_onnx_path, providers=[\"CUDAExecutionProvider\"])\n\tbuild_onnxrt_session = SessionFromOnnx(new_onnx_path, providers=[\"CPUExecutionProvider\"])\n\tprint(\"load TensorRT engine\")\n\tengine_bytes = BytesFromPath(trt_model_path)\n\tdeserialize_engine = EngineFromBytes(engine_bytes)\n\t# Runners\n", "runners = [\n\t    OnnxrtRunner(build_onnxrt_session),\n\t    # TrtRunner(deserialize_engine),\n\t]\n\t# Runner Execution\n\tresults = Comparator.run(runners, data_loader=data_loader)\n\tsuccess = True\n\t# Accuracy Comparison for fp32\n\t# compare_func = CompareFunc.simple(rtol={'': 5e-4}, atol={'': 5e-4})\n\t# Accuracy Comparison for fp16\n", "compare_func = CompareFunc.simple(rtol={'': 5e-3}, atol={'': 5e-3})\n\tsuccess &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\t# Report Results\n\tif not success:\n\t    raise PolygraphyException('FAILED')\n"]}
{"filename": "tensorrt_export/onnx_trt_compare_fp16.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n\t# code gen with onnx_trt_compare.sh\n\t# but i edit the code to make it more readable\n\timport tensorrt as trt\n\timport os\n\timport shutil\n\timport numpy as np\n\tfrom polygraphy.logger import G_LOGGER\n\tG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\n", "from colored import stylize, fg\n\tfrom polygraphy import constants\n\timport onnx\n\tfrom polygraphy.backend.common import SaveBytes\n\tfrom polygraphy.backend.onnx import modify_outputs, onnx_from_path, ModifyOutputs\n\tfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\n\tfrom polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\n\tfrom polygraphy.common import TensorMetadata\n\tfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\n\tfrom polygraphy.exception import PolygraphyException\n", "from polygraphy.backend.trt import network_from_onnx_path\n\t# --dir info--\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\timport sys\n\tsys.path.append(project_dir)\n\tfrom tensorrt_export.onnx2trt_no_cache import (\n\t    get_network_profiles,\n\t    MyLogger,\n\t)\n", "output_dir = os.path.join(project_dir, \"output\")\n\tnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\n\tif not os.path.exists(new_onnx_dir):\n\t    os.mkdir(new_onnx_dir)\n\telse:\n\t    for file in os.listdir(new_onnx_dir):\n\t        os.remove(os.path.join(new_onnx_dir, file))\n\tnew_onnx_dir2 = os.path.join(output_dir, \"onnx_output_no_cache_new2\")\n\tif not os.path.exists(new_onnx_dir2):\n\t    os.mkdir(new_onnx_dir2)\n", "else:\n\t    for file in os.listdir(new_onnx_dir2):\n\t        os.remove(os.path.join(new_onnx_dir2, file))\n\tonnx_path = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\n\tnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\n\tnew_onnx_path2 = os.path.join(new_onnx_dir2, \"chatglm2_6b.onnx\")\n\tmodel_dir = os.path.join(project_dir, \"models\")\n\ttrt_model_path = os.path.join(model_dir, \"model-no-cache-FP16-MarkAll.plan\")\n\ttime_cache_path = os.path.join(output_dir, \"fp16_markAll_no_cache.cache\")\n\tuse_time_cache = True\n", "num_layers = 1\n\t# Data Loader\n\tdtype = np.dtype(np.int32)\n\tdata_loader = DataLoader(\n\t    input_metadata=TensorMetadata()\n\t    .add('input_ids', dtype=dtype, shape=(1, 512))\n\t    .add('position_ids', dtype=np.int32, shape=(1, 512))\n\t)\n\t# load onnx\n\tprint(\"loading onnx model from\", onnx_path)\n", "onnx_model = onnx_from_path(onnx_path)\n\t# this layer will output None in onnxrt\n\tbool_tensor_list = [\"/transformer/encoder/layers.0/mlp/Sigmoid_output_0\"]\n\tfor node in onnx_model.graph.node:\n\t    # print(node.name, node.op_type)\n\t    # this layer is a bool tensor, it will cause error when run TensorRT engine\n\t    if node.op_type == \"Equal\":\n\t        print(\"find bool\", node.name)\n\t        bool_tensor_list.extend(node.output)\n\tinput_list = onnx_model.graph.input\n", "input_names = [i.name for i in input_list]\n\toutput_list = onnx_model.graph.output\n\toutput_names = [o.name for o in output_list]\n\texclude_outputs = input_names + bool_tensor_list\n\t# mark all layers as output for onnx model\n\t# warning this will make the onnx model output all layers with no type and no shape\n\tnew_onnx_model = modify_outputs(\n\t    model=onnx_model,\n\t    outputs=constants.MARK_ALL,\n\t    exclude_outputs=exclude_outputs,\n", ")\n\tnew_output_list = new_onnx_model.graph.output\n\tnew_output_names = [o.name for o in new_output_list]\n\tonnx_input_num = len(new_onnx_model.graph.input)\n\tonnx_output_num = len(new_onnx_model.graph.output)\n\tprint(\"onnx input num:\", onnx_input_num, \"onnx output num:\", onnx_output_num)\n\tonnx.save_model(\n\t    new_onnx_model,\n\t    new_onnx_path,\n\t    save_as_external_data=True,\n", "    all_tensors_to_one_file=False\n\t)\n\t# load onnx_runtime\n\tbuild_onnx_rt_session = SessionFromOnnx(new_onnx_path)\n\t# get onnx runtime output tensor info(name, type, shape)\n\tsess = build_onnx_rt_session()\n\tbool_tensor_list = []\n\tsess_output = sess.get_outputs()\n\tfor node in sess_output:\n\t    if node.type == \"tensor(float)\":\n", "        dtype = onnx.TensorProto.FLOAT\n\t    elif node.type == \"tensor(bool)\":\n\t        print(\"find bool\", node.name)\n\t        bool_tensor_list.append(node.name)\n\t        dtype = onnx.TensorProto.BOOL\n\t    elif node.type == \"tensor(int64)\":\n\t        dtype = onnx.TensorProto.INT64\n\t    else:\n\t        print(\"unknown dtype:\", node.type)\n\t        raise ValueError\n", "    output_tensor = onnx.helper.make_tensor_value_info(\n\t        node.name, dtype, None\n\t    )\n\t    # replace the output tensor\n\t    for i, vi in enumerate(new_onnx_model.graph.output):\n\t        if vi.name == node.name:\n\t            new_onnx_model.graph.output[i].CopyFrom(output_tensor)\n\t# save again\n\tfor file in os.listdir(new_onnx_dir):\n\t    shutil.copy(os.path.join(new_onnx_dir, file), new_onnx_dir2)\n", "onnx.save_model(\n\t    new_onnx_model,\n\t    new_onnx_path2,\n\t    save_as_external_data=True,\n\t    all_tensors_to_one_file=False\n\t)\n\tprint(\"===========onnx model loaded=========================\")\n\t# build trt engine\n\tprint(\"===========building trt engine=========================\")\n\t# prepare trt builder\n", "builder = trt.Builder(MyLogger())\n\tbuilder.max_threads = os.cpu_count() // 2\n\tconfig = builder.create_builder_config()\n\t# use fp16\n\tconfig.flags = 1 << int(trt.BuilderFlag.FP16)\n\t# disable TF32\n\tconfig.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n\tprint(\"use fp16? \", config.get_flag(trt.BuilderFlag.FP16))\n\t# read time cache\n\tif use_time_cache:\n", "    if os.path.exists(time_cache_path):\n\t        time_cache = open(time_cache_path, \"rb\").read()\n\t        if time_cache is None:\n\t            time_cache = b\"\"\n\t            print(stylize(\"read time cache failed\", fg(\"red\")))\n\t        else:\n\t            print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n\t    else:\n\t        time_cache = b\"\"\n\t        print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n", "    # set time cache\n\t    cache = config.create_timing_cache(time_cache)\n\t    config.set_timing_cache(cache, False)\n\tprofile_list = get_network_profiles(builder, num_layers=num_layers)\n\tfor profile in profile_list:\n\t    config.add_optimization_profile(profile)\n\t_b, network, _p = NetworkFromOnnxPath(new_onnx_path2)()\n\t# network = network_from_onnx_path(onnx_path)\n\t# set_network_outputs = ModifyNetworkOutputs(\n\t#     network=network,\n", "#     outputs=constants.MARK_ALL,\n\t#     exclude_outputs=bool_tensor_list\n\t# )\n\t# wo_b, network, _p = set_network_outputs()\n\tnetwork_input_number = network.num_inputs\n\tnetwork_output_number = network.num_outputs\n\tprint(\"TensorRT input num:\", network_input_number, \"TensorRT output num:\", network_output_number)\n\tserialized_engine = builder.build_serialized_network(network, config)\n\tif serialized_engine is not None:\n\t    with open(trt_model_path, \"wb\") as f:\n", "        f.write(serialized_engine)\n\t    # save_engine(trt_engine, tensorrt_engine_path)\n\t    print(\"==tensorRT engine compile done==\")\n\t# save time cache\n\tif use_time_cache and not os.path.exists(time_cache_path):\n\t    time_cache = config.get_timing_cache()\n\t    if time_cache is not None:\n\t        time_cache_data = time_cache.serialize()\n\t        open(time_cache_path, \"wb\").write(time_cache_data)\n\t        print(\n", "            stylize(\n\t                \"save time cache to {}\".format(time_cache_path),\n\t                fg(\"green\")\n\t            )\n\t        )\n\t# profiles = [\n\t#     Profile().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', min=[0, 1, 32, 128], opt=[0, 1, 32, 128], max=[0, 1, 32, 128])\n\t# ]\n\t# create_trt_config = CreateTrtConfig(memory_pool_limits=2 * (1024 ** 3), profiles=profile_list)\n\t# build_engine = EngineBytesFromNetwork(set_network_outputs, config=create_trt_config)\n", "# save_engine_bytes = SaveBytes(build_engine, path=trt_model_path)\n\tsave_engine_bytes = SaveBytes(serialized_engine, path=trt_model_path)\n\tdeserialize_engine = EngineFromBytes(save_engine_bytes)\n\tprint(\"===========trt engine build OK=========================\")\n\t# Runners\n\trunners = [\n\t    OnnxrtRunner(build_onnx_rt_session),\n\t    TrtRunner(deserialize_engine),\n\t]\n\t# Runner Execution\n", "results = Comparator.run(runners, data_loader=data_loader)\n\tsuccess = True\n\t# Accuracy Comparison\n\tcompare_func = CompareFunc.simple(rtol={'': 1e-4}, atol={'': 1e-4})\n\tsuccess &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\t# Report Results\n\tif not success:\n\t    raise PolygraphyException('FAILED')\n"]}
{"filename": "tensorrt_export/trt_check_no_past.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\tfrom colored import stylize, fg\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\t# from kernel.pykernel_no_past_old import KernelNoPast\n\tfrom kernel.pykernel_no_past_new import KernelNoPast\n\tdef check_value(pre_value: torch.Tensor, true_value: torch.Tensor, diff=1e-3):\n", "    if pre_value.shape != true_value.shape:\n\t        raise Exception(\"compare shape must be same!\")\n\t    max_diff = (pre_value - true_value).abs_().max().item()\n\t    if max_diff > diff:\n\t        print(stylize(f\"compare diff failed, diff is {max_diff}\", fg(\"red\")))\n\t    else:\n\t        print(stylize(\"compare diff OK!\", fg(\"green\")))\n\t    return max_diff\n\tdef main():\n\t    assert torch.cuda.is_available(), print(\"you must has cuda to run TensorRT\")\n", "    output_dir = os.path.join(project_dir, \"output\")\n\t    model_dir = os.path.join(project_dir, \"models\")\n\t    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n\t    input_path = os.path.join(output_dir, \"pt_input1.pt\")\n\t    output_path = os.path.join(output_dir, \"pt_output1.pt\")\n\t    device = torch.device(\"cuda:0\")\n\t    input_dict = torch.jit.load(input_path)\n\t    batch_size = 1\n\t    num_layers = 28\n\t    output_dict = torch.jit.load(output_path)\n", "    input_ids: torch.Tensor = input_dict.input_ids.int().to(device)\n\t    position_ids: torch.Tensor = input_dict.position_ids.int().to(device)\n\t    input_tensors = (input_ids, position_ids)\n\t    kernel = KernelNoPast(engine_path1, batch_size, num_layers)\n\t    output_tensors = kernel.forward(input_tensors)\n\t    # compare output\n\t    max_diff_ = 0\n\t    # compare logits\n\t    logits = output_dict.logits.to(device)\n\t    pred_logits = output_tensors[-1]\n", "    logits_diff = check_value(logits, pred_logits)\n\t    print(\"=\" * 20)\n\t    print(\"compare logits\")\n\t    if logits_diff > max_diff_:\n\t        max_diff_ = logits_diff\n\t    # compare past key values\n\t    for i in range(num_layers):\n\t        present_key_name = f\"present_key_values.{i}.key\"\n\t        present_value_name = f\"present_key_values.{i}.value\"\n\t        true_present_key = getattr(output_dict, present_key_name).to(device)\n", "        true_present_value = getattr(output_dict, present_value_name).to(device)\n\t        pre_present_key = output_tensors[i * 2]\n\t        pre_present_value = output_tensors[i * 2 + 1]\n\t        print(\"=\" * 20)\n\t        print(\"compare \", present_key_name)\n\t        temp_diff = check_value(pre_present_key, true_present_key)\n\t        if temp_diff > max_diff_:\n\t            max_diff_ = temp_diff\n\t        print(\"=\" * 20)\n\t        print(\"compare \", present_value_name)\n", "        temp_diff = check_value(pre_present_value, true_present_value)\n\t        if temp_diff > max_diff_:\n\t            max_diff_ = temp_diff\n\t    print(f\"max diff is {max_diff_}\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tensorrt_export/compare.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n\t# Generation Command: /home/tlntin/anaconda3/envs/torch/bin/polygraphy run ../output/onnx_output/chatglm_6b.onnx --onnxrt --trt --workspace 1000000000 --save-engine=../models/model-FP32-MarkAll.plan --atol 1e-3 --rtol 1e-3 --verbose --onnx-outputs mark all --trt-outputs mark all --trt-opt-shapes input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value:[0,1,32,128]  --input-shapes input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value:[0,1,32,128] --gen-script compare.py\n\t# This script compares /home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx between ONNX-Runtime and TensorRT.\n\tfrom polygraphy.logger import G_LOGGER\n\tG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\n\tfrom polygraphy import constants\n\tfrom polygraphy.backend.common import SaveBytes\n\tfrom polygraphy.backend.onnx import BytesFromOnnx, ModifyOutputs as ModifyOnnxOutputs, OnnxFromPath\n\tfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\n", "from polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\n\tfrom polygraphy.common import TensorMetadata\n\tfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\n\tfrom polygraphy.exception import PolygraphyException\n\t# Data Loader\n\tdata_loader = DataLoader(input_metadata=TensorMetadata().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', None, [0, 1, 32, 128]))\n\t# Loaders\n\tload_onnx = OnnxFromPath('/home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx')\n\tmodify_outputs = ModifyOnnxOutputs(load_onnx, outputs=constants.MARK_ALL)\n\tserialize_onnx = BytesFromOnnx(modify_outputs)\n", "build_onnxrt_session = SessionFromOnnx(serialize_onnx)\n\tparse_network_from_onnx = NetworkFromOnnxPath('/home/tlntin/PycharmProjects/ChatGLM-6B-TensorRT/output/onnx_output/chatglm_6b.onnx')\n\tset_network_outputs = ModifyNetworkOutputs(parse_network_from_onnx, outputs=constants.MARK_ALL)\n\tprofiles = [\n\t    Profile().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', min=[0, 1, 32, 128], opt=[0, 1, 32, 128], max=[0, 1, 32, 128])\n\t]\n\tcreate_trt_config = CreateTrtConfig(max_workspace_size=1000000000, profiles=profiles)\n\tbuild_engine = EngineBytesFromNetwork(set_network_outputs, config=create_trt_config)\n\tsave_engine_bytes = SaveBytes(build_engine, path='../models/model-FP32-MarkAll.plan')\n\tdeserialize_engine = EngineFromBytes(save_engine_bytes)\n", "# Runners\n\trunners = [\n\t    OnnxrtRunner(build_onnxrt_session),\n\t    TrtRunner(deserialize_engine),\n\t]\n\t# Runner Execution\n\tresults = Comparator.run(runners, data_loader=data_loader)\n\tsuccess = True\n\t# Accuracy Comparison\n\tcompare_func = CompareFunc.simple(rtol={'': 0.001}, atol={'': 0.001})\n", "success &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\t# Report Results\n\tif not success:\n\t    raise PolygraphyException('FAILED')\n"]}
{"filename": "tensorrt_export/test_data_outputs.py", "chunked_list": ["import os\n\timport torch\n\timport onnxruntime as ort\n\tfrom polygraphy.comparator import RunResults\n\tfrom polygraphy.json import save_json\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\n\tnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\n", "input_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n\toutput_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n\tinput_dict = torch.jit.load(input_path1)\n\tbatch_size = 1\n\tnum_layers = 1\n\toutput_dict = torch.jit.load(output_path1)\n\tinput_ids = input_dict.input_ids.numpy()\n\tposition_ids = input_dict.position_ids.numpy()\n\tsession = ort.InferenceSession(new_onnx_path, providers=[\"CPUExecutionProvider\"])\n\toutputs = session.get_outputs()\n", "output_names = [out.name for out in outputs]\n\toutputs = session.run(output_names, {\"input_ids\": input_ids, \"position_ids\": position_ids})\n\tprint(len(outputs))\n\tprint(type(outputs))\n\toutput_dict = {name: value for (name, value) in zip(output_names, outputs)}\n\tfor k, v in output_dict.items():\n\t    if v is None:\n\t        print(k, \"is None\")\n\tcustom_outputs = RunResults()\n\tcustom_outputs.add(out_list=[output_dict], runner_name=\"onnxrt\")\n", "custom_outputs.save(\"custom_outputs.json\")\n"]}
{"filename": "tensorrt_export/onnx2trt_no_cache.py", "chunked_list": ["import tensorrt as trt\n\timport os\n\timport time\n\tfrom colored import fg, stylize\n\timport json\n\t# import onnx\n\tfrom polygraphy.backend.trt import network_from_onnx_path\n\tfrom itertools import tee\n\tfrom tensorrt import MemoryPoolType, PreviewFeature\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n", "project_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\ttime_cache_path = os.path.join(output_dir, \"fp16_no_cache.cache\")\n\t# default is 1, maybe you can try 2, 4, 8, 16\n\tbatch_size = 1\n\tuse_time_cache = True\n\tmax_length = 2048\n\topt_length = max_length // 2\n\t# if use force use fp16, may reduce the accuracy and memory usage\n\tforce_use_fp16 = False\n", "# default 3, max 5, 5 is the best but need more GPU memory and time\n\tbuilder_optimization_level = 3\n\t# lower memory GPU can try this option with True \\\n\t# it can use CPU memory/CPU compute to run some layers, but may reduce the speed\n\tall_gpu_fallback = False\n\tif batch_size > 1 and builder_optimization_level != 5:\n\t    raise Exception(\"batch size > 1, please use builder_optimization_level = 5\")\n\tclass MyLogger(trt.ILogger):\n\t    def __init__(self):\n\t        trt.ILogger.__init__(self)\n", "    def log(self, severity, msg):\n\t        if severity == trt.Logger.ERROR:\n\t            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.WARNING:\n\t            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # ÈªÑËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.INTERNAL_ERROR:\n\t            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.INFO:\n\t            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # ÁªøËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.VERBOSE:\n", "            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # ËìùËâ≤Â≠ó‰Ωì\n\t        else:\n\t            print(\"[UNKNOWN] \" + msg)\n\tdef get_network_profiles(trt_builder, num_layers=28):\n\t    # ----profile1 when past_key_values is None----\n\t    profile1 = trt_builder.create_optimization_profile()\n\t    profile1.set_shape(\n\t        \"input_ids\",\n\t        (1, 1),\n\t        (batch_size, opt_length),\n", "        (batch_size, max_length),\n\t    )\n\t    profile1.set_shape(\n\t        \"position_ids\",\n\t        (1, 1),\n\t        (batch_size, opt_length),\n\t        (batch_size, max_length),\n\t    )\n\t    profiles = [profile1]\n\t    return profiles\n", "def get_network_definition(trt_network):\n\t    def pairwise(iterable):\n\t        a, b = tee(iterable)\n\t        next(b, None)\n\t        return zip(a, b)\n\t    layer_type_set = set() \n\t    num_layers = trt_network.num_layers\n\t    indices = list(range(num_layers))\n\t    for i, i_next in pairwise(indices):\n\t        layer = trt_network.get_layer(i)\n", "        next_layer = trt_network.get_layer(i_next)\n\t        if not all([\n\t            layer.get_output(i).is_execution_tensor\n\t            for i in range(layer.num_outputs)\n\t        ]):\n\t            continue\n\t        if layer.get_output_type(0) != trt.float32:\n\t            continue\n\t        layer_type_set.add(str(layer.type))\n\t        if layer.type == trt.LayerType.ELEMENTWISE and \\\n", "                next_layer.type == trt.LayerType.REDUCE:\n\t            layer.__class__ = getattr(trt, \"IElementWiseLayer\")\n\t            if layer.op == trt.ElementWiseOperation.POW:\n\t                layer.precision = trt.float32\n\t                layer.set_output_type(0, trt.float32)\n\t            next_layer.precision = trt.float32\n\t            next_layer.set_output_type(0, trt.float32)\n\t        #else:\n\t        #    layer.precision = trt.DataType.HALF\n\t    layer_type_path = os.path.join(output_dir, \"layer_type.json\")\n", "    with open(layer_type_path, \"wt\") as f1:\n\t        json.dump(list(layer_type_set), f1, indent=4)\n\t    return trt_network\n\tif __name__ == \"__main__\":\n\t    onnx_path = os.path.join(\n\t        output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\"\n\t    )\n\t    model_dir = os.path.join(project_dir, \"models\")\n\t    if not os.path.exists(model_dir):\n\t        os.mkdir(model_dir)\n", "    tensorrt_engine_path = os.path.join(\n\t        model_dir, f\"chatglm6b2-bs{batch_size}_no_cache.plan\"\n\t    )\n\t    builder = trt.Builder(MyLogger())\n\t    builder.max_threads = os.cpu_count() // 2\n\t    config = builder.create_builder_config()\n\t    profile_list = get_network_profiles(builder)\n\t    for profile in profile_list:\n\t        config.add_optimization_profile(profile)\n\t    # use fp16\n", "    # config.flags = 1 << int(trt.BuilderFlag.FP16)\n\t    # disable tf32\n\t    config.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n\t    # use obey precision constraints\n\t    config.flags = config.flags | (1 << int(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS))\n\t    # config.set_memory_pool_limit(MemoryPoolType.WORKSPACE, 2 * 1024 * 1024 * 1024)\n\t    # use preview features\n\t    preview_features = [\n\t        # PreviewFeature.PROFILE_SHARING_0806,\n\t        PreviewFeature.FASTER_DYNAMIC_SHAPES_0805,\n", "        # PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805\n\t    ]\n\t    for feature in preview_features:\n\t        config.set_preview_feature(feature, True)\n\t    config.builder_optimization_level = builder_optimization_level\n\t    # use time cache\n\t    time_cache = b\"\"\n\t    # read time cache\n\t    if use_time_cache:\n\t        if os.path.exists(time_cache_path):\n", "            time_cache = open(time_cache_path, \"rb\").read()\n\t            if time_cache is None:\n\t                time_cache = b\"\"\n\t                print(stylize(\"read time cache failed\", fg(\"red\")))\n\t            else:\n\t                print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n\t        else:\n\t            time_cache = b\"\"\n\t            print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\t        # set time cache\n", "        cache = config.create_timing_cache(time_cache)\n\t        config.set_timing_cache(cache, False)\n\t    # load onnx model\n\t    print(\"loading onnx model from \", onnx_path)\n\t    # network =  builder.create_network(\n\t    #     1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n\t    # )\n\t    # trt_parser = trt.OnnxParser(network, builder.logger)\n\t    # onnx_model = onnx.load(onnx_path)\n\t    # trt_parser.parse(onnx_model)\n", "    _, network, _ = network_from_onnx_path(onnx_path)\n\t    network = get_network_definition(network)\n\t    print(\"=============tensorRT inference config =====================\")\n\t    if builder_optimization_level == 3:\n\t        print(\"==tensorRT engine begin compile, maybe you need wait 10-25 minute ==\")\n\t    elif builder_optimization_level == 5:\n\t        print(\"==tensorRT engine begin compile, maybe you need wait 30-60 minute ==\")\n\t    else:\n\t        print(\"==tensorRT engine begin compile, maybe you need wait a moment ==\")\n\t    # trt_engine = engine_from_network(\n", "    #     (trt_builder, network, onnx_parser),\n\t    #     trt_inference_config\n\t    # )\n\t    serialized_engine = builder.build_serialized_network(network, config)\n\t    if serialized_engine is not None:\n\t        # ‰øùÂ≠òÂºïÊìéÂà∞Êñá‰ª∂\n\t        with open(tensorrt_engine_path, \"wb\") as f:\n\t            f.write(serialized_engine)\n\t        # save_engine(trt_engine, tensorrt_engine_path)\n\t        print(\"==tensorRT engine compile done==\")\n", "    else:\n\t        raise RuntimeError(\"build engine failed\")\n\t    # save time cache\n\t    if use_time_cache and not os.path.exists(time_cache_path):\n\t        time_cache = config.get_timing_cache()\n\t        if time_cache is not None:\n\t            time_cache_data = time_cache.serialize()\n\t            open(time_cache_path, \"wb\").write(time_cache_data)\n\t            print(\n\t                stylize(\n", "                    \"save time cache to {}\".format(time_cache_path),\n\t                    fg(\"green\")\n\t                )\n\t            )"]}
{"filename": "tensorrt_export/onnx2trt_with_cache.py", "chunked_list": ["import tensorrt as trt\n\timport os\n\timport time\n\tfrom colored import fg, stylize\n\timport json\n\t# import onnx\n\tfrom polygraphy.backend.trt import network_from_onnx_path\n\tfrom itertools import tee\n\tfrom tensorrt import MemoryPoolType, PreviewFeature\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n", "project_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\ttime_cache_path = os.path.join(output_dir, \"fp16_with_cache.cache\")\n\t# default is 1, maybe you can try 2, 4, 8, 16\n\tbatch_size = 1\n\tuse_time_cache = True\n\tmax_input_length = 2048\n\topt_input_length = max_input_length // 2\n\t# if use force use fp16, may reduce the accuracy and memory usage\n\tforce_use_fp16 = False\n", "# default 3, max 5, 5 is the best but need more GPU memory and time\n\tbuilder_optimization_level = 3\n\t# lower memory GPU can try this option with True \\\n\t# it can use CPU memory/CPU compute to run some layers, but may reduce the speed\n\tall_gpu_fallback = False\n\tif batch_size > 1 and builder_optimization_level != 5:\n\t    raise Exception(\"batch size > 1, please use builder_optimization_level = 5\")\n\tclass MyLogger(trt.ILogger):\n\t    def __init__(self):\n\t        trt.ILogger.__init__(self)\n", "    def log(self, severity, msg):\n\t        if severity == trt.Logger.ERROR:\n\t            print(stylize(\"[ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.WARNING:\n\t            print(stylize(\"[WARNING] \" + msg, fg(\"yellow\")))  # ÈªÑËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.INTERNAL_ERROR:\n\t            print(stylize(\"[INTERNAL_ERROR] \" + msg, fg(\"red\")))  # Á∫¢Ëâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.INFO:\n\t            print(stylize(\"[INFO] \" + msg, fg(\"green\")))  # ÁªøËâ≤Â≠ó‰Ωì\n\t        elif severity == trt.Logger.VERBOSE:\n", "            print(stylize(\"[VERBOSE] \" + msg, fg(\"blue\")))  # ËìùËâ≤Â≠ó‰Ωì\n\t        else:\n\t            print(\"[UNKNOWN] \" + msg)\n\tdef get_network_profiles(trt_builder, num_layers=28):\n\t    # ----profile2 when past_key_values is not None----\n\t    profile2 = trt_builder.create_optimization_profile()\n\t    profile2.set_shape(\n\t        \"input_ids\",\n\t        (1, 1),\n\t        (batch_size, opt_input_length),\n", "        (batch_size, max_input_length),\n\t    )\n\t    profile2.set_shape(\n\t        \"position_ids\",\n\t        (1, 1),\n\t        (batch_size, opt_input_length),\n\t        (batch_size, max_input_length),\n\t    )\n\t    for layer_idx in range(num_layers):\n\t        input_names = [\n", "            f\"past_key_values.{layer_idx}.key\",\n\t            f\"past_key_values.{layer_idx}.value\"\n\t        ]\n\t        for name in input_names:\n\t            profile2.set_shape(\n\t                name,\n\t                (0, 1, 2, 128),\n\t                (opt_input_length - 1, batch_size, 2, 128),\n\t                (max_input_length - 1, batch_size, 2, 128),\n\t            )\n", "    profiles = [profile2]\n\t    return profiles\n\tdef get_network_definition(trt_network):\n\t    def pairwise(iterable):\n\t        a, b = tee(iterable)\n\t        next(b, None)\n\t        return zip(a, b)\n\t    layer_type_set = set() \n\t    num_layers = trt_network.num_layers\n\t    indices = list(range(num_layers))\n", "    for i, i_next in pairwise(indices):\n\t        layer = trt_network.get_layer(i)\n\t        next_layer = trt_network.get_layer(i_next)\n\t        if not all([\n\t            layer.get_output(i).is_execution_tensor\n\t            for i in range(layer.num_outputs)\n\t        ]):\n\t            continue\n\t        if layer.get_output_type(0) != trt.float32:\n\t            continue\n", "        layer_type_set.add(str(layer.type))\n\t        if layer.type == trt.LayerType.ELEMENTWISE and next_layer.type == trt.LayerType.REDUCE:\n\t            layer.__class__ = getattr(trt, \"IElementWiseLayer\")\n\t            if layer.op == trt.ElementWiseOperation.POW:\n\t                layer.precision = trt.float32\n\t                layer.set_output_type(0, trt.float32)\n\t            next_layer.precision = trt.float32\n\t            next_layer.set_output_type(0, trt.float32)\n\t        #else:\n\t        #    layer.precision = trt.DataType.HALF\n", "    layer_type_path = os.path.join(output_dir, \"layer_type.json\")\n\t    with open(layer_type_path, \"wt\") as f1:\n\t        json.dump(list(layer_type_set), f1, indent=4)\n\t    return trt_network\n\tif __name__ == \"__main__\":\n\t    onnx_path = os.path.join(output_dir, \"onnx_output\", \"chatglm2_6b.onnx\")\n\t    model_dir = os.path.join(project_dir, \"models\")\n\t    if not os.path.exists(model_dir):\n\t        os.mkdir(model_dir)\n\t    tensorrt_engine_path = os.path.join(\n", "        model_dir,\n\t        f\"chatglm6b2-bs{batch_size}_with_cache.plan\"\n\t    )\n\t    builder = trt.Builder(MyLogger())\n\t    builder.max_threads = os.cpu_count() // 2\n\t    config = builder.create_builder_config()\n\t    profile_list = get_network_profiles(builder)\n\t    for profile in profile_list:\n\t        config.add_optimization_profile(profile)\n\t    # use fp16\n", "    config.flags = 1 << int(trt.BuilderFlag.FP16)\n\t    # disable tf32\n\t    config.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n\t    # use obey precision constraints\n\t    config.flags = config.flags | (1 << int(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS))\n\t    # config.set_memory_pool_limit(MemoryPoolType.WORKSPACE, 2 * 1024 * 1024 * 1024)\n\t    # use prewview features\n\t    preview_features = [\n\t        # PreviewFeature.PROFILE_SHARING_0806,\n\t        PreviewFeature.FASTER_DYNAMIC_SHAPES_0805,\n", "        # PreviewFeature.DISABLE_EXTERNAL_TACTIC_SOURCES_FOR_CORE_0805\n\t    ]\n\t    for feature in preview_features:\n\t        config.set_preview_feature(feature, True)\n\t    config.builder_optimization_level = builder_optimization_level\n\t    # use time cache\n\t    time_cache = b\"\"\n\t    # read time cache\n\t    if use_time_cache:\n\t        if os.path.exists(time_cache_path):\n", "            time_cache = open(time_cache_path, \"rb\").read()\n\t            if time_cache is None:\n\t                time_cache = b\"\"\n\t                print(stylize(\"read time cache failed\", fg(\"red\")))\n\t            else:\n\t                print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n\t        else:\n\t            time_cache = b\"\"\n\t            print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\t        # set time cache\n", "        cache = config.create_timing_cache(time_cache)\n\t        config.set_timing_cache(cache, False)\n\t    # load onnx model\n\t    print(\"loading onnx model from \", onnx_path)\n\t    # network =  builder.create_network(\n\t    #     1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n\t    # )\n\t    # trt_parser = trt.OnnxParser(network, builder.logger)\n\t    # onnx_model = onnx.load(onnx_path)\n\t    # trt_parser.parse(onnx_model)\n", "    _, network, _ = network_from_onnx_path(onnx_path)\n\t    network = get_network_definition(network)\n\t    print(\"=============tensorRT inference config =====================\")\n\t    if builder_optimization_level == 3:\n\t        print(\"==tensorRT engine begin compile, maybe you need wait 10-25 minute ==\")\n\t    elif builder_optimization_level == 5:\n\t        print(\"==tensorRT engine begin compile, maybe you need wait 30-60 minute ==\")\n\t    else:\n\t        print(\"==tensorRT engine begin compile, maybe you need wait a moment ==\")\n\t    # trt_engine = engine_from_network(\n", "    #     (trt_builder, network, onnx_parser),\n\t    #     trt_inference_config\n\t    # )\n\t    serialized_engine = builder.build_serialized_network(network, config)\n\t    if serialized_engine is not None:\n\t        # ‰øùÂ≠òÂºïÊìéÂà∞Êñá‰ª∂\n\t        with open(tensorrt_engine_path, \"wb\") as f:\n\t            f.write(serialized_engine)\n\t        # save_engine(trt_engine, tensorrt_engine_path)\n\t        print(\"==tensorRT engine compile done==\")\n", "    else:\n\t        raise RuntimeError(\"build engine failed\")\n\t    # save time cache\n\t    if use_time_cache and not os.path.exists(time_cache_path):\n\t        time_cache = config.get_timing_cache()\n\t        if time_cache is not None:\n\t            time_cache_data = time_cache.serialize()\n\t            open(time_cache_path, \"wb\").write(time_cache_data)\n\t            print(stylize(f\"save time cache to {time_cache_path}\", fg(\"green\")))"]}
{"filename": "tensorrt_export/test_data_inputs.py", "chunked_list": ["import os\n\timport torch\n\tfrom polygraphy.json import save_json\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\toutput_dir = os.path.join(project_dir, \"output\")\n\tinput_path1 = os.path.join(output_dir, \"pt_input1.pt\")\n\toutput_path1 = os.path.join(output_dir, \"pt_output1.pt\")\n\tinput_dict = torch.jit.load(input_path1)\n\toutput_dict = torch.jit.load(output_path1)\n", "input_ids = input_dict.input_ids.numpy()\n\tposition_ids = input_dict.position_ids.numpy()\n\tdef load_data():\n\t    for _ in range(5):\n\t        yield {\"input_ids\": input_ids, \"position_ids\": position_ids}\n\tinput_data = list(load_data())\n\tsave_json(input_data, \"custom_inputs.json\", description=\"custom inputs\")"]}
{"filename": "tensorrt_export/read_trt_profile.py", "chunked_list": ["import tensorrt as trt\n\timport os\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tmodel_dir = os.path.join(project_dir, \"models\")\n\ttrt_enginge_path = os.path.join(model_dir, \"chatglm6b-bs1-18.5G.plan\")\n\t# Load TensorRT Engine\n\twith open(trt_enginge_path, 'rb') as f, trt.Runtime(trt.Logger(trt.Logger.WARNING)) as runtime:\n\t    engine = runtime.deserialize_cuda_engine(f.read())\n\t# print input and output\n", "print(\"Inputs and Output:\")\n\tn_io = engine.num_io_tensors\n\ttensor_names = []\n\ttensor_modes = []\n\tfor i in range(n_io):\n\t    tensor_name = engine.get_tensor_name(i)\n\t    tensor_names.append(tensor_name)\n\t    tensor_mode = engine.get_tensor_mode(tensor_name)\n\t    tensor_modes.append(tensor_mode)\n\t    tensor_shape = engine.get_tensor_shape(tensor_name)\n", "    if tensor_mode == trt.TensorIOMode.INPUT:\n\t        print(f\"input_name:'{tensor_name}', input_shape: {tensor_shape}\", )\n\t    else:\n\t        print(f\"output_name:'{tensor_name}', input_shape: {tensor_shape}\", )\n\tprint(\"Profile configuration:\")\n\tfor i in range(engine.num_optimization_profiles):\n\t    print('=' * 20) \n\t    print(\" - profile index: \", i)\n\t    for j in range(n_io):\n\t        tensor_name = tensor_names[j]\n", "        tensor_mode = tensor_modes[j]\n\t        if tensor_mode == trt.TensorIOMode.INPUT:\n\t            # profile = engine.get_profile_shape(i, j)\n\t            profile = engine.get_tensor_profile_shape(tensor_name, i)\n\t            print(\"   - input name: \", tensor_name)\n\t            print(\"     min shape: \", profile[0])\n\t            print(\"     opt shape: \", profile[1])\n\t            print(\"     max shape: \", profile[2])\n"]}
{"filename": "tensorrt_export/trt_check_with_past.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\tfrom colored import stylize, fg\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\tsys.path.append(project_dir)\n\t# from kernel.pykernel_no_past import KernelNoPast\n\tfrom kernel.pykernel_with_past import KernelWithPast\n\tdef check_value(pre_value: torch.Tensor, true_value: torch.Tensor, diff=1e-3):\n", "    if pre_value.shape != true_value.shape:\n\t        raise Exception(\"compare shape must be same!\")\n\t    max_diff = (pre_value - true_value).abs_().max().item()\n\t    if max_diff > diff:\n\t        print(stylize(f\"compare diff failed, diff is {max_diff}\", fg(\"red\")))\n\t    else:\n\t        print(stylize(\"compare diff OK!\", fg(\"green\")))\n\t    return max_diff\n\tdef main():\n\t    assert torch.cuda.is_available(), print(\"you must has cuda to run TensorRT\")\n", "    output_dir = os.path.join(project_dir, \"output\")\n\t    model_dir = os.path.join(project_dir, \"models\")\n\t    engine_path1 = os.path.join(model_dir, \"chatglm6b2-bs1_with_cache.plan\")\n\t    input_path = os.path.join(output_dir, \"pt_input2.pt\")\n\t    output_path = os.path.join(output_dir, \"pt_output2.pt\")\n\t    device = torch.device(\"cuda:0\")\n\t    input_dict = torch.jit.load(input_path)\n\t    batch_size = 1\n\t    num_layers = 28\n\t    output_dict = torch.jit.load(output_path)\n", "    input_ids: torch.Tensor = input_dict.input_ids.int().to(device)\n\t    position_ids: torch.Tensor = input_dict.position_ids.int().to(device)\n\t    input_tensors = [input_ids, position_ids]\n\t    for i in range(num_layers):\n\t        input_names = [\n\t            f\"past_key_values.{i}.key\",\n\t            f\"past_key_values.{i}.value\"\n\t        ]\n\t        for name in input_names:\n\t            one_key_value = getattr(input_dict, name).to(device)\n", "            input_tensors.append(one_key_value)\n\t    kernel = KernelWithPast(engine_path1, batch_size, num_layers)\n\t    output_tensors = kernel.forward(tuple(input_tensors))\n\t    # compare output\n\t    max_diff_ = 0\n\t    # compare logits\n\t    logits = output_dict.logits.to(device)\n\t    pred_logits = output_tensors[-1]\n\t    logits_diff = check_value(logits, pred_logits)\n\t    print(\"=\" * 20)\n", "    print(\"compare logits\")\n\t    if logits_diff > max_diff_:\n\t        max_diff_ = logits_diff\n\t    # compare past key values\n\t    for i in range(num_layers):\n\t        present_key_name = f\"present_key_values.{i}.key\"\n\t        present_value_name = f\"present_key_values.{i}.value\"\n\t        true_present_key = getattr(output_dict, present_key_name).to(device)\n\t        true_present_value = getattr(output_dict, present_value_name).to(device)\n\t        pre_present_key = output_tensors[i * 2]\n", "        pre_present_value = output_tensors[i * 2 + 1]\n\t        print(\"=\" * 20)\n\t        print(\"compare: \", present_key_name)\n\t        temp_diff = check_value(pre_present_key, true_present_key)\n\t        if temp_diff > max_diff_:\n\t            max_diff_ = temp_diff\n\t        print(\"=\" * 20)\n\t        print(\"compare: \", present_value_name)\n\t        temp_diff = check_value(pre_present_value, true_present_value)\n\t        if temp_diff > max_diff_:\n", "            max_diff_ = temp_diff\n\t    print(f\"max diff is {max_diff_}\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tensorrt_export/test.py", "chunked_list": ["import onnx\n\tfrom onnx import helper, TensorProto\n\timport onnxruntime as ort\n\timport os\n\timport numpy as np\n\tproject_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\tonnx_path = os.path.join(project_dir, \"output\", \"onnx_output\", \"chatglm_6b.onnx\")\n\tnew_onnx_path = os.path.join(project_dir, \"output\", \"new_onnx_output\", \"chatglm_6b.onnx\")\n\t# Âä†ËΩΩ ONNX Ê®°Âûã\n\tonnx_model = onnx.load(onnx_path)\n", "# Ëé∑ÂèñÊâÄÊúâËäÇÁÇπÂêçÁß∞\n\tnode_names = set(node.name for node in onnx_model.graph.node)\n\t# Ëé∑ÂèñÊâÄÊúâËæìÂÖ•ÂíåËæìÂá∫Âº†ÈáèÂêçÁß∞\n\tinput_names = set(input.name for input in onnx_model.graph.input)\n\toutput_names = set(output.name for output in onnx_model.graph.output)\n\t# Â∞ÜÊâÄÊúâËäÇÁÇπÊ†áËÆ∞‰∏∫ËæìÂá∫\n\tfor node in onnx_model.graph.node:\n\t    for output in node.output:\n\t        # Â¶ÇÊûúËæìÂá∫Âº†Èáè‰∏çÊòØËæìÂÖ•Âº†ÈáèÔºåÂàôÂ∞ÜÂÖ∂Ê†áËÆ∞‰∏∫ËæìÂá∫\n\t        if output not in input_names:\n", "            onnx_model.graph.output.append(helper.make_tensor_value_info(output, TensorProto.UNDEFINED, None))\n\t# ‰øùÂ≠ò‰øÆÊîπÂêéÁöÑ ONNX Ê®°Âûã\n\tonnx.save(\n\t  onnx_model, \n\t  new_onnx_path,\n\t  save_as_external_data=True,\n\t  all_tensors_to_one_file=False\n\t)\n\tsess = ort.InferenceSession(new_onnx_path, providers=[\"CPUExecutionProvider\"])\n\tinput_feed = {\n", "    \"input_ids\": np.random.randint(0, 100, size=(1, 512)).astype(np.int64),\n\t    \"position_ids\": np.random.randint(0, 100, size=(1, 2, 512)).astype(np.int64),\n\t    \"attention_mask\": np.random.randint(0, 1, size=(1, 1, 512, 512)).astype(np.bool_),\n\t    \"past_key_values.0.decoder.key\": np.random.randint(0, 100, size=(1, 1, 32, 128)).astype(np.float32),\n\t    \"past_key_values.0.decoder.value\": np.random.randint(0, 100, size=(1, 1, 32, 128)).astype(np.float32),\n\t}\n\toutput = sess.run(None, input_feed)\n\tprint(output)"]}
{"filename": "tensorrt_export/onnx_trt_compare_fp32.py", "chunked_list": ["#!/usr/bin/env python3\n\t# Template auto-generated by polygraphy [v0.47.1] on 06/05/23 at 12:36:31\n\t# code gen with onnx_trt_compare.sh\n\t# but i edit the code to make it more readable\n\timport tensorrt as trt\n\timport os\n\timport shutil\n\timport numpy as np\n\tfrom polygraphy.logger import G_LOGGER\n\tG_LOGGER.module_severity = {'': G_LOGGER.VERBOSE}\n", "from colored import stylize, fg\n\tfrom polygraphy import constants\n\timport onnx\n\tfrom polygraphy.backend.common import SaveBytes\n\tfrom polygraphy.backend.onnx import modify_outputs, onnx_from_path, ModifyOutputs\n\tfrom polygraphy.backend.onnxrt import OnnxrtRunner, SessionFromOnnx\n\tfrom polygraphy.backend.trt import CreateConfig as CreateTrtConfig, EngineBytesFromNetwork, EngineFromBytes, ModifyNetworkOutputs, NetworkFromOnnxPath, Profile, TrtRunner\n\tfrom polygraphy.common import TensorMetadata\n\tfrom polygraphy.comparator import Comparator, CompareFunc, DataLoader\n\tfrom polygraphy.exception import PolygraphyException\n", "from polygraphy.backend.trt import network_from_onnx_path\n\t# --dir info--\n\tnow_dir = os.path.dirname(os.path.abspath(__file__))\n\tproject_dir = os.path.dirname(now_dir)\n\timport sys\n\tsys.path.append(project_dir)\n\tfrom tensorrt_export.onnx2trt_no_cache import (\n\t    get_network_profiles,\n\t    MyLogger,\n\t)\n", "output_dir = os.path.join(project_dir, \"output\")\n\tnew_onnx_dir = os.path.join(output_dir, \"onnx_output_no_cache_new\")\n\tif not os.path.exists(new_onnx_dir):\n\t    os.mkdir(new_onnx_dir)\n\telse:\n\t    for file in os.listdir(new_onnx_dir):\n\t        os.remove(os.path.join(new_onnx_dir, file))\n\tnew_onnx_dir2 = os.path.join(output_dir, \"onnx_output_no_cache_new2\")\n\tif not os.path.exists(new_onnx_dir2):\n\t    os.mkdir(new_onnx_dir2)\n", "else:\n\t    for file in os.listdir(new_onnx_dir2):\n\t        os.remove(os.path.join(new_onnx_dir2, file))\n\tonnx_path = os.path.join(output_dir, \"onnx_output_no_cache\", \"chatglm2_6b.onnx\")\n\tnew_onnx_path = os.path.join(new_onnx_dir, \"chatglm2_6b.onnx\")\n\tnew_onnx_path2 = os.path.join(new_onnx_dir2, \"chatglm2_6b.onnx\")\n\tmodel_dir = os.path.join(project_dir, \"models\")\n\ttrt_model_path = os.path.join(model_dir, \"model-no-cache-FP32-MarkAll.plan\")\n\ttime_cache_path = os.path.join(output_dir, \"fp32_markAll_no_cache.cache\")\n\tuse_time_cache = True\n", "num_layers = 1\n\t# Data Loader\n\tdtype = np.dtype(np.int32)\n\tdata_loader = DataLoader(\n\t    input_metadata=TensorMetadata()\n\t    .add('input_ids', dtype=dtype, shape=(1, 512))\n\t    .add('position_ids', dtype=np.int32, shape=(1, 512))\n\t)\n\t# load onnx\n\tprint(\"loading onnx model from\", onnx_path)\n", "onnx_model = onnx_from_path(onnx_path)\n\t# this layer will output None in onnxrt\n\tbool_tensor_list = [\"/transformer/encoder/layers.0/mlp/Sigmoid_output_0\"]\n\tfor node in onnx_model.graph.node:\n\t    # print(node.name, node.op_type)\n\t    # this layer is a bool tensor, it will cause error when run TensorRT engine\n\t    if node.op_type == \"Equal\":\n\t        print(\"find bool\", node.name)\n\t        bool_tensor_list.extend(node.output)\n\tinput_list = onnx_model.graph.input\n", "input_names = [i.name for i in input_list]\n\toutput_list = onnx_model.graph.output\n\toutput_names = [o.name for o in output_list]\n\texclude_outputs = input_names + bool_tensor_list\n\t# mark all layers as output for onnx model\n\t# warning this will make the onnx model output all layers with no type and no shape\n\tnew_onnx_model = modify_outputs(\n\t    model=onnx_model,\n\t    outputs=constants.MARK_ALL,\n\t    exclude_outputs=exclude_outputs,\n", ")\n\tnew_output_list = new_onnx_model.graph.output\n\tnew_output_names = [o.name for o in new_output_list]\n\tonnx_input_num = len(new_onnx_model.graph.input)\n\tonnx_output_num = len(new_onnx_model.graph.output)\n\tprint(\"onnx input num:\", onnx_input_num, \"onnx output num:\", onnx_output_num)\n\tonnx.save_model(\n\t    new_onnx_model,\n\t    new_onnx_path,\n\t    save_as_external_data=True,\n", "    all_tensors_to_one_file=False\n\t)\n\t# load onnx_runtime\n\tbuild_onnx_rt_session = SessionFromOnnx(new_onnx_path)\n\t# get onnx runtime output tensor info(name, type, shape)\n\tsess = build_onnx_rt_session()\n\tbool_tensor_list = []\n\tsess_output = sess.get_outputs()\n\tfor node in sess_output:\n\t    if node.type == \"tensor(float)\":\n", "        dtype = onnx.TensorProto.FLOAT\n\t    elif node.type == \"tensor(bool)\":\n\t        print(\"find bool\", node.name)\n\t        bool_tensor_list.append(node.name)\n\t        dtype = onnx.TensorProto.BOOL\n\t    elif node.type == \"tensor(int64)\":\n\t        dtype = onnx.TensorProto.INT64\n\t    else:\n\t        print(\"unknown dtype:\", node.type)\n\t        raise ValueError\n", "    output_tensor = onnx.helper.make_tensor_value_info(\n\t        node.name, dtype, None\n\t    )\n\t    # replace the output tensor\n\t    for i, vi in enumerate(new_onnx_model.graph.output):\n\t        if vi.name == node.name:\n\t            new_onnx_model.graph.output[i].CopyFrom(output_tensor)\n\t# save again\n\tfor file in os.listdir(new_onnx_dir):\n\t    shutil.copy(os.path.join(new_onnx_dir, file), new_onnx_dir2)\n", "onnx.save_model(\n\t    new_onnx_model,\n\t    new_onnx_path2,\n\t    save_as_external_data=True,\n\t    all_tensors_to_one_file=False\n\t)\n\tprint(\"===========onnx model loaded=========================\")\n\t# build trt engine\n\tprint(\"===========building trt engine=========================\")\n\t# prepare trt builder\n", "builder = trt.Builder(MyLogger())\n\tbuilder.max_threads = os.cpu_count() // 2\n\tconfig = builder.create_builder_config()\n\tconfig.flags = config.flags & ~(1 << int(trt.BuilderFlag.TF32))\n\t# read time cache\n\tif use_time_cache:\n\t    if os.path.exists(time_cache_path):\n\t        time_cache = open(time_cache_path, \"rb\").read()\n\t        if time_cache is None:\n\t            time_cache = b\"\"\n", "            print(stylize(\"read time cache failed\", fg(\"red\")))\n\t        else:\n\t            print(stylize(f\"read time cache from {time_cache_path}\", fg(\"green\")))\n\t    else:\n\t        time_cache = b\"\"\n\t        print(stylize(\"time cache will init with empty.\", fg(\"green\")))\n\t    # set time cache\n\t    cache = config.create_timing_cache(time_cache)\n\t    config.set_timing_cache(cache, False)\n\tprofile_list = get_network_profiles(builder, num_layers=num_layers)\n", "for profile in profile_list:\n\t    config.add_optimization_profile(profile)\n\t_b, network, _p = NetworkFromOnnxPath(new_onnx_path2)()\n\t# network = network_from_onnx_path(onnx_path)\n\t# set_network_outputs = ModifyNetworkOutputs(\n\t#     network=network,\n\t#     outputs=constants.MARK_ALL,\n\t#     exclude_outputs=bool_tensor_list\n\t# )\n\t# wo_b, network, _p = set_network_outputs()\n", "network_input_number = network.num_inputs\n\tnetwork_output_number = network.num_outputs\n\tprint(\"TensorRT input num:\", network_input_number, \"TensorRT output num:\", network_output_number)\n\tserialized_engine = builder.build_serialized_network(network, config)\n\tif serialized_engine is not None:\n\t    with open(trt_model_path, \"wb\") as f:\n\t        f.write(serialized_engine)\n\t    # save_engine(trt_engine, tensorrt_engine_path)\n\t    print(\"==tensorRT engine compile done==\")\n\t# save time cache\n", "if use_time_cache and not os.path.exists(time_cache_path):\n\t    time_cache = config.get_timing_cache()\n\t    if time_cache is not None:\n\t        time_cache_data = time_cache.serialize()\n\t        open(time_cache_path, \"wb\").write(time_cache_data)\n\t        print(\n\t            stylize(\n\t                \"save time cache to {}\".format(time_cache_path),\n\t                fg(\"green\")\n\t            )\n", "        )\n\t# profiles = [\n\t#     Profile().add('input_ids:[1,512] position_ids:[1,2,512] attention_mask:[1,1,512,512] past_key_values.0.decorder.key:[0,1,32,128] past_key_values.0.decorder.value', min=[0, 1, 32, 128], opt=[0, 1, 32, 128], max=[0, 1, 32, 128])\n\t# ]\n\t# create_trt_config = CreateTrtConfig(memory_pool_limits=2 * (1024 ** 3), profiles=profile_list)\n\t# build_engine = EngineBytesFromNetwork(set_network_outputs, config=create_trt_config)\n\t# save_engine_bytes = SaveBytes(build_engine, path=trt_model_path)\n\tsave_engine_bytes = SaveBytes(serialized_engine, path=trt_model_path)\n\tdeserialize_engine = EngineFromBytes(save_engine_bytes)\n\tprint(\"===========trt engine build OK=========================\")\n", "# Runners\n\trunners = [\n\t    OnnxrtRunner(build_onnx_rt_session),\n\t    TrtRunner(deserialize_engine),\n\t]\n\t# Runner Execution\n\tresults = Comparator.run(runners, data_loader=data_loader)\n\tsuccess = True\n\t# Accuracy Comparison\n\tcompare_func = CompareFunc.simple(rtol={'': 0.001}, atol={'': 0.001})\n", "success &= bool(Comparator.compare_accuracy(results, compare_func=compare_func))\n\t# Report Results\n\tif not success:\n\t    raise PolygraphyException('FAILED')\n"]}
