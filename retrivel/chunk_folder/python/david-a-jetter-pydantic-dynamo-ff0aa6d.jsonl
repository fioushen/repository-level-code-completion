{"filename": "dodo.py", "chunked_list": ["from doit.action import CmdAction\n\tdef python_version():\n\t    return \"python --version\"\n\tdef poetry_version():\n\t    return \"poetry --version\"\n\tdef poetry_env_info():\n\t    return \"poetry env info\"\n\tinfo = [CmdAction(python_version), CmdAction(poetry_version), CmdAction(poetry_env_info)]\n\tdef install_deps():\n\t    return \"poetry install\"\n", "def task_black():\n\t    return {\n\t        \"actions\": [\n\t            *info,\n\t            CmdAction(install_deps),\n\t            CmdAction(\"poetry run black pydantic_dynamo\"),\n\t            CmdAction(\"poetry run black tests\"),\n\t        ]\n\t    }\n\tdef task_test():\n", "    return {\n\t        \"actions\": [\n\t            *info,\n\t            CmdAction(install_deps),\n\t            CmdAction(\n\t                \"poetry run python -m pytest tests/test_unit \"\n\t                \"--cov=pydantic_dynamo --cov-report xml:unit-coverage.xml\"\n\t            ),\n\t            CmdAction(\"poetry run black --check pydantic_dynamo\"),\n\t            CmdAction(\"poetry run black --check tests\"),\n", "            CmdAction(\"poetry run mypy pydantic_dynamo tests\"),\n\t            CmdAction(\"poetry run flake8 pydantic_dynamo --ignore=E203,W503\"),\n\t            CmdAction(\"poetry run flake8 tests --ignore=E203,W503\"),\n\t            CmdAction('poetry run python -m pytest tests/test_integration -k \"not long_running\"'),\n\t            CmdAction('poetry run python -m pytest tests/test_integration -k \"long_running\"'),\n\t        ],\n\t        \"verbosity\": 2,\n\t    }\n\tdef task_build():\n\t    return {\n", "        \"actions\": [CmdAction(\"poetry build\")],\n\t        \"task_dep\": [\"test\"],\n\t        \"verbosity\": 2,\n\t    }\n\tdef task_publish():\n\t    return {\n\t        \"actions\": [CmdAction(\"poetry publish\")],\n\t        \"task_dep\": [\"build\"],\n\t        \"verbosity\": 2,\n\t    }\n"]}
{"filename": "tests/models.py", "chunked_list": ["from datetime import date, time, datetime\n\tfrom enum import Enum\n\tfrom typing import Optional, Dict, Any, List, Set\n\tfrom pydantic import BaseModel\n\tclass ExtraModel(BaseModel):\n\t    class Config:\n\t        extra = \"allow\"\n\tclass FieldModel(BaseModel):\n\t    test_field: str\n\t    failures: Optional[int]\n", "class ComposedFieldModel(BaseModel):\n\t    composed: FieldModel\n\t    test_field: str\n\t    failures: Optional[int]\n\tclass CountEnum(Enum):\n\t    One = \"one\"\n\t    Two = \"two\"\n\t    Three = \"three\"\n\tclass Example(BaseModel):\n\t    dict_field: Dict[str, Any]\n", "    model_field: FieldModel\n\t    list_field: List[Any]\n\t    set_field: Set[Any]\n\t    date_field: date\n\t    time_field: time\n\t    datetime_field: datetime\n\t    enum_field: CountEnum\n\t    int_field: int\n\t    optional_field: Optional[int]\n"]}
{"filename": "tests/factories.py", "chunked_list": ["import random\n\tfrom datetime import date, time, timezone\n\tfrom typing import Iterable, TypeVar, Dict\n\timport factory\n\tfrom boto3.dynamodb.conditions import Attr\n\tfrom pydantic_dynamo.models import UpdateCommand, PartitionedContent\n\tfrom faker import Faker\n\tfrom pydantic_dynamo.utils import UpdateItemArguments\n\tfrom tests.models import Example, FieldModel, CountEnum\n\tfake = Faker()\n", "T = TypeVar(\"T\")\n\tdef random_element(items: Iterable[T]) -> T:\n\t    return random.choice([el for el in items])\n\tdef boto_exception(code: str) -> Exception:\n\t    response = {\"Error\": {\"Code\": code}}\n\t    ex = Exception()\n\t    ex.response = response  # type: ignore[attr-defined]\n\t    return ex\n\tclass UpdateCommandFactory(factory.Factory):\n\t    class Meta:\n", "        model = UpdateCommand\n\t    set_commands = factory.Faker(\"pydict\")\n\t    increment_attrs = factory.LazyFunction(lambda: {fake.bs(): _ for _ in range(5)})\n\tclass UpdateItemArgumentsFactory(factory.Factory):\n\t    class Meta:\n\t        model = UpdateItemArguments\n\t    update_expression = factory.Faker(\"bs\")\n\t    condition_expression = factory.LazyFunction(lambda: Attr(fake.bothify()).eq(fake.bothify()))\n\t    attribute_names = factory.LazyFunction(\n\t        lambda: {fake.bothify(): fake.bothify() for _ in range(3)}\n", "    )\n\t    attribute_values = factory.LazyFunction(lambda: fake.pydict())\n\tclass FieldModelFactory(factory.Factory):\n\t    class Meta:\n\t        model = FieldModel\n\t    test_field = factory.Faker(\"bs\")\n\t    failures = factory.Faker(\"pyint\")\n\tclass ExampleFactory(factory.Factory):\n\t    class Meta:\n\t        model = Example\n", "    dict_field = factory.LazyFunction(lambda: {fake.bothify(): FieldModel(test_field=fake.bs())})\n\t    model_field = factory.SubFactory(FieldModelFactory)\n\t    list_field = factory.List((fake.bs() for _ in range(5)))\n\t    set_field = factory.LazyFunction(lambda: {fake.bs() for _ in range(5)})\n\t    date_field = factory.LazyFunction(lambda: date.fromisoformat(fake.date()))\n\t    time_field = factory.LazyFunction(lambda: time.fromisoformat(fake.time()))\n\t    datetime_field = factory.Faker(\"date_time\", tzinfo=timezone.utc)\n\t    enum_field = factory.LazyFunction(lambda: random_element(CountEnum))\n\t    int_field = factory.Faker(\"pyint\")\n\tclass ExamplePartitionedContentFactory(factory.Factory):\n", "    class Meta:\n\t        model = PartitionedContent[Example]\n\t    partition_ids = factory.List((fake.bothify() for _ in range(2)))\n\t    content_ids = factory.List((fake.bothify() for _ in range(2)))\n\t    item = factory.SubFactory(ExampleFactory)\n\tdef example_content_to_db_item(\n\t    partition_key: str,\n\t    partition_prefix: str,\n\t    partition_name: str,\n\t    sort_key: str,\n", "    content_type: str,\n\t    example: PartitionedContent[Example],\n\t) -> Dict:\n\t    db_item = {\n\t        partition_key: \"#\".join((partition_prefix, partition_name, *example.partition_ids)),\n\t        sort_key: \"#\".join((content_type, *example.content_ids)),\n\t        \"_object_version\": example.current_version,\n\t        **example.item.dict(),\n\t    }\n\t    if expiry := example.expiry:\n", "        db_item[\"_ttl\"] = int(expiry.timestamp())\n\t    return db_item\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_unit/test_repository.py", "chunked_list": ["import random\n\tfrom datetime import datetime, timezone\n\tfrom unittest.mock import patch, MagicMock\n\tfrom faker import Faker\n\tfrom pydantic_dynamo.models import PartitionedContent, UpdateCommand, FilterCommand\n\tfrom pydantic_dynamo.repository import (\n\t    DynamoRepository,\n\t)\n\tfrom pydantic_dynamo.utils import clean_dict\n\tfrom tests.models import ExtraModel, FieldModel, ComposedFieldModel, CountEnum\n", "from tests.factories import UpdateItemArgumentsFactory\n\tfake = Faker()\n\tdef _random_enum():\n\t    return random.choice([s for s in CountEnum])\n\t@patch(\"pydantic_dynamo.repository.Session\")\n\tdef test_dynamo_repo_build(session_cls):\n\t    table = MagicMock()\n\t    session = MagicMock()\n\t    session_cls.return_value = session\n\t    resource = MagicMock()\n", "    session.resource.return_value = resource\n\t    resource.Table.return_value = table\n\t    table_name = fake.bs()\n\t    item_class = ExtraModel\n\t    partition = fake.bs()\n\t    partition_key = fake.bs()\n\t    content_type = fake.bs()\n\t    repo = DynamoRepository[ExtraModel].build(\n\t        table_name, item_class, partition, partition_key, content_type\n\t    )\n", "    assert repo._item_class == item_class\n\t    assert repo._partition_prefix == partition\n\t    assert repo._partition_name == partition_key\n\t    assert repo._content_type == content_type\n\t    assert repo._table == table\n\t    assert session.resource.call_args[0] == (\"dynamodb\",)\n\t    assert resource.Table.call_args[0] == (f\"{table_name}\",)\n\t@patch(\"pydantic_dynamo.repository.internal_timestamp\")\n\tdef test_dynamo_repo_put(internal_timestamp):\n\t    now = datetime.now(tz=timezone.utc)\n", "    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\t    partition = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_ids = [fake.bs()]\n\t    partition_type = fake.bs()\n\t    content_ids = [fake.bs()]\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    repo = DynamoRepository[ExtraModel](\n", "        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_type,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n", "    item_dict = fake.pydict()\n\t    item = ExtraModel(**item_dict)\n\t    content = PartitionedContent(partition_ids=partition_ids, content_ids=content_ids, item=item)\n\t    repo.put(content)\n\t    assert table.put_item.call_args[1] == {\n\t        \"Item\": {\n\t            partition_key: f\"{partition}#{partition_type}#{partition_ids[0]}\",\n\t            sort_key: f\"{content_type}#{content_ids[0]}\",\n\t            \"_object_version\": 1,\n\t            \"_timestamp\": now.isoformat(),\n", "            **clean_dict(item_dict),\n\t        }\n\t    }\n\t@patch(\"pydantic_dynamo.repository.internal_timestamp\")\n\tdef test_dynamo_repo_put_batch(internal_timestamp):\n\t    now = datetime.now(tz=timezone.utc)\n\t    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\t    partition = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_ids = [fake.bs()]\n", "    partition_type = fake.bs()\n\t    content_ids = [fake.bs()]\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    writer = MagicMock()\n\t    table.batch_writer.return_value.__enter__.return_value = writer\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=partition,\n", "        partition_name=partition_type,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    item_dict1 = fake.pydict()\n\t    item_dict2 = fake.pydict()\n", "    contents = (\n\t        PartitionedContent(\n\t            partition_ids=partition_ids, content_ids=content_ids, item=ExtraModel(**item_dict1)\n\t        ),\n\t        PartitionedContent(\n\t            partition_ids=partition_ids, content_ids=content_ids, item=ExtraModel(**item_dict2)\n\t        ),\n\t    )\n\t    repo.put_batch(contents)\n\t    assert writer.put_item.call_args_list == [\n", "        (\n\t            (),\n\t            {\n\t                \"Item\": {\n\t                    partition_key: f\"{partition}#{partition_type}#{partition_ids[0]}\",\n\t                    sort_key: f\"{content_type}#{content_ids[0]}\",\n\t                    \"_object_version\": 1,\n\t                    \"_timestamp\": now.isoformat(),\n\t                    **clean_dict(item_dict1),\n\t                }\n", "            },\n\t        ),\n\t        (\n\t            (),\n\t            {\n\t                \"Item\": {\n\t                    partition_key: f\"{partition}#{partition_type}#{partition_ids[0]}\",\n\t                    sort_key: f\"{content_type}#{content_ids[0]}\",\n\t                    \"_object_version\": 1,\n\t                    \"_timestamp\": now.isoformat(),\n", "                    **clean_dict(item_dict2),\n\t                }\n\t            },\n\t        ),\n\t    ]\n\tdef test_dynamo_repo_get():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    table = MagicMock()\n", "    item_dict = fake.pydict()\n\t    table.get_item.return_value = {\"Item\": item_dict}\n\t    partition_id = [fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n", "        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = repo.get(partition_id, content_id)\n\t    assert actual == ExtraModel(**item_dict)\n\t    assert table.get_item.call_args == (\n", "        (),\n\t        {\n\t            \"Key\": {\n\t                partition_key: f\"{partition}#{partition_name}#{partition_id[0]}\",\n\t                sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n\t            }\n\t        },\n\t    )\n\tdef test_dynamo_repo_get_batch():\n\t    partition = fake.bothify()\n", "    partition_name = fake.bothify()\n\t    content_type = fake.bothify()\n\t    table_name = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    resource = MagicMock()\n\t    item_1 = fake.pydict()\n\t    item_2 = fake.pydict()\n\t    item_3 = fake.pydict()\n\t    unprocessed = [{fake.bothify(): fake.bothify()}]\n", "    resource.batch_get_item.side_effect = [\n\t        {\"Responses\": {table_name: [item_1]}, \"UnprocessedKeys\": unprocessed},\n\t        {\"Responses\": {table_name: [item_2]}},\n\t        {\"Responses\": {table_name: [item_3]}},\n\t    ]\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n", "        table_name=table_name,\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=MagicMock(),\n\t        resource=resource,\n\t    )\n\t    request_ids = [([fake.bothify()], [fake.bothify()]) for _ in range(120)]\n\t    items = repo.get_batch(request_ids)\n\t    assert items == [ExtraModel(**item_1), ExtraModel(**item_2), ExtraModel(**item_3)]\n\t    assert resource.batch_get_item.call_args_list == [\n", "        (\n\t            (),\n\t            {\n\t                \"RequestItems\": {\n\t                    table_name: {\n\t                        \"Keys\": [\n\t                            {\n\t                                partition_key: f\"{partition}#{partition_name}#{rid[0][0]}\",\n\t                                sort_key: f\"{content_type}#{rid[1][0]}\",\n\t                            }\n", "                            for rid in request_ids[:100]\n\t                        ]\n\t                    }\n\t                }\n\t            },\n\t        ),\n\t        ((), {\"RequestItems\": {table_name: {\"Keys\": unprocessed}}}),\n\t        (\n\t            (),\n\t            {\n", "                \"RequestItems\": {\n\t                    table_name: {\n\t                        \"Keys\": [\n\t                            {\n\t                                partition_key: f\"{partition}#{partition_name}#{rid[0][0]}\",\n\t                                sort_key: f\"{content_type}#{rid[1][0]}\",\n\t                            }\n\t                            for rid in request_ids[100:]\n\t                        ]\n\t                    }\n", "                }\n\t            },\n\t        ),\n\t    ]\n\tdef test_dynamo_repo_get_none_inputs():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n", "    table = MagicMock()\n\t    item_dict = fake.pydict()\n\t    table.get_item.return_value = {\"Item\": item_dict}\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n", "        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = repo.get(None, None)\n\t    assert actual == ExtraModel(**item_dict)\n\t    assert table.get_item.call_args == (\n\t        (),\n\t        {\n\t            \"Key\": {\n", "                partition_key: f\"{partition}#{partition_name}#\",\n\t                sort_key: f\"{content_type}#\",\n\t            }\n\t        },\n\t    )\n\tdef test_dynamo_repo_list():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n", "    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    item_dict = fake.pydict()\n\t    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n", "        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    actual = list(repo.list(partition_id, content_id, ascending, limit))\n", "    assert actual == [ExtraModel(**item_dict)]\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert kwargs[\"Limit\"] == limit\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert (\n\t        expression._values[0]._values[1]\n", "        == f\"{partition}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n\t    )\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n\tdef test_dynamo_repo_list_none_inputs():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n", "    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    item_dict = fake.pydict()\n\t    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n", "        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    actual = list(repo.list(None, None, ascending, limit))\n\t    assert actual == [ExtraModel(**item_dict)]\n\t    args, kwargs = table.query.call_args\n", "    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert kwargs[\"Limit\"] == limit\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n", "def test_dynamo_repo_list_no_ids():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    item_dict = fake.pydict()\n\t    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\t    repo = DynamoRepository[ExtraModel](\n", "        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n", "    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    actual = list(repo.list([], [], ascending, limit))\n\t    assert actual == [ExtraModel(**item_dict)]\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert kwargs[\"Limit\"] == limit\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n", "    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\tdef test_dynamo_repo_list_with_filter():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n", "    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    item_dict = {\"test_field\": fake.bs()}\n\t    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\t    partition_id = [fake.bs()]\n\t    content_id = [fake.bs()]\n\t    repo = DynamoRepository[FieldModel](\n\t        item_class=FieldModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n", "        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    filters = FilterCommand(not_exists={\"failures\"})\n", "    actual = list(repo.list(partition_id, content_id, ascending, limit, filters))\n\t    assert actual == [FieldModel(**item_dict)]\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert \"Limit\" not in kwargs\n\t    filter_expression = kwargs[\"FilterExpression\"]\n\t    assert filter_expression.expression_operator == \"attribute_not_exists\"\n\t    assert filter_expression._values[0].name == \"failures\"\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n", "    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#{partition_id[0]}\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}\"\n\tdef test_dynamo_repo_list_between():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n", "    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    item_dict = fake.pydict()\n\t    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_start = [fake.bs(), fake.bs()]\n\t    content_end = [fake.bs(), fake.bs()]\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n", "        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = list(repo.list_between(partition_id, content_start, content_end))\n", "    assert actual == [ExtraModel(**item_dict)]\n\t    args, kwargs = table.query.call_args\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert (\n\t        expression._values[0]._values[1]\n\t        == f\"{partition}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n\t    )\n", "    assert expression._values[1].expression_operator == \"BETWEEN\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert (\n\t        expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}#{content_start[1]}\"\n\t    )\n\t    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}#{content_end[1]}\"\n\tdef test_dynamo_repo_list_between_none_inputs():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n", "    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    item_dict = fake.pydict()\n\t    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n", "        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = list(repo.list_between(None, None, None))\n\t    assert actual == [ExtraModel(**item_dict)]\n\t    args, kwargs = table.query.call_args\n\t    expression = kwargs[\"KeyConditionExpression\"]\n", "    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\tdef test_dynamo_repo_list_between_with_filter():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n", "    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    item_dict = {\"test_field\": fake.bs()}\n\t    table.query.return_value = {\"Items\": [item_dict], \"Count\": fake.pyint()}\n\t    partition_id = [fake.bs()]\n\t    content_start = [fake.bs()]\n\t    content_end = [fake.bs()]\n\t    repo = DynamoRepository[FieldModel](\n", "        item_class=FieldModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n", "    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    filters = FilterCommand(not_exists={\"failures\"})\n\t    actual = list(\n\t        repo.list_between(partition_id, content_start, content_end, ascending, limit, filters)\n\t    )\n\t    assert actual == [FieldModel(**item_dict)]\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert \"Limit\" not in kwargs\n", "    filter_expression = kwargs[\"FilterExpression\"]\n\t    assert filter_expression.expression_operator == \"attribute_not_exists\"\n\t    assert filter_expression._values[0].name == \"failures\"\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#{partition_id[0]}\"\n\t    assert expression._values[1].expression_operator == \"BETWEEN\"\n\t    assert expression._values[1]._values[0].name == sort_key\n", "    assert expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}\"\n\t    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}\"\n\tdef test_content_get_repo_no_items():\n\t    table = MagicMock()\n\t    table.get_item.return_value = {\"Not_Items\": []}\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=ExtraModel,\n\t        partition_prefix=fake.bs(),\n\t        partition_name=fake.bs(),\n\t        content_type=fake.bs(),\n", "        table_name=fake.bs(),\n\t        partition_key=fake.bs(),\n\t        sort_key=fake.bs(),\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = repo.get(fake.bs(), fake.bs())\n\t    assert actual is None\n\t@patch(\"pydantic_dynamo.repository.build_update_args_for_command\")\n\tdef test_dynamo_repo_update(build_update_args):\n", "    update_args = UpdateItemArgumentsFactory()\n\t    build_update_args.return_value = update_args\n\t    partition = fake.bothify()\n\t    partition_name = fake.bothify()\n\t    content_type = fake.bothify()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    repo = DynamoRepository[ComposedFieldModel](\n\t        item_class=ComposedFieldModel,\n", "        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    partition_id = [fake.bothify()]\n", "    content_id = [fake.bothify(), fake.bothify()]\n\t    current_version = fake.pyint()\n\t    command = UpdateCommand(\n\t        set_commands={\n\t            \"test_field\": fake.bs(),\n\t            \"composed\": {\"test_field\": fake.bs(), \"failures\": None},\n\t        },\n\t        increment_attrs={\"failures\": 1},\n\t        current_version=current_version,\n\t    )\n", "    repo.update(partition_id, content_id, command)\n\t    update_a, update_k = table.update_item.call_args\n\t    assert update_k.pop(\"Key\") == {\n\t        partition_key: f\"{partition}#{partition_name}#{partition_id[0]}\",\n\t        sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n\t    }\n\t    assert update_k.pop(\"ConditionExpression\") == update_args.condition_expression\n\t    assert update_k.pop(\"UpdateExpression\") == update_args.update_expression\n\t    assert update_k.pop(\"ExpressionAttributeNames\") == update_args.attribute_names\n\t    assert update_k.pop(\"ExpressionAttributeValues\") == update_args.attribute_values\n", "    assert len(update_k) == 0\n\tdef test_dynamo_repo_delete():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    items = [\n\t        ExtraModel(**{partition_key: fake.bs(), sort_key: fake.bs()}).dict() for _ in range(11)\n", "    ]\n\t    table.query.return_value = {\"Items\": items, \"Count\": fake.pyint()}\n\t    writer = MagicMock()\n\t    table.batch_writer.return_value.__enter__.return_value = writer\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=FieldModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n", "        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    repo.delete(partition_id, content_id)\n\t    args, kwargs = table.query.call_args\n\t    expression = kwargs[\"KeyConditionExpression\"]\n", "    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert (\n\t        expression._values[0]._values[1]\n\t        == f\"{partition}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n\t    )\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n", "    assert writer.delete_item.call_args_list == [\n\t        (\n\t            (),\n\t            {\n\t                \"Key\": {\n\t                    partition_key: item[partition_key],\n\t                    sort_key: item[sort_key],\n\t                }\n\t            },\n\t        )\n", "        for item in items\n\t    ]\n\tdef test_dynamo_repo_delete_none_inputs():\n\t    partition = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    items = [\n", "        ExtraModel(**{partition_key: fake.bs(), sort_key: fake.bs()}).dict() for _ in range(11)\n\t    ]\n\t    table.query.return_value = {\"Items\": items, \"Count\": fake.pyint()}\n\t    writer = MagicMock()\n\t    table.batch_writer.return_value.__enter__.return_value = writer\n\t    repo = DynamoRepository[ExtraModel](\n\t        item_class=FieldModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n", "        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    repo.delete(None, None)\n\t    args, kwargs = table.query.call_args\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n", "    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\t    assert writer.delete_item.call_args_list == [\n\t        (\n\t            (),\n\t            {\n", "                \"Key\": {\n\t                    partition_key: item[partition_key],\n\t                    sort_key: item[sort_key],\n\t                }\n\t            },\n\t        )\n\t        for item in items\n\t    ]\n"]}
{"filename": "tests/test_unit/__init__.py", "chunked_list": []}
{"filename": "tests/test_unit/test_utils.py", "chunked_list": ["import random\n\tfrom copy import deepcopy\n\tfrom datetime import datetime, timezone, date, time\n\tfrom typing import Dict\n\tfrom unittest.mock import patch, MagicMock, AsyncMock\n\timport pytest\n\tfrom faker import Faker\n\tfrom pydantic_dynamo.exceptions import RequestObjectStateError\n\tfrom pydantic_dynamo.models import UpdateCommand, FilterCommand\n\tfrom pydantic_dynamo.utils import (\n", "    chunks,\n\t    utc_now,\n\t    get_error_code,\n\t    clean_dict,\n\t    build_update_args_for_command,\n\t    internal_timestamp,\n\t    execute_update_item,\n\t    validate_command_for_schema,\n\t    validate_filters_for_schema,\n\t)\n", "from tests.factories import (\n\t    UpdateCommandFactory,\n\t    UpdateItemArgumentsFactory,\n\t    ExampleFactory,\n\t    FieldModelFactory,\n\t)\n\tfrom tests.models import ExtraModel, CountEnum, Example, FieldModel\n\tfake = Faker()\n\tdef _random_enum():\n\t    return random.choice([s for s in CountEnum])\n", "@patch(\"pydantic_dynamo.utils.datetime\")\n\tdef test_utc_now(dt_mock):\n\t    now = datetime.now()\n\t    dt_mock.now.return_value = now\n\t    actual = utc_now()\n\t    assert dt_mock.now.call_args == ((), {\"tz\": timezone.utc})\n\t    assert actual == now\n\t@patch(\"pydantic_dynamo.utils.utc_now\")\n\tdef test_internal_timestamp(utc_now):\n\t    now = fake.date_time()\n", "    utc_now.return_value = now\n\t    actual = internal_timestamp()\n\t    assert actual == {\"_timestamp\": now.isoformat()}\n\tdef test_get_error_code():\n\t    ex = RuntimeError()\n\t    code = fake.bs()\n\t    ex.response = {\"Error\": {\"Code\": code}}\n\t    actual = get_error_code(ex)\n\t    assert actual == code\n\tdef test_get_error_code_no_code():\n", "    ex = RuntimeError()\n\t    code = fake.bs()\n\t    ex.response = {\"Error\": {\"Zode\": code}}\n\t    actual = get_error_code(ex)\n\t    assert actual is None\n\tdef test_get_error_code_no_error():\n\t    ex = RuntimeError()\n\t    code = fake.bs()\n\t    ex.response = {\"Zerror\": {\"Zode\": code}}\n\t    actual = get_error_code(ex)\n", "    assert actual is None\n\tdef test_get_error_code_no_response():\n\t    ex = RuntimeError()\n\t    code = fake.bs()\n\t    ex.zesponse = {\"Error\": {\"Code\": code}}\n\t    actual = get_error_code(ex)\n\t    assert actual is None\n\tdef test_chunks_lt_size():\n\t    items = [fake.bothify() for _ in range(2)]\n\t    chunked = list(chunks(items, 3))\n", "    assert len(chunked) == 1\n\t    assert len(chunked[0]) == 2\n\tdef test_chunks_gt_size():\n\t    items = [fake.bothify() for _ in range(2)]\n\t    chunked = list(chunks(items, 1))\n\t    assert len(chunked) == 2\n\t    for chunk in chunked:\n\t        assert len(chunk) == 1\n\tdef test_chunks_gt_size_partial():\n\t    items = [fake.bothify() for _ in range(25)]\n", "    chunked = list(chunks(items, 10))\n\t    assert len(chunked) == 3\n\t    assert len(chunked[0]) == 10\n\t    assert len(chunked[1]) == 10\n\t    assert len(chunked[2]) == 5\n\tasync def test_execute_update_item():\n\t    table = AsyncMock()\n\t    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n\t    args = UpdateItemArgumentsFactory()\n\t    await execute_update_item(table, key, args)\n", "    assert table.update_item.call_args == (\n\t        (),\n\t        {\n\t            \"Key\": key,\n\t            \"UpdateExpression\": args.update_expression,\n\t            \"ExpressionAttributeNames\": args.attribute_names,\n\t            \"ExpressionAttributeValues\": args.attribute_values,\n\t            \"ConditionExpression\": args.condition_expression,\n\t        },\n\t    )\n", "async def test_execute_update_item_condition_none():\n\t    table = AsyncMock()\n\t    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n\t    args = UpdateItemArgumentsFactory(condition_expression=None)\n\t    await execute_update_item(table, key, args)\n\t    assert table.update_item.call_args == (\n\t        (),\n\t        {\n\t            \"Key\": key,\n\t            \"UpdateExpression\": args.update_expression,\n", "            \"ExpressionAttributeNames\": args.attribute_names,\n\t            \"ExpressionAttributeValues\": args.attribute_values,\n\t        },\n\t    )\n\t@patch(\"pydantic_dynamo.utils.get_error_code\")\n\tasync def test_execute_update_conditional_check_failed(get_error_code):\n\t    error_code = \"ConditionalCheckFailedException\"\n\t    get_error_code.return_value = error_code\n\t    table = MagicMock()\n\t    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n", "    args = UpdateItemArgumentsFactory()\n\t    db_ex = Exception(fake.bs())\n\t    table.update_item.side_effect = db_ex\n\t    with pytest.raises(RequestObjectStateError) as ex:\n\t        await execute_update_item(table, key, args)\n\t    assert str(key) in str(ex)\n\t    assert get_error_code.call_args == ((db_ex,), {})\n\t@patch(\"pydantic_dynamo.utils.get_error_code\")\n\tasync def test_execute_update_some_other_exception(get_error_code):\n\t    error_code = fake.bs()\n", "    get_error_code.return_value = error_code\n\t    table = MagicMock()\n\t    key = {fake.bothify(): fake.bothify() for _ in range(2)}\n\t    args = UpdateItemArgumentsFactory()\n\t    db_ex = Exception(fake.bs())\n\t    table.update_item.side_effect = db_ex\n\t    with pytest.raises(Exception) as ex:\n\t        await execute_update_item(table, key, args)\n\t    assert ex.value == db_ex\n\tdef test_clean_dict():\n", "    nested_composed = ExampleFactory()\n\t    composed = ExampleFactory()\n\t    dict_input = {\n\t        \"nested\": {\n\t            \"nested_dict\": fake.pydict(),\n\t            \"nested_enum\": _random_enum(),\n\t            \"nested_list_of_dict\": [fake.pydict() for _ in range(3)],\n\t            \"nested_datetime\": datetime.now(tz=timezone.utc),\n\t            \"nested_list_of_dt\": [datetime.now(tz=timezone.utc) for _ in range(3)],\n\t            \"nested_list_of_date\": [date.today() for _ in range(3)],\n", "            \"nested_list_of_time\": [time() for _ in range(3)],\n\t            \"nested_composed\": nested_composed,\n\t        },\n\t        \"dict\": fake.pydict(),\n\t        \"enum\": _random_enum(),\n\t        \"list_of_dict\": [fake.pydict() for _ in range(3)],\n\t        \"datetime\": datetime.now(tz=timezone.utc),\n\t        \"list_of_dt\": [datetime.now(tz=timezone.utc) for _ in range(3)],\n\t        \"list_of_date\": [date.today() for _ in range(3)],\n\t        \"list_of_time\": [time() for _ in range(3)],\n", "        \"composed\": composed,\n\t    }\n\t    og = deepcopy(dict_input)\n\t    cleaned = clean_dict(dict_input)\n\t    assert cleaned[\"nested\"][\"nested_dict\"].keys() == og[\"nested\"][\"nested_dict\"].keys()\n\t    assert cleaned[\"nested\"][\"nested_enum\"] == og[\"nested\"][\"nested_enum\"].value\n\t    assert len(cleaned[\"nested\"][\"nested_list_of_dict\"]) == len(og[\"nested\"][\"nested_list_of_dict\"])\n\t    assert cleaned[\"nested\"][\"nested_datetime\"] == og[\"nested\"][\"nested_datetime\"].isoformat()\n\t    assert cleaned[\"nested\"][\"nested_list_of_dt\"] == [\n\t        dt.isoformat() for dt in og[\"nested\"][\"nested_list_of_dt\"]\n", "    ]\n\t    assert cleaned[\"nested\"][\"nested_list_of_date\"] == [\n\t        dt.isoformat() for dt in og[\"nested\"][\"nested_list_of_date\"]\n\t    ]\n\t    assert cleaned[\"nested\"][\"nested_list_of_time\"] == [\n\t        t.isoformat() for t in og[\"nested\"][\"nested_list_of_time\"]\n\t    ]\n\t    nested_composed_expected = _example_to_expected(nested_composed)\n\t    assert sorted(cleaned[\"nested\"][\"nested_composed\"].pop(\"set_field\")) == sorted(\n\t        nested_composed_expected.pop(\"set_field\")\n", "    )\n\t    assert cleaned[\"nested\"][\"nested_composed\"] == nested_composed_expected\n\t    assert cleaned[\"dict\"].keys() == og[\"dict\"].keys()\n\t    assert cleaned[\"enum\"] == og[\"enum\"].value\n\t    assert len(cleaned[\"list_of_dict\"]) == len(og[\"list_of_dict\"])\n\t    assert cleaned[\"datetime\"] == og[\"datetime\"].isoformat()\n\t    assert cleaned[\"list_of_dt\"] == [dt.isoformat() for dt in og[\"list_of_dt\"]]\n\t    assert cleaned[\"list_of_date\"] == [dt.isoformat() for dt in og[\"list_of_date\"]]\n\t    assert cleaned[\"list_of_time\"] == [t.isoformat() for t in og[\"list_of_time\"]]\n\t    # Need to explicitly assert on set_field because it's not reliably sorted\n", "    expected = _example_to_expected(composed)\n\t    assert sorted(cleaned[\"composed\"].pop(\"set_field\")) == sorted(expected.pop(\"set_field\"))\n\t    assert cleaned[\"composed\"] == expected\n\t# dict_field only supports strings or instances of FieldModel\n\tdef _example_to_expected(example: Example) -> Dict:\n\t    return {\n\t        \"dict_field\": {\n\t            k: v.dict() if isinstance(v, FieldModel) else v for k, v in example.dict_field.items()\n\t        },\n\t        \"model_field\": {\n", "            \"test_field\": example.model_field.test_field,\n\t            \"failures\": example.model_field.failures,\n\t        },\n\t        \"list_field\": [v.isoformat() if isinstance(v, datetime) else v for v in example.list_field],\n\t        \"set_field\": [v.isoformat() if isinstance(v, datetime) else v for v in example.set_field],\n\t        \"date_field\": example.date_field.isoformat(),\n\t        \"time_field\": example.time_field.isoformat(),\n\t        \"datetime_field\": example.datetime_field.isoformat(),\n\t        \"enum_field\": example.enum_field.value,\n\t        \"int_field\": example.int_field,\n", "        \"optional_field\": example.optional_field,\n\t    }\n\tdef test_clean_dict_pydantic_base_model():\n\t    unclean = {\n\t        \"items\": [ExtraModel(a=\"a\", b=\"b\"), ExtraModel(c=ExtraModel(c=\"c\"), d=[ExtraModel(d=\"d\")])]\n\t    }\n\t    cleaned = clean_dict(unclean)\n\t    assert cleaned[\"items\"] == [{\"a\": \"a\", \"b\": \"b\"}, {\"c\": {\"c\": \"c\"}, \"d\": [{\"d\": \"d\"}]}]\n\t@patch(\"pydantic_dynamo.utils.utc_now\")\n\tdef test_build_update_item_arguments(utc_now):\n", "    now = datetime.now(tz=timezone.utc)\n\t    utc_now.return_value = now\n\t    some_string = fake.bs()\n\t    some_enum = _random_enum()\n\t    some_example = ExampleFactory()\n\t    incr_attr = fake.bs()\n\t    incr_attr2 = fake.bs()\n\t    incr = fake.pyint()\n\t    current_version = fake.pyint()\n\t    el1 = fake.bs()\n", "    el2 = fake.bs()\n\t    el3 = fake.bs()\n\t    expiry = datetime.now(tz=timezone.utc)\n\t    command = UpdateCommand(\n\t        current_version=current_version,\n\t        set_commands={\n\t            \"some_attr\": some_string,\n\t            \"some_enum\": some_enum,\n\t            \"some_object\": some_example.dict(),\n\t        },\n", "        increment_attrs={incr_attr: incr, incr_attr2: 1},\n\t        append_attrs={\"some_list\": [el1, el2], \"some_list_2\": [el3]},\n\t        expiry=expiry,\n\t    )\n\t    key = {\"partition_key\": fake.bs(), \"sort_key\": fake.bs()}\n\t    update_args = build_update_args_for_command(command, key=key)\n\t    actual_condition = update_args.condition_expression\n\t    assert actual_condition.expression_operator == \"AND\"\n\t    assert actual_condition._values[1]._values[0].name == \"_object_version\"\n\t    assert actual_condition._values[1]._values[1] == current_version\n", "    assert actual_condition._values[0]._values[1].expression_operator == \"=\"\n\t    assert actual_condition._values[0]._values[1]._values[0].name == \"sort_key\"\n\t    assert actual_condition._values[0]._values[1]._values[1] == key[\"sort_key\"]\n\t    assert actual_condition._values[0]._values[0].expression_operator == \"=\"\n\t    assert actual_condition._values[0]._values[0]._values[0].name == \"partition_key\"\n\t    assert actual_condition._values[0]._values[0]._values[1] == key[\"partition_key\"]\n\t    assert (\n\t        update_args.update_expression == \"SET #att0 = :val0, #att1 = :val1, \"\n\t        \"#att2.#att3 = :val2, #att2.#att4 = :val3, \"\n\t        \"#att2.#att5 = :val4, #att2.#att6 = :val5, \"\n", "        \"#att2.#att7 = :val6, #att2.#att8 = :val7, \"\n\t        \"#att2.#att9 = :val8, #att2.#att10 = :val9, \"\n\t        \"#att2.#att11 = :val10, #att2.#att12 = :val11, \"\n\t        \"#att13 = :val12, #att14 = :val13, \"\n\t        \"#att15 = if_not_exists(#att15, :zero) + :val14, \"\n\t        \"#att16 = if_not_exists(#att16, :zero) + :val15, \"\n\t        \"#att17 = if_not_exists(#att17, :zero) + :val16, \"\n\t        \"#att18 = list_append(if_not_exists(#att18, :empty_list), :val17), \"\n\t        \"#att19 = list_append(if_not_exists(#att19, :empty_list), :val18)\"\n\t    )\n", "    assert update_args.attribute_names == {\n\t        \"#att0\": \"some_attr\",\n\t        \"#att1\": \"some_enum\",\n\t        \"#att2\": \"some_object\",\n\t        \"#att3\": \"dict_field\",\n\t        \"#att4\": \"model_field\",\n\t        \"#att5\": \"list_field\",\n\t        \"#att6\": \"set_field\",\n\t        \"#att7\": \"date_field\",\n\t        \"#att8\": \"time_field\",\n", "        \"#att9\": \"datetime_field\",\n\t        \"#att10\": \"enum_field\",\n\t        \"#att11\": \"int_field\",\n\t        \"#att12\": \"optional_field\",\n\t        \"#att13\": \"_timestamp\",\n\t        \"#att14\": \"_ttl\",\n\t        \"#att15\": incr_attr,\n\t        \"#att16\": incr_attr2,\n\t        \"#att17\": \"_object_version\",\n\t        \"#att18\": \"some_list\",\n", "        \"#att19\": \"some_list_2\",\n\t    }\n\t    assert sorted(update_args.attribute_values.pop(\":val5\")) == sorted(list(some_example.set_field))\n\t    assert update_args.attribute_values == {\n\t        \":zero\": 0,\n\t        \":empty_list\": [],\n\t        \":val0\": some_string,\n\t        \":val1\": some_enum.value,\n\t        \":val2\": {k: v.dict() for k, v in some_example.dict_field.items()},\n\t        \":val3\": some_example.model_field.dict(),\n", "        \":val4\": some_example.list_field,\n\t        \":val6\": some_example.date_field.isoformat(),\n\t        \":val7\": some_example.time_field.isoformat(),\n\t        \":val8\": some_example.datetime_field.isoformat(),\n\t        \":val9\": some_example.enum_field.value,\n\t        \":val10\": some_example.int_field,\n\t        \":val11\": None,\n\t        \":val12\": now.isoformat(),\n\t        \":val13\": int(expiry.timestamp()),\n\t        \":val14\": incr,\n", "        \":val15\": 1,\n\t        \":val16\": 1,\n\t        \":val17\": [el1, el2],\n\t        \":val18\": [el3],\n\t    }\n\tdef test_build_update_item_arguments_null_version():\n\t    command = UpdateCommandFactory(current_version=None)\n\t    update_args = build_update_args_for_command(command)\n\t    assert update_args.condition_expression is None\n\t# TODO: Fix this flaky test\n", "# def test_build_filter_expression():\n\t#     filters = FilterCommand(\n\t#         not_exists={\"ne_a\", \"ne_b\"},\n\t#         equals={\"eq_a\": True, \"eq_b\": 1},\n\t#         not_equals={\"dne_a\": \"dne_b\", \"dne_c\": False},\n\t#     )\n\t#     expression = _build_filter_expression(filters)\n\t#     assert expression == (\n\t#         Attr(\"ne_a\").not_exists()\n\t#         & Attr(\"ne_b\").not_exists()\n", "#         & Attr(\"eq_a\").eq(True)\n\t#         & Attr(\"eq_b\").eq(1)\n\t#         & Attr(\"dne_a\").ne(\"dne_b\")\n\t#         & Attr(\"dne_c\").ne(False)\n\t#     )\n\tdef test_validate_command_for_schema_happy_path():\n\t    validate_command_for_schema(\n\t        Example.schema(),\n\t        UpdateCommand(\n\t            set_commands={\n", "                \"dict_field\": fake.pydict(),\n\t                \"model_field\": FieldModelFactory().dict(),\n\t                \"set_field\": set((fake.bothify() for _ in range(2))),\n\t            },\n\t            increment_attrs={\"int_field\": fake.pyint()},\n\t            append_attrs={\"list_field\": [fake.bs()]},\n\t        ),\n\t    )\n\tdef test_validate_command_for_schema_invalid_set_command():\n\t    command = UpdateCommandFactory()\n", "    with pytest.raises(ValueError) as ex:\n\t        validate_command_for_schema(FieldModel.schema(), command)\n\t    exception_value = str(ex.value)\n\t    assert \"FieldModel\" in exception_value\n\t    for attr in command.set_commands.keys():\n\t        assert attr in exception_value\n\tdef test_validate_command_for_schema_non_dict_field_to_dict():\n\t    command = UpdateCommand(set_commands={\"date_field\": {fake.bs(): fake.bs()}})\n\t    with pytest.raises(ValueError) as ex:\n\t        validate_command_for_schema(Example.schema(), command)\n", "    exception_value = str(ex.value)\n\t    assert \"Example\" in exception_value\n\t    assert \"date_field\" in exception_value\n\tdef test_validate_command_for_schema_invalid_nested_model_field():\n\t    bs_key = fake.bs()\n\t    command = UpdateCommand(set_commands={\"model_field\": {bs_key: fake.bs()}})\n\t    with pytest.raises(ValueError) as ex:\n\t        validate_command_for_schema(Example.schema(), command)\n\t    exception_value = str(ex.value)\n\t    assert \"Example\" in exception_value\n", "    assert bs_key in exception_value\n\tdef test_validate_command_for_schema_invalid_incr_command():\n\t    command = UpdateCommandFactory(set_commands={})\n\t    with pytest.raises(ValueError) as ex:\n\t        validate_command_for_schema(FieldModel.schema(), command)\n\t    exception_value = str(ex.value)\n\t    assert \"FieldModel\" in exception_value\n\t    for attr in command.increment_attrs:\n\t        assert attr in exception_value\n\tdef test_validate_command_for_schema_non_integer_incr_command():\n", "    command = UpdateCommand(increment_attrs={\"test_field\": 1})\n\t    with pytest.raises(ValueError) as ex:\n\t        validate_command_for_schema(FieldModel.schema(), command)\n\t    exception_value = str(ex.value)\n\t    assert \"FieldModel\" in exception_value\n\t    for attr in command.increment_attrs:\n\t        assert attr in exception_value\n\tdef test_validate_filters_for_schema_invalid_filter():\n\t    filters = FilterCommand(not_exists={fake.bs()})\n\t    with pytest.raises(ValueError) as ex:\n", "        validate_filters_for_schema(FieldModel.schema(), filters)\n\t    exception_value = str(ex.value)\n\t    assert \"FieldModel\" in exception_value\n\t    for attr in filters.not_exists:\n\t        assert attr in exception_value\n"]}
{"filename": "tests/test_unit/test_v2/test_repository.py", "chunked_list": ["import asyncio\n\timport random\n\tfrom datetime import datetime, timezone\n\tfrom unittest.mock import patch, MagicMock, AsyncMock\n\tfrom faker import Faker\n\tfrom pydantic_dynamo.models import PartitionedContent, UpdateCommand, FilterCommand\n\tfrom pydantic_dynamo.v2.repository import (\n\t    DynamoRepository,\n\t)\n\tfrom pydantic_dynamo.utils import clean_dict\n", "from tests.models import FieldModel, ComposedFieldModel, Example\n\tfrom tests.factories import (\n\t    UpdateItemArgumentsFactory,\n\t    ExamplePartitionedContentFactory,\n\t    example_content_to_db_item,\n\t)\n\tfrom pydantic_dynamo.v2.models import GetResponse\n\tfake = Faker()\n\tasync def test_context_manager():\n\t    partition = fake.bs()\n", "    content_type = fake.bs()\n\t    partition_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    async with DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition,\n\t        partition_name=partition_type,\n\t        content_type=content_type,\n", "        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    ) as repo:\n\t        assert isinstance(repo, DynamoRepository)\n\t@patch(\"pydantic_dynamo.v2.repository.internal_timestamp\")\n\tasync def test_dynamo_repo_put(internal_timestamp):\n\t    now = datetime.now(tz=timezone.utc)\n", "    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\t    partition = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition,\n", "        partition_name=partition_type,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    expiry = fake.date_time()\n\t    content = ExamplePartitionedContentFactory(expiry=expiry)\n", "    await repo.put(content)\n\t    assert table.put_item.call_args[1] == {\n\t        \"Item\": {\n\t            partition_key: f\"{partition}#{partition_type}#\" + \"#\".join(content.partition_ids),\n\t            sort_key: f\"{content_type}#\" + \"#\".join(content.content_ids),\n\t            \"_object_version\": 1,\n\t            \"_timestamp\": now.isoformat(),\n\t            \"_ttl\": int(expiry.timestamp()),\n\t            **clean_dict(content.item.dict()),\n\t        }\n", "    }\n\t@patch(\"pydantic_dynamo.v2.repository.internal_timestamp\")\n\tasync def test_dynamo_repo_put_batch(internal_timestamp):\n\t    now = datetime.now(tz=timezone.utc)\n\t    internal_timestamp.return_value = {\"_timestamp\": now.isoformat()}\n\t    partition = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_ids = [fake.bs()]\n\t    partition_type = fake.bs()\n\t    content_ids = [fake.bs()]\n", "    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    writer = AsyncMock()\n\t    table.batch_writer.return_value.__aenter__.return_value = writer\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition,\n\t        partition_name=partition_type,\n\t        content_type=content_type,\n", "        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    expiry = fake.date_time()\n\t    contents = [\n\t        ExamplePartitionedContentFactory(\n\t            partition_ids=partition_ids, content_ids=content_ids, expiry=expiry\n", "        ),\n\t        ExamplePartitionedContentFactory(\n\t            partition_ids=partition_ids, content_ids=content_ids, expiry=expiry\n\t        ),\n\t    ]\n\t    await repo.put_batch(contents)\n\t    assert writer.put_item.call_args_list == [\n\t        (\n\t            (),\n\t            {\n", "                \"Item\": {\n\t                    partition_key: f\"{partition}#{partition_type}#\"\n\t                    + \"#\".join(contents[0].partition_ids),\n\t                    sort_key: f\"{content_type}#\" + \"#\".join(contents[0].content_ids),\n\t                    \"_object_version\": 1,\n\t                    \"_timestamp\": now.isoformat(),\n\t                    \"_ttl\": int(expiry.timestamp()),\n\t                    **clean_dict(contents[0].item.dict()),\n\t                }\n\t            },\n", "        ),\n\t        (\n\t            (),\n\t            {\n\t                \"Item\": {\n\t                    partition_key: f\"{partition}#{partition_type}#\"\n\t                    + \"#\".join(contents[1].partition_ids),\n\t                    sort_key: f\"{content_type}#\" + \"#\".join(contents[1].content_ids),\n\t                    \"_object_version\": 1,\n\t                    \"_timestamp\": now.isoformat(),\n", "                    \"_ttl\": int(expiry.timestamp()),\n\t                    **clean_dict(contents[1].item.dict()),\n\t                }\n\t            },\n\t        ),\n\t    ]\n\tasync def test_dynamo_repo_get():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n", "    table = AsyncMock()\n\t    partition_id = [fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    content = ExamplePartitionedContentFactory()\n\t    table.get_item.return_value = {\n\t        \"Item\": example_content_to_db_item(\n\t            partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t        )\n", "    }\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n", "        resource=MagicMock(),\n\t        consistent_reads=True,\n\t    )\n\t    actual = await repo.get(partition_id, content_id)\n\t    assert actual == GetResponse(content=content)\n\t    assert table.get_item.call_args == (\n\t        (),\n\t        {\n\t            \"Key\": {\n\t                partition_key: f\"{partition_prefix}#{partition_name}#{partition_id[0]}\",\n", "                sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n\t            },\n\t            \"ConsistentRead\": True,\n\t        },\n\t    )\n\tasync def test_dynamo_repo_get_no_object_version():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    table = AsyncMock()\n", "    partition_id = [fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    content = ExamplePartitionedContentFactory(item__object_version=1)\n\t    response_item = example_content_to_db_item(\n\t        partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t    )\n\t    response_item.pop(\"_object_version\")\n\t    table.get_item.return_value = {\"Item\": response_item}\n", "    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n", "        consistent_reads=True,\n\t    )\n\t    actual = await repo.get(partition_id, content_id)\n\t    assert actual == GetResponse(content=content)\n\t    assert table.get_item.call_args == (\n\t        (),\n\t        {\n\t            \"Key\": {\n\t                partition_key: f\"{partition_prefix}#{partition_name}#{partition_id[0]}\",\n\t                sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n", "            },\n\t            \"ConsistentRead\": True,\n\t        },\n\t    )\n\tasync def test_dynamo_repo_get_none_inputs():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n", "    content = ExamplePartitionedContentFactory()\n\t    table = AsyncMock()\n\t    table.get_item.return_value = {\n\t        \"Item\": example_content_to_db_item(\n\t            partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t        )\n\t    }\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n", "        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = await repo.get(None, None)\n\t    assert actual == GetResponse(content=content)\n", "    assert table.get_item.call_args == (\n\t        (),\n\t        {\n\t            \"Key\": {\n\t                partition_key: f\"{partition_prefix}#{partition_name}#\",\n\t                sort_key: f\"{content_type}#\",\n\t            },\n\t            \"ConsistentRead\": False,\n\t        },\n\t    )\n", "async def test_content_get_repo_no_items():\n\t    table = AsyncMock()\n\t    table.get_item.return_value = {\"Not_Items\": []}\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=fake.bs(),\n\t        partition_name=fake.bs(),\n\t        content_type=fake.bs(),\n\t        table_name=fake.bs(),\n\t        partition_key=fake.bs(),\n", "        sort_key=fake.bs(),\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = await repo.get(fake.bs(), fake.bs())\n\t    assert actual == GetResponse(content=None)\n\tasync def test_dynamo_repo_get_batch():\n\t    partition_prefix = fake.bothify()\n\t    partition_name = fake.bothify()\n\t    content_type = fake.bothify()\n", "    table_name = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    resource = AsyncMock()\n\t    items = [\n\t        example_content_to_db_item(\n\t            partition_key,\n\t            partition_prefix,\n\t            partition_name,\n\t            sort_key,\n", "            content_type,\n\t            ExamplePartitionedContentFactory(partition_ids=[str(i)], content_ids=[str(i)]),\n\t        )\n\t        for i in range(5)\n\t    ]\n\t    unprocessed = [{fake.bothify(): fake.bothify()}]\n\t    resource.batch_get_item.side_effect = [\n\t        {\n\t            \"Responses\": {table_name: items[:2]},\n\t            \"UnprocessedKeys\": unprocessed,\n", "        },\n\t        {\"Responses\": {table_name: items[2:4]}},\n\t        {\"Responses\": {table_name: [items[-1]]}},\n\t    ]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=table_name,\n", "        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=MagicMock(),\n\t        resource=resource,\n\t    )\n\t    # Max key count for each request is 100\n\t    request_ids = [([fake.bothify()], [fake.bothify()]) for _ in range(120)]\n\t    actual = [\n\t        content async for response in repo.get_batch(request_ids) for content in response.contents\n\t    ]\n", "    expected = [\n\t        PartitionedContent[Example](\n\t            partition_ids=[str(i)], content_ids=[str(i)], item=Example(**item)\n\t        )\n\t        for i, item in enumerate(items)\n\t    ]\n\t    assert actual == expected\n\t    assert resource.batch_get_item.call_args_list == [\n\t        (\n\t            (),\n", "            {\n\t                \"RequestItems\": {\n\t                    table_name: {\n\t                        \"Keys\": [\n\t                            {\n\t                                partition_key: f\"{partition_prefix}#{partition_name}#{rid[0][0]}\",\n\t                                sort_key: f\"{content_type}#{rid[1][0]}\",\n\t                            }\n\t                            for rid in request_ids[:100]\n\t                        ],\n", "                        \"ConsistentRead\": False,\n\t                    }\n\t                }\n\t            },\n\t        ),\n\t        (\n\t            (),\n\t            {\n\t                \"RequestItems\": {\n\t                    table_name: {\n", "                        \"Keys\": unprocessed,\n\t                        \"ConsistentRead\": False,\n\t                    }\n\t                }\n\t            },\n\t        ),\n\t        (\n\t            (),\n\t            {\n\t                \"RequestItems\": {\n", "                    table_name: {\n\t                        \"Keys\": [\n\t                            {\n\t                                partition_key: f\"{partition_prefix}#{partition_name}#{rid[0][0]}\",\n\t                                sort_key: f\"{content_type}#{rid[1][0]}\",\n\t                            }\n\t                            for rid in request_ids[100:]\n\t                        ],\n\t                        \"ConsistentRead\": False,\n\t                    }\n", "                }\n\t            },\n\t        ),\n\t    ]\n\tasync def test_dynamo_repo_list():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n", "    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(3)\n\t    table.query.return_value = {\n\t        \"Items\": [\n\t            example_content_to_db_item(\n\t                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t            )\n\t            for content in contents\n\t        ],\n\t        \"Count\": fake.pyint(),\n", "    }\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n", "        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    actual = [\n\t        content\n\t        async for response in repo.list(partition_id, content_id, ascending, limit)\n\t        for content in response.contents\n", "    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ConsistentRead\"] is False\n\t    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert kwargs[\"Limit\"] == limit\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n", "    assert (\n\t        expression._values[0]._values[1]\n\t        == f\"{partition_prefix}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n\t    )\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n\tasync def test_dynamo_repo_list_last_evaluated_under_limit():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n", "    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(4)\n\t    start_key = fake.bothify()\n\t    table.query.side_effect = [\n\t        {\n\t            \"Items\": [\n\t                example_content_to_db_item(\n", "                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t                )\n\t                for content in contents[:3]\n\t            ],\n\t            \"LastEvaluatedKey\": start_key,\n\t            \"Count\": 3,\n\t        },\n\t        {\n\t            \"Items\": [\n\t                example_content_to_db_item(\n", "                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t                )\n\t                for content in contents[3:4]\n\t            ],\n\t            \"Count\": 1,\n\t        },\n\t    ]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n", "        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n", "    ascending = random.choice((True, False))\n\t    limit = 5\n\t    actual = [\n\t        content\n\t        async for response in repo.list(partition_id, content_id, ascending, limit)\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    assert len(table.query.call_args_list) == 2\n\t    _, kwargs2 = table.query.call_args_list[1]\n", "    assert kwargs2[\"ExclusiveStartKey\"] == start_key\n\tasync def test_dynamo_repo_list_last_evaluated_over_limit():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(6)\n\t    start_key = fake.bothify()\n", "    table.query.side_effect = [\n\t        {\n\t            \"Items\": [\n\t                example_content_to_db_item(\n\t                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t                )\n\t                for content in contents[:3]\n\t            ],\n\t            \"LastEvaluatedKey\": start_key,\n\t            \"Count\": 3,\n", "        },\n\t        {\n\t            \"Items\": [\n\t                example_content_to_db_item(\n\t                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t                )\n\t                for content in contents[3:6]\n\t            ],\n\t            \"Count\": 3,\n\t        },\n", "    ]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n", "        resource=MagicMock(),\n\t    )\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n\t    ascending = random.choice((True, False))\n\t    limit = 5\n\t    actual = [\n\t        content\n\t        async for response in repo.list(partition_id, content_id, ascending, limit)\n\t        for content in response.contents\n", "    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    assert len(table.query.call_args_list) == 2\n\t    _, kwargs2 = table.query.call_args_list[1]\n\t    assert kwargs2[\"ExclusiveStartKey\"] == start_key\n\tasync def test_dynamo_repo_list_last_evaluated_over_limit_after_evaluated_key():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n", "    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(8)\n\t    start_key1 = fake.bothify()\n\t    start_key2 = fake.bothify()\n\t    table.query.side_effect = [\n\t        {\n\t            \"Items\": [\n\t                example_content_to_db_item(\n\t                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n", "                )\n\t                for content in contents[:3]\n\t            ],\n\t            \"LastEvaluatedKey\": start_key1,\n\t            \"Count\": 3,\n\t        },\n\t        {\n\t            \"Items\": [\n\t                example_content_to_db_item(\n\t                    partition_key, partition_prefix, partition_name, sort_key, content_type, content\n", "                )\n\t                for content in contents[3:6]\n\t            ],\n\t            \"LastEvaluatedKey\": start_key2,\n\t            \"Count\": 3,\n\t        },\n\t    ]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n", "        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n", "    ascending = random.choice((True, False))\n\t    limit = 5\n\t    actual = [\n\t        content\n\t        async for response in repo.list(partition_id, content_id, ascending, limit)\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents[:6])\n\t    assert len(table.query.call_args_list) == 2\n\t    _, kwargs2 = table.query.call_args_list[1]\n", "    assert kwargs2[\"ExclusiveStartKey\"] == start_key1\n\tasync def test_dynamo_repo_list_none_inputs():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(3)\n\t    table.query.return_value = {\n", "        \"Items\": [\n\t            example_content_to_db_item(\n\t                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t            )\n\t            for content in contents\n\t        ],\n\t        \"Count\": fake.pyint(),\n\t    }\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n", "        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t        consistent_reads=True,\n\t    )\n", "    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    actual = [\n\t        content\n\t        async for response in repo.list(None, None, ascending, limit)\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ConsistentRead\"] is True\n", "    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert kwargs[\"Limit\"] == limit\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n", "async def test_dynamo_repo_list_no_ids():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(3)\n\t    table.query.return_value = {\n\t        \"Items\": [\n", "            example_content_to_db_item(\n\t                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t            )\n\t            for content in contents\n\t        ],\n\t        \"Count\": fake.pyint(),\n\t    }\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n", "        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n", "    actual = [\n\t        content\n\t        async for response in repo.list([], [], ascending, limit)\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert kwargs[\"Limit\"] == limit\n\t    expression = kwargs[\"KeyConditionExpression\"]\n", "    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\tasync def test_dynamo_repo_list_with_filter():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n", "    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(3)\n\t    table.query.return_value = {\n\t        \"Items\": [\n\t            example_content_to_db_item(\n\t                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t            )\n", "            for content in contents\n\t        ],\n\t        \"Count\": fake.pyint(),\n\t    }\n\t    partition_id = [fake.bs()]\n\t    content_id = [fake.bs()]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n", "        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n\t    filters = FilterCommand(not_exists={\"optional_field\"})\n", "    actual = [\n\t        content\n\t        async for response in repo.list(partition_id, content_id, ascending, limit, filters)\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    args, kwargs = table.query.call_args\n\t    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert \"Limit\" not in kwargs\n\t    filter_expression = kwargs[\"FilterExpression\"]\n", "    assert filter_expression.expression_operator == \"attribute_not_exists\"\n\t    assert filter_expression._values[0].name == \"optional_field\"\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert (\n\t        expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#{partition_id[0]}\"\n\t    )\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n", "    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}\"\n\tasync def test_dynamo_repo_list_between():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(3)\n", "    table.query.return_value = {\n\t        \"Items\": [\n\t            example_content_to_db_item(\n\t                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t            )\n\t            for content in contents\n\t        ],\n\t        \"Count\": fake.pyint(),\n\t    }\n\t    partition_id = [fake.bs(), fake.bs()]\n", "    content_start = [fake.bs(), fake.bs()]\n\t    content_end = [fake.bs(), fake.bs()]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n", "        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    actual = [\n\t        content\n\t        async for response in repo.list_between(partition_id, content_start, content_end)\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    args, kwargs = table.query.call_args\n", "    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert (\n\t        expression._values[0]._values[1]\n\t        == f\"{partition_prefix}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n\t    )\n\t    assert expression._values[1].expression_operator == \"BETWEEN\"\n\t    assert expression._values[1]._values[0].name == sort_key\n", "    assert (\n\t        expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}#{content_start[1]}\"\n\t    )\n\t    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}#{content_end[1]}\"\n\tasync def test_dynamo_repo_list_between_none_inputs():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n", "    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(3)\n\t    table.query.return_value = {\n\t        \"Items\": [\n\t            example_content_to_db_item(\n\t                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t            )\n\t            for content in contents\n\t        ],\n\t        \"Count\": fake.pyint(),\n", "    }\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n", "        resource=MagicMock(),\n\t    )\n\t    actual = [\n\t        content\n\t        async for response in repo.list_between(None, None, None)\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    args, kwargs = table.query.call_args\n\t    expression = kwargs[\"KeyConditionExpression\"]\n", "    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\tasync def test_dynamo_repo_list_between_with_filter():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n", "    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(3)\n\t    table.query.return_value = {\n\t        \"Items\": [\n\t            example_content_to_db_item(\n\t                partition_key, partition_prefix, partition_name, sort_key, content_type, content\n\t            )\n", "            for content in contents\n\t        ],\n\t        \"Count\": fake.pyint(),\n\t    }\n\t    partition_id = [fake.bs()]\n\t    content_start = [fake.bs()]\n\t    content_end = [fake.bs()]\n\t    repo = DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=partition_prefix,\n", "        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    ascending = random.choice((True, False))\n\t    limit = fake.pyint()\n", "    filters = FilterCommand(not_exists={\"optional_field\"})\n\t    actual = [\n\t        content\n\t        async for response in repo.list_between(\n\t            partition_id, content_start, content_end, ascending, limit, filters\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(contents)\n\t    args, kwargs = table.query.call_args\n", "    assert kwargs[\"ScanIndexForward\"] == ascending\n\t    assert \"Limit\" not in kwargs\n\t    filter_expression = kwargs[\"FilterExpression\"]\n\t    assert filter_expression.expression_operator == \"attribute_not_exists\"\n\t    assert filter_expression._values[0].name == \"optional_field\"\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert (\n", "        expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#{partition_id[0]}\"\n\t    )\n\t    assert expression._values[1].expression_operator == \"BETWEEN\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#{content_start[0]}\"\n\t    assert expression._values[1]._values[2] == f\"{content_type}#{content_end[0]}\"\n\t@patch(\"pydantic_dynamo.v2.repository.build_update_args_for_command\")\n\tasync def test_dynamo_repo_update(build_update_args):\n\t    update_args = UpdateItemArgumentsFactory()\n\t    build_update_args.return_value = update_args\n", "    partition = fake.bothify()\n\t    partition_name = fake.bothify()\n\t    content_type = fake.bothify()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = AsyncMock()\n\t    repo = DynamoRepository[ComposedFieldModel](\n\t        item_class=ComposedFieldModel,\n\t        partition_prefix=partition,\n\t        partition_name=partition_name,\n", "        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify(), fake.bothify()]\n\t    current_version = fake.pyint()\n", "    command = UpdateCommand(\n\t        set_commands={\n\t            \"test_field\": fake.bs(),\n\t            \"composed\": {\"test_field\": fake.bs(), \"failures\": None},\n\t        },\n\t        increment_attrs={\"failures\": 1},\n\t        current_version=current_version,\n\t    )\n\t    await repo.update(partition_id, content_id, command)\n\t    update_a, update_k = table.update_item.call_args\n", "    assert update_k.pop(\"Key\") == {\n\t        partition_key: f\"{partition}#{partition_name}#{partition_id[0]}\",\n\t        sort_key: f\"{content_type}#{content_id[0]}#{content_id[1]}\",\n\t    }\n\t    assert update_k.pop(\"ConditionExpression\") == update_args.condition_expression\n\t    assert update_k.pop(\"UpdateExpression\") == update_args.update_expression\n\t    assert update_k.pop(\"ExpressionAttributeNames\") == update_args.attribute_names\n\t    assert update_k.pop(\"ExpressionAttributeValues\") == update_args.attribute_values\n\t    assert len(update_k) == 0\n\tasync def test_dynamo_repo_delete():\n", "    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n\t    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(11)\n\t    items = [\n\t        example_content_to_db_item(\n\t            partition_key, partition_prefix, partition_name, sort_key, content_type, c\n", "        )\n\t        for c in contents\n\t    ]\n\t    f = asyncio.Future()\n\t    f.set_result({\"Items\": items, \"Count\": fake.pyint()})\n\t    table.query.return_value = f\n\t    writer = AsyncMock()\n\t    table.batch_writer.return_value.__aenter__.return_value = writer\n\t    partition_id = [fake.bs(), fake.bs()]\n\t    content_id = [fake.bs(), fake.bs()]\n", "    repo = DynamoRepository[Example](\n\t        item_class=FieldModel,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n\t        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n", "    )\n\t    await repo.delete(partition_id, content_id)\n\t    args, kwargs = table.query.call_args\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n\t    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert (\n\t        expression._values[0]._values[1]\n\t        == f\"{partition_prefix}#{partition_name}#{partition_id[0]}#{partition_id[1]}\"\n", "    )\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#{content_id[0]}#{content_id[1]}\"\n\t    assert writer.delete_item.call_args_list == [\n\t        (\n\t            (),\n\t            {\n\t                \"Key\": {\n\t                    partition_key: item[partition_key],\n", "                    sort_key: item[sort_key],\n\t                }\n\t            },\n\t        )\n\t        for item in items\n\t    ]\n\tasync def test_dynamo_repo_delete_none_inputs():\n\t    partition_prefix = fake.bs()\n\t    partition_name = fake.bs()\n\t    content_type = fake.bs()\n", "    partition_key = fake.bs()\n\t    sort_key = fake.bs()\n\t    table = MagicMock()\n\t    contents = ExamplePartitionedContentFactory.build_batch(11)\n\t    items = [\n\t        example_content_to_db_item(\n\t            partition_key, partition_prefix, partition_name, sort_key, content_type, c\n\t        )\n\t        for c in contents\n\t    ]\n", "    f = asyncio.Future()\n\t    f.set_result({\"Items\": items, \"Count\": fake.pyint()})\n\t    table.query.return_value = f\n\t    writer = AsyncMock()\n\t    table.batch_writer.return_value.__aenter__.return_value = writer\n\t    repo = DynamoRepository[Example](\n\t        item_class=FieldModel,\n\t        partition_prefix=partition_prefix,\n\t        partition_name=partition_name,\n\t        content_type=content_type,\n", "        table_name=fake.bs(),\n\t        partition_key=partition_key,\n\t        sort_key=sort_key,\n\t        table=table,\n\t        resource=MagicMock(),\n\t    )\n\t    await repo.delete(None, None)\n\t    args, kwargs = table.query.call_args\n\t    expression = kwargs[\"KeyConditionExpression\"]\n\t    assert expression.expression_operator == \"AND\"\n", "    assert expression._values[0].expression_operator == \"=\"\n\t    assert expression._values[0]._values[0].name == partition_key\n\t    assert expression._values[0]._values[1] == f\"{partition_prefix}#{partition_name}#\"\n\t    assert expression._values[1].expression_operator == \"begins_with\"\n\t    assert expression._values[1]._values[0].name == sort_key\n\t    assert expression._values[1]._values[1] == f\"{content_type}#\"\n\t    assert writer.delete_item.call_args_list == [\n\t        (\n\t            (),\n\t            {\n", "                \"Key\": {\n\t                    partition_key: item[partition_key],\n\t                    sort_key: item[sort_key],\n\t                }\n\t            },\n\t        )\n\t        for item in items\n\t    ]\n"]}
{"filename": "tests/test_unit/test_v2/__init__.py", "chunked_list": []}
{"filename": "tests/test_unit/test_v2/test_sync_repository.py", "chunked_list": ["from unittest.mock import MagicMock\n\tfrom tests.models import Example\n\tfrom pydantic_dynamo.v2.sync_repository import SyncDynamoRepository\n\tdef test_context_manager():\n\t    with SyncDynamoRepository[Example](async_repo=MagicMock()) as repo:\n\t        assert isinstance(repo, SyncDynamoRepository)\n"]}
{"filename": "tests/test_unit/test_v2/test_write_once.py", "chunked_list": ["from typing import AsyncIterable, List\n\tfrom unittest.mock import AsyncMock, MagicMock\n\tfrom faker import Faker\n\tfrom tests.factories import ExamplePartitionedContentFactory\n\tfrom pydantic_dynamo.v2.models import BatchResponse\n\tfrom pydantic_dynamo.v2.write_once import WriteOnceRepository\n\tfake = Faker()\n\tasync def test_write_once_empty_input():\n\t    core = MagicMock()\n\t    repo = WriteOnceRepository(core)\n", "    new = await repo.write([])\n\t    assert new == []\n\t    assert core.list_between.call_count == 0\n\t    assert core.put_batch.call_count == 0\n\tasync def test_write_once_no_existing():\n\t    core = MagicMock()\n\t    core.list_between.return_value.__aiter__.return_value = []\n\t    put_batch = AsyncMock()\n\t    core.put_batch = put_batch\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n", "    repo = WriteOnceRepository(core)\n\t    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n\t    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        for c in content_ids\n\t    ]\n\t    new = await repo.write(data)\n\t    assert core.list_between.call_args_list == [\n\t        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n", "        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n\t    ]\n\t    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n\t    sorted_expected = sorted(data)\n\t    assert sorted(actual_put) == sorted_expected\n\t    assert sorted(new) == sorted_expected\n\tasync def test_write_once_async_iter_no_existing():\n\t    core = MagicMock()\n\t    core.list_between.return_value.__aiter__.return_value = []\n\t    put_batch = AsyncMock()\n", "    core.put_batch = put_batch\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n\t    repo = WriteOnceRepository(core)\n\t    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        async for c in _async_content_ids()\n\t    ]\n\t    new = await repo.write(data)\n\t    assert core.list_between.call_args_list == [\n", "        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n\t        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n\t    ]\n\t    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n\t    sorted_expected = sorted(data)\n\t    assert sorted(actual_put) == sorted_expected\n\t    assert sorted(new) == sorted_expected\n\tasync def test_write_once_some_existing():\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n\t    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n", "    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        for c in content_ids\n\t    ]\n\t    core = MagicMock()\n\t    response_one = MagicMock()\n\t    response_one.__aiter__.return_value = [BatchResponse(contents=[data[0]])]\n\t    response_two = MagicMock()\n\t    response_two.__aiter__.return_value = [BatchResponse(contents=[data[-1]])]\n", "    core.list_between.side_effect = [response_one, response_two]\n\t    put_batch = AsyncMock()\n\t    core.put_batch = put_batch\n\t    repo = WriteOnceRepository(core)\n\t    new = await repo.write(data)\n\t    assert core.list_between.call_args_list == [\n\t        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n\t        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n\t    ]\n\t    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n", "    sorted_expected = sorted(data[i] for i in range(8) if i not in (0, 7))\n\t    assert sorted(actual_put) == sorted_expected\n\t    assert sorted(new) == sorted_expected\n\tasync def test_write_once_async_iter_some_existing():\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n\t    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        async for c in _async_content_ids()\n\t    ]\n", "    core = MagicMock()\n\t    response_one = MagicMock()\n\t    response_one.__aiter__.return_value = [BatchResponse(contents=[data[0]])]\n\t    response_two = MagicMock()\n\t    response_two.__aiter__.return_value = [BatchResponse(contents=[data[-1]])]\n\t    core.list_between.side_effect = [response_one, response_two]\n\t    put_batch = AsyncMock()\n\t    core.put_batch = put_batch\n\t    repo = WriteOnceRepository(core)\n\t    new = await repo.write(data)\n", "    assert core.list_between.call_args_list == [\n\t        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n\t        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n\t    ]\n\t    actual_put = [c for args, kwargs in core.put_batch.call_args_list for c in args[0]]\n\t    sorted_expected = sorted(data[i] for i in range(8) if i not in (0, 7))\n\t    assert sorted(actual_put) == sorted_expected\n\t    assert sorted(new) == sorted_expected\n\tasync def test_write_once_all_existing():\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n", "    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n\t    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        for c in content_ids\n\t    ]\n\t    core = MagicMock()\n\t    response_one = MagicMock()\n\t    response_one.__aiter__.return_value = [\n\t        BatchResponse(contents=data[0:2]),\n", "        BatchResponse(contents=data[2:4]),\n\t    ]\n\t    response_two = MagicMock()\n\t    response_two.__aiter__.return_value = [\n\t        BatchResponse(contents=data[4:6]),\n\t        BatchResponse(contents=data[6:8]),\n\t    ]\n\t    core.list_between.side_effect = [response_one, response_two]\n\t    put_batch = AsyncMock()\n\t    core.put_batch = put_batch\n", "    repo = WriteOnceRepository(core)\n\t    new = await repo.write(data)\n\t    assert core.list_between.call_args_list == [\n\t        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n\t        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n\t    ]\n\t    assert core.put_batch.call_count == 0\n\t    assert new == []\n\tasync def test_write_once_async_iter_all_existing():\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n", "    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        async for c in _async_content_ids()\n\t    ]\n\t    core = MagicMock()\n\t    response_one = MagicMock()\n\t    response_one.__aiter__.return_value = [\n\t        BatchResponse(contents=data[0:2]),\n\t        BatchResponse(contents=data[2:4]),\n", "    ]\n\t    response_two = MagicMock()\n\t    response_two.__aiter__.return_value = [\n\t        BatchResponse(contents=data[4:6]),\n\t        BatchResponse(contents=data[6:8]),\n\t    ]\n\t    core.list_between.side_effect = [response_one, response_two]\n\t    put_batch = AsyncMock()\n\t    core.put_batch = put_batch\n\t    repo = WriteOnceRepository(core)\n", "    new = await repo.write(data)\n\t    assert core.list_between.call_args_list == [\n\t        ((partition_ids[0], [\"A\"], [\"Z\"]), {}),\n\t        ((partition_ids[1], [\"A\"], [\"Z\"]), {}),\n\t    ]\n\t    assert core.put_batch.call_count == 0\n\t    assert new == []\n\tasync def _async_content_ids() -> AsyncIterable[List[str]]:\n\t    yield [\"Z\"]\n\t    yield [\"B\"]\n", "    yield [\"A\"]\n\t    yield [\"D\"]\n"]}
{"filename": "tests/test_integration/__init__.py", "chunked_list": []}
{"filename": "tests/test_integration/conftest.py", "chunked_list": ["from uuid import uuid4\n\timport pytest\n\timport pytest_asyncio\n\tfrom aioboto3 import Session\n\tfrom testcontainers.core.container import DockerContainer\n\tfrom pydantic_dynamo.v2.repository import DynamoRepository\n\tfrom pydantic_dynamo.v2.sync_repository import SyncDynamoRepository\n\tfrom tests.models import Example\n\tPARTITION_KEY = \"_table_item_id\"\n\tSORT_KEY = \"_table_content_id\"\n", "@pytest.fixture(scope=\"session\")\n\tdef local_db():\n\t    with DockerContainer(\"amazon/dynamodb-local\").with_bind_ports(8000, 8000).with_command(\n\t        \"-jar DynamoDBLocal.jar\"\n\t    ).with_exposed_ports(8080) as local_db:\n\t        yield local_db\n\t@pytest_asyncio.fixture\n\tasync def local_db_resource(local_db):\n\t    session = Session()\n\t    boto3_kwargs = {\n", "        \"service_name\": \"dynamodb\",\n\t        \"endpoint_url\": \"http://localhost:8000\",\n\t        \"region_name\": \"local\",\n\t        \"aws_access_key_id\": \"key\",\n\t        \"aws_secret_access_key\": \"secret\",\n\t    }\n\t    table_name = str(uuid4())\n\t    async with session.resource(**boto3_kwargs) as resource:\n\t        await resource.create_table(\n\t            TableName=table_name,\n", "            KeySchema=[\n\t                {\"AttributeName\": PARTITION_KEY, \"KeyType\": \"HASH\"},\n\t                {\"AttributeName\": SORT_KEY, \"KeyType\": \"RANGE\"},\n\t            ],\n\t            AttributeDefinitions=[\n\t                {\"AttributeName\": PARTITION_KEY, \"AttributeType\": \"S\"},\n\t                {\"AttributeName\": SORT_KEY, \"AttributeType\": \"S\"},\n\t            ],\n\t            ProvisionedThroughput={\"ReadCapacityUnits\": 10, \"WriteCapacityUnits\": 10},\n\t        )\n", "        yield resource, table_name\n\t        table = await resource.Table(table_name)\n\t        await table.delete()\n\t@pytest_asyncio.fixture\n\tasync def v2_example_repo(local_db_resource):\n\t    resource, table_name = local_db_resource\n\t    yield DynamoRepository[Example](\n\t        item_class=Example,\n\t        partition_prefix=\"test\",\n\t        partition_name=\"integration\",\n", "        content_type=\"example\",\n\t        table_name=table_name,\n\t        partition_key=PARTITION_KEY,\n\t        sort_key=SORT_KEY,\n\t        table=await resource.Table(table_name),\n\t        resource=resource,\n\t    )\n\t@pytest.fixture\n\tdef sync_v2_example_repo(v2_example_repo):\n\t    yield SyncDynamoRepository[Example](async_repo=v2_example_repo)\n"]}
{"filename": "tests/test_integration/test_v2/test_repository.py", "chunked_list": ["from datetime import datetime, timezone, timedelta\n\tfrom zoneinfo import ZoneInfo\n\timport pytest\n\tfrom faker import Faker\n\tfrom pydantic_dynamo.exceptions import RequestObjectStateError\n\tfrom pydantic_dynamo.models import FilterCommand, UpdateCommand, PartitionedContent\n\tfrom pydantic_dynamo.v2.models import GetResponse\n\tfrom tests.factories import ExamplePartitionedContentFactory, ExampleFactory\n\tfrom tests.models import CountEnum, Example, FieldModel\n\tfake = Faker()\n", "async def test_get_none(v2_example_repo):\n\t    actual = await v2_example_repo.get(None, None)\n\t    expected = GetResponse()\n\t    assert actual == expected\n\tdef test_sync_get_none(sync_v2_example_repo):\n\t    actual = sync_v2_example_repo.get(None, None)\n\t    expected = GetResponse()\n\t    assert actual == expected\n\tasync def test_put_then_get_item(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n", "    content_ids = [fake.bothify()]\n\t    content = ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t    await v2_example_repo.put(content)\n\t    actual = await v2_example_repo.get(partition_ids, content_ids)\n\t    expected = GetResponse(content=content)\n\t    assert actual == expected\n\tdef test_sync_put_then_get_item(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids = [fake.bothify()]\n\t    content = ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n", "    sync_v2_example_repo.put(content)\n\t    actual = sync_v2_example_repo.get(partition_ids, content_ids)\n\t    expected = GetResponse(content=content)\n\t    assert actual == expected\n\tasync def test_put_batch_then_get_batch(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(125)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n", "    ]\n\t    await v2_example_repo.put_batch(contents)\n\t    get_ids = [(partition_ids, content_ids) for content_ids in content_ids_list]\n\t    actual = [\n\t        content\n\t        async for response in v2_example_repo.get_batch(get_ids)\n\t        for content in response.contents\n\t    ]\n\t    actual.sort()\n\t    contents.sort()\n", "    assert actual == contents\n\tdef test_sync_put_batch_then_get_batch(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(125)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n\t    get_ids = [(partition_ids, content_ids) for content_ids in content_ids_list]\n", "    actual = [\n\t        content\n\t        for response in sync_v2_example_repo.get_batch(get_ids)\n\t        for content in response.contents\n\t    ]\n\t    actual.sort()\n\t    contents.sort()\n\t    assert actual == contents\n\tasync def test_put_batch_then_list_with_prefix_filter_and_sorting(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n", "    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    await v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n\t        content\n\t        async for response in v2_example_repo.list(\n\t            partition_ids, content_prefix=[\"0\"], sort_ascending=True\n", "        )\n\t        for content in response.contents\n\t    ]\n\t    expected = contents[:100]\n\t    expected.sort()\n\t    assert ascending_actual == expected\n\t    descending_actual = [\n\t        content\n\t        async for response in v2_example_repo.list(\n\t            partition_ids, content_prefix=[\"0\"], sort_ascending=False\n", "        )\n\t        for content in response.contents\n\t    ]\n\t    expected.sort(reverse=True)\n\t    assert descending_actual == expected\n\tdef test_sync_put_batch_then_list_with_prefix_filter_and_sorting(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n", "        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list(\n\t            partition_ids, content_prefix=[\"0\"], sort_ascending=True\n\t        )\n\t        for content in response.contents\n\t    ]\n", "    expected = contents[:100]\n\t    expected.sort()\n\t    assert ascending_actual == expected\n\t    descending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list(\n\t            partition_ids, content_prefix=[\"0\"], sort_ascending=False\n\t        )\n\t        for content in response.contents\n\t    ]\n", "    expected.sort(reverse=True)\n\t    assert descending_actual == expected\n\tasync def test_put_batch_then_list_limit(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    await v2_example_repo.put_batch(contents)\n", "    actual = [\n\t        content\n\t        async for response in v2_example_repo.list(partition_ids, content_prefix=None, limit=2)\n\t        for content in response.contents\n\t    ]\n\t    expected = contents[:2]\n\t    assert actual == expected\n\tdef test_sync_put_batch_then_list_limit(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n", "    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n\t    actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list(partition_ids, content_prefix=None, limit=2)\n\t        for content in response.contents\n\t    ]\n", "    expected = contents[:2]\n\t    assert actual == expected\n\tasync def test_put_batch_then_list_filter(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    await v2_example_repo.put_batch(contents)\n", "    actual = [\n\t        content\n\t        async for response in v2_example_repo.list(\n\t            partition_ids,\n\t            content_prefix=None,\n\t            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    actual.sort()\n", "    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents))\n\t    expected.sort()\n\t    assert actual == expected\n\tdef test_sync_put_batch_then_list_filter(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n", "    sync_v2_example_repo.put_batch(contents)\n\t    actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list(\n\t            partition_ids,\n\t            content_prefix=None,\n\t            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n\t        )\n\t        for content in response.contents\n\t    ]\n", "    actual.sort()\n\t    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents))\n\t    expected.sort()\n\t    assert actual == expected\n\tasync def test_put_batch_then_list_between_sorting(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    now = datetime.now(tz=timezone.utc)\n\t    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n", "        for content_ids in content_ids_list\n\t    ]\n\t    await v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n\t        content\n\t        async for response in v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            sort_ascending=True,\n", "        )\n\t        for content in response.contents\n\t    ]\n\t    expected = contents[1:9]\n\t    assert ascending_actual == expected\n\t    descending_actual = [\n\t        content\n\t        async for response in v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n", "            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            sort_ascending=False,\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    expected.sort(key=lambda c: c.content_ids[0], reverse=True)\n\t    assert descending_actual == expected\n\tdef test_sync_put_batch_then_list_between_sorting(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    now = datetime.now(tz=timezone.utc)\n", "    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n", "            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            sort_ascending=True,\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    expected = contents[1:9]\n\t    assert ascending_actual == expected\n\t    descending_actual = [\n\t        content\n", "        for response in sync_v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            sort_ascending=False,\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    expected.sort(key=lambda c: c.content_ids[0], reverse=True)\n\t    assert descending_actual == expected\n", "async def test_put_batch_then_list_between_limit(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    now = datetime.now(tz=timezone.utc)\n\t    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    await v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n", "        content\n\t        async for response in v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            limit=5,\n\t            sort_ascending=True,\n\t        )\n\t        for content in response.contents\n\t    ]\n", "    expected = contents[1:6]\n\t    assert ascending_actual == expected\n\t    descending_actual = [\n\t        content\n\t        async for response in v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            limit=5,\n\t            sort_ascending=False,\n", "        )\n\t        for content in response.contents\n\t    ]\n\t    descending_expected = sorted(contents, key=lambda c: c.content_ids[0], reverse=True)[1:6]\n\t    assert descending_actual == descending_expected\n\tdef test_sync_put_batch_then_list_between_limit(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    now = datetime.now(tz=timezone.utc)\n\t    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n\t    contents = [\n", "        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n", "            limit=5,\n\t            sort_ascending=True,\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    expected = contents[1:6]\n\t    assert ascending_actual == expected\n\t    descending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list_between(\n", "            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            limit=5,\n\t            sort_ascending=False,\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    descending_expected = sorted(contents, key=lambda c: c.content_ids[0], reverse=True)[1:6]\n\t    assert descending_actual == descending_expected\n", "async def test_put_batch_then_list_between_filter(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    now = datetime.now(tz=timezone.utc)\n\t    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    await v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n", "        content\n\t        async for response in v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents[1:9]))\n", "    assert ascending_actual == expected\n\tdef test_sync_put_batch_then_list_between_filter(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    now = datetime.now(tz=timezone.utc)\n\t    content_ids_list = [[(now + timedelta(seconds=i)).isoformat()] for i in range(10)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n", "    ascending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list_between(\n\t            partition_id=partition_ids,\n\t            content_start=[(now + timedelta(seconds=1)).isoformat()],\n\t            content_end=[(now + timedelta(seconds=8)).isoformat()],\n\t            filters=FilterCommand(equals={\"enum_field\": \"one\"}),\n\t        )\n\t        for content in response.contents\n\t    ]\n", "    expected = list(filter(lambda c: c.item.enum_field == CountEnum.One, contents[1:9]))\n\t    assert ascending_actual == expected\n\tasync def test_update_happy_path(v2_example_repo):\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n\t    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n\t    await v2_example_repo.put(content)\n\t    new_dt = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n\t    new_str = fake.bs()\n\t    # TTL stored as integer where we lose microsecond granularity\n", "    new_expiry = (new_dt + timedelta(days=99)).replace(microsecond=0)\n\t    await v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n\t        UpdateCommand(\n\t            current_version=1,\n\t            set_commands={\"datetime_field\": new_dt},\n\t            increment_attrs={\"int_field\": 2},\n\t            append_attrs={\"list_field\": [new_str]},\n\t            expiry=new_expiry,\n", "        ),\n\t    )\n\t    updated = await v2_example_repo.get(partition_id, content_id)\n\t    og_dict = content.item.dict()\n\t    og_dict.pop(\"datetime_field\")\n\t    expected = PartitionedContent[Example](\n\t        partition_ids=partition_id,\n\t        content_ids=content_id,\n\t        item=Example(\n\t            datetime_field=new_dt,\n", "            int_field=og_dict.pop(\"int_field\") + 2,\n\t            list_field=og_dict.pop(\"list_field\") + [new_str],\n\t            **og_dict\n\t        ),\n\t        current_version=2,\n\t        expiry=new_expiry,\n\t    )\n\t    assert updated.content == expected\n\tdef test_sync_update_happy_path(sync_v2_example_repo):\n\t    partition_id = [fake.bothify()]\n", "    content_id = [fake.bothify()]\n\t    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n\t    sync_v2_example_repo.put(content)\n\t    new_dt = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n\t    new_str = fake.bs()\n\t    # TTL stored as integer where we lose microsecond granularity\n\t    new_expiry = (new_dt + timedelta(days=99)).replace(microsecond=0)\n\t    sync_v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n", "        UpdateCommand(\n\t            current_version=1,\n\t            set_commands={\"datetime_field\": new_dt},\n\t            increment_attrs={\"int_field\": 2},\n\t            append_attrs={\"list_field\": [new_str]},\n\t            expiry=new_expiry,\n\t        ),\n\t    )\n\t    updated = sync_v2_example_repo.get(partition_id, content_id)\n\t    og_dict = content.item.dict()\n", "    og_dict.pop(\"datetime_field\")\n\t    expected = PartitionedContent[Example](\n\t        partition_ids=partition_id,\n\t        content_ids=content_id,\n\t        item=Example(\n\t            datetime_field=new_dt,\n\t            int_field=og_dict.pop(\"int_field\") + 2,\n\t            list_field=og_dict.pop(\"list_field\") + [new_str],\n\t            **og_dict\n\t        ),\n", "        current_version=2,\n\t        expiry=new_expiry,\n\t    )\n\t    assert updated.content == expected\n\tasync def test_update_nested_model_field(v2_example_repo):\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n\t    init_value = fake.bs()\n\t    content = ExamplePartitionedContentFactory(\n\t        partition_ids=partition_id,\n", "        content_ids=content_id,\n\t        item=ExampleFactory(model_field=FieldModel(test_field=init_value, failures=1)),\n\t    )\n\t    await v2_example_repo.put(content)\n\t    await v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n\t        UpdateCommand(set_commands={\"model_field\": {\"failures\": 2}}),\n\t    )\n\t    updated = await v2_example_repo.get(partition_id, content_id)\n", "    expected = GetResponse(\n\t        content=PartitionedContent[Example](\n\t            partition_ids=partition_id,\n\t            content_ids=content_id,\n\t            item=Example(\n\t                model_field=FieldModel(test_field=init_value, failures=2),\n\t                **content.item.dict(exclude={\"model_field\"})\n\t            ),\n\t            current_version=2,\n\t        )\n", "    )\n\t    assert updated == expected\n\tdef test_sync_update_nested_model_field(sync_v2_example_repo):\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n\t    init_value = fake.bs()\n\t    content = ExamplePartitionedContentFactory(\n\t        partition_ids=partition_id,\n\t        content_ids=content_id,\n\t        item=ExampleFactory(model_field=FieldModel(test_field=init_value, failures=1)),\n", "    )\n\t    sync_v2_example_repo.put(content)\n\t    sync_v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n\t        UpdateCommand(set_commands={\"model_field\": {\"failures\": 2}}),\n\t    )\n\t    updated = sync_v2_example_repo.get(partition_id, content_id)\n\t    expected = GetResponse(\n\t        content=PartitionedContent[Example](\n", "            partition_ids=partition_id,\n\t            content_ids=content_id,\n\t            item=Example(\n\t                model_field=FieldModel(test_field=init_value, failures=2),\n\t                **content.item.dict(exclude={\"model_field\"})\n\t            ),\n\t            current_version=2,\n\t        )\n\t    )\n\t    assert updated == expected\n", "async def test_update_nested_dict_field(v2_example_repo):\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n\t    init_value = fake.bs()\n\t    content = ExamplePartitionedContentFactory(\n\t        partition_ids=partition_id,\n\t        content_ids=content_id,\n\t        item=ExampleFactory(dict_field={\"one\": init_value, \"two\": init_value}),\n\t    )\n\t    await v2_example_repo.put(content)\n", "    new_value = fake.bs()\n\t    await v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n\t        UpdateCommand(set_commands={\"dict_field\": {\"two\": new_value}}),\n\t    )\n\t    updated = await v2_example_repo.get(partition_id, content_id)\n\t    expected = GetResponse(\n\t        content=PartitionedContent[Example](\n\t            partition_ids=partition_id,\n", "            content_ids=content_id,\n\t            item=Example(\n\t                dict_field={\"one\": init_value, \"two\": new_value},\n\t                **content.item.dict(exclude={\"dict_field\"})\n\t            ),\n\t            current_version=2,\n\t        )\n\t    )\n\t    assert updated == expected\n\tdef test_sync_update_nested_dict_field(sync_v2_example_repo):\n", "    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n\t    init_value = fake.bs()\n\t    content = ExamplePartitionedContentFactory(\n\t        partition_ids=partition_id,\n\t        content_ids=content_id,\n\t        item=ExampleFactory(dict_field={\"one\": init_value, \"two\": init_value}),\n\t    )\n\t    sync_v2_example_repo.put(content)\n\t    new_value = fake.bs()\n", "    sync_v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n\t        UpdateCommand(set_commands={\"dict_field\": {\"two\": new_value}}),\n\t    )\n\t    updated = sync_v2_example_repo.get(partition_id, content_id)\n\t    expected = GetResponse(\n\t        content=PartitionedContent[Example](\n\t            partition_ids=partition_id,\n\t            content_ids=content_id,\n", "            item=Example(\n\t                dict_field={\"one\": init_value, \"two\": new_value},\n\t                **content.item.dict(exclude={\"dict_field\"})\n\t            ),\n\t            current_version=2,\n\t        )\n\t    )\n\t    assert updated == expected\n\tasync def test_update_requires_exists_but_doesnt(v2_example_repo):\n\t    partition_id = [fake.bothify()]\n", "    content_id = [fake.bothify()]\n\t    with pytest.raises(RequestObjectStateError) as ex:\n\t        await v2_example_repo.update(\n\t            partition_id=partition_id, content_id=content_id, command=UpdateCommand()\n\t        )\n\t    assert partition_id[0] in str(ex)\n\t    assert content_id[0] in str(ex)\n\tdef test_sync_update_requires_exists_but_doesnt(sync_v2_example_repo):\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n", "    with pytest.raises(RequestObjectStateError) as ex:\n\t        sync_v2_example_repo.update(\n\t            partition_id=partition_id, content_id=content_id, command=UpdateCommand()\n\t        )\n\t    assert partition_id[0] in str(ex)\n\t    assert content_id[0] in str(ex)\n\tasync def test_update_mismatched_version(v2_example_repo):\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n\t    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n", "    await v2_example_repo.put(content)\n\t    new_expiry = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n\t    # First update should succeed and increment version to 2\n\t    await v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n\t        UpdateCommand(\n\t            current_version=1,\n\t            expiry=new_expiry,\n\t        ),\n", "    )\n\t    # Second update should fail because db has current_version of 2\n\t    with pytest.raises(RequestObjectStateError) as ex:\n\t        await v2_example_repo.update(\n\t            partition_id,\n\t            content_id,\n\t            UpdateCommand(\n\t                current_version=1,\n\t                expiry=new_expiry,\n\t            ),\n", "        )\n\t    assert partition_id[0] in str(ex)\n\t    assert content_id[0] in str(ex)\n\tdef test_sync_update_mismatched_version(sync_v2_example_repo):\n\t    partition_id = [fake.bothify()]\n\t    content_id = [fake.bothify()]\n\t    content = ExamplePartitionedContentFactory(partition_ids=partition_id, content_ids=content_id)\n\t    sync_v2_example_repo.put(content)\n\t    new_expiry = datetime.now(tz=ZoneInfo(\"America/New_York\"))\n\t    # First update should succeed and increment version to 2\n", "    sync_v2_example_repo.update(\n\t        partition_id,\n\t        content_id,\n\t        UpdateCommand(\n\t            current_version=1,\n\t            expiry=new_expiry,\n\t        ),\n\t    )\n\t    # Second update should fail because db has current_version of 2\n\t    with pytest.raises(RequestObjectStateError) as ex:\n", "        sync_v2_example_repo.update(\n\t            partition_id,\n\t            content_id,\n\t            UpdateCommand(\n\t                current_version=1,\n\t                expiry=new_expiry,\n\t            ),\n\t        )\n\t    assert partition_id[0] in str(ex)\n\t    assert content_id[0] in str(ex)\n", "async def test_put_batch_then_delete(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    # 25 seems to be the boto3 default size for batch writes, 100 is the limit for batch gets\n\t    # 137 ensures we span both of those limits and do so with partial size batches\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    await v2_example_repo.put_batch(contents)\n", "    await v2_example_repo.delete(partition_ids, [\"0\"])\n\t    remaining = [\n\t        content\n\t        async for response in v2_example_repo.list(\n\t            partition_ids,\n\t            None,\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    assert remaining == contents[100:]\n", "def test_sync_put_batch_then_delete(sync_v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    # 25 seems to be the boto3 default size for batch writes, 100 is the limit for batch gets\n\t    # 137 ensures we span both of those limits and do so with partial size batches\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(137)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n", "    sync_v2_example_repo.delete(partition_ids, [\"0\"])\n\t    remaining = [\n\t        content\n\t        for response in sync_v2_example_repo.list(\n\t            partition_ids,\n\t            None,\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    assert remaining == contents[100:]\n"]}
{"filename": "tests/test_integration/test_v2/__init__.py", "chunked_list": []}
{"filename": "tests/test_integration/test_v2/test_repository_long_running.py", "chunked_list": ["from faker import Faker\n\tfrom tests.factories import ExamplePartitionedContentFactory\n\tfake = Faker()\n\tasync def test_put_batch_then_list_no_prefix_filter_with_sorting(v2_example_repo):\n\t    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(2250)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n", "    await v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n\t        content\n\t        async for response in v2_example_repo.list(\n\t            partition_ids, content_prefix=None, sort_ascending=True\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    contents.sort()\n\t    assert ascending_actual == contents\n", "    descending_actual = [\n\t        content\n\t        async for response in v2_example_repo.list(\n\t            partition_ids, content_prefix=None, sort_ascending=False\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    contents.sort(reverse=True)\n\t    assert descending_actual == contents\n\tdef test_sync_put_batch_then_list_no_prefix_filter_with_sorting(sync_v2_example_repo):\n", "    partition_ids = [fake.bothify()]\n\t    content_ids_list = [[str(i).rjust(3, \"0\")] for i in range(2250)]\n\t    contents = [\n\t        ExamplePartitionedContentFactory(partition_ids=partition_ids, content_ids=content_ids)\n\t        for content_ids in content_ids_list\n\t    ]\n\t    sync_v2_example_repo.put_batch(contents)\n\t    ascending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list(\n", "            partition_ids, content_prefix=None, sort_ascending=True\n\t        )\n\t        for content in response.contents\n\t    ]\n\t    contents.sort()\n\t    assert ascending_actual == contents\n\t    descending_actual = [\n\t        content\n\t        for response in sync_v2_example_repo.list(\n\t            partition_ids, content_prefix=None, sort_ascending=False\n", "        )\n\t        for content in response.contents\n\t    ]\n\t    contents.sort(reverse=True)\n\t    assert descending_actual == contents\n"]}
{"filename": "tests/test_integration/test_v2/test_write_once.py", "chunked_list": ["from faker import Faker\n\tfrom tests.factories import ExamplePartitionedContentFactory\n\tfrom tests.models import Example\n\tfrom pydantic_dynamo.v2.repository import DynamoRepository\n\tfrom pydantic_dynamo.v2.write_once import WriteOnceRepository\n\tfake = Faker()\n\tasync def test_write_once_repo(v2_example_repo: DynamoRepository[Example]):\n\t    repo = WriteOnceRepository[Example](async_repo=v2_example_repo)\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(2)]\n\t    content_ids = [[\"Z\"], [\"B\"], [\"A\"], [\"D\"]]\n", "    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        for c in content_ids\n\t    ]\n\t    written = await repo.write(data)\n\t    actual = [\n\t        c\n\t        for partition_id in partition_ids\n\t        async for response in v2_example_repo.list(partition_id, None)\n", "        for c in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(data)\n\t    assert sorted(actual) == sorted(written)\n\t    written_again = await repo.write(data)\n\t    assert len(written_again) == 0\n\tasync def test_write_once_repo_one_item(v2_example_repo: DynamoRepository[Example]):\n\t    repo = WriteOnceRepository[Example](async_repo=v2_example_repo)\n\t    partition_ids = [[fake.bothify(), fake.bothify()] for _ in range(1)]\n\t    content_ids = [[\"A\"]]\n", "    data = [\n\t        ExamplePartitionedContentFactory(partition_ids=p, content_ids=c)\n\t        for p in partition_ids\n\t        for c in content_ids\n\t    ]\n\t    written = await repo.write(data)\n\t    actual = [\n\t        c\n\t        for partition_id in partition_ids\n\t        async for response in v2_example_repo.list(partition_id, None)\n", "        for c in response.contents\n\t    ]\n\t    assert sorted(actual) == sorted(data)\n\t    assert sorted(actual) == sorted(written)\n\t    written_again = await repo.write(data)\n\t    assert len(written_again) == 0\n"]}
{"filename": "pydantic_dynamo/models.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom datetime import datetime\n\tfrom typing import (\n\t    Generic,\n\t    TypeVar,\n\t    List,\n\t    Optional,\n\t    Dict,\n\t    Any,\n\t    Union,\n", "    Set,\n\t    Sequence,\n\t    Tuple,\n\t    Iterator,\n\t    Iterable,\n\t)\n\tfrom pydantic import BaseModel\n\tfrom pydantic.generics import GenericModel\n\tObjT = TypeVar(\"ObjT\", bound=BaseModel)\n\tclass PartitionedContent(GenericModel, Generic[ObjT]):\n", "    partition_ids: List[str]\n\t    content_ids: List[str]\n\t    item: ObjT\n\t    current_version: int = 1\n\t    expiry: Optional[datetime]\n\t    def __lt__(self, other):\n\t        return (self.partition_ids + self.content_ids) < (other.partition_ids + other.content_ids)\n\t    def __gt__(self, other):\n\t        return (self.partition_ids + self.content_ids) > (other.partition_ids + other.content_ids)\n\t    def __le__(self, other):\n", "        return (self.partition_ids + self.content_ids) <= (other.partition_ids + other.content_ids)\n\t    def __ge__(self, other):\n\t        return (self.partition_ids + self.content_ids) >= (other.partition_ids + other.content_ids)\n\tclass UpdateCommand(BaseModel):\n\t    current_version: Optional[int]\n\t    set_commands: Dict[str, Any] = {}\n\t    increment_attrs: Dict[str, int] = {}\n\t    append_attrs: Dict[str, Optional[List[Union[str, Dict]]]] = {}\n\t    expiry: Optional[datetime]\n\tclass FilterCommand(BaseModel):\n", "    not_exists: Set[str] = set()\n\t    equals: Dict[str, Any] = {}\n\t    not_equals: Dict[str, Any] = {}\n\tclass ReadOnlyAbstractRepository(ABC, Generic[ObjT]):\n\t    @abstractmethod\n\t    def get(\n\t        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n\t    ) -> Optional[ObjT]:\n\t        pass\n\t    @abstractmethod\n", "    def get_batch(\n\t        self,\n\t        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n\t    ) -> List[ObjT]:\n\t        pass\n\t    @abstractmethod\n\t    def list(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n", "        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[ObjT]:\n\t        pass\n\t    @abstractmethod\n\t    def list_between(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_start: Optional[Sequence[str]],\n", "        content_end: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[ObjT]:\n\t        pass\n\tclass AbstractRepository(ReadOnlyAbstractRepository[ObjT], ABC):\n\t    @abstractmethod\n\t    def put(self, content: PartitionedContent[ObjT]) -> None:\n\t        pass\n", "    @abstractmethod\n\t    def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n\t        pass\n\t    @abstractmethod\n\t    def update(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_id: Optional[Sequence[str]],\n\t        command: UpdateCommand,\n\t        require_exists: bool,\n", "    ) -> None:\n\t        pass\n\t    @abstractmethod\n\t    def delete(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t    ) -> None:\n\t        pass\n"]}
{"filename": "pydantic_dynamo/__init__.py", "chunked_list": []}
{"filename": "pydantic_dynamo/utils.py", "chunked_list": ["import logging\n\tfrom datetime import datetime, timezone, date, time\n\tfrom enum import Enum\n\tfrom io import StringIO\n\tfrom typing import Optional, Sequence, Iterable, List, Dict, Any, Set\n\tfrom boto3.dynamodb.conditions import ConditionBase, Attr\n\tfrom pydantic import BaseModel\n\tfrom pydantic_dynamo.constants import INTERNAL_TIMESTAMP_KEY, INTERNAL_TTL, INTERNAL_OBJECT_VERSION\n\tfrom pydantic_dynamo.exceptions import RequestObjectStateError\n\tfrom pydantic_dynamo.models import UpdateCommand, FilterCommand\n", "logger = logging.getLogger(__name__)\n\tdef utc_now() -> datetime:\n\t    return datetime.now(tz=timezone.utc)\n\tdef internal_timestamp() -> Dict[str, str]:\n\t    return {INTERNAL_TIMESTAMP_KEY: utc_now().isoformat()}\n\tdef get_error_code(ex: Exception) -> Optional[str]:\n\t    if hasattr(ex, \"response\"):\n\t        return ex.response.get(\"Error\", {}).get(\"Code\")  # type: ignore[no-any-return,attr-defined]\n\t    return None\n\tdef chunks(items: Sequence, size: int) -> Iterable[Sequence]:\n", "    \"\"\"Yield successive n-sized chunks from items.\"\"\"\n\t    for i in range(0, len(items), size):\n\t        yield items[i : (i + size)]\n\tclass UpdateItemArguments(BaseModel):\n\t    class Config:\n\t        arbitrary_types_allowed = True\n\t    update_expression: str\n\t    condition_expression: Optional[ConditionBase]\n\t    attribute_names: Dict[str, str]\n\t    attribute_values: Dict[str, Any]\n", "async def execute_update_item(table, key: Dict, args: UpdateItemArguments) -> None:\n\t    try:\n\t        update_kwargs = {\n\t            \"Key\": key,\n\t            \"UpdateExpression\": args.update_expression,\n\t            \"ExpressionAttributeNames\": args.attribute_names,\n\t            \"ExpressionAttributeValues\": args.attribute_values,\n\t        }\n\t        if args.condition_expression:\n\t            update_kwargs[\"ConditionExpression\"] = args.condition_expression\n", "        await table.update_item(**update_kwargs)\n\t    except Exception as ex:\n\t        code = get_error_code(ex)\n\t        if code == \"ConditionalCheckFailedException\":\n\t            raise RequestObjectStateError(\n\t                f\"Object version condition failed for: {str(key)}\"\n\t            ) from ex\n\t        raise\n\tdef clean_dict(item_dict: Dict) -> Dict:\n\t    dicts = [item_dict]\n", "    while len(dicts) > 0:\n\t        current_dict = dicts.pop()\n\t        for k, v in current_dict.items():\n\t            if isinstance(v, Dict):\n\t                dicts.append(v)\n\t            elif isinstance(v, BaseModel):\n\t                current_dict[k] = clean_dict(v.dict())\n\t            elif isinstance(v, (List, Set)):\n\t                if len(v) > 0:\n\t                    first = next(iter(v))\n", "                    if isinstance(first, Dict):\n\t                        for obj in v:\n\t                            dicts.append(obj)\n\t                    elif isinstance(first, BaseModel):\n\t                        current_dict[k] = [clean_dict(obj.dict()) for obj in v]\n\t                    else:\n\t                        current_dict[k] = [clean_value(el) for el in v]\n\t            else:\n\t                current_dict[k] = clean_value(v)\n\t    return item_dict\n", "def clean_value(value: Any) -> Any:\n\t    if isinstance(value, (date, time, datetime)):\n\t        return value.isoformat()\n\t    elif isinstance(value, Enum):\n\t        return value.value\n\t    elif isinstance(value, BaseModel):\n\t        return value.dict()\n\t    else:\n\t        return value\n\tdef build_update_args_for_command(\n", "    command: UpdateCommand,\n\t    key: Optional[Dict[str, str]] = None,\n\t) -> UpdateItemArguments:\n\t    set_attrs = command.set_commands\n\t    set_attrs[INTERNAL_TIMESTAMP_KEY] = utc_now()\n\t    if expiry_dt := command.expiry:\n\t        set_attrs[INTERNAL_TTL] = int(expiry_dt.timestamp())\n\t    set_attrs.pop(INTERNAL_OBJECT_VERSION, None)\n\t    update_expression = StringIO()\n\t    update_expression.write(\"SET \")\n", "    names = {}\n\t    values: Dict[str, Any] = {\":zero\": 0}\n\t    attr_count = 0\n\t    val_count = 0\n\t    for k, v in set_attrs.items():\n\t        base_attr_id = f\"#att{attr_count}\"\n\t        names[base_attr_id] = k\n\t        if isinstance(v, Dict):\n\t            for set_k, set_v in v.items():\n\t                if attr_count > 0:\n", "                    update_expression.write(\", \")\n\t                attr_count += 1\n\t                prop_attr_id = f\"#att{attr_count}\"\n\t                attr_id = f\"{base_attr_id}.{prop_attr_id}\"\n\t                val_id = f\":val{val_count}\"\n\t                update_expression.write(f\"{attr_id} = {val_id}\")\n\t                names[prop_attr_id] = set_k\n\t                values[val_id] = set_v\n\t                val_count += 1\n\t            attr_count += 1\n", "        else:\n\t            val_id = f\":val{val_count}\"\n\t            if attr_count > 0:\n\t                update_expression.write(\", \")\n\t            update_expression.write(f\"{base_attr_id} = {val_id}\")\n\t            values[val_id] = v\n\t            val_count += 1\n\t            attr_count += 1\n\t    add_attrs = command.increment_attrs\n\t    add_attrs.update(**{INTERNAL_OBJECT_VERSION: 1})\n", "    add_attrs.pop(INTERNAL_TIMESTAMP_KEY, None)\n\t    for k, v in add_attrs.items():\n\t        attr_id = f\"#att{attr_count}\"\n\t        val_id = f\":val{val_count}\"\n\t        if attr_count > 0:\n\t            update_expression.write(\", \")\n\t        update_expression.write(f\"{attr_id} = if_not_exists({attr_id}, :zero) + {val_id}\")\n\t        names[attr_id] = k\n\t        values[val_id] = v\n\t        attr_count += 1\n", "        val_count += 1\n\t    append_attrs = command.append_attrs\n\t    append_attrs.pop(INTERNAL_TIMESTAMP_KEY, None)\n\t    append_attrs.pop(INTERNAL_OBJECT_VERSION, None)\n\t    if len(append_attrs) > 0:\n\t        values[\":empty_list\"] = []\n\t    for k, v in append_attrs.items():\n\t        attr_id = f\"#att{attr_count}\"\n\t        val_id = f\":val{val_count}\"\n\t        if attr_count > 0:\n", "            update_expression.write(\", \")\n\t        update_expression.write(\n\t            f\"{attr_id} = list_append(if_not_exists({attr_id}, :empty_list), {val_id})\"\n\t        )\n\t        names[attr_id] = k\n\t        values[val_id] = 1 if v is None else v\n\t        attr_count += 1\n\t        val_count += 1\n\t    condition = build_update_condition(command, key)\n\t    clean_values = clean_dict(values)\n", "    arguments = UpdateItemArguments(\n\t        update_expression=update_expression.getvalue(),\n\t        condition_expression=condition,\n\t        attribute_names=names,\n\t        attribute_values=clean_values,\n\t    )\n\t    logger.info(\n\t        \"Generated update item argument expression\",\n\t        extra={\"expression\": arguments.update_expression},\n\t    )\n", "    return arguments\n\tdef build_update_condition(\n\t    command: UpdateCommand,\n\t    key: Optional[Dict[str, str]] = None,\n\t) -> Optional[ConditionBase]:\n\t    condition = None\n\t    if key:\n\t        for k, v in key.items():\n\t            key_condition = Attr(k).eq(v)\n\t            if condition:\n", "                condition = condition & key_condition\n\t            else:\n\t                condition = key_condition\n\t    if command.current_version:\n\t        version_condition = Attr(INTERNAL_OBJECT_VERSION).eq(command.current_version)\n\t        if condition:\n\t            condition = condition & version_condition\n\t        else:\n\t            condition = version_condition\n\t    return condition\n", "def build_filter_expression(filters: FilterCommand) -> Optional[ConditionBase]:\n\t    condition: Optional[ConditionBase] = None\n\t    for attr in filters.not_exists:\n\t        new_condition = Attr(attr).not_exists()\n\t        if condition is None:\n\t            condition = new_condition\n\t        else:\n\t            condition = condition & new_condition\n\t    clean_equals = clean_dict(filters.equals)\n\t    for k, v in clean_equals.items():\n", "        new_condition = Attr(k).eq(v)\n\t        if condition is None:\n\t            condition = new_condition\n\t        else:\n\t            condition = condition & new_condition\n\t    clean_not_equals = clean_dict(filters.not_equals)\n\t    for k, v in clean_not_equals.items():\n\t        new_condition = Attr(k).ne(v)\n\t        if condition is None:\n\t            condition = new_condition\n", "        else:\n\t            condition = condition & new_condition\n\t    return condition\n\tdef validate_command_for_schema(schema: Dict, command: UpdateCommand) -> None:\n\t    schema_props: Dict = schema.get(\"properties\")  # type: ignore[assignment]\n\t    set_error_keys = []\n\t    for k, v in command.set_commands.items():\n\t        schema_prop = schema_props.get(k)\n\t        if not schema_prop:\n\t            set_error_keys.append(k)\n", "            continue\n\t        if isinstance(v, Dict):\n\t            if schema_prop_ref := schema_prop.get(\"$ref\"):\n\t                schema_prop_key = schema_prop_ref.split(\"/\")[-1]\n\t                nested_schema = schema[\"definitions\"][schema_prop_key]\n\t                nested_props: Dict = nested_schema.get(\"properties\")  # type: ignore[assignment]\n\t                for nested_k, nested_v in v.items():\n\t                    if nested_k not in nested_props:\n\t                        set_error_keys.append(f\"{k}.{nested_k}\")\n\t            elif schema_prop[\"type\"] != \"object\":\n", "                set_error_keys.append(k)\n\t    if len(set_error_keys) > 0:\n\t        raise ValueError(\n\t            f\"command contains set attrs not found in {schema.get('title')} type: \"\n\t            f\"{','.join(set_error_keys)}\"\n\t        )\n\t    increment_attrs = command.increment_attrs.keys()\n\t    validate_attrs_in_schema(\n\t        schema,\n\t        (\n", "            *increment_attrs,\n\t            *command.append_attrs.keys(),\n\t        ),\n\t    )\n\t    incr_error_keys = []\n\t    for k in increment_attrs:\n\t        prop = schema_props[k]\n\t        if prop.get(\"type\") != \"integer\":\n\t            incr_error_keys.append(k)\n\t    if len(incr_error_keys) > 0:\n", "        raise ValueError(\n\t            \"command contains increment_attrs that are not integers in \"\n\t            f\"{schema.get('title')} type: \"\n\t            f\"{','.join(incr_error_keys)}\"\n\t        )\n\tdef validate_filters_for_schema(schema: Dict, filters: FilterCommand) -> None:\n\t    validate_attrs_in_schema(\n\t        schema,\n\t        (*filters.not_exists, *filters.equals.keys(), *filters.not_equals.keys()),\n\t    )\n", "def validate_attrs_in_schema(schema: Dict, attrs: Iterable[str]) -> None:\n\t    schema_props = schema[\"properties\"]\n\t    invalid_keys = [a for a in attrs if a not in schema_props]\n\t    if len(invalid_keys) > 0:\n\t        raise ValueError(\n\t            f\"command contains attrs not found in {schema.get('title')} type: \"\n\t            f\"{','.join(invalid_keys)}\"\n\t        )\n"]}
{"filename": "pydantic_dynamo/constants.py", "chunked_list": ["from typing import List\n\tLAST_EVALUATED_KEY = \"LastEvaluatedKey\"\n\tFILTER_EXPRESSION = \"FilterExpression\"\n\tINTERNAL_TIMESTAMP_KEY = \"_timestamp\"\n\tINTERNAL_OBJECT_VERSION = \"_object_version\"\n\tINTERNAL_TTL = \"_ttl\"\n\tEMPTY_LIST: List[str] = []\n"]}
{"filename": "pydantic_dynamo/repository.py", "chunked_list": ["from __future__ import annotations\n\timport logging\n\tfrom boto3 import Session\n\tfrom boto3.dynamodb.conditions import Key\n\tfrom typing import (\n\t    Optional,\n\t    Dict,\n\t    Iterable,\n\t    Type,\n\t    List,\n", "    Any,\n\t    Union,\n\t    Iterator,\n\t    Tuple,\n\t    Sequence,\n\t)\n\tfrom pydantic_dynamo.constants import (\n\t    EMPTY_LIST,\n\t    INTERNAL_OBJECT_VERSION,\n\t    INTERNAL_TTL,\n", "    FILTER_EXPRESSION,\n\t    LAST_EVALUATED_KEY,\n\t)\n\tfrom pydantic_dynamo.exceptions import RequestObjectStateError\n\tfrom pydantic_dynamo.utils import (\n\t    chunks,\n\t    get_error_code,\n\t    internal_timestamp,\n\t    validate_filters_for_schema,\n\t    build_filter_expression,\n", "    validate_command_for_schema,\n\t    clean_dict,\n\t    build_update_args_for_command,\n\t)\n\tfrom pydantic_dynamo.models import (\n\t    UpdateCommand,\n\t    FilterCommand,\n\t    ObjT,\n\t    PartitionedContent,\n\t    AbstractRepository,\n", ")\n\tlogger = logging.getLogger(__name__)\n\tclass DynamoRepository(AbstractRepository[ObjT]):\n\t    @classmethod\n\t    def build(\n\t        cls,\n\t        table_name: str,\n\t        item_class: Type[ObjT],\n\t        partition_prefix: str,\n\t        partition_name: str,\n", "        content_type: str,\n\t    ) -> DynamoRepository[ObjT]:\n\t        resource = Session().resource(\"dynamodb\")\n\t        table = resource.Table(table_name)\n\t        return cls(\n\t            item_class=item_class,\n\t            partition_prefix=partition_prefix,\n\t            partition_name=partition_name,\n\t            content_type=content_type,\n\t            table_name=table_name,\n", "            partition_key=\"_table_item_id\",\n\t            sort_key=\"_table_content_id\",\n\t            table=table,\n\t            resource=resource,\n\t        )\n\t    def __init__(\n\t        self,\n\t        *,\n\t        item_class: Type[ObjT],\n\t        partition_prefix: str,\n", "        partition_name: str,\n\t        content_type: str,\n\t        table_name: str,\n\t        partition_key: str,\n\t        sort_key: str,\n\t        table,\n\t        resource,\n\t    ):\n\t        self._item_class = item_class\n\t        self._item_schema = self._item_class.schema()\n", "        self._partition_prefix = partition_prefix\n\t        self._partition_name = partition_name\n\t        self._content_type = content_type\n\t        self._table_name = table_name\n\t        self._partition_key = partition_key\n\t        self._sort_key = sort_key\n\t        self._table = table\n\t        self._resource = resource\n\t    @property\n\t    def context(self) -> Dict[str, str]:\n", "        return {\n\t            \"item_class\": self._item_class.__name__,\n\t            \"partition_prefix\": self._partition_id(EMPTY_LIST),\n\t            \"content_prefix\": self._content_id(EMPTY_LIST),\n\t        }\n\t    def put(self, content: PartitionedContent[ObjT]) -> None:\n\t        log_context: Dict[str, Any] = {\n\t            \"partition_id\": content.partition_ids,\n\t            \"content_id\": content.content_ids,\n\t            **self.context,\n", "        }\n\t        logger.info(\"Putting single content\", extra=log_context)\n\t        self._put_content(self._table, content)\n\t        logger.info(\"Put single content\", extra=log_context)\n\t    def put_batch(self, batch: Iterable[PartitionedContent[ObjT]]) -> None:\n\t        logger.info(\"Putting batch content\", extra=self.context)\n\t        count = 0\n\t        with self._table.batch_writer() as writer:\n\t            for content in batch:\n\t                self._put_content(writer, content)\n", "                count += 1\n\t        logger.info(\"Finished putting batch content\", extra={\"count\": count, **self.context})\n\t    def _put_content(self, table, content: PartitionedContent[ObjT]) -> None:\n\t        item_dict = clean_dict(content.item.dict())\n\t        item_dict[INTERNAL_OBJECT_VERSION] = 1\n\t        item_dict.update(**internal_timestamp())\n\t        put_item: Dict[str, Union[str, int]] = {\n\t            self._partition_key: self._partition_id(content.partition_ids),\n\t            self._sort_key: self._content_id(content.content_ids),\n\t            **item_dict,\n", "        }\n\t        if expiry := content.expiry:\n\t            put_item[INTERNAL_TTL] = int(expiry.timestamp())\n\t        table.put_item(Item=put_item)\n\t    def update(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_id: Optional[Sequence[str]],\n\t        command: UpdateCommand,\n\t        require_exists: bool = True,\n", "    ) -> None:\n\t        validate_command_for_schema(self._item_schema, command)\n\t        key = {\n\t            self._partition_key: self._partition_id(partition_id),\n\t            self._sort_key: self._content_id(content_id),\n\t        }\n\t        build_kwargs: Dict[str, Any] = {\n\t            \"command\": command,\n\t        }\n\t        if require_exists:\n", "            build_kwargs[\"key\"] = key\n\t        args = build_update_args_for_command(**build_kwargs)  # type: ignore\n\t        try:\n\t            update_kwargs = {\n\t                \"Key\": key,\n\t                \"UpdateExpression\": args.update_expression,\n\t                \"ExpressionAttributeNames\": args.attribute_names,\n\t                \"ExpressionAttributeValues\": args.attribute_values,\n\t            }\n\t            if args.condition_expression:\n", "                update_kwargs[\"ConditionExpression\"] = args.condition_expression\n\t            self._table.update_item(**update_kwargs)\n\t            logger.info(\"Finished updating item\")\n\t        except Exception as ex:\n\t            code = get_error_code(ex)\n\t            if code == \"ConditionalCheckFailedException\":\n\t                raise RequestObjectStateError(\n\t                    f\"Object existence or version condition failed for: {str(key)}\"\n\t                ) from ex\n\t            raise\n", "    def get(\n\t        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n\t    ) -> Optional[ObjT]:\n\t        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_id is None:\n\t            content_id = EMPTY_LIST\n\t        log_context = {\n\t            \"partition_id\": partition_id,\n\t            \"content_id\": content_id,\n", "            **self.context,\n\t        }\n\t        logger.info(\"Getting item from table by key\", extra=log_context)\n\t        response = self._table.get_item(\n\t            Key={\n\t                self._partition_key: self._partition_id(partition_id),\n\t                self._sort_key: self._content_id(content_id),\n\t            }\n\t        )\n\t        db_item = response.get(\"Item\")\n", "        if db_item:\n\t            logger.info(\"Found item from table by key\", extra=log_context)\n\t            item = self._item_class(**db_item)\n\t        else:\n\t            logger.info(\"No item found in table by key\", extra=log_context)\n\t            item = None\n\t        return item\n\t    def get_batch(\n\t        self,\n\t        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n", "    ) -> List[ObjT]:\n\t        records: List[ObjT] = []\n\t        batch_number = 0\n\t        for request_id_batch in chunks(request_ids, size=100):\n\t            batch_number += 1\n\t            logger.info(\n\t                \"Starting batch get items request\",\n\t                extra={\n\t                    \"batch_number\": batch_number,\n\t                    \"batch_size\": len(request_id_batch),\n", "                },\n\t            )\n\t            batch_keys = [\n\t                {\n\t                    self._partition_key: self._partition_id(partition_id),\n\t                    self._sort_key: self._content_id(content_id),\n\t                }\n\t                for partition_id, content_id in request_id_batch\n\t            ]\n\t            batch_response: Dict[str, Any] = self._extend_batch(batch_keys, records)\n", "            while unprocessed_keys := batch_response.get(\"UnprocessedKeys\"):\n\t                logger.info(\n\t                    \"Getting unprocessed keys\",\n\t                    extra={\n\t                        \"batch_number\": batch_number,\n\t                        \"batch_size\": len(unprocessed_keys),\n\t                    },\n\t                )\n\t                batch_response = self._extend_batch(unprocessed_keys, records)\n\t        return records\n", "    def _extend_batch(self, request_keys: List[Dict[str, str]], records: List[ObjT]):\n\t        batch_response = self._resource.batch_get_item(\n\t            RequestItems={self._table_name: {\"Keys\": request_keys}}\n\t        )\n\t        records.extend(\n\t            (self._item_class(**i) for i in batch_response[\"Responses\"].get(self._table_name, []))\n\t        )\n\t        return batch_response\n\t    def list(\n\t        self,\n", "        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[ObjT]:\n\t        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_prefix is None:\n\t            content_prefix = EMPTY_LIST\n", "        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n\t            self._sort_key\n\t        ).begins_with(self._content_id(content_prefix))\n\t        logger.info(\"Starting query for content with prefix query\")\n\t        items = self._query_all_data(condition, sort_ascending, limit, filters)\n\t        yield from (self._item_class(**item) for item in items)\n\t    def list_between(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_start: Optional[Sequence[str]],\n", "        content_end: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[ObjT]:\n\t        log_context = {\n\t            \"partition_id\": partition_id,\n\t            \"content_start\": content_start,\n\t            \"content_end\": content_end,\n\t            **self.context,\n", "        }\n\t        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_start is None:\n\t            content_start = EMPTY_LIST\n\t        if content_end is None:\n\t            content_end = EMPTY_LIST\n\t        if content_start == content_end:\n\t            logger.info(\n\t                \"Content start and end filters are equal. Deferring to list\",\n", "                extra=log_context,\n\t            )\n\t            yield from self.list(partition_id, content_start)\n\t        else:\n\t            partition_id = self._partition_id(partition_id)\n\t            sort_start = self._content_id(content_start)\n\t            sort_end = self._content_id(content_end)\n\t            condition = Key(self._partition_key).eq(partition_id) & Key(self._sort_key).between(\n\t                low_value=sort_start, high_value=sort_end\n\t            )\n", "            logger.info(\n\t                \"Starting query for content in range query\",\n\t                extra={\n\t                    \"partition_id\": partition_id,\n\t                    \"low_value\": sort_start,\n\t                    \"high_value\": sort_end,\n\t                    **log_context,\n\t                },\n\t            )\n\t            items = self._query_all_data(condition, sort_ascending, limit, filters)\n", "            yield from (self._item_class(**item) for item in items)\n\t    def delete(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t    ) -> None:\n\t        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_prefix is None:\n\t            content_prefix = EMPTY_LIST\n", "        log_context = {\n\t            \"partition_id\": partition_id,\n\t            \"content_prefix\": content_prefix,\n\t            **self.context,\n\t        }\n\t        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n\t            self._sort_key\n\t        ).begins_with(self._content_id(content_prefix))\n\t        logger.info(\"Starting query for content to delete with prefix query\", extra=log_context)\n\t        items = self._query_all_data(condition)\n", "        with self._table.batch_writer() as writer:\n\t            for item in items:\n\t                writer.delete_item(\n\t                    Key={\n\t                        self._partition_key: item[self._partition_key],\n\t                        self._sort_key: item[self._sort_key],\n\t                    }\n\t                )\n\t        logger.info(\"Finished deleting content from prefix query\", extra=log_context)\n\t    def _partition_id(self, partition_ids: Optional[Union[str, Sequence[str]]]) -> str:\n", "        if partition_ids is None:\n\t            partition_ids = EMPTY_LIST\n\t        if isinstance(partition_ids, str):\n\t            return f\"{self._partition_prefix}#{self._partition_name}#{partition_ids}\"\n\t        else:\n\t            return f\"{self._partition_prefix}#{self._partition_name}#{'#'.join(partition_ids)}\"\n\t    def _content_id(self, content_ids: Optional[Union[str, Sequence[str]]]) -> str:\n\t        if content_ids is None:\n\t            content_ids = EMPTY_LIST\n\t        if isinstance(content_ids, str):\n", "            return f\"{self._content_type}#{content_ids}\"\n\t        else:\n\t            return f\"{self._content_type}#{'#'.join(content_ids)}\"\n\t    def _query_all_data(\n\t        self,\n\t        key_condition_expression: Key,\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterable[Dict]:\n", "        query_kwargs = {\n\t            \"KeyConditionExpression\": key_condition_expression,\n\t            \"ScanIndexForward\": sort_ascending,\n\t        }\n\t        if filters:\n\t            validate_filters_for_schema(self._item_schema, filters)\n\t            if filter_expression := build_filter_expression(filters):\n\t                query_kwargs[FILTER_EXPRESSION] = filter_expression\n\t        if limit and FILTER_EXPRESSION not in query_kwargs:\n\t            query_kwargs[\"Limit\"] = limit\n", "        response = self._table.query(**query_kwargs)\n\t        total_count = response[\"Count\"]\n\t        items = response.get(\"Items\", [])\n\t        yield from items\n\t        while last_evaluated_key := response.get(LAST_EVALUATED_KEY):\n\t            # TODO: Add tests for limit\n\t            if limit and total_count >= limit:\n\t                return\n\t            logger.info(\n\t                \"Getting next batch of items from content table\",\n", "                extra={\n\t                    \"last_evaluated_key\": last_evaluated_key,\n\t                },\n\t            )\n\t            query_kwargs[\"ExclusiveStartKey\"] = last_evaluated_key\n\t            response = self._table.query(**query_kwargs)\n\t            total_count += response[\"Count\"]\n\t            items = response.get(\"Items\", [])\n\t            yield from items\n"]}
{"filename": "pydantic_dynamo/exceptions.py", "chunked_list": ["class RequestObjectStateError(Exception):\n\t    pass\n"]}
{"filename": "pydantic_dynamo/v2/models.py", "chunked_list": ["from abc import abstractmethod\n\tfrom contextlib import AbstractContextManager, AbstractAsyncContextManager\n\tfrom typing import (\n\t    Generic,\n\t    Optional,\n\t    Iterable,\n\t    Sequence,\n\t    Tuple,\n\t    AsyncIterator,\n\t    Iterator,\n", ")\n\tfrom pydantic.generics import GenericModel\n\tfrom pydantic_dynamo.models import ObjT, PartitionedContent, FilterCommand, UpdateCommand\n\tclass GetResponse(GenericModel, Generic[ObjT]):\n\t    content: Optional[PartitionedContent[ObjT]]\n\tclass BatchResponse(GenericModel, Generic[ObjT]):\n\t    contents: Iterable[PartitionedContent[ObjT]]\n\tclass ReadOnlyAbstractRepository(AbstractAsyncContextManager, Generic[ObjT]):\n\t    @abstractmethod\n\t    async def get(\n", "        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n\t    ) -> GetResponse[ObjT]:\n\t        pass\n\t    @abstractmethod\n\t    def get_batch(\n\t        self,\n\t        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n\t    ) -> AsyncIterator[BatchResponse[ObjT]]:\n\t        pass\n\t    @abstractmethod\n", "    def list(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> AsyncIterator[BatchResponse[ObjT]]:\n\t        pass\n\t    @abstractmethod\n", "    def list_between(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_start: Optional[Sequence[str]],\n\t        content_end: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> AsyncIterator[BatchResponse[ObjT]]:\n\t        pass\n", "class AbstractRepository(ReadOnlyAbstractRepository[ObjT]):\n\t    @abstractmethod\n\t    async def put(self, content: PartitionedContent[ObjT]) -> None:\n\t        pass\n\t    @abstractmethod\n\t    async def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n\t        pass\n\t    @abstractmethod\n\t    async def update(\n\t        self,\n", "        partition_id: Optional[Sequence[str]],\n\t        content_id: Optional[Sequence[str]],\n\t        command: UpdateCommand,\n\t        require_exists: bool = True,\n\t    ) -> None:\n\t        pass\n\t    @abstractmethod\n\t    async def delete(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n", "        content_prefix: Optional[Sequence[str]],\n\t    ) -> None:\n\t        pass\n\tclass SyncReadOnlyAbstractRepository(AbstractContextManager, Generic[ObjT]):\n\t    @abstractmethod\n\t    def get(\n\t        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n\t    ) -> GetResponse[ObjT]:\n\t        pass\n\t    @abstractmethod\n", "    def get_batch(\n\t        self,\n\t        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n\t    ) -> Iterator[BatchResponse[ObjT]]:\n\t        pass\n\t    @abstractmethod\n\t    def list(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n", "        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[BatchResponse[ObjT]]:\n\t        pass\n\t    @abstractmethod\n\t    def list_between(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_start: Optional[Sequence[str]],\n", "        content_end: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[BatchResponse[ObjT]]:\n\t        pass\n\tclass SyncAbstractRepository(SyncReadOnlyAbstractRepository[ObjT]):\n\t    @abstractmethod\n\t    def put(self, content: PartitionedContent[ObjT]) -> None:\n\t        pass\n", "    @abstractmethod\n\t    def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n\t        pass\n\t    @abstractmethod\n\t    def update(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_id: Optional[Sequence[str]],\n\t        command: UpdateCommand,\n\t        require_exists: bool = True,\n", "    ) -> None:\n\t        pass\n\t    @abstractmethod\n\t    def delete(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t    ) -> None:\n\t        pass\n"]}
{"filename": "pydantic_dynamo/v2/__init__.py", "chunked_list": []}
{"filename": "pydantic_dynamo/v2/write_once.py", "chunked_list": ["import logging\n\tfrom collections import defaultdict\n\tfrom typing import Generic, Iterable, List, Dict, Sequence, AsyncIterable, Union\n\tfrom pydantic_dynamo.models import ObjT, PartitionedContent\n\tfrom pydantic_dynamo.v2.models import AbstractRepository\n\tlogger = logging.getLogger(__name__)\n\tclass WriteOnceRepository(Generic[ObjT]):\n\t    def __init__(self, async_repo: AbstractRepository[ObjT]):\n\t        self._async_repo = async_repo\n\t    async def write(\n", "        self,\n\t        input_contents: Union[\n\t            Iterable[PartitionedContent[ObjT]], AsyncIterable[PartitionedContent[ObjT]]\n\t        ],\n\t    ) -> List[PartitionedContent[ObjT]]:\n\t        \"\"\"\n\t        :param input_contents: contents to write if, they do not exist\n\t            or are not identical to existing key's content\n\t        :return: list of contents actually written after checking existing data\n\t        \"\"\"\n", "        partitioned_lists: Dict[Sequence[str], List[PartitionedContent[ObjT]]] = defaultdict(list)\n\t        if isinstance(input_contents, AsyncIterable):\n\t            async for input_content in input_contents:\n\t                partitioned_lists[tuple(input_content.partition_ids)].append(input_content)\n\t        else:\n\t            for input_content in input_contents:\n\t                partitioned_lists[tuple(input_content.partition_ids)].append(input_content)\n\t        if len(partitioned_lists) == 0:\n\t            logger.info(\"Empty input content to save\")\n\t            return []\n", "        new_contents: List[PartitionedContent[ObjT]] = []\n\t        for partition_key, contents in partitioned_lists.items():\n\t            contents.sort()\n\t            content_start = contents[0]\n\t            content_end = contents[-1]\n\t            existing_map = {\n\t                tuple(existing.content_ids): existing\n\t                async for response in self._async_repo.list_between(\n\t                    list(partition_key), content_start.content_ids, content_end.content_ids\n\t                )\n", "                for existing in response.contents\n\t            }\n\t            for input_content in contents:\n\t                existing_item = existing_map.get(tuple(input_content.content_ids))\n\t                if existing_item is None or existing_item != input_content:\n\t                    new_contents.append(input_content)\n\t        if new_contents:\n\t            logger.info(\n\t                \"New contents found to save\",\n\t                extra={\n", "                    \"new_count\": len(new_contents),\n\t                },\n\t            )\n\t            await self._async_repo.put_batch(new_contents)\n\t        else:\n\t            logger.info(\"No new input content found to save\")\n\t        return new_contents\n"]}
{"filename": "pydantic_dynamo/v2/repository.py", "chunked_list": ["from __future__ import annotations\n\timport logging\n\tfrom datetime import datetime, timezone\n\tfrom typing import (\n\t    Type,\n\t    Dict,\n\t    List,\n\t    Any,\n\t    Iterable,\n\t    Union,\n", "    Optional,\n\t    Sequence,\n\t    Tuple,\n\t    AsyncGenerator,\n\t    AsyncIterator,\n\t)\n\tfrom boto3.dynamodb.conditions import Key\n\tfrom pydantic_dynamo.constants import (\n\t    EMPTY_LIST,\n\t    INTERNAL_OBJECT_VERSION,\n", "    INTERNAL_TTL,\n\t    FILTER_EXPRESSION,\n\t    LAST_EVALUATED_KEY,\n\t)\n\tfrom pydantic_dynamo.models import ObjT, PartitionedContent, UpdateCommand, FilterCommand\n\tfrom pydantic_dynamo.utils import (\n\t    clean_dict,\n\t    internal_timestamp,\n\t    validate_command_for_schema,\n\t    chunks,\n", "    validate_filters_for_schema,\n\t    build_filter_expression,\n\t    build_update_args_for_command,\n\t    execute_update_item,\n\t)\n\tfrom pydantic_dynamo.v2.models import AbstractRepository, GetResponse, BatchResponse\n\tlogger = logging.getLogger(__name__)\n\tclass DynamoRepository(AbstractRepository[ObjT]):\n\t    def __init__(\n\t        self,\n", "        *,\n\t        item_class: Type[ObjT],\n\t        partition_prefix: str,\n\t        partition_name: str,\n\t        content_type: str,\n\t        table_name: str,\n\t        partition_key: str,\n\t        sort_key: str,\n\t        table,\n\t        resource,\n", "        consistent_reads: bool = False,\n\t    ):\n\t        \"\"\"\n\t        :param item_class: pydantic model class reference representing a single\n\t            database record. This should match ObjT\n\t        :param partition_prefix: first part of the concatenated value used in the\n\t            partition key value.\n\t            Shared amongst all records saved and read by this repository.\n\t        :param partition_name: second part of the concatenated value used in the\n\t            partition key value.\n", "            Shared amongst all records saved and read by this repository.\n\t        :param content_type: first part of the concatenated value used in the sort key value.\n\t            Shared amongst all records saved and read by this repository.\n\t        :param table_name: the region-specific name of the DynamoDB table\n\t        :param partition_key: the name of the attribute defined as the table's partition key\n\t        :param sort_key: the name of the attribute defined as the table's sort key\n\t        :param table: instance of the Table service object\n\t            from `await resource.Table(table_name)`\n\t        :param resource: instance of the resource service object\n\t            from `async with aioboto3.session.Session().resource(**kwargs)`\n", "        :param consistent_reads: boolean to determine if read operations should be performed\n\t            with strong consistency. Default is False and uses eventual consistency\n\t        \"\"\"\n\t        self._item_class = item_class\n\t        self._item_schema = self._item_class.schema()\n\t        self._partition_prefix = partition_prefix\n\t        self._partition_name = partition_name\n\t        self._content_type = content_type\n\t        self._table_name = table_name\n\t        self._partition_key = partition_key\n", "        self._sort_key = sort_key\n\t        self._table = table\n\t        self._resource = resource\n\t        self._consistent_reads = consistent_reads\n\t    async def __aexit__(self, exc_type, exc_val, exc_tb):\n\t        pass\n\t    @property\n\t    def context(self) -> Dict[str, str]:\n\t        return {\n\t            \"item_class\": self._item_class.__name__,\n", "            \"partition_prefix\": self._partition_id(EMPTY_LIST),\n\t            \"content_prefix\": self._content_id(EMPTY_LIST),\n\t        }\n\t    async def put(self, content: PartitionedContent[ObjT]) -> None:\n\t        log_context: Dict[str, Any] = {\n\t            \"partition_id\": content.partition_ids,\n\t            \"content_id\": content.content_ids,\n\t            **self.context,\n\t        }\n\t        logger.info(\"Putting single content\", extra=log_context)\n", "        await self._put_content(self._table, content)\n\t        logger.info(\"Put single content\", extra=log_context)\n\t    async def put_batch(self, batch: Iterable[PartitionedContent[ObjT]]) -> None:\n\t        logger.info(\"Putting batch content\", extra=self.context)\n\t        count = 0\n\t        async with self._table.batch_writer() as writer:\n\t            for content in batch:\n\t                await self._put_content(writer, content)\n\t                count += 1\n\t        logger.info(\"Finished putting batch content\", extra={\"count\": count, **self.context})\n", "    async def _put_content(self, table, content: PartitionedContent[ObjT]) -> None:\n\t        item_dict = clean_dict(content.item.dict())\n\t        item_dict[INTERNAL_OBJECT_VERSION] = content.current_version\n\t        item_dict.update(**internal_timestamp())\n\t        put_item: Dict[str, Union[str, int]] = {\n\t            self._partition_key: self._partition_id(content.partition_ids),\n\t            self._sort_key: self._content_id(content.content_ids),\n\t            **item_dict,\n\t        }\n\t        if expiry := content.expiry:\n", "            put_item[INTERNAL_TTL] = int(expiry.timestamp())\n\t        await table.put_item(Item=put_item)\n\t    async def update(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_id: Optional[Sequence[str]],\n\t        command: UpdateCommand,\n\t        require_exists: bool = True,\n\t    ) -> None:\n\t        validate_command_for_schema(self._item_schema, command)\n", "        key = {\n\t            self._partition_key: self._partition_id(partition_id),\n\t            self._sort_key: self._content_id(content_id),\n\t        }\n\t        build_kwargs: Dict[str, Any] = {\n\t            \"command\": command,\n\t        }\n\t        if require_exists:\n\t            build_kwargs[\"key\"] = key\n\t        args = build_update_args_for_command(**build_kwargs)  # type: ignore\n", "        await execute_update_item(self._table, key, args)\n\t    async def get(\n\t        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n\t    ) -> GetResponse[ObjT]:\n\t        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_id is None:\n\t            content_id = EMPTY_LIST\n\t        log_context = {\n\t            \"partition_id\": partition_id,\n", "            \"content_id\": content_id,\n\t            **self.context,\n\t        }\n\t        logger.info(\"Getting item from table by key\", extra=log_context)\n\t        response = await self._table.get_item(\n\t            Key={\n\t                self._partition_key: self._partition_id(partition_id),\n\t                self._sort_key: self._content_id(content_id),\n\t            },\n\t            ConsistentRead=self._consistent_reads,\n", "        )\n\t        db_item = response.get(\"Item\")\n\t        if db_item:\n\t            logger.info(\"Found item from table by key\", extra=log_context)\n\t            content = self._db_item_to_object(db_item)\n\t        else:\n\t            logger.info(\"No item found in table by key\", extra=log_context)\n\t            content = None\n\t        return GetResponse(content=content)\n\t    async def get_batch(\n", "        self,\n\t        request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]],\n\t    ) -> AsyncIterator[BatchResponse[ObjT]]:\n\t        batch_number = 0\n\t        for request_id_batch in chunks(request_ids, size=100):\n\t            batch_number += 1\n\t            logger.info(\n\t                \"Starting batch get items request\",\n\t                extra={\n\t                    \"batch_number\": batch_number,\n", "                    \"batch_size\": len(request_id_batch),\n\t                },\n\t            )\n\t            batch_keys = [\n\t                {\n\t                    self._partition_key: self._partition_id(partition_id),\n\t                    self._sort_key: self._content_id(content_id),\n\t                }\n\t                for partition_id, content_id in request_id_batch\n\t            ]\n", "            batch_response = await self._resource.batch_get_item(\n\t                RequestItems={\n\t                    self._table_name: {\"Keys\": batch_keys, \"ConsistentRead\": self._consistent_reads}\n\t                }\n\t            )\n\t            yield BatchResponse(\n\t                contents=(\n\t                    self._db_item_to_object(db_item)\n\t                    for db_item in batch_response[\"Responses\"].get(self._table_name, [])\n\t                )\n", "            )\n\t            while unprocessed_keys := batch_response.get(\"UnprocessedKeys\"):\n\t                logger.info(\n\t                    \"Getting unprocessed keys\",\n\t                    extra={\n\t                        \"batch_number\": batch_number,\n\t                        \"batch_size\": len(unprocessed_keys),\n\t                    },\n\t                )\n\t                batch_response = await self._resource.batch_get_item(\n", "                    RequestItems={\n\t                        self._table_name: {\n\t                            \"Keys\": unprocessed_keys,\n\t                            \"ConsistentRead\": self._consistent_reads,\n\t                        }\n\t                    }\n\t                )\n\t                yield BatchResponse(\n\t                    contents=(\n\t                        self._db_item_to_object(db_item)\n", "                        for db_item in batch_response[\"Responses\"].get(self._table_name, [])\n\t                    )\n\t                )\n\t    async def list(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n", "    ) -> AsyncIterator[BatchResponse[ObjT]]:\n\t        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_prefix is None:\n\t            content_prefix = EMPTY_LIST\n\t        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n\t            self._sort_key\n\t        ).begins_with(self._content_id(content_prefix))\n\t        logger.info(\"Starting query for content with prefix query\")\n\t        contents = [\n", "            self._db_item_to_object(db_item)\n\t            async for items in self._query_all_data(condition, sort_ascending, limit, filters)\n\t            for db_item in items\n\t        ]\n\t        yield BatchResponse(contents=contents)\n\t    async def list_between(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_start: Optional[Sequence[str]],\n\t        content_end: Optional[Sequence[str]],\n", "        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> AsyncIterator[BatchResponse[ObjT]]:\n\t        log_context = {\n\t            \"partition_id\": partition_id,\n\t            \"content_start\": content_start,\n\t            \"content_end\": content_end,\n\t            **self.context,\n\t        }\n", "        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_start is None:\n\t            content_start = EMPTY_LIST\n\t        if content_end is None:\n\t            content_end = EMPTY_LIST\n\t        if content_start == content_end:\n\t            logger.info(\n\t                \"Content start and end filters are equal. Deferring to list\",\n\t                extra=log_context,\n", "            )\n\t            async for response in self.list(partition_id, content_start):\n\t                yield response\n\t        else:\n\t            partition_id = self._partition_id(partition_id)\n\t            sort_start = self._content_id(content_start)\n\t            sort_end = self._content_id(content_end)\n\t            condition = Key(self._partition_key).eq(partition_id) & Key(self._sort_key).between(\n\t                low_value=sort_start, high_value=sort_end\n\t            )\n", "            logger.info(\n\t                \"Starting query for content in range query\",\n\t                extra={\n\t                    \"partition_id\": partition_id,\n\t                    \"low_value\": sort_start,\n\t                    \"high_value\": sort_end,\n\t                    **log_context,\n\t                },\n\t            )\n\t            contents = [\n", "                self._db_item_to_object(db_item)\n\t                async for items in self._query_all_data(condition, sort_ascending, limit, filters)\n\t                for db_item in items\n\t            ]\n\t            yield BatchResponse(contents=contents)\n\t    def _db_item_to_object(self, db_item: Dict[str, Any]) -> PartitionedContent[ObjT]:\n\t        expiry: Optional[datetime] = None\n\t        if db_expiry := db_item.pop(INTERNAL_TTL, None):\n\t            expiry = datetime.fromtimestamp(int(db_expiry), tz=timezone.utc)\n\t        item_kwargs = {\n", "            \"partition_ids\": db_item.pop(self._partition_key)\n\t            .replace(self._partition_id(EMPTY_LIST), \"\", 1)\n\t            .split(\"#\"),\n\t            \"content_ids\": db_item.pop(self._sort_key)\n\t            .replace(self._content_id(EMPTY_LIST), \"\", 1)\n\t            .split(\"#\"),\n\t            \"item\": self._item_class(**db_item),\n\t            \"expiry\": expiry,\n\t        }\n\t        if version := db_item.pop(INTERNAL_OBJECT_VERSION, None):\n", "            item_kwargs[\"current_version\"] = version\n\t        return PartitionedContent[self._item_class](**item_kwargs)  # type: ignore[name-defined]\n\t    async def delete(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t    ) -> None:\n\t        if partition_id is None:\n\t            partition_id = EMPTY_LIST\n\t        if content_prefix is None:\n", "            content_prefix = EMPTY_LIST\n\t        log_context = {\n\t            \"partition_id\": partition_id,\n\t            \"content_prefix\": content_prefix,\n\t            **self.context,\n\t        }\n\t        condition = Key(self._partition_key).eq(self._partition_id(partition_id)) & Key(\n\t            self._sort_key\n\t        ).begins_with(self._content_id(content_prefix))\n\t        logger.info(\"Starting query for content to delete with prefix query\", extra=log_context)\n", "        responses = self._query_all_data(\n\t            condition, select_fields=(self._partition_key, self._sort_key)\n\t        )\n\t        async with self._table.batch_writer() as writer:\n\t            async for items in responses:\n\t                for item in items:\n\t                    await writer.delete_item(\n\t                        Key={\n\t                            self._partition_key: item[self._partition_key],\n\t                            self._sort_key: item[self._sort_key],\n", "                        }\n\t                    )\n\t        logger.info(\"Finished deleting content from prefix query\", extra=log_context)\n\t    def _partition_id(self, partition_ids: Optional[Union[str, Sequence[str]]]) -> str:\n\t        if partition_ids is None:\n\t            partition_ids = EMPTY_LIST\n\t        if isinstance(partition_ids, str):\n\t            return f\"{self._partition_prefix}#{self._partition_name}#{partition_ids}\"\n\t        else:\n\t            return f\"{self._partition_prefix}#{self._partition_name}#{'#'.join(partition_ids)}\"\n", "    def _content_id(self, content_ids: Optional[Union[str, Sequence[str]]]) -> str:\n\t        if content_ids is None:\n\t            content_ids = EMPTY_LIST\n\t        if isinstance(content_ids, str):\n\t            return f\"{self._content_type}#{content_ids}\"\n\t        else:\n\t            return f\"{self._content_type}#{'#'.join(content_ids)}\"\n\t    async def _query_all_data(\n\t        self,\n\t        key_condition_expression: Key,\n", "        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t        select_fields: Optional[Sequence[str]] = None,\n\t    ) -> AsyncGenerator[List[Dict], None]:\n\t        query_kwargs = {\n\t            \"KeyConditionExpression\": key_condition_expression,\n\t            \"ScanIndexForward\": sort_ascending,\n\t            \"ConsistentRead\": self._consistent_reads,\n\t        }\n", "        if filters:\n\t            validate_filters_for_schema(self._item_schema, filters)\n\t            if filter_expression := build_filter_expression(filters):\n\t                query_kwargs[FILTER_EXPRESSION] = filter_expression\n\t        if limit and FILTER_EXPRESSION not in query_kwargs:\n\t            query_kwargs[\"Limit\"] = limit\n\t        if select_fields:\n\t            expression_attribute_names = {\n\t                f\"#att{i}\": field for i, field in enumerate(select_fields)\n\t            }\n", "            query_kwargs.update(\n\t                **{\n\t                    \"Select\": \"SPECIFIC_ATTRIBUTES\",\n\t                    \"ProjectionExpression\": \", \".join(expression_attribute_names.keys()),\n\t                    \"ExpressionAttributeNames\": expression_attribute_names,\n\t                }\n\t            )\n\t        response = await self._table.query(**query_kwargs)\n\t        total_count = response[\"Count\"]\n\t        items = response.get(\"Items\", [])\n", "        yield items\n\t        while last_evaluated_key := response.get(LAST_EVALUATED_KEY):\n\t            # TODO: Add tests for limit\n\t            if limit and total_count >= limit:\n\t                return\n\t            logger.info(\n\t                \"Getting next batch of items from content table\",\n\t                extra={\n\t                    \"last_evaluated_key\": last_evaluated_key,\n\t                },\n", "            )\n\t            query_kwargs[\"ExclusiveStartKey\"] = last_evaluated_key\n\t            response = await self._table.query(**query_kwargs)\n\t            total_count += response[\"Count\"]\n\t            items = response.get(\"Items\", [])\n\t            yield items\n"]}
{"filename": "pydantic_dynamo/v2/sync_repository.py", "chunked_list": ["import asyncio\n\tfrom asyncio import AbstractEventLoop\n\tfrom typing import Optional, Sequence, Iterable, Tuple, AsyncIterable, TypeVar, Iterator\n\tfrom pydantic_dynamo.models import ObjT, FilterCommand, UpdateCommand, PartitionedContent\n\tfrom pydantic_dynamo.v2.models import (\n\t    SyncAbstractRepository,\n\t    BatchResponse,\n\t    GetResponse,\n\t    AbstractRepository,\n\t)\n", "Output = TypeVar(\"Output\")\n\tclass SyncDynamoRepository(SyncAbstractRepository[ObjT]):\n\t    def __init__(self, async_repo: AbstractRepository[ObjT]):\n\t        self._async_repo = async_repo\n\t    def __exit__(self, exc_type, exc_val, exc_tb):\n\t        pass\n\t    def put(self, content: PartitionedContent[ObjT]) -> None:\n\t        loop = asyncio.get_event_loop()\n\t        loop.run_until_complete(self._async_repo.put(content))\n\t    def put_batch(self, content: Iterable[PartitionedContent[ObjT]]) -> None:\n", "        loop = asyncio.get_event_loop()\n\t        loop.run_until_complete(self._async_repo.put_batch(content))\n\t    def update(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_id: Optional[Sequence[str]],\n\t        command: UpdateCommand,\n\t        require_exists: bool = True,\n\t    ) -> None:\n\t        loop = asyncio.get_event_loop()\n", "        loop.run_until_complete(\n\t            self._async_repo.update(partition_id, content_id, command, require_exists)\n\t        )\n\t    def delete(\n\t        self, partition_id: Optional[Sequence[str]], content_prefix: Optional[Sequence[str]]\n\t    ) -> None:\n\t        loop = asyncio.get_event_loop()\n\t        loop.run_until_complete(self._async_repo.delete(partition_id, content_prefix))\n\t    def get(\n\t        self, partition_id: Optional[Sequence[str]], content_id: Optional[Sequence[str]]\n", "    ) -> GetResponse[ObjT]:\n\t        loop = asyncio.get_event_loop()\n\t        return loop.run_until_complete(self._async_repo.get(partition_id, content_id))\n\t    def get_batch(\n\t        self, request_ids: Sequence[Tuple[Optional[Sequence[str]], Optional[Sequence[str]]]]\n\t    ) -> Iterator[BatchResponse[ObjT]]:\n\t        loop = asyncio.get_event_loop()\n\t        return iter_over_async(self._async_repo.get_batch(request_ids), loop)\n\t    def list(\n\t        self,\n", "        partition_id: Optional[Sequence[str]],\n\t        content_prefix: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[BatchResponse[ObjT]]:\n\t        loop = asyncio.get_event_loop()\n\t        return iter_over_async(\n\t            self._async_repo.list(partition_id, content_prefix, sort_ascending, limit, filters),\n\t            loop,\n", "        )\n\t    def list_between(\n\t        self,\n\t        partition_id: Optional[Sequence[str]],\n\t        content_start: Optional[Sequence[str]],\n\t        content_end: Optional[Sequence[str]],\n\t        sort_ascending: bool = True,\n\t        limit: Optional[int] = None,\n\t        filters: Optional[FilterCommand] = None,\n\t    ) -> Iterator[BatchResponse[ObjT]]:\n", "        loop = asyncio.get_event_loop()\n\t        return iter_over_async(\n\t            self._async_repo.list_between(\n\t                partition_id, content_start, content_end, sort_ascending, limit, filters\n\t            ),\n\t            loop,\n\t        )\n\tdef iter_over_async(\n\t    async_iterable: AsyncIterable[Output], loop: AbstractEventLoop\n\t) -> Iterator[Output]:\n", "    async_iterator = async_iterable.__aiter__()\n\t    async def get_next():\n\t        try:\n\t            next_obj = await async_iterator.__anext__()\n\t            return False, next_obj\n\t        except StopAsyncIteration:\n\t            return True, None\n\t    while True:\n\t        done, next_obj = loop.run_until_complete(get_next())\n\t        if done:\n", "            break\n\t        yield next_obj\n"]}
