{"filename": "prfiesta/spinner.py", "chunked_list": ["from logging import Logger\n\tfrom rich.spinner import Spinner\n\tfrom prfiesta import SPINNER_STYLE\n\tdef update_spinner(message: str, spinner: Spinner, logger: Logger) -> None:\n\t    logger.debug(message)\n\t    if spinner:\n\t        spinner.update(text=message, style=SPINNER_STYLE)\n"]}
{"filename": "prfiesta/environment.py", "chunked_list": ["import os\n\tfrom github.Consts import DEFAULT_BASE_URL as GITHUB_DEFAULT_BASE_URL\n\tclass GitHubEnvironment:\n\t    def get_token(self) -> str:\n\t        \"\"\"Gets the authentication token for this environment.\"\"\"\n\t        token = os.environ.get('GITHUB_ENTERPRISE_TOKEN', os.environ.get('GITHUB_TOKEN'))\n\t        if not token:\n\t            raise ValueError('GITHUB_ENTERPRISE_TOKEN or GITHUB_TOKEN must be set')\n\t        return token\n\t    def get_url(self) -> str:\n", "        \"\"\"Gets the URL for the git provider.\"\"\"\n\t        return os.environ.get('GH_HOST', GITHUB_DEFAULT_BASE_URL)\n"]}
{"filename": "prfiesta/__main__.py", "chunked_list": ["import logging\n\tfrom datetime import datetime\n\timport click\n\timport cloup\n\tfrom rich.live import Live\n\tfrom rich.spinner import Spinner\n\tfrom rich.text import Text\n\tfrom prfiesta import SPINNER_STYLE, __version__\n\tfrom prfiesta.collectors.github import GitHubCollector\n\tfrom prfiesta.environment import GitHubEnvironment\n", "from prfiesta.output import output_frame\n\tlogger = logging.getLogger(__name__)\n\tgithub_environment = GitHubEnvironment()\n\t@cloup.command()\n\t@cloup.option_group(\n\t    'general options',\n\t    cloup.option('-u', '--users', required=True, multiple=True, help='The GitHub Users to search for. Can be multiple'),\n\t)\n\t@cloup.option_group(\n\t    'date filter options',\n", "    cloup.option('-a', '--after', type=click.DateTime(formats=['%Y-%m-%d']), help='Only search for pull requests after this date e.g 2023-01-01'),\n\t    cloup.option('-b', '--before', type=click.DateTime(formats=['%Y-%m-%d']), help='Only search for pull requests before this date e.g 2023-04-30'),\n\t    cloup.option('-d', '--use-updated', is_flag=True, default=False, help='filter on when the pr was last updated rather then created'),\n\t)\n\t@cloup.option_group(\n\t    'user filter options',\n\t    cloup.option('-i', '--use-involves', is_flag=True, default=False, help='collect prs where the users are the author or assignee or mentioned or commented'),\n\t    cloup.option('-r', '--use-reviewed-by', is_flag=True, default=False, help='collect prs where the users reviewed them'),\n\t    cloup.option('-rr', '--use-review-requested', is_flag=True, default=False, help='collect prs where the users were requested a review'),\n\t    constraint=cloup.constraints.mutually_exclusive,\n", "    help='Collect alternative details for the users. If omitted then just collect the prs that a user has authored.',\n\t)\n\t@cloup.option_group(\n\t    'output options',\n\t    cloup.option('-o', '--output', default=None, help='The output location'),\n\t    cloup.option(\n\t        '-ot', '--output-type', type=click.Choice(['csv', 'parquet', 'duckdb']), default='csv', show_default=True, show_choices=True, help='The output format'),\n\t    cloup.option('-dc', '--drop-columns', multiple=True, help='Drop columns from the output dataframe'),\n\t)\n\t@cloup.option_group(\n", "    'authentication options',\n\t    cloup.option('-x', '--url', help='The URL of the Git provider to use'),\n\t    cloup.option('-t', '--token', help='The Authentication token to use'),\n\t)\n\t@cloup.version_option(__version__)\n\tdef main(**kwargs) -> None:\n\t    users: tuple[str] = kwargs.get('users')\n\t    token: str = kwargs.get('token') or github_environment.get_token()\n\t    url: str = kwargs.get('url') or github_environment.get_url()\n\t    output: str = kwargs.get('output')\n", "    output_type: str = kwargs.get('output_type')\n\t    before: datetime = kwargs.get('before')\n\t    after: datetime = kwargs.get('after')\n\t    drop_columns: list[str] = list(kwargs.get('drop_columns'))\n\t    use_updated: bool = kwargs.get('use_updated')\n\t    use_involves: bool = kwargs.get('use_involves')\n\t    use_reviewed_by: bool = kwargs.get('use_reviewed_by')\n\t    use_review_requested: bool = kwargs.get('use_review_requested')\n\t    logger.info('[bold green]PR Fiesta ü¶úü•≥')\n\t    spinner = Spinner('dots', text=Text('Loading', style=SPINNER_STYLE))\n", "    with Live(spinner, refresh_per_second=20, transient=True):\n\t        collector = GitHubCollector(token=token, url=url, spinner=spinner, drop_columns=drop_columns)\n\t        pr_frame = collector.collect(\n\t            *users,\n\t            after=after,\n\t            before=before,\n\t            use_updated=use_updated,\n\t            use_involves=use_involves,\n\t            use_reviewed_by=use_reviewed_by,\n\t            use_review_requested=use_review_requested)\n", "        if not pr_frame.empty:\n\t            logger.info('Found [bold green]%s[/bold green] pull requests!', pr_frame.shape[0])\n\t            output_frame(pr_frame, output_type, spinner=spinner, output_name=output)\n\t            logger.info('Time to analyze üîé See https://github.com/kiran94/prfiesta/blob/main/docs/analysis.md for some inspiration!')\n\tif __name__ == '__main__':  # pragma: nocover\n\t    main()\n"]}
{"filename": "prfiesta/__init__.py", "chunked_list": ["import importlib.metadata\n\timport logging\n\timport os\n\tfrom rich.logging import RichHandler\n\t__version__ = importlib.metadata.version(__name__)\n\tLOGGING_LEVEL=os.environ.get('LOGGING_LEVEL', logging.INFO)\n\tLOGGING_FORMAT=os.environ.get('LOGGING_FORMAT', '%(message)s')\n\tSPINNER_STYLE=os.environ.get('SPINNER_STYLE', 'blue')\n\tlogging.basicConfig(\n\t    level=LOGGING_LEVEL,\n", "    format=LOGGING_FORMAT,\n\t    handlers=[RichHandler(markup=True, show_path=False, show_time=False, show_level=True)],\n\t)\n"]}
{"filename": "prfiesta/output.py", "chunked_list": ["import logging\n\timport os\n\timport re\n\tfrom datetime import datetime\n\tfrom typing import Literal\n\timport duckdb\n\timport pandas as pd\n\tfrom rich.spinner import Spinner\n\tfrom prfiesta.spinner import update_spinner\n\tlogger = logging.getLogger(__name__)\n", "OUTPUT_TYPE = Literal['csv', 'parquet', 'duckdb']\n\tWIN_ILLEGAL_FILENAME = r'[:\\/\\\\\\*\\?\\\"\\<\\>\\|]'\n\tdef output_frame(\n\t        frame: pd.DataFrame,\n\t        output_type: OUTPUT_TYPE,\n\t        spinner: Spinner,\n\t        output_name: str = None,\n\t        timestamp: datetime = None) -> None:\n\t    if not timestamp:\n\t        timestamp = datetime.now()\n", "    if not output_name:\n\t        output_name = f\"export.{timestamp.strftime('%Y%m%d%H%M%S')}.{output_type}\"\n\t    if os.name == 'nt' and re.search(WIN_ILLEGAL_FILENAME, output_name):\n\t        msg = f'{output_name} is an invalid filename on windows'\n\t        raise ValueError(msg)\n\t    update_spinner(f'Writing export to {output_name}', spinner, logger)\n\t    if output_type == 'csv':\n\t        frame.to_csv(output_name, index=False)\n\t    elif output_type == 'parquet':\n\t        frame.to_parquet(output_name, index=False)\n", "    elif output_type == 'duckdb':\n\t        duckdb_table = f\"prfiesta_{timestamp.strftime('%Y%m%d_%H%M%S')}\"\n\t        update_spinner(f'connecting to duckdb {output_name}', spinner, logger)\n\t        conn = duckdb.connect(output_name)\n\t        update_spinner(f'exporting to duckdb table {duckdb_table}', spinner, logger)\n\t        conn.execute(f'CREATE TABLE {duckdb_table} AS SELECT * FROM frame') # noqa: duckdb_table is always constructed internally\n\t        conn.close()\n\t    else:\n\t        raise ValueError('unknown output_type %s', output_type)\n\t    logger.info('Exported to %s!', output_name)\n"]}
{"filename": "prfiesta/collectors/__init__.py", "chunked_list": []}
{"filename": "prfiesta/collectors/github.py", "chunked_list": ["import logging\n\tfrom datetime import datetime\n\tfrom typing import List, Optional, Tuple\n\timport pandas as pd\n\tfrom github import Github\n\tfrom github.GithubException import RateLimitExceededException\n\tfrom rich.spinner import Spinner\n\tfrom prfiesta.environment import GitHubEnvironment\n\tfrom prfiesta.spinner import update_spinner\n\tlogger = logging.getLogger(__name__)\n", "class GitHubCollector:\n\t    def __init__(self, **kwargs) -> None:\n\t        environment = GitHubEnvironment()\n\t        token = kwargs.get('token') or environment.get_token()\n\t        self._url = kwargs.get('url') or environment.get_url()\n\t        self._github = Github(token, base_url=self._url)\n\t        self._spinner: Spinner = kwargs.get('spinner')\n\t        self._sort_column = ['updated_at']\n\t        self._drop_columns = kwargs.get('drop_columns') or ['node_id', 'performed_via_github_app']\n\t        self._move_to_end_columns = [\n", "            'url',\n\t            'repository_url',\n\t            'html_url',\n\t            'timeline_url',\n\t            'labels_url',\n\t            'comments_url',\n\t            'events_url',\n\t        ]\n\t        self._datetime_columns = [\n\t            'created_at',\n", "            'updated_at',\n\t            'closed_at',\n\t        ]\n\t    def collect(\n\t            self,\n\t            *users: Tuple[str],\n\t            after: Optional[datetime] = None,\n\t            before: Optional[datetime] = None,\n\t            use_updated: Optional[bool] = False,\n\t            use_involves: Optional[bool] = False,\n", "            use_reviewed_by: Optional[bool] = False,\n\t            use_review_requested: Optional[bool] = False,\n\t        ) -> pd.DataFrame:\n\t        query = self._construct_query(users, after, before, use_updated, use_involves, use_reviewed_by, use_review_requested)\n\t        update_spinner(f'Searching {self._url} with[bold blue] {query}', self._spinner,  logger)\n\t        pull_request_data = None\n\t        try:\n\t            pulls = self._github.search_issues(query=query)\n\t            pull_request_data: list[dict] = []\n\t            for pr in pulls:\n", "                pull_request_data.append(pr.__dict__['_rawData'])\n\t        except RateLimitExceededException as e:\n\t            logger.warning('üôá You were rate limited by the GitHub API, try requesting less data.')\n\t            logger.debug(e)\n\t            return pd.DataFrame()\n\t        if not pull_request_data:\n\t            logger.warning('Did not find any results for this search criteria!')\n\t            return pd.DataFrame()\n\t        update_spinner('Post Processing', self._spinner, logger)\n\t        pr_frame = pd.json_normalize(pull_request_data)\n", "        pr_frame = pr_frame.drop(columns=self._drop_columns, errors='ignore')\n\t        pr_frame = pr_frame.sort_values(by=self._sort_column, ascending=False)\n\t        pr_frame = self._parse_datetime_columns(pr_frame)\n\t        pr_frame['repository_name'] = pr_frame['repository_url'].str.extract(r'(.*)\\/repos\\/(?P<repository_name>(.*))')['repository_name']\n\t        pr_frame = self._move_column_to_end(pr_frame)\n\t        return pr_frame\n\t    @staticmethod\n\t    def _construct_query(\n\t            users: List[str],\n\t            after: Optional[datetime] = None,\n", "            before: Optional[datetime] = None,\n\t            use_updated: Optional[bool] = False,\n\t            use_involves: Optional[bool] = False,\n\t            use_reviewed_by: Optional[bool] = False,\n\t            use_review_requested: Optional[bool] = False,\n\t        ) -> str:\n\t        \"\"\"\n\t        Constructs a GitHub Search Query\n\t        that returns pull requests made by the passed users and options.\n\t        Examples\n", "        --------\n\t            type:pr author:user1\n\t            type:pr author:user2 created:<=2021-01-01\n\t            type:pr author:user1 author:user2 created:2021-01-01..2021-03-01\n\t            type:pr author:user2 updated:>=2021-01-01\n\t            type:pr involves:user2\n\t            type:pr reviewed-by:user1\n\t            type:pr review-requested:user1\n\t        All dates are inclusive.\n\t        See GitHub Docs for full options https://docs.github.com/en/search-github/searching-on-github/searching-issues-and-pull-requests\n", "        \"\"\"\n\t        query: List[str] = []\n\t        query.append('type:pr')\n\t        author_filter = 'author'\n\t        if use_involves:\n\t            author_filter = 'involves'\n\t        elif use_reviewed_by:\n\t            author_filter = 'reviewed-by'\n\t        elif use_review_requested:\n\t            author_filter = 'review-requested'\n", "        logger.debug('using author filter %s', author_filter)\n\t        for u in users:\n\t            query.append(f'{author_filter}:{u}')\n\t        time_filter = 'created'\n\t        if use_updated:\n\t            time_filter = 'updated'\n\t        logger.debug('using time filter %s', time_filter)\n\t        if before and after:\n\t            query.append(f\"{time_filter}:{after.strftime('%Y-%m-%d')}..{before.strftime('%Y-%m-%d')}\")\n\t        elif before:\n", "            query.append(f\"{time_filter}:<={before.strftime('%Y-%m-%d')}\")\n\t        elif after:\n\t            query.append(f\"{time_filter}:>={after.strftime('%Y-%m-%d')}\")\n\t        return ' '.join(query)\n\t    def _move_column_to_end(self, df: pd.DataFrame) -> pd.DataFrame:\n\t        for col in self._move_to_end_columns:\n\t            try:\n\t                df.insert(len(df.columns)-1, col, df.pop(col))\n\t                df.drop(columns=col)\n\t            except KeyError:\n", "                # This can happen if the user provides a custom _drop_columns which\n\t                # removes the column before we can move it to the end\n\t                logger.debug('Attempted to move column %s but it did not exist', col)\n\t        return df\n\t    def _parse_datetime_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n\t        for col in self._datetime_columns:\n\t            df[col] = pd.to_datetime(df[col], errors='ignore')\n\t        return df\n\tif __name__ == '__main__':  # pragma: nocover\n\t    g = GitHubCollector()\n", "    logger.info(g._construct_query(['kiran94', 'hello'], datetime(2021, 1, 1), datetime(2021, 3, 1)))\n"]}
{"filename": "prfiesta/analysis/plot.py", "chunked_list": ["import calendar\n\timport logging\n\tfrom typing import Optional, Union\n\timport matplotlib.pyplot as plt\n\timport pandas as pd\n\timport seaborn as sns\n\tfrom matplotlib.axes import Axes\n\tlogger = logging.getLogger(__name__)\n\t_months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n\tdef plot_state_distribution(data: pd.DataFrame, **kwargs) -> Union[plt.Figure, plt.Axes, pd.DataFrame]:\n", "    ax: Optional[Axes] = kwargs.get('ax')\n\t    palette: Optional[str] = kwargs.get('palette')\n\t    title: Optional[str] = kwargs.get('title', 'State Distribution')\n\t    hue: Optional[str] = kwargs.get('hue', 'repository_name')\n\t    if ax:\n\t        ax.set_title(title)\n\t    return sns.histplot(data, x='state', hue=hue, ax=ax, palette=palette)\n\tdef plot_overall_timeline(data: pd.DataFrame, **kwargs) -> Union[plt.Figure, plt.Axes, pd.DataFrame]:\n\t    ax: Optional[Axes] = kwargs.get('ax')\n\t    palette: Optional[str] = kwargs.get('palette')\n", "    title: Optional[str] = kwargs.get('title', 'Overall Contributions')\n\t    hue: Optional[str] = kwargs.get('hue', 'month')\n\t    temp = data.copy()\n\t    temp['month'] = temp['created_at'].dt.month_name()\n\t    temp['year'] = temp['created_at'].dt.year.astype(str)\n\t    temp = temp.groupby(['month', 'year'])['id'].count()\n\t    temp = temp.reset_index()\n\t    # X Axis Ordering (Year)\n\t    x_order = temp['year'].unique().tolist()\n\t    x_order = [int(x) for x in x_order]\n", "    x_order.sort()\n\t    x_order = [str(x) for x in x_order]\n\t    # Hue Ordering (Months)\n\t    sorted_months = sorted(_months, key=lambda x: list(calendar.month_name).index(x))\n\t    p = sns.barplot(temp, x='year', y='id', hue=hue, ax=ax, order=x_order, hue_order=sorted_months, palette=palette)\n\t    if ax:\n\t        ax.set_title(title)\n\t        ax.legend(loc='upper left')\n\t    return p\n\tdef plot_author_associations(data: pd.DataFrame, **kwargs) -> Union[plt.Figure, plt.Axes, pd.DataFrame]:\n", "    ax: Optional[Axes] = kwargs.get('ax')\n\t    palette: Optional[str] = kwargs.get('palette')\n\t    title: Optional[str] = kwargs.get('title', 'Author Associations')\n\t    temp = data.groupby('author_association')['id'].count()\n\t    temp.name = 'count'\n\t    return temp.plot.pie(ax=ax, title=title, legend=True, colormap=palette)\n\tdef plot_conventional_commit_breakdown(data: pd.DataFrame, **kwargs) -> Union[plt.Figure, plt.Axes, pd.DataFrame]:\n\t    ax: Optional[Axes] = kwargs.get('ax')\n\t    palette: Optional[str] = kwargs.get('palette')\n\t    title: Optional[str] = kwargs.get('title', 'Conventional Commit Breakdown')\n", "    hue: Optional[str] = kwargs.get('hue', 'type')\n\t    conventional_commit_frame = data['title'] \\\n\t        .str \\\n\t        .extract(r'^(?P<type>feat|fix|docs|style|refactor|test|chore|build|ci|perf)(\\((?P<scope>[A-Za-z-]+)\\))?: (?P<subject>[^\\n]+)$').copy()\n\t    conventional_commit_frame = conventional_commit_frame.drop(columns=[1, 'subject'])\n\t    conventional_commit_frame = pd.concat([conventional_commit_frame, data['repository_name']], axis=1)\n\t    conventional_commit_frame = conventional_commit_frame.dropna(subset=['type'])\n\t    if conventional_commit_frame.empty:\n\t        logger.warning('passed data did not seem to have any conventional commits')\n\t        return None\n", "    type_count = conventional_commit_frame.groupby(['type', 'repository_name']).count().reset_index()\n\t    type_count = type_count[type_count['scope'] != 0]\n\t    type_count = type_count.sort_values(by='scope', ascending=False)\n\t    type_count = type_count.rename(columns={'scope': 'count'})\n\t    p = sns.barplot(type_count, y='repository_name', x='count', hue=hue, ax=ax, palette=palette)\n\t    if ax:\n\t        ax.set_title(title)\n\t        ax.legend(loc='upper right')\n\t    return p\n\tdef plot_reactions(data: pd.DataFrame, **kwargs) -> Union[plt.Figure, plt.Axes, pd.DataFrame]:\n", "    ax: Optional[Axes] = kwargs.get('ax')\n\t    palette: Optional[str] = kwargs.get('palette')\n\t    title: Optional[str] = kwargs.get('title', 'Reactions')\n\t    hue: Optional[str] = kwargs.get('hue')\n\t    threshold: Optional[int] = kwargs.get('threshold')\n\t    reaction_columns = [x for x in data.columns.tolist() if x.startswith('reactions.') and x not in ['reactions.url']]\n\t    reaction_df = data[reaction_columns]\n\t    reaction_df = reaction_df[reaction_df['reactions.total_count'] > 0]\n\t    reaction_df = reaction_df.drop(columns=['reactions.total_count'])\n\t    if reaction_df.empty:\n", "        logger.warning('passed data did not seem to have any reactions üôÅ')\n\t        return None\n\t    if threshold:\n\t        reaction_df = reaction_df[reaction_df < threshold]\n\t    p = sns.scatterplot(reaction_df, ax=ax, palette=palette, hue=hue)\n\t    if ax:\n\t        ax.set_title(title)\n\t        ax.legend(loc='upper right')\n\t    return p\n"]}
{"filename": "prfiesta/analysis/view.py", "chunked_list": ["from datetime import datetime, timezone\n\timport pandas as pd\n\tfrom IPython.display import HTML, DisplayObject\n\tfrom natural.date import duration\n\tdef _enrich_pr_link(data: pd.DataFrame) -> pd.DataFrame:\n\t    def make_link(row: pd.Series) -> str:\n\t        return f'<a href=\"{row[\"html_url\"]}\">{row[\"title\"]}</a>'\n\t    data['title'] = data.apply(make_link, axis=1)\n\t    return data.drop(columns='html_url')\n\tdef view_pull_requests(data: pd.DataFrame, **kwargs) -> DisplayObject:\n", "    as_frame: bool = kwargs.get('as_frame', False)\n\t    relative_dates: bool = kwargs.get('relative_dates', True)\n\t    head: int = kwargs.get('head')\n\t    temp = data[['number', 'title', 'repository_name', 'updated_at', 'html_url']].copy()\n\t    temp = _enrich_pr_link(temp)\n\t    if relative_dates:\n\t        temp['updated_at'] = pd.to_datetime(temp['updated_at'])\n\t        temp['updated_at'] = temp['updated_at'].apply(duration, now=datetime.now(timezone.utc))\n\t    if head:\n\t        temp = temp.head(head)\n", "    if as_frame:\n\t        return temp\n\t    return HTML(temp.to_html(escape=False, index=False))\n"]}
{"filename": "prfiesta/analysis/__init__.py", "chunked_list": []}
{"filename": "tests/test_main.py", "chunked_list": ["import os\n\tfrom datetime import datetime\n\tfrom typing import List\n\tfrom unittest.mock import ANY, Mock, call, patch\n\timport pandas as pd\n\timport pytest\n\tfrom click.testing import CliRunner\n\tfrom prfiesta.__main__ import main\n\tFAILURE_CODE = 2\n\tdef test_main_missing_users() -> None:\n", "    runner = CliRunner()\n\t    result = runner.invoke(main, [''])\n\t    assert \"Missing option '-u' / '--users'\" in result.output\n\t    assert result.exit_code == FAILURE_CODE\n\t@pytest.mark.parametrize('arguments', [\n\t    pytest.param(['--users', 'user', '--use-involves', '--use-reviewed-by', '--use-review-requested']),\n\t    pytest.param(['--users', 'user', '--use-involves', '--use-reviewed-by']),\n\t    pytest.param(['--users', 'user', '--use-reviewed-by', '--use-review-requested']),\n\t    pytest.param(['--users', 'user', '--use-involves', '--use-review-requested']),\n\t])\n", "def test_main_author_filters_mutually_exclusive(arguments: List[str]) -> None:\n\t    runner = CliRunner()\n\t    result = runner.invoke(main, arguments)\n\t    assert 'the following parameters are mutually exclusive' in result.output\n\t    assert result.exit_code == FAILURE_CODE\n\t@pytest.mark.parametrize(('params', 'expected_collect_params', 'collect_response', 'expected_output_type', 'expected_output'), [\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user'],\n\t        [call('test_user', after=None, before=None, use_updated=False, use_involves=False, use_reviewed_by=False, use_review_requested=False)],\n", "        pd.DataFrame(),\n\t        'csv',\n\t        None,\n\t        id='user_provided',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--after', '2020-01-01'],\n\t        [call('test_user', after=datetime(2020, 1, 1), before=None, use_updated=False, use_involves=False, use_reviewed_by=False, use_review_requested=False)],\n\t        pd.DataFrame(),\n", "        'csv',\n\t        None,\n\t        id='with_after',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--before', '2020-01-01'],\n\t        [call('test_user', before=datetime(2020, 1, 1), after=None, use_updated=False, use_involves=False, use_reviewed_by=False, use_review_requested=False)],\n\t        pd.DataFrame(),\n\t        'csv',\n", "        None,\n\t        id='with_before',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--before', '2020-01-01', '--after', '2009-01-01'],\n\t        [call(\n\t            'test_user',\n\t            before=datetime(2020, 1, 1),\n\t            after=datetime(2009, 1, 1),\n", "            use_updated=False,\n\t            use_involves=False,\n\t            use_reviewed_by=False,\n\t            use_review_requested=False)],\n\t        pd.DataFrame(),\n\t        'csv',\n\t        None,\n\t        id='with_before_and_after',\n\t    ),\n\t    pytest.param\n", "    (\n\t        ['--users', 'test_user'],\n\t        [call('test_user', after=None, before=None, use_updated=False, use_involves=False, use_reviewed_by=False, use_review_requested=False)],\n\t        pd.DataFrame(\n\t                data=[(1, 2, 3)],\n\t                columns=['col1', 'col2', 'col3'],\n\t        ),\n\t        'csv',\n\t        None,\n\t        id='with_collected_responses',\n", "    ),\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--output-type', 'parquet'],\n\t        [call('test_user', after=None, before=None, use_updated=False, use_involves=False, use_reviewed_by=False, use_review_requested=False)],\n\t        pd.DataFrame(\n\t                data=[(1, 2, 3)],\n\t                columns=['col1', 'col2', 'col3'],\n\t        ),\n\t        'parquet',\n", "        None,\n\t        id='with_parquet',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--before', '2020-01-01', '--use-updated'],\n\t        [call('test_user', before=datetime(2020, 1, 1), after=None, use_updated=True, use_involves=False, use_reviewed_by=False, use_review_requested=False)],\n\t        pd.DataFrame(),\n\t        'csv',\n\t        None,\n", "        id='with_use_updated',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--before', '2020-01-01', '--use-involves'],\n\t        [call('test_user', before=datetime(2020, 1, 1), after=None, use_updated=False, use_involves=True, use_reviewed_by=False, use_review_requested=False)],\n\t        pd.DataFrame(),\n\t        'csv',\n\t        None,\n\t        id='with_use_involves',\n", "    ),\n\t    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--use-reviewed-by'],\n\t        [call('test_user', after=None, before=None, use_updated=False, use_involves=False, use_reviewed_by=True, use_review_requested=False)],\n\t        pd.DataFrame(),\n\t        'csv',\n\t        None,\n\t        id='user_reviewed_by',\n\t    ),\n", "    pytest.param\n\t    (\n\t        ['--users', 'test_user', '--use-review-requested'],\n\t        [call('test_user', after=None, before=None, use_updated=False, use_involves=False, use_reviewed_by=False, use_review_requested=True)],\n\t        pd.DataFrame(),\n\t        'csv',\n\t        None,\n\t        id='user_review_requested',\n\t    ),\n\t    pytest.param\n", "    (\n\t        ['--users', 'test_user', '--output-type', 'duckdb', '--output', 'my.duckdb'],\n\t        [call('test_user', after=None, before=None, use_updated=False, use_involves=False, use_reviewed_by=False, use_review_requested=False)],\n\t        pd.DataFrame(data={'a': [1, 2, 3]}),\n\t        'duckdb',\n\t        'my.duckdb',\n\t        id='with_duckdb',\n\t    ),\n\t])\n\t@patch('prfiesta.__main__.Spinner')\n", "@patch('prfiesta.__main__.Live')\n\t@patch('prfiesta.__main__.GitHubCollector')\n\t@patch('prfiesta.__main__.output_frame')\n\t@patch.dict(os.environ, {'GITHUB_TOKEN': 'token'}, clear=True)\n\tdef test_main(\n\t        mock_output_frame: Mock,\n\t        mock_collector: Mock,\n\t        mock_live: Mock,\n\t        mock_spinner: Mock,\n\t        params: List[str],\n", "        expected_collect_params: List,\n\t        collect_response: pd.DataFrame,\n\t        expected_output_type: str,\n\t        expected_output: str,\n\t    ) -> None:\n\t    mock_collector.return_value.collect.return_value = collect_response\n\t    runner = CliRunner()\n\t    result = runner.invoke(main, params)\n\t    assert mock_live.called\n\t    assert mock_spinner.called\n", "    assert mock_collector.call_args_list == [call(token=ANY, url='https://api.github.com', spinner=mock_spinner.return_value, drop_columns=[])]\n\t    assert mock_collector.return_value.collect.call_args_list == expected_collect_params\n\t    if not collect_response.empty:\n\t        assert mock_output_frame.call_args_list \\\n\t            == [call(collect_response, expected_output_type, spinner=mock_spinner.return_value, output_name=expected_output)]\n\t    assert result.exit_code == 0\n"]}
{"filename": "tests/test_spinners.py", "chunked_list": ["from unittest.mock import Mock, call\n\timport pytest\n\tfrom prfiesta import SPINNER_STYLE\n\tfrom prfiesta.spinner import update_spinner\n\t@pytest.mark.parametrize(('message', 'spinner', 'logger'), [\n\t    pytest.param('my_message', Mock(), Mock(), id='all_parameters_supplied'),\n\t    pytest.param('my_message', None, Mock(), id='spinner_none'),\n\t])\n\tdef test_update_spinner(message: str, spinner: Mock, logger: Mock) -> None:\n\t    update_spinner(message, spinner, logger)\n", "    assert logger.debug.call_args_list == [call(message)]\n\t    if spinner:\n\t        assert spinner.update.call_args_list == [call(text=message, style=SPINNER_STYLE)]\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_environment.py", "chunked_list": ["import os\n\tfrom unittest.mock import patch\n\timport pytest\n\tfrom github.Consts import DEFAULT_BASE_URL\n\tfrom prfiesta.environment import GitHubEnvironment\n\t@patch.dict(os.environ, {'GITHUB_ENTERPRISE_TOKEN': 'enterprise_token'}, clear=True)\n\tdef test_environment_get_token_enterprise() -> None:\n\t    gh = GitHubEnvironment()\n\t    assert gh.get_token() == 'enterprise_token'\n\t@patch.dict(os.environ, {'GITHUB_TOKEN': 'token'}, clear=True)\n", "def test_environment_get_token() -> None:\n\t    gh = GitHubEnvironment()\n\t    assert gh.get_token() == 'token'\n\t@patch.dict(os.environ, {}, clear=True)\n\tdef test_environment_get_token_none_set() -> None:\n\t    gh = GitHubEnvironment()\n\t    with pytest.raises(ValueError, match='GITHUB_ENTERPRISE_TOKEN or GITHUB_TOKEN must be set'):\n\t        gh.get_token()\n\t@patch.dict(os.environ, {'GITHUB_TOKEN': 'token', 'GITHUB_ENTERPRISE_TOKEN': 'enterprise_token'}, clear=True)\n\tdef test_environment_both_environment_set() -> None:\n", "    gh = GitHubEnvironment()\n\t    assert gh.get_token() == 'enterprise_token'\n\t@patch.dict(os.environ, {'GH_HOST': 'host'}, clear=True)\n\tdef test_environemnt_get_url_host_set() -> None:\n\t    gh = GitHubEnvironment()\n\t    assert gh.get_url() == 'host'\n\tdef test_environment_get_url_host_not_set() -> None:\n\t    gh = GitHubEnvironment()\n\t    assert gh.get_url() == DEFAULT_BASE_URL\n"]}
{"filename": "tests/test_output.py", "chunked_list": ["from datetime import datetime\n\tfrom unittest.mock import ANY, Mock, call, patch\n\timport pytest\n\tfrom prfiesta.output import output_frame\n\t@pytest.mark.parametrize(('output_type', 'timestamp'), [\n\t    pytest.param('csv', datetime(2021, 1, 1), id='csv_with_timestamp'),\n\t    pytest.param('parquet', datetime(2021, 1, 1), id='parquet_with_timestamp'),\n\t    pytest.param('csv', None, id='csv_without_timestamp'),\n\t    pytest.param('parquet', None, id='parquet_without_timestamp'),\n\t])\n", "def test_output_frame(output_type: str, timestamp: datetime) -> None:\n\t    mock_frame: Mock = Mock()\n\t    mock_spinner: Mock = Mock()\n\t    output_frame(mock_frame, output_type, mock_spinner, timestamp=timestamp)\n\t    assert mock_spinner.update.called\n\t    if timestamp and output_type == 'csv':\n\t        assert [call('export.20210101000000.csv', index=False)] == mock_frame.to_csv.call_args_list\n\t    elif timestamp and output_type == 'parquet':\n\t        assert [call('export.20210101000000.parquet', index=False)] == mock_frame.to_parquet.call_args_list\n\t    elif not timestamp and output_type == 'csv':\n", "        assert [call(ANY, index=False)] == mock_frame.to_csv.call_args_list\n\t    elif not timestamp and output_type == 'parquet':\n\t        assert [call(ANY, index=False)] == mock_frame.to_parquet.call_args_list\n\tdef test_output_frame_unknown_type() -> None:\n\t    mock_frame: Mock = Mock()\n\t    mock_spinner: Mock = Mock()\n\t    with pytest.raises(ValueError, match='unknown output_type'):\n\t        output_frame(mock_frame, 'unknown_type', mock_spinner, timestamp=datetime(2021, 1, 1))\n\t@patch('prfiesta.output.duckdb')\n\tdef test_output_duckdb(mock_duckdb: Mock) -> None:\n", "    mock_frame: Mock = Mock()\n\t    mock_spinner: Mock = Mock()\n\t    mock_duckdb_connection = Mock()\n\t    mock_duckdb.connect.return_value = mock_duckdb_connection\n\t    timestamp = datetime(2021, 1, 1)\n\t    output_frame(mock_frame, 'duckdb', mock_spinner, timestamp=timestamp)\n\t    assert mock_duckdb.connect.called\n\t    assert mock_duckdb_connection.execute.call_args_list == [call('CREATE TABLE prfiesta_20210101_000000 AS SELECT * FROM frame')]\n\t    assert mock_duckdb_connection.close.called\n\t@pytest.mark.parametrize('filename',[\n", "    'illegal:filename.csv',\n\t    '<illegalfilename.csv',\n\t    'illegalfilename.csv>',\n\t    'illegal\\\"filename.csv',\n\t    '/illegalfilename.csv',\n\t    'illegalfilename.\\\\csv',\n\t    'illegalfilename|.csv',\n\t    '*illegalfilename.csv',\n\t    'illegalfilename.csv?',\n\t])\n", "@patch('os.name', 'nt')\n\tdef test_output_illegal_filename_windows(filename: str) -> None:\n\t    with pytest.raises(ValueError, match='is an invalid filename on windows'):\n\t        output_frame(frame=Mock(), output_type='csv', spinner=Mock(), output_name=filename, timestamp=datetime(2021, 1, 1))\n"]}
{"filename": "tests/collectors/__init__.py", "chunked_list": []}
{"filename": "tests/collectors/test_github.py", "chunked_list": ["from datetime import datetime\n\tfrom typing import List, Tuple\n\tfrom unittest.mock import Mock, call, patch\n\timport pytest\n\tfrom github.GithubException import RateLimitExceededException\n\tfrom prfiesta.collectors.github import GitHubCollector\n\t_mock_issue1 = Mock()\n\t_mock_issue1.__dict__ = {\n\t        '_rawData': {\n\t            'url': 'my_url',\n", "            'repository_url': 'https://api.github.com/repos/user/repo',\n\t            'html_url': '',\n\t            'timeline_url': '',\n\t            'labels_url': '',\n\t            'comments_url': '',\n\t            'events_url': '',\n\t            'node_id': '',\n\t            'performed_via_github_app': '',\n\t            'active_lock_reason': '',\n\t            'created_at': '2021-01-01',\n", "            'updated_at': datetime(2021, 1, 1),\n\t            'closed_at': datetime(2021, 1, 1),\n\t            'milestone.created_at': '2020-03-04',\n\t            'milestone.updated_at': datetime(2021, 1, 1),\n\t            'milestone.due_on': datetime(2021, 1, 1),\n\t            'milestone.closed_at': datetime(2021, 1, 1),\n\t        },\n\t}\n\t_mock_issue2 = Mock()\n\t_mock_issue2.__dict__ = {\n", "        '_rawData': {\n\t            'url': 'my_url',\n\t            'repository_url': 'https://api.github.com/repos/user1/repo',\n\t            'html_url': '',\n\t            'timeline_url': '',\n\t            'labels_url': '',\n\t            'comments_url': '',\n\t            'events_url': '',\n\t            'node_id': '',\n\t            'performed_via_github_app': '',\n", "            'active_lock_reason': '',\n\t            'created_at': datetime(2021, 1, 1),\n\t            'updated_at': datetime(2021, 1, 2),\n\t            'closed_at': datetime(2021, 1, 1),\n\t            'milestone.created_at': datetime(2021, 1, 1),\n\t            'milestone.updated_at': datetime(2021, 1, 1),\n\t            'milestone.due_on': datetime(2021, 1, 1),\n\t            'milestone.closed_at': datetime(2021, 1, 1),\n\t        },\n\t}\n", "@pytest.mark.parametrize(('collect_users', 'collect_params', 'github_issues', 'expected_github_query'), [\n\t    pytest.param\n\t    (\n\t        ('user1', 'user2'),\n\t        {},\n\t        [],\n\t        None,\n\t        id='no_results',\n\t    ),\n\t    pytest.param\n", "    (\n\t        ('user1',),\n\t        {},\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr author:user1',\n\t        id='user_returned_results',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ('user1', 'user2'),\n", "        {},\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr author:user1 author:user2',\n\t        id='users_returned_results',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ('user1', 'user2'),\n\t        {'after': datetime(2009, 1, 1)},\n\t        [ _mock_issue1, _mock_issue2 ],\n", "        'type:pr author:user1 author:user2 created:>=2009-01-01',\n\t        id='after_returned_results',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ('user1', 'user2'),\n\t        {'before': datetime(2009, 1, 1)},\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr author:user1 author:user2 created:<=2009-01-01',\n\t        id='before_returned_resutls',\n", "    ),\n\t    pytest.param\n\t    (\n\t        ('user1', 'user2'),\n\t        {'after': datetime(2009, 1, 1),  'before': datetime(2010, 1, 1)},\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr author:user1 author:user2 created:2009-01-01..2010-01-01',\n\t        id='before_and_after_returned_results',\n\t    ),\n\t    pytest.param\n", "    (\n\t        ('user1',),\n\t        {'after': datetime(2009, 1, 1), 'use_updated': True},\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr author:user1 updated:>=2009-01-01',\n\t        id='updated_after_returned_results',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ('user1',),\n", "        {'before': datetime(2009, 1, 1), 'use_updated': True},\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr author:user1 updated:<=2009-01-01',\n\t        id='updated_before_returned_resutls',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ('user1',),\n\t        {'after': datetime(2009, 1, 1),  'before': datetime(2010, 1, 1), 'use_updated': True},\n\t        [ _mock_issue1, _mock_issue2 ],\n", "        'type:pr author:user1 updated:2009-01-01..2010-01-01',\n\t        id='updated_before_and_after_returned_results',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ('user1', 'user2'),\n\t        { 'use_involves': True },\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr involves:user1 involves:user2',\n\t        id='involves_user',\n", "    ),\n\t    pytest.param\n\t    (\n\t        ('user1',),\n\t        {'after': datetime(2009, 1, 1),  'before': datetime(2010, 1, 1), 'use_updated': True, 'use_involves': True},\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr involves:user1 updated:2009-01-01..2010-01-01',\n\t        id='involves_updated_before_and_after_returned_results',\n\t    ),\n\t    pytest.param\n", "    (\n\t        ('user1', 'user2'),\n\t        { 'use_reviewed_by': True },\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr reviewed-by:user1 reviewed-by:user2',\n\t        id='reviewed_by_user',\n\t    ),\n\t    pytest.param\n\t    (\n\t        ('user1', 'user2'),\n", "        { 'use_review_requested': True },\n\t        [ _mock_issue1, _mock_issue2 ],\n\t        'type:pr review-requested:user1 review-requested:user2',\n\t        id='review_requested_user',\n\t    ),\n\t])\n\t@patch('prfiesta.collectors.github.Github')\n\tdef test_collect(\n\t        mock_github: Mock,\n\t        collect_users: Tuple[str],\n", "        collect_params: dict,\n\t        github_issues: List[Mock],\n\t        expected_github_query: str,\n\t    ) -> None:\n\t    spinner_mock = Mock()\n\t    collector_params = {\n\t        'token': 'dummy_token',\n\t        'url': 'dummy_url',\n\t        'spinner': spinner_mock,\n\t    }\n", "    mock_github.return_value.search_issues.return_value = github_issues\n\t    gc = GitHubCollector(**collector_params)\n\t    returned = gc.collect(*collect_users, **collect_params)\n\t    assert mock_github.call_args_list == [call('dummy_token', base_url='dummy_url')]\n\t    assert gc._spinner == collector_params['spinner']\n\t    assert spinner_mock.update.called\n\t    if not github_issues:\n\t        assert returned.empty\n\t        return\n\t    assert mock_github.return_value.search_issues.call_args_list == [call(query=expected_github_query)]\n", "    # Ensure the rows and columns came as expected\n\t    assert returned.shape == (2, 16)\n\t    # Ensure none of the drop columns came through\n\t    assert not set(gc._drop_columns).intersection(set(returned.columns.tolist()))\n\t    # Ensure that updated at is orderered in decreasing order\n\t    assert returned['updated_at'].is_monotonic_decreasing\n\t    # Ensure the all the input repos came through\n\t    assert set(returned['repository_name'].unique().tolist()) == {'user/repo', 'user1/repo'}\n\t    # Ensure all the datetime columns were converted\n\t    for col in returned[gc._datetime_columns].columns.tolist():\n", "        assert str(returned[col].dtype) == 'datetime64[ns]', f'{col} was not a datetime column'\n\t@patch('prfiesta.collectors.github.Github')\n\tdef test_collect_rate_limit(mock_github: Mock) -> None:\n\t    mock_github.return_value.search_issues.side_effect = RateLimitExceededException(429, {}, {})\n\t    spinner_mock = Mock()\n\t    collector_params = {\n\t        'token': 'dummy_token',\n\t        'url': 'dummy_url',\n\t        'spinner': spinner_mock,\n\t    }\n", "    gc = GitHubCollector(**collector_params)\n\t    result = gc.collect('user')\n\t    assert result.empty\n\t@patch('prfiesta.collectors.github.Github')\n\tdef test_collect_custom_drop_columns(mock_github: Mock) -> None:\n\t    mock_github.return_value.search_issues.return_value = [_mock_issue1]\n\t    collector_params = {\n\t        'token': 'dummy_token',\n\t        'url': 'dummy_url',\n\t        'drop_columns': ['comments_url'],\n", "    }\n\t    gc = GitHubCollector(**collector_params)\n\t    result = gc.collect('user1')\n\t    columns = result.columns.tolist()\n\t    assert 'comments_url' not in columns\n\t    # These are default drop columns\n\t    # Since we are overriding it in this scenario, they should still exist in the output column\n\t    assert 'node_id' in columns\n\t    assert 'performed_via_github_app' in columns\n"]}
{"filename": "tests/analysis/test_view.py", "chunked_list": ["from datetime import datetime, timezone\n\tfrom typing import Dict\n\tfrom unittest.mock import Mock, patch\n\timport pandas as pd\n\timport pytest\n\tfrom prfiesta.analysis.view import view_pull_requests\n\t_now = datetime.now(timezone.utc)\n\t@pytest.mark.parametrize(('data', 'options', 'expected'), [\n\t    pytest.param(\n\t        pd.DataFrame(data =\n", "        {\n\t            'number': [1, 2],\n\t            'title': ['title', 'title2'],\n\t            'repository_name': ['repository', 'repository'],\n\t            'updated_at': [datetime.now(timezone.utc), datetime.now(timezone.utc)],\n\t            'html_url': ['url1', 'url2'],\n\t         }),\n\t        {'as_frame': True},\n\t        pd.DataFrame(data={\n\t            'number': [1, 2],\n", "            'title': ['<a href=\"url1\">title</a>', '<a href=\"url2\">title2</a>'],\n\t            'repository_name': ['repository']*2,\n\t            'updated_at': ['just now']*2,\n\t        }),\n\t        id='as_frame_with_defaults',\n\t    ),\n\t    pytest.param(\n\t        pd.DataFrame(data =\n\t        {\n\t            'number': [1, 2],\n", "            'title': ['title', 'title2'],\n\t            'repository_name': ['repository', 'repository'],\n\t            'updated_at': [_now, _now],\n\t            'html_url': ['url1', 'url2'],\n\t         }),\n\t        {'as_frame': True, 'relative_dates': False},\n\t        pd.DataFrame(data={\n\t            'number': [1, 2],\n\t            'title': ['<a href=\"url1\">title</a>', '<a href=\"url2\">title2</a>'],\n\t            'repository_name': ['repository']*2,\n", "            'updated_at': [_now, _now],\n\t        }),\n\t        id='as_frame_with_no_relative_dates',\n\t    ),\n\t    pytest.param(\n\t        pd.DataFrame(data =\n\t        {\n\t            'number': [1, 2],\n\t            'title': ['title', 'title2'],\n\t            'repository_name': ['repository', 'repository'],\n", "            'updated_at': [datetime.now(timezone.utc), datetime.now(timezone.utc)],\n\t            'html_url': ['url1', 'url2'],\n\t         }),\n\t        {'as_frame': True, 'head': 1},\n\t        pd.DataFrame(data={\n\t            'number': [1],\n\t            'title': ['<a href=\"url1\">title</a>'],\n\t            'repository_name': ['repository'],\n\t            'updated_at': ['just now'],\n\t        }),\n", "        id='as_frame_with_head',\n\t    ),\n\t    pytest.param(\n\t        pd.DataFrame(data =\n\t        {\n\t            'number': [1, 2],\n\t            'title': ['title', 'title2'],\n\t            'repository_name': ['repository', 'repository'],\n\t            'updated_at': [datetime.now(timezone.utc), datetime.now(timezone.utc)],\n\t            'html_url': ['url1', 'url2'],\n", "         }),\n\t        {},\n\t        pd.DataFrame(data={\n\t            'number': [1, 2],\n\t            'title': ['<a href=\"url1\">title</a>', '<a href=\"url2\">title2</a>'],\n\t            'repository_name': ['repository']*2,\n\t            'updated_at': ['just now']*2,\n\t        }),\n\t        id='html_default',\n\t    ),\n", "    pytest.param(\n\t        pd.DataFrame(data =\n\t        {\n\t            'number': [1, 2],\n\t            'title': ['title', 'title2'],\n\t            'repository_name': ['repository', 'repository'],\n\t            'updated_at': [datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S%z'), datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S%z')],\n\t            'html_url': ['url1', 'url2'],\n\t         }),\n\t        {'as_frame': True},\n", "        pd.DataFrame(data={\n\t            'number': [1, 2],\n\t            'title': ['<a href=\"url1\">title</a>', '<a href=\"url2\">title2</a>'],\n\t            'repository_name': ['repository']*2,\n\t            'updated_at': ['just now']*2,\n\t        }),\n\t        id='string_datetime',\n\t    ),\n\t])\n\t@patch('prfiesta.analysis.view.HTML')\n", "def test_view_pull_request(\n\t        mock_html: Mock,\n\t        data: pd.DataFrame,\n\t        options: Dict,\n\t        expected: pd.DataFrame) -> None:\n\t    mock_html.return_value = 'DUMMY'\n\t    result = view_pull_requests(data, **options)\n\t    if 'as_frame' in options and options['as_frame']:\n\t        pd.testing.assert_frame_equal(expected, result)\n\t    else:\n", "        assert mock_html.called\n\t        assert result == mock_html.return_value\n"]}
{"filename": "tests/analysis/__init__.py", "chunked_list": []}
{"filename": "tests/analysis/test_plot.py", "chunked_list": ["from datetime import datetime\n\tfrom typing import Dict\n\tfrom unittest.mock import ANY, Mock, call, patch\n\timport pandas as pd\n\timport pytest\n\tfrom prfiesta.analysis.plot import (\n\t    _months,\n\t    plot_author_associations,\n\t    plot_conventional_commit_breakdown,\n\t    plot_overall_timeline,\n", "    plot_reactions,\n\t    plot_state_distribution,\n\t)\n\t_mock_state_distribution_axis = Mock()\n\t_mock_overall_timeline_axis = Mock()\n\t_mock_author_association_axis = Mock()\n\t_mock_conventional_commit_axis = Mock()\n\t_mock_reaction_commit_axis = Mock()\n\t@pytest.mark.parametrize(('options', 'expected_options'), [\n\t    pytest.param(\n", "        {},\n\t        { 'x': 'state', 'hue': 'repository_name', 'ax': None, 'palette': None },\n\t        id='default'),\n\t    pytest.param(\n\t        {'ax': _mock_state_distribution_axis},\n\t        { 'x': 'state', 'hue': 'repository_name', 'ax': _mock_state_distribution_axis, 'palette': None },\n\t        id='with_custom_axis'),\n\t    pytest.param(\n\t        {'hue': 'user.login'},\n\t        { 'x': 'state', 'hue': 'user.login', 'ax': None, 'palette': None },\n", "        id='with_custom_hue'),\n\t    pytest.param(\n\t        {'palette': 'tab10'},\n\t        { 'x': 'state', 'hue': 'repository_name', 'ax': None, 'palette': 'tab10' },\n\t        id='with_custom_hue'),\n\t    pytest.param(\n\t        {'ax': _mock_state_distribution_axis, 'title': 'my_title'},\n\t        { 'x': 'state', 'hue': 'repository_name', 'ax': _mock_state_distribution_axis, 'palette': None },\n\t        id='with_custom_title'),\n\t])\n", "@patch('prfiesta.analysis.plot.sns')\n\tdef test_plot_state_distribution(mock_seaborn: Mock, options: Dict, expected_options: Dict) -> None:\n\t    data = pd.DataFrame(data={\n\t        'state': ['opened', 'closed'],\n\t        'repository_name': ['repo', 'repo2'],\n\t        'user.login': ['user', 'user2'],\n\t    })\n\t    mock_histplot = Mock()\n\t    mock_seaborn.histplot.return_value = mock_histplot\n\t    result = plot_state_distribution(data, **options)\n", "    if ('title' in options) and (options['title']):\n\t        assert _mock_state_distribution_axis.set_title.call_args_list == [call(options['title'])]\n\t    assert mock_histplot == result\n\t    assert mock_seaborn.histplot.call_args_list == [call(data, **expected_options)]\n\t    _mock_state_distribution_axis.reset_mock()\n\t@pytest.mark.parametrize(('options', 'expected_options'), [\n\t   pytest.param(\n\t        {},\n\t        {'x':'year', 'y':'id', 'hue':'month', 'ax': None, 'order':['2021', '2022'], 'hue_order':_months, 'palette':None},\n\t        id='default',\n", "    ),\n\t   pytest.param(\n\t        {'ax': _mock_overall_timeline_axis },\n\t        {'x':'year', 'y':'id', 'hue':'month', 'ax': _mock_overall_timeline_axis, 'order':['2021', '2022'], 'hue_order':_months, 'palette':None},\n\t        id='with_custom_axis',\n\t    ),\n\t   pytest.param(\n\t        {'palette': 'tab10'},\n\t        {'x':'year', 'y':'id', 'hue':'month', 'ax': None, 'order':['2021', '2022'], 'hue_order':_months, 'palette':'tab10'},\n\t        id='with_custom_pallete',\n", "    ),\n\t   pytest.param(\n\t        { 'ax': _mock_overall_timeline_axis, 'title': 'my_title'},\n\t        {'x':'year', 'y':'id', 'hue':'month', 'ax': _mock_overall_timeline_axis, 'order':['2021', '2022'], 'hue_order':_months, 'palette':None},\n\t        id='with_custom_title',\n\t    ),\n\t   pytest.param(\n\t        {'hue': 'custom_field'},\n\t        {'x':'year', 'y':'id', 'hue':'custom_field', 'ax': None, 'order':['2021', '2022'], 'hue_order':_months, 'palette':None},\n\t        id='with_custom_hue',\n", "    ),\n\t])\n\t@patch('prfiesta.analysis.plot.sns')\n\tdef test_plot_overall_timeline(mock_seaborn: Mock, options: Dict, expected_options: Dict) -> None:\n\t    mock_barplot = Mock()\n\t    mock_seaborn.barplot.return_value = mock_barplot\n\t    data = pd.DataFrame(data={\n\t        'created_at': [datetime(2021, 1, 1), datetime(2021, 2, 1), datetime(2022, 4, 14)],\n\t        'id': ['id1', 'id2', 'id3'],\n\t    })\n", "    result = plot_overall_timeline(data, **options)\n\t    assert mock_barplot == result\n\t    assert mock_seaborn.barplot.call_args_list == [call(ANY, **expected_options)]\n\t    if ('title' in options) and (options['title']):\n\t        assert _mock_overall_timeline_axis.set_title.call_args_list == [call(options['title'])]\n\t    _mock_overall_timeline_axis.reset_mock()\n\t@pytest.mark.parametrize(('options', 'expected_options'), [\n\t   pytest.param(\n\t        {},\n\t        {'ax': None, 'title': 'Author Associations', 'legend': True, 'colormap': None},\n", "        id='default',\n\t    ),\n\t   pytest.param(\n\t        {'ax': _mock_author_association_axis},\n\t        {'ax': _mock_author_association_axis, 'title': 'Author Associations', 'legend': True, 'colormap': None},\n\t        id='with_custom_axis',\n\t    ),\n\t   pytest.param(\n\t        {'title': 'my_custom_title'},\n\t        {'ax': None, 'title': 'my_custom_title', 'legend': True, 'colormap': None},\n", "        id='with_custom_title',\n\t    ),\n\t   pytest.param(\n\t        {'palette': 'my_colors'},\n\t        {'ax': None, 'title': 'Author Associations', 'legend': True, 'colormap': 'my_colors'},\n\t        id='with_custom_pallete',\n\t    ),\n\t])\n\tdef test_plot_author_associations(options: Dict, expected_options: Dict) -> None:\n\t    agg_mock = Mock()\n", "    data = Mock()\n\t    data.groupby = Mock(return_value = {'id': Mock()})\n\t    data.groupby.return_value['id'] = agg_mock\n\t    plot_author_associations(data, **options)\n\t    assert data.groupby.called\n\t    assert agg_mock.count.called\n\t    assert agg_mock.count.return_value.plot.pie.call_args_list == [call(**expected_options)]\n\t@pytest.mark.parametrize(('data', 'options', 'expected_options'),[\n\t   pytest.param(\n\t        pd.DataFrame({\n", "            'title': ['feat(hello): my change', 'fix: my other change'],\n\t            'repository_name': ['repo1', 'repo2'],\n\t        }),\n\t        {},\n\t        {'y': 'repository_name', 'x': 'count', 'hue': 'type', 'ax': None, 'palette': None},\n\t        id='default',\n\t    ),\n\t   pytest.param(\n\t        pd.DataFrame({\n\t            'title': ['refactor(hello): my change', 'chore: my other change'],\n", "            'repository_name': ['repo1', 'repo2'],\n\t        }),\n\t        {'ax': _mock_conventional_commit_axis},\n\t        {'y': 'repository_name', 'x': 'count', 'hue': 'type', 'ax': _mock_conventional_commit_axis, 'palette': None},\n\t        id='with_custom_axis',\n\t    ),\n\t   pytest.param(\n\t        pd.DataFrame({\n\t            'title': ['docs(hello): my change', 'style: my other change'],\n\t            'repository_name': ['repo1', 'repo2'],\n", "        }),\n\t        {'palette': 'my_colors'},\n\t        {'y': 'repository_name', 'x': 'count', 'hue': 'type', 'ax': None, 'palette': 'my_colors'},\n\t        id='with_custom_pallete',\n\t    ),\n\t   pytest.param(\n\t        pd.DataFrame({\n\t            'title': ['feat(hello): my change', 'fix: my other change'],\n\t            'repository_name': ['repo1', 'repo2'],\n\t        }),\n", "        {'ax': _mock_conventional_commit_axis, 'title': 'my_title'},\n\t        {'y': 'repository_name', 'x': 'count', 'hue': 'type', 'ax': _mock_conventional_commit_axis, 'palette': None},\n\t        id='with_custom_title',\n\t    ),\n\t   pytest.param(\n\t        pd.DataFrame({\n\t            'title': ['feat(hello): my change', 'fix: my other change'],\n\t            'repository_name': ['repo1', 'repo2'],\n\t        }),\n\t        {'hue': 'my_field'},\n", "        {'y': 'repository_name', 'x': 'count', 'hue': 'my_field', 'ax': None, 'palette': None},\n\t        id='with_custom_hue',\n\t    ),\n\t])\n\t@patch('prfiesta.analysis.plot.sns')\n\tdef test_plot_conventional_commit_breakdown(mock_seaborn: Mock, data: pd.DataFrame, options: Dict, expected_options: Dict) -> None:\n\t    mock_barplot = Mock()\n\t    mock_seaborn.barplot.return_value = mock_barplot\n\t    result = plot_conventional_commit_breakdown(data, **options)\n\t    assert mock_barplot == result\n", "    assert mock_seaborn.barplot.call_args_list == [call(ANY, **expected_options)]\n\t    if ('title' in options) and (options['title']):\n\t        assert _mock_conventional_commit_axis.set_title.call_args_list == [call(options['title'])]\n\t    _mock_conventional_commit_axis.reset_mock()\n\t@patch('prfiesta.analysis.plot.sns')\n\tdef test_plot_conventional_commit_breakdown_no_valid_commits(mock_seaborn: Mock) -> None:\n\t    mock_seaborn.barplot.return_value = Mock()\n\t    data = pd.DataFrame({\n\t            'title': ['my change', 'fixed a thing'],\n\t            'repository_name': ['repo1', 'repo2'],\n", "    })\n\t    options = {}\n\t    assert not plot_conventional_commit_breakdown(data, **options)\n\t    assert not mock_seaborn.called\n\t    assert not mock_seaborn.barplot.called\n\t@pytest.mark.parametrize(('data', 'options', 'expected_options'), [\n\t    pytest.param(\n\t        pd.DataFrame(data={\n\t            'reactions.total_count': [1, 0, 1],\n\t            'reactions.+1': [1, 0, 1],\n", "            'reactions.-1': [0, 0, 1],\n\t            'reactions.laugh': [1, 1, 1],\n\t            'reactions.hooray': [1, 0, 0],\n\t            'reactions.confused': [0, 0, 1],\n\t        }),\n\t        {},\n\t        {'ax': None, 'palette': None, 'hue': None },\n\t        id='default',\n\t    ),\n\t    pytest.param(\n", "        pd.DataFrame(data={\n\t            'reactions.total_count': [1, 0, 1],\n\t            'reactions.+1': [1, 0, 1],\n\t            'reactions.-1': [0, 0, 1],\n\t            'reactions.laugh': [1, 1, 1],\n\t            'reactions.hooray': [1, 0, 0],\n\t            'reactions.confused': [0, 0, 1],\n\t        }),\n\t        {'ax': _mock_reaction_commit_axis},\n\t        {'ax': _mock_reaction_commit_axis, 'palette': None, 'hue': None },\n", "        id='with_custom_axis',\n\t    ),\n\t    pytest.param(\n\t        pd.DataFrame(data={\n\t            'reactions.total_count': [1, 0, 1],\n\t            'reactions.+1': [1, 0, 1],\n\t            'reactions.-1': [0, 0, 1],\n\t            'reactions.laugh': [1, 1, 1],\n\t            'reactions.hooray': [1, 0, 0],\n\t            'reactions.confused': [0, 0, 1],\n", "        }),\n\t        {'palette': 'tab10'},\n\t        {'ax': None, 'palette': 'tab10', 'hue': None },\n\t        id='with_custom_pallete',\n\t    ),\n\t    pytest.param(\n\t        pd.DataFrame(data={\n\t            'reactions.total_count': [1, 0, 1],\n\t            'reactions.+1': [1, 0, 1],\n\t            'reactions.-1': [0, 0, 1],\n", "            'reactions.laugh': [1, 1, 1],\n\t            'reactions.hooray': [1, 0, 0],\n\t            'reactions.confused': [0, 0, 1],\n\t        }),\n\t        {'hue': 'custom'},\n\t        {'ax': None, 'palette': None, 'hue': 'custom' },\n\t        id='with_custom_hue',\n\t    ),\n\t])\n\t@patch('prfiesta.analysis.plot.sns')\n", "def test_plot_reactions(mock_seaborn: Mock, data: pd.DataFrame, options: Dict, expected_options: Dict) -> None:\n\t    mock_scatter = Mock()\n\t    mock_seaborn.scatterplot.return_value = mock_scatter\n\t    result = plot_reactions(data, **options)\n\t    expected_df = data[data['reactions.total_count'] > 0].copy()\n\t    expected_df = expected_df.drop(columns='reactions.total_count')\n\t    assert mock_scatter == result\n\t    assert mock_seaborn.scatterplot.call_args_list == [call(ANY, **expected_options)]\n\t    pd.testing.assert_frame_equal(expected_df, mock_seaborn.scatterplot.call_args_list[0][0][0])\n\t@patch('prfiesta.analysis.plot.sns')\n", "def test_plot_reaction_no_reactions(mock_seaborn: Mock) -> None:\n\t    data = pd.DataFrame(data={\n\t        'reactions.total_count': [0, 0, 0],\n\t        'reactions.+1': [0, 0, 0],\n\t        'reactions.-1': [0, 0, 0],\n\t        'reactions.laugh': [0, 0, 0],\n\t        'reactions.hooray': [0, 0, 0],\n\t        'reactions.confused': [0, 0, 0],\n\t    })\n\t    assert not plot_reactions(data)\n", "    assert not mock_seaborn.scatterplot.called\n\t@patch('prfiesta.analysis.plot.sns')\n\tdef test_plot_reaction_threshold(mock_seaborn: Mock) -> None:\n\t    threshold = 15\n\t    data = pd.DataFrame(data={\n\t        'reactions.total_count': [1, 1, 1],\n\t        'reactions.+1': [threshold*3, 2, 2],\n\t        'reactions.-1': [1, 1, 1],\n\t        'reactions.laugh': [0, 0, threshold*2],\n\t        'reactions.hooray': [0, 0, 0],\n", "        'reactions.confused': [0, 0, 0],\n\t    })\n\t    plot_reactions(data, threshold=threshold)\n\t    expected_df = data.copy()\n\t    expected_df = expected_df[expected_df < threshold]\n\t    expected_df = expected_df.drop(columns=['reactions.total_count'])\n\t    pd.testing.assert_frame_equal(expected_df, mock_seaborn.scatterplot.call_args_list[0][0][0])\n"]}
