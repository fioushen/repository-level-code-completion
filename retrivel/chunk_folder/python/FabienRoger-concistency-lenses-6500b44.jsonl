{"filename": "tests/test_distance.py", "chunked_list": ["import torch as th\n\tfrom torch.distributions import Categorical, kl_divergence\n\tfrom tuned_lens.stats import js_distance, js_divergence\n\tdef test_js_divergence():\n\t    p = Categorical(logits=th.randn(10))\n\t    q = Categorical(logits=th.randn(10))\n\t    m = Categorical(probs=0.5 * (p.probs + q.probs))  # type: ignore\n\t    kl_fwd = kl_divergence(p, m)\n\t    kl_bwd = kl_divergence(q, m)\n\t    gt_js = 0.5 * (kl_fwd + kl_bwd)\n", "    our_js_fwd = js_divergence(p.logits, q.logits)  # type: ignore\n\t    our_js_bwd = js_divergence(q.logits, p.logits)  # type: ignore\n\t    th.testing.assert_close(gt_js, our_js_fwd)\n\t    th.testing.assert_close(our_js_fwd, our_js_bwd)  # Symmetry\n\tdef test_js_distance():\n\t    a = th.randn(1000, 3)\n\t    b = th.randn(1000, 3)\n\t    c = th.randn(1000, 3)\n\t    dist_ab = js_distance(a, b)\n\t    dist_bc = js_distance(b, c)\n", "    dist_ac = js_distance(a, c)\n\t    # Triangle inequality\n\t    assert th.all(dist_ab + dist_bc >= dist_ac)\n"]}
{"filename": "tests/test_data.py", "chunked_list": ["import math\n\timport transformers as tr\n\tfrom datasets import Dataset\n\tfrom tuned_lens import data\n\tdef test_chunk_and_tokenize(\n\t    text_dataset: Dataset, small_model_tokenizer: tr.PreTrainedTokenizerBase\n\t):\n\t    max_length = 128\n\t    chunked, _ = data.chunk_and_tokenize(\n\t        text_dataset,\n", "        small_model_tokenizer,\n\t        load_from_cache_file=False,\n\t        max_length=max_length,\n\t    )\n\t    length = min(small_model_tokenizer.model_max_length, max_length)\n\t    for i in range(len(chunked)):\n\t        assert len(chunked[i][\"input_ids\"]) == length\n\tdef test_compute_nats_to_bpb_ratio(\n\t    text_dataset: Dataset, gpt2_tokenizer: tr.PreTrainedTokenizerBase\n\t):\n", "    max_length = 128\n\t    _, ratio = data.chunk_and_tokenize(\n\t        text_dataset, gpt2_tokenizer, load_from_cache_file=True, max_length=max_length\n\t    )\n\t    # We expect the ratio to be around 0.29, see https://arxiv.org/pdf/2101.00027.pdf,\n\t    # section 3.1\n\t    assert 0.2 / math.log(2) < ratio < 0.4 / math.log(2)\n"]}
{"filename": "tests/test_unembed.py", "chunked_list": ["import torch as th\n\timport transformers as tr\n\tfrom tuned_lens.model_surgery import get_final_norm\n\tfrom tuned_lens.nn import Unembed\n\tdef back_translate(unembed: Unembed, h: th.Tensor, tol: float = 1e-4) -> th.Tensor:\n\t    \"\"\"Project hidden states into logits and then back into hidden states.\"\"\"\n\t    scale = h.norm(dim=-1, keepdim=True) / h.shape[-1] ** 0.5\n\t    logits = unembed(h)\n\t    return unembed.invert(logits, h0=th.randn_like(h), tol=tol).preimage * scale\n\tdef test_correctness(random_small_model: tr.PreTrainedModel):\n", "    # One problem: we want to check that we handle GPT-J's unembedding bias\n\t    # correctly, but it's zero-initialized. Give it a random Gaussian bias.\n\t    U = random_small_model.get_output_embeddings()\n\t    if U.bias is not None:\n\t        U.bias.data.normal_()\n\t    unembed = Unembed(random_small_model)\n\t    ln_f = get_final_norm(random_small_model)\n\t    x = th.randn(1, 1, random_small_model.config.hidden_size)\n\t    y = U(ln_f(x)).log_softmax(-1)  # type: ignore[attr-defined]\n\t    th.testing.assert_close(y, unembed(x).log_softmax(-1))\n", "    x_hat = back_translate(unembed, x, tol=1e-5)\n\t    th.testing.assert_close(y.exp(), unembed(x_hat).softmax(-1), atol=5e-4, rtol=0.01)\n"]}
{"filename": "tests/test_subspaces.py", "chunked_list": ["import pytest\n\timport torch as th\n\tfrom tuned_lens.causal import remove_subspace\n\t@pytest.mark.parametrize(\"d\", list(range(1, 1000, 100)))\n\tdef test_remove_subspace(d: int):\n\t    a = th.randn(10, d, dtype=th.float64)\n\t    for k in range(1, d, 10):\n\t        b = th.randn(d, k, dtype=th.float64)\n\t        inner = a @ b\n\t        a_ = remove_subspace(a, b, mode=\"zero\")\n", "        inner_ = a_ @ b\n\t        th.testing.assert_close(inner_, th.zeros_like(inner_))\n\t        a_ = remove_subspace(a, b, mode=\"mean\")\n\t        inner_ = a_ @ b\n\t        th.testing.assert_close(inner_, inner.mean(0, keepdim=True).expand_as(inner_))\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_load_artifact.py", "chunked_list": ["from tuned_lens.load_artifacts import load_lens_artifacts\n\tdef test_load_lens_artifact_smoke():\n\t    load_lens_artifacts(\"gpt2\", \"AlignmentResearch/tuned-lens\")\n"]}
{"filename": "tests/test_model_surgery.py", "chunked_list": ["import pytest\n\timport torch as th\n\tfrom transformers import PreTrainedModel, models\n\tfrom tuned_lens import model_surgery\n\tdef test_get_final_layer_norm_raises(opt_random_model: PreTrainedModel):\n\t    opt_random_model.base_model.decoder.final_layer_norm = None\n\t    with pytest.raises(ValueError):\n\t        assert model_surgery.get_final_norm(opt_random_model)\n\tdef test_get_final_layer_norm(random_small_model: PreTrainedModel):\n\t    ln = model_surgery.get_final_norm(random_small_model)\n", "    assert isinstance(ln, (th.nn.LayerNorm, models.llama.modeling_llama.LlamaRMSNorm))\n\tdef test_get_layers_from_model(random_small_model: PreTrainedModel):\n\t    path, layers = model_surgery.get_transformer_layers(random_small_model)\n\t    assert isinstance(layers, th.nn.ModuleList)\n\t    assert isinstance(path, str)\n\t    assert len(layers) == random_small_model.config.num_hidden_layers\n"]}
{"filename": "tests/test_lenses.py", "chunked_list": ["from pathlib import Path\n\timport mock\n\timport pytest\n\timport torch as th\n\timport transformers as trf\n\tfrom tuned_lens.load_artifacts import load_lens_artifacts\n\tfrom tuned_lens.nn.lenses import LogitLens, TunedLens, TunedLensConfig\n\tfrom tuned_lens.nn.unembed import Unembed\n\t@pytest.fixture\n\tdef model_config():\n", "    config = mock.MagicMock(trf.PretrainedConfig)\n\t    config.hidden_size = 128\n\t    config.vocab_size = 100\n\t    config.num_hidden_layers = 3\n\t    return config\n\t@pytest.fixture\n\tdef model(model_config):\n\t    model = mock.MagicMock(trf.PreTrainedModel)\n\t    model.config = model_config\n\t    model.get_output_embeddings = mock.MagicMock(return_value=th.nn.Linear(128, 100))\n", "    return model\n\t@pytest.fixture\n\tdef unembed():\n\t    mock_unembed = mock.MagicMock(Unembed)\n\t    W = th.randn(100, 128)\n\t    mock_unembed.forward = lambda x: th.matmul(x, W.T)\n\t    mock_unembed.unembedding_hash.return_value = 42\n\t    return mock_unembed\n\t@pytest.fixture\n\tdef logit_lens(unembed):\n", "    logit_lens = LogitLens(unembed)\n\t    return logit_lens\n\t@pytest.fixture\n\tdef tuned_lens_config():\n\t    return TunedLensConfig(\n\t        base_model_name_or_path=\"test-model\",\n\t        d_model=128,\n\t        num_hidden_layers=3,\n\t        bias=True,\n\t    )\n", "@pytest.fixture\n\tdef random_tuned_lens(tuned_lens_config, unembed):\n\t    tuned_lens = TunedLens(\n\t        unembed,\n\t        tuned_lens_config,\n\t    )\n\t    return tuned_lens\n\tdef test_logit_lens_smoke(logit_lens):\n\t    randn = th.randn(1, 10, 128)\n\t    logit_lens(randn, 0)\n", "def test_tuned_lens_from_model(random_small_model: trf.PreTrainedModel):\n\t    tuned_lens = TunedLens.from_model(random_small_model)\n\t    assert tuned_lens.config.d_model == random_small_model.config.hidden_size\n\tdef test_tuned_lens_forward(random_tuned_lens: TunedLens):\n\t    randn = th.randn(1, 10, 128)\n\t    logits_forward = random_tuned_lens.forward(randn, 0)\n\t    logits = random_tuned_lens.unembed.forward(randn + random_tuned_lens[0](randn))\n\t    assert th.allclose(logits_forward, logits)\n\tdef test_tuned_lens_save_and_load(\n\t    unembed: Unembed, random_tuned_lens: TunedLens, tmp_path: Path\n", "):\n\t    randn = th.randn(1, 10, 128)\n\t    logits_before = random_tuned_lens(randn, 1)\n\t    random_tuned_lens.save(tmp_path)\n\t    reloaded_tuned_lens = TunedLens.from_unembed_and_pretrained(\n\t        lens_resource_id=tmp_path, unembed=unembed\n\t    )\n\t    logits_after = reloaded_tuned_lens(randn, 1)\n\t    assert th.allclose(logits_before, logits_after)\n\tdef test_from_model_and_pretrained_propogates_kwargs(\n", "    random_tuned_lens: TunedLens, unembed: Unembed, tmp_path: Path\n\t):\n\t    random_tuned_lens.save(tmp_path)\n\t    with mock.patch(\n\t        \"tuned_lens.load_artifacts.load_lens_artifacts\",\n\t        mock.MagicMock(\n\t            load_lens_artifacts,\n\t            return_value=(tmp_path / \"config.json\", tmp_path / \"params.pt\"),\n\t        ),\n\t    ) as mock_load_lens_artifacts:\n", "        mock_load_lens_artifacts.__code__.co_varnames = (\n\t            \"resource_id\",\n\t            \"unembed\",\n\t            \"revision\",\n\t        )\n\t        TunedLens.from_unembed_and_pretrained(\n\t            lens_resource_id=\"does not use\", unembed=unembed, revision=\"foo\"\n\t        )\n\t        assert mock_load_lens_artifacts.call_args.kwargs[\"revision\"] == \"foo\"\n\t        with pytest.raises(TypeError):\n", "            # Should not just be able to pass any kwarg\n\t            TunedLens.from_unembed_and_pretrained(\n\t                lens_resource_id=\"does not use\",\n\t                unembed=unembed,\n\t                revision=\"foo\",\n\t                bad_kwarg=\"bar\",\n\t            )\n\t        with pytest.raises(TypeError):\n\t            # Should not be able to specify both resource_id and and lens_resource_id\n\t            TunedLens.from_unembed_and_pretrained(\n", "                lens_resource_id=\"does not use\", unembed=unembed, resource_id=\"bar\"\n\t            )\n"]}
{"filename": "tests/test_utils.py", "chunked_list": ["import numpy as np\n\tfrom tuned_lens.utils import tensor_hash\n\tdef test_tensor_hash():\n\t    random = np.random.default_rng(42)\n\t    a = random.normal(size=(10, 1000)).astype(np.float32)\n\t    b = random.normal(size=(10, 1000)).astype(np.float32)\n\t    assert tensor_hash(a) != tensor_hash(b)\n\t    assert tensor_hash(a) == tensor_hash(a)\n\t    assert tensor_hash(a) == tensor_hash(a.astype(np.float16))\n"]}
{"filename": "tests/test_stats.py", "chunked_list": ["import random\n\timport torch as th\n\tfrom torch.distributions import Dirichlet, kl_divergence\n\tfrom tuned_lens.stats import LogitStats\n\tdef test_logit_stats_correctness():\n\t    \"\"\"Test that `LogitStats` recovers the true Dirichlet within a small error.\"\"\"\n\t    th.manual_seed(42)\n\t    x = Dirichlet(th.tensor([1.0, 1.0, 1.0]))\n\t    logits1 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)\n\t    logits2 = x.sample(th.Size([10000])).log() + random.uniform(-0.1, 0.1)\n", "    stats = LogitStats()\n\t    stats.update(logits1)\n\t    stats.update(logits2)\n\t    x2 = stats.mle()\n\t    assert kl_divergence(x, x2) < 1e-3\n"]}
{"filename": "tests/conftest.py", "chunked_list": ["from pathlib import Path\n\timport pytest\n\timport torch as th\n\timport transformers as tr\n\tfrom datasets import Dataset\n\t@pytest.fixture(scope=\"module\")\n\tdef text_dataset_path() -> Path:\n\t    dir_path = Path(__file__).parent.absolute()\n\t    return Path(dir_path, \"test_data\", \"pile_text.jsonl\")\n\t@pytest.fixture(scope=\"module\")\n", "def text_dataset(text_dataset_path: Path) -> Dataset:\n\t    dataset = Dataset.from_json(str(text_dataset_path))\n\t    assert isinstance(dataset, Dataset)\n\t    return dataset\n\t@pytest.fixture(\n\t    scope=\"module\",\n\t    params=[\n\t        \"EleutherAI/pythia-70m-deduped\",\n\t        \"bigscience/bloom-560m\",\n\t        \"EleutherAI/gpt-neo-125M\",\n", "        \"facebook/opt-125m\",\n\t        \"mockmodel/llama-tiny\",\n\t        \"gpt2\",\n\t    ],\n\t)\n\tdef random_small_model(request: str) -> tr.PreTrainedModel:\n\t    small_model_name = request.param\n\t    th.manual_seed(42)\n\t    # We use a random model with the correct config instead of downloading the\n\t    # whole pretrained checkpoint.\n", "    if small_model_name == \"mockmodel/llama-tiny\":\n\t        config = tr.LlamaConfig(\n\t            vocab_size=32_000,\n\t            hidden_size=128,\n\t            num_hidden_layers=4,\n\t            num_attention_heads=4,\n\t        )\n\t    else:\n\t        config = tr.AutoConfig.from_pretrained(small_model_name)\n\t    model = tr.AutoModelForCausalLM.from_config(config)\n", "    model.eval()\n\t    return model\n\t@pytest.fixture(\n\t    scope=\"module\",\n\t    params=[\n\t        \"EleutherAI/pythia-70m-deduped\",\n\t        \"bigscience/bloom-560m\",\n\t        \"EleutherAI/gpt-neo-125M\",\n\t        \"facebook/opt-125m\",\n\t        \"gpt2\",\n", "    ],\n\t)\n\tdef small_model_tokenizer(request: str) -> tr.PreTrainedTokenizerBase:\n\t    return tr.AutoTokenizer.from_pretrained(request.param, use_fast=True)\n\t@pytest.fixture(scope=\"module\")\n\tdef gpt2_tokenizer():\n\t    return tr.AutoTokenizer.from_pretrained(\"gpt2\", use_fast=True)\n\t@pytest.fixture(scope=\"module\")\n\tdef opt_random_model() -> tr.PreTrainedModel:\n\t    config = tr.AutoConfig.from_pretrained(\"facebook/opt-125m\")\n", "    model = tr.AutoModelForCausalLM.from_config(config)\n\t    model.eval()\n\t    return model\n\t@pytest.fixture(scope=\"module\")\n\tdef gpt2_random_model_local_path(\n\t    tmpdir_factory, gpt2_tokenizer: tr.PreTrainedTokenizerBase\n\t):\n\t    config = tr.AutoConfig.from_pretrained(\"gpt2\")\n\t    model = tr.AutoModelForCausalLM.from_config(config)\n\t    assert isinstance(model, tr.PreTrainedModel)\n", "    tmp_path = tmpdir_factory.mktemp(\"gpt2_random_model_local\")\n\t    model.save_pretrained(tmp_path)\n\t    gpt2_tokenizer.save_pretrained(tmp_path)\n\t    return tmp_path\n"]}
{"filename": "tests/plotting/test_prediction_trajectory.py", "chunked_list": ["import numpy as np\n\timport pytest\n\tfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\tfrom tuned_lens.nn.lenses import LogitLens\n\tfrom tuned_lens.plotting import PredictionTrajectory\n\t@pytest.fixture\n\tdef prediction_trajectory_no_tokenizer():\n\t    layers = 3\n\t    num_tokens = 10\n\t    vocab_size = 12\n", "    return PredictionTrajectory(\n\t        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n\t        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n\t        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n\t    )\n\t@pytest.fixture\n\tdef prediction_other__trajectory_no_tokenizer():\n\t    layers = 3\n\t    num_tokens = 10\n\t    vocab_size = 12\n", "    return PredictionTrajectory(\n\t        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n\t        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n\t        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n\t    )\n\t@pytest.fixture\n\tdef prediction_trajectory_with_tokenizer():\n\t    layers = 3\n\t    num_tokens = 10\n\t    vocab_size = 12\n", "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n\t    return PredictionTrajectory(\n\t        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n\t        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n\t        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n\t        tokenizer=tokenizer,\n\t    )\n\t@pytest.fixture\n\tdef model_and_tokenizer():\n\t    model_name = \"gpt2\"\n", "    model = AutoModelForCausalLM.from_pretrained(model_name)\n\t    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\t    return model, tokenizer\n\t@pytest.fixture\n\tdef lens(model_and_tokenizer):\n\t    model, _ = model_and_tokenizer\n\t    return LogitLens.from_model(model)\n\tdef test_prediction_trajectory_from_lens_and_model_smoke(model_and_tokenizer, lens):\n\t    model, tokenizer = model_and_tokenizer\n\t    input_ids = tokenizer.encode(\"Hello world!\")\n", "    traj = PredictionTrajectory.from_lens_and_model(\n\t        lens, model, input_ids, tokenizer=tokenizer\n\t    )\n\t    assert traj.num_layers == model.config.n_layer\n\t    assert traj.num_tokens == len(input_ids)\n\t    assert traj.vocab_size == model.config.vocab_size\n\tdef test_largest_prob_labels_smoke(prediction_trajectory_with_tokenizer):\n\t    labels = prediction_trajectory_with_tokenizer.largest_prob_labels(\n\t        min_prob=0.1, topk=5\n\t    )\n", "    assert labels.label_strings.shape == (3, 10)\n\t    assert labels.sequence_labels.shape == (10,)\n\t    assert labels.hover_over_entries.shape == (3, 10, 5)\n\tdef test_largest_delta_in_prob_labels_smoke(prediction_trajectory_with_tokenizer):\n\t    layers = 3\n\t    num_tokens = 10\n\t    vocab_size = 12\n\t    other = PredictionTrajectory(\n\t        log_probs=np.zeros((layers, num_tokens, vocab_size), dtype=np.float32),\n\t        input_ids=np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),\n", "        targets=np.array([2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),\n\t        tokenizer=prediction_trajectory_with_tokenizer.tokenizer,\n\t    )\n\t    labels = prediction_trajectory_with_tokenizer.largest_delta_in_prob_labels(\n\t        other, min_prob_delta=0.1, topk=5\n\t    )\n\t    assert labels.label_strings.shape == (3, 10)\n\t    assert labels.sequence_labels.shape == (10,)\n\t    assert labels.hover_over_entries.shape == (3, 10, 5)\n\tdef test_cross_entropy_smoke(prediction_trajectory_no_tokenizer):\n", "    traj = prediction_trajectory_no_tokenizer\n\t    ce_stat = traj.cross_entropy()\n\t    assert ce_stat.name == \"Cross Entropy\"\n\t    assert ce_stat.units == \"nats\"\n\t    assert ce_stat.labels is None\n\t    assert ce_stat.stats.shape == (3, 10)\n\tdef test_entropy_smoke(prediction_trajectory_no_tokenizer):\n\t    traj = prediction_trajectory_no_tokenizer\n\t    entropy_stat = traj.entropy()\n\t    assert entropy_stat.name == \"Entropy\"\n", "    assert entropy_stat.units == \"nats\"\n\t    assert entropy_stat.labels is None\n\t    assert entropy_stat.stats.shape == (3, 10)\n\tdef test_forward_kl_smoke(prediction_trajectory_no_tokenizer):\n\t    traj = prediction_trajectory_no_tokenizer\n\t    forward_kl_stat = traj.forward_kl()\n\t    assert forward_kl_stat.name == \"Forward KL\"\n\t    assert forward_kl_stat.units == \"nats\"\n\t    assert forward_kl_stat.labels is None\n\t    assert forward_kl_stat.stats.shape == (3, 10)\n", "def test_max_probability_smoke(prediction_trajectory_no_tokenizer):\n\t    traj = prediction_trajectory_no_tokenizer\n\t    max_probability_stat = traj.max_probability()\n\t    assert max_probability_stat.name == \"Max Probability\"\n\t    assert max_probability_stat.units == \"probs\"\n\t    assert max_probability_stat.labels is None\n\t    assert max_probability_stat.stats.shape == (3, 10)\n\tdef test_kl_divergence_smoke(\n\t    prediction_trajectory_no_tokenizer, prediction_other__trajectory_no_tokenizer\n\t):\n", "    traj = prediction_trajectory_no_tokenizer\n\t    other = prediction_other__trajectory_no_tokenizer\n\t    kl_stat = traj.kl_divergence(other)\n\t    assert kl_stat.name == \"KL(Self | Other)\"\n\t    assert kl_stat.units == \"nats\"\n\t    assert kl_stat.labels is None\n\t    assert kl_stat.stats.shape == (3, 10)\n\t    assert np.isclose(kl_stat.stats, 0.0).all()\n\tdef test_js_divergence_smoke(\n\t    prediction_trajectory_no_tokenizer, prediction_other__trajectory_no_tokenizer\n", "):\n\t    traj = prediction_trajectory_no_tokenizer\n\t    other = prediction_other__trajectory_no_tokenizer\n\t    js_stat = traj.js_divergence(other)\n\t    assert js_stat.name == \"JS(Self | Other)\"\n\t    assert js_stat.units == \"nats\"\n\t    assert js_stat.labels is None\n\t    assert js_stat.stats.shape == (3, 10)\n\t    assert np.isclose(js_stat.stats, 0.0).all()\n\tdef test_total_variation_smoke(\n", "    prediction_trajectory_no_tokenizer, prediction_other__trajectory_no_tokenizer\n\t):\n\t    traj = prediction_trajectory_no_tokenizer\n\t    other = prediction_other__trajectory_no_tokenizer\n\t    js_stat = traj.total_variation(other)\n\t    assert js_stat.name == \"TV(Self | Other)\"\n\t    assert js_stat.units == \"probs\"\n\t    assert js_stat.labels is None\n\t    assert js_stat.stats.shape == (3, 10)\n\t    assert np.isclose(js_stat.stats, 0.0).all()\n"]}
{"filename": "tests/plotting/test_trajectory_plotting.py", "chunked_list": ["import numpy as np\n\timport pytest\n\tfrom plotly import graph_objects as go\n\tfrom tuned_lens.plotting.trajectory_plotting import (\n\t    TrajectoryLabels,\n\t    TrajectoryStatistic,\n\t    _stride_keep_last,\n\t)\n\tdef test_stride_keep_last():\n\t    x = np.array([1, 2, 3, 4, 5])\n", "    assert np.array_equal(_stride_keep_last(x, 1), x)\n\t    assert np.array_equal(_stride_keep_last(x, 2), np.array([1, 3, 5]))\n\t    assert np.array_equal(_stride_keep_last(x, 3), np.array([1, 4, 5]))\n\t    assert np.array_equal(_stride_keep_last(x, 4), np.array([1, 5]))\n\t    assert np.array_equal(_stride_keep_last(x, 5), np.array([1, 5]))\n\tdef test_trajectory_statistic_post_init():\n\t    stats = np.zeros((2, 2), dtype=float)\n\t    labels = TrajectoryLabels(\n\t        label_strings=np.zeros((2, 2), dtype=np.str_),\n\t        sequence_labels=np.zeros(2, dtype=np.str_),\n", "    )\n\t    with pytest.raises(AssertionError):\n\t        TrajectoryStatistic(\"test\", np.zeros((2, 3), dtype=float), labels)\n\t    stats = np.zeros((3, 3), dtype=float)\n\t    labels = TrajectoryLabels(\n\t        label_strings=np.zeros((3, 3), dtype=np.str_),\n\t        sequence_labels=np.zeros(3, dtype=np.str_),\n\t    )\n\t    ts = TrajectoryStatistic(\"test\", stats, labels)\n\t    assert ts is not None\n", "def test_trajectory_statistic_num_layers():\n\t    stats = np.zeros((2, 2), dtype=float)\n\t    ts = TrajectoryStatistic(\"test\", stats)\n\t    assert ts.num_layers == 2\n\t    stats = np.zeros((3, 3), dtype=float)\n\t    ts = TrajectoryStatistic(\"test\", stats)\n\t    assert ts.num_layers == 3\n\tdef test_trajectory_statistic_heatmap():\n\t    stats = np.zeros((2, 2), dtype=float)\n\t    ts = TrajectoryStatistic(\"test\", stats)\n", "    heatmap = ts.heatmap()\n\t    assert isinstance(heatmap, go.Heatmap)\n\tdef test_trajectory_statistic_figure():\n\t    stats = np.zeros((2, 2), dtype=float)\n\t    ts = TrajectoryStatistic(\"test\", stats)\n\t    figure = ts.figure()\n\t    assert isinstance(figure, go.Figure)\n"]}
{"filename": "tests/plotting/test_token_formatter.py", "chunked_list": ["import pytest\n\tfrom tuned_lens.plotting import TokenFormatter\n\t@pytest.fixture\n\tdef formatter():\n\t    return TokenFormatter()\n\tdef test_format_non_string(formatter):\n\t    assert formatter.format(None) == \"<unk>\"\n\t    assert formatter.format(123) == \"<unk>\"\n\tdef test_format_ellipsis(formatter):\n\t    token = \"ThisIsALongToken\"\n", "    expected = \"ThisIs…\"\n\t    assert formatter.format(token) == expected\n\tdef test_format_no_ellipsis(formatter):\n\t    formatter.max_string_len = None\n\t    token = \"ThisIsALongToken\"\n\t    expected = \"ThisIsALongToken\"\n\t    assert formatter.format(token) == expected\n\tdef test_format_newline_token_replacement(formatter):\n\t    formatter.max_string_len = None\n\t    token = \"HelloĊWorld\"\n", "    expected = \"Hello\\\\nWorld\"\n\t    assert formatter.format(token) == expected\n\tdef test_format_whitespace_token_replacement(formatter):\n\t    formatter.max_string_len = None\n\t    token = \"HelloĠWorld\"\n\t    expected = \"Hello_World\"\n\t    assert formatter.format(token) == expected\n\tdef test_format_multiple_replacements(formatter):\n\t    formatter.max_string_len = None\n\t    token = \"Line1ĊLine2ĠLine3\"\n", "    expected = \"Line1\\\\nLine2_Line3\"\n\t    assert formatter.format(token) == expected\n"]}
{"filename": "tests/scripts/test_integration.py", "chunked_list": ["from pathlib import Path\n\tfrom tuned_lens.__main__ import main\n\tdef test_eval_subcommand(\n\t    text_dataset_path: Path, gpt2_random_model_local_path: Path, tmp_path: Path\n\t):\n\t    # Note we do not specify a lens here, so we are using the logit lens\n\t    args = (\n\t        f\"eval --data.name {text_dataset_path}\"\n\t        f\" --model.name {gpt2_random_model_local_path}\"\n\t        \" --limit 20 --max_length 128\"\n", "        f\" --output {tmp_path}\"\n\t    )\n\t    args = args.split()\n\t    main(args)\n\tdef test_train_subcommand(\n\t    text_dataset_path: Path, gpt2_random_model_local_path: Path, tmp_path: Path\n\t):\n\t    args = (\n\t        f\"train --data.name {text_dataset_path}\"\n\t        f\" --model.name {gpt2_random_model_local_path}\"\n", "        \" --max_length 128\"\n\t        f\" --output {tmp_path}\"\n\t    )\n\t    args = args.split()\n\t    main(args)\n"]}
{"filename": "tests/scripts/__init__.py", "chunked_list": []}
{"filename": "docs/source/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# For the full list of built-in configuration values, see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\t# -- Project information -----------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\tfrom importlib import metadata\n\tproject = \"tuned-lens\"\n\tcopyright = \"2023, FAR AI\"\n\thtml_title = \"Tuned Lens\"\n", "html_favicon = (\n\t    \"data:image/svg+xml,\"\n\t    \"<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22>\"\n\t    \"<text y=%22.9em%22 font-size=%2290%22>🔎</text>\"\n\t    \"</svg>\"\n\t)\n\tauthor = (\n\t    \"Nora Belrose\"\n\t    \" Zach Furman,\"\n\t    \" Logan Smith,\"\n", "    \" Danny Halawi,\"\n\t    \" Lev McKinney,\"\n\t    \" Igor Ostrovsky,\"\n\t    \" Stella Biderman,\"\n\t    \" Jacob Steinhardt\"\n\t)\n\trelease = metadata.version(\"tuned_lens\")\n\textensions = [\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx.ext.autodoc\",\n", "    \"sphinx.ext.autosummary\",\n\t    \"sphinx.ext.autosectionlabel\",\n\t    \"sphinx_autodoc_typehints\",\n\t    \"sphinx.ext.doctest\",\n\t    \"myst_parser\",\n\t    \"nbsphinx\",\n\t]\n\tnapoleon_google_docstring = True\n\tnapoleon_use_param = False\n\tnapoleon_use_ivar = True\n", "templates_path = [\"_templates\"]\n\texclude_patterns = [\"build\", \"Thumbs.db\", \".DS_Store\", \"**.ipynb_checkpoints\"]\n\thtml_theme = \"furo\"\n\thtml_static_path = [\"_static\"]\n\thtml_theme_options = {\n\t    \"source_repository\": \"https://github.com/AlignmentResearch/tuned-lens\",\n\t    \"source_branch\": \"main\",\n\t    \"source_directory\": \"docs/source\",\n\t    \"light_css_variables\": {\n\t        \"sidebar-item-font-size\": \"85%\",\n", "    },\n\t}\n"]}
{"filename": "tuned_lens/__main__.py", "chunked_list": ["\"\"\"Script to train or evaluate a set of tuned lenses for a language model.\"\"\"\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional, Union\n\tfrom simple_parsing import ArgumentParser, ConflictResolution\n\tfrom torch.distributed.elastic.multiprocessing.errors import record\n\tfrom .scripts.eval_loop import Eval\n\tfrom .scripts.train_loop import Train\n\t@dataclass\n\tclass Main:\n\t    \"\"\"Routes to the subcommands.\"\"\"\n", "    command: Union[Train, Eval]\n\t    def execute(self):\n\t        \"\"\"Run the script.\"\"\"\n\t        self.command.execute()\n\t@record\n\tdef main(args: Optional[list[str]] = None):\n\t    \"\"\"Entry point for the CLI.\"\"\"\n\t    parser = ArgumentParser(conflict_resolution=ConflictResolution.EXPLICIT)\n\t    parser.add_arguments(Main, dest=\"prog\")\n\t    args = parser.parse_args(args=args)\n", "    prog: Main = args.prog\n\t    prog.execute()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "tuned_lens/load_artifacts.py", "chunked_list": ["\"\"\"Load lens artifacts from the hub or locally storage.\"\"\"\n\timport os\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\tfrom huggingface_hub import hf_hub_download\n\tdef load_lens_artifacts(\n\t    resource_id: str,\n\t    repo_id: Optional[str] = None,\n\t    repo_type: Optional[str] = None,\n\t    revision: Optional[str] = None,\n", "    config_file: str = \"config.json\",\n\t    ckpt_file: str = \"params.pt\",\n\t    subfolder: str = \"lens\",\n\t    cache_dir: Optional[str] = None,\n\t) -> tuple[Path, Path]:\n\t    \"\"\"First checks for lens resource locally then tries to download it from the hub.\n\t    Args:\n\t        resource_id: The id of the lens resource.\n\t        repo_id: The repository to download the lens from. Defaults to\n\t            'AlignmentResearch/tuned-lens'. However, this default can be overridden by\n", "            setting the TUNED_LENS_REPO_ID environment variable.\n\t        repo_type: The type of repository to download the lens from. Defaults to\n\t            'space'. However, this default can be overridden by setting the\n\t            TUNED_LENS_REPO_TYPE environment variable.\n\t        config_file: The name of the config file in the folder contain the lens.\n\t        ckpt_file: The name of the checkpoint file in the folder contain the lens.\n\t        revision: The revision of the lens to download.\n\t        subfolder: The subfolder of the repository to download the lens from.\n\t        cache_dir: The directory to cache the lens in.\n\t    Returns:\n", "        * The path to the config.json file\n\t        * The path to the params.pt file\n\t    Raises:\n\t        ValueError: if the lens resource could not be found.\n\t    \"\"\"\n\t    if repo_id is None:\n\t        if os.environ.get(\"TUNED_LENS_REPO_ID\"):\n\t            repo_id = os.environ[\"TUNED_LENS_REPO_ID\"]\n\t        else:\n\t            repo_id = \"AlignmentResearch/tuned-lens\"\n", "    if repo_type is None:\n\t        if os.environ.get(\"TUNED_LENS_REPO_TYPE\"):\n\t            repo_type = os.environ[\"TUNED_LENS_REPO_TYPE\"]\n\t        else:\n\t            repo_type = \"space\"\n\t    # Fist check if the resource id is a path to a folder that exists\n\t    local_path = Path(resource_id)\n\t    if (local_path / config_file).exists() and (local_path / ckpt_file).exists():\n\t        return local_path / config_file, local_path / ckpt_file\n\t    subfolder = \"/\".join((subfolder, resource_id))\n", "    params_path = hf_hub_download(\n\t        filename=ckpt_file,\n\t        repo_id=repo_id,\n\t        repo_type=repo_type,\n\t        revision=revision,\n\t        subfolder=subfolder,\n\t        cache_dir=cache_dir,\n\t    )\n\t    config_path = hf_hub_download(\n\t        filename=config_file,\n", "        repo_id=repo_id,\n\t        repo_type=repo_type,\n\t        revision=revision,\n\t        subfolder=subfolder,\n\t        cache_dir=cache_dir,\n\t    )\n\t    if config_path is not None and params_path is not None:\n\t        return Path(config_path), Path(params_path)\n\t    raise ValueError(\"Could not find lens resource locally or on the hf hub.\")\n"]}
{"filename": "tuned_lens/__init__.py", "chunked_list": ["\"\"\"The tuned lens package.\"\"\"\n\tfrom .nn import TunedLens\n"]}
{"filename": "tuned_lens/utils.py", "chunked_list": ["\"\"\"Utilities for distributed training and handling nested collections of tensors.\"\"\"\n\timport hashlib\n\tfrom itertools import islice\n\tfrom typing import Any, Callable, Iterable, Sequence, Type, TypeVar, Union, cast\n\timport numpy as np\n\timport torch as th\n\timport torch.distributed as dist\n\tfrom numpy.typing import NDArray\n\tT = TypeVar(\"T\")\n\tdef assert_type(typ: Type[T], obj: Any) -> T:\n", "    \"\"\"Assert that an object is of a given type at runtime and return it.\"\"\"\n\t    if not isinstance(obj, typ):\n\t        raise TypeError(f\"Expected {typ.__name__}, got {type(obj).__name__}\")\n\t    return cast(typ, obj)\n\tdef maybe_all_cat(x: th.Tensor) -> th.Tensor:\n\t    \"\"\"Concatenate a tensor across all processes.\"\"\"\n\t    if not dist.is_initialized():\n\t        return x\n\t    buffer = x.new_empty([dist.get_world_size() * x.shape[0], *x.shape[1:]])\n\t    dist.all_gather_into_tensor(buffer, x)\n", "    return buffer\n\tdef maybe_all_gather_lists(lst: list) -> list:\n\t    \"\"\"Gather a list of objects from all processes.\"\"\"\n\t    if not dist.is_initialized():\n\t        return lst\n\t    lists = [[] for _ in range(dist.get_world_size())]\n\t    dist.all_gather_object(lists, lst)\n\t    return sum(lists, [])\n\tdef maybe_all_reduce(x: th.Tensor, op: str = \"mean\") -> th.Tensor:\n\t    \"\"\"Reduce a tensor across all processes.\"\"\"\n", "    if not dist.is_initialized():\n\t        return x\n\t    if op == \"sum\":\n\t        dist.all_reduce(x, op=dist.ReduceOp.SUM)\n\t    elif op == \"mean\":\n\t        dist.all_reduce(x, op=dist.ReduceOp.SUM)\n\t        x /= dist.get_world_size()\n\t    else:\n\t        raise ValueError(f\"Unknown reduction op '{op}'\")\n\t    return x\n", "def maybe_unpack(x):\n\t    \"\"\"Unpack a tuple if it's a tuple, otherwise return the value.\"\"\"\n\t    if isinstance(x, tuple):\n\t        x, *_ = x\n\t    return x\n\tdef shift_labels(x: th.Tensor, shift: int):\n\t    \"\"\"Shift labels by a given amount.\n\t    Args:\n\t        x: (batch x seq_len) labels to shift.\n\t        shift: Amount to shift by. Positive values take from the start, negative values\n", "            negative values take from the end.\n\t    Returns:\n\t        (batch x (seq_len - shift)) labels shifted by the given amount.\n\t    \"\"\"\n\t    if shift > 0:\n\t        return x[:, shift:]\n\t    if shift < 0:\n\t        return x[:, :shift]\n\t    return x\n\tdef shift_preds(x: th.Tensor, shift: int):\n", "    \"\"\"Shift predictions by a given amount.\n\t    Args:\n\t        x: (batch x seq_len) predictions to shift.\n\t        shift: Amount to shift by. Positive values take from the end, negative values\n\t            from the start.\n\t    Returns:\n\t        (batch x (seq_len - shift)) predictions shifted by the given amount.\n\t    \"\"\"\n\t    if shift > 0:\n\t        return x[:, :-shift]\n", "    if shift < 0:\n\t        return x[:, -shift:]\n\t    return x\n\tT = TypeVar(\"T\")\n\t# Backported from Python 3.10\n\tdef pairwise(it: Iterable[T]) -> Iterable[tuple[T, T]]:\n\t    \"\"\"Iterate over pairs of elements in an iterable.\"\"\"\n\t    yield from zip(it, islice(it, 1, None))\n\t# Define pytree type recursively- this works for Pylance but unfortunately not MyPy\n\tAnyTree = Union[th.Tensor, dict[Any, \"AnyTree\"], list[\"AnyTree\"], tuple[\"AnyTree\", ...]]\n", "TreeType = TypeVar(\"TreeType\", bound=AnyTree)\n\tdef pytree_flatten(tree: AnyTree) -> Iterable[th.Tensor]:\n\t    \"\"\"Recursively iterate over all tensors in a pytree, in topological order.\"\"\"\n\t    # Stopping condition\n\t    if isinstance(tree, th.Tensor):\n\t        yield tree\n\t    # Recursive case\n\t    elif isinstance(tree, dict):\n\t        for elem in tree.values():\n\t            yield from pytree_flatten(elem)\n", "    elif isinstance(tree, Sequence):\n\t        for elem in tree:\n\t            yield from pytree_flatten(elem)\n\tdef pytree_map(\n\t    func: Callable[[th.Tensor], Any], tree: TreeType, strict: bool = True\n\t) -> TreeType:\n\t    \"\"\"Recursively apply a function to all tensors in a pytree.\n\t    Args:\n\t        func: Function to apply to each tensor.\n\t        tree: Pytree to apply the function to.\n", "        strict: If True, raise an error if a non-tensor leaf is encountered.\n\t    Returns:\n\t        A new pytree with the same structure. Non-tensor leaves are copied.\n\t    \"\"\"\n\t    # Stopping condition\n\t    if isinstance(tree, th.Tensor):\n\t        return func(tree)\n\t    # Recursive case\n\t    if isinstance(tree, dict):\n\t        return {k: pytree_map(func, v) for k, v in tree.items()}\n", "    if isinstance(tree, list):\n\t        return [pytree_map(func, v) for v in tree]\n\t    if isinstance(tree, tuple):\n\t        return tuple(pytree_map(func, v) for v in tree)\n\t    if strict:\n\t        raise TypeError(\n\t            f\"Found leaf '{tree}' of unsupported type '{type(tree).__name__}'- use \"\n\t            f\"`strict=False` to ignore\"\n\t        )\n\t    else:\n", "        return tree\n\tdef pytree_cat(trees: Sequence[AnyTree], dim: int = 0) -> AnyTree:\n\t    \"\"\"Concatenate pytrees along a given dimension.\n\t    All pytrees are expected to use the same collection; undefined behavior\n\t    will occur if this is not the case.\n\t    Args:\n\t        trees: Sequence of pytrees containing tensors to concatenate.\n\t        dim: Dimension to concatenate along.\n\t    Returns:\n\t        - A new pytree with the same structure.\n", "    \"\"\"\n\t    transposed_iter = zip(*(pytree_flatten(tree) for tree in trees))\n\t    leaf_iter = (th.cat(seq, dim) for seq in transposed_iter)\n\t    try:\n\t        return pytree_map(lambda _: next(leaf_iter), trees[0])  # type: ignore\n\t    except (RuntimeError, StopIteration) as e:\n\t        # Calling next() on an exhausted generator raises a RuntimeError, annoyingly\n\t        if isinstance(e, StopIteration) or \"StopIteration\" in str(e):\n\t            raise TypeError(\"All pytrees must have the same structure\") from e\n\t        else:\n", "            raise\n\tdef pytree_stack(trees: Sequence, dim: int = 0) -> AnyTree:\n\t    \"\"\"Stack pytrees along a given dimension.\n\t    All pytrees are expected to use the same collection; undefined behavior\n\t    will occur if this is not the case.\n\t    Args:\n\t        trees: Sequence of pytrees containing tensors to stack.\n\t        dim: Dimension to concatenate along.\n\t    Returns:\n\t        A new pytree with the same structure.\n", "    \"\"\"\n\t    if not len(trees):\n\t        raise ValueError(\"Cannot stack empty sequence of pytrees\")\n\t    transposed_iter = zip(*(pytree_flatten(tree) for tree in trees))\n\t    leaf_iter = (th.stack(seq, dim) for seq in transposed_iter)\n\t    try:\n\t        return pytree_map(lambda _: next(leaf_iter), trees[0])  # type: ignore\n\t    except (RuntimeError, StopIteration) as e:\n\t        # Calling next() on an exhausted generator raises a RuntimeError, annoyingly\n\t        if isinstance(e, StopIteration) or \"StopIteration\" in str(e):\n", "            raise TypeError(\"All pytrees must have the same structure\") from e\n\t        else:\n\t            raise\n\tdef revcumsum(x: Sequence[th.Tensor]) -> list[th.Tensor]:\n\t    \"\"\"Reverse cumulative sum of a sequence of tensors.\"\"\"\n\t    if not len(x):\n\t        return []\n\t    running_total = th.zeros_like(x[0])\n\t    sums = [running_total.add_(r).clone() for r in reversed(x)]\n\t    sums.reverse()\n", "    return sums\n\tdef send_to_device(tree: TreeType, device: th.device) -> TreeType:\n\t    \"\"\"Recursively send all tensors in a pytree to a device.\"\"\"\n\t    return pytree_map(lambda t: t.to(device), tree)\n\tdef tensor_hash(tensor: NDArray) -> str:\n\t    \"\"\"Fast hash of a matrix that is robust to dtype and small perturbations.\n\t    Note this relies on the ordering of the elements in the matrix, so it is\n\t    if the matrix is in any way sorted this will not work well. In addition,\n\t    this hash is intended for large tensors 64 + elements.\n\t    \"\"\"\n", "    return hashlib.sha256(str.encode(np.array_str(tensor, precision=1))).hexdigest()\n"]}
{"filename": "tuned_lens/data.py", "chunked_list": ["\"\"\"Tools for tokenizing and manipulating text datasets.\"\"\"\n\timport math\n\tfrom multiprocessing import cpu_count\n\tfrom typing import TypeVar, Union\n\tfrom datasets import Dataset, DatasetDict\n\tfrom transformers import PreTrainedTokenizerBase\n\tT = TypeVar(\"T\", bound=Union[Dataset, DatasetDict])\n\tdef chunk_and_tokenize(\n\t    data: T,\n\t    tokenizer: PreTrainedTokenizerBase,\n", "    *,\n\t    format: str = \"torch\",\n\t    num_proc: int = min(cpu_count() // 2, 16),\n\t    text_key: str = \"text\",\n\t    max_length: int = 2048,\n\t    return_final_batch: bool = False,\n\t    load_from_cache_file: bool = True,\n\t) -> tuple[T, float]:\n\t    \"\"\"Perform GPT-style chunking and tokenization on a dataset.\n\t    The resulting dataset will consist entirely of chunks exactly `max_length` tokens\n", "    long. Long sequences will be split into multiple chunks, and short sequences will\n\t    be merged with their neighbors, using `eos_token` as a separator. The fist token\n\t    will also always be an `eos_token`.\n\t    Args:\n\t        data: The dataset to chunk and tokenize.\n\t        tokenizer: The tokenizer to use.\n\t        format: The format to return the dataset in, passed to `Dataset.with_format`.\n\t        num_proc: The number of processes to use for tokenization.\n\t        text_key: The key in the dataset to use as the text to tokenize.\n\t        max_length: The maximum length of a batch of input ids.\n", "        return_final_batch: Whether to return the final batch, which may be smaller\n\t            than the others.\n\t        load_from_cache_file: Whether to load from the cache file.\n\t    Returns:\n\t        * The chunked and tokenized dataset.\n\t        * The ratio of nats to bits per byte see https://arxiv.org/pdf/2101.00027.pdf,\n\t            section 3.1.\n\t    \"\"\"\n\t    def _tokenize_fn(x: dict[str, list]):\n\t        chunk_size = min(tokenizer.model_max_length, max_length)\n", "        sep = tokenizer.eos_token or \"<|endoftext|>\"\n\t        joined_text = sep.join([\"\"] + x[text_key])\n\t        output = tokenizer(\n\t            # Concatenate all the samples together, separated by the EOS token.\n\t            joined_text,  # start with an eos token\n\t            max_length=chunk_size,\n\t            return_attention_mask=False,\n\t            return_overflowing_tokens=True,\n\t            truncation=True,\n\t        )\n", "        total_tokens = sum(len(ids) for ids in output[\"input_ids\"])\n\t        total_bytes = len(joined_text.encode(\"utf-8\"))\n\t        assert (\n\t            \"overflowing_tokens\" not in output\n\t        ), \"We should not have any overflowing tokens.\"\n\t        if not return_final_batch:\n\t            # We know that the last sample will almost always be less than the max\n\t            # number of tokens, and we don't want to pad, so we just drop it.\n\t            output = {k: v[:-1] for k, v in output.items()}\n\t        output_batch_size = len(output[\"input_ids\"])\n", "        if output_batch_size == 0:\n\t            raise ValueError(\n\t                \"Not enough data to create a single batch complete batch.\"\n\t                \" Either allow the final batch to be returned,\"\n\t                \" or supply more data.\"\n\t            )\n\t        # We need to output this in order to compute the number of bits per byte\n\t        div, rem = divmod(total_tokens, output_batch_size)\n\t        output[\"length\"] = [div] * output_batch_size\n\t        output[\"length\"][-1] += rem\n", "        div, rem = divmod(total_bytes, output_batch_size)\n\t        output[\"bytes\"] = [div] * output_batch_size\n\t        output[\"bytes\"][-1] += rem\n\t        return output\n\t    data = data.map(\n\t        _tokenize_fn,\n\t        # Batching is important for ensuring that we don't waste tokens\n\t        # since we always throw away the last element of the batch we\n\t        # want to keep the batch size as large as possible\n\t        batched=True,\n", "        batch_size=2048,\n\t        num_proc=num_proc,\n\t        remove_columns=get_columns_all_equal(data),\n\t        load_from_cache_file=load_from_cache_file,\n\t    )\n\t    total_bytes: float = sum(data[\"bytes\"])\n\t    total_tokens: float = sum(data[\"length\"])\n\t    return data.with_format(format, columns=[\"input_ids\"]), (\n\t        total_tokens / total_bytes\n\t    ) / math.log(2)\n", "def get_columns_all_equal(dataset: Union[Dataset, DatasetDict]) -> list[str]:\n\t    \"\"\"Get a single list of columns in a `Dataset` or `DatasetDict`.\n\t    We assert the columms are the same across splits if it's a `DatasetDict`.\n\t    Args:\n\t        dataset: The dataset to get the columns from.\n\t    Returns:\n\t        A list of columns.\n\t    \"\"\"\n\t    if isinstance(dataset, DatasetDict):\n\t        cols_by_split = dataset.column_names.values()\n", "        columns = next(iter(cols_by_split))\n\t        if not all(cols == columns for cols in cols_by_split):\n\t            raise ValueError(\"All splits must have the same columns\")\n\t        return columns\n\t    return dataset.column_names\n"]}
{"filename": "tuned_lens/model_surgery.py", "chunked_list": ["\"\"\"Tools for finding and modifying components in a transformer model.\"\"\"\n\tfrom contextlib import contextmanager\n\tfrom typing import Any, Generator, TypeVar, Union\n\timport torch as th\n\timport transformers as tr\n\tfrom transformers import models\n\tdef get_value_for_key(obj: Any, key: str) -> Any:\n\t    \"\"\"Get a value using `__getitem__` if `key` is numeric and `getattr` otherwise.\"\"\"\n\t    return obj[int(key)] if key.isdigit() else getattr(obj, key)\n\tdef set_value_for_key_(obj: Any, key: str, value: Any) -> None:\n", "    \"\"\"Set value in-place if `key` is numeric and `getattr` otherwise.\"\"\"\n\t    if key.isdigit():\n\t        obj[int(key)] = value\n\t    else:\n\t        setattr(obj, key, value)\n\tdef get_key_path(model: th.nn.Module, key_path: str) -> Any:\n\t    \"\"\"Get a value by key path, e.g. `layers.0.attention.query.weight`.\"\"\"\n\t    for key in key_path.split(\".\"):\n\t        model = get_value_for_key(model, key)\n\t    return model\n", "def set_key_path_(\n\t    model: th.nn.Module, key_path: str, value: Union[th.nn.Module, th.Tensor]\n\t) -> None:\n\t    \"\"\"Set a value by key path in-place, e.g. `layers.0.attention.query.weight`.\"\"\"\n\t    keys = key_path.split(\".\")\n\t    for key in keys[:-1]:\n\t        model = get_value_for_key(model, key)\n\t    setattr(model, keys[-1], value)\n\tT = TypeVar(\"T\", bound=th.nn.Module)\n\t@contextmanager\n", "def assign_key_path(model: T, key_path: str, value: Any) -> Generator[T, None, None]:\n\t    \"\"\"Temporarily set a value by key path while in the context.\"\"\"\n\t    old_value = get_key_path(model, key_path)\n\t    set_key_path_(model, key_path, value)\n\t    try:\n\t        yield model\n\t    finally:\n\t        set_key_path_(model, key_path, old_value)\n\tNorm = Union[th.nn.LayerNorm, models.llama.modeling_llama.LlamaRMSNorm]\n\tdef get_final_norm(model: tr.PreTrainedModel) -> Norm:\n", "    \"\"\"Get the final norm from a model.\n\t    This isn't standardized across models, so this will need to be updated as\n\t    we add new models.\n\t    \"\"\"\n\t    if not hasattr(model, \"base_model\"):\n\t        raise ValueError(\"Model does not have a `base_model` attribute.\")\n\t    base_model = model.base_model\n\t    if isinstance(base_model, models.opt.modeling_opt.OPTModel):\n\t        final_layer_norm = base_model.decoder.final_layer_norm\n\t    elif isinstance(base_model, models.gpt_neox.modeling_gpt_neox.GPTNeoXModel):\n", "        final_layer_norm = base_model.final_layer_norm\n\t    elif isinstance(\n\t        base_model,\n\t        (\n\t            models.bloom.modeling_bloom.BloomModel,\n\t            models.gpt2.modeling_gpt2.GPT2Model,\n\t            models.gpt_neo.modeling_gpt_neo.GPTNeoModel,\n\t            models.gptj.modeling_gptj.GPTJModel,\n\t        ),\n\t    ):\n", "        final_layer_norm = base_model.ln_f\n\t    elif isinstance(base_model, models.llama.modeling_llama.LlamaModel):\n\t        final_layer_norm = base_model.norm\n\t    else:\n\t        raise NotImplementedError(f\"Unknown model type {type(base_model)}\")\n\t    if final_layer_norm is None:\n\t        raise ValueError(\"Model does not have a final layer norm.\")\n\t    assert isinstance(final_layer_norm, Norm.__args__)  # type: ignore\n\t    return final_layer_norm\n\tdef get_transformer_layers(model: tr.PreTrainedModel) -> tuple[str, th.nn.ModuleList]:\n", "    \"\"\"Get the decoder layers from a model.\n\t    Args:\n\t        model: The model to search.\n\t    Returns:\n\t        A tuple containing the key path to the layer list and the list itself.\n\t    Raises:\n\t        ValueError: If no such list exists.\n\t    \"\"\"\n\t    if not hasattr(model, \"base_model\"):\n\t        raise ValueError(\"Model does not have a `base_model` attribute.\")\n", "    path_to_layers = [\"base_model\"]\n\t    base_model = model.base_model\n\t    if isinstance(base_model, models.opt.modeling_opt.OPTModel):\n\t        path_to_layers += [\"decoder\", \"layers\"]\n\t    elif isinstance(base_model, models.gpt_neox.modeling_gpt_neox.GPTNeoXModel):\n\t        path_to_layers += [\"layers\"]\n\t    elif isinstance(\n\t        base_model,\n\t        (\n\t            models.bloom.modeling_bloom.BloomModel,\n", "            models.gpt2.modeling_gpt2.GPT2Model,\n\t            models.gpt_neo.modeling_gpt_neo.GPTNeoModel,\n\t            models.gptj.modeling_gptj.GPTJModel,\n\t        ),\n\t    ):\n\t        path_to_layers += [\"h\"]\n\t    elif isinstance(base_model, models.llama.modeling_llama.LlamaModel):\n\t        path_to_layers += [\"layers\"]\n\t    else:\n\t        raise NotImplementedError(f\"Unknown model type {type(base_model)}\")\n", "    path_to_layers = \".\".join(path_to_layers)\n\t    return path_to_layers, get_key_path(model, path_to_layers)\n\t@contextmanager\n\tdef delete_layers(model: T, indices: list[int]) -> Generator[T, None, None]:\n\t    \"\"\"Temporarily delete the layers at `indices` from `model` while in the context.\"\"\"\n\t    list_path, layer_list = get_transformer_layers(model)\n\t    modified_list = th.nn.ModuleList(layer_list)\n\t    for i in sorted(indices, reverse=True):\n\t        del modified_list[i]\n\t    set_key_path_(model, list_path, modified_list)\n", "    try:\n\t        yield model\n\t    finally:\n\t        set_key_path_(model, list_path, layer_list)\n\t@contextmanager\n\tdef permute_layers(model: T, indices: list[int]) -> Generator[T, None, None]:\n\t    \"\"\"Temporarily permute the layers of `model` by `indices` while in the context.\n\t    The number of indices provided may be not be equal to the number of\n\t    layers in the model. Layers will be dropped or duplicated accordingly.\n\t    \"\"\"\n", "    list_path, layer_list = get_transformer_layers(model)\n\t    permuted_list = th.nn.ModuleList([layer_list[i] for i in indices])\n\t    set_key_path_(model, list_path, permuted_list)\n\t    try:\n\t        yield model\n\t    finally:\n\t        set_key_path_(model, list_path, layer_list)\n\tdef permute_layers_(model: th.nn.Module, indices: list[int]):\n\t    \"\"\"Permute the layers of `model` by `indices` in-place.\n\t    The number of indices provided may be not be equal to the number of\n", "    layers in the model. Layers will be dropped or duplicated accordingly.\n\t    \"\"\"\n\t    list_path, layer_list = get_transformer_layers(model)\n\t    permuted_list = th.nn.ModuleList([layer_list[i] for i in indices])\n\t    set_key_path_(model, list_path, permuted_list)\n\t@contextmanager\n\tdef replace_layers(\n\t    model: T, indices: list[int], replacements: list[th.nn.Module]\n\t) -> Generator[T, None, None]:\n\t    \"\"\"Replace the layers at `indices` with `replacements` while in the context.\"\"\"\n", "    list_path, layer_list = get_transformer_layers(model)\n\t    modified_list = th.nn.ModuleList(layer_list)\n\t    for i, replacement in zip(indices, replacements):\n\t        modified_list[i] = replacement\n\t    set_key_path_(model, list_path, modified_list)\n\t    try:\n\t        yield model\n\t    finally:\n\t        set_key_path_(model, list_path, layer_list)\n"]}
{"filename": "tuned_lens/plotting/trajectory_plotting.py", "chunked_list": ["\"\"\"Contains utility classes for creating heatmap visualizations.\"\"\"\n\tfrom dataclasses import dataclass\n\tfrom typing import Any, Dict, Optional\n\timport numpy as np\n\tfrom numpy.typing import NDArray\n\tfrom plotly import graph_objects as go\n\t@dataclass\n\tclass TrajectoryLabels:\n\t    \"\"\"Contains sets of labels for each layer and position in the residual stream.\"\"\"\n\t    # (n_layers x sequence_length) label for each layer and position in the stream.\n", "    label_strings: NDArray[np.str_]\n\t    # (sequence_length) labels for the sequence dimension typically the input tokens.\n\t    sequence_labels: NDArray[np.str_]\n\t    # (n_layers x sequence_length x k) k entries to display when hovering over a cell.\n\t    # For example, the top k prediction from the lens at each layer.\n\t    hover_over_entries: Optional[NDArray[np.str_]] = None\n\t@dataclass\n\tclass TrajectoryStatistic:\n\t    \"\"\"This class represents a trajectory statistic that can be visualized.\n\t    For example, the entropy of the lens predictions at each layer.\n", "    \"\"\"\n\t    # The name of the statistic.\n\t    name: str\n\t    # (n_layers x sequence_length) value of the statistic at each layer and position.\n\t    stats: NDArray[np.float32]\n\t    # labels for each layer and position in the stream. For example, the top 1\n\t    # prediction from the lens at each layer.\n\t    labels: Optional[TrajectoryLabels] = None\n\t    # The units of the statistic.\n\t    units: Optional[str] = None\n", "    # The maximum value of the statistic.\n\t    max: Optional[float] = None\n\t    # The minimum value of the statistic.\n\t    min: Optional[float] = None\n\t    def __post_init__(self) -> None:\n\t        \"\"\"Validate class invariants.\"\"\"\n\t        assert len(self.stats.shape) == 2\n\t        assert self.labels is None or (\n\t            self.labels.label_strings.shape == self.stats.shape\n\t            and self.labels.sequence_labels.shape[0] == self.stats.shape[1]\n", "        )\n\t    @property\n\t    def num_layers(self) -> int:\n\t        \"\"\"Return the number of layers in the stream.\"\"\"\n\t        return self.stats.shape[0]\n\t    def heatmap(\n\t        self,\n\t        layer_stride: int = 1,\n\t        colorscale: str = \"rdbu_r\",\n\t        **kwargs,\n", "    ) -> go.Heatmap:\n\t        \"\"\"Returns a Plotly Heatmap object for this statistic.\n\t        Args:\n\t            layer_stride : The number of layers between each layer plotted.\n\t            colorscale : The colorscale to use for the heatmap.\n\t            **kwargs : Additional keyword arguments to pass to the Heatmap constructor.\n\t        Returns:\n\t            A plotly Heatmap where the x-axis is the sequence dimension, the y-axis is\n\t            the layer dimension, and the color of each cell is the value of\n\t            the statistic.\n", "        \"\"\"\n\t        labels = np.array([\"input\", *map(str, range(1, self.num_layers - 1)), \"output\"])\n\t        color_matrix = self.stats\n\t        color_matrix = _stride_keep_last(color_matrix, layer_stride)\n\t        labels = _stride_keep_last(labels, layer_stride)\n\t        heatmap_kwargs: Dict[str, Any] = dict(\n\t            y=labels,\n\t            z=color_matrix,\n\t            colorbar=dict(\n\t                title=f\"{self.name} ({self.units})\",\n", "                titleside=\"right\",\n\t            ),\n\t            zmax=self.max,\n\t            zmin=self.min,\n\t        )\n\t        if self.labels is not None:\n\t            label_strings = self.labels.label_strings\n\t            label_strings = _stride_keep_last(label_strings, layer_stride)\n\t            # Hack to ensure that Plotly doesn't de-duplicate the x-axis labels\n\t            x_labels = [\n", "                x + \"\\u200c\" * i for i, x in enumerate(self.labels.sequence_labels)\n\t            ]\n\t            heatmap_kwargs.update(\n\t                colorscale=colorscale,\n\t                text=label_strings,\n\t                texttemplate=\"<b>%{text}</b>\",\n\t                x=x_labels,\n\t            )\n\t            if self.labels.hover_over_entries is not None:\n\t                hover_over_entries = _stride_keep_last(\n", "                    self.labels.hover_over_entries, layer_stride\n\t                )\n\t                heatmap_kwargs.update(\n\t                    customdata=hover_over_entries,\n\t                    hoverlabel=dict(bgcolor=\"rgb(42, 42, 50)\"),\n\t                    hovertemplate=\"<br>\".join(\n\t                        f\" %{{customdata[{i}]}}\"\n\t                        for i in range(hover_over_entries.shape[2])\n\t                    )\n\t                    + \"<extra></extra>\",\n", "                )\n\t        heatmap_kwargs.update(kwargs)\n\t        return go.Heatmap(**heatmap_kwargs)\n\t    def figure(\n\t        self,\n\t        title: str = \"\",\n\t        layer_stride: int = 1,\n\t        colorscale: str = \"rdbu_r\",\n\t        token_width: int = 80,\n\t    ) -> go.Figure:\n", "        \"\"\"Produce a heatmap plot of the statistic.\n\t        Args:\n\t            title : The title of the plot.\n\t            layer_stride : The number of layers between each layer we plot.\n\t            colorscale : The colorscale to use for the heatmap.\n\t            token_width : The width of each token in the plot.\n\t        Returns:\n\t            The plotly heatmap figure.\n\t        \"\"\"\n\t        heatmap = self.heatmap(layer_stride, colorscale)\n", "        figure_width = 200 + token_width * self.stats.shape[1]\n\t        fig = go.Figure(heatmap).update_layout(\n\t            title_text=title,\n\t            title_x=0.5,\n\t            width=figure_width,\n\t            xaxis_title=\"Input\",\n\t            yaxis_title=\"Layer\",\n\t        )\n\t        return fig\n\tdef _stride_keep_last(x: NDArray, stride: int):\n", "    return np.concatenate([x[:-1:stride], [x[-1]]])\n"]}
{"filename": "tuned_lens/plotting/__init__.py", "chunked_list": ["\"\"\"Provides tools for plotting.\"\"\"\n\tfrom .prediction_trajectory import PredictionTrajectory\n\tfrom .token_formatter import TokenFormatter\n\tfrom .trajectory_plotting import TrajectoryLabels, TrajectoryStatistic\n"]}
{"filename": "tuned_lens/plotting/prediction_trajectory.py", "chunked_list": ["\"\"\"Plot a lens table for some given text and model.\"\"\"\n\timport logging\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional, Sequence, Union\n\timport numpy as np\n\timport torch as th\n\tfrom numpy.typing import NDArray\n\tfrom transformers import (\n\t    PreTrainedModel,\n\t    PreTrainedTokenizer,\n", "    PreTrainedTokenizerFast,\n\t)\n\tfrom ..nn.lenses import Lens\n\tfrom .token_formatter import TokenFormatter\n\tfrom .trajectory_plotting import TrajectoryLabels, TrajectoryStatistic\n\tTokenizer = Union[PreTrainedTokenizer, PreTrainedTokenizerFast]\n\t@dataclass\n\tclass PredictionTrajectory:\n\t    \"\"\"Contains the trajectory predictions for a sequence of tokens.\n\t    A prediction trajectory is the set of next token predictions produced by the\n", "    conjunction of a lens and a model when evaluated on a specific sequence of tokens.\n\t    This class include multiple methods for visualizing different\n\t    aspects of the trajectory.\n\t    \"\"\"\n\t    # The log probabilities of the predictions for each hidden layer + the models logits\n\t    # Shape: (num_layers, seq_len, vocab_size)\n\t    log_probs: NDArray[np.float32]\n\t    input_ids: NDArray[np.int64]\n\t    targets: Optional[NDArray[np.int64]] = None\n\t    tokenizer: Optional[Tokenizer] = None\n", "    def __post_init__(self) -> None:\n\t        \"\"\"Validate class invariants.\"\"\"\n\t        assert len(self.log_probs.shape) == 3, \"log_probs.shape: {}\".format(\n\t            self.log_probs.shape\n\t        )\n\t        assert (\n\t            self.log_probs.shape[1] == self.input_ids.shape[0]\n\t        ), \"log_probs.shape: {}, input_ids.shape: {}\".format(\n\t            self.log_probs.shape, self.input_ids.shape\n\t        )\n", "        assert (\n\t            self.targets is None or self.targets.shape[0] == self.input_ids.shape[0]\n\t        ), \"targets.shape: {}, input_ids.shape: {}\".format(\n\t            self.targets.shape, self.input_ids.shape\n\t        )\n\t    @property\n\t    def num_layers(self) -> int:\n\t        \"\"\"Returns the number of layers in the stream.\"\"\"\n\t        return self.log_probs.shape[0] - 1\n\t    @property\n", "    def num_tokens(self) -> int:\n\t        \"\"\"Returns the number of tokens in this slice of the sequence.\"\"\"\n\t        return self.log_probs.shape[1]\n\t    @property\n\t    def vocab_size(self) -> int:\n\t        \"\"\"Returns the size of the vocabulary.\"\"\"\n\t        return self.log_probs.shape[2]\n\t    @property\n\t    def model_log_probs(self) -> NDArray[np.float32]:\n\t        \"\"\"Returns the log probs of the model.\"\"\"\n", "        return self.log_probs[-1, ...]\n\t    @property\n\t    def probs(self) -> NDArray[np.float32]:\n\t        \"\"\"Returns the probabilities of the predictions.\"\"\"\n\t        return np.exp(self.log_probs)\n\t    @classmethod\n\t    def from_lens_and_model(\n\t        cls,\n\t        lens: Lens,\n\t        model: PreTrainedModel,\n", "        input_ids: Sequence[int],\n\t        tokenizer: Optional[Tokenizer] = None,\n\t        targets: Optional[Sequence[int]] = None,\n\t        start_pos: int = 0,\n\t        end_pos: Optional[int] = None,\n\t        mask_input: bool = False,\n\t    ) -> \"PredictionTrajectory\":\n\t        \"\"\"Constructs a slice of the model's prediction trajectory.\n\t        Args:\n\t            lens : The lens to use for constructing the latent predictions.\n", "            model : The model to get the predictions from.\n\t            tokenizer : The tokenizer to use for decoding the predictions.\n\t            input_ids : The input ids to pass to the model.\n\t            targets : The targets for the input sequence.\n\t            start_pos : The start position of the slice across the sequence dimension.\n\t            end_pos : The end position of the slice accross the sequence dimension.\n\t            mask_input : whether to forbid the lens from predicting the input tokens.\n\t        Returns:\n\t            A PredictionTrajectory object containing the requested slice.\n\t        \"\"\"\n", "        with th.no_grad():\n\t            input_ids_th = th.tensor(input_ids, dtype=th.int64, device=model.device)\n\t            outputs = model(input_ids_th.unsqueeze(0), output_hidden_states=True)\n\t        # Slice arrays the specified range\n\t        model_log_probs = (\n\t            outputs.logits[..., start_pos:end_pos, :]\n\t            .log_softmax(-1)\n\t            .squeeze()\n\t            .detach()\n\t            .cpu()\n", "            .numpy()\n\t        )\n\t        stream = [h[..., start_pos:end_pos, :] for h in outputs.hidden_states]\n\t        input_ids_np = np.array(input_ids[start_pos:end_pos])\n\t        targets_np = (\n\t            np.array(targets[start_pos:end_pos]) if targets is not None else None\n\t        )\n\t        # Create the stream of log probabilities from the lens\n\t        traj_log_probs = []\n\t        for i, h in enumerate(stream[:-1]):\n", "            logits = lens.forward(h, i)\n\t            if mask_input:\n\t                logits[..., input_ids_np] = -th.finfo(h.dtype).max\n\t            traj_log_probs.append(\n\t                logits.log_softmax(dim=-1).squeeze().detach().cpu().numpy()\n\t            )\n\t        # Add model predictions\n\t        if traj_log_probs[-1].shape[-1] != model_log_probs.shape[-1]:\n\t            logging.warning(\n\t                \"Lens vocab size does not match model vocab size.\"\n", "                \"Truncating model outputs to match lens vocab size.\"\n\t            )\n\t        # Handle the case where the model has more/less tokens than the lens\n\t        min_logit = -np.finfo(model_log_probs.dtype).max\n\t        trunc_model_log_probs = np.full_like(traj_log_probs[-1], min_logit)\n\t        trunc_model_log_probs[..., : model_log_probs.shape[-1]] = model_log_probs\n\t        traj_log_probs.append(trunc_model_log_probs)\n\t        return cls(\n\t            tokenizer=tokenizer,\n\t            log_probs=np.array(traj_log_probs),\n", "            targets=targets_np,\n\t            input_ids=input_ids_np,\n\t        )\n\t    def _get_topk_tokens_and_values(\n\t        self,\n\t        k: int,\n\t        sort_by: NDArray[np.float32],\n\t        values: NDArray[np.float32],\n\t    ) -> NDArray[np.str_]:\n\t        # Get the top-k tokens & probabilities for each\n", "        topk_inds = np.argpartition(sort_by, -k, axis=-1)[..., -k:]\n\t        topk_sort_by = np.take_along_axis(sort_by, topk_inds, axis=-1)\n\t        topk_values = np.take_along_axis(values, topk_inds, axis=-1)\n\t        # Ensure that the top-k tokens are sorted by probability\n\t        sorted_top_k_inds = np.argsort(-topk_sort_by, axis=-1)\n\t        topk_inds = np.take_along_axis(topk_inds, sorted_top_k_inds, axis=-1)\n\t        topk_values = np.take_along_axis(topk_values, sorted_top_k_inds, axis=-1)\n\t        # reshape topk_ind from (layers, seq, k) to (layers*seq*k),\n\t        # convert_ids_to_tokens, then reshape back to (layers, seq, k)\n\t        topk_tokens = self.tokenizer.convert_ids_to_tokens(topk_inds.flatten().tolist())\n", "        topk_tokens = np.array(topk_tokens).reshape(topk_inds.shape)\n\t        return topk_tokens, topk_values\n\t    def largest_prob_labels(\n\t        self,\n\t        formatter: Optional[TokenFormatter] = None,\n\t        min_prob: np.float_ = np.finfo(np.float32).eps,\n\t        topk: int = 10,\n\t    ) -> TrajectoryLabels:\n\t        \"\"\"Labels for the prediction trajectory based on the most probable tokens.\n\t        Args:\n", "            formatter : The formatter to use for formatting the tokens.\n\t            min_prob : The minimum probability for a token to used as a label.\n\t            topk : The number of top tokens to include in the hover over menu.\n\t        Raises:\n\t            ValueError: If the tokenizer is not set.\n\t        Returns:\n\t            a set of stream labels that can be applied to a trajectory statistic.\n\t        \"\"\"\n\t        if self.tokenizer is None:\n\t            raise ValueError(\"Tokenizer must be set to get labels.\")\n", "        if formatter is None:\n\t            formatter = TokenFormatter()\n\t        input_tokens = self.tokenizer.convert_ids_to_tokens(self.input_ids.tolist())\n\t        entry_format_fn = np.vectorize(\n\t            lambda token, percent: f\"{formatter.format(token)} {percent:.2f}%\"\n\t        )\n\t        topk_tokens, topk_probs = self._get_topk_tokens_and_values(\n\t            k=topk, sort_by=self.log_probs, values=self.probs\n\t        )\n\t        top_tokens = topk_tokens[..., 0]\n", "        top_probs = topk_probs[..., 0]\n\t        label_strings = np.where(\n\t            top_probs > min_prob, formatter.vectorized_format(top_tokens), \"\"\n\t        )\n\t        return TrajectoryLabels(\n\t            label_strings=label_strings,\n\t            sequence_labels=formatter.vectorized_format(input_tokens),\n\t            hover_over_entries=entry_format_fn(topk_tokens, topk_probs * 100),\n\t        )\n\t    def largest_delta_in_prob_labels(\n", "        self,\n\t        other: \"PredictionTrajectory\",\n\t        formatter: Optional[TokenFormatter] = None,\n\t        min_prob_delta: np.float_ = np.finfo(np.float32).eps,\n\t        topk: int = 10,\n\t    ) -> TrajectoryLabels:\n\t        \"\"\"Labels for a trajectory statistic based on the largest change in probability.\n\t        Args:\n\t            other : The other prediction trajectory to compare to.\n\t            formatter : A TokenFormatter to use for formatting the labels.\n", "            min_prob_delta : The minimum change in probability to include a label.\n\t            topk : The number of top tokens to include in the hover over menu.\n\t        Raises:\n\t            ValueError: If the tokenizer is not set.\n\t        Returns:\n\t            A set of stream labels that can be added to a trajectory statistic.\n\t        \"\"\"\n\t        if self.tokenizer is None:\n\t            raise ValueError(\"Tokenizer must be set to get labels.\")\n\t        if formatter is None:\n", "            formatter = TokenFormatter()\n\t        input_tokens = self.tokenizer.convert_ids_to_tokens(self.input_ids.tolist())\n\t        entry_format_fn = np.vectorize(\n\t            lambda token, percent: f\"{formatter.format(token)} Δ{percent:.2f}%\"\n\t        )\n\t        deltas = other.probs - self.probs\n\t        topk_tokens, topk_deltas = self._get_topk_tokens_and_values(\n\t            k=topk, sort_by=np.abs(deltas), values=deltas\n\t        )\n\t        top_tokens = topk_tokens[..., 0]\n", "        top_deltas = topk_deltas[..., 0]\n\t        label_strings = np.where(\n\t            np.abs(top_deltas) > min_prob_delta,\n\t            formatter.vectorized_format(top_tokens),\n\t            \"\",\n\t        )\n\t        return TrajectoryLabels(\n\t            label_strings=label_strings,\n\t            sequence_labels=formatter.vectorized_format(input_tokens),\n\t            hover_over_entries=entry_format_fn(topk_tokens, 100 * topk_deltas),\n", "        )\n\t    def cross_entropy(self, **kwargs) -> TrajectoryStatistic:\n\t        \"\"\"The cross entropy of the predictions to the targets.\n\t        Args:\n\t            **kwargs: are passed to largest_prob_labels.\n\t        Returns:\n\t            A TrajectoryStatistic with the cross entropy of the predictions to the\n\t            targets.\n\t        \"\"\"\n\t        if self.targets is None:\n", "            raise ValueError(\"Cannot compute cross entropy without targets.\")\n\t        assert self.targets.shape == self.log_probs[-1].shape[:-1], (\n\t            \"Batch and sequence lengths of targets and log probs must match.\"\n\t            f\"Got {self.targets.shape} and {self.log_probs[-1].shape[:-1]}.\"\n\t        )\n\t        return TrajectoryStatistic(\n\t            name=\"Cross Entropy\",\n\t            units=\"nats\",\n\t            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n\t            stats=-self.log_probs[:, np.arange(self.num_tokens), self.targets],\n", "        )\n\t    def entropy(self, **kwargs) -> TrajectoryStatistic:\n\t        \"\"\"The entropy of the predictions.\n\t        Args:\n\t            **kwargs: are passed to largest_prob_labels.\n\t        Returns:\n\t            A TrajectoryStatistic with the entropy of the predictions.\n\t        \"\"\"\n\t        return TrajectoryStatistic(\n\t            name=\"Entropy\",\n", "            units=\"nats\",\n\t            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n\t            stats=-np.sum(np.exp(self.log_probs) * self.log_probs, axis=-1),\n\t        )\n\t    def forward_kl(self, **kwargs) -> TrajectoryStatistic:\n\t        \"\"\"KL divergence of the lens predictions to the model predictions.\n\t        Args:\n\t            **kwargs: are passed to largest_prob_labels.\n\t        Returns:\n\t            A TrajectoryStatistic with the KL divergence of the lens predictions to the\n", "            final output of the model.\n\t        \"\"\"\n\t        model_log_probs = self.model_log_probs.reshape(\n\t            1, self.num_tokens, self.vocab_size\n\t        )\n\t        return TrajectoryStatistic(\n\t            name=\"Forward KL\",\n\t            units=\"nats\",\n\t            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n\t            stats=np.sum(\n", "                np.exp(model_log_probs) * (model_log_probs - self.log_probs), axis=-1\n\t            ),\n\t        )\n\t    def max_probability(self, **kwargs) -> TrajectoryStatistic:\n\t        \"\"\"Max probability of the among the predictions.\n\t        Args:\n\t            **kwargs: are passed to largest_prob_labels.\n\t        Returns:\n\t            A TrajectoryStatistic with the max probability of the among the predictions.\n\t        \"\"\"\n", "        return TrajectoryStatistic(\n\t            name=\"Max Probability\",\n\t            units=\"probs\",\n\t            labels=self.largest_prob_labels(**kwargs) if self.tokenizer else None,\n\t            stats=np.exp(self.log_probs.max(-1)),\n\t        )\n\t    def kl_divergence(\n\t        self, other: \"PredictionTrajectory\", **kwargs\n\t    ) -> TrajectoryStatistic:\n\t        \"\"\"Compute the KL divergence between self and other prediction trajectory.\n", "        Args:\n\t            other : The other prediction trajectory to compare to.\n\t            **kwargs: are passed to largest_delta_in_prob_labels.\n\t        Returns:\n\t            A TrajectoryStatistic with the KL divergence between self and other.\n\t        \"\"\"\n\t        kl_div = np.sum(self.probs * (self.log_probs - other.log_probs), axis=-1)\n\t        return TrajectoryStatistic(\n\t            name=\"KL(Self | Other)\",\n\t            units=\"nats\",\n", "            stats=kl_div,\n\t            labels=self.largest_delta_in_prob_labels(other, **kwargs)\n\t            if self.tokenizer\n\t            else None,\n\t            min=0,\n\t            max=None,\n\t        )\n\t    def js_divergence(\n\t        self, other: \"PredictionTrajectory\", **kwargs\n\t    ) -> TrajectoryStatistic:\n", "        \"\"\"Compute the JS divergence between self and other prediction trajectory.\n\t        Args:\n\t            other : The other prediction trajectory to compare to.\n\t            **kwargs: are passed to largest_delta_in_prob_labels.\n\t        Returns:\n\t            A TrajectoryStatistic with the JS divergence between self and other.\n\t        \"\"\"\n\t        js_div = 0.5 * np.sum(\n\t            self.probs * (self.log_probs - other.log_probs), axis=-1\n\t        ) + 0.5 * np.sum(other.probs * (other.log_probs - self.log_probs), axis=-1)\n", "        return TrajectoryStatistic(\n\t            name=\"JS(Self | Other)\",\n\t            units=\"nats\",\n\t            stats=js_div,\n\t            labels=self.largest_delta_in_prob_labels(other, **kwargs)\n\t            if self.tokenizer\n\t            else None,\n\t            min=0,\n\t            max=None,\n\t        )\n", "    def total_variation(\n\t        self, other: \"PredictionTrajectory\", **kwargs\n\t    ) -> TrajectoryStatistic:\n\t        \"\"\"Total variation distance between self and other prediction trajectory.\n\t        Args:\n\t            other : The other prediction trajectory to compare to.\n\t            **kwargs: are passed to largest_delta_in_prob_labels.\n\t        Returns:\n\t            A TrajectoryStatistic with the total variational distance between\n\t            self and other.\n", "        \"\"\"\n\t        t_var = np.abs(self.probs - other.probs).max(axis=-1)\n\t        return TrajectoryStatistic(\n\t            name=\"TV(Self | Other)\",\n\t            units=\"probs\",\n\t            stats=t_var,\n\t            labels=self.largest_delta_in_prob_labels(other, **kwargs)\n\t            if self.tokenizer\n\t            else None,\n\t            min=0,\n", "            max=1,\n\t        )\n"]}
{"filename": "tuned_lens/plotting/token_formatter.py", "chunked_list": ["\"\"\"Contains a class for formatting tokens for display in plots.\"\"\"\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional\n\timport numpy as np\n\t@dataclass\n\tclass TokenFormatter:\n\t    \"\"\"Format tokens for display in a plots.\"\"\"\n\t    ellipsis: str = \"…\"\n\t    newline_replacement: str = \"\\\\n\"\n\t    newline_token: str = \"Ċ\"\n", "    whitespace_token: str = \"Ġ\"\n\t    whitespace_replacement: str = \"_\"\n\t    max_string_len: Optional[int] = 7\n\t    def __post_init__(self) -> None:\n\t        \"\"\"Post init hook to vectorize the format function.\"\"\"\n\t        self.vectorized_format = np.vectorize(self.format)\n\t    def format(self, token: str) -> str:\n\t        \"\"\"Format a token for display in a plot.\"\"\"\n\t        if not isinstance(token, str):\n\t            return \"<unk>\"\n", "        if self.max_string_len is not None and len(token) > self.max_string_len:\n\t            token = token[: self.max_string_len - len(self.ellipsis)] + self.ellipsis\n\t        token = token.replace(self.newline_token, self.newline_replacement)\n\t        token = token.replace(self.whitespace_token, self.whitespace_replacement)\n\t        return token\n"]}
{"filename": "tuned_lens/causal/subspaces.py", "chunked_list": ["\"\"\"Provides tools for extracting causal bases from models and ablating subspaces.\"\"\"\n\tfrom contextlib import contextmanager\n\tfrom typing import Iterable, Literal, NamedTuple, Optional, Sequence\n\timport torch as th\n\timport torch.distributed as dist\n\timport torch.nn.functional as F\n\tfrom tqdm.auto import trange\n\tfrom ..model_surgery import get_transformer_layers\n\tfrom ..nn import Lens\n\tfrom ..utils import maybe_all_reduce\n", "from .utils import derange\n\t@contextmanager\n\tdef ablate_subspace(\n\t    model: th.nn.Module,\n\t    A: th.Tensor,\n\t    layer_index: int,\n\t    mode: Literal[\"mean\", \"resample\", \"zero\"] = \"zero\",\n\t    orthonormal: bool = False,\n\t):\n\t    \"\"\"Context manager that ablates a subspace of activations.\n", "    Args:\n\t        model: A hugging face transformer model.\n\t        A: Either a 2D matrix whose column space is to be removed, or a 1D vector whose\n\t            span is to be removed.\n\t        layer_index: The index of the layer to ablate.\n\t        mode: Which method to use for removing information along the subspace.\n\t            Defaults to `\"zero\"`.\n\t        orthonormal: if True, `A` is assumed to be orthonormal.\n\t    \"\"\"\n\t    _, layers = get_transformer_layers(model)\n", "    def wrapper(_, __, outputs):\n\t        h, *extras = outputs\n\t        h_ = remove_subspace(h, A, mode, orthonormal)\n\t        return h_, *extras\n\t    handle = layers[layer_index].register_forward_hook(wrapper)  # type: ignore\n\t    try:\n\t        yield model\n\t    finally:\n\t        handle.remove()\n\tclass CausalBasis(NamedTuple):\n", "    \"\"\"An ordered orthonormal basis for a subspace of activations.\n\t    Attributes:\n\t        energies: A vector of shape (k,) containing the energies of the\n\t            basis vectors. Each energy is the expected KL divergence of\n\t            the post-intervention logits wrt the control logits when the\n\t            corresponding basis vector is ablated.\n\t        vectors: A matrix of shape (d, k) where d is the ambient dimension\n\t            and k is the dimension of the subspace. The columns of this\n\t            matrix are basis vectors, ordered by decreasing energy.\n\t    \"\"\"\n", "    energies: th.Tensor\n\t    vectors: th.Tensor\n\tdef extract_causal_bases(\n\t    lens: Lens,\n\t    hiddens: Sequence[th.Tensor],\n\t    k: int,\n\t    *,\n\t    labels: Optional[th.Tensor] = None,\n\t    max_iter: int = 100,\n\t    mode: Literal[\"mean\", \"resample\", \"zero\"] = \"mean\",\n", ") -> Iterable[CausalBasis]:\n\t    \"\"\"Extract causal bases for probes at each layer of a model.\n\t    Args:\n\t        lens: A lens to compute causal bases for.\n\t        hiddens: A sequence of hidden states from the model.\n\t        k: The number of basis vectors to compute for each layer.\n\t        max_iter: The maximum number of iterations to run L-BFGS for each vector.\n\t        mode: Which method to use for removing information along the subspace.\n\t            Defaults to `\"zero\"`.\n\t    \"\"\"\n", "    lens.requires_grad_(False)\n\t    device = hiddens[0].device\n\t    dtype = hiddens[0].dtype\n\t    d = hiddens[0].shape[-1]\n\t    hiddens = [h.detach() for h in hiddens]\n\t    num_layers = len(hiddens) - 1\n\t    assert k <= d\n\t    if k < 1:\n\t        k = d\n\t    eye = th.eye(d, device=device, dtype=dtype)\n", "    show_pbar = not dist.is_initialized() or dist.get_rank() == 0\n\t    pbar = trange(num_layers * k) if show_pbar else None\n\t    # Outer loop iterates over layers\n\t    for i in range(num_layers):\n\t        U = lens.unembed.unembedding.weight.data.T\n\t        logits = lens(hiddens[i], i)\n\t        log_p = logits.log_softmax(-1)\n\t        U = lens.transform_hidden(U, i)  # TODO not sure if we need transposes here\n\t        # Compute the baseline loss up front so that we can subtract it\n\t        # from the post-ablation losses to get the loss increment\n", "        if labels is not None:\n\t            base_loss = F.cross_entropy(\n\t                log_p[:, :-1].flatten(0, -2), labels[:, 1:].flatten()\n\t            )\n\t        else:\n\t            base_loss = 0.0\n\t        # Initialize basis vectors with left singular vectors of U\n\t        u, *_ = th.linalg.svd(U, full_matrices=False)\n\t        basis = CausalBasis(th.zeros(k, device=device), u[:, :k].float())\n\t        # Inner loop iterates over directions\n", "        p = log_p.exp()\n\t        for j in range(k):\n\t            if pbar:\n\t                pbar.set_description(f\"Layer {i + 1}/{num_layers}, vector {j + 1}/{k}\")\n\t            # Construct the operator for projecting away from the previously\n\t            # identified basis vectors\n\t            if j:\n\t                A = basis.vectors[:, :j]\n\t                proj = eye - A @ A.T\n\t            else:\n", "                proj = eye\n\t            def project(x: th.Tensor) -> th.Tensor:\n\t                # Project away from previously identified basis vectors\n\t                x = proj @ x\n\t                # Project to the unit sphere\n\t                return x / (x.norm() + th.finfo(x.dtype).eps)\n\t            basis.vectors[:, j] = project(basis.vectors[:, j])\n\t            v = th.nn.Parameter(basis.vectors[:, j])\n\t            nfev = 0\n\t            energy_delta = th.tensor(0.0, device=device)\n", "            last_energy = th.tensor(0.0, device=device)\n\t            opt = th.optim.LBFGS(\n\t                [v],\n\t                line_search_fn=\"strong_wolfe\",\n\t                max_iter=max_iter,\n\t            )\n\t            def closure():\n\t                nonlocal energy_delta, nfev, last_energy\n\t                nfev += 1\n\t                opt.zero_grad(set_to_none=False)\n", "                v_ = project(v)\n\t                h_ = remove_subspace(hiddens[i], v_, mode=mode, orthonormal=True)\n\t                logits = lens(h_, i)\n\t                if labels is not None:\n\t                    loss = -F.cross_entropy(\n\t                        logits[:, :-1].flatten(0, 1), labels[:, 1:].flatten()\n\t                    )\n\t                else:\n\t                    log_q = logits.log_softmax(-1)\n\t                    loss = -th.sum(p * (log_p - log_q), dim=-1).mean()\n", "                loss.backward()\n\t                maybe_all_reduce(loss)\n\t                maybe_all_reduce(v.grad)  # type: ignore[arg-type]\n\t                assert v.grad is not None\n\t                new_energy = -loss.detach() - base_loss\n\t                energy_delta = new_energy - last_energy\n\t                last_energy = new_energy\n\t                if pbar:\n\t                    pbar.set_postfix(energy=last_energy.item())\n\t                if not loss.isfinite():\n", "                    print(\"Loss is not finite\")\n\t                    loss = th.tensor(0.0, device=device)\n\t                    opt.zero_grad(set_to_none=False)\n\t                return loss\n\t            while nfev < max_iter:\n\t                opt.step(closure)  # type: ignore\n\t                v.data = project(v.data)\n\t                if abs(energy_delta / last_energy) < 1e-4:\n\t                    break\n\t            basis.vectors[:, j] = project(v.data)\n", "            basis.energies[j] = last_energy\n\t            if pbar:\n\t                pbar.update()\n\t        indices = basis.energies.argsort(descending=True)\n\t        yield CausalBasis(basis.energies[indices], basis.vectors[:, indices])\n\tdef remove_subspace(\n\t    u: th.Tensor,\n\t    A: th.Tensor,\n\t    mode: Literal[\"mean\", \"resample\", \"zero\"] = \"zero\",\n\t    orthonormal: bool = False,\n", ") -> th.Tensor:\n\t    \"\"\"Remove all information in `u` along the column space of `A`.\n\t    This can be done by zero, mean, or resample ablation. With zero ablation,\n\t    `u` is projected onto the orthogonal complement of col(`A`), so the resulting\n\t    vectors are orthogonal to every column in `A`. With mean ablation, `u` is projected\n\t    onto the subspace s.t. the angles between the resulting vectors and the columns of\n\t    `A` are equal to their mean values. With resample ablation, the variation in `u`\n\t    is shuffled across vectors.\n\t    Args:\n\t        u: The vectors to be projected.\n", "        A: Either a 2D matrix whose column space is to be removed, or a 1D vector whose\n\t            span is to be removed.\n\t        mode: Which method to use for removing information along the subspace.\n\t            Defaults to `\"zero\"`.\n\t        orthonormal: Whether to assume `A` is orthonormal. Defaults to `False`.\n\t    Returns:\n\t        th.Tensor: The transformed vectors.\n\t    \"\"\"\n\t    if A.ndim == 1:\n\t        A = A[..., None]\n", "    d, _ = A.shape\n\t    if u.shape[-1] != d:\n\t        raise ValueError(f\"Last dimension of u must be {d}, but is {u.shape[-1]}\")\n\t    # https://en.wikipedia.org/wiki/Projection_(linear_algebra)#Properties_and_special_cases\n\t    if orthonormal:\n\t        proj = A @ A.mT\n\t    else:\n\t        proj = A @ th.linalg.solve(A.mT @ A, A.mT)\n\t    if mode == \"zero\":\n\t        dummy = -u\n", "    else:\n\t        samples = u.flatten(0, -2)\n\t        N = samples.shape[0]\n\t        if N < 2:\n\t            raise ValueError(\"Need at least 2 vectors for mean and resample ablation\")\n\t        if mode == \"mean\":\n\t            dummy = samples.mean(0) - u\n\t        elif mode == \"resample\":\n\t            # Shuffle the rows of `samples` without fixed points.\n\t            dummy = derange(samples).view_as(u) - u\n", "        else:\n\t            raise ValueError(f\"Unknown mode {mode}\")\n\t    return u + th.einsum(\"ij,...j->...i\", proj, dummy)\n"]}
{"filename": "tuned_lens/causal/__init__.py", "chunked_list": ["\"\"\"Tools for finding and intervening on important subspaces of the residual stream.\"\"\"\n\tfrom .subspaces import (\n\t    CausalBasis,\n\t    ablate_subspace,\n\t    extract_causal_bases,\n\t    remove_subspace,\n\t)\n\tfrom .utils import derange, sample_derangement\n"]}
{"filename": "tuned_lens/causal/utils.py", "chunked_list": ["from typing import Optional\n\timport torch as th\n\tdef derange(batch: th.Tensor, generator: Optional[th.Generator] = None) -> th.Tensor:\n\t    \"\"\"Shuffle a tensor along axis 0, making sure there are no fixed points.\"\"\"\n\t    # Things get more complicated if there are multiple ranks. We perform the\n\t    # derangement *hierarchically*, first generating a shared permutation of the ranks\n\t    indices = sample_derangement(\n\t        batch.shape[0], device=batch.device, generator=generator\n\t    )\n\t    return batch[indices]\n", "def sample_derangement(\n\t    n: int,\n\t    device: th.device = th.device(\"cpu\"),\n\t    generator: Optional[th.Generator] = None,\n\t) -> th.Tensor:\n\t    \"\"\"Uniformly sample a random permutation with no fixed points.\"\"\"\n\t    if n < 2:\n\t        raise ValueError(\"Derangements only exist for n > 1\")\n\t    indices = th.arange(n, device=device)\n\t    permutation = th.randperm(n, device=device, generator=generator)\n", "    # Reject any permutations with fixed points. This seems inefficient,\n\t    # but the expected number of th.randperm calls is actually O(1); it\n\t    # asymptotically approaches e ≈ 2.7.\n\t    # See https://www.cs.upc.edu/~conrado/research/talks/analco08.pdf.\n\t    while th.any(permutation == indices):\n\t        permutation = th.randperm(n, device=device, generator=generator)\n\t    return permutation\n"]}
{"filename": "tuned_lens/causal/ablation.py", "chunked_list": ["\"\"\"Provides tools for ablating layers of a transformer model.\"\"\"\n\tfrom contextlib import contextmanager\n\tfrom typing import Literal\n\timport torch as th\n\tfrom ..model_surgery import get_transformer_layers\n\tfrom .utils import derange\n\t@contextmanager\n\tdef ablate_layer(\n\t    model: th.nn.Module,\n\t    layer_index: int,\n", "    method: Literal[\"resample\", \"mean\", \"zero\"],\n\t    *,\n\t    mode: Literal[\"batch\", \"token\"] = \"batch\",\n\t):\n\t    \"\"\"Replace residual outputs of the specified layer with dummy values.\n\t    If the method is \"resample\", the residuals are replaced with corresponding\n\t    residuals from a randomly sampled sequence in the batch. If the method is \"mean\",\n\t    the residuals are replaced with their minibatch means. If the method is \"zero\",\n\t    all residuals are replaced with the zero vector.\n\t    Args:\n", "        model: The model to modify.\n\t        layer_index: The index of the layer to modify.\n\t        method: How to ablate the layer see above.\n\t        mode: Whether to compute the mean only over the batch dimension or over the\n\t            batch and token dimensions.\n\t    \"\"\"\n\t    assert layer_index >= 0\n\t    def ablate_hook(_, inputs, outputs):\n\t        x, *_ = inputs\n\t        y, *extras = outputs\n", "        if method == \"zero\":\n\t            return x, *extras\n\t        residuals = y - x\n\t        original_shape = x.shape\n\t        if mode == \"token\":\n\t            x = x.flatten(0, 1)\n\t            residuals = residuals.flatten(0, 1)\n\t        batch_size = x.shape[0]\n\t        if batch_size < 2:\n\t            raise ValueError(\"Mean ablation requires a batch size >= 2\")\n", "        if method == \"resample\":\n\t            ablated = x + derange(residuals)\n\t        elif method == \"mean\":\n\t            ablated = x + residuals.mean(0, keepdim=True)\n\t        else:\n\t            raise ValueError(f\"Unknown ablation method: {method}\")\n\t        return ablated.reshape(original_shape), *extras\n\t    _, layers = get_transformer_layers(model)\n\t    handle = layers[layer_index].register_forward_hook(ablate_hook)  # type: ignore\n\t    try:\n", "        yield model\n\t    finally:\n\t        handle.remove()\n"]}
{"filename": "tuned_lens/scripts/explanation_generation.py", "chunked_list": ["from tuned_lens.scripts.generation_utils import threaded_generations\n\tfrom tuned_lens.scripts.ingredients import (\n\t    Data,\n\t)\n\tfrom transformers import AutoTokenizer\n\timport os\n\tfrom fire import Fire\n\timport json\n\tSYSTEM_PROMPT = \"\"\"As a language model explainer, your task is to analyze a given truncated text and deduce what information is present in the activations of a language model at any layer in the final position. Do not attempt to complete the text provided by the user, and directly provide information in the following format:\n\tCurrent word: <last word or punctuation of the text>\n", "Current sentence: <current sentence in which the last word is present, summarized if and only if it is more than 10 words long, and otherwise repeated at-verbatim>\n\tProbable next words: <comma-separated list of 5 most probable next words/punctuation>\n\tText style: <comma-separated list of 10 words describing the style of the text, its tone, the writer position, ...>\n\tParagraph summary: <summary of the paragraph, of at most 30 words>\n\tText summary: <summary of the text, of at most 30 words>\n\tProbable next sentences: <list of the 3 most likely completions of this text, *each should be exactly 8 words long*: interrupt yourself in the middle of the sentence if you exceed 8 words!! Hint: completions should porbably start with one of the most probable next words>\n\t1. <first completion>\n\t2. <second completion>\n\t3. <third completion>\n\tThe prompt of the user will always be of the form\n", "\"<Text that ends with word or punctuation <w>>\n\t->\"\n\tand you should start immediatly with\n\tCurrent word: <w>\"\"\"\n\tdef run(\n\t    pile_path: str = \"~/datasets/pile/val.jsonl\",\n\t    write_path: str = \"data.jsonl\",\n\t    model_name: str = \"EleutherAI/pythia-70m-deduped\",\n\t    max_samples: int = 40000,\n\t    max_length: int = 256,\n", "    additional_tokens_kept: int = 20,\n\t    min_length: int = 20,\n\t    nb_solutions: int = 1,\n\t    model: str = \"gpt-3.5-turbo\",\n\t    temperature: float = 0.0,\n\t):\n\t    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\t    expanded_path = os.path.expanduser(pile_path)\n\t    data = Data(name=[expanded_path], max_length=max_length + additional_tokens_kept)\n\t    dataset, _ = data.load(tokenizer)\n", "    eos = \"<|endoftext|>\"\n\t    def tokenize(toks):\n\t        return tokenizer.decode(toks).split(eos)[-1]\n\t    texts = []\n\t    i = 0\n\t    while len(texts) < max_samples and i < len(dataset):\n\t        pt = dataset[len(texts)]\n\t        if len(pt[\"input_ids\"]) > min_length + additional_tokens_kept:\n\t            texts.append(\n\t                (\n", "                    tokenize(pt[\"input_ids\"][:-additional_tokens_kept]),\n\t                    tokenize(pt[\"input_ids\"][-additional_tokens_kept:]),\n\t                )\n\t            )\n\t        i += 1\n\t    prompts = [\n\t        ((text, e), [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": text + \"\\n->\"}])\n\t        for text, e in texts\n\t    ]\n\t    results = threaded_generations(prompts, nb_solutions=nb_solutions, model=model, temperature=temperature, n_threads=50)\n", "    expanded_write_path = os.path.expanduser(write_path)\n\t    with open(expanded_write_path, \"w\") as f:\n\t        for r in results:\n\t            json.dump(r, f)\n\t            f.write(\"\\n\")\n\tif __name__ == \"__main__\":\n\t    Fire(run)\n"]}
{"filename": "tuned_lens/scripts/explore_explanation_generation.py", "chunked_list": ["from tuned_lens.scripts.generation_utils import threaded_generations\n\tfrom tuned_lens.scripts.ingredients import (\n\t    Data,\n\t)\n\tfrom transformers import AutoTokenizer\n\tSYSTEM_PROMPT = \"\"\"As a language model explainer, your task is to analyze a given truncated text and deduce what information is present in the activations of a language model at any layer in the final position. Do not attempt to complete the text provided by the user, and directly provide information in the following format:\n\tCurrent word: <last word or punctuation of the text>\n\tCurrent sentence: <current sentence in which the last word is present, summarized if and only if it is more than 10 words long, and otherwise repeated at-verbatim>\n\tProbable next words: <comma-separated list of 5 most probable next words/punctuation>\n\tText style: <comma-separated list of 3-10 words describing the style of the text>\n", "Paragraph summary: <summary of the paragraph, of at most 30 words>\n\tText summary: <summary of the text, of at most 30 words>\n\tProbable next sentences: <list of the 3 most likely completions of this text, *each should be exactly 8 words long*: interrupt yourself in the middle of the sentence if you exceed 8 words!! Hint: completions should porbably start with one of the most probable next words>\n\t1. <first completion>\n\t2. <second completion>\n\t3. <third completion>\"\"\"\n\tpile_path = \"/home/fabien/datasets/pile/val.jsonl\"\n\tmodel_name = \"EleutherAI/pythia-70m-deduped\"\n\ttokenizer = AutoTokenizer.from_pretrained(model_name)\n\tdata = Data(name=[pile_path], max_length=256)\n", "dataset, _ = data.load(tokenizer)\n\t# %%\n\t# %%\n\tprint(tokenizer.decode(dataset[5][\"input_ids\"]))\n\t# %%\n\teos = \"<|endoftext|>\"\n\tn = 10\n\tdef tokenize(toks):\n\t    return tokenizer.decode(toks).split(eos)[-1]\n\ttexts = [(tokenize(dataset[i][\"input_ids\"][:-3]), tokenize(dataset[i][\"input_ids\"][-3:])) for i in range(n)]\n", "for t, e in texts:\n\t    if eos in t:\n\t        print(t, \"|\", e)\n\t# %%\n\tprompts = [\n\t    ((text, e), [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": text}]) for text, e in texts\n\t]\n\tresults = threaded_generations(prompts, nb_solutions=2, model=\"gpt-3.5-turbo\")\n\t# %%\n\tfor r in results:\n", "    print(\"|\".join(r[0]))\n\t    print(\"=================>\")\n\t    for s in r[1]:\n\t        print(s)\n\t        print(\"-\" * 80)\n\t    print(\"=\" * 80)\n\t# %%\n"]}
{"filename": "tuned_lens/scripts/ingredients.py", "chunked_list": ["\"\"\"Shared configuration for the scripts.\"\"\"\n\timport enum\n\timport os\n\tfrom dataclasses import dataclass\n\tfrom functools import partial\n\tfrom typing import Optional, Union\n\timport torch as th\n\timport torch.distributed as dist\n\tfrom datasets import Dataset, DatasetDict, load_dataset\n\tfrom simple_parsing import field\n", "from torch.distributed.fsdp import (\n\t    CPUOffload,\n\t    FullyShardedDataParallel,\n\t    MixedPrecision,\n\t)\n\tfrom torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n\tfrom torch.distributed.optim import ZeroRedundancyOptimizer\n\tfrom torch.nn.parallel import DistributedDataParallel as DDP\n\tfrom transformers import (\n\t    AutoModelForCausalLM,\n", "    AutoTokenizer,\n\t    PreTrainedModel,\n\t    PreTrainedTokenizerBase,\n\t    get_linear_schedule_with_warmup,\n\t)\n\tfrom tuned_lens.data import (\n\t    chunk_and_tokenize,\n\t)\n\tfrom tuned_lens.model_surgery import get_transformer_layers\n\tfrom tuned_lens.nn.lenses import Lens\n", "from tuned_lens.utils import (\n\t    TreeType,\n\t    send_to_device,\n\t)\n\t@dataclass\n\tclass Data:\n\t    \"\"\"Configuration for the dataset.\"\"\"\n\t    name: list[str] = field(default_factory=lambda: [\"the_pile\", \"all\"], nargs=\"*\")\n\t    \"\"\"Name of dataset to use. Can either be a local .jsonl file or a name\n\t    suitable to be passed to the HuggingFace load_dataset function.\"\"\"\n", "    split: str = \"validation\"\n\t    \"\"\"Split of the dataset to use.\"\"\"\n\t    text_column: str = \"text\"\n\t    \"\"\"Column of the dataset containing text to run the model on.\"\"\"\n\t    revision: Optional[str] = None\n\t    \"\"\"The revision of the dataset to use\"\"\"\n\t    max_length: int = 2048\n\t    \"\"\"The maximum length of the input sequences.\"\"\"\n\t    def load(self, tokenizer: PreTrainedTokenizerBase) -> tuple[Dataset, float]:\n\t        \"\"\"Load the dataset, tokenize it and compute nats_to_bpb.\"\"\"\n", "        print(f\"Loading dataset '{' '.join(self.name)}'\")\n\t        if len(self.name) == 1 and self.name[0].endswith(\".jsonl\"):\n\t            dataset = Dataset.from_json(self.name[0])\n\t            assert isinstance(dataset, Dataset)\n\t        else:\n\t            dataset = load_dataset(*self.name, split=self.split, revision=self.revision)\n\t            if not isinstance(dataset, (Dataset, DatasetDict)):\n\t                raise ValueError(\n\t                    \"Only Dataset and DatasetDict instances are supported.\"\n\t                )\n", "        processed, nats_to_bpb = chunk_and_tokenize(\n\t            dataset, tokenizer, text_key=self.text_column, max_length=self.max_length\n\t        )\n\t        print(f\"Using nats per token to bits per byte ratio: {nats_to_bpb}\")\n\t        assert isinstance(processed, Dataset)\n\t        return processed, nats_to_bpb\n\t@dataclass\n\tclass Model:\n\t    \"\"\"Configuration for the model and tokenizer.\"\"\"\n\t    name: str\n", "    \"\"\"Name of model to use in the Huggingface Hub.\"\"\"\n\t    revision: str = \"main\"\n\t    \"\"\"Git revision to use for pretrained models.\"\"\"\n\t    slow_tokenizer: bool = field(action=\"store_true\")\n\t    \"\"\"Use a slow tokenizer.\"\"\"\n\t    tokenizer: Optional[str] = None\n\t    \"\"\"Name of pretrained tokenizer to use from the Huggingface Hub. If None, will use\n\t    AutoTokenizer.from_pretrained('<model name>').\"\"\"\n\t    tokenizer_type: Optional[str] = None\n\t    \"\"\"Name of tokenizer class to use. If None, will use AutoTokenizer.\"\"\"\n", "    def load_tokenizer(self, must_use_cache: bool = False) -> PreTrainedTokenizerBase:\n\t        \"\"\"Load the tokenizer from huggingface hub.\"\"\"\n\t        tokenizer = AutoTokenizer.from_pretrained(\n\t            self.tokenizer or self.name,\n\t            revision=self.revision,\n\t            use_fast=not self.slow_tokenizer,\n\t            tokenizer_type=self.tokenizer_type,\n\t            local_files_only=must_use_cache,\n\t        )\n\t        assert isinstance(tokenizer, PreTrainedTokenizerBase)\n", "        return tokenizer\n\t    def load(\n\t        self, must_use_cache: bool = False\n\t    ) -> tuple[PreTrainedModel, PreTrainedTokenizerBase]:\n\t        \"\"\"Load the model and tokenizer.\"\"\"\n\t        print(f\"Loading pretrained weights for '{self.name}'...\")\n\t        model = AutoModelForCausalLM.from_pretrained(  # type: ignore\n\t            self.name,\n\t            low_cpu_mem_usage=True,\n\t            revision=self.revision,\n", "            torch_dtype=\"auto\",\n\t            local_files_only=must_use_cache,\n\t        )\n\t        assert isinstance(model, PreTrainedModel)\n\t        model.eval()\n\t        model.requires_grad_(False)\n\t        return model, self.load_tokenizer(must_use_cache=must_use_cache)\n\tclass OptimizerOption(enum.Enum):\n\t    \"\"\"Options for the optimizer to use when training the model.\"\"\"\n\t    ADAM = \"adam\"\n", "    SGD = \"sgd\"\n\t@dataclass\n\tclass Optimizer:\n\t    \"\"\"Configuration for the optimizer.\"\"\"\n\t    weight_decay: float = 1e-3\n\t    \"\"\"Weight decay coefficient.\"\"\"\n\t    lr_scale: float = 1.0\n\t    \"\"\"The default LR (1e-3 for Adam, 1.0 for SGD) is scaled by this factor.\"\"\"\n\t    momentum: float = 0.9\n\t    \"\"\"Momentum coefficient for SGD, or beta1 for Adam.\"\"\"\n", "    zero: Optional[bool] = field(action=\"store_true\")\n\t    \"\"\"Use ZeroRedundancyOptimizer.\"\"\"\n\t    optimizer: OptimizerOption = OptimizerOption.SGD\n\t    \"\"\"The type of optimizer to use.\"\"\"\n\t    warmup_steps: Optional[int] = None\n\t    \"\"\"Number of warmup steps. Defaults to min(0.2 * num_steps, 1000) for Adam and 0\n\t    for SGD.\"\"\"\n\t    def create_scheduler(\n\t        self, opt: th.optim.Optimizer, num_steps: int\n\t    ) -> th.optim.lr_scheduler.LambdaLR:\n", "        \"\"\"Create the LR scheduler.\"\"\"\n\t        if self.warmup_steps is None:\n\t            # Adam generally performs poorly without an LR warmup\n\t            if self.optimizer == \"adam\":\n\t                self.warmup_steps = min(1000, num_steps // 5)\n\t                print(f\"Using {self.warmup_steps} LR warmup steps for Adam\")\n\t            else:\n\t                self.warmup_steps = 0\n\t        scheduler = get_linear_schedule_with_warmup(\n\t            opt, self.warmup_steps, num_steps - self.warmup_steps\n", "        )\n\t        return scheduler\n\t    def create_optim(self, params: list[th.nn.Parameter]) -> th.optim.Optimizer:\n\t        \"\"\"Create the optimizer.\"\"\"\n\t        # Don't train things that don't need gradients\n\t        β = self.momentum\n\t        if self.optimizer == OptimizerOption.SGD:\n\t            config = dict(\n\t                # PyTorch's implementation effectively scales the LR by 1 / (1 - β),\n\t                # so we undo that here. See https://www.youtube.com/watch?v=k8fTYJPd3_I\n", "                # for discussion. Once we do this, the optimal LR seems to be unity.\n\t                lr=self.lr_scale * (1 - β),\n\t                momentum=β,\n\t                # Empirically Nesterov momentum seems to improve convergence speed.\n\t                nesterov=True,\n\t                weight_decay=self.weight_decay,\n\t            )\n\t            opt_class = th.optim.SGD\n\t        elif self.optimizer == OptimizerOption.ADAM:\n\t            config = dict(\n", "                # Helps convergence slightly by ensuring that the LR actually decays\n\t                amsgrad=True,\n\t                betas=(β, 0.999),\n\t                lr=self.lr_scale * 1e-3,\n\t                weight_decay=self.weight_decay,\n\t            )\n\t            opt_class = th.optim.Adam\n\t        else:\n\t            raise ValueError(f\"Unknown optimizer '{self.optimizer}'\")\n\t        if self.zero:\n", "            opt = ZeroRedundancyOptimizer(params, optimizer_class=opt_class, **config)\n\t        else:\n\t            opt = opt_class(params, **config)  # type: ignore[call-arg]\n\t        return opt\n\t@dataclass\n\tclass Distributed:\n\t    \"\"\"Configuration and utilities for distributing the model.\"\"\"\n\t    fsdp: bool = field(action=\"store_true\")\n\t    \"\"\"Run the model with Fully Sharded Data Parallelism.\"\"\"\n\t    cpu_offload: bool = field(action=\"store_true\")\n", "    \"\"\"Use CPU offloading. Must be combined with fsdp\"\"\"\n\t    @property\n\t    def rank(self) -> int:\n\t        \"\"\"The rank of this process.\n\t        Note that in general this is not the same as the local rank.\n\t        However, for single-node training, the local rank is the same as the\n\t        global rank.\n\t        \"\"\"\n\t        return int(os.environ[\"RANK\"]) if dist.is_initialized() else 0\n\t    @property\n", "    def local_rank(self) -> int:\n\t        \"\"\"The local rank of this process.\"\"\"\n\t        return int(os.environ[\"LOCAL_RANK\"]) if dist.is_initialized() else 0\n\t    @property\n\t    def world_size(self) -> int:\n\t        \"\"\"Get the world size from torch.distributed.\"\"\"\n\t        return int(os.environ[\"WORLD_SIZE\"]) if dist.is_initialized() else 1\n\t    @property\n\t    def primary(self) -> bool:\n\t        \"\"\"Whether this is the rank 0 process.\"\"\"\n", "        return self.rank == 0\n\t    @property\n\t    def device(self) -> th.device:\n\t        \"\"\"The device associated with this process.\"\"\"\n\t        return th.device(\"cuda\") if th.cuda.is_available() else th.device(\"cpu\")\n\t    def shard_model(\n\t        self, model: PreTrainedModel\n\t    ) -> Union[FullyShardedDataParallel, PreTrainedModel]:\n\t        \"\"\"Shard the model using Fully Sharded Data Parallelism.\"\"\"\n\t        if self.fsdp:\n", "            _, layers = get_transformer_layers(model)\n\t            layer_cls = type(layers[0])\n\t            print(f\"Using '{layer_cls.__name__}' for transformer_auto_wrap_policy.\")\n\t            return FullyShardedDataParallel(\n\t                model,\n\t                auto_wrap_policy=partial(\n\t                    transformer_auto_wrap_policy, transformer_layer_cls={layer_cls}\n\t                ),\n\t                cpu_offload=CPUOffload(offload_params=self.cpu_offload),\n\t                device_id=self.rank,\n", "                # This turns out to be important for training speed\n\t                forward_prefetch=True,\n\t                mixed_precision=MixedPrecision(\n\t                    param_dtype=th.float16,\n\t                    reduce_dtype=th.float16,\n\t                    buffer_dtype=th.float16,\n\t                ),\n\t            )\n\t        elif self.cpu_offload:\n\t            raise ValueError(\"CPU offload requires FSDP.\")\n", "        else:\n\t            model.to(self.device)\n\t            return model\n\t    def distribute_lens(self, lens: Lens) -> Union[DDP, Lens]:\n\t        \"\"\"Distribute the lens using DistributedDataParallel and send lens to device.\"\"\"\n\t        if self.world_size > 1:\n\t            lens.to(self.device)\n\t            return DDP(lens, device_ids=[self.local_rank], find_unused_parameters=True)\n\t        else:\n\t            return lens.to(self.device)\n", "    def shard_dataset(self, dataset: Dataset) -> Dataset:\n\t        \"\"\"Shard the dataset based on local rank.\"\"\"\n\t        if dist.is_initialized():\n\t            dataset = dataset.shard(self.world_size, self.rank)\n\t        return dataset\n\t    def init(self) -> None:\n\t        \"\"\"Initialize distributed process group if started with elastic launch.\"\"\"\n\t        # Support both distributed and non-distributed training\n\t        local_rank = os.environ.get(\"LOCAL_RANK\")\n\t        if local_rank is not None:\n", "            dist.init_process_group(\"nccl\")\n\t            assert (\n\t                th.cuda.is_available()\n\t            ), \"CUDA must be available for distributed training\"\n\t            th.cuda.set_device(self.local_rank)\n\t    def barrier(self) -> None:\n\t        \"\"\"Barrier for all processes.\"\"\"\n\t        if dist.is_initialized():\n\t            dist.barrier()\n\t    def send_to_device(self, pytree: TreeType) -> TreeType:\n", "        \"\"\"Move pytree to the current device.\"\"\"\n\t        return send_to_device(pytree, self.device)\n"]}
{"filename": "tuned_lens/scripts/eval_loop.py", "chunked_list": ["\"\"\"Evaluation loop for the tuned lens model.\"\"\"\n\tfrom collections import defaultdict\n\tfrom dataclasses import dataclass\n\tfrom itertools import islice\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\timport torch as th\n\tfrom simple_parsing import field\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import tqdm\n", "from tuned_lens.nn.lenses import Lens, LogitLens, TunedLens\n\tfrom tuned_lens.scripts.ingredients import (\n\t    Data,\n\t    Distributed,\n\t    Model,\n\t)\n\tfrom tuned_lens.stats import LogitStats\n\tfrom tuned_lens.utils import (\n\t    maybe_all_reduce,\n\t    pytree_map,\n", "    pytree_stack,\n\t    shift_labels,\n\t    shift_preds,\n\t)\n\t@dataclass\n\tclass Eval:\n\t    \"\"\"Type hinting for CLI args.\"\"\"\n\t    data: Data\n\t    model: Model\n\t    dist: Distributed\n", "    lens: Optional[str] = field(alias=[\"-l\"], default=None)\n\t    \"\"\"Path to the tuned lens model.\"\"\"\n\t    seed: int = 42\n\t    \"\"\"Random seed used for data shuffling.\"\"\"\n\t    grad_alignment: Optional[bool] = field(action=\"store_true\")\n\t    \"\"\"Evaluate gradient alignment.\"\"\"\n\t    limit: Optional[int] = None\n\t    \"\"\"Number of batches to evaluate on. If None, will use the entire dataset.\"\"\"\n\t    output: Optional[Path] = field(alias=[\"-o\"], default=None)\n\t    \"\"\"JSON file to save the eval results to.\"\"\"\n", "    transfer: Optional[bool] = field(action=\"store_true\")\n\t    \"\"\"Evaluate how well probes transfer to other layers.\"\"\"\n\t    token_shift: Optional[int] = None\n\t    \"\"\"How to shift the labels wrt the input tokens (1 = next token, 0 = current token,\n\t    -1 = previous token, etc.)\"\"\"\n\t    per_gpu_batch_size: int = 1\n\t    \"\"\"Number of samples to try to fit on a GPU at once.\"\"\"\n\t    residual_stats: bool = field(action=\"store_true\")\n\t    def load_lens(self, model) -> Lens:\n\t        \"\"\"Load the tuned lens model.\"\"\"\n", "        if self.lens is None:\n\t            return LogitLens.from_model(model)\n\t        else:\n\t            return TunedLens.from_model_and_pretrained(model, self.lens)\n\t    @th.autocast(\"cuda\", enabled=th.cuda.is_available())\n\t    @th.no_grad()\n\t    def execute(self):\n\t        \"\"\"Trains a TunedLens model against a transformer on a dataset.\"\"\"\n\t        # Load model, tokenizer, data, and lens\n\t        self.dist.init()\n", "        model = tokenizer = data = lens = nats_to_bpb = model_name = None\n\t        if self.dist.primary:\n\t            # Let the primary processes populate the cache\n\t            model, tokenizer = self.model.load()\n\t            data, nats_to_bpb = self.data.load(tokenizer)\n\t            lens = self.load_lens(model)\n\t        self.dist.barrier()  # Wait for primary to finish filling the cache\n\t        if not self.dist.primary:\n\t            # Let the non-primary processes load from the cache\n\t            model, tokenizer = self.model.load(must_use_cache=True)\n", "            data, nats_to_bpb = self.data.load(tokenizer)\n\t            lens = self.load_lens(model)\n\t        assert model and tokenizer and data and lens and nats_to_bpb\n\t        model = self.dist.shard_model(model)\n\t        # Note since we are not training we can just move the lens to the device.\n\t        # No need to use DDP\n\t        lens = lens.to(self.dist.device)\n\t        data = self.dist.shard_dataset(data)\n\t        dl = DataLoader(\n\t            data.shuffle(seed=self.seed),  # type: ignore[arg-type],\n", "            batch_size=self.per_gpu_batch_size,\n\t        )\n\t        lens.eval()\n\t        if self.limit:\n\t            dl = islice(dl, self.limit)\n\t            total = self.limit\n\t        else:\n\t            total = len(dl)\n\t        *_, model_name = model.config.name_or_path.split(\"/\")\n\t        if self.lens:\n", "            root_dir = Path(self.lens, \"tuned-lens-eval\")\n\t        else:\n\t            root_dir = Path(model_name, \"logit_lens_eval\")\n\t        if self.output is not None:\n\t            root_dir = self.output\n\t        root_dir.mkdir(exist_ok=True, parents=True)\n\t        L = model.config.num_hidden_layers\n\t        batches = []\n\t        transfer_batches = []\n\t        final_logit_stats = LogitStats()\n", "        lens_statistics = [LogitStats() for _ in range(L)]\n\t        self.dist.barrier()\n\t        print(f\"All processes initialized. Running evaluation on {total} batches.\")\n\t        pbar = tqdm(dl, desc=\"Evaluating\", position=self.dist.rank, total=total)\n\t        for batch in pbar:\n\t            batch = self.dist.send_to_device(batch)\n\t            with th.no_grad():\n\t                output = model(**batch, output_hidden_states=True)\n\t            hidden_states = output.hidden_states[-1:]\n\t            shift = self.token_shift if self.token_shift is not None else 1\n", "            final_lps = output.logits.log_softmax(dim=-1)\n\t            final_probs = final_lps.exp()\n\t            labels = shift_labels(batch[\"input_ids\"], shift)\n\t            batch_output = defaultdict(dict)\n\t            transfer_ces = th.zeros(L, L, device=final_lps.device)\n\t            transfer_kls = th.zeros(L, L, device=final_lps.device)\n\t            # Compute tuned lens eval and statistics if applicable\n\t            for j, h in zip(range(L), hidden_states):\n\t                name = f\"layer_{j}\"\n\t                lens_lps = lens(h, idx=j).log_softmax(dim=-1)\n", "                lens_probs = lens_lps.exp()\n\t                batch_output[\"lens_ce\"][name] = th.nn.functional.cross_entropy(\n\t                    shift_preds(lens_lps, shift).flatten(0, 1),\n\t                    labels.flatten(),\n\t                    reduction=\"none\",\n\t                )\n\t                batch_output[\"lens_entropy\"][name] = th.sum(\n\t                    -lens_probs * lens_lps, dim=-1\n\t                )\n\t                batch_output[\"lens_kl\"][name] = th.sum(\n", "                    final_probs * (final_lps - lens_lps), dim=-1\n\t                )\n\t                lens_statistics[j].update(lens_lps, assume_normalized=True)\n\t                if self.transfer:\n\t                    # Each iteration of the loop processes a different *probe*\n\t                    # layer i for the test layer j.\n\t                    for i in range(L):\n\t                        transfer_lps = lens(h, idx=i).log_softmax(dim=-1)\n\t                        transfer_ces[i, j] = th.nn.functional.cross_entropy(\n\t                            shift_preds(transfer_lps, shift).flatten(0, 1),\n", "                            labels.flatten(),\n\t                        )\n\t                        transfer_kls[i, j] = th.sum(\n\t                            lens_probs * (lens_lps - transfer_lps), dim=-1\n\t                        ).mean()\n\t            final_logit_stats.update(final_lps, assume_normalized=True)\n\t            batch_output[\"baseline_ce\"][\"final\"] = th.nn.functional.cross_entropy(\n\t                shift_preds(final_lps, shift).flatten(0, 1),\n\t                labels.flatten(),\n\t                reduction=\"none\",\n", "            )\n\t            batch_output[\"baseline_entropy\"][\"final\"] = th.sum(\n\t                -final_probs * final_lps, dim=-1\n\t            )\n\t            batches.append(pytree_map(th.mean, batch_output))  # type: ignore[arg-type]\n\t            transfer_batches.append(\n\t                {\n\t                    \"transfer_ce\": transfer_ces,\n\t                    \"transfer_kl\": transfer_kls,\n\t                }\n", "            )\n\t            # Keep the processes synced\n\t            self.dist.barrier()\n\t        pbar.close()\n\t        agg = pytree_map(lambda x: nats_to_bpb * x.mean(), pytree_stack(batches))\n\t        agg = pytree_map(lambda x: maybe_all_reduce(x), agg)\n\t        if self.dist.primary:\n\t            th.save(agg, root_dir / \"aggregate_metrics.pt\")\n\t        if self.transfer:\n\t            agg_transfer = pytree_map(\n", "                lambda x: nats_to_bpb * x.mean(0), pytree_stack(transfer_batches)\n\t            )\n\t            agg_transfer = pytree_map(lambda x: maybe_all_reduce(x), agg_transfer)\n\t            if self.dist.primary:\n\t                th.save(agg_transfer, root_dir / \"aggregate_transfer_metrics.pt\")\n\t        for stats in lens_statistics:\n\t            stats.all_reduce_()\n\t        if self.dist.primary:\n\t            th.save(lens_statistics, root_dir / \"lens_logit_stats.pt\")\n"]}
{"filename": "tuned_lens/scripts/train_loop.py", "chunked_list": ["\"\"\"Training loop for training a TunedLens model against a transformer on a dataset.\"\"\"\n\timport dataclasses\n\timport enum\n\tfrom collections import defaultdict\n\tfrom dataclasses import dataclass\n\tfrom itertools import islice\n\tfrom pathlib import Path\n\tfrom typing import Optional\n\timport numpy as np\n\timport scipy as sp\n", "import torch as th\n\tfrom simple_parsing import field\n\tfrom torch.utils.data import DataLoader\n\tfrom tqdm.auto import tqdm\n\tfrom transformers import PreTrainedModel\n\tfrom tuned_lens import TunedLens\n\tfrom tuned_lens.scripts.ingredients import (\n\t    Data,\n\t    Distributed,\n\t    Model,\n", "    Optimizer,\n\t)\n\tfrom tuned_lens.utils import maybe_all_reduce, shift_labels, shift_preds\n\tclass LossChoice(enum.Enum):\n\t    \"\"\"Options of what loss to select when training the model.\"\"\"\n\t    CE = \"ce\"\n\t    KL = \"kl\"\n\t@dataclass\n\tclass Train:\n\t    \"\"\"Training loop for the tuned lens.\"\"\"\n", "    model: Model\n\t    \"\"\"Model configuration.\"\"\"\n\t    data: Data\n\t    \"\"\"Data configuration.\"\"\"\n\t    opt: Optimizer\n\t    \"\"\"Optimizer configuration.\"\"\"\n\t    dist: Distributed\n\t    \"\"\"Configuration for how to distribute the training.\"\"\"\n\t    seed: int = 42\n\t    \"\"\"Random seed for data shuffling.\"\"\"\n", "    lens_name_or_path: Optional[str] = field(alias=[\"-l\"], default=None)\n\t    constant: Optional[bool] = field(action=\"store_true\")\n\t    \"\"\"Train only the bias term.\"\"\"\n\t    num_steps: int = 250\n\t    \"\"\"Number of training steps.\"\"\"\n\t    output: Optional[Path] = field(alias=[\"-o\"], default=None)\n\t    \"\"\"File to save the lenses to. Defaults to the model name.\"\"\"\n\t    pre_ln: Optional[bool] = field(action=\"store_true\")\n\t    \"\"\"Apply layer norm before, and not after, each probe.\"\"\"\n\t    separate_unembeddings: Optional[bool] = field(action=\"store_true\")\n", "    \"\"\"Learn a separate unembedding for each layer.\"\"\"\n\t    tokens_per_step: int = 2**18\n\t    \"\"\"Number of tokens per step.\"\"\"\n\t    wandb: Optional[str] = None\n\t    \"\"\"Name of run in Weights & Biases.\"\"\"\n\t    wandb_upload_checkpoints: Optional[bool] = field(action=\"store_true\")\n\t    \"\"\"Upload checkpoints to Weights & Biases.\"\"\"\n\t    token_shift: Optional[int] = None\n\t    \"\"\"How to shift the labels wrt the input tokens (1 = next token, 0 = current token,\n\t    -1 = previous token, etc.)\"\"\"\n", "    per_gpu_batch_size: int = 1\n\t    \"\"\"Number of samples to try to fit on a GPU at once.\"\"\"\n\t    loss: LossChoice = LossChoice.KL\n\t    \"\"\"Loss function to use.\"\"\"\n\t    consistency_weight: float = 0.0\n\t    start_temp: float = 1.0\n\t    end_temp: float = 0\n\t    no_bias: bool = field(action=\"store_true\")\n\t    def get_lens(self, model: PreTrainedModel) -> TunedLens:\n\t        \"\"\"Load or create a TunedLens model.\"\"\"\n", "        if self.lens_name_or_path is None:\n\t            lens = TunedLens.from_model(model, bias=not self.no_bias)\n\t        else:\n\t            lens = TunedLens.from_model_and_pretrained(model, self.lens_name_or_path)\n\t        lens_size = sum(p.numel() * p.element_size() for p in lens.parameters())\n\t        print(f\"Tuned lens parameter memory usage: {lens_size / 2 ** 20:.2f} MB\")\n\t        if self.constant:\n\t            for probe in lens:\n\t                probe.weight.requires_grad_(False)\n\t        return lens\n", "    def _init_logging(self, model_name: str, lens: TunedLens):\n\t        \"\"\"Initialize logging to weights and biases.\"\"\"\n\t        if not self.dist.primary or not self.wandb:\n\t            return\n\t        import wandb\n\t        wandb.init(\n\t            config=dataclasses.asdict(self),\n\t            group=model_name,\n\t            name=self.wandb,\n\t        )\n", "        wandb.watch(lens)\n\t    def _log(\n\t        self,\n\t        opt: th.optim.Optimizer,\n\t        step: int,\n\t        losses: dict[str, list[float]],\n\t        tuned_lens: TunedLens,\n\t        temperature: float,\n\t    ):\n\t        \"\"\"Log statistics about the training process to weights and biases.\"\"\"\n", "        if not self.dist.primary or not self.wandb:\n\t            return\n\t        import wandb\n\t        log_dict = {}\n\t        log_dict.update({f\"loss/{k}\": th.tensor(v).mean() for k, v in losses.items()})\n\t        log_dict[\"temperature\"] = temperature\n\t        # Log statistics about optimizer & probes\n\t        for i, probe in enumerate(tuned_lens):\n\t            name = \"input\" if i == 0 else f\"{i - 1}.ffn\"\n\t            states = [opt.state[p] for p in probe.parameters()]\n", "            # Approximate the true grad norm using the optimizer's moving\n\t            # avg\n\t            corr = 1 - self.opt.momentum**step\n\t            if self.opt.optimizer == \"sgd\" and not self.opt.zero:\n\t                log_dict[\"grad_norm/\" + name] = th.cat(\n\t                    [\n\t                        # Undo PyTorch's scaling of the gradient by\n\t                        # 1 / (1 - β)\n\t                        (1 - self.opt.momentum) * s[\"momentum_buffer\"].flatten() / corr\n\t                        for s in states\n", "                    ]\n\t                ).norm()\n\t            elif self.opt.optimizer == \"adam\" and not self.opt.zero:\n\t                log_dict[\"grad_norm/\" + name] = th.cat(\n\t                    [s[\"exp_avg\"].flatten() / corr for s in states]\n\t                ).norm()\n\t            if isinstance(probe, th.nn.Linear):\n\t                if not self.no_bias:\n\t                    log_dict[\"bias_norm/\" + name] = probe.bias.data.norm()\n\t                log_dict[\"weight_norm/\" + name] = probe.weight.data.norm()\n", "        for key, weight in tuned_lens.named_parameters():\n\t            log_dict[f\"ln_bias_{key}\"] = weight.data.norm()\n\t        wandb.log(log_dict)\n\t    def _save_model(self, tuned_lens: TunedLens, model_name: str):\n\t        \"\"\"Save the model to disk and try to upload to wandb if enabled.\"\"\"\n\t        if not self.dist.primary:\n\t            return\n\t        assert model_name is not None\n\t        output = model_name if self.output is None else self.output\n\t        print(f\"Saving lens to {output}\")\n", "        tuned_lens.save(output)\n\t        if self.wandb and self.wandb_upload_checkpoints:\n\t            import wandb\n\t            artifact = wandb.Artifact(\n\t                name=\"final_checkpoint\",\n\t                type=\"checkpoint\",\n\t                description=\"A trained lens.\",\n\t            )\n\t            artifact.add_dir(output)\n\t            wandb.log_artifact(artifact)\n", "    def calculate_gradient_accumulation_steps(self, tokens_per_sample: int) -> int:\n\t        \"\"\"Calculate the number of batches of data to process before taking a step.\"\"\"\n\t        # chunk_and_tokenize ensures the samples are all the same length\n\t        samples_per_step, rem = divmod(self.tokens_per_step, tokens_per_sample)\n\t        if rem:\n\t            raise ValueError(\n\t                f\"Number of tokens per step ({self.tokens_per_step:_}) must be \"\n\t                f\"divisible by the number of tokens per sample ({tokens_per_sample}).\"\n\t            )\n\t        global_batch_size = self.per_gpu_batch_size * self.dist.world_size\n", "        grad_acc_steps, rem = divmod(samples_per_step, global_batch_size)\n\t        if rem:\n\t            # If the number of samples per step isn't divisible by the global batch\n\t            # size, use ceil division and let the user know about it.\n\t            grad_acc_steps += 1\n\t            adjusted_count = grad_acc_steps * global_batch_size * tokens_per_sample\n\t            print(\n\t                f\"Note: Increasing grad acc steps from {grad_acc_steps - 1} to \"\n\t                f\"{grad_acc_steps} to maintain load balance across \"\n\t                f\"{self.dist.world_size} GPUs.\"\n", "            )\n\t            print(\n\t                f\"Using {adjusted_count:_} tokens per training step \"\n\t                f\"({self.tokens_per_step:_} requested).\"\n\t            )\n\t        else:\n\t            print(f\"Gradient accumulation steps: {grad_acc_steps}\")\n\t            print(f\"Using {self.tokens_per_step:_} tokens per training step.\")\n\t        return grad_acc_steps\n\t    def execute(self):\n", "        \"\"\"Trains a TunedLens model against a transformer on a dataset.\"\"\"\n\t        # Load model, tokenizer, data, and lens\n\t        self.dist.init()\n\t        model = tokenizer = data = lens = nats_to_bpb = model_name = None\n\t        if self.dist.primary:\n\t            # Let the primary processes populate the cache\n\t            model, tokenizer = self.model.load()\n\t            data, nats_to_bpb = self.data.load(tokenizer)\n\t            lens = self.get_lens(model)\n\t            *_, model_name = model.config.name_or_path.split(\"/\")\n", "            self._init_logging(model_name, lens)\n\t        self.dist.barrier()  # Wait for primary to finish filling the cache\n\t        if not self.dist.primary:\n\t            # Let the non-primary processes load from the cache\n\t            model, tokenizer = self.model.load(must_use_cache=True)\n\t            data, nats_to_bpb = self.data.load(tokenizer)\n\t            lens = self.get_lens(model)\n\t        assert model and tokenizer and data and lens and nats_to_bpb\n\t        # Shard the model using fully shared data parallel\n\t        model = self.dist.shard_model(model)\n", "        # Distribute the lens across the GPUS using distributed data parallel\n\t        ddp_lens = self.dist.distribute_lens(lens)\n\t        # Shard the dataset for use with distributed data parallel\n\t        data = self.dist.shard_dataset(data)\n\t        dl = DataLoader(\n\t            data.shuffle(seed=self.seed),  # type: ignore[arg-type]\n\t            batch_size=self.per_gpu_batch_size,\n\t        )\n\t        # Don't train the unembedding matrix or final layer norm\n\t        params = [p for p in ddp_lens.parameters() if p.requires_grad]\n", "        # Create the optimizer and scheduler\n\t        opt = self.opt.create_optim(params)\n\t        scheduler = self.opt.create_scheduler(opt, self.num_steps)\n\t        tokens_per_sample = len(data[0][\"input_ids\"])\n\t        grad_acc_steps = self.calculate_gradient_accumulation_steps(tokens_per_sample)\n\t        if self.dist.cpu_offload and grad_acc_steps > 1:\n\t            raise ValueError(\"CPU offloading cannot be used with gradient accumulation\")\n\t        losses = defaultdict(list)\n\t        total_batches = self.num_steps * grad_acc_steps\n\t        # Wait for all processes to finish setup\n", "        self.dist.barrier()\n\t        print(\"All processes have completed setup. Starting training.\")\n\t        # Main training loop\n\t        pbar = tqdm(islice(dl, total_batches), desc=\"Training\", total=total_batches)\n\t        for batch_idx, batch in enumerate(pbar, start=1):\n\t            assert isinstance(batch, dict)\n\t            batch = self.dist.send_to_device(batch)\n\t            with th.no_grad():\n\t                output = model(**batch, output_hidden_states=True)\n\t            final_logits = output.logits\n", "            hidden_stats = output.hidden_states[:-1]\n\t            shift = self.token_shift\n\t            if self.loss == LossChoice.CE:\n\t                labels = batch[\"input_ids\"]\n\t                # Predict the *next* token by default w/ cross entropy\n\t                if shift is None:\n\t                    shift = 1\n\t            elif self.loss == LossChoice.KL:\n\t                labels = final_logits.log_softmax(dim=-1)\n\t                # Match the *current* token distribution by default\n", "                if shift is None:\n\t                    shift = 0\n\t            else:\n\t                raise NotImplementedError(f\"Unknown loss {self.loss}\")\n\t            labels = shift_labels(labels, shift)\n\t            # cosine schedule\n\t            u = batch_idx / (total_batches - 1)\n\t            c = 1 - ((np.cos(np.pi * u) + 1) / 2) ** 2\n\t            reconstruct_temp = self.start_temp + (self.end_temp - self.start_temp) * c\n\t            # We do this sequentially to save VRAM\n", "            for i, h in enumerate(hidden_stats):\n\t                # bfloat16 has larger dynamic range than float16 and seems to be better\n\t                # for computing log softmax & KL loss\n\t                with th.autocast(self.dist.device.type, dtype=th.bfloat16):\n\t                    logits_pre_shift = ddp_lens(h, idx=i)\n\t                    logits = shift_preds(logits_pre_shift, shift)\n\t                    if self.loss == LossChoice.CE:\n\t                        loss = th.nn.functional.cross_entropy(\n\t                            logits.flatten(0, -2), labels.flatten()\n\t                        )\n", "                    elif self.loss == LossChoice.KL:\n\t                        loss = th.sum(\n\t                            labels.exp() * (labels - logits.log_softmax(-1)), dim=-1\n\t                        ).mean()\n\t                    else:\n\t                        raise NotImplementedError\n\t                    loss *= nats_to_bpb\n\t                    logging_loss = loss.detach()\n\t                    logging_loss = maybe_all_reduce(logging_loss).item()\n\t                    if self.consistency_weight > 0:\n", "                        if reconstruct_temp < 1e-6:\n\t                            probs = th.nn.functional.one_hot(\n\t                                logits.argmax(dim=-1), logits.shape[-1]\n\t                            )\n\t                        else:\n\t                            probs = th.nn.functional.softmax(\n\t                                logits_pre_shift / reconstruct_temp, dim=-1\n\t                            )\n\t                        reconstruction = ddp_lens.reconstruct(probs, idx=i)\n\t                        # use cosine similiarity as reconstruction loss\n", "                        reconstruction_loss = (\n\t                            1\n\t                            - th.nn.functional.cosine_similarity(\n\t                                h, reconstruction, dim=-1\n\t                            ).mean()\n\t                        )\n\t                        logging_reconstruction_loss = reconstruction_loss.detach()\n\t                        logging_reconstruction_loss = maybe_all_reduce(\n\t                            logging_reconstruction_loss\n\t                        ).item()\n", "                        loss += self.consistency_weight * reconstruction_loss\n\t                        # sparse if p_max > 0.9\n\t                        p_max = probs.max(dim=-1)[0]\n\t                        count_sparse = (p_max > 0.9).float().mean().item()\n\t                    if self.dist.primary:\n\t                        losses[f\"loss_{i}\"].append(loss.detach().item())\n\t                        if self.consistency_weight > 0:\n\t                            losses[f\"translator_{i}\"].append(logging_loss)\n\t                            losses[f\"reconstruction_{i}\"].append(\n\t                                logging_reconstruction_loss\n", "                            )\n\t                            losses[f\"sparsity_{i}\"].append(count_sparse)\n\t                    scaled_loss = loss / grad_acc_steps\n\t                scaled_loss.backward()\n\t            step, rem = divmod(batch_idx, grad_acc_steps)\n\t            if rem == 0:\n\t                pbar.set_postfix(\n\t                    {\n\t                        \"reconstruction_5\": th.tensor(losses[\"reconstruction_5\"])\n\t                        .mean()\n", "                        .item()\n\t                    }\n\t                )\n\t                th.nn.utils.clip_grad_norm_(lens.parameters(), 1.0)\n\t                opt.step()\n\t                opt.zero_grad(set_to_none=False)\n\t                scheduler.step()\n\t                self._log(opt, step, losses, lens, reconstruct_temp)\n\t                losses.clear()\n\t        if self.dist.primary:\n", "            assert model_name is not None\n\t            output = model_name if self.output is None else self.output\n\t            print(f\"Saving lens to {output}\")\n\t            lens.save(output)\n"]}
{"filename": "tuned_lens/scripts/__init__.py", "chunked_list": ["\"\"\"Implementations of subcommands.\"\"\"\n"]}
{"filename": "tuned_lens/scripts/train_injected.py", "chunked_list": ["import random\n\tfrom transformers import AutoTokenizer, AutoModelForCausalLM, GPTNeoXForCausalLM\n\timport os\n\timport json\n\timport torch\n\tfrom tqdm import trange\n\timport wandb\n\tfrom fire import Fire\n\tdef run(\n\t    batch_size: int = 32,\n", "    n_epochs: int = 3,\n\t    completions_path: str = \"data.jsonl\",\n\t    model_name: str = \"EleutherAI/pythia-410m-deduped\",\n\t    layers: list[int] = [0, 6, 10, 16],\n\t    lr: float = 5e-4,\n\t    max_length: int = 512,\n\t    generate_every: int = 20,\n\t):\n\t    for k, v in locals().items():\n\t        print(f\"{k}: {v}\")\n", "    wandb.init(project=\"sft-lens\", config=locals())\n\t    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\t    ref_model = GPTNeoXForCausalLM.from_pretrained(model_name).to(device)\n\t    decoder_model = GPTNeoXForCausalLM.from_pretrained(model_name).to(device)\n\t    tokenizer.pad_token_id = tokenizer.eos_token_id\n\t    with open(completions_path, \"r\") as f:\n\t        completions = [json.loads(l) for l in f]\n\t    all_texts: list[tuple[str, str, str]] = []\n\t    for (text, text_end), answers in completions:\n", "        for answer in answers:\n\t            all_texts.append((text, text_end, answer))\n\t    ref_model_dim = ref_model.config.hidden_size\n\t    decoder_model_dim = decoder_model.config.hidden_size\n\t    adapters = [torch.nn.Linear(ref_model_dim, decoder_model_dim).to(device) for _ in layers]\n\t    for adapter in adapters:\n\t        # identity initialization\n\t        adapter.weight.data.copy_(torch.eye(ref_model_dim, device=device))\n\t    adapter_parameters = sum([list(adapter.parameters()) for adapter in adapters], [])\n\t    optimizer = torch.optim.Adam(list(decoder_model.parameters()) + adapter_parameters, lr=lr)\n", "    # linear lr warmup\n\t    warmup_steps = 0.05 * len(all_texts) * n_epochs / batch_size\n\t    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(1, step / warmup_steps))\n\t    eos = tokenizer.eos_token\n\t    # dry save to check things work\n\t    e = 0\n\t    os.makedirs(f\"checkpoints/{wandb.run.id}/epoch_{e}\", exist_ok=True)\n\t    decoder_model.save_pretrained(f\"checkpoints/{wandb.run.id}/epoch_{e}\")\n\t    torch.save(\n\t        {layers[i]: adapter for i, adapter in enumerate(adapters)},\n", "        f\"checkpoints/{wandb.run.id}/epoch_{e}/adapters\",\n\t    )\n\t    for e in range(n_epochs):\n\t        pbar = trange(0, len(all_texts), batch_size)\n\t        random.shuffle(all_texts)\n\t        for i in pbar:\n\t            batch = all_texts[i : i + batch_size]\n\t            with torch.no_grad():\n\t                inputs = tokenizer(\n\t                    [eos + t[0] for t in batch],\n", "                    return_tensors=\"pt\",\n\t                    padding=True,\n\t                    truncation=True,\n\t                    max_length=max_length,\n\t                )\n\t                # get activations at every layer\n\t                outputs = ref_model(**inputs.to(device), output_hidden_states=True)\n\t                # get activations at last position of text\n\t                last_tok_pos = inputs[\"attention_mask\"].sum(dim=1) - 1\n\t                last_pos_activations = [\n", "                    outputs[\"hidden_states\"][l][torch.arange(len(outputs[\"hidden_states\"][l])), last_tok_pos, :]\n\t                    for l in layers\n\t                ]\n\t            losses = []\n\t            generations = []\n\t            for layer_i, (layer, state) in enumerate(zip(layers, last_pos_activations)):\n\t                expected_strings = [f\"{eos}Layer {layer}\\n{t[2]}{eos}\" for t in batch]\n\t                expected_tokens = tokenizer(\n\t                    expected_strings, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length\n\t                ).to(device)\n", "                def state_insertion_hook(module, inp, out):\n\t                    if out.shape[1] > 1:\n\t                        out[:, 0, :] = adapters[layer_i](state[: len(out), :])\n\t                        return out\n\t                    else:\n\t                        # in generation, we don't inject\n\t                        return out\n\t                handle = decoder_model.gpt_neox.embed_in.register_forward_hook(state_insertion_hook)\n\t                try:\n\t                    if (i // batch_size) % generate_every == 0:\n", "                        # generate sequence with injected state of batch elt\n\t                        with torch.no_grad():\n\t                            inputs = tokenizer(f\"{eos}Layer {layer}\\n\", return_tensors=\"pt\")\n\t                            top_generation_tokens = decoder_model.generate(\n\t                                **inputs.to(device),\n\t                                do_sample=False,\n\t                                max_new_tokens=200,\n\t                                pad_token_id=tokenizer.eos_token_id,\n\t                            )\n\t                            top_generation_string = tokenizer.decode(top_generation_tokens[0])\n", "                            generations.append(top_generation_string)\n\t                    # compute logits\n\t                    logits: torch.Tensor\n\t                    logits = decoder_model(**expected_tokens, labels=expected_tokens.input_ids).logits[:, :-1, :]\n\t                    labels = expected_tokens.input_ids[:, 1:]\n\t                    loss_per_pos = torch.nn.functional.cross_entropy(\n\t                        logits.reshape(-1, logits.shape[-1]), labels.reshape(-1), reduction=\"none\"\n\t                    ).reshape(labels.shape)\n\t                    masked_loss = loss_per_pos * expected_tokens.attention_mask[:, :-1]\n\t                    loss = masked_loss.sum() / expected_tokens.attention_mask[:, :-1].sum()\n", "                    losses.append(loss.item())\n\t                    loss /= len(last_pos_activations)\n\t                    loss.backward()\n\t                finally:\n\t                    handle.remove()\n\t            # clip grad\n\t            torch.nn.utils.clip_grad_norm_(decoder_model.parameters(), 1.0)\n\t            optimizer.step()\n\t            optimizer.zero_grad()\n\t            scheduler.step()\n", "            pbar.set_description(f\"loss: {sum(losses)/len(losses):.4f}\")\n\t            to_log = {\n\t                \"loss\": sum(losses) / len(losses),\n\t                **{f\"losses/layer_{layers[i]}\": loss for i, loss in enumerate(losses)},\n\t                \"lr\": scheduler.get_last_lr()[0],\n\t            }\n\t            if generations:\n\t                generation_table = wandb.Table(columns=[\"layer\", \"generation\", \"text\", \"text end\", \"label\"])\n\t                for layer_i, s in enumerate(generations):\n\t                    generation_table.add_data(layers[layer_i], s, *batch[0])\n", "                to_log[\"generations\"] = generation_table\n\t            wandb.log(to_log)\n\t        os.makedirs(f\"checkpoints/{wandb.run.id}/epoch_{e}\", exist_ok=True)\n\t        decoder_model.save_pretrained(f\"checkpoints/{wandb.run.id}/epoch_{e}\")\n\t        torch.save(\n\t            {layers[i]: adapter for i, adapter in enumerate(adapters)},\n\t            f\"checkpoints/{wandb.run.id}/epoch_{e}/adapters\",\n\t        )\n\t    wandb.finish()\n\tif __name__ == \"__main__\":\n", "    Fire(run)\n"]}
{"filename": "tuned_lens/scripts/generation_utils.py", "chunked_list": ["import re\n\timport threading\n\timport time\n\tfrom time import sleep\n\tfrom typing import Optional, TypeVar\n\timport attrs\n\timport openai\n\tfrom tqdm import tqdm\n\timport tiktoken\n\ttokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n", "@attrs.frozen\n\tclass GenerationParams:\n\t    temperature: float\n\t    model: str\n\tTOKENS_PER_MODEL = {\n\t    \"gpt-3.5-turbo\": 4096,\n\t    \"gpt-4\": 8192,\n\t}\n\tMAX_RPM_PER_MODEL = {\n\t    \"gpt-3.5-turbo\": 3500,\n", "    \"gpt-4\": 200,\n\t}\n\tMAX_TPM_PER_MODEL = {\n\t    \"gpt-3.5-turbo\": 90_000,\n\t    \"gpt-4\": 40_000,\n\t}\n\tMAX_TOKENS_MARGIN = 20\n\tdef strip_triple_backquotes(s) -> Optional[str]:\n\t    \"\"\"Remove code inside fences.\n\t    Support both raw fences and python fences (with triple backquotes).\n", "    Return None if code fence is not closed.\n\t    \"\"\"\n\t    closed_fence_result = re.search(r\"(```(?:python\\n)?)([\\s\\S]*?)(```)\", s)\n\t    if closed_fence_result is not None:\n\t        return closed_fence_result.group(2)\n\t    else:\n\t        unclosed_fence_result = re.search(r\"```(.*?)\", s)\n\t        if unclosed_fence_result is not None:\n\t            return None\n\t        else:\n", "            return s\n\tdef nb_tokens_in_prompt(prompt_messages: list[dict[str, str]]) -> int:\n\t    return sum(len(tokenizer.encode(s[\"content\"])) for s in prompt_messages) + MAX_TOKENS_MARGIN\n\tdef generate(\n\t    prompt_messages: list[dict[str, str]],\n\t    nb_solutions: int = 5,\n\t    max_new_tokens: Optional[int] = None,\n\t    temperature: float = 0.7,\n\t    model: str = \"gpt-3.5-turbo\",\n\t    max_attemps: int = 10,\n", "    only_accept_finished: bool = True,\n\t    sleep_time: float = 0,\n\t    max_sleep_time_when_fail: float = 30,\n\t):\n\t    if max_new_tokens is None:\n\t        assert model in TOKENS_PER_MODEL\n\t    # chat models\n\t    if model == \"gpt-3.5-turbo\" or model == \"gpt-4\":\n\t        for attempt in range(max_attemps):\n\t            try:\n", "                st = time.time()\n\t                max_tokens_possible = TOKENS_PER_MODEL[model] - nb_tokens_in_prompt(prompt_messages)\n\t                if max_new_tokens is None:\n\t                    max_new_tokens = max_tokens_possible\n\t                else:\n\t                    max_new_tokens = min(max_new_tokens, max_tokens_possible)\n\t                completion = openai.ChatCompletion.create(\n\t                    model=model,\n\t                    messages=prompt_messages,\n\t                    temperature=temperature,\n", "                    max_tokens=max_new_tokens,\n\t                    n=nb_solutions,\n\t                )\n\t                generations = [\n\t                    choice[\"message\"][\"content\"]\n\t                    for choice in completion[\"choices\"]\n\t                    if (choice[\"finish_reason\"] == \"stop\") or (not only_accept_finished)\n\t                ]\n\t                request_time = time.time() - st\n\t                if request_time < sleep_time:\n", "                    sleep(sleep_time - request_time)\n\t                break\n\t            except Exception as e:\n\t                generations = []\n\t                if attempt == max_attemps - 1:\n\t                    print(f\"Failed {max_attemps} times, last error: {e}\")\n\t                sleep(max_sleep_time_when_fail * 2 ** (-(max_attemps - attempt - 1)))\n\t    else:\n\t        raise ValueError(f\"Model {model} not supported\")\n\t    return generations\n", "T = TypeVar(\"T\")\n\tdef threaded_generations(\n\t    prompts_messages: list[tuple[T, list[dict[str, str]]]],\n\t    nb_solutions: int = 1,\n\t    max_new_tokens: Optional[int] = None,\n\t    temperature: float = 0.7,\n\t    model: str = \"gpt-3.5-turbo\",\n\t    max_attemps: int = 10,\n\t    only_accept_finished: bool = True,\n\t    n_threads: int = 5,\n", ") -> list[tuple[T, list[str]]]:\n\t    threads_results: list[list[tuple[T, list[str]]]] = [[] for _ in range(n_threads)]\n\t    pbar = tqdm(total=len(prompts_messages))\n\t    lock = threading.Lock()\n\t    active_threads = n_threads\n\t    def thread_task(thread_id: int):\n\t        nonlocal active_threads\n\t        start = thread_id * len(prompts_messages) // n_threads\n\t        end = (thread_id + 1) * len(prompts_messages) // n_threads\n\t        prompts_to_process = prompts_messages[start:end]\n", "        thread_results = []\n\t        for obj, prompt in prompts_to_process:\n\t            effective_max_rpm = min(MAX_RPM_PER_MODEL[model], MAX_TPM_PER_MODEL[model] / nb_tokens_in_prompt(prompt))\n\t            sleep_time = (60 / effective_max_rpm) * active_threads\n\t            generations = generate(\n\t                prompt, nb_solutions, max_new_tokens, temperature, model, max_attemps, only_accept_finished, sleep_time\n\t            )\n\t            thread_results.append((obj, generations))\n\t            with lock:\n\t                pbar.update(1)\n", "        threads_results[thread_id] = thread_results\n\t        with lock:\n\t            active_threads -= 1\n\t    threads = []\n\t    for thread_id in range(n_threads):\n\t        thread = threading.Thread(target=thread_task, args=(thread_id,))\n\t        thread.start()\n\t        threads.append(thread)\n\t    for thread in threads:\n\t        thread.join()\n", "    pbar.close()\n\t    return sum(threads_results, [])\n"]}
{"filename": "tuned_lens/nn/__init__.py", "chunked_list": ["\"\"\"A set of PyTorch modules for transforming the residual streams of models.\"\"\"\n\tfrom .lenses import Lens, LogitLens, TunedLens, TunedLensConfig\n\tfrom .unembed import (\n\t    InversionOutput,\n\t    Unembed,\n\t)\n"]}
{"filename": "tuned_lens/nn/lenses.py", "chunked_list": ["\"\"\"Provides lenses for decoding hidden states into logits.\"\"\"\n\timport abc\n\timport inspect\n\timport json\n\tfrom copy import deepcopy\n\tfrom dataclasses import asdict, dataclass\n\tfrom logging import warning\n\tfrom pathlib import Path\n\tfrom typing import Dict, Generator, Optional, Union\n\timport torch as th\n", "from transformers import PreTrainedModel\n\tfrom tuned_lens import load_artifacts\n\tfrom tuned_lens.nn.unembed import Unembed\n\tclass Lens(abc.ABC, th.nn.Module):\n\t    \"\"\"Abstract base class for all Lens.\"\"\"\n\t    unembed: Unembed\n\t    def __init__(self, unembed: Unembed):\n\t        \"\"\"Create a Lens.\n\t        Args:\n\t            unembed: The unembed operation to use.\n", "        \"\"\"\n\t        super().__init__()\n\t        self.unembed = unembed\n\t    @abc.abstractmethod\n\t    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:\n\t        \"\"\"Convert a hidden state to the final hidden just before the unembeding.\n\t        Args:\n\t            h: The hidden state to convert.\n\t            idx: The layer of the transformer these hidden states come from.\n\t        \"\"\"\n", "        ...\n\t    @abc.abstractmethod\n\t    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:\n\t        \"\"\"Decode hidden states into logits.\"\"\"\n\t        ...\n\tclass LogitLens(Lens):\n\t    \"\"\"Unembeds the residual stream into logits.\"\"\"\n\t    unembed: Unembed\n\t    def __init__(\n\t        self,\n", "        unembed: Unembed,\n\t    ):\n\t        \"\"\"Create a Logit Lens.\n\t        Args:\n\t            unembed: The unembed operation to use.\n\t        \"\"\"\n\t        super().__init__(unembed)\n\t    @classmethod\n\t    def from_model(\n\t        cls,\n", "        model: PreTrainedModel,\n\t    ) -> \"LogitLens\":\n\t        \"\"\"Create a LogitLens from a pretrained model.\n\t        Args:\n\t            model: A pretrained model from the transformers library you wish to inspect.\n\t        \"\"\"\n\t        unembed = Unembed(model)\n\t        return cls(unembed)\n\t    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:\n\t        \"\"\"For the LogitLens, this is the identity function.\"\"\"\n", "        del idx\n\t        return h\n\t    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:\n\t        \"\"\"Decode a hidden state into logits.\n\t        Args:\n\t            h: The hidden state to decode.\n\t            idx: the layer of the transformer these hidden states come from.\n\t        \"\"\"\n\t        del idx\n\t        return self.unembed.forward(h)\n", "@dataclass\n\tclass TunedLensConfig:\n\t    \"\"\"A configuration for a TunedLens.\"\"\"\n\t    # The name of the base model this lens was tuned for.\n\t    base_model_name_or_path: str\n\t    # The hidden size of the base model.\n\t    d_model: int\n\t    # The number of layers in the base model.\n\t    num_hidden_layers: int\n\t    # whether to use a bias in the linear translators.\n", "    bias: bool = True\n\t    # The revision of the base model this lens was tuned for.\n\t    base_model_revision: Optional[str] = None\n\t    # The hash of the base's unembed model this lens was tuned for.\n\t    unemebd_hash: Optional[str] = None\n\t    # The name of the lens type.\n\t    lens_type: str = \"linear_tuned_lens\"\n\t    def to_dict(self):\n\t        \"\"\"Convert this config to a dictionary.\"\"\"\n\t        return asdict(self)\n", "    @classmethod\n\t    def from_dict(cls, config_dict: Dict):\n\t        \"\"\"Create a config from a dictionary.\"\"\"\n\t        config_dict = deepcopy(config_dict)\n\t        # Drop unrecognized config keys\n\t        unrecognized = set(config_dict) - set(inspect.getfullargspec(cls).args)\n\t        for key in unrecognized:\n\t            warning(f\"Ignoring config key '{key}'\")\n\t            del config_dict[key]\n\t        return cls(**config_dict)\n", "class AddBias(th.nn.Module):\n\t    \"\"\"An elementwise affine transformation.\"\"\"\n\t    def __init__(self, d_model: int):\n\t        \"\"\"Create an elementwise affine transformation.\n\t        Args:\n\t            d_model: The dimensionality of the input.\n\t            bias: Whether to use a bias.\n\t        \"\"\"\n\t        super().__init__()\n\t        self.bias = th.nn.Parameter(th.zeros(d_model))\n", "    def forward(self, x: th.Tensor) -> th.Tensor:\n\t        \"\"\"Apply the affine transformation to the input.\n\t        Args:\n\t            x: The input to transform.\n\t        \"\"\"\n\t        return x + self.bias\n\tclass TunedLens(Lens):\n\t    \"\"\"A tuned lens for decoding hidden states into logits.\"\"\"\n\t    config: TunedLensConfig\n\t    unembed: Unembed\n", "    layer_translators: th.nn.ModuleList\n\t    probs_ln: th.nn.LayerNorm\n\t    def __init__(\n\t        self,\n\t        unembed: Unembed,\n\t        config: TunedLensConfig,\n\t    ):\n\t        \"\"\"Create a TunedLens.\n\t        Args:\n\t            unembed: The unembed operation to use.\n", "            config: The configuration for this lens.\n\t        \"\"\"\n\t        super().__init__(unembed)\n\t        self.config = config\n\t        unembed_hash = unembed.unembedding_hash()\n\t        config.unemebd_hash = unembed_hash\n\t        translator = th.nn.Linear(config.d_model, config.d_model, bias=config.bias)\n\t        # translator.weight.data.zero_()\n\t        # identity init\n\t        translator.weight.data.copy_(th.eye(config.d_model))\n", "        if config.bias:\n\t            translator.bias.data.zero_()\n\t        # Don't include the final layer since it does not need a translator\n\t        self.layer_translators = th.nn.ModuleList(\n\t            [\n\t                th.nn.utils.parametrizations.orthogonal(deepcopy(translator))\n\t                for _ in range(self.config.num_hidden_layers)\n\t            ]\n\t        )\n\t        self.vocab_size = unembed.unembedding.weight.shape[0]\n", "        self.probs_ln = AddBias(self.vocab_size)\n\t    def __getitem__(self, item: int) -> th.nn.Module:\n\t        \"\"\"Get the probe module at the given index.\"\"\"\n\t        return self.layer_translators[item]\n\t    def __iter__(self) -> Generator[th.nn.Module, None, None]:\n\t        \"\"\"Get iterator over the translators within the lens.\"\"\"\n\t        yield from self.layer_translators\n\t    @classmethod\n\t    def from_model(\n\t        cls,\n", "        model: PreTrainedModel,\n\t        model_revision: Optional[str] = None,\n\t        bias: bool = True,\n\t    ) -> \"TunedLens\":\n\t        \"\"\"Create a lens from a pretrained model.\n\t        Args:\n\t            model: The model to create the lens from.\n\t            model_revision: The git revision of the model to used.\n\t            bias: Whether to use a bias in the linear translators.\n\t        Returns:\n", "            A TunedLens instance.\n\t        \"\"\"\n\t        unembed = Unembed(model)\n\t        config = TunedLensConfig(\n\t            base_model_name_or_path=model.config.name_or_path,\n\t            base_model_revision=model_revision,\n\t            d_model=model.config.hidden_size,\n\t            num_hidden_layers=model.config.num_hidden_layers,\n\t            bias=bias,\n\t        )\n", "        return cls(unembed, config)\n\t    @classmethod\n\t    def from_model_and_pretrained(\n\t        cls,\n\t        model: PreTrainedModel,\n\t        lens_resource_id: Optional[str] = None,\n\t        **kwargs,\n\t    ) -> \"TunedLens\":\n\t        \"\"\"Load a tuned lens from a folder or hugging face hub.\n\t        Args:\n", "            model: The model to create the lens from.\n\t            lens_resource_id: The resource id of the lens to load. Defaults to the\n\t                model's name_or_path.\n\t            **kwargs: Additional arguments to pass to\n\t                :func:`tuned_lens.load_artifacts.load_lens_artifacts` and\n\t                `th.load <https://pytorch.org/docs/stable/generated/torch.load.html>`_.\n\t        Returns:\n\t            A TunedLens instance whose unembeding is derived from the given model\n\t            and whose layer translators are loaded from the given resource id.\n\t        \"\"\"\n", "        if lens_resource_id is None:\n\t            lens_resource_id = model.config.name_or_path\n\t        return cls.from_unembed_and_pretrained(Unembed(model), lens_resource_id, **kwargs)\n\t    @classmethod\n\t    def from_unembed_and_pretrained(\n\t        cls,\n\t        unembed: Unembed,\n\t        lens_resource_id: str,\n\t        **kwargs,\n\t    ) -> \"TunedLens\":\n", "        \"\"\"Load a tuned lens from a folder or hugging face hub.\n\t        Args:\n\t            unembed: The unembed operation to use for the lens.\n\t            lens_resource_id: The resource id of the lens to load.\n\t            **kwargs: Additional arguments to pass to\n\t                :func:`tuned_lens.load_artifacts.load_lens_artifacts` and\n\t                `th.load <https://pytorch.org/docs/stable/generated/torch.load.html>`_.\n\t        Returns:\n\t            A TunedLens instance.\n\t        \"\"\"\n", "        # Validate kwargs\n\t        load_artifact_varnames = load_artifacts.load_lens_artifacts.__code__.co_varnames\n\t        config_path, ckpt_path = load_artifacts.load_lens_artifacts(\n\t            resource_id=lens_resource_id,\n\t            **{k: v for k, v in kwargs.items() if k in load_artifact_varnames},\n\t        )\n\t        with open(config_path, \"r\") as f:\n\t            config = TunedLensConfig.from_dict(json.load(f))\n\t        # validate the unembed is the same as the one used to train the lens\n\t        if config.unemebd_hash and unembed.unembedding_hash() != config.unemebd_hash:\n", "            warning(\n\t                \"The unembeding matrix hash does not match the lens' hash.\"\n\t                \"This lens may have been trained with a different unembedding.\"\n\t            )\n\t        # Create the lens\n\t        lens = cls(unembed, config)\n\t        th_load_kwargs = {**{k: v for k, v in kwargs.items() if k not in load_artifact_varnames}}\n\t        # Load parameters\n\t        state = th.load(ckpt_path, **th_load_kwargs)\n\t        lens.layer_translators.load_state_dict(state[\"translators\"])\n", "        lens.probs_ln.load_state_dict(state[\"probs_ln\"])\n\t        return lens\n\t    def save(\n\t        self,\n\t        path: Union[Path, str],\n\t        ckpt: str = \"params.pt\",\n\t        config: str = \"config.json\",\n\t    ) -> None:\n\t        \"\"\"Save the lens to a directory.\n\t        Args:\n", "            path : The path to the directory to save the lens to.\n\t            ckpt : The name of the checkpoint file to save the parameters to.\n\t            config : The name of the config file to save the config to.\n\t        \"\"\"\n\t        path = Path(path)\n\t        path.mkdir(exist_ok=True, parents=True)\n\t        state_dict = {\n\t            \"translators\": self.layer_translators.state_dict(),\n\t            \"probs_ln\": self.probs_ln.state_dict(),\n\t        }\n", "        th.save(state_dict, path / ckpt)\n\t        with open(path / config, \"w\") as f:\n\t            json.dump(self.config.to_dict(), f)\n\t    def transform_hidden(self, h: th.Tensor, idx: int) -> th.Tensor:\n\t        \"\"\"Transform hidden state from layer `idx`.\"\"\"\n\t        # Note that we add the translator output residually, in contrast to the formula\n\t        # in the paper. By parametrizing it this way we ensure that weight decay\n\t        # regularizes the transform toward the identity, not the zero transformation.\n\t        return self[idx](h)\n\t        # return h + self[idx](h)\n", "    def forward(self, h: th.Tensor, idx: int) -> th.Tensor:\n\t        \"\"\"Transform and then decode the hidden states into logits.\"\"\"\n\t        h = self.transform_hidden(h, idx)\n\t        return self.unembed.forward(h)\n\t    def __len__(self) -> int:\n\t        \"\"\"Return the number of layer translators in the lens.\"\"\"\n\t        return len(self.layer_translators)\n\t    def reconstruct(self, probs: th.Tensor, idx: int) -> th.Tensor:\n\t        \"\"\"Reconstruct the hidden states from the logits.\"\"\"\n\t        scaled_probs = self.probs_ln(probs)\n", "        unembed_matrix = self.unembed.unembedding.weight  # (vocab_size, d_model)\n\t        translator_matrix = self.layer_translators[idx].weight  # (d_model, d_model)\n\t        return th.einsum(\"...v, vd, dh -> ...h\", scaled_probs, unembed_matrix, translator_matrix)\n"]}
{"filename": "tuned_lens/nn/unembed.py", "chunked_list": ["\"\"\"Provides a class for mapping transformer hidden states to logits (and vice versa).\"\"\"\n\timport copy\n\tfrom dataclasses import dataclass\n\tfrom typing import Literal, Optional, cast\n\timport torch as th\n\tfrom torch.distributions import Distribution\n\tfrom transformers import PreTrainedModel\n\tfrom tuned_lens import model_surgery\n\tfrom tuned_lens.utils import tensor_hash\n\t@dataclass\n", "class InversionOutput:\n\t    \"\"\"Output of `Unemebd.invert`.\"\"\"\n\t    preimage: th.Tensor\n\t    grad_norm: th.Tensor\n\t    kl: th.Tensor\n\t    loss: th.Tensor\n\t    nfev: int\n\tclass Unembed(th.nn.Module):\n\t    \"\"\"Module that maps transformer hidden states to logits (and vice versa).\"\"\"\n\t    final_norm: model_surgery.Norm\n", "    unembedding: th.nn.Linear\n\t    def __init__(\n\t        self,\n\t        model: PreTrainedModel,\n\t    ):\n\t        \"\"\"Initialize unmebed.\n\t        Args:\n\t            model: A HuggingFace model from which to extract the unembedding matrix.\n\t        \"\"\"\n\t        super().__init__()\n", "        final_norm = model_surgery.get_final_norm(model)\n\t        unembeding_matrix = model.get_output_embeddings()\n\t        if not isinstance(unembeding_matrix, th.nn.Linear):\n\t            # With nn.Linear we know that the unembedding matrix is .weight;\n\t            # we don't want to guess incorrectly for other module classes.\n\t            raise ValueError(\"Currently we only support nn.Linear unembeddings.\")\n\t        self.final_norm = copy.deepcopy(final_norm)\n\t        self.unembedding = copy.deepcopy(unembeding_matrix)\n\t        # In general we don't want to finetune the unembed operation.\n\t        self.requires_grad_(False)\n", "    def unembedding_hash(self) -> str:\n\t        \"\"\"Hash the unmbedding matrix to identify the model.\"\"\"\n\t        parameter = self.unembedding.weight.data.detach().cpu().numpy()\n\t        return tensor_hash(parameter)\n\t    def forward(self, h: th.Tensor) -> th.Tensor:\n\t        \"\"\"Convert hidden states into logits.\"\"\"\n\t        return self.unembedding(self.final_norm(h))\n\t    def invert(\n\t        self,\n\t        logits: th.Tensor,\n", "        *,\n\t        h0: Optional[th.Tensor] = None,\n\t        max_iter: int = 1000,\n\t        optimizer: Literal[\"lbfgs\", \"sgd\"] = \"lbfgs\",\n\t        prior_weight: float = 0.0,\n\t        prior: Optional[Distribution] = None,\n\t        step_size: float = 1.0,\n\t        tol: float = 1e-3,\n\t        weight: Optional[th.Tensor] = None,\n\t    ) -> InversionOutput:\n", "        \"\"\"Project logits onto the image of the unemebed operation.\n\t        When the hidden state dimension is smaller than the vocabulary size, the\n\t        unembed operation cannot perfectly represent arbitrary logits, since its image\n\t        is restricted to a subspace; this phenomenon is known as the softmax bottleneck\n\t        (cf. https://arxiv.org/abs/1711.03953). Because of this, the inverse can only\n\t        be approximate in general. Here, we use gradient-based optimization to find a\n\t        hidden state that minimizes the KL divergence from the target distribution p to\n\t        unembeded logits q(h): h* = argmin_h KL(p || q(h)).\n\t        Args:\n\t            logits: Tensor of shape `[..., vocab_size]` containing logits to invert.\n", "            h0: Initial guess for the hidden state. If `None`, the least-squares\n\t                solution of the linear equation xU = logits is used, where U is the\n\t                unembedding matrix.\n\t            max_iter: Maximum number of iterations for the optimizer to take.\n\t            optimizer: Optimization algorithm to use. Currently, only \"lbfgs\" and \"sgd\"\n\t                are supported.\n\t            prior_weight: The weight of the prior distribution is given in the loss.\n\t            prior: Prior distribution over hidden states used to regularize\n\t                the inversion.\n\t            step_size: The step size for the optimizer.\n", "            tol: Tolerance for the inversion objective.\n\t            weight: Optional tensor of shape `[..., vocab_size]` containing weights\n\t                for each vocabulary item. If `None`, all classes are weighted equally.\n\t        \"\"\"\n\t        d_model = cast(int, self.unembedding.in_features)\n\t        leading_dims = logits.shape[:-1]\n\t        if h0 is None:\n\t            # Initialize with the Moore-Penrose pseudoinverse\n\t            h0 = th.zeros((*leading_dims, d_model), device=logits.device)\n\t        # Sanity check the shape of the initial hidden state. Can silently lead to\n", "        # incorrect results due to broadcasting if we don't check this.\n\t        elif h0.shape != (*leading_dims, d_model):\n\t            raise ValueError(\n\t                f\"Initial hidden state has shape {h0.shape} but should have shape \"\n\t                f\"{(*leading_dims, d_model)} given logits shape {logits.shape}.\"\n\t            )\n\t        h_star = th.nn.Parameter(h0)\n\t        if optimizer == \"lbfgs\":\n\t            opt = th.optim.LBFGS(\n\t                [h_star],\n", "                line_search_fn=\"strong_wolfe\",\n\t                lr=step_size,\n\t                max_iter=max_iter,\n\t                tolerance_change=tol,\n\t            )\n\t        elif optimizer == \"sgd\":\n\t            opt = th.optim.SGD([h_star], lr=step_size)\n\t        else:\n\t            raise ValueError(f\"Unknown optimizer '{optimizer}'\")\n\t        log_p = logits.log_softmax(dim=-1)\n", "        p = log_p.exp()\n\t        if weight is not None:\n\t            p *= weight\n\t        def compute_loss(h: th.Tensor) -> tuple[th.Tensor, th.Tensor]:\n\t            log_q = self(h).log_softmax(-1)\n\t            kl = th.sum(p * (log_p - log_q), dim=-1).nanmean()\n\t            loss = kl.clone()\n\t            if prior_weight and prior is not None:\n\t                # We evaluate the prior density on the post-norm hidden state,\n\t                # to prevent the pre-norm hidden from collapsing towards zero.\n", "                h_ = self.final_norm(h)\n\t                loss += prior_weight * -prior.log_prob(h_).mean()\n\t            return loss, kl\n\t        nfev = 0  # Number of function evals, like in scipy.optimize.minimize\n\t        loss, kl = log_p.new_tensor(th.inf), log_p.new_tensor(th.inf)\n\t        def closure():\n\t            nonlocal nfev, loss, kl\n\t            nfev += 1\n\t            opt.zero_grad(set_to_none=False)\n\t            loss, kl = compute_loss(h_star)\n", "            if not loss.isfinite():\n\t                raise RuntimeError(\"Inversion objective is not finite.\")\n\t            loss.backward()\n\t            return loss\n\t        grad_norm = log_p.new_tensor(th.inf)\n\t        while nfev < max_iter:\n\t            opt.step(closure)  # type: ignore\n\t            final_grad = h_star.grad\n\t            assert final_grad is not None\n\t            grad_norm = final_grad.norm()\n", "            if grad_norm < tol or loss < tol:\n\t                break\n\t        with th.no_grad():\n\t            output = InversionOutput(\n\t                preimage=self.final_norm(h_star.data),\n\t                grad_norm=grad_norm,\n\t                kl=kl.detach(),\n\t                loss=loss.detach(),\n\t                nfev=nfev,\n\t            )\n", "        return output\n"]}
{"filename": "tuned_lens/stats/logit_stats.py", "chunked_list": ["\"\"\"Online MLE for the Dirichlet distribution from which logits are sampled.\"\"\"\n\tfrom typing import Optional\n\timport torch as th\n\tfrom torch.distributions import Dirichlet\n\tfrom ..utils import maybe_all_reduce\n\tclass LogitStats:\n\t    \"\"\"Online MLE for the Dirichlet distribution from which logits are sampled.\n\t    Shape and device are lazily inferred from the first stream that is passed to\n\t    `update()`. Only a running mean of the log-likelihoods for each class is stored,\n\t    so memory use is negligible and constant in the number of samples. The maximum\n", "    likelihood distribution is computed on request using L-BFGS.\n\t    \"\"\"\n\t    n: int\n\t    marginal_probs: Optional[th.Tensor]\n\t    sufficient_stats: Optional[th.Tensor]\n\t    def __init__(self):\n\t        \"\"\"Create a LogitStats object.\"\"\"\n\t        self.n = 0\n\t        self.marginal_probs = None\n\t        self.sufficient_stats = None\n", "    def all_reduce_(self):\n\t        \"\"\"All-reduce the stats across all processes.\"\"\"\n\t        if self.sufficient_stats is not None:\n\t            maybe_all_reduce(self.sufficient_stats)\n\t    @th.no_grad()\n\t    def update(self, logits: th.Tensor, assume_normalized: bool = False):\n\t        \"\"\"Update the sufficient statistics with a new batch of logits.\"\"\"\n\t        K = logits.shape[-1]\n\t        logits = logits.reshape(-1, K).float()\n\t        if not assume_normalized:\n", "            logits = logits.log_softmax(dim=-1)\n\t        N = logits.shape[0]\n\t        if self.marginal_probs is None:\n\t            self.marginal_probs = logits.new_zeros(K)\n\t        if self.sufficient_stats is None:\n\t            self.sufficient_stats = logits.new_zeros(K)\n\t        elif self.sufficient_stats.shape[-1] != K:\n\t            raise ValueError(f\"Expected {self.sufficient_stats.shape[-1]} but got {K}\")\n\t        # Online mean update for the marginal probabilities\n\t        delta = logits.exp().mean(0) - self.marginal_probs\n", "        self.n += N\n\t        self.marginal_probs += delta * N / self.n\n\t        # Online mean update for the sufficient statistics\n\t        delta = logits.mean(0) - self.sufficient_stats\n\t        self.sufficient_stats += delta * N / self.n\n\t    def mle(self, max_iter: int = 100, tol: float = 1e-4) -> Dirichlet:\n\t        \"\"\"Compute the MLE for the Dirichlet generating the logits seen so far.\"\"\"\n\t        if self.sufficient_stats is None:\n\t            raise ValueError(\"No sufficient statistics available\")\n\t        log_alpha = th.nn.Parameter(th.zeros_like(self.sufficient_stats))\n", "        opt = th.optim.LBFGS(\n\t            [log_alpha],\n\t            line_search_fn=\"strong_wolfe\",\n\t            max_iter=max_iter,\n\t            tolerance_change=tol,\n\t        )\n\t        def closure():\n\t            opt.zero_grad(set_to_none=False)\n\t            # See http://jonathan-huang.org/research/dirichlet/dirichlet.pdf,\n\t            # page 5 for the formula\n", "            alpha = log_alpha.exp()\n\t            normalizer = alpha.sum().lgamma() - alpha.lgamma().sum()\n\t            loss = -(normalizer + (alpha - 1) @ self.sufficient_stats)\n\t            loss.backward()\n\t            return loss\n\t        opt.step(closure)  # type: ignore\n\t        return Dirichlet(log_alpha.data.exp())\n"]}
{"filename": "tuned_lens/stats/__init__.py", "chunked_list": ["\"\"\"Statistics for evaluating lens performance.\"\"\"\n\tfrom .distance import (\n\t    js_distance,\n\t    js_divergence,\n\t    kl_divergence,\n\t)\n\tfrom .logit_stats import LogitStats\n"]}
{"filename": "tuned_lens/stats/distance.py", "chunked_list": ["\"\"\"Various distance metrics for probability distributions.\"\"\"\n\timport math\n\timport torch as th\n\tdef js_divergence(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:\n\t    \"\"\"Compute the Jensen-Shannon divergence between two sets of logits.\n\t    Conceptually, the JSD is the info value of learning which of two distributions,\n\t    P or Q, that a random variable is drawn from, starting from a uniform prior over\n\t    P and Q. Since the entropy of a Bernoulli variable is at most ln(2), the JSD is\n\t    guaranteed to be in the range [0, ln(2)]. It is also symmetric and finite even\n\t    for distributions with disjoint supports.\n", "    Mathematically, the JSD is simply [KL(P || M) + KL(Q || M)] / 2, where M\n\t    is the mean of P and Q.\n\t    \"\"\"\n\t    log_p = logit_p.log_softmax(dim)\n\t    log_q = logit_q.log_softmax(dim)\n\t    # Mean of P and Q\n\t    log_m = th.stack([log_p, log_q]).sub(math.log(2)).logsumexp(0)\n\t    kl_p = th.sum(log_p.exp() * (log_p - log_m), dim)\n\t    kl_q = th.sum(log_q.exp() * (log_q - log_m), dim)\n\t    return 0.5 * (kl_p + kl_q)\n", "def js_distance(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:\n\t    \"\"\"Compute the square root of the Jensen-Shannon divergence of two logit vectors.\"\"\"\n\t    return js_divergence(logit_p, logit_q, dim).sqrt()\n\tdef kl_divergence(logit_p: th.Tensor, logit_q: th.Tensor, dim: int = -1) -> th.Tensor:\n\t    \"\"\"Compute the KL divergence between two sets of logits.\"\"\"\n\t    log_p = logit_p.log_softmax(dim)\n\t    log_q = logit_q.log_softmax(dim)\n\t    return th.sum(log_p.exp() * (log_p - log_q), dim)\n\tdef sqrtmh(x: th.Tensor) -> th.Tensor:\n\t    \"\"\"Unique PSD square root of a Hermitian positive semi-definite matrix.\"\"\"\n", "    dtype = x.dtype\n\t    # This is actually precision-sensitive\n\t    L, Q = th.linalg.eigh(x.double())\n\t    res = Q * L.clamp(0.0).sqrt() @ Q.mH\n\t    return res.to(dtype)\n"]}
