{"filename": "setup.py", "chunked_list": ["import os\n\timport sys\n\tfrom pathlib import Path\n\timport winreg\n\tdef main():\n\t    if not check_ffmpeg_path():\n\t        cmd = input(\"需要安装ffmpeg,是否继续?(y/n):\")\n\t        if cmd.lower() != 'y':\n\t            sys.exit(1)\n\t        # ffmpeg 路径添加到环境变量中\n", "        add_ffmpeg_path()\n\tdef check_ffmpeg_path():\n\t    path_list = os.environ['Path'].split(';')\n\t    ffmpeg_found = False\n\t    for path in path_list:\n\t        if 'ffmpeg' in path.lower() and 'bin' in path.lower():\n\t            ffmpeg_found = True\n\t            break\n\t    return ffmpeg_found\n\tdef add_ffmpeg_path():\n", "    ffmpeg_bin_path = Path('.\\\\ffmpeg\\\\bin')\n\t    if ffmpeg_bin_path.is_dir():\n\t        abs_path = str(ffmpeg_bin_path.resolve())\n\t        try:\n\t            key = winreg.OpenKey(\n\t                winreg.HKEY_CURRENT_USER,\n\t                r\"Environment\",\n\t                0,\n\t                winreg.KEY_READ | winreg.KEY_WRITE\n\t            )\n", "            try:\n\t                current_path, _ = winreg.QueryValueEx(key, \"Path\")\n\t                if abs_path not in current_path:\n\t                    new_path = f\"{current_path};{abs_path}\"\n\t                    winreg.SetValueEx(key, \"Path\", 0, winreg.REG_EXPAND_SZ, new_path)\n\t                    print(f\"Added FFmpeg path to user variable 'Path': {abs_path}\")\n\t                else:\n\t                    print(\"FFmpeg path already exists in the user variable 'Path'.\")\n\t            finally:\n\t                winreg.CloseKey(key)\n", "        except WindowsError:\n\t            print(\"Error: Unable to modify user variable 'Path'.\")\n\t            sys.exit(1)\n\t    else:\n\t        print(\"Error: ffmpeg\\\\bin folder not found in the current path.\")\n\t        sys.exit(1)\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "app-shared.py", "chunked_list": ["# Run the app with no audio file restrictions\n\tfrom app import create_ui\n\tfrom src.config import ApplicationConfig\n\tcreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1, share=True))"]}
{"filename": "app.py", "chunked_list": ["from datetime import datetime\n\timport math\n\tfrom typing import Iterator, Union\n\timport argparse\n\tfrom io import StringIO\n\timport os\n\timport pathlib\n\timport tempfile\n\timport zipfile\n\timport numpy as np\n", "import torch\n\tfrom src.config import ApplicationConfig, VadInitialPromptMode\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.hooks.subTaskProgressListener import SubTaskProgressListener\n\tfrom src.hooks.whisperProgressHook import create_progress_listener_handle\n\tfrom src.languages import get_language_names\n\tfrom src.modelCache import ModelCache\n\tfrom src.source import get_audio_source_collection\n\tfrom src.vadParallel import ParallelContext, ParallelTranscription\n\t# External programs\n", "import ffmpeg\n\t# UI\n\timport gradio as gr\n\tfrom src.download import ExceededMaximumDuration, download_url\n\tfrom src.utils import slugify, write_srt, write_vtt\n\tfrom src.vad import AbstractTranscription, NonSpeechStrategy, PeriodicTranscriptionConfig, TranscriptionConfig, VadPeriodicTranscription, VadSileroTranscription\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperContainer\n\tfrom src.whisper.whisperFactory import create_whisper_container\n\tfrom src.description import get_description\n\t# Configure more application defaults in config.json5\n", "# Gradio seems to truncate files without keeping the extension, so we need to truncate the file prefix ourself \n\tMAX_FILE_PREFIX_LENGTH = 17\n\t# Limit auto_parallel to a certain number of CPUs (specify vad_cpu_cores to get a higher number)\n\tMAX_AUTO_CPU_CORES = 8\n\tWHISPER_MODELS = [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v1\", \"large-v2\"]\n\tclass VadOptions:\n\t    def __init__(self, vad: str = None, vadMergeWindow: float = 5, vadMaxMergeSize: float = 150, vadPadding: float = 1, vadPromptWindow: float = 1, \n\t                                        vadInitialPromptMode: Union[VadInitialPromptMode, str] = VadInitialPromptMode.PREPREND_FIRST_SEGMENT):\n\t        self.vad = vad\n\t        self.vadMergeWindow = vadMergeWindow\n", "        self.vadMaxMergeSize = vadMaxMergeSize\n\t        self.vadPadding = vadPadding\n\t        self.vadPromptWindow = vadPromptWindow\n\t        self.vadInitialPromptMode = vadInitialPromptMode if isinstance(vadInitialPromptMode, VadInitialPromptMode) \\\n\t                                        else VadInitialPromptMode.from_string(vadInitialPromptMode)\n\tclass WhisperTranscriber:\n\t    def __init__(self, input_audio_max_duration: float = None, vad_process_timeout: float = None, \n\t                 vad_cpu_cores: int = 1, delete_uploaded_files: bool = False, output_dir: str = None, \n\t                 app_config: ApplicationConfig = None):\n\t        self.model_cache = ModelCache()\n", "        self.parallel_device_list = None\n\t        self.gpu_parallel_context = None\n\t        self.cpu_parallel_context = None\n\t        self.vad_process_timeout = vad_process_timeout\n\t        self.vad_cpu_cores = vad_cpu_cores\n\t        self.vad_model = None\n\t        self.inputAudioMaxDuration = input_audio_max_duration\n\t        self.deleteUploadedFiles = delete_uploaded_files\n\t        self.output_dir = output_dir\n\t        self.app_config = app_config\n", "    def set_parallel_devices(self, vad_parallel_devices: str):\n\t        self.parallel_device_list = [ device.strip() for device in vad_parallel_devices.split(\",\") ] if vad_parallel_devices else None\n\t    def set_auto_parallel(self, auto_parallel: bool):\n\t        if auto_parallel:\n\t            if torch.cuda.is_available():\n\t                self.parallel_device_list = [ str(gpu_id) for gpu_id in range(torch.cuda.device_count())]\n\t            self.vad_cpu_cores = min(os.cpu_count(), MAX_AUTO_CPU_CORES)\n\t            print(\"[Auto parallel] Using GPU devices \" + str(self.parallel_device_list) + \" and \" + str(self.vad_cpu_cores) + \" CPU cores for VAD/transcription.\")\n\t    # Entry function for the simple tab\n\t    def transcribe_webui_simple(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow):\n", "        return self.transcribe_webui_simple_progress(modelName, languageName, urlData, multipleFiles, microphoneData, task, vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow)\n\t    # Entry function for the simple tab progress\n\t    def transcribe_webui_simple_progress(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, \n\t                                progress=gr.Progress()):\n\t        vadOptions = VadOptions(vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, self.app_config.vad_initial_prompt_mode)\n\t        return self.transcribe_webui(modelName, languageName, urlData, multipleFiles, microphoneData, task, vadOptions, progress=progress)\n\t    # Entry function for the full tab\n\t    def transcribe_webui_full(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode, \n\t                                initial_prompt: str, temperature: float, best_of: int, beam_size: int, patience: float, length_penalty: float, suppress_tokens: str, \n", "                                condition_on_previous_text: bool, fp16: bool, temperature_increment_on_fallback: float, \n\t                                compression_ratio_threshold: float, logprob_threshold: float, no_speech_threshold: float):\n\t        return self.transcribe_webui_full_progress(modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode,\n\t                                initial_prompt, temperature, best_of, beam_size, patience, length_penalty, suppress_tokens,\n\t                                condition_on_previous_text, fp16, temperature_increment_on_fallback,\n\t                                compression_ratio_threshold, logprob_threshold, no_speech_threshold)\n\t    # Entry function for the full tab with progress\n\t    def transcribe_webui_full_progress(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                                    vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode, \n", "                                    initial_prompt: str, temperature: float, best_of: int, beam_size: int, patience: float, length_penalty: float, suppress_tokens: str, \n\t                                    condition_on_previous_text: bool, fp16: bool, temperature_increment_on_fallback: float, \n\t                                    compression_ratio_threshold: float, logprob_threshold: float, no_speech_threshold: float, \n\t                                    progress=gr.Progress()):\n\t        # Handle temperature_increment_on_fallback\n\t        if temperature_increment_on_fallback is not None:\n\t            temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n\t        else:\n\t            temperature = [temperature]\n\t        vadOptions = VadOptions(vad, vadMergeWindow, vadMaxMergeSize, vadPadding, vadPromptWindow, vadInitialPromptMode)\n", "        return self.transcribe_webui(modelName, languageName, urlData, multipleFiles, microphoneData, task, vadOptions,\n\t                                     initial_prompt=initial_prompt, temperature=temperature, best_of=best_of, beam_size=beam_size, patience=patience, length_penalty=length_penalty, suppress_tokens=suppress_tokens,\n\t                                     condition_on_previous_text=condition_on_previous_text, fp16=fp16,\n\t                                     compression_ratio_threshold=compression_ratio_threshold, logprob_threshold=logprob_threshold, no_speech_threshold=no_speech_threshold, \n\t                                     progress=progress)\n\t    def transcribe_webui(self, modelName, languageName, urlData, multipleFiles, microphoneData, task, \n\t                         vadOptions: VadOptions, progress: gr.Progress = None, **decodeOptions: dict):\n\t        try:\n\t            sources = self.__get_source(urlData, multipleFiles, microphoneData)\n\t            try:\n", "                selectedLanguage = languageName.lower() if len(languageName) > 0 else None\n\t                selectedModel = modelName if modelName is not None else \"base\"\n\t                model = create_whisper_container(whisper_implementation=self.app_config.whisper_implementation, \n\t                                                 model_name=selectedModel, compute_type=self.app_config.compute_type, \n\t                                                 cache=self.model_cache, models=self.app_config.models)\n\t                # Result\n\t                download = []\n\t                zip_file_lookup = {}\n\t                text = \"\"\n\t                vtt = \"\"\n", "                # Write result\n\t                downloadDirectory = tempfile.mkdtemp()\n\t                source_index = 0\n\t                outputDirectory = self.output_dir if self.output_dir is not None else downloadDirectory\n\t                # Progress\n\t                total_duration = sum([source.get_audio_duration() for source in sources])\n\t                current_progress = 0\n\t                # A listener that will report progress to Gradio\n\t                root_progress_listener = self._create_progress_listener(progress)\n\t                # Execute whisper\n", "                for source in sources:\n\t                    source_prefix = \"\"\n\t                    source_audio_duration = source.get_audio_duration()\n\t                    if (len(sources) > 1):\n\t                        # Prefix (minimum 2 digits)\n\t                        source_index += 1\n\t                        source_prefix = str(source_index).zfill(2) + \"_\"\n\t                        print(\"Transcribing \", source.source_path)\n\t                    scaled_progress_listener = SubTaskProgressListener(root_progress_listener, \n\t                                                   base_task_total=total_duration,\n", "                                                   sub_task_start=current_progress,\n\t                                                   sub_task_total=source_audio_duration)\n\t                    # Transcribe\n\t                    result = self.transcribe_file(model, source.source_path, selectedLanguage, task, vadOptions, scaled_progress_listener, **decodeOptions)\n\t                    filePrefix = slugify(source_prefix + source.get_short_name(), allow_unicode=True)\n\t                    # Update progress\n\t                    current_progress += source_audio_duration\n\t                    source_download, source_text, source_vtt = self.write_result(result, filePrefix, outputDirectory)\n\t                    if len(sources) > 1:\n\t                        # Add new line separators\n", "                        if (len(source_text) > 0):\n\t                            source_text += os.linesep + os.linesep\n\t                        if (len(source_vtt) > 0):\n\t                            source_vtt += os.linesep + os.linesep\n\t                        # Append file name to source text too\n\t                        source_text = source.get_full_name() + \":\" + os.linesep + source_text\n\t                        source_vtt = source.get_full_name() + \":\" + os.linesep + source_vtt\n\t                    # Add to result\n\t                    download.extend(source_download)\n\t                    text += source_text\n", "                    vtt += source_vtt\n\t                    if (len(sources) > 1):\n\t                        # Zip files support at least 260 characters, but we'll play it safe and use 200\n\t                        zipFilePrefix = slugify(source_prefix + source.get_short_name(max_length=200), allow_unicode=True)\n\t                        # File names in ZIP file can be longer\n\t                        for source_download_file in source_download:\n\t                            # Get file postfix (after last -)\n\t                            filePostfix = os.path.basename(source_download_file).split(\"-\")[-1]\n\t                            zip_file_name = zipFilePrefix + \"-\" + filePostfix\n\t                            zip_file_lookup[source_download_file] = zip_file_name\n", "                # Create zip file from all sources\n\t                if len(sources) > 1:\n\t                    downloadAllPath = os.path.join(downloadDirectory, \"All_Output-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \".zip\")\n\t                    with zipfile.ZipFile(downloadAllPath, 'w', zipfile.ZIP_DEFLATED) as zip:\n\t                        for download_file in download:\n\t                            # Get file name from lookup\n\t                            zip_file_name = zip_file_lookup.get(download_file, os.path.basename(download_file))\n\t                            zip.write(download_file, arcname=zip_file_name)\n\t                    download.insert(0, downloadAllPath)\n\t                return download, text, vtt\n", "            finally:\n\t                # Cleanup source\n\t                if self.deleteUploadedFiles:\n\t                    for source in sources:\n\t                        print(\"Deleting source file \" + source.source_path)\n\t                        try:\n\t                            os.remove(source.source_path)\n\t                        except Exception as e:\n\t                            # Ignore error - it's just a cleanup\n\t                            print(\"Error deleting source file \" + source.source_path + \": \" + str(e))\n", "        except ExceededMaximumDuration as e:\n\t            return [], (\"[ERROR]: Maximum remote video length is \" + str(e.maxDuration) + \"s, file was \" + str(e.videoDuration) + \"s\"), \"[ERROR]\"\n\t    def transcribe_file(self, model: AbstractWhisperContainer, audio_path: str, language: str, task: str = None, \n\t                        vadOptions: VadOptions = VadOptions(), \n\t                        progressListener: ProgressListener = None, **decodeOptions: dict):\n\t        initial_prompt = decodeOptions.pop('initial_prompt', None)\n\t        if progressListener is None:\n\t            # Default progress listener\n\t            progressListener = ProgressListener()\n\t        if ('task' in decodeOptions):\n", "            task = decodeOptions.pop('task')\n\t        # Callable for processing an audio file\n\t        whisperCallable = model.create_callback(language, task, initial_prompt, initial_prompt_mode=vadOptions.vadInitialPromptMode, **decodeOptions)\n\t        # The results\n\t        if (vadOptions.vad == 'silero-vad'):\n\t            # Silero VAD where non-speech gaps are transcribed\n\t            process_gaps = self._create_silero_config(NonSpeechStrategy.CREATE_SEGMENT, vadOptions)\n\t            result = self.process_vad(audio_path, whisperCallable, self.vad_model, process_gaps, progressListener=progressListener)\n\t        elif (vadOptions.vad == 'silero-vad-skip-gaps'):\n\t            # Silero VAD where non-speech gaps are simply ignored\n", "            skip_gaps = self._create_silero_config(NonSpeechStrategy.SKIP, vadOptions)\n\t            result = self.process_vad(audio_path, whisperCallable, self.vad_model, skip_gaps, progressListener=progressListener)\n\t        elif (vadOptions.vad == 'silero-vad-expand-into-gaps'):\n\t            # Use Silero VAD where speech-segments are expanded into non-speech gaps\n\t            expand_gaps = self._create_silero_config(NonSpeechStrategy.EXPAND_SEGMENT, vadOptions)\n\t            result = self.process_vad(audio_path, whisperCallable, self.vad_model, expand_gaps, progressListener=progressListener)\n\t        elif (vadOptions.vad == 'periodic-vad'):\n\t            # Very simple VAD - mark every 5 minutes as speech. This makes it less likely that Whisper enters an infinite loop, but\n\t            # it may create a break in the middle of a sentence, causing some artifacts.\n\t            periodic_vad = VadPeriodicTranscription()\n", "            period_config = PeriodicTranscriptionConfig(periodic_duration=vadOptions.vadMaxMergeSize, max_prompt_window=vadOptions.vadPromptWindow)\n\t            result = self.process_vad(audio_path, whisperCallable, periodic_vad, period_config, progressListener=progressListener)\n\t        else:\n\t            if (self._has_parallel_devices()):\n\t                # Use a simple period transcription instead, as we need to use the parallel context\n\t                periodic_vad = VadPeriodicTranscription()\n\t                period_config = PeriodicTranscriptionConfig(periodic_duration=math.inf, max_prompt_window=1)\n\t                result = self.process_vad(audio_path, whisperCallable, periodic_vad, period_config, progressListener=progressListener)\n\t            else:\n\t                # Default VAD\n", "                result = whisperCallable.invoke(audio_path, 0, None, None, progress_listener=progressListener)\n\t        return result\n\t    def _create_progress_listener(self, progress: gr.Progress):\n\t        if (progress is None):\n\t            # Dummy progress listener\n\t            return ProgressListener()\n\t        class ForwardingProgressListener(ProgressListener):\n\t            def __init__(self, progress: gr.Progress):\n\t                self.progress = progress\n\t            def on_progress(self, current: Union[int, float], total: Union[int, float]):\n", "                # From 0 to 1\n\t                self.progress(current / total)\n\t            def on_finished(self):\n\t                self.progress(1)\n\t        return ForwardingProgressListener(progress)\n\t    def process_vad(self, audio_path, whisperCallable, vadModel: AbstractTranscription, vadConfig: TranscriptionConfig, \n\t                    progressListener: ProgressListener = None):\n\t        if (not self._has_parallel_devices()):\n\t            # No parallel devices, so just run the VAD and Whisper in sequence\n\t            return vadModel.transcribe(audio_path, whisperCallable, vadConfig, progressListener=progressListener)\n", "        gpu_devices = self.parallel_device_list\n\t        if (gpu_devices is None or len(gpu_devices) == 0):\n\t            # No GPU devices specified, pass the current environment variable to the first GPU process. This may be NULL.\n\t            gpu_devices = [os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)]\n\t        # Create parallel context if needed\n\t        if (self.gpu_parallel_context is None):\n\t            # Create a context wih processes and automatically clear the pool after 1 hour of inactivity\n\t            self.gpu_parallel_context = ParallelContext(num_processes=len(gpu_devices), auto_cleanup_timeout_seconds=self.vad_process_timeout)\n\t        # We also need a CPU context for the VAD\n\t        if (self.cpu_parallel_context is None):\n", "            self.cpu_parallel_context = ParallelContext(num_processes=self.vad_cpu_cores, auto_cleanup_timeout_seconds=self.vad_process_timeout)\n\t        parallel_vad = ParallelTranscription()\n\t        return parallel_vad.transcribe_parallel(transcription=vadModel, audio=audio_path, whisperCallable=whisperCallable,  \n\t                                                config=vadConfig, cpu_device_count=self.vad_cpu_cores, gpu_devices=gpu_devices, \n\t                                                cpu_parallel_context=self.cpu_parallel_context, gpu_parallel_context=self.gpu_parallel_context, \n\t                                                progress_listener=progressListener) \n\t    def _has_parallel_devices(self):\n\t        return (self.parallel_device_list is not None and len(self.parallel_device_list) > 0) or self.vad_cpu_cores > 1\n\t    def _concat_prompt(self, prompt1, prompt2):\n\t        if (prompt1 is None):\n", "            return prompt2\n\t        elif (prompt2 is None):\n\t            return prompt1\n\t        else:\n\t            return prompt1 + \" \" + prompt2\n\t    def _create_silero_config(self, non_speech_strategy: NonSpeechStrategy, vadOptions: VadOptions):\n\t        # Use Silero VAD \n\t        if (self.vad_model is None):\n\t            self.vad_model = VadSileroTranscription()\n\t        config = TranscriptionConfig(non_speech_strategy = non_speech_strategy, \n", "                max_silent_period=vadOptions.vadMergeWindow, max_merge_size=vadOptions.vadMaxMergeSize, \n\t                segment_padding_left=vadOptions.vadPadding, segment_padding_right=vadOptions.vadPadding, \n\t                max_prompt_window=vadOptions.vadPromptWindow)\n\t        return config\n\t    def write_result(self, result: dict, source_name: str, output_dir: str):\n\t        if not os.path.exists(output_dir):\n\t            os.makedirs(output_dir)\n\t        text = result[\"text\"]\n\t        language = result[\"language\"]\n\t        languageMaxLineWidth = self.__get_max_line_width(language)\n", "        print(\"Max line width \" + str(languageMaxLineWidth))\n\t        vtt = self.__get_subs(result[\"segments\"], \"vtt\", languageMaxLineWidth)\n\t        srt = self.__get_subs(result[\"segments\"], \"srt\", languageMaxLineWidth)\n\t        output_files = []\n\t        output_files.append(self.__create_file(srt, output_dir, source_name + \"-subs.srt\"));\n\t        output_files.append(self.__create_file(vtt, output_dir, source_name + \"-subs.vtt\"));\n\t        output_files.append(self.__create_file(text, output_dir, source_name + \"-transcript.txt\"));\n\t        return output_files, text, vtt\n\t    def clear_cache(self):\n\t        self.model_cache.clear()\n", "        self.vad_model = None\n\t    def __get_source(self, urlData, multipleFiles, microphoneData):\n\t        return get_audio_source_collection(urlData, multipleFiles, microphoneData, self.inputAudioMaxDuration)\n\t    def __get_max_line_width(self, language: str) -> int:\n\t        if (language and language.lower() in [\"japanese\", \"ja\", \"chinese\", \"zh\"]):\n\t            # Chinese characters and kana are wider, so limit line length to 40 characters\n\t            return 40\n\t        else:\n\t            # TODO: Add more languages\n\t            # 80 latin characters should fit on a 1080p/720p screen\n", "            return 80\n\t    def __get_subs(self, segments: Iterator[dict], format: str, maxLineWidth: int) -> str:\n\t        segmentStream = StringIO()\n\t        if format == 'vtt':\n\t            write_vtt(segments, file=segmentStream, maxLineWidth=maxLineWidth)\n\t        elif format == 'srt':\n\t            write_srt(segments, file=segmentStream, maxLineWidth=maxLineWidth)\n\t        else:\n\t            raise Exception(\"Unknown format \" + format)\n\t        segmentStream.seek(0)\n", "        return segmentStream.read()\n\t    def __create_file(self, text: str, directory: str, fileName: str) -> str:\n\t        # Write the text to a file\n\t        with open(os.path.join(directory, fileName), 'w+', encoding=\"utf-8\") as file:\n\t            file.write(text)\n\t        return file.name\n\t    def close(self):\n\t        print(\"Closing parallel contexts\")\n\t        self.clear_cache()\n\t        if (self.gpu_parallel_context is not None):\n", "            self.gpu_parallel_context.close()\n\t        if (self.cpu_parallel_context is not None):\n\t            self.cpu_parallel_context.close()\n\tdef create_ui(app_config: ApplicationConfig):\n\t    ui = WhisperTranscriber(app_config.input_audio_max_duration, app_config.vad_process_timeout, app_config.vad_cpu_cores, \n\t                            app_config.delete_uploaded_files, app_config.output_dir, app_config)\n\t    # Specify a list of devices to use for parallel processing\n\t    ui.set_parallel_devices(app_config.vad_parallel_devices)\n\t    ui.set_auto_parallel(app_config.auto_parallel)\n\t    is_whisper = False\n", "    if app_config.whisper_implementation == \"whisper\":\n\t        implementation_name = \"Whisper\"\n\t        is_whisper = True\n\t    elif app_config.whisper_implementation in [\"faster-whisper\", \"faster_whisper\"]:\n\t        implementation_name = \"Faster Whisper\"\n\t    else:\n\t        # Try to convert from camel-case to title-case\n\t        implementation_name = app_config.whisper_implementation.title().replace(\"_\", \" \").replace(\"-\", \" \")\n\t    ui_description = implementation_name + \" is a general-purpose speech recognition model. It is trained on a large dataset of diverse \" \n\t    ui_description += \" audio and is also a multi-task model that can perform multilingual speech recognition \"\n", "    ui_description += \" as well as speech translation and language identification. \"\n\t    ui_description += \"\\n\\n\\n\\n\"+implementation_name+\" 是一个通用的语音识别模型。它是在大量不同语音数据集上进行训练的多任务模型，可以执行多语言语音识别、语音翻译和语言识别。\"\n\t    ui_description += \"\\n\\n\\n\\nFor longer audio files (>10 minutes) not in English, it is recommended that you select Silero VAD (Voice Activity Detector) in the VAD option.\"\n\t    ui_description += \"\\n\\n\\n\\n对于长度超过10分钟且非英语的音频文件，建议在语音活动检测选项中选择Silero VAD（Voice Activity Detector）。\"\n\t    # Recommend faster-whisper\n\t    if is_whisper:\n\t        ui_description += \"\\n\\n\\n\\nFor faster inference on GPU, try [faster-whisper](https://huggingface.co/spaces/aadnk/faster-whisper-webui).\"\n\t    if app_config.input_audio_max_duration > 0:\n\t        ui_description += \"\\n\\n\" + \"Max audio file length: \" + str(app_config.input_audio_max_duration) + \" s\"\n\t        ui_description += \"\\n\\n\" + \"音频文件最大长度: \" + str(app_config.input_audio_max_duration) + \" s\"\n", "    ui_article = \"Read the [documentation here](https://gitlab.com/aadnk/whisper-webui/-/blob/main/docs/options.md).\"\n\t    whisper_models = app_config.get_model_names()\n\t    simple_inputs = lambda : [\n\t        gr.Dropdown(choices=whisper_models, value=app_config.default_model_name, label=\"Model/模型\"),\n\t        gr.Dropdown(choices=sorted(get_language_names()), label=\"Language/语言(为空时自动识别)\", value=app_config.language),\n\t        gr.Text(label=\"URL (YouTube, etc.)/链接(YouTube,其他)\"),\n\t        gr.File(label=\"Upload Files/上传文件\", file_count=\"multiple\"),\n\t        gr.Audio(source=\"microphone\", type=\"filepath\", label=\"Microphone Input/麦克风输入\"),\n\t        gr.Dropdown(choices=[\"transcribe\", \"translate\"], label=\"Task/任务\", value=app_config.task),\n\t        gr.Dropdown(choices=[\"none\", \"silero-vad\", \"silero-vad-skip-gaps\", \"silero-vad-expand-into-gaps\", \"periodic-vad\"], value=app_config.default_vad, label=\"VAD/语音活性检测\"),\n", "        gr.Number(label=\"VAD - Merge Window (s)/VAD - 合并窗口（秒）\", precision=0, value=app_config.vad_merge_window),\n\t        gr.Number(label=\"VAD - Max Merge Size (s)/VAD - 最大合并大小（秒）\", precision=0, value=app_config.vad_max_merge_size),\n\t        gr.Number(label=\"VAD - Padding (s)/VAD - 填充（秒）\", precision=None, value=app_config.vad_padding),\n\t        gr.Number(label=\"VAD - Prompt Window (s)/VAD - 提示窗口（秒）\", precision=None, value=app_config.vad_prompt_window),\n\t    ]\n\t    is_queue_mode = app_config.queue_concurrency_count is not None and app_config.queue_concurrency_count > 0    \n\t    simple_transcribe = gr.Interface(fn=ui.transcribe_webui_simple_progress if is_queue_mode else ui.transcribe_webui_simple, \n\t                                     description=ui_description, article=ui_article, inputs=simple_inputs(), outputs=[\n\t        gr.File(label=\"Download/下载\"),\n\t        gr.Text(label=\"Transcription/转录\"), \n", "        gr.Text(label=\"Segments/分段\")\n\t    ])\n\t    full_description = ui_description + \"\\n\\n\\n\\n\" + \"Be careful when changing some of the options in the full interface - this can cause the model to crash.\"\n\t    full_description = full_description + \"\\n\\n\\n\\n\" + \"<font style='color:red'>在完整界面中更改某些选项时要小心，否则可能会导致模型崩溃。</font>\"\n\t    full_transcribe = gr.Interface(fn=ui.transcribe_webui_full_progress if is_queue_mode else ui.transcribe_webui_full,\n\t                                   description=full_description, article=ui_article, inputs=[\n\t        *simple_inputs(),\n\t        gr.Dropdown(choices=[\"prepend_first_segment\", \"prepend_all_segments\"], value=app_config.vad_initial_prompt_mode, label=\"VAD - Initial Prompt Mode/VAD - 初始提示模式\"),\n\t        gr.TextArea(label=\"Initial Prompt/初始提示\"),\n\t        gr.Number(label=\"Temperature/温度\", value=app_config.temperature),\n", "        gr.Number(label=\"Best Of - Non-zero temperature/最佳结果 - 非零温度\", value=app_config.best_of, precision=0),\n\t        gr.Number(label=\"Beam Size - Zero temperature/束搜索大小 - 零温度\", value=app_config.beam_size, precision=0),\n\t        gr.Number(label=\"Patience - Zero temperature/耐心 - 零温度\", value=app_config.patience),\n\t        gr.Number(label=\"Length Penalty - Any temperature/长度惩罚 - 任何温度\", value=app_config.length_penalty), \n\t        gr.Text(label=\"Suppress Tokens - Comma-separated list of token IDs/抑制标记 - 逗号分隔的标记ID列表\", value=app_config.suppress_tokens),\n\t        gr.Checkbox(label=\"Condition on previous text/以前文为条件\", value=app_config.condition_on_previous_text),\n\t        gr.Checkbox(label=\"FP16\", value=app_config.fp16),\n\t        gr.Number(label=\"Temperature increment on fallback/回退时温度增量\", value=app_config.temperature_increment_on_fallback),\n\t        gr.Number(label=\"Compression ratio threshold/压缩比阈值\", value=app_config.compression_ratio_threshold),\n\t        gr.Number(label=\"Logprob threshold/Logprob阈值\", value=app_config.logprob_threshold),\n", "        gr.Number(label=\"No speech threshold/无语音阈值\", value=app_config.no_speech_threshold)\n\t    ], outputs=[\n\t        gr.File(label=\"Download/下载\"),\n\t        gr.Text(label=\"Transcription/转录\"), \n\t        gr.Text(label=\"Segments/分段\")\n\t    ])\n\t    document = gr.Markdown(\n\t        get_description\n\t    )\n\t    demo = gr.TabbedInterface([simple_transcribe, full_transcribe,document], tab_names=[\"Simple/基础版\", \"Full/完整版\",\"document(说明文档)\"])\n", "    # Queue up the demo\n\t    if is_queue_mode:\n\t        demo.queue(concurrency_count=app_config.queue_concurrency_count)\n\t        print(\"Queue mode enabled (concurrency count: \" + str(app_config.queue_concurrency_count) + \")\")\n\t    else:\n\t        print(\"Queue mode disabled - progress bars will not be shown.\")\n\t    demo.launch(share=app_config.share, server_name=app_config.server_name, server_port=app_config.server_port,quiet=True)\n\t    # Clean up\n\t    ui.close()\n\tif __name__ == '__main__':\n", "    default_app_config = ApplicationConfig.create_default()\n\t    whisper_models = default_app_config.get_model_names()\n\t    # Environment variable overrides\n\t    default_whisper_implementation = os.environ.get(\"WHISPER_IMPLEMENTATION\", default_app_config.whisper_implementation)\n\t    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t    parser.add_argument(\"--input_audio_max_duration\", type=int, default=default_app_config.input_audio_max_duration, \\\n\t                        help=\"Maximum audio file length in seconds, or -1 for no limit.\") # 600\n\t    parser.add_argument(\"--share\", type=bool, default=default_app_config.share, \\\n\t                        help=\"True to share the app on HuggingFace.\") # False\n\t    parser.add_argument(\"--server_name\", type=str, default=default_app_config.server_name, \\\n", "                        help=\"The host or IP to bind to. If None, bind to localhost.\") # None\n\t    parser.add_argument(\"--server_port\", type=int, default=default_app_config.server_port, \\\n\t                        help=\"The port to bind to.\") # 7860\n\t    parser.add_argument(\"--queue_concurrency_count\", type=int, default=default_app_config.queue_concurrency_count, \\\n\t                        help=\"The number of concurrent requests to process.\") # 1\n\t    parser.add_argument(\"--default_model_name\", type=str, choices=whisper_models, default=default_app_config.default_model_name, \\\n\t                        help=\"The default model name.\") # medium\n\t    parser.add_argument(\"--default_vad\", type=str, default=default_app_config.default_vad, \\\n\t                        help=\"The default VAD.\") # silero-vad\n\t    parser.add_argument(\"--vad_initial_prompt_mode\", type=str, default=default_app_config.vad_initial_prompt_mode, choices=[\"prepend_all_segments\", \"prepend_first_segment\"], \\\n", "                        help=\"Whether or not to prepend the initial prompt to each VAD segment (prepend_all_segments), or just the first segment (prepend_first_segment)\") # prepend_first_segment\n\t    parser.add_argument(\"--vad_parallel_devices\", type=str, default=default_app_config.vad_parallel_devices, \\\n\t                        help=\"A commma delimited list of CUDA devices to use for parallel processing. If None, disable parallel processing.\") # \"\"\n\t    parser.add_argument(\"--vad_cpu_cores\", type=int, default=default_app_config.vad_cpu_cores, \\\n\t                        help=\"The number of CPU cores to use for VAD pre-processing.\") # 1\n\t    parser.add_argument(\"--vad_process_timeout\", type=float, default=default_app_config.vad_process_timeout, \\\n\t                        help=\"The number of seconds before inactivate processes are terminated. Use 0 to close processes immediately, or None for no timeout.\") # 1800\n\t    parser.add_argument(\"--auto_parallel\", type=bool, default=default_app_config.auto_parallel, \\\n\t                        help=\"True to use all available GPUs and CPU cores for processing. Use vad_cpu_cores/vad_parallel_devices to specify the number of CPU cores/GPUs to use.\") # False\n\t    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=default_app_config.output_dir, \\\n", "                        help=\"directory to save the outputs\")\n\t    parser.add_argument(\"--whisper_implementation\", type=str, default=default_whisper_implementation, choices=[\"whisper\", \"faster-whisper\"],\\\n\t                        help=\"the Whisper implementation to use\")\n\t    parser.add_argument(\"--compute_type\", type=str, default=default_app_config.compute_type, choices=[\"default\", \"auto\", \"int8\", \"int8_float16\", \"int16\", \"float16\", \"float32\"], \\\n\t                        help=\"the compute type to use for inference\")\n\t    args = parser.parse_args().__dict__\n\t    updated_config = default_app_config.update(**args)\n\t    create_ui(app_config=updated_config)"]}
{"filename": "app-network.py", "chunked_list": ["# Run the app with no audio file restrictions, and make it available on the network\n\tfrom app import create_ui\n\tfrom src.config import ApplicationConfig\n\tcreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1, server_name=\"0.0.0.0\"))"]}
{"filename": "app-local.py", "chunked_list": ["# Run the app with no audio file restrictions\n\tfrom app import create_ui\n\tfrom src.config import ApplicationConfig\n\tcreate_ui(ApplicationConfig.create_default(input_audio_max_duration=-1))"]}
{"filename": "cli.py", "chunked_list": ["import argparse\n\timport os\n\timport pathlib\n\tfrom urllib.parse import urlparse\n\timport warnings\n\timport numpy as np\n\timport torch\n\tfrom app import VadOptions, WhisperTranscriber\n\tfrom src.config import ApplicationConfig, VadInitialPromptMode\n\tfrom src.download import download_url\n", "from src.languages import get_language_names\n\tfrom src.utils import optional_float, optional_int, str2bool\n\tfrom src.whisper.whisperFactory import create_whisper_container\n\tdef cli():\n\t    app_config = ApplicationConfig.create_default()\n\t    whisper_models = app_config.get_model_names()\n\t    # For the CLI, we fallback to saving the output to the current directory\n\t    output_dir = app_config.output_dir if app_config.output_dir is not None else \".\"\n\t    # Environment variable overrides\n\t    default_whisper_implementation = os.environ.get(\"WHISPER_IMPLEMENTATION\", app_config.whisper_implementation)\n", "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\t    parser.add_argument(\"audio\", nargs=\"+\", type=str, \\\n\t                        help=\"audio file(s) to transcribe\")\n\t    parser.add_argument(\"--model\", default=app_config.default_model_name, choices=whisper_models, \\\n\t                        help=\"name of the Whisper model to use\") # medium\n\t    parser.add_argument(\"--model_dir\", type=str, default=app_config.model_dir, \\\n\t                        help=\"the path to save model files; uses ~/.cache/whisper by default\")\n\t    parser.add_argument(\"--device\", default=app_config.device, \\\n\t                        help=\"device to use for PyTorch inference\")\n\t    parser.add_argument(\"--output_dir\", \"-o\", type=str, default=output_dir, \\\n", "                        help=\"directory to save the outputs\")\n\t    parser.add_argument(\"--verbose\", type=str2bool, default=app_config.verbose, \\\n\t                        help=\"whether to print out the progress and debug messages\")\n\t    parser.add_argument(\"--whisper_implementation\", type=str, default=default_whisper_implementation, choices=[\"whisper\", \"faster-whisper\"],\\\n\t                        help=\"the Whisper implementation to use\")\n\t    parser.add_argument(\"--task\", type=str, default=app_config.task, choices=[\"transcribe\", \"translate\"], \\\n\t                        help=\"whether to perform X->X speech recognition ('transcribe') or X->English translation ('translate')\")\n\t    parser.add_argument(\"--language\", type=str, default=app_config.language, choices=sorted(get_language_names()), \\\n\t                        help=\"language spoken in the audio, specify None to perform language detection\")\n\t    parser.add_argument(\"--vad\", type=str, default=app_config.default_vad, choices=[\"none\", \"silero-vad\", \"silero-vad-skip-gaps\", \"silero-vad-expand-into-gaps\", \"periodic-vad\"], \\\n", "                        help=\"The voice activity detection algorithm to use\") # silero-vad\n\t    parser.add_argument(\"--vad_initial_prompt_mode\", type=str, default=app_config.vad_initial_prompt_mode, choices=[\"prepend_all_segments\", \"prepend_first_segment\"], \\\n\t                        help=\"Whether or not to prepend the initial prompt to each VAD segment (prepend_all_segments), or just the first segment (prepend_first_segment)\") # prepend_first_segment\n\t    parser.add_argument(\"--vad_merge_window\", type=optional_float, default=app_config.vad_merge_window, \\\n\t                        help=\"The window size (in seconds) to merge voice segments\")\n\t    parser.add_argument(\"--vad_max_merge_size\", type=optional_float, default=app_config.vad_max_merge_size,\\\n\t                         help=\"The maximum size (in seconds) of a voice segment\")\n\t    parser.add_argument(\"--vad_padding\", type=optional_float, default=app_config.vad_padding, \\\n\t                        help=\"The padding (in seconds) to add to each voice segment\")\n\t    parser.add_argument(\"--vad_prompt_window\", type=optional_float, default=app_config.vad_prompt_window, \\\n", "                        help=\"The window size of the prompt to pass to Whisper\")\n\t    parser.add_argument(\"--vad_cpu_cores\", type=int, default=app_config.vad_cpu_cores, \\\n\t                        help=\"The number of CPU cores to use for VAD pre-processing.\") # 1\n\t    parser.add_argument(\"--vad_parallel_devices\", type=str, default=app_config.vad_parallel_devices, \\\n\t                        help=\"A commma delimited list of CUDA devices to use for parallel processing. If None, disable parallel processing.\") # \"\"\n\t    parser.add_argument(\"--auto_parallel\", type=bool, default=app_config.auto_parallel, \\\n\t                        help=\"True to use all available GPUs and CPU cores for processing. Use vad_cpu_cores/vad_parallel_devices to specify the number of CPU cores/GPUs to use.\") # False\n\t    parser.add_argument(\"--temperature\", type=float, default=app_config.temperature, \\\n\t                        help=\"temperature to use for sampling\")\n\t    parser.add_argument(\"--best_of\", type=optional_int, default=app_config.best_of, \\\n", "                        help=\"number of candidates when sampling with non-zero temperature\")\n\t    parser.add_argument(\"--beam_size\", type=optional_int, default=app_config.beam_size, \\\n\t                        help=\"number of beams in beam search, only applicable when temperature is zero\")\n\t    parser.add_argument(\"--patience\", type=float, default=app_config.patience, \\\n\t                        help=\"optional patience value to use in beam decoding, as in https://arxiv.org/abs/2204.05424, the default (1.0) is equivalent to conventional beam search\")\n\t    parser.add_argument(\"--length_penalty\", type=float, default=app_config.length_penalty, \\\n\t                        help=\"optional token length penalty coefficient (alpha) as in https://arxiv.org/abs/1609.08144, uses simple lengt normalization by default\")\n\t    parser.add_argument(\"--suppress_tokens\", type=str, default=app_config.suppress_tokens, \\\n\t                        help=\"comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations\")\n\t    parser.add_argument(\"--initial_prompt\", type=str, default=app_config.initial_prompt, \\\n", "                        help=\"optional text to provide as a prompt for the first window.\")\n\t    parser.add_argument(\"--condition_on_previous_text\", type=str2bool, default=app_config.condition_on_previous_text, \\\n\t                        help=\"if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop\")\n\t    parser.add_argument(\"--fp16\", type=str2bool, default=app_config.fp16, \\\n\t                        help=\"whether to perform inference in fp16; True by default\")\n\t    parser.add_argument(\"--compute_type\", type=str, default=app_config.compute_type, choices=[\"default\", \"auto\", \"int8\", \"int8_float16\", \"int16\", \"float16\", \"float32\"], \\\n\t                        help=\"the compute type to use for inference\")\n\t    parser.add_argument(\"--temperature_increment_on_fallback\", type=optional_float, default=app_config.temperature_increment_on_fallback, \\\n\t                        help=\"temperature to increase when falling back when the decoding fails to meet either of the thresholds below\")\n\t    parser.add_argument(\"--compression_ratio_threshold\", type=optional_float, default=app_config.compression_ratio_threshold, \\\n", "                        help=\"if the gzip compression ratio is higher than this value, treat the decoding as failed\")\n\t    parser.add_argument(\"--logprob_threshold\", type=optional_float, default=app_config.logprob_threshold, \\\n\t                        help=\"if the average log probability is lower than this value, treat the decoding as failed\")\n\t    parser.add_argument(\"--no_speech_threshold\", type=optional_float, default=app_config.no_speech_threshold, \\\n\t                        help=\"if the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence\")\n\t    args = parser.parse_args().__dict__\n\t    model_name: str = args.pop(\"model\")\n\t    model_dir: str = args.pop(\"model_dir\")\n\t    output_dir: str = args.pop(\"output_dir\")\n\t    device: str = args.pop(\"device\")\n", "    os.makedirs(output_dir, exist_ok=True)\n\t    whisper_implementation = args.pop(\"whisper_implementation\")\n\t    print(f\"Using {whisper_implementation} for Whisper\")\n\t    if model_name.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n\t        warnings.warn(f\"{model_name} is an English-only model but receipted '{args['language']}'; using English instead.\")\n\t        args[\"language\"] = \"en\"\n\t    temperature = args.pop(\"temperature\")\n\t    temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n\t    if temperature_increment_on_fallback is not None:\n\t        temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n", "    else:\n\t        temperature = [temperature]\n\t    vad = args.pop(\"vad\")\n\t    vad_initial_prompt_mode = args.pop(\"vad_initial_prompt_mode\")\n\t    vad_merge_window = args.pop(\"vad_merge_window\")\n\t    vad_max_merge_size = args.pop(\"vad_max_merge_size\")\n\t    vad_padding = args.pop(\"vad_padding\")\n\t    vad_prompt_window = args.pop(\"vad_prompt_window\")\n\t    vad_cpu_cores = args.pop(\"vad_cpu_cores\")\n\t    auto_parallel = args.pop(\"auto_parallel\")\n", "    compute_type = args.pop(\"compute_type\")\n\t    transcriber = WhisperTranscriber(delete_uploaded_files=False, vad_cpu_cores=vad_cpu_cores, app_config=app_config)\n\t    transcriber.set_parallel_devices(args.pop(\"vad_parallel_devices\"))\n\t    transcriber.set_auto_parallel(auto_parallel)\n\t    model = create_whisper_container(whisper_implementation=whisper_implementation, model_name=model_name, \n\t                                     device=device, compute_type=compute_type, download_root=model_dir, models=app_config.models)\n\t    if (transcriber._has_parallel_devices()):\n\t        print(\"Using parallel devices:\", transcriber.parallel_device_list)\n\t    for audio_path in args.pop(\"audio\"):\n\t        sources = []\n", "        # Detect URL and download the audio\n\t        if (uri_validator(audio_path)):\n\t            # Download from YouTube/URL directly\n\t            for source_path in  download_url(audio_path, maxDuration=-1, destinationDirectory=output_dir, playlistItems=None):\n\t                source_name = os.path.basename(source_path)\n\t                sources.append({ \"path\": source_path, \"name\": source_name })\n\t        else:\n\t            sources.append({ \"path\": audio_path, \"name\": os.path.basename(audio_path) })\n\t        for source in sources:\n\t            source_path = source[\"path\"]\n", "            source_name = source[\"name\"]\n\t            vadOptions = VadOptions(vad, vad_merge_window, vad_max_merge_size, vad_padding, vad_prompt_window, \n\t                                    VadInitialPromptMode.from_string(vad_initial_prompt_mode))\n\t            result = transcriber.transcribe_file(model, source_path, temperature=temperature, vadOptions=vadOptions, **args)\n\t            transcriber.write_result(result, source_name, output_dir)\n\t    transcriber.close()\n\tdef uri_validator(x):\n\t    try:\n\t        result = urlparse(x)\n\t        return all([result.scheme, result.netloc])\n", "    except:\n\t        return False\n\tif __name__ == '__main__':\n\t    cli()"]}
{"filename": "tests/vad_test.py", "chunked_list": ["import pprint\n\timport unittest\n\timport numpy as np\n\timport sys\n\tsys.path.append('../whisper-webui')\n\tfrom src.vad import AbstractTranscription, TranscriptionConfig, VadSileroTranscription\n\tclass TestVad(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n\t        super(TestVad, self).__init__(*args, **kwargs)\n\t        self.transcribe_calls = []\n", "    def test_transcript(self):\n\t        mock = MockVadTranscription()\n\t        self.transcribe_calls.clear()\n\t        result = mock.transcribe(\"mock\", lambda segment : self.transcribe_segments(segment))\n\t        self.assertListEqual(self.transcribe_calls, [ \n\t            [30, 30],\n\t            [100, 100]\n\t        ])\n\t        self.assertListEqual(result['segments'],\n\t            [{'end': 50.0, 'start': 40.0, 'text': 'Hello world '},\n", "            {'end': 120.0, 'start': 110.0, 'text': 'Hello world '}]\n\t        )\n\t    def transcribe_segments(self, segment):\n\t        self.transcribe_calls.append(segment.tolist())\n\t        # Dummy text\n\t        return {\n\t            'text': \"Hello world \",\n\t            'segments': [\n\t                {\n\t                    \"start\": 10.0,\n", "                    \"end\": 20.0,\n\t                    \"text\": \"Hello world \"\n\t                }   \n\t            ],\n\t            'language': \"\"\n\t        }\n\tclass MockVadTranscription(AbstractTranscription):\n\t    def __init__(self):\n\t        super().__init__()\n\t    def get_audio_segment(self, str, start_time: str = None, duration: str = None):\n", "        start_time_seconds = float(start_time.removesuffix(\"s\"))\n\t        duration_seconds = float(duration.removesuffix(\"s\"))\n\t        # For mocking, this just returns a simple numppy array\n\t        return np.array([start_time_seconds, duration_seconds], dtype=np.float64)\n\t    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, duration: float):\n\t        result = []\n\t        result.append( {  'start': 30, 'end': 60 } )\n\t        result.append( {  'start': 100, 'end': 200 } )\n\t        return result\n\tif __name__ == '__main__':\n", "    unittest.main()"]}
{"filename": "tests/segments_test.py", "chunked_list": ["import sys\n\timport unittest\n\tsys.path.append('../whisper-webui')\n\tfrom src.segments import merge_timestamps\n\tclass TestSegments(unittest.TestCase):\n\t    def __init__(self, *args, **kwargs):\n\t        super(TestSegments, self).__init__(*args, **kwargs)\n\t    def test_merge_segments(self):\n\t        segments = [\n\t            {'start': 10.0, 'end': 20.0},\n", "            {'start': 22.0, 'end': 27.0},\n\t            {'start': 31.0, 'end': 35.0},\n\t            {'start': 45.0, 'end': 60.0},\n\t            {'start': 61.0, 'end': 65.0},\n\t            {'start': 68.0, 'end': 98.0},\n\t            {'start': 100.0, 'end': 102.0},\n\t            {'start': 110.0, 'end': 112.0}\n\t        ]\n\t        result = merge_timestamps(segments, merge_window=5, max_merge_size=30, padding_left=1, padding_right=1)\n\t        self.assertListEqual(result, [\n", "            {'start': 9.0, 'end': 36.0},\n\t            {'start': 44.0, 'end': 66.0},\n\t            {'start': 67.0, 'end': 99.0},\n\t            {'start': 99.0, 'end': 103.0},\n\t            {'start': 109.0, 'end': 113.0}\n\t        ])\n\t    def test_overlap_next(self):\n\t        segments = [\n\t            {'start': 5.0, 'end': 39.182},\n\t            {'start': 39.986, 'end': 40.814}\n", "        ]\n\t        result = merge_timestamps(segments, merge_window=5, max_merge_size=30, padding_left=1, padding_right=1)\n\t        self.assertListEqual(result, [\n\t            {'start': 4.0, 'end': 39.584},\n\t            {'start': 39.584, 'end': 41.814}\n\t        ])\n\tif __name__ == '__main__':\n\t    unittest.main()"]}
{"filename": "src/vad.py", "chunked_list": ["from abc import ABC, abstractmethod\n\tfrom collections import Counter, deque\n\timport time\n\tfrom typing import Any, Deque, Iterator, List, Dict\n\tfrom pprint import pprint\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.hooks.subTaskProgressListener import SubTaskProgressListener\n\tfrom src.hooks.whisperProgressHook import create_progress_listener_handle\n\tfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\tfrom src.segments import merge_timestamps\n", "from src.whisper.abstractWhisperContainer import AbstractWhisperCallback\n\t# Workaround for https://github.com/tensorflow/tensorflow/issues/48797\n\ttry:\n\t    import tensorflow as tf\n\texcept ModuleNotFoundError:\n\t    # Error handling\n\t    pass\n\timport torch\n\timport ffmpeg\n\timport numpy as np\n", "from src.utils import format_timestamp\n\tfrom enum import Enum\n\tclass NonSpeechStrategy(Enum):\n\t    \"\"\"\n\t    Ignore non-speech frames segments.\n\t    \"\"\"\n\t    SKIP = 1\n\t    \"\"\"\n\t    Just treat non-speech segments as speech.\n\t    \"\"\"\n", "    CREATE_SEGMENT = 2\n\t    \"\"\"\n\t    Expand speech segments into subsequent non-speech segments.\n\t    \"\"\"\n\t    EXPAND_SEGMENT = 3\n\t# Defaults for Silero\n\tSPEECH_TRESHOLD = 0.3\n\t# Minimum size of segments to process\n\tMIN_SEGMENT_DURATION = 1\n\t# The maximum time for texts from old segments to be used in the next segment \n", "MAX_PROMPT_WINDOW = 0 # seconds (0 = disabled)\n\tPROMPT_NO_SPEECH_PROB = 0.1 # Do not pass the text from segments with a no speech probability higher than this\n\tVAD_MAX_PROCESSING_CHUNK = 60 * 60 # 60 minutes of audio\n\tclass TranscriptionConfig(ABC):\n\t    def __init__(self, non_speech_strategy: NonSpeechStrategy = NonSpeechStrategy.SKIP, \n\t                       segment_padding_left: float = None, segment_padding_right = None, max_silent_period: float = None, \n\t                       max_merge_size: float = None, max_prompt_window: float = None, initial_segment_index = -1):\n\t        self.non_speech_strategy = non_speech_strategy\n\t        self.segment_padding_left = segment_padding_left\n\t        self.segment_padding_right = segment_padding_right\n", "        self.max_silent_period = max_silent_period\n\t        self.max_merge_size = max_merge_size\n\t        self.max_prompt_window = max_prompt_window\n\t        self.initial_segment_index = initial_segment_index\n\tclass PeriodicTranscriptionConfig(TranscriptionConfig):\n\t    def __init__(self, periodic_duration: float, non_speech_strategy: NonSpeechStrategy = NonSpeechStrategy.SKIP, \n\t                       segment_padding_left: float = None, segment_padding_right = None, max_silent_period: float = None, \n\t                       max_merge_size: float = None, max_prompt_window: float = None, initial_segment_index = -1):\n\t        super().__init__(non_speech_strategy, segment_padding_left, segment_padding_right, max_silent_period, max_merge_size, max_prompt_window, initial_segment_index)\n\t        self.periodic_duration = periodic_duration\n", "class AbstractTranscription(ABC):\n\t    def __init__(self, sampling_rate: int = 16000):\n\t        self.sampling_rate = sampling_rate\n\t    def get_audio_segment(self, str, start_time: str = None, duration: str = None):\n\t        return load_audio(str, self.sampling_rate, start_time, duration)\n\t    def is_transcribe_timestamps_fast(self):\n\t        \"\"\"\n\t        Determine if get_transcribe_timestamps is fast enough to not need parallelization.\n\t        \"\"\"\n\t        return False\n", "    @abstractmethod\n\t    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, end_time: float):\n\t        \"\"\"\n\t        Get the start and end timestamps of the sections that should be transcribed by this VAD method.\n\t        Parameters\n\t        ----------\n\t        audio: str\n\t            The audio file.\n\t        config: TranscriptionConfig\n\t            The transcription configuration.\n", "        Returns\n\t        -------\n\t        A list of start and end timestamps, in fractional seconds.\n\t        \"\"\"\n\t        return \n\t    def get_merged_timestamps(self, timestamps: List[Dict[str, Any]], config: TranscriptionConfig, total_duration: float):\n\t        \"\"\"\n\t        Get the start and end timestamps of the sections that should be transcribed by this VAD method,\n\t        after merging the given segments using the specified configuration.\n\t        Parameters\n", "        ----------\n\t        audio: str\n\t            The audio file. \n\t        config: TranscriptionConfig\n\t            The transcription configuration.\n\t        Returns\n\t        -------\n\t        A list of start and end timestamps, in fractional seconds.\n\t        \"\"\"\n\t        merged = merge_timestamps(timestamps, config.max_silent_period, config.max_merge_size, \n", "                                  config.segment_padding_left, config.segment_padding_right)\n\t        if config.non_speech_strategy != NonSpeechStrategy.SKIP:\n\t            # Expand segments to include the gaps between them\n\t            if (config.non_speech_strategy == NonSpeechStrategy.CREATE_SEGMENT):\n\t                # When we have a prompt window, we create speech segments betwen each segment if we exceed the merge size\n\t                merged = self.fill_gaps(merged, total_duration=total_duration, max_expand_size=config.max_merge_size)\n\t            elif config.non_speech_strategy == NonSpeechStrategy.EXPAND_SEGMENT: \n\t                # With no prompt window, it is better to just expand the segments (this effectively passes the prompt to the next segment)\n\t                merged = self.expand_gaps(merged, total_duration=total_duration)\n\t            else:\n", "                raise Exception(\"Unknown non-speech strategy: \" + str(config.non_speech_strategy))\n\t            print(\"Transcribing non-speech:\")\n\t            pprint(merged)\n\t        return merged\n\t    def transcribe(self, audio: str, whisperCallable: AbstractWhisperCallback, config: TranscriptionConfig, \n\t                   progressListener: ProgressListener = None):\n\t        \"\"\"\n\t        Transcribe the given audo file.\n\t        Parameters\n\t        ----------\n", "        audio: str\n\t            The audio file.\n\t        whisperCallable: WhisperCallback\n\t            A callback object to call to transcribe each segment.\n\t        Returns\n\t        -------\n\t        A list of start and end timestamps, in fractional seconds.\n\t        \"\"\"\n\t        try:\n\t            max_audio_duration = self.get_audio_duration(audio, config)\n", "            timestamp_segments = self.get_transcribe_timestamps(audio, config, 0, max_audio_duration)\n\t            # Get speech timestamps from full audio file\n\t            merged = self.get_merged_timestamps(timestamp_segments, config, max_audio_duration)\n\t            # A deque of transcribed segments that is passed to the next segment as a prompt\n\t            prompt_window = deque()\n\t            print(\"Processing timestamps:\")\n\t            pprint(merged)\n\t            result = {\n\t                'text': \"\",\n\t                'segments': [],\n", "                'language': \"\"\n\t            }\n\t            languageCounter = Counter()\n\t            detected_language = None\n\t            segment_index = config.initial_segment_index\n\t            # Calculate progress \n\t            progress_start_offset = merged[0]['start'] if len(merged) > 0 else 0\n\t            progress_total_duration = sum([segment['end'] - segment['start'] for segment in merged])\n\t            # For each time segment, run whisper\n\t            for segment in merged:\n", "                segment_index += 1\n\t                segment_start = segment['start']\n\t                segment_end = segment['end']\n\t                segment_expand_amount = segment.get('expand_amount', 0)\n\t                segment_gap = segment.get('gap', False)\n\t                segment_duration = segment_end - segment_start\n\t                if segment_duration < MIN_SEGMENT_DURATION:\n\t                    continue\n\t                # Audio to run on Whisper\n\t                segment_audio = self.get_audio_segment(audio, start_time = str(segment_start), duration = str(segment_duration))\n", "                # Previous segments to use as a prompt\n\t                segment_prompt = ' '.join([segment['text'] for segment in prompt_window]) if len(prompt_window) > 0 else None\n\t                # Detected language\n\t                detected_language = languageCounter.most_common(1)[0][0] if len(languageCounter) > 0 else None\n\t                print(\"Running whisper from \", format_timestamp(segment_start), \" to \", format_timestamp(segment_end), \", duration: \", \n\t                    segment_duration, \"expanded: \", segment_expand_amount, \"prompt: \", segment_prompt, \"language: \", detected_language)\n\t                perf_start_time = time.perf_counter()\n\t                scaled_progress_listener = SubTaskProgressListener(progressListener, base_task_total=progress_total_duration, \n\t                                                                   sub_task_start=segment_start - progress_start_offset, sub_task_total=segment_duration) \n\t                segment_result = whisperCallable.invoke(segment_audio, segment_index, segment_prompt, detected_language, progress_listener=scaled_progress_listener)\n", "                perf_end_time = time.perf_counter()\n\t                print(\"Whisper took {} seconds\".format(perf_end_time - perf_start_time))\n\t                adjusted_segments = self.adjust_timestamp(segment_result[\"segments\"], adjust_seconds=segment_start, max_source_time=segment_duration)\n\t                # Propagate expand amount to the segments\n\t                if (segment_expand_amount > 0):\n\t                    segment_without_expansion = segment_duration - segment_expand_amount\n\t                    for adjusted_segment in adjusted_segments:\n\t                        adjusted_segment_end = adjusted_segment['end']\n\t                        # Add expand amount if the segment got expanded\n\t                        if (adjusted_segment_end > segment_without_expansion):\n", "                            adjusted_segment[\"expand_amount\"] = adjusted_segment_end - segment_without_expansion\n\t                # Append to output\n\t                result['text'] += segment_result['text']\n\t                result['segments'].extend(adjusted_segments)\n\t                # Increment detected language\n\t                if not segment_gap:\n\t                    languageCounter[segment_result['language']] += 1\n\t                # Update prompt window\n\t                self.__update_prompt_window(prompt_window, adjusted_segments, segment_end, segment_gap, config)\n\t            if detected_language is not None:\n", "                result['language'] = detected_language\n\t        finally:\n\t            # Notify progress listener that we are done\n\t            if progressListener is not None:\n\t                progressListener.on_finished()\n\t        return result\n\t    def get_audio_duration(self, audio: str, config: TranscriptionConfig):\n\t        return get_audio_duration(audio)\n\t    def __update_prompt_window(self, prompt_window: Deque, adjusted_segments: List, segment_end: float, segment_gap: bool, config: TranscriptionConfig):\n\t        if (config.max_prompt_window is not None and config.max_prompt_window > 0):\n", "            # Add segments to the current prompt window (unless it is a speech gap)\n\t            if not segment_gap:\n\t                for segment in adjusted_segments:\n\t                    if segment.get('no_speech_prob', 0) <= PROMPT_NO_SPEECH_PROB:\n\t                        prompt_window.append(segment)\n\t            while (len(prompt_window) > 0):\n\t                first_end_time = prompt_window[0].get('end', 0)\n\t                # Time expanded in the segments should be discounted from the prompt window\n\t                first_expand_time = prompt_window[0].get('expand_amount', 0)\n\t                if (first_end_time - first_expand_time < segment_end - config.max_prompt_window):\n", "                    prompt_window.popleft()\n\t                else:\n\t                    break\n\t    def include_gaps(self, segments: Iterator[dict], min_gap_length: float, total_duration: float):\n\t        result = []\n\t        last_end_time = 0\n\t        for segment in segments:\n\t            segment_start = float(segment['start'])\n\t            segment_end = float(segment['end'])\n\t            if (last_end_time != segment_start):\n", "                delta = segment_start - last_end_time\n\t                if (min_gap_length is None or delta >= min_gap_length):\n\t                    result.append( { 'start': last_end_time, 'end': segment_start, 'gap': True } )\n\t            last_end_time = segment_end\n\t            result.append(segment)\n\t        # Also include total duration if specified\n\t        if (total_duration is not None and last_end_time < total_duration):\n\t            delta = total_duration - segment_start\n\t            if (min_gap_length is None or delta >= min_gap_length):\n\t                result.append( { 'start': last_end_time, 'end': total_duration, 'gap': True } )\n", "        return result\n\t    # Expand the end time of each segment to the start of the next segment\n\t    def expand_gaps(self, segments: List[Dict[str, Any]], total_duration: float):\n\t        result = []\n\t        if len(segments) == 0:\n\t            return result\n\t        # Add gap at the beginning if needed\n\t        if (segments[0]['start'] > 0):\n\t            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\t        for i in range(len(segments) - 1):\n", "            current_segment = segments[i]\n\t            next_segment = segments[i + 1]\n\t            delta = next_segment['start'] - current_segment['end']\n\t            # Expand if the gap actually exists\n\t            if (delta >= 0):\n\t                current_segment = current_segment.copy()\n\t                current_segment['expand_amount'] = delta\n\t                current_segment['end'] = next_segment['start']\n\t            result.append(current_segment)\n\t        # Add last segment\n", "        last_segment = segments[-1]\n\t        result.append(last_segment)\n\t        # Also include total duration if specified\n\t        if (total_duration is not None):\n\t            last_segment = result[-1]\n\t            if (last_segment['end'] < total_duration):\n\t                last_segment = last_segment.copy()\n\t                last_segment['end'] = total_duration\n\t                result[-1] = last_segment\n\t        return result\n", "    def fill_gaps(self, segments: List[Dict[str, Any]], total_duration: float, max_expand_size: float = None):\n\t        result = []\n\t        if len(segments) == 0:\n\t            return result\n\t        # Add gap at the beginning if needed\n\t        if (segments[0]['start'] > 0):\n\t            result.append({ 'start': 0, 'end': segments[0]['start'], 'gap': True } )\n\t        for i in range(len(segments) - 1):\n\t            expanded = False\n\t            current_segment = segments[i]\n", "            next_segment = segments[i + 1]\n\t            delta = next_segment['start'] - current_segment['end']\n\t            if (max_expand_size is not None and delta <= max_expand_size):\n\t                # Just expand the current segment\n\t                current_segment = current_segment.copy()\n\t                current_segment['expand_amount'] = delta\n\t                current_segment['end'] = next_segment['start']\n\t                expanded = True\n\t            result.append(current_segment)\n\t            # Add a gap to the next segment if needed\n", "            if (delta >= 0 and not expanded):\n\t                result.append({ 'start': current_segment['end'], 'end': next_segment['start'], 'gap': True } )\n\t        # Add last segment\n\t        last_segment = segments[-1]\n\t        result.append(last_segment)\n\t        # Also include total duration if specified\n\t        if (total_duration is not None):\n\t            last_segment = result[-1]\n\t            delta = total_duration - last_segment['end']\n\t            if (delta > 0):\n", "                if (max_expand_size is not None and delta <= max_expand_size):\n\t                    # Expand the last segment\n\t                    last_segment = last_segment.copy()\n\t                    last_segment['expand_amount'] = delta\n\t                    last_segment['end'] = total_duration\n\t                    result[-1] = last_segment\n\t                else:\n\t                    result.append({ 'start': last_segment['end'], 'end': total_duration, 'gap': True } )\n\t        return result\n\t    def adjust_timestamp(self, segments: Iterator[dict], adjust_seconds: float, max_source_time: float = None):\n", "        result = []\n\t        for segment in segments:\n\t            segment_start = float(segment['start'])\n\t            segment_end = float(segment['end'])\n\t            # Filter segments?\n\t            if (max_source_time is not None):\n\t                if (segment_start > max_source_time):\n\t                    continue\n\t                segment_end = min(max_source_time, segment_end)\n\t                new_segment = segment.copy()\n", "            # Add to start and end\n\t            new_segment['start'] = segment_start + adjust_seconds\n\t            new_segment['end'] = segment_end + adjust_seconds\n\t            result.append(new_segment)\n\t        return result\n\t    def multiply_timestamps(self, timestamps: List[Dict[str, Any]], factor: float):\n\t        result = []\n\t        for entry in timestamps:\n\t            start = entry['start']\n\t            end = entry['end']\n", "            result.append({\n\t                'start': start * factor,\n\t                'end': end * factor\n\t            })\n\t        return result\n\tclass VadSileroTranscription(AbstractTranscription):\n\t    def __init__(self, sampling_rate: int = 16000, cache: ModelCache = None):\n\t        super().__init__(sampling_rate=sampling_rate)\n\t        self.model = None\n\t        self.cache = cache\n", "        self._initialize_model()\n\t    def _initialize_model(self):\n\t        if (self.cache is not None):\n\t            model_key = \"VadSileroTranscription\"\n\t            self.model, self.get_speech_timestamps = self.cache.get(model_key, self._create_model)\n\t            print(\"Loaded Silerio model from cache.\")\n\t        else:\n\t            self.model, self.get_speech_timestamps = self._create_model()\n\t            print(\"Created Silerio model\")\n\t    def _create_model(self):\n", "        # model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n\t        model, utils = torch.hub.load(repo_or_dir='models/silero-vad', model='silero_vad',source=\"local\")\n\t        # Silero does not benefit from multi-threading\n\t        torch.set_num_threads(1) # JIT\n\t        (get_speech_timestamps, _, _, _, _) = utils\n\t        return model, get_speech_timestamps\n\t    def get_transcribe_timestamps(self, audio: str, config: TranscriptionConfig, start_time: float, end_time: float):\n\t        result = []\n\t        print(\"Getting timestamps from audio file: {}, start: {}, duration: {}\".format(audio, start_time, end_time))\n\t        perf_start_time = time.perf_counter()\n", "        # Divide procesisng of audio into chunks\n\t        chunk_start = start_time\n\t        while (chunk_start < end_time):\n\t            chunk_duration = min(end_time - chunk_start, VAD_MAX_PROCESSING_CHUNK)\n\t            print(\"Processing VAD in chunk from {} to {}\".format(format_timestamp(chunk_start), format_timestamp(chunk_start + chunk_duration)))\n\t            wav = self.get_audio_segment(audio, str(chunk_start), str(chunk_duration))\n\t            sample_timestamps = self.get_speech_timestamps(wav, self.model, sampling_rate=self.sampling_rate, threshold=SPEECH_TRESHOLD)\n\t            seconds_timestamps = self.multiply_timestamps(sample_timestamps, factor=1 / self.sampling_rate) \n\t            adjusted = self.adjust_timestamp(seconds_timestamps, adjust_seconds=chunk_start, max_source_time=chunk_start + chunk_duration)\n\t            #pprint(adjusted)\n", "            result.extend(adjusted)\n\t            chunk_start += chunk_duration\n\t        perf_end_time = time.perf_counter()\n\t        print(\"VAD processing took {} seconds\".format(perf_end_time - perf_start_time))\n\t        return result\n\t    def __getstate__(self):\n\t        # We only need the sampling rate\n\t        return { 'sampling_rate': self.sampling_rate }\n\t    def __setstate__(self, state):\n\t        self.sampling_rate = state['sampling_rate']\n", "        self.model = None\n\t        # Use the global cache\n\t        self.cache = GLOBAL_MODEL_CACHE\n\t        self._initialize_model()\n\t# A very simple VAD that just marks every N seconds as speech\n\tclass VadPeriodicTranscription(AbstractTranscription):\n\t    def __init__(self, sampling_rate: int = 16000):\n\t        super().__init__(sampling_rate=sampling_rate)\n\t    def is_transcribe_timestamps_fast(self):\n\t        # This is a very fast VAD - no need to parallelize it\n", "        return True\n\t    def get_transcribe_timestamps(self, audio: str, config: PeriodicTranscriptionConfig, start_time: float, end_time: float):\n\t        result = []\n\t        # Generate a timestamp every N seconds\n\t        start_timestamp = start_time\n\t        while (start_timestamp < end_time):\n\t            end_timestamp = min(start_timestamp + config.periodic_duration, end_time)\n\t            segment_duration = end_timestamp - start_timestamp\n\t            # Minimum duration is 1 second\n\t            if (segment_duration >= 1):\n", "                result.append( {  'start': start_timestamp, 'end': end_timestamp } )\n\t            start_timestamp = end_timestamp\n\t        return result\n\tdef get_audio_duration(file: str):\n\t    return float(ffmpeg.probe(file)[\"format\"][\"duration\"])\n\tdef load_audio(file: str, sample_rate: int = 16000, \n\t               start_time: str = None, duration: str = None):\n\t    \"\"\"\n\t    Open an audio file and read as mono waveform, resampling as necessary\n\t    Parameters\n", "    ----------\n\t    file: str\n\t        The audio file to open\n\t    sr: int\n\t        The sample rate to resample the audio if necessary\n\t    start_time: str\n\t        The start time, using the standard FFMPEG time duration syntax, or None to disable.\n\t    duration: str\n\t        The duration, using the standard FFMPEG time duration syntax, or None to disable.\n\t    Returns\n", "    -------\n\t    A NumPy array containing the audio waveform, in float32 dtype.\n\t    \"\"\"\n\t    try:\n\t        inputArgs = {'threads': 0}\n\t        if (start_time is not None):\n\t            inputArgs['ss'] = start_time\n\t        if (duration is not None):\n\t            inputArgs['t'] = duration\n\t        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n", "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n\t        out, _ = (\n\t            ffmpeg.input(file, **inputArgs)\n\t            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sample_rate)\n\t            .run(cmd=\"ffmpeg\", capture_stdout=True, capture_stderr=True)\n\t        )\n\t    except ffmpeg.Error as e:\n\t        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\")\n\t    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0"]}
{"filename": "src/segments.py", "chunked_list": ["from typing import Any, Dict, List\n\timport copy\n\tdef merge_timestamps(timestamps: List[Dict[str, Any]], merge_window: float = 5, max_merge_size: float = 30, padding_left: float = 1, padding_right: float = 1):\n\t    result = []\n\t    if len(timestamps) == 0:\n\t        return result\n\t    if max_merge_size is None:\n\t        return timestamps\n\t    if padding_left is None:\n\t        padding_left = 0\n", "    if padding_right is None:\n\t        padding_right = 0\n\t    processed_time = 0\n\t    current_segment = None\n\t    for i in range(len(timestamps)):\n\t        next_segment = timestamps[i]\n\t        delta = next_segment['start'] - processed_time\n\t        # Note that segments can still be longer than the max merge size, they just won't be merged in that case\n\t        if current_segment is None or (merge_window is not None and delta > merge_window) \\\n\t                 or next_segment['end'] - current_segment['start'] > max_merge_size:\n", "            # Finish the current segment\n\t            if current_segment is not None:\n\t                # Add right padding\n\t                finish_padding = min(padding_right, delta / 2) if delta < padding_left + padding_right else padding_right\n\t                current_segment['end'] += finish_padding\n\t                delta -= finish_padding\n\t                result.append(current_segment)\n\t            # Start a new segment\n\t            current_segment = copy.deepcopy(next_segment)\n\t            # Pad the segment\n", "            current_segment['start'] = current_segment['start'] - min(padding_left, delta)\n\t            processed_time = current_segment['end']\n\t        else:\n\t            # Merge the segment\n\t            current_segment['end'] = next_segment['end']\n\t            processed_time = current_segment['end']\n\t    # Add the last segment\n\t    if current_segment is not None:\n\t        current_segment['end'] += padding_right\n\t        result.append(current_segment)\n", "    return result"]}
{"filename": "src/vadParallel.py", "chunked_list": ["import multiprocessing\n\tfrom queue import Empty\n\timport threading\n\timport time\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.vad import AbstractTranscription, TranscriptionConfig, get_audio_duration\n\tfrom multiprocessing import Pool, Queue\n\tfrom typing import Any, Dict, List, Union\n\timport os\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback\n", "class _ProgressListenerToQueue(ProgressListener):\n\t    def __init__(self, progress_queue: Queue):\n\t        self.progress_queue = progress_queue\n\t        self.progress_total = 0\n\t        self.prev_progress = 0\n\t    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t        delta = current - self.prev_progress\n\t        self.prev_progress = current\n\t        self.progress_total = total\n\t        self.progress_queue.put(delta)\n", "    def on_finished(self):\n\t        if self.progress_total > self.prev_progress:\n\t            delta = self.progress_total - self.prev_progress\n\t            self.progress_queue.put(delta)\n\t            self.prev_progress = self.progress_total\n\tclass ParallelContext:\n\t    def __init__(self, num_processes: int = None, auto_cleanup_timeout_seconds: float = None):\n\t        self.num_processes = num_processes\n\t        self.auto_cleanup_timeout_seconds = auto_cleanup_timeout_seconds\n\t        self.lock = threading.Lock()\n", "        self.ref_count = 0\n\t        self.pool = None\n\t        self.cleanup_timer = None\n\t    def get_pool(self):\n\t        # Initialize pool lazily\n\t        if (self.pool is None):\n\t            context = multiprocessing.get_context('spawn')\n\t            self.pool = context.Pool(self.num_processes)\n\t        self.ref_count = self.ref_count + 1\n\t        if (self.auto_cleanup_timeout_seconds is not None):\n", "            self._stop_auto_cleanup()\n\t        return self.pool\n\t    def return_pool(self, pool):\n\t        if (self.pool == pool and self.ref_count > 0):\n\t            self.ref_count = self.ref_count - 1\n\t            if (self.ref_count == 0):\n\t                if (self.auto_cleanup_timeout_seconds is not None):\n\t                    self._start_auto_cleanup()\n\t    def _start_auto_cleanup(self):\n\t        if (self.cleanup_timer is not None):\n", "            self.cleanup_timer.cancel()\n\t        self.cleanup_timer = threading.Timer(self.auto_cleanup_timeout_seconds, self._execute_cleanup)\n\t        self.cleanup_timer.start()\n\t        print(\"Started auto cleanup of pool in \" + str(self.auto_cleanup_timeout_seconds) + \" seconds\")\n\t    def _stop_auto_cleanup(self):\n\t        if (self.cleanup_timer is not None):\n\t            self.cleanup_timer.cancel()\n\t            self.cleanup_timer = None\n\t            print(\"Stopped auto cleanup of pool\")\n\t    def _execute_cleanup(self):\n", "        print(\"Executing cleanup of pool\")\n\t        if (self.ref_count == 0):\n\t            self.close()\n\t    def close(self):\n\t        self._stop_auto_cleanup()\n\t        if (self.pool is not None):\n\t            print(\"Closing pool of \" + str(self.num_processes) + \" processes\")\n\t            self.pool.close()\n\t            self.pool.join()\n\t        self.pool = None\n", "class ParallelTranscriptionConfig(TranscriptionConfig):\n\t    def __init__(self, device_id: str, override_timestamps, initial_segment_index, copy: TranscriptionConfig = None):\n\t        super().__init__(copy.non_speech_strategy, copy.segment_padding_left, copy.segment_padding_right, copy.max_silent_period, copy.max_merge_size, copy.max_prompt_window, initial_segment_index)\n\t        self.device_id = device_id\n\t        self.override_timestamps = override_timestamps\n\tclass ParallelTranscription(AbstractTranscription):\n\t    # Silero VAD typically takes about 3 seconds per minute, so there's no need to split the chunks \n\t    # into smaller segments than 2 minute (min 6 seconds per CPU core)\n\t    MIN_CPU_CHUNK_SIZE_SECONDS = 2 * 60\n\t    def __init__(self, sampling_rate: int = 16000):\n", "        super().__init__(sampling_rate=sampling_rate)\n\t    def transcribe_parallel(self, transcription: AbstractTranscription, audio: str, whisperCallable: AbstractWhisperCallback, config: TranscriptionConfig, \n\t                            cpu_device_count: int, gpu_devices: List[str], cpu_parallel_context: ParallelContext = None, gpu_parallel_context: ParallelContext = None, \n\t                            progress_listener: ProgressListener = None):\n\t        total_duration = get_audio_duration(audio)\n\t        # First, get the timestamps for the original audio\n\t        if (cpu_device_count > 1 and not transcription.is_transcribe_timestamps_fast()):\n\t            merged = self._get_merged_timestamps_parallel(transcription, audio, config, total_duration, cpu_device_count, cpu_parallel_context)\n\t        else:\n\t            timestamp_segments = transcription.get_transcribe_timestamps(audio, config, 0, total_duration)\n", "            merged = transcription.get_merged_timestamps(timestamp_segments, config, total_duration)\n\t        # We must make sure the whisper model is downloaded\n\t        if (len(gpu_devices) > 1):\n\t            whisperCallable.model_container.ensure_downloaded()\n\t        # Split into a list for each device\n\t        # TODO: Split by time instead of by number of chunks\n\t        merged_split = list(self._split(merged, len(gpu_devices)))\n\t        # Parameters that will be passed to the transcribe function\n\t        parameters = []\n\t        segment_index = config.initial_segment_index\n", "        processing_manager = multiprocessing.Manager()\n\t        progress_queue = processing_manager.Queue()\n\t        for i in range(len(gpu_devices)):\n\t            # Note that device_segment_list can be empty. But we will still create a process for it,\n\t            # as otherwise we run the risk of assigning the same device to multiple processes.\n\t            device_segment_list = list(merged_split[i]) if i < len(merged_split) else []\n\t            device_id = gpu_devices[i]\n\t            print(\"Device \" + str(device_id) + \" (index \" + str(i) + \") has \" + str(len(device_segment_list)) + \" segments\")\n\t            # Create a new config with the given device ID\n\t            device_config = ParallelTranscriptionConfig(device_id, device_segment_list, segment_index, config)\n", "            segment_index += len(device_segment_list)\n\t            progress_listener_to_queue = _ProgressListenerToQueue(progress_queue)\n\t            parameters.append([audio, whisperCallable, device_config, progress_listener_to_queue]);\n\t        merged = {\n\t            'text': '',\n\t            'segments': [],\n\t            'language': None\n\t        }\n\t        created_context = False\n\t        perf_start_gpu = time.perf_counter()\n", "        # Spawn a separate process for each device\n\t        try:\n\t            if (gpu_parallel_context is None):\n\t                gpu_parallel_context = ParallelContext(len(gpu_devices))\n\t                created_context = True\n\t            # Get a pool of processes\n\t            pool = gpu_parallel_context.get_pool()\n\t            # Run the transcription in parallel\n\t            results_async = pool.starmap_async(self.transcribe, parameters)\n\t            total_progress = 0\n", "            while not results_async.ready():\n\t                try:\n\t                    delta = progress_queue.get(timeout=5)  # Set a timeout of 5 seconds\n\t                except Empty:\n\t                    continue\n\t                total_progress += delta\n\t                if progress_listener is not None:\n\t                    progress_listener.on_progress(total_progress, total_duration)\n\t            results = results_async.get()\n\t            # Call the finished callback\n", "            if progress_listener is not None:\n\t                progress_listener.on_finished()\n\t            for result in results:\n\t                # Merge the results\n\t                if (result['text'] is not None):\n\t                    merged['text'] += result['text']\n\t                if (result['segments'] is not None):\n\t                    merged['segments'].extend(result['segments'])\n\t                if (result['language'] is not None):\n\t                    merged['language'] = result['language']\n", "        finally:\n\t            # Return the pool to the context\n\t            if (gpu_parallel_context is not None):\n\t                gpu_parallel_context.return_pool(pool)\n\t            # Always close the context if we created it\n\t            if (created_context):\n\t                gpu_parallel_context.close()\n\t        perf_end_gpu = time.perf_counter()\n\t        print(\"Parallel transcription took \" + str(perf_end_gpu - perf_start_gpu) + \" seconds\")\n\t        return merged\n", "    def _get_merged_timestamps_parallel(self, transcription: AbstractTranscription, audio: str, config: TranscriptionConfig, total_duration: float, \n\t                                       cpu_device_count: int, cpu_parallel_context: ParallelContext = None):\n\t        parameters = []\n\t        chunk_size = max(total_duration / cpu_device_count, self.MIN_CPU_CHUNK_SIZE_SECONDS)\n\t        chunk_start = 0\n\t        cpu_device_id = 0\n\t        perf_start_time = time.perf_counter()\n\t        # Create chunks that will be processed on the CPU\n\t        while (chunk_start < total_duration):\n\t            chunk_end = min(chunk_start + chunk_size, total_duration)\n", "            if (chunk_end - chunk_start < 1):\n\t                # No need to process chunks that are less than 1 second\n\t                break\n\t            print(\"Parallel VAD: Executing chunk from \" + str(chunk_start) + \" to \" + \n\t                    str(chunk_end) + \" on CPU device \" + str(cpu_device_id))\n\t            parameters.append([audio, config, chunk_start, chunk_end]);\n\t            cpu_device_id += 1\n\t            chunk_start = chunk_end\n\t        created_context = False\n\t        # Spawn a separate process for each device\n", "        try:\n\t            if (cpu_parallel_context is None):\n\t                cpu_parallel_context = ParallelContext(cpu_device_count)\n\t                created_context = True\n\t            # Get a pool of processes\n\t            pool = cpu_parallel_context.get_pool()\n\t            # Run the transcription in parallel. Note that transcription must be picklable.\n\t            results = pool.starmap(transcription.get_transcribe_timestamps, parameters)\n\t            timestamps = []\n\t            # Flatten the results\n", "            for result in results:\n\t                timestamps.extend(result)\n\t            merged = transcription.get_merged_timestamps(timestamps, config, total_duration)\n\t            perf_end_time = time.perf_counter()\n\t            print(\"Parallel VAD processing took {} seconds\".format(perf_end_time - perf_start_time))\n\t            return merged\n\t        finally:\n\t            # Return the pool to the context\n\t            if (cpu_parallel_context is not None):\n\t                cpu_parallel_context.return_pool(pool)\n", "            # Always close the context if we created it\n\t            if (created_context):\n\t                cpu_parallel_context.close()\n\t    def get_transcribe_timestamps(self, audio: str, config: ParallelTranscriptionConfig, start_time: float, duration: float):\n\t        return []\n\t    def get_merged_timestamps(self,  timestamps: List[Dict[str, Any]], config: ParallelTranscriptionConfig, total_duration: float):\n\t        # Override timestamps that will be processed\n\t        if (config.override_timestamps is not None):\n\t            print(\"(get_merged_timestamps) Using override timestamps of size \" + str(len(config.override_timestamps)))\n\t            return config.override_timestamps\n", "        return super().get_merged_timestamps(timestamps, config, total_duration)\n\t    def transcribe(self, audio: str, whisperCallable: AbstractWhisperCallback, config: ParallelTranscriptionConfig, \n\t                   progressListener: ProgressListener = None):\n\t        # Override device ID the first time\n\t        if (os.environ.get(\"INITIALIZED\", None) is None):\n\t            os.environ[\"INITIALIZED\"] = \"1\"\n\t            # Note that this may be None if the user didn't specify a device. In that case, Whisper will\n\t            # just use the default GPU device.\n\t            if (config.device_id is not None):\n\t                print(\"Using device \" + config.device_id)\n", "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.device_id\n\t        return super().transcribe(audio, whisperCallable, config, progressListener)\n\t    def _split(self, a, n):\n\t        \"\"\"Split a list into n approximately equal parts.\"\"\"\n\t        k, m = divmod(len(a), n)\n\t        return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n"]}
{"filename": "src/description.py", "chunked_list": ["class Description():\n\t    def __init__(self):\n\t        pass\n\tdef get_description():\n\t    description =\"\"\"\n\t        # 标准选项\n\t        要将音频文件转录或翻译，可以复制来自网站的URL（YT-DLP支持的所有[网站](https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md)都可以，包括YouTube）。\n\t        否则，上传音频文件（在文件选择器中选择“所有文件（。）”以选择任何文件类型，包括视频文件）或使用麦克风。\n\t        对于较长的音频文件（> 10分钟），建议在VAD选项中选择Silero VAD（Voice Activity Detector），特别是如果您正在使用`large-v1`模型。请注意，`large-v2`模型更加宽容，但您可能仍然想使用稍高的“VAD - Max Merge Size（s）”（60秒或更长时间）的VAD。\n\t        ## 模型\n", "        选择Whisper将用于转录音频的模型：\n\t        | 尺寸      | 参数 | 仅支持英语的模型 | 多语言模型 | 所需的显存 (VRAM) | 相对速度 |\n\t        |-----------|------------|--------------------|--------------------|---------------|----------------|\n\t        | tiny      | 39 M       | tiny.en            | tiny               | ~1 GB         | ~32x           |\n\t        | base      | 74 M       | base.en            | base               | ~1 GB         | ~16x           |\n\t        | small     | 244 M      | small.en           | small              | ~2 GB         | ~6x            |\n\t        | medium    | 769 M      | medium.en          | medium             | ~5 GB         | ~2x            |\n\t        | large     | 1550 M     | N/A                | large              | ~10 GB        | 1x             |\n\t        | large-v2  | 1550 M     | N/A                | large              | ~10 GB        | 1x             |\n\t        ## 语言\n", "        选择语言，或将其留空以让Whisper自动检测语言。\n\t        请注意，如果所选语言和音频中的语言不同，Whisper可能会开始将音频翻译为所选语言。\n\t        例如，如果音频是英语，但您选择了日语，则模型可能会将音频翻译为日语。\n\t        ## 输入\n\t        “URL（YouTube等）”，“上传文件”或“麦克风输入”选项允许您将音频输入发送到模型。\n\t        ### 多个文件\n\t        请注意，界面只会处理给定的URL或上传的文件（包括麦克风）中的一个，而不是两者都处理。\n\t        但是，您可以通过“上传文件”选项或YouTube的播放列表上传多个文件。然后将依次处理每个音频文件，并将生成的SRT / VTT /转录文本在“下载”部分中提供。\n\t        当处理多个文件时，界面还会生成一个“All_Output” zip文件，其中包含所有文本输出文件。\n\t        ## 任务\n", "        选择任务 - 选择“transcribe”将音频转录为文本，选择“translate”将音频翻译为英文。\n\t        ## 语音活性检测VAD\n\t        使用VAD将提高每个转录行的定时准确性，并防止Whisper陷入无限循环，反复检测相同的句子。缺点是这可能会对文本准确性产生影响，特别是对于出现在音频中的唯一单词或名称。您可以通过增加提示窗口来弥补这一点。\n\t        请注意，Whisper非常适合处理英语，不太容易出现与不良定时和无限循环相关的问题。因此，您可能只需要为其他语言（例如日语）或音频非常长时使用VAD。\n\t        * none\n\t            * 在整个音频输入上运行Whisper\n\t        * silero-vad\n\t            * 使用Silero VAD检测包含语音的部分，并在每个部分上独立运行Whisper。同时，Whisper还将在每个语音部分之间的间隙上运行，方法是将该部分扩展到最大合并大小或在非语音部分上独立运行Whisper。\n\t        * silero-vad-expand-into-gaps\n\t        * 使用Silero VAD检测包含语音的部分，并在每个部分上独立运行Whisper。每个语音部分都将被扩展以覆盖相邻的非语音部分。例如，如果一个持续一分钟的音频文件包含语音部分00:00-00:10（A）和00:30-00:40（B），则第一部分（A）将扩展到00:00-00:30，（B）将扩展到00:30-00:60。\n", "        * silero-vad-skip-gaps\n\t        * 与上述相同，但是根据Silero的结果跳过不包含语音的部分。这样会稍微快一些，但可能会导致对话被跳过。\n\t        * periodic-vad\n\t        * 每隔“VAD - Max Merge Size”秒创建语音部分。这非常快速简单，但可能会将一个句子或单词分成两个部分。\n\t        ## VAD - 合并窗口\n\t        如果设置了该参数，则任何相邻的语音部分，其时间间隔不超过该参数所设置的秒数，将自动合并。\n\t        ## VAD - 最大合并大小（秒）\n\t        如果相邻的语音部分的长度达到该参数所设置的秒数，则禁用它们的合并。\n\t        ## VAD - 填充时间（秒）\n\t        每个语音部分的开头和结尾增加的秒数（浮点数）。将其设置为大于零的数字可确保Whisper更有可能在语音部分开头正确转录句子。然而，这也增加了Whisper给每个转录行分配错误时间戳的概率。默认值为1秒。\n", "        ## VAD - 提示窗口（秒）\n\t        如果语音部分开始的时间和上一行结束的时间间隔不超过该参数所设置的秒数，则检测到的行的文本将作为提示包含在下一个语音部分中。例如，如果一行结束于10:00，下一个语音部分从10:04开始，则如果提示窗口为4秒或更长时间（10:04-10:00 = 4秒），将包含该行的文本。\n\t        请注意，如果使用silero-vad或silero-vad-expand-into-gaps，则不会将检测到的行包含在语音部分之间的间隙中的提示中。\n\t        # 命令行选项\n\t        app.py和cli.py都接受命令行选项，例如启用多个CPU / GPU核心上的并行执行，设置默认模型名称/VAD等。有关更多信息，请查阅根文件夹中的README文件。\n\t        # 其他选项\n\t        除上述选项外，还有一个“完整”的选项界面，允许您设置Whisper模型中所有可用的选项。这些选项如下：\n\t        ## 初始提示\n\t        提供可选文本作为前30秒窗口的提示。Whisper将尝试使用此文本作为转录的起点，但您也可以发挥创意，为转录输出指定样式或格式。\n\t        例如，如果您使用提示“hello how is it going always use lowercase no punctuation goodbye one two three start stop i you me they”，则Whisper会倾向于输出小写字母和无标点符号，并且可能会倾向于更频繁地输出提示中的单词。\n", "        ## 温度\n\t        进行采样时使用的温度。默认为0（零）。较高的温度会产生更多随机输出，而较低的温度则更加确定性。\n\t        ## 最佳结果 - 非零温度\n\t        使用非零温度进行采样时要从中采样的候选项数量。默认值为5。\n\t        ## 束搜索大小 - 零温度\n\t       使用零温度进行采样时在束搜索中使用的束数。默认值为5。\n\t        ## 耐心 - 零温度\n\t        在零温度下进行采样时，在束搜索中使用的耐心值。如https://arxiv.org/abs/2204.05424 所述， 默认值（1.0）等效于传统的束搜索。\n\t        ## 长度惩罚 - 任何温度\n\t        在任何温度下进行采样时使用的标记长度惩罚系数（alpha）。如https://arxiv.org/abs/1609.08144 所述，默认情况下使用简单的长度归一化。\n", "        ## 抑制标记 - 逗号分隔的标记ID列表\n\t        在采样过程中抑制的标记ID的逗号分隔列表。默认值“-1”将抑制除常见标点符号以外的大多数特殊字符。\n\t        ## 以前文为条件\n\t        如果为True，则将模型的先前输出提供为下一个窗口的提示。禁用此选项可能会导致窗口之间的文本不一致，但模型不太容易陷入失败循环中。\n\t        ## FP16\n\t        是否在fp16下执行推理。默认情况下为True。\n\t        ## 回退时温度增量\n\t        当解码未达到以下任一阈值时，降级时要增加的温度。默认为0.2。\n\t        ## 压缩比阈值\n\t        如果gzip压缩比高于此值，则将解码视为失败。默认为2.4。\n", "        ## Logprob阈值\n\t        如果平均对数概率低于此值，则将解码视为失败。默认为-1.0。\n\t        ## 无语音阈值\n\t        如果“<|nospeech|>”标记的概率高于此值且由于“logprob_threshold”而解码失败，则将该段视为静音。默认为0.6。\n\t    \"\"\"\n\t    return description \n"]}
{"filename": "src/config.py", "chunked_list": ["from enum import Enum\n\timport urllib\n\timport os\n\tfrom typing import List\n\tfrom urllib.parse import urlparse\n\timport json5\n\timport torch\n\tfrom tqdm import tqdm\n\tclass ModelConfig:\n\t    def __init__(self, name: str, url: str, path: str = None, type: str = \"whisper\"):\n", "        \"\"\"\n\t        Initialize a model configuration.\n\t        name: Name of the model\n\t        url: URL to download the model from\n\t        path: Path to the model file. If not set, the model will be downloaded from the URL.\n\t        type: Type of model. Can be whisper or huggingface.\n\t        \"\"\"\n\t        self.name = name\n\t        self.url = url\n\t        self.path = path\n", "        self.type = type\n\tclass VadInitialPromptMode(Enum):\n\t    PREPEND_ALL_SEGMENTS = 1\n\t    PREPREND_FIRST_SEGMENT = 2\n\t    @staticmethod\n\t    def from_string(s: str):\n\t        normalized = s.lower() if s is not None else None\n\t        if normalized == \"prepend_all_segments\":\n\t            return VadInitialPromptMode.PREPEND_ALL_SEGMENTS\n\t        elif normalized == \"prepend_first_segment\":\n", "            return VadInitialPromptMode.PREPREND_FIRST_SEGMENT\n\t        else:\n\t            raise ValueError(f\"Invalid value for VadInitialPromptMode: {s}\")\n\tclass ApplicationConfig:\n\t    def __init__(self, models: List[ModelConfig] = [], input_audio_max_duration: int = 600, \n\t                 share: bool = False, server_name: str = None, server_port: int = 7860, \n\t                 queue_concurrency_count: int = 1, delete_uploaded_files: bool = True,\n\t                 whisper_implementation: str = \"whisper\",\n\t                 default_model_name: str = \"medium\", default_vad: str = \"silero-vad\", \n\t                 vad_parallel_devices: str = \"\", vad_cpu_cores: int = 1, vad_process_timeout: int = 1800, \n", "                 auto_parallel: bool = False, output_dir: str = None,\n\t                 model_dir: str = None, device: str = None, \n\t                 verbose: bool = True, task: str = \"transcribe\", language: str = None,\n\t                 vad_initial_prompt_mode: str = \"prepend_first_segment \", \n\t                 vad_merge_window: float = 5, vad_max_merge_size: float = 30,\n\t                 vad_padding: float = 1, vad_prompt_window: float = 3,\n\t                 temperature: float = 0, best_of: int = 5, beam_size: int = 5,\n\t                 patience: float = None, length_penalty: float = None,\n\t                 suppress_tokens: str = \"-1\", initial_prompt: str = None,\n\t                 condition_on_previous_text: bool = True, fp16: bool = True,\n", "                 compute_type: str = \"float16\", \n\t                 temperature_increment_on_fallback: float = 0.2, compression_ratio_threshold: float = 2.4,\n\t                 logprob_threshold: float = -1.0, no_speech_threshold: float = 0.6):\n\t        self.models = models\n\t        # WebUI settings\n\t        self.input_audio_max_duration = input_audio_max_duration\n\t        self.share = share\n\t        self.server_name = server_name\n\t        self.server_port = server_port\n\t        self.queue_concurrency_count = queue_concurrency_count\n", "        self.delete_uploaded_files = delete_uploaded_files\n\t        self.whisper_implementation = whisper_implementation\n\t        self.default_model_name = default_model_name\n\t        self.default_vad = default_vad\n\t        self.vad_parallel_devices = vad_parallel_devices\n\t        self.vad_cpu_cores = vad_cpu_cores\n\t        self.vad_process_timeout = vad_process_timeout\n\t        self.auto_parallel = auto_parallel\n\t        self.output_dir = output_dir\n\t        self.model_dir = model_dir\n", "        self.device = device\n\t        self.verbose = verbose\n\t        self.task = task\n\t        self.language = language\n\t        self.vad_initial_prompt_mode = vad_initial_prompt_mode\n\t        self.vad_merge_window = vad_merge_window\n\t        self.vad_max_merge_size = vad_max_merge_size\n\t        self.vad_padding = vad_padding\n\t        self.vad_prompt_window = vad_prompt_window\n\t        self.temperature = temperature\n", "        self.best_of = best_of\n\t        self.beam_size = beam_size\n\t        self.patience = patience\n\t        self.length_penalty = length_penalty\n\t        self.suppress_tokens = suppress_tokens\n\t        self.initial_prompt = initial_prompt\n\t        self.condition_on_previous_text = condition_on_previous_text\n\t        self.fp16 = fp16\n\t        self.compute_type = compute_type\n\t        self.temperature_increment_on_fallback = temperature_increment_on_fallback\n", "        self.compression_ratio_threshold = compression_ratio_threshold\n\t        self.logprob_threshold = logprob_threshold\n\t        self.no_speech_threshold = no_speech_threshold\n\t    def get_model_names(self):\n\t        return [ x.name for x in self.models ]\n\t    def update(self, **new_values):\n\t        result = ApplicationConfig(**self.__dict__)\n\t        for key, value in new_values.items():\n\t            setattr(result, key, value)\n\t        return result\n", "    @staticmethod\n\t    def create_default(**kwargs):\n\t        app_config = ApplicationConfig.parse_file(os.environ.get(\"WHISPER_WEBUI_CONFIG\", \"config.json5\"))\n\t        # Update with kwargs\n\t        if len(kwargs) > 0:\n\t            app_config = app_config.update(**kwargs)\n\t        return app_config\n\t    @staticmethod\n\t    def parse_file(config_path: str):\n\t        import json5\n", "        with open(config_path, \"r\") as f:\n\t            # Load using json5\n\t            data = json5.load(f)\n\t            data_models = data.pop(\"models\", [])\n\t            models = [ ModelConfig(**x) for x in data_models ]\n\t            return ApplicationConfig(models, **data)\n"]}
{"filename": "src/__init__.py", "chunked_list": []}
{"filename": "src/utils.py", "chunked_list": ["import textwrap\n\timport unicodedata\n\timport re\n\timport zlib\n\tfrom typing import Iterator, TextIO\n\timport tqdm\n\timport urllib3\n\tdef exact_div(x, y):\n\t    assert x % y == 0\n\t    return x // y\n", "def str2bool(string):\n\t    str2val = {\"True\": True, \"False\": False}\n\t    if string in str2val:\n\t        return str2val[string]\n\t    else:\n\t        raise ValueError(f\"Expected one of {set(str2val.keys())}, got {string}\")\n\tdef optional_int(string):\n\t    return None if string == \"None\" else int(string)\n\tdef optional_float(string):\n\t    return None if string == \"None\" else float(string)\n", "def compression_ratio(text) -> float:\n\t    return len(text) / len(zlib.compress(text.encode(\"utf-8\")))\n\tdef format_timestamp(seconds: float, always_include_hours: bool = False, fractionalSeperator: str = '.'):\n\t    assert seconds >= 0, \"non-negative timestamp expected\"\n\t    milliseconds = round(seconds * 1000.0)\n\t    hours = milliseconds // 3_600_000\n\t    milliseconds -= hours * 3_600_000\n\t    minutes = milliseconds // 60_000\n\t    milliseconds -= minutes * 60_000\n\t    seconds = milliseconds // 1_000\n", "    milliseconds -= seconds * 1_000\n\t    hours_marker = f\"{hours:02d}:\" if always_include_hours or hours > 0 else \"\"\n\t    return f\"{hours_marker}{minutes:02d}:{seconds:02d}{fractionalSeperator}{milliseconds:03d}\"\n\tdef write_txt(transcript: Iterator[dict], file: TextIO):\n\t    for segment in transcript:\n\t        print(segment['text'].strip(), file=file, flush=True)\n\tdef write_vtt(transcript: Iterator[dict], file: TextIO, maxLineWidth=None):\n\t    print(\"WEBVTT\\n\", file=file)\n\t    for segment in transcript:\n\t        text = process_text(segment['text'], maxLineWidth).replace('-->', '->')\n", "        print(\n\t            f\"{format_timestamp(segment['start'])} --> {format_timestamp(segment['end'])}\\n\"\n\t            f\"{text}\\n\",\n\t            file=file,\n\t            flush=True,\n\t        )\n\tdef write_srt(transcript: Iterator[dict], file: TextIO, maxLineWidth=None):\n\t    \"\"\"\n\t    Write a transcript to a file in SRT format.\n\t    Example usage:\n", "        from pathlib import Path\n\t        from whisper.utils import write_srt\n\t        result = transcribe(model, audio_path, temperature=temperature, **args)\n\t        # save SRT\n\t        audio_basename = Path(audio_path).stem\n\t        with open(Path(output_dir) / (audio_basename + \".srt\"), \"w\", encoding=\"utf-8\") as srt:\n\t            write_srt(result[\"segments\"], file=srt)\n\t    \"\"\"\n\t    for i, segment in enumerate(transcript, start=1):\n\t        text = process_text(segment['text'].strip(), maxLineWidth).replace('-->', '->')\n", "        # write srt lines\n\t        print(\n\t            f\"{i}\\n\"\n\t            f\"{format_timestamp(segment['start'], always_include_hours=True, fractionalSeperator=',')} --> \"\n\t            f\"{format_timestamp(segment['end'], always_include_hours=True, fractionalSeperator=',')}\\n\"\n\t            f\"{text}\\n\",\n\t            file=file,\n\t            flush=True,\n\t        )\n\tdef process_text(text: str, maxLineWidth=None):\n", "    if (maxLineWidth is None or maxLineWidth < 0):\n\t        return text\n\t    lines = textwrap.wrap(text, width=maxLineWidth, tabsize=4)\n\t    return '\\n'.join(lines)\n\tdef slugify(value, allow_unicode=False):\n\t    \"\"\"\n\t    Taken from https://github.com/django/django/blob/master/django/utils/text.py\n\t    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n\t    dashes to single dashes. Remove characters that aren't alphanumerics,\n\t    underscores, or hyphens. Convert to lowercase. Also strip leading and\n", "    trailing whitespace, dashes, and underscores.\n\t    \"\"\"\n\t    value = str(value)\n\t    if allow_unicode:\n\t        value = unicodedata.normalize('NFKC', value)\n\t    else:\n\t        value = unicodedata.normalize('NFKD', value).encode('ascii', 'ignore').decode('ascii')\n\t    value = re.sub(r'[^\\w\\s-]', '', value.lower())\n\t    return re.sub(r'[-\\s]+', '-', value).strip('-_')\n\tdef download_file(url: str, destination: str):\n", "        with urllib3.request.urlopen(url) as source, open(destination, \"wb\") as output:\n\t            with tqdm(\n\t                total=int(source.info().get(\"Content-Length\")),\n\t                ncols=80,\n\t                unit=\"iB\",\n\t                unit_scale=True,\n\t                unit_divisor=1024,\n\t            ) as loop:\n\t                while True:\n\t                    buffer = source.read(8192)\n", "                    if not buffer:\n\t                        break\n\t                    output.write(buffer)\n\t                    loop.update(len(buffer))"]}
{"filename": "src/download.py", "chunked_list": ["from tempfile import mkdtemp\n\tfrom typing import List\n\tfrom yt_dlp import YoutubeDL\n\timport yt_dlp\n\tfrom yt_dlp.postprocessor import PostProcessor\n\tclass FilenameCollectorPP(PostProcessor):\n\t    def __init__(self):\n\t        super(FilenameCollectorPP, self).__init__(None)\n\t        self.filenames = []\n\t    def run(self, information):\n", "        self.filenames.append(information[\"filepath\"])\n\t        return [], information\n\tdef download_url(url: str, maxDuration: int = None, destinationDirectory: str = None, playlistItems: str = \"1\") -> List[str]: \n\t    try:\n\t        return _perform_download(url, maxDuration=maxDuration, outputTemplate=None, destinationDirectory=destinationDirectory, playlistItems=playlistItems)\n\t    except yt_dlp.utils.DownloadError as e:\n\t        # In case of an OS error, try again with a different output template\n\t        if e.msg and e.msg.find(\"[Errno 36] File name too long\") >= 0:\n\t            return _perform_download(url, maxDuration=maxDuration, outputTemplate=\"%(title).10s %(id)s.%(ext)s\")\n\t        pass\n", "def _perform_download(url: str, maxDuration: int = None, outputTemplate: str = None, destinationDirectory: str = None, playlistItems: str = \"1\"):\n\t    # Create a temporary directory to store the downloaded files\n\t    if destinationDirectory is None:\n\t        destinationDirectory = mkdtemp()\n\t    ydl_opts = {\n\t        \"format\": \"bestaudio/best\",\n\t        'paths': {\n\t            'home': destinationDirectory\n\t        }\n\t    }\n", "    if (playlistItems):\n\t        ydl_opts['playlist_items'] = playlistItems\n\t    # Add output template if specified\n\t    if outputTemplate:\n\t        ydl_opts['outtmpl'] = outputTemplate\n\t    filename_collector = FilenameCollectorPP()\n\t    with YoutubeDL(ydl_opts) as ydl:\n\t        if maxDuration and maxDuration > 0:\n\t            info = ydl.extract_info(url, download=False)\n\t            entries = \"entries\" in info and info[\"entries\"] or [info]\n", "            total_duration = 0\n\t            # Compute total duration\n\t            for entry in entries:\n\t                total_duration += float(entry[\"duration\"])\n\t            if total_duration >= maxDuration:\n\t                raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=maxDuration, message=\"Video is too long\")\n\t        ydl.add_post_processor(filename_collector)\n\t        ydl.download([url])\n\t    if len(filename_collector.filenames) <= 0:\n\t        raise Exception(\"Cannot download \" + url)\n", "    result = []\n\t    for filename in filename_collector.filenames:\n\t        result.append(filename)\n\t        print(\"Downloaded \" + filename)\n\t    return result \n\tclass ExceededMaximumDuration(Exception):\n\t    def __init__(self, videoDuration, maxDuration, message):\n\t        self.videoDuration = videoDuration\n\t        self.maxDuration = maxDuration\n\t        super().__init__(message)"]}
{"filename": "src/languages.py", "chunked_list": ["class Language():\n\t    def __init__(self, code, name):\n\t        self.code = code\n\t        self.name = name\n\t    def __str__(self):\n\t        return \"Language(code={}, name={})\".format(self.code, self.name)\n\tLANGUAGES = [\n\t    Language('en', 'English'),\n\t    Language('zh', 'Chinese'),   \n\t    Language('de', 'German'),    \n", "    Language('es', 'Spanish'),   \n\t    Language('ru', 'Russian'),   \n\t    Language('ko', 'Korean'),    \n\t    Language('fr', 'French'),    \n\t    Language('ja', 'Japanese'),  \n\t    Language('pt', 'Portuguese'),\n\t    Language('tr', 'Turkish'),   \n\t    Language('pl', 'Polish'),    \n\t    Language('ca', 'Catalan'),   \n\t    Language('nl', 'Dutch'),     \n", "    Language('ar', 'Arabic'),\n\t    Language('sv', 'Swedish'),\n\t    Language('it', 'Italian'),\n\t    Language('id', 'Indonesian'),\n\t    Language('hi', 'Hindi'),\n\t    Language('fi', 'Finnish'),\n\t    Language('vi', 'Vietnamese'),\n\t    Language('he', 'Hebrew'),\n\t    Language('uk', 'Ukrainian'),\n\t    Language('el', 'Greek'),\n", "    Language('ms', 'Malay'),\n\t    Language('cs', 'Czech'),\n\t    Language('ro', 'Romanian'),\n\t    Language('da', 'Danish'),\n\t    Language('hu', 'Hungarian'),\n\t    Language('ta', 'Tamil'),\n\t    Language('no', 'Norwegian'),\n\t    Language('th', 'Thai'),\n\t    Language('ur', 'Urdu'),\n\t    Language('hr', 'Croatian'),\n", "    Language('bg', 'Bulgarian'),\n\t    Language('lt', 'Lithuanian'),\n\t    Language('la', 'Latin'),\n\t    Language('mi', 'Maori'),\n\t    Language('ml', 'Malayalam'),\n\t    Language('cy', 'Welsh'),\n\t    Language('sk', 'Slovak'),\n\t    Language('te', 'Telugu'),\n\t    Language('fa', 'Persian'),\n\t    Language('lv', 'Latvian'),\n", "    Language('bn', 'Bengali'),\n\t    Language('sr', 'Serbian'),\n\t    Language('az', 'Azerbaijani'),\n\t    Language('sl', 'Slovenian'),\n\t    Language('kn', 'Kannada'),\n\t    Language('et', 'Estonian'),\n\t    Language('mk', 'Macedonian'),\n\t    Language('br', 'Breton'),\n\t    Language('eu', 'Basque'),\n\t    Language('is', 'Icelandic'),\n", "    Language('hy', 'Armenian'),\n\t    Language('ne', 'Nepali'),\n\t    Language('mn', 'Mongolian'),\n\t    Language('bs', 'Bosnian'),\n\t    Language('kk', 'Kazakh'),\n\t    Language('sq', 'Albanian'),\n\t    Language('sw', 'Swahili'),\n\t    Language('gl', 'Galician'),\n\t    Language('mr', 'Marathi'),\n\t    Language('pa', 'Punjabi'),\n", "    Language('si', 'Sinhala'),\n\t    Language('km', 'Khmer'),\n\t    Language('sn', 'Shona'),\n\t    Language('yo', 'Yoruba'),\n\t    Language('so', 'Somali'),\n\t    Language('af', 'Afrikaans'),\n\t    Language('oc', 'Occitan'),\n\t    Language('ka', 'Georgian'),\n\t    Language('be', 'Belarusian'),\n\t    Language('tg', 'Tajik'),\n", "    Language('sd', 'Sindhi'),\n\t    Language('gu', 'Gujarati'),\n\t    Language('am', 'Amharic'),\n\t    Language('yi', 'Yiddish'),\n\t    Language('lo', 'Lao'),\n\t    Language('uz', 'Uzbek'),\n\t    Language('fo', 'Faroese'),\n\t    Language('ht', 'Haitian creole'),\n\t    Language('ps', 'Pashto'),\n\t    Language('tk', 'Turkmen'),\n", "    Language('nn', 'Nynorsk'),\n\t    Language('mt', 'Maltese'),\n\t    Language('sa', 'Sanskrit'),\n\t    Language('lb', 'Luxembourgish'),\n\t    Language('my', 'Myanmar'),\n\t    Language('bo', 'Tibetan'),\n\t    Language('tl', 'Tagalog'),\n\t    Language('mg', 'Malagasy'),\n\t    Language('as', 'Assamese'),\n\t    Language('tt', 'Tatar'),\n", "    Language('haw', 'Hawaiian'),\n\t    Language('ln', 'Lingala'),\n\t    Language('ha', 'Hausa'),\n\t    Language('ba', 'Bashkir'),\n\t    Language('jw', 'Javanese'),\n\t    Language('su', 'Sundanese')\n\t]\n\t_TO_LANGUAGE_CODE = {\n\t    **{language.code: language for language in LANGUAGES},\n\t    \"burmese\": \"my\",\n", "    \"valencian\": \"ca\",\n\t    \"flemish\": \"nl\",\n\t    \"haitian\": \"ht\",\n\t    \"letzeburgesch\": \"lb\",\n\t    \"pushto\": \"ps\",\n\t    \"panjabi\": \"pa\",\n\t    \"moldavian\": \"ro\",\n\t    \"moldovan\": \"ro\",\n\t    \"sinhalese\": \"si\",\n\t    \"castilian\": \"es\",\n", "}\n\t_FROM_LANGUAGE_NAME = {\n\t    **{language.name.lower(): language for language in LANGUAGES}\n\t}\n\tdef get_language_from_code(language_code, default=None) -> Language:\n\t    \"\"\"Return the language name from the language code.\"\"\"\n\t    return _TO_LANGUAGE_CODE.get(language_code, default)\n\tdef get_language_from_name(language, default=None) -> Language:\n\t    \"\"\"Return the language code from the language name.\"\"\"\n\t    return _FROM_LANGUAGE_NAME.get(language.lower() if language else None, default)\n", "def get_language_names():\n\t    \"\"\"Return a list of language names.\"\"\"\n\t    return [language.name for language in LANGUAGES]\n\tif __name__ == \"__main__\":\n\t    # Test lookup\n\t    print(get_language_from_code('en'))\n\t    print(get_language_from_name('English'))\n\t    print(get_language_names())"]}
{"filename": "src/modelCache.py", "chunked_list": ["class ModelCache:\n\t    def __init__(self):\n\t        self._cache = dict()\n\t    def get(self, model_key: str, model_factory):\n\t        result = self._cache.get(model_key)\n\t        if result is None:\n\t            result = model_factory()\n\t            self._cache[model_key] = result\n\t        return result\n\t    def clear(self):\n", "        self._cache.clear()\n\t# A global cache of models. This is mainly used by the daemon processes to avoid loading the same model multiple times.\n\tGLOBAL_MODEL_CACHE = ModelCache()"]}
{"filename": "src/source.py", "chunked_list": ["# Gradio seems to truncate files without keeping the extension, so we need to truncate the file prefix ourself \n\timport os\n\timport pathlib\n\tfrom typing import List\n\timport zipfile\n\timport ffmpeg\n\tfrom more_itertools import unzip\n\tfrom src.download import ExceededMaximumDuration, download_url\n\tMAX_FILE_PREFIX_LENGTH = 17\n\tclass AudioSource:\n", "    def __init__(self, source_path, source_name = None, audio_duration = None):\n\t        self.source_path = source_path\n\t        self.source_name = source_name\n\t        self._audio_duration = audio_duration\n\t        # Load source name if not provided\n\t        if (self.source_name is None):\n\t            file_path = pathlib.Path(self.source_path)\n\t            self.source_name = file_path.name\n\t    def get_audio_duration(self):\n\t        if self._audio_duration is None:\n", "            self._audio_duration = float(ffmpeg.probe(self.source_path)[\"format\"][\"duration\"])\n\t        return self._audio_duration\n\t    def get_full_name(self):\n\t        return self.source_name\n\t    def get_short_name(self, max_length: int = MAX_FILE_PREFIX_LENGTH):\n\t        file_path = pathlib.Path(self.source_name)\n\t        short_name = file_path.stem[:max_length] + file_path.suffix\n\t        return short_name\n\t    def __str__(self) -> str:\n\t        return self.source_path\n", "class AudioSourceCollection:\n\t    def __init__(self, sources: List[AudioSource]):\n\t        self.sources = sources\n\t    def __iter__(self):\n\t        return iter(self.sources)\n\tdef get_audio_source_collection(urlData: str, multipleFiles: List, microphoneData: str, input_audio_max_duration: float = -1) -> List[AudioSource]:\n\t    output: List[AudioSource] = []\n\t    if urlData:\n\t        # Download from YouTube. This could also be a playlist or a channel.\n\t        output.extend([ AudioSource(x) for x in download_url(urlData, input_audio_max_duration, playlistItems=None) ])\n", "    else:\n\t        # Add input files\n\t        if (multipleFiles is not None):\n\t            output.extend([ AudioSource(x.name) for x in multipleFiles ])\n\t        if (microphoneData is not None):\n\t            output.append(AudioSource(microphoneData))\n\t    total_duration = 0\n\t    # Calculate total audio length. We do this even if input_audio_max_duration\n\t    # is disabled to ensure that all the audio files are valid.\n\t    for source in output:\n", "        audioDuration = ffmpeg.probe(source.source_path)[\"format\"][\"duration\"]\n\t        total_duration += float(audioDuration)\n\t        # Save audio duration\n\t        source._audio_duration = float(audioDuration)\n\t    # Ensure the total duration of the audio is not too long\n\t    if input_audio_max_duration > 0:\n\t        if float(total_duration) > input_audio_max_duration:\n\t            raise ExceededMaximumDuration(videoDuration=total_duration, maxDuration=input_audio_max_duration, message=\"Video(s) is too long\")\n\t    # Return a list of audio sources\n\t    return output"]}
{"filename": "src/hooks/progressListener.py", "chunked_list": ["from typing import Union\n\tclass ProgressListener:\n\t    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t        self.total = total\n\t    def on_finished(self):\n\t        pass"]}
{"filename": "src/hooks/whisperProgressHook.py", "chunked_list": ["import sys\n\timport threading\n\tfrom typing import List, Union\n\timport tqdm\n\tfrom src.hooks.progressListener import ProgressListener\n\tclass ProgressListenerHandle:\n\t    def __init__(self, listener: ProgressListener):\n\t        self.listener = listener\n\t    def __enter__(self):\n\t        register_thread_local_progress_listener(self.listener)\n", "    def __exit__(self, exc_type, exc_val, exc_tb):\n\t        unregister_thread_local_progress_listener(self.listener)\n\t        if exc_type is None:\n\t            self.listener.on_finished()\n\tclass _CustomProgressBar(tqdm.tqdm):\n\t    def __init__(self, *args, **kwargs):\n\t        super().__init__(*args, **kwargs)\n\t        self._current = self.n  # Set the initial value\n\t    def update(self, n):\n\t        super().update(n)\n", "        # Because the progress bar might be disabled, we need to manually update the progress\n\t        self._current += n\n\t        # Inform listeners\n\t        listeners = _get_thread_local_listeners()\n\t        for listener in listeners:\n\t            listener.on_progress(self._current, self.total)\n\t_thread_local = threading.local()\n\tdef _get_thread_local_listeners():\n\t    if not hasattr(_thread_local, 'listeners'):\n\t        _thread_local.listeners = []\n", "    return _thread_local.listeners\n\t_hooked = False\n\tdef init_progress_hook():\n\t    global _hooked\n\t    if _hooked:\n\t        return\n\t    # Inject into tqdm.tqdm of Whisper, so we can see progress\n\t    import whisper.transcribe \n\t    transcribe_module = sys.modules['whisper.transcribe']\n\t    transcribe_module.tqdm.tqdm = _CustomProgressBar\n", "    _hooked = True\n\tdef register_thread_local_progress_listener(progress_listener: ProgressListener):\n\t    # This is a workaround for the fact that the progress bar is not exposed in the API\n\t    init_progress_hook()\n\t    listeners = _get_thread_local_listeners()\n\t    listeners.append(progress_listener)\n\tdef unregister_thread_local_progress_listener(progress_listener: ProgressListener):\n\t    listeners = _get_thread_local_listeners()\n\t    if progress_listener in listeners:\n\t        listeners.remove(progress_listener)\n", "def create_progress_listener_handle(progress_listener: ProgressListener):\n\t    return ProgressListenerHandle(progress_listener)\n\t# Example usage\n\tif __name__ == '__main__':\n\t    class PrintingProgressListener:\n\t        def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t            print(f\"Progress: {current}/{total}\")\n\t        def on_finished(self):\n\t            print(\"Finished\")\n\t    import whisper\n", "    model = whisper.load_model(\"medium\")\n\t    with create_progress_listener_handle(PrintingProgressListener()) as listener:\n\t        # Set verbose to None to disable the progress bar, as we are using our own\n\t        result = model.transcribe(\"J:\\\\Dev\\\\OpenAI\\\\whisper\\\\tests\\\\Noriko\\\\out.mka\", language=\"Japanese\", fp16=False, verbose=None)\n\t        print(result)\n\t    print(\"Done\")"]}
{"filename": "src/hooks/subTaskProgressListener.py", "chunked_list": ["from src.hooks.progressListener import ProgressListener\n\tfrom typing import Union\n\tclass SubTaskProgressListener(ProgressListener):\n\t    \"\"\"\n\t    A sub task listener that reports the progress of a sub task to a base task listener\n\t    Parameters\n\t    ----------\n\t    base_task_listener : ProgressListener\n\t        The base progress listener to accumulate overall progress in.\n\t    base_task_total : float\n", "        The maximum total progress that will be reported to the base progress listener.\n\t    sub_task_start : float\n\t        The starting progress of a sub task, in respect to the base progress listener.\n\t    sub_task_total : float\n\t        The total amount of progress a sub task will report to the base progress listener.\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n\t        base_task_listener: ProgressListener,\n\t        base_task_total: float,\n", "        sub_task_start: float,\n\t        sub_task_total: float,\n\t    ):\n\t        self.base_task_listener = base_task_listener\n\t        self.base_task_total = base_task_total\n\t        self.sub_task_start = sub_task_start\n\t        self.sub_task_total = sub_task_total\n\t    def on_progress(self, current: Union[int, float], total: Union[int, float]):\n\t        sub_task_progress_frac = current / total\n\t        sub_task_progress = self.sub_task_start + self.sub_task_total * sub_task_progress_frac\n", "        self.base_task_listener.on_progress(sub_task_progress, self.base_task_total)\n\t    def on_finished(self):\n\t        self.base_task_listener.on_progress(self.sub_task_start + self.sub_task_total, self.base_task_total)"]}
{"filename": "src/whisper/whisperFactory.py", "chunked_list": ["from typing import List\n\tfrom src import modelCache\n\tfrom src.config import ModelConfig\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperContainer\n\tdef create_whisper_container(whisper_implementation: str, \n\t                             model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                             download_root: str = None,\n\t                             cache: modelCache = None, models: List[ModelConfig] = []) -> AbstractWhisperContainer:\n\t    print(\"Creating whisper container for \" + whisper_implementation)\n\t    if (whisper_implementation == \"whisper\"):\n", "        from src.whisper.whisperContainer import WhisperContainer\n\t        return WhisperContainer(model_name=model_name, device=device, compute_type=compute_type, download_root=download_root, cache=cache, models=models)\n\t    elif (whisper_implementation == \"faster-whisper\" or whisper_implementation == \"faster_whisper\"):\n\t        from src.whisper.fasterWhisperContainer import FasterWhisperContainer\n\t        return FasterWhisperContainer(model_name=model_name, device=device, compute_type=compute_type, download_root=download_root, cache=cache, models=models)\n\t    else:\n\t        raise ValueError(\"Unknown Whisper implementation: \" + whisper_implementation)"]}
{"filename": "src/whisper/whisperContainer.py", "chunked_list": ["# External programs\n\timport abc\n\timport os\n\timport sys\n\tfrom typing import List\n\tfrom urllib.parse import urlparse\n\timport torch\n\timport urllib3\n\tfrom src.hooks.progressListener import ProgressListener\n\timport whisper\n", "from whisper import Whisper\n\tfrom src.config import ModelConfig, VadInitialPromptMode\n\tfrom src.hooks.whisperProgressHook import create_progress_listener_handle\n\tfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\tfrom src.utils import download_file\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback, AbstractWhisperContainer\n\tclass WhisperContainer(AbstractWhisperContainer):\n\t    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                 download_root: str = None,\n\t                 cache: ModelCache = None, models: List[ModelConfig] = []):\n", "        if device is None:\n\t            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\t        super().__init__(model_name, device, compute_type, download_root, cache, models)\n\t    def ensure_downloaded(self):\n\t        \"\"\"\n\t        Ensure that the model is downloaded. This is useful if you want to ensure that the model is downloaded before\n\t        passing the container to a subprocess.\n\t        \"\"\"\n\t        # Warning: Using private API here\n\t        try:\n", "            root_dir = self.download_root\n\t            model_config = self._get_model_config()\n\t            if root_dir is None:\n\t                root_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"whisper\")\n\t            if self.model_name in whisper._MODELS:\n\t                whisper._download(whisper._MODELS[self.model_name], root_dir, False)\n\t            else:\n\t                # If the model is not in the official list, see if it needs to be downloaded\n\t                model_config.download_url(root_dir)\n\t            return True\n", "        except Exception as e:\n\t            # Given that the API is private, it could change at any time. We don't want to crash the program\n\t            print(\"Error pre-downloading model: \" + str(e))\n\t            return False\n\t    def _get_model_config(self) -> ModelConfig:\n\t        \"\"\"\n\t        Get the model configuration for the model.\n\t        \"\"\"\n\t        for model in self.models:\n\t            if model.name == self.model_name:\n", "                return model\n\t        return None\n\t    def _create_model(self):\n\t        print(\"Loading whisper model \" + self.model_name)\n\t        model_config = self._get_model_config()\n\t        # Note that the model will not be downloaded in the case of an official Whisper model\n\t        model_path = self._get_model_path(model_config, self.download_root)\n\t        return whisper.load_model(model_path, device=self.device, download_root=self.download_root)\n\t    def create_callback(self, language: str = None, task: str = None, initial_prompt: str = None, \n\t                        initial_prompt_mode: VadInitialPromptMode = VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n", "                        **decodeOptions: dict) -> AbstractWhisperCallback:\n\t        \"\"\"\n\t        Create a WhisperCallback object that can be used to transcript audio files.\n\t        Parameters\n\t        ----------\n\t        language: str\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        initial_prompt: str\n", "            The initial prompt to use for the transcription.\n\t        initial_prompt_mode: VadInitialPromptMode\n\t            The mode to use for the initial prompt. If set to PREPEND_FIRST_SEGMENT, the initial prompt will be prepended to the first segment of audio.\n\t            If set to PREPEND_ALL_SEGMENTS, the initial prompt will be prepended to all segments of audio.\n\t        decodeOptions: dict\n\t            Additional options to pass to the decoder. Must be pickleable.\n\t        Returns\n\t        -------\n\t        A WhisperCallback object.\n\t        \"\"\"\n", "        return WhisperCallback(self, language=language, task=task, initial_prompt=initial_prompt, initial_prompt_mode=initial_prompt_mode, **decodeOptions)\n\t    def _get_model_path(self, model_config: ModelConfig, root_dir: str = None):\n\t        from src.conversion.hf_converter import convert_hf_whisper\n\t        \"\"\"\n\t        Download the model.\n\t        Parameters\n\t        ----------\n\t        model_config: ModelConfig\n\t            The model configuration.\n\t        \"\"\"\n", "        # See if path is already set\n\t        if model_config.path is not None:\n\t            return model_config.path\n\t        if root_dir is None:\n\t            root_dir = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"whisper\")\n\t        model_type = model_config.type.lower() if model_config.type is not None else \"whisper\"\n\t        if model_type in [\"huggingface\", \"hf\"]:\n\t            model_config.path = model_config.url\n\t            destination_target = os.path.join(root_dir, model_config.name + \".pt\")\n\t            # Convert from HuggingFace format to Whisper format\n", "            if os.path.exists(destination_target):\n\t                print(f\"File {destination_target} already exists, skipping conversion\")\n\t            else:\n\t                print(\"Saving HuggingFace model in Whisper format to \" + destination_target)\n\t                convert_hf_whisper(model_config.url, destination_target)\n\t            model_config.path = destination_target\n\t        elif model_type in [\"whisper\", \"w\"]:\n\t            model_config.path = model_config.url\n\t            # See if URL is just a file\n\t            if model_config.url in whisper._MODELS:\n", "                # No need to download anything - Whisper will handle it\n\t                model_config.path = model_config.url\n\t            elif model_config.url.startswith(\"file://\"):\n\t                # Get file path\n\t                model_config.path = urlparse(model_config.url).path\n\t            # See if it is an URL\n\t            elif model_config.url.startswith(\"http://\") or model_config.url.startswith(\"https://\"):\n\t                # Extension (or file name)\n\t                extension = os.path.splitext(model_config.url)[-1]\n\t                download_target = os.path.join(root_dir, model_config.name + extension)\n", "                if os.path.exists(download_target) and not os.path.isfile(download_target):\n\t                    raise RuntimeError(f\"{download_target} exists and is not a regular file\")\n\t                if not os.path.isfile(download_target):\n\t                    download_file(model_config.url, download_target)\n\t                else:\n\t                    print(f\"File {download_target} already exists, skipping download\")\n\t                model_config.path = download_target\n\t            # Must be a local file\n\t            else:\n\t                model_config.path = model_config.url\n", "        else:\n\t            raise ValueError(f\"Unknown model type {model_type}\")\n\t        return model_config.path\n\tclass WhisperCallback(AbstractWhisperCallback):\n\t    def __init__(self, model_container: WhisperContainer, language: str = None, task: str = None, initial_prompt: str = None, \n\t                 initial_prompt_mode: VadInitialPromptMode=VadInitialPromptMode.PREPREND_FIRST_SEGMENT, **decodeOptions: dict):\n\t        self.model_container = model_container\n\t        self.language = language\n\t        self.task = task\n\t        self.initial_prompt = initial_prompt\n", "        self.initial_prompt_mode = initial_prompt_mode\n\t        self.decodeOptions = decodeOptions\n\t    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n\t        \"\"\"\n\t        Peform the transcription of the given audio file or data.\n\t        Parameters\n\t        ----------\n\t        audio: Union[str, np.ndarray, torch.Tensor]\n\t            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n\t        segment_index: int\n", "            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        progress_listener: ProgressListener\n\t            A callback to receive progress updates.\n\t        \"\"\"\n\t        model = self.model_container.get_model()\n\t        if progress_listener is not None:\n\t            with create_progress_listener_handle(progress_listener):\n\t                return self._transcribe(model, audio, segment_index, prompt, detected_language)\n", "        else:\n\t            return self._transcribe(model, audio, segment_index, prompt, detected_language)\n\t    def _transcribe(self, model: Whisper, audio, segment_index: int, prompt: str, detected_language: str):\n\t        decodeOptions = self.decodeOptions.copy()\n\t        # Add fp16\n\t        if self.model_container.compute_type in [\"fp16\", \"float16\"]:\n\t            decodeOptions[\"fp16\"] = True\n\t        initial_prompt = self._get_initial_prompt(self.initial_prompt, self.initial_prompt_mode, prompt, segment_index)\n\t        return model.transcribe(audio, \\\n\t            language=self.language if self.language else detected_language, task=self.task, \\\n", "            initial_prompt=initial_prompt, \\\n\t            **decodeOptions\n\t        )"]}
{"filename": "src/whisper/abstractWhisperContainer.py", "chunked_list": ["import abc\n\tfrom typing import List\n\tfrom src.config import ModelConfig, VadInitialPromptMode\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.modelCache import GLOBAL_MODEL_CACHE, ModelCache\n\tclass AbstractWhisperCallback:\n\t    @abc.abstractmethod\n\t    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n\t        \"\"\"\n\t        Peform the transcription of the given audio file or data.\n", "        Parameters\n\t        ----------\n\t        audio: Union[str, np.ndarray, torch.Tensor]\n\t            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n\t        segment_index: int\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        progress_listener: ProgressListener\n\t            A callback to receive progress updates.\n", "        \"\"\"\n\t        raise NotImplementedError()\n\t    def _get_initial_prompt(self, initial_prompt: str, initial_prompt_mode: VadInitialPromptMode, \n\t                               prompt: str, segment_index: int):\n\t        if (initial_prompt_mode == VadInitialPromptMode.PREPEND_ALL_SEGMENTS):\n\t            return self._concat_prompt(initial_prompt, prompt)\n\t        elif (initial_prompt_mode == VadInitialPromptMode.PREPREND_FIRST_SEGMENT):\n\t            return self._concat_prompt(initial_prompt, prompt) if segment_index == 0 else prompt\n\t        else:\n\t            raise ValueError(f\"Unknown initial prompt mode {initial_prompt_mode}\")\n", "    def _concat_prompt(self, prompt1, prompt2):\n\t        if (prompt1 is None):\n\t            return prompt2\n\t        elif (prompt2 is None):\n\t            return prompt1\n\t        else:\n\t            return prompt1 + \" \" + prompt2\n\tclass AbstractWhisperContainer:\n\t    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                 download_root: str = None,\n", "                 cache: ModelCache = None, models: List[ModelConfig] = []):\n\t        self.model_name = model_name\n\t        self.device = device\n\t        self.compute_type = compute_type\n\t        self.download_root = download_root\n\t        self.cache = cache\n\t        # Will be created on demand\n\t        self.model = None\n\t        # List of known models\n\t        self.models = models\n", "    def get_model(self):\n\t        if self.model is None:\n\t            if (self.cache is None):\n\t                self.model = self._create_model()\n\t            else:\n\t                model_key = \"WhisperContainer.\" + self.model_name + \":\" + (self.device if self.device else '')\n\t                self.model = self.cache.get(model_key, self._create_model)\n\t        return self.model\n\t    @abc.abstractmethod\n\t    def _create_model(self):\n", "        raise NotImplementedError()\n\t    def ensure_downloaded(self):\n\t        pass\n\t    @abc.abstractmethod\n\t    def create_callback(self, language: str = None, task: str = None, initial_prompt: str = None, \n\t                        initial_prompt_mode: VadInitialPromptMode = VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n\t                        **decodeOptions: dict) -> AbstractWhisperCallback:\n\t        \"\"\"\n\t        Create a WhisperCallback object that can be used to transcript audio files.\n\t        Parameters\n", "        ----------\n\t        language: str\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        initial_prompt: str\n\t            The initial prompt to use for the transcription.\n\t        initial_prompt_mode: VadInitialPromptMode\n\t            The mode to use for the initial prompt. If set to PREPEND_FIRST_SEGMENT, the initial prompt will be prepended to the first segment of audio.\n\t            If set to PREPEND_ALL_SEGMENTS, the initial prompt will be prepended to all segments of audio.\n", "        decodeOptions: dict\n\t            Additional options to pass to the decoder. Must be pickleable.\n\t        Returns\n\t        -------\n\t        A WhisperCallback object.\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    # This is required for multiprocessing\n\t    def __getstate__(self):\n\t        return { \n", "            \"model_name\": self.model_name, \n\t            \"device\": self.device, \n\t            \"download_root\": self.download_root, \n\t            \"models\": self.models, \n\t            \"compute_type\": self.compute_type \n\t        }\n\t    def __setstate__(self, state):\n\t        self.model_name = state[\"model_name\"]\n\t        self.device = state[\"device\"]\n\t        self.download_root = state[\"download_root\"]\n", "        self.models = state[\"models\"]\n\t        self.compute_type = state[\"compute_type\"]\n\t        self.model = None\n\t        # Depickled objects must use the global cache\n\t        self.cache = GLOBAL_MODEL_CACHE"]}
{"filename": "src/whisper/fasterWhisperContainer.py", "chunked_list": ["import os\n\tfrom typing import List, Union\n\tfrom faster_whisper import WhisperModel, download_model\n\tfrom src.config import ModelConfig, VadInitialPromptMode\n\tfrom src.hooks.progressListener import ProgressListener\n\tfrom src.languages import get_language_from_name\n\tfrom src.modelCache import ModelCache\n\tfrom src.whisper.abstractWhisperContainer import AbstractWhisperCallback, AbstractWhisperContainer\n\tfrom src.utils import format_timestamp\n\tclass FasterWhisperContainer(AbstractWhisperContainer):\n", "    def __init__(self, model_name: str, device: str = None, compute_type: str = \"float16\",\n\t                       download_root: str = None,\n\t                       cache: ModelCache = None, models: List[ModelConfig] = []):\n\t        super().__init__(model_name, device, compute_type, download_root, cache, models)\n\t    def ensure_downloaded(self):\n\t        \"\"\"\n\t        Ensure that the model is downloaded. This is useful if you want to ensure that the model is downloaded before\n\t        passing the container to a subprocess.\n\t        \"\"\"\n\t        model_config = self._get_model_config()\n", "        if os.path.isdir(model_config.url):\n\t            model_config.path = model_config.url\n\t        else:\n\t            model_config.path = download_model(model_config.url, output_dir=self.download_root)\n\t    def _get_model_config(self) -> ModelConfig:\n\t        \"\"\"\n\t        Get the model configuration for the model.\n\t        \"\"\"\n\t        for model in self.models:\n\t            if model.name == self.model_name:\n", "                return model\n\t        return None\n\t    def _create_model(self):\n\t        print(\"Loading faster whisper model \" + self.model_name + \" for device \" + str(self.device))\n\t        model_config = self._get_model_config()\n\t        if model_config.type == \"whisper\" and model_config.url not in [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\"]:\n\t            raise Exception(\"FasterWhisperContainer does not yet support Whisper models. Use ct2-transformers-converter to convert the model to a faster-whisper model.\")\n\t        device = self.device\n\t        if (device is None):\n\t            device = \"auto\"\n", "        # model_path = FASTER_WHISPER_MODELS_PATH[model_config.url]\n\t        model_path = os.path.join(os.path.join(\"models\", \"faster-whisper\"),model_config.url)\n\t        # model = WhisperModel(model_config.url, device=device, compute_type=self.compute_type)\n\t        model = WhisperModel(model_size_or_path=model_path, device=device, compute_type=self.compute_type)\n\t        return model\n\t    def create_callback(self, language: str = None, task: str = None, initial_prompt: str = None, \n\t                        initial_prompt_mode: VadInitialPromptMode = VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n\t                        **decodeOptions: dict) -> AbstractWhisperCallback:\n\t        \"\"\"\n\t        Create a WhisperCallback object that can be used to transcript audio files.\n", "        Parameters\n\t        ----------\n\t        language: str\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n\t        initial_prompt: str\n\t            The initial prompt to use for the transcription.\n\t        initial_prompt_mode: VadInitialPromptMode\n\t            The mode to use for the initial prompt. If set to PREPEND_FIRST_SEGMENT, the initial prompt will be prepended to the first segment of audio.\n", "            If set to PREPEND_ALL_SEGMENTS, the initial prompt will be prepended to all segments of audio.\n\t        decodeOptions: dict\n\t            Additional options to pass to the decoder. Must be pickleable.\n\t        Returns\n\t        -------\n\t        A WhisperCallback object.\n\t        \"\"\"\n\t        return FasterWhisperCallback(self, language=language, task=task, initial_prompt=initial_prompt, initial_prompt_mode=initial_prompt_mode, **decodeOptions)\n\tclass FasterWhisperCallback(AbstractWhisperCallback):\n\t    def __init__(self, model_container: FasterWhisperContainer, language: str = None, task: str = None, \n", "                 initial_prompt: str = None, initial_prompt_mode: VadInitialPromptMode=VadInitialPromptMode.PREPREND_FIRST_SEGMENT, \n\t                 **decodeOptions: dict):\n\t        self.model_container = model_container\n\t        self.language = language\n\t        self.task = task\n\t        self.initial_prompt = initial_prompt\n\t        self.initial_prompt_mode = initial_prompt_mode\n\t        self.decodeOptions = decodeOptions\n\t        self._printed_warning = False\n\t    def invoke(self, audio, segment_index: int, prompt: str, detected_language: str, progress_listener: ProgressListener = None):\n", "        \"\"\"\n\t        Peform the transcription of the given audio file or data.\n\t        Parameters\n\t        ----------\n\t        audio: Union[str, np.ndarray, torch.Tensor]\n\t            The audio file to transcribe, or the audio data as a numpy array or torch tensor.\n\t        segment_index: int\n\t            The target language of the transcription. If not specified, the language will be inferred from the audio content.\n\t        task: str\n\t            The task - either translate or transcribe.\n", "        progress_listener: ProgressListener\n\t            A callback to receive progress updates.\n\t        \"\"\"\n\t        model: WhisperModel = self.model_container.get_model()\n\t        language_code = self._lookup_language_code(self.language) if self.language else None\n\t        # Copy decode options and remove options that are not supported by faster-whisper\n\t        decodeOptions = self.decodeOptions.copy()\n\t        verbose = decodeOptions.pop(\"verbose\", None)\n\t        logprob_threshold = decodeOptions.pop(\"logprob_threshold\", None)\n\t        patience = decodeOptions.pop(\"patience\", None)\n", "        length_penalty = decodeOptions.pop(\"length_penalty\", None)\n\t        suppress_tokens = decodeOptions.pop(\"suppress_tokens\", None)\n\t        if (decodeOptions.pop(\"fp16\", None) is not None):\n\t            if not self._printed_warning:\n\t                print(\"WARNING: fp16 option is ignored by faster-whisper - use compute_type instead.\")\n\t            self._printed_warning = True\n\t        # Fix up decode options\n\t        if (logprob_threshold is not None):\n\t            decodeOptions[\"log_prob_threshold\"] = logprob_threshold\n\t        decodeOptions[\"patience\"] = float(patience) if patience is not None else 1.0\n", "        decodeOptions[\"length_penalty\"] = float(length_penalty) if length_penalty is not None else 1.0\n\t        # See if supress_tokens is a string - if so, convert it to a list of ints\n\t        decodeOptions[\"suppress_tokens\"] = self._split_suppress_tokens(suppress_tokens)\n\t        initial_prompt = self._get_initial_prompt(self.initial_prompt, self.initial_prompt_mode, prompt, segment_index)\n\t        segments_generator, info = model.transcribe(audio, \\\n\t            language=language_code if language_code else detected_language, task=self.task, \\\n\t            initial_prompt=initial_prompt, \\\n\t            **decodeOptions\n\t        )\n\t        segments = []\n", "        for segment in segments_generator:\n\t            segments.append(segment)\n\t            if progress_listener is not None:\n\t                progress_listener.on_progress(segment.end, info.duration)\n\t            if verbose:\n\t                print(\"[{}->{}] {}\".format(format_timestamp(segment.start, True), format_timestamp(segment.end, True),\n\t                                          segment.text))\n\t        text = \" \".join([segment.text for segment in segments])\n\t        # Convert the segments to a format that is easier to serialize\n\t        whisper_segments = [{\n", "            \"text\": segment.text,\n\t            \"start\": segment.start,\n\t            \"end\": segment.end,\n\t            # Extra fields added by faster-whisper\n\t            \"words\": [{\n\t                \"start\": word.start,\n\t                \"end\": word.end,\n\t                \"word\": word.word,\n\t                \"probability\": word.probability\n\t            } for word in (segment.words if segment.words is not None else []) ]\n", "        } for segment in segments]\n\t        result = {\n\t            \"segments\": whisper_segments,\n\t            \"text\": text,\n\t            \"language\": info.language if info else None,\n\t            # Extra fields added by faster-whisper\n\t            \"language_probability\": info.language_probability if info else None,\n\t            \"duration\": info.duration if info else None\n\t        }\n\t        if progress_listener is not None:\n", "            progress_listener.on_finished()\n\t        return result\n\t    def _split_suppress_tokens(self, suppress_tokens: Union[str, List[int]]):\n\t        if (suppress_tokens is None):\n\t            return None\n\t        if (isinstance(suppress_tokens, list)):\n\t            return suppress_tokens\n\t        return [int(token) for token in suppress_tokens.split(\",\")]\n\t    def _lookup_language_code(self, language: str):\n\t        language = get_language_from_name(language)\n", "        if language is None:\n\t            raise ValueError(\"Invalid language: \" + language)\n\t        return language.code\n"]}
{"filename": "src/conversion/hf_converter.py", "chunked_list": ["# https://github.com/bayartsogt-ya/whisper-multiple-hf-datasets\n\tfrom copy import deepcopy\n\timport torch\n\tWHISPER_MAPPING = {\n\t    \"layers\": \"blocks\",\n\t    \"fc1\": \"mlp.0\",\n\t    \"fc2\": \"mlp.2\",\n\t    \"final_layer_norm\": \"mlp_ln\",\n\t    \"layers\": \"blocks\",\n\t    \".self_attn.q_proj\": \".attn.query\",\n", "    \".self_attn.k_proj\": \".attn.key\",\n\t    \".self_attn.v_proj\": \".attn.value\",\n\t    \".self_attn_layer_norm\": \".attn_ln\",\n\t    \".self_attn.out_proj\": \".attn.out\",\n\t    \".encoder_attn.q_proj\": \".cross_attn.query\",\n\t    \".encoder_attn.k_proj\": \".cross_attn.key\",\n\t    \".encoder_attn.v_proj\": \".cross_attn.value\",\n\t    \".encoder_attn_layer_norm\": \".cross_attn_ln\",\n\t    \".encoder_attn.out_proj\": \".cross_attn.out\",\n\t    \"decoder.layer_norm.\": \"decoder.ln.\",\n", "    \"encoder.layer_norm.\": \"encoder.ln_post.\",\n\t    \"embed_tokens\": \"token_embedding\",\n\t    \"encoder.embed_positions.weight\": \"encoder.positional_embedding\",\n\t    \"decoder.embed_positions.weight\": \"decoder.positional_embedding\",\n\t    \"layer_norm\": \"ln_post\",\n\t}\n\tdef rename_keys(s_dict):\n\t    keys = list(s_dict.keys())\n\t    for key in keys:\n\t        new_key = key\n", "        for k, v in WHISPER_MAPPING.items():\n\t            if k in key:\n\t                new_key = new_key.replace(k, v)\n\t        print(f\"{key} -> {new_key}\")\n\t        s_dict[new_key] = s_dict.pop(key)\n\t    return s_dict\n\tdef convert_hf_whisper(hf_model_name_or_path: str, whisper_state_path: str):\n\t    from transformers import WhisperForConditionalGeneration\n\t    transformer_model = WhisperForConditionalGeneration.from_pretrained(hf_model_name_or_path)\n\t    config = transformer_model.config\n", "    # first build dims\n\t    dims = {\n\t        'n_mels': config.num_mel_bins,\n\t        'n_vocab': config.vocab_size,\n\t        'n_audio_ctx': config.max_source_positions,\n\t        'n_audio_state': config.d_model,\n\t        'n_audio_head': config.encoder_attention_heads,\n\t        'n_audio_layer': config.encoder_layers,\n\t        'n_text_ctx': config.max_target_positions,\n\t        'n_text_state': config.d_model,\n", "        'n_text_head': config.decoder_attention_heads,\n\t        'n_text_layer': config.decoder_layers\n\t    }\n\t    state_dict = deepcopy(transformer_model.model.state_dict())\n\t    state_dict = rename_keys(state_dict)\n\t    torch.save({\"dims\": dims, \"model_state_dict\": state_dict}, whisper_state_path)"]}
