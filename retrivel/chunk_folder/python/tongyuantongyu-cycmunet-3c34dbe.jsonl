{"filename": "tensorrt-conda/recipe/run_test.py", "chunked_list": ["from vapoursynth import core\n\tprint(core.cycmunet.CycMuNetVersion())\n"]}
{"filename": "mindspore/inference.py", "chunked_list": ["import time\n\tfrom collections import namedtuple\n\timport numpy as np\n\timport mindspore as ms\n\tfrom model import CycMuNet\n\tfrom util.normalize import Normalizer\n\tfrom dataset.video import VideoFrameDataset\n\tdummyArg = namedtuple('dummyArg', (\n\t    'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n\t    'stop_at_conf'))\n", "size = 128\n\targs = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='batch',\n\t                all_frames=True, stop_at_conf=False)\n\tds_path = r\"/home/ma-user/work/cctv-scaled/\"\n\t# ds_path = r\"./test-files/\"\n\tif __name__ == '__main__':\n\t    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\t    # ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"Ascend\")\n\t    # ms.set_context(mode=ms.GRAPH_MODE, device_target=\"CPU\")\n\t    # ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"CPU\")\n", "    print('Init done')\n\t    model = CycMuNet(args)\n\t    ms.load_checkpoint(\"model-files/2x_yuv420_cycle3_layer4.ckpt\", model)\n\t    # model = model.to_float(ms.float16)\n\t    print('Load done')\n\t    nm = Normalizer()\n\t    ds_test = VideoFrameDataset(ds_path + \"index-test.txt\", size, args.upscale_factor, True, nm)\n\t    ds_test = ds_test.batch(1)\n\t    start = time.time()\n\t    for n, data in enumerate(ds_test.create_tuple_iterator()):\n", "        start = time.time()\n\t        # data = [t.astype(ms.float16) for t in data]\n\t        inputs = [data[6:8], data[10:12]]\n\t        model(*inputs)\n\t        print(f\"#{n:0>3} inference in {time.time() - start}s\")\n"]}
{"filename": "mindspore/train.py", "chunked_list": ["import os\n\timport time\n\tfrom collections import namedtuple\n\timport argparse\n\timport numpy as np\n\timport tqdm\n\timport mindspore as ms\n\tfrom mindspore import nn, ops\n\tfrom model import CycMuNet, TrainModel\n\tfrom util.rmse import RMSELoss\n", "from util.normalize import Normalizer\n\tfrom dataset.video import VideoFrameDataset\n\tprint(\"Initialized.\")\n\tdummyArg = namedtuple('dummyArg', (\n\t    'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames', 'stop_at_conf'))\n\tsize = 128\n\targs = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='batch',\n\t                all_frames=True, stop_at_conf=False)\n\tepochs = 1\n\tbatch_size = 1\n", "learning_rate = 0.001\n\tsave_prefix = 'monitor'\n\tds_path = r\"/home/ma-user/work/cctv-scaled/\"\n\tpretrained = \"/home/ma-user/work/cycmunet-ms/model-files/2x_yuv420_cycle3_layer4.ckpt\"\n\tsave_path = \"/home/ma-user/work/cycmunet-ms/checkpoints/\"\n\t# ds_path = r\"./test-files/index-train.txt\"\n\t# pretrained = \"./model-files/2x_yuv420_cycle3_layer4.ckpt\"\n\t# save_path = \"./checkpoints/\"\n\t# ds_path = r\"D:\\Python\\cycmunet-ms\\test-files/\"\n\t# pretrained = \"\"\n", "# parser = argparse.ArgumentParser(\n\t#     prog='CycMuNet+ MindSpore Training')\n\t#\n\t# parser.add_argument('--dataset')\n\t# parser.add_argument('--pretrained')\n\t# parser.add_argument('--save')\n\t#\n\t# cmd_args = parser.parse_args()\n\t#\n\t# ds_path = cmd_args.dataset\n", "# pretrained = cmd_args.pretrained\n\t# save_path = cmd_args.save\n\tsave_prefix = f'{save_prefix}_{args.upscale_factor}x_l{args.layers}_c{args.cycle_count}'\n\tnetwork = CycMuNet(args)\n\tif pretrained:\n\t    ms.load_checkpoint(pretrained, network)\n\tms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\tinp = (ms.Tensor(np.zeros((batch_size, 1, size, size), dtype=np.float32)),\n\t       ms.Tensor(np.zeros((batch_size, 2, size // 2, size // 2), dtype=np.float32)))\n\tnetwork.compile(inp, inp)\n", "inp = None\n\tloss_fn = RMSELoss()\n\tnm = Normalizer()\n\tds_train = VideoFrameDataset(ds_path + \"index-train.txt\", size, args.upscale_factor, True, nm)\n\tds_test = VideoFrameDataset(ds_path + \"index-test.txt\", size, args.upscale_factor, True, nm)\n\tds_train = ds_train.batch(batch_size)\n\tds_test = ds_test.batch(batch_size)\n\tscheduler = nn.CosineDecayLR(min_lr=1e-7, max_lr=learning_rate, decay_steps=640000)\n\toptimizer = nn.AdaMax(network.trainable_params(), learning_rate=learning_rate)\n\tmodel = TrainModel(network, loss_fn)\n", "model = ms.Model(model, optimizer=optimizer, eval_network=model, boost_level=\"O1\")\n\tdef save_model(epoch):\n\t    if epoch == -1:\n\t        name = \"snapshot\"\n\t    else:\n\t        name = f\"epoch_{epoch}\"\n\t    if not os.path.exists(save_path):\n\t        os.makedirs(save_path)\n\t    output_path = save_path + f\"{save_prefix}_{name}.ckpt\"\n\t    ms.save_checkpoint(network, str(output_path))\n", "    print(f\"Checkpoint saved to {output_path}\")\n\tprint(\"Start train.\")\n\tprofiler = ms.Profiler(output_path='./profiler_data')\n\tfor t in range(1, epochs + 1):\n\t    try:\n\t        print(f\"Epoch {t}\\n-------------------------------\")\n\t        model.train(t, ds_train, dataset_sink_mode=True)\n\t        save_model(t)\n\t    except KeyboardInterrupt:\n\t        save_model(-1)\n", "profiler.analyse()\n\tprint(\"Done.\")\n"]}
{"filename": "mindspore/build_mindir.py", "chunked_list": ["from collections import namedtuple\n\timport time\n\timport numpy as np\n\timport mindspore as ms\n\tfrom model import CycMuNet\n\tdef export_rgb(checkpoint, size):\n\t    dummyArg = namedtuple('dummyArg', (\n\t        'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n\t        'stop_at_conf'))\n\t    args = dummyArg(nf=64, groups=8, upscale_factor=4, format='rgb', layers=3, cycle_count=5, batch_mode='sequence',\n", "                    all_frames=False, stop_at_conf=False)\n\t    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\t    print('Init done')\n\t    build_start = time.time()\n\t    model = CycMuNet(args)\n\t    ms.load_checkpoint(checkpoint, model)\n\t    inp = ms.Tensor(np.ones((2, 3, *size), dtype=np.float32))\n\t    inp = inp.astype(ms.float16)\n\t    model = model.to_float(ms.float16)\n\t    model.compile(inp)\n", "    print(f'Load done in {time.time() - build_start}s')\n\t    # verify\n\t    model(inp)\n\t    ms.export(model, inp, file_name=model, file_format='MINDIR')\n\t    print('Export done')\n\tdef export_yuv(checkpoint, size):\n\t    dummyArg = namedtuple('dummyArg', (\n\t        'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n\t        'stop_at_conf'))\n\t    args = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='sequence',\n", "                    all_frames=False, stop_at_conf=False)\n\t    ms.set_context(mode=ms.GRAPH_MODE, device_target=\"Ascend\")\n\t    print('Init done')\n\t    build_start = time.time()\n\t    model = CycMuNet(args)\n\t    ms.load_checkpoint(checkpoint, model)\n\t    inp_y = ms.Tensor(np.zeros((2, 1, *size), dtype=np.float16))\n\t    inp_uv = ms.Tensor(np.zeros((2, 2, size[0] // 2, size[1] // 2), dtype=np.float16))\n\t    model = model.to_float(ms.float16)\n\t    model.compile(inp_y, inp_uv)\n", "    print(f'Load done in {time.time() - build_start}s')\n\t    # verify\n\t    model(inp_y, inp_uv)\n\t    ms.export(model, inp_y, inp_uv, file_name=model, file_format='MINDIR')\n\t    print('Export done')\n\tif __name__ == '__main__':\n\t    # export_rgb('model-files/2x_rgb_base.ckpt', 'model-files/cycmunet_2x_rgb', (64, 64))\n\t    export_yuv('model-files/2x_yuv420_cycle3_layer4.ckpt', 'model-files/cycmunet_2x_yuv420_cycle3_layer4', (1920, 1088))\n"]}
{"filename": "mindspore/convert_weight.py", "chunked_list": ["import re\n\tfrom collections import namedtuple\n\timport mindspore as ms\n\tfrom mindspore import ops\n\timport torch\n\timport model\n\tdef transform_dcnpack(weights):\n\t    result = {\n\t        'dcn_weight': weights['dcn.weight'],\n\t        'dcn_bias': weights['dcn.bias'],\n", "        'conv_mask.weight': weights['conv_mask.weight'],\n\t        'conv_mask.bias': weights['conv_mask.bias'],\n\t    }\n\t    w = weights['conv_offset.weight'].reshape(72, 2, 64, 3, 3)\n\t    b = weights['conv_offset.bias'].reshape(72, 2)\n\t    w = w[:, ::-1, ...].transpose(1, 0, 2, 3, 4).reshape(144, 64, 3, 3)\n\t    b = b[:, ::-1, ...].transpose(1, 0).reshape(144)\n\t    result['conv_offset.weight'] = w\n\t    result['conv_offset.bias'] = b\n\t    return result\n", "if __name__ == '__main__':\n\t    torch_source = 'checkpoints/2x_cycle3_yuv420_sparsity_epoch_20.pth'\n\t    ms_normal = 'checkpoints/2x_yuv420_cycle3_layer4.ckpt'\n\t    ms_sep = 'checkpoints/2x_sep_yuv420_cycle3_layer4.ckpt'\n\t    dummyArg = namedtuple('dummyArg', (\n\t        'nf', 'groups', 'upscale_factor', 'format', 'layers', 'cycle_count', 'batch_mode', 'all_frames',\n\t        'stop_at_conf'))\n\t    size = (64, 64)\n\t    args = dummyArg(nf=64, groups=8, upscale_factor=2, format='yuv420', layers=4, cycle_count=3, batch_mode='sequence',\n\t                    all_frames=False, stop_at_conf=False)\n", "    print('Init done')\n\t    rewrite_names = {\n\t        \".Pro_align.conv1x1.\": 3,\n\t        \".Pro_align.conv1_3x3.\": 2,\n\t        \".offset_conv1.\": 2,\n\t        \".offset_conv2.\": 2,\n\t        \".fea_conv.\": 2,\n\t        \"ff.fusion.\": 2,\n\t        \"mu.conv.\": 2 * args.cycle_count + 1\n\t    }\n", "    rewrite_names_re = {\n\t        r\"(merge1?\\.(\\d+)\\.)\": lambda match: int(match[2]) + 1,\n\t    }\n\t    # normal model\n\t    model_normal = model.CycMuNet(args)\n\t    source = torch.load(torch_source, map_location=torch.device('cpu'))\n\t    source = {k: v for k, v in source.items() if '__weight_mma_mask' not in k}\n\t    template = model_normal.parameters_dict()\n\t    dest = dict()\n\t    pending_dcn = dict()\n", "    for k, v in source.items():\n\t        if '.dcnpack.' in k:\n\t            module, name = k.split('.dcnpack.')\n\t            if module in pending_dcn:\n\t                pending_dcn[module][name] = v.numpy()\n\t            else:\n\t                pending_dcn[module] = {name: v.numpy()}\n\t            continue\n\t        for name in rewrite_names:\n\t            k = k.replace(name, name + 'conv.')\n", "        for re_name in rewrite_names_re:\n\t            k = re.sub(re_name, \"\\\\1conv.\", k)\n\t        if k in template:\n\t            dest[k] = ms.Parameter(v.numpy())\n\t        else:\n\t            print(f\"Unknown parameter {k} ignored.\")\n\t    for m, ws in pending_dcn.items():\n\t        for name, w in transform_dcnpack(ws).items():\n\t            dest[f'{m}.dcnpack.{name}'] = ms.Parameter(w)\n\t    print(ms.load_param_into_net(model_normal, dest, strict_load=True))\n", "    ms.save_checkpoint(model_normal, ms_normal)\n\t    print('Done normal model')\n\t    # sep model: concat + conv is separated to multiple conv + add, to reduce memory footprint\n\t    model_separate = model.cycmunet_sep(args)\n\t    template = model_separate.parameters_dict()\n\t    dest = dict()\n\t    pending_dcn = dict()\n\t    def filter_catconv(k, tensor):\n\t        for name, n in rewrite_names.items():\n\t            if name in k:\n", "                t = ms.Parameter(tensor.numpy())\n\t                if k.endswith('.weight'):\n\t                    dest.update({k.replace(name, f'{name}convs.{i}.'): ms.Parameter(v) for i, v in\n\t                                 enumerate(ops.split(t, axis=1, output_num=n))})\n\t                elif k.endswith('.bias'):\n\t                    dest[k.replace(name, name + 'convs.0.')] = t\n\t                return True\n\t        for name, get_n in rewrite_names_re.items():\n\t            search_result = re.search(name, k)\n\t            if not search_result:\n", "                continue\n\t            n = get_n(search_result)\n\t            t = ms.Parameter(tensor.numpy())\n\t            if k.endswith('.weight'):\n\t                dest.update({re.sub(name, f'\\\\1convs.{i}.', k): ms.Parameter(v) for i, v in\n\t                             enumerate(ops.split(t, axis=1, output_num=n))})\n\t            elif k.endswith('.bias'):\n\t                dest[re.sub(name, f'\\\\1convs.0.', k)] = t\n\t            return True\n\t        return False\n", "    for k, v in source.items():\n\t        if '.dcnpack.' in k:\n\t            module, name = k.split('.dcnpack.')\n\t            if module in pending_dcn:\n\t                pending_dcn[module][name] = v.numpy()\n\t            else:\n\t                pending_dcn[module] = {name: v.numpy()}\n\t            continue\n\t        if filter_catconv(k, v):\n\t            continue\n", "        if k in template:\n\t            dest[k] = ms.Parameter(v.numpy())\n\t        else:\n\t            print(f\"Unknown parameter {k} ignored.\")\n\t    for m, ws in pending_dcn.items():\n\t        for name, w in transform_dcnpack(ws).items():\n\t            dest[f'{m}.dcnpack.{name}'] = ms.Parameter(w)\n\t    print(ms.load_param_into_net(model_separate, dest, strict_load=True))\n\t    ms.save_checkpoint(model_separate, ms_sep)\n\t    print('Done separate model')\n"]}
{"filename": "mindspore/model/model.py", "chunked_list": ["from collections.abc import Iterable\n\timport mindspore as ms\n\tfrom mindspore import nn, ops\n\tfrom .deform_conv import DCN_sep_compat\n\t# half_pixel not supported on cpu now. use align_corners for test temporarily.\n\t# coordinate_transformation_mode = 'half_pixel'\n\tcoordinate_transformation_mode = 'align_corners'\n\tdef Conv2d(in_channels,\n\t           out_channels,\n\t           kernel_size,\n", "           stride=1,\n\t           padding=0,\n\t           dilation=1,\n\t           has_bias=True):\n\t    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n\t                     pad_mode='pad', padding=padding, dilation=dilation, has_bias=has_bias)\n\tdef Conv2dTranspose(in_channels,\n\t                    out_channels,\n\t                    kernel_size,\n\t                    stride=1,\n", "                    dilation=1):\n\t    return nn.Conv2dTranspose(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n\t                              stride=stride, pad_mode='same', dilation=dilation, has_bias=True)\n\tdef float_tuple(*tuple_):\n\t    tuple_ = tuple_[0] if isinstance(tuple_[0], Iterable) else tuple_\n\t    return tuple(float(i) for i in tuple_)\n\tdef transpose(mat):\n\t    count = len(mat[0])\n\t    ret = [[] for _ in range(count)]\n\t    for i in mat:\n", "        for j in range(count):\n\t            ret[j].append(i[j])\n\t    return ret\n\tdef cat_simp(elements, axis=1):\n\t    if len(elements) == 1:\n\t        return elements[0]\n\t    else:\n\t        return ms.ops.concat(elements, axis=axis)\n\tclass CatConv(nn.Cell):\n\t    def __init__(self, in_channels, in_count, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n", "        super().__init__()\n\t        self.arg_pack = {\n\t            'in_channels': in_channels,\n\t            'in_count': in_count,\n\t            'out_channels': out_channels,\n\t            'kernel_size': kernel_size,\n\t            'stride': stride,\n\t            'padding': padding,\n\t            'dilation': dilation\n\t        }\n", "        self.in_count = in_count\n\t        self.conv = Conv2d(in_channels * in_count, out_channels, kernel_size, stride, padding, dilation)\n\t    def construct(self, *inputs):\n\t        return self.conv(ops.concat(inputs, axis=1))\n\tclass Pro_align(nn.Cell):\n\t    def __init__(self, args):\n\t        super(Pro_align, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.conv1x1 = CatConv(self.nf, 3, self.nf, 1, 1, 0)\n", "        self.conv3x3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n\t        self.conv1_3x3 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def construct(self, l1, l2, l3):\n\t        r1 = self.lrelu(self.conv3x3(l1))\n\t        r2 = self.lrelu(self.conv3x3(l2))\n\t        r3 = self.lrelu(self.conv3x3(l3))\n\t        fuse = self.lrelu(self.conv1x1(r1, r2, r3))\n\t        r1 = self.lrelu(self.conv1_3x3(r1, fuse))\n\t        r2 = self.lrelu(self.conv1_3x3(r2, fuse))\n", "        r3 = self.lrelu(self.conv1_3x3(r3, fuse))\n\t        return l1 + r1, l2 + r2, l3 + r3\n\tclass SR(nn.Cell):\n\t    def __init__(self, args):\n\t        super(SR, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n\t        self.Pro_align = Pro_align(args)\n\t        self.conv1x1 = Conv2d(self.nf, self.nf, 1, 1, 0)\n", "        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def upsample(self, x):\n\t        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n\t                            coordinate_transformation_mode=coordinate_transformation_mode)\n\t        return self.lrelu(self.conv1x1(x))\n\t    def construct(self, l1, l2, l3):\n\t        l1, l2, l3 = self.Pro_align(l1, l2, l3)\n\t        return tuple(self.upsample(i) for i in (l1, l2, l3))\n\tclass DR(nn.Cell):\n\t    def __init__(self, args):\n", "        super(DR, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.factor = (1, 1, 1 / self.args.upscale_factor, 1 / self.args.upscale_factor)\n\t        self.Pro_align = Pro_align(args)\n\t        self.conv = Conv2d(self.nf, self.nf, 1, 1, 0)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def downsample(self, x):\n\t        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n\t                            coordinate_transformation_mode=coordinate_transformation_mode)\n", "        return self.lrelu(self.conv(x))\n\t    def construct(self, l1, l2, l3):\n\t        l1 = self.downsample(l1)\n\t        l2 = self.downsample(l2)\n\t        l3 = self.downsample(l3)\n\t        return self.Pro_align(l1, l2, l3)\n\tclass Up_projection(nn.Cell):\n\t    def __init__(self, args):\n\t        super(Up_projection, self).__init__()\n\t        self.args = args\n", "        self.SR = SR(args)\n\t        self.DR = DR(args)\n\t        self.SR1 = SR(args)\n\t    def construct(self, l1, l2, l3):\n\t        h1, h2, h3 = self.SR(l1, l2, l3)\n\t        d1, d2, d3 = self.DR(h1, h2, h3)\n\t        r1, r2, r3 = d1 - l1, d2 - l2, d3 - l3\n\t        s1, s2, s3 = self.SR1(r1, r2, r3)\n\t        return h1 + s1, h2 + s3, h3 + s3\n\tclass Down_projection(nn.Cell):\n", "    def __init__(self, args):\n\t        super(Down_projection, self).__init__()\n\t        self.args = args\n\t        self.SR = SR(args)\n\t        self.DR = DR(args)\n\t        self.DR1 = DR(args)\n\t    def construct(self, h1, h2, h3):\n\t        l1, l2, l3 = self.DR(h1, h2, h3)\n\t        s1, s2, s3 = self.SR(l1, l2, l3)\n\t        r1, r2, r3 = s1 - h1, s2 - h2, s3 - h3\n", "        d1, d2, d3 = self.DR1(r1, r2, r3)\n\t        return l1 + d1, l2 + d2, l3 + d3\n\tclass FusionPyramidLayer(nn.Cell):\n\t    def __init__(self, args, first_layer: bool):\n\t        super(FusionPyramidLayer, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n\t        self.offset_conv1 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\t        self.offset_conv3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n", "        self.dcnpack = DCN_sep_compat(self.nf, self.nf, self.nf, 3, stride=1, padding=1, dilation=1,\n\t                                      deformable_groups=self.groups)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t        if not first_layer:\n\t            self.offset_conv2 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\t            self.fea_conv = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\t    def construct(self, current_sources, last_offset, last_feature):\n\t        offset = self.lrelu(self.offset_conv1(*current_sources))\n\t        if last_offset is not None:\n\t            last_offset = ops.interpolate(last_offset, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n", "                                          coordinate_transformation_mode=coordinate_transformation_mode)\n\t            offset = self.lrelu(self.offset_conv2(offset, last_offset * 2))\n\t        offset = self.lrelu(self.offset_conv3(offset))\n\t        feature = self.dcnpack(current_sources[0], offset)\n\t        if last_feature is not None:\n\t            last_feature = ops.interpolate(last_feature, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n\t                                           coordinate_transformation_mode=coordinate_transformation_mode)\n\t            feature = self.fea_conv(feature, last_feature)\n\t        feature = self.lrelu(feature)\n\t        return offset, feature\n", "class ResidualBlock_noBN(nn.Cell):\n\t    '''Residual block w/o BN\n\t    ---Conv-ReLU-Conv-+-\n\t     |________________|\n\t    '''\n\t    def __init__(self, nf=64):\n\t        super(ResidualBlock_noBN, self).__init__()\n\t        self.conv1 = Conv2d(nf, nf, 3, 1, 1)\n\t        self.conv2 = Conv2d(nf, nf, 3, 1, 1)\n\t        self.relu = ops.ReLU()\n", "        # TODO: initialization\n\t        # initialize_weights([self.conv1, self.conv2], 0.1)\n\t    def construct(self, x):\n\t        identity = x\n\t        out = self.relu(self.conv1(x))\n\t        out = self.conv2(out)\n\t        return identity + out\n\tclass cycmunet_head(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_head, self).__init__()\n", "        self.args = args\n\t        self.nf = self.args.nf\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t        if self.args.format == 'rgb':\n\t            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n\t            self._construct = self.construct_rgb\n\t        elif self.args.format == 'yuv444':\n\t            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n\t            self._construct = self.construct_yuv444\n\t        elif self.args.format == 'yuv422':\n", "            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n\t            self.conv_up = Conv2dTranspose(2, self.nf, (1, 3), (1, 2))\n\t            self._construct = self.construct_yuv42x\n\t        elif self.args.format == 'yuv420':\n\t            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n\t            self.conv_up = Conv2dTranspose(2, self.nf, 3, 2)\n\t            self._construct = self.construct_yuv42x\n\t        else:\n\t            raise ValueError(f'unknown input pixel format: {self.args.format}')\n\t    def construct_rgb(self, x):\n", "        x = self.lrelu(self.conv_first(x))\n\t        return x\n\t    def construct_yuv444(self, yuv):\n\t        x = ms.ops.concat(yuv, axis=1)\n\t        x = self.lrelu(self.conv_first(x))\n\t        return x\n\t    def construct_yuv42x(self, yuv):\n\t        y, uv = yuv\n\t        y = self.conv_first_y(y)\n\t        uv = self.conv_up(uv)\n", "        x = self.lrelu(y + uv)\n\t        return x\n\t    def construct(self, *args):\n\t        return self._construct(*args)\n\tclass cycmunet_feature_extract(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_feature_extract, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n", "        self.layers = self.args.layers\n\t        self.front_RBs = 5\n\t        self.feature_extraction = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.front_RBs)))\n\t        self.fea_conv1s = nn.CellList([Conv2d(self.nf, self.nf, 3, 2, 1) for _ in range(self.layers - 1)])\n\t        self.fea_conv2s = nn.CellList([Conv2d(self.nf, self.nf, 3, 1, 1) for _ in range(self.layers - 1)])\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def construct(self, x):\n\t        features = [self.feature_extraction(x)]\n\t        for i in range(self.layers - 1):\n\t            feature = features[-1]\n", "            feature = self.lrelu(self.fea_conv1s[i](feature))\n\t            feature = self.lrelu(self.fea_conv2s[i](feature))\n\t            features.append(feature)\n\t        return tuple(features[::-1])  # lowest dimension layer at first\n\tclass cycmunet_feature_fusion(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_feature_fusion, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n", "        self.layers = self.args.layers\n\t        # from small to big.\n\t        self.modules12 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n\t        self.modules21 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n\t        self.fusion = CatConv(self.nf, 2, self.nf, 1, 1)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    @staticmethod\n\t    def fuse_features(modules, f1, f2):\n\t        offset, feature = None, None\n\t        for idx, sources in enumerate(zip(f1, f2)):\n", "            offset, feature = modules[idx](sources, offset, feature)\n\t        return feature\n\t    def construct(self, f1, f2):\n\t        feature1 = self.fuse_features(self.modules12, f1, f2)\n\t        feature2 = self.fuse_features(self.modules21, f2, f1)\n\t        fused_feature = self.fusion(feature1, feature2)\n\t        return fused_feature, None  # TODO detect scene change here\n\tclass cycmunet_mutual_cycle(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_mutual_cycle, self).__init__()\n", "        self.args = args\n\t        self.nf = self.args.nf\n\t        self.cycle_count = self.args.cycle_count\n\t        self.all_frames = self.args.all_frames\n\t        self.merge = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n\t        self.merge1 = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n\t        self.down = nn.CellList([Down_projection(args) for _ in range(self.cycle_count)])\n\t        self.up = nn.CellList([Up_projection(args) for _ in range(self.cycle_count + 1)])\n\t        self.conv = CatConv(self.nf, 2 * self.cycle_count + 1, self.nf, 1, 1, 0)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n", "    def construct(self, lf0, lf1, lf2):\n\t        l_out, h_out = [(lf0, lf1, lf2)], []\n\t        l_feats = []\n\t        for j1 in range(self.cycle_count):\n\t            l_feats = tuple(self.lrelu(self.merge[j1](*frame_outs)) for frame_outs in transpose(l_out))\n\t            h_feat = self.up[j1](*l_feats)\n\t            h_out.append(h_feat)\n\t            h_feats = tuple(self.lrelu(self.merge1[j1](*frame_outs)) for frame_outs in transpose(h_out))\n\t            l_feat = self.down[j1](*h_feats)\n\t            l_out.append(l_feat)\n", "        lf_out, hf_out = [l_out[-1]], []\n\t        for j2 in range(self.cycle_count):\n\t            l_feats = tuple(self.lrelu(self.merge[j2](*frame_outs)) for frame_outs in transpose(lf_out))\n\t            h_feat = self.up[j2](*l_feats)\n\t            hf_out.append(h_feat)\n\t            l_feat = self.down[j2](*h_feat)\n\t            lf_out.append(l_feat)\n\t        hf_out.append(self.up[self.cycle_count](*l_feats))\n\t        if self.all_frames:\n\t            h_outs = transpose(h_out + hf_out)  # packed 3 frames\n", "            _, l1_out, _ = transpose(l_out + lf_out[1:])\n\t            h_outs = list(self.lrelu(self.conv(*h_frame)) for h_frame in h_outs)\n\t            l1_out = self.lrelu(self.conv(*l1_out))\n\t            return tuple(h_outs + [l1_out])\n\t        else:\n\t            h1_out, h2_out, _ = transpose(h_out + hf_out)\n\t            h1_out = self.lrelu(self.conv(*h1_out))\n\t            h2_out = self.lrelu(self.conv(*h2_out))\n\t            return h1_out, h2_out\n\tclass cycmunet_feature_recon(nn.Cell):\n", "    def __init__(self, args):\n\t        super(cycmunet_feature_recon, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.back_RBs = 40\n\t        self.recon_trunk = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.back_RBs)))\n\t    def construct(self, x):\n\t        out = self.recon_trunk(x)\n\t        return out\n\tclass cycmunet_tail(nn.Cell):\n", "    def __init__(self, args):\n\t        super(cycmunet_tail, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        if self.args.format == 'rgb':\n\t            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n\t            self._construct = self.construct_rgb\n\t        elif self.args.format == 'yuv444':\n\t            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n\t            self._construct = self.construct_yuv444\n", "        elif self.args.format == 'yuv422':\n\t            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n\t            self.conv_last_uv = Conv2d(self.nf, 2, (1, 3), (1, 2), (0, 0, 1, 1))\n\t            self._construct = self.construct_yuv42x\n\t        elif self.args.format == 'yuv420':\n\t            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n\t            self.conv_last_uv = Conv2d(self.nf, 2, 3, 2, 1)\n\t            self._construct = self.construct_yuv42x\n\t        else:\n\t            raise ValueError(f'unknown input pixel format: {self.args.format}')\n", "    def construct_rgb(self, x):\n\t        out = self.conv_last2(x)\n\t        return out,\n\t    def construct_yuv444(self, x):\n\t        out = self.conv_last2(x)\n\t        return out[:, :1, ...], out[:, 1:, ...]\n\t    def construct_yuv42x(self, x):\n\t        y = self.conv_last_y(x)\n\t        uv = self.conv_last_uv(x)\n\t        return y, uv\n", "    def construct(self, *args):\n\t        return self._construct(*args)\n\tclass CycMuNet(nn.Cell):\n\t    def __init__(self, args):\n\t        super(CycMuNet, self).__init__()\n\t        self.args = args\n\t        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n\t        self.upsample_mode = 'bilinear'\n\t        self.batch_mode = self.args.batch_mode\n\t        self.stop_at_conf = self.args.stop_at_conf\n", "        self.all_frames = self.args.all_frames\n\t        self.layers = self.args.layers\n\t        self.head = cycmunet_head(args)\n\t        self.fe = cycmunet_feature_extract(args)\n\t        self.ff = cycmunet_feature_fusion(args)\n\t        self.mu = cycmunet_mutual_cycle(args)\n\t        self.fr = cycmunet_feature_recon(args)\n\t        self.tail = cycmunet_tail(args)\n\t    def merge_hf(self, lf, hf):\n\t        return ops.interpolate(lf, scales=float_tuple(self.factor), mode='bilinear',\n", "                               coordinate_transformation_mode=coordinate_transformation_mode) + hf\n\t    def mu_fr(self, *lf):\n\t        mu_out = self.mu(*lf)\n\t        if self.all_frames:\n\t            # *hf, lf1 = mu_out\n\t            hf, lf1 = mu_out[:-1], mu_out[-1]\n\t            hf = tuple(self.merge_hf(l, self.fr(h)) for l, h in zip(lf, hf))\n\t            outs = list(hf) + [lf1]\n\t            outs = tuple(self.tail(i) for i in outs)\n\t        else:\n", "            outs = tuple(self.tail(self.merge_hf(l, self.fr(h))) for l, h in zip(lf, mu_out))\n\t        return outs\n\t    def construct_batch(self, f0, f2):\n\t        lf0s = self.fe(self.head(f0))\n\t        lf2s = self.fe(self.head(f2))\n\t        lf0, lf2 = lf0s[self.layers - 1], lf2s[self.layers - 1]\n\t        lf1, _ = self.ff(lf0s, lf2s)\n\t        if self.stop_at_conf:  # TODO detect frame difference and exit if too big\n\t            return\n\t        return self.mu_fr(lf0, lf1, lf2)\n", "    def construct_sequence(self, x_or_yuv):\n\t        x = self.head(x_or_yuv)\n\t        ls = self.fe(x)\n\t        lf0, lf2 = ls[2][:-1], ls[2][1:]\n\t        lf1, _ = self.ff([layer[:-1] for layer in ls], [layer[1:] for layer in ls])\n\t        return self.mu_fr(lf0, lf1, lf2)\n\t    def construct(self, f0, f2=None):\n\t        if self.batch_mode == 'batch':\n\t            outs = self.construct_batch(f0, f2)\n\t        elif self.batch_mode == 'sequence':\n", "            outs = self.construct_sequence(f0)\n\t        else:\n\t            raise ValueError(f\"Invalid batch_mode: {self.args.batch_mode}\")\n\t        return outs\n"]}
{"filename": "mindspore/model/model_sep.py", "chunked_list": ["from collections.abc import Iterable\n\timport mindspore as ms\n\tfrom mindspore import nn, ops\n\tfrom .deform_conv import DCN_sep_compat\n\t# half_pixel not supported on cpu now. use align_corners for test temporarily.\n\tcoordinate_transformation_mode = 'half_pixel'\n\t# coordinate_transformation_mode = 'align_corners'\n\tdef Conv2d(in_channels,\n\t           out_channels,\n\t           kernel_size,\n", "           stride=1,\n\t           padding=0,\n\t           dilation=1,\n\t           has_bias=True):\n\t    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n\t                     pad_mode='pad', padding=padding, dilation=dilation, has_bias=has_bias)\n\tdef Conv2dTranspose(in_channels,\n\t                    out_channels,\n\t                    kernel_size,\n\t                    stride=1,\n", "                    dilation=1):\n\t    return nn.Conv2dTranspose(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n\t                              stride=stride, pad_mode='same', dilation=dilation, has_bias=True)\n\tdef float_tuple(*tuple_):\n\t    tuple_ = tuple_[0] if isinstance(tuple_[0], Iterable) else tuple_\n\t    return tuple(float(i) for i in tuple_)\n\tdef transpose(mat):\n\t    count = len(mat[0])\n\t    ret = [[] for _ in range(count)]\n\t    for i in mat:\n", "        for j in range(count):\n\t            ret[j].append(i[j])\n\t    return ret\n\tdef cat_simp(elements, axis=1):\n\t    if len(elements) == 1:\n\t        return elements[0]\n\t    else:\n\t        return ms.ops.concat(elements, axis=axis)\n\tclass CatConv(nn.Cell):\n\t    def __init__(self, in_channels, in_count, out_channels, kernel_size, stride=1, padding=0, dilation=1):\n", "        super().__init__()\n\t        self.convs = nn.CellList(\n\t            [Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, i == 0) for i in\n\t             range(in_count)])\n\t    def construct(self, *inputs):\n\t        output = None\n\t        for idx, inp in enumerate(inputs):\n\t            if output is None:\n\t                output = self.convs[idx](inp)\n\t            else:\n", "                output += self.convs[idx](inp)\n\t        return output\n\tclass Pro_align(nn.Cell):\n\t    def __init__(self, args):\n\t        super(Pro_align, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.conv1x1 = CatConv(self.nf, 3, self.nf, 1, 1, 0)\n\t        self.conv3x3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n\t        self.conv1_3x3 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n", "        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def construct(self, l1, l2, l3):\n\t        r1 = self.lrelu(self.conv3x3(l1))\n\t        r2 = self.lrelu(self.conv3x3(l2))\n\t        r3 = self.lrelu(self.conv3x3(l3))\n\t        fuse = self.lrelu(self.conv1x1(r1, r2, r3))\n\t        r1 = self.lrelu(self.conv1_3x3(r1, fuse))\n\t        r2 = self.lrelu(self.conv1_3x3(r2, fuse))\n\t        r3 = self.lrelu(self.conv1_3x3(r3, fuse))\n\t        return l1 + r1, l2 + r2, l3 + r3\n", "class SR(nn.Cell):\n\t    def __init__(self, args):\n\t        super(SR, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n\t        self.Pro_align = Pro_align(args)\n\t        self.conv1x1 = Conv2d(self.nf, self.nf, 1, 1, 0)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def upsample(self, x):\n", "        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n\t                            coordinate_transformation_mode=coordinate_transformation_mode)\n\t        return self.lrelu(self.conv1x1(x))\n\t    def construct(self, l1, l2, l3):\n\t        l1, l2, l3 = self.Pro_align(l1, l2, l3)\n\t        return tuple(self.upsample(i) for i in (l1, l2, l3))\n\tclass DR(nn.Cell):\n\t    def __init__(self, args):\n\t        super(DR, self).__init__()\n\t        self.args = args\n", "        self.nf = self.args.nf\n\t        self.factor = (1, 1, 1 / self.args.upscale_factor, 1 / self.args.upscale_factor)\n\t        self.Pro_align = Pro_align(args)\n\t        self.conv = Conv2d(self.nf, self.nf, 1, 1, 0)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def downsample(self, x):\n\t        x = ops.interpolate(x, scales=float_tuple(self.factor), mode='bilinear',\n\t                            coordinate_transformation_mode=coordinate_transformation_mode)\n\t        return self.lrelu(self.conv(x))\n\t    def construct(self, l1, l2, l3):\n", "        l1 = self.downsample(l1)\n\t        l2 = self.downsample(l2)\n\t        l3 = self.downsample(l3)\n\t        return self.Pro_align(l1, l2, l3)\n\tclass Up_projection(nn.Cell):\n\t    def __init__(self, args):\n\t        super(Up_projection, self).__init__()\n\t        self.args = args\n\t        self.SR = SR(args)\n\t        self.DR = DR(args)\n", "        self.SR1 = SR(args)\n\t    def construct(self, l1, l2, l3):\n\t        h1, h2, h3 = self.SR(l1, l2, l3)\n\t        d1, d2, d3 = self.DR(h1, h2, h3)\n\t        r1, r2, r3 = d1 - l1, d2 - l2, d3 - l3\n\t        s1, s2, s3 = self.SR1(r1, r2, r3)\n\t        return h1 + s1, h2 + s3, h3 + s3\n\tclass Down_projection(nn.Cell):\n\t    def __init__(self, args):\n\t        super(Down_projection, self).__init__()\n", "        self.args = args\n\t        self.SR = SR(args)\n\t        self.DR = DR(args)\n\t        self.DR1 = DR(args)\n\t    def construct(self, h1, h2, h3):\n\t        l1, l2, l3 = self.DR(h1, h2, h3)\n\t        s1, s2, s3 = self.SR(l1, l2, l3)\n\t        r1, r2, r3 = s1 - h1, s2 - h2, s3 - h3\n\t        d1, d2, d3 = self.DR1(r1, r2, r3)\n\t        return l1 + d1, l2 + d2, l3 + d3\n", "class FusionPyramidLayer(nn.Cell):\n\t    def __init__(self, args, first_layer: bool):\n\t        super(FusionPyramidLayer, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n\t        self.offset_conv1 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\t        self.offset_conv3 = Conv2d(self.nf, self.nf, 3, 1, 1)\n\t        self.dcnpack = DCN_sep_compat(self.nf, self.nf, self.nf, 3, stride=1, padding=1, dilation=1,\n\t                                      deformable_groups=self.groups)\n", "        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t        if not first_layer:\n\t            self.offset_conv2 = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\t            self.fea_conv = CatConv(self.nf, 2, self.nf, 3, 1, 1)\n\t    def construct(self, current_sources, last_offset, last_feature):\n\t        offset = self.lrelu(self.offset_conv1(*current_sources))\n\t        if last_offset is not None:\n\t            last_offset = ops.interpolate(last_offset, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n\t                                          coordinate_transformation_mode=coordinate_transformation_mode)\n\t            offset = self.lrelu(self.offset_conv2(offset, last_offset * 2))\n", "        offset = self.lrelu(self.offset_conv3(offset))\n\t        feature = self.dcnpack(current_sources[0], offset)\n\t        if last_feature is not None:\n\t            last_feature = ops.interpolate(last_feature, scales=float_tuple(1, 1, 2, 2), mode='bilinear',\n\t                                           coordinate_transformation_mode=coordinate_transformation_mode)\n\t            feature = self.fea_conv(feature, last_feature)\n\t        feature = self.lrelu(feature)\n\t        return offset, feature\n\tclass ResidualBlock_noBN(nn.Cell):\n\t    '''Residual block w/o BN\n", "    ---Conv-ReLU-Conv-+-\n\t     |________________|\n\t    '''\n\t    def __init__(self, nf=64):\n\t        super(ResidualBlock_noBN, self).__init__()\n\t        self.conv1 = Conv2d(nf, nf, 3, 1, 1)\n\t        self.conv2 = Conv2d(nf, nf, 3, 1, 1)\n\t        self.relu = ops.ReLU()\n\t        # TODO: initialization\n\t        # initialize_weights([self.conv1, self.conv2], 0.1)\n", "    def construct(self, x):\n\t        identity = x\n\t        out = self.relu(self.conv1(x))\n\t        out = self.conv2(out)\n\t        return identity + out\n\tclass cycmunet_head(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_head, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n", "        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t        if self.args.format == 'rgb':\n\t            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n\t            self._construct = self.construct_rgb\n\t        elif self.args.format == 'yuv444':\n\t            self.conv_first = Conv2d(3, self.nf, 3, 1, 1)\n\t            self._construct = self.construct_yuv444\n\t        elif self.args.format == 'yuv422':\n\t            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n\t            self.conv_up = Conv2dTranspose(2, self.nf, (1, 3), (1, 2))\n", "            self._construct = self.construct_yuv42x\n\t        elif self.args.format == 'yuv420':\n\t            self.conv_first_y = Conv2d(1, self.nf, 3, 1, 1)\n\t            self.conv_up = Conv2dTranspose(2, self.nf, 3, 2)\n\t            self._construct = self.construct_yuv42x\n\t        else:\n\t            raise ValueError(f'unknown input pixel format: {self.args.format}')\n\t    def construct_rgb(self, x):\n\t        x = self.lrelu(self.conv_first(x))\n\t        return x\n", "    def construct_yuv444(self, yuv):\n\t        x = ms.ops.concat(yuv, axis=1)\n\t        x = self.lrelu(self.conv_first(x))\n\t        return x\n\t    def construct_yuv42x(self, yuv):\n\t        y, uv = yuv\n\t        y = self.conv_first_y(y)\n\t        uv = self.conv_up(uv)\n\t        x = self.lrelu(y + uv)\n\t        return x\n", "    def construct(self, *args):\n\t        return self._construct(*args)\n\tclass cycmunet_feature_extract(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_feature_extract, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n\t        self.layers = self.args.layers\n\t        self.front_RBs = 5\n", "        self.feature_extraction = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.front_RBs)))\n\t        self.fea_conv1s = nn.CellList([Conv2d(self.nf, self.nf, 3, 2, 1) for _ in range(self.layers - 1)])\n\t        self.fea_conv2s = nn.CellList([Conv2d(self.nf, self.nf, 3, 1, 1) for _ in range(self.layers - 1)])\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def construct(self, x):\n\t        features = [self.feature_extraction(x)]\n\t        for i in range(self.layers - 1):\n\t            feature = features[-1]\n\t            feature = self.lrelu(self.fea_conv1s[i](feature))\n\t            feature = self.lrelu(self.fea_conv2s[i](feature))\n", "            features.append(feature)\n\t        return tuple(features[::-1])  # lowest dimension layer at first\n\tclass cycmunet_feature_fusion(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_feature_fusion, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n\t        self.layers = self.args.layers\n\t        # from small to big.\n", "        self.modules12 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n\t        self.modules21 = nn.CellList([FusionPyramidLayer(args, i == 0) for i in range(self.layers)])\n\t        self.fusion = CatConv(self.nf, 2, self.nf, 1, 1)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    @staticmethod\n\t    def fuse_features(modules, f1, f2):\n\t        offset, feature = None, None\n\t        for idx, sources in enumerate(zip(f1, f2)):\n\t            offset, feature = modules[idx](sources, offset, feature)\n\t        return feature\n", "    def construct(self, f1, f2):\n\t        feature1 = self.fuse_features(self.modules12, f1, f2)\n\t        feature2 = self.fuse_features(self.modules21, f2, f1)\n\t        fused_feature = self.fusion(feature1, feature2)\n\t        return fused_feature, None  # TODO detect scene change here\n\tclass cycmunet_mutual_cycle(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_mutual_cycle, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n", "        self.cycle_count = self.args.cycle_count\n\t        self.all_frames = self.args.all_frames\n\t        self.merge = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n\t        self.merge1 = nn.CellList([CatConv(64, i + 1, 64, 1, 1, 0) for i in range(self.cycle_count)])\n\t        self.down = nn.CellList([Down_projection(args) for _ in range(self.cycle_count)])\n\t        self.up = nn.CellList([Up_projection(args) for _ in range(self.cycle_count + 1)])\n\t        self.conv = CatConv(self.nf, 2 * self.cycle_count + 1, self.nf, 1, 1, 0)\n\t        self.lrelu = nn.LeakyReLU(alpha=0.1)\n\t    def construct(self, lf0, lf1, lf2):\n\t        l_out, h_out = [(lf0, lf1, lf2)], []\n", "        l_feats = []\n\t        for j1 in range(self.cycle_count):\n\t            l_feats = tuple(self.lrelu(self.merge[j1](*frame_outs)) for frame_outs in transpose(l_out))\n\t            h_feat = self.up[j1](*l_feats)\n\t            h_out.append(h_feat)\n\t            h_feats = tuple(self.lrelu(self.merge1[j1](*frame_outs)) for frame_outs in transpose(h_out))\n\t            l_feat = self.down[j1](*h_feats)\n\t            l_out.append(l_feat)\n\t        lf_out, hf_out = [l_out[-1]], []\n\t        for j2 in range(self.cycle_count):\n", "            l_feats = tuple(self.lrelu(self.merge[j2](*frame_outs)) for frame_outs in transpose(lf_out))\n\t            h_feat = self.up[j2](*l_feats)\n\t            hf_out.append(h_feat)\n\t            l_feat = self.down[j2](*h_feat)\n\t            lf_out.append(l_feat)\n\t        hf_out.append(self.up[self.cycle_count](*l_feats))\n\t        if self.all_frames:\n\t            h_outs = transpose(h_out + hf_out)  # packed 3 frames\n\t            _, l1_out, _ = transpose(l_out + lf_out[1:])\n\t            h_outs = list(self.lrelu(self.conv(*h_frame)) for h_frame in h_outs)\n", "            l1_out = self.lrelu(self.conv(*l1_out))\n\t            return tuple(h_outs + [l1_out])\n\t        else:\n\t            h1_out, h2_out, _ = transpose(h_out + hf_out)\n\t            h1_out = self.lrelu(self.conv(*h1_out))\n\t            h2_out = self.lrelu(self.conv(*h2_out))\n\t            return h1_out, h2_out\n\tclass cycmunet_feature_recon(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_feature_recon, self).__init__()\n", "        self.args = args\n\t        self.nf = self.args.nf\n\t        self.back_RBs = 40\n\t        self.recon_trunk = nn.SequentialCell(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.back_RBs)))\n\t    def construct(self, x):\n\t        out = self.recon_trunk(x)\n\t        return out\n\tclass cycmunet_tail(nn.Cell):\n\t    def __init__(self, args):\n\t        super(cycmunet_tail, self).__init__()\n", "        self.args = args\n\t        self.nf = self.args.nf\n\t        if self.args.format == 'rgb':\n\t            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n\t            self._construct = self.construct_rgb\n\t        elif self.args.format == 'yuv444':\n\t            self.conv_last2 = Conv2d(self.nf, 3, 3, 1, 1)\n\t            self._construct = self.construct_yuv444\n\t        elif self.args.format == 'yuv422':\n\t            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n", "            self.conv_last_uv = Conv2d(self.nf, 2, (1, 3), (1, 2), (0, 0, 1, 1))\n\t            self._construct = self.construct_yuv42x\n\t        elif self.args.format == 'yuv420':\n\t            self.conv_last_y = Conv2d(self.nf, 1, 3, 1, 1)\n\t            self.conv_last_uv = Conv2d(self.nf, 2, 3, 2, 1)\n\t            self._construct = self.construct_yuv42x\n\t        else:\n\t            raise ValueError(f'unknown input pixel format: {self.args.format}')\n\t    def construct_rgb(self, x):\n\t        out = self.conv_last2(x)\n", "        return out,\n\t    def construct_yuv444(self, x):\n\t        out = self.conv_last2(x)\n\t        return out[:, :1, ...], out[:, 1:, ...]\n\t    def construct_yuv42x(self, x):\n\t        y = self.conv_last_y(x)\n\t        uv = self.conv_last_uv(x)\n\t        return y, uv\n\t    def construct(self, *args):\n\t        return self._construct(*args)\n", "class CycMuNet(nn.Cell):\n\t    def __init__(self, args):\n\t        super(CycMuNet, self).__init__()\n\t        self.args = args\n\t        self.factor = (1, 1, self.args.upscale_factor, self.args.upscale_factor)\n\t        self.upsample_mode = 'bilinear'\n\t        self.batch_mode = self.args.batch_mode\n\t        self.stop_at_conf = self.args.stop_at_conf\n\t        self.all_frames = self.args.all_frames\n\t        self.head = cycmunet_head(args)\n", "        self.fe = cycmunet_feature_extract(args)\n\t        self.ff = cycmunet_feature_fusion(args)\n\t        self.mu = cycmunet_mutual_cycle(args)\n\t        self.fr = cycmunet_feature_recon(args)\n\t        self.tail = cycmunet_tail(args)\n\t    def merge_hf(self, lf, hf):\n\t        return ops.interpolate(lf, scales=float_tuple(self.factor), mode='bilinear',\n\t                               coordinate_transformation_mode=coordinate_transformation_mode) + hf\n\t    def mu_fr(self, *lf):\n\t        mu_out = self.mu(*lf)\n", "        if self.all_frames:\n\t            # *hf, lf1 = mu_out\n\t            hf, lf1 = mu_out[:-1], mu_out[-1]\n\t            hf = tuple(self.merge_hf(l, self.fr(h)) for l, h in zip(lf, hf))\n\t            outs = list(hf) + [lf1]\n\t            outs = tuple(self.tail(i) for i in outs)\n\t        else:\n\t            outs = tuple(self.tail(self.merge_hf(l, self.fr(h))) for l, h in zip(lf, mu_out))\n\t        return outs\n\t    def construct_batch(self, f0, f2):\n", "        lf0s = self.fe(self.head(f0))\n\t        lf2s = self.fe(self.head(f2))\n\t        lf0, lf2 = lf0s[self.layers - 1], lf2s[self.layers - 1]\n\t        lf1, _ = self.ff(lf0s, lf2s)\n\t        if self.stop_at_conf:  # TODO detect frame difference and exit if too big\n\t            return\n\t        return self.mu_fr(lf0, lf1, lf2)\n\t    def construct_sequence(self, x_or_yuv):\n\t        x = self.head(x_or_yuv)\n\t        ls = self.fe(x)\n", "        lf0, lf2 = ls[2][:-1], ls[2][1:]\n\t        lf1, _ = self.ff([layer[:-1] for layer in ls], [layer[1:] for layer in ls])\n\t        return self.mu_fr(lf0, lf1, lf2)\n\t    def construct(self, f0, f2=None):\n\t        if self.batch_mode == 'batch':\n\t            outs = self.construct_batch(f0, f2)\n\t        elif self.batch_mode == 'sequence':\n\t            outs = self.construct_sequence(f0)\n\t        else:\n\t            raise ValueError(f\"Invalid batch_mode: {self.args.batch_mode}\")\n", "        return outs\n"]}
{"filename": "mindspore/model/train.py", "chunked_list": ["from mindspore import nn\n\tclass TrainModel(nn.Cell):\n\t    def __init__(self, network, loss):\n\t        super().__init__()\n\t        self.net = network\n\t        self.loss = loss\n\t    def loss_frame(self, expected, actual):\n\t        return self.loss(expected[0], actual[0]) + self.loss(expected[1], actual[1])\n\t    def construct(self, *data):\n\t        inputs = [data[6:8], data[10:12]]\n", "        expected = [data[0:2], data[2:4], data[4:6], data[8:10]]\n\t        actual = self.net(*inputs)\n\t        loss = self.loss_frame(expected[0], actual[0]) * 0.5 + \\\n\t               self.loss_frame(expected[1], actual[1]) + \\\n\t               self.loss_frame(expected[2], actual[2]) * 0.5 + \\\n\t               self.loss_frame(expected[3], actual[3])\n\t        return loss\n"]}
{"filename": "mindspore/model/__init__.py", "chunked_list": ["from .model import CycMuNet\n\tfrom .model_sep import CycMuNet as CycMuNetSep\n\tfrom .train import TrainModel\n"]}
{"filename": "mindspore/model/deform_conv.py", "chunked_list": ["import math\n\timport mindspore as ms\n\tfrom mindspore import nn, ops\n\tfrom mindspore.common import initializer as init\n\tfrom mindspore._checkparam import twice\n\tdef _Conv2d(in_channels,\n\t            out_channels,\n\t            kernel_size,\n\t            stride=1,\n\t            padding=0,\n", "            dilation=1):\n\t    return nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n\t                     pad_mode='pad', padding=padding, dilation=dilation, has_bias=True)\n\tclass DCN_sep(nn.Cell):\n\t    def __init__(self,\n\t                 in_channels,\n\t                 in_channels_features,\n\t                 out_channels,\n\t                 kernel_size,\n\t                 stride=1,\n", "                 padding=0,\n\t                 dilation=1,\n\t                 groups=1,\n\t                 deformable_groups=1,\n\t                 bias=True,\n\t                 mask=True):\n\t        super(DCN_sep, self).__init__()\n\t        kernel_size_ = twice(kernel_size)\n\t        self.dcn_weight = ms.Parameter(\n\t            ms.Tensor(\n", "                shape=(out_channels, in_channels // groups, kernel_size_[0], kernel_size_[1]),\n\t                dtype=ms.float32,\n\t                init=init.HeUniform(negative_slope=math.sqrt(5))\n\t            )\n\t        )\n\t        fan_in = in_channels // groups * kernel_size_[0] * kernel_size_[1]\n\t        bound = 1 / math.sqrt(fan_in)\n\t        self.dcn_bias = ms.Parameter(\n\t            ms.Tensor(\n\t                shape=(out_channels,),\n", "                dtype=ms.float32,\n\t                init=init.Uniform(bound)\n\t            )\n\t        ) if bias else None\n\t        self.dcn_kwargs = {\n\t            'kernel_size': kernel_size_,\n\t            'strides': (1, 1, *twice(stride)),\n\t            'padding': (padding,) * 4 if isinstance(padding, int) else padding,\n\t            'dilations': (1, 1, *twice(dilation)),\n\t            'groups': groups,\n", "            'deformable_groups': deformable_groups,\n\t            'modulated': mask\n\t        }\n\t        offset_channels = deformable_groups * kernel_size_[0] * kernel_size_[1]\n\t        self.conv_offset = _Conv2d(in_channels_features, offset_channels * 2, kernel_size=kernel_size,\n\t                                   stride=stride, padding=padding, dilation=dilation)\n\t        if mask:\n\t            self.conv_mask = _Conv2d(in_channels_features, offset_channels, kernel_size=kernel_size,\n\t                                     stride=stride, padding=padding, dilation=dilation)\n\t        else:\n", "            raise NotImplementedError()\n\t        self.relu = nn.ReLU()\n\t    def construct(self, input, feature):\n\t        offset = self.conv_offset(feature)\n\t        mask = ops.sigmoid(self.conv_mask(feature))\n\t        offsets = ops.concat([offset, mask], axis=1)\n\t        return ops.deformable_conv2d(input, self.dcn_weight, offsets, bias=self.dcn_bias, **self.dcn_kwargs)\n\t# Same as DCN_sep but compatible with Ascend.\n\t# Can be removed once deformable_groups can be values other than 1 on Ascend.\n\tclass DCN_sep_compat(nn.Cell):\n", "    def __init__(self,\n\t                 in_channels,\n\t                 in_channels_features,\n\t                 out_channels,\n\t                 kernel_size,\n\t                 stride=1,\n\t                 padding=0,\n\t                 dilation=1,\n\t                 groups=1,\n\t                 deformable_groups=1,\n", "                 bias=True,\n\t                 mask=True):\n\t        super(DCN_sep_compat, self).__init__()\n\t        if deformable_groups == 1:\n\t            raise ValueError(\"Use DCN_sep\")\n\t        if groups != 1:\n\t            raise NotImplementedError()\n\t        self.separated = groups != 1\n\t        kernel_size_ = twice(kernel_size)\n\t        self.deformable_groups = deformable_groups\n", "        self.dcn_weight = ms.Parameter(\n\t            ms.Tensor(\n\t                shape=(out_channels, in_channels // groups, kernel_size_[0], kernel_size_[1]),\n\t                dtype=ms.float32,\n\t                init=init.HeUniform(negative_slope=math.sqrt(5))\n\t            )\n\t        )\n\t        fan_in = in_channels // groups * kernel_size_[0] * kernel_size_[1]\n\t        bound = 1 / math.sqrt(fan_in)\n\t        self.dcn_bias = ms.Parameter(\n", "            ms.Tensor(\n\t                shape=(out_channels,),\n\t                dtype=ms.float32,\n\t                init=init.Uniform(bound)\n\t            )\n\t        ) if bias else None\n\t        self.dcn_kwargs = {\n\t            'kernel_size': kernel_size_,\n\t            'strides': (1, 1, *twice(stride)),\n\t            'padding': (padding,) * 4 if isinstance(padding, int) else padding,\n", "            'dilations': (1, 1, *twice(dilation)),\n\t            'groups': 1,\n\t            'deformable_groups': 1,\n\t            'modulated': mask is not None\n\t        }\n\t        offset_channels = deformable_groups * kernel_size_[0] * kernel_size_[1]\n\t        self.conv_offset = _Conv2d(in_channels_features, offset_channels * 2, kernel_size=kernel_size,\n\t                                   stride=stride, padding=padding, dilation=dilation)\n\t        if mask:\n\t            self.conv_mask = _Conv2d(in_channels_features, offset_channels, kernel_size=kernel_size,\n", "                                     stride=stride, padding=padding, dilation=dilation)\n\t        else:\n\t            raise NotImplementedError()\n\t        self.relu = nn.ReLU()\n\t    def construct(self, input, feature):\n\t        offset = self.conv_offset(feature)\n\t        mask = ops.sigmoid(self.conv_mask(feature))\n\t        offset_y, offset_x = ops.split(offset, axis=1, output_num=2)\n\t        inputs = ops.split(input, axis=1, output_num=self.deformable_groups)\n\t        dcn_weights = ops.split(self.dcn_weight, axis=1, output_num=self.deformable_groups)\n", "        offset_ys = ops.split(offset_y, axis=1, output_num=self.deformable_groups)\n\t        offset_xs = ops.split(offset_y, axis=1, output_num=self.deformable_groups)\n\t        masks = ops.split(mask, axis=1, output_num=self.deformable_groups)\n\t        output = None\n\t        for i in range(self.deformable_groups):\n\t            offsets = ops.concat([offset_ys[i], offset_xs[i], masks[i]], axis=1)\n\t            if output is None:\n\t                output = ops.deformable_conv2d(inputs[i], dcn_weights[i], offsets, bias=self.dcn_bias,\n\t                                               **self.dcn_kwargs)\n\t            else:\n", "                output += ops.deformable_conv2d(inputs[i], dcn_weights[i], offsets, bias=None, **self.dcn_kwargs)\n\t        return output"]}
{"filename": "mindspore/dataset/triplet.py", "chunked_list": ["import pathlib\n\timport random\n\tfrom typing import List\n\timport cv2\n\timport mindspore as ms\n\tfrom mindspore import dataset\n\tfrom mindspore.dataset import vision, transforms\n\timport numpy as np\n\tfrom PIL import Image, ImageFilter\n\tclass ImageTripletGenerator:\n", "    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n\t        self.dataset_base = pathlib.Path(index_file).parent\n\t        self.triplets = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n\t                         if i if not i.startswith('#')]\n\t        self.patch_size = patch_size\n\t        self.scale_factor = scale_factor\n\t        self.augment = augment\n\t        self.rand = random.Random(seed)\n\t    def _load_triplet(self, path):\n\t        path = self.dataset_base / \"sequences\" / path\n", "        images = [Image.open(path / f\"im{i + 1}.png\") for i in range(3)]\n\t        if not (images[0].size == images[1].size and images[0].size == images[2].size):\n\t            raise ValueError(\"triplet has different dimensions\")\n\t        return images\n\t    def _prepare_images(self, images: List[Image.Image]):\n\t        w, h = images[0].size\n\t        f = self.scale_factor\n\t        s = self.patch_size * f\n\t        dh, dw = self.rand.randrange(0, h - s, 2) * f, self.rand.randrange(0, w - s, 2) * f\n\t        images = [i.crop((dw, dh, dw + s, dh + s)) for i in images]\n", "        return images\n\t    trans_groups = {\n\t        'none': [None],\n\t        'rotate': [None, Image.ROTATE_90, Image.ROTATE_180, Image.ROTATE_270],\n\t        'mirror': [None, Image.FLIP_LEFT_RIGHT],\n\t        'flip': [None, Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM, Image.ROTATE_180],\n\t        'all': [None] + [e.value for e in Image.Transpose],\n\t    }\n\t    trans_names = [e.name for e in Image.Transpose]\n\t    def _augment_images(self, images: List[Image.Image], trans_mode='all'):\n", "        trans_action = 'none'\n\t        trans_op = self.rand.choice(self.trans_groups[trans_mode])\n\t        if trans_op is not None:\n\t            images = [i.transpose(trans_op) for i in images]\n\t            trans_action = self.trans_names[trans_op]\n\t        return images, trans_action\n\t    scale_filters = [Image.BILINEAR, Image.BICUBIC, Image.LANCZOS]\n\t    def _scale_images(self, images: List[Image.Image]):\n\t        f = self.scale_factor\n\t        return [i.resize((i.width // f, i.height // f), self.rand.choice(self.scale_filters)) for i in images]\n", "    def _degrade_images(self, images: List[Image.Image]):\n\t        degrade_action = None\n\t        decision = self.rand.randrange(4)\n\t        if decision == 1:\n\t            degrade_action = 'box'\n\t            arr = [np.array(Image.blend(j, j.copy().filter(ImageFilter.BoxBlur(1)), 0.5)) for j in images]\n\t        elif decision == 2:\n\t            degrade_action = 'gaussian'\n\t            radius = self.rand.random() * 2\n\t            arr = [np.array(j.filter(ImageFilter.GaussianBlur(radius))) for j in images]\n", "        elif decision == 3:\n\t            degrade_action = 'halo'\n\t            radius = 1 + self.rand.random() * 2\n\t            modulation = 0.1 + radius * 0.3\n\t            contour = [np.array(i.copy().filter(ImageFilter.CONTOUR).filter(ImageFilter.GaussianBlur(radius)))\n\t                       for i in images]\n\t            arr = [cv2.addWeighted(np.array(i), 1, j, modulation, 0) for i, j in zip(images, contour)]\n\t        else:\n\t            arr = [np.array(i) for i in images]\n\t        return arr, degrade_action\n", "    def __len__(self):\n\t        return len(self.triplets)\n\t    def __getitem__(self, idx):\n\t        triplet = self._load_triplet(self.triplets[idx])\n\t        triplet = self._prepare_images(triplet)  # crop to requested size\n\t        original, _ = self._augment_images(triplet)  # flip and rotates\n\t        lf1 = original[1]\n\t        lf1 = np.array(lf1.resize((lf1.width // self.scale_factor, lf1.height // self.scale_factor), Image.LANCZOS))\n\t        degraded, _ = self._degrade_images(self._scale_images([original[0], original[2]]))\n\t        degraded.insert(1, lf1)\n", "        return (*original, *degraded)\n\tdef ImageTripletDataset(index_file, patch_size, scale_factor, augment, normalizer):\n\t    ds = dataset.GeneratorDataset(\n\t        ImageTripletGenerator(index_file, patch_size, scale_factor, augment),\n\t        column_names=[f'{s}{i}' for s in ('h', 'l') for i in range(3)],\n\t    )\n\t    mean, std = normalizer.rgb_dist()\n\t    for col in [f'{s}{i}' for s in ('h', 'l') for i in range(3)]:\n\t        ds = ds.map([\n\t            transforms.TypeCast(ms.float32),\n", "            vision.Rescale(1.0 / 255.0, 0),\n\t            vision.Normalize(mean, std, is_hwc=True),\n\t            vision.HWC2CHW()\n\t        ], col)\n\t    return ds\n"]}
{"filename": "mindspore/dataset/video.py", "chunked_list": ["import bisect\n\timport collections\n\timport functools\n\timport itertools\n\timport pathlib\n\timport random\n\timport time\n\tfrom typing import List, Tuple\n\timport av\n\timport av.logging\n", "import cv2\n\timport numpy as np\n\timport mindspore as ms\n\tfrom mindspore import dataset\n\tfrom mindspore.dataset import vision, transforms\n\tav.logging.set_level(av.logging.FATAL)\n\tperf_debug = False\n\tclass Video:\n\t    def __init__(self, file, kf):\n\t        self.container = av.open(file)\n", "        self.stream = self.container.streams.video[0]\n\t        self.stream.thread_type = \"AUTO\"\n\t        self.at = 0\n\t        self.kf = kf\n\t    def get_frames(self, pts, n=1):\n\t        frames = []\n\t        if bisect.bisect_left(self.kf, pts) != bisect.bisect_left(self.kf, self.at) or pts <= self.at:\n\t            self.container.seek(pts, stream=self.stream)\n\t        found = False\n\t        first = True\n", "        if perf_debug:\n\t            print(f'Seek {pts} done at {time.perf_counter()}')\n\t        for frame in self.container.decode(video=0):\n\t            if first:\n\t                if perf_debug:\n\t                    print(f'Search {pts} from {frame.pts} at {time.perf_counter()}')\n\t                first = False\n\t            if not found and frame.pts != pts:\n\t                continue\n\t            found = True\n", "            if perf_debug:\n\t                print(f'Found {frame.pts} at {time.perf_counter()}')\n\t            self.at = frame.pts\n\t            yuv = frame.to_ndarray()\n\t            h, w = frame.height, frame.width\n\t            y, uv = yuv[:h, :].reshape(1, h, w), yuv[h:, :].reshape(2, h // 2, w // 2)\n\t            frames.append((y, uv))\n\t            if len(frames) == n:\n\t                return frames\n\t        raise ValueError(\"unexpected end\")\n", "    def __del__(self):\n\t        self.container.close()\n\tvideo_info = collections.namedtuple('video_info', [\n\t    'org',\n\t    'deg',\n\t    'frames',\n\t    'pts_org',\n\t    'pts_deg',\n\t    'key_org',\n\t    'key_deg'\n", "])\n\tdef flatten_once(it):\n\t    return itertools.chain.from_iterable(it)\n\tclass VideoFrameGenerator:\n\t    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n\t        self.dataset_base = pathlib.PurePath(index_file).parent\n\t        index_lines = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n\t                       if i if not i.startswith('#')]\n\t        files = [tuple(i.split(',')) for i in index_lines]\n\t        self.files = []\n", "        self.indexes = []\n\t        for org, deg, frames, pts_org, pts_deg, key_org, key_deg in files:\n\t            info = video_info(\n\t                org,\n\t                deg,\n\t                int(frames),\n\t                tuple(int(i) for i in pts_org.split(' ')),\n\t                tuple(int(i) for i in pts_deg.split(' ')),\n\t                tuple(int(i) for i in key_org.split(' ')),\n\t                tuple(int(i) for i in key_deg.split(' ')),\n", "            )\n\t            self.files.append(info)\n\t            self.indexes.append(info.frames)\n\t        self.indexes = list(itertools.accumulate(self.indexes))\n\t        self.patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size\n\t        self.scale_factor = scale_factor\n\t        self.augment = augment\n\t        self.rand = random.Random(seed)\n\t    @functools.lru_cache(2)\n\t    def get_video(self, v_idx):\n", "        info = self.files[v_idx]\n\t        return Video(str(self.dataset_base / info.org), info.key_org), \\\n\t            Video(str(self.dataset_base / info.deg), info.key_deg), info.pts_org, info.pts_deg\n\t    def _augment_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n\t        if self.rand.random() > 0.5:\n\t            org = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in org]\n\t            deg = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in deg]\n\t        return org, deg\n\t    def _prepare_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n\t        _, h, w = deg[0][0].shape\n", "        sw, sh = self.patch_size\n\t        sh_uv, sw_uv = sh // 2, sw // 2\n\t        dh, dw = self.rand.randrange(0, h - sh, 2), self.rand.randrange(0, w - sw, 2)\n\t        dh_uv, dw_uv = dh // 2, dw // 2\n\t        deg = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in deg]\n\t        f = self.scale_factor\n\t        size, size_uv = (sw, sh), (sw_uv, sh_uv)\n\t        sh, sw, sh_uv, sw_uv = sh * f, sw * f, sh_uv * f, sw_uv * f\n\t        dh, dw, dh_uv, dw_uv = dh * f, dw * f, dh_uv * f, dw_uv * f\n\t        org = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in org]\n", "        deg1_y = cv2.resize(org[1][0][0], size, interpolation=cv2.INTER_LANCZOS4)\n\t        deg1_u = cv2.resize(org[1][1][0], size_uv, interpolation=cv2.INTER_LANCZOS4)\n\t        deg1_v = cv2.resize(org[1][1][1], size_uv, interpolation=cv2.INTER_LANCZOS4)\n\t        deg.insert(1, (deg1_y.reshape((1, *size[::-1])), np.stack((deg1_u, deg1_v)).reshape((2, *size_uv[::-1]))))\n\t        return org, deg\n\t    def __len__(self):\n\t        return self.indexes[-1]\n\t    def __getitem__(self, idx):\n\t        start = time.perf_counter()\n\t        v_idx = bisect.bisect_right(self.indexes, idx)\n", "        f_idx = idx if v_idx == 0 else idx - self.indexes[v_idx - 1]\n\t        org, deg, pts_org, pts_deg = self.get_video(v_idx)\n\t        org_frames = org.get_frames(pts_org[f_idx], 3)\n\t        deg_frames = deg.get_frames(pts_deg[f_idx], 3)\n\t        deg_frames.pop(1)\n\t        org_frames, deg_frames = self._prepare_frame(org_frames, deg_frames)\n\t        if self.augment:\n\t            org_frames, deg_frames = self._augment_frame(org_frames, deg_frames)\n\t        ret = (*flatten_once(org_frames), *flatten_once(deg_frames))\n\t        if perf_debug:\n", "            print(f'Prepared data {idx}, in {time.perf_counter() - start}')\n\t        return ret\n\tdef VideoFrameDataset(index_file, patch_size, scale_factor, augment, normalizer):\n\t    ds = dataset.GeneratorDataset(\n\t        VideoFrameGenerator(index_file, patch_size, scale_factor, augment),\n\t        column_names=[f'{s}{i}_{p}' for s in ('h', 'l') for i in range(3) for p in ('y', 'uv')],\n\t        shuffle=False,\n\t        python_multiprocessing=True,\n\t        num_parallel_workers=3,\n\t    )\n", "    mean, std = normalizer.yuv_dist()\n\t    for col in [f'{s}{i}' for s in ('h', 'l') for i in range(3)]:\n\t        ds = ds.map([\n\t            transforms.TypeCast(ms.float32),\n\t            vision.Rescale(1.0 / 255.0, 0),\n\t            vision.Normalize(mean[:1], std[:1], is_hwc=False)\n\t        ], col + '_y')\n\t        ds = ds.map([\n\t            transforms.TypeCast(ms.float32),\n\t            vision.Rescale(1.0 / 255.0, 0),\n", "            vision.Normalize(mean[1:], std[1:], is_hwc=False)\n\t        ], col + '_uv')\n\t    return ds\n"]}
{"filename": "mindspore/util/normalize.py", "chunked_list": ["import math\n\tclass Normalizer:\n\t    sqrt1_2 = 1 / math.sqrt(2)\n\t    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), kr=0.2126, kb=0.0722, depth=8):\n\t        self.mean = mean\n\t        self.std = std\n\t        self.krgb = (kr, 1 - kr - kb, kb)\n\t        self.depth = depth\n\t        self.uv_bias = (1 << (depth - 1)) / ((1 << depth) - 1)\n\t    @staticmethod\n", "    def _inv(mean, std):\n\t        inv_std = tuple(1 / i for i in std)\n\t        inv_mean = tuple(-j * i for i, j in zip(inv_std, mean))\n\t        return inv_mean, inv_std\n\t    def rgb_dist(self):\n\t        return self.mean, self.std\n\t    def _yuv_dist(self):\n\t        rm, gm, bm = self.mean\n\t        rs, gs, bs = self.std\n\t        kr, kg, kb = self.krgb\n", "        ym = rm * kr + gm * kg + bm * kb\n\t        ys = math.sqrt((rs * kr) ** 2 + (gs * kg) ** 2 + (bs * kb) ** 2)\n\t        um = (bm - ym) / (1 - kb) / 2 + self.uv_bias\n\t        us = math.sqrt(bs * bs + ys * ys) / (1 - kb) / 2\n\t        vm = (rm - ym) / (1 - kr) / 2 + self.uv_bias\n\t        vs = math.sqrt(rs * rs + ys * ys) / (1 - kr) / 2\n\t        return [ym, um, vm], [ys, us, vs]\n\t    def yuv_dist(self, mode='yuv420'):\n\t        mean, std = self._yuv_dist()\n\t        if mode == 'yuv422':\n", "            std[1], std[2] = std[1] * self.sqrt1_2, std[2] * self.sqrt1_2\n\t        elif mode == 'yuv420':\n\t            std[1], std[2] = std[1] * 0.5, std[2] * 0.5\n\t        return mean, std\n"]}
{"filename": "mindspore/util/rmse.py", "chunked_list": ["from mindspore import nn\n\tfrom mindspore.ops import functional as F\n\tclass RMSELoss(nn.LossBase):\n\t    def __init__(self, epsilon=0.001):\n\t        super(RMSELoss, self).__init__()\n\t        self.epsilon = epsilon * epsilon\n\t        self.MSELoss = nn.MSELoss()\n\t    def construct(self, logits, label):\n\t        rmse_loss = F.sqrt(self.MSELoss(logits, label) + self.epsilon)\n\t        return rmse_loss\n"]}
{"filename": "mindspore/util/converter.py", "chunked_list": []}
{"filename": "torch/cycmunet_test.py", "chunked_list": ["import math\n\timport logging\n\timport os\n\timport time\n\timport tqdm\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import DataLoader\n\timport torch.backends.cuda\n\timport torch.backends.cudnn\n", "import torchvision.utils\n\tfrom pytorch_msssim import SSIM\n\tfrom model import CycMuNet\n\tfrom model.util import converter, normalizer\n\timport dataset\n\tfrom cycmunet.model import model_arg\n\tfrom cycmunet.run import test_arg\n\tmodel_args = model_arg(nf=64,\n\t                       groups=8,\n\t                       upscale_factor=2,\n", "                       format='yuv420',\n\t                       layers=4,\n\t                       cycle_count=3\n\t                       )\n\ttest_args = test_arg(\n\t    size=(256, 256),\n\t    checkpoint='checkpoints/monitor-ugly-sparsity_2x_l4_c3_epoch_2.pth',\n\t    dataset_indexes=[\n\t        \"/root/videos/cctv-scaled/index-test-good.txt\",\n\t        \"/root/videos/cctv-scaled/index-test-ugly.txt\",\n", "        \"/root/videos/cctv-scaled/index-test-smooth.txt\",\n\t        \"/root/videos/cctv-scaled/index-test-sharp.txt\",\n\t    ],\n\t    preview_interval=100,\n\t    seed=0,\n\t    batch_size=4,\n\t    fp16=True,\n\t)\n\ttorch.backends.cudnn.benchmark = True\n\ttorch.backends.cudnn.allow_tf32 = True\n", "torch.backends.cuda.matmul.allow_tf32 = True\n\ttorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\ttorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\tforce_data_dtype = torch.float16 if test_args.fp16 else None\n\t# --------------------------------------\n\t# Start of code\n\tpreview_interval = 100 \\\n\t    if (len(test_args.dataset_indexes) == 1 or math.gcd(100, len(test_args.dataset_indexes)) == 1) \\\n\t    else 101\n\tnrow = 1 if test_args.size[0] * 9 > test_args.size[1] * 16 else 3\n", "torch.manual_seed(test_args.seed)\n\ttorch.cuda.manual_seed(test_args.seed)\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s [%(name)s]: %(message)s')\n\tch = logging.StreamHandler()\n\tch.setFormatter(formatter)\n\tch.setLevel(logging.DEBUG)\n\tlogger = logging.getLogger('test_progress')\n\tlogger.addHandler(ch)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger_init = logging.getLogger('initialization')\n", "logger_init.addHandler(ch)\n\tlogger_init.setLevel(logging.DEBUG)\n\tcvt = converter()\n\tnorm = normalizer()\n\tdataset_types = {\n\t    'triplet': dataset.ImageSequenceDataset,\n\t    'video': dataset.VideoFrameDataset\n\t}\n\tDataset = dataset_types[test_args.dataset_type]\n\tif len(test_args.dataset_indexes) == 1:\n", "    ds_test = Dataset(test_args.dataset_indexes[0],\n\t                      test_args.size,\n\t                      model_args.upscale_factor,\n\t                      augment=True,\n\t                      seed=test_args.seed)\n\telse:\n\t    ds_test = dataset.InterleavedDataset(*[\n\t        Dataset(dataset_index,\n\t                test_args.size,\n\t                model_args.upscale_factor,\n", "                augment=True,\n\t                seed=test_args.seed + i)\n\t        for i, dataset_index in enumerate(test_args.dataset_indexes)])\n\tds_test = DataLoader(ds_test,\n\t                     num_workers=1,\n\t                     batch_size=test_args.batch_size,\n\t                     shuffle=Dataset.want_shuffle,  # Video dataset friendly\n\t                     drop_last=True)\n\tmodel = CycMuNet(model_args)\n\tmodel.eval()\n", "num_params = 0\n\tfor param in model.parameters():\n\t    num_params += param.numel()\n\tlogger_init.info(f\"Model has {num_params} parameters.\")\n\tif not os.path.exists(test_args.checkpoint):\n\t    logger_init.error(f\"Checkpoint weight {test_args.checkpoint} not exist.\")\n\t    exit(1)\n\tstate_dict = torch.load(test_args.checkpoint, map_location=lambda storage, loc: storage)\n\tload_result = model.load_state_dict(state_dict, strict=False)\n\tif load_result.unexpected_keys:\n", "    logger_init.warning(f\"Unknown parameters ignored: {load_result.unexpected_keys}\")\n\tif load_result.missing_keys:\n\t    logger_init.warning(f\"Missing parameters not initialized: {load_result.missing_keys}\")\n\tlogger_init.info(\"Checkpoint loaded.\")\n\tmodel = model.cuda()\n\tif force_data_dtype:\n\t    model = model.to(force_data_dtype)\n\tepsilon = (1 / 255) ** 2\n\tdef rmse(a, b):\n\t    return torch.mean(torch.sqrt((a - b) ** 2 + epsilon))\n", "ssim_module = SSIM(data_range=1.0, nonnegative_ssim=True).cuda()\n\tdef ssim(a, b):\n\t    return 1 - ssim_module(a, b)\n\tdef recursive_cuda(li, force_data_dtype):\n\t    if isinstance(li, (list, tuple)):\n\t        return tuple(recursive_cuda(i, force_data_dtype) for i in li)\n\t    else:\n\t        if force_data_dtype is not None:\n\t            return li.cuda().to(force_data_dtype)\n\t        else:\n", "            return li.cuda()\n\tif __name__ == '__main__':\n\t    with torch.no_grad():\n\t        total_loss = [0.0] * 4\n\t        total_iter = len(ds_test)\n\t        with tqdm.tqdm(total=total_iter, desc=f\"Test\") as progress:\n\t            for it, data in enumerate(ds_test):\n\t                (hf0, hf1, hf2), (lf0, lf1, lf2) = recursive_cuda(data, force_data_dtype)\n\t                if Dataset.pix_type == 'yuv':\n\t                    target = [cvt.yuv2rgb(*inp) for inp in (hf0, hf1, hf2, lf1)]\n", "                else:\n\t                    target = [hf0, hf1, hf2, lf1]\n\t                if it % preview_interval == 0:\n\t                    if Dataset.pix_type == 'yuv':\n\t                        org = [F.interpolate(cvt.yuv2rgb(y[0:1], uv[0:1]),\n\t                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n\t                                             mode='nearest').detach().float().cpu()\n\t                               for y, uv in (lf0, lf1, lf2)]\n\t                    else:\n\t                        org = [F.interpolate(lf[0:1],\n", "                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n\t                                             mode='nearest').detach().float().cpu()\n\t                               for lf in (lf0, lf1, lf2)]\n\t                if Dataset.pix_type == 'rgb':\n\t                    lf0, lf2 = cvt.rgb2yuv(lf0), cvt.rgb2yuv(lf2)\n\t                t0 = time.perf_counter()\n\t                lf0, lf2 = norm.normalize_yuv_420(*lf0), norm.normalize_yuv_420(*lf2)\n\t                outs = model(lf0, lf2, batch_mode='batch')\n\t                t1 = time.perf_counter()\n\t                t_forward = t1 - t0\n", "                actual = [cvt.yuv2rgb(*norm.denormalize_yuv_420(*out)).float() for out in outs]\n\t                if it % preview_interval == 0:\n\t                    out = [i[0:1].detach().float().cpu() for i in actual[:3]]\n\t                    ref = [i[0:1].detach().float().cpu() for i in target[:3]]\n\t                    for idx, ts in enumerate(zip(org, out, ref)):\n\t                        torchvision.utils.save_image(torch.concat(ts), f\"./result/out{idx}.png\",\n\t                                                     value_range=(0, 1), nrow=nrow, padding=0)\n\t                rmse_loss = [rmse(a, t).item() for a, t in zip(actual, target)]\n\t                ssim_loss = [ssim(a, t).item() for a, t in zip(actual, target)]\n\t                t2 = time.perf_counter()\n", "                t_loss = t2 - t1\n\t                rmse_h = sum(rmse_loss[:3]) / 3\n\t                rmse_l = rmse_loss[3]\n\t                ssim_h = sum(ssim_loss[:3]) / 3\n\t                ssim_l = ssim_loss[3]\n\t                total_loss[0] += rmse_h\n\t                total_loss[1] += rmse_l\n\t                total_loss[2] += ssim_h\n\t                total_loss[3] += ssim_l\n\t                progress.set_postfix(ordered_dict={\n", "                    \"rmse_h\": f\"{rmse_h:.4f}\",\n\t                    \"rmse_l\": f\"{rmse_l:.4f}\",\n\t                    \"ssim_h\": f\"{ssim_h:.4f}\",\n\t                    \"ssim_l\": f\"{ssim_l:.4f}\",\n\t                    \"f\": f\"{t_forward:.4f}s\",\n\t                    \"l\": f\"{t_loss:.4f}s\",\n\t                })\n\t                progress.update()\n\t        logger.info(f\"Test Complete: \"\n\t                    f\"RMSE HQ: {total_loss[0] / total_iter:.4f} \"\n", "                    f\"RMSE LQ: {total_loss[1] / total_iter:.4f} \"\n\t                    f\"SSIM HQ: {total_loss[2] / total_iter:.4f} \"\n\t                    f\"SSIM LQ: {total_loss[3] / total_iter:.4f}\")\n"]}
{"filename": "torch/cycmunet_train.py", "chunked_list": ["import math\n\timport logging\n\timport os\n\timport pathlib\n\timport time\n\timport tqdm\n\timport torch\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import DataLoader\n\timport torch.optim as optim\n", "import torch.backends.cuda\n\timport torch.backends.cudnn\n\timport torchvision.utils\n\tfrom pytorch_msssim import SSIM\n\tfrom model import CycMuNet\n\tfrom model.util import converter, normalizer\n\timport dataset\n\tfrom cycmunet.model import model_arg\n\tfrom cycmunet.run import train_arg\n\t# ------------------------------------------\n", "# Configs\n\tmodel_args = model_arg(nf=64,\n\t                       groups=8,\n\t                       upscale_factor=2,\n\t                       format='yuv420',\n\t                       layers=4,\n\t                       cycle_count=3\n\t                       )\n\ttrain_args = train_arg(\n\t    size=(128, 128),\n", "    pretrained=\"/root/cycmunet-new/checkpoints/monitor-ugly_2x_l4_c3_epoch_19.pth\",\n\t    # dataset_type=\"video\",\n\t    # dataset_indexes=[\n\t    #     \"/root/videos/cctv-scaled/index-train-good.txt\",\n\t    #     \"/root/videos/cctv-scaled/index-train-ugly.txt\",\n\t    #     \"/root/videos/cctv-scaled/index-train-smooth.txt\",\n\t    #     \"/root/videos/cctv-scaled/index-train-sharp.txt\",\n\t    # ],\n\t    dataset_type=\"triplet\",\n\t    dataset_indexes=[\n", "        \"/root/dataset/vimeo_triplet/tri_trainlist.txt\"\n\t    ],\n\t    preview_interval=100,\n\t    seed=0,\n\t    lr=0.001,\n\t    start_epoch=1,\n\t    end_epoch=11,\n\t    sparsity=True,\n\t    batch_size=2,\n\t    autocast=False,\n", "    loss_type='rmse',\n\t    save_path='checkpoints',\n\t    save_prefix='triplet',\n\t)\n\ttorch.backends.cudnn.benchmark = True\n\ttorch.backends.cudnn.allow_tf32 = True\n\ttorch.backends.cuda.matmul.allow_tf32 = True\n\ttorch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = False\n\ttorch.backends.cuda.matmul.allow_bf16_reduced_precision_reduction = False\n\t# --------------------------------------\n", "# Start of code\n\tpreview_interval = 100 \\\n\t    if (len(train_args.dataset_indexes) == 1 or math.gcd(100, len(train_args.dataset_indexes)) == 1) \\\n\t    else 101\n\tsave_prefix = f'{train_args.save_prefix}_{model_args.upscale_factor}x_l{model_args.layers}_c{model_args.cycle_count}'\n\tsave_path = pathlib.Path(train_args.save_path)\n\tnrow = 1 if train_args.size[0] * 9 > train_args.size[1] * 16 else 3\n\ttorch.manual_seed(train_args.seed)\n\ttorch.cuda.manual_seed(train_args.seed)\n\tformatter = logging.Formatter('%(asctime)s %(levelname)s [%(name)s]: %(message)s')\n", "ch = logging.StreamHandler()\n\tch.setFormatter(formatter)\n\tch.setLevel(logging.DEBUG)\n\tlogger = logging.getLogger('train_progress')\n\tlogger.addHandler(ch)\n\tlogger.setLevel(logging.DEBUG)\n\tlogger_init = logging.getLogger('initialization')\n\tlogger_init.addHandler(ch)\n\tlogger_init.setLevel(logging.DEBUG)\n\tcvt = converter()\n", "norm = normalizer()\n\tdataset_types = {\n\t    'triplet': dataset.ImageSequenceDataset,\n\t    'video': dataset.VideoFrameDataset\n\t}\n\tDataset = dataset_types[train_args.dataset_type]\n\tif len(train_args.dataset_indexes) == 1:\n\t    ds_train = Dataset(train_args.dataset_indexes[0],\n\t                       train_args.size,\n\t                       model_args.upscale_factor,\n", "                       augment=True,\n\t                       seed=train_args.seed)\n\telse:\n\t    ds_train = dataset.InterleavedDataset(*[\n\t        Dataset(dataset_index,\n\t                train_args.size,\n\t                model_args.upscale_factor,\n\t                augment=True,\n\t                seed=train_args.seed + i)\n\t        for i, dataset_index in enumerate(train_args.dataset_indexes)])\n", "ds_train = DataLoader(ds_train,\n\t                      num_workers=1,\n\t                      batch_size=train_args.batch_size,\n\t                      shuffle=Dataset.want_shuffle,  # Video dataset friendly\n\t                      drop_last=True)\n\tmodel = CycMuNet(model_args)\n\tmodel.train()\n\tmodel_updated = False\n\tnum_params = 0\n\tfor param in model.parameters():\n", "    num_params += param.numel()\n\tlogger_init.info(f\"Model has {num_params} parameters.\")\n\tif train_args.pretrained:\n\t    if not os.path.exists(train_args.pretrained):\n\t        logger_init.warning(f\"Pretrained weight {train_args.pretrained} not exist.\")\n\t    state_dict = torch.load(train_args.pretrained, map_location=lambda storage, loc: storage)\n\t    load_result = model.load_state_dict(state_dict, strict=False)\n\t    if load_result.unexpected_keys:\n\t        logger_init.warning(f\"Unknown parameters ignored: {load_result.unexpected_keys}\")\n\t    if load_result.missing_keys:\n", "        logger_init.warning(f\"Missing parameters not initialized: {load_result.missing_keys}\")\n\t    logger_init.info(\"Pretrained weights loaded.\")\n\tmodel = model.cuda()\n\toptimizer = optim.Adamax(model.parameters(), lr=train_args.lr, betas=(0.9, 0.999), eps=1e-8)\n\t# Or, train only some parts\n\t# optimizer = optim.Adamax(itertools.chain(\n\t#         model.head.parameters(),\n\t#         model.fe.parameters(),\n\t#         model.fr.parameters(),\n\t#         model.tail.parameters()\n", "# ), lr=args.lr, betas=(0.9, 0.999), eps=1e-8)\n\tscheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=40000, eta_min=1e-7)\n\tnum_params_train = 0\n\tfor group in optimizer.param_groups:\n\t    for params in group.get('params', []):\n\t        num_params_train += params.numel()\n\tlogger_init.info(f\"Model has {num_params} parameters to train.\")\n\tif train_args.sparsity:\n\t    from apex.contrib.sparsity import ASP\n\t    target_layers = []\n", "    target_layers.extend('mu.' + name for name, _ in model.mu.named_modules())\n\t    target_layers.extend('fr.' + name for name, _ in model.fr.named_modules())\n\t    ASP.init_model_for_pruning(model,\n\t                               mask_calculator=\"m4n2_1d\",\n\t                               allowed_layer_names=target_layers,\n\t                               verbosity=2,\n\t                               whitelist=[torch.nn.Linear, torch.nn.Conv2d],\n\t                               allow_recompute_mask=False,\n\t                               allow_permutation=False,\n\t                               )\n", "    ASP.init_optimizer_for_pruning(optimizer)\n\t    # import torch.fx\n\t    # original_symbolic_trace = torch.fx.symbolic_trace\n\t    # torch.fx.symbolic_trace = functools.partial(original_symbolic_trace, concrete_args={\n\t    #     'batch_mode': '_no_use_sparsity_pseudo',\n\t    #     'stop_at_conf': False,\n\t    #     'all_frames': True\n\t    # })\n\t    ASP.compute_sparse_masks()\n\t    # torch.fx.symbolic_trace = original_symbolic_trace\n", "    logger.info('Training with sparsity.')\n\tepsilon = (1 / 255) ** 2\n\tdef rmse(a, b):\n\t    return torch.mean(torch.sqrt((a - b) ** 2 + epsilon))\n\tssim_module = SSIM(data_range=1.0, nonnegative_ssim=True).cuda()\n\tdef ssim(a, b):\n\t    return 1 - ssim_module(a, b)\n\tdef recursive_cuda(li, force_data_dtype):\n\t    if isinstance(li, (list, tuple)):\n\t        return tuple(recursive_cuda(i, force_data_dtype) for i in li)\n", "    else:\n\t        if force_data_dtype is not None:\n\t            return li.cuda().to(force_data_dtype)\n\t        else:\n\t            return li.cuda()\n\tdef train(epoch):\n\t    epoch_loss = 0\n\t    total_iter = len(ds_train)\n\t    loss_coeff = [1, 0.5, 1, 0.5]\n\t    with tqdm.tqdm(total=total_iter, desc=f\"Epoch {epoch}\") as progress:\n", "        for it, data in enumerate(ds_train):\n\t            optimizer.zero_grad()\n\t            def compute_loss(force_data_dtype=None):\n\t                (hf0, hf1, hf2), (lf0, lf1, lf2) = recursive_cuda(data, force_data_dtype)\n\t                if Dataset.pix_type == 'yuv':\n\t                    target = [cvt.yuv2rgb(*inp) for inp in (hf0, hf1, hf2, lf1)]\n\t                else:\n\t                    target = [hf0, hf1, hf2, lf1]\n\t                if it % preview_interval == 0:\n\t                    if Dataset.pix_type == 'yuv':\n", "                        org = [F.interpolate(cvt.yuv2rgb(y[0:1], uv[0:1]),\n\t                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n\t                                             mode='nearest').detach().float().cpu()\n\t                               for y, uv in (lf0, lf1, lf2)]\n\t                    else:\n\t                        org = [F.interpolate(lf[0:1],\n\t                                             scale_factor=(model_args.upscale_factor, model_args.upscale_factor),\n\t                                             mode='nearest').detach().float().cpu()\n\t                               for lf in (lf0, lf1, lf2)]\n\t                if Dataset.pix_type == 'rgb':\n", "                    lf0, lf2 = cvt.rgb2yuv(lf0), cvt.rgb2yuv(lf2)\n\t                t0 = time.perf_counter()\n\t                lf0, lf2 = norm.normalize_yuv_420(*lf0), norm.normalize_yuv_420(*lf2)\n\t                outs = model(lf0, lf2, batch_mode='batch')\n\t                t1 = time.perf_counter()\n\t                actual = [cvt.yuv2rgb(*norm.denormalize_yuv_420(*out)) for out in outs]\n\t                if train_args.loss_type == 'rmse':\n\t                    loss = [rmse(a, t) * c for a, t, c in zip(actual, target, loss_coeff)]\n\t                elif train_args.loss_type == 'ssim':\n\t                    loss = [ssim(a, t) * c for a, t, c in zip(actual, target, loss_coeff)]\n", "                else:\n\t                    raise ValueError(\"Unknown loss type: \" + train_args.loss_type)\n\t                assert not any(torch.any(torch.isnan(i)).item() for i in loss)\n\t                t2 = time.perf_counter()\n\t                if it % preview_interval == 0:\n\t                    out = [i[0:1].detach().float().cpu() for i in actual[:3]]\n\t                    ref = [i[0:1].detach().float().cpu() for i in target[:3]]\n\t                    for idx, ts in enumerate(zip(org, out, ref)):\n\t                        torchvision.utils.save_image(torch.concat(ts), f\"./result/out{idx}.png\",\n\t                                                     value_range=(0, 1), nrow=nrow, padding=0)\n", "                return loss, t1 - t0, t2 - t1\n\t            if train_args.autocast:\n\t                with torch.autocast(device_type='cuda', dtype=torch.float16):\n\t                    loss, t_forward, t_loss = compute_loss(torch.float16)\n\t            else:\n\t                loss, t_forward, t_loss = compute_loss()\n\t            total_loss = sum(loss)\n\t            epoch_loss += total_loss.item()\n\t            t3 = time.perf_counter()\n\t            total_loss.backward()\n", "            optimizer.step()\n\t            scheduler.step()\n\t            t_backward = time.perf_counter() - t3\n\t            global model_updated\n\t            model_updated = True\n\t            progress.set_postfix(ordered_dict={\n\t                \"loss\": f\"{total_loss.item():.4f}\",\n\t                \"lr\": f\"{optimizer.param_groups[0]['lr']:.6e}\",\n\t                \"f\": f\"{t_forward:.4f}s\",\n\t                \"l\": f\"{t_loss:.4f}s\",\n", "                \"b\": f\"{t_backward:.4f}s\",\n\t            })\n\t            progress.update()\n\t    logger.info(f\"Epoch {epoch} Complete: Avg. Loss: {epoch_loss / total_iter:.4f}\")\n\tdef save_model(epoch):\n\t    if epoch == -1:\n\t        name = \"snapshot\"\n\t    else:\n\t        name = f\"epoch_{epoch}\"\n\t    if not os.path.exists(save_path):\n", "        os.makedirs(save_path)\n\t    output_path = save_path / f\"{save_prefix}_{name}.pth\"\n\t    torch.save(model.state_dict(), output_path)\n\t    logger.info(f\"Checkpoint saved to {output_path}\")\n\tif __name__ == '__main__':\n\t    try:\n\t        for epoch in range(train_args.start_epoch, train_args.end_epoch):\n\t            # with torch.autograd.detect_anomaly():\n\t            #     train(epoch)\n\t            train(epoch)\n", "            save_model(epoch)\n\t    except KeyboardInterrupt:\n\t        if model_updated:\n\t            save_model(-1)\n"]}
{"filename": "torch/cycmunet_export_onnx.py", "chunked_list": ["import os\n\timport pathlib\n\timport torch\n\tfrom torch.onnx import symbolic_helper\n\timport onnx\n\timport onnx.shape_inference\n\timport onnxsim\n\timport onnx_graphsurgeon as gs\n\tfrom model import CycMuNet, use_fold_catconv\n\tfrom cycmunet.model import model_arg\n", "model_args = model_arg(nf=64,\n\t                       groups=8,\n\t                       upscale_factor=2,\n\t                       format='yuv420',\n\t                       layers=4,\n\t                       cycle_count=3\n\t                       )\n\tcheckpoint_file = 'checkpoints/triplet_s2_2x_l4_c3_snapshot.pth'\n\toutput_path = 'onnx/triplet_new'\n\tsize = 1 << model_args.layers\n", "size_in = (size, size)\n\tsize_out = tuple(i * model_args.upscale_factor for i in size_in)\n\toutput_path = pathlib.Path(output_path)\n\tconfig_string = f\"_{model_args.upscale_factor}x_l{model_args.layers}\"\n\tif model_args.format == 'yuv420':\n\t    size_uv_in = tuple(i // 2 for i in size_in)\n\t    config_string += '_yuv1-1'\n\tfe_onnx = str(output_path / f'fe{config_string}.onnx')\n\tff_onnx = str(output_path / f'ff{config_string}.onnx')\n\tos.makedirs(output_path, exist_ok=True)\n", "# Placeholder to export DeformConv\n\t@symbolic_helper.parse_args(\"v\", \"v\", \"v\", \"v\", \"v\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"i\", \"b\")\n\tdef symbolic_deform_conv2d_forward(g,\n\t                                   input,\n\t                                   weight,\n\t                                   offset,\n\t                                   mask,\n\t                                   bias,\n\t                                   stride_h,\n\t                                   stride_w,\n", "                                   pad_h,\n\t                                   pad_w,\n\t                                   dil_h,\n\t                                   dil_w,\n\t                                   n_weight_grps,\n\t                                   n_offset_grps,\n\t                                   use_mask):\n\t    if n_weight_grps != 1 or not use_mask:\n\t        raise NotImplementedError()\n\t    return g.op(\"custom::DeformConv2d\", input, offset, mask, weight, bias, stride_i=[stride_h, stride_w],\n", "                padding_i=[pad_h, pad_w], dilation_i=[dil_h, dil_w], deformable_groups_i=n_offset_grps)\n\t# Register custom symbolic function\n\ttorch.onnx.register_custom_op_symbolic(\"torchvision::deform_conv2d\", symbolic_deform_conv2d_forward, 13)\n\tdef clean_fp16_subnormal(t: torch.Tensor):\n\t    threshold = 0.00006103515625\n\t    mask = torch.logical_and(t > -threshold, t < threshold)\n\t    t[mask] = 0\n\t    return t\n\tdef as_module(func):\n\t    class mod(torch.nn.Module):\n", "        def __init__(self):\n\t            super().__init__()\n\t        def forward(self, *x):\n\t            return func(*x)\n\t    return mod().eval()\n\tdef simplify(name):\n\t    model, other = onnxsim.simplify(name)\n\t    graph = gs.import_onnx(model)\n\t    graph.fold_constants().cleanup()\n\t    for n in graph.nodes:\n", "        if n.op == 'DeformConv2d':\n\t            if n.outputs[0].outputs[0].op == 'LeakyRelu':\n\t                lrelu = n.outputs[0].outputs[0]\n\t                n.attrs['activation_type'] = 3\n\t                n.attrs['alpha'] = lrelu.attrs['alpha']\n\t                n.attrs['beta'] = 0.0\n\t                n.outputs = lrelu.outputs\n\t                lrelu.inputs = []\n\t                lrelu.outputs = []\n\t            else:\n", "                n.attrs['activation_type'] = -1\n\t                n.attrs['alpha'] = 0.0\n\t                n.attrs['beta'] = 0.0\n\t    graph.cleanup().toposort()\n\t    model = gs.export_onnx(graph)\n\t    onnx.save_model(model, name)\n\t    print(f'Simplify {name} done')\n\tuse_fold_catconv()\n\tmodel = CycMuNet(model_args)\n\tstate_dict = torch.load(checkpoint_file, map_location='cpu')\n", "state_dict = {k: clean_fp16_subnormal(v) for k, v in state_dict.items() if '__weight_mma_mask' not in k}\n\tmodel.load_state_dict(state_dict)\n\tmodel = model.eval()\n\tfor v in model.parameters(recurse=True):\n\t    v.requires_grad = False\n\tif __name__ == '__main__':\n\t    with torch.no_grad():\n\t        print(\"Exporting fe...\")\n\t        if model_args.format == 'rgb':\n\t            fe_i = torch.zeros((2, 3, *size))\n", "            dynamic_axes = {\n\t                \"x\": {\n\t                    0: \"batch_size\",\n\t                    2: \"input_height\",\n\t                    3: \"input_width\"\n\t                },\n\t            }\n\t        elif model_args.format == 'yuv420':\n\t            fe_i = tuple([torch.zeros((2, 1, *size_in)), torch.zeros((2, 2, *size_uv_in))])\n\t            dynamic_axes = {\n", "                \"y\": {\n\t                    0: \"batch_size\",\n\t                    2: \"input_height\",\n\t                    3: \"input_width\"\n\t                },\n\t                \"uv\": {\n\t                    0: \"batch_size\",\n\t                    2: \"input_height_uv\",\n\t                    3: \"input_width_uv\"\n\t                },\n", "            }\n\t        else:\n\t            raise NotImplementedError()\n\t        input_names = list(dynamic_axes.keys())\n\t        output_names = [f'l{i}' for i in range(model_args.layers)[::-1]]\n\t        dynamic_axes.update({f'l{i}': {\n\t            0: \"batch_size\",\n\t            2: f\"feature_height_{i}\",\n\t            3: f\"feature_width_{i}\"\n\t        } for i in range(model_args.layers)[::-1]})\n", "        @as_module\n\t        def fe(*x_or_y_uv: torch.Tensor):\n\t            if model_args.format == 'rgb':\n\t                return model.head_fe(x_or_y_uv[0])\n\t            else:\n\t                return model.head_fe(x_or_y_uv)\n\t        torch.onnx.export(fe, fe_i, fe_onnx, opset_version=13,\n\t                          export_params=True,\n\t                          input_names=input_names, output_names=output_names,\n\t                          dynamic_axes=dynamic_axes)\n", "        print(\"Exporting ff...\")\n\t        ff_i = []\n\t        input_axes = dict()\n\t        cur_size = size_in\n\t        for i in range(model_args.layers)[::-1]:\n\t            ff_i.insert(0, torch.zeros(1, model_args.nf, *cur_size))\n\t            cur_size = tuple((i + 1) // 2 for i in cur_size)\n\t        for i in range(model_args.layers):\n\t            axes = {\n\t                0: \"batch_size\",\n", "                2: f\"feature_height_{i}\",\n\t                3: f\"feature_width_{i}\"\n\t            }\n\t            input_axes[f'f0l{i}'] = axes\n\t            input_axes[f'f2l{i}'] = axes\n\t        input_names = [f'f0l{i}' for i in range(model_args.layers)[::-1]] + \\\n\t                      [f'f2l{i}' for i in range(model_args.layers)[::-1]]\n\t        output_names = ['f1']\n\t        dynamic_axes = dict(input_axes)\n\t        dynamic_axes[f'f1'] = {\n", "            0: \"batch_size\",\n\t            2: f\"feature_height_{model_args.layers - 1}\",\n\t            3: f\"feature_width_{model_args.layers - 1}\"\n\t        }\n\t        if model_args.format == 'rgb':\n\t            output_axes = {\n\t                \"h0\": {\n\t                    0: \"batch_size\",\n\t                    2: \"output_height\",\n\t                    3: \"output_width\"\n", "                },\n\t                \"h1\": {\n\t                    0: \"batch_size\",\n\t                    2: \"output_height\",\n\t                    3: \"output_width\"\n\t                },\n\t            }\n\t        elif model_args.format == 'yuv420':\n\t            output_axes = {\n\t                \"h0_y\": {\n", "                    0: \"batch_size\",\n\t                    2: \"output_height\",\n\t                    3: \"output_width\"\n\t                },\n\t                \"h0_uv\": {\n\t                    0: \"batch_size\",\n\t                    2: \"output_height_uv\",\n\t                    3: \"output_width_uv\"\n\t                },\n\t                \"h1_y\": {\n", "                    0: \"batch_size\",\n\t                    2: \"output_height\",\n\t                    3: \"output_width\"\n\t                },\n\t                \"h1_uv\": {\n\t                    0: \"batch_size\",\n\t                    2: \"output_height_uv\",\n\t                    3: \"output_width_uv\"\n\t                },\n\t            }\n", "        else:\n\t            raise NotImplementedError()\n\t        output_names = list(output_axes.keys())\n\t        dynamic_axes = dict(input_axes)\n\t        dynamic_axes.update(output_axes)\n\t        @as_module\n\t        def ff(input1, input2):\n\t            fea = [input1[-1], model.ff(input1, input2)[0], input2[-1]]\n\t            outs = model.mu_fr_tail(fea, all_frames=False)\n\t            return outs\n", "        torch.onnx.export(ff, (ff_i, ff_i),\n\t                          str(output_path / f'ff{config_string}.onnx'), opset_version=13,\n\t                          export_params=True,\n\t                          input_names=input_names, output_names=output_names,\n\t                          dynamic_axes=dynamic_axes)\n\t    simplify(fe_onnx)\n\t    simplify(ff_onnx)\n"]}
{"filename": "torch/cycmunet/model.py", "chunked_list": ["from collections import namedtuple\n\tmodel_arg = namedtuple('model_arg', ('nf',  # number of feature channel\n\t                                     'groups',  # number of deformable convolution group\n\t                                     'upscale_factor',  # model upscale factor\n\t                                     'format',  # model I/O format (rgb, yuv420)\n\t                                     'layers',  # feature fusion pyramid layers\n\t                                     'cycle_count'  # mutual cycle count\n\t                                     ))\n"]}
{"filename": "torch/cycmunet/run.py", "chunked_list": ["from collections import namedtuple\n\t_share_args = ('size',  # input size\n\t               'dataset_type',  # type of dataset\n\t               'dataset_indexes',  # index files for dataset\n\t               'preview_interval',  # interval to save network output for previewing\n\t               'batch_size',  # process batch size\n\t               'seed',  # seed for random number generators\n\t               )\n\t_train_args = ('lr',  # init learning rate\n\t               'pretrained',  # pretrained checkpoint\n", "               'start_epoch',  # start epoch index\n\t               'end_epoch',  # end epoch index (exclusive)\n\t               'sparsity',  # train network with sparsity\n\t               'autocast',  # train with auto mixed precision\n\t               'loss_type',  # loss type for optimization\n\t               'save_path',  # checkpoint save path\n\t               'save_prefix',  # prefix of checkpoint file name\n\t               )\n\t_test_args = ('checkpoints',  # checkpoint to test\n\t              'fp16',  # use fp16 to run network forward\n", "              )\n\ttrain_arg = namedtuple('train_arg', (*_share_args, *_train_args))\n\ttest_arg = namedtuple('test_arg', (*_share_args, *_test_args))\n"]}
{"filename": "torch/model/part.py", "chunked_list": ["from typing import Tuple\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.init as init\n\timport torch.nn.functional as F\n\tfrom torch.nn.common_types import _size_2_t\n\tfrom torch.nn.modules.utils import _pair\n\timport torchvision.ops\n\tfrom .util import cat_conv\n\tclass ResidualBlock_noBN(nn.Module):\n", "    '''Residual block w/o BN\n\t    ---Conv-ReLU-Conv-+-\n\t     |________________|\n\t    '''\n\t    def __init__(self, nf=64):\n\t        super(ResidualBlock_noBN, self).__init__()\n\t        self.conv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\t        self.conv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n\t        # initialization\n\t        self.init_weights(self.conv1)\n", "        self.init_weights(self.conv2)\n\t    @staticmethod\n\t    def init_weights(conv):\n\t        init.kaiming_normal_(conv.weight, a=0, mode='fan_in')\n\t        conv.weight.data *= 0.1  # for residual block\n\t        if conv.bias is not None:\n\t            conv.bias.data.zero_()\n\t    def forward(self, x):\n\t        identity = x\n\t        out = F.relu(self.conv1(x), inplace=True)\n", "        out = self.conv2(out)\n\t        return identity + out\n\tclass DCN_sep(nn.Module):\n\t    def __init__(self,\n\t                 in_channels: int,\n\t                 in_channels_features: int,\n\t                 out_channels: int,\n\t                 kernel_size: _size_2_t,\n\t                 stride: _size_2_t = 1,\n\t                 padding: _size_2_t = 0,\n", "                 dilation: _size_2_t = 1,\n\t                 groups: int = 1,\n\t                 deformable_groups: int = 1,\n\t                 bias: bool = True,\n\t                 mask: bool = True):\n\t        super(DCN_sep, self).__init__()\n\t        self.dcn = torchvision.ops.DeformConv2d(in_channels, out_channels, kernel_size, stride, padding, dilation,\n\t                                                groups, bias)\n\t        kernel_size_ = _pair(kernel_size)\n\t        offset_channels = deformable_groups * kernel_size_[0] * kernel_size_[1]\n", "        self.conv_offset = nn.Conv2d(in_channels_features, offset_channels * 2, kernel_size=kernel_size,\n\t                                     stride=stride, padding=padding, dilation=dilation, bias=True)\n\t        self.conv_mask = nn.Conv2d(in_channels_features, offset_channels, kernel_size=kernel_size,\n\t                                   stride=stride, padding=padding, dilation=dilation, bias=True) if mask else None\n\t        self.relu = nn.ReLU(inplace=True)\n\t    def forward(self, input: torch.Tensor, feature: torch.Tensor):\n\t        offset = self.conv_offset(feature)\n\t        mask = torch.sigmoid(self.conv_mask(feature)) if self.conv_mask else None\n\t        return self.dcn(input, offset, mask)\n\tclass PCDLayer(nn.Module):\n", "    \"\"\" Alignment module using Pyramid, Cascading and Deformable convolution\"\"\"\n\t    def __init__(self, args, first_layer: bool):\n\t        super(PCDLayer, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n\t        self.offset_conv1 = nn.Conv2d(2 * self.nf, self.nf, 3, 1, 1)\n\t        self.offset_conv3 = nn.Conv2d(self.nf, self.nf, 3, 1, 1)\n\t        self.dcnpack = DCN_sep(self.nf, self.nf, self.nf, 3, stride=1, padding=1, dilation=1,\n\t                               deformable_groups=self.groups)\n", "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\t        if not first_layer:\n\t            self.offset_conv2 = nn.Conv2d(2 * self.nf, self.nf, 3, 1, 1)\n\t            self.fea_conv = nn.Conv2d(2 * self.nf, self.nf, 3, 1, 1)\n\t    def forward(self, current_sources: Tuple[torch.Tensor, torch.Tensor],\n\t                last_offset: torch.Tensor, last_feature: torch.Tensor):\n\t        offset = self.lrelu(cat_conv(self.offset_conv1, current_sources))\n\t        if last_offset is not None:\n\t            last_offset = F.interpolate(last_offset, scale_factor=2, mode='bilinear', align_corners=False)\n\t            _, _, h, w = offset.shape\n", "            last_offset = last_offset[..., :h, :w]\n\t            offset = self.lrelu(cat_conv(self.offset_conv2, (offset, last_offset * 2)))\n\t        offset = self.lrelu(self.offset_conv3(offset))\n\t        feature = self.dcnpack(current_sources[0], offset)\n\t        if last_feature is not None:\n\t            last_feature = F.interpolate(last_feature, scale_factor=2, mode='bilinear', align_corners=False)\n\t            _, _, h, w = feature.shape\n\t            last_feature = last_feature[..., :h, :w]\n\t            feature = cat_conv(self.fea_conv, (feature, last_feature))\n\t        feature = self.lrelu(feature)\n", "        return offset, feature\n"]}
{"filename": "torch/model/__init__.py", "chunked_list": ["from .util import use_fold_catconv\n\tfrom .cycmunet import CycMuNet\n"]}
{"filename": "torch/model/util.py", "chunked_list": ["import functools\n\timport itertools\n\timport math\n\tfrom typing import Tuple, Union\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\timport torchvision\n\tRGBOrYUV = Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]\n\t_use_fold_catconv = False\n", "def use_fold_catconv(value=True):\n\t    global _use_fold_catconv\n\t    _use_fold_catconv = value\n\tdef cat_simp(ts, *args, **kwargs):\n\t    \"\"\"auto eliminate cat if there's only one input\"\"\"\n\t    if len(ts) == 1:\n\t        return ts[0]\n\t    return torch.cat(ts, *args, **kwargs)\n\tdef cat_conv(conv: nn.Conv2d, tensors, scale=None):\n\t    \"\"\"separate cat+conv into multiple conv to reduce memory footprint\"\"\"\n", "    if _use_fold_catconv:\n\t        w = conv.weight.detach()\n\t        b = conv.bias.detach()\n\t        if scale is not None:\n\t            w *= scale\n\t            b *= scale\n\t        output = None\n\t        channels = [0]\n\t        channels.extend(itertools.accumulate(int(tensor.shape[1]) for tensor in tensors))\n\t        for ti, cb, ce in zip(tensors, channels, channels[1:]):\n", "            c = ti.shape[1]\n\t            convi = nn.Conv2d(c, conv.out_channels, conv.kernel_size, conv.stride, conv.padding,\n\t                              conv.dilation, bias=output is None).eval()\n\t            convi.weight = nn.Parameter(w[:, cb:ce, :, :], requires_grad=False)\n\t            if output is None:\n\t                convi.bias = nn.Parameter(b, requires_grad=False)\n\t            outputi = convi(ti)\n\t            output = outputi if output is None else output + outputi\n\t        return output\n\t    else:\n", "        return conv(torch.cat(tensors, dim=1))\n\t# mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n\tclass normalizer:\n\t    nm = torchvision.transforms.Normalize\n\t    sqrt2 = math.sqrt(2)\n\t    def __init__(self, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), kr=0.2126, kb=0.0722, depth=8):\n\t        self.mean = mean\n\t        self.std = std\n\t        self.krgb = (kr, 1 - kr - kb, kb)\n\t        self.depth = depth\n", "        self.uv_bias = (1 << (depth - 1)) / ((1 << depth) - 1)\n\t    @staticmethod\n\t    def _inv(mean, std):\n\t        inv_std = tuple(1 / i for i in std)\n\t        inv_mean = tuple(-j * i for i, j in zip(inv_std, mean))\n\t        return inv_mean, inv_std\n\t    def _yuv_dist(self):\n\t        rm, gm, bm = self.mean\n\t        rs, gs, bs = self.std\n\t        kr, kg, kb = self.krgb\n", "        ym = rm * kr + gm * kg + bm * kb\n\t        ys = math.sqrt((rs * kr) ** 2 + (gs * kg) ** 2 + (bs * kb) ** 2)\n\t        um = (bm - ym) / (1 - kb) / 2 + self.uv_bias\n\t        us = math.sqrt(bs * bs + ys * ys) / (1 - kb) / 2\n\t        vm = (rm - ym) / (1 - kr) / 2 + self.uv_bias\n\t        vs = math.sqrt(rs * rs + ys * ys) / (1 - kr) / 2\n\t        return [ym, um, vm], [ys, us, vs]\n\t    def normalize_rgb(self, rgb: torch.Tensor):\n\t        return self.nm(self.mean, self.std)(rgb)\n\t    def denormalize_rgb(self, rgb: torch.Tensor):\n", "        return self.nm(*self._inv(self.mean, self.std))(rgb)\n\t    def normalize_yuv_444(self, yuv: torch.Tensor):\n\t        return self.nm(*self._yuv_dist())(yuv)\n\t    def denormalize_yuv_444(self, yuv: torch.Tensor):\n\t        return self.nm(*self._inv(*self._yuv_dist()))(yuv)\n\t    def _normalize_yuv_42x(self, y: torch.Tensor, uv: torch.Tensor, scale):\n\t        mean, std = self._yuv_dist()\n\t        std[1], std[2] = std[1] * scale, std[2] * scale\n\t        y = self.nm(mean[0], std[0])(y)\n\t        uv = self.nm(mean[1:], std[1:])(uv)\n", "        return y, uv\n\t    def _denormalize_yuv_42x(self, y: torch.Tensor, uv: torch.Tensor, scale):\n\t        mean, std = self._yuv_dist()\n\t        std[1], std[2] = std[1] * scale, std[2] * scale\n\t        mean, std = self._inv(mean, std)\n\t        y = self.nm(mean[0], std[0])(y)\n\t        uv = self.nm(mean[1:], std[1:])(uv)\n\t        return y, uv\n\t    def normalize_yuv_422(self, y: torch.Tensor, uv: torch.Tensor):\n\t        return self._normalize_yuv_42x(y, uv, 1 / self.sqrt2)\n", "    def denormalize_yuv_422(self, y: torch.Tensor, uv: torch.Tensor):\n\t        return self._denormalize_yuv_42x(y, uv, 1 / self.sqrt2)\n\t    def normalize_yuv_420(self, y: torch.Tensor, uv: torch.Tensor):\n\t        return self._normalize_yuv_42x(y, uv, 1 / 2)\n\t    def denormalize_yuv_420(self, y: torch.Tensor, uv: torch.Tensor):\n\t        return self._denormalize_yuv_42x(y, uv, 1 / 2)\n\tclass converter:\n\t    def __init__(self, kr=0.2126, kb=0.0722, depth=8, format='yuv420', upsample_mode='bilinear'):\n\t        self.krgb = (kr, 1 - kr - kb, kb)\n\t        self.depth = depth\n", "        self.uv_bias = (1 << (depth - 1)) / ((1 << depth) - 1)\n\t        match format:\n\t            case 'yuv444':\n\t                self.downsample = lambda x: x\n\t                self.upsample = lambda x: x\n\t            case 'yuv422':\n\t                self.downsample = functools.partial(F.interpolate, scale_factor=(1, 1 / 2), mode='bilinear',\n\t                                                    align_corners=False)\n\t                self.upsample = functools.partial(F.interpolate, scale_factor=(1, 2), mode=upsample_mode,\n\t                                                  align_corners=False)\n", "            case 'yuv420':\n\t                self.downsample = functools.partial(F.interpolate, scale_factor=(1 / 2, 1 / 2), mode='bilinear',\n\t                                                    align_corners=False)\n\t                self.upsample = functools.partial(F.interpolate, scale_factor=(2, 2), mode=upsample_mode,\n\t                                                  align_corners=False)\n\t    def rgb2yuv(self, x: torch.Tensor):\n\t        kr, kg, kb = self.krgb\n\t        r, g, b = torch.chunk(x, 3, 1)\n\t        y = kr * r + kg * g + kb * b\n\t        u = (y - b) / (kb - 1) / 2 + self.uv_bias\n", "        v = (y - r) / (kr - 1) / 2 + self.uv_bias\n\t        uv = torch.cat((u, v), dim=1)\n\t        return y, self.downsample(uv)\n\t    def yuv2rgb(self, y: torch.Tensor, uv: torch.Tensor):\n\t        kr, kg, kb = self.krgb\n\t        uv = self.upsample(uv - self.uv_bias)\n\t        u, v = torch.chunk(uv, 2, 1)\n\t        r = y + 2 * (1 - kr) * v\n\t        b = y + 2 * (1 - kb) * u\n\t        g = y - 2 * (1 - kr) * kr * v - 2 * (1 - kb) * kb * u\n", "        return torch.cat((r, g, b), dim=1)\n"]}
{"filename": "torch/model/cycmunet/part.py", "chunked_list": ["import torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom ..util import cat_conv\n\t\"\"\"CycMuNet Private Network Build Block\"\"\"\n\tclass Pro_align(nn.Module):\n\t    def __init__(self, args):\n\t        super(Pro_align, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.conv1x1 = nn.Conv2d(self.nf * 3, self.nf, 1, 1, 0)\n", "        self.conv3x3 = nn.Conv2d(self.nf, self.nf, 3, 1, 1)\n\t        self.conv1_3x3 = nn.Conv2d(self.nf * 2, self.nf, 3, 1, 1)\n\t        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\t    def forward(self, l1, l2, l3):\n\t        r1 = self.lrelu(self.conv3x3(l1))\n\t        r2 = self.lrelu(self.conv3x3(l2))\n\t        r3 = self.lrelu(self.conv3x3(l3))\n\t        fuse = self.lrelu(cat_conv(self.conv1x1, [r1, r2, r3]))\n\t        r1 = self.lrelu(cat_conv(self.conv1_3x3, [r1, fuse]))\n\t        r2 = self.lrelu(cat_conv(self.conv1_3x3, [r2, fuse]))\n", "        r3 = self.lrelu(cat_conv(self.conv1_3x3, [r3, fuse]))\n\t        return l1 + r1, l2 + r2, l3 + r3\n\tclass SR(nn.Module):\n\t    def __init__(self, args):\n\t        super(SR, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.factor = (self.args.upscale_factor, self.args.upscale_factor)\n\t        self.Pro_align = Pro_align(args)\n\t        self.conv1x1 = nn.Conv2d(self.nf, self.nf, 1, 1, 0)\n", "        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\t    def upsample(self, x):\n\t        x = F.interpolate(x, scale_factor=self.factor, mode='bilinear', align_corners=False)\n\t        return self.lrelu(self.conv1x1(x))\n\t    def forward(self, l1, l2, l3):\n\t        l1, l2, l3 = self.Pro_align(l1, l2, l3)\n\t        return tuple(self.upsample(i) for i in (l1, l2, l3))\n\tclass DR(nn.Module):\n\t    def __init__(self, args):\n\t        super(DR, self).__init__()\n", "        self.args = args\n\t        self.nf = self.args.nf\n\t        self.factor = (1 / self.args.upscale_factor, 1 / self.args.upscale_factor)\n\t        self.Pro_align = Pro_align(args)\n\t        self.conv = nn.Conv2d(self.nf, self.nf, 1, 1, 0)\n\t        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\t    def downsample(self, x):\n\t        x = F.interpolate(x, scale_factor=self.factor, mode='bilinear', align_corners=False)\n\t        return self.lrelu(self.conv(x))\n\t    def forward(self, l1, l2, l3):\n", "        l1 = self.downsample(l1)\n\t        l2 = self.downsample(l2)\n\t        l3 = self.downsample(l3)\n\t        return self.Pro_align(l1, l2, l3)\n\tclass Up_projection(nn.Module):\n\t    def __init__(self, args):\n\t        super(Up_projection, self).__init__()\n\t        self.args = args\n\t        self.SR = SR(args)\n\t        self.DR = DR(args)\n", "        self.SR1 = SR(args)\n\t    def forward(self, l1, l2, l3):\n\t        h1, h2, h3 = self.SR(l1, l2, l3)\n\t        d1, d2, d3 = self.DR(h1, h2, h3)\n\t        r1, r2, r3 = d1 - l1, d2 - l2, d3 - l3\n\t        s1, s2, s3 = self.SR1(r1, r2, r3)\n\t        return h1 + s1, h2 + s3, h3 + s3\n\tclass Down_projection(nn.Module):\n\t    def __init__(self, args):\n\t        super(Down_projection, self).__init__()\n", "        self.args = args\n\t        self.SR = SR(args)\n\t        self.DR = DR(args)\n\t        self.DR1 = DR(args)\n\t    def forward(self, h1, h2, h3):\n\t        l1, l2, l3 = self.DR(h1, h2, h3)\n\t        s1, s2, s3 = self.SR(l1, l2, l3)\n\t        r1, r2, r3 = s1 - h1, s2 - h2, s3 - h3\n\t        d1, d2, d3 = self.DR1(r1, r2, r3)\n\t        return l1 + d1, l2 + d2, l3 + d3\n"]}
{"filename": "torch/model/cycmunet/__init__.py", "chunked_list": ["from typing import Union\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom ..util import RGBOrYUV\n\tfrom .module import head, feature_extract, feature_fusion, mutual_cycle, feature_recon, tail\n\tclass CycMuNet(nn.Module):\n\t    def __init__(self, args):\n\t        super(CycMuNet, self).__init__()\n\t        self.args = args\n\t        self.factor = (self.args.upscale_factor, self.args.upscale_factor)\n", "        self.upsample_mode = 'bilinear'\n\t        self.head = head(args)\n\t        self.fe = feature_extract(args)\n\t        self.ff = feature_fusion(args)\n\t        self.mu = mutual_cycle(args)\n\t        self.fr = feature_recon(args)\n\t        self.tail = tail(args)\n\t    def merge_hf(self, lf, hf):\n\t        return F.interpolate(lf, scale_factor=self.factor, mode='bilinear', align_corners=False) + hf\n\t    def head_fe(self, x_or_yuv: RGBOrYUV):\n", "        x = self.head(x_or_yuv)\n\t        return self.fe(x)\n\t    def mu_fr_tail(self, lf, all_frames):\n\t        mu_out = self.mu(*lf, all_frames=all_frames)\n\t        if all_frames:\n\t            *hf, lf1 = mu_out\n\t            lf1 = self.fr(lf1)\n\t            hf = tuple(self.merge_hf(l, self.fr(h)) for l, h in zip(lf, hf))\n\t            outs = tuple(self.tail(i) for i in (*hf, lf1))\n\t        else:\n", "            outs = tuple(self.tail(self.merge_hf(l, self.fr(h))) for l, h in zip(lf, mu_out))\n\t        return outs\n\t    def forward_batch(self, lf0: RGBOrYUV, lf2: RGBOrYUV, all_frames=True, stop_at_conf=False):\n\t        lf0s, lf2s = self.head_fe(lf0), self.head_fe(lf2)\n\t        lf1, _ = self.ff(lf0s, lf2s)\n\t        if stop_at_conf:  # TODO detect frame difference and exit if too big\n\t            return\n\t        lf = (lf0s[-1], lf1, lf2s[-1])\n\t        return self.mu_fr_tail(lf, all_frames)\n\t    def forward_sequence(self, x_or_yuv: RGBOrYUV, all_frames=False):\n", "        ls = self.head_fe(x_or_yuv)\n\t        n = ls[0].shape[0]\n\t        lf1, _ = self.ff([layer[:n - 1] for layer in ls], [layer[1:] for layer in ls])\n\t        lf = (ls[-1][:n - 1], lf1, ls[-1][1:])\n\t        return self.mu_fr_tail(lf, all_frames)\n\t    # This is for symbolic tracing for sparsity\n\t    def pseudo_forward_sparsity(self, lf0, lf1, lf2):\n\t        hf0, *_ = self.mu(lf0, lf1, lf2, all_frames=True)\n\t        return self.fr(hf0)\n\t    def forward(self, lf0: RGBOrYUV, lf2: Union[RGBOrYUV, None] = None, sparsity_ex=None, /, batch_mode='batch',\n", "                **kwargs):\n\t        if batch_mode == '_no_use_sparsity_pseudo':\n\t            return self.pseudo_forward_sparsity(lf0, lf2, sparsity_ex)\n\t        if batch_mode == 'batch':\n\t            outs = self.forward_batch(lf0, lf2, **kwargs)\n\t        elif batch_mode == 'sequence':\n\t            outs = self.forward_sequence(lf0, **kwargs)\n\t        else:\n\t            raise ValueError(f\"Invalid batch_mode: {batch_mode}\")\n\t        return tuple(outs)\n", "__all__ = ['CycMuNet']\n"]}
{"filename": "torch/model/cycmunet/module.py", "chunked_list": ["from typing import Tuple, List\n\timport torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tfrom ..util import cat_conv\n\tfrom ..part import ResidualBlock_noBN, PCDLayer\n\tfrom .part import Down_projection, Up_projection\n\t\"\"\"CycMuNet model partitions\"\"\"\n\tclass head(nn.Module):\n\t    def __init__(self, args):\n", "        super(head, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\t        match self.args.format:\n\t            case 'rgb':\n\t                self.conv_first = nn.Conv2d(3, self.nf, 3, 1, 1)\n\t                self.forward = self.forward_rgb\n\t            case 'yuv444':\n\t                self.conv_first = nn.Conv2d(3, self.nf, 3, 1, 1)\n", "                self.forward = self.forward_yuv444\n\t            case 'yuv422':\n\t                self.conv_first_y = nn.Conv2d(1, self.nf, 3, 1, 1)\n\t                self.conv_up = nn.ConvTranspose2d(2, self.nf, (1, 3), (1, 2), (0, 1), (0, 1))\n\t                self.forward = self.forward_yuv42x\n\t            case 'yuv420':\n\t                self.conv_first_y = nn.Conv2d(1, self.nf, 3, 1, 1)\n\t                self.conv_up = nn.ConvTranspose2d(2, self.nf, 3, 2, 1, 1)\n\t                self.forward = self.forward_yuv42x\n\t            case unk:\n", "                raise ValueError(f'unknown input pixel format: {unk}')\n\t    def forward_rgb(self, x: torch.Tensor):\n\t        x = self.lrelu(self.conv_first(x))\n\t        return x\n\t    def forward_yuv444(self, yuv: Tuple[torch.Tensor, torch.Tensor]):\n\t        x = torch.cat(yuv, dim=1)\n\t        x = self.lrelu(self.conv_first(x))\n\t        return x\n\t    def forward_yuv42x(self, yuv: Tuple[torch.Tensor, torch.Tensor]):\n\t        y, uv = yuv\n", "        y = self.conv_first_y(y)\n\t        uv = self.conv_up(uv)\n\t        x = self.lrelu(y + uv)\n\t        return x\n\tclass feature_extract(nn.Module):\n\t    def __init__(self, args):\n\t        super(feature_extract, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n", "        self.layers = self.args.layers\n\t        self.front_RBs = 5\n\t        self.feature_extraction = nn.Sequential(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.front_RBs)))\n\t        self.fea_conv1s = nn.ModuleList(nn.Conv2d(self.nf, self.nf, 3, 2, 1, bias=True) for _ in range(self.layers - 1))\n\t        self.fea_conv2s = nn.ModuleList(nn.Conv2d(self.nf, self.nf, 3, 1, 1, bias=True) for _ in range(self.layers - 1))\n\t        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\t    def forward(self, x: torch.Tensor):\n\t        features: List[torch.Tensor] = [self.feature_extraction(x)]\n\t        for i in range(self.layers - 1):\n\t            feature = features[-1]\n", "            _, _, h, w = feature.shape\n\t            h = torch.div(h + 1, 2, rounding_mode=\"trunc\") * 2 - h\n\t            w = torch.div(w + 1, 2, rounding_mode=\"trunc\") * 2 - w\n\t            feature = F.pad(feature, (0, w, 0, h), mode=\"replicate\")\n\t            feature = self.lrelu(self.fea_conv1s[i](feature))\n\t            feature = self.lrelu(self.fea_conv2s[i](feature))\n\t            features.append(feature)\n\t        return tuple(features[::-1])  # lowest dimension layer at first\n\tclass feature_fusion(nn.Module):\n\t    def __init__(self, args):\n", "        super(feature_fusion, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.groups = self.args.groups\n\t        self.layers = self.args.layers\n\t        # from small to big.\n\t        self.modules12 = nn.ModuleList(PCDLayer(args, i == 0) for i in range(self.layers))\n\t        self.modules21 = nn.ModuleList(PCDLayer(args, i == 0) for i in range(self.layers))\n\t        self.fusion = nn.Conv2d(2 * self.nf, self.nf, 1, 1)\n\t        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n", "    @staticmethod\n\t    def fuse_features(modules, f1, f2):\n\t        offset, feature = None, None\n\t        for idx, sources in enumerate(zip(f1, f2)):\n\t            offset, feature = modules[idx](sources, offset, feature)\n\t        return feature\n\t    def forward(self, f1, f2):\n\t        feature1 = self.fuse_features(self.modules12, f1, f2)\n\t        feature2 = self.fuse_features(self.modules21, f2, f1)\n\t        fused_feature = cat_conv(self.fusion, (feature1, feature2))\n", "        return fused_feature, None\n\tclass mutual_cycle(nn.Module):\n\t    def __init__(self, args):\n\t        super(mutual_cycle, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.cycle_count = self.args.cycle_count\n\t        self.merge = nn.ModuleList(nn.Conv2d(64 * (i + 1), 64, 1, 1, 0) for i in range(self.cycle_count))\n\t        self.merge1 = nn.ModuleList(nn.Conv2d(64 * (i + 1), 64, 1, 1, 0) for i in range(self.cycle_count))\n\t        self.down = nn.ModuleList(Down_projection(args) for _ in range(self.cycle_count))\n", "        self.up = nn.ModuleList(Up_projection(args) for _ in range(self.cycle_count + 1))\n\t        self.conv = nn.Conv2d(self.nf * (2 * self.cycle_count + 1), self.nf, 1, 1, 0)\n\t        self.lrelu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n\t    def forward(self, lf0, lf1, lf2, all_frames=False):\n\t        assert self.cycle_count > 0\n\t        l_out, h_out = [(lf0, lf1, lf2)], []\n\t        for j in range(self.cycle_count):\n\t            l_feats = tuple(self.lrelu(cat_conv(self.merge[j], frame_outs)) for frame_outs in zip(*l_out))\n\t            h_feat = self.up[j](*l_feats)\n\t            h_out.append(h_feat)\n", "            h_feats = tuple(self.lrelu(cat_conv(self.merge1[j], frame_outs)) for frame_outs in zip(*h_out))\n\t            l_feat = self.down[j](*h_feats)\n\t            l_out.append(l_feat)\n\t        lf_out, hf_out = [l_out[-1]], []\n\t        for j in range(self.cycle_count):\n\t            l_feats = tuple(self.lrelu(cat_conv(self.merge[j], frame_outs)) for frame_outs in zip(*lf_out))\n\t            h_feat = self.up[j](*l_feats)\n\t            hf_out.append(h_feat)\n\t            l_feat = self.down[j](*h_feat)\n\t            lf_out.append(l_feat)\n", "        hf_out.append(self.up[self.cycle_count](*l_feats))\n\t        if all_frames:\n\t            h_outs = zip(*h_out, *hf_out)  # packed 3 frames\n\t            _, l1_out, _ = zip(*l_out, *lf_out[1:])\n\t            h_outs = tuple(self.lrelu(cat_conv(self.conv, h_frame)) for h_frame in h_outs)\n\t            l1_out = self.lrelu(cat_conv(self.conv, l1_out))\n\t            return *h_outs, l1_out\n\t        else:\n\t            h1_out, h2_out, _ = zip(*h_out, *hf_out)\n\t            h1_out = self.lrelu(cat_conv(self.conv, h1_out))\n", "            h2_out = self.lrelu(cat_conv(self.conv, h2_out))\n\t            return h1_out, h2_out\n\tclass feature_recon(nn.Module):\n\t    def __init__(self, args):\n\t        super(feature_recon, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        self.back_RBs = 40\n\t        self.factor = (self.args.upscale_factor, self.args.upscale_factor)\n\t        self.recon_trunk = nn.Sequential(*(ResidualBlock_noBN(nf=self.nf) for _ in range(self.back_RBs)))\n", "    def forward(self, x):\n\t        out = self.recon_trunk(x)\n\t        return out\n\tclass tail(nn.Module):\n\t    def __init__(self, args):\n\t        super(tail, self).__init__()\n\t        self.args = args\n\t        self.nf = self.args.nf\n\t        match self.args.format:\n\t            case 'rgb':\n", "                self.conv_last2 = nn.Conv2d(self.nf, 3, 3, 1, 1)\n\t                self.forward = self.forward_rgb\n\t            case 'yuv444':\n\t                self.conv_last2 = nn.Conv2d(self.nf, 3, 3, 1, 1)\n\t                self.forward = self.forward_yuv444\n\t            case 'yuv422':\n\t                self.conv_last_y = nn.Conv2d(self.nf, 1, 3, 1, 1)\n\t                self.conv_last_uv = nn.Conv2d(self.nf, 2, (1, 3), (1, 2), (0, 1))\n\t                self.forward = self.forward_yuv42x\n\t            case 'yuv420':\n", "                self.conv_last_y = nn.Conv2d(self.nf, 1, 3, 1, 1)\n\t                self.conv_last_uv = nn.Conv2d(self.nf, 2, 3, 2, 1)\n\t                self.forward = self.forward_yuv42x\n\t            case unk:\n\t                raise ValueError(f'unknown input pixel format: {unk}')\n\t    def forward_rgb(self, x):\n\t        out = self.conv_last2(x)\n\t        return out,\n\t    def forward_yuv444(self, x):\n\t        out = self.conv_last2(x)\n", "        return out[:, :1, ...], out[:, 1:, ...]\n\t    def forward_yuv42x(self, x):\n\t        y = self.conv_last_y(x)\n\t        uv = self.conv_last_uv(x)\n\t        return y, uv\n"]}
{"filename": "torch/dataset/sequence.py", "chunked_list": ["import glob\n\timport itertools\n\timport pathlib\n\timport random\n\tfrom typing import List\n\timport torch.utils.data as data\n\timport numpy as np\n\timport torchvision.transforms\n\tfrom PIL import Image, ImageFilter\n\tclass ImageSequenceDataset(data.Dataset):\n", "    want_shuffle = True\n\t    pix_type = 'rgb'\n\t    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n\t        self.dataset_base = pathlib.Path(index_file).parent\n\t        self.sequences = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n\t                          if i if not i.startswith('#')]\n\t        self.patch_size = patch_size\n\t        self.scale_factor = scale_factor\n\t        self.augment = augment\n\t        self.rand = random.Random(seed)\n", "        self.transform = torchvision.transforms.ToTensor()\n\t    def _load_sequence(self, path):\n\t        path = self.dataset_base / \"sequences\" / path\n\t        files = glob.glob(\"*.png\", root_dir=path)\n\t        assert len(files) > 1\n\t        images = [Image.open(file) for file in files]\n\t        if not all(i.size != images[0].size for i in images[1:]):\n\t            raise ValueError(\"sequence has different dimensions\")\n\t        return images\n\t    def _prepare_images(self, images: List[Image.Image]):\n", "        w, h = images[0].size\n\t        f = self.scale_factor\n\t        sw, sh = self.patch_size\n\t        sw, sh = sw * f, sh * f\n\t        assert h >= sh and w >= sw\n\t        dh, dw = self.rand.randint(0, h - sh), self.rand.randint(0, w - sw)\n\t        images = [i.crop((dw, dh, dw + sw, dh + sh)) for i in images]\n\t        return images\n\t    trans_groups = {\n\t        'none': [None],\n", "        'rotate': [None, Image.ROTATE_90, Image.ROTATE_180, Image.ROTATE_270],\n\t        'mirror': [None, Image.FLIP_LEFT_RIGHT],\n\t        'flip': [None, Image.FLIP_LEFT_RIGHT, Image.FLIP_TOP_BOTTOM, Image.ROTATE_180],\n\t        'all': [None] + [e.value for e in Image.Transpose],\n\t    }\n\t    trans_names = [e.name for e in Image.Transpose]\n\t    def _augment_images(self, images: List[Image.Image], trans_mode='all'):\n\t        trans_action = 'none'\n\t        trans_op = self.rand.choice(self.trans_groups[trans_mode])\n\t        if trans_op is not None:\n", "            images = [i.transpose(trans_op) for i in images]\n\t            trans_action = self.trans_names[trans_op]\n\t        return images, trans_action\n\t    scale_filters = [Image.BILINEAR, Image.BICUBIC, Image.LANCZOS]\n\t    def _scale_images(self, images: List[Image.Image]):\n\t        f = self.scale_factor\n\t        return [i.resize((i.width // f, i.height // f), self.rand.choice(self.scale_filters)) for i in images]\n\t    def _degrade_images(self, images: List[Image.Image]):\n\t        degrade_action = None\n\t        decision = self.rand.randrange(4)\n", "        if decision == 1:\n\t            degrade_action = 'box'\n\t            percent = 0.5 + 0.5 * self.rand.random()\n\t            images = [Image.blend(j, j.copy().filter(ImageFilter.BoxBlur(1)), percent) for j in images]\n\t        elif decision == 2:\n\t            degrade_action = 'gaussian'\n\t            radius = self.rand.random()\n\t            images = [j.filter(ImageFilter.GaussianBlur(radius)) for j in images]\n\t        elif decision == 3:\n\t            degrade_action = 'halo'\n", "            percent = 0.5 + 0.5 * self.rand.random()\n\t            images = [Image.blend(i,\n\t                                  i.resize((i.width // 2, i.height // 2), resample=Image.LANCZOS)\n\t                                  .resize(i.size, resample=Image.BILINEAR), percent)\n\t                      for i in images]\n\t        return images, degrade_action\n\t    def __len__(self):\n\t        return len(self.sequences)\n\t    def __getitem__(self, idx):\n\t        sequence = self._load_sequence(self.sequences[idx])\n", "        sequence = self._prepare_images(sequence)  # crop to requested size\n\t        original, _ = self._augment_images(sequence)  # flip and rotates\n\t        lfs_pred = [np.array(lf.resize((lf.width // self.scale_factor, lf.height // self.scale_factor), Image.LANCZOS))\n\t                    for lf in original[1::2]]\n\t        lfs_deg = self._scale_images(original[::2])\n\t        # lfs_deg, _ = self._degrade_images(lfs_deg)\n\t        degraded = [i for i in itertools.zip_longest(lfs_deg, lfs_pred) if i is not None]\n\t        original = [self.transform(i) for i in original]\n\t        degraded = [self.transform(i) for i in degraded]\n\t        return original, degraded\n"]}
{"filename": "torch/dataset/__init__.py", "chunked_list": ["from .sequence import ImageSequenceDataset\n\tfrom .video import VideoFrameDataset\n\tfrom .util import InterleavedDataset\n"]}
{"filename": "torch/dataset/util.py", "chunked_list": ["import bisect\n\timport torch.utils.data as data\n\tclass InterleavedDataset(data.Dataset):\n\t    def __init__(self, *datasets: data.Dataset):\n\t        self.datasets = datasets\n\t        if not all(hasattr(i, '__len__') for i in datasets):\n\t            raise AttributeError('need datasets with known length')\n\t        sizes = [len(i) for i in datasets]\n\t        self.total = sum(sizes)\n\t        self.sizes = [0] + sorted(set(sizes))\n", "        self.index = {\n\t            0: datasets\n\t        }\n\t        total, last_n = 0, len(datasets)\n\t        for last_size, size in zip(self.sizes, self.sizes[1:]):\n\t            total += (size - last_size) * last_n\n\t            this_datasets = [ds for ds in datasets if len(ds) > size]\n\t            self.index[total] = this_datasets\n\t            last_n = len(this_datasets)\n\t        self.index.popitem()\n", "        self.index_keys = list(self.index.keys())\n\t    def __len__(self):\n\t        return self.total\n\t    def __getitem__(self, idx):\n\t        stage = bisect.bisect_right(self.index_keys, idx) - 1\n\t        offset = self.sizes[stage]\n\t        begin = self.index_keys[stage]\n\t        idx -= begin\n\t        datasets = self.index[begin]\n\t        n, i = idx % len(datasets), idx // len(datasets)\n", "        return datasets[n][offset + i]\n"]}
{"filename": "torch/dataset/video.py", "chunked_list": ["import bisect\n\timport collections\n\timport functools\n\timport itertools\n\timport pathlib\n\timport random\n\tfrom typing import List, Tuple\n\timport av\n\timport av.logging\n\timport numpy as np\n", "import cv2\n\timport torch\n\timport torch.utils.data as data\n\tav.logging.set_level(av.logging.FATAL)\n\tclass Video:\n\t    def __init__(self, file, kf):\n\t        self.container = av.open(file)\n\t        self.stream = self.container.streams.video[0]\n\t        self.stream.thread_type = \"AUTO\"\n\t        self.at = 0\n", "        self.kf = kf\n\t    def get_frames(self, pts, n=1):\n\t        frames = []\n\t        if bisect.bisect_left(self.kf, pts) != bisect.bisect_left(self.kf, self.at) or pts <= self.at:\n\t            self.container.seek(pts, stream=self.stream)\n\t        found = False\n\t        for frame in self.container.decode(video=0):\n\t            if not found and frame.pts != pts:\n\t                continue\n\t            found = True\n", "            self.at = frame.pts\n\t            yuv = frame.to_ndarray()\n\t            h, w = frame.height, frame.width\n\t            y, uv = yuv[:h, :].reshape(1, h, w), yuv[h:, :].reshape(2, h // 2, w // 2)\n\t            frames.append((y, uv))\n\t            if len(frames) == n:\n\t                return frames\n\t        raise ValueError(\"unexpected end\")\n\t    def __del__(self):\n\t        self.container.close()\n", "video_info = collections.namedtuple('video_info', [\n\t    'org',\n\t    'deg',\n\t    'frames',\n\t    'pts_org',\n\t    'pts_deg',\n\t    'key_org',\n\t    'key_deg'\n\t])\n\tclass VideoFrameDataset(data.Dataset):\n", "    want_shuffle = False\n\t    pix_type = 'yuv'\n\t    def __init__(self, index_file, patch_size, scale_factor, augment, seed=0):\n\t        self.dataset_base = pathlib.PurePath(index_file).parent\n\t        index_lines = [i for i in open(index_file, 'r', encoding='utf-8').read().split('\\n')\n\t                       if i if not i.startswith('#')]\n\t        files = [tuple(i.split(',')) for i in index_lines]\n\t        self.files = []\n\t        self.indexes = []\n\t        for org, deg, frames, pts_org, pts_deg, key_org, key_deg in files:\n", "            info = video_info(\n\t                org,\n\t                deg,\n\t                int(frames),\n\t                tuple(int(i) for i in pts_org.split(' ')),\n\t                tuple(int(i) for i in pts_deg.split(' ')),\n\t                tuple(int(i) for i in key_org.split(' ')),\n\t                tuple(int(i) for i in key_deg.split(' ')),\n\t            )\n\t            self.files.append(info)\n", "            self.indexes.append(info.frames)\n\t        self.indexes = list(itertools.accumulate(self.indexes))\n\t        self.patch_size = (patch_size, patch_size) if isinstance(patch_size, int) else patch_size\n\t        self.scale_factor = scale_factor\n\t        self.augment = augment\n\t        self.rand = random.Random(seed)\n\t    @staticmethod\n\t    def transform(yuv):\n\t        return tuple(torch.from_numpy(i).contiguous().to(dtype=torch.float32).div(255) for i in yuv)\n\t    @functools.lru_cache(2)\n", "    def get_video(self, v_idx):\n\t        info = self.files[v_idx]\n\t        return Video(str(self.dataset_base / info.org), info.key_org), \\\n\t            Video(str(self.dataset_base / info.deg), info.key_deg), info.pts_org, info.pts_deg\n\t    def _augment_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n\t        if self.rand.random() > 0.5:\n\t            org = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in org]\n\t            deg = [(y[..., ::-1].copy(), uv[..., ::-1].copy()) for y, uv in deg]\n\t        return org, deg\n\t    def _prepare_frame(self, org: List[Tuple[np.ndarray]], deg: List[Tuple[np.ndarray]]):\n", "        _, h, w = deg[0][0].shape\n\t        sw, sh = self.patch_size\n\t        sh_uv, sw_uv = sh // 2, sw // 2\n\t        assert h >= sh and w >= sw\n\t        dh, dw = self.rand.randrange(0, h - sh + 2, 2), self.rand.randrange(0, w - sw + 2, 2)\n\t        dh_uv, dw_uv = dh // 2, dw // 2\n\t        deg = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in deg]\n\t        f = self.scale_factor\n\t        size, size_uv = (sw, sh), (sw_uv, sh_uv)\n\t        sh, sw, sh_uv, sw_uv = sh * f, sw * f, sh_uv * f, sw_uv * f\n", "        dh, dw, dh_uv, dw_uv = dh * f, dw * f, dh_uv * f, dw_uv * f\n\t        org = [(y[:, dh:dh+sh, dw:dw+sw], uv[:, dh_uv:dh_uv+sh_uv, dw_uv:dw_uv+sw_uv]) for y, uv in org]\n\t        deg1_y = cv2.resize(org[1][0][0], size, interpolation=cv2.INTER_LANCZOS4)\n\t        deg1_u = cv2.resize(org[1][1][0], size_uv, interpolation=cv2.INTER_LANCZOS4)\n\t        deg1_v = cv2.resize(org[1][1][1], size_uv, interpolation=cv2.INTER_LANCZOS4)\n\t        deg.insert(1, (deg1_y.reshape((1, *size[::-1])), np.stack((deg1_u, deg1_v)).reshape((2, *size_uv[::-1]))))\n\t        return org, deg\n\t    def __len__(self):\n\t        return self.indexes[-1]\n\t    def __getitem__(self, idx):\n", "        v_idx = bisect.bisect_right(self.indexes, idx)\n\t        f_idx = idx if v_idx == 0 else idx - self.indexes[v_idx - 1]\n\t        org, deg, pts_org, pts_deg = self.get_video(v_idx)\n\t        org_frames = org.get_frames(pts_org[f_idx], 3)\n\t        deg_frames = deg.get_frames(pts_deg[f_idx], 3)\n\t        deg_frames.pop(1)\n\t        org_frames, deg_frames = self._prepare_frame(org_frames, deg_frames)\n\t        if self.augment:\n\t            org_frames, deg_frames = self._augment_frame(org_frames, deg_frames)\n\t        org_frames = [self.transform(i) for i in org_frames]\n", "        deg_frames = [self.transform(i) for i in deg_frames]\n\t        return org_frames, deg_frames\n"]}
