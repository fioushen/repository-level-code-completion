{"filename": "setup.py", "chunked_list": ["from setuptools import find_packages, setup\n\tsetup(\n\t    name=\"muse-maskgit-pytorch\",\n\t    packages=find_packages(exclude=[]),\n\t    version=\"0.1.6\",\n\t    license=\"MIT\",\n\t    description=\"MUSE - Text-to-Image Generation via Masked Generative Transformers, in Pytorch\",\n\t    author=\"Phil Wang\",\n\t    author_email=\"lucidrains@gmail.com\",\n\t    long_description_content_type=\"text/markdown\",\n", "    url=\"https://github.com/lucidrains/muse-maskgit-pytorch\",\n\t    keywords=[\n\t        \"artificial intelligence\",\n\t        \"deep learning\",\n\t        \"transformers\",\n\t        \"attention mechanism\",\n\t        \"text-to-image\",\n\t    ],\n\t    extras_require={\n\t        \"dev\": [\n", "            \"pre-commit>=3.3.2\",\n\t            \"black>=23.3.0\",\n\t            \"ruff>=0.0.272\",\n\t        ]\n\t    },\n\t    install_requires=[\n\t        \"accelerate\",\n\t        \"diffusers\",\n\t        \"datasets\",\n\t        \"beartype\",\n", "        \"einops>=0.6\",\n\t        \"ema-pytorch\",\n\t        \"omegaconf>=2.3.0\",\n\t        \"pillow\",\n\t        \"sentencepiece\",\n\t        \"torch>=2.0\",\n\t        \"torchmetrics<0.8.0\",\n\t        \"pytorch-lightning>=2.0.0\",\n\t        \"taming-transformers @ git+https://github.com/neggles/taming-transformers.git@v0.0.2\",\n\t        \"transformers\",\n", "        \"torchvision\",\n\t        \"torch_optimizer\",\n\t        \"tqdm\",\n\t        \"timm\",\n\t        \"tqdm-loggable\",\n\t        \"vector-quantize-pytorch>=0.10.14\",\n\t        \"lion-pytorch\",\n\t        \"omegaconf\",\n\t        \"xformers>=0.0.20\",\n\t    ],\n", "    classifiers=[\n\t        \"Development Status :: 4 - Beta\",\n\t        \"Intended Audience :: Developers\",\n\t        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n\t        \"License :: OSI Approved :: MIT License\",\n\t        \"Programming Language :: Python :: 3.6\",\n\t    ],\n\t)\n"]}
{"filename": "train_muse_vae.py", "chunked_list": ["import argparse\n\timport glob\n\timport os\n\timport re\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional, Union\n\timport wandb\n\tfrom accelerate.utils import ProjectConfiguration\n\tfrom datasets import load_dataset\n\tfrom omegaconf import OmegaConf\n", "from muse_maskgit_pytorch import (\n\t    VQGanVAE,\n\t    VQGanVAETaming,\n\t    VQGanVAETrainer,\n\t    get_accelerator,\n\t)\n\tfrom muse_maskgit_pytorch.dataset import (\n\t    ImageDataset,\n\t    get_dataset_from_dataroot,\n\t    split_dataset_into_dataloaders,\n", ")\n\t# disable bitsandbytes welcome message.\n\tos.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--webdataset\", type=str, default=None, help=\"Path to webdataset if using one.\")\n\tparser.add_argument(\n\t    \"--only_save_last_checkpoint\",\n\t    action=\"store_true\",\n\t    help=\"Only save last checkpoint.\",\n\t)\n", "parser.add_argument(\n\t    \"--validation_image_scale\",\n\t    default=1,\n\t    type=float,\n\t    help=\"Factor by which to scale the validation images.\",\n\t)\n\tparser.add_argument(\n\t    \"--no_center_crop\",\n\t    action=\"store_true\",\n\t    help=\"Don't do center crop.\",\n", ")\n\tparser.add_argument(\n\t    \"--no_flip\",\n\t    action=\"store_true\",\n\t    help=\"Don't flip image.\",\n\t)\n\tparser.add_argument(\n\t    \"--random_crop\",\n\t    action=\"store_true\",\n\t    help=\"Crop the images at random locations instead of cropping from the center.\",\n", ")\n\tparser.add_argument(\n\t    \"--dataset_save_path\",\n\t    type=str,\n\t    default=\"dataset\",\n\t    help=\"Path to save the dataset if you are making one from a directory\",\n\t)\n\tparser.add_argument(\n\t    \"--clear_previous_experiments\",\n\t    action=\"store_true\",\n", "    help=\"Whether to clear previous experiments.\",\n\t)\n\tparser.add_argument(\"--max_grad_norm\", type=float, default=None, help=\"Max gradient norm.\")\n\tparser.add_argument(\n\t    \"--discr_max_grad_norm\",\n\t    type=float,\n\t    default=None,\n\t    help=\"Max gradient norm for discriminator.\",\n\t)\n\tparser.add_argument(\"--seed\", type=int, default=42, help=\"Seed.\")\n", "parser.add_argument(\"--valid_frac\", type=float, default=0.05, help=\"validation fraction.\")\n\tparser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use ema.\")\n\tparser.add_argument(\"--ema_beta\", type=float, default=0.995, help=\"Ema beta.\")\n\tparser.add_argument(\"--ema_update_after_step\", type=int, default=1, help=\"Ema update after step.\")\n\tparser.add_argument(\n\t    \"--ema_update_every\",\n\t    type=int,\n\t    default=1,\n\t    help=\"Ema update every this number of steps.\",\n\t)\n", "parser.add_argument(\n\t    \"--apply_grad_penalty_every\",\n\t    type=int,\n\t    default=4,\n\t    help=\"Apply gradient penalty every this number of steps.\",\n\t)\n\tparser.add_argument(\n\t    \"--image_column\",\n\t    type=str,\n\t    default=\"image\",\n", "    help=\"The column of the dataset containing an image.\",\n\t)\n\tparser.add_argument(\n\t    \"--caption_column\",\n\t    type=str,\n\t    default=\"caption\",\n\t    help=\"The column of the dataset containing a caption or a list of captions.\",\n\t)\n\tparser.add_argument(\n\t    \"--log_with\",\n", "    type=str,\n\t    default=\"wandb\",\n\t    help=(\n\t        'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n\t        ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n\t    ),\n\t)\n\tparser.add_argument(\n\t    \"--project_name\",\n\t    type=str,\n", "    default=\"muse_vae\",\n\t    help=(\"Name to use for the project to identify it when saved to a tracker such as wandb or tensorboard.\"),\n\t)\n\tparser.add_argument(\n\t    \"--run_name\",\n\t    type=str,\n\t    default=None,\n\t    help=(\n\t        \"Name to use for the run to identify it when saved to a tracker such\"\n\t        \" as wandb or tensorboard. If not specified a random one will be generated.\"\n", "    ),\n\t)\n\tparser.add_argument(\n\t    \"--wandb_user\",\n\t    type=str,\n\t    default=None,\n\t    help=(\n\t        \"Specify the name for the user or the organization in which the project will be saved when using wand.\"\n\t    ),\n\t)\n", "parser.add_argument(\n\t    \"--mixed_precision\",\n\t    type=str,\n\t    default=\"no\",\n\t    choices=[\"no\", \"fp8\", \"fp16\", \"bf16\"],\n\t    help=\"Precision to train on.\",\n\t)\n\tparser.add_argument(\n\t    \"--use_8bit_adam\",\n\t    action=\"store_true\",\n", "    help=\"Whether to use the 8bit adam optimiser\",\n\t)\n\tparser.add_argument(\n\t    \"--results_dir\",\n\t    type=str,\n\t    default=\"results\",\n\t    help=\"Path to save the training samples and checkpoints\",\n\t)\n\tparser.add_argument(\n\t    \"--logging_dir\",\n", "    type=str,\n\t    default=None,\n\t    help=\"Path to log the losses and LR\",\n\t)\n\t# vae_trainer args\n\tparser.add_argument(\n\t    \"--dataset_name\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Name of the huggingface dataset used.\",\n", ")\n\tparser.add_argument(\n\t    \"--hf_split_name\",\n\t    type=str,\n\t    default=\"train\",\n\t    help=\"Subset or split to use from the dataset when using a dataset form HuggingFace.\",\n\t)\n\tparser.add_argument(\n\t    \"--streaming\",\n\t    action=\"store_true\",\n", "    help=\"Whether to stream the huggingface dataset\",\n\t)\n\tparser.add_argument(\n\t    \"--train_data_dir\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Dataset folder where your input images for training are.\",\n\t)\n\tparser.add_argument(\n\t    \"--num_train_steps\",\n", "    type=int,\n\t    default=-1,\n\t    help=\"Total number of steps to train for. eg. 50000. | Use only if you want to stop training early\",\n\t)\n\tparser.add_argument(\n\t    \"--num_epochs\",\n\t    type=int,\n\t    default=5,\n\t    help=\"Total number of epochs to train for. eg. 5.\",\n\t)\n", "parser.add_argument(\"--dim\", type=int, default=128, help=\"Model dimension.\")\n\tparser.add_argument(\"--batch_size\", type=int, default=1, help=\"Batch Size.\")\n\tparser.add_argument(\"--lr\", type=float, default=1e-5, help=\"Learning Rate.\")\n\tparser.add_argument(\n\t    \"--gradient_accumulation_steps\",\n\t    type=int,\n\t    default=1,\n\t    help=\"Gradient Accumulation.\",\n\t)\n\tparser.add_argument(\n", "    \"--save_results_every\",\n\t    type=int,\n\t    default=100,\n\t    help=\"Save results every this number of steps.\",\n\t)\n\tparser.add_argument(\n\t    \"--save_model_every\",\n\t    type=int,\n\t    default=500,\n\t    help=\"Save the model every this number of steps.\",\n", ")\n\tparser.add_argument(\n\t    \"--checkpoint_limit\",\n\t    type=int,\n\t    default=None,\n\t    help=\"Keep only X number of checkpoints and delete the older ones.\",\n\t)\n\tparser.add_argument(\"--vq_codebook_size\", type=int, default=256, help=\"Image Size.\")\n\tparser.add_argument(\"--vq_codebook_dim\", type=int, default=256, help=\"VQ Codebook dimensions.\")\n\tparser.add_argument(\n", "    \"--channels\", type=int, default=3, help=\"Number of channels for the VAE. Use 3 for RGB or 4 for RGBA.\"\n\t)\n\tparser.add_argument(\"--layers\", type=int, default=4, help=\"Number of layers for the VAE.\")\n\tparser.add_argument(\"--discr_layers\", type=int, default=4, help=\"Number of layers for the VAE discriminator.\")\n\tparser.add_argument(\n\t    \"--image_size\",\n\t    type=int,\n\t    default=256,\n\t    help=\"Image size. You may want to start with small images, and then curriculum learn to larger ones, but because the vae is all convolution, it should generalize to 512 (as in paper) without training on it\",\n\t)\n", "parser.add_argument(\n\t    \"--lr_scheduler\",\n\t    type=str,\n\t    default=\"constant\",\n\t    help='The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]',\n\t)\n\tparser.add_argument(\n\t    \"--scheduler_power\",\n\t    type=float,\n\t    default=1.0,\n", "    help=\"Controls the power of the polynomial decay schedule used by the CosineScheduleWithWarmup scheduler. \"\n\t    \"It determines the rate at which the learning rate decreases during the schedule.\",\n\t)\n\tparser.add_argument(\n\t    \"--lr_warmup_steps\",\n\t    type=int,\n\t    default=0,\n\t    help=\"Number of steps for the warmup in the lr scheduler.\",\n\t)\n\tparser.add_argument(\n", "    \"--num_cycles\",\n\t    type=int,\n\t    default=1,\n\t    help=\"Number of cycles for the lr scheduler.\",\n\t)\n\tparser.add_argument(\n\t    \"--resume_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Path to the last saved checkpoint. 'results/vae.steps.pt'\",\n", ")\n\tparser.add_argument(\n\t    \"--weight_decay\",\n\t    type=float,\n\t    default=0.0,\n\t    help=\"Optimizer weight_decay to use. Default: 0.0\",\n\t)\n\tparser.add_argument(\n\t    \"--taming_model_path\",\n\t    type=str,\n", "    default=None,\n\t    help=\"path to your trained VQGAN weights. This should be a .ckpt file. (only valid when taming option is enabled)\",\n\t)\n\tparser.add_argument(\n\t    \"--taming_config_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"path to your trained VQGAN config. This should be a .yaml file. (only valid when taming option is enabled)\",\n\t)\n\tparser.add_argument(\n", "    \"--optimizer\",\n\t    type=str,\n\t    default=\"Lion\",\n\t    help=\"Optimizer to use. Choose between: ['Adam', 'AdamW','Lion']. Default: Lion\",\n\t)\n\tparser.add_argument(\n\t    \"--cache_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"The path to cache huggingface models\",\n", ")\n\tparser.add_argument(\n\t    \"--no_cache\",\n\t    action=\"store_true\",\n\t    help=\"Do not save the dataset pyarrow cache/files to disk to save disk space and reduce the time it takes to launch the training.\",\n\t)\n\tparser.add_argument(\n\t    \"--latest_checkpoint\",\n\t    action=\"store_true\",\n\t    help=\"Whether to use the latest checkpoint\",\n", ")\n\tparser.add_argument(\n\t    \"--do_not_save_config\",\n\t    action=\"store_true\",\n\t    default=False,\n\t    help=\"Generate example YAML configuration file\",\n\t)\n\tparser.add_argument(\n\t    \"--use_l2_recon_loss\",\n\t    action=\"store_true\",\n", "    help=\"Use F.mse_loss instead of F.l1_loss.\",\n\t)\n\t@dataclass\n\tclass Arguments:\n\t    total_params: Optional[int] = None\n\t    only_save_last_checkpoint: bool = False\n\t    validation_image_scale: float = 1.0\n\t    no_center_crop: bool = False\n\t    no_flip: bool = False\n\t    random_crop: bool = False\n", "    dataset_save_path: Optional[str] = None\n\t    clear_previous_experiments: bool = False\n\t    max_grad_norm: Optional[float] = None\n\t    discr_max_grad_norm: Optional[float] = None\n\t    num_tokens: int = 256\n\t    seq_len: int = 1024\n\t    seed: int = 42\n\t    valid_frac: float = 0.05\n\t    use_ema: bool = False\n\t    ema_beta: float = 0.995\n", "    ema_update_after_step: int = 1\n\t    ema_update_every: int = 1\n\t    apply_grad_penalty_every: int = 4\n\t    image_column: str = \"image\"\n\t    caption_column: str = \"caption\"\n\t    log_with: str = \"wandb\"\n\t    mixed_precision: str = \"no\"\n\t    use_8bit_adam: bool = False\n\t    results_dir: str = \"results\"\n\t    logging_dir: Optional[str] = None\n", "    resume_path: Optional[str] = None\n\t    dataset_name: Optional[str] = None\n\t    streaming: bool = False\n\t    train_data_dir: Optional[str] = None\n\t    num_train_steps: int = -1\n\t    num_epochs: int = 5\n\t    dim: int = 128\n\t    batch_size: int = 512\n\t    lr: float = 1e-5\n\t    gradient_accumulation_steps: int = 1\n", "    save_results_every: int = 100\n\t    save_model_every: int = 500\n\t    checkpoint_limit: Union[int, str] = None\n\t    vq_codebook_size: int = 256\n\t    vq_codebook_dim: int = 256\n\t    cond_drop_prob: float = 0.5\n\t    image_size: int = 256\n\t    lr_scheduler: str = \"constant\"\n\t    scheduler_power: float = 1.0\n\t    lr_warmup_steps: int = 0\n", "    num_cycles: int = 1\n\t    taming_model_path: Optional[str] = None\n\t    taming_config_path: Optional[str] = None\n\t    optimizer: str = \"Lion\"\n\t    weight_decay: float = 0.0\n\t    cache_path: Optional[str] = None\n\t    no_cache: bool = False\n\t    latest_checkpoint: bool = False\n\t    do_not_save_config: bool = False\n\t    use_l2_recon_loss: bool = False\n", "    debug: bool = False\n\t    config_path: Optional[str] = None\n\tdef preprocess_webdataset(args, image):\n\t    return {args.image_column: image}\n\tdef main():\n\t    args = parser.parse_args(namespace=Arguments())\n\t    if args.config_path:\n\t        print(\"Using config file and ignoring CLI args\")\n\t        try:\n\t            conf = OmegaConf.load(args.config_path)\n", "            conf_keys = conf.keys()\n\t            args_to_convert = vars(args)\n\t            for key in conf_keys:\n\t                try:\n\t                    args_to_convert[key] = conf[key]\n\t                except KeyError:\n\t                    print(f\"Error parsing config - {key}: {conf[key]} | Using default or parsed\")\n\t        except FileNotFoundError:\n\t            print(\"Could not find config, using default and parsed values...\")\n\t    project_config = ProjectConfiguration(\n", "        project_dir=args.logging_dir if args.logging_dir else os.path.join(args.results_dir, \"logs\"),\n\t        total_limit=args.checkpoint_limit,\n\t        automatic_checkpoint_naming=True,\n\t    )\n\t    accelerator = get_accelerator(\n\t        log_with=args.log_with,\n\t        gradient_accumulation_steps=args.gradient_accumulation_steps,\n\t        mixed_precision=args.mixed_precision,\n\t        project_config=project_config,\n\t        even_batches=True,\n", "    )\n\t    if accelerator.is_main_process:\n\t        accelerator.init_trackers(\n\t            args.project_name,\n\t            config=vars(args),\n\t            init_kwargs={\n\t                \"wandb\": {\n\t                    \"entity\": f\"{args.wandb_user or wandb.api.default_entity}\",\n\t                    \"name\": args.run_name,\n\t                },\n", "            },\n\t        )\n\t    if args.webdataset is not None:\n\t        import webdataset as wds\n\t        dataset = wds.WebDataset(args.webdataset).shuffle(1000).decode(\"rgb\").to_tuple(\"png\")\n\t        dataset = dataset.map(lambda image: preprocess_webdataset(args, image))\n\t    elif args.train_data_dir:\n\t        dataset = get_dataset_from_dataroot(\n\t            args.train_data_dir,\n\t            image_column=args.image_column,\n", "            caption_column=args.caption_column,\n\t            save_path=args.dataset_save_path,\n\t            save=not args.no_cache,\n\t        )\n\t    elif args.dataset_name:\n\t        if args.cache_path:\n\t            dataset = load_dataset(args.dataset_name, streaming=args.streaming, cache_dir=args.cache_path)[\n\t                \"train\"\n\t            ]\n\t        else:\n", "            dataset = load_dataset(args.dataset_name, streaming=args.streaming, cache_dir=args.cache_path)[\n\t                \"train\"\n\t            ]\n\t        if args.streaming:\n\t            if dataset.info.dataset_size is None:\n\t                print(\"Dataset doesn't support streaming, disabling streaming\")\n\t                args.streaming = False\n\t                if args.cache_path:\n\t                    dataset = load_dataset(args.dataset_name, cache_dir=args.cache_path)[args.hf_split_name]\n\t                else:\n", "                    dataset = load_dataset(args.dataset_name)[args.hf_split_name]\n\t    if args.resume_path is not None and len(args.resume_path) > 1:\n\t        load = True\n\t        accelerator.print(f\"Using Muse VQGanVAE, loading from {args.resume_path}\")\n\t        vae = VQGanVAE(\n\t            dim=args.dim,\n\t            vq_codebook_dim=args.vq_codebook_dim,\n\t            vq_codebook_size=args.vq_codebook_size,\n\t            l2_recon_loss=args.use_l2_recon_loss,\n\t            channels=args.channels,\n", "            layers=args.layers,\n\t            discr_layers=args.discr_layers,\n\t            accelerator=accelerator,\n\t        )\n\t        if args.latest_checkpoint:\n\t            accelerator.print(\"Finding latest checkpoint...\")\n\t            orig_vae_path = args.resume_path\n\t            if os.path.isfile(args.resume_path) or \".pt\" in args.resume_path:\n\t                # If args.vae_path is a file, split it into directory and filename\n\t                args.resume_path, _ = os.path.split(args.resume_path)\n", "            checkpoint_files = glob.glob(os.path.join(args.resume_path, \"vae.*.pt\"))\n\t            if checkpoint_files:\n\t                latest_checkpoint_file = max(\n\t                    checkpoint_files, key=lambda x: int(re.search(r\"vae\\.(\\d+)\\.pt\", x).group(1))\n\t                )\n\t                # Check if latest checkpoint is empty or unreadable\n\t                if os.path.getsize(latest_checkpoint_file) == 0 or not os.access(\n\t                    latest_checkpoint_file, os.R_OK\n\t                ):\n\t                    accelerator.print(\n", "                        f\"Warning: latest checkpoint {latest_checkpoint_file} is empty or unreadable.\"\n\t                    )\n\t                    if len(checkpoint_files) > 1:\n\t                        # Use the second last checkpoint as a fallback\n\t                        latest_checkpoint_file = max(\n\t                            checkpoint_files[:-1], key=lambda x: int(re.search(r\"vae\\.(\\d+)\\.pt\", x).group(1))\n\t                        )\n\t                        accelerator.print(\"Using second last checkpoint: \", latest_checkpoint_file)\n\t                    else:\n\t                        accelerator.print(\"No usable checkpoint found.\")\n", "                        load = False\n\t                elif latest_checkpoint_file != orig_vae_path:\n\t                    accelerator.print(\"Resuming VAE from latest checkpoint: \", latest_checkpoint_file)\n\t                else:\n\t                    accelerator.print(\"Using checkpoint specified in vae_path: \", orig_vae_path)\n\t                args.resume_path = latest_checkpoint_file\n\t            else:\n\t                accelerator.print(\"No checkpoints found in directory: \", args.resume_path)\n\t                load = False\n\t        else:\n", "            accelerator.print(\"Resuming VAE from: \", args.resume_path)\n\t        if load:\n\t            vae.load(args.resume_path, map=\"cpu\")\n\t            resume_from_parts = args.resume_path.split(\".\")\n\t            for i in range(len(resume_from_parts) - 1, -1, -1):\n\t                if resume_from_parts[i].isdigit():\n\t                    current_step = int(resume_from_parts[i])\n\t                    accelerator.print(f\"Found step {current_step} for the VAE model.\")\n\t                    break\n\t            if current_step == 0:\n", "                accelerator.print(\"No step found for the VAE model.\")\n\t        else:\n\t            accelerator.print(\"No step found for the VAE model.\")\n\t            current_step = 0\n\t    elif args.taming_model_path is not None and args.taming_config_path is not None:\n\t        print(f\"Using Taming VQGanVAE, loading from {args.taming_model_path}\")\n\t        vae = VQGanVAETaming(\n\t            vqgan_model_path=args.taming_model_path,\n\t            vqgan_config_path=args.taming_config_path,\n\t            accelerator=accelerator,\n", "        )\n\t        args.num_tokens = vae.codebook_size\n\t        args.seq_len = vae.get_encoded_fmap_size(args.image_size) ** 2\n\t    else:\n\t        print(\"Initialising empty VAE\")\n\t        vae = VQGanVAE(\n\t            dim=args.dim,\n\t            vq_codebook_dim=args.vq_codebook_dim,\n\t            vq_codebook_size=args.vq_codebook_size,\n\t            channels=args.channels,\n", "            layers=args.layers,\n\t            discr_layers=args.discr_layers,\n\t            accelerator=accelerator,\n\t        )\n\t        current_step = 0\n\t    # Use the parameters() method to get an iterator over all the learnable parameters of the model\n\t    total_params = sum(p.numel() for p in vae.parameters())\n\t    args.total_params = total_params\n\t    print(f\"Total number of parameters: {format(total_params, ',d')}\")\n\t    dataset = ImageDataset(\n", "        dataset,\n\t        args.image_size,\n\t        image_column=args.image_column,\n\t        center_crop=not args.no_center_crop,\n\t        flip=not args.no_flip,\n\t        stream=args.streaming,\n\t        random_crop=args.random_crop,\n\t        alpha_channel=False if args.channels == 3 else True,\n\t    )\n\t    # dataloader\n", "    dataloader, validation_dataloader = split_dataset_into_dataloaders(\n\t        dataset, args.valid_frac, args.seed, args.batch_size\n\t    )\n\t    trainer = VQGanVAETrainer(\n\t        vae,\n\t        dataloader,\n\t        validation_dataloader,\n\t        accelerator,\n\t        current_step=current_step + 1 if current_step != 0 else current_step,\n\t        num_train_steps=args.num_train_steps,\n", "        lr=args.lr,\n\t        lr_scheduler_type=args.lr_scheduler,\n\t        lr_warmup_steps=args.lr_warmup_steps,\n\t        max_grad_norm=args.max_grad_norm,\n\t        discr_max_grad_norm=args.discr_max_grad_norm,\n\t        save_results_every=args.save_results_every,\n\t        save_model_every=args.save_model_every,\n\t        results_dir=args.results_dir,\n\t        logging_dir=args.logging_dir if args.logging_dir else os.path.join(args.results_dir, \"logs\"),\n\t        use_ema=args.use_ema,\n", "        ema_beta=args.ema_beta,\n\t        ema_update_after_step=args.ema_update_after_step,\n\t        ema_update_every=args.ema_update_every,\n\t        apply_grad_penalty_every=args.apply_grad_penalty_every,\n\t        gradient_accumulation_steps=args.gradient_accumulation_steps,\n\t        clear_previous_experiments=args.clear_previous_experiments,\n\t        validation_image_scale=args.validation_image_scale,\n\t        only_save_last_checkpoint=args.only_save_last_checkpoint,\n\t        optimizer=args.optimizer,\n\t        use_8bit_adam=args.use_8bit_adam,\n", "        num_cycles=args.num_cycles,\n\t        scheduler_power=args.scheduler_power,\n\t        num_epochs=args.num_epochs,\n\t        args=args,\n\t    )\n\t    trainer.train()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "infer_vae.py", "chunked_list": ["import argparse\n\timport glob\n\timport hashlib\n\timport os\n\timport random\n\timport re\n\tfrom dataclasses import dataclass\n\tfrom datetime import datetime\n\tfrom typing import Optional\n\timport accelerate\n", "import PIL\n\timport torch\n\tfrom accelerate.utils import ProjectConfiguration\n\tfrom datasets import Dataset, Image, load_dataset\n\tfrom torchvision.utils import save_image\n\tfrom tqdm import tqdm\n\tfrom muse_maskgit_pytorch import (\n\t    VQGanVAE,\n\t    VQGanVAETaming,\n\t    get_accelerator,\n", ")\n\tfrom muse_maskgit_pytorch.dataset import (\n\t    ImageDataset,\n\t    get_dataset_from_dataroot,\n\t)\n\tfrom muse_maskgit_pytorch.vqvae import VQVAE\n\t# Create the parser\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--no_center_crop\",\n", "    action=\"store_true\",\n\t    help=\"Don't do center crop.\",\n\t)\n\tparser.add_argument(\n\t    \"--random_crop\",\n\t    action=\"store_true\",\n\t    help=\"Crop the images at random locations instead of cropping from the center.\",\n\t)\n\tparser.add_argument(\n\t    \"--no_flip\",\n", "    action=\"store_true\",\n\t    help=\"Don't flip image.\",\n\t)\n\tparser.add_argument(\n\t    \"--random_image\",\n\t    action=\"store_true\",\n\t    help=\"Get a random image from the dataset to use for the reconstruction.\",\n\t)\n\tparser.add_argument(\n\t    \"--dataset_save_path\",\n", "    type=str,\n\t    default=\"dataset\",\n\t    help=\"Path to save the dataset if you are making one from a directory\",\n\t)\n\tparser.add_argument(\n\t    \"--seed\",\n\t    type=int,\n\t    default=42,\n\t    help=\"Seed for reproducibility. If set to -1 a random seed will be generated.\",\n\t)\n", "parser.add_argument(\"--valid_frac\", type=float, default=0.05, help=\"validation fraction.\")\n\tparser.add_argument(\n\t    \"--image_column\",\n\t    type=str,\n\t    default=\"image\",\n\t    help=\"The column of the dataset containing an image.\",\n\t)\n\tparser.add_argument(\n\t    \"--mixed_precision\",\n\t    type=str,\n", "    default=\"no\",\n\t    choices=[\"no\", \"fp16\", \"bf16\"],\n\t    help=\"Precision to train on.\",\n\t)\n\tparser.add_argument(\n\t    \"--results_dir\",\n\t    type=str,\n\t    default=\"results\",\n\t    help=\"Path to save the training samples and checkpoints\",\n\t)\n", "parser.add_argument(\n\t    \"--logging_dir\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Path to log the losses and LR\",\n\t)\n\t# vae_trainer args\n\tparser.add_argument(\n\t    \"--vae_path\",\n\t    type=str,\n", "    default=None,\n\t    help=\"Path to the vae model. eg. 'results/vae.steps.pt'\",\n\t)\n\tparser.add_argument(\n\t    \"--dataset_name\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Name of the huggingface dataset used.\",\n\t)\n\tparser.add_argument(\n", "    \"--train_data_dir\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Dataset folder where your input images for training are.\",\n\t)\n\tparser.add_argument(\"--dim\", type=int, default=128, help=\"Model dimension.\")\n\tparser.add_argument(\"--batch_size\", type=int, default=512, help=\"Batch Size.\")\n\tparser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning Rate.\")\n\tparser.add_argument(\"--vq_codebook_size\", type=int, default=256, help=\"Image Size.\")\n\tparser.add_argument(\"--vq_codebook_dim\", type=int, default=256, help=\"VQ Codebook dimensions.\")\n", "parser.add_argument(\n\t    \"--channels\", type=int, default=3, help=\"Number of channels for the VAE. Use 3 for RGB or 4 for RGBA.\"\n\t)\n\tparser.add_argument(\"--layers\", type=int, default=4, help=\"Number of layers for the VAE.\")\n\tparser.add_argument(\"--discr_layers\", type=int, default=4, help=\"Number of layers for the VAE discriminator.\")\n\tparser.add_argument(\n\t    \"--image_size\",\n\t    type=int,\n\t    default=256,\n\t    help=\"Image size. You may want to start with small images, and then curriculum learn to larger ones, but because the vae is all convolution, it should generalize to 512 (as in paper) without training on it\",\n", ")\n\tparser.add_argument(\n\t    \"--chunk_size\",\n\t    type=int,\n\t    default=256,\n\t    help=\"This is used to split big images into smaller chunks so we can still reconstruct them no matter the size.\",\n\t)\n\tparser.add_argument(\n\t    \"--min_chunk_size\",\n\t    type=int,\n", "    default=8,\n\t    help=\"We use a minimum chunk size to ensure that the image is always reconstructed correctly.\",\n\t)\n\tparser.add_argument(\n\t    \"--overlap_size\",\n\t    type=int,\n\t    default=256,\n\t    help=\"The overlap size used with --chunk_size to overlap the chunks and make sure the whole image is reconstructe as well as make sure we remove artifacts caused by doing the reconstrucion in chunks.\",\n\t)\n\tparser.add_argument(\n", "    \"--min_overlap_size\",\n\t    type=int,\n\t    default=1,\n\t    help=\"We use a minimum overlap size to ensure that the image is always reconstructed correctly.\",\n\t)\n\tparser.add_argument(\n\t    \"--taming_model_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"path to your trained VQGAN weights. This should be a .ckpt file. (only valid when taming option is enabled)\",\n", ")\n\tparser.add_argument(\n\t    \"--taming_config_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"path to your trained VQGAN config. This should be a .yaml file. (only valid when taming option is enabled)\",\n\t)\n\tparser.add_argument(\n\t    \"--input_image\",\n\t    type=str,\n", "    default=None,\n\t    help=\"Path to an image to use as input for reconstruction instead of using one from the dataset.\",\n\t)\n\tparser.add_argument(\n\t    \"--input_folder\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Path to a folder with images to use as input for creating a dataset for reconstructing all the imgaes in it instead of just one image.\",\n\t)\n\tparser.add_argument(\n", "    \"--exclude_folders\",\n\t    type=str,\n\t    default=None,\n\t    help=\"List of folders we want to exclude when doing reconstructions from an input folder.\",\n\t)\n\tparser.add_argument(\n\t    \"--gpu\",\n\t    type=int,\n\t    default=0,\n\t    help=\"GPU to use in case we want to use a specific GPU for inference.\",\n", ")\n\tparser.add_argument(\n\t    \"--max_retries\",\n\t    type=int,\n\t    default=30,\n\t    help=\"Max number of times to retry in case the reconstruction fails due to OOM or any other error.\",\n\t)\n\tparser.add_argument(\n\t    \"--latest_checkpoint\",\n\t    action=\"store_true\",\n", "    help=\"Use the latest checkpoint using the vae_path folder instead of using just a specific vae_path.\",\n\t)\n\tparser.add_argument(\n\t    \"--use_paintmind\",\n\t    action=\"store_true\",\n\t    help=\"Use PaintMind VAE..\",\n\t)\n\t@dataclass\n\tclass Arguments:\n\t    only_save_last_checkpoint: bool = False\n", "    validation_image_scale: float = 1.0\n\t    no_center_crop: bool = False\n\t    no_flip: bool = False\n\t    random_crop: bool = False\n\t    random_image: bool = False\n\t    dataset_save_path: Optional[str] = None\n\t    clear_previous_experiments: bool = False\n\t    max_grad_norm: Optional[float] = None\n\t    discr_max_grad_norm: Optional[float] = None\n\t    num_tokens: int = 256\n", "    seq_len: int = 1024\n\t    seed: int = 42\n\t    valid_frac: float = 0.05\n\t    use_ema: bool = False\n\t    ema_beta: float = 0.995\n\t    ema_update_after_step: int = 1\n\t    ema_update_every: int = 1\n\t    apply_grad_penalty_every: int = 4\n\t    image_column: str = \"image\"\n\t    caption_column: str = \"caption\"\n", "    log_with: str = \"wandb\"\n\t    mixed_precision: str = \"no\"\n\t    use_8bit_adam: bool = False\n\t    results_dir: str = \"results\"\n\t    logging_dir: Optional[str] = None\n\t    resume_path: Optional[str] = None\n\t    dataset_name: Optional[str] = None\n\t    streaming: bool = False\n\t    train_data_dir: Optional[str] = None\n\t    num_train_steps: int = -1\n", "    num_epochs: int = 5\n\t    dim: int = 128\n\t    batch_size: int = 512\n\t    lr: float = 1e-5\n\t    gradient_accumulation_steps: int = 1\n\t    save_results_every: int = 100\n\t    save_model_every: int = 500\n\t    vq_codebook_size: int = 256\n\t    vq_codebook_dim: int = 256\n\t    cond_drop_prob: float = 0.5\n", "    image_size: int = 256\n\t    lr_scheduler: str = \"constant\"\n\t    scheduler_power: float = 1.0\n\t    lr_warmup_steps: int = 0\n\t    num_cycles: int = 1\n\t    taming_model_path: Optional[str] = None\n\t    taming_config_path: Optional[str] = None\n\t    optimizer: str = \"Lion\"\n\t    weight_decay: float = 0.0\n\t    cache_path: Optional[str] = None\n", "    no_cache: bool = False\n\t    latest_checkpoint: bool = False\n\t    do_not_save_config: bool = False\n\t    use_l2_recon_loss: bool = False\n\t    debug: bool = False\n\t    config_path: Optional[str] = None\n\t    generate_config: bool = False\n\tdef seed_to_int(s):\n\t    if type(s) is int:\n\t        return s\n", "    if s is None or s == \"\":\n\t        return random.randint(0, 2**32 - 1)\n\t    if \",\" in s:\n\t        s = s.split(\",\")\n\t    if type(s) is list:\n\t        seed_list = []\n\t        for seed in s:\n\t            if seed is None or seed == \"\":\n\t                seed_list.append(random.randint(0, 2**32 - 1))\n\t            else:\n", "                seed_list = s\n\t        return seed_list\n\t    n = abs(int(s) if s.isdigit() else random.Random(s).randint(0, 2**32 - 1))\n\t    while n >= 2**32:\n\t        n = n >> 32\n\t    return n\n\tdef main():\n\t    args = parser.parse_args(namespace=Arguments())\n\t    project_config = ProjectConfiguration(\n\t        project_dir=args.logging_dir if args.logging_dir else os.path.join(args.results_dir, \"logs\"),\n", "        automatic_checkpoint_naming=True,\n\t    )\n\t    accelerator: accelerate.Accelerator = get_accelerator(\n\t        log_with=args.log_with,\n\t        gradient_accumulation_steps=args.gradient_accumulation_steps,\n\t        mixed_precision=args.mixed_precision,\n\t        project_config=project_config,\n\t        even_batches=True,\n\t    )\n\t    # set pytorch seed for reproducibility\n", "    torch.manual_seed(seed_to_int(args.seed))\n\t    if args.train_data_dir and not args.input_image and not args.input_folder:\n\t        dataset = get_dataset_from_dataroot(\n\t            args.train_data_dir,\n\t            image_column=args.image_column,\n\t            save_path=args.dataset_save_path,\n\t        )\n\t    elif args.dataset_name and not args.input_image and not args.input_folder:\n\t        dataset = load_dataset(args.dataset_name)[\"train\"]\n\t    elif args.input_image and not args.input_folder:\n", "        # Create dataset from single input image\n\t        dataset = Dataset.from_dict({\"image\": [args.input_image]}).cast_column(\"image\", Image())\n\t    if args.input_folder:\n\t        # Create dataset from input folder\n\t        extensions = [\"jpg\", \"jpeg\", \"png\", \"webp\"]\n\t        exclude_folders = args.exclude_folders.split(\",\") if args.exclude_folders else []\n\t        filepaths = []\n\t        for root, dirs, files in os.walk(args.input_folder, followlinks=True):\n\t            # Resolve symbolic link to actual path and exclude based on actual path\n\t            resolved_root = os.path.realpath(root)\n", "            for exclude_folder in exclude_folders:\n\t                if exclude_folder in resolved_root:\n\t                    dirs[:] = []\n\t                    break\n\t            for file in files:\n\t                if file.lower().endswith(tuple(extensions)):\n\t                    filepaths.append(os.path.join(root, file))\n\t        if not filepaths:\n\t            print(f\"No images with extensions {extensions} found in {args.input_folder}.\")\n\t            exit(1)\n", "        dataset = Dataset.from_dict({\"image\": filepaths}).cast_column(\"image\", Image())\n\t    if args.vae_path and args.taming_model_path:\n\t        raise Exception(\"You can't pass vae_path and taming args at the same time.\")\n\t    if args.vae_path and not args.use_paintmind:\n\t        accelerator.print(\"Loading Muse VQGanVAE\")\n\t        vae = VQGanVAE(\n\t            dim=args.dim,\n\t            vq_codebook_size=args.vq_codebook_size,\n\t            vq_codebook_dim=args.vq_codebook_dim,\n\t            channels=args.channels,\n", "            layers=args.layers,\n\t            discr_layers=args.discr_layers,\n\t        ).to(accelerator.device if args.gpu == 0 else f\"cuda:{args.gpu}\")\n\t        if args.latest_checkpoint:\n\t            accelerator.print(\"Finding latest checkpoint...\")\n\t            orig_vae_path = args.vae_path\n\t            if os.path.isfile(args.vae_path) or \".pt\" in args.vae_path:\n\t                # If args.vae_path is a file, split it into directory and filename\n\t                args.vae_path, _ = os.path.split(args.vae_path)\n\t            checkpoint_files = glob.glob(os.path.join(args.vae_path, \"vae.*.pt\"))\n", "            if checkpoint_files:\n\t                latest_checkpoint_file = max(\n\t                    checkpoint_files,\n\t                    key=lambda x: int(re.search(r\"vae\\.(\\d+)\\.pt$\", x).group(1))\n\t                    if not x.endswith(\"ema.pt\")\n\t                    else -1,\n\t                )\n\t                # Check if latest checkpoint is empty or unreadable\n\t                if os.path.getsize(latest_checkpoint_file) == 0 or not os.access(\n\t                    latest_checkpoint_file, os.R_OK\n", "                ):\n\t                    accelerator.print(\n\t                        f\"Warning: latest checkpoint {latest_checkpoint_file} is empty or unreadable.\"\n\t                    )\n\t                    if len(checkpoint_files) > 1:\n\t                        # Use the second last checkpoint as a fallback\n\t                        latest_checkpoint_file = max(\n\t                            checkpoint_files[:-1],\n\t                            key=lambda x: int(re.search(r\"vae\\.(\\d+)\\.pt$\", x).group(1))\n\t                            if not x.endswith(\"ema.pt\")\n", "                            else -1,\n\t                        )\n\t                        accelerator.print(\"Using second last checkpoint: \", latest_checkpoint_file)\n\t                    else:\n\t                        accelerator.print(\"No usable checkpoint found.\")\n\t                elif latest_checkpoint_file != orig_vae_path:\n\t                    accelerator.print(\"Resuming VAE from latest checkpoint: \", latest_checkpoint_file)\n\t                else:\n\t                    accelerator.print(\"Using checkpoint specified in vae_path: \", orig_vae_path)\n\t                args.vae_path = latest_checkpoint_file\n", "            else:\n\t                accelerator.print(\"No checkpoints found in directory: \", args.vae_path)\n\t        else:\n\t            accelerator.print(\"Resuming VAE from: \", args.vae_path)\n\t        vae.load(args.vae_path)\n\t    if args.use_paintmind:\n\t        # load VAE\n\t        accelerator.print(\"Loading VQVAE from 'neggles/vaedump/vit-s-vqgan-f4' ...\")\n\t        vae: VQVAE = VQVAE.from_pretrained(\"neggles/vaedump\", subfolder=\"vit-s-vqgan-f4\")\n\t    elif args.taming_model_path:\n", "        print(\"Loading Taming VQGanVAE\")\n\t        vae = VQGanVAETaming(\n\t            vqgan_model_path=args.taming_model_path,\n\t            vqgan_config_path=args.taming_config_path,\n\t        )\n\t        args.num_tokens = vae.codebook_size\n\t        args.seq_len = vae.get_encoded_fmap_size(args.image_size) ** 2\n\t    # move vae to device\n\t    vae = vae.to(accelerator.device if args.gpu == 0 else f\"cuda:{args.gpu}\")\n\t    # Use the parameters() method to get an iterator over all the learnable parameters of the model\n", "    total_params = sum(p.numel() for p in vae.parameters())\n\t    print(f\"Total number of parameters: {format(total_params, ',d')}\")\n\t    # then you plug the vae and transformer into your MaskGit as so\n\t    dataset = ImageDataset(\n\t        dataset,\n\t        args.image_size,\n\t        image_column=args.image_column,\n\t        center_crop=True if not args.no_center_crop and not args.random_crop else False,\n\t        flip=not args.no_flip,\n\t        random_crop=args.random_crop if args.random_crop else False,\n", "        alpha_channel=False if args.channels == 3 else True,\n\t    )\n\t    if args.input_image and not args.input_folder:\n\t        image_id = 0 if not args.random_image else random.randint(0, len(dataset))\n\t        os.makedirs(f\"{args.results_dir}/outputs\", exist_ok=True)\n\t        save_image(\n\t            dataset[image_id],\n\t            f\"{args.results_dir}/outputs/input.{str(args.input_image).split('.')[-1]}\",\n\t            format=\"PNG\",\n\t        )\n", "        _, ids, _ = vae.encode(\n\t            dataset[image_id][None].to(accelerator.device if args.gpu == 0 else f\"cuda:{args.gpu}\")\n\t        )\n\t        recon = vae.decode_from_ids(ids)\n\t        save_image(recon, f\"{args.results_dir}/outputs/output.{str(args.input_image).split('.')[-1]}\")\n\t    if not args.input_image and not args.input_folder:\n\t        image_id = 0 if not args.random_image else random.randint(0, len(dataset))\n\t        os.makedirs(f\"{args.results_dir}/outputs\", exist_ok=True)\n\t        save_image(dataset[image_id], f\"{args.results_dir}/outputs/input.png\")\n\t        _, ids, _ = vae.encode(\n", "            dataset[image_id][None].to(accelerator.device if args.gpu == 0 else f\"cuda:{args.gpu}\")\n\t        )\n\t        recon = vae.decode_from_ids(ids)\n\t        save_image(recon, f\"{args.results_dir}/outputs/output.png\")\n\t    if args.input_folder:\n\t        # Create output directory and save input images and reconstructions as grids\n\t        output_dir = os.path.join(args.results_dir, \"outputs\", os.path.basename(args.input_folder))\n\t        os.makedirs(output_dir, exist_ok=True)\n\t        for i in tqdm(range(len(dataset))):\n\t            retries = 0\n", "            while True:\n\t                try:\n\t                    save_image(dataset[i], f\"{output_dir}/input.png\")\n\t                    if not args.use_paintmind:\n\t                        # encode\n\t                        _, ids, _ = vae.encode(\n\t                            dataset[i][None].to(accelerator.device if args.gpu == 0 else f\"cuda:{args.gpu}\")\n\t                        )\n\t                        # decode\n\t                        recon = vae.decode_from_ids(ids)\n", "                        # print (recon.shape) # torch.Size([1, 3, 512, 1136])\n\t                        save_image(recon, f\"{output_dir}/output.png\")\n\t                    else:\n\t                        # encode\n\t                        encoded, _, _ = vae.encode(\n\t                            dataset[i][None].to(accelerator.device if args.gpu == 0 else f\"cuda:{args.gpu}\")\n\t                        )\n\t                        # decode\n\t                        recon = vae.decode(encoded).squeeze(0)\n\t                        recon = torch.clamp(recon, -1.0, 1.0)\n", "                        save_image(recon, f\"{output_dir}/output.png\")\n\t                    # Load input and output images\n\t                    input_image = PIL.Image.open(f\"{output_dir}/input.png\")\n\t                    output_image = PIL.Image.open(f\"{output_dir}/output.png\")\n\t                    # Create horizontal grid with input and output images\n\t                    grid_image = PIL.Image.new(\n\t                        \"RGB\" if args.channels == 3 else \"RGBA\",\n\t                        (input_image.width + output_image.width, input_image.height),\n\t                    )\n\t                    grid_image.paste(input_image, (0, 0))\n", "                    grid_image.paste(output_image, (input_image.width, 0))\n\t                    # Save grid\n\t                    now = datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")\n\t                    hash = hashlib.sha1(input_image.tobytes()).hexdigest()\n\t                    filename = f\"{hash}_{now}-{os.path.basename(args.vae_path)}.png\"\n\t                    grid_image.save(f\"{output_dir}/{filename}\", format=\"PNG\")\n\t                    # Remove input and output images after the grid was made.\n\t                    os.remove(f\"{output_dir}/input.png\")\n\t                    os.remove(f\"{output_dir}/output.png\")\n\t                    del _\n", "                    del ids\n\t                    del recon\n\t                    torch.cuda.empty_cache()\n\t                    torch.cuda.ipc_collect()\n\t                    break  # Exit the retry loop if there were no errors\n\t                except RuntimeError as e:\n\t                    if \"out of memory\" in str(e) and retries < args.max_retries:\n\t                        retries += 1\n\t                        # print(f\"Out of Memory. Retry #{retries}\")\n\t                        torch.cuda.empty_cache()\n", "                        torch.cuda.ipc_collect()\n\t                        continue  # Retry the loop\n\t                    else:\n\t                        if \"out of memory\" not in str(e):\n\t                            print(e)\n\t                        else:\n\t                            print(f\"Skipping image {i} after {retries} retries due to out of memory error\")\n\t                        break  # Exit the retry loop after too many retries\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "train_muse_maskgit.py", "chunked_list": ["import argparse\n\timport glob\n\timport logging\n\timport os\n\timport re\n\tfrom dataclasses import dataclass\n\tfrom typing import Optional, Union\n\timport accelerate\n\timport datasets\n\timport diffusers\n", "import transformers\n\timport wandb\n\tfrom accelerate.utils import ProjectConfiguration\n\tfrom datasets import load_dataset\n\tfrom diffusers.optimization import SchedulerType, get_scheduler\n\tfrom omegaconf import OmegaConf\n\tfrom rich import inspect\n\tfrom torch.optim import Optimizer\n\ttry:\n\t    import torch_xla\n", "    import torch_xla.core.xla_model as xm\n\texcept ImportError:\n\t    print(\"TPU support has been disabled, please install torch_xla and train on an XLA device\")\n\t    torch_xla = None\n\t    xm = None\n\tfrom muse_maskgit_pytorch import (\n\t    MaskGit,\n\t    MaskGitTrainer,\n\t    MaskGitTransformer,\n\t    VQGanVAE,\n", "    VQGanVAETaming,\n\t    get_accelerator,\n\t)\n\tfrom muse_maskgit_pytorch.dataset import (\n\t    ImageTextDataset,\n\t    LocalTextImageDataset,\n\t    URLTextDataset,\n\t    get_dataset_from_dataroot,\n\t    split_dataset_into_dataloaders,\n\t)\n", "from muse_maskgit_pytorch.trainers.base_accelerated_trainer import get_optimizer\n\t# remove some unnecessary errors from transformer shown on the console.\n\ttransformers.logging.set_verbosity_error()\n\t# disable bitsandbytes welcome message.\n\tos.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n\t# Create the parser\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\n\t    \"--only_save_last_checkpoint\",\n\t    action=\"store_true\",\n", "    help=\"Only save last checkpoint.\",\n\t)\n\tparser.add_argument(\n\t    \"--validation_image_scale\",\n\t    default=1.0,\n\t    type=float,\n\t    help=\"Factor by which to scale the validation images.\",\n\t)\n\tparser.add_argument(\n\t    \"--no_center_crop\",\n", "    action=\"store_true\",\n\t    help=\"Don't do center crop.\",\n\t)\n\tparser.add_argument(\n\t    \"--random_crop\",\n\t    action=\"store_true\",\n\t    help=\"Crop the images at random locations instead of cropping from the center.\",\n\t)\n\tparser.add_argument(\n\t    \"--no_flip\",\n", "    action=\"store_true\",\n\t    help=\"Don't flip image.\",\n\t)\n\tparser.add_argument(\n\t    \"--dataset_save_path\",\n\t    type=str,\n\t    default=\"dataset\",\n\t    help=\"Path to save the dataset if you are making one from a directory\",\n\t)\n\tparser.add_argument(\n", "    \"--clear_previous_experiments\",\n\t    action=\"store_true\",\n\t    help=\"Whether to clear previous experiments.\",\n\t)\n\tparser.add_argument(\n\t    \"--num_tokens\",\n\t    type=int,\n\t    default=256,\n\t    help=\"Number of tokens. Must be same as codebook size above\",\n\t)\n", "parser.add_argument(\n\t    \"--seq_len\",\n\t    type=int,\n\t    default=1024,\n\t    help=\"The sequence length. Must be equivalent to fmap_size ** 2 in vae\",\n\t)\n\tparser.add_argument(\"--depth\", type=int, default=2, help=\"The depth of model\")\n\tparser.add_argument(\"--dim_head\", type=int, default=64, help=\"Attention head dimension\")\n\tparser.add_argument(\"--heads\", type=int, default=8, help=\"Attention heads\")\n\tparser.add_argument(\"--ff_mult\", type=int, default=4, help=\"Feed forward expansion factor\")\n", "parser.add_argument(\"--t5_name\", type=str, default=\"t5-small\", help=\"Name of your t5 model\")\n\tparser.add_argument(\"--cond_image_size\", type=int, default=None, help=\"Conditional image size.\")\n\tparser.add_argument(\n\t    \"--validation_prompt\",\n\t    type=str,\n\t    default=\"A photo of a dog\",\n\t    help=\"Validation prompt(s), pipe | separated\",\n\t)\n\tparser.add_argument(\n\t    \"--timesteps\",\n", "    type=int,\n\t    default=18,\n\t    help=\"Number of steps to use when generating the validation image. Defautl: 18\",\n\t)\n\tparser.add_argument(\"--max_grad_norm\", type=float, default=None, help=\"Max gradient norm.\")\n\tparser.add_argument(\"--seed\", type=int, default=42, help=\"Training seed.\")\n\tparser.add_argument(\n\t    \"--valid_frac\", type=float, default=0.05, help=\"Fraction of dataset to use for validation.\"\n\t)\n\tparser.add_argument(\"--use_ema\", action=\"store_true\", help=\"Whether to use ema.\")\n", "parser.add_argument(\"--ema_beta\", type=float, default=0.995, help=\"Ema beta.\")\n\tparser.add_argument(\"--ema_update_after_step\", type=int, default=1, help=\"Ema update after step.\")\n\tparser.add_argument(\n\t    \"--ema_update_every\",\n\t    type=int,\n\t    default=1,\n\t    help=\"Ema update every this number of steps.\",\n\t)\n\tparser.add_argument(\n\t    \"--apply_grad_penalty_every\",\n", "    type=int,\n\t    default=4,\n\t    help=\"Apply gradient penalty every this number of steps.\",\n\t)\n\tparser.add_argument(\n\t    \"--image_column\",\n\t    type=str,\n\t    default=\"image\",\n\t    help=\"The column of the dataset containing an image.\",\n\t)\n", "parser.add_argument(\n\t    \"--caption_column\",\n\t    type=str,\n\t    default=\"caption\",\n\t    help=\"The column of the dataset containing a caption or a list of captions.\",\n\t)\n\tparser.add_argument(\n\t    \"--log_with\",\n\t    type=str,\n\t    default=\"wandb\",\n", "    help=(\n\t        'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n\t        ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n\t    ),\n\t)\n\tparser.add_argument(\n\t    \"--project_name\",\n\t    type=str,\n\t    default=\"muse_maskgit\",\n\t    help=(\"Name to use for the project to identify it when saved to a tracker such as wandb or tensorboard.\"),\n", ")\n\tparser.add_argument(\n\t    \"--run_name\",\n\t    type=str,\n\t    default=None,\n\t    help=(\n\t        \"Name to use for the run to identify it when saved to a tracker such\"\n\t        \" as wandb or tensorboard. If not specified a random one will be generated.\"\n\t    ),\n\t)\n", "parser.add_argument(\n\t    \"--wandb_user\",\n\t    type=str,\n\t    default=None,\n\t    help=(\n\t        \"Specify the name for the user or the organization in which the project will be saved when using wand.\"\n\t    ),\n\t)\n\tparser.add_argument(\n\t    \"--mixed_precision\",\n", "    type=str,\n\t    default=\"no\",\n\t    choices=[\"no\", \"fp16\", \"bf16\"],\n\t    help=\"Precision to train on.\",\n\t)\n\tparser.add_argument(\n\t    \"--use_8bit_adam\",\n\t    action=\"store_true\",\n\t    help=\"Whether to use the 8bit adam optimiser\",\n\t)\n", "parser.add_argument(\n\t    \"--results_dir\",\n\t    type=str,\n\t    default=\"results\",\n\t    help=\"Path to save the training samples and checkpoints\",\n\t)\n\tparser.add_argument(\n\t    \"--logging_dir\",\n\t    type=str,\n\t    default=None,\n", "    help=\"Path to log the losses and LR\",\n\t)\n\t# vae_trainer args\n\tparser.add_argument(\n\t    \"--vae_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Path to the vae model. eg. 'results/vae.steps.pt'\",\n\t)\n\tparser.add_argument(\n", "    \"--dataset_name\",\n\t    type=str,\n\t    default=None,\n\t    help=\"ID of HuggingFace dataset to use (cannot be used with --train_data_dir)\",\n\t)\n\tparser.add_argument(\n\t    \"--hf_split_name\",\n\t    type=str,\n\t    default=\"train\",\n\t    help=\"Subset or split to use from the dataset when using a dataset form HuggingFace.\",\n", ")\n\tparser.add_argument(\n\t    \"--streaming\",\n\t    action=\"store_true\",\n\t    help=\"If using a HuggingFace dataset, whether to stream it or not.\",\n\t)\n\tparser.add_argument(\n\t    \"--train_data_dir\",\n\t    type=str,\n\t    default=None,\n", "    help=\"Local dataset folder to use (cannot be used with --dataset_name)\",\n\t)\n\tparser.add_argument(\n\t    \"--num_train_steps\",\n\t    type=int,\n\t    default=-1,\n\t    help=\"Total number of steps to train for. eg. 50000. | Use only if you want to stop training early\",\n\t)\n\tparser.add_argument(\n\t    \"--num_epochs\",\n", "    type=int,\n\t    default=5,\n\t    help=\"Total number of epochs to train for. eg. 5.\",\n\t)\n\tparser.add_argument(\n\t    \"--dim\",\n\t    type=int,\n\t    default=128,\n\t    help=\"Model dimension.\",\n\t)\n", "parser.add_argument(\n\t    \"--batch_size\",\n\t    type=int,\n\t    default=512,\n\t    help=\"Batch Size.\",\n\t)\n\tparser.add_argument(\n\t    \"--lr\",\n\t    type=float,\n\t    default=1e-4,\n", "    help=\"Learning Rate.\",\n\t)\n\tparser.add_argument(\n\t    \"--gradient_accumulation_steps\",\n\t    type=int,\n\t    default=1,\n\t    help=\"Gradient Accumulation Steps\",\n\t)\n\tparser.add_argument(\n\t    \"--save_results_every\",\n", "    type=int,\n\t    default=100,\n\t    help=\"Save results every N steps.\",\n\t)\n\tparser.add_argument(\n\t    \"--save_model_every\",\n\t    type=int,\n\t    default=500,\n\t    help=\"Save the model every N steps.\",\n\t)\n", "parser.add_argument(\n\t    \"--checkpoint_limit\",\n\t    type=int,\n\t    default=None,\n\t    help=\"Keep only X number of checkpoints and delete the older ones.\",\n\t)\n\tparser.add_argument(\n\t    \"--vq_codebook_size\",\n\t    type=int,\n\t    default=256,\n", "    help=\"Image Size.\",\n\t)\n\tparser.add_argument(\"--vq_codebook_dim\", type=int, default=256, help=\"VQ Codebook dimensions.\")\n\tparser.add_argument(\n\t    \"--channels\", type=int, default=3, help=\"Number of channels for the VAE. Use 3 for RGB or 4 for RGBA.\"\n\t)\n\tparser.add_argument(\"--layers\", type=int, default=4, help=\"Number of layers for the VAE.\")\n\tparser.add_argument(\"--discr_layers\", type=int, default=4, help=\"Number of layers for the VAE discriminator.\")\n\tparser.add_argument(\n\t    \"--cond_drop_prob\",\n", "    type=float,\n\t    default=0.5,\n\t    help=\"Conditional dropout, for classifier free guidance.\",\n\t)\n\tparser.add_argument(\n\t    \"--image_size\",\n\t    type=int,\n\t    default=256,\n\t    help=\"Image size. You may want to start with small images, and then curriculum learn to larger ones, but because the vae is all convolution, it should generalize to 512 (as in paper) without training on it\",\n\t)\n", "parser.add_argument(\n\t    \"--lr_scheduler\",\n\t    type=str,\n\t    default=\"constant\",\n\t    help='The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"]',\n\t)\n\tparser.add_argument(\n\t    \"--scheduler_power\",\n\t    type=float,\n\t    default=1.0,\n", "    help=\"Controls the power of the polynomial decay schedule used by the CosineScheduleWithWarmup scheduler. \"\n\t    \"It determines the rate at which the learning rate decreases during the schedule.\",\n\t)\n\tparser.add_argument(\n\t    \"--lr_warmup_steps\",\n\t    type=int,\n\t    default=0,\n\t    help=\"Number of steps for the warmup in the lr scheduler.\",\n\t)\n\tparser.add_argument(\n", "    \"--num_cycles\",\n\t    type=int,\n\t    default=1,\n\t    help=\"Number of cycles for the lr scheduler.\",\n\t)\n\tparser.add_argument(\n\t    \"--resume_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"Path to the last saved checkpoint. 'results/maskgit.steps.pt'\",\n", ")\n\tparser.add_argument(\n\t    \"--taming_model_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"path to your trained VQGAN weights. This should be a .ckpt file. (only valid when taming option is enabled)\",\n\t)\n\tparser.add_argument(\n\t    \"--taming_config_path\",\n\t    type=str,\n", "    default=None,\n\t    help=\"path to your trained VQGAN config. This should be a .yaml file. (only valid when taming option is enabled)\",\n\t)\n\tparser.add_argument(\n\t    \"--optimizer\",\n\t    type=str,\n\t    default=\"Adafactor\",\n\t    help=\"Optimizer to use. Choose between: ['Adam', 'AdamW','Lion', 'Adafactor', \"\n\t    \"'AdaBound', 'AdaMod', 'AccSGD', 'AdamP', 'AggMo', 'DiffGrad', 'Lamb', \"\n\t    \"'NovoGrad', 'PID', 'QHAdam', 'QHM', 'RAdam', 'SGDP', 'SGDW', 'Shampoo', \"\n", "    \"'SWATS', 'Yogi']. Default: Lion\",\n\t)\n\tparser.add_argument(\n\t    \"--weight_decay\",\n\t    type=float,\n\t    default=0.0,\n\t    help=\"Optimizer weight_decay to use. Default: 0.0\",\n\t)\n\tparser.add_argument(\n\t    \"--cache_path\",\n", "    type=str,\n\t    default=None,\n\t    help=\"The path to cache huggingface models\",\n\t)\n\tparser.add_argument(\n\t    \"--no_cache\",\n\t    action=\"store_true\",\n\t    help=\"Do not save the dataset pyarrow cache/files to disk to save disk space and reduce the time it takes to launch the training.\",\n\t)\n\tparser.add_argument(\n", "    \"--link\",\n\t    action=\"store_true\",\n\t    help=\"whether to load a dataset with links instead of image (image column becomes URL column)\",\n\t)\n\tparser.add_argument(\n\t    \"--latest_checkpoint\",\n\t    action=\"store_true\",\n\t    help=\"Automatically find and use the latest checkpoint in the folder.\",\n\t)\n\tparser.add_argument(\n", "    \"--do_not_save_config\",\n\t    action=\"store_true\",\n\t    default=False,\n\t    help=\"Generate example YAML configuration file\",\n\t)\n\tparser.add_argument(\n\t    \"--use_l2_recon_loss\",\n\t    action=\"store_true\",\n\t    help=\"Use F.mse_loss instead of F.l1_loss.\",\n\t)\n", "parser.add_argument(\n\t    \"--debug\",\n\t    action=\"store_true\",\n\t    help=\"debug logging on\",\n\t)\n\tparser.add_argument(\n\t    \"--config_path\",\n\t    type=str,\n\t    default=None,\n\t    help=\"debug logging on\",\n", ")\n\tparser.add_argument(\n\t    \"--attention_type\",\n\t    type=str,\n\t    default=\"flash\",\n\t    help=\"what type of attention to use [ein, flash, xformers] | Default: flash\",\n\t)\n\t@dataclass\n\tclass Arguments:\n\t    total_params: Optional[int] = None\n", "    only_save_last_checkpoint: bool = False\n\t    validation_image_scale: float = 1.0\n\t    no_center_crop: bool = False\n\t    no_flip: bool = False\n\t    dataset_save_path: Optional[str] = None\n\t    clear_previous_experiments: bool = False\n\t    num_tokens: int = 256\n\t    seq_len: int = 1024\n\t    depth: int = 2\n\t    dim_head: int = 64\n", "    heads: int = 8\n\t    ff_mult: int = 4\n\t    t5_name: str = \"t5-small\"\n\t    cond_image_size: Optional[int] = None\n\t    validation_prompt: str = \"A photo of a dog\"\n\t    timesteps: int = 18\n\t    max_grad_norm: Optional[float] = None\n\t    seed: int = 42\n\t    valid_frac: float = 0.05\n\t    use_ema: bool = False\n", "    ema_beta: float = 0.995\n\t    ema_update_after_step: int = 1\n\t    ema_update_every: int = 1\n\t    apply_grad_penalty_every: int = 4\n\t    image_column: str = \"image\"\n\t    caption_column: str = \"caption\"\n\t    log_with: str = \"wandb\"\n\t    mixed_precision: str = \"no\"\n\t    use_8bit_adam: bool = False\n\t    results_dir: str = \"results\"\n", "    logging_dir: Optional[str] = None\n\t    vae_path: Optional[str] = None\n\t    dataset_name: Optional[str] = None\n\t    hf_split_name: Optional[str] = None\n\t    streaming: bool = False\n\t    train_data_dir: Optional[str] = None\n\t    num_train_steps: int = -1\n\t    num_epochs: int = 5\n\t    dim: int = 128\n\t    batch_size: int = 512\n", "    lr: float = 1e-4\n\t    gradient_accumulation_steps: int = 1\n\t    save_results_every: int = 100\n\t    save_model_every: int = 500\n\t    checkpoint_limit: Union[int, str] = None\n\t    vq_codebook_size: int = 256\n\t    vq_codebook_dim: int = 256\n\t    cond_drop_prob: float = 0.5\n\t    image_size: int = 256\n\t    lr_scheduler: str = \"constant\"\n", "    scheduler_power: float = 1.0\n\t    lr_warmup_steps: int = 0\n\t    num_cycles: int = 1\n\t    resume_path: Optional[str] = None\n\t    taming_model_path: Optional[str] = None\n\t    taming_config_path: Optional[str] = None\n\t    optimizer: str = \"Lion\"\n\t    weight_decay: float = 0.0\n\t    cache_path: Optional[str] = None\n\t    no_cache: bool = False\n", "    link: bool = False\n\t    latest_checkpoint: bool = False\n\t    do_not_save_config: bool = False\n\t    use_l2_recon_loss: bool = False\n\t    debug: bool = False\n\t    config_path: Optional[str] = None\n\t    attention_type: str = \"flash\"\n\tdef main():\n\t    args = parser.parse_args(namespace=Arguments())\n\t    if accelerate.utils.is_rich_available():\n", "        from rich import print\n\t        from rich.traceback import install as traceback_install\n\t        traceback_install(show_locals=args.debug, width=120, word_wrap=True)\n\t    if args.config_path:\n\t        print(\"Using config file and ignoring CLI args\")\n\t        try:\n\t            conf = OmegaConf.load(args.config_path)\n\t            conf_keys = conf.keys()\n\t            args_to_convert = vars(args)\n\t            for key in conf_keys:\n", "                try:\n\t                    args_to_convert[key] = conf[key]\n\t                except KeyError:\n\t                    print(f\"Error parsing config - {key}: {conf[key]} | Using default or parsed\")\n\t        except FileNotFoundError:\n\t            print(\"Could not find config, using default and parsed values...\")\n\t    # Set up debug logging as early as possible\n\t    if args.debug is True:\n\t        logging.basicConfig(level=logging.DEBUG)\n\t        transformers.logging.set_verbosity_debug()\n", "        datasets.logging.set_verbosity_debug()\n\t        diffusers.logging.set_verbosity_debug()\n\t    else:\n\t        logging.basicConfig(level=logging.INFO)\n\t    project_config = ProjectConfiguration(\n\t        project_dir=args.logging_dir if args.logging_dir else os.path.join(args.results_dir, \"logs\"),\n\t        total_limit=args.checkpoint_limit,\n\t        automatic_checkpoint_naming=True,\n\t    )\n\t    accelerator: accelerate.Accelerator = get_accelerator(\n", "        log_with=args.log_with,\n\t        gradient_accumulation_steps=args.gradient_accumulation_steps,\n\t        mixed_precision=args.mixed_precision,\n\t        project_config=project_config,\n\t        even_batches=True,\n\t    )\n\t    # Get these errors out of the way early\n\t    if args.vae_path and args.taming_model_path:\n\t        raise ValueError(\"Can't pass both vae_path and taming_model_path at the same time!\")\n\t    if args.train_data_dir and args.dataset_name:\n", "        raise ValueError(\"Can't pass both train_data_dir and dataset_name at the same time!\")\n\t    if accelerator.is_main_process:\n\t        accelerator.print(f\"Preparing MaskGit for training on {accelerator.device.type}\")\n\t        if args.debug:\n\t            inspect(args, docs=False)\n\t        accelerate.utils.set_seed(args.seed)\n\t    # Load the dataset (main process first to download, rest will load from cache)\n\t    with accelerator.main_process_first():\n\t        if args.train_data_dir is not None:\n\t            if args.no_cache:\n", "                pass\n\t            else:\n\t                dataset = get_dataset_from_dataroot(\n\t                    args.train_data_dir,\n\t                    image_column=args.image_column,\n\t                    caption_column=args.caption_column,\n\t                    save_path=args.dataset_save_path,\n\t                )\n\t        elif args.dataset_name is not None:\n\t            dataset = load_dataset(\n", "                args.dataset_name,\n\t                streaming=args.streaming,\n\t                cache_dir=args.cache_path,\n\t                save_infos=True,\n\t                split=\"train\",\n\t            )\n\t            if args.streaming:\n\t                if args.cache_path:\n\t                    dataset = load_dataset(args.dataset_name, cache_dir=args.cache_path)[args.hf_split_name]\n\t                else:\n", "                    dataset = load_dataset(args.dataset_name)[args.hf_split_name]\n\t        else:\n\t            raise ValueError(\"You must pass either train_data_dir or dataset_name (but not both)\")\n\t    # Load the VAE\n\t    with accelerator.main_process_first():\n\t        if args.vae_path:\n\t            print(\"Loading Muse VQGanVAE\")\n\t            if args.latest_checkpoint:\n\t                print(\"Finding latest VAE checkpoint...\")\n\t                orig_vae_path = args.vae_path\n", "                if os.path.isfile(args.vae_path) or \".pt\" in args.vae_path:\n\t                    # If args.vae_path is a file, split it into directory and filename\n\t                    args.vae_path, _ = os.path.split(args.vae_path)\n\t                checkpoint_files = glob.glob(os.path.join(args.vae_path, \"vae.*.pt\"))\n\t                if checkpoint_files:\n\t                    latest_checkpoint_file = max(\n\t                        checkpoint_files, key=lambda x: int(re.search(r\"vae\\.(\\d+)\\.pt\", x).group(1))\n\t                    )\n\t                    # Check if latest checkpoint is empty or unreadable\n\t                    if os.path.getsize(latest_checkpoint_file) == 0 or not os.access(\n", "                        latest_checkpoint_file, os.R_OK\n\t                    ):\n\t                        print(\n\t                            f\"Warning: latest VAE checkpoint {latest_checkpoint_file} is empty or unreadable.\"\n\t                        )\n\t                        if len(checkpoint_files) > 1:\n\t                            # Use the second last checkpoint as a fallback\n\t                            latest_checkpoint_file = max(\n\t                                checkpoint_files[:-1],\n\t                                key=lambda x: int(re.search(r\"vae\\.(\\d+)\\.pt\", x).group(1)),\n", "                            )\n\t                            print(\"Using second last VAE checkpoint: \", latest_checkpoint_file)\n\t                        else:\n\t                            print(\"No usable checkpoint found.\")\n\t                    elif latest_checkpoint_file != orig_vae_path:\n\t                        print(\"Resuming VAE from latest checkpoint: \", latest_checkpoint_file)\n\t                    else:\n\t                        print(\"Using VAE checkpoint specified in vae_path: \", orig_vae_path)\n\t                    args.vae_path = latest_checkpoint_file\n\t                else:\n", "                    print(\"No VAE checkpoints found in directory: \", args.vae_path)\n\t            else:\n\t                print(\"Resuming VAE from: \", args.vae_path)\n\t            # use config next to checkpoint if there is one and merge the cli arguments to it\n\t            # the cli arguments will take priority so we can use it to override any value we want.\n\t            # if os.path.exists(f\"{args.vae_path}.yaml\"):\n\t            # print(\"Config file found, reusing config from it. Use cli arguments to override any desired value.\")\n\t            # conf = OmegaConf.load(f\"{args.vae_path}.yaml\")\n\t            # cli_conf = OmegaConf.from_cli()\n\t            ## merge the config file and the cli arguments.\n", "            # conf = OmegaConf.merge(conf, cli_conf)\n\t            vae = VQGanVAE(\n\t                dim=args.dim,\n\t                vq_codebook_dim=args.vq_codebook_dim,\n\t                vq_codebook_size=args.vq_codebook_size,\n\t                l2_recon_loss=args.use_l2_recon_loss,\n\t                channels=args.channels,\n\t                layers=args.layers,\n\t                discr_layers=args.discr_layers,\n\t            ).to(accelerator.device)\n", "            vae.load(args.vae_path)\n\t        elif args.taming_model_path is not None and args.taming_config_path is not None:\n\t            print(f\"Using Taming VQGanVAE, loading from {args.taming_model_path}\")\n\t            vae = VQGanVAETaming(\n\t                vqgan_model_path=args.taming_model_path,\n\t                vqgan_config_path=args.taming_config_path,\n\t                accelerator=accelerator,\n\t            )\n\t            args.num_tokens = vae.codebook_size\n\t            args.seq_len = vae.get_encoded_fmap_size(args.image_size) ** 2\n", "        else:\n\t            raise ValueError(\n\t                \"You must pass either vae_path or taming_model_path + taming_config_path (but not both)\"\n\t            )\n\t    # freeze VAE before parsing to transformer\n\t    vae.requires_grad_(False)\n\t    # then you plug the vae and transformer into your MaskGit like so:\n\t    # (1) create your transformer / attention network\n\t    if args.attention_type == \"flash\":\n\t        xformers = False\n", "        flash = True\n\t    elif args.attention_type == \"xformers\":\n\t        xformers = True\n\t        flash = True\n\t    elif args.attention_type == \"ein\":\n\t        xformers = False\n\t        flash = False\n\t    else:\n\t        raise NotImplementedError(f'Attention of type \"{args.attention_type}\" does not exist')\n\t    transformer: MaskGitTransformer = MaskGitTransformer(\n", "        # num_tokens must be same as codebook size above\n\t        num_tokens=args.num_tokens if args.num_tokens else args.vq_codebook_size,\n\t        # seq_len must be equivalent to fmap_size ** 2 in vae\n\t        seq_len=args.seq_len,\n\t        dim=args.dim,\n\t        depth=args.depth,\n\t        dim_head=args.dim_head,\n\t        heads=args.heads,\n\t        # feedforward expansion factor\n\t        ff_mult=args.ff_mult,\n", "        # name of your T5 model configuration\n\t        t5_name=args.t5_name,\n\t        cache_path=args.cache_path,\n\t        flash=flash,\n\t        xformers=xformers,\n\t    )\n\t    # (2) pass your trained VAE and the base transformer to MaskGit\n\t    maskgit = MaskGit(\n\t        vae=vae,  # vqgan vae\n\t        transformer=transformer,  # transformer\n", "        accelerator=accelerator,  # accelerator\n\t        image_size=args.image_size,  # image size\n\t        cond_drop_prob=args.cond_drop_prob,  # conditional dropout, for classifier free guidance\n\t        cond_image_size=args.cond_image_size,\n\t    )\n\t    # load the maskgit transformer from disk if we have previously trained one\n\t    with accelerator.main_process_first():\n\t        if args.resume_path is not None and len(args.resume_path) > 1:\n\t            load = True\n\t            accelerator.print(\"Loading Muse MaskGit...\")\n", "            if args.latest_checkpoint:\n\t                accelerator.print(\"Finding latest MaskGit checkpoint...\")\n\t                orig_vae_path = args.resume_path\n\t                if os.path.isfile(args.resume_path) or \".pt\" in args.resume_path:\n\t                    # If args.resume_path is a file, split it into directory and filename\n\t                    args.resume_path, _ = os.path.split(args.resume_path)\n\t                if args.cond_image_size:\n\t                    checkpoint_files = glob.glob(os.path.join(args.resume_path, \"maskgit_superres.*.pt\"))\n\t                else:\n\t                    checkpoint_files = glob.glob(os.path.join(args.resume_path, \"maskgit.*.pt\"))\n", "                if checkpoint_files:\n\t                    if args.cond_image_size:\n\t                        latest_checkpoint_file = max(\n\t                            checkpoint_files,\n\t                            key=lambda x: int(re.search(r\"maskgit_superres\\.(\\d+)\\.pt\", x).group(1)),\n\t                        )\n\t                    else:\n\t                        latest_checkpoint_file = max(\n\t                            checkpoint_files, key=lambda x: int(re.search(r\"maskgit\\.(\\d+)\\.pt\", x).group(1))\n\t                        )\n", "                    # Check if latest checkpoint is empty or unreadable\n\t                    if os.path.getsize(latest_checkpoint_file) == 0 or not os.access(\n\t                        latest_checkpoint_file, os.R_OK\n\t                    ):\n\t                        accelerator.print(\n\t                            f\"Warning: latest MaskGit checkpoint {latest_checkpoint_file} is empty or unreadable.\"\n\t                        )\n\t                        if len(checkpoint_files) > 1:\n\t                            # Use the second last checkpoint as a fallback\n\t                            if args.cond_image_size:\n", "                                latest_checkpoint_file = max(\n\t                                    checkpoint_files[:-1],\n\t                                    key=lambda x: int(re.search(r\"maskgit_superres\\.(\\d+)\\.pt\", x).group(1)),\n\t                                )\n\t                            else:\n\t                                latest_checkpoint_file = max(\n\t                                    checkpoint_files[:-1],\n\t                                    key=lambda x: int(re.search(r\"maskgit\\.(\\d+)\\.pt\", x).group(1)),\n\t                                )\n\t                            accelerator.print(\n", "                                \"Using second last MaskGit checkpoint: \", latest_checkpoint_file\n\t                            )\n\t                        else:\n\t                            accelerator.print(\"No usable MaskGit checkpoint found.\")\n\t                            load = False\n\t                    elif latest_checkpoint_file != orig_vae_path:\n\t                        accelerator.print(\"Resuming MaskGit from latest checkpoint: \", latest_checkpoint_file)\n\t                    else:\n\t                        accelerator.print(\n\t                            \"Using MaskGit checkpoint specified in resume_path: \", orig_vae_path\n", "                        )\n\t                    args.resume_path = latest_checkpoint_file\n\t                else:\n\t                    accelerator.print(\"No MaskGit checkpoints found in directory: \", args.resume_path)\n\t                    load = False\n\t            else:\n\t                accelerator.print(\"Resuming MaskGit from: \", args.resume_path)\n\t            if load:\n\t                maskgit.load(args.resume_path)\n\t                resume_from_parts = args.resume_path.split(\".\")\n", "                for i in range(len(resume_from_parts) - 1, -1, -1):\n\t                    if resume_from_parts[i].isdigit():\n\t                        current_step = int(resume_from_parts[i])\n\t                        accelerator.print(f\"Found step {current_step} for the MaskGit model.\")\n\t                        break\n\t                if current_step == 0:\n\t                    accelerator.print(\"No step found for the MaskGit model.\")\n\t            else:\n\t                current_step = 0\n\t        else:\n", "            accelerator.print(\"Initialized new empty MaskGit model.\")\n\t            current_step = 0\n\t    # Use the parameters() method to get an iterator over all the learnable parameters of the model\n\t    total_params = sum(p.numel() for p in maskgit.parameters())\n\t    args.total_params = total_params\n\t    print(f\"Total number of parameters: {format(total_params, ',d')}\")\n\t    # Create the dataset objects\n\t    with accelerator.main_process_first():\n\t        if args.no_cache and args.train_data_dir:\n\t            dataset = LocalTextImageDataset(\n", "                args.train_data_dir,\n\t                args.image_size,\n\t                tokenizer=transformer.tokenizer,\n\t                center_crop=False if args.no_center_crop else True,\n\t                flip=False if args.no_flip else True,\n\t                using_taming=False if not args.taming_model_path else True,\n\t                random_crop=args.random_crop if args.random_crop else False,\n\t                alpha_channel=False if args.channels == 3 else True,\n\t            )\n\t        elif args.link:\n", "            if not args.dataset_name:\n\t                raise AssertionError(\"You can only use links in huggingface datasets\")\n\t            dataset = URLTextDataset(\n\t                dataset,\n\t                args.image_size,\n\t                transformer.tokenizer,\n\t                image_column=args.image_column,\n\t                caption_column=args.caption_column,\n\t                center_crop=False if args.no_center_crop else True,\n\t                flip=False if args.no_flip else True,\n", "                using_taming=False if not args.taming_model_path else True,\n\t            )\n\t        else:\n\t            dataset = ImageTextDataset(\n\t                dataset,\n\t                args.image_size,\n\t                transformer.tokenizer,\n\t                image_column=args.image_column,\n\t                caption_column=args.caption_column,\n\t                center_crop=False if args.no_center_crop else True,\n", "                flip=False if args.no_flip else True,\n\t                stream=args.streaming,\n\t                using_taming=False if not args.taming_model_path else True,\n\t            )\n\t    # Create the dataloaders\n\t    dataloader, validation_dataloader = split_dataset_into_dataloaders(\n\t        dataset,\n\t        args.valid_frac if not args.streaming else 0,\n\t        args.seed,\n\t        args.batch_size,\n", "    )\n\t    # Create the optimizer and scheduler, wrap optimizer in scheduler\n\t    optimizer: Optimizer = get_optimizer(\n\t        args.use_8bit_adam, args.optimizer, set(transformer.parameters()), args.lr, args.weight_decay\n\t    )\n\t    if args.num_train_steps > 0:\n\t        num_lr_steps = args.num_train_steps * args.gradient_accumulation_steps\n\t    else:\n\t        num_lr_steps = args.num_epochs * len(dataloader)\n\t    scheduler: SchedulerType = get_scheduler(\n", "        args.lr_scheduler,\n\t        optimizer=optimizer,\n\t        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n\t        num_training_steps=num_lr_steps,\n\t        num_cycles=args.num_cycles,\n\t        power=args.scheduler_power,\n\t    )\n\t    # Prepare the model, optimizer, and dataloaders for distributed training\n\t    maskgit, optimizer, dataloader, validation_dataloader, scheduler = accelerator.prepare(\n\t        maskgit, optimizer, dataloader, validation_dataloader, scheduler\n", "    )\n\t    # Wait for everyone to catch up, then print some info and initialize the trackers\n\t    accelerator.wait_for_everyone()\n\t    accelerator.print(f\"[{accelerator.process_index}] Ready to create trainer!\")\n\t    if accelerator.distributed_type == accelerate.DistributedType.TPU:\n\t        proc_idx = accelerator.process_index + 1\n\t        n_procs = accelerator.num_processes\n\t        local_proc_idx = accelerator.local_process_index + 1\n\t        xm_ord = xm.get_ordinal() + 1\n\t        xm_world = xm.xrt_world_size()\n", "        xm_local_ord = xm.get_local_ordinal() + 1\n\t        xm_master_ord = xm.is_master_ordinal()\n\t        is_main = accelerator.is_main_process\n\t        is_local_main = accelerator.is_local_main_process\n\t        with accelerator.local_main_process_first():\n\t            print(\n\t                f\"[P{proc_idx:03d}]: PID {proc_idx}/{n_procs}, local #{local_proc_idx}, \",\n\t                f\"XLA ord={xm_ord}/{xm_world}, local={xm_local_ord}, master={xm_master_ord} \",\n\t                f\"Accelerate: main={is_main}, local main={is_local_main} \",\n\t            )\n", "    if accelerator.is_main_process:\n\t        accelerator.init_trackers(\n\t            args.project_name,\n\t            config=vars(args),\n\t            init_kwargs={\n\t                \"wandb\": {\n\t                    \"entity\": f\"{args.wandb_user or wandb.api.default_entity}\",\n\t                    \"name\": args.run_name,\n\t                },\n\t            },\n", "        )\n\t    # Create the trainer\n\t    accelerator.wait_for_everyone()\n\t    trainer = MaskGitTrainer(\n\t        maskgit=maskgit,\n\t        dataloader=dataloader,\n\t        valid_dataloader=validation_dataloader,\n\t        accelerator=accelerator,\n\t        optimizer=optimizer,\n\t        scheduler=scheduler,\n", "        current_step=current_step + 1 if current_step != 0 else current_step,\n\t        num_train_steps=args.num_train_steps,\n\t        batch_size=args.batch_size,\n\t        max_grad_norm=args.max_grad_norm,\n\t        save_results_every=args.save_results_every,\n\t        save_model_every=args.save_model_every,\n\t        results_dir=args.results_dir,\n\t        logging_dir=args.logging_dir if args.logging_dir else os.path.join(args.results_dir, \"logs\"),\n\t        use_ema=args.use_ema,\n\t        ema_update_after_step=args.ema_update_after_step,\n", "        ema_update_every=args.ema_update_every,\n\t        apply_grad_penalty_every=args.apply_grad_penalty_every,\n\t        gradient_accumulation_steps=args.gradient_accumulation_steps,\n\t        validation_prompts=args.validation_prompt.split(\"|\"),\n\t        timesteps=args.timesteps,\n\t        clear_previous_experiments=args.clear_previous_experiments,\n\t        validation_image_scale=args.validation_image_scale,\n\t        only_save_last_checkpoint=args.only_save_last_checkpoint,\n\t        num_epochs=args.num_epochs,\n\t        args=args,\n", "    )\n\t    # Prepare the trainer for distributed training\n\t    accelerator.print(\"MaskGit Trainer initialized, preparing for training...\")\n\t    trainer = accelerator.prepare(trainer)\n\t    # Train the model!\n\t    accelerator.print(\"Starting training!\")\n\t    trainer.train()\n\t    # Clean up and wait for other processes to finish (loggers etc.)\n\t    if accelerator.is_main_process:\n\t        accelerator.print(\"Training complete!\")\n", "        accelerator.end_training()\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "scripts/vqvae_test.py", "chunked_list": ["import logging\n\tfrom pathlib import Path\n\timport torch\n\tfrom huggingface_hub import hf_hub_download\n\tfrom PIL import Image\n\tfrom torchvision import transforms as T\n\tfrom torchvision.utils import save_image\n\tfrom muse_maskgit_pytorch.vqvae import VQVAE\n\tlogging.basicConfig(level=logging.INFO)\n\tlogger = logging.getLogger(__name__)\n", "# where to find the model and the test images\n\tmodel_repo = \"neggles/vaedump\"\n\tmodel_subdir = \"vit-s-vqgan-f4\"\n\ttest_images = [\"testimg_1.png\", \"testimg_2.png\"]\n\t# where to save the preprocessed and reconstructed images\n\timage_dir = Path.cwd().joinpath(\"temp\")\n\timage_dir.mkdir(exist_ok=True, parents=True)\n\t# image transforms for the VQVAE\n\ttransform_enc = T.Compose([T.Resize(512), T.RandomCrop(256), T.ToTensor()])\n\ttransform_dec = T.Compose([T.ConvertImageDtype(torch.uint8), T.ToPILImage()])\n", "def get_save_path(path: Path, append: str) -> Path:\n\t    # append a string to the filename before the extension\n\t    # n.b. only keeps the final suffix, e.g. \"foo.xyz.png\" -> \"foo-prepro.png\"\n\t    return path.with_name(f\"{path.stem}-{append}{path.suffix}\")\n\tdef main():\n\t    torch_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\t    dtype = torch.float32\n\t    # load VAE\n\t    logger.info(f\"Loading VQVAE from {model_repo}/{model_subdir}...\")\n\t    vae: VQVAE = VQVAE.from_pretrained(model_repo, subfolder=model_subdir, torch_dtype=dtype)\n", "    vae = vae.to(torch_device)\n\t    logger.info(f\"Loaded VQVAE from {model_repo} to {vae.device} with dtype {vae.dtype}\")\n\t    # download and process images\n\t    for image in test_images:\n\t        image_path = hf_hub_download(model_repo, subfolder=\"images\", filename=image, local_dir=image_dir)\n\t        image_path = Path(image_path)\n\t        logger.info(f\"Downloaded {image_path}, size {image_path.stat().st_size} bytes\")\n\t        # preprocess\n\t        image_obj = Image.open(image_path).convert(\"RGB\")\n\t        image_tensor: torch.Tensor = transform_enc(image_obj)\n", "        save_path = get_save_path(image_path, \"prepro\")\n\t        save_image(image_tensor, save_path, normalize=True, range=(-1.0, 1.0))\n\t        logger.info(f\"Saved preprocessed image to {save_path}\")\n\t        # encode\n\t        encoded, _, _ = vae.encode(image_tensor.unsqueeze(0).to(vae.device))\n\t        # decode\n\t        reconstructed = vae.decode(encoded).squeeze(0)\n\t        reconstructed = torch.clamp(reconstructed, -1.0, 1.0)\n\t        # save\n\t        save_path = get_save_path(image_path, \"recon\")\n", "        save_image(reconstructed, save_path, normalize=True, range=(-1.0, 1.0))\n\t        logger.info(f\"Saved reconstructed image to {save_path}\")\n\t        # compare\n\t        image_prepro = transform_dec(image_tensor)\n\t        image_recon = transform_dec(reconstructed)\n\t        canvas = Image.new(\"RGB\", (512 + 12, 256 + 8), (248, 248, 242))\n\t        canvas.paste(image_prepro, (4, 4))\n\t        canvas.paste(image_recon, (256 + 8, 4))\n\t        save_path = get_save_path(image_path, \"compare\")\n\t        canvas.save(save_path)\n", "        logger.info(f\"Saved comparison image to {save_path}\")\n\t    logger.info(\"Done!\")\n\tif __name__ == \"__main__\":\n\t    main()\n"]}
{"filename": "muse_maskgit_pytorch/distributed_utils.py", "chunked_list": ["\"\"\"\n\tUtility functions for optional distributed execution.\n\tTo use,\n\t1. set the `BACKENDS` to the ones you want to make available,\n\t2. in the script, wrap the argument parser with `wrap_arg_parser`,\n\t3. in the script, set and use the backend by calling\n\t   `set_backend_from_args`.\n\tYou can check whether a backend is in use with the `using_backend`\n\tfunction.\n\t\"\"\"\n", "is_distributed = None\n\t\"\"\"Whether we are distributed.\"\"\"\n\tbackend = None\n\t\"\"\"Backend in usage.\"\"\"\n\tdef require_set_backend():\n\t    \"\"\"Raise an `AssertionError` when the backend has not been set.\"\"\"\n\t    assert backend is not None, (\n\t        \"distributed backend is not set. Please call \"\n\t        \"`distributed_utils.set_backend_from_args` at the start of your script\"\n\t    )\n", "def using_backend(test_backend):\n\t    \"\"\"Return whether the backend is set to `test_backend`.\n\t    `test_backend` may be a string of the name of the backend or\n\t    its class.\n\t    \"\"\"\n\t    require_set_backend()\n\t    if isinstance(test_backend, str):\n\t        return backend.BACKEND_NAME == test_backend\n\t    return isinstance(backend, test_backend)\n"]}
{"filename": "muse_maskgit_pytorch/muse_maskgit_pytorch.py", "chunked_list": ["import math\n\tfrom functools import partial\n\tfrom os import PathLike\n\tfrom pathlib import Path\n\tfrom random import random\n\tfrom typing import Callable, List, Optional, Union\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision.transforms as T\n\tfrom accelerate import Accelerator\n", "from beartype import beartype\n\tfrom einops import rearrange, repeat\n\tfrom torch import einsum, isnan, nn\n\tfrom tqdm.auto import tqdm\n\tfrom transformers import T5EncoderModel, T5Tokenizer\n\tfrom .attn import ein_attn, sdp_attn\n\tfrom .t5 import DEFAULT_T5_NAME, get_encoded_dim, get_model_and_tokenizer, t5_encode_text\n\tfrom .vqgan_vae import VQGanVAE\n\tfrom .vqgan_vae_taming import VQGanVAETaming\n\ttry:\n", "    from .attn import xformers_attn\n\t    xformer_attn = True\n\texcept ImportError:\n\t    xformer_attn = False\n\t# helpers\n\tdef exists(val):\n\t    return val is not None\n\tdef default(val, d):\n\t    return val if val is not None else d\n\tdef eval_decorator(fn):\n", "    def inner(model, *args, **kwargs):\n\t        was_training = model.training\n\t        model.eval()\n\t        out = fn(model, *args, **kwargs)\n\t        model.train(was_training)\n\t        return out\n\t    return inner\n\tdef l2norm(t):\n\t    return F.normalize(t, dim=-1)\n\t# tensor helpers\n", "def get_mask_subset_prob(mask, prob, min_mask=0):\n\t    batch, seq, device = *mask.shape, mask.device\n\t    num_to_mask = (mask.sum(dim=-1, keepdim=True) * prob).clamp(min=min_mask)\n\t    logits = torch.rand((batch, seq), device=device)\n\t    logits = logits.masked_fill(~mask, -1)\n\t    randperm = logits.argsort(dim=-1).float()\n\t    num_padding = (~mask).sum(dim=-1, keepdim=True)\n\t    randperm -= num_padding\n\t    subset_mask = randperm < num_to_mask\n\t    subset_mask.masked_fill_(~mask, False)\n", "    return subset_mask\n\t# classes\n\tclass LayerNorm(nn.Module):\n\t    def __init__(self, dim):\n\t        super().__init__()\n\t        self.gamma = nn.Parameter(torch.ones(dim))\n\t        self.register_buffer(\"beta\", torch.zeros(dim))\n\t    def forward(self, x):\n\t        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n\tclass GEGLU(nn.Module):\n", "    \"\"\"https://arxiv.org/abs/2002.05202\"\"\"\n\t    def forward(self, x):\n\t        x, gate = x.chunk(2, dim=-1)\n\t        return gate * F.gelu(x)\n\tdef FeedForward(dim, mult=4):\n\t    \"\"\"https://arxiv.org/abs/2110.09456\"\"\"\n\t    inner_dim = int(dim * mult * 2 / 3)\n\t    return nn.Sequential(\n\t        LayerNorm(dim),\n\t        nn.Linear(dim, inner_dim * 2, bias=False),\n", "        GEGLU(),\n\t        LayerNorm(inner_dim),\n\t        nn.Linear(inner_dim, dim, bias=False),\n\t    )\n\tclass TransformerBlocks(nn.Module):\n\t    def __init__(self, *, dim, depth, dim_head=64, heads=8, ff_mult=4, flash=True, xformers=False):\n\t        super().__init__()\n\t        self.layers = nn.ModuleList([])\n\t        for _ in range(depth):\n\t            if flash:\n", "                if xformers and xformer_attn:\n\t                    self.layers.append(\n\t                        nn.ModuleList(\n\t                            [\n\t                                xformers_attn.Attention(dim=dim, dim_head=dim_head, heads=heads),\n\t                                xformers_attn.Attention(\n\t                                    dim=dim, dim_head=dim_head, heads=heads, cross_attend=True\n\t                                ),\n\t                                FeedForward(dim=dim, mult=ff_mult),\n\t                            ]\n", "                        )\n\t                    )\n\t                else:\n\t                    self.layers.append(\n\t                        nn.ModuleList(\n\t                            [\n\t                                sdp_attn.Attention(dim=dim, dim_head=dim_head, heads=heads),\n\t                                sdp_attn.Attention(\n\t                                    dim=dim, dim_head=dim_head, heads=heads, cross_attend=True\n\t                                ),\n", "                                FeedForward(dim=dim, mult=ff_mult),\n\t                            ]\n\t                        )\n\t                    )\n\t            else:\n\t                self.layers.append(\n\t                    nn.ModuleList(\n\t                        [\n\t                            ein_attn.Attention(dim=dim, dim_head=dim_head, heads=heads),\n\t                            ein_attn.Attention(dim=dim, dim_head=dim_head, heads=heads, cross_attend=True),\n", "                            FeedForward(dim=dim, mult=ff_mult),\n\t                        ]\n\t                    )\n\t                )\n\t        self.norm = LayerNorm(dim)\n\t    def forward(self, x, context=None, context_mask=None):\n\t        for attn, cross_attn, ff in self.layers:\n\t            x = attn(x) + x\n\t            x = cross_attn(x, context=context, context_mask=context_mask) + x\n\t            x = ff(x) + x\n", "        return self.norm(x)\n\t# transformer - it's all we need\n\tclass Transformer(nn.Module):\n\t    def __init__(\n\t        self,\n\t        *,\n\t        num_tokens: int,\n\t        dim: int,\n\t        seq_len: int,\n\t        dim_out: Optional[int] = None,\n", "        t5_name: str = DEFAULT_T5_NAME,\n\t        self_cond: bool = False,\n\t        add_mask_id: bool = False,\n\t        cache_path: PathLike = None,\n\t        **kwargs,\n\t    ):\n\t        super().__init__()\n\t        self.dim = dim\n\t        self.mask_id = num_tokens if add_mask_id else None\n\t        self.num_tokens = num_tokens\n", "        self.token_emb = nn.Embedding(num_tokens + int(add_mask_id), dim)\n\t        self.pos_emb = nn.Embedding(seq_len, dim)\n\t        self.seq_len = seq_len\n\t        self.transformer_blocks = TransformerBlocks(dim=dim, **kwargs)\n\t        self.norm = LayerNorm(dim)\n\t        self.dim_out = default(dim_out, num_tokens)\n\t        self.to_logits = nn.Linear(dim, self.dim_out, bias=False)\n\t        # text conditioning\n\t        t5, tokenizer = get_model_and_tokenizer(t5_name, cache_path)\n\t        self.t5: T5EncoderModel = t5\n", "        self.tokenizer: T5Tokenizer = tokenizer\n\t        self.t5.eval()\n\t        text_embed_dim = get_encoded_dim(t5_name)\n\t        self.text_embed_proj = (\n\t            nn.Linear(text_embed_dim, dim, bias=False) if text_embed_dim != dim else nn.Identity()\n\t        )\n\t        # optional self conditioning\n\t        self.self_cond = self_cond\n\t        self.self_cond_to_init_embed = FeedForward(dim)\n\t    def encode_text(self, *args, **kwargs):\n", "        kwargs.update(tokenizer=self.tokenizer, t5=self.t5)\n\t        return t5_encode_text(*args, **kwargs)\n\t    def forward_with_cond_scale(self, *args, cond_scale=3.0, return_embed=False, **kwargs):\n\t        if cond_scale == 1:\n\t            return self.forward(*args, return_embed=return_embed, cond_drop_prob=0.0, **kwargs)\n\t        logits, embed = self.forward(*args, return_embed=True, cond_drop_prob=0.0, **kwargs)\n\t        null_logits = self.forward(*args, cond_drop_prob=1.0, **kwargs)\n\t        scaled_logits = null_logits + (logits - null_logits) * cond_scale\n\t        return (scaled_logits, embed) if return_embed else scaled_logits\n\t    def forward_with_neg_prompt(\n", "        self,\n\t        *args,\n\t        text_embed: torch.Tensor,\n\t        neg_text_embed: torch.Tensor,\n\t        cond_scale=3.0,\n\t        return_embed=False,\n\t        **kwargs,\n\t    ):\n\t        neg_logits = self.forward(*args, neg_text_embed=neg_text_embed, cond_drop_prob=0.0, **kwargs)\n\t        pos_logits, embed = self.forward(\n", "            *args, return_embed=True, text_embed=text_embed, cond_drop_prob=0.0, **kwargs\n\t        )\n\t        scaled_logits = neg_logits + (pos_logits - neg_logits) * cond_scale\n\t        if return_embed:\n\t            return scaled_logits, embed\n\t        return scaled_logits\n\t    def forward(\n\t        self,\n\t        x,\n\t        return_embed=False,\n", "        return_logits=False,\n\t        labels=None,\n\t        ignore_index=0,\n\t        self_cond_embed=None,\n\t        cond_drop_prob=0.0,\n\t        conditioning_token_ids: Optional[torch.Tensor] = None,\n\t        texts: Optional[List[str]] = None,\n\t        text_embeds: Optional[torch.Tensor] = None,\n\t    ):\n\t        device, b, n = x.device, *x.shape\n", "        assert n <= self.seq_len\n\t        # prepare texts\n\t        if texts is not None and text_embeds is not None:\n\t            raise ValueError(\"only one of texts or text_embeds should be passed in\")\n\t        if texts is not None:\n\t            text_embeds = self.encode_text(texts)\n\t        context = self.text_embed_proj(text_embeds)\n\t        context_mask = (text_embeds != 0).any(dim=-1)\n\t        # classifier free guidance\n\t        if self.training and cond_drop_prob > 0.0:\n", "            mask = prob_mask_like((b, 1), 1.0 - cond_drop_prob, device)\n\t            context_mask = context_mask & mask\n\t        # concat conditioning image token ids if needed\n\t        if exists(conditioning_token_ids):\n\t            conditioning_token_ids = rearrange(conditioning_token_ids, \"b ... -> b (...)\")\n\t            cond_token_emb = self.token_emb(conditioning_token_ids)\n\t            context = torch.cat((context, cond_token_emb), dim=-2)\n\t            context_mask = F.pad(context_mask, (0, conditioning_token_ids.shape[-1]), value=True)\n\t        # embed tokens\n\t        x = self.token_emb(x)\n", "        x = x + self.pos_emb(torch.arange(n, device=device))\n\t        if self.self_cond:\n\t            if not exists(self_cond_embed):\n\t                self_cond_embed = torch.zeros_like(x)\n\t            x = x + self.self_cond_to_init_embed(self_cond_embed)\n\t        embed = self.transformer_blocks(x, context=context, context_mask=context_mask)\n\t        logits = self.to_logits(embed)\n\t        if return_embed:\n\t            return logits, embed\n\t        if not exists(labels):\n", "            return logits\n\t        if self.dim_out == 1:\n\t            loss = F.binary_cross_entropy_with_logits(rearrange(logits, \"... 1 -> ...\"), labels)\n\t        else:\n\t            loss = F.cross_entropy(rearrange(logits, \"b n c -> b c n\"), labels, ignore_index=ignore_index)\n\t        if not return_logits:\n\t            return loss\n\t        return loss, logits\n\t# self critic wrapper\n\tclass SelfCritic(nn.Module):\n", "    def __init__(self, net):\n\t        super().__init__()\n\t        self.net = net\n\t        self.to_pred = nn.Linear(net.dim, 1)\n\t    def forward_with_cond_scale(self, x, *args, **kwargs):\n\t        _, embeds = self.net.forward_with_cond_scale(x, *args, return_embed=True, **kwargs)\n\t        return self.to_pred(embeds)\n\t    def forward_with_neg_prompt(self, x, *args, **kwargs):\n\t        _, embeds = self.net.forward_with_neg_prompt(x, *args, return_embed=True, **kwargs)\n\t        return self.to_pred(embeds)\n", "    def forward(self, x, *args, labels=None, **kwargs):\n\t        _, embeds = self.net(x, *args, return_embed=True, **kwargs)\n\t        logits = self.to_pred(embeds)\n\t        if not exists(labels):\n\t            return logits\n\t        logits = rearrange(logits, \"... 1 -> ...\")\n\t        return F.binary_cross_entropy_with_logits(logits, labels)\n\t# specialized transformers\n\tclass MaskGitTransformer(Transformer):\n\t    def __init__(self, *args, **kwargs):\n", "        if kwargs.pop(\"add_mask_id\", True) is not True:\n\t            raise ValueError(\"MaskGitTransformer does not accept add_mask_id argument\")\n\t        super().__init__(*args, add_mask_id=True, **kwargs)\n\tclass TokenCritic(Transformer):\n\t    def __init__(self, *args, **kwargs):\n\t        if kwargs.pop(\"dim_out\", 1) != 1:\n\t            raise ValueError(\"TokenCritic does not accept dim_out argument\")\n\t        super().__init__(*args, dim_out=1, **kwargs)\n\t# classifier free guidance functions\n\tdef uniform(shape, min=0, max=1, device=None):\n", "    return torch.zeros(shape, device=device).float().uniform_(0, 1)\n\tdef prob_mask_like(shape, prob, device=None):\n\t    if prob == 1:\n\t        return torch.ones(shape, device=device, dtype=torch.bool)\n\t    elif prob == 0:\n\t        return torch.zeros(shape, device=device, dtype=torch.bool)\n\t    else:\n\t        return uniform(shape, device=device) < prob\n\t# sampling helpers\n\tdef log(t, eps=1e-20):\n", "    return torch.log(t.clamp(min=eps))\n\tdef gumbel_noise(t):\n\t    noise = torch.zeros_like(t).uniform_(0, 1)\n\t    return -log(-log(noise))\n\tdef gumbel_sample(t, temperature=1.0, dim=-1):\n\t    return ((t / max(temperature, 1e-10)) + gumbel_noise(t)).argmax(dim=dim)\n\tdef top_k(logits, thres=0.9):\n\t    k = math.ceil((1 - thres) * logits.shape[-1])\n\t    val, ind = logits.topk(k, dim=-1)\n\t    probs = torch.full_like(logits, float(\"-inf\"))\n", "    probs.scatter_(2, ind, val)\n\t    return probs\n\t# noise schedules\n\tdef cosine_schedule(t):\n\t    return torch.cos(t * math.pi * 0.5)\n\t# main maskgit classes\n\t@beartype\n\tclass MaskGit(nn.Module):\n\t    def __init__(\n\t        self,\n", "        image_size,\n\t        transformer: MaskGitTransformer,\n\t        accelerator: Optional[Accelerator] = None,\n\t        noise_schedule: Callable = cosine_schedule,\n\t        token_critic: Optional[TokenCritic] = None,\n\t        self_token_critic: bool = False,\n\t        vae: Optional[Union[VQGanVAE, VQGanVAETaming]] = None,\n\t        cond_vae: Optional[Union[VQGanVAE, VQGanVAETaming]] = None,\n\t        cond_image_size: Optional[int] = None,\n\t        cond_drop_prob: float = 0.5,\n", "        self_cond_prob: float = 0.9,\n\t        no_mask_token_prob: float = 0.0,\n\t        critic_loss_weight: float = 1.0,\n\t    ):\n\t        super().__init__()\n\t        self.accelerator = accelerator\n\t        self.vae = vae.copy_for_eval() if vae is not None else None\n\t        if cond_vae is not None:\n\t            if cond_image_size is None:\n\t                raise ValueError(\"cond_image_size must be specified if conditioning\")\n", "            self.cond_vae = cond_vae.eval()\n\t        else:\n\t            self.cond_vae = self.vae\n\t        self.image_size = image_size\n\t        self.cond_image_size = cond_image_size\n\t        self.resize_image_for_cond_image = exists(cond_image_size)\n\t        self.cond_drop_prob = cond_drop_prob\n\t        self.transformer = transformer\n\t        self.self_cond = transformer.self_cond\n\t        if not self.vae.codebook_size == self.cond_vae.codebook_size == transformer.num_tokens:\n", "            raise ValueError(\"transformer num_tokens must be set to be equal to the vae codebook size\")\n\t        self.mask_id = transformer.mask_id\n\t        self.noise_schedule = noise_schedule\n\t        if token_critic and self_token_critic:\n\t            raise ValueError(\"cannot have both self_token_critic and token_critic\")\n\t        self.token_critic = SelfCritic(transformer) if self_token_critic else token_critic\n\t        self.critic_loss_weight = critic_loss_weight\n\t        # self conditioning\n\t        self.self_cond_prob = self_cond_prob\n\t        # percentage of tokens to be [mask]ed to remain the same token, so that transformer produces better embeddings across all tokens as done in original BERT paper\n", "        # may be needed for self conditioning\n\t        self.no_mask_token_prob = no_mask_token_prob\n\t    @property\n\t    def device(self):\n\t        return self.accelerator.device if self.accelerator else next(self.parameters()).device\n\t    def save(self, path):\n\t        if self.accelerator:\n\t            self.accelerator.save(self.state_dict(), path)\n\t        else:\n\t            torch.save(self.state_dict(), path)\n", "    def load(self, path):\n\t        path = Path(path)\n\t        if not path.exists() and path.is_file():\n\t            raise ValueError(f\"cannot find file {path} (does not exist or is not a file)\")\n\t        state_dict = torch.load(str(path), map_location=\"cpu\")\n\t        self.load_state_dict(state_dict)\n\t    def print(self, *args, **kwargs):\n\t        return self.accelerator.print(*args, **kwargs) if self.accelerator else print(*args, **kwargs)\n\t    @torch.no_grad()\n\t    @eval_decorator\n", "    def generate(\n\t        self,\n\t        texts: List[str],\n\t        negative_texts: Optional[List[str]] = None,\n\t        cond_images: Optional[torch.Tensor] = None,\n\t        fmap_size=None,\n\t        temperature=1.0,\n\t        topk_filter_thres=0.9,\n\t        can_remask_prev_masked=False,\n\t        force_not_use_token_critic=False,\n", "        timesteps=18,  # ideal number of steps is 18 in maskgit paper\n\t        cond_scale=3,\n\t        critic_noise_scale=1,\n\t    ):\n\t        fmap_size = default(fmap_size, self.vae.get_encoded_fmap_size(self.image_size))\n\t        # begin with all image token ids masked\n\t        device = next(self.parameters()).device\n\t        seq_len = fmap_size**2\n\t        batch_size = len(texts)\n\t        shape = (batch_size, seq_len)\n", "        ids = torch.full(shape, self.mask_id, dtype=torch.long, device=device)\n\t        scores = torch.zeros(shape, dtype=torch.float32, device=device)\n\t        starting_temperature = temperature\n\t        cond_ids = None\n\t        text_embeds = self.transformer.encode_text(texts)\n\t        demask_fn = self.transformer.forward_with_cond_scale\n\t        # whether to use token critic for scores\n\t        use_token_critic = exists(self.token_critic) and not force_not_use_token_critic\n\t        if use_token_critic:\n\t            token_critic_fn = self.token_critic.forward_with_cond_scale\n", "        # negative prompting, as in paper\n\t        neg_text_embeds = None\n\t        if exists(negative_texts):\n\t            assert len(texts) == len(negative_texts)\n\t            neg_text_embeds = self.transformer.encode_text(negative_texts)\n\t            demask_fn = partial(\n\t                self.transformer.forward_with_neg_prompt,\n\t                neg_text_embeds=neg_text_embeds,\n\t            )\n\t            if use_token_critic:\n", "                token_critic_fn = partial(\n\t                    self.token_critic.forward_with_neg_prompt,\n\t                    neg_text_embeds=neg_text_embeds,\n\t                )\n\t        if self.resize_image_for_cond_image:\n\t            if cond_images is None:\n\t                raise ValueError(\"conditioning image must be passed in to generate for super res maskgit\")\n\t            with torch.no_grad():\n\t                _, cond_ids, _ = self.cond_vae.encode(cond_images)\n\t        self_cond_embed = None\n", "        for timestep, steps_until_x0 in tqdm(\n\t            zip(\n\t                torch.linspace(0, 1, timesteps, device=device),\n\t                reversed(range(timesteps)),\n\t            ),\n\t            total=timesteps,\n\t        ):\n\t            rand_mask_prob = self.noise_schedule(timestep)\n\t            num_token_masked = max(int((rand_mask_prob * seq_len).item()), 1)\n\t            masked_indices = scores.topk(num_token_masked, dim=-1).indices\n", "            ids = ids.scatter(1, masked_indices, self.mask_id)\n\t            logits, embed = demask_fn(\n\t                ids,\n\t                text_embeds=text_embeds,\n\t                self_cond_embed=self_cond_embed,\n\t                conditioning_token_ids=cond_ids,\n\t                cond_scale=cond_scale,\n\t                return_embed=True,\n\t            )\n\t            self_cond_embed = embed if self.self_cond else None\n", "            filtered_logits = top_k(logits, topk_filter_thres)\n\t            temperature = starting_temperature * (steps_until_x0 / timesteps)  # temperature is annealed\n\t            pred_ids = gumbel_sample(filtered_logits, temperature=temperature, dim=-1)\n\t            is_mask = ids == self.mask_id\n\t            ids = torch.where(is_mask, pred_ids, ids)\n\t            if use_token_critic:\n\t                scores = token_critic_fn(\n\t                    ids,\n\t                    text_embeds=text_embeds,\n\t                    conditioning_token_ids=cond_ids,\n", "                    cond_scale=cond_scale,\n\t                )\n\t                scores = rearrange(scores, \"... 1 -> ...\")\n\t                scores = scores + (uniform(scores.shape, device=device) - 0.5) * critic_noise_scale * (\n\t                    steps_until_x0 / timesteps\n\t                )\n\t            else:\n\t                probs_without_temperature = logits.softmax(dim=-1)\n\t                scores = 1 - probs_without_temperature.gather(2, pred_ids[..., None])\n\t                scores = rearrange(scores, \"... 1 -> ...\")\n", "                if not can_remask_prev_masked:\n\t                    scores = scores.masked_fill(~is_mask, -1e5)\n\t                else:\n\t                    assert (\n\t                        self.no_mask_token_prob > 0.0\n\t                    ), \"without training with some of the non-masked tokens forced to predict, not sure if the logits will be meaningful for these token\"\n\t        # get ids\n\t        ids = rearrange(ids, \"b (i j) -> b i j\", i=fmap_size, j=fmap_size)\n\t        if not exists(self.vae):\n\t            return ids\n", "        images = self.vae.decode_from_ids(ids)\n\t        return images\n\t    def forward(\n\t        self,\n\t        images_or_ids: torch.Tensor,\n\t        ignore_index=-1,\n\t        cond_images: Optional[torch.Tensor] = None,\n\t        cond_token_ids: Optional[torch.Tensor] = None,\n\t        texts: Optional[List[str]] = None,\n\t        text_embeds: Optional[torch.Tensor] = None,\n", "        cond_drop_prob=None,\n\t        train_only_generator=False,\n\t        sample_temperature=None,\n\t    ):\n\t        # tokenize if needed\n\t        if images_or_ids.dtype == torch.float:\n\t            if self.vae is None:\n\t                raise ValueError(\"you must pass in a vae if you want to train from raw images\")\n\t            if not all([height_or_width == self.image_size for height_or_width in images_or_ids.shape[-2:]]):\n\t                raise ValueError(\"the image you passed in is not of the correct dimensions\")\n", "            with torch.no_grad():\n\t                _, ids, _ = self.vae.encode(images_or_ids)\n\t        elif self.resize_image_for_cond_image is True:\n\t            raise ValueError(\n\t                \"you cannot pass in raw image token ids if you want autoresizing of images for conditioning\"\n\t            )\n\t        else:\n\t            ids = images_or_ids\n\t        # validate text embedding arguments\n\t        if text_embeds is not None and texts is not None:\n", "            raise ValueError(\"cannot pass in both text and text embeddings\")\n\t        elif text_embeds is None and texts is None:\n\t            raise ValueError(\"must pass in either text or text embeddings\")\n\t        # get some basic variables\n\t        ids = rearrange(ids, \"b ... -> b (...)\")\n\t        batch, seq_len, device, cond_drop_prob = (\n\t            *ids.shape,\n\t            ids.device,\n\t            default(cond_drop_prob, self.cond_drop_prob),\n\t        )\n", "        # take care of creating conditioning image if required\n\t        if self.resize_image_for_cond_image:\n\t            cond_images = F.interpolate(images_or_ids, self.cond_image_size, mode=\"nearest\")\n\t        # tokenize conditional images if needed\n\t        if cond_images is not None:\n\t            if cond_token_ids is not None:\n\t                raise ValueError(\n\t                    \"if conditioning on low resolution, cannot pass in both images and token ids\"\n\t                )\n\t            if self.cond_vae is None:\n", "                raise ValueError(\n\t                    \"you must pass in a cond vae if you want to condition on low resolution images\"\n\t                )\n\t            assert all(\n\t                [height_or_width == self.cond_image_size for height_or_width in cond_images.shape[-2:]]\n\t            )\n\t            with torch.no_grad():\n\t                _, cond_token_ids, _ = self.cond_vae.encode(cond_images)\n\t        # prepare mask\n\t        rand_time = uniform((batch,), device=self.device)\n", "        rand_mask_probs = self.noise_schedule(rand_time)\n\t        num_token_masked = (seq_len * rand_mask_probs).round().clamp(min=1)\n\t        mask_id = self.mask_id\n\t        batch_randperm = torch.rand((batch, seq_len), device=self.device).argsort(dim=-1)\n\t        mask = batch_randperm < rearrange(num_token_masked, \"b -> b 1\")\n\t        mask_id = self.transformer.mask_id\n\t        labels = torch.where(mask, ids, ignore_index)\n\t        if self.no_mask_token_prob > 0.0:\n\t            no_mask_mask = get_mask_subset_prob(mask, self.no_mask_token_prob)\n\t            mask &= ~no_mask_mask\n", "        x: torch.Tensor = torch.where(mask, mask_id, ids)\n\t        # encode text if needed\n\t        if text_embeds is None and texts is not None:\n\t            text_embeds = self.transformer.encode_text(texts)\n\t        # make sure we have text embeddings now\n\t        if text_embeds is None:\n\t            raise ValueError(\"No text embeddings found, if text was passed it did not encode correctly\")\n\t        # self conditioning\n\t        self_cond_embed = None\n\t        if self.transformer.self_cond and random() < self.self_cond_prob:\n", "            with torch.no_grad():\n\t                _, self_cond_embed = self.transformer(\n\t                    x,\n\t                    text_embeds=text_embeds,\n\t                    conditioning_token_ids=cond_token_ids,\n\t                    cond_drop_prob=0.0,\n\t                    return_embed=True,\n\t                )\n\t                self_cond_embed.detach_()\n\t        # get loss\n", "        ce_loss, logits = self.transformer(\n\t            x,\n\t            text_embeds=text_embeds,\n\t            self_cond_embed=self_cond_embed,\n\t            conditioning_token_ids=cond_token_ids,\n\t            labels=labels,\n\t            cond_drop_prob=cond_drop_prob,\n\t            ignore_index=ignore_index,\n\t            return_logits=True,\n\t        )\n", "        if isnan(ce_loss):\n\t            self.print(f\"ERROR: found NaN loss: {ce_loss}\")\n\t            raise ValueError(\"NaN loss\")\n\t        if not exists(self.token_critic) or train_only_generator:\n\t            return ce_loss\n\t        # token critic loss\n\t        sampled_ids = gumbel_sample(logits, temperature=default(sample_temperature, random()))\n\t        critic_input = torch.where(mask, sampled_ids, x)\n\t        critic_labels = (ids != critic_input).float()\n\t        bce_loss = self.token_critic(\n", "            critic_input,\n\t            text_embeds=text_embeds,\n\t            conditioning_token_ids=cond_token_ids,\n\t            labels=critic_labels,\n\t            cond_drop_prob=cond_drop_prob,\n\t        )\n\t        return ce_loss + self.critic_loss_weight * bce_loss\n\t# final Muse class\n\t@beartype\n\tclass Muse(nn.Module):\n", "    def __init__(self, base: MaskGit, superres: MaskGit):\n\t        super().__init__()\n\t        self.base_maskgit = base.eval()\n\t        assert superres.resize_image_for_cond_image\n\t        self.superres_maskgit = superres.eval()\n\t    @torch.no_grad()\n\t    def forward(\n\t        self,\n\t        texts: List[str],\n\t        cond_scale=3.0,\n", "        temperature=1.0,\n\t        timesteps=18,\n\t        superres_timesteps=None,\n\t        return_lowres=False,\n\t        return_pil_images=True,\n\t    ):\n\t        lowres_image = self.base_maskgit.generate(\n\t            texts=texts,\n\t            cond_scale=cond_scale,\n\t            temperature=temperature,\n", "            timesteps=timesteps,\n\t        )\n\t        superres_image = self.superres_maskgit.generate(\n\t            texts=texts,\n\t            cond_scale=cond_scale,\n\t            cond_images=lowres_image,\n\t            temperature=temperature,\n\t            timesteps=default(superres_timesteps, timesteps),\n\t        )\n\t        if return_pil_images:\n", "            lowres_image = list(map(T.ToPILImage(), lowres_image))\n\t            superres_image = list(map(T.ToPILImage(), superres_image))\n\t        if not return_lowres:\n\t            return superres_image\n\t        return superres_image, lowres_image\n"]}
{"filename": "muse_maskgit_pytorch/vqgan_vae_taming.py", "chunked_list": ["import copy\n\timport importlib\n\tfrom math import log, sqrt\n\tfrom pathlib import Path\n\tfrom urllib.parse import urlparse\n\timport requests\n\timport torch\n\timport torch.nn.functional as F\n\tfrom accelerate import Accelerator\n\tfrom einops import rearrange\n", "from omegaconf import DictConfig, OmegaConf\n\tfrom taming.models.vqgan import VQModel\n\tfrom torch import nn\n\tfrom tqdm_loggable.auto import tqdm\n\t# constants\n\tCACHE_PATH = Path.home().joinpath(\".cache/taming\")\n\tVQGAN_VAE_PATH = \"https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1\"\n\tVQGAN_VAE_CONFIG_PATH = \"https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1\"\n\t# helpers methods\n\tdef exists(val):\n", "    return val is not None\n\tdef default(val, d):\n\t    return val if exists(val) else d\n\tdef download(url, filename=None, root=CACHE_PATH, chunk_size=1024):\n\t    filename = default(filename, urlparse(url).path.split(\"/\")[-1])\n\t    root_dir = Path(root)\n\t    target_path = root_dir.joinpath(filename)\n\t    if target_path.exists():\n\t        if target_path.isfile():\n\t            return str(target_path)\n", "        raise RuntimeError(f\"{target_path} exists and is not a regular file\")\n\t    target_tmp = target_path.with_name(f\".{target_path.name}.tmp\")\n\t    resp = requests.get(url, stream=True)\n\t    resp.raise_for_status()\n\t    filesize = int(resp.headers.get(\"content-length\", 0))\n\t    with target_tmp.open(\"wb\") as f:\n\t        for data in tqdm(\n\t            resp.iter_content(chunk_size=chunk_size),\n\t            desc=filename,\n\t            total=filesize,\n", "            unit=\"iB\",\n\t            unit_scale=True,\n\t            unit_divisor=1024,\n\t        ):\n\t            f.write(data)\n\t    target_tmp.rename(target_path)\n\t    return target_path\n\t# VQGAN from Taming Transformers paper\n\t# https://arxiv.org/abs/2012.09841\n\tdef get_obj_from_str(string, reload=False):\n", "    module, cls = string.rsplit(\".\", 1)\n\t    if reload:\n\t        module_imp = importlib.import_module(module)\n\t        importlib.reload(module_imp)\n\t    return getattr(importlib.import_module(module, package=None), cls)\n\tdef instantiate_from_config(config):\n\t    if \"target\" not in config:\n\t        raise KeyError(\"Expected key `target` to instantiate.\")\n\t    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n\tclass VQGanVAETaming(nn.Module):\n", "    def __init__(self, vqgan_model_path=None, vqgan_config_path=None, accelerator: Accelerator = None):\n\t        super().__init__()\n\t        if accelerator is None:\n\t            accelerator = Accelerator()\n\t        # Download model if needed\n\t        if vqgan_model_path is None:\n\t            CACHE_PATH.mkdir(parents=True, exist_ok=True)\n\t            model_filename = \"vqgan.1024.model.ckpt\"\n\t            config_filename = \"vqgan.1024.config.yml\"\n\t            with accelerator.local_main_process_first():\n", "                config_path = download(VQGAN_VAE_CONFIG_PATH, config_filename)\n\t                model_path = download(VQGAN_VAE_PATH, model_filename)\n\t        else:\n\t            config_path = Path(vqgan_config_path)\n\t            model_path = Path(vqgan_model_path)\n\t        with accelerator.local_main_process_first():\n\t            config: DictConfig = OmegaConf.load(config_path)\n\t            model: VQModel = instantiate_from_config(config[\"model\"])\n\t            state = torch.load(model_path, map_location=\"cpu\")[\"state_dict\"]\n\t            model.load_state_dict(state, strict=False)\n", "        print(f\"Loaded VQGAN from {model_path} and {config_path}\")\n\t        self.model: VQModel = model\n\t        # f as used in https://github.com/CompVis/taming-transformers#overview-of-pretrained-models\n\t        f = config.model.params.ddconfig.resolution / config.model.params.ddconfig.attn_resolutions[0]\n\t        self.num_layers = int(log(f) / log(2))\n\t        self.channels = 3\n\t        self.image_size = 256\n\t        self.num_tokens = config.model.params.n_embed\n\t        self.is_gumbel = False  # isinstance(self.model, GumbelVQ)\n\t        self.codebook_size = config[\"model\"][\"params\"][\"n_embed\"]\n", "    @torch.no_grad()\n\t    def get_codebook_indices(self, img):\n\t        b = img.shape[0]\n\t        img = (2 * img) - 1\n\t        _, _, [_, _, indices] = self.model.encode(img)\n\t        if self.is_gumbel:\n\t            return rearrange(indices, \"b h w -> b (h w)\", b=b)\n\t        return rearrange(indices, \"(b n) -> b n\", b=b)\n\t    def get_encoded_fmap_size(self, image_size):\n\t        return image_size // (2**self.num_layers)\n", "    def decode_from_ids(self, img_seq):\n\t        img_seq = rearrange(img_seq, \"b h w -> b (h w)\")\n\t        b, n = img_seq.shape\n\t        one_hot_indices = F.one_hot(img_seq, num_classes=self.num_tokens).float()\n\t        z = (\n\t            one_hot_indices @ self.model.quantize.embed.weight\n\t            if self.is_gumbel\n\t            else (one_hot_indices @ self.model.quantize.embedding.weight)\n\t        )\n\t        z = rearrange(z, \"b (h w) c -> b c h w\", h=int(sqrt(n)))\n", "        img = self.model.decode(z)\n\t        img = (img.clamp(-1.0, 1.0) + 1) * 0.5\n\t        return img\n\t    def encode(self, im_seq):\n\t        # encode output\n\t        # fmap, loss, (perplexity, min_encodings, min_encodings_indices) = self.model.encode(im_seq)\n\t        fmap, loss, (_, _, min_encodings_indices) = self.model.encode(im_seq)\n\t        b, _, h, w = fmap.shape\n\t        min_encodings_indices = rearrange(min_encodings_indices, \"(b h w) -> b h w\", h=h, w=w, b=b)\n\t        return fmap, min_encodings_indices, loss\n", "    def decode_ids(self, ids):\n\t        return self.model.decode_code(ids)\n\t    def copy_for_eval(self):\n\t        device = next(self.parameters()).device\n\t        vae_copy = copy.deepcopy(self.cpu())\n\t        vae_copy.eval()\n\t        return vae_copy.to(device)\n\t    def forward(self, img):\n\t        raise NotImplementedError(\"Forward not implemented for Taming VAE\")\n"]}
{"filename": "muse_maskgit_pytorch/dataset.py", "chunked_list": ["import os\n\timport random\n\timport shutil\n\timport sys\n\timport time\n\tfrom pathlib import Path\n\tfrom threading import Thread\n\timport datasets\n\timport torch\n\tfrom datasets import Image, load_from_disk\n", "from PIL import (\n\t    Image as pImage,\n\t    ImageFile,\n\t)\n\tfrom torch.utils.data import DataLoader, Dataset, random_split\n\tfrom torchvision import transforms as T\n\ttry:\n\t    import torch_xla\n\t    import torch_xla.core.xla_model as xm\n\t    from tqdm_loggable.auto import tqdm\n", "except ImportError:\n\t    from tqdm import tqdm\n\tfrom io import BytesIO\n\timport requests\n\tfrom transformers import T5Tokenizer\n\tfrom muse_maskgit_pytorch.t5 import MAX_LENGTH\n\tImageFile.LOAD_TRUNCATED_IMAGES = True\n\tpImage.MAX_IMAGE_PIXELS = None\n\tclass ImageDataset(Dataset):\n\t    def __init__(\n", "        self,\n\t        dataset,\n\t        image_size,\n\t        image_column=\"image\",\n\t        flip=True,\n\t        center_crop=True,\n\t        stream=False,\n\t        using_taming=False,\n\t        random_crop=False,\n\t        alpha_channel=True,\n", "    ):\n\t        super().__init__()\n\t        self.dataset = dataset\n\t        self.image_column = image_column\n\t        self.stream = stream\n\t        transform_list = [\n\t            T.Resize(image_size),\n\t        ]\n\t        if flip:\n\t            transform_list.append(T.RandomHorizontalFlip())\n", "        if center_crop and not random_crop:\n\t            transform_list.append(T.CenterCrop(image_size))\n\t        if random_crop:\n\t            transform_list.append(T.RandomCrop(image_size, pad_if_needed=True))\n\t        if alpha_channel:\n\t            transform_list.append(T.Lambda(lambda img: img.convert(\"RGBA\") if img.mode != \"RGBA\" else img))\n\t        else:\n\t            transform_list.append(T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img))\n\t        transform_list.append(T.ToTensor())\n\t        self.transform = T.Compose(transform_list)\n", "        self.using_taming = using_taming\n\t    def __len__(self):\n\t        if not self.stream:\n\t            return len(self.dataset)\n\t        else:\n\t            raise AssertionError(\"Streaming doesnt support grabbing dataset length\")\n\t    def __getitem__(self, index):\n\t        image = self.dataset[index][self.image_column]\n\t        if self.using_taming:\n\t            return self.transform(image) - 0.5\n", "        else:\n\t            return self.transform(image)\n\tclass ImageTextDataset(ImageDataset):\n\t    def __init__(\n\t        self,\n\t        dataset,\n\t        image_size: int,\n\t        tokenizer: T5Tokenizer,\n\t        image_column=\"image\",\n\t        caption_column=\"caption\",\n", "        flip=True,\n\t        center_crop=True,\n\t        stream=False,\n\t        using_taming=False,\n\t        random_crop=False,\n\t    ):\n\t        super().__init__(\n\t            dataset,\n\t            image_size=image_size,\n\t            image_column=image_column,\n", "            flip=flip,\n\t            stream=stream,\n\t            center_crop=center_crop,\n\t            using_taming=using_taming,\n\t            random_crop=random_crop,\n\t        )\n\t        self.caption_column: str = caption_column\n\t        self.tokenizer: T5Tokenizer = tokenizer\n\t    def __getitem__(self, index):\n\t        image = self.dataset[index][self.image_column]\n", "        descriptions = self.dataset[index][self.caption_column]\n\t        if self.caption_column is None or descriptions is None:\n\t            text = \"\"\n\t        elif isinstance(descriptions, list):\n\t            if len(descriptions) == 0:\n\t                text = \"\"\n\t            else:\n\t                text = random.choice(descriptions)\n\t        else:\n\t            text = descriptions\n", "        # max length from the paper\n\t        encoded = self.tokenizer.batch_encode_plus(\n\t            [str(text)],\n\t            return_tensors=\"pt\",\n\t            padding=\"max_length\",\n\t            max_length=MAX_LENGTH,\n\t            truncation=True,\n\t        )\n\t        input_ids = encoded.input_ids\n\t        attn_mask = encoded.attention_mask\n", "        if self.using_taming:\n\t            return self.transform(image) - 0.5, input_ids[0], attn_mask[0]\n\t        else:\n\t            return self.transform(image), input_ids[0], attn_mask[0]\n\tclass URLTextDataset(ImageDataset):\n\t    def __init__(\n\t        self,\n\t        dataset,\n\t        image_size: int,\n\t        tokenizer: T5Tokenizer,\n", "        image_column=\"image\",\n\t        caption_column=\"caption\",\n\t        flip=True,\n\t        center_crop=True,\n\t        using_taming=True,\n\t    ):\n\t        super().__init__(\n\t            dataset,\n\t            image_size=image_size,\n\t            image_column=image_column,\n", "            flip=flip,\n\t            center_crop=center_crop,\n\t            using_taming=using_taming,\n\t        )\n\t        self.caption_column: str = caption_column\n\t        self.tokenizer: T5Tokenizer = tokenizer\n\t    def __getitem__(self, index):\n\t        try:\n\t            image = pImage.open(BytesIO(requests.get(self.dataset[index][self.image_column]).content))\n\t        except ConnectionError:\n", "            try:\n\t                print(\"Image request failure, attempting next image\")\n\t                index += 1\n\t                image = pImage.open(BytesIO(requests.get(self.dataset[index][self.image_column]).content))\n\t            except ConnectionError:\n\t                raise ConnectionError(\"Unable to request image from the Dataset\")\n\t        descriptions = self.dataset[index][self.caption_column]\n\t        if self.caption_column is None or descriptions is None:\n\t            text = \"\"\n\t        elif isinstance(descriptions, list):\n", "            if len(descriptions) == 0:\n\t                text = \"\"\n\t            else:\n\t                text = random.choice(descriptions)\n\t        else:\n\t            text = descriptions\n\t        # max length from the paper\n\t        encoded = self.tokenizer.batch_encode_plus(\n\t            [str(text)],\n\t            return_tensors=\"pt\",\n", "            padding=\"max_length\",\n\t            max_length=MAX_LENGTH,\n\t            truncation=True,\n\t        )\n\t        input_ids = encoded.input_ids\n\t        attn_mask = encoded.attention_mask\n\t        if self.using_taming:\n\t            return self.transform(image) - 0.5, input_ids[0], attn_mask[0]\n\t        else:\n\t            return self.transform(image), input_ids[0], attn_mask[0]\n", "class LocalTextImageDataset(Dataset):\n\t    def __init__(\n\t        self,\n\t        path,\n\t        image_size,\n\t        tokenizer,\n\t        flip=True,\n\t        center_crop=True,\n\t        using_taming=False,\n\t        random_crop=False,\n", "        alpha_channel=False,\n\t    ):\n\t        super().__init__()\n\t        self.tokenizer = tokenizer\n\t        self.using_taming = using_taming\n\t        print(\"Building dataset...\")\n\t        extensions = [\"jpg\", \"jpeg\", \"png\", \"webp\"]\n\t        self.image_paths = []\n\t        self.caption_pair = []\n\t        self.images = []\n", "        for ext in extensions:\n\t            self.image_paths.extend(list(Path(path).rglob(f\"*.{ext}\")))\n\t        random.shuffle(self.image_paths)\n\t        for image_path in tqdm(self.image_paths):\n\t            # check image size and ignore images with 0 byte.\n\t            if os.path.getsize(image_path) == 0:\n\t                continue\n\t            caption_path = image_path.with_suffix(\".txt\")\n\t            if os.path.exists(str(caption_path)):\n\t                captions = str(caption_path)\n", "            else:\n\t                captions = \"\"\n\t            self.images.append(image_path)\n\t            self.caption_pair.append(captions)\n\t        transform_list = [\n\t            T.Resize(image_size),\n\t        ]\n\t        if flip:\n\t            transform_list.append(T.RandomHorizontalFlip())\n\t        if center_crop and not random_crop:\n", "            transform_list.append(T.CenterCrop(image_size))\n\t        if random_crop:\n\t            transform_list.append(T.RandomCrop(image_size, pad_if_needed=True))\n\t        if alpha_channel:\n\t            transform_list.append(T.Lambda(lambda img: img.convert(\"RGBA\") if img.mode != \"RGBA\" else img))\n\t        else:\n\t            transform_list.append(T.Lambda(lambda img: img.convert(\"RGB\") if img.mode != \"RGB\" else img))\n\t        transform_list.append(T.ToTensor())\n\t        self.transform = T.Compose(transform_list)\n\t    def __len__(self):\n", "        return len(self.caption_pair)\n\t    def __getitem__(self, index):\n\t        image = self.images[index]\n\t        image = pImage.open(image)\n\t        descriptions = self.caption_pair[index]\n\t        if descriptions is None or descriptions == \"\":\n\t            text = \"\"\n\t        else:\n\t            text = Path(descriptions).read_text(encoding=\"utf-8\").split(\"\\n\")\n\t        # max length from the paper\n", "        encoded = self.tokenizer.batch_encode_plus(\n\t            [str(text)],\n\t            return_tensors=\"pt\",\n\t            padding=\"max_length\",\n\t            max_length=MAX_LENGTH,\n\t            truncation=True,\n\t        )\n\t        input_ids = encoded.input_ids\n\t        attn_mask = encoded.attention_mask\n\t        if self.using_taming:\n", "            return self.transform(image) - 0.5, input_ids[0], attn_mask[0]\n\t        else:\n\t            return self.transform(image), input_ids[0], attn_mask[0]\n\tdef get_directory_size(path):\n\t    total_size = 0\n\t    for dirpath, dirnames, filenames in os.walk(path):\n\t        for f in filenames:\n\t            fp = os.path.join(dirpath, f)\n\t            total_size += os.path.getsize(fp)\n\t    return total_size\n", "def save_dataset_with_progress(dataset, save_path):\n\t    # Estimate the total size of the dataset in bytes\n\t    total_size = sys.getsizeof(dataset)\n\t    # Start saving the dataset in a separate thread\n\t    save_thread = Thread(target=dataset.save_to_disk, args=(save_path,))\n\t    save_thread.start()\n\t    # Create a tqdm progress bar and update it periodically\n\t    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as pbar:\n\t        while save_thread.is_alive():\n\t            if os.path.exists(save_path):\n", "                size = get_directory_size(save_path)\n\t                # Update the progress bar based on the current size of the saved file\n\t                pbar.update(size - pbar.n)  # Update by the difference between current and previous size\n\t            time.sleep(1)\n\tdef get_dataset_from_dataroot(\n\t    data_root,\n\t    image_column=\"image\",\n\t    caption_column=\"caption\",\n\t    save_path=\"dataset\",\n\t    save=True,\n", "):\n\t    # Check if data_root is a symlink and resolve it to its target location if it is\n\t    if os.path.islink(data_root):\n\t        data_root = os.path.realpath(data_root)\n\t    if os.path.exists(save_path):\n\t        # Get the modified time of save_path\n\t        save_path_mtime = os.stat(save_path).st_mtime\n\t        if save:\n\t            # Traverse the directory tree of data_root and get the modified time of all files and subdirectories\n\t            print(\"Checking modified date of all the files and subdirectories in the dataset folder.\")\n", "            data_root_mtime = max(\n\t                os.stat(os.path.join(root, f)).st_mtime\n\t                for root, dirs, files in os.walk(data_root)\n\t                for f in files + dirs\n\t            )\n\t            # Check if data_root is newer than save_path\n\t            if data_root_mtime > save_path_mtime:\n\t                print(\n\t                    \"The data_root folder has being updated recently. Removing previously saved dataset and updating it.\"\n\t                )\n", "                shutil.rmtree(save_path, ignore_errors=True)\n\t            else:\n\t                print(\"The dataset is up-to-date. Loading...\")\n\t                # Load the dataset from save_path if it is up-to-date\n\t                return load_from_disk(save_path)\n\t    extensions = [\"jpg\", \"jpeg\", \"png\", \"webp\"]\n\t    image_paths = []\n\t    for ext in extensions:\n\t        image_paths.extend(list(Path(data_root).rglob(f\"*.{ext}\")))\n\t    random.shuffle(image_paths)\n", "    data_dict = {image_column: [], caption_column: []}\n\t    for image_path in tqdm(image_paths):\n\t        # check image size and ignore images with 0 byte.\n\t        if os.path.getsize(image_path) == 0:\n\t            continue\n\t        caption_path = image_path.with_suffix(\".txt\")\n\t        if os.path.exists(str(caption_path)):\n\t            captions = caption_path.read_text(encoding=\"utf-8\").split(\"\\n\")\n\t            captions = list(filter(lambda t: len(t) > 0, captions))\n\t        else:\n", "            captions = []\n\t        image_path = str(image_path)\n\t        data_dict[image_column].append(image_path)\n\t        data_dict[caption_column].append(captions)\n\t    dataset = datasets.Dataset.from_dict(data_dict)\n\t    dataset = dataset.cast_column(image_column, Image())\n\t    if save:\n\t        save_dataset_with_progress(dataset, save_path)\n\t    return dataset\n\tdef split_dataset_into_dataloaders(dataset, valid_frac=0.05, seed=42, batch_size=1):\n", "    print(f\"Dataset length: {len(dataset)} samples\")\n\t    if valid_frac > 0:\n\t        train_size = int((1 - valid_frac) * len(dataset))\n\t        valid_size = len(dataset) - train_size\n\t        print(f\"Splitting dataset into {train_size} training samples and {valid_size} validation samples\")\n\t        split_generator = torch.Generator().manual_seed(seed)\n\t        train_dataset, validation_dataset = random_split(\n\t            dataset,\n\t            [train_size, valid_size],\n\t            generator=split_generator,\n", "        )\n\t    else:\n\t        print(\"Using shared dataset for training and validation\")\n\t        train_dataset = dataset\n\t        validation_dataset = dataset\n\t    dataloader = DataLoader(\n\t        train_dataset,\n\t        batch_size=batch_size,\n\t        shuffle=True,\n\t    )\n", "    validation_dataloader = DataLoader(\n\t        validation_dataset,\n\t        batch_size=batch_size,\n\t        shuffle=False,\n\t    )\n\t    return dataloader, validation_dataloader\n"]}
{"filename": "muse_maskgit_pytorch/__init__.py", "chunked_list": ["from .muse_maskgit_pytorch import MaskGit, MaskGitTransformer, Muse, TokenCritic, Transformer\n\tfrom .trainers import MaskGitTrainer, VQGanVAETrainer, get_accelerator\n\tfrom .vqgan_vae import VQGanVAE\n\tfrom .vqgan_vae_taming import VQGanVAETaming\n\t__all__ = [\n\t    \"VQGanVAE\",\n\t    \"VQGanVAETaming\",\n\t    \"Transformer\",\n\t    \"MaskGit\",\n\t    \"Muse\",\n", "    \"MaskGitTransformer\",\n\t    \"TokenCritic\",\n\t    \"VQGanVAETrainer\",\n\t    \"MaskGitTrainer\",\n\t    \"get_accelerator\",\n\t]\n"]}
{"filename": "muse_maskgit_pytorch/vqgan_vae.py", "chunked_list": ["import copy\n\tfrom functools import partial, wraps\n\tfrom pathlib import Path\n\timport timm\n\timport torch\n\timport torch.nn.functional as F\n\timport torchvision\n\tfrom accelerate import Accelerator\n\tfrom beartype import beartype\n\tfrom einops import rearrange, repeat\n", "from torch import nn\n\tfrom torch.autograd import grad as torch_grad\n\tfrom vector_quantize_pytorch import VectorQuantize as VQ\n\t# constants\n\tMList = nn.ModuleList\n\t# helper functions\n\tdef exists(val):\n\t    return val is not None\n\tdef default(val, d):\n\t    return val if val is not None else d\n", "# decorators\n\tdef eval_decorator(fn):\n\t    def inner(model, *args, **kwargs):\n\t        was_training = model.training\n\t        model.eval()\n\t        out = fn(model, *args, **kwargs)\n\t        model.train(was_training)\n\t        return out\n\t    return inner\n\tdef remove_vgg(fn):\n", "    @wraps(fn)\n\t    def inner(self, *args, **kwargs):\n\t        has_vgg = hasattr(self, \"_vgg\")\n\t        if has_vgg:\n\t            vgg = self._vgg\n\t            delattr(self, \"_vgg\")\n\t        out = fn(self, *args, **kwargs)\n\t        if has_vgg:\n\t            self._vgg = vgg\n\t        return out\n", "    return inner\n\t# keyword argument helpers\n\tdef pick_and_pop(keys, d):\n\t    values = list(map(lambda key: d.pop(key), keys))\n\t    return dict(zip(keys, values))\n\tdef group_dict_by_key(cond, d):\n\t    return_val = [dict(), dict()]\n\t    for key in d.keys():\n\t        match = bool(cond(key))\n\t        ind = int(not match)\n", "        return_val[ind][key] = d[key]\n\t    return (*return_val,)\n\tdef string_begins_with(prefix, string_input):\n\t    return string_input.startswith(prefix)\n\tdef group_by_key_prefix(prefix, d):\n\t    return group_dict_by_key(partial(string_begins_with, prefix), d)\n\tdef groupby_prefix_and_trim(prefix, d):\n\t    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n\t    kwargs_without_prefix = dict(\n\t        map(lambda x: (x[0][len(prefix) :], x[1]), tuple(kwargs_with_prefix.items()))\n", "    )\n\t    return kwargs_without_prefix, kwargs\n\t# tensor helper functions\n\tdef log(t, eps=1e-10):\n\t    return torch.log(t + eps)\n\tdef gradient_penalty(images, output, weight=10):\n\t    gradients = torch_grad(\n\t        outputs=output,\n\t        inputs=images,\n\t        grad_outputs=torch.ones(output.size(), device=images.device),\n", "        create_graph=True,\n\t        retain_graph=True,\n\t        only_inputs=True,\n\t    )[0]\n\t    gradients = rearrange(gradients, \"b ... -> b (...)\")\n\t    return weight * ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n\tdef leaky_relu(p: float = 0.1):\n\t    return nn.LeakyReLU(p)\n\tdef safe_div(numer, denom, eps=1e-8):\n\t    return numer / denom.clamp(min=eps)\n", "# gan losses\n\tdef hinge_discr_loss(fake, real):\n\t    return (F.relu(1 + fake) + F.relu(1 - real)).mean()\n\tdef hinge_gen_loss(fake):\n\t    return -fake.mean()\n\tdef bce_discr_loss(fake, real):\n\t    return (-log(1 - torch.sigmoid(fake)) - log(torch.sigmoid(real))).mean()\n\tdef bce_gen_loss(fake):\n\t    return -log(torch.sigmoid(fake)).mean()\n\tdef grad_layer_wrt_loss(loss, layer):\n", "    return torch_grad(\n\t        outputs=loss,\n\t        inputs=layer,\n\t        grad_outputs=torch.ones_like(loss),\n\t        retain_graph=True,\n\t    )[0].detach()\n\t# vqgan vae\n\tclass LayerNormChan(nn.Module):\n\t    def __init__(self, dim, eps=1e-5):\n\t        super().__init__()\n", "        self.eps = eps\n\t        self.gamma = nn.Parameter(torch.ones(1, dim, 1, 1))\n\t    def forward(self, x):\n\t        var = torch.var(x, dim=1, unbiased=False, keepdim=True)\n\t        mean = torch.mean(x, dim=1, keepdim=True)\n\t        return (x - mean) * var.clamp(min=self.eps).rsqrt() * self.gamma\n\t# discriminator\n\tclass Discriminator(nn.Module):\n\t    def __init__(self, dims, channels=4, groups=16, init_kernel_size=5):\n\t        super().__init__()\n", "        dim_pairs = zip(dims[:-1], dims[1:])\n\t        self.layers = MList(\n\t            [\n\t                nn.Sequential(\n\t                    nn.Conv2d(channels, dims[0], init_kernel_size, padding=init_kernel_size // 2),\n\t                    leaky_relu(),\n\t                )\n\t            ]\n\t        )\n\t        for dim_in, dim_out in dim_pairs:\n", "            self.layers.append(\n\t                nn.Sequential(\n\t                    nn.Conv2d(dim_in, dim_out, 4, stride=2, padding=1),\n\t                    nn.GroupNorm(groups, dim_out),\n\t                    leaky_relu(),\n\t                )\n\t            )\n\t        dim = dims[-1]\n\t        # return 5 x 5, for PatchGAN-esque training\n\t        self.to_logits = nn.Sequential(nn.Conv2d(dim, dim, 1), leaky_relu(), nn.Conv2d(dim, 1, 4))\n", "    def forward(self, x):\n\t        for net in self.layers:\n\t            x = net(x)\n\t        return self.to_logits(x)\n\t# resnet encoder / decoder\n\tclass ResnetEncDec(nn.Module):\n\t    def __init__(\n\t        self,\n\t        dim: int,\n\t        *,\n", "        channels=4,\n\t        layers=4,\n\t        layer_mults=None,\n\t        num_resnet_blocks=1,\n\t        resnet_groups=16,\n\t        first_conv_kernel_size=5,\n\t    ):\n\t        super().__init__()\n\t        assert (\n\t            dim % resnet_groups == 0\n", "        ), f\"dimension {dim} must be divisible by {resnet_groups} (groups for the groupnorm)\"\n\t        self.layers = layers\n\t        self.encoders = MList([])\n\t        self.decoders = MList([])\n\t        layer_mults = default(layer_mults, [2**x for x in range(layers)])\n\t        if len(layer_mults) != layers:\n\t            raise ValueError(\"layer multipliers must be equal to designated number of layers\")\n\t        layer_dims = [dim * mult for mult in layer_mults]\n\t        dims = (dim, *layer_dims)\n\t        self.encoded_dim = dims[-1]\n", "        dim_pairs = zip(dims[:-1], dims[1:])\n\t        if not isinstance(num_resnet_blocks, tuple):\n\t            num_resnet_blocks = (*((0,) * (layers - 1)), num_resnet_blocks)\n\t        if len(num_resnet_blocks) != layers:\n\t            raise ValueError(\"number of resnet blocks must be equal to number of layers\")\n\t        for _, (dim_in, dim_out), layer_num_resnet_blocks in zip(range(layers), dim_pairs, num_resnet_blocks):\n\t            self.encoders.append(\n\t                nn.Sequential(nn.Conv2d(dim_in, dim_out, 4, stride=2, padding=1), leaky_relu())\n\t            )\n\t            self.decoders.insert(0, nn.Sequential(nn.ConvTranspose2d(dim_out, dim_in, 4, 2, 1), leaky_relu()))\n", "            for _ in range(layer_num_resnet_blocks):\n\t                self.encoders.append(ResBlock(dim_out, groups=resnet_groups))\n\t                self.decoders.insert(0, GLUResBlock(dim_out, groups=resnet_groups))\n\t        self.encoders.insert(\n\t            0, nn.Conv2d(channels, dim, first_conv_kernel_size, padding=first_conv_kernel_size // 2)\n\t        )\n\t        self.decoders.append(nn.Conv2d(dim, channels, 1))\n\t    def get_encoded_fmap_size(self, image_size: int):\n\t        return image_size // (2**self.layers)\n\t    @property\n", "    def last_dec_layer(self):\n\t        return self.decoders[-1].weight\n\t    def encode(self, x):\n\t        for enc in self.encoders:\n\t            x = enc(x)\n\t        return x\n\t    def decode(self, x):\n\t        for dec in self.decoders:\n\t            x = dec(x)\n\t        return x\n", "class GLUResBlock(nn.Module):\n\t    def __init__(self, chan, groups=16):\n\t        super().__init__()\n\t        self.net = nn.Sequential(\n\t            nn.Conv2d(chan, chan * 2, 3, padding=1),\n\t            nn.GLU(dim=1),\n\t            nn.GroupNorm(groups, chan),\n\t            nn.Conv2d(chan, chan * 2, 3, padding=1),\n\t            nn.GLU(dim=1),\n\t            nn.GroupNorm(groups, chan),\n", "            nn.Conv2d(chan, chan, 1),\n\t        )\n\t    def forward(self, x):\n\t        return self.net(x) + x\n\tclass ResBlock(nn.Module):\n\t    def __init__(self, chan, groups=16):\n\t        super().__init__()\n\t        self.net = nn.Sequential(\n\t            nn.Conv2d(chan, chan, 3, padding=1),\n\t            nn.GroupNorm(groups, chan),\n", "            leaky_relu(),\n\t            nn.Conv2d(chan, chan, 3, padding=1),\n\t            nn.GroupNorm(groups, chan),\n\t            leaky_relu(),\n\t            nn.Conv2d(chan, chan, 1),\n\t        )\n\t    def forward(self, x):\n\t        return self.net(x) + x\n\tclass TimmFeatureEncDec(nn.Module):\n\t    def __init__(self, backbone=\"convnext_base\"):\n", "        self.timm_model = timm.create_model(\n\t            backbone,\n\t            pretrained=True,\n\t            features_only=True,\n\t            exportable=True,\n\t            out_indices=self.idx,\n\t        )\n\t        return\n\t    def encode(self, x):\n\t        for enc in self.encoders:\n", "            x = enc(x)\n\t        return x\n\t    def decode(self, x):\n\t        for dec in self.decoders:\n\t            x = dec(x)\n\t        return x\n\tclass HuggingfaceEncDec(nn.Module):\n\t    def __init__(self):\n\t        return\n\t    def forward(self):\n", "        return\n\tclass WaveletTransformerEncDec(nn.Module):\n\t    def __init__(self):\n\t        return\n\t    def forward(self):\n\t        return\n\t# main vqgan-vae classes\n\t@beartype\n\tclass VQGanVAE(nn.Module):\n\t    def __init__(\n", "        self,\n\t        *,\n\t        dim: int,\n\t        accelerator: Accelerator = None,\n\t        channels=4,\n\t        layers=4,\n\t        l2_recon_loss=False,\n\t        use_hinge_loss=True,\n\t        vgg=None,\n\t        vq_codebook_dim=256,\n", "        vq_codebook_size=512,\n\t        vq_decay=0.8,\n\t        vq_commitment_weight=1.0,\n\t        vq_kmeans_init=True,\n\t        vq_use_cosine_sim=True,\n\t        use_vgg_and_gan=True,\n\t        discr_layers=4,\n\t        **kwargs,\n\t    ):\n\t        super().__init__()\n", "        vq_kwargs, kwargs = groupby_prefix_and_trim(\"vq_\", kwargs)\n\t        encdec_kwargs, kwargs = groupby_prefix_and_trim(\"encdec_\", kwargs)\n\t        self.accelerator = accelerator\n\t        self.channels = channels\n\t        self.codebook_size = vq_codebook_size\n\t        self.dim_divisor = 2**layers\n\t        self.enc_dec = ResnetEncDec(dim=dim, channels=channels, layers=layers, **encdec_kwargs)\n\t        self.vq = VQ(\n\t            dim=self.enc_dec.encoded_dim,\n\t            codebook_dim=vq_codebook_dim,\n", "            codebook_size=vq_codebook_size,\n\t            decay=vq_decay,\n\t            commitment_weight=vq_commitment_weight,\n\t            accept_image_fmap=True,\n\t            kmeans_init=vq_kmeans_init,\n\t            use_cosine_sim=vq_use_cosine_sim,\n\t            **vq_kwargs,\n\t        )\n\t        # reconstruction loss\n\t        self.recon_loss_fn = F.mse_loss if l2_recon_loss else F.l1_loss\n", "        # turn off GAN and perceptual loss if grayscale\n\t        self._vgg = None\n\t        self.discr = None\n\t        self.use_vgg_and_gan = use_vgg_and_gan\n\t        if not use_vgg_and_gan:\n\t            return\n\t        # preceptual loss\n\t        if exists(vgg):\n\t            self._vgg = vgg\n\t        # gan related losses\n", "        layer_mults = list(map(lambda t: 2**t, range(discr_layers)))\n\t        layer_dims = [dim * mult for mult in layer_mults]\n\t        dims = (dim, *layer_dims)\n\t        self.discr = Discriminator(dims=dims, channels=channels)\n\t        self.discr_loss = hinge_discr_loss if use_hinge_loss else bce_discr_loss\n\t        self.gen_loss = hinge_gen_loss if use_hinge_loss else bce_gen_loss\n\t    @property\n\t    def device(self):\n\t        return self.accelerator.device if self.accelerator else next(self.parameters()).device\n\t    @property\n", "    def vgg(self):\n\t        if exists(self._vgg):\n\t            return self._vgg\n\t        vgg = torchvision.models.vgg16(pretrained=True)\n\t        vgg.features[0] = nn.Conv2d(self.channels, 64, kernel_size=3, stride=1, padding=1)\n\t        vgg.classifier = nn.Sequential(*vgg.classifier[:-2])\n\t        self._vgg = vgg.to(self.device)\n\t        return self._vgg\n\t    @property\n\t    def encoded_dim(self):\n", "        return self.enc_dec.encoded_dim\n\t    def get_encoded_fmap_size(self, image_size):\n\t        return self.enc_dec.get_encoded_fmap_size(image_size)\n\t    def copy_for_eval(self):\n\t        device = next(self.parameters()).device\n\t        vae_copy = copy.deepcopy(self.cpu())\n\t        if vae_copy.use_vgg_and_gan:\n\t            del vae_copy.discr\n\t            del vae_copy._vgg\n\t        vae_copy.eval()\n", "        return vae_copy.to(device)\n\t    @remove_vgg\n\t    def state_dict(self, *args, **kwargs):\n\t        return super().state_dict(*args, **kwargs)\n\t    @remove_vgg\n\t    def load_state_dict(self, *args, **kwargs):\n\t        return super().load_state_dict(*args, **kwargs)\n\t    def save(self, path):\n\t        if self.accelerator is not None:\n\t            self.accelerator.save(self.state_dict(), path)\n", "        else:\n\t            torch.save(self.state_dict(), path)\n\t    def load(self, path, map=None):\n\t        path = Path(path)\n\t        assert path.exists()\n\t        state_dict = torch.load(str(path), map_location=map)\n\t        self.load_state_dict(state_dict)\n\t    @property\n\t    def codebook(self):\n\t        return self.vq.codebook\n", "    def encode(self, fmap):\n\t        fmap = self.enc_dec.encode(fmap)\n\t        fmap, indices, commit_loss = self.vq(fmap)\n\t        return fmap, indices, commit_loss\n\t    def decode_from_ids(self, ids):\n\t        codes = self.codebook[ids]\n\t        fmap = self.vq.project_out(codes)\n\t        fmap = rearrange(fmap, \"b h w c -> b c h w\")\n\t        return self.decode(fmap)\n\t    def decode(self, fmap):\n", "        return self.enc_dec.decode(fmap)\n\t    def forward(\n\t        self,\n\t        img,\n\t        return_loss=False,\n\t        return_discr_loss=False,\n\t        return_recons=False,\n\t        add_gradient_penalty=True,\n\t        relu_loss=True,\n\t    ):\n", "        batch, channels, height, width, device = *img.shape, img.device\n\t        for dim_name, size in ((\"height\", height), (\"width\", width)):\n\t            assert (size % self.dim_divisor) == 0, f\"{dim_name} must be divisible by {self.dim_divisor}\"\n\t        assert (\n\t            channels == self.channels\n\t        ), \"number of channels on image or sketch is not equal to the channels set on this VQGanVAE\"\n\t        fmap, indices, commit_loss = self.encode(img)\n\t        fmap = self.decode(fmap)\n\t        if not return_loss and not return_discr_loss:\n\t            return fmap\n", "        assert (\n\t            return_loss ^ return_discr_loss\n\t        ), \"you should either return autoencoder loss or discriminator loss, but not both\"\n\t        # whether to return discriminator loss\n\t        if return_discr_loss:\n\t            assert exists(self.discr), \"discriminator must exist to train it\"\n\t            fmap.detach_()\n\t            img.requires_grad_()\n\t            fmap_discr_logits, img_discr_logits = map(self.discr, (fmap, img))\n\t            discr_loss = self.discr_loss(fmap_discr_logits, img_discr_logits)\n", "            if add_gradient_penalty:\n\t                gp = gradient_penalty(img, img_discr_logits)\n\t                loss = discr_loss + gp\n\t            if return_recons:\n\t                if relu_loss:\n\t                    return F.relu(loss), fmap\n\t                else:\n\t                    return loss, fmap\n\t            if relu_loss:\n\t                return F.relu(loss)\n", "            else:\n\t                return loss\n\t        # reconstruction loss\n\t        recon_loss = self.recon_loss_fn(fmap, img)\n\t        # early return if training on grayscale\n\t        if not self.use_vgg_and_gan:\n\t            if return_recons:\n\t                if relu_loss:\n\t                    return F.relu(recon_loss), fmap\n\t                else:\n", "                    return recon_loss, fmap\n\t            if relu_loss:\n\t                return F.relu(recon_loss)\n\t            else:\n\t                return recon_loss\n\t        # perceptual loss\n\t        img_vgg_input = img\n\t        fmap_vgg_input = fmap\n\t        if img.shape[1] == 1:\n\t            # handle grayscale for vgg\n", "            img_vgg_input, fmap_vgg_input = map(\n\t                lambda t: repeat(t, \"b 1 ... -> b c ...\", c=3),\n\t                (img_vgg_input, fmap_vgg_input),\n\t            )\n\t        img_vgg_feats = self.vgg(img_vgg_input)\n\t        recon_vgg_feats = self.vgg(fmap_vgg_input)\n\t        perceptual_loss = F.mse_loss(img_vgg_feats, recon_vgg_feats)\n\t        if relu_loss:\n\t            perceptual_loss = F.relu(perceptual_loss)\n\t        # generator loss\n", "        gen_loss = self.gen_loss(self.discr(fmap))\n\t        if relu_loss:\n\t            gen_loss = F.relu(gen_loss)\n\t        # calculate adaptive weight\n\t        last_dec_layer = self.enc_dec.last_dec_layer\n\t        norm_grad_wrt_gen_loss = grad_layer_wrt_loss(gen_loss, last_dec_layer).norm(p=2)\n\t        norm_grad_wrt_perceptual_loss = grad_layer_wrt_loss(perceptual_loss, last_dec_layer).norm(p=2)\n\t        adaptive_weight = safe_div(norm_grad_wrt_perceptual_loss, norm_grad_wrt_gen_loss)\n\t        adaptive_weight.clamp_(max=1e4)\n\t        # combine losses\n", "        # recon loss is reconstruction loss mse\n\t        # perceptual loss is loss in vgg features mse\n\t        # commit loss is loss in quanitizing in vq mse\n\t        # gan loss is\n\t        if relu_loss:\n\t            loss = (\n\t                F.relu(recon_loss)\n\t                + F.relu(perceptual_loss)\n\t                + F.relu(commit_loss)\n\t                + F.relu(adaptive_weight) * F.relu(gen_loss)\n", "            )\n\t        else:\n\t            loss = recon_loss + perceptual_loss + commit_loss + adaptive_weight * gen_loss\n\t        if return_recons:\n\t            if relu_loss:\n\t                return F.relu(loss), fmap\n\t            else:\n\t                return loss, fmap\n\t        if relu_loss:\n\t            return F.relu(loss)\n", "        else:\n\t            return loss\n"]}
{"filename": "muse_maskgit_pytorch/t5.py", "chunked_list": ["import warnings\n\tfrom dataclasses import dataclass, field\n\tfrom functools import cached_property\n\tfrom os import PathLike\n\tfrom typing import Dict, List, Optional, Tuple, Union\n\timport torch\n\tfrom beartype import beartype\n\tfrom torch import Tensor\n\tfrom transformers import T5Config, T5EncoderModel, T5Tokenizer\n\t# disable t5 warnings and a few others to keep the console clean and nice.\n", "warnings.filterwarnings(\"ignore\")\n\t# dataclass for T5 model info\n\t@dataclass\n\tclass T5ModelInfo:\n\t    name: str\n\t    cache_dir: Optional[PathLike] = None\n\t    dtype: Optional[torch.dtype] = torch.float32\n\t    config: T5Config = field(init=False)\n\t    def __post_init__(self):\n\t        self.config = T5Config.from_pretrained(self.name, cache_dir=self.cache_dir)\n", "        self._model = None\n\t        self._tokenizer = None\n\t    # Using cached_property to avoid loading the model/tokenizer until needed\n\t    @cached_property\n\t    def model(self) -> T5EncoderModel:\n\t        if not self._model:\n\t            self._model = T5EncoderModel.from_pretrained(\n\t                self.name, cache_dir=self.cache_dir, torch_dtype=self.dtype\n\t            )\n\t        return self._model\n", "    @cached_property\n\t    def tokenizer(self) -> T5Tokenizer:\n\t        if not self._tokenizer:\n\t            self._tokenizer = T5Tokenizer.from_pretrained(\n\t                self.name, cache_dir=self.cache_dir, torch_dtype=self.dtype\n\t            )\n\t        return self._tokenizer\n\t# config\n\tMAX_LENGTH = 512\n\tDEFAULT_T5_NAME = \"google/t5-v1_1-base\"\n", "T5_OBJECTS: Dict[str, T5ModelInfo] = {}\n\tdef get_model_and_tokenizer(\n\t    name: str, cache_path: Optional[PathLike] = None, dtype: torch.dtype = torch.float32\n\t) -> Tuple[T5EncoderModel, T5Tokenizer]:\n\t    global T5_OBJECTS\n\t    if name not in T5_OBJECTS.keys():\n\t        T5_OBJECTS[name] = T5ModelInfo(name=name, cache_dir=cache_path, dtype=dtype)\n\t    return T5_OBJECTS[name].model, T5_OBJECTS[name].tokenizer\n\tdef get_encoded_dim(\n\t    name: str, cache_path: Optional[PathLike] = None, dtype: torch.dtype = torch.float32\n", ") -> int:\n\t    global T5_OBJECTS\n\t    if name not in T5_OBJECTS.keys():\n\t        T5_OBJECTS[name] = T5ModelInfo(name=name, cache_dir=cache_path, dtype=dtype)\n\t    return T5_OBJECTS[name].config.d_model\n\t# encoding text\n\t@beartype\n\tdef t5_encode_text_from_encoded(\n\t    input_ids: Tensor,\n\t    attn_mask: Tensor,\n", "    t5: T5EncoderModel,\n\t    output_device: Optional[Union[torch.device, str]] = None,\n\t) -> Tensor:\n\t    device = t5.device\n\t    input_ids, attn_mask = input_ids.to(device), attn_mask.to(device)\n\t    with torch.no_grad():\n\t        output = t5(input_ids=input_ids, attention_mask=attn_mask)\n\t        encoded_text = output.last_hidden_state.detach()\n\t    attn_mask = attn_mask.bool()\n\t    encoded_text: Tensor = encoded_text.masked_fill(attn_mask[..., None], 0.0)\n", "    return encoded_text if output_device is None else encoded_text.to(output_device)\n\t@beartype\n\tdef t5_encode_text(\n\t    texts: Union[str, List[str]],\n\t    tokenizer: T5Tokenizer,\n\t    t5: T5EncoderModel,\n\t    output_device: Optional[Union[torch.device, str]] = None,\n\t) -> Tensor:\n\t    if isinstance(texts, str):\n\t        texts = [texts]\n", "    encoded = tokenizer.batch_encode_plus(\n\t        texts,\n\t        return_tensors=\"pt\",\n\t        padding=\"max_length\",\n\t        max_length=MAX_LENGTH,\n\t        truncation=True,\n\t    )\n\t    return t5_encode_text_from_encoded(encoded[\"input_ids\"], encoded[\"attention_mask\"], t5, output_device)\n"]}
{"filename": "muse_maskgit_pytorch/trainers/base_accelerated_trainer.py", "chunked_list": ["from os import PathLike\n\tfrom pathlib import Path\n\tfrom shutil import rmtree\n\tfrom typing import Optional, Union\n\timport accelerate\n\timport numpy as np\n\timport torch\n\tfrom accelerate import Accelerator, DistributedDataParallelKwargs, DistributedType\n\tfrom beartype import beartype\n\tfrom datasets import Dataset\n", "from lion_pytorch import Lion\n\tfrom PIL import Image\n\tfrom torch import nn\n\tfrom torch.optim import Adam, AdamW, Optimizer\n\tfrom torch.utils.data import DataLoader, random_split\n\tfrom torch_optimizer import (\n\t    PID,\n\t    QHM,\n\t    SGDP,\n\t    SGDW,\n", "    SWATS,\n\t    AccSGD,\n\t    AdaBound,\n\t    AdaMod,\n\t    AdamP,\n\t    AggMo,\n\t    DiffGrad,\n\t    Lamb,\n\t    NovoGrad,\n\t    QHAdam,\n", "    RAdam,\n\t    Shampoo,\n\t    Yogi,\n\t)\n\tfrom transformers.optimization import Adafactor\n\ttry:\n\t    from accelerate.data_loader import MpDeviceLoaderWrapper\n\texcept ImportError:\n\t    MpDeviceLoaderWrapper = DataLoader\n\t    pass\n", "try:\n\t    from bitsandbytes.optim import Adam8bit, AdamW8bit, Lion8bit\n\texcept ImportError:\n\t    Adam8bit = AdamW8bit = Lion8bit = None\n\ttry:\n\t    import wandb\n\texcept ImportError:\n\t    wandb = None\n\tddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n\tdef noop(*args, **kwargs):\n", "    pass\n\t# helper functions\n\tdef identity(t, *args, **kwargs):\n\t    return t\n\tdef cast_tuple(t):\n\t    return t if isinstance(t, (tuple, list)) else (t,)\n\tdef yes_or_no(question):\n\t    answer = input(f\"{question} (y/n) \")\n\t    return answer.lower() in (\"yes\", \"y\")\n\tdef pair(val):\n", "    return val if isinstance(val, tuple) else (val, val)\n\tdef convert_image_to_fn(img_type, image):\n\t    if image.mode != img_type:\n\t        return image.convert(img_type)\n\t    return image\n\t# image related helpers fnuctions and dataset\n\tdef get_accelerator(*args, **kwargs):\n\t    kwargs_handlers = kwargs.get(\"kwargs_handlers\", [])\n\t    if ddp_kwargs not in kwargs_handlers:\n\t        kwargs_handlers.append(ddp_kwargs)\n", "        kwargs.update(kwargs_handlers=kwargs_handlers)\n\t    accelerator = Accelerator(*args, **kwargs)\n\t    return accelerator\n\tdef split_dataset(dataset: Dataset, valid_frac: float, accelerator: Accelerator, seed: int = 42):\n\t    if valid_frac > 0:\n\t        train_size = int((1 - valid_frac) * len(dataset))\n\t        valid_size = len(dataset) - train_size\n\t        ds, valid_ds = random_split(\n\t            dataset,\n\t            [train_size, valid_size],\n", "            generator=torch.Generator().manual_seed(seed),\n\t        )\n\t        accelerator.print(\n\t            f\"training with dataset of {len(ds)} samples and validating with randomly splitted {len(valid_ds)} samples\"\n\t        )\n\t    else:\n\t        valid_ds = ds\n\t        accelerator.print(f\"training with shared training and valid dataset of {len(ds)} samples\")\n\t    return ds, valid_ds\n\t# main trainer class\n", "def get_optimizer(\n\t    use_8bit_adam: bool,\n\t    optimizer: str,\n\t    parameters: dict,\n\t    lr: float,\n\t    weight_decay: float,\n\t    optimizer_kwargs: dict = {},\n\t):\n\t    if use_8bit_adam is True and Adam8bit is None:\n\t        print(\n", "            \"Please install bitsandbytes to use 8-bit optimizers. You can do so by running `pip install \"\n\t            \"bitsandbytes` | Defaulting to non 8-bit equivalent...\"\n\t        )\n\t    bnb_supported_optims = [\"Adam\", \"AdamW\", \"Lion\"]\n\t    if use_8bit_adam and optimizer not in bnb_supported_optims:\n\t        print(f\"8bit is not supported by the {optimizer} optimizer, Using standard {optimizer} instead.\")\n\t    # optimizers\n\t    if optimizer == \"Adam\":\n\t        return (\n\t            Adam8bit(parameters, lr=lr, weight_decay=weight_decay, **optimizer_kwargs)\n", "            if use_8bit_adam and Adam8bit is not None\n\t            else Adam(parameters, lr=lr, weight_decay=weight_decay, **optimizer_kwargs)\n\t        )\n\t    elif optimizer == \"AdamW\":\n\t        return (\n\t            AdamW8bit(parameters, lr=lr, weight_decay=weight_decay, **optimizer_kwargs)\n\t            if use_8bit_adam and AdamW8bit is not None\n\t            else AdamW(parameters, lr=lr, weight_decay=weight_decay, **optimizer_kwargs)\n\t        )\n\t    elif optimizer == \"Lion\":\n", "        # Reckless reuse of the use_8bit_adam flag\n\t        return (\n\t            Lion8bit(parameters, lr=lr, weight_decay=weight_decay, **optimizer_kwargs)\n\t            if use_8bit_adam and Lion8bit is not None\n\t            else Lion(parameters, lr=lr, weight_decay=weight_decay, **optimizer_kwargs)\n\t        )\n\t    elif optimizer == \"Adafactor\":\n\t        return Adafactor(\n\t            parameters,\n\t            lr=lr,\n", "            weight_decay=weight_decay,\n\t            relative_step=False,\n\t            scale_parameter=False,\n\t            **optimizer_kwargs,\n\t        )\n\t    elif optimizer == \"AccSGD\":\n\t        return AccSGD(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"AdaBound\":\n\t        return AdaBound(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"AdaMod\":\n", "        return AdaMod(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"AdamP\":\n\t        return AdamP(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"AggMo\":\n\t        return AggMo(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"DiffGrad\":\n\t        return DiffGrad(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"Lamb\":\n\t        return Lamb(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"NovoGrad\":\n", "        return NovoGrad(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"PID\":\n\t        return PID(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"QHAdam\":\n\t        return QHAdam(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"QHM\":\n\t        return QHM(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"RAdam\":\n\t        return RAdam(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"SGDP\":\n", "        return SGDP(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"SGDW\":\n\t        return SGDW(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"Shampoo\":\n\t        return Shampoo(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"SWATS\":\n\t        return SWATS(parameters, lr=lr, weight_decay=weight_decay)\n\t    elif optimizer == \"Yogi\":\n\t        return Yogi(parameters, lr=lr, weight_decay=weight_decay)\n\t    else:\n", "        raise NotImplementedError(f\"{optimizer} optimizer not supported yet.\")\n\t@beartype\n\tclass BaseAcceleratedTrainer(nn.Module):\n\t    def __init__(\n\t        self,\n\t        dataloader: Union[DataLoader, MpDeviceLoaderWrapper],\n\t        valid_dataloader: Union[DataLoader, MpDeviceLoaderWrapper],\n\t        accelerator: Accelerator,\n\t        *,\n\t        current_step: int,\n", "        num_train_steps: int,\n\t        num_epochs: int = 5,\n\t        max_grad_norm: Optional[int] = None,\n\t        save_results_every: int = 100,\n\t        save_model_every: int = 1000,\n\t        results_dir: Union[str, PathLike] = Path.cwd().joinpath(\"results\"),\n\t        logging_dir: Union[str, PathLike] = Path.cwd().joinpath(\"results/logs\"),\n\t        apply_grad_penalty_every: int = 4,\n\t        gradient_accumulation_steps: int = 1,\n\t        clear_previous_experiments: bool = False,\n", "        validation_image_scale: Union[int, float] = 1.0,\n\t        only_save_last_checkpoint: bool = False,\n\t    ):\n\t        super().__init__()\n\t        self.model: nn.Module = None\n\t        # instantiate accelerator\n\t        self.gradient_accumulation_steps: int = gradient_accumulation_steps\n\t        self.accelerator: Accelerator = accelerator\n\t        self.logging_dir: Path = Path(logging_dir) if not isinstance(logging_dir, Path) else logging_dir\n\t        self.results_dir: Path = Path(results_dir) if not isinstance(results_dir, Path) else results_dir\n", "        # training params\n\t        self.only_save_last_checkpoint: bool = only_save_last_checkpoint\n\t        self.validation_image_scale: Union[int, float] = validation_image_scale\n\t        self.register_buffer(\"steps\", torch.Tensor([current_step]))\n\t        self.num_train_steps: int = num_train_steps\n\t        self.num_epochs = num_epochs\n\t        self.max_grad_norm: Optional[Union[int, float]] = max_grad_norm\n\t        self.dl = dataloader\n\t        self.valid_dl = valid_dataloader\n\t        self.dl_iter = iter(self.dl)\n", "        self.valid_dl_iter = iter(self.valid_dl)\n\t        self.save_model_every: int = save_model_every\n\t        self.save_results_every: int = save_results_every\n\t        self.apply_grad_penalty_every: int = apply_grad_penalty_every\n\t        # Clear previous experiment data if requested\n\t        if clear_previous_experiments is True and self.accelerator.is_local_main_process:\n\t            if self.results_dir.exists():\n\t                rmtree(self.results_dir, ignore_errors=True)\n\t        # Make sure logging and results directories exist\n\t        self.logging_dir.mkdir(parents=True, exist_ok=True)\n", "        self.results_dir.mkdir(parents=True, exist_ok=True)\n\t        self.optim: Optimizer = None\n\t        self.print = self.accelerator.print\n\t        self.log = self.accelerator.log\n\t        self.on_tpu = self.accelerator.distributed_type == accelerate.DistributedType.TPU\n\t    def save(self, path):\n\t        if not self.accelerator.is_main_process:\n\t            return\n\t        pkg = dict(\n\t            model=self.accelerator.get_state_dict(self.model),\n", "            optim=self.optim.state_dict(),\n\t        )\n\t        self.accelerator.save(pkg, path)\n\t    def load(self, path: Union[str, PathLike]):\n\t        if not isinstance(path, Path):\n\t            path = Path(path)\n\t        if not path.exists():\n\t            raise FileNotFoundError(f\"Checkpoint file {path} does not exist.\")\n\t        pkg = torch.load(path, map_location=\"cpu\")\n\t        model = self.accelerator.unwrap_model(self.model)\n", "        model.load_state_dict(pkg[\"model\"])\n\t        self.optim.load_state_dict(pkg[\"optim\"])\n\t        return pkg\n\t    def log_validation_images(self, images, step, prompts=None):\n\t        if self.validation_image_scale != 1:\n\t            # Calculate the new height based on the scale factor\n\t            new_height = int(np.array(images[0]).shape[0] * self.validation_image_scale)\n\t            # Calculate the aspect ratio of the original image\n\t            aspect_ratio = np.array(images[0]).shape[1] / np.array(images[0]).shape[0]\n\t            # Calculate the new width based on the new height and aspect ratio\n", "            new_width = int(new_height * aspect_ratio)\n\t            # Resize the images using the new width and height\n\t            output_size = (new_width, new_height)\n\t            images_pil = [Image.fromarray(np.array(image)) for image in images]\n\t            images_pil_resized = [image_pil.resize(output_size) for image_pil in images_pil]\n\t            images = [np.array(image_pil) for image_pil in images_pil_resized]\n\t        for tracker in self.accelerator.trackers:\n\t            if tracker.name == \"tensorboard\":\n\t                np_images = np.stack([np.asarray(img) for img in images])\n\t                tracker.writer.add_images(\"validation\", np_images, step, dataformats=\"NHWC\")\n", "            if tracker.name == \"wandb\":\n\t                tracker.log(\n\t                    {\n\t                        \"validation\": [\n\t                            wandb.Image(image, caption=\"\" if not prompts else prompts[i])\n\t                            for i, image in enumerate(images)\n\t                        ]\n\t                    }\n\t                )\n\t    @property\n", "    def device(self):\n\t        return self.accelerator.device\n\t    @property\n\t    def is_distributed(self):\n\t        return (\n\t            False\n\t            if self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1\n\t            else True\n\t        )\n\t    @property\n", "    def is_main_process(self):\n\t        return self.accelerator.is_main_process\n\t    @property\n\t    def is_local_main_process(self):\n\t        return self.accelerator.is_local_main_process\n\t    def train_step(self):\n\t        raise NotImplementedError(\"You are calling train_step on the base trainer with no models\")\n\t    def train(self, log_fn=noop):\n\t        self.model.train()\n\t        while self.steps < self.num_train_steps:\n", "            with self.accelerator.autocast():\n\t                logs = self.train_step()\n\t            log_fn(logs)\n\t        self.print(\"training complete\")\n"]}
{"filename": "muse_maskgit_pytorch/trainers/vqvae_trainers.py", "chunked_list": ["import torch\n\tfrom accelerate import Accelerator\n\tfrom diffusers.optimization import get_scheduler\n\tfrom einops import rearrange\n\tfrom ema_pytorch import EMA\n\tfrom omegaconf import OmegaConf\n\tfrom PIL import Image\n\tfrom torch.optim.lr_scheduler import LRScheduler\n\tfrom torch.utils.data import DataLoader\n\tfrom torchvision.utils import make_grid, save_image\n", "from tqdm import tqdm\n\tfrom muse_maskgit_pytorch.trainers.base_accelerated_trainer import (\n\t    BaseAcceleratedTrainer,\n\t    get_optimizer,\n\t)\n\tfrom muse_maskgit_pytorch.vqgan_vae import VQGanVAE\n\tdef noop(*args, **kwargs):\n\t    pass\n\tdef accum_log(log, new_logs):\n\t    for key, new_value in new_logs.items():\n", "        old_value = log.get(key, 0.0)\n\t        log[key] = old_value + new_value\n\t    return log\n\tdef exists(val):\n\t    return val is not None\n\tclass VQGanVAETrainer(BaseAcceleratedTrainer):\n\t    def __init__(\n\t        self,\n\t        vae: VQGanVAE,\n\t        dataloader: DataLoader,\n", "        valid_dataloader: DataLoader,\n\t        accelerator: Accelerator,\n\t        *,\n\t        current_step,\n\t        num_train_steps,\n\t        num_epochs: int = 5,\n\t        gradient_accumulation_steps=1,\n\t        max_grad_norm=None,\n\t        save_results_every=100,\n\t        save_model_every=1000,\n", "        results_dir=\"./results\",\n\t        logging_dir=\"./results/logs\",\n\t        apply_grad_penalty_every=4,\n\t        lr=3e-4,\n\t        lr_scheduler_type=\"constant\",\n\t        lr_warmup_steps=500,\n\t        discr_max_grad_norm=None,\n\t        use_ema=True,\n\t        ema_beta=0.995,\n\t        ema_update_after_step=0,\n", "        ema_update_every=1,\n\t        clear_previous_experiments=False,\n\t        validation_image_scale: float = 1.0,\n\t        only_save_last_checkpoint=False,\n\t        optimizer=\"Adam\",\n\t        weight_decay=0.0,\n\t        use_8bit_adam=False,\n\t        num_cycles=1,\n\t        scheduler_power=1.0,\n\t        args=None,\n", "    ):\n\t        super().__init__(\n\t            dataloader,\n\t            valid_dataloader,\n\t            accelerator,\n\t            current_step=current_step,\n\t            num_train_steps=num_train_steps,\n\t            num_epochs=num_epochs,\n\t            gradient_accumulation_steps=gradient_accumulation_steps,\n\t            max_grad_norm=max_grad_norm,\n", "            save_results_every=save_results_every,\n\t            save_model_every=save_model_every,\n\t            results_dir=results_dir,\n\t            logging_dir=logging_dir,\n\t            apply_grad_penalty_every=apply_grad_penalty_every,\n\t            clear_previous_experiments=clear_previous_experiments,\n\t            validation_image_scale=validation_image_scale,\n\t            only_save_last_checkpoint=only_save_last_checkpoint,\n\t        )\n\t        # arguments used for the training script,\n", "        # we are going to use them later to save them to a config file.\n\t        self.args = args\n\t        self.current_step = current_step\n\t        # vae\n\t        self.model = vae\n\t        all_parameters = set(vae.parameters())\n\t        discr_parameters = set(vae.discr.parameters())\n\t        vae_parameters = all_parameters - discr_parameters\n\t        # optimizers\n\t        self.optim = get_optimizer(use_8bit_adam, optimizer, vae_parameters, lr, weight_decay)\n", "        self.discr_optim = get_optimizer(use_8bit_adam, optimizer, discr_parameters, lr, weight_decay)\n\t        if self.num_train_steps > 0:\n\t            self.num_lr_steps = self.num_train_steps * self.gradient_accumulation_steps\n\t        else:\n\t            self.num_lr_steps = self.num_epochs * len(self.dl)\n\t        self.lr_scheduler: LRScheduler = get_scheduler(\n\t            lr_scheduler_type,\n\t            optimizer=self.optim,\n\t            num_warmup_steps=lr_warmup_steps * self.gradient_accumulation_steps,\n\t            num_training_steps=self.num_lr_steps,\n", "            num_cycles=num_cycles,\n\t            power=scheduler_power,\n\t        )\n\t        self.lr_scheduler_discr: LRScheduler = get_scheduler(\n\t            lr_scheduler_type,\n\t            optimizer=self.discr_optim,\n\t            num_warmup_steps=lr_warmup_steps * self.gradient_accumulation_steps,\n\t            num_training_steps=self.num_lr_steps,\n\t            num_cycles=num_cycles,\n\t            power=scheduler_power,\n", "        )\n\t        self.discr_max_grad_norm = discr_max_grad_norm\n\t        # prepare with accelerator\n\t        (\n\t            self.model,\n\t            self.optim,\n\t            self.discr_optim,\n\t            self.dl,\n\t            self.valid_dl,\n\t            self.lr_scheduler,\n", "            self.lr_scheduler_discr,\n\t        ) = accelerator.prepare(\n\t            self.model,\n\t            self.optim,\n\t            self.discr_optim,\n\t            self.dl,\n\t            self.valid_dl,\n\t            self.lr_scheduler,\n\t            self.lr_scheduler_discr,\n\t        )\n", "        self.model.train()\n\t        self.use_ema = use_ema\n\t        if use_ema:\n\t            self.ema_model = EMA(\n\t                vae,\n\t                update_after_step=ema_update_after_step,\n\t                update_every=ema_update_every,\n\t            )\n\t            self.ema_model = accelerator.prepare(self.ema_model)\n\t        if not self.on_tpu:\n", "            if self.num_train_steps <= 0:\n\t                self.training_bar = tqdm(initial=int(self.steps.item()), total=len(self.dl) * self.num_epochs)\n\t            else:\n\t                self.training_bar = tqdm(initial=int(self.steps.item()), total=self.num_train_steps)\n\t            self.info_bar = tqdm(total=0, bar_format=\"{desc}\")\n\t    def load(self, path):\n\t        pkg = super().load(path)\n\t        self.discr_optim.load_state_dict(pkg[\"discr_optim\"])\n\t    def save(self, path):\n\t        if not self.is_local_main_process:\n", "            return\n\t        pkg = dict(\n\t            model=self.get_state_dict(self.model),\n\t            optim=self.optim.state_dict(),\n\t            discr_optim=self.discr_optim.state_dict(),\n\t        )\n\t        self.accelerator.save(pkg, path)\n\t    def log_validation_images(self, logs, steps):\n\t        log_imgs = []\n\t        self.model.eval()\n", "        try:\n\t            valid_data = next(self.valid_dl_iter)\n\t        except StopIteration:\n\t            self.valid_dl_iter = iter(self.valid_dl)\n\t            valid_data = next(self.valid_dl_iter)\n\t        valid_data = valid_data.to(self.device)\n\t        recons = self.model(valid_data, return_recons=True)\n\t        # else save a grid of images\n\t        imgs_and_recons = torch.stack((valid_data, recons), dim=0)\n\t        imgs_and_recons = rearrange(imgs_and_recons, \"r b ... -> (b r) ...\")\n", "        imgs_and_recons = imgs_and_recons.detach().cpu().float().clamp(0.0, 1.0)\n\t        grid = make_grid(imgs_and_recons, nrow=2, normalize=True, value_range=(0, 1))\n\t        logs[\"reconstructions\"] = grid\n\t        save_file = str(self.results_dir / f\"{steps}.png\")\n\t        save_image(grid, save_file)\n\t        log_imgs.append(Image.open(save_file))\n\t        super().log_validation_images(log_imgs, steps, prompts=[\"vae\"])\n\t        self.model.train()\n\t    def train(self):\n\t        self.steps = self.steps + 1\n", "        device = self.device\n\t        self.model.train()\n\t        if self.accelerator.is_main_process:\n\t            proc_label = f\"[P{self.accelerator.process_index:03d}][Master]\"\n\t        else:\n\t            proc_label = f\"[P{self.accelerator.process_index:03d}][Worker]\"\n\t        for epoch in range(self.current_step // len(self.dl), self.num_epochs):\n\t            for img in self.dl:\n\t                loss = 0.0\n\t                steps = int(self.steps.item())\n", "                apply_grad_penalty = (steps % self.apply_grad_penalty_every) == 0\n\t                discr = self.model.module.discr if self.is_distributed else self.model.discr\n\t                if self.use_ema:\n\t                    ema_model = self.ema_model.module if self.is_distributed else self.ema_model\n\t                # logs\n\t                logs = {}\n\t                # update vae (generator)\n\t                img = img.to(device)\n\t                with self.accelerator.autocast():\n\t                    loss = self.model(img, add_gradient_penalty=apply_grad_penalty, return_loss=True)\n", "                self.accelerator.backward(loss / self.gradient_accumulation_steps)\n\t                if self.max_grad_norm is not None and self.accelerator.sync_gradients:\n\t                    self.accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n\t                accum_log(logs, {\"Train/vae_loss\": loss.item() / self.gradient_accumulation_steps})\n\t                self.lr_scheduler.step()\n\t                self.lr_scheduler_discr.step()\n\t                self.optim.step()\n\t                self.optim.zero_grad()\n\t                loss = 0.0\n\t                # update discriminator\n", "                if exists(discr):\n\t                    self.discr_optim.zero_grad()\n\t                    with torch.cuda.amp.autocast():\n\t                        loss = self.model(img, return_discr_loss=True)\n\t                    self.accelerator.backward(loss / self.gradient_accumulation_steps)\n\t                    if self.discr_max_grad_norm is not None and self.accelerator.sync_gradients:\n\t                        self.accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n\t                    accum_log(\n\t                        logs,\n\t                        {\"Train/discr_loss\": loss.item() / self.gradient_accumulation_steps},\n", "                    )\n\t                    self.discr_optim.step()\n\t                # log\n\t                if self.on_tpu:\n\t                    self.accelerator.print(\n\t                        f\"[E{epoch + 1}][{steps:05d}]{proc_label}: \"\n\t                        f\"vae loss: {logs['Train/vae_loss']} - \"\n\t                        f\"discr loss: {logs['Train/discr_loss']} - \"\n\t                        f\"lr: {self.lr_scheduler.get_last_lr()[0]}\"\n\t                    )\n", "                else:\n\t                    self.training_bar.update()\n\t                    # Note: we had to remove {proc_label} from the description\n\t                    # to short it so it doenst go beyond one line on each step.\n\t                    self.info_bar.set_description_str(\n\t                        f\"[E{epoch + 1}][{steps:05d}]: \"\n\t                        f\"vae loss: {logs['Train/vae_loss']} - \"\n\t                        f\"discr loss: {logs['Train/discr_loss']} - \"\n\t                        f\"lr: {self.lr_scheduler.get_last_lr()[0]}\"\n\t                    )\n", "                logs[\"lr\"] = self.lr_scheduler.get_last_lr()[0]\n\t                self.accelerator.log(logs, step=steps)\n\t                # update exponential moving averaged generator\n\t                if self.use_ema:\n\t                    ema_model.update()\n\t                # sample results every so often\n\t                if (steps % self.save_results_every) == 0:\n\t                    self.accelerator.print(\n\t                        f\"\\n[E{epoch + 1}][{steps}] | Logging validation images to {str(self.results_dir)}\"\n\t                    )\n", "                    self.log_validation_images(logs, steps)\n\t                # save model every so often\n\t                self.accelerator.wait_for_everyone()\n\t                if self.is_main_process and (steps % self.save_model_every) == 0:\n\t                    self.accelerator.print(f\"\\nStep: {steps} | Saving model to {str(self.results_dir)}\")\n\t                    state_dict = self.accelerator.unwrap_model(self.model).state_dict()\n\t                    file_name = f\"vae.{steps}.pt\" if not self.only_save_last_checkpoint else \"vae.pt\"\n\t                    model_path = str(self.results_dir / file_name)\n\t                    self.accelerator.save(state_dict, model_path)\n\t                    if self.args and not self.args.do_not_save_config:\n", "                        # save config file next to the model file.\n\t                        conf = OmegaConf.create(vars(self.args))\n\t                        OmegaConf.save(conf, f\"{model_path}.yaml\")\n\t                    if self.use_ema:\n\t                        ema_state_dict = self.accelerator.unwrap_model(self.ema_model).state_dict()\n\t                        file_name = (\n\t                            f\"vae.{steps}.ema.pt\" if not self.only_save_last_checkpoint else \"vae.ema.pt\"\n\t                        )\n\t                        model_path = str(self.results_dir / file_name)\n\t                        self.accelerator.save(ema_state_dict, model_path)\n", "                        if self.args and not self.args.do_not_save_config:\n\t                            # save config file next to the model file.\n\t                            conf = OmegaConf.create(vars(self.args))\n\t                            OmegaConf.save(conf, f\"{model_path}.yaml\")\n\t                self.steps += 1\n\t            # if self.num_train_steps > 0 and int(self.steps.item()) >= self.num_train_steps:\n\t            # self.accelerator.print(\n\t            # f\"\\n[E{epoch + 1}][{steps}]{proc_label}: \" f\"[STOP EARLY]: Stopping training early...\"\n\t            # )\n\t            # break\n", "        # Loop finished, save model\n\t        self.accelerator.wait_for_everyone()\n\t        if self.is_main_process:\n\t            self.accelerator.print(\n\t                f\"[E{self.num_epochs}][{steps:05d}]{proc_label}: saving model to {str(self.results_dir)}\"\n\t            )\n\t            state_dict = self.accelerator.unwrap_model(self.model).state_dict()\n\t            file_name = f\"vae.{steps}.pt\" if not self.only_save_last_checkpoint else \"vae.pt\"\n\t            model_path = str(self.results_dir / file_name)\n\t            self.accelerator.save(state_dict, model_path)\n", "            if self.args and not self.args.do_not_save_config:\n\t                # save config file next to the model file.\n\t                conf = OmegaConf.create(vars(self.args))\n\t                OmegaConf.save(conf, f\"{model_path}.yaml\")\n\t            if self.use_ema:\n\t                ema_state_dict = self.accelerator.unwrap_model(self.ema_model).state_dict()\n\t                file_name = f\"vae.{steps}.ema.pt\" if not self.only_save_last_checkpoint else \"vae.ema.pt\"\n\t                model_path = str(self.results_dir / file_name)\n\t                self.accelerator.save(ema_state_dict, model_path)\n\t                if self.args and not self.args.do_not_save_config:\n", "                    # save config file next to the model file.\n\t                    conf = OmegaConf.create(vars(self.args))\n\t                    OmegaConf.save(conf, f\"{model_path}.yaml\")\n"]}
{"filename": "muse_maskgit_pytorch/trainers/__init__.py", "chunked_list": ["from .base_accelerated_trainer import get_accelerator\n\tfrom .maskgit_trainer import MaskGitTrainer\n\tfrom .vqvae_trainers import VQGanVAETrainer\n\t__all__ = [\n\t    \"VQGanVAETrainer\",\n\t    \"MaskGitTrainer\",\n\t    \"get_accelerator\",\n\t]\n"]}
{"filename": "muse_maskgit_pytorch/trainers/maskgit_trainer.py", "chunked_list": ["from typing import List\n\timport torch  # noqa: F401\n\timport torch.nn.functional as F\n\tfrom accelerate import Accelerator\n\tfrom diffusers.optimization import SchedulerType\n\tfrom ema_pytorch import EMA\n\tfrom omegaconf import OmegaConf\n\tfrom PIL import Image\n\tfrom torch.optim import Optimizer\n\tfrom torch.utils.data import DataLoader\n", "from torchvision.utils import save_image\n\tfrom muse_maskgit_pytorch.muse_maskgit_pytorch import MaskGit\n\tfrom muse_maskgit_pytorch.t5 import t5_encode_text_from_encoded\n\tfrom muse_maskgit_pytorch.trainers.base_accelerated_trainer import BaseAcceleratedTrainer\n\ttry:\n\t    import torch_xla\n\t    import torch_xla.core.xla_model as xm\n\t    import torch_xla.debug.metrics as met\n\texcept ImportError:\n\t    torch_xla = None\n", "    xm = None\n\t    met = None\n\tfrom tqdm import tqdm\n\tclass MaskGitTrainer(BaseAcceleratedTrainer):\n\t    def __init__(\n\t        self,\n\t        maskgit: MaskGit,\n\t        dataloader: DataLoader,\n\t        valid_dataloader: DataLoader,\n\t        accelerator: Accelerator,\n", "        optimizer: Optimizer,\n\t        scheduler: SchedulerType,\n\t        *,\n\t        current_step: int,\n\t        num_train_steps: int,\n\t        num_epochs: int = 5,\n\t        batch_size: int,\n\t        gradient_accumulation_steps: int = 1,\n\t        max_grad_norm: float = None,\n\t        save_results_every: int = 100,\n", "        save_model_every: int = 1000,\n\t        log_metrics_every: int = 10,\n\t        results_dir=\"./results\",\n\t        logging_dir=\"./results/logs\",\n\t        apply_grad_penalty_every=4,\n\t        use_ema=True,\n\t        ema_update_after_step=0,\n\t        ema_update_every=1,\n\t        validation_prompts=[\"a photo of a dog\"],\n\t        timesteps=18,\n", "        clear_previous_experiments=False,\n\t        validation_image_scale: float = 1.0,\n\t        only_save_last_checkpoint=False,\n\t        args=None,\n\t    ):\n\t        super().__init__(\n\t            dataloader=dataloader,\n\t            valid_dataloader=valid_dataloader,\n\t            accelerator=accelerator,\n\t            current_step=current_step,\n", "            num_train_steps=num_train_steps,\n\t            num_epochs=num_epochs,\n\t            gradient_accumulation_steps=gradient_accumulation_steps,\n\t            max_grad_norm=max_grad_norm,\n\t            save_results_every=save_results_every,\n\t            save_model_every=save_model_every,\n\t            results_dir=results_dir,\n\t            logging_dir=logging_dir,\n\t            apply_grad_penalty_every=apply_grad_penalty_every,\n\t            clear_previous_experiments=clear_previous_experiments,\n", "            validation_image_scale=validation_image_scale,\n\t            only_save_last_checkpoint=only_save_last_checkpoint,\n\t        )\n\t        self.save_results_every = save_results_every\n\t        self.log_metrics_every = log_metrics_every\n\t        self.batch_size = batch_size\n\t        self.current_step = current_step\n\t        self.timesteps = timesteps\n\t        # arguments used for the training script,\n\t        # we are going to use them later to save them to a config file.\n", "        self.args = args\n\t        # maskgit\n\t        maskgit.vae.requires_grad_(False)\n\t        maskgit.transformer.t5.requires_grad_(False)\n\t        self.model: MaskGit = maskgit\n\t        self.optim: Optimizer = optimizer\n\t        self.lr_scheduler: SchedulerType = scheduler\n\t        self.use_ema = use_ema\n\t        self.validation_prompts: List[str] = validation_prompts\n\t        if use_ema:\n", "            ema_model = EMA(\n\t                self.model,\n\t                update_after_step=ema_update_after_step,\n\t                update_every=ema_update_every,\n\t            )\n\t            self.ema_model = ema_model\n\t        else:\n\t            self.ema_model = None\n\t        if not self.on_tpu:\n\t            if self.num_train_steps <= 0:\n", "                self.training_bar = tqdm(initial=int(self.steps.item()), total=len(self.dl) * self.num_epochs)\n\t            else:\n\t                self.training_bar = tqdm(initial=int(self.steps.item()), total=self.num_train_steps)\n\t            self.info_bar = tqdm(total=0, bar_format=\"{desc}\")\n\t    def save_validation_images(\n\t        self, validation_prompts, step: int, cond_image=None, cond_scale=3, temperature=1, timesteps=18\n\t    ):\n\t        # moved the print to the top of the function so it shows before the progress bar for reability.\n\t        if validation_prompts:\n\t            self.accelerator.print(\n", "                f\"\\nStep: {step} | Logging with prompts: {[' | '.join(validation_prompts)]}\"\n\t            )\n\t        images = self.model.generate(\n\t            validation_prompts,\n\t            cond_images=cond_image,\n\t            cond_scale=cond_scale,\n\t            temperature=temperature,\n\t            timesteps=timesteps,\n\t        ).to(self.accelerator.device)\n\t        save_dir = self.results_dir.joinpath(\"MaskGit\")\n", "        save_dir.mkdir(exist_ok=True, parents=True)\n\t        save_file = save_dir.joinpath(f\"maskgit_{step}.png\")\n\t        if self.accelerator.is_main_process:\n\t            save_image(images, save_file, \"png\")\n\t            self.log_validation_images([Image.open(save_file)], step, [\"|\".join(validation_prompts)])\n\t        return save_file\n\t    def train(self):\n\t        self.steps = self.steps + 1\n\t        self.model.train()\n\t        if self.accelerator.is_main_process:\n", "            proc_label = f\"[P{self.accelerator.process_index}][Master]\"\n\t        else:\n\t            proc_label = f\"[P{self.accelerator.process_index}][Worker]\"\n\t        # logs\n\t        for epoch in range(self.current_step // len(self.dl), self.num_epochs):\n\t            for imgs, input_ids, attn_mask in iter(self.dl):\n\t                train_loss = 0.0\n\t                steps = int(self.steps.item())\n\t                with torch.no_grad():\n\t                    text_embeds = t5_encode_text_from_encoded(\n", "                        input_ids, attn_mask, self.model.transformer.t5, self.accelerator.device\n\t                    )\n\t                with self.accelerator.accumulate(self.model), self.accelerator.autocast():\n\t                    loss = self.model(imgs, text_embeds=text_embeds)\n\t                    self.accelerator.backward(loss)\n\t                    if self.max_grad_norm is not None and self.accelerator.sync_gradients:\n\t                        self.accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n\t                    self.optim.step()\n\t                    self.lr_scheduler.step()\n\t                    self.optim.zero_grad()\n", "                    if self.use_ema:\n\t                        self.ema_model.update()\n\t                    gathered_loss = self.accelerator.gather_for_metrics(loss)\n\t                    train_loss = gathered_loss.mean() / self.gradient_accumulation_steps\n\t                    logs = {\"loss\": train_loss, \"lr\": self.lr_scheduler.get_last_lr()[0]}\n\t                    if self.on_tpu:\n\t                        self.accelerator.print(\n\t                            f\"\\n[E{epoch + 1}][{steps}]{proc_label}: \"\n\t                            f\"maskgit loss: {logs['loss']} - lr: {logs['lr']}\"\n\t                        )\n", "                    else:\n\t                        self.training_bar.update()\n\t                        self.info_bar.set_description_str(\n\t                            f\"[E{epoch + 1}]{proc_label}: \" f\"maskgit loss: {logs['loss']} - lr: {logs['lr']}\"\n\t                        )\n\t                    self.accelerator.log(logs, step=steps)\n\t                if not (steps % self.save_model_every):\n\t                    self.accelerator.print(\n\t                        f\"\\n[E{epoch + 1}][{steps}]{proc_label}: \" f\"saving model to {self.results_dir}\"\n\t                    )\n", "                    state_dict = self.accelerator.unwrap_model(self.model).state_dict()\n\t                    maskgit_save_name = \"maskgit_superres\" if self.model.cond_image_size else \"maskgit\"\n\t                    file_name = (\n\t                        f\"{maskgit_save_name}.{steps}.pt\"\n\t                        if not self.only_save_last_checkpoint\n\t                        else f\"{maskgit_save_name}.pt\"\n\t                    )\n\t                    model_path = self.results_dir.joinpath(file_name)\n\t                    self.accelerator.wait_for_everyone()\n\t                    self.accelerator.save(state_dict, model_path)\n", "                    if self.args and not self.args.do_not_save_config:\n\t                        # save config file next to the model file.\n\t                        conf = OmegaConf.create(vars(self.args))\n\t                        OmegaConf.save(conf, f\"{model_path}.yaml\")\n\t                    if self.use_ema:\n\t                        self.accelerator.print(\n\t                            f\"\\n[E{epoch + 1}][{steps}]{proc_label}: \"\n\t                            f\"saving EMA model to {self.results_dir}\"\n\t                        )\n\t                        ema_state_dict = self.accelerator.unwrap_model(self.ema_model).state_dict()\n", "                        file_name = (\n\t                            f\"{maskgit_save_name}.{steps}.ema.pt\"\n\t                            if not self.only_save_last_checkpoint\n\t                            else f\"{maskgit_save_name}.ema.pt\"\n\t                        )\n\t                        model_path = str(self.results_dir / file_name)\n\t                        self.accelerator.wait_for_everyone()\n\t                        self.accelerator.save(ema_state_dict, model_path)\n\t                        if self.args and not self.args.do_not_save_config:\n\t                            # save config file next to the model file.\n", "                            conf = OmegaConf.create(vars(self.args))\n\t                            OmegaConf.save(conf, f\"{model_path}.yaml\")\n\t                if not (steps % self.save_results_every):\n\t                    cond_image = None\n\t                    if self.model.cond_image_size:\n\t                        cond_image = F.interpolate(imgs, self.model.cond_image_size, mode=\"nearest\")\n\t                        self.validation_prompts = [\"\"] * self.batch_size\n\t                    if self.on_tpu:\n\t                        self.accelerator.print(f\"\\n[E{epoch + 1}]{proc_label}: \" f\"Logging validation images\")\n\t                    else:\n", "                        self.info_bar.set_description_str(\n\t                            f\"[E{epoch + 1}]{proc_label}: \" f\"Logging validation images\"\n\t                        )\n\t                    saved_image = self.save_validation_images(\n\t                        self.validation_prompts,\n\t                        steps,\n\t                        cond_image=cond_image,\n\t                        timesteps=self.timesteps,\n\t                    )\n\t                    if self.on_tpu:\n", "                        self.accelerator.print(\n\t                            f\"\\n[E{epoch + 1}][{steps}]{proc_label}: saved to {saved_image}\"\n\t                        )\n\t                    else:\n\t                        self.info_bar.set_description_str(\n\t                            f\"[E{epoch + 1}]{proc_label}: \" f\"saved to {saved_image}\"\n\t                        )\n\t                if met is not None and not (steps % self.log_metrics_every):\n\t                    if self.on_tpu:\n\t                        self.accelerator.print(f\"\\n[E{epoch + 1}][{steps}]{proc_label}: metrics:\")\n", "                    else:\n\t                        self.info_bar.set_description_str(f\"[E{epoch + 1}]{proc_label}: metrics:\")\n\t                self.steps += 1\n\t            # if self.num_train_steps > 0 and int(self.steps.item()) >= self.num_train_steps:\n\t            # if self.on_tpu:\n\t            # self.accelerator.print(\n\t            # f\"\\n[E{epoch + 1}][{int(self.steps.item())}]{proc_label}\"\n\t            # f\"[STOP EARLY]: Stopping training early...\"\n\t            # )\n\t            # else:\n", "            # self.info_bar.set_description_str(\n\t            # f\"[E{epoch + 1}]{proc_label}\" f\"[STOP EARLY]: Stopping training early...\"\n\t            # )\n\t            # break\n\t        # loop complete, save final model\n\t        self.accelerator.print(\n\t            f\"\\n[E{epoch + 1}][{steps}]{proc_label}[FINAL]: saving model to {self.results_dir}\"\n\t        )\n\t        state_dict = self.accelerator.unwrap_model(self.model).state_dict()\n\t        maskgit_save_name = \"maskgit_superres\" if self.model.cond_image_size else \"maskgit\"\n", "        file_name = (\n\t            f\"{maskgit_save_name}.{steps}.pt\"\n\t            if not self.only_save_last_checkpoint\n\t            else f\"{maskgit_save_name}.pt\"\n\t        )\n\t        model_path = self.results_dir.joinpath(file_name)\n\t        self.accelerator.wait_for_everyone()\n\t        self.accelerator.save(state_dict, model_path)\n\t        if self.args and not self.args.do_not_save_config:\n\t            # save config file next to the model file.\n", "            conf = OmegaConf.create(vars(self.args))\n\t            OmegaConf.save(conf, f\"{model_path}.yaml\")\n\t        if self.use_ema:\n\t            self.accelerator.print(f\"\\n[{steps}]{proc_label}[FINAL]: saving EMA model to {self.results_dir}\")\n\t            ema_state_dict = self.accelerator.unwrap_model(self.ema_model).state_dict()\n\t            file_name = (\n\t                f\"{maskgit_save_name}.{steps}.ema.pt\"\n\t                if not self.only_save_last_checkpoint\n\t                else f\"{maskgit_save_name}.ema.pt\"\n\t            )\n", "            model_path = str(self.results_dir / file_name)\n\t            self.accelerator.wait_for_everyone()\n\t            self.accelerator.save(ema_state_dict, model_path)\n\t            if self.args and not self.args.do_not_save_config:\n\t                # save config file next to the model file.\n\t                conf = OmegaConf.create(vars(self.args))\n\t                OmegaConf.save(conf, f\"{model_path}.yaml\")\n\t        cond_image = None\n\t        if self.model.cond_image_size:\n\t            self.accelerator.print(\n", "                \"With conditional image training, we recommend keeping the validation prompts to empty strings\"\n\t            )\n\t            cond_image = F.interpolate(imgs, self.model.cond_image_size, mode=\"nearest\")\n\t        steps = int(self.steps.item()) + 1  # get the final step count, plus one\n\t        self.accelerator.print(f\"\\n[{steps}]{proc_label}: Logging validation images\")\n\t        saved_image = self.save_validation_images(self.validation_prompts, steps, cond_image=cond_image)\n\t        self.accelerator.print(f\"\\n[{steps}]{proc_label}: saved to {saved_image}\")\n\t        if met is not None and not (steps % self.log_metrics_every):\n\t            self.accelerator.print(f\"\\n[{steps}]{proc_label}: metrics:\")\n"]}
{"filename": "muse_maskgit_pytorch/modules/mlp.py", "chunked_list": ["# Copyright (c) Meta Platforms, Inc. and affiliates.\n\t# All rights reserved.\n\t#\n\t# This source code is licensed under the license found in the\n\t# LICENSE file in the root directory of this source tree.\n\tfrom typing import Optional\n\timport torch.nn.functional as F\n\tfrom torch import Tensor, nn\n\tclass SwiGLUFFN(nn.Module):\n\t    def __init__(\n", "        self,\n\t        in_features: int,\n\t        hidden_features: Optional[int] = None,\n\t        out_features: Optional[int] = None,\n\t        bias: bool = True,\n\t    ) -> None:\n\t        super().__init__()\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n\t        self.w12 = nn.Linear(in_features, 2 * hidden_features, bias=bias)\n", "        self.w3 = nn.Linear(hidden_features, out_features, bias=bias)\n\t    def forward(self, x: Tensor) -> Tensor:\n\t        x12 = self.w12(x)\n\t        x1, x2 = x12.chunk(2, dim=-1)\n\t        hidden = F.silu(x1) * x2\n\t        return self.w3(hidden)\n\ttry:\n\t    from xformers.ops import SwiGLU\n\texcept ImportError:\n\t    SwiGLU = SwiGLUFFN\n", "class SwiGLUFFNFused(SwiGLU):\n\t    def __init__(\n\t        self,\n\t        in_features: int,\n\t        hidden_features: Optional[int] = None,\n\t        out_features: Optional[int] = None,\n\t        bias: bool = True,\n\t    ) -> None:\n\t        out_features = out_features or in_features\n\t        hidden_features = hidden_features or in_features\n", "        hidden_features = (int(hidden_features * 2 / 3) + 7) // 8 * 8\n\t        super().__init__(\n\t            in_features=in_features,\n\t            hidden_features=hidden_features,\n\t            out_features=out_features,\n\t            bias=bias,\n\t        )\n"]}
{"filename": "muse_maskgit_pytorch/modules/__init__.py", "chunked_list": ["from .attention import CrossAttention, MemoryEfficientCrossAttention\n\tfrom .mlp import SwiGLU, SwiGLUFFN, SwiGLUFFNFused\n\t__all__ = [\n\t    \"SwiGLU\",\n\t    \"SwiGLUFFN\",\n\t    \"SwiGLUFFNFused\",\n\t    \"CrossAttention\",\n\t    \"MemoryEfficientCrossAttention\",\n\t]\n"]}
{"filename": "muse_maskgit_pytorch/modules/attention.py", "chunked_list": ["from inspect import isfunction\n\tfrom typing import Any, Callable, Optional\n\tfrom einops import rearrange\n\tfrom torch import nn\n\ttry:\n\t    from xformers.ops import memory_efficient_attention\n\texcept ImportError:\n\t    memory_efficient_attention = None\n\tdef exists(x):\n\t    return x is not None\n", "def default(val, d):\n\t    if exists(val):\n\t        return val\n\t    return d() if isfunction(d) else d\n\tclass CrossAttention(nn.Module):\n\t    def __init__(\n\t        self,\n\t        query_dim,\n\t        context_dim=None,\n\t        heads=8,\n", "        dim_head=64,\n\t        dropout=0.0,\n\t    ):\n\t        super().__init__()\n\t        inner_dim = dim_head * heads\n\t        context_dim = default(context_dim, query_dim)\n\t        self.scale = dim_head**-0.5\n\t        self.heads = heads\n\t        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n\t        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n", "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\t        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n\t    def forward(self, x, context=None):\n\t        h = self.heads\n\t        q = self.to_q(x)\n\t        context = default(context, x)\n\t        k = self.to_k(context)\n\t        v = self.to_v(context)\n\t        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> (b h) n d\", h=h), (q, k, v))\n\t        q = q * self.scale\n", "        sim = q @ k.transpose(-2, -1)\n\t        sim = sim.softmax(dim=-1)\n\t        out = sim @ v\n\t        out = rearrange(out, \"(b h) n d -> b n (h d)\", h=h)\n\t        return self.to_out(out)\n\tclass MemoryEfficientCrossAttention(nn.Module):\n\t    # https://github.com/MatthieuTPHR/diffusers/blob/d80b531ff8060ec1ea982b65a1b8df70f73aa67c/src/diffusers/models/attention.py#L223\n\t    def __init__(\n\t        self,\n\t        query_dim,\n", "        context_dim=None,\n\t        heads=8,\n\t        dim_head=64,\n\t        dropout=0.0,\n\t    ):\n\t        super().__init__()\n\t        inner_dim = dim_head * heads\n\t        context_dim = default(context_dim, query_dim)\n\t        self.heads = heads\n\t        self.dim_head = dim_head\n", "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n\t        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n\t        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n\t        self.to_out = nn.Sequential(nn.Linear(inner_dim, query_dim), nn.Dropout(dropout))\n\t        self.attention_op: Optional[Callable] = None\n\t    def forward(self, x, context=None):\n\t        q = self.to_q(x)\n\t        context = default(context, x)\n\t        k = self.to_k(context)\n\t        v = self.to_v(context)\n", "        b, _, _ = q.shape\n\t        q, k, v = map(\n\t            lambda t: t.unsqueeze(3)\n\t            .reshape(b, t.shape[1], self.heads, self.dim_head)\n\t            .permute(0, 2, 1, 3)\n\t            .reshape(b * self.heads, t.shape[1], self.dim_head)\n\t            .contiguous(),\n\t            (q, k, v),\n\t        )\n\t        out = memory_efficient_attention(q, k, v, attn_bias=None, op=self.attention_op)\n", "        out = (\n\t            out.unsqueeze(0)\n\t            .reshape(b, self.heads, out.shape[1], self.dim_head)\n\t            .permute(0, 2, 1, 3)\n\t            .reshape(b, out.shape[1], self.heads * self.dim_head)\n\t        )\n\t        return self.to_out(out)\n"]}
{"filename": "muse_maskgit_pytorch/attn/sdp_attn.py", "chunked_list": ["from typing import Optional\n\timport torch\n\timport torch.nn.functional as F\n\tfrom einops import rearrange, repeat\n\tfrom torch import BoolTensor, FloatTensor, nn\n\tfrom torch.nn.functional import scaled_dot_product_attention\n\tdef l2norm(t):\n\t    return F.normalize(t, dim=-1)\n\tclass LayerNorm(nn.Module):\n\t    def __init__(self, dim):\n", "        super().__init__()\n\t        self.gamma = nn.Parameter(torch.ones(dim))\n\t        self.register_buffer(\"beta\", torch.zeros(dim))\n\t    def forward(self, x):\n\t        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, dim_head=64, heads=8, cross_attend=False, scale=8):\n\t        super().__init__()\n\t        self.heads = heads\n\t        inner_dim = dim_head * heads\n", "        self.cross_attend = cross_attend\n\t        self.norm = LayerNorm(dim)\n\t        self.null_kv = nn.Parameter(torch.randn(2, heads, 1, dim_head))\n\t        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n\t        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n\t        typical_scale = dim_head**-0.5\n\t        scale_ratio = scale / typical_scale\n\t        self.q_scale = nn.Parameter(torch.full((dim_head,), scale_ratio))\n\t        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\t        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n", "    def forward(\n\t        self, x: FloatTensor, context: Optional[FloatTensor] = None, context_mask: Optional[BoolTensor] = None\n\t    ):\n\t        assert (context is None) != self.cross_attend\n\t        h = self.heads\n\t        # TODO: you could fuse this layernorm with the linear that follows it, e.g. via TransformerEngine\n\t        x = self.norm(x)\n\t        kv_input = context if self.cross_attend else x\n\t        # TODO: to_q and to_kvs could be combined into one to_qkv\n\t        q, k, v = (self.to_q(x), *self.to_kv(kv_input).chunk(2, dim=-1))\n", "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n\t        nk, nv = self.null_kv\n\t        nk, nv = map(lambda t: repeat(t, \"h 1 d -> b h 1 d\", b=x.shape[0]), (nk, nv))\n\t        k = torch.cat((nk, k), dim=-2)\n\t        v = torch.cat((nv, v), dim=-2)\n\t        q, k = map(l2norm, (q, k))\n\t        q = q * self.q_scale\n\t        k = k * self.k_scale\n\t        if context_mask is not None:\n\t            context_mask = rearrange(context_mask, \"b j -> b 1 1 j\")\n", "            context_mask = F.pad(context_mask, (1, 0), value=True)\n\t        out: FloatTensor = scaled_dot_product_attention(q, k, v, context_mask)\n\t        out = rearrange(out, \"b h n d -> b n (h d)\")\n\t        return self.to_out(out)\n"]}
{"filename": "muse_maskgit_pytorch/attn/xformers_attn.py", "chunked_list": ["from typing import Optional\n\timport torch\n\timport torch.nn.functional as F\n\tfrom einops import rearrange, repeat\n\tfrom torch import BoolTensor, FloatTensor, nn\n\tfrom xformers.ops import memory_efficient_attention\n\tdef l2norm(t):\n\t    return F.normalize(t, dim=-1)\n\tclass LayerNorm(nn.Module):\n\t    def __init__(self, dim):\n", "        super().__init__()\n\t        self.gamma = nn.Parameter(torch.ones(dim))\n\t        self.register_buffer(\"beta\", torch.zeros(dim))\n\t    def forward(self, x):\n\t        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, dim_head=64, heads=8, cross_attend=False, scale=8):\n\t        super().__init__()\n\t        self.heads = heads\n\t        inner_dim = dim_head * heads\n", "        self.cross_attend = cross_attend\n\t        self.norm = LayerNorm(dim)\n\t        self.null_kv = nn.Parameter(torch.randn(2, heads, 1, dim_head))\n\t        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n\t        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n\t        typical_scale = dim_head**-0.5\n\t        scale_ratio = scale / typical_scale\n\t        self.q_scale = nn.Parameter(torch.full((dim_head,), scale_ratio))\n\t        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\t        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n", "    def forward(\n\t        self, x: FloatTensor, context: Optional[FloatTensor] = None, context_mask: Optional[BoolTensor] = None\n\t    ):\n\t        assert (context is None) != self.cross_attend\n\t        h = self.heads\n\t        # TODO: you could fuse this layernorm with the linear that follows it, e.g. via TransformerEngine\n\t        x = self.norm(x)\n\t        kv_input = context if self.cross_attend else x\n\t        # TODO: to_q and to_kvs could be combined into one to_qkv\n\t        q, k, v = (self.to_q(x), *self.to_kv(kv_input).chunk(2, dim=-1))\n", "        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b n h d\", h=h), (q, k, v))\n\t        nk, nv = self.null_kv\n\t        nk, nv = map(lambda t: repeat(t, \"h 1 d -> b 1 h d\", b=x.shape[0]), (nk, nv))\n\t        k = torch.cat((nk, k), dim=-3)\n\t        v = torch.cat((nv, v), dim=-3)\n\t        q, k = map(l2norm, (q, k))\n\t        q = q * self.q_scale\n\t        k = k * self.k_scale\n\t        if context_mask is None:\n\t            attn_bias = None\n", "        else:\n\t            context_mask = F.pad(context_mask, (1, 0), value=True)\n\t            context_mask = rearrange(context_mask, \"b j -> b 1 1 j\")\n\t            attn_bias = torch.where(context_mask is True, 0.0, -10000.0)\n\t            attn_bias = attn_bias.expand(-1, h, q.size(1), -1)\n\t        out: FloatTensor = memory_efficient_attention(q, k, v, attn_bias)\n\t        out = rearrange(out, \"b n h d -> b n (h d)\")\n\t        return self.to_out(out)\n"]}
{"filename": "muse_maskgit_pytorch/attn/__init__.py", "chunked_list": []}
{"filename": "muse_maskgit_pytorch/attn/attn_test.py", "chunked_list": ["import torch\n\tfrom torch import BoolTensor, FloatTensor, allclose, arange, manual_seed, no_grad, randn\n\tfrom torch.nn.functional import pad\n\tfrom muse_maskgit_pytorch.attn.ein_attn import Attention as EinAttn\n\tfrom muse_maskgit_pytorch.attn.xformers_attn import Attention as XformersAttn\n\tdevice = torch.device(\"cuda\")\n\tdtype = torch.float32\n\tseed = 42\n\t# realistically this would be 320 in stable-diffusion, but I'm going smaller during testing\n\tvision_dim = 64\n", "attn_init_params = {\n\t    \"dim\": vision_dim,\n\t    \"dim_head\": 64,\n\t    # realistically this would be at least 5\n\t    \"heads\": 2,\n\t    \"cross_attend\": True,\n\t    \"scale\": 8,\n\t}\n\twith no_grad():\n\t    # seed RNG before we initialize any layers, so that both will end up with same params\n", "    manual_seed(seed)\n\t    ein_attn = EinAttn(**attn_init_params).to(device, dtype).eval()\n\t    # commented-out scaled dot product attention because it didn't support flash attn, so we'll try with xformers instead.\n\t    # manual_seed(seed)\n\t    # sdp_attn = SDPAttn(**attn_init_params).to(device, dtype).eval()\n\t    manual_seed(seed)\n\t    xfo_attn = XformersAttn(**attn_init_params).to(device, dtype).eval()\n\t    batch_size = 2\n\t    # realistically this would be 64**2 in stable-diffusion\n\t    vision_tokens = 32**2  # 1024\n", "    # generate rand on-CPU for cross-platform determinism of results\n\t    x: FloatTensor = randn(batch_size, vision_tokens, vision_dim, dtype=dtype).to(device)\n\t    # I've said text here simply as an example of something you could cross-attend to\n\t    text_tokens = 16  # CLIP would be 77\n\t    # for a *general* cross-attention Module:\n\t    # kv_in_dim could differ from q_in_dim, but this attention Module requires x and context to have same dim.\n\t    text_dim = vision_dim\n\t    context: FloatTensor = randn(batch_size, text_tokens, text_dim, dtype=dtype).to(device)\n\t    # attend to just the first two tokens in each text condition (e.g. if both were uncond, so [BOS, EOS] followed by PAD tokens)\n\t    context_mask: BoolTensor = (arange(text_tokens, device=device) < 2).expand(batch_size, -1).contiguous()\n", "    # for xformers cutlassF kernel: masks are only supported for keys whose lengths are multiples of 8:\n\t    # https://gist.github.com/Birch-san/0c36d228e1d4b881a06d1c6e5289d569\n\t    # so, we add whatever we feel like to the end of the key to extend it to a multiple of 8,\n\t    # and add \"discard\" tokens to the mask to get rid of the excess\n\t    # note: muse will add an extra \"null\" token to our context, so we'll account for that in advance\n\t    mask_length = context_mask.shape[-1] + 1\n\t    extra_tokens_needed = 8 - (mask_length % 8)\n\t    # 0-pad mask to multiple of 8 tokens\n\t    xfo_context_mask = pad(context_mask, (0, extra_tokens_needed))\n\t    # replicate-pad embedding to multiple of 8 tokens (mask will hide the extra tokens)\n", "    xfo_context = pad(context, (0, 0, 0, extra_tokens_needed), \"replicate\")\n\t    ein_result: FloatTensor = ein_attn.forward(x, context, context_mask)\n\t    # sdp attn works, but only supports flash attn when context_mask is None.\n\t    # with sdp_kernel(enable_math=False):\n\t    #     sdp_result: FloatTensor = sdp_attn.forward(x, context, context_mask)\n\t    xfo_attn: FloatTensor = xfo_attn.forward(x, xfo_context, xfo_context_mask)\n\t    # default rtol\n\t    rtol = 1e-5\n\t    # atol would normally be 1e-8\n\t    atol = 5e-7\n", "    # assert allclose(ein_result, sdp_result, rtol=rtol, atol=atol), f\"looks like attention implementations weren't equivalent, to tolerance rtol={rtol}, atol={atol}\"\n\t    if not allclose(ein_result, xfo_attn, rtol=rtol, atol=atol):\n\t        raise RuntimeError(\n\t            f\"looks like attention implementations weren't equivalent, to tolerance rtol={rtol}, atol={atol}\"\n\t        )\n"]}
{"filename": "muse_maskgit_pytorch/attn/ein_attn.py", "chunked_list": ["import torch\n\timport torch.nn.functional as F\n\tfrom einops import rearrange, repeat\n\tfrom torch import einsum, nn\n\t# helpers\n\tdef exists(val):\n\t    return val is not None\n\tdef l2norm(t):\n\t    return F.normalize(t, dim=-1)\n\tclass LayerNorm(nn.Module):\n", "    def __init__(self, dim):\n\t        super().__init__()\n\t        self.gamma = nn.Parameter(torch.ones(dim))\n\t        self.register_buffer(\"beta\", torch.zeros(dim))\n\t    def forward(self, x):\n\t        return F.layer_norm(x, x.shape[-1:], self.gamma, self.beta)\n\tclass Attention(nn.Module):\n\t    def __init__(self, dim, dim_head=64, heads=8, cross_attend=False, scale=8):\n\t        super().__init__()\n\t        self.scale = scale\n", "        self.heads = heads\n\t        inner_dim = dim_head * heads\n\t        self.cross_attend = cross_attend\n\t        self.norm = LayerNorm(dim)\n\t        self.null_kv = nn.Parameter(torch.randn(2, heads, 1, dim_head))\n\t        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n\t        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n\t        self.q_scale = nn.Parameter(torch.ones(dim_head))\n\t        self.k_scale = nn.Parameter(torch.ones(dim_head))\n\t        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n", "    def forward(self, x, context=None, context_mask=None):\n\t        assert not (exists(context) ^ self.cross_attend)\n\t        h = self.heads\n\t        x = self.norm(x)\n\t        kv_input = context if self.cross_attend else x\n\t        q, k, v = (self.to_q(x), *self.to_kv(kv_input).chunk(2, dim=-1))\n\t        q, k, v = map(lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=h), (q, k, v))\n\t        nk, nv = self.null_kv\n\t        nk, nv = map(lambda t: repeat(t, \"h 1 d -> b h 1 d\", b=x.shape[0]), (nk, nv))\n\t        k = torch.cat((nk, k), dim=-2)\n", "        v = torch.cat((nv, v), dim=-2)\n\t        q, k = map(l2norm, (q, k))\n\t        q = q * self.q_scale\n\t        k = k * self.k_scale\n\t        sim = einsum(\"b h i d, b h j d -> b h i j\", q, k) * self.scale\n\t        if exists(context_mask):\n\t            context_mask = rearrange(context_mask, \"b j -> b 1 1 j\")\n\t            context_mask = F.pad(context_mask, (1, 0), value=True)\n\t            mask_value = -torch.finfo(sim.dtype).max\n\t            sim = sim.masked_fill(~context_mask, mask_value)\n", "        attn = sim.softmax(dim=-1)\n\t        out = einsum(\"b h i j, b h j d -> b h i d\", attn, v)\n\t        out = rearrange(out, \"b h n d -> b n (h d)\")\n\t        return self.to_out(out)\n"]}
{"filename": "muse_maskgit_pytorch/vqvae/layers.py", "chunked_list": ["import torch\n\tfrom diffusers.utils import is_xformers_available\n\tfrom einops import rearrange\n\tfrom einops.layers.torch import Rearrange\n\tfrom torch import nn\n\tfrom muse_maskgit_pytorch.modules import CrossAttention, MemoryEfficientCrossAttention, SwiGLUFFNFused\n\tdef pair(t):\n\t    return t if isinstance(t, tuple) else (t, t)\n\tclass FeedForward(nn.Module):\n\t    def __init__(self, dim, mlp_dim, dropout=0.0):\n", "        super().__init__()\n\t        self.w_1 = nn.Linear(dim, mlp_dim)\n\t        self.act = nn.GELU()\n\t        self.dropout = nn.Dropout(p=dropout)\n\t        self.w_2 = nn.Linear(mlp_dim, dim)\n\t    def forward(self, x):\n\t        x = self.w_1(x)\n\t        x = self.act(x)\n\t        x = self.dropout(x)\n\t        x = self.w_2(x)\n", "        return x\n\tclass LayerScale(nn.Module):\n\t    def __init__(self, dim, init_values=1e-5, inplace=False):\n\t        super().__init__()\n\t        self.inplace = inplace\n\t        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n\t    def forward(self, x):\n\t        return x.mul_(self.gamma) if self.inplace else x * self.gamma\n\tclass Layer(nn.Module):\n\t    ATTENTION_MODES = {\"vanilla\": CrossAttention, \"xformer\": MemoryEfficientCrossAttention}\n", "    def __init__(self, dim, dim_head, mlp_dim, num_head=8, dropout=0.0):\n\t        super().__init__()\n\t        attn_mode = \"xformer\" if is_xformers_available() else \"vanilla\"\n\t        attn_cls = self.ATTENTION_MODES[attn_mode]\n\t        self.norm1 = nn.LayerNorm(dim)\n\t        self.attn1 = attn_cls(query_dim=dim, heads=num_head, dim_head=dim_head, dropout=dropout)\n\t        self.norm2 = nn.LayerNorm(dim)\n\t        self.ffnet = SwiGLUFFNFused(in_features=dim, hidden_features=mlp_dim)\n\t    def forward(self, x):\n\t        x = self.attn1(self.norm1(x)) + x\n", "        x = self.ffnet(self.norm2(x)) + x\n\t        return x\n\tclass Transformer(nn.Module):\n\t    def __init__(self, dim, depth, num_head, dim_head, mlp_dim, dropout=0.0):\n\t        super().__init__()\n\t        self.layers = nn.Sequential(*[Layer(dim, dim_head, mlp_dim, num_head, dropout) for i in range(depth)])\n\t    def forward(self, x):\n\t        x = self.layers(x)\n\t        return x\n\tclass Encoder(nn.Module):\n", "    def __init__(\n\t        self,\n\t        image_size,\n\t        patch_size,\n\t        dim,\n\t        depth,\n\t        num_head,\n\t        mlp_dim,\n\t        in_channels=3,\n\t        dim_head=64,\n", "        dropout=0.0,\n\t    ):\n\t        super().__init__()\n\t        self.image_size = image_size\n\t        self.patch_size = patch_size\n\t        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n\t        self.to_patch_embedding = nn.Sequential(\n\t            nn.Conv2d(in_channels, dim, kernel_size=patch_size, stride=patch_size, bias=False),\n\t            Rearrange(\"b c h w -> b (h w) c\"),\n\t        )\n", "        scale = dim**-0.5\n\t        num_patches = (image_size // patch_size) ** 2\n\t        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, dim) * scale)\n\t        self.norm_pre = nn.LayerNorm(dim)\n\t        self.transformer = Transformer(dim, depth, num_head, dim_head, mlp_dim, dropout)\n\t        self.initialize_weights()\n\t    def initialize_weights(self):\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n", "            torch.nn.init.xavier_uniform_(m.weight)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def forward(self, x):\n\t        x = self.to_patch_embedding(x)\n\t        x = x + self.position_embedding\n\t        x = self.norm_pre(x)\n", "        x = self.transformer(x)\n\t        return x\n\tclass Decoder(nn.Module):\n\t    def __init__(\n\t        self,\n\t        image_size,\n\t        patch_size,\n\t        dim,\n\t        depth,\n\t        num_head,\n", "        mlp_dim,\n\t        out_channels=3,\n\t        dim_head=64,\n\t        dropout=0.0,\n\t    ):\n\t        super().__init__()\n\t        self.image_size = image_size\n\t        self.patch_size = patch_size\n\t        assert image_size % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n\t        scale = dim**-0.5\n", "        num_patches = (image_size // patch_size) ** 2\n\t        self.position_embedding = nn.Parameter(torch.randn(1, num_patches, dim) * scale)\n\t        self.transformer = Transformer(dim, depth, num_head, dim_head, mlp_dim, dropout)\n\t        self.norm = nn.LayerNorm(dim)\n\t        self.proj = nn.Linear(dim, out_channels * patch_size * patch_size, bias=True)\n\t        self.initialize_weights()\n\t    def initialize_weights(self):\n\t        self.apply(self._init_weights)\n\t    def _init_weights(self, m):\n\t        if isinstance(m, nn.Linear):\n", "            torch.nn.init.xavier_uniform_(m.weight)\n\t            if isinstance(m, nn.Linear) and m.bias is not None:\n\t                nn.init.constant_(m.bias, 0)\n\t        elif isinstance(m, nn.LayerNorm):\n\t            nn.init.constant_(m.bias, 0)\n\t            nn.init.constant_(m.weight, 1.0)\n\t    def forward(self, x):\n\t        x = x + self.position_embedding\n\t        x = self.transformer(x)\n\t        x = self.norm(x)\n", "        x = self.proj(x)\n\t        x = rearrange(\n\t            x,\n\t            \"b (h w) (p1 p2 c) -> b c (h p1) (w p2)\",\n\t            h=self.image_size // self.patch_size,\n\t            p1=self.patch_size,\n\t            p2=self.patch_size,\n\t        )\n\t        return x\n"]}
{"filename": "muse_maskgit_pytorch/vqvae/config.py", "chunked_list": ["from pydantic import BaseModel, Field\n\tclass EncoderConfig(BaseModel):\n\t    image_size: int = Field(...)\n\t    patch_size: int = Field(...)\n\t    dim: int = Field(...)\n\t    depth: int = Field(...)\n\t    num_head: int = Field(...)\n\t    mlp_dim: int = Field(...)\n\t    in_channels: int = Field(...)\n\t    dim_head: int = Field(...)\n", "    dropout: float = Field(...)\n\tclass DecoderConfig(BaseModel):\n\t    image_size: int = Field(...)\n\t    patch_size: int = Field(...)\n\t    dim: int = Field(...)\n\t    depth: int = Field(...)\n\t    num_head: int = Field(...)\n\t    mlp_dim: int = Field(...)\n\t    out_channels: int = Field(...)\n\t    dim_head: int = Field(...)\n", "    dropout: float = Field(...)\n\tclass VQVAEConfig(BaseModel):\n\t    n_embed: int = Field(...)\n\t    embed_dim: int = Field(...)\n\t    beta: float = Field(...)\n\t    enc: EncoderConfig = Field(...)\n\t    dec: DecoderConfig = Field(...)\n\tVIT_S_CONFIG = VQVAEConfig(\n\t    n_embed=8192,\n\t    embed_dim=32,\n", "    beta=0.25,\n\t    enc=EncoderConfig(\n\t        image_size=256,\n\t        patch_size=8,\n\t        dim=512,\n\t        depth=8,\n\t        num_head=8,\n\t        mlp_dim=2048,\n\t        in_channels=3,\n\t        dim_head=64,\n", "        dropout=0.0,\n\t    ),\n\t    dec=DecoderConfig(\n\t        image_size=256,\n\t        patch_size=8,\n\t        dim=512,\n\t        depth=8,\n\t        num_head=8,\n\t        mlp_dim=2048,\n\t        out_channels=3,\n", "        dim_head=64,\n\t        dropout=0.0,\n\t    ),\n\t)\n"]}
{"filename": "muse_maskgit_pytorch/vqvae/quantize.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\timport torch.nn.functional as F\n\tclass VectorQuantize(nn.Module):\n\t    def __init__(self, n_e, vq_embed_dim, beta=0.25):\n\t        super().__init__()\n\t        self.n_e = n_e\n\t        self.vq_embed_dim = vq_embed_dim\n\t        self.beta = beta\n\t        self.embedding = nn.Embedding(self.n_e, self.vq_embed_dim)\n", "        self.embedding.weight.data.normal_()\n\t    def forward(self, z):\n\t        z = F.normalize(z, p=2, dim=-1)\n\t        z_flattened = z.view(-1, self.vq_embed_dim)\n\t        embed_norm = F.normalize(self.embedding.weight, p=2, dim=-1)\n\t        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\t        d = (\n\t            torch.sum(z_flattened**2, dim=1, keepdim=True)\n\t            + torch.sum(embed_norm**2, dim=1)\n\t            - 2 * torch.einsum(\"bd,nd->bn\", z_flattened, embed_norm)\n", "        )\n\t        encoding_indices = torch.argmin(d, dim=1).view(*z.shape[:-1])\n\t        z_q = self.embedding(encoding_indices).view(z.shape)\n\t        z_q = F.normalize(z_q, p=2, dim=-1)\n\t        # compute loss for embedding\n\t        loss = self.beta * torch.mean((z_q.detach() - z) ** 2) + torch.mean((z_q - z.detach()) ** 2)\n\t        # preserve gradients\n\t        z_q = z + (z_q - z).detach()\n\t        return z_q, loss, encoding_indices\n\t    def decode_ids(self, indices):\n", "        z_q = self.embedding(indices)\n\t        z_q = F.normalize(z_q, p=2, dim=-1)\n\t        return z_q\n"]}
{"filename": "muse_maskgit_pytorch/vqvae/discriminator.py", "chunked_list": ["import functools\n\timport torch.nn as nn\n\tdef weights_init(m):\n\t    classname = m.__class__.__name__\n\t    if classname.find(\"Conv\") != -1:\n\t        nn.init.normal_(m.weight.data, 0.0, 0.02)\n\t    elif classname.find(\"BatchNorm\") != -1:\n\t        nn.init.normal_(m.weight.data, 1.0, 0.02)\n\t        nn.init.constant_(m.bias.data, 0)\n\tclass NLayerDiscriminator(nn.Module):\n", "    \"\"\"Defines a PatchGAN discriminator\"\"\"\n\t    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n\t        \"\"\"Construct a PatchGAN discriminator\n\t        Parameters:\n\t            input_nc (int)  -- the number of channels in input images\n\t            ndf (int)       -- the number of filters in the last conv layer\n\t            n_layers (int)  -- the number of conv layers in the discriminator\n\t            norm_layer      -- normalization layer\n\t        \"\"\"\n\t        super().__init__()\n", "        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n\t            use_bias = norm_layer.func == nn.InstanceNorm2d\n\t        else:\n\t            use_bias = norm_layer == nn.InstanceNorm2d\n\t        kw = 4\n\t        padw = 1\n\t        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n\t        nf_mult = 1\n\t        nf_mult_prev = 1\n\t        for n in range(1, n_layers):  # gradually increase the number of filters\n", "            nf_mult_prev = nf_mult\n\t            nf_mult = min(2**n, 8)\n\t            sequence += [\n\t                nn.Conv2d(\n\t                    ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias\n\t                ),\n\t                norm_layer(ndf * nf_mult),\n\t                nn.LeakyReLU(0.2, True),\n\t            ]\n\t        nf_mult_prev = nf_mult\n", "        nf_mult = min(2**n_layers, 8)\n\t        sequence += [\n\t            nn.Conv2d(\n\t                ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias\n\t            ),\n\t            norm_layer(ndf * nf_mult),\n\t            nn.LeakyReLU(0.2, True),\n\t        ]\n\t        sequence += [\n\t            nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)\n", "        ]  # output 1 channel prediction map\n\t        self.model = nn.Sequential(*sequence)\n\t        self.apply(self.init_func)\n\t    def forward(self, input):\n\t        \"\"\"Standard forward.\"\"\"\n\t        return self.model(input)\n\t    def init_func(self, m):  # define the initialization function\n\t        init_gain = 0.02\n\t        classname = m.__class__.__name__\n\t        if hasattr(m, \"weight\") and (classname.find(\"Conv\") != -1 or classname.find(\"Linear\") != -1):\n", "            nn.init.normal_(m.weight.data, 0.0, init_gain)\n\t            if hasattr(m, \"bias\") and m.bias is not None:\n\t                nn.init.constant_(m.bias.data, 0.0)\n\t        elif (\n\t            classname.find(\"BatchNorm2d\") != -1\n\t        ):  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n\t            nn.init.normal_(m.weight.data, 1.0, init_gain)\n\t            nn.init.constant_(m.bias.data, 0.0)\n"]}
{"filename": "muse_maskgit_pytorch/vqvae/__init__.py", "chunked_list": ["from .config import VQVAEConfig\n\tfrom .vqvae import VQVAE\n\t__all__ = [\n\t    \"VQVAE\",\n\t    \"VQVAEConfig\",\n\t]\n"]}
{"filename": "muse_maskgit_pytorch/vqvae/vqvae.py", "chunked_list": ["import logging\n\timport torch\n\timport torch.nn as nn\n\tfrom diffusers import ConfigMixin, ModelMixin\n\tfrom diffusers.configuration_utils import register_to_config\n\tfrom .layers import Decoder, Encoder\n\tfrom .quantize import VectorQuantize\n\tlogger = logging.getLogger(__name__)\n\tclass VQVAE(ModelMixin, ConfigMixin):\n\t    @register_to_config\n", "    def __init__(self, n_embed, embed_dim, beta, enc, dec, **kwargs):\n\t        super().__init__()\n\t        self.encoder = Encoder(**enc)\n\t        self.decoder = Decoder(**dec)\n\t        self.prev_quant = nn.Linear(enc[\"dim\"], embed_dim)\n\t        self.quantize = VectorQuantize(n_embed, embed_dim, beta)\n\t        self.post_quant = nn.Linear(embed_dim, dec[\"dim\"])\n\t    def freeze(self):\n\t        self.eval()\n\t        self.requires_grad_(False)\n", "    def encode(self, x):\n\t        x = self.encoder(x)\n\t        x = self.prev_quant(x)\n\t        x, loss, indices = self.quantize(x)\n\t        return x, loss, indices\n\t    def decode(self, x):\n\t        x = self.post_quant(x)\n\t        x = self.decoder(x)\n\t        return x.clamp(-1.0, 1.0)\n\t    def forward(self, inputs: torch.FloatTensor):\n", "        z, loss, _ = self.encode(inputs)\n\t        rec = self.decode(z)\n\t        return rec, loss\n\t    def encode_to_ids(self, inputs):\n\t        _, _, indices = self.encode(inputs)\n\t        return indices\n\t    def decode_from_ids(self, indice):\n\t        z_q = self.quantize.decode_ids(indice)\n\t        img = self.decode(z_q)\n\t        return img\n", "    def __call__(self, inputs: torch.FloatTensor):\n\t        return self.forward(inputs)\n"]}
