{"filename": "tests/test_energy_efficiency_optimization.py", "chunked_list": ["import unittest\n\tfrom src.utils.config import Config\n\tfrom src.utils.logger import Logger\n\tfrom src.data_preparation.data_loader import DataLoader\n\tfrom src.data_preparation.data_cleaning import DataCleaning\n\tfrom src.data_preparation.data_extraction import DataExtraction\n\tfrom src.data_preparation.data_transformation import DataTransformation\n\tfrom src.dynamic_network_optimization.energy_efficiency_optimization import EnergyEfficiencyOptimization\n\tclass TestEnergyEfficiencyOptimization(unittest.TestCase):\n\t    @classmethod\n", "    def setUpClass(cls):\n\t        cls.config = Config()\n\t        cls.logger = Logger()\n\t        cls.data_loader = DataLoader(cls.config, cls.logger)\n\t        cls.data_cleaning = DataCleaning(cls.config, cls.logger)\n\t        cls.data_extraction = DataExtraction(cls.config, cls.logger)\n\t        cls.data_transformation = DataTransformation(cls.config, cls.logger)\n\t        cls.eeo = EnergyEfficiencyOptimization(cls.config, cls.logger)\n\t    def test_energy_efficiency_optimization(self):\n\t        # Load test data\n", "        test_data = self.data_loader.load_test_data(\"test_data.csv\")\n\t        # Clean test data\n\t        test_data = self.data_cleaning.clean_test_data(test_data)\n\t        # Extract relevant features from test data\n\t        test_data = self.data_extraction.extract_test_data(test_data)\n\t        # Transform test data\n\t        test_data = self.data_transformation.transform_test_data(test_data)\n\t        # Run energy efficiency optimization\n\t        result = self.eeo.run(test_data)\n\t        # Assert that result is not empty\n", "        self.assertIsNotNone(result)\n\t        # Assert that result is a dictionary\n\t        self.assertIsInstance(result, dict)\n\t        # Assert that result contains expected keys\n\t        self.assertSetEqual(set(result.keys()), {\"nodes\", \"edges\", \"cost\"})\n\t        # Assert that result values are of correct type and format\n\t        self.assertIsInstance(result[\"nodes\"], list)\n\t        self.assertIsInstance(result[\"edges\"], list)\n\t        self.assertIsInstance(result[\"cost\"], float)\n\t        self.assertGreater(result[\"cost\"], 0.0)\n", "if __name__ == \"__main__\":\n\t    unittest.main()\n"]}
{"filename": "tests/test_network_anomaly_detection.py", "chunked_list": ["import unittest\n\tfrom unittest.mock import Mock, patch\n\tfrom src.utils.logger import Logger\n\tfrom src.utils.data_loader import DataLoader\n\tfrom src.utils.data_preprocessing import DataPreprocessor\n\tfrom src.models.network_anomaly_detection import NetworkAnomalyDetection\n\tclass TestNetworkAnomalyDetection(unittest.TestCase):\n\t    @classmethod\n\t    def setUpClass(cls):\n\t        cls.logger = Logger()\n", "        cls.logger.disable_logging()\n\t        cls.data_loader = DataLoader()\n\t        cls.preprocessor = DataPreprocessor()\n\t        cls.nad_model = NetworkAnomalyDetection()\n\t    @patch('src.models.network_anomaly_detection.NetworkAnomalyDetection.detect_anomaly')\n\t    def test_detect_anomaly(self, mock_detect_anomaly):\n\t        # Define test data\n\t        test_data = self.data_loader.load_data('test_data.csv')\n\t        preprocessed_data = self.preprocessor.preprocess_data(test_data)\n\t        test_features = preprocessed_data.drop(columns=['timestamp'])\n", "        # Mock the predict method and return a dummy prediction\n\t        mock_detect_anomaly.return_value = [0, 0, 1, 1, 0]\n\t        # Test the predict method\n\t        predictions = self.nad_model.detect_anomaly(test_features)\n\t        self.assertEqual(len(predictions), len(test_data))\n\t        self.assertListEqual(predictions, mock_detect_anomaly.return_value)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/test_dynamic_network_optimization.py", "chunked_list": ["import unittest\n\timport numpy as np\n\tfrom src.models.dynamic_network_optimization import DynamicNetworkOptimization\n\tclass TestDynamicNetworkOptimization(unittest.TestCase):\n\t    def setUp(self):\n\t        # Initialize test data\n\t        self.network = np.array([\n\t            [0, 10, 15, 20],\n\t            [10, 0, 35, 25],\n\t            [15, 35, 0, 30],\n", "            [20, 25, 30, 0]\n\t        ])\n\t        self.demand = np.array([\n\t            [0, 200, 300, 100],\n\t            [200, 0, 100, 300],\n\t            [300, 100, 0, 200],\n\t            [100, 300, 200, 0]\n\t        ])\n\t    def test_constructor(self):\n\t        # Test if object is initialized with the correct attributes\n", "        dno = DynamicNetworkOptimization(self.network, self.demand)\n\t        self.assertTrue((dno.network == self.network).all())\n\t        self.assertTrue((dno.demand == self.demand).all())\n\t        self.assertEqual(dno.num_nodes, len(self.network))\n\t    def test_optimal_flow(self):\n\t        # Test if the optimal flow is calculated correctly\n\t        dno = DynamicNetworkOptimization(self.network, self.demand)\n\t        flow = dno.optimal_flow()\n\t        self.assertEqual(flow.sum(), self.demand.sum())\n\t        # Add more specific tests based on known solutions or theoretical bounds\n", "    def test_infeasible_demand(self):\n\t        # Test if the function raises an error when demand is infeasible\n\t        dno = DynamicNetworkOptimization(self.network, self.demand + 1000)\n\t        with self.assertRaises(ValueError):\n\t            dno.optimal_flow()\n\t    def test_negative_flow(self):\n\t        # Test if the function raises an error when negative flow is generated\n\t        dno = DynamicNetworkOptimization(self.network, self.demand)\n\t        dno.network[0, 1] = -10\n\t        with self.assertRaises(ValueError):\n", "            dno.optimal_flow()\n\t    def test_empty_network(self):\n\t        # Test if the function raises an error when the network is empty\n\t        dno = DynamicNetworkOptimization(np.array([]), self.demand)\n\t        with self.assertRaises(ValueError):\n\t            dno.optimal_flow()\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "tests/test_predictive_network_planning.py", "chunked_list": ["import pytest\n\timport numpy as np\n\tfrom unittest.mock import patch, MagicMock\n\tfrom src.models.predictive_network_planning.predictive_network_planning import PredictiveNetworkPlanning\n\tclass TestPredictiveNetworkPlanning:\n\t    @pytest.fixture(scope='module')\n\t    def network_planning_model(self):\n\t        model = PredictiveNetworkPlanning()\n\t        return model\n\t    def test_model_loads(self, network_planning_model):\n", "        assert network_planning_model is not None\n\t    def test_model_predict(self, network_planning_model):\n\t        # mock input data\n\t        input_data = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n\t        expected_output = np.array([0, 1])\n\t        # mock predict function of the model\n\t        with patch.object(network_planning_model, 'predict') as mock_predict:\n\t            mock_predict.return_value = expected_output\n\t            # call predict function with input data\n\t            output = network_planning_model.predict(input_data)\n", "            # assert output is as expected\n\t            assert np.array_equal(output, expected_output)\n\t            mock_predict.assert_called_once_with(input_data)\n\t    def test_model_train(self, network_planning_model):\n\t        # mock input and target data\n\t        input_data = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n\t        target_data = np.array([0, 1])\n\t        # mock fit function of the model\n\t        with patch.object(network_planning_model, 'fit') as mock_fit:\n\t            mock_fit.return_value = MagicMock()\n", "            # call fit function with input and target data\n\t            network_planning_model.fit(input_data, target_data)\n\t            # assert fit function was called once with correct arguments\n\t            mock_fit.assert_called_once_with(input_data, target_data)\n"]}
{"filename": "tests/test_data_preparation.py", "chunked_list": ["import unittest\n\timport pandas as pd\n\tfrom src.utils.data_preparation.data_cleaning import clean_data\n\tfrom src.utils.data_preparation.data_extraction import extract_data\n\tfrom src.utils.data_preparation.data_transformation import transform_data\n\tclass TestDataPreparation(unittest.TestCase):\n\t    def setUp(self):\n\t        # Set up test data\n\t        self.raw_data = pd.read_csv(\"tests/test_data/raw_data.csv\")\n\t    def test_clean_data(self):\n", "        # Test data cleaning function\n\t        cleaned_data = clean_data(self.raw_data)\n\t        self.assertIsInstance(cleaned_data, pd.DataFrame)\n\t        self.assertEqual(len(cleaned_data), 4)\n\t        self.assertEqual(cleaned_data.isna().sum().sum(), 0)\n\t    def test_extract_data(self):\n\t        # Test data extraction function\n\t        extracted_data = extract_data(self.raw_data)\n\t        self.assertIsInstance(extracted_data, pd.DataFrame)\n\t        self.assertEqual(len(extracted_data), 4)\n", "        self.assertEqual(len(extracted_data.columns), 3)\n\t    def test_transform_data(self):\n\t        # Test data transformation function\n\t        transformed_data = transform_data(self.raw_data)\n\t        self.assertIsInstance(transformed_data, pd.DataFrame)\n\t        self.assertEqual(len(transformed_data), 4)\n\t        self.assertEqual(len(transformed_data.columns), 2)\n\tif __name__ == '__main__':\n\t    unittest.main()\n"]}
{"filename": "src/main.py", "chunked_list": ["import argparse\n\tfrom datetime import datetime\n\tfrom utils import config\n\tfrom utils.logger import Logger\n\tfrom data_preparation.data_extraction import extract_data\n\tfrom data_preparation.data_cleaning import clean_data\n\tfrom data_preparation.data_transformation import transform_data\n\tfrom models.predictive_network_planning.predict import make_predictions\n\tdef main(args):\n\t    # Set up logger\n", "    log_file = f\"{config.LOGS_DIR}/{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.log\"\n\t    logger = Logger(log_file)\n\t    # Extract data\n\t    logger.log(\"Extracting data...\")\n\t    raw_data = extract_data(args.data_file)\n\t    # Clean data\n\t    logger.log(\"Cleaning data...\")\n\t    clean_data = clean_data(raw_data)\n\t    # Transform data\n\t    logger.log(\"Transforming data...\")\n", "    transformed_data = transform_data(clean_data)\n\t    # Make predictions\n\t    logger.log(\"Making predictions...\")\n\t    predictions = make_predictions(transformed_data)\n\t    # Save predictions to file\n\t    logger.log(\"Saving predictions to file...\")\n\t    predictions_file = f\"{config.PREDICTIONS_DIR}/{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.csv\"\n\t    predictions.to_csv(predictions_file, index=False)\n\t    logger.log(\"Finished.\")\n\tif __name__ == \"__main__\":\n", "    parser = argparse.ArgumentParser(description=\"Run the main program.\")\n\t    parser.add_argument(\"data_file\", type=str, help=\"Path to the raw data file.\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "src/utils/config.py", "chunked_list": ["import yaml\n\tclass Config:\n\t    def __init__(self, config_file):\n\t        with open(config_file, \"r\") as f:\n\t            self._config = yaml.safe_load(f)\n\t    def get(self, key, default=None):\n\t        return self._config.get(key, default)\n\t    def set(self, key, value):\n\t        self._config[key] = value\n\t    def save(self, config_file):\n", "        with open(config_file, \"w\") as f:\n\t            yaml.safe_dump(self._config, f, default_flow_style=False)\n"]}
{"filename": "src/utils/logger.py", "chunked_list": ["import logging\n\tfrom logging.handlers import TimedRotatingFileHandler\n\tfrom datetime import datetime\n\tfrom pathlib import Path\n\tfrom config import LOG_DIR, LOG_LEVEL\n\tclass Logger:\n\t    def __init__(self, logger_name, log_file_name):\n\t        self.logger = logging.getLogger(logger_name)\n\t        self.logger.setLevel(LOG_LEVEL)\n\t        self.log_file_name = log_file_name\n", "        self.log_file_path = Path(LOG_DIR) / self.log_file_name\n\t        self._configure_logger()\n\t    def _configure_logger(self):\n\t        formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n\t        stream_handler = logging.StreamHandler()\n\t        stream_handler.setLevel(LOG_LEVEL)\n\t        stream_handler.setFormatter(formatter)\n\t        self.logger.addHandler(stream_handler)\n\t        file_handler = TimedRotatingFileHandler(\n\t            filename=self.log_file_path,\n", "            when=\"midnight\",\n\t            backupCount=7,\n\t            utc=True,\n\t        )\n\t        file_handler.setLevel(LOG_LEVEL)\n\t        file_handler.setFormatter(formatter)\n\t        self.logger.addHandler(file_handler)\n\t    def info(self, message):\n\t        self.logger.info(message)\n\t    def error(self, message):\n", "        self.logger.error(message)\n\t    def warning(self, message):\n\t        self.logger.warning(message)\n\t    def critical(self, message):\n\t        self.logger.critical(message)\n\t    def exception(self, message):\n\t        self.logger.exception(message)\n"]}
{"filename": "src/utils/api.py", "chunked_list": ["import logging\n\tfrom typing import Dict, List\n\tfrom fastapi import FastAPI\n\tfrom pydantic import BaseModel\n\tfrom models.predictive_network_planning.model import PredictiveNetworkPlanningModel\n\tfrom utils import load_model, predict_single_sample\n\t# Define the input data schema\n\tclass InputData(BaseModel):\n\t    input: List[float]\n\t# Define the response schema\n", "class PredictionResult(BaseModel):\n\t    prediction: float\n\t# Initialize the FastAPI app\n\tapp = FastAPI()\n\t# Load the trained model at startup\n\tmodel = PredictiveNetworkPlanningModel(input_dim=4, hidden_dim=64, num_layers=2, output_dim=1, dropout=0.5)\n\tload_model(model, \"models/predictive_network_planning/best_model.pt\")\n\t@app.post(\"/predict\", response_model=PredictionResult)\n\tasync def predict(data: InputData) -> Dict[str, float]:\n\t    # Extract the input data\n", "    input_data = data.input\n\t    # Predict on the input data\n\t    output = predict_single_sample(model, input_data)\n\t    # Create the response\n\t    response = {\"prediction\": output}\n\t    return response\n\t# Set up logging\n\tlogging.basicConfig(filename=\"logs/api.log\", level=logging.INFO)\n\tif __name__ == \"__main__\":\n\t    import uvicorn\n", "    # Start the FastAPI app using Uvicorn\n\t    uvicorn.run(\"api:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n"]}
{"filename": "src/utils/data_loader.py", "chunked_list": ["import pandas as pd\n\timport torch\n\tfrom torch.utils.data import Dataset\n\tclass PNPDataset(Dataset):\n\t    def __init__(self, data_file: str):\n\t        self.data = pd.read_csv(data_file)\n\t        self.input_data = self.data.iloc[:, :-1].values\n\t        self.output_data = self.data.iloc[:, -1:].values\n\t        self.input_dim = self.input_data.shape[1]\n\t        self.output_dim = self.output_data.shape[1]\n", "    def __len__(self):\n\t        return len(self.data)\n\t    def __getitem__(self, idx):\n\t        inputs = torch.Tensor(self.input_data[idx])\n\t        targets = torch.Tensor(self.output_data[idx])\n\t        return inputs, targets\n"]}
{"filename": "src/utils/model_prediction.py", "chunked_list": ["from typing import List, Dict\n\timport numpy as np\n\timport tensorflow as tf\n\tfrom utils.logger import get_logger\n\tlogger = get_logger(__name__)\n\tdef predict(model: tf.keras.models.Model, data: np.ndarray) -> np.ndarray:\n\t    \"\"\"\n\t    Predicts the output of the model for the given input data.\n\t    Args:\n\t        model: A trained TensorFlow Keras model.\n", "        data: A 3D numpy array of shape (num_samples, timesteps, features) containing the input data.\n\t    Returns:\n\t        A 2D numpy array of shape (num_samples, output_dim) containing the model's predictions.\n\t    \"\"\"\n\t    logger.info(\"Starting model prediction...\")\n\t    predictions = model.predict(data)\n\t    logger.info(\"Prediction completed!\")\n\t    return predictions\n\tdef postprocess(predictions: np.ndarray, output_dim: int) -> List[Dict[str, float]]:\n\t    \"\"\"\n", "    Post-processes the predictions and converts them to a list of dictionaries.\n\t    Args:\n\t        predictions: A 2D numpy array of shape (num_samples, output_dim) containing the model's predictions.\n\t        output_dim: An integer indicating the number of output dimensions.\n\t    Returns:\n\t        A list of dictionaries, where each dictionary contains the predicted values for each output dimension.\n\t    \"\"\"\n\t    logger.info(\"Starting post-processing...\")\n\t    prediction_list = []\n\t    for i in range(predictions.shape[0]):\n", "        prediction_dict = {}\n\t        for j in range(output_dim):\n\t            prediction_dict[f\"output_{j}\"] = float(predictions[i][j])\n\t        prediction_list.append(prediction_dict)\n\t    logger.info(\"Post-processing completed!\")\n\t    return prediction_list\n"]}
{"filename": "src/utils/data_preprocessing.py", "chunked_list": ["import pandas as pd\n\tfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\tdef preprocess_data(data_file_path: str, scaler_type: str = 'min_max') -> pd.DataFrame:\n\t    # Load data\n\t    data = pd.read_csv(data_file_path)\n\t    # Drop irrelevant columns\n\t    data.drop(['id', 'timestamp'], axis=1, inplace=True)\n\t    # Apply scaling\n\t    if scaler_type == 'min_max':\n\t        scaler = MinMaxScaler()\n", "    elif scaler_type == 'standard':\n\t        scaler = StandardScaler()\n\t    else:\n\t        raise ValueError(f\"Unknown scaler type: {scaler_type}\")\n\t    data.iloc[:, :-1] = scaler.fit_transform(data.iloc[:, :-1])\n\t    # Split into input and target\n\t    X = data.iloc[:, :-1]\n\t    y = data.iloc[:, -1]\n\t    return X, y\n"]}
{"filename": "src/data_preparation/data_cleaning.py", "chunked_list": ["import pandas as pd\n\tfrom typing import List\n\tdef drop_columns(df: pd.DataFrame, columns_to_drop: List[str]) -> pd.DataFrame:\n\t    \"\"\"\n\t    Removes the specified columns from a DataFrame.\n\t    Args:\n\t        df: The DataFrame to remove columns from.\n\t        columns_to_drop: A list of strings specifying the names of the columns to remove.\n\t    Returns:\n\t        A new DataFrame with the specified columns removed.\n", "    \"\"\"\n\t    return df.drop(columns=columns_to_drop)\n\tdef drop_null_rows(df: pd.DataFrame, subset: List[str] = None) -> pd.DataFrame:\n\t    \"\"\"\n\t    Removes any rows in the DataFrame that contain null values.\n\t    Args:\n\t        df: The DataFrame to remove null rows from.\n\t        subset: A list of column names to check for null values in. If None, all columns are checked.\n\t    Returns:\n\t        A new DataFrame with the null rows removed.\n", "    \"\"\"\n\t    return df.dropna(subset=subset)\n\tdef replace_null_values(df: pd.DataFrame, replacement_dict: dict) -> pd.DataFrame:\n\t    \"\"\"\n\t    Replaces null values in the DataFrame with specified values.\n\t    Args:\n\t        df: The DataFrame to replace null values in.\n\t        replacement_dict: A dictionary where the keys are the names of columns to replace null values in, and the\n\t                          values are the values to replace null values with.\n\t    Returns:\n", "        A new DataFrame with null values replaced.\n\t    \"\"\"\n\t    return df.fillna(value=replacement_dict)\n\tdef replace_values(df: pd.DataFrame, replacement_dict: dict) -> pd.DataFrame:\n\t    \"\"\"\n\t    Replaces specified values in the DataFrame with other specified values.\n\t    Args:\n\t        df: The DataFrame to replace values in.\n\t        replacement_dict: A dictionary where the keys are the values to replace, and the values are the values to\n\t                          replace them with.\n", "    Returns:\n\t        A new DataFrame with values replaced.\n\t    \"\"\"\n\t    return df.replace(to_replace=replacement_dict)\n"]}
{"filename": "src/data_preparation/data_extraction.py", "chunked_list": ["import os\n\timport pandas as pd\n\tfrom urllib.request import urlretrieve\n\tfrom zipfile import ZipFile\n\tfrom utils.logger import get_logger\n\tclass DataExtractor:\n\t    \"\"\"Class for extracting data from remote sources\"\"\"\n\t    def __init__(self, data_dir):\n\t        self.data_dir = data_dir\n\t        self.logger = get_logger(__name__)\n", "    def extract_data(self, url, filename):\n\t        \"\"\"\n\t        Downloads and extracts data from remote source\n\t        Args:\n\t            url (str): URL to download data from\n\t            filename (str): Name of the file to save data to\n\t        \"\"\"\n\t        file_path = os.path.join(self.data_dir, filename)\n\t        if not os.path.exists(file_path):\n\t            self.logger.info(f\"Downloading data from {url}\")\n", "            urlretrieve(url, file_path)\n\t            self.logger.info(\"Data downloaded successfully\")\n\t        with ZipFile(file_path, 'r') as zip_obj:\n\t            self.logger.info(f\"Extracting data from {filename}\")\n\t            zip_obj.extractall(self.data_dir)\n\t            self.logger.info(\"Data extracted successfully\")\n\t    def read_csv(self, file_path):\n\t        \"\"\"\n\t        Reads CSV data into a pandas dataframe\n\t        Args:\n", "            file_path (str): Path to CSV file\n\t        Returns:\n\t            pandas.DataFrame: DataFrame containing data from CSV file\n\t        \"\"\"\n\t        self.logger.info(f\"Reading data from {file_path}\")\n\t        df = pd.read_csv(file_path)\n\t        self.logger.info(\"Data loaded successfully\")\n\t        return df\n"]}
{"filename": "src/data_preparation/data_transformation.py", "chunked_list": ["import pandas as pd\n\tfrom sklearn.preprocessing import MinMaxScaler\n\tfrom src.utils.config import read_config\n\tdef transform_data(input_path: str, output_path: str) -> None:\n\t    # Load data from input file\n\t    df = pd.read_csv(input_path)\n\t    # Apply data transformations\n\t    config = read_config(\"config.yml\")\n\t    for col in config[\"data_transformation\"][\"columns\"]:\n\t        if col[\"type\"] == \"log\":\n", "            df[col[\"name\"]] = df[col[\"name\"]].apply(lambda x: np.log(x) if x > 0 else 0)\n\t        elif col[\"type\"] == \"sqrt\":\n\t            df[col[\"name\"]] = df[col[\"name\"]].apply(lambda x: np.sqrt(x) if x > 0 else 0)\n\t        elif col[\"type\"] == \"inverse\":\n\t            df[col[\"name\"]] = df[col[\"name\"]].apply(lambda x: 1/x if x > 0 else 0)\n\t    # Scale data\n\t    scaler = MinMaxScaler()\n\t    scaled_data = scaler.fit_transform(df)\n\t    # Save transformed data to output file\n\t    pd.DataFrame(scaled_data, columns=df.columns).to_csv(output_path, index=False)\n"]}
{"filename": "src/models/dynamic_network_optimization/predict.py", "chunked_list": ["import torch\n\timport numpy as np\n\tfrom utils import load_data, preprocess_data\n\tfrom model import NetworkOptimizer\n\tdef predict(model_path, data_path):\n\t    # Load the model from the specified path\n\t    model = NetworkOptimizer()\n\t    model.load_state_dict(torch.load(model_path))\n\t    model.eval()\n\t    # Load and preprocess the data from the specified path\n", "    data = load_data(data_path)\n\t    preprocessed_data = preprocess_data(data)\n\t    # Convert the preprocessed data to a tensor\n\t    input_tensor = torch.from_numpy(preprocessed_data).float()\n\t    # Make predictions using the model\n\t    with torch.no_grad():\n\t        output_tensor = model(input_tensor)\n\t        output = output_tensor.numpy()\n\t    # Postprocess the predictions and return the results\n\t    result = postprocess_predictions(output)\n", "    return result\n\tdef postprocess_predictions(predictions):\n\t    # Convert the predictions to a list of recommended actions\n\t    actions = []\n\t    for prediction in predictions:\n\t        if prediction > 0.5:\n\t            actions.append(\"Add a new network node\")\n\t        else:\n\t            actions.append(\"Remove an existing network node\")\n\t    return actions\n"]}
{"filename": "src/models/dynamic_network_optimization/model.py", "chunked_list": ["import tensorflow as tf\n\tclass DynamicNetworkOptimizationModel(tf.keras.Model):\n\t    def __init__(self):\n\t        super(DynamicNetworkOptimizationModel, self).__init__()\n\t        # Define layers and architecture of the model\n\t        self.conv1 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')\n\t        self.pool1 = tf.keras.layers.MaxPooling2D((2, 2))\n\t        self.conv2 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')\n\t        self.pool2 = tf.keras.layers.MaxPooling2D((2, 2))\n\t        self.flatten = tf.keras.layers.Flatten()\n", "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n\t        self.dropout = tf.keras.layers.Dropout(0.2)\n\t        self.dense2 = tf.keras.layers.Dense(1, activation='sigmoid')\n\t    def call(self, inputs):\n\t        # Define the forward pass of the model\n\t        x = self.conv1(inputs)\n\t        x = self.pool1(x)\n\t        x = self.conv2(x)\n\t        x = self.pool2(x)\n\t        x = self.flatten(x)\n", "        x = self.dense1(x)\n\t        x = self.dropout(x)\n\t        output = self.dense2(x)\n\t        return output\n"]}
{"filename": "src/models/dynamic_network_optimization/train.py", "chunked_list": ["import tensorflow as tf\n\tfrom tensorflow.keras.layers import Dense, LSTM\n\tfrom tensorflow.keras.models import Sequential\n\tdef build_model(input_shape, output_shape):\n\t    model = Sequential()\n\t    model.add(LSTM(units=64, input_shape=input_shape, return_sequences=True))\n\t    model.add(Dense(units=32, activation='relu'))\n\t    model.add(Dense(units=output_shape, activation='sigmoid'))\n\t    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\t    return model\n", "def train_model(X_train, y_train):\n\t    model = build_model(X_train.shape[1:], y_train.shape[1])\n\t    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n\t    return model, history\n"]}
{"filename": "src/models/dynamic_network_optimization/utils.py", "chunked_list": ["import numpy as np\n\timport pandas as pd\n\tdef load_data(file_path):\n\t    \"\"\"\n\t    Load data from a given file path.\n\t    Args:\n\t        file_path (str): The path to the file to load.\n\t    Returns:\n\t        data (pd.DataFrame): A pandas DataFrame containing the loaded data.\n\t    \"\"\"\n", "    data = pd.read_csv(file_path)\n\t    return data\n\tdef preprocess_data(data):\n\t    \"\"\"\n\t    Preprocess the loaded data.\n\t    Args:\n\t        data (pd.DataFrame): A pandas DataFrame containing the loaded data.\n\t    Returns:\n\t        processed_data (np.ndarray): A numpy array containing the preprocessed data.\n\t    \"\"\"\n", "    # Perform some data preprocessing steps, such as scaling, normalization, etc.\n\t    processed_data = data.to_numpy()\n\t    processed_data = np.divide(processed_data, 100)\n\t    return processed_data\n\tdef save_data(data, file_path):\n\t    \"\"\"\n\t    Save processed data to a given file path.\n\t    Args:\n\t        data (np.ndarray): A numpy array containing the processed data.\n\t        file_path (str): The path to the file to save.\n", "    \"\"\"\n\t    pd.DataFrame(data).to_csv(file_path, index=False)\n"]}
{"filename": "src/models/energy_efficiency_optimization/predict.py", "chunked_list": ["import argparse\n\timport os\n\timport sys\n\timport pandas as pd\n\timport torch\n\tfrom utils import load_data, preprocess_data, load_model\n\tdef main(args):\n\t    # Load the data\n\t    data_path = os.path.join(args.data_dir, args.data_file)\n\t    data = load_data(data_path)\n", "    # Preprocess the data\n\t    data = preprocess_data(data)\n\t    # Load the model\n\t    model_path = os.path.join(args.model_dir, args.model_file)\n\t    model = load_model(model_path)\n\t    # Make predictions\n\t    with torch.no_grad():\n\t        inputs = torch.tensor(data.values).float()\n\t        predictions = model(inputs).squeeze().tolist()\n\t    # Save predictions to file\n", "    output_path = os.path.join(args.output_dir, args.output_file)\n\t    with open(output_path, 'w') as f:\n\t        for pred in predictions:\n\t            f.write(str(pred) + '\\n')\n\t    print(f\"Predictions saved to {output_path}.\")\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser()\n\t    # Data arguments\n\t    parser.add_argument('--data_dir', type=str, default='data', help=\"Directory containing data file.\")\n\t    parser.add_argument('--data_file', type=str, default='test.csv', help=\"Data file name.\")\n", "    # Model arguments\n\t    parser.add_argument('--model_dir', type=str, default='models/energy_efficiency_optimization', help=\"Directory containing model file.\")\n\t    parser.add_argument('--model_file', type=str, default='model.pt', help=\"Model file name.\")\n\t    # Output arguments\n\t    parser.add_argument('--output_dir', type=str, default='output', help=\"Directory to save predictions file.\")\n\t    parser.add_argument('--output_file', type=str, default='predictions.txt', help=\"Predictions file name.\")\n\t    args = parser.parse_args()\n\t    main(args)\n"]}
{"filename": "src/models/energy_efficiency_optimization/model.py", "chunked_list": ["import torch.nn as nn\n\tclass EnergyEfficiencyModel(nn.Module):\n\t    \"\"\"\n\t    PyTorch model for energy efficiency optimization in 5G OpenRAN.\n\t    \"\"\"\n\t    def __init__(self, input_size, hidden_size, output_size):\n\t        super(EnergyEfficiencyModel, self).__init__()\n\t        self.fc1 = nn.Linear(input_size, hidden_size)\n\t        self.relu = nn.ReLU()\n\t        self.fc2 = nn.Linear(hidden_size, output_size)\n", "    def forward(self, x):\n\t        out = self.fc1(x)\n\t        out = self.relu(out)\n\t        out = self.fc2(out)\n\t        return out\n"]}
{"filename": "src/models/energy_efficiency_optimization/train.py", "chunked_list": ["import torch\n\timport torch.optim as optim\n\timport torch.nn.functional as F\n\tfrom torch.utils.data import DataLoader\n\tfrom utils import CustomDataset\n\tclass EnergyEfficiencyModel(torch.nn.Module):\n\t    def __init__(self, input_size, hidden_size, output_size):\n\t        super(EnergyEfficiencyModel, self).__init__()\n\t        self.fc1 = torch.nn.Linear(input_size, hidden_size)\n\t        self.fc2 = torch.nn.Linear(hidden_size, output_size)\n", "    def forward(self, x):\n\t        x = F.relu(self.fc1(x))\n\t        x = self.fc2(x)\n\t        return x\n\tdef train_model(model, train_loader, optimizer, criterion, device):\n\t    model.train()\n\t    train_loss = 0\n\t    for batch_idx, (data, target) in enumerate(train_loader):\n\t        data, target = data.to(device), target.to(device)\n\t        optimizer.zero_grad()\n", "        output = model(data)\n\t        loss = criterion(output, target)\n\t        train_loss += loss.item()\n\t        loss.backward()\n\t        optimizer.step()\n\t    avg_loss = train_loss/len(train_loader)\n\t    return avg_loss\n\tdef main():\n\t    # Hyperparameters\n\t    input_size = 8\n", "    hidden_size = 64\n\t    output_size = 1\n\t    learning_rate = 0.01\n\t    num_epochs = 50\n\t    batch_size = 16\n\t    # Load data\n\t    train_dataset = CustomDataset(\"data/energy_efficiency/train.csv\")\n\t    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n\t    # Initialize model and optimizer\n\t    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n", "    model = EnergyEfficiencyModel(input_size, hidden_size, output_size).to(device)\n\t    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n\t    # Train model\n\t    criterion = torch.nn.MSELoss()\n\t    for epoch in range(1, num_epochs+1):\n\t        loss = train_model(model, train_loader, optimizer, criterion, device)\n\t        print('Epoch: [{}/{}]\\tTrain Loss: {:.4f}'.format(epoch, num_epochs, loss))\n\t    # Save model\n\t    torch.save(model.state_dict(), \"models/energy_efficiency.pt\")\n\tif __name__ == '__main__':\n", "    main()\n"]}
{"filename": "src/models/energy_efficiency_optimization/utils.py", "chunked_list": ["import numpy as np\n\timport pandas as pd\n\tfrom typing import List\n\tdef read_data(filename: str) -> pd.DataFrame:\n\t    \"\"\"\n\t    Reads the dataset from the given filename and returns a pandas DataFrame object\n\t    \"\"\"\n\t    df = pd.read_csv(filename)\n\t    return df\n\tdef prepare_data(df: pd.DataFrame, input_features: List[str], target: str) -> Tuple[np.ndarray, np.ndarray]:\n", "    \"\"\"\n\t    Preprocesses the dataset and returns the input and target arrays\n\t    \"\"\"\n\t    X = df[input_features].values\n\t    y = df[target].values\n\t    return X, y\n\tdef normalize_data(X: np.ndarray) -> np.ndarray:\n\t    \"\"\"\n\t    Normalizes the input data to have zero mean and unit variance\n\t    \"\"\"\n", "    mean = np.mean(X, axis=0)\n\t    std = np.std(X, axis=0)\n\t    X_norm = (X - mean) / std\n\t    return X_norm\n\tdef denormalize_data(X_norm: np.ndarray, X: np.ndarray) -> np.ndarray:\n\t    \"\"\"\n\t    Denormalizes the input data back to its original scale\n\t    \"\"\"\n\t    mean = np.mean(X, axis=0)\n\t    std = np.std(X, axis=0)\n", "    X_denorm = X_norm * std + mean\n\t    return X_denorm\n\tdef save_model(model, filename: str) -> None:\n\t    \"\"\"\n\t    Saves the trained model to disk\n\t    \"\"\"\n\t    joblib.dump(model, filename)\n\tdef load_model(filename: str):\n\t    \"\"\"\n\t    Loads the trained model from disk\n", "    \"\"\"\n\t    return joblib.load(filename)\n"]}
{"filename": "src/models/network_anomaly_detection/predict.py", "chunked_list": ["import argparse\n\timport joblib\n\timport numpy as np\n\tfrom utils import load_data, preprocess_data\n\t# Define argument parser\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--model_path\", type=str, required=True, help=\"Path to saved model file\")\n\tparser.add_argument(\"--data_path\", type=str, required=True, help=\"Path to data file\")\n\targs = parser.parse_args()\n\t# Load saved model\n", "model = joblib.load(args.model_path)\n\t# Load and preprocess data\n\tdata = load_data(args.data_path)\n\tpreprocessed_data = preprocess_data(data)\n\t# Make predictions\n\tpredictions = model.predict(preprocessed_data)\n\t# Print predicted labels\n\tprint(\"Predictions:\")\n\tprint(predictions)\n"]}
{"filename": "src/models/network_anomaly_detection/model.py", "chunked_list": ["import tensorflow as tf\n\tfrom tensorflow.keras.layers import Dense, Dropout\n\tfrom tensorflow.keras.models import Sequential\n\tfrom tensorflow.keras.optimizers import Adam\n\tfrom sklearn.model_selection import train_test_split\n\tclass NetworkAnomalyDetectionModel:\n\t    def __init__(self, input_shape):\n\t        self.input_shape = input_shape\n\t        self.model = self._create_model()\n\t    def _create_model(self):\n", "        model = Sequential([\n\t            Dense(256, activation='relu', input_shape=self.input_shape),\n\t            Dropout(0.5),\n\t            Dense(128, activation='relu'),\n\t            Dropout(0.3),\n\t            Dense(1, activation='sigmoid')\n\t        ])\n\t        optimizer = Adam(learning_rate=0.001)\n\t        model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\t        return model\n", "    def train(self, x_train, y_train, x_val, y_val, batch_size, epochs):\n\t        self.model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val))\n\t    def predict(self, x):\n\t        return self.model.predict(x)\n"]}
{"filename": "src/models/network_anomaly_detection/train.py", "chunked_list": ["import argparse\n\timport pandas as pd\n\timport torch\n\tfrom torch.utils.data import DataLoader\n\tfrom models.network_anomaly_detection import NetworkAnomalyDetection\n\tfrom utils import NetworkAnomalyDataset, load_data, train\n\tif __name__ == '__main__':\n\t    parser = argparse.ArgumentParser(description='Train network anomaly detection model')\n\t    parser.add_argument('--data_path', type=str, default='data/network_data.csv', help='Path to network data file')\n\t    parser.add_argument('--batch_size', type=int, default=32, help='Batch size for training')\n", "    parser.add_argument('--num_epochs', type=int, default=50, help='Number of training epochs')\n\t    parser.add_argument('--lr', type=float, default=0.001, help='Learning rate for optimizer')\n\t    args = parser.parse_args()\n\t    # Load and preprocess data\n\t    df = pd.read_csv(args.data_path)\n\t    data = load_data(df)\n\t    # Split data into training and validation sets\n\t    train_set = NetworkAnomalyDataset(data[:8000])\n\t    val_set = NetworkAnomalyDataset(data[8000:])\n\t    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True)\n", "    val_loader = DataLoader(val_set, batch_size=args.batch_size)\n\t    # Instantiate model and optimizer\n\t    model = NetworkAnomalyDetection(input_size=6, hidden_size=32, num_layers=2)\n\t    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\t    # Train model\n\t    train(model, train_loader, val_loader, optimizer, num_epochs=args.num_epochs)\n\t    # Save trained model\n\t    torch.save(model.state_dict(), 'network_anomaly_detection.pt')\n"]}
{"filename": "src/models/network_anomaly_detection/utils.py", "chunked_list": ["import numpy as np\n\tfrom sklearn.preprocessing import StandardScaler\n\tclass DataUtils:\n\t    def __init__(self):\n\t        self.scaler = StandardScaler()\n\t    def preprocess_data(self, x):\n\t        x = self.scaler.fit_transform(x)\n\t        return x\n\t    def split_data(self, x, y, test_size=0.2):\n\t        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=42)\n", "        x_train = self.preprocess_data(x_train)\n\t        x_test = self.preprocess_data(x_test)\n\t        y_train = np.asarray(y_train).astype('float32')\n\t        y_test = np.asarray(y_test).astype('float32')\n\t        return x_train, x_test, y_train, y_test\n"]}
{"filename": "src/models/predictive_network_planning/predict.py", "chunked_list": ["import argparse\n\timport logging\n\timport os\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom sklearn.preprocessing import MinMaxScaler\n\tfrom torch.utils.data import DataLoader\n\tfrom models.predictive_network_planning.model import PredictiveNetworkPlanningModel\n\tfrom models.predictive_network_planning.utils import PNPDataset, collate_fn\n", "def predict(model: torch.nn.Module, data: PNPDataset, scaler: MinMaxScaler, device: str) -> np.ndarray:\n\t    loader = DataLoader(data, batch_size=1, collate_fn=collate_fn)\n\t    outputs = []\n\t    model.eval()\n\t    with torch.no_grad():\n\t        for batch in loader:\n\t            inputs, targets = batch\n\t            inputs, targets = inputs.to(device), targets.to(device)\n\t            output = model(inputs)\n\t            outputs.append(output.item())\n", "    outputs = np.array(outputs).reshape(-1, 1)\n\t    outputs = scaler.inverse_transform(outputs)\n\t    return outputs\n\tdef main(args: argparse.Namespace) -> None:\n\t    # Load model\n\t    model = PredictiveNetworkPlanningModel(input_dim=args.input_dim,\n\t                                           hidden_dim=args.hidden_dim,\n\t                                           num_layers=args.num_layers,\n\t                                           output_dim=args.output_dim,\n\t                                           dropout=args.dropout)\n", "    device = torch.device(args.device)\n\t    model.load_state_dict(torch.load(args.model_path, map_location=device))\n\t    model.to(device)\n\t    # Load data and scaler\n\t    data = pd.read_csv(args.data_file)\n\t    scaler = MinMaxScaler()\n\t    scaler.fit(data.values)\n\t    # Convert data to PyTorch dataset\n\t    data = PNPDataset(data_file=args.data_file)\n\t    inputs, targets = data[0]\n", "    input_dim = inputs.shape[-1]\n\t    output_dim = targets.shape[-1]\n\t    # Make prediction\n\t    prediction = predict(model, data, scaler, device)\n\t    logging.info(f\"Input Dimension: {input_dim}, Output Dimension: {output_dim}\")\n\t    logging.info(f\"Prediction: {prediction}\")\n\t    # Save prediction\n\t    os.makedirs(args.output_dir, exist_ok=True)\n\t    np.savetxt(os.path.join(args.output_dir, args.output_file), prediction, delimiter=\",\")\n"]}
{"filename": "src/models/predictive_network_planning/model.py", "chunked_list": ["import torch\n\timport torch.nn as nn\n\tclass PredictiveNetwork(nn.Module):\n\t    def __init__(self, input_size, hidden_size, output_size):\n\t        super().__init__()\n\t        self.fc1 = nn.Linear(input_size, hidden_size)\n\t        self.relu = nn.ReLU()\n\t        self.fc2 = nn.Linear(hidden_size, hidden_size)\n\t        self.fc3 = nn.Linear(hidden_size, output_size)\n\t    def forward(self, x):\n", "        out = self.fc1(x)\n\t        out = self.relu(out)\n\t        out = self.fc2(out)\n\t        out = self.relu(out)\n\t        out = self.fc3(out)\n\t        return out\n"]}
{"filename": "src/models/predictive_network_planning/train.py", "chunked_list": ["import argparse\n\timport logging\n\timport os\n\tfrom datetime import datetime\n\tfrom typing import List, Tuple\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\timport torch.nn as nn\n\timport torch.optim as optim\n", "from sklearn.metrics import mean_absolute_error\n\tfrom torch.utils.data import DataLoader\n\tfrom models.predictive_network_planning.model import PredictiveNetworkPlanningModel\n\tfrom models.predictive_network_planning.utils import PNPDataset, collate_fn\n\tdef train(model: nn.Module,\n\t          train_data: PNPDataset,\n\t          valid_data: PNPDataset,\n\t          epochs: int,\n\t          batch_size: int,\n\t          lr: float,\n", "          weight_decay: float,\n\t          device: str,\n\t          save_path: str) -> Tuple[List[float], List[float]]:\n\t    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\t    valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n\t    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n\t    criterion = nn.MSELoss()\n\t    train_loss = []\n\t    valid_loss = []\n\t    best_valid_loss = np.inf\n", "    for epoch in range(epochs):\n\t        model.train()\n\t        running_loss = 0.0\n\t        for batch in train_loader:\n\t            optimizer.zero_grad()\n\t            inputs, targets = batch\n\t            inputs, targets = inputs.to(device), targets.to(device)\n\t            outputs = model(inputs)\n\t            loss = criterion(outputs, targets)\n\t            loss.backward()\n", "            optimizer.step()\n\t            running_loss += loss.item() * inputs.size(0)\n\t        epoch_loss = running_loss / len(train_data)\n\t        train_loss.append(epoch_loss)\n\t        model.eval()\n\t        running_loss = 0.0\n\t        with torch.no_grad():\n\t            for batch in valid_loader:\n\t                inputs, targets = batch\n\t                inputs, targets = inputs.to(device), targets.to(device)\n", "                outputs = model(inputs)\n\t                loss = criterion(outputs, targets)\n\t                running_loss += loss.item() * inputs.size(0)\n\t            epoch_loss = running_loss / len(valid_data)\n\t            valid_loss.append(epoch_loss)\n\t            if epoch_loss < best_valid_loss:\n\t                best_valid_loss = epoch_loss\n\t                torch.save(model.state_dict(), save_path)\n\t        logging.info(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss[-1]:.4f}, Valid Loss: {valid_loss[-1]:.4f}\")\n\t    return train_loss, valid_loss\n", "def main(args: argparse.Namespace) -> None:\n\t    # Load data\n\t    train_data = PNPDataset(data_file=args.train_data_file)\n\t    valid_data = PNPDataset(data_file=args.valid_data_file)\n\t    # Initialize model\n\t    model = PredictiveNetworkPlanningModel(input_dim=train_data.input_dim,\n\t                                           hidden_dim=args.hidden_dim,\n\t                                           num_layers=args.num_layers,\n\t                                           output_dim=train_data.output_dim,\n\t                                           dropout=args.dropout)\n", "    device = torch.device(args.device)\n\t    model.to(device)\n\t    # Train model\n\t    train_loss, valid_loss = train(model=model,\n\t                                    train_data=train_data,\n\t                                    valid_data=valid_data,\n\t                                    epochs=args.epochs,\n\t                                    batch_size=args.batch_size,\n\t                                    lr=args.lr,\n\t                                    weight_decay=args.weight_decay,\n", "                                    device=device,\n\t                                    save_path=args.save_path)\n\t    # Save training and validation loss\n\t    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n\t    os.makedirs(args.log_dir, exist_ok=True)\n\t    log_file = os.path.join(args.log_dir, f\"{args.log_prefix}_{timestamp}.log\")\n\t    with open(log_file, \"w\") as f:\n\t        f.write(f\"Training Loss: {train_loss}\\n\")\n\t        f.write(f\"Validation Loss: {valid_loss}\\n\")\n"]}
{"filename": "src/models/predictive_network_planning/utils.py", "chunked_list": ["import numpy as np\n\timport pandas as pd\n\tfrom sklearn.preprocessing import StandardScaler\n\tdef load_data(data_path):\n\t    \"\"\"\n\t    Load data from csv file.\n\t    Args:\n\t        data_path (str): Path to the csv file.\n\t    Returns:\n\t        X (numpy array): Features of the dataset.\n", "        y (numpy array): Labels of the dataset.\n\t    \"\"\"\n\t    df = pd.read_csv(data_path)\n\t    X = df.iloc[:, :-1].values\n\t    y = df.iloc[:, -1].values\n\t    return X, y\n\tdef preprocess_data(X_train, X_test):\n\t    \"\"\"\n\t    Preprocess data using standard scaling.\n\t    Args:\n", "        X_train (numpy array): Features of the training set.\n\t        X_test (numpy array): Features of the test set.\n\t    Returns:\n\t        X_train_scaled (numpy array): Scaled features of the training set.\n\t        X_test_scaled (numpy array): Scaled features of the test set.\n\t    \"\"\"\n\t    scaler = StandardScaler()\n\t    X_train_scaled = scaler.fit_transform(X_train)\n\t    X_test_scaled = scaler.transform(X_test)\n\t    return X_train_scaled, X_test_scaled\n"]}
