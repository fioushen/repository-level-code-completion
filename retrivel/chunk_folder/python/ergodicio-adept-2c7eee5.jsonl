{"filename": "train-dispersion.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n\t#  research@ergodic.io\n\timport yaml, os\n\timport numpy as np\n\tfrom jax.config import config\n\tconfig.update(\"jax_enable_x64\", True)\n\t# config.update(\"jax_debug_nans\", True)\n\t# config.update(\"jax_disable_jit\", True)\n\timport jax\n\tfrom jax import numpy as jnp\n", "import tempfile, time\n\timport mlflow, optax\n\timport equinox as eqx\n\tfrom tqdm import tqdm\n\tfrom matplotlib import pyplot as plt\n\timport helpers\n\tfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\n\tfrom utils import misc\n\tdef _modify_defaults_(defaults, k0):\n\t    wepw = float(np.sqrt(1.0 + 3.0 * k0**2.0))\n", "    defaults[\"save\"][\"func\"][\"is_on\"] = False\n\t    defaults[\"physics\"][\"landau_damping\"] = True\n\t    defaults[\"physics\"][\"electron\"][\"trapping\"][\"kld\"] = k0\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = k0\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = wepw\n\t    xmax = float(2.0 * np.pi / k0)\n\t    defaults[\"grid\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"kx\"][\"kxmax\"] = k0\n\t    return defaults\n", "def train_loop():\n\t    weight_key = jax.random.PRNGKey(420)\n\t    dispersion_models = {\n\t        \"w_of_k\": eqx.nn.MLP(1, 1, 4, 3, activation=jnp.tanh, final_activation=jnp.tanh, key=weight_key)\n\t    }\n\t    optimizer = optax.adam(0.1)\n\t    opt_state = optimizer.init(eqx.filter(dispersion_models, eqx.is_array))\n\t    # modify config\n\t    num_ks = 8\n\t    batch_size = 4\n", "    k0s = np.linspace(0.2, 0.4, num_ks)\n\t    all_ks = np.linspace(0.01, 0.5, 1024)\n\t    all_ws = np.sqrt(1 + 3 * all_ks**2.0)\n\t    rng = np.random.default_rng(420)\n\t    mlflow.set_experiment(\"train-dispersion\")\n\t    with mlflow.start_run(run_name=\"disp-opt\", nested=True) as mlflow_run:\n\t        for epoch in range(100):\n\t            rng.shuffle(k0s)\n\t            these_batches = k0s.reshape((-1, batch_size))\n\t            epoch_loss = 0.0\n", "            for batch, this_batch in tqdm(enumerate(these_batches), total=len(these_batches)):\n\t                grads = []\n\t                for sim, k0 in enumerate(this_batch):\n\t                    with open(\"./tests/configs/resonance.yaml\", \"r\") as file:\n\t                        defaults = yaml.safe_load(file)\n\t                    mod_defaults = _modify_defaults_(defaults, float(k0))\n\t                    with mlflow.start_run(run_name=f\"{epoch=}-{batch=}-{sim=}\", nested=True) as mlflow_run:\n\t                        mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n\t                        misc.log_params(mod_defaults)\n\t                        mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n", "                        mod_defaults = helpers.get_save_quantities(mod_defaults)\n\t                        with tempfile.TemporaryDirectory() as td:\n\t                            # run\n\t                            t0 = time.time()\n\t                            state = helpers.init_state(mod_defaults)\n\t                            k0 = (k0 - 0.2) / 0.25\n\t                            def loss(models):\n\t                                w0 = jnp.squeeze(0.5 * models[\"w_of_k\"](jnp.array([k0])) + 1.1)\n\t                                mod_defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = w0\n\t                                vf = helpers.VectorField(mod_defaults, models=False)\n", "                                args = {\"driver\": mod_defaults[\"drivers\"]}\n\t                                results = diffeqsolve(\n\t                                    terms=ODETerm(vf),\n\t                                    solver=Tsit5(),\n\t                                    t0=mod_defaults[\"grid\"][\"tmin\"],\n\t                                    t1=mod_defaults[\"grid\"][\"tmax\"],\n\t                                    max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n\t                                    dt0=mod_defaults[\"grid\"][\"dt\"],\n\t                                    y0=state,\n\t                                    args=args,\n", "                                    saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"]),\n\t                                )\n\t                                nk1 = (\n\t                                    jnp.abs(jnp.fft.fft(results.ys[\"electron\"][\"n\"], axis=1)[:, 1])\n\t                                    * 2.0\n\t                                    / mod_defaults[\"grid\"][\"nx\"]\n\t                                )\n\t                                return -jnp.amax(nk1), results\n\t                            vg_func = eqx.filter_value_and_grad(loss, has_aux=True)\n\t                            (loss_val, results), grad = eqx.filter_jit(vg_func)(dispersion_models)\n", "                            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n\t                            t0 = time.time()\n\t                            helpers.post_process(results, mod_defaults, td)\n\t                            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n\t                            # log artifacts\n\t                            mlflow.log_artifacts(td)\n\t                    grads.append(grad)\n\t                grads = misc.all_reduce_gradients(grads, len(grads))\n\t                updates, opt_state = optimizer.update(grads, opt_state, dispersion_models)\n\t                dispersion_models = eqx.apply_updates(dispersion_models, updates)\n", "                loss_val = float(loss_val)\n\t                mlflow.log_metrics({\"run_loss\": loss_val}, step=sim + epoch * len(k0s))\n\t                epoch_loss = epoch_loss + loss_val\n\t                # pbar.set_description(f\"{loss_val=:.2e}, {epoch_loss=:.2e}, average_loss={epoch_loss/(sim+1):.2e}\")\n\t            learned_ws = (\n\t                0.5 * eqx.filter_vmap(dispersion_models[\"w_of_k\"])((jnp.array(all_ks[:, None]) - 0.2) / 0.25) + 1.1\n\t            )\n\t            chosen_ws = 0.5 * eqx.filter_vmap(dispersion_models[\"w_of_k\"])((jnp.array(k0s[:, None]) - 0.2) / 0.25) + 1.1\n\t            fig, ax = plt.subplots(1, 1, figsize=(7, 4), tight_layout=True)\n\t            ax.plot(all_ks, all_ws, label=\"actual\")\n", "            ax.plot(k0s, chosen_ws, \"x\", label=\"training data\")\n\t            ax.plot(all_ks, np.squeeze(learned_ws), label=\"prediction\")\n\t            ax.legend(fontsize=14)\n\t            ax.grid()\n\t            ax.set_xlabel(r\"$k\\lambda_D$\", fontsize=14)\n\t            ax.set_ylabel(r\"$\\omega_0$\", fontsize=14)\n\t            with tempfile.TemporaryDirectory() as td:\n\t                fig.savefig(os.path.join(td, f\"{epoch=}\"), bbox_inches=\"tight\")\n\t                mlflow.log_artifacts(td, \"validation-plots\")\n\t            plt.close(fig)\n", "            mlflow.log_metrics({\"epoch_loss\": epoch_loss})\n\t            mlflow.log_metrics({\"val_loss\": float(np.sqrt(np.mean(np.square(all_ws - learned_ws))))})\n\tif __name__ == \"__main__\":\n\t    train_loop()\n"]}
{"filename": "run.py", "chunked_list": ["from jax import config\n\tconfig.update(\"jax_enable_x64\", True)\n\t# config.update(\"jax_disable_jit\", True)\n\timport yaml, mlflow\n\tfrom utils.runner import run\n\tif __name__ == \"__main__\":\n\t    with open(\"configs/epw.yaml\", \"r\") as fi:\n\t    # with open(\"tests/configs/resonance.yaml\", \"r\") as fi:\n\t        cfg = yaml.safe_load(fi)\n\t    mlflow.set_experiment(cfg[\"mlflow\"][\"experiment\"])\n", "    # modify config\n\t    with mlflow.start_run(run_name=cfg[\"mlflow\"][\"run\"]) as mlflow_run:\n\t        run(cfg)\n"]}
{"filename": "run_job.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n\t#  research@ergodic.io\n\timport sys, os\n\tfrom utils.runner import start_run\n\tif __name__ == \"__main__\":\n\t    run_id = sys.argv[1]\n\t    run_type = sys.argv[2]\n\t    start_run(run_type, run_id)\n"]}
{"filename": "train-damping.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n\t#  research@ergodic.io\n\timport yaml, os\n\tfrom itertools import product\n\timport numpy as np\n\tfrom jax.config import config\n\tconfig.update(\"jax_enable_x64\", True)\n\t# config.update(\"jax_debug_nans\", True)\n\t# config.update(\"jax_disable_jit\", True)\n\tfrom jax import numpy as jnp\n", "import xarray as xr\n\timport tempfile, time\n\timport mlflow, optax\n\timport equinox as eqx\n\tfrom tqdm import tqdm\n\timport helpers\n\tfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\n\tfrom utils import misc, plotters\n\tdef _modify_defaults_(defaults, k0, a0, nuee):\n\t    wepw = float(np.sqrt(1.0 + 3.0 * k0**2.0))\n", "    defaults[\"physics\"][\"landau_damping\"] = True\n\t    defaults[\"physics\"][\"electron\"][\"trapping\"][\"nuee\"] = nuee\n\t    defaults[\"physics\"][\"electron\"][\"trapping\"][\"kld\"] = k0\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = k0\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = wepw\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"a0\"] = a0\n\t    xmax = float(2.0 * np.pi / k0)\n\t    defaults[\"grid\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"kx\"][\"kxmax\"] = k0\n", "    return defaults\n\tdef loss(models, this_cfg, state, actual_nk1):\n\t    vf = helpers.VectorField(this_cfg, models=models)\n\t    args = {\"driver\": this_cfg[\"drivers\"]}\n\t    results = diffeqsolve(\n\t        terms=ODETerm(vf),\n\t        solver=Tsit5(),\n\t        t0=this_cfg[\"grid\"][\"tmin\"],\n\t        t1=this_cfg[\"grid\"][\"tmax\"],\n\t        max_steps=this_cfg[\"grid\"][\"max_steps\"],\n", "        dt0=this_cfg[\"grid\"][\"dt\"],\n\t        y0=state,\n\t        args=args,\n\t        saveat=SaveAt(ts=this_cfg[\"save\"][\"t\"][\"ax\"], fn=this_cfg[\"save\"][\"func\"][\"callable\"]),\n\t    )\n\t    nk1 = jnp.abs(jnp.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1]) * 2.0 / this_cfg[\"grid\"][\"nx\"]\n\t    return jnp.mean(jnp.square((actual_nk1 - nk1) / jnp.amax(actual_nk1))), results\n\tdef train_loop():\n\t    vg_func = eqx.filter_jit(eqx.filter_value_and_grad(loss, has_aux=True))\n\t    with open(\"configs/damping.yaml\", \"r\") as file:\n", "        defaults = yaml.safe_load(file)\n\t    trapping_models = helpers.get_models(defaults[\"models\"])\n\t    optimizer = optax.adam(0.1)\n\t    opt_state = optimizer.init(eqx.filter(trapping_models, eqx.is_array))\n\t    batch_size = 3\n\t    # modify config\n\t    fks = xr.open_dataset(\"./epws.nc\")\n\t    nus = np.copy(fks.coords[r\"$\\nu_{ee}$\"].data[::3])\n\t    k0s = np.copy(fks.coords[\"$k_0$\"].data[::2])\n\t    a0s = np.copy(fks.coords[\"$a_0$\"].data[::3])\n", "    all_sims = np.array(list(product(nus, k0s, a0s)))\n\t    rng = np.random.default_rng(420)\n\t    train_sims = rng.choice(\n\t        np.arange(all_sims.shape[0]), int(0.8 * all_sims.shape[0] / batch_size) * batch_size, replace=False\n\t    )\n\t    val_sims = np.array(list(set(np.arange(all_sims.shape[0])) - set(train_sims)))\n\t    mlflow.set_experiment(\"damping-rates-epw-new-model\")\n\t    with mlflow.start_run(run_name=\"damping-opt\", nested=True) as mlflow_run:\n\t        for epoch in range(100):\n\t            epoch_loss = 0.0\n", "            rng.shuffle(train_sims)\n\t            train_batches = all_sims[train_sims].reshape((-1, batch_size, 3))\n\t            for i_batch, batch in (pbar := tqdm(enumerate(train_batches), total=len(train_batches))):\n\t                grads = []\n\t                for sim, (nuee, k0, a0) in tqdm(enumerate(batch), total=batch_size):\n\t                    with open(\"configs/damping.yaml\", \"r\") as file:\n\t                        defaults = yaml.safe_load(file)\n\t                    mod_defaults = _modify_defaults_(defaults, float(k0), float(a0), float(nuee))\n\t                    locs = {\"$k_0$\": k0, \"$a_0$\": a0, r\"$\\nu_{ee}$\": nuee}\n\t                    actual_nk1 = xr.DataArray(\n", "                        fks[\"n-(k_x)\"].loc[locs].data[:, 1], coords=((\"t\", fks.coords[\"t\"].data),)\n\t                    ).data\n\t                    with mlflow.start_run(run_name=f\"{epoch=}-{batch=}-{sim=}\", nested=True) as mlflow_run:\n\t                        mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n\t                        misc.log_params(mod_defaults)\n\t                        mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n\t                        mod_defaults = helpers.get_save_quantities(mod_defaults)\n\t                        with tempfile.TemporaryDirectory() as td:\n\t                            # run\n\t                            t0 = time.time()\n", "                            state = helpers.init_state(mod_defaults)\n\t                            (loss_val, results), grad = vg_func(trapping_models, mod_defaults, state, actual_nk1)\n\t                            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n\t                            t0 = time.time()\n\t                            helpers.post_process(results, mod_defaults, td)\n\t                            plotters.mva(actual_nk1, mod_defaults, results, td, fks.coords)\n\t                            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n\t                            # log artifacts\n\t                            mlflow.log_artifacts(td)\n\t                    grads.append(grad)\n", "                grads = misc.all_reduce_gradients(grads, len(grads))\n\t                updates, opt_state = optimizer.update(grads, opt_state, trapping_models)\n\t                trapping_models = eqx.apply_updates(trapping_models, updates)\n\t                loss_val = float(loss_val)\n\t                mlflow.log_metrics({\"run_loss\": loss_val}, step=sim + epoch * 100)\n\t                epoch_loss = epoch_loss + loss_val\n\t                pbar.set_description(f\"{loss_val=:.2e}, {epoch_loss=:.2e}, average_loss={epoch_loss/(sim+1):.2e}\")\n\t            mlflow.log_metrics({\"epoch_loss\": epoch_loss})\n\tdef remote_train_loop():\n\t    with open(\"configs/damping.yaml\", \"r\") as file:\n", "        defaults = yaml.safe_load(file)\n\t    trapping_models = helpers.get_models(defaults[\"models\"])\n\t    optimizer = optax.adam(0.1)\n\t    opt_state = optimizer.init(eqx.filter(trapping_models, eqx.is_array))\n\t    batch_size = 16\n\t    # modify config\n\t    fks = xr.open_dataset(\"./epws.nc\")\n\t    nus = np.copy(fks.coords[r\"$\\nu_{ee}$\"].data[::3])\n\t    k0s = np.copy(fks.coords[\"$k_0$\"].data[::2])\n\t    a0s = np.copy(fks.coords[\"$a_0$\"].data[::3])\n", "    all_sims = np.array(list(product(nus, k0s, a0s)))\n\t    rng = np.random.default_rng(420)\n\t    train_sims = rng.choice(\n\t        np.arange(all_sims.shape[0]), int(0.8 * all_sims.shape[0] / batch_size) * batch_size, replace=False\n\t    )\n\t    val_sims = np.array(list(set(np.arange(all_sims.shape[0])) - set(train_sims)))\n\t    mlflow.set_experiment(\"damping-rates-epw-new-model\")\n\t    with mlflow.start_run(run_name=\"damping-opt\", nested=True) as mlflow_run:\n\t        for epoch in range(100):\n\t            epoch_loss = 0.0\n", "            rng.shuffle(train_sims)\n\t            train_batches = all_sims[train_sims].reshape((-1, batch_size, 3))\n\t            for i_batch, batch in (pbar := tqdm(enumerate(train_batches), total=len(train_batches))):\n\t                run_ids, job_done = [], []\n\t                for sim, (nuee, k0, a0) in enumerate(batch):\n\t                    run_ids, job_done = queue_sim(\n\t                        fks, nuee, k0, a0, run_ids, job_done, trapping_models, epoch, i_batch, sim, t_or_v=\"grad\"\n\t                    )\n\t                trapping_models = update_w_and_b(\n\t                    job_done, run_ids, optimizer, opt_state, eqx.filter(trapping_models, eqx.is_array)\n", "                )\n\t                batch_loss = float(\n\t                    np.average(\n\t                        np.array([misc.get_this_metric_of_this_run(\"loss\", queued_run_id) for queued_run_id in run_ids])\n\t                    )\n\t                )\n\t                mlflow.log_metrics({\"batch_loss\": batch_loss}, step=i_batch + epoch * len(train_batches))\n\t                epoch_loss = epoch_loss + batch_loss\n\t                pbar.set_description(f\"{batch_loss=:.2e}, {epoch_loss=:.2e}, average_loss={epoch_loss/(sim+1):.2e}\")\n\t            mlflow.log_metrics({\"epoch_loss\": epoch_loss}, step=epoch)\n", "            # validation\n\t            run_ids, job_done = [], []\n\t            for sim, (nuee, k0, a0) in enumerate(all_sims[val_sims]):\n\t                run_ids, job_done = queue_sim(\n\t                    fks, nuee, k0, a0, run_ids, job_done, trapping_models, epoch, 0, sim, t_or_v=\"val\"\n\t                )\n\t            wait_for_jobs(job_done, run_ids)\n\t            val_loss = float(\n\t                np.average(\n\t                    np.array([misc.get_this_metric_of_this_run(\"val_loss\", queued_run_id) for queued_run_id in run_ids])\n", "                )\n\t            )\n\t            mlflow.log_metrics({\"val_epoch_loss\": val_loss}, step=epoch)\n\tdef wait_for_jobs(job_done, run_ids):\n\t    while not all(job_done):\n\t        for i, run_id in enumerate(run_ids):\n\t            time.sleep(4.2 / len(run_ids))\n\t            job_done[i] = misc.is_job_done(run_id)\n\tdef update_w_and_b(job_done, run_ids, optimizer, opt_state, w_and_b):\n\t    wait_for_jobs(job_done, run_ids)\n", "    gradients = []\n\t    with tempfile.TemporaryDirectory() as td:\n\t        for queued_run_id in run_ids:\n\t            mlflow.artifacts.download_artifacts(run_id=queued_run_id, artifact_path=\"grads.eqx\", dst_path=td)\n\t            gradients.append(eqx.tree_deserialise_leaves(os.path.join(td, \"grads.eqx\"), w_and_b))\n\t    gradients = misc.all_reduce_gradients(gradients, len(run_ids))\n\t    updates, opt_state = optimizer.update(gradients, opt_state, w_and_b)\n\t    w_and_b = eqx.apply_updates(w_and_b, updates)\n\t    return w_and_b\n\tdef queue_sim(fks, nuee, k0, a0, run_ids, job_done, w_and_b, epoch, i_batch, sim, t_or_v=\"grad\"):\n", "    with open(\"configs/damping.yaml\", \"r\") as file:\n\t        defaults = yaml.safe_load(file)\n\t    mod_defaults = _modify_defaults_(defaults, float(k0), float(a0), float(nuee))\n\t    locs = {\"$k_0$\": k0, \"$a_0$\": a0, r\"$\\nu_{ee}$\": nuee}\n\t    actual_nk1 = xr.DataArray(fks[\"n-(k_x)\"].loc[locs].data[:, 1], coords=((\"t\", fks.coords[\"t\"].data),))\n\t    with mlflow.start_run(run_name=f\"{epoch=}-batch={i_batch}-{sim=}\", nested=True) as mlflow_run:\n\t        with tempfile.TemporaryDirectory() as td:\n\t            with open(os.path.join(td, \"config.yaml\"), \"w\") as fp:\n\t                yaml.dump(mod_defaults, fp)\n\t            actual_nk1.to_netcdf(os.path.join(td, \"ground_truth.nc\"))\n", "            # with open(os.path.join(td, \"weights.pkl\"), \"wb\") as fi:\n\t            #     pickle.dump(w_and_b, fi)\n\t            eqx.tree_serialise_leaves(os.path.join(td, \"weights.eqx\"), w_and_b)\n\t            mlflow.log_artifacts(td)\n\t        misc.queue_sim(\n\t            {\n\t                \"job_name\": f\"epw-{t_or_v}-epoch-{epoch}-batch-{i_batch}-sim-{sim}\",\n\t                \"run_id\": mlflow_run.info.run_id,\n\t                \"sim_type\": \"fluid\",\n\t                \"run_type\": t_or_v,\n", "                \"machine\": \"continuum-cpu\",\n\t            }\n\t        )\n\t        mlflow.set_tags({\"status\": \"queued\"})\n\t        run_ids.append(mlflow_run.info.run_id)\n\t        job_done.append(False)\n\t    return run_ids, job_done\n\tif __name__ == \"__main__\":\n\t    train_loop()\n"]}
{"filename": "utils/runner.py", "chunked_list": ["from typing import Dict\n\timport os, time, tempfile, yaml\n\tfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5, Solution\n\tfrom jax import numpy as jnp\n\timport numpy as np\n\timport mlflow\n\timport equinox as eqx\n\timport xarray as xr\n\tfrom adept.es1d import helpers\n\tfrom utils import plotters, misc\n", "def start_run(run_type, run_id):\n\t    if run_type == \"forward\":\n\t        just_forward(run_id, nested=False)\n\t    elif run_type == \"grad\":\n\t        remote_gradient(run_id)\n\t    elif run_type == \"val\":\n\t        remote_val(run_id)\n\t    else:\n\t        raise NotImplementedError\n\tdef run(cfg: Dict) -> Solution:\n", "    with tempfile.TemporaryDirectory() as td:\n\t        with open(os.path.join(td, \"config.yaml\"), \"w\") as fi:\n\t            yaml.dump(cfg, fi)\n\t        # get derived quantities\n\t        cfg[\"grid\"] = helpers.get_derived_quantities(cfg[\"grid\"])\n\t        misc.log_params(cfg)\n\t        cfg[\"grid\"] = helpers.get_solver_quantities(cfg[\"grid\"])\n\t        cfg = helpers.get_save_quantities(cfg)\n\t        models = helpers.get_models(cfg[\"models\"])\n\t        state = helpers.init_state(cfg)\n", "        # run\n\t        t0 = time.time()\n\t        # @eqx.filter_jit\n\t        def _run_():\n\t            vf = helpers.VectorField(cfg, models=models)\n\t            return diffeqsolve(\n\t                terms=ODETerm(vf),\n\t                solver=Tsit5(),\n\t                t0=cfg[\"grid\"][\"tmin\"],\n\t                t1=cfg[\"grid\"][\"tmax\"],\n", "                max_steps=cfg[\"grid\"][\"max_steps\"],\n\t                dt0=cfg[\"grid\"][\"dt\"],\n\t                y0=state,\n\t                args={\"driver\": cfg[\"drivers\"]},\n\t                saveat=SaveAt(ts=cfg[\"save\"][\"t\"][\"ax\"], fn=cfg[\"save\"][\"func\"][\"callable\"]),\n\t            )\n\t        result = _run_()\n\t        mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n\t        t0 = time.time()\n\t        helpers.post_process(result, cfg, td)\n", "        mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n\t        # log artifacts\n\t        mlflow.log_artifacts(td)\n\t    # fin\n\t    return result\n\tdef remote_gradient(run_id):\n\t    with mlflow.start_run(run_id=run_id, nested=True) as mlflow_run:\n\t        with tempfile.TemporaryDirectory() as td:\n\t            mod_defaults = misc.get_cfg(artifact_uri=mlflow_run.info.artifact_uri, temp_path=td)\n\t            actual_nk1 = xr.open_dataarray(\n", "                misc.download_file(\"ground_truth.nc\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td)\n\t            )\n\t            mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n\t            misc.log_params(mod_defaults)\n\t            mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n\t            mod_defaults = helpers.get_save_quantities(mod_defaults)\n\t            t0 = time.time()\n\t            state = helpers.init_state(mod_defaults)\n\t            mod_defaults[\"models\"][\"file\"] = misc.download_file(\n\t                \"weights.eqx\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td\n", "            )\n\t            models = helpers.get_models(mod_defaults[\"models\"])\n\t            def loss(these_models):\n\t                vf = helpers.VectorField(mod_defaults, models=these_models)\n\t                args = {\"driver\": mod_defaults[\"drivers\"]}\n\t                results = diffeqsolve(\n\t                    terms=ODETerm(vf),\n\t                    solver=Tsit5(),\n\t                    t0=mod_defaults[\"grid\"][\"tmin\"],\n\t                    t1=mod_defaults[\"grid\"][\"tmax\"],\n", "                    max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n\t                    dt0=mod_defaults[\"grid\"][\"dt\"],\n\t                    y0=state,\n\t                    args=args,\n\t                    saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"], fn=mod_defaults[\"save\"][\"func\"][\"callable\"]),\n\t                )\n\t                nk1 = (\n\t                    jnp.abs(jnp.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1])\n\t                    * 2.0\n\t                    / mod_defaults[\"grid\"][\"nx\"]\n", "                )\n\t                return (\n\t                    jnp.mean(\n\t                        jnp.square(\n\t                            (np.log10(actual_nk1.data + 1e-20) - jnp.log10(nk1 + 1e-20))\n\t                            / np.log10(np.amax(actual_nk1.data))\n\t                        )\n\t                        * jnp.exp(-2 * (1 - (mod_defaults[\"save\"][\"t\"][\"ax\"] / mod_defaults[\"save\"][\"t\"][\"tmax\"])))\n\t                    ),\n\t                    results,\n", "                )\n\t            vg_func = eqx.filter_value_and_grad(loss, has_aux=True)\n\t            (loss_val, results), grad = eqx.filter_jit(vg_func)(models)\n\t            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4), \"loss\": float(loss_val)})\n\t            # dump gradients\n\t            eqx.tree_serialise_leaves(os.path.join(td, \"grads.eqx\"), grad)\n\t            t0 = time.time()\n\t            helpers.post_process(results, mod_defaults, td)\n\t            plotters.mva(actual_nk1.data, mod_defaults, results, td, actual_nk1.coords)\n\t            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n", "            # log artifacts\n\t            mlflow.log_artifacts(td)\n\t            mlflow.set_tags({\"status\": \"completed\"})\n\tdef remote_val(run_id):\n\t    with mlflow.start_run(run_id=run_id, nested=True) as mlflow_run:\n\t        with tempfile.TemporaryDirectory() as td:\n\t            mod_defaults = misc.get_cfg(artifact_uri=mlflow_run.info.artifact_uri, temp_path=td)\n\t            actual_nk1 = xr.open_dataarray(\n\t                misc.download_file(\"ground_truth.nc\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td)\n\t            )\n", "            mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n\t            misc.log_params(mod_defaults)\n\t            mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n\t            mod_defaults = helpers.get_save_quantities(mod_defaults)\n\t            t0 = time.time()\n\t            state = helpers.init_state(mod_defaults)\n\t            mod_defaults[\"models\"][\"file\"] = misc.download_file(\n\t                \"weights.eqx\", artifact_uri=mlflow_run.info.artifact_uri, destination_path=td\n\t            )\n\t            models = helpers.get_models(mod_defaults[\"models\"])\n", "            def loss(these_models):\n\t                vf = helpers.VectorField(mod_defaults, models=these_models)\n\t                args = {\"driver\": mod_defaults[\"drivers\"]}\n\t                results = diffeqsolve(\n\t                    terms=ODETerm(vf),\n\t                    solver=Tsit5(),\n\t                    t0=mod_defaults[\"grid\"][\"tmin\"],\n\t                    t1=mod_defaults[\"grid\"][\"tmax\"],\n\t                    max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n\t                    dt0=mod_defaults[\"grid\"][\"dt\"],\n", "                    y0=state,\n\t                    args=args,\n\t                    saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"], fn=mod_defaults[\"save\"][\"func\"][\"callable\"]),\n\t                )\n\t                nk1 = (\n\t                    jnp.abs(jnp.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1])\n\t                    * 2.0\n\t                    / mod_defaults[\"grid\"][\"nx\"]\n\t                )\n\t                return (\n", "                    jnp.mean(\n\t                        jnp.square(\n\t                            (np.log10(actual_nk1.data + 1e-20) - jnp.log10(nk1 + 1e-20))\n\t                            / np.log10(np.amax(actual_nk1.data))\n\t                        )\n\t                        * jnp.exp(-2 * (1 - (mod_defaults[\"save\"][\"t\"][\"ax\"] / mod_defaults[\"save\"][\"t\"][\"tmax\"])))\n\t                    ),\n\t                    results,\n\t                )\n\t            loss_val, results = eqx.filter_jit(loss)(models)\n", "            mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4), \"val_loss\": float(loss_val)})\n\t            t0 = time.time()\n\t            helpers.post_process(results, mod_defaults, td)\n\t            plotters.mva(actual_nk1.data, mod_defaults, results, td, actual_nk1.coords)\n\t            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4)})\n\t            # log artifacts\n\t            mlflow.log_artifacts(td)\n\t            mlflow.set_tags({\"status\": \"completed\"})\n"]}
{"filename": "utils/plotters.py", "chunked_list": ["import os\n\tfrom matplotlib import pyplot as plt\n\timport numpy as np\n\tdef mva(actual_nk1, mod_defaults, results, td, coords):\n\t    fig, ax = plt.subplots(1, 2, figsize=(10, 4), tight_layout=True)\n\t    ax[0].plot(coords[\"t\"].data, actual_nk1, label=\"Vlasov\")\n\t    ax[0].plot(\n\t        mod_defaults[\"save\"][\"t\"][\"ax\"],\n\t        (np.abs(np.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1]) * 2.0 / mod_defaults[\"grid\"][\"nx\"]),\n\t        label=\"NN + Fluid\",\n", "    )\n\t    ax[1].semilogy(coords[\"t\"].data, actual_nk1, label=\"Vlasov\")\n\t    ax[1].semilogy(\n\t        mod_defaults[\"save\"][\"t\"][\"ax\"],\n\t        (np.abs(np.fft.fft(results.ys[\"x\"][\"electron\"][\"n\"], axis=1)[:, 1]) * 2.0 / mod_defaults[\"grid\"][\"nx\"]),\n\t        label=\"NN + Fluid\",\n\t    )\n\t    ax[0].set_xlabel(r\"t ($\\omega_p^{-1}$)\", fontsize=12)\n\t    ax[1].set_xlabel(r\"t ($\\omega_p^{-1}$)\", fontsize=12)\n\t    ax[0].set_ylabel(r\"$|\\hat{n}|^{1}$\", fontsize=12)\n", "    ax[0].grid()\n\t    ax[1].grid()\n\t    ax[0].legend(fontsize=14)\n\t    fig.savefig(os.path.join(td, \"plots\", \"vlasov_v_fluid.png\"), bbox_inches=\"tight\")\n\t    plt.close(fig)\n"]}
{"filename": "utils/__init__.py", "chunked_list": []}
{"filename": "utils/nn.py", "chunked_list": ["from typing import Any, Literal, Optional, TypeVar, Union, Callable, Tuple\n\timport jax.random as jrandom\n\tfrom jax import nn as jnn\n\timport numpy as np\n\tfrom jaxtyping import Array\n\tfrom equinox._custom_types import PRNGKey\n\tfrom equinox import Module, static_field\n\tclass Linear(Module):\n\t    \"\"\"Performs a linear transformation.\"\"\"\n\t    weight: Array\n", "    bias: Optional[Array]\n\t    in_features: Union[int, Literal[\"scalar\"]] = static_field()\n\t    out_features: Union[int, Literal[\"scalar\"]] = static_field()\n\t    use_bias: bool = static_field()\n\t    def __init__(\n\t        self,\n\t        in_features: Union[int, Literal[\"scalar\"]],\n\t        out_features: Union[int, Literal[\"scalar\"]],\n\t        use_bias: bool = True,\n\t        *,\n", "        key: PRNGKey,\n\t    ):\n\t        \"\"\"**Arguments:**\n\t        - `in_features`: The input size. The input to the layer should be a vector of\n\t            shape `(in_features,)`\n\t        - `out_features`: The output size. The output from the layer will be a vector\n\t            of shape `(out_features,)`.\n\t        - `use_bias`: Whether to add on a bias as well.\n\t        - `key`: A `jax.random.PRNGKey` used to provide randomness for parameter\n\t            initialisation. (Keyword only argument.)\n", "        Note that `in_features` also supports the string `\"scalar\"` as a special value.\n\t        In this case the input to the layer should be of shape `()`.\n\t        Likewise `out_features` can also be a string `\"scalar\"`, in which case the\n\t        output from the layer will have shape `()`.\n\t        \"\"\"\n\t        super().__init__()\n\t        wkey, bkey = jrandom.split(key, 2)\n\t        in_features_ = 1 if in_features == \"scalar\" else in_features\n\t        out_features_ = 1 if out_features == \"scalar\" else out_features\n\t        std_dev = 1.0 / np.sqrt(in_features)\n", "        self.weight = jrandom.truncated_normal(wkey, -2.0, 2.0, (out_features_, in_features_)) * std_dev\n\t        if use_bias:\n\t            self.bias = jrandom.truncated_normal(bkey, -2.0, 2.0, (out_features_,)) * std_dev\n\t        else:\n\t            self.bias = None\n\t        self.in_features = in_features\n\t        self.out_features = out_features\n\t        self.use_bias = use_bias\n\t    def __call__(self, x: Array, *, key: Optional[PRNGKey] = None) -> Array:\n\t        \"\"\"**Arguments:**\n", "        - `x`: The input. Should be a JAX array of shape `(in_features,)`. (Or shape\n\t            `()` if `in_features=\"scalar\"`.)\n\t        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n\t            (Keyword only argument.)\n\t        !!! info\n\t            If you want to use higher order tensors as inputs (for example featuring \"\n\t            \"batch dimensions) then use `jax.vmap`. For example, for an input `x` of \"\n\t            \"shape `(batch, in_features)`, using\n\t            ```python\n\t            linear = equinox.nn.Linear(...)\n", "            jax.vmap(linear)(x)\n\t            ```\n\t            will produce the appropriate output of shape `(batch, out_features)`.\n\t        **Returns:**\n\t        A JAX array of shape `(out_features,)`. (Or shape `()` if\n\t        `out_features=\"scalar\"`.)\n\t        \"\"\"\n\t        x = self.weight @ x\n\t        x = x + self.bias\n\t        return x\n", "class MLP(Module):\n\t    \"\"\"Standard Multi-Layer Perceptron; also known as a feed-forward network.\n\t    !!! faq\n\t        If you get a TypeError saying an object is not a valid JAX type, see the\n\t            [FAQ](https://docs.kidger.site/equinox/faq/).\"\"\"\n\t    layers: Tuple[Linear, ...]\n\t    activation: Callable\n\t    final_activation: Callable\n\t    in_size: Union[int, Literal[\"scalar\"]] = static_field()\n\t    out_size: Union[int, Literal[\"scalar\"]] = static_field()\n", "    width_size: int = static_field()\n\t    depth: int = static_field()\n\t    def __init__(\n\t        self,\n\t        in_size: Union[int, Literal[\"scalar\"]],\n\t        out_size: Union[int, Literal[\"scalar\"]],\n\t        width_size: int,\n\t        depth: int,\n\t        activation: Callable = jnn.relu,\n\t        final_activation: Callable = jnn.relu,\n", "        *,\n\t        key: PRNGKey,\n\t        **kwargs,\n\t    ):\n\t        \"\"\"**Arguments**:\n\t        - `in_size`: The input size. The input to the module should be a vector of\n\t            shape `(in_features,)`\n\t        - `out_size`: The output size. The output from the module will be a vector\n\t            of shape `(out_features,)`.\n\t        - `width_size`: The size of each hidden layer.\n", "        - `depth`: The number of hidden layers, including the output layer.\n\t            For example, `depth=2` results in an network with layers:\n\t            [`Linear(in_size, width_size)`, `Linear(width_size, width_size)`,\n\t            `Linear(width_size, out_size)`].\n\t        - `activation`: The activation function after each hidden layer. Defaults to\n\t            ReLU.\n\t        - `final_activation`: The activation function after the output layer. Defaults\n\t            to the identity.\n\t        - `key`: A `jax.random.PRNGKey` used to provide randomness for parameter\n\t            initialisation. (Keyword only argument.)\n", "        Note that `in_size` also supports the string `\"scalar\"` as a special value.\n\t        In this case the input to the module should be of shape `()`.\n\t        Likewise `out_size` can also be a string `\"scalar\"`, in which case the\n\t        output from the module will have shape `()`.\n\t        \"\"\"\n\t        super().__init__(**kwargs)\n\t        keys = jrandom.split(key, depth + 1)\n\t        layers = []\n\t        if depth == 0:\n\t            layers.append(Linear(in_size, out_size, key=keys[0]))\n", "        else:\n\t            layers.append(Linear(in_size, width_size, key=keys[0]))\n\t            for i in range(depth - 1):\n\t                layers.append(Linear(width_size, width_size, key=keys[i + 1]))\n\t            layers.append(Linear(width_size, out_size, key=keys[-1]))\n\t        self.layers = tuple(layers)\n\t        self.in_size = in_size\n\t        self.out_size = out_size\n\t        self.width_size = width_size\n\t        self.depth = depth\n", "        self.activation = activation\n\t        self.final_activation = final_activation\n\t    def __call__(self, x: Array, *, key: Optional[PRNGKey] = None) -> Array:\n\t        \"\"\"**Arguments:**\n\t        - `x`: A JAX array with shape `(in_size,)`. (Or shape `()` if\n\t            `in_size=\"scalar\"`.)\n\t        - `key`: Ignored; provided for compatibility with the rest of the Equinox API.\n\t            (Keyword only argument.)\n\t        **Returns:**\n\t        A JAX array with shape `(out_size,)`. (Or shape `()` if `out_size=\"scalar\"`.)\n", "        \"\"\"\n\t        for layer in self.layers[:-1]:\n\t            x = layer(x)\n\t            x = self.activation(x)\n\t        x = self.layers[-1](x)\n\t        x = self.final_activation(x)\n\t        return x\n"]}
{"filename": "utils/misc.py", "chunked_list": ["import flatdict, mlflow, os, boto3, botocore, shutil, pickle, yaml, operator\n\tfrom urllib.parse import urlparse\n\tfrom mlflow.tracking import MlflowClient\n\timport jax\n\timport equinox as eqx\n\tdef log_params(cfg):\n\t    flattened_dict = dict(flatdict.FlatDict(cfg, delimiter=\".\"))\n\t    num_entries = len(flattened_dict.keys())\n\t    if num_entries > 100:\n\t        num_batches = num_entries % 100\n", "        fl_list = list(flattened_dict.items())\n\t        for i in range(num_batches):\n\t            end_ind = min((i + 1) * 100, num_entries)\n\t            trunc_dict = {k: v for k, v in fl_list[i * 100 : end_ind]}\n\t            mlflow.log_params(trunc_dict)\n\t    else:\n\t        mlflow.log_params(flattened_dict)\n\tdef get_cfg(artifact_uri, temp_path):\n\t    dest_file_path = download_file(\"config.yaml\", artifact_uri, temp_path)\n\t    with open(dest_file_path, \"r\") as file:\n", "        cfg = yaml.safe_load(file)\n\t    return cfg\n\tdef get_weights(artifact_uri, temp_path, models):\n\t    dest_file_path = download_file(\"weights.eqx\", artifact_uri, temp_path)\n\t    if dest_file_path is not None:\n\t        # with open(dest_file_path, \"rb\") as file:\n\t        #     weights = pickle.load(file)\n\t        # return weights\n\t        return eqx.tree_deserialise_leaves(dest_file_path, like=models)\n\t    else:\n", "        return None\n\tdef download_file(fname, artifact_uri, destination_path):\n\t    file_uri = mlflow.get_artifact_uri(fname)\n\t    dest_file_path = os.path.join(destination_path, fname)\n\t    if \"s3\" in artifact_uri:\n\t        s3 = boto3.client(\"s3\")\n\t        out = urlparse(file_uri, allow_fragments=False)\n\t        bucket_name = out.netloc\n\t        rest_of_path = out.path\n\t        try:\n", "            s3.download_file(bucket_name, rest_of_path[1:], dest_file_path)\n\t        except botocore.exceptions.ClientError as e:\n\t            return None\n\t    elif \"file\" in artifact_uri:\n\t        file_uri = file_uri[7:]\n\t        if os.path.exists(file_uri):\n\t            shutil.copyfile(file_uri, dest_file_path)\n\t        else:\n\t            return None\n\t    else:\n", "        raise NotImplementedError\n\t    return dest_file_path\n\tdef is_job_done(run_id):\n\t    return MlflowClient().get_run(run_id).data.tags[\"status\"] == \"completed\"\n\tdef get_this_metric_of_this_run(metric_name, run_id):\n\t    run = MlflowClient().get_run(run_id)\n\t    return run.data.metrics[metric_name]\n\tdef download_and_open_file_from_this_run(fname, run_id, destination_path):\n\t    mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=fname, dst_path=destination_path)\n\t    with open(os.path.join(destination_path, fname), \"rb\") as f:\n", "        this_file = pickle.load(f)\n\t    return this_file\n\tdef all_reduce_gradients(gradients, num):\n\t    if num > 1:\n\t        def _safe_add(a1, a2):\n\t            if a1 is None:\n\t                return a2\n\t            else:\n\t                return a1 + a2\n\t        def _is_none(x):\n", "            return x is None\n\t        def _safe_divide(a1):\n\t            if a1 is None:\n\t                return a1\n\t            else:\n\t                return a1 / num\n\t        summed_gradients = jax.tree_map(_safe_add, gradients[0], gradients[1], is_leaf=_is_none)\n\t        for i in range(2, num):\n\t            summed_gradients = jax.tree_map(_safe_add, summed_gradients, gradients[i], is_leaf=_is_none)\n\t        average_gradient = jax.tree_map(_safe_divide, summed_gradients, is_leaf=_is_none)\n", "    else:\n\t        average_gradient = gradients[0]\n\t    return average_gradient\n\tdef get_jq(client: boto3.client, desired_machine: str):\n\t    queues = client.describe_job_queues()\n\t    for queue in queues[\"jobQueues\"]:\n\t        if desired_machine == queue[\"jobQueueName\"]:\n\t            return queue[\"jobQueueArn\"]\n\tdef get_jd(client: boto3.client, sim_type: str, desired_machine: str):\n\t    jobdefs = client.describe_job_definitions()\n", "    for jobdef in jobdefs[\"jobDefinitions\"]:\n\t        if (\n\t            desired_machine in jobdef[\"jobDefinitionName\"]\n\t            and jobdef[\"status\"] == \"ACTIVE\"\n\t            and sim_type in jobdef[\"jobDefinitionName\"]\n\t        ):\n\t            return jobdef[\"jobDefinitionArn\"]\n\tdef queue_sim(sim_request):\n\t    client = boto3.client(\"batch\", region_name=\"us-east-1\")\n\t    job_template = {\n", "        \"jobQueue\": get_jq(client, sim_request[\"machine\"]),\n\t        \"jobDefinition\": get_jd(client, sim_request[\"sim_type\"], sim_request[\"machine\"]),\n\t        \"jobName\": sim_request[\"job_name\"],\n\t        \"parameters\": {\"run_id\": sim_request[\"run_id\"], \"run_type\": sim_request[\"run_type\"]},\n\t        \"retryStrategy\": {\"attempts\": 10, \"evaluateOnExit\": [{\"action\": \"RETRY\", \"onStatusReason\": \"Host EC2*\"}]},\n\t    }\n\t    submissionResult = client.submit_job(**job_template)\n\t    return submissionResult\n"]}
{"filename": "tests/test_resonance_search.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n\t#  research@ergodic.io\n\timport yaml, pytest\n\tfrom itertools import product\n\timport numpy as np\n\tfrom jax.config import config\n\tconfig.update(\"jax_enable_x64\", True)\n\t# config.update(\"jax_debug_nans\", True)\n\t# config.update(\"jax_disable_jit\", True)\n\timport jax, mlflow, diffrax, optax\n", "from jax import numpy as jnp\n\timport tempfile, time\n\tfrom diffrax import diffeqsolve, ODETerm, SaveAt, Tsit5\n\tfrom utils import misc\n\tfrom jaxopt import OptaxSolver\n\timport equinox as eqx\n\tfrom tqdm import tqdm\n\tfrom adept.es1d import helpers\n\tfrom theory.electrostatic import get_roots_to_electrostatic_dispersion\n\tdef load_cfg(rand_k0, gamma, adjoint):\n", "    with open(\"./tests/configs/resonance_search.yaml\", \"r\") as file:\n\t        defaults = yaml.safe_load(file)\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n\t    defaults[\"physics\"][\"electron\"][\"gamma\"] = gamma\n\t    defaults[\"adjoint\"] = adjoint\n\t    if gamma == \"kinetic\":\n\t        wepw = np.real(get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0))\n\t        defaults[\"mlflow\"][\"run\"] = \"kinetic\"\n\t    else:\n\t        wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n", "        defaults[\"mlflow\"][\"run\"] = \"bohm-gross\"\n\t    xmax = float(2.0 * np.pi / rand_k0)\n\t    defaults[\"grid\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\t    return defaults, wepw\n\t@pytest.mark.parametrize(\"adjoint\", [\"Recursive\", \"Backsolve\"])\n\t@pytest.mark.parametrize(\"gamma\", [\"kinetic\", 3.0])\n\tdef test_resonance_search(gamma, adjoint):\n\t    mlflow.set_experiment(\"test-res-search\")\n\t    with mlflow.start_run(run_name=\"res-search-opt\") as mlflow_run:\n", "        vg_func, sim_k0, actual_w0 = get_vg_func(gamma, adjoint)\n\t        mod_defaults, _ = load_cfg(sim_k0, gamma, adjoint)\n\t        mod_defaults[\"grid\"] = helpers.get_derived_quantities(mod_defaults[\"grid\"])\n\t        misc.log_params(mod_defaults)\n\t        mod_defaults[\"grid\"] = helpers.get_solver_quantities(mod_defaults[\"grid\"])\n\t        mod_defaults = helpers.get_save_quantities(mod_defaults)\n\t        rng_key = jax.random.PRNGKey(420)\n\t        w0 = 1.1 + 0.05 * jax.random.normal(rng_key, [1], dtype=jnp.float64)\n\t        # optimizer = optax.adam(0.1)\n\t        # opt_state = optimizer.init(w0)\n", "        t0 = time.time()\n\t        optimizer = OptaxSolver(fun=vg_func, value_and_grad=True, has_aux=True, opt=optax.adam(learning_rate=0.1))\n\t        opt_state = optimizer.init_state(w0)\n\t        mlflow.log_metrics({\"init_time\": round(time.time() - t0, 4)})\n\t        mlflow.log_metrics({\"w0\": float(w0)}, step=0)\n\t        mlflow.log_metrics({\"actual_w0\": actual_w0}, step=0)\n\t        for i in tqdm(range(40)):\n\t            w0, opt_state, loss = run_one_step(i, w0, vg_func, mod_defaults, optimizer, opt_state)\n\t            mlflow.log_metrics({\"w0\": float(w0), \"actual_w0\": actual_w0, \"loss\": float(loss)}, step=i + 1)\n\t        print(f\"{gamma=}, {adjoint=}\")\n", "        print(f\"{actual_w0=}, {float(w0)=}\")\n\t        np.testing.assert_allclose(actual_w0, float(w0), rtol=0.03)\n\tdef run_one_step(i, w0, vg_func, mod_defaults, optimizer, opt_state):\n\t    with mlflow.start_run(run_name=f\"res-search-run-{i}\", nested=True) as mlflow_run:\n\t        mlflow.log_param(\"w0\", w0)\n\t        t0 = time.time()\n\t        # (loss, results), grad = vg_func(w0)\n\t        # updates, opt_state = optimizer.update(grad, opt_state, w0)\n\t        # w0 = optax.apply_updates(w0, updates)\n\t        w0, opt_state = optimizer.update(params=w0, state=opt_state)\n", "        loss = opt_state.error\n\t        results = opt_state.aux\n\t        mlflow.log_metrics({\"run_time\": round(time.time() - t0, 4)})\n\t        with tempfile.TemporaryDirectory() as td:\n\t            t0 = time.time()\n\t            helpers.post_process(results, mod_defaults, td)\n\t            mlflow.log_metrics({\"postprocess_time\": round(time.time() - t0, 4), \"loss\": float(loss)})\n\t            # log artifacts\n\t            mlflow.log_artifacts(td)\n\t    return w0, opt_state, loss\n", "def get_vg_func(gamma, adjoint):\n\t    rng = np.random.default_rng(420)\n\t    sim_k0 = rng.uniform(0.26, 0.4)\n\t    defaults, actual_w0 = load_cfg(sim_k0, gamma, adjoint)\n\t    defaults[\"grid\"] = helpers.get_derived_quantities(defaults[\"grid\"])\n\t    misc.log_params(defaults)\n\t    defaults[\"grid\"] = helpers.get_solver_quantities(defaults[\"grid\"])\n\t    defaults = helpers.get_save_quantities(defaults)\n\t    pulse_dict = {\"driver\": defaults[\"drivers\"]}\n\t    state = helpers.init_state(defaults)\n", "    loss_fn = get_loss(state, pulse_dict, defaults)\n\t    vg_func = eqx.filter_jit(jax.value_and_grad(loss_fn, argnums=0, has_aux=True))\n\t    return vg_func, sim_k0, actual_w0\n\tdef get_loss(state, pulse_dict, mod_defaults):\n\t    if mod_defaults[\"adjoint\"] == \"Recursive\":\n\t        adjoint = diffrax.RecursiveCheckpointAdjoint()\n\t    elif mod_defaults[\"adjoint\"] == \"Backsolve\":\n\t        adjoint = diffrax.BacksolveAdjoint(solver=Tsit5())\n\t    else:\n\t        raise NotImplementedError\n", "    def loss(w0):\n\t        pulse_dict[\"driver\"][\"ex\"][\"0\"][\"w0\"] = w0\n\t        vf = helpers.VectorField(mod_defaults, models=False)\n\t        results = diffeqsolve(\n\t            terms=ODETerm(vf),\n\t            solver=Tsit5(),\n\t            t0=mod_defaults[\"grid\"][\"tmin\"],\n\t            t1=mod_defaults[\"grid\"][\"tmax\"],\n\t            max_steps=mod_defaults[\"grid\"][\"max_steps\"],\n\t            dt0=mod_defaults[\"grid\"][\"dt\"],\n", "            adjoint=adjoint,\n\t            y0=state,\n\t            args=pulse_dict,\n\t            saveat=SaveAt(ts=mod_defaults[\"save\"][\"t\"][\"ax\"]),\n\t        )\n\t        nk1 = jnp.abs(jnp.fft.fft(results.ys[\"electron\"][\"n\"], axis=1)[:, 1])\n\t        return -jnp.amax(nk1), results\n\t    return loss\n\tif __name__ == \"__main__\":\n\t    for gamma, adjoint in product([\"kinetic\", 3.0], [\"Recursive\", \"Backsolve\"]):\n", "        test_resonance_search(gamma, adjoint)\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_resonance.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n\t#  research@ergodic.io\n\timport yaml, pytest\n\timport numpy as np\n\tfrom jax.config import config\n\tconfig.update(\"jax_enable_x64\", True)\n\t# config.update(\"jax_disable_jit\", True)\n\tfrom jax import numpy as jnp\n\timport mlflow\n\tfrom theory import electrostatic\n", "from utils.runner import run\n\tdef _modify_defaults_(defaults, rng, gamma):\n\t    rand_k0 = np.round(rng.uniform(0.25, 0.4), 3)\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n\t    defaults[\"physics\"][\"electron\"][\"gamma\"] = gamma\n\t    if gamma == \"kinetic\":\n\t        root = np.real(electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0))\n\t        defaults[\"mlflow\"][\"run\"] = \"kinetic\"\n\t    else:\n\t        root = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n", "        defaults[\"mlflow\"][\"run\"] = \"bohm-gross\"\n\t    xmax = float(2.0 * np.pi / rand_k0)\n\t    defaults[\"grid\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\t    defaults[\"mlflow\"][\"experiment\"] = \"test-resonance\"\n\t    return defaults, float(root)\n\t@pytest.mark.parametrize(\"gamma\", [\"kinetic\", 3.0])\n\tdef test_single_resonance(gamma):\n\t    with open(\"./tests/configs/resonance.yaml\", \"r\") as file:\n\t        defaults = yaml.safe_load(file)\n", "    # modify config\n\t    rng = np.random.default_rng()\n\t    mod_defaults, actual_resonance = _modify_defaults_(defaults, rng, gamma)\n\t    # run\n\t    mlflow.set_experiment(mod_defaults[\"mlflow\"][\"experiment\"])\n\t    # modify config\n\t    with mlflow.start_run(run_name=mod_defaults[\"mlflow\"][\"run\"]) as mlflow_run:\n\t        result = run(mod_defaults)\n\t    kx = (\n\t        np.fft.fftfreq(\n", "            mod_defaults[\"save\"][\"x\"][\"nx\"], d=mod_defaults[\"save\"][\"x\"][\"ax\"][2] - mod_defaults[\"save\"][\"x\"][\"ax\"][1]\n\t        )\n\t        * 2.0\n\t        * np.pi\n\t    )\n\t    one_over_kx = np.zeros_like(kx)\n\t    one_over_kx[1:] = 1.0 / kx[1:]\n\t    efs = jnp.real(\n\t        jnp.fft.ifft(\n\t            1j\n", "            * one_over_kx[None, :]\n\t            * jnp.fft.fft(result.ys[\"x\"][\"ion\"][\"n\"][:, :] - result.ys[\"x\"][\"electron\"][\"n\"][:, :])\n\t        )\n\t    )\n\t    ek1 = np.fft.fft(efs, axis=1)[:, 1]\n\t    env, freq = electrostatic.get_nlfs(ek1, result.ts[1] - result.ts[0])\n\t    frslc = slice(-80, -10)\n\t    print(\n\t        f\"Frequency check \\n\"\n\t        f\"measured: {np.round(np.mean(freq[frslc]), 5)}, \"\n", "        f\"desired: {np.round(actual_resonance, 5)}, \"\n\t    )\n\t    measured_resonance = np.mean(freq[frslc])\n\t    np.testing.assert_almost_equal(measured_resonance, actual_resonance, decimal=2)\n\tif __name__ == \"__main__\":\n\t    for gamma in [\"kinetic\", 3.0]:\n\t        test_single_resonance(gamma)\n"]}
{"filename": "tests/test_landau_damping.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n\t#  research@ergodic.io\n\timport yaml\n\timport numpy as np\n\tfrom jax.config import config\n\tconfig.update(\"jax_enable_x64\", True)\n\t# config.update(\"jax_disable_jit\", True)\n\tfrom jax import numpy as jnp\n\timport mlflow\n\tfrom theory import electrostatic\n", "from utils.runner import run\n\tdef _modify_defaults_(defaults, rng):\n\t    rand_k0 = np.round(rng.uniform(0.25, 0.4), 3)\n\t    wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n\t    root = electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0)\n\t    print(rand_k0, wepw, root)\n\t    defaults[\"physics\"][\"landau_damping\"] = True\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = float(wepw)\n\t    xmax = float(2.0 * np.pi / rand_k0)\n", "    defaults[\"grid\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"kx\"][\"kxmax\"] = rand_k0\n\t    defaults[\"mlflow\"][\"experiment\"] = \"test-landau-damping\"\n\t    return defaults, float(np.imag(root))\n\tdef test_single_resonance():\n\t    with open(\"./tests/configs/resonance.yaml\", \"r\") as file:\n\t        defaults = yaml.safe_load(file)\n\t    # modify config\n\t    rng = np.random.default_rng()\n", "    mod_defaults, actual_damping_rate = _modify_defaults_(defaults, rng)\n\t    # run\n\t    mlflow.set_experiment(mod_defaults[\"mlflow\"][\"experiment\"])\n\t    # modify config\n\t    with mlflow.start_run(run_name=mod_defaults[\"mlflow\"][\"run\"]) as mlflow_run:\n\t        result = run(mod_defaults)\n\t    kx = (\n\t        np.fft.fftfreq(\n\t            mod_defaults[\"save\"][\"x\"][\"nx\"], d=mod_defaults[\"save\"][\"x\"][\"ax\"][2] - mod_defaults[\"save\"][\"x\"][\"ax\"][1]\n\t        )\n", "        * 2.0\n\t        * np.pi\n\t    )\n\t    one_over_kx = np.zeros_like(kx)\n\t    one_over_kx[1:] = 1.0 / kx[1:]\n\t    efs = jnp.real(jnp.fft.ifft(1j * one_over_kx[None, :] * jnp.fft.fft(1 - result.ys[\"x\"][\"electron\"][\"n\"][:, :])))\n\t    ek1 = (2.0 / mod_defaults[\"grid\"][\"nx\"] * np.abs(np.fft.fft(efs, axis=1)[:, 1])) ** 2.0\n\t    frslc = slice(-100, -50)\n\t    measured_damping_rate = np.mean(np.gradient(ek1[frslc], (result.ts[1] - result.ts[0])) / ek1[frslc])\n\t    print(\n", "        f\"Landau Damping rate check \\n\"\n\t        f\"measured: {np.round(measured_damping_rate, 5)}, \"\n\t        f\"actual: {np.round(2*actual_damping_rate, 5)}, \"\n\t    )\n\t    np.testing.assert_almost_equal(measured_damping_rate, 2 * actual_damping_rate, decimal=2)\n\tif __name__ == \"__main__\":\n\t    test_single_resonance()\n"]}
{"filename": "tests/test_against_vlasov.py", "chunked_list": ["#  Copyright (c) Ergodic LLC 2023\n\t#  research@ergodic.io\n\timport yaml\n\timport numpy as np\n\tfrom jax.config import config\n\tconfig.update(\"jax_enable_x64\", True)\n\t# config.update(\"jax_disable_jit\", True)\n\tfrom jax import numpy as jnp\n\timport mlflow\n\timport xarray as xr\n", "from theory import electrostatic\n\tfrom utils.runner import run\n\tdef _modify_defaults_(defaults):\n\t    rand_k0 = 0.358\n\t    wepw = np.sqrt(1.0 + 3.0 * rand_k0**2.0)\n\t    root = electrostatic.get_roots_to_electrostatic_dispersion(1.0, 1.0, rand_k0)\n\t    defaults[\"physics\"][\"landau_damping\"] = True\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"k0\"] = float(rand_k0)\n\t    defaults[\"drivers\"][\"ex\"][\"0\"][\"w0\"] = float(wepw)\n\t    xmax = float(2.0 * np.pi / rand_k0)\n", "    # defaults[\"save\"][\"field\"][\"xmax_to_store\"] = float(2.0 * np.pi / rand_k0)\n\t    defaults[\"grid\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"x\"][\"xmax\"] = xmax\n\t    defaults[\"save\"][\"kx\"][\"kxmax\"] = rand_k0\n\t    defaults[\"mlflow\"][\"experiment\"] = \"test-against-vlasov\"\n\t    return defaults, float(np.imag(root))\n\tdef test_single_resonance():\n\t    with open(\"./tests/configs/vlasov_comparison.yaml\", \"r\") as file:\n\t        defaults = yaml.safe_load(file)\n\t    # modify config\n", "    mod_defaults, actual_damping_rate = _modify_defaults_(defaults)\n\t    # run\n\t    mlflow.set_experiment(mod_defaults[\"mlflow\"][\"experiment\"])\n\t    # modify config\n\t    with mlflow.start_run(run_name=mod_defaults[\"mlflow\"][\"run\"]) as mlflow_run:\n\t        result = run(mod_defaults)\n\t    vds = xr.open_dataset(\"tests/vlasov-reference/all-fields-kx.nc\", engine=\"h5netcdf\")\n\t    nk1_fluid = result.ys[\"kx\"][\"electron\"][\"n\"][\"mag\"][:, 1]\n\t    nk1_vlasov = vds[\"n-(k_x)\"][:, 1].data\n\t    t_fluid = result.ts\n", "    t_vlasov = vds.coords[\"t\"].data\n\t    fluid_slc = slice(80, 160)\n\t    vlasov_slc = slice(700, 850)\n\t    vlasov_damping_rate = np.mean(\n\t        np.gradient(nk1_vlasov[vlasov_slc], (t_vlasov[1] - t_vlasov[0])) / nk1_vlasov[vlasov_slc]\n\t    )\n\t    fluid_damping_rate = np.mean(np.gradient(nk1_fluid[fluid_slc], (t_fluid[1] - t_fluid[0])) / nk1_fluid[fluid_slc])\n\t    print(f\"{vlasov_damping_rate=}, {fluid_damping_rate=}\")\n\t    print(f\"{np.amax(nk1_vlasov)=}, {np.amax(nk1_fluid)=}\")\n\t    np.testing.assert_almost_equal(vlasov_damping_rate, fluid_damping_rate, decimal=2)\n", "    np.testing.assert_allclose(np.amax(nk1_fluid), np.amax(nk1_vlasov), rtol=0.05)\n\tif __name__ == \"__main__\":\n\t    test_single_resonance()\n"]}
{"filename": "adept/__init__.py", "chunked_list": []}
{"filename": "adept/es1d/pushers.py", "chunked_list": ["from typing import Dict\n\timport jax\n\tfrom jax import numpy as jnp\n\timport numpy as np\n\timport equinox as eqx\n\tfrom theory.electrostatic import get_complex_frequency_table\n\tdef get_envelope(p_wL, p_wR, p_L, p_R, ax):\n\t    return 0.5 * (jnp.tanh((ax - p_L) / p_wL) - jnp.tanh((ax - p_R) / p_wR))\n\tclass WaveSolver(eqx.Module):\n\t    dx: float\n", "    c: float\n\t    c_sq: float\n\t    dt: float\n\t    const: float\n\t    one_over_const: float\n\t    def __init__(self, c: jnp.float64, dx: jnp.float64, dt: jnp.float64):\n\t        self.dx = dx\n\t        self.c = c\n\t        self.c_sq = c**2.0\n\t        c_over_dx = c / dx\n", "        self.dt = dt\n\t        self.const = c_over_dx * dt\n\t        # abc_const = (const - 1.0) / (const + 1.0)\n\t        self.one_over_const = 1.0 / dt / c_over_dx\n\t    def apply_2nd_order_abc(self, aold, a, anew):\n\t        \"\"\"\n\t        Second order absorbing boundary conditions\n\t        :param aold:\n\t        :param a:\n\t        :param anew:\n", "        :param dt:\n\t        :return:\n\t        \"\"\"\n\t        coeff = -1.0 / (self.one_over_const + 2.0 + self.const)\n\t        # # 2nd order ABC\n\t        a_left = (self.one_over_const - 2.0 + self.const) * (anew[1] + aold[0])\n\t        a_left += 2.0 * (self.const - self.one_over_const) * (a[0] + a[2] - anew[0] - aold[1])\n\t        a_left -= 4.0 * (self.one_over_const + self.const) * a[1]\n\t        a_left *= coeff\n\t        a_left -= aold[2]\n", "        a_left = jnp.array([a_left])\n\t        a_right = (self.one_over_const - 2.0 + self.const) * (anew[-2] + aold[-1])\n\t        a_right += 2.0 * (self.const - self.one_over_const) * (a[-1] + a[-3] - anew[-1] - aold[-2])\n\t        a_right -= 4.0 * (self.one_over_const + self.const) * a[-2]\n\t        a_right *= coeff\n\t        a_right -= aold[-3]\n\t        a_right = jnp.array([a_right])\n\t        # commenting out first order damping\n\t        # a_left = jnp.array([a[1] + abc_const * (anew[0] - a[0])])\n\t        # a_right = jnp.array([a[-2] + abc_const * (anew[-1] - a[-1])])\n", "        return jnp.concatenate([a_left, anew, a_right])\n\t    def __call__(self, a: jnp.ndarray, aold: jnp.ndarray, djy_array: jnp.ndarray, electron_charge: jnp.ndarray):\n\t        if self.c > 0:\n\t            d2dx2 = (a[:-2] - 2.0 * a[1:-1] + a[2:]) / self.dx**2.0\n\t            anew = (\n\t                2.0 * a[1:-1]\n\t                - aold[1:-1]\n\t                + self.dt**2.0 * (self.c_sq * d2dx2 - electron_charge * a[1:-1] + djy_array)\n\t            )\n\t            return self.apply_2nd_order_abc(aold, a, anew), a\n", "        else:\n\t            return a, aold\n\tclass Driver(eqx.Module):\n\t    xax: jax.Array\n\t    def __init__(self, xax):\n\t        self.xax = xax\n\t    def __call__(self, this_pulse: Dict, current_time: jnp.float64):\n\t        kk = this_pulse[\"k0\"]\n\t        ww = this_pulse[\"w0\"]\n\t        dw = this_pulse[\"dw0\"]\n", "        t_L = this_pulse[\"t_c\"] - this_pulse[\"t_w\"] * 0.5\n\t        t_R = this_pulse[\"t_c\"] + this_pulse[\"t_w\"] * 0.5\n\t        t_wL = this_pulse[\"t_r\"]\n\t        t_wR = this_pulse[\"t_r\"]\n\t        x_L = this_pulse[\"x_c\"] - this_pulse[\"x_w\"] * 0.5\n\t        x_R = this_pulse[\"x_c\"] + this_pulse[\"x_w\"] * 0.5\n\t        x_wL = this_pulse[\"x_r\"]\n\t        x_wR = this_pulse[\"x_r\"]\n\t        envelope_t = get_envelope(t_wL, t_wR, t_L, t_R, current_time)\n\t        envelope_x = get_envelope(x_wL, x_wR, x_L, x_R, self.xax)\n", "        return (\n\t            envelope_t * envelope_x * jnp.abs(kk) * this_pulse[\"a0\"] * jnp.sin(kk * self.xax - (ww + dw) * current_time)\n\t        )\n\tclass PoissonSolver(eqx.Module):\n\t    one_over_kx: jax.Array\n\t    def __init__(self, one_over_kx):\n\t        self.one_over_kx = one_over_kx\n\t    def __call__(self, dn):\n\t        return jnp.real(jnp.fft.ifft(1j * self.one_over_kx * jnp.fft.fft(dn)))\n\tclass StepAmpere(eqx.Module):\n", "    def __call__(self, n, u):\n\t        return n * u\n\tdef gradient(arr, kx):\n\t    return jnp.real(jnp.fft.ifft(1j * kx * jnp.fft.fft(arr)))\n\tclass DensityStepper(eqx.Module):\n\t    kx: jax.Array\n\t    def __init__(self, kx):\n\t        self.kx = kx\n\t    def __call__(self, n, u):\n\t        return -u * gradient(n, self.kx) - n * gradient(u, self.kx)\n", "class VelocityStepper(eqx.Module):\n\t    kx: jax.Array\n\t    wr_corr: jax.Array\n\t    wis: jax.Array\n\t    def __init__(self, kx, kxr, one_over_kxr, physics):\n\t        self.kx = kx\n\t        wrs, wis, klds = get_complex_frequency_table(1024, True if physics[\"gamma\"] == \"kinetic\" else False)\n\t        wrs = jnp.array(jnp.interp(kxr, klds, wrs, left=1.0, right=wrs[-1]))\n\t        if physics[\"gamma\"] == \"kinetic\":\n\t            self.wr_corr = (jnp.square(wrs) - 1.0) * one_over_kxr**2.0\n", "        else:\n\t            self.wr_corr = 1.0\n\t        if physics[\"landau_damping\"]:\n\t            self.wis = jnp.array(jnp.interp(kxr, klds, wis, left=0.0, right=wis[-1]))\n\t        else:\n\t            self.wis = jnp.zeros_like(kxr)\n\t    def landau_damping_term(self, u):\n\t        return 2 * jnp.real(jnp.fft.irfft(self.wis * jnp.fft.rfft(u)))\n\t    def restoring_force_term(self, gradp_over_nm):\n\t        return jnp.real(jnp.fft.irfft(self.wr_corr * jnp.fft.rfft(gradp_over_nm)))\n", "    def __call__(self, n, u, p_over_m, q_over_m_times_e, delta):\n\t        return (\n\t            -u * gradient(u, self.kx)\n\t            - self.restoring_force_term(gradient(p_over_m, self.kx) / n)\n\t            - q_over_m_times_e\n\t            + self.landau_damping_term(u) / (1.0 + delta**2)\n\t        )\n\tclass EnergyStepper(eqx.Module):\n\t    kx: jax.Array\n\t    gamma: float\n", "    def __init__(self, kx, physics):\n\t        self.kx = kx\n\t        if physics[\"gamma\"] == \"kinetic\":\n\t            self.gamma = 1.0\n\t        else:\n\t            self.gamma = physics[\"gamma\"]\n\t    def __call__(self, n, u, p_over_m, q_over_m_times_e):\n\t        return (\n\t            -u * gradient(p_over_m, self.kx)\n\t            - self.gamma * p_over_m * gradient(u, self.kx)\n", "            - 2 * n * u * q_over_m_times_e\n\t        )\n\tclass ParticleTrapper(eqx.Module):\n\t    kxr: np.ndarray\n\t    kx: jax.Array\n\t    model_kld: float\n\t    wrs: jax.Array\n\t    wis: jax.Array\n\t    table_klds: jax.Array\n\t    norm_kld: jnp.float64\n", "    norm_nuee: jnp.float64\n\t    vph: jnp.float64\n\t    nu_g_model: eqx.Module\n\t    # nu_d_model: eqx.Module\n\t    def __init__(self, cfg, species=\"electron\", models=None):\n\t        nuee = cfg[\"physics\"][species][\"trapping\"][\"nuee\"]\n\t        if cfg[\"physics\"][species][\"gamma\"] == \"kinetic\":\n\t            kinetic_real_epw = True\n\t        else:\n\t            kinetic_real_epw = False\n", "        self.kxr = cfg[\"grid\"][\"kxr\"]\n\t        self.kx = cfg[\"grid\"][\"kx\"]\n\t        table_wrs, table_wis, table_klds = get_complex_frequency_table(1024, kinetic_real_epw)\n\t        self.model_kld = cfg[\"physics\"][species][\"trapping\"][\"kld\"]\n\t        self.wrs = jnp.interp(cfg[\"grid\"][\"kxr\"], table_klds, table_wrs, left=1.0, right=table_wrs[-1])\n\t        self.wis = jnp.interp(cfg[\"grid\"][\"kxr\"], table_klds, table_wis, left=0.0, right=0.0)\n\t        self.table_klds = table_klds\n\t        self.norm_kld = (self.model_kld - 0.26) / 0.14\n\t        self.norm_nuee = (jnp.log10(nuee) + 7.0) / -4.0\n\t        self.vph = jnp.interp(self.model_kld, table_klds, table_wrs, left=1.0, right=table_wrs[-1]) / self.model_kld\n", "        # Make models\n\t        if models:\n\t            self.nu_g_model = models[\"nu_g\"]\n\t        else:\n\t            self.nu_g_model = lambda x: 1e-3\n\t    def __call__(self, e, delta, args):\n\t        ek = jnp.fft.rfft(e, axis=0) * 2.0 / self.kx.size\n\t        norm_e = (jnp.log10(jnp.interp(self.model_kld, self.kxr, jnp.abs(ek)) + 1e-10) + 10.0) / -10.0\n\t        func_inputs = jnp.stack([norm_e, self.norm_kld, self.norm_nuee], axis=-1)\n\t        # jax.debug.print(\"{x}\", x=func_inputs)\n", "        growth_rates = 10 ** (3 * jnp.squeeze(self.nu_g_model(func_inputs)))\n\t        return -self.vph * gradient(delta, self.kx) + growth_rates * jnp.abs(\n\t            jnp.fft.irfft(ek * self.kx.size / 2.0 * self.wis)\n\t        ) / (1.0 + delta**2.0)\n"]}
{"filename": "adept/es1d/__init__.py", "chunked_list": []}
{"filename": "adept/es1d/helpers.py", "chunked_list": ["from collections import defaultdict\n\tfrom typing import Callable, Dict\n\tfrom functools import partial\n\timport os\n\timport jax.random\n\timport numpy as np\n\tfrom matplotlib import pyplot as plt\n\timport xarray as xr\n\tfrom jax import tree_util as jtu\n\tfrom flatdict import FlatDict\n", "import equinox as eqx\n\tfrom jax import numpy as jnp\n\tfrom adept.es1d import pushers\n\tfrom utils import nn\n\tdef save_arrays(result, td, cfg, label):\n\t    if label is None:\n\t        label = \"x\"\n\t        flattened_dict = dict(FlatDict(result.ys, delimiter=\"-\"))\n\t    else:\n\t        flattened_dict = dict(FlatDict(result.ys[label], delimiter=\"-\"))\n", "    data_vars = {\n\t        k: xr.DataArray(v, coords=((\"t\", cfg[\"save\"][\"t\"][\"ax\"]), (label, cfg[\"save\"][label][\"ax\"])))\n\t        for k, v in flattened_dict.items()\n\t    }\n\t    saved_arrays_xr = xr.Dataset(data_vars)\n\t    saved_arrays_xr.to_netcdf(os.path.join(td, \"binary\", f\"state_vs_{label}.nc\"))\n\t    return saved_arrays_xr\n\tdef plot_xrs(which, td, xrs):\n\t    os.makedirs(os.path.join(td, \"plots\", which))\n\t    os.makedirs(os.path.join(td, \"plots\", which, \"ion\"))\n", "    os.makedirs(os.path.join(td, \"plots\", which, \"electron\"))\n\t    for k, v in xrs.items():\n\t        fname = f\"{'-'.join(k.split('-')[1:])}.png\"\n\t        fig, ax = plt.subplots(1, 1, figsize=(7, 4), tight_layout=True)\n\t        v.plot(ax=ax, cmap=\"gist_ncar\")\n\t        ax.grid()\n\t        fig.savefig(os.path.join(td, \"plots\", which, k.split(\"-\")[0], fname), bbox_inches=\"tight\")\n\t        plt.close(fig)\n\t        if which == \"kx\":\n\t            os.makedirs(os.path.join(td, \"plots\", which, \"ion\", \"hue\"), exist_ok=True)\n", "            os.makedirs(os.path.join(td, \"plots\", which, \"electron\", \"hue\"), exist_ok=True)\n\t            # only plot\n\t            if v.coords[\"kx\"].size > 8:\n\t                hue_skip = v.coords[\"kx\"].size // 8\n\t            else:\n\t                hue_skip = 1\n\t            for log in [True, False]:\n\t                fig, ax = plt.subplots(1, 1, figsize=(7, 4), tight_layout=True)\n\t                v[:, ::hue_skip].plot(ax=ax, hue=\"kx\")\n\t                ax.set_yscale(\"log\" if log else \"linear\")\n", "                ax.grid()\n\t                fig.savefig(\n\t                    os.path.join(\n\t                        td, \"plots\", which, k.split(\"-\")[0], f\"hue\", f\"{'-'.join(k.split('-')[1:])}-log-{log}.png\"\n\t                    ),\n\t                    bbox_inches=\"tight\",\n\t                )\n\t                plt.close(fig)\n\tdef post_process(result, cfg: Dict, td: str) -> None:\n\t    os.makedirs(os.path.join(td, \"binary\"))\n", "    os.makedirs(os.path.join(td, \"plots\"))\n\t    if cfg[\"save\"][\"func\"][\"is_on\"]:\n\t        if cfg[\"save\"][\"x\"][\"is_on\"]:\n\t            xrs = save_arrays(result, td, cfg, label=\"x\")\n\t            plot_xrs(\"x\", td, xrs)\n\t        if cfg[\"save\"][\"kx\"][\"is_on\"]:\n\t            xrs = save_arrays(result, td, cfg, label=\"kx\")\n\t            plot_xrs(\"kx\", td, xrs)\n\t    else:\n\t        xrs = save_arrays(result, td, cfg, label=None)\n", "        plot_xrs(\"x\", td, xrs)\n\tdef get_derived_quantities(cfg_grid: Dict) -> Dict:\n\t    \"\"\"\n\t    This function just updates the config with the derived quantities that are only integers or strings.\n\t    This is run prior to the log params step\n\t    :param cfg_grid:\n\t    :return:\n\t    \"\"\"\n\t    cfg_grid[\"dx\"] = cfg_grid[\"xmax\"] / cfg_grid[\"nx\"]\n\t    cfg_grid[\"dt\"] = 0.05 * cfg_grid[\"dx\"]\n", "    cfg_grid[\"nt\"] = int(cfg_grid[\"tmax\"] / cfg_grid[\"dt\"] + 1)\n\t    cfg_grid[\"tmax\"] = cfg_grid[\"dt\"] * cfg_grid[\"nt\"]\n\t    if cfg_grid[\"nt\"] > 1e6:\n\t        cfg_grid[\"max_steps\"] = int(1e6)\n\t        print(r\"Only running $10^6$ steps\")\n\t    else:\n\t        cfg_grid[\"max_steps\"] = cfg_grid[\"nt\"] + 4\n\t    return cfg_grid\n\tdef get_solver_quantities(cfg_grid: Dict) -> Dict:\n\t    \"\"\"\n", "    This function just updates the config with the derived quantities that are arrays\n\t    This is run after the log params step\n\t    :param cfg_grid:\n\t    :return:\n\t    \"\"\"\n\t    cfg_grid = {\n\t        **cfg_grid,\n\t        **{\n\t            \"x\": jnp.linspace(\n\t                cfg_grid[\"xmin\"] + cfg_grid[\"dx\"] / 2, cfg_grid[\"xmax\"] - cfg_grid[\"dx\"] / 2, cfg_grid[\"nx\"]\n", "            ),\n\t            \"t\": jnp.linspace(0, cfg_grid[\"tmax\"], cfg_grid[\"nt\"]),\n\t            \"kx\": jnp.fft.fftfreq(cfg_grid[\"nx\"], d=cfg_grid[\"dx\"]) * 2.0 * np.pi,\n\t            \"kxr\": jnp.fft.rfftfreq(cfg_grid[\"nx\"], d=cfg_grid[\"dx\"]) * 2.0 * np.pi,\n\t        },\n\t    }\n\t    one_over_kx = np.zeros_like(cfg_grid[\"kx\"])\n\t    one_over_kx[1:] = 1.0 / cfg_grid[\"kx\"][1:]\n\t    cfg_grid[\"one_over_kx\"] = jnp.array(one_over_kx)\n\t    one_over_kxr = np.zeros_like(cfg_grid[\"kxr\"])\n", "    one_over_kxr[1:] = 1.0 / cfg_grid[\"kxr\"][1:]\n\t    cfg_grid[\"one_over_kxr\"] = jnp.array(one_over_kxr)\n\t    return cfg_grid\n\tdef get_save_quantities(cfg: Dict) -> Dict:\n\t    \"\"\"\n\t    This function updates the config with the quantities required for the diagnostics and saving routines\n\t    :param cfg:\n\t    :return:\n\t    \"\"\"\n\t    cfg[\"save\"][\"func\"] = {**cfg[\"save\"][\"func\"], **{\"callable\": get_save_func(cfg)}}\n", "    cfg[\"save\"][\"t\"][\"ax\"] = jnp.linspace(cfg[\"save\"][\"t\"][\"tmin\"], cfg[\"save\"][\"t\"][\"tmax\"], cfg[\"save\"][\"t\"][\"nt\"])\n\t    return cfg\n\tdef init_state(cfg: Dict) -> Dict:\n\t    \"\"\"\n\t    This function initializes the state\n\t    :param cfg:\n\t    :return:\n\t    \"\"\"\n\t    state = {}\n\t    for species in [\"ion\", \"electron\"]:\n", "        state[species] = dict(\n\t            n=jnp.ones(cfg[\"grid\"][\"nx\"]),\n\t            p=jnp.full(cfg[\"grid\"][\"nx\"], cfg[\"physics\"][species][\"T0\"]),\n\t            u=jnp.zeros(cfg[\"grid\"][\"nx\"]),\n\t            delta=jnp.zeros(cfg[\"grid\"][\"nx\"]),\n\t        )\n\t    return state\n\tclass VectorField(eqx.Module):\n\t    \"\"\"\n\t    This function returns the function that defines $d_state / dt$\n", "    All the pushers are chosen and initialized here and a single time-step is defined here.\n\t    We use the time-integrators provided by diffrax, and therefore, only need $d_state / dt$ here\n\t    :param cfg:\n\t    :return:\n\t    \"\"\"\n\t    cfg: Dict\n\t    pusher_dict: Dict\n\t    push_driver: Callable\n\t    poisson_solver: Callable\n\t    def __init__(self, cfg, models):\n", "        super().__init__()\n\t        self.cfg = cfg\n\t        self.pusher_dict = {\"ion\": {}, \"electron\": {}}\n\t        for species_name in [\"ion\", \"electron\"]:\n\t            self.pusher_dict[species_name][\"push_n\"] = pushers.DensityStepper(cfg[\"grid\"][\"kx\"])\n\t            self.pusher_dict[species_name][\"push_u\"] = pushers.VelocityStepper(\n\t                cfg[\"grid\"][\"kx\"], cfg[\"grid\"][\"kxr\"], cfg[\"grid\"][\"one_over_kxr\"], cfg[\"physics\"][species_name]\n\t            )\n\t            self.pusher_dict[species_name][\"push_e\"] = pushers.EnergyStepper(\n\t                cfg[\"grid\"][\"kx\"], cfg[\"physics\"][species_name]\n", "            )\n\t            if cfg[\"physics\"][species_name][\"trapping\"][\"is_on\"]:\n\t                self.pusher_dict[species_name][\"particle_trapper\"] = pushers.ParticleTrapper(cfg, species_name, models)\n\t        self.push_driver = pushers.Driver(cfg[\"grid\"][\"x\"])\n\t        # if \"ey\" in self.cfg[\"drivers\"]:\n\t        #     self.wave_solver = pushers.WaveSolver(cfg[\"grid\"][\"c\"], cfg[\"grid\"][\"dx\"], cfg[\"grid\"][\"dt\"])\n\t        self.poisson_solver = pushers.PoissonSolver(cfg[\"grid\"][\"one_over_kx\"])\n\t    def __call__(self, t: float, y: Dict, args: Dict):\n\t        \"\"\"\n\t        This function is used by the time integrators specified in diffrax\n", "        :param t:\n\t        :param y:\n\t        :param args:\n\t        :return:\n\t        \"\"\"\n\t        e = self.poisson_solver(\n\t            self.cfg[\"physics\"][\"ion\"][\"charge\"] * y[\"ion\"][\"n\"]\n\t            + self.cfg[\"physics\"][\"electron\"][\"charge\"] * y[\"electron\"][\"n\"]\n\t        )\n\t        ed = 0.0\n", "        for p_ind in self.cfg[\"drivers\"][\"ex\"].keys():\n\t            ed += self.push_driver(args[\"driver\"][\"ex\"][p_ind], t)\n\t        # if \"ey\" in self.cfg[\"drivers\"]:\n\t        #     ad = 0.0\n\t        #     for p_ind in self.cfg[\"drivers\"][\"ey\"].keys():\n\t        #         ad += self.push_driver(args[\"pulse\"][\"ey\"][p_ind], t)\n\t        #     a = self.wave_solver(a, aold, djy_array, charge)\n\t        #     total_a = y[\"a\"] + ad\n\t        #     ponderomotive_force = -0.5 * jnp.gradient(jnp.square(total_a), self.cfg[\"grid\"][\"dx\"])[1:-1]\n\t        total_e = e + ed  # + ponderomotive_force\n", "        dstate_dt = {\"ion\": {}, \"electron\": {}}\n\t        for species_name in [\"ion\", \"electron\"]:\n\t            n = y[species_name][\"n\"]\n\t            u = y[species_name][\"u\"]\n\t            p = y[species_name][\"p\"]\n\t            delta = y[species_name][\"delta\"]\n\t            if self.cfg[\"physics\"][species_name][\"is_on\"]:\n\t                q_over_m = self.cfg[\"physics\"][species_name][\"charge\"] / self.cfg[\"physics\"][species_name][\"mass\"]\n\t                p_over_m = p / self.cfg[\"physics\"][species_name][\"mass\"]\n\t                dstate_dt[species_name][\"n\"] = self.pusher_dict[species_name][\"push_n\"](n, u)\n", "                dstate_dt[species_name][\"u\"] = self.pusher_dict[species_name][\"push_u\"](\n\t                    n, u, p_over_m, q_over_m * total_e, delta\n\t                )\n\t                dstate_dt[species_name][\"p\"] = self.pusher_dict[species_name][\"push_e\"](n, u, p_over_m, q_over_m * e)\n\t            else:\n\t                dstate_dt[species_name][\"n\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n\t                dstate_dt[species_name][\"u\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n\t                dstate_dt[species_name][\"p\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n\t            if self.cfg[\"physics\"][species_name][\"trapping\"][\"is_on\"]:\n\t                dstate_dt[species_name][\"delta\"] = self.pusher_dict[species_name][\"particle_trapper\"](e, delta, args)\n", "            else:\n\t                dstate_dt[species_name][\"delta\"] = jnp.zeros(self.cfg[\"grid\"][\"nx\"])\n\t        return dstate_dt\n\tdef get_save_func(cfg):\n\t    if cfg[\"save\"][\"func\"][\"is_on\"]:\n\t        if cfg[\"save\"][\"x\"][\"is_on\"]:\n\t            dx = (cfg[\"save\"][\"x\"][\"xmax\"] - cfg[\"save\"][\"x\"][\"xmin\"]) / cfg[\"save\"][\"x\"][\"nx\"]\n\t            cfg[\"save\"][\"x\"][\"ax\"] = jnp.linspace(\n\t                cfg[\"save\"][\"x\"][\"xmin\"] + dx / 2.0, cfg[\"save\"][\"x\"][\"xmax\"] - dx / 2.0, cfg[\"save\"][\"x\"][\"nx\"]\n\t            )\n", "            save_x = partial(jnp.interp, cfg[\"save\"][\"x\"][\"ax\"], cfg[\"grid\"][\"x\"])\n\t        if cfg[\"save\"][\"kx\"][\"is_on\"]:\n\t            cfg[\"save\"][\"kx\"][\"ax\"] = jnp.linspace(\n\t                cfg[\"save\"][\"kx\"][\"kxmin\"], cfg[\"save\"][\"kx\"][\"kxmax\"], cfg[\"save\"][\"kx\"][\"nkx\"]\n\t            )\n\t            def save_kx(field):\n\t                complex_field = jnp.fft.rfft(field, axis=0) * 2.0 / cfg[\"grid\"][\"nx\"]\n\t                interped_field = jnp.interp(cfg[\"save\"][\"kx\"][\"ax\"], cfg[\"grid\"][\"kxr\"], complex_field)\n\t                return {\"mag\": jnp.abs(interped_field), \"ang\": jnp.angle(interped_field)}\n\t        def save_func(t, y, args):\n", "            save_dict = {}\n\t            if cfg[\"save\"][\"x\"][\"is_on\"]:\n\t                save_dict[\"x\"] = jtu.tree_map(save_x, y)\n\t            if cfg[\"save\"][\"kx\"][\"is_on\"]:\n\t                save_dict[\"kx\"] = jtu.tree_map(save_kx, y)\n\t            return save_dict\n\t    else:\n\t        cfg[\"save\"][\"x\"][\"ax\"] = cfg[\"grid\"][\"x\"]\n\t        save_func = None\n\t    return save_func\n", "def get_models(model_config: Dict) -> defaultdict[eqx.Module]:\n\t    if model_config:\n\t        model_keys = jax.random.split(jax.random.PRNGKey(420), len(model_config.keys()))\n\t        model_dict = defaultdict(eqx.Module)\n\t        for (term, config), this_key in zip(model_config.items(), model_keys):\n\t            if term == \"file\":\n\t                pass\n\t            else:\n\t                for act in [\"activation\", \"final_activation\"]:\n\t                    if config[act] == \"tanh\":\n", "                        config[act] = jnp.tanh\n\t                model_dict[term] = nn.MLP(**{**config, \"key\": this_key})\n\t        if model_config[\"file\"]:\n\t            model_dict = eqx.tree_deserialise_leaves(model_config[\"file\"], model_dict)\n\t        return model_dict\n\t    else:\n\t        return False\n"]}
{"filename": "theory/__init__.py", "chunked_list": []}
{"filename": "theory/electrostatic.py", "chunked_list": ["# MIT License\n\t#\n\t# Copyright (c) 2022 Ergodic LLC\n\t#\n\t# Permission is hereby granted, free of charge, to any person obtaining a copy\n\t# of this software and associated documentation files (the \"Software\"), to deal\n\t# in the Software without restriction, including without limitation the rights\n\t# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n\t# copies of the Software, and to permit persons to whom the Software is\n\t# furnished to do so, subject to the following conditions:\n", "#\n\t# The above copyright notice and this permission notice shall be included in all\n\t# copies or substantial portions of the Software.\n\t#\n\t# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n\t# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n\t# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n\t# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n\t# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n\t# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n", "# SOFTWARE.\n\tfrom typing import Tuple\n\timport numpy as np\n\timport scipy\n\tfrom scipy import signal\n\tdef get_nlfs(ek, dt):\n\t    \"\"\"\n\t    Calculate the shift in frequency with respect to a reference\n\t    This can be done by subtracting a signal at the reference frequency from the\n\t    given signal\n", "    :param ek:\n\t    :param dt:\n\t    :return:\n\t    \"\"\"\n\t    midpt = int(ek.shape[0] / 2)\n\t    window = 1\n\t    # Calculate hilbert transform\n\t    analytic_signal = signal.hilbert(window * np.real(ek))\n\t    # Determine envelope\n\t    amplitude_envelope = np.abs(analytic_signal)\n", "    # Phase = angle(signal)    ---- needs unwrapping because of periodicity\n\t    instantaneous_phase = np.unwrap(np.angle(analytic_signal))\n\t    # f(t) = dphase/dt\n\t    instantaneous_frequency = np.gradient(instantaneous_phase, dt)  ### Sampling rate!\n\t    # delta_f(t) = f(t) - driver_freq\n\t    # Smooth the answer\n\t    b, a = signal.butter(8, 0.125)\n\t    instantaneous_frequency_smooth = signal.filtfilt(b, a, instantaneous_frequency, padlen=midpt)\n\t    return amplitude_envelope, instantaneous_frequency_smooth\n\tdef plasma_dispersion(value):\n", "    \"\"\"\n\t    This function leverages the Fadeeva function in scipy to calculate the Z function\n\t    :param value:\n\t    :return:\n\t    \"\"\"\n\t    return scipy.special.wofz(value) * np.sqrt(np.pi) * 1j\n\tdef plasma_dispersion_prime(value):\n\t    \"\"\"\n\t    This is a simple relation for Z-prime, which happens to be directly proportional to Z\n\t    :param value:\n", "    :return:\n\t    \"\"\"\n\t    return -2.0 * (1.0 + value * plasma_dispersion(value))\n\tdef get_roots_to_electrostatic_dispersion(wp_e, vth_e, k0, maxwellian_convention_factor=2.0, initial_root_guess=None):\n\t    \"\"\"\n\t    This function calculates the root of the plasma dispersion relation\n\t    :param wp_e:\n\t    :param vth_e:\n\t    :param k0:\n\t    :param maxwellian_convention_factor:\n", "    :param initial_root_guess:\n\t    :return:\n\t    \"\"\"\n\t    from scipy import optimize\n\t    plasma_epsilon, initial_root_guess = get_dispersion_function(\n\t        wp_e, vth_e, k0, maxwellian_convention_factor, initial_root_guess\n\t    )\n\t    epsilon_root = optimize.newton(plasma_epsilon, initial_root_guess)\n\t    return epsilon_root * k0 * vth_e * np.sqrt(maxwellian_convention_factor)\n\tdef get_dispersion_function(wp_e, vth_e, k0, maxwellian_convention_factor=2.0, initial_root_guess=None):\n", "    \"\"\"\n\t    This function calculates the root of the plasma dispersion relation\n\t    :param wp_e:\n\t    :param vth_e:\n\t    :param k0:\n\t    :param maxwellian_convention_factor:\n\t    :param initial_root_guess:\n\t    :return:\n\t    \"\"\"\n\t    if initial_root_guess is None:\n", "        initial_root_guess = np.sqrt(wp_e**2.0 + 3 * (k0 * vth_e) ** 2.0)\n\t    chi_e = np.power((wp_e / (vth_e * k0)), 2.0) / maxwellian_convention_factor\n\t    def plasma_epsilon(x):\n\t        val = 1.0 - chi_e * plasma_dispersion_prime(x)\n\t        return val\n\t    return plasma_epsilon, initial_root_guess\n\tdef calc_depsdw(kld):\n\t    \"\"\"\n\t    Used for frequency shift calculations\n\t    :param kld:\n", "    :return:\n\t    \"\"\"\n\t    depsdw = {}\n\t    wax = np.linspace(1.0, 1.5, 2048)\n\t    # Approximate epsilon\n\t    epsilon_approx = 1 - 1.0 / wax**2.0 - 3 * 1 / wax**2.0 * (kld / wax) ** 2.0\n\t    # Exact\n\t    disp_fn, _ = get_dispersion_function(1.0, 1.0, kld)\n\t    # Make array of epsilons\n\t    disp_arr = np.array([np.real(disp_fn(w / (kld * np.sqrt(2.0)))) for w in wax])\n", "    depsdw[\"exact\"] = np.gradient(disp_arr, wax[2] - wax[1])\n\t    depsdw[\"approx\"] = np.gradient(epsilon_approx, wax[2] - wax[1])\n\t    wr = np.real(\n\t        get_roots_to_electrostatic_dispersion(1.0, 1.0, kld, maxwellian_convention_factor=2.0, initial_root_guess=None)\n\t    )\n\t    iw = np.argmin(np.abs(wax - wr))\n\t    return depsdw[\"exact\"][iw], depsdw[\"approx\"][iw]\n\tdef get_complex_frequency_table(num: int, kinetic_real_epw: bool) -> Tuple[np.array, np.array, np.array]:\n\t    \"\"\"\n\t    This function creates a table of the complex plasma frequency for $0.2 < k \\lambda_D < 0.4$ in `num` steps\n", "    :param kinetic_real_epw:\n\t    :param num:\n\t    :return:\n\t    \"\"\"\n\t    klds = np.linspace(0.02, 0.4, num)\n\t    wrs = np.zeros(num)\n\t    wis = np.zeros(num)\n\t    for i, kld in enumerate(klds):\n\t        ww = get_roots_to_electrostatic_dispersion(1.0, 1.0, kld)\n\t        if kinetic_real_epw:\n", "            wrs[i] = np.real(ww)\n\t        else:\n\t            wrs[i] = np.sqrt(1.0 + 3.0 * kld**2.0)\n\t        wis[i] = np.imag(ww)\n\t    return wrs, wis, klds\n"]}
