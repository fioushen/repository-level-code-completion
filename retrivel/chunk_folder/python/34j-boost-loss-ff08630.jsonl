{"filename": "setup.py", "chunked_list": ["#!/usr/bin/env python\n\t# This is a shim to allow GitHub to detect the package, build is done with poetry\n\t# Taken from https://github.com/Textualize/rich\n\timport setuptools\n\tif __name__ == \"__main__\":\n\t    setuptools.setup(name=\"boost-loss\")\n"]}
{"filename": "tests/test_sklearn.py", "chunked_list": ["from unittest import TestCase\n\tfrom catboost import CatBoostClassifier, CatboostError, CatBoostRegressor\n\tfrom parameterized import parameterized\n\tfrom sklearn.datasets import load_diabetes, load_digits\n\tfrom sklearn.model_selection import train_test_split\n\tfrom sklearn.pipeline import Pipeline\n\tfrom boost_loss.sklearn import patch, patch_catboost\n\tclass TestPatchCatboost(TestCase):\n\t    @parameterized.expand([\"RMSE\", \"RMSEWithUncertainty\"])\n\t    def test_patch_catboost_reg(self, objective: str) -> None:\n", "        estimator = patch_catboost(\n\t            CatBoostRegressor(iterations=100, objective=objective)\n\t        )\n\t        X, y = load_diabetes(return_X_y=True)\n\t        X_train, X_test, y_train, y_test = train_test_split(X, y)\n\t        estimator.fit(X_train, y_train)\n\t        y_pred = estimator.predict(X_test)\n\t        y_pred_var = estimator.predict_var(X_test)\n\t        self.assertEqual(y_test.shape, y_pred.shape)\n\t        self.assertEqual(y_test.shape, y_pred_var.shape)\n", "        self.assertFalse(hasattr(CatBoostRegressor(), \"predict_var\"))\n\t        # assert method properly created for each object\n\t        with self.assertRaises(CatboostError):\n\t            patch_catboost(\n\t                CatBoostRegressor(iterations=100, objective=objective)\n\t            ).predict_var(X_test)\n\t    def test_patch_catboost_clf(self) -> None:\n\t        estimator = patch_catboost(\n\t            CatBoostClassifier(iterations=100, loss_function=\"MultiClass\")\n\t        )\n", "        X, y = load_digits(return_X_y=True)\n\t        X_train, X_test, y_train, y_test = train_test_split(X, y)\n\t        estimator.fit(X_train, y_train)\n\t        y_pred = estimator.predict(X_test)\n\t        y_pred_var = estimator.predict_var(X_test)\n\t        self.assertEqual(y_test.shape, y_pred.shape)\n\t        self.assertEqual(y_test.shape, y_pred_var.shape)\n\t        self.assertFalse(hasattr(CatBoostClassifier(), \"predict_var\"))\n\t        with self.assertRaises(CatboostError):\n\t            patch_catboost(\n", "                CatBoostClassifier(iterations=100, loss_function=\"MultiClass\")\n\t            ).predict_var(X_test)\n\t    def test_patch(self) -> None:\n\t        estimator = patch(\n\t            Pipeline(\n\t                [\n\t                    (\n\t                        \"catboost\",\n\t                        CatBoostClassifier(iterations=100, loss_function=\"MultiClass\"),\n\t                    )\n", "                ]\n\t            )\n\t        )\n\t        X, y = load_digits(return_X_y=True)\n\t        X_train, X_test, y_train, y_test = train_test_split(X, y)\n\t        estimator.fit(X_train, y_train)\n\t        y_pred = estimator.predict(X_test)\n\t        y_pred_var = estimator[-1].predict_var(X_test)\n\t        self.assertEqual(y_test.shape, y_pred.shape)\n\t        self.assertEqual(y_test.shape, y_pred_var.shape)\n", "try:\n\t    from ngboost import NGBRegressor\n\t    from boost_loss.sklearn import patch_ngboost\n\t    class TestPatchNGBoost(TestCase):\n\t        def setUp(self) -> None:\n\t            X, y = load_diabetes(return_X_y=True)\n\t            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n\t                X, y\n\t            )\n\t        def test_patch_ngboost(self) -> None:\n", "            estimator = patch_ngboost(NGBRegressor(n_estimators=100))\n\t            estimator.fit(self.X_train, self.y_train)\n\t            y_pred = estimator.predict(self.X_test)\n\t            y_pred_var = estimator.predict_var(self.X_test)\n\t            self.assertEqual(self.y_test.shape, y_pred.shape)\n\t            self.assertEqual(self.y_test.shape, y_pred_var.shape)\n\t            self.assertFalse(hasattr(NGBRegressor(), \"predict_var\"))\n\t            with self.assertRaises(BaseException):\n\t                patch_ngboost(NGBRegressor(n_estimators=100)).predict_var(self.X_test)\n\texcept ImportError:\n", "    pass\n"]}
{"filename": "tests/test_torch.py", "chunked_list": ["import warnings\n\tfrom unittest import SkipTest, TestCase\n\timport numpy as np\n\tfrom parameterized import parameterized_class\n\tfrom boost_loss.base import LossBase\n\tfrom boost_loss.regression.regression import L2Loss\n\ttry:\n\t    from boost_loss.torch import TorchLossBase, _LNLossTorch, _LNLossTorch_\n\texcept ValueError as e:\n\t    raise SkipTest(f\"Failed to import TorchLossBase from PyTorch. Skip the test. {e}\")\n", "else:\n\t    PARAMETERS = [\n\t        (_LNLossTorch_(n=2, divide_n_loss=False), L2Loss(divide_n_grad=False)),\n\t        (_LNLossTorch(n=2, divide_n_grad=True), L2Loss(divide_n_grad=True)),\n\t    ]\n\tfrom .test_base import assert_array_almost_equal\n\ttry:\n\t    from torch.nn.modules.loss import MSELoss\n\t    PARAMETERS.append(\n\t        (TorchLossBase.from_callable_torch(MSELoss())(), L2Loss(divide_n_grad=False))\n", "    )\n\texcept ValueError as e:\n\t    warnings.warn(f\"Failed to import MSELoss from PyTorch. Skip the test. {e}\")\n\t@parameterized_class((\"loss_torch\", \"loss_base\"), PARAMETERS)\n\tclass TestLossTorch(TestCase):\n\t    loss_torch: TorchLossBase\n\t    loss_base: LossBase\n\t    def setUp(self) -> None:\n\t        self.y_pred = np.random.randn(10)\n\t        self.y_true = np.random.randn(10)\n", "    def test_consistent(self) -> None:\n\t        loss_torch = self.loss_torch\n\t        loss_base = self.loss_base\n\t        assert_array_almost_equal(\n\t            np.mean(loss_torch.loss(self.y_true, self.y_pred)),\n\t            np.mean(loss_base.loss(self.y_true, self.y_pred)),\n\t        )\n\t        assert_array_almost_equal(\n\t            loss_torch.grad_hess(self.y_true, self.y_pred)[0],\n\t            loss_base.grad_hess(self.y_true, self.y_pred)[0],\n", "        )\n\t        assert_array_almost_equal(\n\t            loss_torch.grad_hess(self.y_true, self.y_pred)[1],\n\t            loss_base.grad_hess(self.y_true, self.y_pred)[1],\n\t        )\n"]}
{"filename": "tests/__init__.py", "chunked_list": []}
{"filename": "tests/test_base.py", "chunked_list": ["from __future__ import annotations\n\tfrom functools import lru_cache\n\tfrom unittest import SkipTest, TestCase\n\timport catboost as cb\n\timport lightgbm as lgb\n\timport numpy as np\n\timport xgboost as xgb\n\tfrom numpy.testing import assert_allclose\n\tfrom parameterized import parameterized_class\n\tfrom sklearn.datasets import load_diabetes\n", "from sklearn.model_selection import train_test_split\n\tfrom sklearn.preprocessing import StandardScaler\n\tfrom boost_loss.base import LossBase\n\tfrom boost_loss.regression.regression import L2Loss, LogCoshLoss\n\tfrom boost_loss.sklearn import apply_custom_loss\n\tdef assert_array_almost_equal(a, b):\n\t    try:\n\t        assert_allclose(a, b, rtol=1e-2, atol=1e-7)\n\t    except Exception as e:\n\t        raise AssertionError(f\"{a[:3]}... != {b[:3]}...\") from e\n", "class TestBase(TestCase):\n\t    loss: LossBase\n\t    loss_name: str\n\t    loss_names: dict[str, dict[str, str | None]] = {\n\t        \"catboost\": {\n\t            \"l2\": \"RMSE\",\n\t            \"l1\": \"MAE\",\n\t            \"poisson\": \"Poisson\",\n\t            \"logcosh\": \"LogCosh\",\n\t        },\n", "        \"lightgbm\": {\n\t            \"l2\": \"regression\",\n\t            \"l1\": \"regression_l1\",\n\t            \"poisson\": \"poisson\",\n\t            \"logcosh\": None,\n\t        },\n\t        \"xgboost\": {\n\t            \"l2\": \"reg:squarederror\",\n\t            \"l1\": \"reg:linear\",\n\t            \"poisson\": \"count:poisson\",\n", "            \"logcosh\": None,\n\t        },\n\t    }\n\t    def setUp(self) -> None:\n\t        self.X, self.y = load_diabetes(return_X_y=True)\n\t        self.seed = 0\n\t        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n\t            self.X, self.y, random_state=0\n\t        )\n\t        x_scaler, y_scaler = StandardScaler(), StandardScaler()\n", "        self.X_train = x_scaler.fit_transform(self.X_train)\n\t        self.X_test = x_scaler.transform(self.X_test)\n\t        self.y_train = y_scaler.fit_transform(self.y_train.reshape(-1, 1)).ravel()\n\t        self.y_test = y_scaler.transform(self.y_test.reshape(-1, 1)).ravel()\n\t    def tearDown(self) -> None:\n\t        if hasattr(self, \"y_pred\"):\n\t            score = self.loss.loss(self.y_test, self.y_pred)\n\t            print(f\"Score: {score}\")\n\t    @lru_cache\n\t    def catboost_baseline(self):\n", "        if self.loss_names[\"catboost\"][self.loss_name] is None:\n\t            raise SkipTest(f\"CatBoost does not support {self.loss_name} loss.\")\n\t        model = cb.CatBoostRegressor(\n\t            loss_function=self.loss_names[\"catboost\"][self.loss_name],\n\t            eval_metric=self.loss_names[\"catboost\"][self.loss_name],\n\t            iterations=100,\n\t        )\n\t        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n\t        return model.predict(self.X_test)\n\t    @lru_cache\n", "    def lightgbm_baseline(self):\n\t        if self.loss_names[\"lightgbm\"][self.loss_name] is None:\n\t            raise SkipTest(f\"LightGBM does not support {self.loss_name} loss.\")\n\t        model = lgb.LGBMRegressor(\n\t            objective=self.loss_names[\"lightgbm\"][self.loss_name],\n\t        )\n\t        model.fit(\n\t            self.X_train,\n\t            self.y_train,\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n", "        )\n\t        return model.predict(self.X_test)\n\t    @lru_cache\n\t    def xgboost_baseline(self):\n\t        if self.loss_names[\"xgboost\"][self.loss_name] is None:\n\t            raise SkipTest(f\"XGBoost does not support {self.loss_name} loss.\")\n\t        model = xgb.XGBRegressor(\n\t            objective=self.loss_names[\"xgboost\"][self.loss_name],\n\t        )\n\t        model.fit(\n", "            self.X_train,\n\t            self.y_train,\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n\t        )\n\t        return model.predict(self.X_test)\n\t@parameterized_class(\n\t    (\"loss\", \"loss_name\"),\n\t    [\n\t        # (L1Loss(), \"l1\"),\n\t        (L2Loss(), \"l2\"),\n", "        (LogCoshLoss(), \"logcosh\"),\n\t    ],\n\t)\n\tclass TestBasic(TestBase):\n\t    def test_catboost_sklearn(self):\n\t        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n\t        model = cb.CatBoostRegressor(\n\t            loss_function=self.loss,\n\t            eval_metric=self.loss,\n\t            iterations=100,\n", "            learning_rate=0.1,\n\t        )\n\t        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n\t        self.y_pred = model.predict(self.X_test)\n\t        y_pred_baseline = self.catboost_baseline()\n\t        self.assertAlmostEqual(\n\t            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n\t            / self.loss.eval_metric_xgb_sklearn(self.y_test, y_pred_baseline),\n\t            1,\n\t            places=0,\n", "        )\n\t        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n\t        assert_array_almost_equal(self.y_pred, y_pred_baseline)  # type: ignore\n\t    def test_catboost_native(self):\n\t        model = cb.CatBoostRegressor(\n\t            loss_function=self.loss,\n\t            eval_metric=self.loss,\n\t            iterations=100,\n\t            learning_rate=0.1,\n\t        )\n", "        train_pool = cb.Pool(self.X_train, self.y_train)\n\t        test_pool = cb.Pool(self.X_test, self.y_test)\n\t        model.fit(train_pool, eval_set=test_pool)\n\t        self.y_pred = model.predict(test_pool)\n\t        self.assertAlmostEqual(\n\t            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n\t            / self.loss.eval_metric_xgb_sklearn(self.y_test, self.catboost_baseline()),\n\t            1,\n\t            places=0,\n\t        )\n", "        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n\t        assert_array_almost_equal(self.y_pred, self.catboost_baseline())  # type: ignore\n\t    def test_catboost_sklearn_apply(self):\n\t        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n\t        model = cb.CatBoostRegressor(\n\t            iterations=100,\n\t            learning_rate=0.1,\n\t        )\n\t        model = apply_custom_loss(model, self.loss, target_transformer=None)\n\t        model.fit(self.X_train, self.y_train, eval_set=(self.X_test, self.y_test))\n", "        self.y_pred = model.predict(self.X_test)\n\t        y_pred_baseline = self.catboost_baseline()\n\t        self.assertAlmostEqual(\n\t            self.loss.eval_metric_xgb_sklearn(self.y_test, self.y_pred)\n\t            / self.loss.eval_metric_xgb_sklearn(self.y_test, y_pred_baseline),\n\t            1,\n\t            places=0,\n\t        )\n\t        raise SkipTest(\"Custom RMSE is somewhat not consistent with CatBoost's RMSE\")\n\t        assert_array_almost_equal(self.y_pred, y_pred_baseline)  # type: ignore\n", "    def test_lightgbm_sklearn(self):\n\t        model = lgb.LGBMRegressor(objective=self.loss)\n\t        model.fit(\n\t            self.X_train,\n\t            self.y_train,\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n\t            eval_metric=self.loss.eval_metric_lgb,\n\t        )\n\t        self.y_pred = model.predict(self.X_test)\n\t        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n", "    def test_lightgbm_native(self):\n\t        train_set = lgb.Dataset(self.X_train, self.y_train)\n\t        test_set = lgb.Dataset(self.X_test, self.y_test)\n\t        booster = lgb.train(\n\t            {\"seed\": self.seed},\n\t            train_set=train_set,\n\t            valid_sets=[train_set, test_set],\n\t            fobj=self.loss,\n\t            feval=self.loss.eval_metric_lgb,\n\t        )\n", "        self.y_pred = booster.predict(self.X_test)\n\t        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\t    def test_lightgbm_sklearn_apply(self):\n\t        model = lgb.LGBMRegressor()\n\t        model = apply_custom_loss(model, self.loss, target_transformer=None)\n\t        model.fit(\n\t            self.X_train,\n\t            self.y_train,\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n\t            eval_metric=self.loss.eval_metric_lgb,\n", "        )\n\t        self.y_pred = model.predict(self.X_test)\n\t        assert_array_almost_equal(self.y_pred, self.lightgbm_baseline())\n\t    def test_xgboost_sklearn(self):\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n\t        model = xgb.XGBRegressor(\n\t            objective=self.loss, eval_metric=self.loss.eval_metric_xgb_sklearn\n\t        )\n\t        model.fit(\n", "            self.X_train,\n\t            self.y_train,\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n\t        )\n\t        self.y_pred = model.predict(self.X_test)\n\t        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\t    def test_xgboost_native(self):\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#scikit-learn-interface\n\t        train_set = xgb.DMatrix(self.X_train, self.y_train)\n\t        test_set = xgb.DMatrix(self.X_test, self.y_test)\n", "        booster = xgb.train(\n\t            {\"seed\": self.seed},\n\t            train_set,\n\t            num_boost_round=100,  # it is fucking that default value is different\n\t            evals=[(train_set, \"train\"), (test_set, \"test\")],\n\t            obj=self.loss,\n\t            custom_metric=self.loss.eval_metric_xgb_native,\n\t        )\n\t        self.y_pred = booster.predict(test_set)\n\t        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n", "    def test_xgboost_sklearn_apply(self):\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n\t        model = xgb.XGBRegressor()\n\t        model = apply_custom_loss(model, self.loss, target_transformer=None)\n\t        model.fit(\n\t            self.X_train,\n\t            self.y_train,\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n\t        )\n", "        self.y_pred = model.predict(self.X_test)\n\t        assert_array_almost_equal(self.y_pred, self.xgboost_baseline())\n\t    def test_sklearn_sklearn(self):\n\t        raise SkipTest(\"Not implemented yet\")\n\t@parameterized_class(\n\t    (\"loss\", \"loss_name\"),\n\t    [\n\t        (L2Loss(), \"l2\"),\n\t    ],\n\t)\n", "class TestWeighted(TestBase):\n\t    def setUp(self) -> None:\n\t        super().setUp()\n\t        self.w_train = np.random.rand(len(self.y_train))\n\t        self.w_test = np.random.rand(len(self.y_test))\n\t    def test_catboost_sklearn(self):\n\t        # https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n\t        model = cb.CatBoostRegressor(\n\t            loss_function=self.loss,\n\t            eval_metric=self.loss,\n", "            iterations=100,\n\t            learning_rate=0.1,\n\t        )\n\t        model.fit(\n\t            self.X_train,\n\t            self.y_train,\n\t            sample_weight=self.w_train,\n\t            eval_set=(self.X_test, self.y_test),\n\t        )\n\t        # no api to pass sample_weight to eval_set\n", "        self.y_pred = model.predict(self.X_test)\n\t    def test_catboost_native(self):\n\t        model = cb.CatBoostRegressor(\n\t            loss_function=self.loss,\n\t            eval_metric=self.loss,\n\t            iterations=100,\n\t            learning_rate=0.1,\n\t        )\n\t        train_pool = cb.Pool(self.X_train, self.y_train, weight=self.w_train)\n\t        test_pool = cb.Pool(self.X_test, self.y_test, weight=self.w_test)\n", "        model.fit(train_pool, eval_set=test_pool)\n\t        self.y_pred = model.predict(test_pool)\n\t    def test_lightgbm_sklearn(self):\n\t        model = lgb.LGBMRegressor(objective=self.loss)\n\t        model.fit(\n\t            self.X_train,\n\t            self.y_train,\n\t            sample_weight=self.w_train,\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n\t            eval_metric=self.loss.eval_metric_lgb,\n", "            eval_sample_weight=[self.w_train, self.w_test],\n\t        )\n\t        self.y_pred = model.predict(self.X_test)\n\t    def test_lightgbm_native(self):\n\t        train_set = lgb.Dataset(self.X_train, self.y_train, weight=self.w_train)\n\t        test_set = lgb.Dataset(self.X_test, self.y_test, weight=self.w_test)\n\t        booster = lgb.train(\n\t            {\"seed\": self.seed},\n\t            train_set=train_set,\n\t            valid_sets=[train_set, test_set],\n", "            fobj=self.loss,\n\t            feval=self.loss.eval_metric_lgb,\n\t        )\n\t        self.y_pred = booster.predict(self.X_test)\n\t    def test_xgboost_sklearn(self):\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-objective-function\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#customized-metric-function\n\t        model = xgb.XGBRegressor(\n\t            objective=self.loss, eval_metric=self.loss.eval_metric_xgb_sklearn\n\t        )\n", "        model.fit(\n\t            self.X_train,\n\t            self.y_train,\n\t            sample_weight=self.w_train,\n\t            sample_weight_eval_set=[self.w_train, self.w_test],\n\t            eval_set=[(self.X_train, self.y_train), (self.X_test, self.y_test)],\n\t        )\n\t        self.y_pred = model.predict(self.X_test)\n\t    def test_xgboost_native(self):\n\t        # https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html#scikit-learn-interface\n", "        train_set = xgb.DMatrix(self.X_train, self.y_train, weight=self.w_train)\n\t        test_set = xgb.DMatrix(self.X_test, self.y_test, weight=self.w_test)\n\t        booster = xgb.train(\n\t            {\"seed\": self.seed},\n\t            train_set,\n\t            num_boost_round=100,  # it is fucking that default value is different\n\t            evals=[(train_set, \"train\"), (test_set, \"test\")],\n\t            obj=self.loss,\n\t            custom_metric=self.loss.eval_metric_xgb_native,\n\t        )\n", "        self.y_pred = booster.predict(test_set)\n\t    def test_sklearn_sklearn(self):\n\t        raise SkipTest(\"Not implemented yet\")\n"]}
{"filename": "tests/regression/test_regression.py", "chunked_list": ["from unittest import TestCase\n\timport numpy as np\n\tfrom boost_loss.regression.regression import L1Loss, L2Loss\n\tclass TestRegression(TestCase):\n\t    def test_l2(self):\n\t        l2 = L2Loss()\n\t        y_pred = np.random.rand(10)\n\t        y_true = np.random.rand(10)\n\t        loss = l2.loss(y_true=y_true, y_pred=y_pred)\n\t        grad = l2.grad(y_true=y_true, y_pred=y_pred)\n", "        hess = l2.hess(y_true=y_true, y_pred=y_pred)\n\t        self.assertEqual(grad.tolist(), (y_pred - y_true).tolist())\n\t        self.assertEqual(hess.tolist(), np.ones_like(y_pred).tolist())\n\t        self.assertEqual(loss.tolist(), ((y_pred - y_true) ** 2).tolist())\n\t    def test_l1(self):\n\t        l1 = L1Loss()\n\t        y_pred = np.random.rand(10)\n\t        y_true = np.random.rand(10)\n\t        loss = l1.loss(y_true=y_true, y_pred=y_pred)\n\t        grad = l1.grad(y_true=y_true, y_pred=y_pred)\n", "        hess = l1.hess(y_true=y_true, y_pred=y_pred)\n\t        self.assertEqual(grad.tolist(), np.sign(y_pred - y_true).tolist())\n\t        # NOTE: not np.zeros_like because it does not work\n\t        self.assertEqual(hess.tolist(), np.ones_like(y_pred).tolist())\n\t        self.assertEqual(loss.tolist(), np.abs(y_pred - y_true).tolist())\n"]}
{"filename": "tests/regression/test_sklearn.py", "chunked_list": []}
{"filename": "tests/regression/test_asymmetric.py", "chunked_list": []}
{"filename": "tests/regression/__init__.py", "chunked_list": []}
{"filename": "docs/conf.py", "chunked_list": ["# Configuration file for the Sphinx documentation builder.\n\t#\n\t# This file only contains a selection of the most common options. For a full\n\t# list see the documentation:\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\tfrom pathlib import Path\n\tfrom typing import Any\n\tfrom sphinx.application import Sphinx\n\tfrom sphinx.ext import apidoc\n\t# -- Project information -----------------------------------------------------\n", "# https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information\n\tproject = \"Boost Loss\"\n\tcopyright = \"2023, 34j\"\n\tauthor = \"34j\"\n\trelease = \"0.0.0\"\n\t# -- General configuration ---------------------------------------------------\n\t# https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration\n\t# Add any Sphinx extension module names here, as strings. They can be\n\t# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n\t# ones.\n", "extensions = [\n\t    \"myst_parser\",\n\t    \"sphinx.ext.napoleon\",\n\t    \"sphinx.ext.autodoc\",\n\t    \"sphinx.ext.viewcode\",\n\t]\n\tnapoleon_google_docstring = False\n\t# The suffix of source filenames.\n\tsource_suffix = [\".rst\", \".md\"]\n\t# Add any paths that contain templates here, relative to this directory.\n", "templates_path = [\"_templates\"]\n\t# List of patterns, relative to source directory, that match files and\n\t# directories to ignore when looking for source files.\n\t# This pattern also affects html_static_path and html_extra_path.\n\texclude_patterns: list[str] = []\n\t# -- Options for HTML output -------------------------------------------------\n\t# The theme to use for HTML and HTML Help pages.  See the documentation for\n\t# a list of builtin themes.\n\t#\n\thtml_theme = \"sphinx_rtd_theme\"\n", "# Add any paths that contain custom static files (such as style sheets) here,\n\t# relative to this directory. They are copied after the builtin static files,\n\t# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n\thtml_static_path = [\"_static\"]\n\t# -- Automatically run sphinx-apidoc -----------------------------------------\n\tdef run_apidoc(_: Any) -> None:\n\t    docs_path = Path(__file__).parent\n\t    module_path = docs_path.parent / \"src\" / \"boost_loss\"\n\t    apidoc.main(\n\t        [\n", "            \"--force\",\n\t            \"--module-first\",\n\t            \"-o\",\n\t            docs_path.as_posix(),\n\t            module_path.as_posix(),\n\t        ]\n\t    )\n\tdef setup(app: Sphinx) -> None:\n\t    app.connect(\"builder-inited\", run_apidoc)\n"]}
{"filename": "src/boost_loss/base.py", "chunked_list": ["from __future__ import annotations\n\timport warnings\n\tfrom abc import ABCMeta\n\tfrom logging import getLogger\n\tfrom numbers import Real\n\tfrom typing import Any, Callable, Sequence, final\n\timport attrs\n\timport humps\n\timport lightgbm as lgb\n\timport numpy as np\n", "import xgboost as xgb\n\tfrom numpy.typing import NDArray\n\tfrom typing_extensions import Self\n\tLOG = getLogger(__name__)\n\tdef _dataset_to_ndarray(\n\t    y: NDArray | lgb.Dataset | xgb.DMatrix,\n\t) -> tuple[NDArray, NDArray]:\n\t    if isinstance(y, lgb.Dataset):\n\t        y_ = y.get_label()\n\t        if y_ is None:\n", "            raise ValueError(\"y is None\")\n\t        weight = y.get_weight()\n\t        if weight is None:\n\t            weight = np.ones_like(y_)\n\t        return y_, weight\n\t    if isinstance(y, xgb.DMatrix):\n\t        y_ = y.get_label()\n\t        return y_, np.ones_like(y_)\n\t    return y, np.ones_like(y)\n\tdef _get_name_from_callable(obj: Callable[..., Any]) -> str:\n", "    if hasattr(obj, \"__name__\"):\n\t        return getattr(obj, \"__name__\")\n\t    if hasattr(obj, \"__class__\") and hasattr(getattr(obj, \"__class__\"), \"__name__\"):\n\t        return getattr(getattr(obj, \"__class__\"), \"__name__\")\n\t    raise ValueError(f\"Could not get name from callable {obj}\")\n\tclass LossBase(metaclass=ABCMeta):\n\t    \"\"\"Base class for loss functions.\n\t    Inherit this class to implement custom loss function.\n\t    See Also\n\t    --------\n", "    Catboost:\n\t    https://catboost.ai/en/docs/concepts/python-usages-examples#user-defined-loss-function\n\t    LightGBM:\n\t    https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#custom-objective-function\n\t    XGBoost:\n\t    https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html\n\t    Example\n\t    -------\n\t    >>> from boost_loss.base import LossBase\n\t    >>> import numpy as np\n", "    >>> from numpy.typing import NDArray\n\t    >>>\n\t    >>> class L2Loss(LossBase):\n\t    >>>     def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t    >>>         return (y_true - y_pred) ** 2\n\t    >>>     def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray: # dL/dy_pred\n\t    >>>         return -2 * (y_true - y_pred) # or (y_pred - y_true)\n\t    >>>     def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray: # d2L/dy_pred2\n\t    >>>         return 2 * np.ones_like(y_true) # or np.ones_like(y_true)\n\t    >>>\n", "    >>> from boost.sklearn import apply_custom_loss\n\t    >>> import lightgbm as lgb\n\t    >>> apply_custom_loss(lgb.LGBMRegressor(), L2Loss()).fit(X, y)\n\t    \"\"\"\n\t    is_higher_better: bool = False\n\t    \"\"\"Whether the result of loss function is better when it is higher.\"\"\"\n\t    @classmethod\n\t    @final\n\t    def from_callable(\n\t        cls,\n", "        loss: Callable[[NDArray, NDArray], NDArray | float],\n\t        grad: Callable[[NDArray, NDArray], NDArray],\n\t        hess: Callable[[NDArray, NDArray], NDArray],\n\t        name: str | None = None,\n\t        is_higher_better: bool = False,\n\t    ) -> type[Self]:\n\t        \"\"\"Create this class from loss, grad, and hess callables.\n\t        Parameters\n\t        ----------\n\t        loss : Callable[[NDArray, NDArray], NDArray | float]\n", "            The loss function. If 1-D array is returned,\n\t            the mean of array is calculated.\n\t            Return 1-D array if possible in order to utilize weights in the dataset\n\t            if available.\n\t            (y_true, y_pred) -> loss\n\t        grad : Callable[[NDArray, NDArray], NDArray]\n\t            The 1st order derivative (gradient) of loss w.r.t. y_pred.\n\t            (y_true, y_pred) -> grad\n\t        hess : Callable[[NDArray, NDArray], NDArray]\n\t            The 2nd order derivative (Hessian) of loss w.r.t. y_pred.\n", "            (y_true, y_pred) -> hess\n\t        name : str | None, optional\n\t            The name of loss function.\n\t            If None, it tries to infer from loss function, by default None\n\t        is_higher_better : bool, optional\n\t            Whether the result of loss function is better when it is higher,\n\t            by default False\n\t        Returns\n\t        -------\n\t        type[Self]\n", "            The subclass of this class.\n\t        Raises\n\t        ------\n\t        ValueError\n\t            If name is None and it can't infer from loss function.\n\t        \"\"\"\n\t        if name is None:\n\t            try:\n\t                name = _get_name_from_callable(loss)\n\t            except ValueError as e:\n", "                raise ValueError(\n\t                    \"Could not infer name from loss function. Please specify name.\"\n\t                ) from e\n\t        return type(\n\t            name,\n\t            (cls,),\n\t            dict(\n\t                loss=staticmethod(loss),\n\t                grad=staticmethod(grad),\n\t                hess=staticmethod(hess),\n", "                is_higher_better=is_higher_better,\n\t            ),\n\t        )\n\t    @property\n\t    def _grad_hess_sign(self) -> int:\n\t        return -1 if self.is_higher_better else 1\n\t    def __init_subclass__(cls, **kwargs: Any) -> None:\n\t        grad_inherited = cls.grad is not LossBase.grad\n\t        hess_inherited = cls.hess is not LossBase.hess\n\t        grad_hess_inherited = cls.grad_hess is not LossBase.grad_hess\n", "        if grad_inherited and hess_inherited:\n\t            pass\n\t        elif grad_hess_inherited:\n\t            pass\n\t        else:\n\t            raise TypeError(\n\t                f\"Can't instantiate abstract class {cls.__name__} \"\n\t                \"with grad_hess or both grad and hess not implemented\"\n\t            )\n\t        super().__init_subclass__(**kwargs)\n", "    @property\n\t    def name(self) -> str:\n\t        \"\"\"Name of loss function.\n\t        Returns\n\t        -------\n\t        str\n\t            Snake case of class name. e.g. `LogCoshLoss` -> `log_cosh_loss`.\n\t        \"\"\"\n\t        return humps.decamelize(self.__class__.__name__.replace(\"Loss\", \"\"))\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n", "        \"\"\"The 1st order derivative (gradient) of loss w.r.t. y_pred.\n\t        Parameters\n\t        ----------\n\t        y_true : NDArray\n\t            The true target values.\n\t        y_pred : NDArray\n\t            The predicted target values.\n\t        Returns\n\t        -------\n\t        NDArray\n", "            The gradient of loss function. 1-D array with shape (n_samples,).\n\t        Raises\n\t        ------\n\t        NotImplementedError\n\t            If not implemented.\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        \"\"\"The 2nd order derivative (hessian) of loss w.r.t. y_pred.\n\t        Parameters\n", "        ----------\n\t        y_true : NDArray\n\t            The true target values.\n\t        y_pred : NDArray\n\t            The predicted target values.\n\t        Returns\n\t        -------\n\t        NDArray\n\t            The hessian of loss function. 1-D array with shape (n_samples,).\n\t        Raises\n", "        ------\n\t        NotImplementedError\n\t            If not implemented.\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n\t        \"\"\"Loss function. If 1-D array is returned, the mean of array is calculated.\n\t        Return 1-D array if possible in order to utilize weights in the dataset\n\t        if available.\n\t        Parameters\n", "        ----------\n\t        y_true : NDArray\n\t            The true target values.\n\t        y_pred : NDArray\n\t            The predicted target values.\n\t        Returns\n\t        -------\n\t        NDArray | float\n\t            The loss function. 1-D array with shape (n_samples,) or float.\n\t        Raises\n", "        ------\n\t        NotImplementedError\n\t            If not implemented.\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        \"\"\"Gradient and hessian of loss function. Override this method if you want to\n\t        calculate both gradient and hessian at the same time.\n\t        Parameters\n\t        ----------\n", "        y_true : NDArray\n\t            The true target values.\n\t        y_pred : NDArray\n\t            The predicted target values.\n\t        Returns\n\t        -------\n\t        tuple[NDArray, NDArray]\n\t            The gradient and hessian of loss function.\n\t            1-D array with shape (n_samples,).\n\t        \"\"\"\n", "        return self.grad(y_true=y_true, y_pred=y_pred), self.hess(\n\t            y_true=y_true, y_pred=y_pred\n\t        )\n\t    def _grad_hess_weighted(\n\t        self, y_true: NDArray, y_pred: NDArray, weight: NDArray\n\t    ) -> tuple[NDArray, NDArray]:\n\t        grad, hess = self.grad_hess(y_true=y_true, y_pred=y_pred)\n\t        if np.any(hess < 0):\n\t            negative_rate = np.mean(hess < 0)\n\t            warnings.warn(\n", "                f\"Found negative hessian in {negative_rate:.2%} samples.\"\n\t                \"This may cause convergence issue and cause CatBoostError in CatBoost.\"\n\t                \"If LightGBM or XGBoost is used, the estimator will return \"\n\t                \"nonsense values (like all 0s if 100%).\",\n\t                RuntimeWarning,\n\t            )\n\t        grad, hess = grad * weight, hess * weight\n\t        grad, hess = grad * self._grad_hess_sign, hess * self._grad_hess_sign\n\t        return grad, hess\n\t    @final\n", "    def __call__(\n\t        self,\n\t        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n\t        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n\t    ) -> tuple[NDArray, NDArray]:\n\t        \"\"\"Sklearn-compatible interface (Sklearn, LightGBM, XGBoost)\"\"\"\n\t        if isinstance(y_pred, lgb.Dataset) or isinstance(y_pred, xgb.DMatrix):\n\t            # NOTE: swap (it is so fucking that the order is inconsistent)\n\t            y_true, y_pred = y_pred, y_true\n\t        y_true, weight = _dataset_to_ndarray(y=y_true)\n", "        y_pred, _ = _dataset_to_ndarray(y=y_pred)\n\t        return self._grad_hess_weighted(y_true=y_true, y_pred=y_pred, weight=weight)\n\t    @final\n\t    def calc_ders_range(\n\t        self,\n\t        preds: Sequence[float],\n\t        targets: Sequence[float],\n\t        weights: Sequence[float] | None = None,\n\t    ) -> list[tuple[float, float]]:\n\t        \"\"\"Catboost-compatible interface\"\"\"\n", "        y_pred = np.array(preds)\n\t        y_true = np.array(targets)\n\t        weight = np.array(weights) if weights is not None else np.ones_like(y_pred)\n\t        grad, hess = self._grad_hess_weighted(\n\t            y_true=y_true, y_pred=y_pred, weight=weight\n\t        )\n\t        # NOTE: in catboost, the definition of loss is the inverse\n\t        grad, hess = -grad, -hess\n\t        return list(zip(grad, hess))\n\t    @final\n", "    def is_max_optimal(self) -> bool:\n\t        \"\"\"Catboost-compatible interface\"\"\"\n\t        return self.is_higher_better\n\t    @final\n\t    def evaluate(\n\t        self,\n\t        approxes: Sequence[float],\n\t        target: Sequence[float],\n\t        weight: Sequence[float] | None = None,\n\t    ) -> tuple[float, float]:\n", "        \"\"\"Catboost-compatible interface\"\"\"\n\t        approxes_ = np.array(approxes[0])\n\t        targets_ = np.array(target)\n\t        weights_ = np.array(weight) if weight is not None else np.ones_like(approxes_)\n\t        loss = self.loss(y_true=targets_, y_pred=approxes_)\n\t        if isinstance(loss, float) and not np.allclose(weights_, 1.0):\n\t            warnings.warn(\"loss() should return ndarray when weight is not all 1.0\")\n\t            return loss, np.nan\n\t        return float(np.sum(loss * weights_)), float(np.sum(weights_))\n\t    @final\n", "    def get_final_error(self, error: float, weight: float | None = None) -> float:\n\t        \"\"\"Catboost-compatible interface\"\"\"\n\t        return error / (weight + 1e-38) if weight is not None else error\n\t    @final\n\t    def eval_metric_lgb(\n\t        self,\n\t        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n\t        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n\t    ) -> tuple[str, float, bool]:\n\t        \"\"\"LightGBM-compatible interface\"\"\"\n", "        if isinstance(y_pred, lgb.Dataset) or isinstance(y_pred, xgb.DMatrix):\n\t            # NOTE: swap (it is so fucking that the order is inconsistent)\n\t            y_true, y_pred = y_pred, y_true\n\t        y_true, weight = _dataset_to_ndarray(y=y_true)\n\t        y_pred, _ = _dataset_to_ndarray(y=y_pred)\n\t        loss = self.loss(y_true=y_true, y_pred=y_pred)\n\t        if isinstance(loss, float) and not np.allclose(weight, 1.0):\n\t            warnings.warn(\"loss() should return ndarray when weight is not all 1.0\")\n\t            return self.name, loss, self.is_higher_better\n\t        return (\n", "            self.name,\n\t            float(np.sum(loss * weight) / (np.sum(weight) + 1e-38)),\n\t            self.is_higher_better,\n\t        )\n\t    @final\n\t    def eval_metric_xgb_native(\n\t        self,\n\t        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n\t        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n\t    ) -> tuple[str, float]:\n", "        \"\"\"XGBoost-native-api-compatible interface\"\"\"\n\t        result = self.eval_metric_lgb(y_true=y_true, y_pred=y_pred)\n\t        return result[0], result[1]\n\t    @final\n\t    def eval_metric_xgb_sklearn(\n\t        self,\n\t        y_true: NDArray | lgb.Dataset | xgb.DMatrix,\n\t        y_pred: NDArray | lgb.Dataset | xgb.DMatrix,\n\t    ) -> float:\n\t        \"\"\"XGBoost-sklearn-api-compatible interface\"\"\"\n", "        result = self.eval_metric_lgb(y_true=y_true, y_pred=y_pred)\n\t        return result[1]\n\t    def __add__(self, other: LossBase) -> LossBase:\n\t        if not isinstance(other, LossBase):\n\t            return NotImplemented  # type: ignore\n\t        return _LossSum(self, other)\n\t    def __sub__(self, other: LossBase) -> LossBase:\n\t        return self.__add__(-other)\n\t    def __mul__(self, other: float | int | Real) -> LossBase:\n\t        if not isinstance(other, Real):\n", "            return NotImplemented\n\t        return _LossMul(self, other)\n\t    def __div__(self, other: float | int | Real) -> LossBase:\n\t        return self.__mul__(1.0 / other)\n\t    def __radd__(self, other: LossBase) -> LossBase:\n\t        return self.__add__(other)\n\t    def __rsub__(self, other: LossBase) -> LossBase:\n\t        return self.__sub__(other)\n\t    def __rmul__(self, other: float | int | Real) -> LossBase:\n\t        return self.__mul__(other)\n", "    def __rdiv__(self, other: float | int | Real) -> LossBase:\n\t        return self.__div__(other)\n\t    def __neg__(self) -> LossBase:\n\t        return self.__mul__(-1.0)\n\t    def __pos__(self) -> Self:\n\t        return self\n\t@attrs.define()\n\tclass _LossSum(LossBase):\n\t    loss1: LossBase\n\t    loss2: LossBase\n", "    @property\n\t    def name(self) -> str:\n\t        return self.loss1.name + \"+\" + self.loss2.name\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n\t        return self.loss1.loss(y_true=y_true, y_pred=y_pred) + self.loss2.loss(\n\t            y_true=y_true, y_pred=y_pred\n\t        )\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return self.loss1.grad(y_true=y_true, y_pred=y_pred) + self.loss2.grad(\n\t            y_true=y_true, y_pred=y_pred\n", "        )\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return self.loss1.hess(y_true=y_true, y_pred=y_pred) + self.loss2.hess(\n\t            y_true=y_true, y_pred=y_pred\n\t        )\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        grad1, hess1 = self.loss1.grad_hess(y_true=y_true, y_pred=y_pred)\n\t        grad2, hess2 = self.loss2.grad_hess(y_true=y_true, y_pred=y_pred)\n\t        return grad1 + grad2, hess1 + hess2\n\t@attrs.define()\n", "class _LossMul(LossBase):\n\t    loss_: LossBase\n\t    factor: float | int | Real\n\t    @property\n\t    def name(self) -> str:\n\t        return f\"{self.factor}*{self.loss_.name}\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n\t        return self.factor * self.loss_.loss(y_true=y_true, y_pred=y_pred)\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return self.factor * self.loss_.grad(y_true=y_true, y_pred=y_pred)\n", "    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return self.factor * self.loss_.hess(y_true=y_true, y_pred=y_pred)\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        grad, hess = self.loss_.grad_hess(y_true=y_true, y_pred=y_pred)\n\t        return self.factor * grad, self.factor * hess\n"]}
{"filename": "src/boost_loss/torch.py", "chunked_list": ["from __future__ import annotations\n\tfrom abc import ABCMeta\n\tfrom typing import Any, Callable, final\n\timport attrs\n\timport numpy as np\n\timport torch\n\tfrom numpy.typing import NDArray\n\tfrom typing_extensions import Self\n\tfrom .base import LossBase, _get_name_from_callable\n\tclass TorchLossBase(LossBase, metaclass=ABCMeta):\n", "    \"\"\"Calculate gradient and hessian using `torch.autograd.grad`.\n\t    One of `loss_torch` and `grad_torch` must be implemented.\n\t    Inspired by\n\t    https://github.com/TomerRonen34/treeboost_autograd/blob/main/treeboost_autograd/pytorch_objective.py # noqa: E501\n\t    \"\"\"\n\t    def __init_subclass__(cls, **kwargs: Any) -> None:\n\t        loss_inherited = cls.loss_torch is not TorchLossBase.loss_torch\n\t        grad_inherited = cls.grad_torch is not TorchLossBase.grad_torch\n\t        if loss_inherited or grad_inherited:\n\t            pass\n", "        else:\n\t            raise TypeError(\n\t                f\"Can't instantiate abstract class {cls.__name__} \"\n\t                \"with loss_torch or grad_torch not implemented\"\n\t            )\n\t        return super().__init_subclass__(**kwargs)\n\t    def loss_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t        \"\"\"The loss function.  If 1-D array is returned,\n\t        the mean of array is calculated.\n\t        Return 1-D array if possible in order to utilize weights in the dataset\n", "        if available.\n\t        Parameters\n\t        ----------\n\t        y_true : torch.Tensor\n\t            The true target values.\n\t        y_pred : torch.Tensor\n\t            The predicted target values.\n\t        Returns\n\t        -------\n\t        torch.Tensor\n", "            0-dim or 1-dim tensor of shape (n_samples,). Return 1-dim tensor if possible\n\t            to utilize weights in the dataset if available.\n\t        Raises\n\t        ------\n\t        NotImplementedError\n\t            If not implemented.\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    def grad_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t        \"\"\"The 1st order derivative of loss w.r.t. y_pred.\n", "        Parameters\n\t        ----------\n\t        y_true : torch.Tensor\n\t            The true target values.\n\t        y_pred : torch.Tensor\n\t            The predicted target values.\n\t        Returns\n\t        -------\n\t        torch.Tensor\n\t            The gradient of loss w.r.t. y_pred. 1-dim tensor of shape (n_samples,).\n", "        Raises\n\t        ------\n\t        NotImplementedError\n\t            If not implemented.\n\t        \"\"\"\n\t        raise NotImplementedError()\n\t    @final\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n\t        loss = self.loss_torch(\n\t            torch.from_numpy(y_true), torch.from_numpy(y_pred)\n", "        ).detach()\n\t        if loss.ndim == 0:\n\t            return loss.item()\n\t        else:\n\t            return loss.numpy()\n\t    @final\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        y_true_ = torch.from_numpy(y_true)\n\t        y_pred_ = torch.from_numpy(y_pred).requires_grad_(True)\n\t        try:\n", "            grad = self.grad_torch(y_true_, y_pred_)\n\t        except NotImplementedError:\n\t            loss = torch.mean(self.loss_torch(y_true_, y_pred_)) * y_true_.size(0)\n\t            grad = torch.autograd.grad(loss, y_pred_, create_graph=True)[0]\n\t            hess = np.array(\n\t                [\n\t                    torch.autograd.grad(grad_, y_pred_, retain_graph=True)[0][i].item()\n\t                    for i, grad_ in enumerate(grad)\n\t                ]\n\t            )\n", "        else:\n\t            hess = np.array(\n\t                [\n\t                    torch.autograd.grad(grad_, y_pred_, create_graph=True)[0][i].item()\n\t                    for i, grad_ in enumerate(grad)\n\t                ]\n\t            )\n\t        return grad.detach().numpy(), hess\n\t    @classmethod\n\t    @final\n", "    def from_callable_torch(\n\t        cls,\n\t        loss: Callable[[torch.Tensor, torch.Tensor], torch.Tensor],\n\t        name: str | None = None,\n\t        is_higher_better: bool = False,\n\t    ) -> type[Self]:\n\t        \"\"\"Create a loss class from a callable.\n\t        Parameters\n\t        ----------\n\t        loss : Callable[[torch.Tensor, torch.Tensor], torch.Tensor]\n", "            The loss function.  If 1-D array is returned,\n\t            the mean of array is calculated.\n\t            Return 1-D array if possible in order to utilize weights in the dataset\n\t            if available.\n\t            (y_true, y_pred) -> loss\n\t        name : str | None, optional\n\t            The name of loss function.\n\t            If None, it tries to infer from loss function, by default None\n\t        is_higher_better : bool, optional\n\t            Whether the result of loss function is better when it is higher,\n", "            by default False\n\t        Returns\n\t        -------\n\t        type[Self]\n\t            The subclass of this class.\n\t        Raises\n\t        ------\n\t        ValueError\n\t            If name is None and it can't infer from loss function.\n\t        \"\"\"\n", "        if name is None:\n\t            try:\n\t                name = _get_name_from_callable(loss)\n\t            except ValueError as e:\n\t                raise ValueError(\n\t                    \"Could not infer name from loss function. Please specify name.\"\n\t                ) from e\n\t        return type(\n\t            name,\n\t            (cls,),\n", "            dict(loss_torch=staticmethod(loss), is_higher_better=is_higher_better),\n\t        )\n\t@attrs.define(kw_only=True)\n\tclass _LNLossTorch_(TorchLossBase):\n\t    \"\"\"L^n loss for PyTorch.\"\"\"\n\t    n: float\n\t    divide_n_loss: bool = False\n\t    divide_n_grad: bool = False\n\t    def loss_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t        if self.divide_n_grad != self.divide_n_loss:\n", "            raise ValueError(\n\t                \"divide_n_grad and divide_n_loss must be the same, \"\n\t                f\"but got {self.divide_n_grad} and {self.divide_n_loss}\"\n\t            )\n\t        return torch.abs(y_true - y_pred) ** self.n / (\n\t            self.n if self.divide_n_loss else 1\n\t        )\n\t@attrs.define(kw_only=True)\n\tclass _LNLossTorch(TorchLossBase):\n\t    \"\"\"L^n loss for PyTorch.\"\"\"\n", "    n: float\n\t    divide_n_loss: bool = False\n\t    divide_n_grad: bool = True\n\t    def loss_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t        return torch.abs(y_pred - y_true) ** self.n / (\n\t            self.n if self.divide_n_loss else 1\n\t        )\n\t    def grad_torch(self, y_true: torch.Tensor, y_pred: torch.Tensor) -> torch.Tensor:\n\t        return (\n\t            torch.sign(y_pred - y_true)\n", "            * torch.abs(y_pred - y_true) ** (self.n - 1)\n\t            * self.n\n\t            / (self.n if self.divide_n_grad else 1)\n\t        )\n"]}
{"filename": "src/boost_loss/__init__.py", "chunked_list": ["__version__ = \"0.2.0\"\n\tfrom .base import LossBase\n\tfrom .debug import DebugLoss, PrintLoss\n\tfrom .resuming import ResumingLoss\n\tfrom .sklearn import apply_custom_loss, patch_catboost\n\ttry:\n\t    from .sklearn import patch_ngboost\n\texcept ImportError:\n\t    pass\n\t__all__ = [\n", "    \"LossBase\",\n\t    \"DebugLoss\",\n\t    \"PrintLoss\",\n\t    \"ResumingLoss\",\n\t    \"apply_custom_loss\",\n\t    \"patch_catboost\",\n\t    \"patch_ngboost\",\n\t]\n"]}
{"filename": "src/boost_loss/sklearn.py", "chunked_list": ["from __future__ import annotations\n\timport functools\n\timport importlib.util\n\tfrom typing import Any, Literal, TypeVar, overload\n\timport catboost as cb\n\timport lightgbm as lgb\n\timport numpy as np\n\timport xgboost as xgb\n\tfrom sklearn.base import BaseEstimator, clone\n\tfrom sklearn.compose import TransformedTargetRegressor\n", "from sklearn.preprocessing import StandardScaler\n\tfrom .base import LossBase\n\tTEstimator = TypeVar(\"TEstimator\", cb.CatBoost, lgb.LGBMModel, xgb.XGBModel)\n\t@overload\n\tdef apply_custom_loss(\n\t    estimator: TEstimator,\n\t    loss: LossBase,\n\t    *,\n\t    copy: bool = ...,\n\t    target_transformer: None = ...,\n", "    recursive: bool = ...,\n\t) -> TEstimator:\n\t    ...\n\t@overload\n\tdef apply_custom_loss(\n\t    estimator: TEstimator,\n\t    loss: LossBase,\n\t    *,\n\t    copy: bool = ...,\n\t    target_transformer: BaseEstimator = ...,\n", "    recursive: bool = ...,\n\t) -> TransformedTargetRegressor:\n\t    ...\n\tdef apply_custom_loss(\n\t    estimator: TEstimator,\n\t    loss: LossBase,\n\t    *,\n\t    copy: bool = True,\n\t    target_transformer: BaseEstimator | Any | None = StandardScaler(),\n\t    recursive: bool = True,\n", ") -> TEstimator | TransformedTargetRegressor:\n\t    \"\"\"Apply custom loss to the estimator.\n\t    Parameters\n\t    ----------\n\t    estimator : TEstimator\n\t        CatBoost, LGBMModel, or XGBModel\n\t    loss : LossBase\n\t        The custom loss to apply\n\t    copy : bool, optional\n\t        Whether to copy the estimator using `sklearn.base.clone`, by default True\n", "    target_transformer : BaseEstimator | Any | None, optional\n\t        The target transformer to use, by default StandardScaler()\n\t        (This option exists because some loss functions require the target\n\t        to be normalized (i.e. `LogCoshLoss`))\n\t    recursive : bool, optional\n\t        Whether to recursively search for estimators inside the estimator\n\t        and apply the custom loss to all of them, by default True\n\t    Returns\n\t    -------\n\t    TEstimator | TransformedTargetRegressor\n", "        The estimator with the custom loss applied\n\t    \"\"\"\n\t    if copy:\n\t        estimator = clone(estimator)\n\t    if isinstance(estimator, cb.CatBoost):\n\t        estimator.set_params(loss_function=loss, eval_metric=loss)\n\t    if isinstance(estimator, lgb.LGBMModel):\n\t        estimator.set_params(objective=loss)\n\t        estimator_fit = estimator.fit\n\t        @functools.wraps(estimator_fit)\n", "        def fit(X: Any, y: Any, **fit_params: Any) -> Any:\n\t            fit_params[\"eval_metric\"] = loss.eval_metric_lgb\n\t            return estimator_fit(X, y, **fit_params)\n\t        setattr(estimator, \"fit\", fit)\n\t    if isinstance(estimator, xgb.XGBModel):\n\t        estimator.set_params(objective=loss, eval_metric=loss.eval_metric_xgb_sklearn)\n\t    if recursive:\n\t        for key, value in estimator.get_params(deep=True).items():\n\t            if hasattr(value, \"fit\"):\n\t                estimator.set_params(\n", "                    **{\n\t                        key: apply_custom_loss(\n\t                            value, loss, copy=False, target_transformer=None\n\t                        )\n\t                    }\n\t                )\n\t    if target_transformer is None:\n\t        return estimator\n\t    return TransformedTargetRegressor(estimator, transformer=clone(target_transformer))\n\tif importlib.util.find_spec(\"ngboost\") is not None:\n", "    from ngboost import NGBoost\n\t    from ngboost.distns import Normal\n\t    from numpy.typing import NDArray\n\t    def patch_ngboost(estimator: NGBoost) -> NGBoost:\n\t        \"\"\"Patch NGBoost to return only the mean prediction in `predict`\n\t        and the variance in `predict_var` to be consistent with other models.\n\t        The patch will not apply if the estimator is cloned using `sklearn.base.clone()`\n\t        and requires re-patching.\n\t        Parameters\n\t        ----------\n", "        estimator : NGBoost\n\t            The NGBoost estimator to patch.\n\t        Returns\n\t        -------\n\t        NGBoost\n\t            The patched NGBoost estimator.\n\t        \"\"\"\n\t        self = estimator\n\t        def predict_var(X: Any, **predict_params: Any) -> NDArray[Any]:\n\t            dist = self.pred_dist(X, **predict_params)\n", "            if not isinstance(dist, Normal):\n\t                raise NotImplementedError\n\t            return dist.var\n\t        setattr(estimator, \"predict_var\", predict_var)\n\t        def predict_std(X: Any, **predict_params: Any) -> NDArray[Any]:\n\t            dist = self.pred_dist(X, **predict_params)\n\t            if not isinstance(dist, Normal):\n\t                raise NotImplementedError\n\t            return dist.scale\n\t        setattr(estimator, \"predict_std\", predict_std)\n", "        return estimator\n\tdef patch_catboost(estimator: cb.CatBoost) -> cb.CatBoost:\n\t    \"\"\"Patch CatBoost to return only the mean prediction in `predict`\n\t    and the variance in `predict_var` to be consistent with other models.\n\t    The patch will not apply if the estimator is cloned using `sklearn.base.clone()`\n\t    and requires re-patching.\n\t    Parameters\n\t    ----------\n\t    estimator : cb.CatBoost\n\t        The CatBoost estimator to patch.\n", "    Returns\n\t    -------\n\t    cb.CatBoost\n\t        The patched CatBoost estimator.\n\t    \"\"\"\n\t    original_predict = estimator.predict\n\t    @functools.wraps(original_predict)\n\t    def predict(\n\t        data: Any,\n\t        prediction_type: Literal[\n", "            \"Probability\", \"Class\", \"RawFormulaVal\", \"Exponent\", \"LogProbability\"\n\t        ] = \"RawFormulaVal\",\n\t        ntree_start: int = 0,\n\t        ntree_end: int = 0,\n\t        thread_count: int = -1,\n\t        verbose: bool | None = None,\n\t        task_type: str = \"CPU\",\n\t    ) -> NDArray[Any]:\n\t        prediction = original_predict(\n\t            data,\n", "            prediction_type,\n\t            ntree_start,\n\t            ntree_end,\n\t            thread_count,\n\t            verbose,\n\t            task_type,\n\t        )\n\t        if prediction.ndim == 2:\n\t            return prediction[:, 0]\n\t        return prediction\n", "    setattr(estimator, \"predict\", predict)\n\t    self = estimator\n\t    def predict_var(\n\t        X: Any,\n\t        prediction_type: Literal[\"knowledge\", \"data\", \"total\"] = \"total\",\n\t        **predict_params: Any,\n\t    ) -> NDArray[Any]:\n\t        uncertainty = self.virtual_ensembles_predict(\n\t            X, prediction_type=\"TotalUncertainty\", **predict_params\n\t        )\n", "        loss_function = self.get_params()[\"loss_function\"]\n\t        knowledge_uncertainty = np.zeros_like(uncertainty[..., 0])\n\t        data_uncertainty = np.zeros_like(uncertainty[..., 0])\n\t        if loss_function == \"RMSEWithUncertainty\":\n\t            knowledge_uncertainty = uncertainty[..., 1]\n\t            data_uncertainty = uncertainty[..., 2]\n\t        elif isinstance(estimator, cb.CatBoostClassifier):\n\t            knowledge_uncertainty = uncertainty[..., 1]\n\t        else:\n\t            data_uncertainty = uncertainty[..., 0]\n", "            knowledge_uncertainty = uncertainty[..., 1] - data_uncertainty\n\t        if prediction_type == \"knowledge\":\n\t            return knowledge_uncertainty\n\t        elif prediction_type == \"data\":\n\t            return data_uncertainty\n\t        elif prediction_type == \"total\":\n\t            return knowledge_uncertainty + data_uncertainty\n\t        else:\n\t            raise ValueError(\n\t                \"prediction_type must be one of ['knowledge', 'data', 'total'], \"\n", "                f\"but got {prediction_type}\"\n\t            )\n\t    setattr(estimator, \"predict_var\", predict_var)\n\t    return estimator\n\tTAny = TypeVar(\"TAny\")\n\tdef patch(estimator: TAny, *, copy: bool = True, recursive: bool = True) -> TAny:\n\t    \"\"\"Patch estimator if it is supported. (`patch_ngboost` and `patch_catboost`.)\n\t    The patch will not apply if the estimator is cloned using `sklearn.base.clone()`\n\t    and requires re-patching.\n\t    Parameters\n", "    ----------\n\t    estimator : TAny\n\t        The estimator to patch.\n\t    copy : bool, optional\n\t        Whether to copy the estimator before patching, by default True\n\t    recursive : bool, optional\n\t        Whether to recursively patch the estimator, by default True\n\t    Returns\n\t    -------\n\t    TAny\n", "        The patched estimator.\n\t    \"\"\"\n\t    if copy:\n\t        estimator = clone(estimator)\n\t    if importlib.util.find_spec(\"ngboost\") is not None:\n\t        if isinstance(estimator, NGBoost):\n\t            return patch_ngboost(estimator)\n\t    if isinstance(estimator, cb.CatBoost):\n\t        return patch_catboost(estimator)\n\t    if recursive and hasattr(estimator, \"get_params\"):\n", "        for _, value in estimator.get_params(deep=True).items():\n\t            patch(value, copy=False, recursive=False)\n\t    return estimator\n"]}
{"filename": "src/boost_loss/debug.py", "chunked_list": ["from __future__ import annotations\n\tfrom logging import getLogger\n\timport attrs\n\tfrom numpy.typing import NDArray\n\tfrom .base import LossBase\n\tLOG = getLogger(__name__)\n\t@attrs.define()\n\tclass DebugLoss(LossBase):\n\t    \"\"\"Calls LOG.debug() every time loss() or grad_hess() is called.\"\"\"\n\t    loss_: LossBase\n", "    def loss(self, y_true: NDArray, y_pred: NDArray) -> float | NDArray:\n\t        loss = self.loss_.loss(y_true=y_true, y_pred=y_pred)\n\t        LOG.debug(f\"y_true: {y_true}, y_pred: {y_pred}, loss: {loss}\")\n\t        return loss\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        grad, hess = self.loss_.grad_hess(y_true=y_true, y_pred=y_pred)\n\t        LOG.debug(f\"y_true: {y_true}, y_pred: {y_pred}, grad: {grad}, hess: {hess}\")\n\t        return grad, hess\n\t@attrs.define()\n\tclass PrintLoss(LossBase):\n", "    \"\"\"Prints every time loss() or grad_hess() is called.\"\"\"\n\t    loss_: LossBase\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> float | NDArray:\n\t        loss = self.loss_.loss(y_true=y_true, y_pred=y_pred)\n\t        print(f\"y_true: {y_true}, y_pred: {y_pred}, loss: {loss}\")\n\t        return loss\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        grad, hess = self.loss_.grad_hess(y_true=y_true, y_pred=y_pred)\n\t        print(f\"y_true: {y_true}, y_pred: {y_pred}, grad: {grad}, hess: {hess}\")\n\t        return grad, hess\n"]}
{"filename": "src/boost_loss/resuming.py", "chunked_list": ["from __future__ import annotations\n\tfrom typing import Sequence\n\timport numpy as np\n\tfrom numpy.typing import NDArray\n\tfrom .base import LossBase\n\tclass ResumingLoss(LossBase):\n\t    def __init__(\n\t        self,\n\t        losses: Sequence[LossBase],\n\t        *,\n", "        weights: Sequence[float] | None = None,\n\t        interval: int = 1,\n\t        random_state: int | None = None,\n\t    ) -> None:\n\t        self.losses = losses\n\t        if weights is None:\n\t            self.weights = np.ones_like(losses)\n\t        else:\n\t            self.weights = np.array(weights)\n\t        self.interval = interval\n", "        self.random_state = random_state\n\t        if self.random_state is None:\n\t            if weights is not None:\n\t                raise ValueError(\"weights must be None when random_state is None\")\n\t        else:\n\t            self.random = np.random.RandomState(self.random_state)\n\t        self._count = 0\n\t        self._idx = 0\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        if self._count % self.interval == 0:\n", "            if self.random_state is None:\n\t                self._idx = self.random.choice(len(self.losses), p=self.weights)\n\t            else:\n\t                self._idx = (self._count // self.interval) % len(self.losses)\n\t        self._count += 1\n\t        return self.losses[self._idx].grad_hess(y_true=y_true, y_pred=y_pred)\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray | float:\n\t        return self.losses[self._idx].loss(y_true=y_true, y_pred=y_pred)\n"]}
{"filename": "src/boost_loss/regression/regression.py", "chunked_list": ["from __future__ import annotations\n\tfrom logging import getLogger\n\timport attrs\n\timport numpy as np\n\tfrom numpy.typing import NDArray\n\tfrom ..base import LossBase\n\tLOG = getLogger(__name__)\n\t# cannot freeze due to FrozenInstanceError in catboost\n\t@attrs.define()\n\tclass LNLoss(LossBase):\n", "    \"\"\"LNLoss = |y_true - y_pred|^n\n\t    - x < 1 is not recommended because the loss is not convex.\n\t    - x >> 2 is not recommended because the gradient is too steep.\"\"\"\n\t    n: float\n\t    \"\"\"The exponent of the loss.\"\"\"\n\t    divide_n_loss: bool = False\n\t    \"\"\"Whether to divide the loss by n. Generally False is used.\"\"\"\n\t    divide_n_grad: bool = True\n\t    \"\"\"Whether to divide the gradient by n. Generally True is used.\"\"\"\n\t    @property\n", "    def name(self) -> str:\n\t        return f\"l{self.n:.2f}\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.abs(y_pred - y_true) ** self.n / (self.n if self.divide_n_loss else 1)\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        y_diff = y_pred - y_true\n\t        return (\n\t            np.abs(y_diff) ** (self.n - 1)\n\t            * np.sign(y_diff)\n\t            * self.n\n", "            / (self.n if self.divide_n_grad else 1)\n\t        )\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        y_diff = y_pred - y_true\n\t        return (\n\t            (self.n - 1)\n\t            * np.abs(y_diff) ** (self.n - 2)\n\t            * self.n\n\t            / (self.n if self.divide_n_grad else 1)\n\t        )\n", "class L1Loss(LNLoss):\n\t    \"\"\"L1 loss = |y_true - y_pred|.\"\"\"\n\t    def __init__(\n\t        self, *, divide_n_loss: bool = False, divide_n_grad: bool = True\n\t    ) -> None:\n\t        \"\"\"L1 loss.\n\t        Parameters\n\t        ----------\n\t        divide_n_loss : bool, optional\n\t            Whether to divide the loss by n, by default False\n", "        divide_n_grad : bool, optional\n\t            Whether to divide the gradient by n, by default True\n\t        \"\"\"\n\t        super().__init__(n=1, divide_n_loss=divide_n_loss, divide_n_grad=divide_n_grad)\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.ones_like(y_pred - y_true)  # zero hess is not allowed\n\tclass L2Loss(LNLoss):\n\t    \"\"\"L2 loss = |y_true - y_pred|^2\"\"\"\n\t    def __init__(\n\t        self, *, divide_n_loss: bool = False, divide_n_grad: bool = True\n", "    ) -> None:\n\t        \"\"\"L2 loss.\n\t        Parameters\n\t        ----------\n\t        divide_n_loss : bool, optional\n\t            Whether to divide the loss by n, by default False\n\t        divide_n_grad : bool, optional\n\t            Whether to divide the gradient by n, by default True\n\t        \"\"\"\n\t        super().__init__(n=2, divide_n_loss=divide_n_loss, divide_n_grad=divide_n_grad)\n", "class LogCoshLoss(LossBase):\n\t    \"\"\"LogCosh loss = log(cosh(y_true - y_pred))\"\"\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.log(np.cosh(y_pred - y_true))\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.tanh(y_pred - y_true)\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.cosh(y_pred - y_true) ** -2\n\tclass HuberLoss(LossBase):\n\t    \"\"\"Huber loss = 0.5 (y_true - y_pred)^2 if |y_true - y_pred| <= delta\n", "    else delta * (|y_true - y_pred| - 0.5 * delta)\"\"\"\n\t    def __init__(self, delta: float = 1.0) -> None:\n\t        \"\"\"Huber loss.\n\t        Parameters\n\t        ----------\n\t        delta : float, optional\n\t            The parameter delta, by default 1.0\n\t        \"\"\"\n\t        self.delta = delta\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n", "        y_diff = y_pred - y_true\n\t        return np.where(\n\t            np.abs(y_diff) <= self.delta,\n\t            0.5 * y_diff**2,\n\t            self.delta * (np.abs(y_diff) - 0.5 * self.delta),\n\t        )\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        y_diff = y_pred - y_true\n\t        return np.where(\n\t            np.abs(y_diff) <= self.delta,\n", "            y_diff,\n\t            np.sign(y_diff) * self.delta,\n\t        )\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.ones_like(y_pred - y_true)  # zero hess is not allowed\n\t        # return np.where(\n\t        #     np.abs(y_pred - y_true) <= self.delta,\n\t        #     np.ones_like(y_true),\n\t        #     np.zeros_like(y_true),\n\t        # )\n", "class FairLoss(LossBase):\n\t    \"\"\"Fair loss = c^2/2 * (abs(y_true - y_pred) -\n\t    c * log(1 + abs(y_true - y_pred)/c))\"\"\"\n\t    def __init__(self, c: float = 1.0) -> None:\n\t        \"\"\"Fair loss.\n\t        Parameters\n\t        ----------\n\t        c : float, optional\n\t            The parameter c, by default 1.0\n\t        \"\"\"\n", "        self.c = c\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return (\n\t            self.c**2\n\t            / 2\n\t            * (\n\t                np.abs(y_true - y_pred)\n\t                - self.c * np.log(1 + np.abs(y_true - y_pred) / self.c)\n\t            )\n\t        )\n", "    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return (\n\t            self.c**2\n\t            / 2\n\t            * (np.sign(y_pred - y_true) - self.c / (self.c + np.abs(y_pred - y_true)))\n\t        )\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return (\n\t            self.c**2\n\t            / 2\n", "            * (np.zeros_like(y_true) + self.c / (self.c + np.abs(y_pred - y_true)) ** 2)\n\t        )\n\tclass PoissonLoss(LossBase):\n\t    \"\"\"Poisson loss = y_pred - y_true * log(y_pred)\"\"\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return y_pred - y_true * np.log(y_pred)\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return 1 - y_true / y_pred\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return y_true / y_pred**2\n", "class LogLoss(LossBase):\n\t    \"\"\"Log loss = log(1 + exp(-y_true * y_pred))\"\"\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.log(1 + np.exp(-y_true * y_pred))\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return -y_true / (1 + np.exp(y_true * y_pred))\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return (\n\t            y_true**2 * np.exp(y_true * y_pred) / (1 + np.exp(y_true * y_pred)) ** 2\n\t        )\n", "class MSLELoss(LossBase):\n\t    \"\"\"Mean squared logarithmic error loss = (log(1 + y_true) - log(1 + y_pred))^2\"\"\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.square(np.log1p(y_true) - np.log1p(y_pred))\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return -2 * (np.log1p(y_true) - np.log1p(y_pred)) / (1 + y_pred)\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return 2 * (np.log1p(y_true) - np.log1p(y_pred)) / (1 + y_pred) ** 2\n\tclass MAPELoss(LossBase):\n\t    \"\"\"Mean absolute percentage error loss = abs(y_true - y_pred) / y_true\"\"\"\n", "    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.abs(y_true - y_pred) / y_true\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.sign(y_pred - y_true) / y_true\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.zeros_like(y_true)\n\tclass SMAPELoss(LossBase):\n\t    \"\"\"Symmetric mean absolute percentage error loss =\n\t    abs(y_true - y_pred) / ((abs(y_true) + abs(y_pred))/2)\"\"\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n", "        return np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.sign(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2)\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return np.zeros_like(y_true)\n\tclass TweedieLoss(LossBase):\n\t    \"\"\"Tweedie loss = -y_true * y_pred^(2-p) / (2-p) + y_pred^(1-p) / (1-p)\"\"\"\n\t    def __init__(self, p: float = 1.5) -> None:\n\t        \"\"\"Tweedie loss.\n\t        Parameters\n", "        ----------\n\t        p : float, optional\n\t            The parameter p, by default 1.5\n\t        \"\"\"\n\t        self.p = p\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return -y_true * y_pred ** (2 - self.p) / (2 - self.p) + y_pred ** (\n\t            1 - self.p\n\t        ) / (1 - self.p)\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n", "        return -y_true * (2 - self.p) * y_pred ** (1 - self.p) + y_pred ** (-self.p)\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return y_true * (2 - self.p) * (1 - self.p) * y_pred ** (\n\t            -self.p - 1\n\t        ) - self.p * y_pred ** (-self.p - 1)\n\tclass GammaLoss(LossBase):\n\t    \"\"\"Gamma loss = y_true / y_pred - log(y_true / y_pred) - 1\"\"\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return y_true / y_pred - np.log(y_true / y_pred) - 1\n\t    def grad(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n", "        return -y_true / y_pred**2 + 1 / y_pred\n\t    def hess(self, y_true: NDArray, y_pred: NDArray) -> NDArray:\n\t        return 2 * y_true / y_pred**3 - 1 / y_pred**2\n"]}
{"filename": "src/boost_loss/regression/__init__.py", "chunked_list": ["from .asymmetric import (\n\t    AsymmetricCompositeLoss,\n\t    AsymmetricLoss,\n\t    ExpectileLoss,\n\t    QuantileLoss,\n\t)\n\tfrom .regression import (\n\t    FairLoss,\n\t    GammaLoss,\n\t    HuberLoss,\n", "    L1Loss,\n\t    L2Loss,\n\t    LNLoss,\n\t    LogCoshLoss,\n\t    LogLoss,\n\t    MAPELoss,\n\t    MSLELoss,\n\t    PoissonLoss,\n\t    SMAPELoss,\n\t    TweedieLoss,\n", ")\n\tfrom .sklearn import VarianceEstimator\n\t__all__ = [\n\t    \"AsymmetricLoss\",\n\t    \"AsymmetricCompositeLoss\",\n\t    \"QuantileLoss\",\n\t    \"ExpectileLoss\",\n\t    \"L1Loss\",\n\t    \"L2Loss\",\n\t    \"LNLoss\",\n", "    \"LogCoshLoss\",\n\t    \"HuberLoss\",\n\t    \"FairLoss\",\n\t    \"PoissonLoss\",\n\t    \"TweedieLoss\",\n\t    \"GammaLoss\",\n\t    \"LogLoss\",\n\t    \"MAPELoss\",\n\t    \"MSLELoss\",\n\t    \"SMAPELoss\",\n", "    \"VarianceEstimator\",\n\t]\n"]}
{"filename": "src/boost_loss/regression/asymmetric.py", "chunked_list": ["from __future__ import annotations\n\timport attrs\n\timport numpy as np\n\tfrom numpy.typing import NDArray\n\tfrom ..base import LossBase\n\tfrom .regression import L1Loss, L2Loss\n\t@attrs.define()\n\tclass AsymmetricCompositeLoss(LossBase):\n\t    \"\"\"Asymmetric composite loss function.\n\t    The loss function is `loss_pred_less` if `y_true < y_pred`,\n", "    otherwise `loss_pred_greater`.\n\t    \"\"\"\n\t    loss_pred_less: LossBase\n\t    \"\"\"The loss function if `y_true < y_pred`.\"\"\"\n\t    loss_pred_greater: LossBase\n\t    \"\"\"The loss function if `y_true >= y_pred`.\"\"\"\n\t    def loss(self, y_true: NDArray, y_pred: NDArray) -> float | NDArray:\n\t        loss = np.where(\n\t            y_true < y_pred,\n\t            self.loss_pred_less.loss(y_true=y_true, y_pred=y_pred),\n", "            self.loss_pred_greater.loss(y_true=y_true, y_pred=y_pred),\n\t        )\n\t        return loss\n\t    def grad_hess(self, y_true: NDArray, y_pred: NDArray) -> tuple[NDArray, NDArray]:\n\t        grad, hess = np.where(\n\t            y_true < y_pred,\n\t            self.loss_pred_less.grad_hess(y_true=y_true, y_pred=y_pred),\n\t            self.loss_pred_greater.grad_hess(y_true=y_true, y_pred=y_pred),\n\t        )\n\t        return grad, hess\n", "class AsymmetricLoss(AsymmetricCompositeLoss):\n\t    \"\"\"Asymmetric loss function.\n\t    The loss function is `loss * (1 - t)` if `y_true < y_pred`, otherwise `loss * t`.\n\t    Generalized from quantile loss (pinball loss, check loss, etc.) and expectile loss.\n\t    \"\"\"\n\t    def __init__(self, loss: LossBase, t: float = 0.5) -> None:\n\t        super().__init__(loss * (1 - t), loss * t)\n\tclass QuantileLoss(AsymmetricLoss):\n\t    \"\"\"[Quantile](https://en.wikipedia.org/wiki/Quantile) loss function.\"\"\"\n\t    def __init__(self, t: float = 0.5) -> None:\n", "        super().__init__(L1Loss(), t)\n\tclass ExpectileLoss(AsymmetricLoss):\n\t    r\"\"\"[Expectile](https://sites.google.com/site/csphilipps/expectiles) loss function.\n\t    - Expectile is a conditional mean if observations in [μ_τ, ∞)\n\t    are τ/(1 - τ) times more likely than the original distribution.\n\t    - Expectiles are not always quantiles of F.\n\t    - Expectile loss is more smooth than quantile loss.\n\t    .. math::\n\t        \\tau \\int_{-\\infty}^{\\mu_\\tau} (y - \\mu_\\tau) dF(y)\n\t        = (1 - \\tau) \\int_{\\mu_\\tau}^{\\infty} (y - \\mu_\\tau) dF(y)\"\"\"\n", "    def __init__(self, t: float = 0.5) -> None:\n\t        super().__init__(L2Loss(), t)\n"]}
{"filename": "src/boost_loss/regression/sklearn.py", "chunked_list": ["from __future__ import annotations\n\tfrom copy import copy\n\tfrom typing import Any, Literal, Sequence\n\timport numpy as np\n\tfrom joblib import Parallel, delayed\n\tfrom numpy.typing import NDArray\n\tfrom sklearn.base import BaseEstimator\n\tfrom typing_extensions import Self\n\tfrom ..base import LossBase\n\tfrom ..sklearn import apply_custom_loss\n", "from .asymmetric import AsymmetricLoss\n\tdef _recursively_set_random_state(estimator: BaseEstimator, random_state: int) -> None:\n\t    if hasattr(estimator, \"random_state\") and hasattr(estimator, \"set_params\"):\n\t        estimator.set_params(random_state=random_state)\n\t    for _, v in copy(estimator.get_params(deep=False)).items():\n\t        if hasattr(v, \"get_params\"):\n\t            _recursively_set_random_state(v, random_state)\n\tclass VarianceEstimator(BaseEstimator):\n\t    \"\"\"Estimator that estimates the distribution by simply using multiple estimators\n\t    with different `t`.\n", "    Compared to [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/) or\n\t    [CatBoost's Uncertainty](https://catboost.ai/en/docs/references/uncertainty),\n\t    this estimator is much slower and does not support \"natural gradient\",\n\t    but does not require any assumption on the distribution.\n\t    Note that NGBoost supports\n\t    [any user-defineddistribution](https://stanfordmlgroup.github.io/ngboost/5-dev.html) # noqa\n\t    but it has to be defined beforehand.\n\t    NGBoost requires mean estimator and log standard deviation estimator\n\t    to be trained simultaneously, which is very difficult to implement\n\t    in sklearn / lightgbm / xgboost. (Need to start and stop fitting per iteration)\n", "    Consider change `Base` parameter in NGBoost.\n\t    (See https://github.com/stanfordmlgroup/ngboost/issues/250)\n\t    \"\"\"\n\t    ts_: Sequence[float]\n\t    m_type: Literal[\"mean\", \"median\"]\n\t    var_type: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"]\n\t    def __init__(\n\t        self,\n\t        estimator: Any,\n\t        loss: LossBase,\n", "        *,\n\t        ts: int | Sequence[float],\n\t        n_jobs: int | None = 1,\n\t        verbose: int = 0,\n\t        random_state: int | None = None,\n\t        m_type: Literal[\"mean\", \"median\"] = \"median\",\n\t        var_type: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"] = \"var\",\n\t        target_transformer: BaseEstimator | Any | None = None,\n\t    ) -> None:\n\t        \"\"\"Estimator that estimates the distribution by simply using multiple estimators\n", "        with different `t`.\n\t        Compared to [NGBoost](https://stanfordmlgroup.github.io/projects/ngboost/) or\n\t        [CatBoost's Uncertainty](https://catboost.ai/en/docs/references/uncertainty),\n\t        this estimator is much slower and does not support \"natural gradient\",\n\t        but does not require any assumption on the distribution.\n\t        Note that NGBoost supports\n\t        [any user-defineddistribution](https://stanfordmlgroup.github.io/ngboost/5-dev.html) # noqa\n\t        but it has to be defined beforehand.\n\t        NGBoost requires mean estimator and log standard deviation estimator\n\t        to be trained simultaneously, which is very difficult to implement\n", "        in sklearn / lightgbm / xgboost. (Need to start and stop fitting per iteration)\n\t        Consider change `Base` parameter in NGBoost.\n\t        (See https://github.com/stanfordmlgroup/ngboost/issues/250)\n\t        Parameters\n\t        ----------\n\t        estimator : Any\n\t            The base estimator to use for fitting the data.\n\t        loss : LossBase\n\t            The loss function to use for fitting the data.\n\t            Generally, `loss` should not be `AsymmetricLoss`.\n", "        ts : int | Sequence[float]\n\t            The list of `t` to use for fitting the data or the number of `t` to use.\n\t            If `ts` is an integer, `np.linspace(1 / (ts * 2), 1 - 1 / (ts * 2), ts)` is used.\n\t        n_jobs : int | None, optional\n\t            The number of jobs to run in parallel for `fit`. `None` means 1.\n\t        verbose : int, optional\n\t            The verbosity level.\n\t        random_state : int | None, optional\n\t            The random state to use for fitting the data. If `None`, the random state is not set.\n\t            If not `None`, new random state generated from `random_state` is set to each estimator.\n", "        m_type : Literal[&quot;mean&quot;, &quot;median&quot;], optional\n\t            M-statistics type to return from `predict` by default, by default \"median\"\n\t        var_type : Literal[&quot;var&quot;, &quot;std&quot;, &quot;range&quot;, &quot;mae&quot;, &quot;mse&quot;], optional\n\t            Variance type to return from `predict` by default, by default \"var\"\n\t        target_transformer : BaseEstimator | Any | None, optional\n\t            The transformer to use for transforming the target, by default None\n\t            If `None`, no `TransformedTargetRegressor` is used.\n\t        Raises\n\t        ------\n\t        TypeError\n", "            Raises if `estimator` does not have `fit` method or `predict` method.\n\t        \"\"\"\n\t        if not hasattr(estimator, \"fit\"):\n\t            raise TypeError(f\"{estimator} does not have fit method\")\n\t        if not hasattr(estimator, \"predict\"):\n\t            raise TypeError(f\"{estimator} does not have predict method\")\n\t        self.estimator = estimator\n\t        self.loss = loss\n\t        self.ts = ts\n\t        self.n_jobs = n_jobs\n", "        self.verbose = verbose\n\t        self.random_state = random_state\n\t        self.m_type = m_type\n\t        self.var_type = var_type\n\t        self.target_transformer = target_transformer\n\t        self.random = np.random.RandomState(random_state)\n\t    def fit(self, X: Any, y: Any, **fit_params: Any) -> Self:\n\t        \"\"\"Fit each estimator with different `t`.\n\t        Parameters\n\t        ----------\n", "        X : Any\n\t            The training input samples.\n\t        y : Any\n\t            The target values.\n\t        Returns\n\t        -------\n\t        Self\n\t            Fitted estimator.\n\t        Raises\n\t        ------\n", "        RuntimeError\n\t            Raises if joblib fails to return the results.\n\t        \"\"\"\n\t        ts = self.ts\n\t        if isinstance(ts, int):\n\t            ts = np.linspace(1 / (ts * 2), 1 - 1 / (ts * 2), ts)\n\t        self.ts_ = ts  # type: ignore\n\t        estimators_ = [\n\t            apply_custom_loss(\n\t                self.estimator,\n", "                AsymmetricLoss(self.loss, t=t),\n\t                target_transformer=self.target_transformer,\n\t            )\n\t            for t in self.ts_\n\t        ]\n\t        if self.random is not None:\n\t            for estimator in estimators_:\n\t                _recursively_set_random_state(\n\t                    estimator, self.random.randint(0, np.iinfo(np.int32).max)\n\t                )\n", "        parallel_result = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(\n\t            [delayed(estimator.fit)(X, y, **fit_params) for estimator in estimators_]\n\t        )\n\t        if parallel_result is None:\n\t            raise RuntimeError(\"joblib.Parallel returned None\")\n\t        self.estimators_ = parallel_result\n\t        return self\n\t    def predict_raw(self, X: Any, **predict_params: Any) -> NDArray[Any]:\n\t        \"\"\"Returns raw predictions of each estimator.\n\t        Parameters\n", "        ----------\n\t        X : Any\n\t            X\n\t        **predict_params : Any\n\t            The parameters to be passed to `predict` method of each estimator.\n\t        Returns\n\t        -------\n\t        NDArray[Any]\n\t            Raw predictions of each estimator with shape (n_estimators, n_samples)\n\t        \"\"\"\n", "        return np.array(\n\t            [estimator.predict(X, **predict_params) for estimator in self.estimators_]\n\t        )\n\t    def predict(\n\t        self,\n\t        X: Any,\n\t        type_: Literal[\"mean\", \"median\", \"var\", \"std\", \"range\", \"mae\", \"mse\"]\n\t        | None = None,\n\t        **predict_params: Any,\n\t    ) -> NDArray[Any]:\n", "        \"\"\"Returns predictions of the ensemble.\n\t        Parameters\n\t        ----------\n\t        X : Any\n\t            X\n\t        type_ : Literal['mean', 'median', 'var', 'std', 'range', 'mae', 'mse'], optional\n\t            Type of the prediction, by default None\n\t            If None, self.m_type is used.\n\t        **predict_params : Any\n\t            The parameters to be passed to `predict` method of each estimator.\n", "        Returns\n\t        -------\n\t        NDArray[Any]\n\t            Predictions of the ensemble with shape (n_samples,)\n\t        Raises\n\t        ------\n\t        ValueError\n\t            When type_ is not supported.\n\t        \"\"\"\n\t        type_ = type_ or self.m_type\n", "        if type_ == \"mean\":\n\t            return self.predict_raw(X, **predict_params).mean(axis=0)\n\t        elif type_ == \"median\":\n\t            return np.median(self.predict_raw(X, **predict_params), axis=0)\n\t        elif type_ == \"var\":\n\t            return self.predict_raw(X, **predict_params).var(axis=0)\n\t        elif type_ == \"std\":\n\t            return self.predict_raw(X, **predict_params).std(axis=0)\n\t        elif type_ == \"range\":\n\t            return self.predict_raw(X, **predict_params).max(axis=0) - self.predict_raw(\n", "                X, **predict_params\n\t            ).min(axis=0)\n\t        elif type_ == \"mae\":\n\t            return np.abs(\n\t                self.predict_raw(X, **predict_params)\n\t                - self.predict_raw(X, **predict_params).mean(axis=0)\n\t            ).mean(axis=0)\n\t        elif type_ == \"mse\":\n\t            return (\n\t                (\n", "                    self.predict_raw(X, **predict_params)\n\t                    - self.predict_raw(X, **predict_params).mean(axis=0)\n\t                )\n\t                ** 2\n\t            ).mean(axis=0)\n\t        else:\n\t            raise ValueError(f\"Unknown type_: {type_}\")\n\t    def predict_var(\n\t        self,\n\t        X: Any,\n", "        type_: Literal[\"var\", \"std\", \"range\", \"mae\", \"mse\"] | None = None,\n\t        **predict_params: Any,\n\t    ) -> NDArray[Any]:\n\t        \"\"\"Returns variance of the ensemble.\n\t        Parameters\n\t        ----------\n\t        X : Any\n\t            X\n\t        type_ : Literal['var', 'std', 'range', 'mae', 'mse'], optional\n\t            Type of the variance, by default None\n", "            If None, self.var_type is used.\n\t        **predict_params : Any\n\t            The parameters to be passed to `predict` method of each estimator.\n\t        Returns\n\t        -------\n\t        NDArray[Any]\n\t            Variance of the ensemble with shape (n_samples,)\n\t        \"\"\"\n\t        type_ = type_ or self.var_type\n\t        return self.predict(X, type_=type_, **predict_params)\n"]}
