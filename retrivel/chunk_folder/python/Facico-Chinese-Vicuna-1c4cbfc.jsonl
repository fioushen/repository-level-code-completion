{"filename": "finetune.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\timport torch.nn as nn\n\timport bitsandbytes as bnb\n\tfrom datasets import load_dataset\n\timport transformers\n\timport argparse\n\timport warnings\n\tfrom huggingface_hub import snapshot_download\n", "assert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\tfrom peft import (\n\t    prepare_model_for_int8_training,\n\t    LoraConfig,\n\t    get_peft_model,\n\t    get_peft_model_state_dict,\n\t    set_peft_model_state_dict,\n", ")\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\n\tparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\n\tparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\n\tparser.add_argument(\"--eval_steps\", type=int, default=200)\n\tparser.add_argument(\"--save_steps\", type=int, default=200)\n\tparser.add_argument(\"--test_size\", type=int, default=200)\n\tparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n", "parser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\n\tparser.add_argument(\"--ignore_data_skip\", type=str, default=\"False\")\n\targs = parser.parse_args()\n\tif not args.wandb:\n\t    os.environ[\"WANDB_MODE\"] = \"disable\"\n\t# optimized for RTX 4090. for larger GPUs, increase some of these?\n\tMICRO_BATCH_SIZE = 4  # this could actually be 5 but i like powers of 2\n\tBATCH_SIZE = 128\n\tMAX_STEPS = None\n\tGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n", "EPOCHS = 3  # we don't always need 3 tbh\n\tLEARNING_RATE = 3e-4  # the Karpathy constant\n\tCUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n\tLORA_R = 8\n\tLORA_ALPHA = 16\n\tLORA_DROPOUT = 0.05\n\tVAL_SET_SIZE = args.test_size #2000\n\tUSE_8bit = True\n\tif USE_8bit is True:\n\t    warnings.warn(\"If your version of bitsandbytes>0.37.2, Please downgrade bitsandbytes's version, for example: pip install bitsandbytes==0.37.2\")\n", "TARGET_MODULES = [\n\t    \"q_proj\",\n\t    \"v_proj\",\n\t]\n\tDATA_PATH = args.data_path \n\tOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\tdevice_map = \"auto\"\n\tworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\tddp = world_size != 1\n\tif ddp:\n", "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n\t    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n\tprint(args.model_path)\n\tmodel = LlamaForCausalLM.from_pretrained(\n\t    args.model_path,\n\t    load_in_8bit=USE_8bit,\n\t    device_map=device_map,\n\t)\n\ttokenizer = LlamaTokenizer.from_pretrained(\n\t    args.model_path, add_eos_token=True\n", ")\n\tif USE_8bit is True:\n\t    model = prepare_model_for_int8_training(model)\n\tconfig = LoraConfig(\n\t    r=LORA_R,\n\t    lora_alpha=LORA_ALPHA,\n\t    target_modules=TARGET_MODULES,\n\t    lora_dropout=LORA_DROPOUT,\n\t    bias=\"none\",\n\t    task_type=\"CAUSAL_LM\",\n", ")\n\tmodel = get_peft_model(model, config)\n\ttokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n\t#tokenizer.padding_side = \"left\"  # Allow batched inference\n\tdata = load_dataset(\"json\", data_files=DATA_PATH)\n\tnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\n\tif args.resume_from_checkpoint:\n\t    if args.lora_remote_checkpoint is not None:\n\t        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n\t    # Check the available weights and load them\n", "    checkpoint_name = os.path.join(\n\t        args.resume_from_checkpoint, \"pytorch_model.bin\"\n\t    )  # Full checkpoint\n\t    if not os.path.exists(checkpoint_name):\n\t        pytorch_bin_path = checkpoint_name\n\t        checkpoint_name = os.path.join(\n\t            args.resume_from_checkpoint, \"adapter_model.bin\"\n\t        )  # only LoRA model - LoRA config above has to fit\n\t        if os.path.exists(checkpoint_name):\n\t            os.rename(checkpoint_name, pytorch_bin_path)\n", "            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n\t        else:\n\t            args.resume_from_checkpoint = (\n\t                None  # So the trainer won't try loading its state\n\t            )\n\t    # The two files above have a different name depending on how they were saved, but are actually the same.\n\t    if os.path.exists(checkpoint_name):\n\t        print(f\"Restarting from {checkpoint_name}\")\n\t        adapters_weights = torch.load(checkpoint_name)\n\t        model = set_peft_model_state_dict(model, adapters_weights)\n", "    else:\n\t        print(f\"Checkpoint {checkpoint_name} not found\")\n\t    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n\t    if os.path.exists(train_args_path):\n\t        import json\n\t        base_train_args = json.load(open(train_args_path, 'r'))\n\t        base_max_steps = base_train_args[\"max_steps\"]\n\t        resume_scale = base_max_steps / now_max_steps\n\t        if base_max_steps > now_max_steps:\n\t            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n", "            EPOCHS = None\n\t            MAX_STEPS = base_max_steps\n\t        else:\n\t            MAX_STEPS = now_max_steps\n\telse:\n\t    MAX_STEPS = now_max_steps\n\tmodel.print_trainable_parameters()\n\tdef generate_prompt(data_point):\n\t    # sorry about the formatting disaster gotta move fast\n\t    if data_point[\"input\"]:\n", "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Input:\n\t{data_point[\"input\"]}\n\t### Response:\n\t{data_point[\"output\"]}\"\"\"\n\t    else:\n\t        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n", "{data_point[\"instruction\"]}\n\t### Response:\n\t{data_point[\"output\"]}\"\"\"\n\tdef tokenize(prompt):\n\t    # there's probably a way to do this with the tokenizer settings\n\t    # but again, gotta move fast\n\t    result = tokenizer(\n\t        prompt,\n\t        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n", "        padding=\"max_length\",\n\t    )\n\t    return {\n\t        \"input_ids\": result[\"input_ids\"][:-1],\n\t        \"attention_mask\": result[\"attention_mask\"][:-1],\n\t    }\n\tdef generate_and_tokenize_prompt(data_point):\n\t    # This function masks out the labels for the input,\n\t    # so that our loss is computed only on the response.\n\t    user_prompt = (\n", "        (\n\t            f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Input:\n\t{data_point[\"input\"]}\n\t### Response:\n\t\"\"\"\n\t        )\n\t        if data_point[\"input\"]\n", "        else (\n\t            f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Response:\n\t\"\"\"\n\t        )\n\t    )\n\t    len_user_prompt_tokens = (\n\t        len(\n", "            tokenizer(\n\t                user_prompt,\n\t                truncation=True,\n\t                max_length=CUTOFF_LEN + 1,\n\t            )[\"input_ids\"]\n\t        )\n\t        - 1\n\t    )  # no eos token\n\t    full_tokens = tokenizer(\n\t        user_prompt + data_point[\"output\"],\n", "        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t        padding=\"max_length\",\n\t    )[\"input_ids\"][:-1]\n\t    return {\n\t        \"input_ids\": full_tokens,\n\t        \"labels\": [-100] * len_user_prompt_tokens\n\t        + full_tokens[len_user_prompt_tokens:],\n\t        \"attention_mask\": [1] * (len(full_tokens)),\n\t    }\n", "if VAL_SET_SIZE > 0:\n\t    train_val = data[\"train\"].train_test_split(\n\t        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n\t    )\n\t    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\t    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\telse:\n\t    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\t    val_data = None\n\ttrainer = transformers.Trainer(\n", "    model=model,\n\t    train_dataset=train_data,\n\t    eval_dataset=val_data,\n\t    args=transformers.TrainingArguments(\n\t        per_device_train_batch_size=MICRO_BATCH_SIZE,\n\t        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n\t        warmup_steps=100,\n\t        num_train_epochs=EPOCHS,\n\t        max_steps=MAX_STEPS,\n\t        learning_rate=LEARNING_RATE,\n", "        fp16=True,\n\t        logging_steps=20,\n\t        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n\t        save_strategy=\"steps\",\n\t        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n\t        save_steps=args.save_steps,\n\t        output_dir=OUTPUT_DIR,\n\t        save_total_limit=30,\n\t        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n\t        ddp_find_unused_parameters=False if ddp else None,\n", "        report_to=\"wandb\" if args.wandb else [],\n\t        ignore_data_skip=args.ignore_data_skip,\n\t    ),\n\t    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\t)\n\tmodel.config.use_cache = False\n\told_state_dict = model.state_dict\n\tmodel.state_dict = (\n\t    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n\t).__get__(model, type(model))\n", "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\tprint(\"\\n If there's a warning about missing keys above, please disregard :)\")\n\ttrainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n\tmodel.save_pretrained(OUTPUT_DIR)\n"]}
{"filename": "finetune_fp16.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\timport torch.nn as nn\n\timport bitsandbytes as bnb\n\tfrom datasets import load_dataset\n\timport transformers\n\timport argparse\n\timport warnings\n\tfrom huggingface_hub import snapshot_download\n", "assert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaForCausalLM, LlamaTokenizer\n\tfrom peft import (\n\t    prepare_model_for_int8_training,\n\t    LoraConfig,\n\t    get_peft_model,\n\t    get_peft_model_state_dict,\n\t    set_peft_model_state_dict,\n", ")\n\tdef get_peft_state_maybe_zero_3(state_dict, bias):\n\t    if hasattr(param, \"ds_id\"):\n\t        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n\t        with zero.GatheredParameters([param]):\n\t            param = param.data.cpu().clone().detach()\n\t    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n\t    return to_return\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\n", "parser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\n\tparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\n\tparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\n\tparser.add_argument(\"--eval_steps\", type=int, default=200)\n\tparser.add_argument(\"--save_steps\", type=int, default=200)\n\tparser.add_argument(\"--test_size\", type=int, default=200)\n\tparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n\tparser.add_argument(\"--ignore_data_skip\", type=str, default=\"False\")\n\tparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\n\tparser.add_argument(\"--local_rank\", type=int, default=-1)\n", "parser.add_argument(\"--deepspeed\", action=\"store_true\", default=False)\n\targs = parser.parse_args()\n\tif not args.wandb:\n\t    os.environ[\"WANDB_MODE\"] = \"disable\"\n\t# optimized for RTX 4090. for larger GPUs, increase some of these?\n\tMICRO_BATCH_SIZE = 2  # this could actually be 5 but i like powers of 2\n\tBATCH_SIZE = 128\n\tMAX_STEPS = None\n\tGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n\tEPOCHS = 3  # we don't always need 3 tbh\n", "LEARNING_RATE = 3e-4  # the Karpathy constant\n\tCUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n\tLORA_R = 8\n\tLORA_ALPHA = 16\n\tLORA_DROPOUT = 0.05\n\tVAL_SET_SIZE = args.test_size #2000\n\tTARGET_MODULES = [\n\t    \"q_proj\",\n\t    \"v_proj\",\n\t]\n", "DATA_PATH = args.data_path\n\tOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\tdevice_map = {\"\": 0} #\"auto\"\n\tworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\tddp = world_size != 1\n\tif ddp:\n\t    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n\t    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n\tprint(args.model_path)\n\tmodel = LlamaForCausalLM.from_pretrained(\n", "    args.model_path,\n\t    load_in_8bit=False,\n\t    torch_dtype=torch.float16,\n\t    device_map=device_map,\n\t).half()\n\ttokenizer = LlamaTokenizer.from_pretrained(\n\t    args.model_path, add_eos_token=True\n\t)\n\t#model = prepare_model_for_int8_training(model)\n\tconfig = LoraConfig(\n", "    r=LORA_R,\n\t    lora_alpha=LORA_ALPHA,\n\t    target_modules=TARGET_MODULES,\n\t    lora_dropout=LORA_DROPOUT,\n\t    bias=\"none\",\n\t    task_type=\"CAUSAL_LM\",\n\t)\n\tmodel = get_peft_model(model, config)\n\ttokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n\t#tokenizer.padding_side = \"left\"  # Allow batched inference\n", "data = load_dataset(\"json\", data_files=DATA_PATH)\n\tnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\n\tif args.resume_from_checkpoint:\n\t    if args.lora_remote_checkpoint is not None:\n\t        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n\t    # Check the available weights and load them\n\t    checkpoint_name = os.path.join(\n\t        args.resume_from_checkpoint, \"pytorch_model.bin\"\n\t    )  # Full checkpoint\n\t    if not os.path.exists(checkpoint_name):\n", "        pytorch_bin_path = checkpoint_name\n\t        checkpoint_name = os.path.join(\n\t            args.resume_from_checkpoint, \"adapter_model.bin\"\n\t        )  # only LoRA model - LoRA config above has to fit\n\t        if os.path.exists(checkpoint_name):\n\t            os.rename(checkpoint_name, pytorch_bin_path)\n\t            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n\t        else:\n\t            args.resume_from_checkpoint = (\n\t                None  # So the trainer won't try loading its state\n", "            )\n\t    # The two files above have a different name depending on how they were saved, but are actually the same.\n\t    if os.path.exists(checkpoint_name):\n\t        print(f\"Restarting from {checkpoint_name}\")\n\t        adapters_weights = torch.load(checkpoint_name)\n\t        model = set_peft_model_state_dict(model, adapters_weights)\n\t    else:\n\t        print(f\"Checkpoint {checkpoint_name} not found\")\n\t    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n\t    if os.path.exists(train_args_path):\n", "        import json\n\t        base_train_args = json.load(open(train_args_path, 'r'))\n\t        base_max_steps = base_train_args[\"max_steps\"]\n\t        resume_scale = base_max_steps / now_max_steps\n\t        if base_max_steps > now_max_steps:\n\t            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n\t            EPOCHS = None\n\t            MAX_STEPS = base_max_steps\n\t        else:\n\t            MAX_STEPS = now_max_steps\n", "else:\n\t    MAX_STEPS = now_max_steps\n\tmodel.print_trainable_parameters()\n\tdef generate_prompt(data_point):\n\t    # sorry about the formatting disaster gotta move fast\n\t    if data_point[\"input\"]:\n\t        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Input:\n", "{data_point[\"input\"]}\n\t### Response:\n\t{data_point[\"output\"]}\"\"\"\n\t    else:\n\t        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Response:\n\t{data_point[\"output\"]}\"\"\"\n\tdef tokenize(prompt):\n", "    # there's probably a way to do this with the tokenizer settings\n\t    # but again, gotta move fast\n\t    result = tokenizer(\n\t        prompt,\n\t        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t        padding=\"max_length\",\n\t    )\n\t    return {\n\t        \"input_ids\": result[\"input_ids\"][:-1],\n", "        \"attention_mask\": result[\"attention_mask\"][:-1],\n\t    }\n\tdef generate_and_tokenize_prompt(data_point):\n\t    # This function masks out the labels for the input,\n\t    # so that our loss is computed only on the response.\n\t    user_prompt = (\n\t        (\n\t            f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n", "### Input:\n\t{data_point[\"input\"]}\n\t### Response:\n\t\"\"\"\n\t        )\n\t        if data_point[\"input\"]\n\t        else (\n\t            f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n", "### Response:\n\t\"\"\"\n\t        )\n\t    )\n\t    len_user_prompt_tokens = (\n\t        len(\n\t            tokenizer(\n\t                user_prompt,\n\t                truncation=True,\n\t                max_length=CUTOFF_LEN + 1,\n", "            )[\"input_ids\"]\n\t        )\n\t        - 1\n\t    )  # no eos token\n\t    full_tokens = tokenizer(\n\t        user_prompt + data_point[\"output\"],\n\t        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t        padding=\"max_length\",\n\t    )[\"input_ids\"][:-1]\n", "    return {\n\t        \"input_ids\": full_tokens,\n\t        \"labels\": [-100] * len_user_prompt_tokens\n\t        + full_tokens[len_user_prompt_tokens:],\n\t        \"attention_mask\": [1] * (len(full_tokens)),\n\t    }\n\tif VAL_SET_SIZE > 0:\n\t    train_val = data[\"train\"].train_test_split(\n\t        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n\t    )\n", "    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\t    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n\telse:\n\t    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n\t    val_data = None\n\ttrainer = transformers.Trainer(\n\t    model=model,\n\t    train_dataset=train_data,\n\t    eval_dataset=val_data,\n\t    args=transformers.TrainingArguments(\n", "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n\t        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n\t        warmup_steps=100,\n\t        num_train_epochs=EPOCHS,\n\t        max_steps=MAX_STEPS,\n\t        learning_rate=LEARNING_RATE,\n\t        fp16=True,\n\t        logging_steps=20,\n\t        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n\t        save_strategy=\"steps\",\n", "        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n\t        save_steps=args.save_steps,\n\t        output_dir=OUTPUT_DIR,\n\t        save_total_limit=30,\n\t        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n\t        ddp_find_unused_parameters=False if ddp else None,\n\t        report_to=\"wandb\" if args.wandb else [],\n\t        ignore_data_skip=args.ignore_data_skip,\n\t        deepspeed=\"sample/zero_config.json\" if args.deepspeed else None,\n\t    ),\n", "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\t)\n\tmodel.config.use_cache = False\n\told_state_dict = model.state_dict\n\tmodel.state_dict = (\n\t    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n\t).__get__(model, type(model))\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\tprint(\"\\n If there's a warning about missing keys above, please disregard :)\")\n", "trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n\tmodel.save_pretrained(OUTPUT_DIR)\n"]}
{"filename": "interaction.py", "chunked_list": ["import sys\n\timport torch\n\tfrom peft import PeftModel\n\timport transformers\n\timport gradio as gr\n\timport argparse\n\timport warnings\n\timport os\n\tassert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n", "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\n\tparser.add_argument(\"--lora_path\", type=str, default=\"./lora-Vicuna/checkpoint-final\")\n\tparser.add_argument(\"--use_local\", type=int, default=1)\n\targs = parser.parse_args()\n\ttokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\tLOAD_8BIT = True\n\tBASE_MODEL = args.model_path\n", "LORA_WEIGHTS = args.lora_path\n\t# fix the path for local checkpoint\n\tlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\n\tprint(lora_bin_path)\n\tif not os.path.exists(lora_bin_path) and args.use_local:\n\t    pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n\t    print(pytorch_bin_path)\n\t    if os.path.exists(pytorch_bin_path):\n\t        os.rename(pytorch_bin_path, lora_bin_path)\n\t        warnings.warn(\"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\")\n", "    else:\n\t        assert ('Checkpoint is not Found!')\n\tif torch.cuda.is_available():\n\t    device = \"cuda\"\n\telse:\n\t    device = \"cpu\"\n\ttry:\n\t    if torch.backends.mps.is_available():\n\t        device = \"mps\"\n\texcept:\n", "    pass\n\tif device == \"cuda\":\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL,\n\t        load_in_8bit=LOAD_8BIT,\n\t        torch_dtype=torch.float16,\n\t        device_map=\"auto\", #device_map={\"\": 0},\n\t    )\n\t    model = PeftModel.from_pretrained(\n\t        model,\n", "        LORA_WEIGHTS,\n\t        torch_dtype=torch.float16,\n\t        device_map=\"auto\", #device_map={\"\": 0},\n\t    )\n\telif device == \"mps\":\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL,\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16,\n\t    )\n", "    model = PeftModel.from_pretrained(\n\t        model,\n\t        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16,\n\t    )\n\telse:\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n\t    )\n", "    model = PeftModel.from_pretrained(\n\t        model,\n\t        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t    )\n\tdef generate_prompt(instruction, input=None):\n\t    if input:\n\t        return f\"\"\"The following is a conversation between an AI assistant called Assistant and a human user called User.\n\t### Instruction:\n\t{instruction}\n", "### Input:\n\t{input}\n\t### Response:\"\"\"\n\t    else:\n\t        return f\"\"\"The following is a conversation between an AI assistant called Assistant and a human user called User.\n\t### Instruction:\n\t{instruction}\n\t### Response:\"\"\"\n\tif not LOAD_8BIT:\n\t    model.half()  # seems to fix bugs for some users.\n", "model.eval()\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\tdef interaction(\n\t    input,\n\t    history,\n\t    temperature=0.1,\n\t    top_p=0.75,\n\t    top_k=40,\n\t    num_beams=4,\n", "    max_new_tokens=128,\n\t    repetition_penalty=1.0,\n\t    max_memory=256,\n\t    **kwargs,\n\t):\n\t    now_input = input\n\t    history = history or []\n\t    if len(history) != 0:\n\t        input = \"\\n\".join([\"User:\" + i[0]+\"\\n\"+\"Assistant:\" + i[1] for i in history]) + \"\\n\" + \"User:\" + input\n\t        if len(input) > max_memory:\n", "            input = input[-max_memory:]\n\t    print(input)\n\t    print(len(input))\n\t    prompt = generate_prompt(input)\n\t    inputs = tokenizer(prompt, return_tensors=\"pt\")\n\t    input_ids = inputs[\"input_ids\"].to(device)\n\t    generation_config = GenerationConfig(\n\t        temperature=temperature,\n\t        top_p=top_p,\n\t        top_k=top_k,\n", "        num_beams=num_beams,\n\t        **kwargs,\n\t    )\n\t    with torch.no_grad():\n\t        generation_output = model.generate(\n\t            input_ids=input_ids,\n\t            generation_config=generation_config,\n\t            return_dict_in_generate=True,\n\t            output_scores=True,\n\t            max_new_tokens=max_new_tokens,\n", "            repetition_penalty=float(repetition_penalty),\n\t        )\n\t    s = generation_output.sequences[0]\n\t    output = tokenizer.decode(s)\n\t    output = output.split(\"### Response:\")[1].strip()\n\t    output = output.replace(\"Belle\", \"Vicuna\")\n\t    if 'User:' in output:\n\t        output = output.split(\"User:\")[0]\n\t    history.append((now_input, output))\n\t    print(history)\n", "    return history, history\n\tchatbot = gr.Chatbot().style(color_map=(\"green\", \"pink\"))\n\tdemo = gr.Interface(\n\t    fn=interaction,\n\t    inputs=[\n\t        gr.components.Textbox(\n\t            lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n\t        ),\n\t        \"state\",\n\t        gr.components.Slider(minimum=0, maximum=1, value=1.0, label=\"Temperature\"),\n", "        gr.components.Slider(minimum=0, maximum=1, value=0.9, label=\"Top p\"),\n\t        gr.components.Slider(minimum=0, maximum=100, step=1, value=60, label=\"Top k\"),\n\t        gr.components.Slider(minimum=1, maximum=5, step=1, value=2, label=\"Beams\"),\n\t        gr.components.Slider(\n\t            minimum=1, maximum=2000, step=1, value=128, label=\"Max new tokens\"\n\t        ),\n\t        gr.components.Slider(\n\t            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n\t        ),\n\t        gr.components.Slider(\n", "            minimum=0, maximum=2000, step=1, value=256, label=\"max memory\"\n\t        ),\n\t    ],\n\t    outputs=[chatbot, \"state\"],\n\t    allow_flagging=\"auto\",\n\t    title=\"Chinese-Vicuna 中文小羊驼\",\n\t    description=\"中文小羊驼由各种高质量的开源instruction数据集，结合Alpaca-lora的代码训练而来，模型基于开源的llama7B，主要贡献是对应的lora模型。由于代码训练资源要求较小，希望为llama中文lora社区做一份贡献。\",\n\t)\n\tdemo.queue().launch(share=True, inbrowser=True)"]}
{"filename": "utils.py", "chunked_list": ["import logging\n\timport sys\n\timport os\n\timport torch\n\timport json\n\tfrom typing import Optional, Tuple, Union, List, Callable\n\tfrom transformers import LlamaForCausalLM\n\tfrom transformers.generation.logits_process import LogitsProcessor\n\tfrom transformers.generation.beam_search import BeamSearchScorer\n\tfrom transformers.deepspeed import is_deepspeed_zero3_enabled\n", "from transformers.generation.utils import (\n\t    LogitsProcessorList,\n\t    StoppingCriteriaList,\n\t    GenerationConfig,\n\t    GenerationMixin,\n\t)\n\timport warnings\n\tfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\n\timport peft\n\timport torch.distributed as dist\n", "from torch import nn\n\timport copy\n\tfrom accelerate.hooks import (\n\t    AlignDevicesHook,\n\t    add_hook_to_module,\n\t    remove_hook_from_submodules,\n\t)\n\tfrom accelerate.utils import get_balanced_memory\n\tfrom huggingface_hub import hf_hub_download\n\tfrom accelerate import dispatch_model, infer_auto_device_map\n", "from peft.utils import PeftType, set_peft_model_state_dict\n\tdef printf(*args,**kargs):\n\t    if os.environ.get('DEBUG',False):\n\t        end = '\\n'\n\t        if 'end' in kargs:\n\t            end = kargs['end']\n\t        print(*args, end=end, flush=True)\n\tclass ColorFormatter(logging.Formatter):\n\t    grey = \"\\x1b[38;20m\"\n\t    blue = \"\\x1b[34;20m\"\n", "    yellow = \"\\x1b[33;20m\"\n\t    red = \"\\x1b[31;20m\"\n\t    bold_red = \"\\x1b[31;1m\"\n\t    reset = \"\\x1b[0m\"\n\t    def __init__(self, fmt):\n\t        super().__init__(fmt)\n\t        self.FORMATS = {\n\t            logging.DEBUG: self.grey + fmt + self.reset,\n\t            logging.INFO: self.blue + fmt + self.reset,\n\t            logging.WARNING: self.yellow + fmt + self.reset,\n", "            logging.ERROR: self.red + fmt + self.reset,\n\t            logging.CRITICAL: self.bold_red + fmt + self.reset\n\t        }\n\t    def format(self, record):\n\t        log_fmt = self.FORMATS.get(record.levelno)\n\t        formatter = logging.Formatter(log_fmt)\n\t        return formatter.format(record)\n\tdef set_console_logger(name):\n\t    logger = logging.getLogger(name)\n\t    logger.setLevel(logging.DEBUG)\n", "    consoleHandler = logging.StreamHandler(sys.stdout)\n\t    consoleHandler.setLevel(logging.INFO)\n\t    consoleHandler.setFormatter(ColorFormatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n\t    logger.addHandler(consoleHandler)\n\t    return logger\n\tdef set_file_logger(name, dir, use_console=False):\n\t    logger = logging.getLogger(name)\n\t    logger.setLevel(logging.DEBUG)\n\t    os.makedirs(dir, exist_ok=True)\n\t    if use_console:\n", "        logger.propagate = False # disable default handler\n\t        consoleHandler = logging.StreamHandler(sys.stdout)\n\t        consoleHandler.setLevel(logging.INFO)\n\t        consoleHandler.setFormatter(ColorFormatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n\t        logger.addHandler(consoleHandler)\n\t    fileHandler = logging.FileHandler(os.path.join(dir,'session.log'), mode='a') \n\t    fileHandler.setLevel(logging.INFO)\n\t    fileHandler.setFormatter(logging.Formatter(\"%(asctime)s | %(levelname)s %(message)s\"))\n\t    logger.addHandler(fileHandler)\n\t    return logger\n", "def to_jsonl(data, path):\n\t    with open(path, 'a') as f:\n\t        for line in data:\n\t            f.write(json.dumps(line,ensure_ascii=False)+'\\n')\n\tdef from_json(path):\n\t    return json.load(open(path))\n\tdef from_jsonl(path):\n\t    return [json.loads(line) for line in open(path, 'r') ]\n\tdef to_json(data, path):\n\t    json.dump(data, open(path, 'w'), ensure_ascii=False)\n", "class StreamGenerationMixin(GenerationMixin):\n\t    # support for streamly generation\n\t    # TODO: group_beam_search\n\t    @torch.no_grad()\n\t    def stream_generate(\n\t        self,\n\t        input_ids: Optional[torch.Tensor] = None,\n\t        generation_config: Optional[GenerationConfig] = None,\n\t        logits_processor: Optional[LogitsProcessorList] = None,\n\t        stopping_criteria: Optional[StoppingCriteriaList] = None,\n", "        prefix_allowed_tokens_fn: Optional[\n\t            Callable[[int, torch.Tensor], List[int]]\n\t        ] = None,\n\t        **kwargs,\n\t    ):\n\t        if is_deepspeed_zero3_enabled() and dist.world_size() > 1:\n\t            synced_gpus = True\n\t        else:\n\t            synced_gpus = False\n\t        if kwargs.get(\"attention_mask\", None) is not None:\n", "            # concat prompt attention mask\n\t            prefix_attention_mask = torch.ones(\n\t                kwargs[\"input_ids\"].shape[0], self.peft_config.num_virtual_tokens\n\t            ).to(kwargs[\"input_ids\"].device)\n\t            kwargs[\"attention_mask\"] = torch.cat(\n\t                (prefix_attention_mask, kwargs[\"attention_mask\"]), dim=1\n\t            )\n\t        if kwargs.get(\"position_ids\", None) is not None:\n\t            warnings.warn(\n\t                \"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\"\n", "            )\n\t            kwargs[\"position_ids\"] = None\n\t        if kwargs.get(\"token_type_ids\", None) is not None:\n\t            warnings.warn(\n\t                \"Token type ids are not supported for parameter efficient tuning. Ignoring token type ids\"\n\t            )\n\t            kwargs[\"token_type_ids\"] = None\n\t        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n\t        if generation_config is None:\n\t            generation_config = self.generation_config\n", "        generation_config = copy.deepcopy(generation_config)\n\t        model_kwargs = generation_config.update(**kwargs)\n\t        bos_token_id, eos_token_id, pad_token_id = (\n\t            generation_config.bos_token_id,\n\t            generation_config.eos_token_id,\n\t            generation_config.pad_token_id,\n\t        )\n\t        if isinstance(eos_token_id, int):\n\t            eos_token_id = [eos_token_id]\n\t        has_default_max_length = (\n", "            kwargs.get(\"max_length\") is None\n\t            and generation_config.max_length is not None\n\t        )\n\t        if has_default_max_length and generation_config.max_new_tokens is None:\n\t            warnings.warn(\n\t                f\"Using `max_length`'s default ({generation_config.max_length}) to control the generation length. \"\n\t                \"This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we\"\n\t                \" recommend using `max_new_tokens` to control the maximum length of the generation.\",\n\t                UserWarning,\n\t            )\n", "        elif generation_config.max_new_tokens is not None:\n\t            generation_config.max_length = (\n\t                generation_config.max_new_tokens + input_ids_seq_length\n\t            )\n\t        if generation_config.min_new_tokens is not None:\n\t            generation_config.min_length = (\n\t                generation_config.min_new_tokens + input_ids_seq_length\n\t            )\n\t        if input_ids_seq_length >= generation_config.max_length:\n\t            input_ids_string = (\n", "                \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n\t            )\n\t        # 2. Set generation parameters if not already defined\n\t        logits_processor = (\n\t            logits_processor if logits_processor is not None else LogitsProcessorList()\n\t        )\n\t        stopping_criteria = (\n\t            stopping_criteria\n\t            if stopping_criteria is not None\n\t            else StoppingCriteriaList()\n", "        )\n\t        # 7. determine generation mode\n\t        is_constraint_gen_mode = (\n\t            generation_config.constraints is not None or generation_config.force_words_ids is not None\n\t        )\n\t        is_contrastive_search_gen_mode = (\n\t            generation_config.top_k is not None\n\t            and generation_config.top_k > 1\n\t            and generation_config.do_sample is False\n\t            and generation_config.penalty_alpha is not None\n", "            and generation_config.penalty_alpha > 0\n\t        )\n\t        is_greedy_gen_mode = (\n\t            (generation_config.num_beams == 1)\n\t            and (generation_config.num_beam_groups == 1)\n\t            and generation_config.do_sample is False\n\t            and not is_constraint_gen_mode\n\t            and not is_contrastive_search_gen_mode\n\t        )\n\t        # beam=1 and do_sample=True\n", "        is_sample_gen_mode = (\n\t            (generation_config.num_beams == 1)\n\t            and (generation_config.num_beam_groups == 1)\n\t            and generation_config.do_sample is True\n\t            and not is_constraint_gen_mode\n\t            and not is_contrastive_search_gen_mode\n\t        )\n\t        is_beam_gen_mode = (\n\t            (generation_config.num_beams > 1)\n\t            and (generation_config.num_beam_groups == 1)\n", "            and generation_config.do_sample is False\n\t            and not is_constraint_gen_mode\n\t            and not is_contrastive_search_gen_mode\n\t        )\n\t        is_beam_sample_gen_mode = (\n\t            (generation_config.num_beams > 1)\n\t            and (generation_config.num_beam_groups == 1)\n\t            and generation_config.do_sample is True\n\t            and not is_constraint_gen_mode\n\t            and not is_contrastive_search_gen_mode\n", "        )\n\t        is_group_beam_gen_mode = (\n\t            (generation_config.num_beams > 1)\n\t            and (generation_config.num_beam_groups > 1)\n\t            and not is_constraint_gen_mode\n\t            and not is_contrastive_search_gen_mode\n\t        )\n\t        # 8. prepare distribution pre_processing samplers\n\t        logits_processor = self._get_logits_processor(\n\t            generation_config=generation_config,\n", "            input_ids_seq_length=input_ids_seq_length,\n\t            encoder_input_ids=input_ids,\n\t            prefix_allowed_tokens_fn=prefix_allowed_tokens_fn,\n\t            logits_processor=logits_processor,\n\t        )\n\t        # 9. prepare stopping criteria\n\t        stopping_criteria = self._get_stopping_criteria(\n\t            generation_config=generation_config, stopping_criteria=stopping_criteria\n\t        )\n\t        logits_warper = self._get_logits_warper(generation_config)\n", "        if is_greedy_gen_mode:\n\t            # 11. run greedy search\n\t            return self.stream_greedy_search(\n\t                input_ids,\n\t                logits_processor,\n\t                stopping_criteria,\n\t                generation_config,\n\t                synced_gpus,\n\t                **model_kwargs,\n\t            )\n", "        elif is_sample_gen_mode:\n\t            # 12. expand input_ids with `num_return_sequences` additional sequences per batch\n\t            input_ids, model_kwargs = self._expand_inputs_for_generation(\n\t                input_ids=input_ids,\n\t                expand_size=generation_config.num_return_sequences,\n\t                is_encoder_decoder=self.config.is_encoder_decoder,\n\t                **model_kwargs,\n\t            )\n\t            return self.stream_sample(\n\t                generation_config,\n", "                input_ids,\n\t                logits_processor,\n\t                logits_warper,\n\t                stopping_criteria,\n\t                synced_gpus,\n\t                **model_kwargs,\n\t            )\n\t        elif is_beam_gen_mode:\n\t            return self.stream_beam_search(\n\t                generation_config,\n", "                input_ids,\n\t                logits_processor,\n\t                stopping_criteria,\n\t                synced_gpus,\n\t                **model_kwargs,\n\t            )\n\t        elif is_beam_sample_gen_mode:\n\t            # interleave input_ids with `num_beams` additional sequences per batch\n\t            return self.stream_beam_sample(\n\t                input_ids,\n", "                logits_processor,\n\t                logits_warper,\n\t                stopping_criteria,\n\t                generation_config,\n\t                synced_gpus,\n\t                **model_kwargs,\n\t            )\n\t        else:\n\t            raise Exception('not implement')\n\t    def stream_sample(\n", "        self,\n\t        generation_config,\n\t        input_ids,\n\t        logits_processor,\n\t        logits_warper,\n\t        stopping_criteria,\n\t        synced_gpus,\n\t        **model_kwargs,\n\t    ):\n\t        bos_token_id, eos_token_id, pad_token_id = (\n", "            generation_config.bos_token_id,\n\t            generation_config.eos_token_id,\n\t            generation_config.pad_token_id,\n\t        )\n\t        if isinstance(eos_token_id, int):\n\t            eos_token_id = [eos_token_id]\n\t        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n\t        # keep track of which sequences are already finished\n\t        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n\t        this_peer_finished = False  # used by synced_gpus only\n", "        scores=()\n\t        # auto-regressive generation\n\t        while True:\n\t            if synced_gpus:\n\t                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n\t                # The following logic allows an early break if all peers finished generating their sequence\n\t                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n\t                # send 0.0 if we finished, 1.0 otherwise\n\t                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n\t                # did all peers finish? the reduced sum will be 0.0 then\n", "                if this_peer_finished_flag.item() == 0.0:\n\t                    break\n\t            # prepare model inputs\n\t            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\t            # forward pass to get next token\n\t            outputs = self(\n\t                **model_inputs,\n\t                return_dict=True,\n\t            )\n\t            if synced_gpus and this_peer_finished:\n", "                continue  # don't waste resources running the code we don't need\n\t            next_token_logits = outputs.logits[:, -1, :]\n\t            # pre-process distribution\n\t            next_token_scores = logits_processor(input_ids, next_token_logits)\n\t            next_token_scores = logits_warper(input_ids, next_token_scores)\n\t            # sample\n\t            probs = nn.functional.softmax(next_token_scores, dim=-1)\n\t            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n\t            # finished sentences should have their next token be a padding token\n\t            if eos_token_id is not None:\n", "                if pad_token_id is None:\n\t                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n\t                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n\t            # update generated ids, model inputs, and length for next step\n\t            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n\t            model_kwargs = self._update_model_kwargs_for_generation(\n\t                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n\t            )\n\t            yield input_ids\n\t            # if eos_token was found in one sentence, set sentence to finished\n", "            if eos_token_id_tensor is not None:\n\t                unfinished_sequences = unfinished_sequences.mul(\n\t                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n\t                )\n\t            # stop when each sentence is finished, or if we exceed the maximum length\n\t            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n\t                if not synced_gpus:\n\t                    break\n\t                else:\n\t                    this_peer_finished = True\n", "        yield input_ids\n\t    def stream_beam_sample(\n\t        self,\n\t        input_ids,\n\t        logits_processor,\n\t        logits_warper,\n\t        stopping_criteria,\n\t        generation_config,\n\t        synced_gpus,\n\t        **model_kwargs,\n", "    ):\n\t        bos_token_id, eos_token_id, pad_token_id = (\n\t            generation_config.bos_token_id,\n\t            generation_config.eos_token_id,\n\t            generation_config.pad_token_id,\n\t        )\n\t        if isinstance(eos_token_id, int):\n\t            eos_token_id = [eos_token_id]\n\t        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n\t        num_beams = generation_config.num_beams\n", "        batch_size, cur_len = input_ids.shape[0], input_ids.shape[-1]\n\t        beam_scorer = BeamSearchScorer(\n\t            batch_size=batch_size,\n\t            num_beams=generation_config.num_beams,\n\t            device=input_ids.device,\n\t            length_penalty=generation_config.length_penalty,\n\t            do_early_stopping=generation_config.early_stopping,\n\t            num_beam_hyps_to_keep=generation_config.num_return_sequences,\n\t            max_length=generation_config.max_length,\n\t        )\n", "        input_ids, model_kwargs = self._expand_inputs_for_generation(\n\t            input_ids=input_ids,\n\t            expand_size=generation_config.num_beams * generation_config.num_return_sequences,\n\t            is_encoder_decoder=self.config.is_encoder_decoder,\n\t            **model_kwargs,\n\t        )\n\t        scores = ()\n\t        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n\t        beam_scores = beam_scores.view((batch_size * num_beams,))\n\t        this_peer_finished = False  # used by synced_gpus only\n", "        while True:\n\t            if synced_gpus:\n\t                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n\t                # The following logic allows an early break if all peers finished generating their sequence\n\t                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n\t                # send 0.0 if we finished, 1.0 otherwise\n\t                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n\t                # did all peers finish? the reduced sum will be 0.0 then\n\t                if this_peer_finished_flag.item() == 0.0:\n\t                    break\n", "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\t            outputs = self(\n\t                **model_inputs,\n\t                return_dict=True,\n\t            )\n\t            if synced_gpus and this_peer_finished:\n\t                cur_len = cur_len + 1\n\t                continue  # don't waste resources running the code we don't need\n\t            next_token_logits = outputs.logits[:, -1, :]\n\t            # hack: adjust tokens for Marian. For Marian we have to make sure that the `pad_token_id`\n", "            # cannot be generated both before and after the `nn.functional.log_softmax` operation.\n\t            next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len)\n\t            next_token_scores = nn.functional.log_softmax(\n\t                next_token_logits, dim=-1\n\t            )  # (batch_size * num_beams, vocab_size)\n\t            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n\t            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(next_token_scores)\n\t            # Note: logits warpers are intentionally applied after adding running beam scores. On some logits warpers\n\t            # (like top_p) this is indiferent, but on others (like temperature) it is not. For reference, see\n\t            # https://github.com/huggingface/transformers/pull/5420#discussion_r449779867\n", "            next_token_scores = logits_warper(input_ids, next_token_scores)\n\t            # reshape for beam search\n\t            vocab_size = next_token_scores.shape[-1]\n\t            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n\t            probs = nn.functional.softmax(next_token_scores, dim=-1)\n\t            next_tokens = torch.multinomial(probs, num_samples=2 * num_beams)\n\t            next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n\t            next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n\t            next_tokens = torch.gather(next_tokens, -1, _indices)\n\t            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n", "            next_tokens = next_tokens % vocab_size\n\t            # stateless\n\t            beam_outputs = beam_scorer.process(\n\t                input_ids,\n\t                next_token_scores,\n\t                next_tokens,\n\t                next_indices,\n\t                pad_token_id=pad_token_id,\n\t                eos_token_id=eos_token_id,\n\t                beam_indices=None,\n", "            )\n\t            beam_scores = beam_outputs[\"next_beam_scores\"]\n\t            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n\t            beam_idx = beam_outputs[\"next_beam_indices\"]\n\t            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n\t            yield input_ids\n\t            model_kwargs = self._update_model_kwargs_for_generation(\n\t                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n\t            )\n\t            if model_kwargs[\"past_key_values\"] is not None:\n", "                model_kwargs[\"past_key_values\"] = self._reorder_cache(model_kwargs[\"past_key_values\"], beam_idx)\n\t            # increase cur_len\n\t            cur_len = cur_len + 1\n\t            if beam_scorer.is_done or stopping_criteria(input_ids, scores):\n\t                if not synced_gpus:\n\t                    break\n\t                else:\n\t                    this_peer_finished = True\n\t        sequence_outputs = beam_scorer.finalize(\n\t            input_ids,\n", "            beam_scores,\n\t            next_tokens,\n\t            next_indices,\n\t            pad_token_id=pad_token_id,\n\t            eos_token_id=eos_token_id,\n\t            max_length=stopping_criteria.max_length,\n\t            beam_indices=None,\n\t        )\n\t        yield sequence_outputs[\"sequences\"]\n\t    def stream_greedy_search(\n", "        self,\n\t        input_ids,\n\t        logits_processor,\n\t        stopping_criteria,\n\t        generation_config,\n\t        synced_gpus,\n\t        **model_kwargs,\n\t    ):\n\t        # init values\n\t        bos_token_id, eos_token_id, pad_token_id = (\n", "            generation_config.bos_token_id,\n\t            generation_config.eos_token_id,\n\t            generation_config.pad_token_id,\n\t        )\n\t        if isinstance(eos_token_id, int):\n\t            eos_token_id = [eos_token_id]\n\t        eos_token_id_tensor = torch.tensor(eos_token_id).to(input_ids.device) if eos_token_id is not None else None\n\t        # init attention / hidden states / scores tuples\n\t        scores = () \n\t        # keep track of which sequences are already finished\n", "        unfinished_sequences = torch.ones(input_ids.shape[0], dtype=torch.long, device=input_ids.device)\n\t        this_peer_finished = False  # used by synced_gpus only\n\t        while True:\n\t            if synced_gpus:\n\t                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n\t                # The following logic allows an early break if all peers finished generating their sequence\n\t                this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(input_ids.device)\n\t                # send 0.0 if we finished, 1.0 otherwise\n\t                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n\t                # did all peers finish? the reduced sum will be 0.0 then\n", "                if this_peer_finished_flag.item() == 0.0:\n\t                    break\n\t            # prepare model inputs\n\t            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\t            # forward pass to get next token\n\t            outputs = self(\n\t                **model_inputs,\n\t                return_dict=True,\n\t            )\n\t            if synced_gpus and this_peer_finished:\n", "                continue  # don't waste resources running the code we don't need\n\t            next_token_logits = outputs.logits[:, -1, :]\n\t            # pre-process distribution\n\t            next_tokens_scores = logits_processor(input_ids, next_token_logits)\n\t            # argmax\n\t            next_tokens = torch.argmax(next_tokens_scores, dim=-1)\n\t            # finished sentences should have their next token be a padding token\n\t            if eos_token_id is not None:\n\t                if pad_token_id is None:\n\t                    raise ValueError(\"If `eos_token_id` is defined, make sure that `pad_token_id` is defined.\")\n", "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n\t            # update generated ids, model inputs, and length for next step\n\t            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n\t            model_kwargs = self._update_model_kwargs_for_generation(\n\t                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n\t            )\n\t            yield input_ids\n\t            # if eos_token was found in one sentence, set sentence to finished\n\t            if eos_token_id_tensor is not None:\n\t                unfinished_sequences = unfinished_sequences.mul(\n", "                    next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n\t                )\n\t            # stop when each sentence is finished, or if we exceed the maximum length\n\t            if unfinished_sequences.max() == 0 or stopping_criteria(input_ids, scores):\n\t                if not synced_gpus:\n\t                    break\n\t                else:\n\t                    this_peer_finished = True\n\t        yield input_ids\n\t    def stream_beam_search(\n", "        self,\n\t        generation_config,\n\t        input_ids,\n\t        logits_processor,\n\t        stopping_criteria,\n\t        synced_gpus,\n\t        **model_kwargs,\n\t    ):\n\t        # 10. go into beam search generation modes\n\t        # 11. prepare beam search scorer\n", "        bos_token_id, eos_token_id, pad_token_id = (\n\t            generation_config.bos_token_id,\n\t            generation_config.eos_token_id,\n\t            generation_config.pad_token_id,\n\t        )\n\t        if isinstance(eos_token_id, int):\n\t            eos_token_id = [eos_token_id]\n\t        num_beams = generation_config.num_beams\n\t        batch_size, input_ids_seq_length = input_ids.shape[0], input_ids.shape[-1]\n\t        beam_scorer = BeamSearchScorer(\n", "            batch_size=batch_size,\n\t            num_beams=generation_config.num_beams,\n\t            device=input_ids.device,\n\t            length_penalty=generation_config.length_penalty,\n\t            do_early_stopping=generation_config.early_stopping,\n\t            num_beam_hyps_to_keep=generation_config.num_return_sequences,\n\t            max_length=generation_config.max_length,\n\t        )\n\t        # 12. interleave input_ids with `num_beams` additional sequences per batch\n\t        input_ids, model_kwargs = self._expand_inputs_for_generation(\n", "            input_ids=input_ids,\n\t            expand_size=generation_config.num_beams,\n\t            is_encoder_decoder=self.config.is_encoder_decoder,\n\t            **model_kwargs,\n\t        )\n\t        # beam_search logits\n\t        batch_beam_size, cur_len = input_ids.shape\n\t        if num_beams * batch_size != batch_beam_size:\n\t            raise ValueError(\n\t                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n", "            )\n\t        beam_scores = torch.zeros(\n\t            (batch_size, num_beams), dtype=torch.float, device=input_ids.device\n\t        )\n\t        beam_scores[:, 1:] = -1e9\n\t        beam_scores = beam_scores.view((batch_size * num_beams,))\n\t        this_peer_finished = False  # used by synced_gpus only\n\t        while True:\n\t            if synced_gpus:\n\t                # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n", "                # The following logic allows an early break if all peers finished generating their sequence\n\t                this_peer_finished_flag = torch.tensor(\n\t                    0.0 if this_peer_finished else 1.0\n\t                ).to(input_ids.device)\n\t                # send 0.0 if we finished, 1.0 otherwise\n\t                dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n\t                # did all peers finish? the reduced sum will be 0.0 then\n\t                if this_peer_finished_flag.item() == 0.0:\n\t                    break\n\t            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n", "            outputs = self(\n\t                **model_inputs,\n\t                return_dict=True,\n\t                output_attentions=False,\n\t                output_hidden_states=False,\n\t            )\n\t            if synced_gpus and this_peer_finished:\n\t                cur_len = cur_len + 1\n\t                continue  # don't waste resources running the code we don't need\n\t            next_token_logits = outputs.logits[:, -1, :]\n", "            # next_token_logits = self.adjust_logits_during_generation(next_token_logits, cur_len=cur_len) hack: adjust tokens for Marian.\n\t            next_token_scores = nn.functional.log_softmax(\n\t                next_token_logits, dim=-1\n\t            )  # (batch_size * num_beams, vocab_size)\n\t            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n\t            next_token_scores = next_token_scores_processed + beam_scores[\n\t                :, None\n\t            ].expand_as(next_token_scores)\n\t            # reshape for beam search\n\t            vocab_size = next_token_scores.shape[-1]\n", "            next_token_scores = next_token_scores.view(\n\t                batch_size, num_beams * vocab_size\n\t            )\n\t            # Sample 2 next tokens for each beam (so we have some spare tokens and match output of beam search)\n\t            next_token_scores, next_tokens = torch.topk(\n\t                next_token_scores, 2 * num_beams, dim=1, largest=True, sorted=True\n\t            )\n\t            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n\t            next_tokens = next_tokens % vocab_size\n\t            # stateless\n", "            beam_outputs = beam_scorer.process(\n\t                input_ids,\n\t                next_token_scores,\n\t                next_tokens,\n\t                next_indices,\n\t                pad_token_id=pad_token_id,\n\t                eos_token_id=eos_token_id,\n\t                beam_indices=None,\n\t            )\n\t            beam_scores = beam_outputs[\"next_beam_scores\"]\n", "            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n\t            beam_idx = beam_outputs[\"next_beam_indices\"]\n\t            input_ids = torch.cat(\n\t                [input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1\n\t            )\n\t            model_kwargs = self._update_model_kwargs_for_generation(\n\t                outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder\n\t            )\n\t            if model_kwargs[\"past_key_values\"] is not None:\n\t                model_kwargs[\"past_key_values\"] = self._reorder_cache(\n", "                    model_kwargs[\"past_key_values\"], beam_idx\n\t                )\n\t            # increase cur_len\n\t            cur_len = cur_len + 1\n\t            yield input_ids\n\t            if beam_scorer.is_done or stopping_criteria(input_ids, None):\n\t                if not synced_gpus:\n\t                    break\n\t                else:\n\t                    this_peer_finished = True\n", "        final_result = beam_scorer.finalize(\n\t            input_ids,\n\t            beam_scores,\n\t            next_tokens,\n\t            next_indices,\n\t            pad_token_id=pad_token_id,\n\t            eos_token_id=eos_token_id,\n\t            max_length=stopping_criteria.max_length,\n\t            beam_indices=None,\n\t        )\n", "        yield final_result[\"sequences\"]\n\tclass StreamLlamaForCausalLM(LlamaForCausalLM, StreamGenerationMixin):\n\t    pass\n\tclass StreamPeftGenerationMixin(PeftModelForCausalLM, StreamGenerationMixin):\n\t    # default it call `model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config)`, not cls!! so inherent PeftModelForCausalLM is non sense\n\t    @classmethod\n\t    def from_pretrained(cls, model, model_id, adapter_name=\"default\", is_trainable=False,  **kwargs):\n\t        # work in peft==0.3.0\n\t        if peft.__version__ >= '0.3.0' and peft.__version__ != '0.3.0.dev0':\n\t            # load the config\n", "            from peft.utils import PromptLearningConfig\n\t            config = LoraConfig.from_pretrained(model_id)\n\t            if (getattr(model, \"hf_device_map\", None) is not None) and len(\n\t                set(model.hf_device_map.values()).intersection({\"cpu\", \"disk\"})\n\t            ) > 0:\n\t                remove_hook_from_submodules(model)\n\t            if isinstance(config, PromptLearningConfig) and is_trainable:\n\t                raise ValueError(\"Cannot set a prompt learning adapter to trainable when loading pretrained adapter.\")\n\t            else:\n\t                config.inference_mode = not is_trainable\n", "            # here is the hack\n\t            model = cls(model, config, adapter_name)\n\t            model.load_adapter(model_id, adapter_name, **kwargs)\n\t            # NOTICE\n\t            model.base_model_prepare_inputs_for_generation = model.base_model.prepare_inputs_for_generation\n\t            model._reorder_cache = model.base_model._reorder_cache\n\t            return model\n\t        else:\n\t            return cls.from_pretrained_old_peft_version(model, model_id, **kwargs)\n\t    @classmethod\n", "    def from_pretrained_old_peft_version(cls, model, model_id, **kwargs):\n\t        # work well in peft@e536616888d51b453ed354a6f1e243fecb02ea08\n\t        # load the config\n\t        config = LoraConfig.from_pretrained(model_id)\n\t        if getattr(model, \"hf_device_map\", None) is not None:\n\t            remove_hook_from_submodules(model)\n\t        # here is the hack\n\t        model = cls(model, config)\n\t        model._reorder_cache = model.base_model._reorder_cache\n\t        # load weights if any\n", "        if os.path.exists(os.path.join(model_id, \"adapter_model.bin\")):\n\t            filename = os.path.join(model_id, \"adapter_model.bin\")\n\t        else:\n\t            try:\n\t                filename = hf_hub_download(model_id, \"adapter_model.bin\")\n\t            except:  # noqa\n\t                raise ValueError(\n\t                    f\"Can't find weights for {model_id} in {model_id} or in the Hugging Face Hub. \"\n\t                    f\"Please check that the file {'adapter_model.bin'} is present at {model_id}.\"\n\t                )\n", "        adapters_weights = torch.load(\n\t            filename,\n\t            map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n\t        )\n\t        # load the weights into the model\n\t        model = set_peft_model_state_dict(model, adapters_weights)\n\t        if getattr(model, \"hf_device_map\", None) is not None:\n\t            device_map = kwargs.get(\"device_map\", \"auto\")\n\t            max_memory = kwargs.get(\"max_memory\", None)\n\t            no_split_module_classes = model._no_split_modules\n", "            if device_map != \"sequential\":\n\t                max_memory = get_balanced_memory(\n\t                    model,\n\t                    max_memory=max_memory,\n\t                    no_split_module_classes=no_split_module_classes,\n\t                    low_zero=(device_map == \"balanced_low_0\"),\n\t                )\n\t            if isinstance(device_map, str):\n\t                device_map = infer_auto_device_map(\n\t                    model,\n", "                    max_memory=max_memory,\n\t                    no_split_module_classes=no_split_module_classes,\n\t                )\n\t            model = dispatch_model(model, device_map=device_map)\n\t            hook = AlignDevicesHook(io_same_device=True)\n\t            if model.peft_config.peft_type == PeftType.LORA:\n\t                add_hook_to_module(model.base_model.model, hook)\n\t            else:\n\t                remove_hook_from_submodules(model.prompt_encoder)\n\t                add_hook_to_module(model.base_model, hook)\n", "        return model\n"]}
{"filename": "prompt.py", "chunked_list": ["import transformers\n\tfrom utils import printf\n\timport copy\n\tclass prompt:\n\t    def __init__(self, tokenizer, max_len, add_eos=True):\n\t        self.tokenizer = tokenizer\n\t        self.max_len = max_len\n\t        self.add_eos=add_eos\n\tclass instruct_prompt(prompt):\n\t    prompt = (\n", "        \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n\t        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n\t    )\n\t    prompt_input = (\n\t        \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n\t        \"### Instruction:{instruction}\\n\\n### Input:{input}\\n\\n### Response:\"\n\t    )\n\t    prompt_history = \"User:{input}\\n\\nAssistant:{output}\\n\\n\"\n\t    prompt_post = \"User:{input}\\n\\nAssistant:\"\n\t    def preprocess_gen(self, data_point):\n", "        if 'history' not in data_point:\n\t        # single instruction format {'instruction':..,'input':..}\n\t            if 'input' in data_point:\n\t                user_prompt = self.prompt_input.format_map(data_point)\n\t            else:\n\t                user_prompt = self.prompt.format_map(data_point)\n\t        else:\n\t        # multi turn format {'history':[..], 'input':[..]}\n\t            user_prompt = \"\\n\".join([\"User:\" + i['input']+\"\\n\"+\"Assistant:\" + i['output'] for i in data_point['history']]) + \"\\nUser:\" + data_point['input'] + \"\\nAssistant:\"\n\t            user_prompt = user_prompt[-self.max_len:]\n", "        user_prompt=self.prompt.format_map({'instruction':user_prompt})\n\t        input_ids = self.tokenizer(user_prompt)[\"input_ids\"]\n\t        return input_ids\n\t    def preprocess_train(self, data_point):\n\t        # single instruction format {'instruction':..,'input':..,'output':..}\n\t        if 'instruction' in data_point:\n\t            if 'input' in data_point:\n\t                user_prompt = self.prompt_input.format_map(data_point)\n\t            else:\n\t                user_prompt = self.prompt.format_map(data_point)\n", "            output = data_point[\"output\"]\n\t        # multi turn format {'input':[..], 'output':[..]}\n\t        else:\n\t            user_prompt = ''\n\t            lens = len(data_point['input'])\n\t            for i in range(lens-1):\n\t                user_prompt += self.prompt_history.format_map({'input':data_point['input'][i],'output':data_point['output'][i]})\n\t            user_prompt += self.prompt_post.format_map({'input':data_point['input'][-1]})\n\t            user_prompt = self.prompt.format_map({'instruction': user_prompt})\n\t            output = data_point['output'][-1]\n", "        len_user_prompt_tokens = (len(self.tokenizer(\n\t            user_prompt,\n\t            truncation=True,\n\t            max_length=self.max_len + 1,\n\t        )[\"input_ids\"])- 1)  # no eos token\n\t        full_tokens = self.tokenizer(\n\t            user_prompt + output,\n\t            truncation=True,\n\t            max_length=self.max_len + 1,\n\t            padding=\"max_length\",\n", "        )[\"input_ids\"][:-1]\n\t        return {\n\t            \"input_ids\": full_tokens,\n\t            \"labels\": [-100] * len_user_prompt_tokens\n\t            + full_tokens[len_user_prompt_tokens:],\n\t            \"attention_mask\": [1] * (len(full_tokens)),\n\t        }\n\t    def data_collator(self,):\n\t        return transformers.DataCollatorForLanguageModeling(self.tokenizer, mlm=False)\n\t    def postprocess(self, text, render=True):\n", "        #import pdb;pdb.set_trace()\n\t        printf(text)\n\t        output = text.split(\"### Response:\")[1].strip()\n\t        output = output.replace(\"Belle\", \"Vicuna\")\n\t        printf(output)\n\t        if '###' in output:\n\t            output = output.split(\"###\")[0]\n\t        if 'User' in output:\n\t            output = output.split(\"User\")[0]\n\t        output = output.replace('�','').replace('</s>', '') \n", "        if render:\n\t            # fix gradio chatbot markdown code render bug\n\t            lines = output.split(\"\\n\")\n\t            for i, line in enumerate(lines):\n\t                if \"```\" in line:\n\t                    if line != \"```\":\n\t                        lines[i] = f'<pre><code class=\"language-{lines[i][3:]}\">'\n\t                    else:\n\t                        lines[i] = '</code></pre>'\n\t                else:\n", "                    if i > 0:\n\t                        lines[i] = \"<br/>\" + line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"__\", '\\_\\_')\n\t            output =  \"\".join(lines)\n\t            # output = output.replace('<br/><pre>','\\n<pre>') work for html; but not for gradio\n\t        return output\n\tclass chat_prompt(prompt):\n\t    prompt_pre = (\n\t        \"The following is a conversation between an AI assistant called Assistant and a human user called User. \"\n\t        \"The assistant is intelligent, knowledgeable and polite to answer questions of user.\\n\\n\"\n\t    )\n", "    prompt_history = \"User:{input}\\n\\nAssistant:{output}\\n\\n\"\n\t    prompt_post = \"User:{input}\\n\\nAssistant:\"\n\t    def preprocess_gen(self, data_point):\n\t        user_prompt = self.prompt_pre\n\t        len_avail = self.max_len - len(self.tokenizer(user_prompt, add_special_tokens=False)['input_ids'])\n\t        input_prompt = self.prompt_post.format_map({'input':data_point['input']})\n\t        len_avail -= len(self.tokenizer(input_prompt, add_special_tokens=False)['input_ids'])\n\t        lens = len(data_point['history'])\n\t        tokenized_lens = []\n\t        for i in range(lens):\n", "            tmp_prompt = self.prompt_history.format_map(data_point['history'][i])\n\t            tokenized_lens.append(len(self.tokenizer(tmp_prompt,add_special_tokens=False)[\"input_ids\"]))\n\t        # 启发式：/2 优先除前面的\n\t        i = 0\n\t        while sum(tokenized_lens) > len_avail and i < lens:\n\t            history = data_point['history'][i]\n\t            tmp_len1 = len(history['input'])\n\t            tmp_len2 = len(history['output'])\n\t            if tmp_len2 > tmp_len1:\n\t                history['output'] = history['output'][:tmp_len2//2]\n", "            else:\n\t                history['input'] = history['input'][:tmp_len1//2]\n\t            prompt = self.prompt_history.format_map(history)\n\t            single_len =(len(self.tokenizer(prompt,add_special_tokens=False)[\"input_ids\"]))\n\t            tokenized_lens[i] = single_len\n\t            i += 1\n\t        total_len = sum(tokenized_lens)\n\t        # 还不够的话 直接截断\n\t        while total_len > len_avail and i < lens - 1 :\n\t            total_len -= tokenized_lens[i]\n", "            data_point['history'] = data_point['history'][1:]\n\t            i += 1\n\t        # 最终合并\n\t        for i in range(lens):\n\t            user_prompt += self.prompt_history.format_map(data_point['history'][i])\n\t        user_prompt += input_prompt\n\t        printf({'real_input:':user_prompt})\n\t        inputs = self.tokenizer(user_prompt)[\"input_ids\"]\n\t        return inputs\n\t    def preprocess_train(self, data_point):\n", "        user_prompt = self.prompt_pre\n\t        lens = len(data_point['input'])\n\t        for i in range(lens-1):\n\t            user_prompt += self.prompt_history.format_map({'input':data_point['input'][i].strip(),'output':data_point['output'][i].strip()})\n\t        user_prompt += self.prompt_post.format_map({'input':data_point['input'][-1].strip()})\n\t        len_user_prompt_tokens = len(self.tokenizer(\n\t            user_prompt,\n\t            truncation=True,\n\t            max_length=self.max_len,\n\t        )[\"input_ids\"]) - 1 # remove extra eos\n", "        if self.add_eos:\n\t            full_tokens = self.tokenizer(\n\t                user_prompt + data_point[\"output\"][-1].strip(),\n\t                truncation=True,\n\t                padding=False,\n\t                max_length=self.max_len,\n\t            )[\"input_ids\"] # need eos\n\t        else:\n\t            full_tokens = self.tokenizer(\n\t                user_prompt + data_point[\"output\"][-1].strip(),\n", "                truncation=True,\n\t                padding=False,\n\t                max_length=self.max_len+1,\n\t            )[\"input_ids\"][:-1] # delete eos\n\t        return {\n\t            \"input_ids\": full_tokens,\n\t            \"labels\": [-100] * len_user_prompt_tokens + full_tokens[len_user_prompt_tokens:],\n\t            \"attention_mask\": [1] * (len(full_tokens)),\n\t        }\n\t    def data_collator(self,):\n", "        return transformers.DataCollatorForSeq2Seq(self.tokenizer)\n\t    def postprocess(self, text, render=False):\n\t        output = text.split(\"Assistant:\")[-1].strip()\n\t        if 'User:' in output:\n\t            output = output.split(\"User:\")[0]\n\t        output = output.replace('�','') \n\t        if render:\n\t            # fix gradio chatbot markdown code render bug\n\t            lines = output.split(\"\\n\")\n\t            for i, line in enumerate(lines):\n", "                if \"```\" in line:\n\t                    if line != \"```\":\n\t                        lines[i] = f'<pre><code class=\"language-{lines[i][3:]}\">'\n\t                    else:\n\t                        lines[i] = '</code></pre>'\n\t                else:\n\t                    if i > 0:\n\t                        lines[i] = \"<br/>\" + line.replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\").replace(\"__\", '\\_\\_')\n\t            output =  \"\".join(lines)\n\t            # output = output.replace('<br/><pre>','\\n<pre>') work for html; but not for gradio\n", "        return output\n\t    def get_data_collator():\n\t        return transformers.DataCollatorForLanguageModeling\n"]}
{"filename": "finetune_chat.py", "chunked_list": ["from peft import (\n\t    prepare_model_for_int8_training,\n\t    LoraConfig,\n\t    PeftModel,\n\t    get_peft_model,\n\t    get_peft_model_state_dict,\n\t    set_peft_model_state_dict,\n\t)\n\tfrom transformers import LlamaForCausalLM, LlamaTokenizer, TrainerCallback, GenerationConfig\n\timport os\n", "import sys\n\timport torch\n\timport torch.nn as nn\n\timport bitsandbytes as bnb\n\tfrom datasets import load_dataset, Dataset\n\timport transformers\n\tfrom huggingface_hub import snapshot_download\n\timport argparse\n\timport warnings\n\tfrom tqdm import tqdm\n", "from functools import partial\n\timport utils\n\timport prompt\n\tassert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\t# 0. prepare args and logger\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--prompt_type\", type=str, default=\"chat\")\n", "parser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\n\tparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\n\tparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\n\tparser.add_argument(\"--num_epoch\", type=int, default=3)\n\tparser.add_argument(\"--micro_batch\", type=int, default=4)\n\tparser.add_argument(\"--total_batch\", type=int, default=128)\n\tparser.add_argument(\"--log_steps\", type=int, default=100)\n\tparser.add_argument(\"--eval_steps\", type=int, default=200)\n\tparser.add_argument(\"--save_steps\", type=int, default=200)\n\tparser.add_argument(\"--warmup_ratio\", type=float, default=0.05)\n", "parser.add_argument(\"--test_size\", type=int, default=200)\n\tparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n\tparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\n\tparser.add_argument(\"--ignore_data_skip\", type=bool, default=False)\n\targs = parser.parse_args()\n\tif not args.wandb:\n\t    os.environ[\"WANDB_MODE\"] = \"disable\"\n\tMICRO_BATCH_SIZE = args.micro_batch  # this could actually be 5 but i like powers of 2\n\tBATCH_SIZE = args.total_batch\n\tMAX_STEPS = None\n", "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n\tEPOCHS = args.num_epoch \n\tLEARNING_RATE = 3e-4  # the Karpathy constant\n\tCUTOFF_LEN = 2048  \n\tLORA_R = 8\n\tLORA_ALPHA = 16\n\tLORA_DROPOUT = 0.05\n\tUSE_8bit = True\n\tVAL_SET_SIZE = args.test_size  # 2000\n\tTARGET_MODULES = [\n", "    \"q_proj\",\n\t    \"v_proj\",\n\t    \"k_proj\",\n\t    \"o_proj\",\n\t    \"down_proj\",\n\t    \"gate_proj\",\n\t    \"up_proj\",\n\t]\n\tDATA_PATH = args.data_path  \n\tOUTPUT_DIR = args.output_path  # \"lora-Vicuna\"\n", "device_map = \"auto\"\n\tworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\tddp = world_size != 1\n\tif ddp:\n\t    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n\t    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n\t# we must make sure batch_size and gradient_accumulation_steps not changed for resuming training.\n\tif args.resume_from_checkpoint:\n\t    old_args_path = os.path.join(args.resume_from_checkpoint, 'training_args.bin')\n\t    if os.path.exists(old_args_path):\n", "        old_args = torch.load(old_args_path)\n\t        if MICRO_BATCH_SIZE != old_args.per_device_train_batch_size:\n\t            raise Exception(\n\t                f'current micro batch size {MICRO_BATCH_SIZE} is not equal to the old {old_args.per_device_train_batch_size},'\n\t                ' This will cause the trainer skips wrong epochs or steps.'\n\t                f'please change your micro batch size to {old_args.per_device_train_batch_size}'\n\t                ' or cancel resuming your training'\n\t                )\n\t        if GRADIENT_ACCUMULATION_STEPS != old_args.gradient_accumulation_steps:\n\t            raise Exception(\n", "                f'current total batch {BATCH_SIZE} is not equal to the old {old_args.gradient_accumulation_steps*old_args.per_device_train_batch_size},'\n\t                ' This will cause the trainer skips wrong epochs or steps.'\n\t                f'please change your total batch size to {old_args.gradient_accumulation_steps*old_args.per_device_train_batch_size}'    \n\t                ' or cancel resuming your training'\n\t            )\n\t    else:\n\t        raise Exception(f'{old_args_path} is not exist!')\n\t    # checkpoint = os.path.join(args.resume_from_checkpoint, 'pytorch_model.bin')\n\tlogger = utils.set_file_logger(__name__,OUTPUT_DIR)\n\t# 1. load dataset\n", "logger.info(f'>>> processing data from {DATA_PATH}')\n\tlogger.info(f'>>> using {args}')\n\ttrain_tokenizer = LlamaTokenizer.from_pretrained(args.model_path, add_eos_token=True)\n\tassert train_tokenizer.eos_token_id == 2, \"Tokenizer eos is wrong!!!\"\n\t# unk. we want this to be different from the eos token\n\ttrain_tokenizer.pad_token_id = 0  \n\t# cannot use eos in generation!\n\t# tokenizer.padding_side = \"left\"  # Allow batched inference\n\ttest_tokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\tif args.prompt_type == 'instruct':\n", "    PROMPT = prompt.instruct_prompt(train_tokenizer, CUTOFF_LEN)\n\telif args.prompt_type == 'chat':\n\t    PROMPT = prompt.chat_prompt(train_tokenizer,CUTOFF_LEN)\n\telse:\n\t    raise Exception('not support')\n\t# check tokenizer\n\tdata = load_dataset('json', data_files=DATA_PATH)\n\timport random;start = random.randint(1, 100)\n\texamples = Dataset.from_dict(data['train'][start:start+5]).map(PROMPT.preprocess_train)\n\tfor example in examples:\n", "    logger.info(f'>>> using prompt {args.prompt_type}, prompt example:\\n { train_tokenizer.decode(example[\"input_ids\"]) }')\n\t    logger.info(f'>>> tokenizer labels: { train_tokenizer.decode([ 0 if l==-100 else l for l in example[\"labels\"]])}')\n\t    logger.info(f'>>> tokenizer example: { example[\"input_ids\"][:10] }...{ example[\"input_ids\"][-10:]}')\n\t# 2. load model and checkpoints\n\tlogger.info(f'>>> load model from {args.model_path}')\n\tif USE_8bit is True:\n\t    assert bnb.__version__ >= '0.37.2', \"Please downgrade bitsandbytes's version, for example: pip install bitsandbytes==0.37.2\"\n\tmodel = LlamaForCausalLM.from_pretrained(\n\t    args.model_path,\n\t    load_in_8bit=USE_8bit,\n", "    device_map=device_map,\n\t    torch_dtype=torch.float16,\n\t)\n\tif USE_8bit is True:\n\t    model = prepare_model_for_int8_training(model)\n\tconfig = LoraConfig(\n\t    r=LORA_R,\n\t    lora_alpha=LORA_ALPHA,\n\t    target_modules=TARGET_MODULES,\n\t    lora_dropout=LORA_DROPOUT,\n", "    bias=\"none\",\n\t    task_type=\"CAUSAL_LM\",\n\t)\n\tmodel = get_peft_model(model, config)\n\tif args.resume_from_checkpoint:\n\t    checkpoint_name = os.path.join(args.resume_from_checkpoint, \"pytorch_model.bin\")\n\t    # adapter_model.bin\n\t    if not os.path.exists(checkpoint_name):\n\t        pytorch_bin_path = checkpoint_name\n\t        checkpoint_name = os.path.join(args.resume_from_checkpoint, \"adapter_model.bin\")\n", "        if os.path.exists(checkpoint_name):\n\t            os.rename(checkpoint_name, pytorch_bin_path)\n\t            logger.warning(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n\t        else:\n\t            args.resume_from_checkpoint = None  # So the trainer won't try loading its state\n\t    # pytorch_model.bin\n\t    if os.path.exists(checkpoint_name):\n\t        logger.info(f'>>> load lora from {checkpoint_name}')\n\t        adapters_weights = torch.load(checkpoint_name)\n\t        set_peft_model_state_dict(model, adapters_weights)\n", "    else:\n\t        raise Exception(f\"Checkpoint {checkpoint_name} not found with resume_from_checkpoint=True!\")\n\ttrainable_params = 0\n\tall_param = 0\n\tfor _, param in model.named_parameters():\n\t    num_params = param.numel()\n\t    # if using DS Zero 3 and the weights are initialized empty\n\t    if num_params == 0 and hasattr(param, \"ds_numel\"):\n\t        num_params = param.ds_numel\n\t    all_param += num_params\n", "    if param.requires_grad:\n\t        trainable_params += num_params\n\tlogger.info(f\">>> trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\")\n\t# 3. speedup dataset processing by multi-process\n\tnum_proc = (os.cpu_count())\n\tif VAL_SET_SIZE > 0:\n\t    train_val = data[\"train\"].train_test_split(test_size=VAL_SET_SIZE, shuffle=True, seed=42)\n\t    train_data = train_val[\"train\"].shuffle().map(PROMPT.preprocess_train, num_proc=num_proc)\n\t    val_data = train_val[\"test\"].shuffle().map(PROMPT.preprocess_train, num_proc=num_proc)\n\telse:\n", "    train_data = data[\"train\"].shuffle().map(PROMPT.preprocess_train, num_proc=num_proc)\n\t    val_data = None\n\tnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\n\tif args.resume_from_checkpoint:\n\t    # the trainer will ignore the state max_steps and caculate max_steps based on epochs,\n\t    # so we mannally set the args.max_step to override it. \n\t    if args.lora_remote_checkpoint is not None:\n\t        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n\t    train_state_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n\t    if os.path.exists(train_state_path):\n", "        import json\n\t        base_train_args = json.load(open(train_state_path, 'r'))\n\t        base_max_steps = base_train_args[\"max_steps\"]\n\t        resume_scale = base_max_steps / now_max_steps\n\t        if base_max_steps > now_max_steps:\n\t            logger.warning(f\"epoch {EPOCHS}:{MAX_STEPS} replace to the base_max_steps {base_max_steps}\")\n\t            EPOCHS = None\n\t            MAX_STEPS = base_max_steps\n\t        else:\n\t            MAX_STEPS = now_max_steps\n", "    assert MAX_STEPS is not None\n\telse:\n\t    MAX_STEPS = now_max_steps\n\t# 4. start training\n\tclass CustomCallback(TrainerCallback):\n\t    def __init__(self, trainer) -> None:\n\t        super().__init__()\n\t        self.trainer = trainer\n\t        self.generation_config = GenerationConfig(\n\t            temperature=1.0,\n", "            top_p=0.75,\n\t            top_k=40,\n\t            num_beams=2,\n\t            bos_token_id=train_tokenizer.bos_token_id,\n\t            eos_token_id=train_tokenizer.eos_token_id,\n\t            pad_token_id=train_tokenizer.pad_token_id,\n\t            max_new_tokens=1024, # max_length=max_new_tokens+input_sequence\n\t            min_new_tokens=1, # min_length=min_new_tokens+input_sequence\n\t            bad_words_ids=test_tokenizer(['\\n\\nUser:','\\n\\nAssistant:'], add_special_tokens=False).input_ids\n\t        )\n", "        self.repetition_penalty=1.3\n\t        self.logger = utils.set_file_logger('transformers.trainer', trainer.args.output_dir)\n\t    def on_log(self, args, state, control, logs, **kwargs):\n\t        logger.info(logs)\n\ttrainer = transformers.Trainer(\n\t    model=model,\n\t    train_dataset=train_data,\n\t    eval_dataset=val_data,\n\t    args=transformers.TrainingArguments(\n\t        per_device_train_batch_size=MICRO_BATCH_SIZE,\n", "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n\t        warmup_ratio=args.warmup_ratio,\n\t        num_train_epochs=EPOCHS,\n\t        max_steps=MAX_STEPS,\n\t        learning_rate=LEARNING_RATE,\n\t        fp16=True,\n\t        logging_steps=args.log_steps,\n\t        logging_first_step=True, # convenient\n\t        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n\t        save_strategy=\"steps\",\n", "        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n\t        save_steps=args.save_steps,\n\t        output_dir=OUTPUT_DIR,\n\t        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n\t        ddp_find_unused_parameters=False if ddp else None,\n\t        report_to=\"wandb\" if args.wandb else [],\n\t        ignore_data_skip=args.ignore_data_skip,\n\t    ),\n\t    data_collator=PROMPT.data_collator()\n\t)\n", "trainer.add_callback(CustomCallback(trainer))\n\tmodel.config.use_cache = False\n\told_state_dict = model.state_dict\n\tmodel.state_dict = (\n\t    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n\t).__get__(model, type(model))\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\ttrainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n\tmodel.save_pretrained(OUTPUT_DIR)\n"]}
{"filename": "test_tokenizer.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\timport transformers\n\timport argparse\n\tfrom transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--model_path\", type=str, default=\"yahma/llama-7b-hf\") #yahma/llama-7b-hf #decapoda-research/llama-7b-hf\n\targs = parser.parse_args()\n\ttokenizer = LlamaTokenizer.from_pretrained(\n", "    args.model_path, add_eos_token=True\n\t)\n\ttest_text = [\"Hello, nice to meet you!\", \"你好很高兴能见到你！\"]\n\tfor text in test_text:\n\t    input_ids = tokenizer.encode(text)\n\t    print(f\"input_ids: {input_ids}\")\n\t    decode_text = tokenizer.decode(input_ids)\n\t    print(f\"decode_text: {decode_text}\")\n\t\"\"\"\n\tCorrect ==>  yahma/llama-7b-hf + newest Transformers(>=4.28.1):\n", "> !!! Beginning with 1 (bos), ending with 2 (eos) !!!\n\tinput_ids: [1, 15043, 29892, 7575, 304, 5870, 366, 29991, 2]\n\tdecode_text: <s> Hello, nice to meet you!</s>\n\tinput_ids: [1, 29871, 30919, 31076, 232, 193, 139, 30528, 31914, 30815, 235, 170, 132, 30780, 30919, 30584, 2]\n\tdecode_text: <s> 你好很高兴能见到你！</s>\n\tCorrect ==> decapoda-research/llama-7b-hf + Old Transformers like our version(transformers @ git+https://github.com/huggingface/transformers.git@0dcb46e7a4a9e587ba84ff35778ab4233a184c11)\n\tinput_ids: [1, 15043, 29892, 7575, 304, 5870, 366, 29991, 2]\n\tdecode_text:  Hello, nice to meet you!\n\tinput_ids: [1, 29871, 30919, 31076, 232, 193, 139, 30528, 31914, 30815, 235, 170, 132, 30780, 30919, 30584, 2]\n\tdecode_text:  你好很高兴能见到你！\n", "Correct ==> decapoda-research/llama-7b-hf + Old Transformers like our version(transformers @ git+https://github.com/huggingface/transformers.git@0dcb46e7a4a9e587ba84ff35778ab4233a184c11)\n\tinput_ids: [1, 15043, 29892, 7575, 304, 5870, 366, 29991, 2]\n\tdecode_text:  Hello, nice to meet you!\n\tinput_ids: [1, 29871, 30919, 31076, 232, 193, 139, 30528, 31914, 30815, 235, 170, 132, 30780, 30919, 30584, 2]\n\tdecode_text:  你好很高兴能见到你！\n\t老版本transformers的问题：代码默认加载tokenizer.model\n\t新版本transformers的修改：新版本默认加载config\n\tdecapoda-research：config的bos=0，eos=1（×），tokenizer.model是正确的\n\tyahma：config的bos=1，eos=2，tokenizer.model是正确的\n\t\"\"\""]}
{"filename": "chat.py", "chunked_list": ["import sys\n\timport torch\n\tfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\n\timport transformers\n\timport json\n\timport gradio as gr\n\timport argparse\n\timport warnings\n\timport os\n\tfrom datetime import datetime\n", "from utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM, printf\n\timport utils\n\timport copy\n\tassert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\timport prompt\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\n", "parser.add_argument(\"--lora_path\", type=str, default='')\n\tparser.add_argument(\"--use_typewriter\", type=int, default=1)\n\tparser.add_argument(\"--prompt_type\", type=str, default='chat')\n\tparser.add_argument(\"--share_link\", type=int, default=0)\n\tparser.add_argument(\"--show_beam\", type=int, default=0)\n\tparser.add_argument(\"--int8\", type=int, default=1)\n\targs = parser.parse_args()\n\targs.fix_token = True\n\tprintf('>>> args:', args)\n\ttokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n", "LOAD_8BIT = args.int8\n\tBASE_MODEL = args.model_path\n\tLORA_WEIGHTS = args.lora_path\n\t# fix the path for local checkpoint\n\tlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\n\tif args.lora_path != '' and os.path.exists(args.lora_path):\n\t    if not os.path.exists(lora_bin_path):\n\t        pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n\t        printf('>>> load lora from', pytorch_bin_path)\n\t        if os.path.exists(pytorch_bin_path):\n", "            os.rename(pytorch_bin_path, lora_bin_path)\n\t            warnings.warn(\n\t                \"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\"\n\t            )\n\t        else:\n\t            assert ('Checkpoint is not Found!')\n\t    else:\n\t        printf('>>> load lora from', lora_bin_path)\n\telse:\n\t    printf('>>> load lora from huggingface url', args.lora_path)\n", "if torch.cuda.is_available():\n\t    device = \"cuda\"\n\telse:\n\t    device = \"cpu\"\n\ttry:\n\t    if torch.backends.mps.is_available():\n\t        device = \"mps\"\n\texcept:\n\t    pass\n\tif device == \"cuda\":\n", "    print(f'>>> load raw models from {BASE_MODEL}')\n\t    if args.lora_path == \"\":\n\t        model = StreamLlamaForCausalLM.from_pretrained(\n\t            BASE_MODEL,\n\t            load_in_8bit=LOAD_8BIT,\n\t            torch_dtype=torch.float16,\n\t            device_map={\"\": 0},\n\t        )    \n\t    else:\n\t        print(f'>>> load lora models from {LORA_WEIGHTS}')\n", "        model = LlamaForCausalLM.from_pretrained(\n\t            BASE_MODEL,\n\t            load_in_8bit=LOAD_8BIT,\n\t            torch_dtype=torch.float16,\n\t            device_map={\"\": 0},\n\t        )\n\t        model = StreamPeftGenerationMixin.from_pretrained(\n\t                model, LORA_WEIGHTS, torch_dtype=torch.float16, load_in_8bit=LOAD_8BIT,  device_map={\"\": 0}\n\t        )\n\telif device == \"mps\":\n", "    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL,\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16,\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n\t        model,\n\t        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16,\n", "    )\n\telse:\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n\t        model,\n\t        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t    )\n", "# fix tokenizer bug\n\tif args.fix_token and tokenizer.eos_token_id != 2:\n\t    warnings.warn(\n\t        \"The tokenizer eos token may be wrong. please check you llama-checkpoint\"\n\t    )\n\t    model.config.bos_token_id = tokenizer.bos_token_id = 1\n\t    model.config.eos_token_id = tokenizer.eos_token_id = 2\n\tmodel.config.pad_token_id = tokenizer.pad_token_id = 0  # same as unk token id\n\tif not LOAD_8BIT:\n\t    model.half()  # seems to fix bugs for some users.\n", "model.eval()\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\tdef save(\n\t    inputs,\n\t    history,\n\t    temperature=0.1,\n\t    top_p=0.75,\n\t    top_k=40,\n\t    num_beams=4,\n", "    max_new_tokens=128,\n\t    min_new_tokens=1,\n\t    repetition_penalty=2.0,\n\t    max_memory=1024,\n\t    do_sample=False,\n\t    prompt_type='0',\n\t    **kwargs, \n\t):\n\t    history = [] if history is None else history\n\t    data_point = {}\n", "    if prompt_type == 'instruct':\n\t        PROMPT = prompt.instruct_prompt(tokenizer,max_memory)\n\t    elif prompt_type == 'chat':\n\t        PROMPT = prompt.chat_prompt(tokenizer,max_memory)\n\t    else:\n\t        raise Exception('not support')\n\t    data_point['history'] = history\n\t    # 实际上是每一步都可以不一样，这里只保存最后一步\n\t    data_point['generation_parameter'] = {\n\t        \"temperature\":temperature,\n", "        \"top_p\":top_p,\n\t        \"top_k\":top_k,\n\t        \"num_beams\":num_beams,\n\t        \"bos_token_id\":tokenizer.bos_token_id,\n\t        \"eos_token_id\":tokenizer.eos_token_id,\n\t        \"pad_token_id\":tokenizer.pad_token_id,\n\t        \"max_new_tokens\":max_new_tokens,\n\t        \"min_new_tokens\":min_new_tokens, \n\t        \"do_sample\":do_sample,\n\t        \"repetition_penalty\":repetition_penalty,\n", "        \"max_memory\":max_memory,\n\t    }\n\t    data_point['info'] = args.__dict__\n\t    print(data_point)\n\t    if args.int8:\n\t        file_name = f\"{args.lora_path}/{args.prompt_type.replace(' ','_')}_int8.jsonl\"\n\t    else:\n\t        file_name = f\"{args.lora_path}/{args.prompt_type.replace(' ','_')}_fp16.jsonl\"\n\t    utils.to_jsonl([data_point], file_name)\n\tdef evaluate(\n", "    inputs,\n\t    history,\n\t    temperature=0.1,\n\t    top_p=0.75,\n\t    top_k=40,\n\t    num_beams=4,\n\t    max_new_tokens=128,\n\t    min_new_tokens=1,\n\t    repetition_penalty=2.0,\n\t    max_memory=1024,\n", "    do_sample=False,\n\t    prompt_type='0',\n\t    **kwargs,\n\t):\n\t    history = [] if history is None else history\n\t    data_point = {}\n\t    if prompt_type == 'instruct':\n\t        PROMPT = prompt.instruct_prompt(tokenizer,max_memory)\n\t    elif prompt_type == 'chat':\n\t        PROMPT = prompt.chat_prompt(tokenizer,max_memory)\n", "    else:\n\t        raise Exception('not support')\n\t    data_point['history'] = copy.deepcopy(history)\n\t    data_point['input'] = inputs\n\t    input_ids = PROMPT.preprocess_gen(data_point)\n\t    printf('------------------------------')\n\t    printf(tokenizer.decode(input_ids))\n\t    input_ids = torch.tensor([input_ids]).to(device) # batch=1\n\t    printf('------------------------------')\n\t    printf('shape',input_ids.size())\n", "    printf('------------------------------')\n\t    generation_config = GenerationConfig(\n\t        temperature=temperature,\n\t        top_p=top_p,\n\t        top_k=top_k,\n\t        num_beams=num_beams,\n\t        bos_token_id=tokenizer.bos_token_id,\n\t        eos_token_id=tokenizer.eos_token_id,\n\t        pad_token_id=tokenizer.pad_token_id,\n\t        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n", "        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n\t        do_sample=do_sample,\n\t        bad_words_ids=tokenizer(['\\n\\nUser:','\\n\\nAssistant:'], add_special_tokens=False).input_ids,\n\t        **kwargs,\n\t    )\n\t    return_text = [(item['input'], item['output']) for item in history]\n\t    out_memory =False\n\t    outputs = None\n\t    with torch.no_grad():\n\t        # 流式输出 / 打字机效果\n", "        # streamly output / typewriter style\n\t        if args.use_typewriter:\n\t            try:\n\t                for generation_output in model.stream_generate(\n\t                    input_ids=input_ids,\n\t                    generation_config=generation_config,\n\t                    return_dict_in_generate=True,\n\t                    output_scores=False,\n\t                    repetition_penalty=float(repetition_penalty),\n\t                ):\n", "                    gen_token = generation_output[0][-1].item()\n\t                    printf(gen_token, end='(')\n\t                    printf(tokenizer.decode(gen_token), end=') ')\n\t                    outputs = tokenizer.batch_decode(generation_output)\n\t                    if args.show_beam:\n\t                        show_text = \"\\n--------------------------------------------\\n\".join(\n\t                            [ PROMPT.postprocess(output)+\" ▌\" for output in outputs]\n\t                        )\n\t                    else:\n\t                        show_text = PROMPT.postprocess(outputs[0])+\" ▌\"\n", "                    yield return_text +[(inputs, show_text)], history\n\t            except torch.cuda.OutOfMemoryError:\n\t                print('CUDA out of memory')\n\t                import gc\n\t                gc.collect()\n\t                torch.cuda.empty_cache()\n\t                out_memory=True\n\t            # finally only one\n\t            printf('[EOS]', end='\\n')\n\t            show_text = PROMPT.postprocess(outputs[0] if outputs is not None else '### Response:')\n", "            return_len = len(show_text)\n\t            if out_memory==True:\n\t                out_memory=False\n\t                show_text+= '<p style=\"color:#FF0000\"> [GPU Out Of Memory] </p> '\n\t            if return_len > 0:\n\t                output = PROMPT.postprocess(outputs[0], render=False)\n\t                history.append({\n\t                    'input': inputs,\n\t                    'output': output,\n\t                })\n", "            return_text += [(inputs, show_text)]\n\t            yield return_text, history\n\t        # common \n\t        else:\n\t            try:\n\t                generation_output = model.generate(\n\t                    input_ids=input_ids,\n\t                    generation_config=generation_config,\n\t                    return_dict_in_generate=True,\n\t                    output_scores=True,\n", "                    max_new_tokens=max_new_tokens,\n\t                    repetition_penalty=float(repetition_penalty),\n\t                )\n\t                s = generation_output.sequences[0]\n\t                output = tokenizer.decode(s)\n\t                output = PROMPT.postprocess(output)\n\t                history.append({\n\t                    'input': inputs,\n\t                    'output': output,\n\t                })\n", "                return_text += [(inputs, output)]\n\t                yield return_text, history\n\t            except torch.cuda.OutOfMemoryError:\n\t                import gc\n\t                gc.collect()\n\t                torch.cuda.empty_cache()\n\t                show_text = '<p style=\"color:#FF0000\"> [GPU Out Of Memory] </p> '\n\t                printf(show_text)\n\t                return_text += [(inputs, show_text)]\n\t                yield return_text, history\n", "def clear():\n\t    import gc\n\t    gc.collect()\n\t    torch.cuda.empty_cache()\n\t    return None, None\n\t# gr.Interface对chatbot的clear有bug，因此我们重新实现了一个基于gr.block的UI逻辑\n\t# gr.Interface has bugs to clear chatbot's history,so we customly implement it based on gr.block\n\twith gr.Blocks() as demo:\n\t    fn = evaluate\n\t    title = gr.Markdown(\n", "        \"<h1 style='text-align: center; margin-bottom: 1rem'>\"\n\t        + \"Chinese-Vicuna 中文小羊驼\"\n\t        + \"</h1>\"\n\t    )\n\t    description = gr.Markdown(\n\t        \"中文小羊驼由各种高质量的开源instruction数据集，结合Alpaca-lora的代码训练而来，模型基于开源的llama7B，主要贡献是对应的lora模型。由于代码训练资源要求较小，希望为llama中文lora社区做一份贡献。\"\n\t    )\n\t    history = gr.components.State()\n\t    with gr.Row().style(equal_height=False):\n\t        with gr.Column(variant=\"panel\"):\n", "            input_component_column = gr.Column()\n\t            with input_component_column:\n\t                input = gr.components.Textbox(\n\t                    lines=2, label=\"Input\", placeholder=\"请输入问题.\"\n\t                )\n\t                temperature = gr.components.Slider(minimum=0, maximum=1, value=1.0, label=\"Temperature\")\n\t                topp = gr.components.Slider(minimum=0, maximum=1, value=0.9, label=\"Top p\")\n\t                topk = gr.components.Slider(minimum=0, maximum=100, step=1, value=60, label=\"Top k\")\n\t                beam_number = gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\")\n\t                max_new_token = gr.components.Slider(\n", "                    minimum=1, maximum=2048, step=1, value=256, label=\"Max New Tokens\"\n\t                )\n\t                min_new_token = gr.components.Slider(\n\t                    minimum=1, maximum=1024, step=1, value=5, label=\"Min New Tokens\"\n\t                )\n\t                repeat_penal = gr.components.Slider(\n\t                    minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n\t                )\n\t                max_memory = gr.components.Slider(\n\t                    minimum=0, maximum=2048, step=1, value=2048, label=\"Max Memory\"\n", "                )\n\t                do_sample = gr.components.Checkbox(label=\"Use sample\")\n\t                # must be str, not number !\n\t                type_of_prompt = gr.components.Dropdown(\n\t                    ['instruct', 'chat'], value=args.prompt_type, label=\"Prompt Type\", info=\"select the specific prompt; use after clear history\"\n\t                )\n\t                input_components = [\n\t                    input, history, temperature, topp, topk, beam_number, max_new_token, min_new_token, repeat_penal, max_memory, do_sample, type_of_prompt\n\t                ]\n\t                input_components_except_states = [input, temperature, topp, topk, beam_number, max_new_token, min_new_token, repeat_penal, max_memory, do_sample, type_of_prompt]\n", "            with gr.Row():\n\t                cancel_btn = gr.Button('Cancel')\n\t                submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n\t                stop_btn = gr.Button(\"Stop\", variant=\"stop\", visible=False)\n\t            with gr.Row():\n\t                reset_btn = gr.Button(\"Reset Parameter\")\n\t                clear_history = gr.Button(\"Clear History\")\n\t        with gr.Column(variant=\"panel\"):\n\t            chatbot = gr.Chatbot().style(height=1024)\n\t            output_components = [ chatbot, history ]  \n", "            with gr.Row():\n\t                save_btn = gr.Button(\"Save Chat\")\n\t        def wrapper(*args):\n\t            # here to support the change between the stop and submit button\n\t            try:\n\t                for output in fn(*args):\n\t                    output = [o for o in output]\n\t                    # output for output_components, the rest for [button, button]\n\t                    yield output + [\n\t                        gr.Button.update(visible=False),\n", "                        gr.Button.update(visible=True),\n\t                    ]\n\t            finally:\n\t                yield [{'__type__': 'generic_update'}, {'__type__': 'generic_update'}] + [ gr.Button.update(visible=True), gr.Button.update(visible=False)]\n\t        def cancel(history, chatbot):\n\t            if history == []:\n\t                return (None, None)\n\t            return history[:-1], chatbot[:-1]\n\t        extra_output = [submit_btn, stop_btn]\n\t        save_btn.click(\n", "            save, \n\t            input_components, \n\t            None, \n\t        )\n\t        pred = submit_btn.click(\n\t            wrapper, \n\t            input_components, \n\t            output_components + extra_output, \n\t            api_name=\"predict\",\n\t            scroll_to_output=True,\n", "            preprocess=True,\n\t            postprocess=True,\n\t            batch=False,\n\t            max_batch_size=4,\n\t        )\n\t        submit_btn.click(\n\t            lambda: (\n\t                submit_btn.update(visible=False),\n\t                stop_btn.update(visible=True),\n\t            ),\n", "            inputs=None,\n\t            outputs=[submit_btn, stop_btn],\n\t            queue=False,\n\t        )\n\t        stop_btn.click(\n\t            lambda: (\n\t                submit_btn.update(visible=True),\n\t                stop_btn.update(visible=False),\n\t            ),\n\t            inputs=None,\n", "            outputs=[submit_btn, stop_btn],\n\t            cancels=[pred],\n\t            queue=False,\n\t        )\n\t        cancel_btn.click(\n\t            cancel,\n\t            inputs=[history, chatbot],\n\t            outputs=[history, chatbot]\n\t        )\n\t        reset_btn.click(\n", "            None, \n\t            [],\n\t            (\n\t                # input_components ; don't work for history...\n\t                input_components_except_states\n\t                + [input_component_column]\n\t            ),  # type: ignore\n\t            _js=f\"\"\"() => {json.dumps([\n\t                getattr(component, \"cleared_value\", None) for component in input_components_except_states ] \n\t                + ([gr.Column.update(visible=True)])\n", "                + ([])\n\t            )}\n\t            \"\"\",\n\t        )\n\t        clear_history.click(clear, None, [history, chatbot], queue=False)\n\tdemo.queue().launch(share=args.share_link)"]}
{"filename": "finetune_4bit.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\timport torch.nn as nn\n\timport bitsandbytes as bnb\n\tfrom datasets import load_dataset, Dataset\n\timport transformers\n\timport argparse\n\timport warnings\n\tfrom huggingface_hub import snapshot_download\n", "assert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n\tfrom peft import (\n\t    prepare_model_for_kbit_training,\n\t    LoraConfig,\n\t    get_peft_model,\n\t    get_peft_model_state_dict,\n\t    set_peft_model_state_dict,\n", ")\n\tdef generate_prompt(data_point):\n\t    # sorry about the formatting disaster gotta move fast\n\t    if data_point[\"input\"]:\n\t        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Input:\n\t{data_point[\"input\"]}\n\t### Response:\n", "{data_point[\"output\"]}\"\"\"\n\t    else:\n\t        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Response:\n\t{data_point[\"output\"]}\"\"\"\n\tdef tokenize(prompt):\n\t    # there's probably a way to do this with the tokenizer settings\n\t    # but again, gotta move fast\n", "    result = tokenizer(\n\t        prompt,\n\t        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t        padding=\"max_length\",\n\t    )\n\t    return {\n\t        \"input_ids\": result[\"input_ids\"][:-1],\n\t        \"attention_mask\": result[\"attention_mask\"][:-1],\n\t    }\n", "def generate_and_tokenize_prompt(data_point):\n\t    # This function masks out the labels for the input,\n\t    # so that our loss is computed only on the response.\n\t    user_prompt = (\n\t        (\n\t            f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Input:\n\t{data_point[\"input\"]}\n", "### Response:\n\t\"\"\"\n\t        )\n\t        if data_point[\"input\"]\n\t        else (\n\t            f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{data_point[\"instruction\"]}\n\t### Response:\n\t\"\"\"\n", "        )\n\t    )\n\t    len_user_prompt_tokens = (\n\t        len(\n\t            tokenizer(\n\t                user_prompt,\n\t                truncation=True,\n\t                max_length=CUTOFF_LEN + 1,\n\t            )[\"input_ids\"]\n\t        )\n", "        - 1\n\t    )  # no eos token\n\t    full_tokens = tokenizer(\n\t        user_prompt + data_point[\"output\"],\n\t        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t        padding=\"max_length\",\n\t    )[\"input_ids\"][:-1]\n\t    return {\n\t        \"input_ids\": full_tokens,\n", "        \"labels\": [-100] * len_user_prompt_tokens\n\t        + full_tokens[len_user_prompt_tokens:],\n\t        \"attention_mask\": [1] * (len(full_tokens)),\n\t    }\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\n\tparser.add_argument(\"--data_path\", type=str, default=\"merge.json\")\n\tparser.add_argument(\"--output_path\", type=str, default=\"lora-Vicuna\")\n\tparser.add_argument(\"--model_path\", type=str, default=\"decapoda-research/llama-7b-hf\")\n\tparser.add_argument(\"--eval_steps\", type=int, default=200)\n", "parser.add_argument(\"--save_steps\", type=int, default=200)\n\tparser.add_argument(\"--test_size\", type=int, default=200)\n\tparser.add_argument(\"--resume_from_checkpoint\", type=str, default=None)\n\tparser.add_argument(\"--lora_remote_checkpoint\", type=str, default=None)\n\tparser.add_argument(\"--ignore_data_skip\", type=str, default=\"False\")\n\targs = parser.parse_args()\n\tif not args.wandb:\n\t    os.environ[\"WANDB_MODE\"] = \"disable\"\n\t# optimized for RTX 4090. for larger GPUs, increase some of these?\n\tMICRO_BATCH_SIZE = 8  # this could actually be 5 but i like powers of 2\n", "BATCH_SIZE = 128\n\tMAX_STEPS = None\n\tGRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n\tEPOCHS = 3  # we don't always need 3 tbh\n\tLEARNING_RATE = 3e-4  # the Karpathy constant\n\tCUTOFF_LEN = 256  # 256 accounts for about 96% of the data\n\tLORA_R = 8\n\tLORA_ALPHA = 16\n\tLORA_DROPOUT = 0.05\n\tVAL_SET_SIZE = args.test_size #2000\n", "TARGET_MODULES = [\n\t    \"q_proj\",\n\t    \"v_proj\",\n\t]\n\tDATA_PATH = args.data_path \n\tOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\tdevice_map = {\"\": 0} #\"auto\"\n\tworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\tddp = world_size != 1\n\tif ddp:\n", "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n\t    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n\tprint(args.model_path)\n\tmodel = LlamaForCausalLM.from_pretrained(\n\t    args.model_path,\n\t    load_in_4bit=True,\n\t    device_map=device_map,\n\t)\n\ttokenizer = LlamaTokenizer.from_pretrained(\n\t    args.model_path, add_eos_token=True\n", ")\n\tmodel.gradient_checkpointing_enable()\n\tmodel = prepare_model_for_kbit_training(model)\n\tconfig = LoraConfig(\n\t    r=LORA_R,\n\t    lora_alpha=LORA_ALPHA,\n\t    target_modules=TARGET_MODULES,\n\t    lora_dropout=LORA_DROPOUT,\n\t    bias=\"none\",\n\t    task_type=\"CAUSAL_LM\",\n", ")\n\tmodel = get_peft_model(model, config)\n\ttokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n\t#tokenizer.padding_side = \"left\"  # Allow batched inference\n\tdata = load_dataset(\"json\", data_files=DATA_PATH)\n\timport random;start = random.randint(1, 100)\n\texamples = Dataset.from_dict(data['train'][start:start+5]).map(generate_and_tokenize_prompt)\n\tfor example in examples:\n\t    print(f'>>> prompt example:\\n { tokenizer.decode(example[\"input_ids\"]) }')\n\t    print(f'>>> tokenizer labels: { tokenizer.decode([ 0 if l==-100 else l for l in example[\"labels\"]])}')\n", "    print(f'>>> tokenizer example: { example[\"input_ids\"][:250] }...{ example[\"input_ids\"][-10:]}')\n\tnow_max_steps = max((len(data[\"train\"]) - VAL_SET_SIZE) // BATCH_SIZE * EPOCHS, EPOCHS)\n\tif args.resume_from_checkpoint:\n\t    if args.lora_remote_checkpoint is not None:\n\t        snapshot_download(repo_id=args.lora_remote_checkpoint, allow_patterns=[\"*.pt\", \"*.bin\", \"*.json\"], local_dir=args.resume_from_checkpoint)\n\t    # Check the available weights and load them\n\t    checkpoint_name = os.path.join(\n\t        args.resume_from_checkpoint, \"pytorch_model.bin\"\n\t    )  # Full checkpoint\n\t    if not os.path.exists(checkpoint_name):\n", "        pytorch_bin_path = checkpoint_name\n\t        checkpoint_name = os.path.join(\n\t            args.resume_from_checkpoint, \"adapter_model.bin\"\n\t        )  # only LoRA model - LoRA config above has to fit\n\t        if os.path.exists(checkpoint_name):\n\t            os.rename(checkpoint_name, pytorch_bin_path)\n\t            warnings.warn(\"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n\t        else:\n\t            args.resume_from_checkpoint = (\n\t                None  # So the trainer won't try loading its state\n", "            )\n\t    # The two files above have a different name depending on how they were saved, but are actually the same.\n\t    if os.path.exists(checkpoint_name):\n\t        print(f\"Restarting from {checkpoint_name}\")\n\t        adapters_weights = torch.load(checkpoint_name)\n\t        model = set_peft_model_state_dict(model, adapters_weights)\n\t    else:\n\t        print(f\"Checkpoint {checkpoint_name} not found\")\n\t    train_args_path = os.path.join(args.resume_from_checkpoint, \"trainer_state.json\")\n\t    if os.path.exists(train_args_path):\n", "        import json\n\t        base_train_args = json.load(open(train_args_path, 'r'))\n\t        base_max_steps = base_train_args[\"max_steps\"]\n\t        resume_scale = base_max_steps / now_max_steps\n\t        if base_max_steps > now_max_steps:\n\t            warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(EPOCHS, base_max_steps))\n\t            EPOCHS = None\n\t            MAX_STEPS = base_max_steps\n\t        else:\n\t            MAX_STEPS = now_max_steps\n", "else:\n\t    MAX_STEPS = now_max_steps\n\tmodel.print_trainable_parameters()\n\tnum_proc = (os.cpu_count())\n\tif VAL_SET_SIZE > 0:\n\t    train_val = data[\"train\"].train_test_split(\n\t        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n\t    )\n\t    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt, num_proc=num_proc)\n\t    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt, num_proc=num_proc)\n", "else:\n\t    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt, num_proc=num_proc)\n\t    val_data = None\n\ttrainer = transformers.Trainer(\n\t    model=model,\n\t    train_dataset=train_data,\n\t    eval_dataset=val_data,\n\t    args=transformers.TrainingArguments(\n\t        per_device_train_batch_size=MICRO_BATCH_SIZE,\n\t        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n", "        warmup_steps=100,\n\t        num_train_epochs=EPOCHS,\n\t        max_steps=MAX_STEPS,\n\t        learning_rate=LEARNING_RATE,\n\t        fp16=True,\n\t        logging_steps=20,\n\t        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n\t        save_strategy=\"steps\",\n\t        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n\t        save_steps=args.save_steps,\n", "        output_dir=OUTPUT_DIR,\n\t        save_total_limit=30,\n\t        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n\t        ddp_find_unused_parameters=False if ddp else None,\n\t        report_to=\"wandb\" if args.wandb else [],\n\t        ignore_data_skip=args.ignore_data_skip,\n\t        optim=\"paged_adamw_8bit\",\n\t    ),\n\t    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\t)\n", "model.config.use_cache = False\n\told_state_dict = model.state_dict\n\tmodel.state_dict = (\n\t    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n\t).__get__(model, type(model))\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\tprint(\"\\n If there's a warning about missing keys above, please disregard :)\")\n\ttrainer.train(resume_from_checkpoint=args.resume_from_checkpoint)\n\tmodel.save_pretrained(OUTPUT_DIR)\n"]}
{"filename": "generate_4bit.py", "chunked_list": ["import sys\n\timport torch\n\tfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\n\timport transformers\n\timport gradio as gr\n\timport argparse\n\timport warnings\n\timport os\n\tfrom utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM\n\tassert (\n", "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig, BitsAndBytesConfig\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--model_path\", type=str, default=\"/model/13B_hf\")\n\tparser.add_argument(\"--lora_path\", type=str, default=\"checkpoint-3000\")\n\tparser.add_argument(\"--use_typewriter\", type=int, default=1)\n\tparser.add_argument(\"--use_local\", type=int, default=1)\n\targs = parser.parse_args()\n\tprint(args)\n", "tokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\tLOAD_8BIT = True\n\tBASE_MODEL = args.model_path\n\tLORA_WEIGHTS = args.lora_path\n\t# fix the path for local checkpoint\n\tlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\n\tprint(lora_bin_path)\n\tif not os.path.exists(lora_bin_path) and args.use_local:\n\t    pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n\t    print(pytorch_bin_path)\n", "    if os.path.exists(pytorch_bin_path):\n\t        os.rename(pytorch_bin_path, lora_bin_path)\n\t        warnings.warn(\n\t            \"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\"\n\t        )\n\t    else:\n\t        assert ('Checkpoint is not Found!')\n\tif torch.cuda.is_available():\n\t    device = \"cuda\"\n\telse:\n", "    device = \"cpu\"\n\ttry:\n\t    if torch.backends.mps.is_available():\n\t        device = \"mps\"\n\texcept:\n\t    pass\n\tbnb_config = BitsAndBytesConfig(\n\t    load_in_4bit=True,\n\t    bnb_4bit_use_double_quant=True,\n\t    bnb_4bit_quant_type=\"nf4\",\n", "    bnb_4bit_compute_dtype=torch.float16\n\t)\n\tif device == \"cuda\":\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL,\n\t        quantization_config=bnb_config,\n\t        torch_dtype=torch.float16,\n\t        device_map=\"auto\", #{\"\": 0},\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n", "        model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map=\"auto\", #{\"\": 0}\n\t    )\n\telif device == \"mps\":\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL,\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16,\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n\t        model,\n", "        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16,\n\t    )\n\telse:\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n\t        model,\n", "        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t    )\n\tmodel.config.bos_token_id = tokenizer.bos_token_id = 1\n\tmodel.config.eos_token_id = tokenizer.eos_token_id = 2\n\tdef generate_prompt(instruction, input=None):\n\t    if input:\n\t        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{instruction}\n", "### Input:\n\t{input}\n\t### Response:\"\"\"\n\t    else:\n\t        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{instruction}\n\t### Response:\"\"\"\n\tif not LOAD_8BIT:\n\t    model.half()  # seems to fix bugs for some users.\n", "model.eval()\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\tdef evaluate(\n\t    input,\n\t    temperature=0.1,\n\t    top_p=0.75,\n\t    top_k=40,\n\t    num_beams=4,\n\t    max_new_tokens=128,\n", "    min_new_tokens=1,\n\t    repetition_penalty=2.0,\n\t    **kwargs,\n\t):\n\t    prompt = generate_prompt(input)\n\t    inputs = tokenizer(prompt, return_tensors=\"pt\")\n\t    input_ids = inputs[\"input_ids\"].to(device)\n\t    generation_config = GenerationConfig(\n\t        temperature=temperature,\n\t        top_p=top_p,\n", "        top_k=top_k,\n\t        num_beams=num_beams,\n\t        bos_token_id=1,\n\t        eos_token_id=2,\n\t        pad_token_id=0,\n\t        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n\t        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n\t        **kwargs,\n\t    )\n\t    with torch.no_grad():\n", "        if args.use_typewriter:\n\t            for generation_output in model.stream_generate(\n\t                input_ids=input_ids,\n\t                generation_config=generation_config,\n\t                return_dict_in_generate=True,\n\t                output_scores=False,\n\t                repetition_penalty=float(repetition_penalty),\n\t            ):\n\t                outputs = tokenizer.batch_decode(generation_output)\n\t                show_text = \"\\n--------------------------------------------\\n\".join(\n", "                    [output.split(\"### Response:\")[1].strip().replace('�','')+\" ▌\" for output in outputs]\n\t                )\n\t                # if show_text== '':\n\t                #     yield last_show_text\n\t                # else:\n\t                yield show_text\n\t            yield outputs[0].split(\"### Response:\")[1].strip().replace('�','')\n\t        else:\n\t            generation_output = model.generate(\n\t                input_ids=input_ids,\n", "                generation_config=generation_config,\n\t                return_dict_in_generate=True,\n\t                output_scores=False,\n\t                repetition_penalty=1.3,\n\t            )\n\t            output = generation_output.sequences[0]\n\t            output = tokenizer.decode(output).split(\"### Response:\")[1].strip()\n\t            print(output)\n\t            yield output\n\tgr.Interface(\n", "    fn=evaluate,\n\t    inputs=[\n\t        gr.components.Textbox(\n\t            lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n\t        ),\n\t        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n\t        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n\t        gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n\t        gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\"),\n\t        gr.components.Slider(\n", "            minimum=1, maximum=2000, step=1, value=256, label=\"Max New Tokens\"\n\t        ),\n\t        gr.components.Slider(\n\t            minimum=1, maximum=300, step=1, value=1, label=\"Min New Tokens\"\n\t        ),\n\t        gr.components.Slider(\n\t            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n\t        ),\n\t    ],\n\t    outputs=[\n", "        gr.inputs.Textbox(\n\t            lines=25,\n\t            label=\"Output\",\n\t        )\n\t    ],\n\t    title=\"Chinese-Vicuna 中文小羊驼\",\n\t    description=\"中文小羊驼由各种高质量的开源instruction数据集，结合Alpaca-lora的代码训练而来，模型基于开源的llama7B，主要贡献是对应的lora模型。由于代码训练资源要求较小，希望为llama中文lora社区做一份贡献。\",\n\t).queue().launch(share=True)\n"]}
{"filename": "generate.py", "chunked_list": ["import sys\n\timport torch\n\tfrom peft import PeftModel, PeftModelForCausalLM, LoraConfig\n\timport transformers\n\timport gradio as gr\n\timport argparse\n\timport warnings\n\timport os\n\tfrom utils import StreamPeftGenerationMixin,StreamLlamaForCausalLM\n\tassert (\n", "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--model_path\", type=str, default=\"/model/13B_hf\")\n\tparser.add_argument(\"--lora_path\", type=str, default=\"checkpoint-3000\")\n\tparser.add_argument(\"--use_typewriter\", type=int, default=1)\n\tparser.add_argument(\"--use_local\", type=int, default=1)\n\targs = parser.parse_args()\n\tprint(args)\n", "tokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\tLOAD_8BIT = True\n\tBASE_MODEL = args.model_path\n\tLORA_WEIGHTS = args.lora_path\n\t# fix the path for local checkpoint\n\tlora_bin_path = os.path.join(args.lora_path, \"adapter_model.bin\")\n\tprint(lora_bin_path)\n\tif not os.path.exists(lora_bin_path) and args.use_local:\n\t    pytorch_bin_path = os.path.join(args.lora_path, \"pytorch_model.bin\")\n\t    print(pytorch_bin_path)\n", "    if os.path.exists(pytorch_bin_path):\n\t        os.rename(pytorch_bin_path, lora_bin_path)\n\t        warnings.warn(\n\t            \"The file name of the lora checkpoint'pytorch_model.bin' is replaced with 'adapter_model.bin'\"\n\t        )\n\t    else:\n\t        assert ('Checkpoint is not Found!')\n\tif torch.cuda.is_available():\n\t    device = \"cuda\"\n\telse:\n", "    device = \"cpu\"\n\ttry:\n\t    if torch.backends.mps.is_available():\n\t        device = \"mps\"\n\texcept:\n\t    pass\n\tif device == \"cuda\":\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL,\n\t        load_in_8bit=LOAD_8BIT,\n", "        torch_dtype=torch.float16,\n\t        device_map=\"auto\", #device_map={\"\": 0},\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n\t        model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map=\"auto\", #device_map={\"\": 0}\n\t    )\n\telif device == \"mps\":\n\t    model = LlamaForCausalLM.from_pretrained(\n\t        BASE_MODEL,\n\t        device_map={\"\": device},\n", "        torch_dtype=torch.float16,\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n\t        model,\n\t        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t        torch_dtype=torch.float16,\n\t    )\n\telse:\n\t    model = LlamaForCausalLM.from_pretrained(\n", "        BASE_MODEL, device_map={\"\": device}, low_cpu_mem_usage=True\n\t    )\n\t    model = StreamPeftGenerationMixin.from_pretrained(\n\t        model,\n\t        LORA_WEIGHTS,\n\t        device_map={\"\": device},\n\t    )\n\tdef generate_prompt(instruction, input=None):\n\t    if input:\n\t        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n", "### Instruction:\n\t{instruction}\n\t### Input:\n\t{input}\n\t### Response:\"\"\"\n\t    else:\n\t        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t### Instruction:\n\t{instruction}\n\t### Response:\"\"\"\n", "if not LOAD_8BIT:\n\t    model.half()  # seems to fix bugs for some users.\n\tmodel.eval()\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\tdef evaluate(\n\t    input,\n\t    temperature=0.1,\n\t    top_p=0.75,\n\t    top_k=40,\n", "    num_beams=4,\n\t    max_new_tokens=128,\n\t    min_new_tokens=1,\n\t    repetition_penalty=2.0,\n\t    **kwargs,\n\t):\n\t    prompt = generate_prompt(input)\n\t    inputs = tokenizer(prompt, return_tensors=\"pt\")\n\t    input_ids = inputs[\"input_ids\"].to(device)\n\t    generation_config = GenerationConfig(\n", "        temperature=temperature,\n\t        top_p=top_p,\n\t        top_k=top_k,\n\t        num_beams=num_beams,\n\t        bos_token_id=1,\n\t        eos_token_id=2,\n\t        pad_token_id=0,\n\t        max_new_tokens=max_new_tokens, # max_length=max_new_tokens+input_sequence\n\t        min_new_tokens=min_new_tokens, # min_length=min_new_tokens+input_sequence\n\t        **kwargs,\n", "    )\n\t    with torch.no_grad():\n\t        if args.use_typewriter:\n\t            for generation_output in model.stream_generate(\n\t                input_ids=input_ids,\n\t                generation_config=generation_config,\n\t                return_dict_in_generate=True,\n\t                output_scores=False,\n\t                repetition_penalty=float(repetition_penalty),\n\t            ):\n", "                outputs = tokenizer.batch_decode(generation_output)\n\t                show_text = \"\\n--------------------------------------------\\n\".join(\n\t                    [output.split(\"### Response:\")[1].strip().replace('�','')+\" ▌\" for output in outputs]\n\t                )\n\t                # if show_text== '':\n\t                #     yield last_show_text\n\t                # else:\n\t                yield show_text\n\t            yield outputs[0].split(\"### Response:\")[1].strip().replace('�','')\n\t        else:\n", "            generation_output = model.generate(\n\t                input_ids=input_ids,\n\t                generation_config=generation_config,\n\t                return_dict_in_generate=True,\n\t                output_scores=False,\n\t                repetition_penalty=1.3,\n\t            )\n\t            output = generation_output.sequences[0]\n\t            output = tokenizer.decode(output).split(\"### Response:\")[1].strip()\n\t            print(output)\n", "            yield output\n\tgr.Interface(\n\t    fn=evaluate,\n\t    inputs=[\n\t        gr.components.Textbox(\n\t            lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n\t        ),\n\t        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n\t        gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n\t        gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n", "        gr.components.Slider(minimum=1, maximum=10, step=1, value=4, label=\"Beams Number\"),\n\t        gr.components.Slider(\n\t            minimum=1, maximum=2000, step=1, value=256, label=\"Max New Tokens\"\n\t        ),\n\t        gr.components.Slider(\n\t            minimum=1, maximum=300, step=1, value=1, label=\"Min New Tokens\"\n\t        ),\n\t        gr.components.Slider(\n\t            minimum=0.1, maximum=10.0, step=0.1, value=2.0, label=\"Repetition Penalty\"\n\t        ),\n", "    ],\n\t    outputs=[\n\t        gr.inputs.Textbox(\n\t            lines=25,\n\t            label=\"Output\",\n\t        )\n\t    ],\n\t    title=\"Chinese-Vicuna 中文小羊驼\",\n\t    description=\"中文小羊驼由各种高质量的开源instruction数据集，结合Alpaca-lora的代码训练而来，模型基于开源的llama7B，主要贡献是对应的lora模型。由于代码训练资源要求较小，希望为llama中文lora社区做一份贡献。\",\n\t).queue().launch(share=True)\n"]}
{"filename": "tools/quant_llama.py", "chunked_list": ["import argparse\n\timport time\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport quant\n\tfrom gptq import GPTQ\n\tfrom datautils import get_loaders\n\tdef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n\t    if type(module) in layers:\n", "        return {name: module}\n\t    res = {}\n\t    for name1, child in module.named_children():\n\t        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n\t    return res\n\tdef get_llama(model):\n\t    def skip(*args, **kwargs):\n\t        pass\n\t    torch.nn.init.kaiming_uniform_ = skip\n\t    torch.nn.init.uniform_ = skip\n", "    torch.nn.init.normal_ = skip\n\t    from transformers import LlamaForCausalLM\n\t    model = LlamaForCausalLM.from_pretrained(model, torch_dtype=torch.float16)\n\t    model.seqlen = 2048\n\t    return model\n\t@torch.no_grad()\n\tdef llama_sequential(model, dataloader, dev):\n\t    print('Starting ...')\n\t    use_cache = model.config.use_cache\n\t    model.config.use_cache = False\n", "    layers = model.model.layers\n\t    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n\t    model.model.norm = model.model.norm.to(dev)\n\t    layers[0] = layers[0].to(dev)\n\t    dtype = next(iter(model.parameters())).dtype\n\t    inps = torch.zeros((args.nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev)\n\t    cache = {'i': 0, 'attention_mask': None}\n\t    class Catcher(nn.Module):\n\t        def __init__(self, module):\n\t            super().__init__()\n", "            self.module = module\n\t        def forward(self, inp, **kwargs):\n\t            inps[cache['i']] = inp\n\t            cache['i'] += 1\n\t            cache['attention_mask'] = kwargs['attention_mask']\n\t            cache['position_ids'] = kwargs['position_ids']\n\t            raise ValueError\n\t    layers[0] = Catcher(layers[0])\n\t    for batch in dataloader:\n\t        try:\n", "            model(batch[0].to(dev))\n\t        except ValueError:\n\t            pass\n\t    layers[0] = layers[0].module\n\t    layers[0] = layers[0].cpu()\n\t    model.model.embed_tokens = model.model.embed_tokens.cpu()\n\t    model.model.norm = model.model.norm.cpu()\n\t    torch.cuda.empty_cache()\n\t    outs = torch.zeros_like(inps)\n\t    attention_mask = cache['attention_mask']\n", "    position_ids = cache['position_ids']\n\t    print('Ready.')\n\t    quantizers = {}\n\t    for i in range(len(layers)):\n\t        print(f'Quantizing layer {i+1}/{len(layers)}..')\n\t        print('+------------------+--------------+------------+-----------+-------+')\n\t        print('|       name       | weight_error | fp_inp_SNR | q_inp_SNR | time  |')\n\t        print('+==================+==============+============+===========+=======+')\n\t        layer = layers[i].to(dev)\n\t        full = find_layers(layer)\n", "        if args.true_sequential:\n\t            sequential = [['self_attn.k_proj', 'self_attn.v_proj', 'self_attn.q_proj'], ['self_attn.o_proj'], ['mlp.up_proj', 'mlp.gate_proj'], ['mlp.down_proj']]\n\t        else:\n\t            sequential = [list(full.keys())]\n\t        for names in sequential:\n\t            subset = {n: full[n] for n in names}\n\t            gptq = {}\n\t            for name in subset:\n\t                gptq[name] = GPTQ(subset[name])\n\t                gptq[name].quantizer.configure(args.wbits, perchannel=True, mse=False)\n", "            def add_batch(name):\n\t                def tmp(_, inp, out):\n\t                    gptq[name].add_batch(inp[0].data, out.data)\n\t                return tmp\n\t            handles = []\n\t            for name in subset:\n\t                handles.append(subset[name].register_forward_hook(add_batch(name)))\n\t            for j in range(args.nsamples):\n\t                outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n\t            for h in handles:\n", "                h.remove()\n\t            for name in subset:\n\t                scale, zero, g_idx, error = gptq[name].fasterquant(percdamp=args.percdamp, groupsize=args.groupsize, actorder=args.act_order, name=name)\n\t                quantizers['model.layers.%d.%s' % (i, name)] = (gptq[name].quantizer.cpu(), scale.cpu(), zero.cpu(), g_idx.cpu(), args.wbits, args.groupsize)\n\t                gptq[name].free()\n\t        for j in range(args.nsamples):\n\t            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n\t        layers[i] = layer.cpu()\n\t        del layer\n\t        del gptq\n", "        torch.cuda.empty_cache()\n\t        inps, outs = outs, inps\n\t        print('+------------------+--------------+------------+-----------+-------+')\n\t        print('\\n')\n\t    model.config.use_cache = use_cache\n\t    return quantizers\n\t@torch.no_grad()\n\tdef llama_eval(model, testenc, dev):\n\t    print('Evaluating ...')\n\t    testenc = testenc.input_ids\n", "    nsamples = testenc.numel() // model.seqlen\n\t    use_cache = model.config.use_cache\n\t    model.config.use_cache = False\n\t    layers = model.model.layers\n\t    model.model.embed_tokens = model.model.embed_tokens.to(dev)\n\t    layers[0] = layers[0].to(dev)\n\t    dtype = next(iter(model.parameters())).dtype\n\t    inps = torch.zeros((nsamples, model.seqlen, model.config.hidden_size), dtype=dtype, device=dev)\n\t    cache = {'i': 0, 'attention_mask': None}\n\t    class Catcher(nn.Module):\n", "        def __init__(self, module):\n\t            super().__init__()\n\t            self.module = module\n\t        def forward(self, inp, **kwargs):\n\t            inps[cache['i']] = inp\n\t            cache['i'] += 1\n\t            cache['attention_mask'] = kwargs['attention_mask']\n\t            cache['position_ids'] = kwargs['position_ids']\n\t            raise ValueError\n\t    layers[0] = Catcher(layers[0])\n", "    for i in range(nsamples):\n\t        batch = testenc[:, (i * model.seqlen):((i + 1) * model.seqlen)].to(dev)\n\t        try:\n\t            model(batch)\n\t        except ValueError:\n\t            pass\n\t    layers[0] = layers[0].module\n\t    layers[0] = layers[0].cpu()\n\t    model.model.embed_tokens = model.model.embed_tokens.cpu()\n\t    torch.cuda.empty_cache()\n", "    outs = torch.zeros_like(inps)\n\t    attention_mask = cache['attention_mask']\n\t    position_ids = cache['position_ids']\n\t    for i in range(len(layers)):\n\t        print(i)\n\t        layer = layers[i].to(dev)\n\t        if args.nearest:\n\t            subset = find_layers(layer)\n\t            for name in subset:\n\t                quantizer = quant.Quantizer()\n", "                quantizer.configure(args.wbits, perchannel=True, sym=args.sym, mse=False)\n\t                W = subset[name].weight.data\n\t                quantizer.find_params(W, weight=True)\n\t                subset[name].weight.data = quantizer.quantize(W).to(next(iter(layer.parameters())).dtype)\n\t        for j in range(nsamples):\n\t            outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]\n\t        layers[i] = layer.cpu()\n\t        del layer\n\t        torch.cuda.empty_cache()\n\t        inps, outs = outs, inps\n", "    if model.model.norm is not None:\n\t        model.model.norm = model.model.norm.to(dev)\n\t    model.lm_head = model.lm_head.to(dev)\n\t    testenc = testenc.to(dev)\n\t    nlls = []\n\t    for i in range(nsamples):\n\t        hidden_states = inps[i].unsqueeze(0)\n\t        if model.model.norm is not None:\n\t            hidden_states = model.model.norm(hidden_states)\n\t        lm_logits = model.lm_head(hidden_states)\n", "        shift_logits = lm_logits[:, :-1, :].contiguous()\n\t        shift_labels = testenc[:, (i * model.seqlen):((i + 1) * model.seqlen)][:, 1:]\n\t        loss_fct = nn.CrossEntropyLoss()\n\t        loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n\t        neg_log_likelihood = loss.float() * model.seqlen\n\t        nlls.append(neg_log_likelihood)\n\t    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n\t    print(ppl.item())\n\t    model.config.use_cache = use_cache\n\t# TODO: perform packing on GPU\n", "def llama_pack(model, quantizers, wbits, groupsize):\n\t    layers = find_layers(model)\n\t    layers = {n: layers[n] for n in quantizers}\n\t    quant.make_quant_linear(model, quantizers, wbits, groupsize)\n\t    qlayers = find_layers(model, [quant.QuantLinear])\n\t    print('Packing ...')\n\t    for name in qlayers:\n\t        print(name)\n\t        quantizers[name], scale, zero, g_idx, _, _ = quantizers[name]\n\t        qlayers[name].pack(layers[name], scale, zero, g_idx)\n", "    print('Done.')\n\t    return model\n\tdef load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n\t    from transformers import LlamaConfig, LlamaForCausalLM, modeling_utils\n\t    config = LlamaConfig.from_pretrained(model)\n\t    def noop(*args, **kwargs):\n\t        pass\n\t    torch.nn.init.kaiming_uniform_ = noop\n\t    torch.nn.init.uniform_ = noop\n\t    torch.nn.init.normal_ = noop\n", "    torch.set_default_dtype(torch.half)\n\t    modeling_utils._init_weights = False\n\t    torch.set_default_dtype(torch.half)\n\t    model = LlamaForCausalLM(config)\n\t    torch.set_default_dtype(torch.float)\n\t    if eval:\n\t        model = model.eval()\n\t    layers = find_layers(model)\n\t    for name in ['lm_head']:\n\t        if name in layers:\n", "            del layers[name]\n\t    quant.make_quant_linear(model, layers, wbits, groupsize)\n\t    del layers\n\t    print('Loading model ...')\n\t    if checkpoint.endswith('.safetensors'):\n\t        from safetensors.torch import load_file as safe_load\n\t        model.load_state_dict(safe_load(checkpoint))\n\t    else:\n\t        model.load_state_dict(torch.load(checkpoint))\n\t    quant.make_quant_attn(model)\n", "    if eval and fused_mlp:\n\t        quant.make_fused_mlp(model)\n\t    if warmup_autotune:\n\t        quant.autotune_warmup_linear(model, transpose=not (eval))\n\t        if eval and fused_mlp:\n\t            quant.autotune_warmup_fused(model)\n\t    model.seqlen = 2048\n\t    print('Done.')\n\t    return model\n\tif __name__ == '__main__':\n", "    parser = argparse.ArgumentParser()\n\t    parser.add_argument('model', type=str, help='llama model to load')\n\t    parser.add_argument('dataset', type=str, choices=['wikitext2', 'ptb', 'c4'], help='Where to extract calibration data from.')\n\t    parser.add_argument('--seed', type=int, default=0, help='Seed for sampling the calibration data.')\n\t    parser.add_argument('--nsamples', type=int, default=128, help='Number of calibration data samples.')\n\t    parser.add_argument('--percdamp', type=float, default=.01, help='Percent of the average Hessian diagonal to use for dampening.')\n\t    parser.add_argument('--wbits', type=int, default=16, choices=[2, 3, 4, 8, 16], help='#bits to use for quantization; use 16 for evaluating base model.')\n\t    parser.add_argument('--groupsize', type=int, default=-1, help='Groupsize to use for quantization; default uses full row.')\n\t    parser.add_argument('--eval', action='store_true', help='evaluate quantized model.')\n\t    parser.add_argument('--save', type=str, default='', help='Save quantized checkpoint under this name.')\n", "    parser.add_argument('--save_safetensors', type=str, default='', help='Save quantized `.safetensors` checkpoint under this name.')\n\t    parser.add_argument('--quant-directory', type=str, default=None, help='Specify the directory for export quantization parameters to toml format. `None` means no export by default.')\n\t    parser.add_argument('--act-order', action='store_true', help='Whether to apply the activation order GPTQ heuristic')\n\t    parser.add_argument('--true-sequential', action='store_true', help='Whether to run in true sequential model.')\n\t    args = parser.parse_args()\n\t    DEV = torch.device('cuda:0')\n\t    gpu_dist = []\n\t    model = get_llama(args.model)\n\t    model.eval()\n\t    dataloader, testloader = get_loaders(args.dataset, nsamples=args.nsamples, seed=args.seed, model=args.model, seqlen=model.seqlen)\n", "    if args.wbits < 16:\n\t        tick = time.time()\n\t        quantizers = llama_sequential(model, dataloader, DEV)\n\t        print(time.time() - tick)\n\t    if args.eval:\n\t        datasets = ['wikitext2', 'ptb', 'c4']\n\t        if args.new_eval:\n\t            datasets = ['wikitext2', 'ptb-new', 'c4-new']\n\t        for dataset in datasets:\n\t            dataloader, testloader = get_loaders(dataset, seed=args.seed, model=args.model, seqlen=model.seqlen)\n", "            print(dataset)\n\t            llama_eval(model, testloader, DEV)\n\t    llama_pack(model, quantizers, args.wbits, args.groupsize)\n\t    torch.save(model.state_dict(), args.save)\n\t# bash : CUDA_VISIBLE_DEVICES=0 proxychains python quant_llama.py ../model/llama7b_hf wikitext2 --wbits 4 --groupsize 128 --save llama7b-4bit-128g.pt"]}
{"filename": "tools/reshard.py", "chunked_list": ["# ref: https://gist.github.com/benob/4850a0210b01672175942203aa36d300\n\timport os\n\timport json\n\timport sys\n\timport torch\n\timport glob\n\t# python test.py 2 xx/checkpoint-1000/ckpt/ outs\n\tif len(sys.argv) != 4:\n\t    print('usage: %s <new-shards> <input-model-path> <output-model-path>' % sys.argv[0], file=sys.stderr)\n\t    sys.exit(1)\n", "num_shards = int(sys.argv[1])\n\tinput_model_dir = sys.argv[2]\n\toutput_model_dir = sys.argv[3]\n\twith open(os.path.join(input_model_dir, 'params.json'), 'r') as fp:\n\t    params = json.loads(fp.read())\n\tassert params['dim'] % num_shards == 0, \"number of shards need to divide parameter dimension %d\" % params['dim']\n\tprint('loading...')\n\tcheckpoints = [torch.load(path, map_location=torch.device('cpu')) for path in glob.glob(os.path.join(input_model_dir, '*.pth'))]\n\tlayer_kind = {\n\t    'tok_embeddings': 'ParallelEmbedding',\n", "    'output': 'ColumnParallelLinear',\n\t    'attention.wq': 'ColumnParallelLinear',\n\t    'attention.wk': 'ColumnParallelLinear',\n\t    'attention.wv': 'ColumnParallelLinear',\n\t    'attention.wo': 'RowParallelLinear',\n\t    'feed_forward.w1': 'ColumnParallelLinear',\n\t    'feed_forward.w2': 'RowParallelLinear',\n\t    'feed_forward.w3': 'ColumnParallelLinear',\n\t    'attention_norm': None,\n\t    'ffn_norm': None,\n", "    'norm': None,\n\t    'rope.freqs': None,\n\t}\n\toutput = [dict() for x in range(num_shards)]\n\tprint('converting...')\n\tfor key in checkpoints[0].keys():\n\t    tensors = [m[key] for m in checkpoints]\n\t    print(key)\n\t    print('  in shapes=', [p.shape for p in tensors])\n\t    for pattern, kind in layer_kind.items():\n", "        if key.replace('.weight', '').endswith(pattern):\n\t            print('  kind=', kind)\n\t            if kind == 'ColumnParallelLinear':\n\t                with torch.no_grad():\n\t                    merged = torch.cat(tensors, 0)\n\t                    slice_size = merged.shape[0] // num_shards\n\t                    for rank in range(num_shards):\n\t                        output[rank][key] = merged[slice_size * rank: slice_size * (rank + 1),:].clone().detach()\n\t            elif kind in ('ParallelEmbedding', 'RowParallelLinear'):\n\t                with torch.no_grad():\n", "                    merged = torch.cat(tensors, 1)\n\t                    slice_size = merged.shape[1] // num_shards\n\t                    for rank in range(num_shards):\n\t                        output[rank][key] = merged[:,slice_size * rank: slice_size * (rank + 1)].clone().detach()\n\t            else:\n\t                for rank in range(num_shards):\n\t                    output[rank][key] = tensors[0]\n\t            print('  out shapes=', [output[rank][key].shape for rank in range(num_shards)])\n\t            print()\n\t            break\n", "    else:\n\t        raise Exception('parameter name not recognized')\n\tprint('saving...')\n\tos.makedirs(output_model_dir, exist_ok=True)\n\twith open(os.path.join(output_model_dir, 'params.json'), 'w') as fp:\n\t    fp.write(json.dumps(params))\n\tfor rank in range(num_shards):\n\t    print(' ', rank)\n\t    torch.save(output[rank], os.path.join(output_model_dir, 'consolidated.%02d.pth' % rank))\n\tprint('done.')"]}
{"filename": "tools/merge_lora.py", "chunked_list": ["# This file is adapted from: https://github.com/tloen/alpaca-lora ( for merge ) and https://gist.github.com/benob/4850a0210b01672175942203aa36d300 ( for shard )\n\t# It can merge the LoRA weights back into the base model for export to PyTorch state_dicts (`consolidated.0x.pth`). The number of shards is according to the user command argument. \n\t# They should help users who want to run inference in projects like llama.cpp or alpaca.cpp.\n\timport os\n\timport json\n\timport torch\n\tfrom peft import PeftModel, LoraConfig\n\timport argparse\n\timport transformers\n\t# args\n", "parser = argparse.ArgumentParser()\n\t# The original base model checkpoint dir\n\tparser.add_argument(\"--model_path\", type=str, default='decapoda-research/llama-7b-hf')\n\t# The finetuned lora model checkpoint dir\n\tparser.add_argument(\"--lora_path\",type=str, default='./lora-Vicuna/checkpoint-3000')\n\t# The output dir\n\tparser.add_argument(\"--out_path\", type=str, default='./lora-Vicuna/checkpoint-3000-with-lora')\n\tparser.add_argument(\"--num_shards\", type=int, default=None)\n\targs = parser.parse_args()\n\t# \n", "assert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaTokenizer, LlamaForCausalLM\n\tparams = {\n\t    '65B':  {\"dim\": 8192, \"multiple_of\": 256, \"n_heads\": 64, \"n_layers\": 80, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n\t    '30B': {\"dim\": 6656, \"multiple_of\": 256, \"n_heads\": 52, \"n_layers\": 60, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n\t    '13B': {\"dim\": 5120, \"multiple_of\": 256, \"n_heads\": 40, \"n_layers\": 40, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n\t    '7B':  {\"dim\": 4096, \"multiple_of\": 256, \"n_heads\": 32, \"n_layers\": 32, \"norm_eps\": 1e-06, \"vocab_size\": -1},\n\t}\n", "NUM_SHARDS = {\n\t    \"7B\": 1,\n\t    \"13B\": 2,\n\t    \"30B\": 4,\n\t    \"65B\": 8,\n\t}\n\tlayer_kind = {\n\t    'tok_embeddings': 'ParallelEmbedding',\n\t    'output': 'ColumnParallelLinear',\n\t    'attention.wq': 'ColumnParallelLinear',\n", "    'attention.wk': 'ColumnParallelLinear',\n\t    'attention.wv': 'ColumnParallelLinear',\n\t    'attention.wo': 'RowParallelLinear',\n\t    'feed_forward.w1': 'ColumnParallelLinear',\n\t    'feed_forward.w2': 'RowParallelLinear',\n\t    'feed_forward.w3': 'ColumnParallelLinear',\n\t    'attention_norm': None,\n\t    'ffn_norm': None,\n\t    'norm': None,\n\t    'rope.freqs': None,\n", "}\n\tprint(f\">>> load model from {args.model_path} and lora from {args.lora_path}....\")\n\ttokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\tbase_model = LlamaForCausalLM.from_pretrained(\n\t    args.model_path,\n\t    load_in_8bit=False,\n\t    torch_dtype=torch.float16,\n\t    device_map={\"\": \"cpu\"},\n\t)\n\tlora_model = PeftModel.from_pretrained(\n", "    base_model,\n\t    args.lora_path,\n\t    device_map={\"\": \"cpu\"},\n\t    torch_dtype=torch.float16,\n\t)\n\t# merge weights\n\tfor layer in lora_model.base_model.model.model.layers:\n\t    layer.self_attn.q_proj.merge_weights = True\n\t    layer.self_attn.v_proj.merge_weights = True\n\tlora_model.train(False)\n", "lora_model_sd = lora_model.state_dict()\n\tn_layers = base_model.config.num_hidden_layers\n\tmodel_size = None\n\tfor size in params.keys():\n\t    if n_layers == params[size][\"n_layers\"]:\n\t        model_size = size\n\t        print(f\">>> automatically recognize model_size={size}\")\n\tif model_size is None:\n\t    raise Exception('cannot recognize model_size! please check if your model is llama-based model')\n\tn_heads = base_model.config.num_attention_heads\n", "assert n_heads == params[model_size][\"n_heads\"]\n\tdim = base_model.config.hidden_size\n\tassert dim == params[model_size][\"dim\"]\n\tdims_per_head = dim // n_heads\n\tbase = 10000.0\n\tinv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n\tif args.num_shards is None:\n\t    num_shards = NUM_SHARDS[model_size]\n\telse:\n\t    num_shards = args.num_shards\n", "print(f'>>> will split model checkpoint in {num_shards} parts')\n\tdef permute(w):\n\t    return (\n\t        w.view(n_heads, dim // n_heads // 2, 2, dim).transpose(1, 2).reshape(dim, dim)\n\t    )\n\tdef unpermute(w):\n\t    return (\n\t        w.view(n_heads, 2, dim // n_heads // 2, dim).transpose(1, 2).reshape(dim, dim)\n\t    )\n\tdef translate_state_dict_key(k):\n", "    k = k.replace(\"base_model.model.\", \"\")\n\t    if k == \"model.embed_tokens.weight\":\n\t        return \"tok_embeddings.weight\"\n\t    elif k == \"model.norm.weight\":\n\t        return \"norm.weight\"\n\t    elif k == \"lm_head.weight\":\n\t        return \"output.weight\"\n\t    elif k.startswith(\"model.layers.\"):\n\t        layer = k.split(\".\")[2]\n\t        if k.endswith(\".self_attn.q_proj.weight\"):\n", "            return f\"layers.{layer}.attention.wq.weight\"\n\t        elif k.endswith(\".self_attn.k_proj.weight\"):\n\t            return f\"layers.{layer}.attention.wk.weight\"\n\t        elif k.endswith(\".self_attn.v_proj.weight\"):\n\t            return f\"layers.{layer}.attention.wv.weight\"\n\t        elif k.endswith(\".self_attn.o_proj.weight\"):\n\t            return f\"layers.{layer}.attention.wo.weight\"\n\t        elif k.endswith(\".mlp.gate_proj.weight\"):\n\t            return f\"layers.{layer}.feed_forward.w1.weight\"\n\t        elif k.endswith(\".mlp.down_proj.weight\"):\n", "            return f\"layers.{layer}.feed_forward.w2.weight\"\n\t        elif k.endswith(\".mlp.up_proj.weight\"):\n\t            return f\"layers.{layer}.feed_forward.w3.weight\"\n\t        elif k.endswith(\".input_layernorm.weight\"):\n\t            return f\"layers.{layer}.attention_norm.weight\"\n\t        elif k.endswith(\".post_attention_layernorm.weight\"):\n\t            return f\"layers.{layer}.ffn_norm.weight\"\n\t        elif k.endswith(\"rotary_emb.inv_freq\") or \"lora\" in k:\n\t            return None\n\t        else:\n", "            print(layer, k)\n\t            raise NotImplementedError\n\t    else:\n\t        print(k)\n\t        raise NotImplementedError\n\tnew_state_dict = {}\n\tfor k, v in lora_model_sd.items():\n\t    new_k = translate_state_dict_key(k)\n\t    if new_k is not None:\n\t        if \"wq\" in new_k or \"wk\" in new_k:\n", "            new_state_dict[new_k] = unpermute(v)\n\t        else:\n\t            new_state_dict[new_k] = v\n\tos.makedirs(args.out_path, exist_ok=True)\n\tif num_shards == 1:\n\t    torch.save(new_state_dict, f\"{args.out_path}/consolidated.00.pth\")\n\t    with open(f\"{args.out_path}/params.json\", \"w\") as f:\n\t        json.dump(params[model_size], f)\n\telse:\n\t    output = [dict() for x in range(num_shards)]\n", "    print('>>> start converting to shards...')\n\t    # sharded the models\n\t    for key in new_state_dict.keys():\n\t        tensors = [new_state_dict[key]]\n\t        print(key)\n\t        print('  in shapes=', [p.shape for p in tensors])\n\t        for pattern, kind in layer_kind.items():\n\t            if key.replace('.weight', '').endswith(pattern):\n\t                print('  kind=', kind)\n\t                if kind == 'ColumnParallelLinear':\n", "                    with torch.no_grad():\n\t                        merged = torch.cat(tensors, 0)\n\t                        slice_size = merged.shape[0] // num_shards\n\t                        for rank in range(num_shards):\n\t                            output[rank][key] = merged[slice_size * rank: slice_size * (rank + 1),:].clone().detach()\n\t                elif kind in ('ParallelEmbedding', 'RowParallelLinear'):\n\t                    with torch.no_grad():\n\t                        merged = torch.cat(tensors, 1)\n\t                        slice_size = merged.shape[1] // num_shards\n\t                        for rank in range(num_shards):\n", "                            output[rank][key] = merged[:,slice_size * rank: slice_size * (rank + 1)].clone().detach()\n\t                else:\n\t                    for rank in range(num_shards):\n\t                        output[rank][key] = tensors[0]\n\t                print('  out shapes=', [output[rank][key].shape for rank in range(num_shards)])\n\t                print()\n\t                break\n\t    print('saving...')\n\t    with open(os.path.join(args.out_path, 'params.json'), 'w') as fp:\n\t        fp.write(json.dumps(params))\n", "    for rank in range(num_shards):\n\t        print(' ', rank)\n\t        torch.save(output[rank], os.path.join(args.out_path, 'consolidated.%02d.pth' % rank))\n\t    print('done.')\n"]}
{"filename": "tools/datautils.py", "chunked_list": ["import numpy as np\n\timport torch\n\tdef set_seed(seed):\n\t    np.random.seed(seed)\n\t    torch.random.manual_seed(seed)\n\tdef get_wikitext2(nsamples, seed, seqlen, model):\n\t    from datasets import load_dataset\n\t    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n\t    testdata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n\t    from transformers import AutoTokenizer\n", "    try:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n\t    except:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n\t    trainenc = tokenizer(\"\\n\\n\".join(traindata['text']), return_tensors='pt')\n\t    testenc = tokenizer(\"\\n\\n\".join(testdata['text']), return_tensors='pt')\n\t    import random\n\t    random.seed(seed)\n\t    trainloader = []\n\t    for _ in range(nsamples):\n", "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n\t        j = i + seqlen\n\t        inp = trainenc.input_ids[:, i:j]\n\t        tar = inp.clone()\n\t        tar[:, :-1] = -100\n\t        trainloader.append((inp, tar))\n\t    return trainloader, testenc\n\tdef get_ptb(nsamples, seed, seqlen, model):\n\t    from datasets import load_dataset\n\t    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n", "    valdata = load_dataset('ptb_text_only', 'penn_treebank', split='validation')\n\t    from transformers import AutoTokenizer\n\t    try:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n\t    except:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n\t    trainenc = tokenizer(\"\\n\\n\".join(traindata['sentence']), return_tensors='pt')\n\t    testenc = tokenizer(\"\\n\\n\".join(valdata['sentence']), return_tensors='pt')\n\t    import random\n\t    random.seed(seed)\n", "    trainloader = []\n\t    for _ in range(nsamples):\n\t        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n\t        j = i + seqlen\n\t        inp = trainenc.input_ids[:, i:j]\n\t        tar = inp.clone()\n\t        tar[:, :-1] = -100\n\t        trainloader.append((inp, tar))\n\t    return trainloader, testenc\n\tdef get_c4(nsamples, seed, seqlen, model):\n", "    from datasets import load_dataset\n\t    traindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train', use_auth_token=False)\n\t    valdata = load_dataset('allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation', use_auth_token=False)\n\t    from transformers import AutoTokenizer\n\t    try:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n\t    except:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n\t    import random\n\t    random.seed(seed)\n", "    trainloader = []\n\t    for _ in range(nsamples):\n\t        while True:\n\t            i = random.randint(0, len(traindata) - 1)\n\t            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n\t            if trainenc.input_ids.shape[1] >= seqlen:\n\t                break\n\t        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n\t        j = i + seqlen\n\t        inp = trainenc.input_ids[:, i:j]\n", "        tar = inp.clone()\n\t        tar[:, :-1] = -100\n\t        trainloader.append((inp, tar))\n\t    import random\n\t    random.seed(0)\n\t    valenc = []\n\t    for _ in range(256):\n\t        while True:\n\t            i = random.randint(0, len(valdata) - 1)\n\t            tmp = tokenizer(valdata[i]['text'], return_tensors='pt')\n", "            if tmp.input_ids.shape[1] >= seqlen:\n\t                break\n\t        i = random.randint(0, tmp.input_ids.shape[1] - seqlen - 1)\n\t        j = i + seqlen\n\t        valenc.append(tmp.input_ids[:, i:j])\n\t    valenc = torch.hstack(valenc)\n\t    class TokenizerWrapper:\n\t        def __init__(self, input_ids):\n\t            self.input_ids = input_ids\n\t    valenc = TokenizerWrapper(valenc)\n", "    return trainloader, valenc\n\tdef get_ptb_new(nsamples, seed, seqlen, model):\n\t    from datasets import load_dataset\n\t    traindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\n\t    testdata = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n\t    from transformers import AutoTokenizer\n\t    try:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n\t    except:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n", "    trainenc = tokenizer(\" \".join(traindata['sentence']), return_tensors='pt')\n\t    testenc = tokenizer(\" \".join(testdata['sentence']), return_tensors='pt')\n\t    import random\n\t    random.seed(seed)\n\t    trainloader = []\n\t    for _ in range(nsamples):\n\t        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n\t        j = i + seqlen\n\t        inp = trainenc.input_ids[:, i:j]\n\t        tar = inp.clone()\n", "        tar[:, :-1] = -100\n\t        trainloader.append((inp, tar))\n\t    return trainloader, testenc\n\tdef get_c4_new(nsamples, seed, seqlen, model):\n\t    from datasets import load_dataset\n\t    traindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')\n\t    valdata = load_dataset('allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation')\n\t    from transformers import AutoTokenizer\n\t    try:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=False)\n", "    except:\n\t        tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)\n\t    import random\n\t    random.seed(seed)\n\t    trainloader = []\n\t    for _ in range(nsamples):\n\t        while True:\n\t            i = random.randint(0, len(traindata) - 1)\n\t            trainenc = tokenizer(traindata[i]['text'], return_tensors='pt')\n\t            if trainenc.input_ids.shape[1] >= seqlen:\n", "                break\n\t        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n\t        j = i + seqlen\n\t        inp = trainenc.input_ids[:, i:j]\n\t        tar = inp.clone()\n\t        tar[:, :-1] = -100\n\t        trainloader.append((inp, tar))\n\t    valenc = tokenizer(' '.join(valdata[:1100]['text']), return_tensors='pt')\n\t    valenc = valenc.input_ids[:, :(256 * seqlen)]\n\t    class TokenizerWrapper:\n", "        def __init__(self, input_ids):\n\t            self.input_ids = input_ids\n\t    valenc = TokenizerWrapper(valenc)\n\t    return trainloader, valenc\n\tdef get_loaders(name, nsamples=128, seed=0, seqlen=2048, model=''):\n\t    if 'wikitext2' in name:\n\t        return get_wikitext2(nsamples, seed, seqlen, model)\n\t    if 'ptb' in name:\n\t        if 'new' in name:\n\t            return get_ptb_new(nsamples, seed, seqlen, model)\n", "        return get_ptb(nsamples, seed, seqlen, model)\n\t    if 'c4' in name:\n\t        if 'new' in name:\n\t            return get_c4_new(nsamples, seed, seqlen, model)\n\t        return get_c4(nsamples, seed, seqlen, model)\n"]}
{"filename": "tools/convert_llama.py", "chunked_list": ["import argparse\n\timport os\n\tfrom transformers.models.llama.convert_llama_weights_to_hf import write_model, write_tokenizer\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n\t    parser.add_argument(\n\t        \"--input_dir\",\n\t        help=\"Location of LLaMA weights, which contains tokenizer.model and model folders\",\n\t    )\n\t    parser.add_argument(\n", "        \"--model_size\",\n\t        choices=[\"7B\", \"13B\", \"30B\", \"65B\", \"tokenizer_only\"],\n\t    )\n\t    parser.add_argument(\n\t        \"--output_dir\",\n\t        help=\"Location to write HF model and tokenizer\",\n\t    )\n\t    args = parser.parse_args()\n\t    if args.model_size != \"tokenizer_only\":\n\t        write_model(\n", "            model_path=os.path.join(args.output_dir, \"llama-{}\".format(args.model_size).lower()),\n\t            input_base_path=os.path.join(args.input_dir, args.model_size),\n\t            model_size=args.model_size,\n\t        )\n\t    write_tokenizer(\n\t        tokenizer_path=os.path.join(args.output_dir, \"llama-{}\".format(args.model_size).lower()),\n\t        input_tokenizer_path=os.path.join(args.input_dir, \"tokenizer.model\"),\n\t    )\n\tif __name__ == \"__main__\":\n\t    main()"]}
{"filename": "tools/gptq.py", "chunked_list": ["import math\n\timport time\n\timport torch\n\timport torch.nn as nn\n\timport transformers\n\timport quant\n\tfrom texttable import Texttable\n\ttorch.backends.cuda.matmul.allow_tf32 = False\n\ttorch.backends.cudnn.allow_tf32 = False\n\tdef torch_snr_error(y_pred: torch.Tensor, y_real: torch.Tensor, reduction: str = 'mean') -> torch.Tensor:\n", "    \"\"\"\n\t    Compute SNR between y_pred(tensor) and y_real(tensor)\n\t    SNR can be calcualted as following equation:\n\t        SNR(pred, real) = (pred - real) ^ 2 / (real) ^ 2\n\t    if x and y are matrixs, SNR error over matrix should be the mean value of SNR error over all elements.\n\t        SNR(pred, real) = mean((pred - real) ^ 2 / (real) ^ 2)\n\t    Args:\n\t        y_pred (torch.Tensor): _description_\n\t        y_real (torch.Tensor): _description_\n\t        reduction (str, optional): _description_. Defaults to 'mean'.\n", "    Raises:\n\t        ValueError: _description_\n\t        ValueError: _description_\n\t    Returns:\n\t        torch.Tensor: _description_\n\t    \"\"\"\n\t    y_pred = y_pred.type(torch.float32)\n\t    y_real = y_real.type(torch.float32)\n\t    if y_pred.shape != y_real.shape:\n\t        raise ValueError(f'Can not compute snr loss for tensors with different shape. '\n", "                         f'({y_pred.shape} and {y_real.shape})')\n\t    reduction = str(reduction).lower()\n\t    if y_pred.ndim == 1:\n\t        y_pred = y_pred.unsqueeze(0)\n\t        y_real = y_real.unsqueeze(0)\n\t    y_pred = y_pred.flatten(start_dim=1)\n\t    y_real = y_real.flatten(start_dim=1)\n\t    noise_power = torch.pow(y_pred - y_real, 2).sum(dim=-1)\n\t    signal_power = torch.pow(y_real, 2).sum(dim=-1)\n\t    snr = (noise_power) / (signal_power + 1e-7)\n", "    if reduction == 'mean':\n\t        return torch.mean(snr)\n\t    elif reduction == 'sum':\n\t        return torch.sum(snr)\n\t    elif reduction == 'none':\n\t        return snr\n\t    else:\n\t        raise ValueError(f'Unsupported reduction method.')\n\tclass GPTQ:\n\t    def __init__(self, layer, observe=False):\n", "        self.layer = layer\n\t        self.dev = self.layer.weight.device\n\t        W = layer.weight.data.clone()\n\t        if isinstance(self.layer, nn.Conv2d):\n\t            W = W.flatten(1)\n\t        if isinstance(self.layer, transformers.Conv1D):\n\t            W = W.t()\n\t        self.rows = W.shape[0]\n\t        self.columns = W.shape[1]\n\t        self.H = torch.zeros((self.columns, self.columns), device=self.dev)\n", "        self.nsamples = 0\n\t        self.quantizer = quant.Quantizer()\n\t        self.observe = observe\n\t    def add_batch(self, inp, out):\n\t        # Hessian H = 2 X XT + λ I\n\t        if self.observe:\n\t            self.inp1 = inp\n\t            self.out1 = out\n\t        else:\n\t            self.inp1 = None\n", "            self.out1 = None\n\t        if len(inp.shape) == 2:\n\t            inp = inp.unsqueeze(0)\n\t        tmp = inp.shape[0]\n\t        if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):\n\t            if len(inp.shape) == 3:\n\t                inp = inp.reshape((-1, inp.shape[-1]))\n\t            inp = inp.t()\n\t        if isinstance(self.layer, nn.Conv2d):\n\t            unfold = nn.Unfold(self.layer.kernel_size, dilation=self.layer.dilation, padding=self.layer.padding, stride=self.layer.stride)\n", "            inp = unfold(inp)\n\t            inp = inp.permute([1, 0, 2])\n\t            inp = inp.flatten(1)\n\t        self.H *= self.nsamples / (self.nsamples + tmp)\n\t        self.nsamples += tmp\n\t        # inp = inp.float()\n\t        inp = math.sqrt(2 / self.nsamples) * inp.float()\n\t        # self.H += 2 / self.nsamples * inp.matmul(inp.t())\n\t        self.H += inp.matmul(inp.t())\n\t    def print_loss(self, name, q_weight, weight_error, timecost):\n", "        table = Texttable()\n\t        name += ' ' * (16 - len(name))\n\t        table.header(['name', 'weight_error', 'fp_inp_SNR', 'q_inp_SNR', 'time'])\n\t        # assign weight\n\t        self.layer.weight.data = q_weight.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)\n\t        if self.inp1 is not None:\n\t            # quantize input to int8\n\t            quantizer = quant.Quantizer()\n\t            quantizer.configure(8, perchannel=False, sym=True, mse=False)\n\t            quantizer.find_params(self.inp1)\n", "            q_in = quantizer.quantize(self.inp1).type(torch.float16)\n\t            q_out = self.layer(q_in)\n\t            # get kinds of SNR\n\t            q_SNR = torch_snr_error(q_out, self.out1).item()\n\t            fp_SNR = torch_snr_error(self.layer(self.inp1), self.out1).item()\n\t        else:\n\t            q_SNR = '-'\n\t            fp_SNR = '-'\n\t        table.add_row([name, weight_error, fp_SNR, q_SNR, timecost])\n\t        print(table.draw().split('\\n')[-2])\n", "    def fasterquant(self, blocksize=128, percdamp=.01, groupsize=-1, actorder=False, name=''):\n\t        self.layer.to(self.dev)\n\t        W = self.layer.weight.data.clone()\n\t        if isinstance(self.layer, nn.Conv2d):\n\t            W = W.flatten(1)\n\t        if isinstance(self.layer, transformers.Conv1D):\n\t            W = W.t()\n\t        W = W.float()\n\t        tick = time.time()\n\t        if not self.quantizer.ready():\n", "            self.quantizer.find_params(W, weight=True)\n\t        H = self.H\n\t        if not self.observe:\n\t            del self.H\n\t        dead = torch.diag(H) == 0\n\t        H[dead, dead] = 1\n\t        W[:, dead] = 0\n\t        if actorder:\n\t            perm = torch.argsort(torch.diag(H), descending=True)\n\t            W = W[:, perm]\n", "            H = H[perm][:, perm]\n\t        Losses = torch.zeros_like(W)\n\t        Q = torch.zeros_like(W)\n\t        damp = percdamp * torch.mean(torch.diag(H))\n\t        diag = torch.arange(self.columns, device=self.dev)\n\t        H[diag, diag] += damp\n\t        H = torch.linalg.cholesky(H)\n\t        H = torch.cholesky_inverse(H)\n\t        H = torch.linalg.cholesky(H, upper=True)\n\t        Hinv = H\n", "        g_idx = []\n\t        scale = []\n\t        zero = []\n\t        now_idx = 1\n\t        for i1 in range(0, self.columns, blocksize):\n\t            i2 = min(i1 + blocksize, self.columns)\n\t            count = i2 - i1\n\t            W1 = W[:, i1:i2].clone()\n\t            Q1 = torch.zeros_like(W1)\n\t            Err1 = torch.zeros_like(W1)\n", "            Losses1 = torch.zeros_like(W1)\n\t            Hinv1 = Hinv[i1:i2, i1:i2]\n\t            for i in range(count):\n\t                w = W1[:, i]\n\t                d = Hinv1[i, i]\n\t                if groupsize != -1:\n\t                    if (i1 + i) % groupsize == 0:\n\t                        self.quantizer.find_params(W[:, (i1 + i):(i1 + i + groupsize)], weight=True)\n\t                    if ((i1 + i) // groupsize) - now_idx == -1:\n\t                        scale.append(self.quantizer.scale)\n", "                        zero.append(self.quantizer.zero)\n\t                        now_idx += 1\n\t                q = self.quantizer.quantize(w.unsqueeze(1)).flatten()\n\t                Q1[:, i] = q\n\t                Losses1[:, i] = (w - q)**2 / d**2\n\t                err1 = (w - q) / d\n\t                W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))\n\t                Err1[:, i] = err1\n\t            Q[:, i1:i2] = Q1\n\t            Losses[:, i1:i2] = Losses1 / 2\n", "            W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])\n\t        torch.cuda.synchronize()\n\t        error = torch.sum(Losses).item()\n\t        groupsize = groupsize if groupsize != -1 else self.columns\n\t        g_idx = [i // groupsize for i in range(self.columns)]\n\t        g_idx = torch.tensor(g_idx, dtype=torch.int32, device=Q.device)\n\t        if actorder:\n\t            invperm = torch.argsort(perm)\n\t            Q = Q[:, invperm]\n\t            g_idx = g_idx[invperm]\n", "        if isinstance(self.layer, transformers.Conv1D):\n\t            Q = Q.t()\n\t        self.print_loss(name=name, q_weight=Q, weight_error=error, timecost=(time.time() - tick))\n\t        if scale == []:\n\t            scale.append(self.quantizer.scale)\n\t            zero.append(self.quantizer.zero)\n\t        scale = torch.cat(scale, dim=1)\n\t        zero = torch.cat(zero, dim=1)\n\t        return scale, zero, g_idx, error\n\t    def free(self):\n", "        self.inp1 = None\n\t        self.out1 = None\n\t        self.H = None\n\t        self.Losses = None\n\t        self.Trace = None\n\t        torch.cuda.empty_cache()\n"]}
{"filename": "tools/convert_pth_to_ggml.py", "chunked_list": ["# This file is from: https://github.com/ggerganov/llama.cpp \n\t# And it converts LLaMA model's pytorch_model.bin to ggml compatible file\n\t# Load the model using Torch\n\t# Iterate over all variables and write them to a binary file.\n\t# For each variable, write the following:\n\t#   - Number of dimensions (int)\n\t#   - Name length (int)\n\t#   - Dimensions (int[n_dims])\n\t#   - Name (char[name_length])\n\t#   - Data (float[n_dims])\n", "#\n\t# By default, the bigger matrices are converted to 16-bit floats.\n\t# This can be disabled by adding the \"use-f32\" CLI argument.\n\t#\n\t# At the start of the ggml file we write the model parameters\n\t# and vocabulary.\n\t#\n\timport os\n\timport sys\n\timport json\n", "import struct\n\timport numpy as np\n\timport torch\n\tfrom sentencepiece import SentencePieceProcessor\n\timport argparse\n\t# args\n\tparser = argparse.ArgumentParser()\n\t# The original base model checkpoint dir\n\tparser.add_argument(\"--dir_model\", type=str, default='lora-Vicuna/checkpoint-3000-with-lora/ckpt')\n\t# The finetuned lora model checkpoint dir\n", "parser.add_argument(\"--dir_out\",type=str, default=None)\n\t# NOTE: you can find it in llama-7b dir\n\tparser.add_argument(\"--fname_tokenizer\", type=str, default=\"lora-Vicuna/llama-7b/tokenizer.model\")\n\t# 0=fp32, 1=fp16\n\tparser.add_argument(\"--ftype\", type=int, default=1)\n\t# NOTE: this parameter is n_parts split of the `consolidated.0x` checkpoint\n\tparser.add_argument(\"--shard\", type=int, default=None)\n\targs = parser.parse_args()\n\tif args.dir_out is None: dir_out = args.dir_model # output in the same directory as the model\n\tdir_model = args.dir_model\n", "ftype=args.ftype\n\tfname_tokenizer=args.fname_tokenizer\n\tfname_hparams   = dir_model + \"/params.json\"\n\t# possible data types\n\t#   ftype == 0 -> float32\n\t#   ftype == 1 -> float16\n\t#\n\t# map from ftype to string\n\tftype_str = [\"f32\", \"f16\"]\n\tif ftype < 0 or ftype > 1:\n", "    print(\"Invalid ftype: \" + str(ftype))\n\t    sys.exit(1)\n\tfname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\"\n\tif os.path.exists(fname_out):\n\t    print(f\"Skip conversion, it already exists: {fname_out}\")\n\t    sys.exit(0)\n\twith open(fname_hparams, \"r\") as f:\n\t    hparams = json.load(f)\n\ttokenizer = SentencePieceProcessor(fname_tokenizer)\n\thparams.update({\"vocab_size\": tokenizer.vocab_size()})\n", "def get_n_parts(dim):\n\t    if dim == 4096:\n\t        return 1\n\t    elif dim == 5120:\n\t        return 2\n\t    elif dim == 6656:\n\t        return 4\n\t    elif dim == 8192:\n\t        return 8\n\t    else:\n", "        print(\"Invalid dim: \" + str(dim))\n\t        sys.exit(1)\n\tif args.shard is None: # default\n\t    n_parts = get_n_parts(hparams[\"dim\"])\n\telse:\n\t    n_parts = args.shard\n\tprint(hparams)\n\tprint('n_parts = ', n_parts)\n\tfor p in range(n_parts):\n\t    print('Processing part ', p)\n", "    fname_model = dir_model + \"/consolidated.0\" + str(p) + \".pth\"\n\t    fname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\"\n\t    if (p > 0):\n\t        fname_out = dir_out + \"/ggml-model-\" + ftype_str[ftype] + \".bin\" + \".\" + str(p)\n\t    model = torch.load(fname_model, map_location=\"cpu\")\n\t    fout = open(fname_out, \"wb\")\n\t    fout.write(struct.pack(\"i\", 0x67676d6c)) # magic: ggml in hex\n\t    fout.write(struct.pack(\"i\", hparams[\"vocab_size\"]))\n\t    fout.write(struct.pack(\"i\", hparams[\"dim\"]))\n\t    fout.write(struct.pack(\"i\", hparams[\"multiple_of\"]))\n", "    fout.write(struct.pack(\"i\", hparams[\"n_heads\"]))\n\t    fout.write(struct.pack(\"i\", hparams[\"n_layers\"]))\n\t    fout.write(struct.pack(\"i\", hparams[\"dim\"] // hparams[\"n_heads\"])) # rot (obsolete)\n\t    fout.write(struct.pack(\"i\", ftype))\n\t    # Is this correct??\n\t    for i in range(tokenizer.vocab_size()):\n\t        if tokenizer.is_unknown(i):\n\t            # \"<unk>\" token (translated as ??)\n\t            text = \" \\u2047 \".encode(\"utf-8\")\n\t            fout.write(struct.pack(\"i\", len(text)))\n", "            fout.write(text)\n\t        elif tokenizer.is_control(i):\n\t            # \"<s>\"/\"</s>\" tokens\n\t            fout.write(struct.pack(\"i\", 0))\n\t        elif tokenizer.is_byte(i):\n\t            # \"<U+XX>\" tokens (which may be invalid UTF-8)\n\t            piece = tokenizer.id_to_piece(i)\n\t            if len(piece) != 6:\n\t                print(\"Invalid token: \" + piece)\n\t                sys.exit(1)\n", "            byte_value = int(piece[3:-1], 16)\n\t            fout.write(struct.pack(\"i\", 1))\n\t            fout.write(struct.pack(\"B\", byte_value))\n\t        else:\n\t            # normal token. Uses U+2581 (LOWER ONE EIGHTH BLOCK) to represent spaces.\n\t            text = tokenizer.id_to_piece(i).replace(\"\\u2581\", \" \").encode(\"utf-8\")\n\t            fout.write(struct.pack(\"i\", len(text)))\n\t            fout.write(text)\n\t    for k, v in model.items():\n\t        name = k\n", "        shape = v.shape\n\t        # skip layers.X.attention.inner_attention.rope.freqs\n\t        if name[-5:] == \"freqs\":\n\t            continue\n\t        print(\"Processing variable: \" + name + \" with shape: \", shape, \" and type: \", v.dtype)\n\t        #data = tf.train.load_variable(dir_model, name).squeeze()\n\t        data = v.numpy().squeeze()\n\t        n_dims = len(data.shape)\n\t        # for efficiency - transpose some matrices\n\t        # \"model/h.*/attn/c_attn/w\"\n", "        # \"model/h.*/attn/c_proj/w\"\n\t        # \"model/h.*/mlp/c_fc/w\"\n\t        # \"model/h.*/mlp/c_proj/w\"\n\t        #if name[-14:] == \"/attn/c_attn/w\" or \\\n\t        #   name[-14:] == \"/attn/c_proj/w\" or \\\n\t        #   name[-11:] == \"/mlp/c_fc/w\" or \\\n\t        #   name[-13:] == \"/mlp/c_proj/w\":\n\t        #    print(\"  Transposing\")\n\t        #    data = data.transpose()\n\t        dshape = data.shape\n", "        # default type is fp16\n\t        ftype_cur = 1\n\t        if ftype == 0 or n_dims == 1:\n\t            print(\"  Converting to float32\")\n\t            data = data.astype(np.float32)\n\t            ftype_cur = 0\n\t        # header\n\t        sname = name.encode('utf-8')\n\t        fout.write(struct.pack(\"iii\", n_dims, len(sname), ftype_cur))\n\t        for i in range(n_dims):\n", "            fout.write(struct.pack(\"i\", dshape[n_dims - 1 - i]))\n\t        fout.write(sname)\n\t        # data\n\t        data.tofile(fout)\n\t    # I hope this deallocates the memory ..\n\t    model = None\n\t    fout.close()\n\t    print(\"Done. Output file: \" + fname_out + \", (part \", p, \")\")\n\t    print(\"\")\n"]}
{"filename": "tools/quant_generate.py", "chunked_list": ["import sys\n\timport torch\n\timport torch.nn as nn\n\timport transformers\n\timport gradio as gr\n\timport argparse\n\timport warnings\n\timport os\n\timport quant\n\tfrom gptq import GPTQ\n", "from datautils import get_loaders\n\tassert (\n\t    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n\t), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n\tfrom transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n\tdef find_layers(module, layers=[nn.Conv2d, nn.Linear], name=''):\n\t    if type(module) in layers:\n\t        return {name: module}\n\t    res = {}\n\t    for name1, child in module.named_children():\n", "        res.update(find_layers(child, layers=layers, name=name + '.' + name1 if name != '' else name1))\n\t    return res\n\tdef load_quant(model, checkpoint, wbits, groupsize=-1, fused_mlp=True, eval=True, warmup_autotune=True):\n\t    from transformers import LlamaConfig, LlamaForCausalLM\n\t    config = LlamaConfig.from_pretrained(model)\n\t    def noop(*args, **kwargs):\n\t        pass\n\t    torch.nn.init.kaiming_uniform_ = noop\n\t    torch.nn.init.uniform_ = noop\n\t    torch.nn.init.normal_ = noop\n", "    torch.set_default_dtype(torch.half)\n\t    transformers.modeling_utils._init_weights = False\n\t    torch.set_default_dtype(torch.half)\n\t    model = LlamaForCausalLM(config)\n\t    torch.set_default_dtype(torch.float)\n\t    if eval:\n\t        model = model.eval()\n\t    layers = find_layers(model)\n\t    for name in ['lm_head']:\n\t        if name in layers:\n", "            del layers[name]\n\t    quant.make_quant_linear(model, layers, wbits, groupsize)\n\t    del layers\n\t    print('Loading model ...')\n\t    model.load_state_dict(torch.load(checkpoint), strict=False)\n\t    quant.make_quant_attn(model)\n\t    if eval and fused_mlp:\n\t        quant.make_fused_mlp(model)\n\t    if warmup_autotune:\n\t        quant.autotune_warmup_linear(model, transpose=not (eval))\n", "        if eval and fused_mlp:\n\t            quant.autotune_warmup_fused(model)\n\t    model.seqlen = 2048\n\t    print('Done.')\n\t    return model\n\tdef generate_prompt(instruction, input=None):\n\t    if input:\n\t        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\t        ### Instruction:\n\t        {instruction}\n", "        ### Input:\n\t        {input}\n\t        ### Response:\"\"\"\n\t    else:\n\t        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\t        ### Instruction:\n\t        {instruction}\n\t        ### Response:\"\"\"\n\tdef main():\n\t    parser = argparse.ArgumentParser()\n", "    parser.add_argument(\"--model_path\",type=str,default=\"decapoda-research/llama-7b-hf\",help=\"llama huggingface model to load\")\n\t    parser.add_argument(\"--quant_path\",type=str,default=\"llama7b-8bit-128g.pt\",help=\"the quantified model path\")\n\t    parser.add_argument(\n\t                        \"--wbits\",\n\t                        type=int,\n\t                        default=4,\n\t                        choices=[2, 3, 4, 8],\n\t                        help=\"bits to use for quantization; use 8 for evaluating base model.\")\n\t    parser.add_argument('--text', type=str, default='the mean of life is', help='input text')\n\t    parser.add_argument('--min_length', type=int, default=10, help='The minimum length of the sequence to be generated.')\n", "    parser.add_argument('--max_length', type=int, default=256, help='The maximum length of the sequence to be generated.')\n\t    parser.add_argument('--top_p',\n\t                        type=float,\n\t                        default=0.95,\n\t                        help='If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to top_p or higher are kept for generation.')\n\t    parser.add_argument('--temperature', type=float, default=0.1, help='The value used to module the next token probabilities.')\n\t    parser.add_argument('--repetition_penalty',type=float, default=2.0, help='The parameter for repetition penalty. 1.0 means no penalty(0~10)')\n\t    parser.add_argument('--groupsize', type=int, default=-1, help='Groupsize to use for quantization; default uses full row.')\n\t    parser.add_argument('--gradio', action='store_true', help='Whether to use gradio to present results.')\n\t    args = parser.parse_args()\n", "    if torch.cuda.is_available():\n\t        device = \"cuda\"\n\t    else:\n\t        device = \"cpu\"\n\t    model = load_quant(args.model_path, args.quant_path, args.wbits, args.groupsize)\n\t    model.to(device)\n\t    tokenizer = LlamaTokenizer.from_pretrained(args.model_path)\n\t    model.eval()\n\t    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t        model = torch.compile(model)\n", "    #[Way1]: drectly generate\n\t    if not args.gradio:\n\t        input_ids = tokenizer.encode(args.text, return_tensors=\"pt\").to(device)\n\t        with torch.no_grad():\n\t            generated_ids = model.generate(\n\t                input_ids,\n\t                min_new_tokens=args.min_length,\n\t                max_new_tokens=args.max_length,\n\t                top_p=args.top_p,\n\t                temperature=args.temperature,\n", "                repetition_penalty=args.repetition_penalty,\n\t            )\n\t        print(\"*\"*80)\n\t        print(\"🦙:\", tokenizer.decode([el.item() for el in generated_ids[0]],skip_special_tokens=True))\n\t    #[Way2]: generate through the gradio interface\n\t    else:   \n\t        def evaluate(\n\t            input,\n\t            temperature=0.1,\n\t            top_p=0.75,\n", "            top_k=40,\n\t            num_beams=1,\n\t            max_new_tokens=128,\n\t            repetition_penalty=1.0,\n\t            **kwargs,\n\t        ):\n\t            prompt = generate_prompt(input)\n\t            inputs = tokenizer(prompt, return_tensors=\"pt\")\n\t            input_ids = inputs[\"input_ids\"].to(device)\n\t            generation_config = GenerationConfig(\n", "                temperature=temperature,\n\t                top_p=top_p,\n\t                top_k=top_k,\n\t                num_beams=num_beams,\n\t                **kwargs,\n\t            )\n\t            with torch.no_grad():\n\t                generation_output = model.generate(\n\t                    input_ids=input_ids,\n\t                    generation_config=generation_config,\n", "                    return_dict_in_generate=True,\n\t                    output_scores=True,\n\t                    max_new_tokens=max_new_tokens,\n\t                    repetition_penalty=float(repetition_penalty),\n\t                )\n\t            s = generation_output.sequences[0]\n\t            output = tokenizer.decode(s,skip_special_tokens=True)\n\t            return output.split(\"### Response:\")[1].strip()\n\t        gr.Interface(\n\t            fn=evaluate,\n", "            inputs=[\n\t                gr.components.Textbox(\n\t                    lines=2, label=\"Input\", placeholder=\"Tell me about alpacas.\"\n\t                ),\n\t                gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n\t                gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n\t                gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n\t                gr.components.Slider(minimum=1, maximum=5, step=1, value=1, label=\"Beams\"),\n\t                gr.components.Slider(\n\t                    minimum=1, maximum=2000, step=1, value=256, label=\"Max tokens\"\n", "                ),\n\t                gr.components.Slider(\n\t                    minimum=0.1, maximum=10.0, step=0.1, value=1.0, label=\"Repetition Penalty\"\n\t                ),\n\t            ],\n\t            outputs=[\n\t                gr.inputs.Textbox(\n\t                    lines=5,\n\t                    label=\"Output\",\n\t                )\n", "            ],\n\t            title=\"Chinese-Vicuna 中文小羊驼\",\n\t            description=\"中文小羊驼由各种高质量的开源instruction数据集，结合Alpaca-lora的代码训练而来，模型基于开源的llama7B，主要贡献是对应的lora模型。由于代码训练资源要求较小，希望为llama中文lora社区做一份贡献。\",\n\t        ).launch(share=True)\n\tif __name__ == '__main__':\n\t    main()\n"]}
{"filename": "tools/application/chatglm_lora_test.py", "chunked_list": ["import os\n\timport tqdm\n\timport joblib\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom transformers import AutoTokenizer, AutoModel\n\timport peft\n\timport loralib as lora\n\tfrom peft import LoraConfig\n", "import json\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.data import Dataset\n\tfrom accelerate import Accelerator, DeepSpeedPlugin\n\tfrom transformers import get_linear_schedule_with_warmup\n\t\"\"\"\n\textra requirements:\n\t    pip install icetk\n\t\"\"\"\n\t# reload the model: no int8, so 14GB is needed\n", "version = 'no.pt' # finetune_0.pt\n\tmodel_dir = '/home/liang/lzy_tmp/models/chatglm-6b'\n\ttokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n\tmodel = AutoModel.from_pretrained(model_dir, trust_remote_code=True)\n\tconfig = LoraConfig(\n\t    peft_type=\"LORA\", \n\t    task_type=\"SEQ_2_SEQ_LM\", \n\t    r=32, \n\t    lora_alpha=32, \n\t    target_modules=[\"q\", \"k\", \"v\"],\n", "    lora_dropout=0.1, \n\t)\n\tclass QKV_layer(torch.nn.Module):\n\t    def __init__(self, in_features, out_features):\n\t        super(QKV_layer, self).__init__()\n\t        self.linear_q = torch.nn.Linear(in_features, out_features//3)\n\t        self.linear_k = torch.nn.Linear(in_features, out_features//3)\n\t        self.linear_v = torch.nn.Linear(in_features, out_features//3)\n\t    def update(self, target_layer):\n\t        self.linear_q.weight.data = target_layer.weight[:target_layer.out_features//3, :].data\n", "        self.linear_q.bias.data = target_layer.bias[:target_layer.out_features//3].data\n\t        self.linear_k.weight.data = target_layer.weight[target_layer.out_features//3:target_layer.out_features//3*2, :].data\n\t        self.linear_k.bias.data = target_layer.bias[target_layer.out_features//3:target_layer.out_features//3*2].data\n\t        self.linear_v.weight.data = target_layer.weight[target_layer.out_features//3*2:, :].data\n\t        self.linear_v.bias.data = target_layer.bias[target_layer.out_features//3*2:].data\n\t    def forward(self, x):\n\t        q = self.linear_q(x)\n\t        k = self.linear_k(x)\n\t        v = self.linear_v(x)\n\t        return torch.concat([q,k,v], dim = -1)\n", "if version != 'no.pt':\n\t    # convert it again\n\t    for key, module in model.named_modules():\n\t        if key.endswith('attention'):\n\t            try:\n\t                qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n\t                qkv_layer.update(module.query_key_value)\n\t                module.query_key_value = qkv_layer\n\t            except:\n\t                pass\n", "            module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)\n\t    # load the LoRA checkpoint\n\t    model.load_state_dict(torch.load(f'/{model_dir}/{version}'), strict=False)\n\tmodel.half().cuda().eval()\n\t# Let's chat!\n\tos.makedirs('outs/chatglm-6b/', exist_ok=True)\n\twith open(f'outs/chatglm-6b/test_{version}.txt','w') as f:\n\t    for text in open('sample/test.jsonl'):\n\t        text = json.loads(text)\n\t        inputs = text['instruction']\n", "        print('Q:', inputs)\n\t        print('Q:', inputs, file=f)\n\t        response, history = model.chat(tokenizer, inputs, history=[])\n\t        print('A:', response)\n\t        print('A:', response, '\\n',file=f)"]}
{"filename": "tools/application/chitchat_finetune.py", "chunked_list": ["import os\n\timport sys\n\timport wandb\n\timport torch\n\timport torch.nn as nn\n\timport bitsandbytes as bnb\n\tfrom datasets import load_dataset\n\timport transformers\n\timport argparse\n\tfrom transformers import LlamaForCausalLM, LlamaTokenizer\n", "from peft import (\n\t    prepare_model_for_int8_training,\n\t    LoraConfig,\n\t    get_peft_model,\n\t    get_peft_model_state_dict,\n\t)\n\t# Used for chitchat dataset\n\t# 用于闲聊对话数据\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--wandb\", action=\"store_true\", default=False)\n", "parser.add_argument(\"--data_path\", type=str, default=\"datasets/chitchat-1e5.json\") # for example: LCCC \n\tparser.add_argument(\"--output_path\", type=str, default=\"outs/13B\")\n\tparser.add_argument(\"--model_path\", type=str, default=\"../model/13B_hf\")\n\tparser.add_argument(\"--eval_steps\", type=int, default=200)\n\tparser.add_argument(\"--save_steps\", type=int, default=200)\n\tparser.add_argument(\"--test_size\", type=int, default=0)\n\targs = parser.parse_args()\n\t# optimized for RTX 4090. for larger GPUs, increase some of these?\n\tMICRO_BATCH_SIZE = 24  # this could actually be 5 but i like powers of 2\n\tBATCH_SIZE = 128\n", "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n\tEPOCHS = 2  # we don't always need 3 tbh\n\tLEARNING_RATE = 3e-4  # the Karpathy constant\n\tCUTOFF_LEN = 341  # max:341\n\tLORA_R = 8\n\tLORA_ALPHA = 16\n\tLORA_DROPOUT = 0.05\n\tVAL_SET_SIZE = args.test_size #2000\n\tTARGET_MODULES = [\n\t    \"q_proj\",\n", "    \"v_proj\",\n\t]\n\tDATA_PATH = args.data_path \n\tOUTPUT_DIR = args.output_path #\"lora-Vicuna\"\n\tdevice_map = \"auto\"\n\tworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\tddp = world_size != 1\n\tif ddp:\n\t    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n\t    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n", "if args.wandb:\n\t    wandb.login(key = '41327ad68395c1a5e5e3827fa5ee97944740250d') # luzhenyi\n\t    wandb.init(\n\t        project=\"LoRA\",\n\t        name=f\"{args.model_path}-{args.data_path}\",\n\t        config=None,\n\t    )\n\telse:\n\t    wandb.init(mode='disabled')\n\ttokenizer = LlamaTokenizer.from_pretrained(\n", "    args.model_path, add_eos_token=True\n\t)\n\ttokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n\tdata = load_dataset(\"json\", data_files=DATA_PATH)\n\tPROMPT_DICT = {\n\t    \"prompt_input\": (\n\t        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n\t        \"Write a response that appropriately completes the request.\\n\\n\"\n\t        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n\t    ),\n", "    \"prompt_no_input\": (\n\t        \"Below is an instruction that describes a task. \"\n\t        \"Write a response that appropriately completes the request.\\n\\n\"\n\t        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n\t    ),\n\t}\n\tCHAT_DICT = {\n\t    'prompt': (\n\t        \"The following is a conversation between an AI assistant called Bot and a human user called User.\"\n\t        \"Bot is is intelligent, knowledgeable, wise and polite.\\n\\n\"\n", "    ),\n\t    'history': (\n\t        \"User:\\n{input}\\n\\nBot:{output}\\n\\n\"\n\t    ),\n\t    'input': (\n\t        \"### User:\\n{input}\\n\\n### Bot:\"\n\t    )\n\t}\n\tdef tokenize(prompt):\n\t    # there's probably a way to do this with the tokenizer settings\n", "    # but again, gotta move fast\n\t    result = tokenizer(\n\t        prompt,\n\t        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t        padding=\"max_length\",\n\t    )\n\t    return {\n\t        \"input_ids\": result[\"input_ids\"][:-1],\n\t        \"attention_mask\": result[\"attention_mask\"][:-1],\n", "    }\n\tdef generate_and_tokenize_prompt(data_point):\n\t    # This function masks out the labels for the input,\n\t    # so that our loss is computed only on the response.\n\t    user_prompt = CHAT_DICT['prompt']\n\t    for history in data_point['history']:\n\t        user_prompt+= CHAT_DICT['history'].format_map(history) \n\t    user_prompt += CHAT_DICT['input'].format_map(data_point)\n\t    len_user_prompt_tokens = (len(tokenizer(\n\t        user_prompt,\n", "        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t    )[\"input_ids\"])- 1)  # no eos token\n\t    full_tokens = tokenizer(\n\t        user_prompt + data_point[\"output\"],\n\t        truncation=True,\n\t        max_length=CUTOFF_LEN + 1,\n\t        padding=\"max_length\", # pad到最长\n\t    )[\"input_ids\"][:-1]\n\t    return {\n", "        \"input_ids\": full_tokens,\n\t        \"labels\": [-100] * len_user_prompt_tokens + full_tokens[len_user_prompt_tokens:],\n\t        \"attention_mask\": [1] * (len(full_tokens)),\n\t    }\n\tif VAL_SET_SIZE > 0:\n\t    train_val = data[\"train\"].train_test_split(\n\t        test_size=VAL_SET_SIZE, shuffle=True, seed=42\n\t    )\n\t    train_data = train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt,num_proc=12)\n\t    val_data = train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt,num_proc=12)\n", "else:\n\t    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt,num_proc=12)\n\t    val_data = None\n\tmodel = LlamaForCausalLM.from_pretrained(\n\t    args.model_path,\n\t    load_in_8bit=True,\n\t    device_map=device_map,\n\t)\n\tmodel = prepare_model_for_int8_training(model)\n\tconfig = LoraConfig(\n", "    r=LORA_R,\n\t    lora_alpha=LORA_ALPHA,\n\t    target_modules=TARGET_MODULES,\n\t    lora_dropout=LORA_DROPOUT,\n\t    bias=\"none\",\n\t    task_type=\"CAUSAL_LM\",\n\t)\n\tmodel = get_peft_model(model, config)\n\ttrainer = transformers.Trainer(\n\t    model=model,\n", "    train_dataset=train_data,\n\t    eval_dataset=val_data,\n\t    args=transformers.TrainingArguments(\n\t        per_device_train_batch_size=MICRO_BATCH_SIZE,\n\t        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n\t        warmup_steps=100,\n\t        num_train_epochs=EPOCHS,\n\t        learning_rate=LEARNING_RATE,\n\t        fp16=True,\n\t        logging_steps=20,\n", "        evaluation_strategy=\"steps\" if VAL_SET_SIZE > 0 else \"no\",\n\t        save_strategy=\"steps\",\n\t        eval_steps=args.eval_steps if VAL_SET_SIZE > 0 else None,\n\t        save_steps=args.save_steps,\n\t        output_dir=OUTPUT_DIR,\n\t        load_best_model_at_end=True if VAL_SET_SIZE > 0 else False,\n\t        ddp_find_unused_parameters=False if ddp else None,\n\t        report_to=\"wandb\" if args.wandb else [],\n\t    ),\n\t    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n", ")\n\tmodel.config.use_cache = False\n\told_state_dict = model.state_dict\n\tmodel.state_dict = (\n\t    lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n\t).__get__(model, type(model))\n\tif torch.__version__ >= \"2\" and sys.platform != \"win32\":\n\t    model = torch.compile(model)\n\ttrainer.train()\n\tmodel.save_pretrained(OUTPUT_DIR)\n", "print(\"\\n If there's a warning about missing keys above, please disregard :)\")"]}
{"filename": "tools/application/chatglm_lora_finetune.py", "chunked_list": ["### Load Model From huggingface\n\timport os\n\timport tqdm\n\timport joblib\n\timport numpy as np\n\timport pandas as pd\n\timport torch\n\tfrom transformers import AutoTokenizer, AutoModel\n\timport wandb\n\timport peft\n", "import loralib as lora\n\tfrom peft import LoraConfig\n\timport json\n\tfrom torch.utils.data import DataLoader\n\tfrom torch.utils.data import Dataset\n\tfrom accelerate import Accelerator, DeepSpeedPlugin\n\tfrom transformers import get_linear_schedule_with_warmup\n\t\"\"\"\n\textra requirements: \n\t    pip install icetk\n", "\"\"\"\n\tcheckpoint = \"/model/chatglm-6b\"\n\tdatafile='datasets/merge.json'\n\tout_dir= 'outs/chatglm-6b'\n\tuse_wandb=True\n\tmixed_precision = 'bf16'\n\taccumulate_step = 8\n\tlog_interval = 100\n\tPer_GPU_BATCH_SIZE = 2\n\tMAX_LENGTH = 256 # have huge impact on VRAM: 968:1, 256:4\n", "config = LoraConfig(\n\t    peft_type=\"LORA\", \n\t    r=32,\n\t    lora_alpha=32,\n\t    target_modules=[\"q\", \"k\", \"v\"],\n\t    lora_dropout=0.1, \n\t)\n\tLR = 2e-5\n\tNUM_EPOCHS = 3\n\twarm_up_ratio = 0.1\n", "device_map = \"auto\"\n\tworld_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n\tddp = world_size != 1\n\tif ddp:\n\t    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n\tif use_wandb:\n\t    wandb.init(\n\t        project=\"LoRA\",\n\t        name=f\"{checkpoint}-{datafile}\",\n\t        config=None,\n", "    )\n\telse:\n\t    wandb.init(mode='disabled')\n\tos.makedirs(out_dir, exist_ok=True)\n\ttokenizer = AutoTokenizer.from_pretrained(\n\t    checkpoint, \n\t    trust_remote_code=True,\n\t    device_map=device_map,\n\t)\n\t# BUG: must remove special token '[MASK]'\n", "# del tokenizer.vocab['MASK'] \n\t### Dataset\n\tEOS_ID = 150005\n\tPROMPT_DICT = {\n\t    \"prompt_input\": (\n\t        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n\t        \"Write a response that appropriately completes the request.\\n\\n\"\n\t        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n\t    ),\n\t    \"prompt_no_input\": (\n", "        \"Below is an instruction that describes a task. \"\n\t        \"Write a response that appropriately completes the request.\\n\\n\"\n\t        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n\t    ),\n\t}\n\twith open(datafile, 'r') as f:\n\t    content = json.load(f)\n\tpairs = []\n\tfor line in content:\n\t    if line['input'] == '':\n", "        prompt = PROMPT_DICT['prompt_no_input'].format_map(line)\n\t    else:\n\t        prompt = PROMPT_DICT['prompt_input'].format_map(line)\n\t    completion = line['output']+'</s>'\n\t    if len(prompt) + len(completion) < MAX_LENGTH:\n\t        pairs.append({'prompt':prompt, 'completion':completion})\n\tclass AlpacaDataset(Dataset):\n\t    def __init__(self, pairs, tokenizer) -> None:\n\t        super().__init__()\n\t        self.pairs = pairs\n", "        self.tokenizer = tokenizer\n\t    def __getitem__(self, index):\n\t        if self.pairs[index]['completion'][-4:] == '</s>':\n\t            prompt = self.tokenizer.encode(self.pairs[index]['prompt'])\n\t            completion = self.tokenizer.encode(self.pairs[index]['completion'][:-4], add_special_tokens=False)\n\t            completion += [EOS_ID]\n\t        else:\n\t            prompt = self.tokenizer.encode(self.pairs[index]['prompt'])\n\t            completion = self.tokenizer.encode(self.pairs[index]['completion'], add_special_tokens=False)\n\t        if 150001 not in prompt:\n", "            prompt = self.pairs[index]['prompt'].replace('[MASK]', '//MASK//').replace('[gMASK]', '//gMASK//')\n\t            completion = self.pairs[index]['completion'].replace('[MASK]', '//MASK//').replace('[gMASK]', '//gMASK//')\n\t            prompt = self.tokenizer.encode(prompt)\n\t            completion = self.tokenizer.encode(completion, add_special_tokens=False)\n\t            if 150001 not in prompt:\n\t                import pdb; pdb.set_trace()\n\t        return {'prompt':prompt, 'completion':completion}\n\t    def __len__(self):\n\t        return len(self.pairs)\n\tdef collate_fn(batch):\n", "    input_ids = []\n\t    labels = []\n\t    position_ids = []\n\t    device='cuda:0'\n\t    _max_length = max([len(obj['prompt'])+len(obj['completion']) for obj in batch])\n\t    attention_mask = torch.ones((len(batch), _max_length, _max_length), device=device)\n\t    attention_mask.tril_()\n\t    for i, obj in enumerate(batch):\n\t        context_length = obj['prompt'].index(150004)\n\t        attention_mask[i, :, :context_length] = 1\n", "        to_pad = _max_length - len(obj['prompt']) - len(obj['completion'])\n\t        input_ids.append(obj['prompt'] + obj['completion'] + [tokenizer.pad_token_id] * to_pad)\n\t        position_ids.append(torch.stack(\n\t            [torch.arange(0, _max_length, device=device), \n\t            torch.concat([torch.zeros(context_length - 1, device=device), \n\t            torch.arange(0, _max_length - context_length + 1, device=device)])]).long()\n\t        )\n\t        labels.append(torch.tensor([-100] * len(obj['prompt']) + obj['completion'] + [-100] * to_pad, device=device).long())\n\t    attention_mask.unsqueeze_(1)\n\t    attention_mask = (attention_mask < 0.5).bool()\n", "    return {'input_ids': torch.tensor(input_ids).long(), \n\t            'attention_mask': attention_mask, \n\t            'labels': torch.stack(labels),\n\t            'position_ids':torch.stack(position_ids)}\n\ttrain_dataset = AlpacaDataset(pairs,tokenizer=tokenizer,)\n\ttrain_dataloader = DataLoader(dataset=train_dataset, collate_fn = collate_fn, shuffle=True, batch_size=Per_GPU_BATCH_SIZE)\n\t# check\n\tfor step, batch in enumerate(t:=tqdm.tqdm(train_dataloader)):\n\t    pass\n\tmodel = AutoModel.from_pretrained(\n", "    checkpoint, \n\t    trust_remote_code=True,\n\t)\n\tdeepspeed_plugin = DeepSpeedPlugin(zero_stage=2, gradient_accumulation_steps=accumulate_step)\n\taccelerator = Accelerator(mixed_precision=mixed_precision, gradient_accumulation_steps=accumulate_step, deepspeed_plugin=deepspeed_plugin)\n\tdevice = accelerator.device\n\t### Insert LoRA to model\n\tclass QKV_layer(torch.nn.Module):\n\t    def __init__(self, in_features, out_features):\n\t        super(QKV_layer, self).__init__()\n", "        self.linear_q = torch.nn.Linear(in_features, out_features//3)\n\t        self.linear_k = torch.nn.Linear(in_features, out_features//3)\n\t        self.linear_v = torch.nn.Linear(in_features, out_features//3)\n\t    def update(self, target_layer):\n\t        self.linear_q.weight.data = target_layer.weight[:target_layer.out_features//3, :].data\n\t        self.linear_q.bias.data = target_layer.bias[:target_layer.out_features//3].data\n\t        self.linear_k.weight.data = target_layer.weight[target_layer.out_features//3:target_layer.out_features//3*2, :].data\n\t        self.linear_k.bias.data = target_layer.bias[target_layer.out_features//3:target_layer.out_features//3*2].data\n\t        self.linear_v.weight.data = target_layer.weight[target_layer.out_features//3*2:, :].data\n\t        self.linear_v.bias.data = target_layer.bias[target_layer.out_features//3*2:].data\n", "    def forward(self, x):\n\t        q = self.linear_q(x)\n\t        k = self.linear_k(x)\n\t        v = self.linear_v(x)\n\t        return torch.concat([q,k,v], dim = -1)\n\tfor key, module in model.named_modules():\n\t    if key.endswith('attention'):\n\t        if isinstance(module.query_key_value, peft.tuners.lora.LoraModel):\n\t            module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value.model)\n\t        else:\n", "            # Here we split the query_key_value layer into three linear layer for LoRA. But you can also use merged linear.\n\t            qkv_layer = QKV_layer(module.query_key_value.in_features, module.query_key_value.out_features) \n\t            qkv_layer.update(module.query_key_value)\n\t            module.query_key_value = qkv_layer\n\t            module.query_key_value = peft.tuners.lora.LoraModel(config, module.query_key_value)\n\tlora.mark_only_lora_as_trainable(model)\n\tmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\n\ttrainable_params = sum([np.prod(p.size()) for p in model_parameters])\n\tnon_trainable_params = sum([np.prod(p.size()) for p in model_parameters])\n\tprint('trainable_params:{} ({:.2f}%), non_trainable_params:{}'.format(\n", "    trainable_params, trainable_params/non_trainable_params*100,non_trainable_params\n\t))\n\t### Training\n\toptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n\tlr_scheduler = get_linear_schedule_with_warmup(\n\t    optimizer=optimizer,\n\t    num_warmup_steps=int(len(train_dataloader) / accumulate_step * warm_up_ratio),\n\t    num_training_steps=(int(len(train_dataloader) / accumulate_step) * NUM_EPOCHS),\n\t)\n\tmodel, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)\n", "model.to(device).train()\n\tfor epoch in range(NUM_EPOCHS):\n\t    total_loss = 0\n\t    for step, batch in enumerate(t:=tqdm.tqdm(train_dataloader)):\n\t        with accelerator.accumulate(model):\n\t            outputs = model(**batch)\n\t            loss_detach = outputs.loss.detach().cpu().float()\n\t            # t.set_description(f\"loss: {loss_detach}\")\n\t            t.set_postfix(loss=loss_detach.item())\n\t            total_loss += loss_detach\n", "            loss = outputs.loss\n\t            if accelerator.is_main_process:\n\t                if step % log_interval == 0:\n\t                    wandb.log({\n\t                        'train/loss': loss_detach.item(),\n\t                    })\n\t            accelerator.backward(loss)\n\t            optimizer.step()\n\t            lr_scheduler.step()\n\t            optimizer.zero_grad()\n", "    accelerator.wait_for_everyone()\n\t    if accelerator.is_main_process:\n\t        peft_model_id = f\"finetune_{epoch}\"\n\t        accelerator.save(lora.lora_state_dict(accelerator.unwrap_model(model)), f'{out_dir}/{peft_model_id}.pt')\n"]}
{"filename": "tools/quant/custom_autotune.py", "chunked_list": ["#https://github.com/fpgaminer/GPTQ-triton\n\t\"\"\"\n\tMostly the same as the autotuner in Triton, but with a few changes like using 40 runs instead of 100.\n\t\"\"\"\n\timport builtins\n\timport math\n\timport time\n\tfrom typing import Dict\n\timport triton\n\tclass Autotuner(triton.KernelInterface):\n", "    def __init__(self, fn, arg_names, configs, key, reset_to_zero, prune_configs_by: Dict = None, nearest_power_of_two: bool = False):\n\t        '''\n\t\t\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n\t\t\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n\t\t\t\t'top_k': number of configs to bench\n\t\t\t\t'prune_num_stages_by'(optional): a function used to prune num_stages. It take configs:List[Config] as its input, and returns pruned configs.\n\t\t\t\t'nearest_power_of_two'(optional): whether to round key arguments to the nearest power of two when caching tuning results\n\t\t\t'''\n\t        if not configs:\n\t            self.configs = [triton.Config({}, num_warps=4, num_stages=2)]\n", "        else:\n\t            self.configs = configs\n\t        self.key_idx = [arg_names.index(k) for k in key]\n\t        self.nearest_power_of_two = nearest_power_of_two\n\t        self.cache = {}\n\t        # hook to reset all required tensor to zeros before relaunching a kernel\n\t        self.hook = lambda args: 0\n\t        if reset_to_zero is not None:\n\t            self.reset_idx = [arg_names.index(k) for k in reset_to_zero]\n\t            def _hook(args):\n", "                for i in self.reset_idx:\n\t                    args[i].zero_()\n\t            self.hook = _hook\n\t        self.arg_names = arg_names\n\t        # prune configs\n\t        if prune_configs_by:\n\t            perf_model, top_k = prune_configs_by['perf_model'], prune_configs_by['top_k']\n\t            if 'early_config_prune' in prune_configs_by:\n\t                early_config_prune = prune_configs_by['early_config_prune']\n\t        else:\n", "            perf_model, top_k, early_config_prune = None, None, None\n\t        self.perf_model, self.configs_top_k = perf_model, top_k\n\t        self.early_config_prune = early_config_prune\n\t        self.fn = fn\n\t    def _bench(self, *args, config, **meta):\n\t        # check for conflicts, i.e. meta-parameters both provided\n\t        # as kwargs and by the autotuner\n\t        conflicts = meta.keys() & config.kwargs.keys()\n\t        if conflicts:\n\t            raise ValueError(f\"Conflicting meta-parameters: {', '.join(conflicts)}.\"\n", "                             \" Make sure that you don't re-define auto-tuned symbols.\")\n\t        # augment meta-parameters with tunable ones\n\t        current = dict(meta, **config.kwargs)\n\t        def kernel_call():\n\t            if config.pre_hook:\n\t                config.pre_hook(self.nargs)\n\t            self.hook(args)\n\t            self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **current)\n\t        try:\n\t            # In testings using only 40 reps seems to be close enough and it appears to be what PyTorch uses\n", "            # PyTorch also sets fast_flush to True, but I didn't see any speedup so I'll leave the default\n\t            return triton.testing.do_bench(kernel_call, percentiles=(0.5, 0.2, 0.8), rep=40)\n\t        except triton.compiler.OutOfResources:\n\t            return (float('inf'), float('inf'), float('inf'))\n\t    def run(self, *args, **kwargs):\n\t        self.nargs = dict(zip(self.arg_names, args))\n\t        if len(self.configs) > 1:\n\t            key = tuple(args[i] for i in self.key_idx)\n\t            # This reduces the amount of autotuning by rounding the keys to the nearest power of two\n\t            # In my testing this gives decent results, and greatly reduces the amount of tuning required\n", "            if self.nearest_power_of_two:\n\t                key = tuple([2**int(math.log2(x) + 0.5) for x in key])\n\t            if key not in self.cache:\n\t                # prune configs\n\t                pruned_configs = self.prune_configs(kwargs)\n\t                bench_start = time.time()\n\t                timings = {config: self._bench(*args, config=config, **kwargs) for config in pruned_configs}\n\t                bench_end = time.time()\n\t                self.bench_time = bench_end - bench_start\n\t                self.cache[key] = builtins.min(timings, key=timings.get)\n", "                self.hook(args)\n\t                self.configs_timings = timings\n\t            config = self.cache[key]\n\t        else:\n\t            config = self.configs[0]\n\t        self.best_config = config\n\t        if config.pre_hook is not None:\n\t            config.pre_hook(self.nargs)\n\t        return self.fn.run(*args, num_warps=config.num_warps, num_stages=config.num_stages, **kwargs, **config.kwargs)\n\t    def prune_configs(self, kwargs):\n", "        pruned_configs = self.configs\n\t        if self.early_config_prune:\n\t            pruned_configs = self.early_config_prune(self.configs, self.nargs)\n\t        if self.perf_model:\n\t            top_k = self.configs_top_k\n\t            if isinstance(top_k, float) and top_k <= 1.0:\n\t                top_k = int(len(self.configs) * top_k)\n\t            if len(pruned_configs) > top_k:\n\t                est_timing = {config: self.perf_model(**self.nargs, **kwargs, **config.kwargs, num_stages=config.num_stages, num_warps=config.num_warps) for config in pruned_configs}\n\t                pruned_configs = sorted(est_timing.keys(), key=lambda x: est_timing[x])[:top_k]\n", "        return pruned_configs\n\t    def warmup(self, *args, **kwargs):\n\t        self.nargs = dict(zip(self.arg_names, args))\n\t        for config in self.prune_configs(kwargs):\n\t            self.fn.warmup(\n\t                *args,\n\t                num_warps=config.num_warps,\n\t                num_stages=config.num_stages,\n\t                **kwargs,\n\t                **config.kwargs,\n", "            )\n\t        self.nargs = None\n\tdef autotune(configs, key, prune_configs_by=None, reset_to_zero=None, nearest_power_of_two=False):\n\t    \"\"\"\n\t\tDecorator for auto-tuning a :code:`triton.jit`'d function.\n\t\t.. highlight:: python\n\t\t.. code-block:: python\n\t\t\t@triton.autotune(configs=[\n\t\t\t\ttriton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),\n\t\t\t\ttriton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),\n", "\t\t\t],\n\t\t\t\tkey=['x_size'] # the two above configs will be evaluated anytime\n\t\t\t\t\t\t\t\t# the value of x_size changes\n\t\t\t)\n\t\t\t@triton.jit\n\t\t\tdef kernel(x_ptr, x_size, **META):\n\t\t\t\tBLOCK_SIZE = META['BLOCK_SIZE']\n\t\t:note: When all the configurations are evaluated, the kernel will run multiple time.\n\t\t\t\tThis means that whatever value the kernel updates will be updated multiple times.\n\t\t\t\tTo avoid this undesired behavior, you can use the `reset_to_zero` argument, which\n", "\t\t\treset the value of the provided tensor to `zero` before running any configuration.\n\t\t:param configs: a list of :code:`triton.Config` objects\n\t\t:type configs: list[triton.Config]\n\t\t:param key: a list of argument names whose change in value will trigger the evaluation of all provided configs.\n\t\t:type key: list[str]\n\t\t:param prune_configs_by: a dict of functions that are used to prune configs, fields:\n\t\t\t'perf_model': performance model used to predicate running time with different configs, returns running time\n\t\t\t'top_k': number of configs to bench\n\t\t\t'early_config_prune'(optional): a function used to do early prune (eg, num_stages). It take configs:List[Config] as its input, and returns pruned configs.\n\t\t:param reset_to_zero: a list of argument names whose value will be reset to zero before evaluating any configs.\n", "\t:type reset_to_zero: list[str]\n\t\t\"\"\"\n\t    def decorator(fn):\n\t        return Autotuner(fn, fn.arg_names, configs, key, reset_to_zero, prune_configs_by, nearest_power_of_two)\n\t    return decorator\n\tdef matmul248_kernel_config_pruner(configs, nargs):\n\t    \"\"\"\n\t    The main purpose of this function is to shrink BLOCK_SIZE_* when the corresponding dimension is smaller.\n\t    \"\"\"\n\t    m = max(2**int(math.ceil(math.log2(nargs['M']))), 16)\n", "    n = max(2**int(math.ceil(math.log2(nargs['N']))), 16)\n\t    k = max(2**int(math.ceil(math.log2(nargs['K']))), 16)\n\t    used = set()\n\t    for config in configs:\n\t        block_size_m = min(m, config.kwargs['BLOCK_SIZE_M'])\n\t        block_size_n = min(n, config.kwargs['BLOCK_SIZE_N'])\n\t        block_size_k = min(k, config.kwargs['BLOCK_SIZE_K'])\n\t        group_size_m = config.kwargs['GROUP_SIZE_M']\n\t        if (block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps) in used:\n\t            continue\n", "        used.add((block_size_m, block_size_n, block_size_k, group_size_m, config.num_stages, config.num_warps))\n\t        yield triton.Config({\n\t            'BLOCK_SIZE_M': block_size_m,\n\t            'BLOCK_SIZE_N': block_size_n,\n\t            'BLOCK_SIZE_K': block_size_k,\n\t            'GROUP_SIZE_M': group_size_m\n\t        },\n\t                            num_stages=config.num_stages,\n\t                            num_warps=config.num_warps)\n"]}
{"filename": "tools/quant/quant_linear.py", "chunked_list": ["import math\n\timport numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.cuda.amp import custom_bwd, custom_fwd\n\ttry:\n\t    import triton\n\t    import triton.language as tl\n\t    from . import custom_autotune\n\t    # code based https://github.com/fpgaminer/GPTQ-triton\n", "    @custom_autotune.autotune(\n\t        configs=[\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n\t                'BLOCK_SIZE_N': 256,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 128,\n", "                'BLOCK_SIZE_N': 128,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n\t                'BLOCK_SIZE_N': 128,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n", "            triton.Config({\n\t                'BLOCK_SIZE_M': 128,\n\t                'BLOCK_SIZE_N': 32,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n\t                'BLOCK_SIZE_N': 64,\n\t                'BLOCK_SIZE_K': 32,\n", "                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n\t                'BLOCK_SIZE_N': 128,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=2, num_warps=8),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n", "                'BLOCK_SIZE_N': 64,\n\t                'BLOCK_SIZE_K': 64,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=3, num_warps=8),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 32,\n\t                'BLOCK_SIZE_N': 32,\n\t                'BLOCK_SIZE_K': 128,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=2, num_warps=4),\n", "        ],\n\t        key=['M', 'N', 'K'],\n\t        nearest_power_of_two=True,\n\t        prune_configs_by={\n\t            'early_config_prune': custom_autotune.matmul248_kernel_config_pruner,\n\t            'perf_model': None,\n\t            'top_k': None,\n\t        },\n\t    )\n\t    @triton.jit\n", "    def matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scales, stride_zeros,\n\t                          NO_GROUP: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n\t        \"\"\"\n\t        Compute the matrix multiplication C = A x B.\n\t        A is of shape (M, K) float16\n\t        B is of shape (K//8, N) int32\n\t        C is of shape (M, N) float16\n\t        scales is of shape (G, N) float16\n\t        zeros is of shape (G, N) float16\n\t        g_ptr is of shape (K) int32 \n", "        \"\"\"\n\t        infearure_per_bits = 32 // bits\n\t        pid = tl.program_id(axis=0)\n\t        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n\t        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n\t        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n\t        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\t        group_id = pid // num_pid_in_group\n\t        first_pid_m = group_id * GROUP_SIZE_M\n\t        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n", "        pid_m = first_pid_m + (pid % group_size_m)\n\t        pid_n = (pid % num_pid_in_group) // group_size_m\n\t        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n\t        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\t        offs_k = tl.arange(0, BLOCK_SIZE_K)\n\t        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n\t        a_mask = (offs_am[:, None] < M)\n\t        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n\t        b_ptrs = b_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)  # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n\t        g_ptrs = g_ptr + offs_k\n", "        # shifter is used to extract the N bits of each element in the 32-bit word from B\n\t        scales_ptrs = scales_ptr + offs_bn[None, :]\n\t        zeros_ptrs = zeros_ptr + (offs_bn[None, :] // infearure_per_bits)\n\t        if NO_GROUP:\n\t            scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t            zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t            zeros = (zeros >> zeros_shifter[None, :]) & maxq\n\t            zeros = (zeros + 1)\n\t        shifter = (offs_k % infearure_per_bits) * bits\n\t        zeros_shifter = (offs_bn % infearure_per_bits) * bits\n", "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\t        for k in range(0, num_pid_k):\n\t            g_idx = tl.load(g_ptrs)\n\t            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n\t            if not NO_GROUP: \n\t                scales = tl.load(scales_ptrs + g_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t                zeros = tl.load(zeros_ptrs + g_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t                zeros = (zeros >> zeros_shifter[None, :]) & maxq\n\t                zeros = (zeros + 1)\n\t            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n", "            b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n\t            # Now we need to unpack b (which is N-bit values) into 32-bit values\n\t            b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n\t            b = (b - zeros) * scales  # Scale and shift\n\t            accumulator += tl.dot(a, b)\n\t            a_ptrs += BLOCK_SIZE_K\n\t            b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n\t            g_ptrs += BLOCK_SIZE_K\n\t        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]\n\t        c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n", "        tl.store(c_ptrs, accumulator, mask=c_mask)\n\t    @custom_autotune.autotune(configs=[\n\t        triton.Config({\n\t            'BLOCK_SIZE_M': 64,\n\t            'BLOCK_SIZE_N': 32,\n\t            'BLOCK_SIZE_K': 256,\n\t            'GROUP_SIZE_M': 8\n\t        }, num_stages=4, num_warps=4),\n\t        triton.Config({\n\t            'BLOCK_SIZE_M': 128,\n", "            'BLOCK_SIZE_N': 32,\n\t            'BLOCK_SIZE_K': 128,\n\t            'GROUP_SIZE_M': 8\n\t        }, num_stages=4, num_warps=4),\n\t        triton.Config({\n\t            'BLOCK_SIZE_M': 64,\n\t            'BLOCK_SIZE_N': 32,\n\t            'BLOCK_SIZE_K': 128,\n\t            'GROUP_SIZE_M': 8\n\t        }, num_stages=4, num_warps=4),\n", "        triton.Config({\n\t            'BLOCK_SIZE_M': 128,\n\t            'BLOCK_SIZE_N': 32,\n\t            'BLOCK_SIZE_K': 32,\n\t            'GROUP_SIZE_M': 8\n\t        }, num_stages=4, num_warps=4),\n\t        triton.Config({\n\t            'BLOCK_SIZE_M': 64,\n\t            'BLOCK_SIZE_N': 32,\n\t            'BLOCK_SIZE_K': 64,\n", "            'GROUP_SIZE_M': 8\n\t        }, num_stages=4, num_warps=4),\n\t        triton.Config({\n\t            'BLOCK_SIZE_M': 64,\n\t            'BLOCK_SIZE_N': 32,\n\t            'BLOCK_SIZE_K': 128,\n\t            'GROUP_SIZE_M': 8\n\t        }, num_stages=2, num_warps=8),\n\t        triton.Config({\n\t            'BLOCK_SIZE_M': 64,\n", "            'BLOCK_SIZE_N': 64,\n\t            'BLOCK_SIZE_K': 64,\n\t            'GROUP_SIZE_M': 8\n\t        }, num_stages=3, num_warps=8),\n\t        triton.Config({\n\t            'BLOCK_SIZE_M': 32,\n\t            'BLOCK_SIZE_N': 128,\n\t            'BLOCK_SIZE_K': 32,\n\t            'GROUP_SIZE_M': 8\n\t        }, num_stages=2, num_warps=4),\n", "    ],\n\t                              key=['M', 'N', 'K'],\n\t                              nearest_power_of_two=True)\n\t    @triton.jit\n\t    def transpose_matmul_248_kernel(a_ptr, b_ptr, c_ptr, scales_ptr, zeros_ptr, g_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, stride_scales,\n\t                                    stride_zeros, NO_GROUP: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n\t        \"\"\"\n\t        Compute the matrix multiplication C = A x B.\n\t        A is of shape (M, N) float16\n\t        B is of shape (K//8, N) int32\n", "        C is of shape (M, K) float16\n\t        scales is of shape (G, N) float16\n\t        zeros is of shape (G, N) float16\n\t        g_ptr is of shape (K) int32 \n\t        \"\"\"\n\t        infearure_per_bits = 32 // bits\n\t        pid = tl.program_id(axis=0)\n\t        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n\t        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n\t        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n", "        num_pid_in_group = GROUP_SIZE_M * num_pid_k\n\t        group_id = pid // num_pid_in_group\n\t        first_pid_m = group_id * GROUP_SIZE_M\n\t        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n\t        pid_m = first_pid_m + (pid % group_size_m)\n\t        pid_k = (pid % num_pid_in_group) // group_size_m\n\t        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n\t        offs_bk = pid_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n\t        offs_n = tl.arange(0, BLOCK_SIZE_N)\n\t        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_n[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n", "        a_mask = (offs_am[:, None] < M)\n\t        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n\t        b_ptrs = b_ptr + ((offs_bk[:, None] // infearure_per_bits) * stride_bk + offs_n[None, :] * stride_bn)  # (BLOCK_SIZE_K, BLOCK_SIZE_N)\n\t        g_ptrs = g_ptr + offs_bk\n\t        g_idx = tl.load(g_ptrs)\n\t        # shifter is used to extract the N bits of each element in the 32-bit word from B\n\t        scales_ptrs = scales_ptr + offs_n[None, :] + g_idx[:, None] * stride_scales\n\t        zeros_ptrs = zeros_ptr + (offs_n[None, :] // infearure_per_bits) + g_idx[:, None] * stride_zeros\n\t        if NO_GROUP:\n\t            scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n", "            zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t            zeros = (zeros >> zeros_shifter[None, :]) & maxq\n\t            zeros = (zeros + 1)\n\t        shifter = (offs_bk % infearure_per_bits) * bits\n\t        zeros_shifter = (offs_n % infearure_per_bits) * bits\n\t        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n\t        for k in range(0, num_pid_n):\n\t            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n\t            if not NO_GROUP:\n\t                scales = tl.load(scales_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n", "                zeros = tl.load(zeros_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t                zeros = (zeros >> zeros_shifter[None, :]) & maxq\n\t                zeros = (zeros + 1)\n\t            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_N)\n\t            b = tl.load(b_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n\t            # Now we need to unpack b (which is N-bit values) into 32-bit values\n\t            b = (b >> shifter[:, None]) & maxq  # Extract the N-bit values\n\t            b = (b - zeros) * scales  # Scale and shift\n\t            b = tl.trans(b)\n\t            accumulator += tl.dot(a, b)\n", "            a_ptrs += BLOCK_SIZE_N\n\t            b_ptrs += BLOCK_SIZE_N\n\t            scales_ptrs += BLOCK_SIZE_N\n\t            zeros_ptrs += (BLOCK_SIZE_N // infearure_per_bits)\n\t        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bk[None, :]\n\t        c_mask = (offs_am[:, None] < M) & (offs_bk[None, :] < K)\n\t        tl.store(c_ptrs, accumulator, mask=c_mask)\n\texcept:\n\t    print('trioton not installed.')\n\tdef matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq, no_group):\n", "    with torch.cuda.device(input.device):\n\t        output = torch.empty((input.shape[0], qweight.shape[1]), device='cuda', dtype=torch.float16)\n\t        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(qweight.shape[1], META['BLOCK_SIZE_N']), )\n\t        matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], input.shape[1], bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),\n\t                                qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0), no_group)\n\t        return output\n\tdef transpose_matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq, no_group):\n\t    with torch.cuda.device(input.device):\n\t        output_dim = (qweight.shape[0] * 32) // bits\n\t        output = torch.empty((input.shape[0], output_dim), device='cuda', dtype=torch.float16)\n", "        grid = lambda META: (triton.cdiv(input.shape[0], META['BLOCK_SIZE_M']) * triton.cdiv(output_dim, META['BLOCK_SIZE_K']), )\n\t        transpose_matmul_248_kernel[grid](input, qweight, output, scales, qzeros, g_idx, input.shape[0], qweight.shape[1], output_dim, bits, maxq, input.stride(0), input.stride(1), qweight.stride(0),\n\t                                          qweight.stride(1), output.stride(0), output.stride(1), scales.stride(0), qzeros.stride(0), no_group)\n\t        return output\n\tclass QuantLinearFunction(torch.autograd.Function):\n\t    @staticmethod\n\t    @custom_fwd(cast_inputs=torch.float16)\n\t    def forward(ctx, input, qweight, scales, qzeros, g_idx, bits, maxq, no_group):\n\t        output = matmul248(input, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n\t        ctx.save_for_backward(qweight, scales, qzeros, g_idx)\n", "        ctx.bits, ctx.maxq, ctx.no_group = bits, maxq, no_group\n\t        return output\n\t    @staticmethod\n\t    @custom_bwd\n\t    def backward(ctx, grad_output):\n\t        qweight, scales, qzeros, g_idx = ctx.saved_tensors\n\t        bits, maxq, no_group = ctx.bits, ctx.maxq, ctx.no_group\n\t        grad_input = None\n\t        if ctx.needs_input_grad[0]:\n\t            grad_input = transpose_matmul248(grad_output, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n", "        return grad_input, None, None, None, None, None, None\n\tclass QuantLinear(nn.Module):\n\t    def __init__(self, bits, groupsize, infeatures, outfeatures, bias):\n\t        super().__init__()\n\t        if bits not in [2, 4, 8]:\n\t            raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n\t        self.infeatures = infeatures\n\t        self.outfeatures = outfeatures\n\t        self.bits = bits\n\t        self.maxq = 2**self.bits - 1\n", "        self.groupsize = groupsize if groupsize != -1 else infeatures\n\t        self.no_group = math.ceil(infeatures / self.groupsize) == 1\n\t        self.register_buffer('qweight', torch.zeros((infeatures // 32 * self.bits, outfeatures), dtype=torch.int32))\n\t        self.register_buffer('qzeros', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures // 32 * self.bits), dtype=torch.int32))\n\t        self.register_buffer('scales', torch.zeros((math.ceil(infeatures / self.groupsize), outfeatures), dtype=torch.float16))\n\t        self.register_buffer('g_idx', torch.tensor([i // self.groupsize for i in range(infeatures)], dtype=torch.int32))\n\t        if bias:\n\t            self.register_buffer('bias', torch.zeros((outfeatures), dtype=torch.float16))\n\t        else:\n\t            self.bias = None\n", "    def pack(self, linear, scales, zeros, g_idx=None):\n\t        self.g_idx = g_idx.clone() if g_idx is not None else self.g_idx\n\t        scales = scales.t().contiguous()\n\t        zeros = zeros.t().contiguous()\n\t        scale_zeros = zeros * scales\n\t        self.scales = scales.clone().half()\n\t        if linear.bias is not None:\n\t            self.bias = linear.bias.clone().half()\n\t        intweight = []\n\t        for idx in range(self.infeatures):\n", "            intweight.append(torch.round((linear.weight.data[:, idx] + scale_zeros[self.g_idx[idx]]) / self.scales[self.g_idx[idx]]).to(torch.int)[:, None])\n\t        intweight = torch.cat(intweight, dim=1)\n\t        intweight = intweight.t().contiguous()\n\t        intweight = intweight.numpy().astype(np.uint32)\n\t        qweight = np.zeros((intweight.shape[0] // 32 * self.bits, intweight.shape[1]), dtype=np.uint32)\n\t        i = 0\n\t        row = 0\n\t        while row < qweight.shape[0]:\n\t            if self.bits in [2, 4, 8]:\n\t                for j in range(i, i + (32 // self.bits)):\n", "                    qweight[row] |= intweight[j] << (self.bits * (j - i))\n\t                i += 32 // self.bits\n\t                row += 1\n\t            else:\n\t                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n\t        qweight = qweight.astype(np.int32)\n\t        self.qweight = torch.from_numpy(qweight)\n\t        zeros -= 1\n\t        zeros = zeros.numpy().astype(np.uint32)\n\t        qzeros = np.zeros((zeros.shape[0], zeros.shape[1] // 32 * self.bits), dtype=np.uint32)\n", "        i = 0\n\t        col = 0\n\t        while col < qzeros.shape[1]:\n\t            if self.bits in [2, 4, 8]:\n\t                for j in range(i, i + (32 // self.bits)):\n\t                    qzeros[:, col] |= zeros[:, j] << (self.bits * (j - i))\n\t                i += 32 // self.bits\n\t                col += 1\n\t            else:\n\t                raise NotImplementedError(\"Only 2,4,8 bits are supported.\")\n", "        qzeros = qzeros.astype(np.int32)\n\t        self.qzeros = torch.from_numpy(qzeros)\n\t    def forward(self, x):\n\t        out_shape = x.shape[:-1] + (self.outfeatures, )\n\t        out = QuantLinearFunction.apply(x.reshape(-1, x.shape[-1]), self.qweight, self.scales, self.qzeros, self.g_idx, self.bits, self.maxq, self.no_group)\n\t        out = out + self.bias if self.bias is not None else out\n\t        return out.reshape(out_shape)\n\tdef make_quant_linear(module, names, bits, groupsize, name=''):\n\t    if isinstance(module, QuantLinear):\n\t        return\n", "    for attr in dir(module):\n\t        tmp = getattr(module, attr)\n\t        name1 = name + '.' + attr if name != '' else attr\n\t        if name1 in names:\n\t            delattr(module, attr)\n\t            setattr(module, attr, QuantLinear(bits, groupsize, tmp.in_features, tmp.out_features, tmp.bias is not None))\n\t    for name1, child in module.named_children():\n\t        make_quant_linear(child, names, bits, groupsize, name + '.' + name1 if name != '' else name1)\n\tdef autotune_warmup_linear(model, transpose=False):\n\t    \"\"\"\n", "    Pre-tunes the quantized kernel\n\t    \"\"\"\n\t    from tqdm import tqdm\n\t    kn_values = {}\n\t    for _, m in model.named_modules():\n\t        if not isinstance(m, QuantLinear):\n\t            continue\n\t        k = m.infeatures\n\t        n = m.outfeatures\n\t        if (k, n) not in kn_values:\n", "            kn_values[(k, n)] = (m.qweight.cuda(), m.scales.cuda(), m.qzeros.cuda(), m.g_idx.cuda(), m.bits, m.maxq, m.no_group)\n\t    print(f'Found {len(kn_values)} unique KN Linear values.')\n\t    print('Warming up autotune cache ...')\n\t    with torch.no_grad():\n\t        for m in tqdm(range(0, 12)):\n\t            m = 2**m  # [1, 2048]\n\t            for (k, n), (qweight, scales, qzeros, g_idx, bits, maxq, no_group) in kn_values.items():\n\t                a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n\t                matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n\t                if transpose:\n", "                    a = torch.randn(m, n, dtype=torch.float16, device='cuda')\n\t                    transpose_matmul248(a, qweight, scales, qzeros, g_idx, bits, maxq, no_group)\n\t    del kn_values\n"]}
{"filename": "tools/quant/fused_attn.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.nn import functional as F\n\tfrom torch.cuda.amp import custom_bwd, custom_fwd\n\tfrom transformers.models.llama.modeling_llama import LlamaAttention, apply_rotary_pos_emb\n\tfrom .quant_linear import *\n\tclass QuantLlamaAttention(nn.Module):\n\t    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\t    def __init__(\n", "        self,\n\t        hidden_size,\n\t        num_heads,\n\t        qkv_proj,\n\t        o_proj,\n\t        rotary_emb,\n\t    ):\n\t        super().__init__()\n\t        self.hidden_size = hidden_size\n\t        self.num_heads = num_heads\n", "        self.head_dim = hidden_size // num_heads\n\t        if (self.head_dim * num_heads) != self.hidden_size:\n\t            raise ValueError(f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n\t                             f\" and `num_heads`: {num_heads}).\")\n\t        self.qkv_proj = qkv_proj\n\t        self.o_proj = o_proj\n\t        self.rotary_emb = rotary_emb\n\t    def _shape(self, tensor, seq_len, bsz):\n\t        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\t    def forward(self, hidden_states, past_key_value=None, attention_mask=None, position_ids=None, output_attentions=False, use_cache=False):\n", "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\t        bsz, q_len, _ = hidden_states.size()\n\t        qkv_states = self.qkv_proj(hidden_states)\n\t        query_states, key_states, value_states = torch.split(qkv_states, self.hidden_size, dim=2)\n\t        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t        key_states = key_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t        value_states = value_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\t        #transformers==4.29.0:\n\t        kv_seq_len = key_states.shape[-2]\n\t        if past_key_value is not None:\n", "            kv_seq_len += past_key_value[0].shape[-2]\n\t        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n\t        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\t        #transformers==4.28.0:\n\t        # kv_seq_len = key_states.shape[-2]\n\t        # offset = 0\n\t        # if past_key_value is not None:\n\t        #     offset = past_key_value[0].shape[-2]\n\t        #     kv_seq_len += offset\n\t        # cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n", "        # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, offset)\n\t        # [bsz, nh, t, hd]\n\t        is_causal = past_key_value is None\n\t        if past_key_value is not None:\n\t            # reuse k, v, self_attention\n\t            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n\t            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\t        past_key_value = (key_states, value_states) if use_cache else None\n\t        with torch.backends.cuda.sdp_kernel(enable_math=False):\n\t            attn_output = F.scaled_dot_product_attention(query_states, key_states, value_states, is_causal=is_causal)\n", "        attn_output = attn_output.transpose(1, 2)\n\t        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\t        attn_output = self.o_proj(attn_output)\n\t        if not output_attentions:\n\t            attn_weights = None\n\t        return attn_output, attn_weights, past_key_value\n\tdef make_quant_attn(model):\n\t    \"\"\"\n\t    Replace all LlamaAttention modules with QuantLlamaAttention modules, fusing the q, k, v projections.\n\t    \"\"\"\n", "    for name, m in model.named_modules():\n\t        if not isinstance(m, LlamaAttention):\n\t            continue\n\t        q_proj = m.q_proj\n\t        k_proj = m.k_proj\n\t        v_proj = m.v_proj\n\t        qweights = torch.cat([q_proj.qweight, k_proj.qweight, v_proj.qweight], dim=1)\n\t        qzeros = torch.cat([q_proj.qzeros, k_proj.qzeros, v_proj.qzeros], dim=1)\n\t        scales = torch.cat([q_proj.scales, k_proj.scales, v_proj.scales], dim=1)\n\t        g_idx = torch.cat([q_proj.g_idx, k_proj.g_idx, v_proj.g_idx], dim=0)\n", "        bias = torch.cat([q_proj.bias, k_proj.bias, v_proj.bias], dim=0) if q_proj.bias is not None else None\n\t        qkv_layer = QuantLinear(q_proj.bits, q_proj.groupsize, q_proj.infeatures, q_proj.outfeatures + k_proj.outfeatures + v_proj.outfeatures, True if q_proj.bias is not None else False)\n\t        qkv_layer.qweight = qweights\n\t        qkv_layer.qzeros = qzeros\n\t        qkv_layer.scales = scales\n\t        qkv_layer.g_idx = g_idx\n\t        qkv_layer.bias = bias\n\t        attn = QuantLlamaAttention(m.hidden_size, m.num_heads, qkv_layer, m.o_proj, m.rotary_emb)\n\t        if '.' in name:\n\t            parent_name = name.rsplit('.', 1)[0]\n", "            child_name = name[len(parent_name) + 1:]\n\t            parent = model.get_submodule(parent_name)\n\t        else:\n\t            parent_name = ''\n\t            parent = model\n\t            child_name = name\n\t        #print(f\"Replacing {name} with quant_attn; parent: {parent_name}, child's name: {child_name}\")\n\t        setattr(parent, child_name, attn)\n"]}
{"filename": "tools/quant/__init__.py", "chunked_list": ["from .quantizer import Quantizer\n\tfrom .fused_attn import QuantLlamaAttention, make_quant_attn\n\tfrom .fused_mlp import QuantLlamaMLP, make_fused_mlp, autotune_warmup_fused\n\tfrom .quant_linear import QuantLinear, make_quant_linear, autotune_warmup_linear\n"]}
{"filename": "tools/quant/fused_mlp.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\tfrom torch.cuda.amp import custom_bwd, custom_fwd\n\tfrom transformers.models.llama.modeling_llama import LlamaMLP\n\ttry:\n\t    import triton\n\t    import triton.language as tl\n\t    from . import custom_autotune\n\t    # code based https://github.com/fpgaminer/GPTQ-triton\n", "    @custom_autotune.autotune(\n\t        configs=[\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 256,\n\t                'BLOCK_SIZE_N': 64,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n", "                'BLOCK_SIZE_N': 256,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 128,\n\t                'BLOCK_SIZE_N': 128,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n", "            triton.Config({\n\t                'BLOCK_SIZE_M': 128,\n\t                'BLOCK_SIZE_N': 64,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n\t                'BLOCK_SIZE_N': 128,\n\t                'BLOCK_SIZE_K': 32,\n", "                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 128,\n\t                'BLOCK_SIZE_N': 32,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),  # 3090\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 128,\n", "                'BLOCK_SIZE_N': 16,\n\t                'BLOCK_SIZE_K': 32,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),  # 3090\n\t            triton.Config({\n\t                'BLOCK_SIZE_M': 32,\n\t                'BLOCK_SIZE_N': 32,\n\t                'BLOCK_SIZE_K': 128,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=2, num_warps=4),  # 3090\n", "            triton.Config({\n\t                'BLOCK_SIZE_M': 64,\n\t                'BLOCK_SIZE_N': 16,\n\t                'BLOCK_SIZE_K': 64,\n\t                'GROUP_SIZE_M': 8\n\t            }, num_stages=4, num_warps=4),  # 3090\n\t        ],\n\t        key=['M', 'N', 'K'],\n\t        nearest_power_of_two=True,\n\t        prune_configs_by={\n", "            'early_config_prune': custom_autotune.matmul248_kernel_config_pruner,\n\t            'perf_model': None,\n\t            'top_k': None,\n\t        },\n\t    )\n\t    @triton.jit\n\t    def fusedmatmul_248_kernel(a_ptr, c_ptr, b1_ptr, scales1_ptr, zeros1_ptr, g1_ptr, b2_ptr, scales2_ptr, zeros2_ptr, g2_ptr, M, N, K, bits, maxq, stride_am, stride_ak, stride_bk, stride_bn,\n\t                               stride_cm, stride_cn, stride_scales, stride_zeros, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):\n\t        \"\"\"\n\t        Computes: C = silu(A * B1) * (A * B2)\n", "        A is of shape (M, K) float16\n\t        B is of shape (K//8, N) int32\n\t        C is of shape (M, N) float16\n\t        scales is of shape (1, N) float16\n\t        zeros is of shape (1, N//8) int32\n\t        \"\"\"\n\t        infearure_per_bits = 32 // bits\n\t        pid = tl.program_id(axis=0)\n\t        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n\t        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n", "        num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)\n\t        num_pid_in_group = GROUP_SIZE_M * num_pid_n\n\t        group_id = pid // num_pid_in_group\n\t        first_pid_m = group_id * GROUP_SIZE_M\n\t        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n\t        pid_m = first_pid_m + (pid % group_size_m)\n\t        pid_n = (pid % num_pid_in_group) // group_size_m\n\t        offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n\t        offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n\t        offs_k = tl.arange(0, BLOCK_SIZE_K)\n", "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n\t        a_mask = (offs_am[:, None] < M)\n\t        # b_ptrs is set up such that it repeats elements along the K axis 8 times\n\t        b1_ptrs = b1_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)\n\t        b2_ptrs = b2_ptr + ((offs_k[:, None] // infearure_per_bits) * stride_bk + offs_bn[None, :] * stride_bn)\n\t        g1_ptrs = g1_ptr + offs_k\n\t        g2_ptrs = g2_ptr + offs_k\n\t        # shifter is used to extract the N bits of each element in the 32-bit word from B\n\t        scales1_ptrs = scales1_ptr + offs_bn[None, :]\n\t        scales2_ptrs = scales2_ptr + offs_bn[None, :]\n", "        zeros1_ptrs = zeros1_ptr + (offs_bn[None, :] // infearure_per_bits)\n\t        zeros2_ptrs = zeros2_ptr + (offs_bn[None, :] // infearure_per_bits)\n\t        shifter = (offs_k % infearure_per_bits) * bits\n\t        zeros_shifter = (offs_bn % infearure_per_bits) * bits\n\t        accumulator1 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\t        accumulator2 = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n\t        for k in range(0, num_pid_k):\n\t            g1_idx = tl.load(g1_ptrs)\n\t            g2_idx = tl.load(g2_ptrs)\n\t            # Fetch scales and zeros; these are per-outfeature and thus reused in the inner loop\n", "            scales1 = tl.load(scales1_ptrs + g1_idx[:, None] * stride_scales)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t            scales2 = tl.load(scales2_ptrs + g2_idx[:, None] * stride_scales)\n\t            zeros1 = tl.load(zeros1_ptrs + g1_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t            zeros1 = (zeros1 >> zeros_shifter[None, :]) & maxq\n\t            zeros1 = (zeros1 + 1)\n\t            zeros2 = tl.load(zeros2_ptrs + g2_idx[:, None] * stride_zeros)  # (BLOCK_SIZE_K, BLOCK_SIZE_N,)\n\t            zeros2 = (zeros2 >> zeros_shifter[None, :]) & maxq\n\t            zeros2 = (zeros2 + 1)\n\t            a = tl.load(a_ptrs, mask=a_mask, other=0.)  # (BLOCK_SIZE_M, BLOCK_SIZE_K)\n\t            b1 = tl.load(b1_ptrs)  # (BLOCK_SIZE_K, BLOCK_SIZE_N), but repeated\n", "            b2 = tl.load(b2_ptrs)\n\t            # Now we need to unpack b (which is N-bit values) into 32-bit values\n\t            b1 = (b1 >> shifter[:, None]) & maxq  # Extract the N-bit values\n\t            b1 = (b1 - zeros1) * scales1  # Scale and shift\n\t            accumulator1 += tl.dot(a, b1)\n\t            b2 = (b2 >> shifter[:, None]) & maxq\n\t            b2 = (b2 - zeros2) * scales2\n\t            accumulator2 += tl.dot(a, b2)\n\t            a_ptrs += BLOCK_SIZE_K\n\t            b1_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n", "            b2_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk\n\t            g1_ptrs += BLOCK_SIZE_K\n\t            g2_ptrs += BLOCK_SIZE_K\n\t        accumulator1 = silu(accumulator1)\n\t        c = accumulator1 * accumulator2\n\t        c = c.to(tl.float16)\n\t        c_ptrs = c_ptr + stride_cm * offs_am[:, None] + stride_cn * offs_bn[None, :]\n\t        c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)\n\t        tl.store(c_ptrs, c, mask=c_mask)\n\t    @triton.jit\n", "    def silu(x):\n\t        return x * tl.sigmoid(x)\n\texcept:\n\t    print('triton not installed.')\n\tclass QuantLlamaMLP(nn.Module):\n\t    def __init__(\n\t        self,\n\t        gate_proj,\n\t        down_proj,\n\t        up_proj,\n", "    ):\n\t        super().__init__()\n\t        self.register_buffer('gate_proj_qweight', gate_proj.qweight)\n\t        self.register_buffer('gate_proj_scales', gate_proj.scales)\n\t        self.register_buffer('gate_proj_qzeros', gate_proj.qzeros)\n\t        self.register_buffer('gate_proj_g_idx', gate_proj.g_idx)\n\t        self.register_buffer('up_proj_qweight', up_proj.qweight)\n\t        self.register_buffer('up_proj_scales', up_proj.scales)\n\t        self.register_buffer('up_proj_qzeros', up_proj.qzeros)\n\t        self.register_buffer('up_proj_g_idx', up_proj.g_idx)\n", "        self.infeatures = gate_proj.infeatures\n\t        self.intermediate_size = gate_proj.outfeatures\n\t        self.outfeatures = down_proj.outfeatures\n\t        self.bits = gate_proj.bits\n\t        self.maxq = gate_proj.maxq\n\t        self.down_proj = down_proj\n\t    def forward(self, x):\n\t        return self.down_proj(self.triton_llama_mlp(x))\n\t    def triton_llama_mlp(self, x):\n\t        with torch.cuda.device(x.device):\n", "            out_shape = x.shape[:-1] + (self.intermediate_size, )\n\t            x = x.reshape(-1, x.shape[-1])\n\t            M, K = x.shape\n\t            N = self.intermediate_size\n\t            c = torch.empty((M, N), device='cuda', dtype=torch.float16)\n\t            grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )\n\t            fusedmatmul_248_kernel[grid](x, c, self.gate_proj_qweight, self.gate_proj_scales, self.gate_proj_qzeros, self.gate_proj_g_idx, self.up_proj_qweight, self.up_proj_scales,\n\t                                         self.up_proj_qzeros, self.up_proj_g_idx, M, N, K, self.bits, self.maxq, x.stride(0), x.stride(1), self.gate_proj_qweight.stride(0),\n\t                                         self.gate_proj_qweight.stride(1), c.stride(0), c.stride(1), self.gate_proj_scales.stride(0), self.gate_proj_qzeros.stride(0))\n\t            c = c.reshape(out_shape)\n", "            return c\n\t    def fused2cuda(self):\n\t        self.gate_proj_qweight = self.gate_proj_qweight.cuda()\n\t        self.gate_proj_scales = self.gate_proj_scales.cuda()\n\t        self.gate_proj_qzeros = self.gate_proj_qzeros.cuda()\n\t        self.gate_proj_g_idx = self.gate_proj_g_idx.cuda()\n\t        self.up_proj_qweight = self.up_proj_qweight.cuda()\n\t        self.up_proj_scales = self.up_proj_scales.cuda()\n\t        self.up_proj_qzeros = self.up_proj_qzeros.cuda()\n\t        self.up_proj_g_idx = self.up_proj_g_idx.cuda()\n", "    def fused2cpu(self):\n\t        self.gate_proj_qweight = self.gate_proj_qweight.cpu()\n\t        self.gate_proj_scales = self.gate_proj_scales.cpu()\n\t        self.gate_proj_qzeros = self.gate_proj_qzeros.cpu()\n\t        self.gate_proj_g_idx = self.gate_proj_g_idx.cpu()\n\t        self.up_proj_qweight = self.up_proj_qweight.cpu()\n\t        self.up_proj_scales = self.up_proj_scales.cpu()\n\t        self.up_proj_qzeros = self.up_proj_qzeros.cpu()\n\t        self.up_proj_g_idx = self.up_proj_g_idx.cpu()\n\tdef make_fused_mlp(m, parent_name=''):\n", "    \"\"\"\n\t    Replace all LlamaMLP modules with QuantLlamaMLP modules, which fuses many of the operations.\n\t    \"\"\"\n\t    if isinstance(m, LlamaMLP):\n\t        return QuantLlamaMLP(m.gate_proj, m.down_proj, m.up_proj)\n\t    for name, child in m.named_children():\n\t        child = make_fused_mlp(child, parent_name=f\"{parent_name}.{name}\")\n\t        if isinstance(child, QuantLlamaMLP):\n\t            setattr(m, name, child)\n\t    return m\n", "def autotune_warmup_fused(model):\n\t    \"\"\"\n\t    Pre-tunes the quantized kernel\n\t    \"\"\"\n\t    from tqdm import tqdm\n\t    kn_values = {}\n\t    for _, m in model.named_modules():\n\t        if not isinstance(m, QuantLlamaMLP):\n\t            continue\n\t        k = m.infeatures\n", "        n = m.intermediate_size\n\t        m.fused2cuda()\n\t        if (k, n) not in kn_values:\n\t            kn_values[(k, n)] = m\n\t    print(f'Found {len(kn_values)} unique fused mlp KN values.')\n\t    print('Warming up autotune cache ...')\n\t    with torch.no_grad():\n\t        for m in tqdm(range(0, 12)):\n\t            m = 2**m  # [1, 2048]\n\t            for (k, n), (modules) in kn_values.items():\n", "                a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n\t                modules.triton_llama_mlp(a)\n\t        for (k, n), (modules) in kn_values.items():\n\t            a = torch.randn(m, k, dtype=torch.float16, device='cuda')\n\t            modules.fused2cpu()\n\t    del kn_values\n"]}
{"filename": "tools/quant/quantizer.py", "chunked_list": ["import numpy as np\n\timport torch\n\timport torch.nn as nn\n\timport math\n\tclass Quantizer(nn.Module):\n\t    def __init__(self, shape=1):\n\t        super(Quantizer, self).__init__()\n\t        self.register_buffer('maxq', torch.tensor(0))\n\t        self.register_buffer('scale', torch.zeros(shape))\n\t        self.register_buffer('zero', torch.zeros(shape))\n", "    def configure(self, bits, perchannel=False, sym=True, mse=False, norm=2.4, grid=100, maxshrink=.8, trits=False):\n\t        self.maxq = torch.tensor(2**bits - 1)\n\t        self.perchannel = perchannel\n\t        self.sym = sym\n\t        self.mse = mse\n\t        self.norm = norm\n\t        self.grid = grid\n\t        self.maxshrink = maxshrink\n\t        if trits:\n\t            self.maxq = torch.tensor(-1)\n", "        self.scale = torch.zeros_like(self.scale)\n\t    def _quantize(self, x, scale, zero, maxq):\n\t        if maxq < 0:\n\t            return (x > scale / 2).float() * scale + (x < zero / 2).float() * zero\n\t        q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)\n\t        return scale * (q - zero)\n\t    def find_params(self, x, weight=False):\n\t        dev = x.device\n\t        self.maxq = self.maxq.to(dev)\n\t        shape = x.shape\n", "        if self.perchannel:\n\t            if weight:\n\t                x = x.flatten(1)\n\t            else:\n\t                if len(shape) == 4:\n\t                    x = x.permute([1, 0, 2, 3])\n\t                    x = x.flatten(1)\n\t                if len(shape) == 3:\n\t                    x = x.reshape((-1, shape[-1])).t()\n\t                if len(shape) == 2:\n", "                    x = x.t()\n\t        else:\n\t            x = x.flatten().unsqueeze(0)\n\t        tmp = torch.zeros(x.shape[0], device=dev)\n\t        xmin = torch.minimum(x.min(1)[0], tmp)\n\t        xmax = torch.maximum(x.max(1)[0], tmp)\n\t        if self.sym:\n\t            xmax = torch.maximum(torch.abs(xmin), xmax)\n\t            tmp = xmin < 0\n\t            if torch.any(tmp):\n", "                xmin[tmp] = -xmax[tmp]\n\t        tmp = (xmin == 0) & (xmax == 0)\n\t        xmin[tmp] = -1\n\t        xmax[tmp] = +1\n\t        if self.maxq < 0:\n\t            self.scale = xmax\n\t            self.zero = xmin\n\t        else:\n\t            self.scale = (xmax - xmin) / self.maxq\n\t            if self.sym:\n", "                self.zero = torch.full_like(self.scale, (self.maxq + 1) / 2)\n\t            else:\n\t                self.zero = torch.round(-xmin / self.scale)\n\t        if self.mse:\n\t            best = torch.full([x.shape[0]], float('inf'), device=dev)\n\t            for i in range(int(self.maxshrink * self.grid)):\n\t                p = 1 - i / self.grid\n\t                xmin1 = p * xmin\n\t                xmax1 = p * xmax\n\t                scale1 = (xmax1 - xmin1) / self.maxq\n", "                zero1 = torch.round(-xmin1 / scale1) if not self.sym else self.zero\n\t                q = self._quantize(x, scale1.unsqueeze(1), zero1.unsqueeze(1), self.maxq)\n\t                q -= x\n\t                q.abs_()\n\t                q.pow_(self.norm)\n\t                err = torch.sum(q, 1)\n\t                tmp = err < best\n\t                if torch.any(tmp):\n\t                    best[tmp] = err[tmp]\n\t                    self.scale[tmp] = scale1[tmp]\n", "                    self.zero[tmp] = zero1[tmp]\n\t        if not self.perchannel:\n\t            if weight:\n\t                tmp = shape[0]\n\t            else:\n\t                tmp = shape[1] if len(shape) != 3 else shape[2]\n\t            self.scale = self.scale.repeat(tmp)\n\t            self.zero = self.zero.repeat(tmp)\n\t        if weight:\n\t            shape = [-1] + [1] * (len(shape) - 1)\n", "            self.scale = self.scale.reshape(shape)\n\t            self.zero = self.zero.reshape(shape)\n\t            return\n\t        if len(shape) == 4:\n\t            self.scale = self.scale.reshape((1, -1, 1, 1))\n\t            self.zero = self.zero.reshape((1, -1, 1, 1))\n\t        if len(shape) == 3:\n\t            self.scale = self.scale.reshape((1, 1, -1))\n\t            self.zero = self.zero.reshape((1, 1, -1))\n\t        if len(shape) == 2:\n", "            self.scale = self.scale.unsqueeze(0)\n\t            self.zero = self.zero.unsqueeze(0)\n\t    def quantize(self, x):\n\t        if self.ready():\n\t            return self._quantize(x, self.scale, self.zero, self.maxq)\n\t        return x\n\t    def enabled(self):\n\t        return self.maxq > 0\n\t    def ready(self):\n\t        return torch.all(self.scale != 0)\n"]}
