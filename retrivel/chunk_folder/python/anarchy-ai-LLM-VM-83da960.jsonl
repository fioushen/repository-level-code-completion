{"filename": "quickstart_REBEL.py", "chunked_list": ["# import our client\n\tfrom llm_vm.client import Client\n\timport os\n\t# Instantiate the client specifying which LLM you want to use\n\tclient = Client(big_model='chat_gpt', small_model='gpt')\n\t# Put in your prompt and go!\n\tresponse = client.complete(prompt = 'Is it warmer in Paris or Timbuktu and what are the temperatures in either city?',\n\t                           context='',\n\t                           openai_key=os.getenv(\"LLM_VM_OPENAI_API_KEY\"), #for REBEL we need an OpenAI key\n\t                           tools=\n", "                           [{'description': 'Find the weather at a location and returns it in celcius.',\n\t                            'dynamic_params': {\"latitude\": 'latitude of as a float',\"longitude\": 'the longitude as a float'},\n\t                            'method': 'GET',\n\t                            'url': \"https://api.open-meteo.com/v1/forecast\",\n\t                            'static_params': {'current_weather': 'true'}}]) #No tools by default, so you have to add your own\n\tprint(response)\n"]}
{"filename": "quickstart_finetune.py", "chunked_list": ["#! /usr/bin/env python3\n\t# import our client\n\tfrom llm_vm.client import Client\n\timport os\n\tfrom llm_vm.config import settings\n\t# Instantiate the client specifying which LLM you want to use\n\tclient = Client(big_model='chat_gpt', small_model='pythia')\n\t# Put in your prompt and go!\n\tresponse = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the currency in myanmmar\",\n\t                           openai_key=settings.openai_api_key,\n", "                           temperature=0.0,\n\t                           data_synthesis=True,\n\t                           finetune=True,)\n\tprint(response)\n\t# response = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the economic situation in France\",\n\t#                            openai_key=settings.openai_api_key,\n\t#                            temperature=0.0,\n\t#                            data_synthesis=True,\n\t#                            finetune=True,)\n\t# print(response)\n", "# response = client.complete(prompt = \"Answer question Q. \",context=\"Q: What is the currency in myanmmar\",\n\t#                            openai_key=settings.openai_api_key,\n\t#                            temperature=0.0,\n\t#                            data_synthesis=True,\n\t#                            finetune=True,)\n\t# print(response)\n\t# Anarchy is a political system in which the state is abolished and the people are free...\n"]}
{"filename": "quickstart_finetune_load.py", "chunked_list": ["#! /usr/bin/env python3\n\t# import our client\n\tfrom llm_vm.client import Client\n\timport os\n\t# Instantiate the client specifying which LLM you want to use\n\tclient = Client(big_model='pythia')\n\t# specify the file name of the finetuned model to load\n\tmodel_name = '<filename_of_your_model>.pt'\n\tclient.load_finetune(model_name)\n\t# Put in your prompt and go!\n", "response = client.complete(prompt = 'What is anarchy?',\n\t                           context='')\n\tprint(response)\n\t# Anarchy is a political system in which the state is abolished and the people are free...\n"]}
{"filename": "test_llm_vm.py", "chunked_list": ["# import llm_vm.client as l\n\timport os, requests, json\n\topenai_key = os.getenv('LLM_VM_OPENAI_API_KEY')\n\turl = \"http://localhost:3002/v1/complete\"\n\tjson_payload = {\"prompt\": \"what is the economic situation in canada?\",\n\t                \"context\": \"\",\n\t                \"temperature\": 0.0,\n\t                \"openai_key\": openai_key,\n\t                # \"finetune\": True,\n\t                }\n", "response = requests.post(url, data=json.dumps(json_payload))\n\tprint(response.text)\n"]}
{"filename": "quickstart.py", "chunked_list": ["#! /usr/bin/env python3\n\t# import our client\n\tfrom llm_vm.client import Client\n\timport os\n\t# Instantiate the client specifying which LLM you want to use\n\tclient = Client(big_model='pythia', small_model='neo')\n\t# Put in your prompt and go!\n\tresponse = client.complete(prompt = 'What is anarchy?',\n\t                           context='')\n\tprint(response)\n", "# Anarchy is a political system in which the state is abolished and the people are free...\n"]}
{"filename": "tests/test_data_synthesis.py", "chunked_list": ["from dotenv import load_dotenv\n\timport os\n\timport openai\n\timport data_synthesis\n\tfrom optimize import *\n\tfrom llm_vm.client import Client\n\tclient = Client(big_model='gpt', small_model='neo')\n\tif __name__ == \"__main__\":\n\t    try:\n\t        load_dotenv()\n", "    except:\n\t        pass\n\t    openai.api_key = os.getenv('LLM_VM_OPENAI_API_KEY')\n\t    print(\"key:\", openai.api_key)\n\t    data_synthesizer = data_synthesis.DataSynthesis(0.87, 50)\n\t    # for one-shot prompt\n\t    prompt = \"What is the currency in myanmmar?\"\n\t    response = client.complete(prompt=prompt, context = \"\", openai_key=\"\",temperature = 0.0)[\"completion\"]\n\t    print(f\"Prompt: {prompt} /nResponse: {response}\")\n\t    # for k-shot prompt\n", "    prompt_list = [\"What is the currency in Madagascar?\", \"What is the currency in myanmmar?\", \"What is the currency in Morocco?\"]\n\t    response_list = []\n\t    for p in prompt_list:\n\t        res = client.complete(prompt=p, context = \"\", openai_key=\"\", temperature = 0.0)\n\t        response_list.append(res[\"completion\"])\n\t    data_synthesizer.data_synthesis(client.optimizer, prompt_list, response_list,openai_key=\"\", temperature=0.0)\n"]}
{"filename": "tests/test_optimize.py", "chunked_list": ["from dotenv import load_dotenv\n\timport os\n\timport openai\n\timport sys\n\tfrom llm_vm.completion.optimize import *\n\thaskell = '''\n\tdef call_gpt(state, cur_prompt: str, stop: str, max_tokens = 20, quality = \"best\", temperature = 0.0):\n\t    if state.verbose > 1:\n\t        print_op(\"\\nGPT input for {\" +stop + \"} \"+ str(len(cur_prompt)) + \".\")\n\t    if state.verbose > 2:\n", "        print_op(prepPrintPromptContext(cur_prompt))\n\t    ask_tokens = max_tokens + len(cur_prompt) / 2.7\n\t    if state.verbose > 0:\n\t        print_op(\"ASK_TOKENS:\", ask_tokens)\n\t    if (ask_tokens) > 2049:\n\t        quality = 'best'\n\t    model = { 'best' : (\"text-davinci-003\", 0.02),\n\t              'okay' : (\"text-curie-001\", 0.002),\n\t             }[quality]\n\t    def calcCost(p):\n", "        return (len(p) / 2700.0) * model[1]\n\t    cost = calcCost(cur_prompt)\n\t    try:\n\t        ans = openai.Completion.create(\n\t            model=model[0],\n\t            max_tokens=max_tokens,\n\t            stop=stop,\n\t            prompt=cur_prompt,\n\t            temperature=temperature\n\t        )\n", "    except Exception as e:\n\t        print_op(\"WTF:\", e)\n\t        state.price += cost\n\t        return \"OpenAI is down!\"\n\t    response_text = ans['choices'][0]['text']\n\t    simpleprice = model[1] * ans['usage']['total_tokens'] / 1000\n\t    if state.verbose > 0:\n\t        print_op(\"SimplePrice: $\"+str(simpleprice))\n\t    state.price += simpleprice\n\t    if state.verbose > 2:\n", "        print_op(\"GPT output:\")\n\t        print_op(prepPrintPromptContext(response_text))\n\t        print_op(\"GPT output fin.\\n\")\n\t    return response_text\n\tdef delete_file(file_name):\n\t    location = os.getcwd()\n\t    path = os.path.join(location, file_name)\n\t    os.remove(path)\n\t    return True\n\tdef call_ChatGPT(cur_prompt, stop = None, max_tokens = 20, temperature = 0.2, gpt4 = False):\n", "    ans = openai.ChatCompletion.create(\n\t        model=\"gpt-3.5-turbo-0301\" if not gpt4 else 'gpt-4',\n\t        max_tokens=max_tokens,\n\t        stop=stop,\n\t        messages=cur_prompt,\n\t        temperature=temperature)\n\t    return ans['choices'][0]['message']['content']\n\t    return response_text\n\tdef call_gpt(cur_prompt: str, stop: str, max_tokens = 20, quality = \"best\", temperature = 0.0, model = \"text-davinci-003\"):\n\t    ans = openai.Completion.create(\n", "        model=model,\n\t        max_tokens=max_tokens,\n\t        stop=stop,\n\t        prompt=cur_prompt,\n\t        temperature=temperature\n\t    )\n\t    return ans['choices'][0]['text']\n\timport gzip\n\timport json\n\tdef create_jsonl_file(data_list: list, file_name: str, compress: bool = True) -> None:\n", "    \"\"\"\n\t    Method saves list of dicts into jsonl file.\n\t    :param data: (list) list of dicts to be stored,\n\t    :param filename: (str) path to the output file. If suffix .jsonl is not given then methods appends\n\t        .jsonl suffix into the file.\n\t    :param compress: (bool) should file be compressed into a gzip archive?\n\t    \"\"\"\n\t    sjsonl = '.jsonl'\n\t    sgz = '.gz'\n\t    # Check filename\n", "    if not file_name.endswith(sjsonl):\n\t        file_name = file_name + sjsonl\n\t    # Save data\n\t    if compress:\n\t        file_name = file_name + sgz\n\t        with gzip.open(file_name, 'w') as compressed:\n\t            for ddict in data_list:\n\t                jout = json.dumps(ddict) + '\\n'\n\t                jout = jout.encode('utf-8')\n\t                compressed.write(jout)\n", "    else:\n\t        with open(file_name, 'w') as out:\n\t            for ddict in data_list:\n\t                jout = json.dumps(ddict) + '\\n'\n\t                out.write(jout)\n\t    return file_name, open(file_name, \"rb\")\n\t'''\n\tdef run_test_stub():\n\t    try:\n\t        load_dotenv()\n", "    except:\n\t        pass\n\t    openai_api_key = os.getenv('LLM_VM_OPENAI_API_KEY')\n\t    openai.api_key =openai_api_key\n\t    # anarchy_key = os.getenv('LLM_VM_ANARCHY_KEY')\n\t    print(\"key:\", openai.api_key[0:5])\n\t    optimizer = LocalOptimizer(MIN_TRAIN_EXS=1,openai_key=openai_api_key)\n\t    #optimizer = HostedOptimizer(openai_key = openai.api_key,\n\t    #                            anarchy_key = anarchy_key,\n\t    #                            MIN_TRAIN_EXS=2)\n", "    i = 0\n\t    optimizer.complete(\"Answer question Q. \",\"Q: What is the currency in myanmmar\", \\\n\t                 temperature = 0.0, data_synthesis = True,\\\n\t                 min_examples_for_synthesis=0,finetune=True)\n\tif __name__ == \"__main__\":\n\t    run_test_stub()\n\t    '''\n\t    for h in haskell.splitlines():\n\t        print(\"At: \", i)\n\t        try:\n", "            print(optimizer.complete(\"Please convert this line to some haskell:\", h + \"\\nHaskell:\", max_tokens = 100, temperature = 0.0))\n\t        except Exception as e:\n\t            print('E:', e)\n\t        time.sleep(2)\n\t        if i > 3 and i < 20:\n\t            time.sleep(120)\n\t        i += 1\n\t    '''\n"]}
{"filename": "src/llm_vm/onsite_llm.py", "chunked_list": ["import abc\n\tfrom abc import ABC,abstractmethod\n\timport openai\n\timport math\n\tfrom transformers import (\n\t    AutoModelForMaskedLM,\n\t    AutoModelForSeq2SeqLM,\n\t    AutoTokenizer,\n\t    BertTokenizer,\n\t    OPTForCausalLM,\n", "    BloomForCausalLM,\n\t    LlamaTokenizer,\n\t    LlamaForCausalLM,\n\t    GPTNeoForCausalLM,\n\t    GPTNeoXForCausalLM,\n\t    GPT2Tokenizer,\n\t    DataCollatorForLanguageModeling,\n\t    TrainingArguments,\n\t    Trainer)\n\timport time\n", "from datetime import datetime\n\timport tempfile\n\timport json\n\timport os\n\timport torch\n\t__private_key_value_models_map =  {}\n\t# []   {\n\t#         \"opt\": Small_Local_OPT,\n\t#         \"bloom\": Small_Local_Bloom,\n\t#         \"neo\": Small_Local_Neo,\n", "#         \"llama\": Small_Local_LLama,\n\t#         \"pythia\": Small_Local_Pythia,\n\t#         \"gpt\": GPT3,\n\t#         \"chat_gpt\": Chat_GPT,\n\t#         \"flan\" : Small_Local_Flan_T5,\n\t#         \"pythia\" : Small_Local_Pythia,\n\t#         }\n\tdef RegisterModelClass(name):\n\t    def regClass(cls):\n\t        __private_key_value_models_map[name]=cls \n", "    return regClass\n\tmodel_keys_registered = __private_key_value_models_map.keys()        \n\t# Dictionary of models to be loaded in ModelConfig\n\tdef load_model_closure(model_name):\n\t    models = __private_key_value_models_map\n\t    return models[model_name]\n\t# this is a hack till we add dynaconf or something?\n\tif os.name == \"nt\":\n\t    homepath = os.path.join('C:\\\\','Users',os.getlogin())\n\telse:\n", "    homepath = os.environ.get(\"HOME\")\n\tmodel_path_default = os.path.join( homepath , \".llm_vm\", \"models\")\n\tos.makedirs(model_path_default, exist_ok = True)\n\tdef create_jsonl_file(data_list):\n\t    out = tempfile.TemporaryFile('w+')\n\t    for a,b in data_list:\n\t        out.write(json.dumps({'prompt': a, 'completion': b}) + \"\\n\")\n\t    out.seek(0)\n\t    return out\n\tclass FinetuningDataset(torch.utils.data.Dataset):\n", "    def __init__(self,iterable_dataset,length):\n\t        self.dataset = list(iterable_dataset)\n\t        self.length = length\n\t    def __len__(self):\n\t        return self.length\n\t    def __getitem__(self, idx):\n\t        return self.dataset[idx]\n\tclass Base_Onsite_LLM(ABC):\n\t    def __init__(self,model_uri=None,tokenizer_kw_args={},model_kw_args={}):\n\t        if model_uri != None :\n", "            self.model_uri= model_uri\n\t        if model_uri is None and self.model_uri is None:\n\t            raise ValueError('A very specific bad thing happened.')\n\t        self.model_name : str = self.model_uri.split('/')[-1] # our default for deriving model name\n\t        self.model=self.model_loader(**model_kw_args)\n\t        self.tokenizer=self.tokenizer_loader(**tokenizer_kw_args)\n\t    @property\n\t    @abstractmethod\n\t    def model_uri(self):\n\t        pass\n", "    @model_uri.setter\n\t    def model_uri(self,val):\n\t        self.model_uri=val # check if this is correct\n\t    # model_name : str = self.model_uri.split('/')[-1]\n\t    @abstractmethod\n\t    def model_loader(self):\n\t        pass\n\t    @abstractmethod\n\t    def tokenizer_loader(self):\n\t        pass\n", "    def load_finetune(self, model_filename):\n\t        self.model.load_state_dict(torch.load(os.path.join(model_path_default,\"finetuned_models\", self.model_name, model_filename)))\n\t    def generate(self,prompt,max_length=100,**kwargs): # both tokenizer and model take kwargs :(\n\t        \"\"\"\n\t        This function uses the class's llm and tokenizer to generate a response given a user's prompt\n\t        Parameters:\n\t            prompt (str): Prompt to send to LLM\n\t            max_length (int): Optional parameter limiting response length\n\t        Returns:\n\t            str: LLM Generated Response\n", "        Example:\n\t           >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n\t           I think it takes about a week for the apple to grow.\n\t        \"\"\"\n\t        inputs=self.tokenizer(prompt,return_tensors=\"pt\")\n\t        generate_ids=self.model.generate(inputs.input_ids,max_length=max_length)\n\t        resp= self.tokenizer.batch_decode(generate_ids,skip_special_tokens=True,clean_up_tokenization_spaces=False)[0]\n\t        # need to drop the len(prompt) prefix with these sequences generally\n\t        # because they include the prompt.\n\t        return resp[len(prompt):]\n", "    def finetune(self,data, optimizer, c_id):\n\t        def asynctune():\n\t            old_model = optimizer.storage.get_model(c_id)\n\t            if old_model is not None:\n\t                self.model.load_state_dict(torch.load(old_model))\n\t            untokenized_final_dataset = []\n\t            for prompt,response in data:\n\t                untokenized_final_dataset.append(prompt + response)\n\t            tokenized_final_dataset = map(self.tokenizer,untokenized_final_dataset)\n\t            self.tokenizer.pad_token = self.tokenizer.eos_token\n", "            data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False)\n\t            optimizer.storage.set_training_in_progress(c_id, True)\n\t            training_args = TrainingArguments(\n\t                output_dir=os.path.join(model_path_default,\"finetuned_models\",),\n\t                evaluation_strategy=\"epoch\",\n\t                learning_rate=2e-5,\n\t                per_device_train_batch_size = 1,\n\t                per_device_eval_batch_size = 1,\n\t                num_train_epochs=1,\n\t                weight_decay=0.01,\n", "                report_to= \"none\",\n\t            )\n\t            test_set = FinetuningDataset(tokenized_final_dataset,len(untokenized_final_dataset))\n\t            trainer = Trainer(\n\t                model=self.model,\n\t                args=training_args,\n\t                train_dataset=test_set,\n\t                eval_dataset=test_set,\n\t                data_collator=data_collator,\n\t            )\n", "            os.makedirs(os.path.join(model_path_default,\"finetuned_models\", self.model_name), exist_ok=True)\n\t            if tokenized_final_dataset:\n\t                trainer.train()\n\t                eval_results = trainer.evaluate()\n\t            optimizer.storage.set_training_in_progress(c_id, False)\n\t            if os.name == \"nt\":\n\t                timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n\t            else:\n\t                timestamp = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n\t            new_model = os.path.join(model_path_default,\"finetuned_models\",self.model_name, timestamp + '_' + self.model_name + \".pt\" )\n", "            open(new_model,\"a\")\n\t            torch.save(self.model.state_dict(), new_model) # the model in memory is different now\n\t            self.model_name = self.model_name + \"_ft_\"+  timestamp\n\t            optimizer.storage.set_model(c_id, new_model)\n\t            return math.exp(eval_results['eval_loss']) #perplexity is the metric we use for finetuning measurement\n\t        return asynctune\n\t    def finetune_immediately(self):\n\t        finetune()()\n\t\"\"\"\n\tthis factorization isn't necessarily the greatest, nor should it be viewed\n", "as likely being more general, aside from covering hugging face transformers\n\t\"\"\"\n\t@RegisterModelClass(\"pythia\")\n\tclass Small_Local_Pythia(Base_Onsite_LLM):\n\t    \"\"\"\n\t    This is a class for ElutherAI's Pythia-70m LLM\n\t    Attributes:\n\t        model_uri (str): Hugging Face Endpoint for LLM\n\t        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n\t        model (LLM): The large language model\n", "    Methods:\n\t        model_loader: Loads the LLM into memory\n\t        tokenizer_loader: Loads the tokenizer into memory\n\t        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n\t    \"\"\"\n\t    # def __init__(self,**kwargs):\n\t    #     # self.model_uri =\n\t    #     super().__init__(kwargs) ## this line is required\n\t    model_uri = \"EleutherAI/pythia-70m-deduped\"\n\t    def model_loader(self):\n", "        return GPTNeoXForCausalLM.from_pretrained(self.model_uri)\n\t    def tokenizer_loader(self):\n\t        return AutoTokenizer.from_pretrained(self.model_uri)\n\t@RegisterModelClass(\"opt\")\n\tclass Small_Local_OPT(Base_Onsite_LLM):\n\t    \"\"\"\n\t    This is a class for Facebook's OPT-350m LLM\n\t    Attributes:\n\t        model_uri (str): Hugging Face Endpoint for LLM\n\t        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n", "        model (LLM): The large language model\n\t    Methods:\n\t        model_loader: Loads the LLM into memory\n\t        tokenizer_loader: Loads the tokenizer into memory\n\t        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n\t    \"\"\"\n\t    model_uri=\"facebook/opt-350m\"\n\t    def model_loader(self):\n\t        return OPTForCausalLM.from_pretrained(self.model_uri)\n\t    def tokenizer_loader(self):\n", "        return AutoTokenizer.from_pretrained(self.model_uri)\n\t@RegisterModelClass(\"bloom\")\n\tclass Small_Local_Bloom(Base_Onsite_LLM):\n\t    \"\"\"\n\t    This is a class for BigScience's bloom-560 LLM\n\t    Attributes:\n\t        model_uri (str): Hugging Face Endpoint for LLM\n\t        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n\t        model (LLM): The large language model\n\t    Methods:\n", "        model_loader: Loads the LLM into memory\n\t        tokenizer_loader: Loads the tokenizer into memory\n\t        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n\t    \"\"\"\n\t    model_uri=\"bigscience/bloom-560m\"\n\t    def model_loader(self):\n\t        return BloomForCausalLM.from_pretrained(self.model_uri)\n\t    def tokenizer_loader(self):\n\t        return AutoTokenizer.from_pretrained(self.model_uri)\n\t@RegisterModelClass(\"neo\")\n", "class Small_Local_Neo(Base_Onsite_LLM):\n\t    \"\"\"\n\t    Attributes:\n\t        model_uri (str): Hugging Face Endpoint for LLM\n\t        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n\t        model (LLM): The large language model\n\t    Methods:\n\t        model_loader: Loads the LLM into memory\n\t        tokenizer_loader: Loads the tokenizer into memory\n\t        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n", "    \"\"\"\n\t    model_uri=\"EleutherAI/gpt-neo-1.3B\"\n\t    def model_loader(self):\n\t        return GPTNeoForCausalLM.from_pretrained(self.model_uri)\n\t    def tokenizer_loader(self):\n\t        return GPT2Tokenizer.from_pretrained(self.model_uri)\n\t@RegisterModelClass(\"llama\")\n\tclass Small_Local_LLama(Base_Onsite_LLM):\n\t    \"\"\"\n\t    This is a class for Openlm-Research's open_llama-3b LLM\n", "    Attributes:\n\t        model_uri (str): Hugging Face Endpoint for LLM\n\t        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n\t        model (LLM): The large language model\n\t    Methods:\n\t        model_loader: Loads the LLM into memory\n\t        tokenizer_loader: Loads the tokenizer into memory\n\t        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n\t    \"\"\"\n\t    model_uri=\"openlm-research/open_llama_3b_v2\"\n", "    def model_loader(self):\n\t        return LlamaForCausalLM.from_pretrained(self.model_uri)\n\t    def tokenizer_loader(self):\n\t        return LlamaTokenizer.from_pretrained(self.model_uri)\n\t@RegisterModelClass(\"flan\")# our yummiest model based on similarity to food\n\tclass Small_Local_Flan_T5(Base_Onsite_LLM):\n\t    \"\"\"\n\t    This is a class for Google's flan-t5 LLM\n\t    Attributes:\n\t        model_uri (str): Hugging Face Endpoint for LLM\n", "        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n\t        model (LLM): The large language model\n\t    Methods:\n\t        model_loader: Loads the LLM into memory\n\t        tokenizer_loader: Loads the tokenizer into memory\n\t        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n\t    \"\"\"\n\t    model_uri=\"google/flan-t5-small\"\n\t    def model_loader(self):\n\t        return AutoModelForSeq2SeqLM.from_pretrained(self.model_uri)\n", "    def tokenizer_loader(self):\n\t        return AutoTokenizer.from_pretrained(self.model_uri)\n\t@RegisterModelClass(\"bert\")\n\tclass Small_Local_BERT(Base_Onsite_LLM):\n\t    \"\"\"\n\t    This is a class for BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\t    The base model needs finetuning in almost all cases.\n\t    Attributes:\n\t        model_uri (str): Hugging Face Endpoint for LLM\n\t        tokenizer (AutoTokenizer): Tokenizer from Transformer's library\n", "        model (LLM): The large language model\n\t    Methods:\n\t        model_loader: Loads the LLM into memory\n\t        tokenizer_loader: Loads the tokenizer into memory\n\t        generate: Generates a response from a given prompt with the loaded LLM and tokenizer\n\t    \"\"\"\n\t    model_uri = \"bert-base-cased\"\n\t    def model_loader(self):\n\t        return AutoModelForMaskedLM.from_pretrained(self.model_uri)\n\t    def tokenizer_loader(self):\n", "        return BertTokenizer.from_pretrained(self.model_uri)\n\t@RegisterModelClass(\"gpt\")\n\tclass GPT3:\n\t    \"\"\"\n\t    This is a class for openAI's completion endpoint\n\t    Methods:\n\t        generate: Generates a response from a given prompt with OpenAI's completion endpoint\n\t    \"\"\"\n\t    def generate(self,prompt, max_length=100,**kwargs): # both tokenizer and model take kwargs :(\n\t        \"\"\"\n", "        This function uses openAI's API to generate a response from the prompt\n\t        Parameters:\n\t            prompt (str): Prompt to send to LLM\n\t            max_length (int): Optional parameter limiting response length\n\t        Returns:\n\t            str: LLM Generated Response\n\t        Example:\n\t            >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n\t            It typically takes about 100-200 days...\n\t        \"\"\"\n", "        ans = openai.Completion.create(prompt= prompt, model=\"text-davinci-003\", **kwargs)\n\t        return ans['choices'][0]['text']\n\t    def finetune(self, dataset, optimizer, c_id):\n\t        old_model = optimizer.storage.get_model(c_id)\n\t        training_file = create_jsonl_file(dataset)\n\t        upload_response = openai.File.create(file=training_file, purpose=\"fine-tune\")\n\t        training_file.close()\n\t        fine_tuning_job = openai.FineTune.create(training_file= upload_response.id)\n\t        print(f\"Fine-tuning job created: {fine_tuning_job}\", flush=True)\n\t        global job_id # global state isn't great, but thats interrupt handlers\n", "        job_id = fine_tuning_job[\"id\"]\n\t        while True:\n\t            fine_tuning_status = openai.FineTune.retrieve(id=job_id)\n\t            status = fine_tuning_status[\"status\"]\n\t            print(f\"Fine-tuning job status: {status}\")\n\t            if status in [\"succeeded\", \"completed\", \"failed\"]:\n\t                break\n\t            time.sleep(30)\n\t        job_id = None #\n\t        new_model_id = fine_tuning_status.fine_tuned_model\n", "        print(\"New_model_id: \", new_model_id, flush=True)\n\t        optimizer.storage.set_model(c_id, new_model_id)\n\t        optimizer.storage.set_training_in_progress(c_id, False)\n\t        if old_model is not None:\n\t            openai.Model.delete(old_model)\n\t@RegisterModelClass(\"chat_gpt\")\n\tclass Chat_GPT:\n\t    \"\"\"\n\t    This is a class for openAI's gpt-3.5-turbo LLM\n\t    Methods:\n", "        generate: Generates a response from a given prompt through OpenAI's endpoint\n\t    \"\"\"\n\t    def generate(self,prompt, max_length=100,**kwargs): # both tokenizer and model take kwargs :(\n\t        \"\"\"\n\t        This function uses openAI's API to generate a response from the prompt\n\t        Parameters:\n\t            prompt (str): Prompt to send to LLM\n\t            max_length (int): Optional parameter limiting response length\n\t        Returns:\n\t            str: LLM Generated Response\n", "        Example:\n\t            >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n\t            It typically takes about 100-200 days...\n\t        \"\"\"\n\t        cur_prompt = [{'role': \"system\", 'content' : prompt}]\n\t        ans = openai.ChatCompletion.create(\n\t            messages=cur_prompt,\n\t            model=\"gpt-3.5-turbo-0301\",\n\t            **kwargs)\n\t        return ans['choices'][0]['message']['content']\n", "    def finetune(self, dataset, optimizer, c_id):\n\t        print(\"fine tuning isn't supported by OpenAI on this model\")\n\t        exit()\n\t        # old_model = optimizer.storage.get_model(c_id)\n\t        # training_file = create_jsonl_file(dataset)\n\t        # upload_response = openai.File.create(file=training_file, purpose=\"fine-tune\")\n\t        # training_file.close()\n\t        # fine_tuning_job = openai.FineTune.create(training_file= upload_response.id)\n\t        # print(f\"Fine-tuning job created: {fine_tuning_job}\", flush=True)\n\t        # global job_id # global state isn't great, but thats interrupt handlers\n", "        # job_id = fine_tuning_job[\"id\"]\n\t        # while True:\n\t        #     fine_tuning_status = openai.FineTune.retrieve(id=job_id)\n\t        #     status = fine_tuning_status[\"status\"]\n\t        #     print(f\"Fine-tuning job status: {status}\")\n\t        #     if status in [\"succeeded\", \"completed\", \"failed\"]:\n\t        #         break\n\t        #     time.sleep(30)\n\t        # job_id = None #\n\t        # new_model_id = fine_tuning_status.fine_tuned_model\n", "        # print(\"New_model_id: \", new_model_id, flush=True)\n\t        # optimizer.storage.set_model(c_id, new_model_id)\n\t        # optimizer.storage.set_training_in_progress(c_id, False)\n\t        # if old_model is not None:\n\t        #     openai.Model.delete(old_model)\n"]}
{"filename": "src/llm_vm/config.py", "chunked_list": ["import re\n\timport os\n\timport argparse\n\tfrom dynaconf import Dynaconf\n\tfrom xdg import XDG_CONFIG_HOME\n\tfrom llm_vm.onsite_llm import model_keys_registered\n\tfrom llm_vm.data_path import project_root\n\t# Parse CLI arguments\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument('-b', '--big_model', type=str, help='Big LLM Model.')\n", "parser.add_argument('-p', '--port', type=int,      help='Port Number.')\n\tparser.add_argument('-s', '--small_model',type=str,help='Small LLM Model.')\n\tparser.add_argument('-H', '--host', type=str,      help='Host Address.')\n\tparser.add_argument('-K', '--openai_key',type=str,help='OpenAI api key')\n\targs = parser.parse_args()\n\t# Set the CLI argument values to environment variables if they are present\n\tif args.big_model is not None:\n\t    os.environ['LLM_VM_BIG_MODEL'] = args.big_model\n\tif args.port is not None:\n\t    os.environ['LLM_VM_PORT'] = str(args.port)\n", "if args.small_model is not None:\n\t    os.environ['LLM_VM_SMALL_MODEL'] = args.small_model\n\tif args.host is not None:\n\t    os.environ['LLM_VM_HOST'] = args.host\n\tif \"openai_api_key\" in args:\n\t    os.environ['LLM_VM_OPENAI_API_KEY'] = args.openai_api_key\n\t# project_root = os.path.abspath(os.getcwd())\n\tprint(\"Project Root: \" + project_root)\n\t# default to the local file, and look in XDG if we can't find it\n\tconfig_files = [\n", "    os.path.join(project_root, \"settings.default.toml\"),\n\t    os.path.join(XDG_CONFIG_HOME, \"settings.toml\"),\n\t    os.path.join(project_root, \"settings.toml\"),\n\t]\n\tsettings = Dynaconf(\n\t    settings_file=config_files,\n\t    load_dotenv=True,\n\t    dotenv_path=\".env\",\n\t    envvar_prefix=\"LLM_VM\",\n\t)\n", "# making MODELS_AVAILABLE a set because it will be used for membership testing\n\tMODELS_AVAILABLE = set(model_keys_registered)\n\tif settings.big_model not in MODELS_AVAILABLE:\n\t    print(settings.big_model + \" is an invalid Model selection for Big LLM Model\")\n\t    exit()\n\tif settings.small_model not in MODELS_AVAILABLE:\n\t    print(settings.small_model + \" is an invalid Model selection for Small LLM Model\")\n\t    exit()\n\t# do we want to do this early test and exit?\n\t# if settings.small_model is \"chat_gpt\":\n", "#     print(\"openai currently doesn't support fine tuning chat_gpt, aka gpt3.5-turbo, exiting\")\n\t#     exit()\n\tdef isOpenAIModel(str):\n\t    str ==\"gpt\" or str ==\"chat_gpt\"\n\tif settings.openai_api_key is None and (isOpenAIModel(settings.small_model) or isOpenAIModel(settings.big_model)):\n\t    print(\"Error: you must have an OpenAI API key set via config files, ./settings.default.toml or via environment variable \")\n\t    print(\"LLM_VM_OPEN_AI_API,if you wish to use their models. Exiting\")\n\t    exit()\n\t# check args.host is a valid IP address\n\tpattern = r'^\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}$'\n", "if not re.match(pattern, settings.host):\n\t    print(\"Invalid IP address. Reverting to the default host, 127.0.0.1\")\n\t    settings.host = '127.0.0.1'\n\telse:\n\t    # validates each number in the IP address is between 0-255\n\t    octets = settings.host.split('.')\n\t    valid_ip = True\n\t    for octet in octets:\n\t        if not 0 <= int(octet) <= 255:\n\t            valid_ip = False\n", "            break\n\t    if not valid_ip:\n\t        print(\"Invalid IP address range. Reverting to the default host, 127.0.0.1\")\n\t        settings.host = '127.0.0.1'\n\tassert settings.big_model in MODELS_AVAILABLE, f\"{settings.big_model} is not a valid Model selection for Big LLM Model\"\n\tassert settings.small_model in MODELS_AVAILABLE, f\"{settings.small_model} is not a valid Model selection for Small LLM Model\"\n\tassert 1024 < settings.port < 65535, f\"{settings.port} is an invalid port number\"\n"]}
{"filename": "src/llm_vm/client.py", "chunked_list": ["import openai\n\timport llm_vm.onsite_llm as llms\n\tfrom llm_vm.onsite_llm import load_model_closure\n\tfrom llm_vm.agents.REBEL import agent\n\tfrom llm_vm.completion.optimize import LocalOptimizer\n\timport llm_vm.config as conf\n\timport os\n\tif conf.settings.big_model is None:\n\t    default_big_model = \"chat_gpt\"\n\telse:\n", "    default_big_model= conf.settings.big_model\n\tif conf.settings.small_model is not  None:\n\t    default_small_model= \"pythia\"\n\telse:    \n\t    default_small_model = conf.settings.small_model\n\tclass Client:\n\t    \"\"\"\n\t    This is a class sets up a Local optimizer around a LLM for easy access.\n\t    Attributes:\n\t        optimizer (LocalOptimizer): Anarchy LLM Optimizer\n", "        openai_key (str): API key for OpenAI access\n\t    Methods:\n\t        complete: The Anarchy completion endpoint giving access to a specified LLM\n\t    \"\"\"\n\t    def __init__(self, big_model = default_big_model, small_model =default_small_model,big_model_config={},small_model_config={}):\n\t        \"\"\"\n\t        This __init__ function allows the user to specify which LLM they want to use upon instantiation.\n\t        Parameters:\n\t            big_model (str): Name of LLM to be used as a reliable source\n\t            small_model (str): Name of small model to be used for fine-tuning\n", "        Returns:\n\t            None\n\t        Example:\n\t            >>> client = Client(big_model = 'neo')\n\t        \"\"\"\n\t        self.teacher = load_model_closure(big_model)(**big_model_config)\n\t        self.student = load_model_closure(small_model)(**small_model_config)\n\t        ## FIXME, do something like $HOME/.llm_vm/finetuned_models/\n\t        if os.path.isdir(\"finetuned_models\") == False:\n\t            os.mkdir(\"finetuned_models\")\n", "        # Specify the model strategy the user will use\n\t        # MODELCONFIG = models.ModelConfig(big_model=big_model, small_model=small_model)\n\t        print(\"Using model: \" + big_model) # announce the primary LLM that is generating results\n\t        # These functions allow for proper initialization of the optimizer\n\t        # def CALL_BIG(prompt, max_len=256, **kwargs):\n\t        #     return self.teacher.generate(prompt, max_len,**kwargs)\n\t        def CALL_SMALL(prompt, max_len=256, **kwargs):\n\t            return self.student.generate(prompt, max_len,**kwargs)\n\t        # load the optimizer into object memory for use by the complete function\n\t        self.optimizer = LocalOptimizer(MIN_TRAIN_EXS=2,openai_key=None, call_big=self.CALL_BIG, call_small= CALL_SMALL,\n", "                                        big_model = self.teacher, small_model = self.student)\n\t        self.rebel_agent = None # only initialized on first use \n\t    # These functions allow for proper initialization of the optimizer\n\t    def CALL_BIG(self, prompt, max_len=256, **kwargs):\n\t        return self.teacher.generate(prompt, max_len,**kwargs)\n\t    def complete(self, prompt,\n\t                 context,\n\t                 openai_key = None,\n\t                 finetune=False,\n\t                 data_synthesis = False,\n", "                 temperature=0,\n\t                 stoptoken = None,\n\t                 tools = None):\n\t        \"\"\"\n\t        This function is Anarchy's completion entry point\n\t        Parameters:\n\t            prompt (str): Prompt to send to LLM for generation\n\t            context (str): Context to send to the LLM for generation\n\t            finetune (bool): Boolean value that begins fine tuning when set to True\n\t            data_synthesis (bool): Boolean value to determine whether data should be synthesized for fine-tuning or not\n", "            temperature (float): An analog\n\t        Returns:\n\t            str: LLM Generated Response\n\t        Example:\n\t           >>> Small_Local_OPT.generate(\"How long does it take for an apple to grow?)\n\t           How long does it take for an apple tree to grow?\n\t        \"\"\"\n\t        static_context = context\n\t        dynamic_prompt = prompt\n\t        use_rebel_agent = False\n", "        kwargs = {}\n\t        kwargs.update({\"temperature\":temperature})\n\t        if openai_key is not None:\n\t            self.openai_key = openai_key\n\t        if stoptoken is not None:\n\t            kwargs.update({\"stop\":stoptoken})\n\t        if tools is not None:\n\t            if type(tools) != list:\n\t                return Exception(\"Wrong data type for tools. Should be a list\")\n\t            else:\n", "                final_tools=[]\n\t                for i in tools:\n\t                    temp_tool_dict = {}\n\t                    temp_args_dict = {}\n\t                    temp_tool_dict.update({\"description\":i[\"description\"]})\n\t                    temp_tool_dict.update({\"dynamic_params\":i[\"dynamic_params\"]})\n\t                    temp_tool_dict.update({\"method\":i[\"method\"]})\n\t                    temp_args_dict.update({\"url\":i[\"url\"]})\n\t                    temp_args_dict.update({\"params\":{}})\n\t                    for j in i[\"static_params\"].keys():\n", "                        temp_args_dict[\"params\"].update({j:i[\"static_params\"][j]})\n\t                    for k in i[\"dynamic_params\"].keys():\n\t                        temp_args_dict[\"params\"].update({k:\"{\"+k+\"}\"})\n\t                    temp_tool_dict.update({\"args\":temp_args_dict})\n\t                    final_tools.append(temp_tool_dict)\n\t                if self.rebel_agent is None:\n\t                    self.rebel_agent = agent.Agent(\"\", [], verbose=1) # initialize only if tools registered\n\t                self.rebel_agent.set_tools(final_tools)\n\t                use_rebel_agent = True\n\t        try:\n", "            if openai_key is not None:\n\t                openai.api_key = self.openai_key\n\t        except:\n\t            return  {\"status\":0, \"resp\":\"Issue with OpenAI key\"}\n\t        self.optimizer.openai_key = openai.api_key\n\t        # self.agent.set_api_key(openai.api_key,\"OPENAI_API_KEY\") # \n\t        if os.getenv(\"OPENAI_API_KEY\") is None and use_rebel_agent==True :\n\t            print(\"warning: you need OPENAI_API_KEY environment variable for \")\n\t        try:\n\t            if not use_rebel_agent:\n", "                completion = self.optimizer.complete(static_context,dynamic_prompt,data_synthesis=data_synthesis,finetune = finetune, **kwargs)\n\t            else:\n\t                completion = self.rebel_agent.run(static_context+dynamic_prompt,[])[0]\n\t        except Exception as e:\n\t            return {\"status\":0, \"resp\": str(e)}\n\t        return {\"completion\":completion, \"status\": 200}\n\t    def load_finetune(self, model_filename=None):\n\t        self.teacher.load_finetune(model_filename)\n"]}
{"filename": "src/llm_vm/data_path.py", "chunked_list": ["import os\n\tfrom pathlib import Path\n\tpath = Path(__file__)\n\tproject_root= str(path.parent.absolute())\n\tif __name__ == 'main':\n\t    print(parent_root)"]}
{"filename": "src/llm_vm/agents/agent_interface.py", "chunked_list": ["\"\"\"\n\tThis file has been temporarily repurposed as an interface for users to\n\tcall any of the three agents (REBEL, BACKWARD_CHAINING, and FLAT) and interact with them.\n\tRunning this file prompts the user to choose any of the agents and ask it questions.\n\t\"\"\"\n\timport llm_vm.agents.REBEL.agent as REBEL\n\timport llm_vm.agents.FLAT.agent as FLAT\n\timport os\n\tkey = os.getenv(\"LLM_VM_OPENAI_API_KEY\")\n\tdef call_agent():\n", "    print(\"Try out any agent!\")\n\t    # stores user input for which agent to try out\n\t    model_choice = 0\n\t    # times that the user was prompted to choose a valid model\n\t    times_asked_for_model = 0\n\t    # if user enters invalid choice, prompt for input until valid\n\t    while True:\n\t        model_choice = input(\"[1] FLAT\\n[2] REBEL\\nChoose your agent:\")\n\t        try:\n\t            # try to cast the input to an integer\n", "            model_choice = int(model_choice)\n\t            if model_choice not in range (1, 4):\n\t                print(\"=====Please enter 1, or 2!=====\")\n\t            else:\n\t                # user has entered a valid input\n\t                break\n\t        except:\n\t            print(\"=====Please enter 1 or 2!=====\")\n\t    # FLAT\n\t    if model_choice == 1:\n", "        # TODO: Add agent call here when FLAT is fixed\n\t        tools =  [{'method': 'GET', \"dynamic_params\": { 'location': 'This string indicates the geographic area to be used when searching for businesses. \\\n\t    Examples: \"New York City\", \"NYC\", \"350 5th Ave, New York, NY 10118\".', 'term': 'Search term, e.g. \"food\" or \"restaurants\". The \\\n\t    term may also be the business\\'s name, such as \"Starbucks\"', 'price': 'Pricing levels to filter the search result with: 1 = \\\n\t    $, 2 = $$, 3 = $$$, 4 = $$$$. The price filter can be a list of comma delimited pricing levels. e.g., \"1, 2, 3\" will filter the \\\n\t    results to show the ones that are $, $$, or $$$.'}, \"description\":\"This tool searches for a business on yelp.  It's useful for finding restaurants and \\\n\t    whatnot.\", 'args' :{'url': 'https://api.yelp.com/v3/businesses/search', 'cert': '', 'json': {}, 'params': {'limit': '1',\n\t                                                                                                              'open_now': 'true', 'location': '{location}', 'term': '{term}', 'price': '{price}'}, 'data': {},\n\t                       'headers': {'authorization': '',\n\t                                   'accept': 'application/json'}}}]\n", "        agent = FLAT.Agent(key, tools, verbose=1)\n\t    elif model_choice == 2:\n\t        tools = REBEL.buildExampleTools()\n\t        agent = REBEL.Agent(key, tools, verbose = 1)\n\t    pass\n\t    mem = []\n\t    last = \"\"\n\t    while True:\n\t        inp = input(last+\"Human: \")\n\t        ret = agent.run(inp, mem)\n", "        mem = ret[1]\n\t        last = \"AI: \"+str(ret[0])+ \"\\n\"\n\tif __name__ == \"__main__\":\n\t    call_agent()\n"]}
{"filename": "src/llm_vm/agents/REBEL/bothandler.py", "chunked_list": ["import openai\n\timport re\n\tdef tool_picker(tools_list, question, starting_tool_num):\n\t    tools=\"\"\n\t    prompt= '''\n\t{tools}\n\tWhich tool (number only), if any, would you use to answer the following question:\n\t{question}\n\t    '''\n\t    count=0\n", "    tools_list =  tools_list[starting_tool_num:]\n\t    for i in tools_list:\n\t        tools+=\"tool \"+str(count)+\": \"+str(i[\"description\"])+\"\\n\"\n\t        count+=1\n\t    tools+=\"tool \"+str(count)+\": \"+str(\"Use this tool when the question can be answered without using any tool or if the question is a greeting or a casual conversation. Also if we need to convert languages other than english, use this tool.\")+\"\\n\"\n\t    prompt=prompt.format(**{\"tools\":tools,\"question\":question})\n\t    prompt += \"\\n\\nYOU JUST ANSWER WITH A NUMBER.\"\n\t    ans = openai.Completion.create(\n\t        model=\"text-davinci-003\",\n\t        max_tokens=256,\n", "        stop=None,\n\t        prompt=prompt,\n\t        temperature=0.4\n\t    )\n\t    try:\n\t        return (calcCost(prompt),str(int(re.sub(\"[^0-9]\", \"\",ans['choices'][0]['text'].strip()))+starting_tool_num))\n\t    except:\n\t        return  (calcCost(prompt),1+starting_tool_num)\n\tdef calcCost(p):\n\t            return (len(p) / 2700.0) * 0.02\n", "def question_split(question,tools_list,memory):\n\t    tools=\"\"\n\t    starting_tool_num=2\n\t    count=starting_tool_num\n\t    for i in range(len(tools_list)-starting_tool_num):\n\t        tools+=\"tool \"+str(i+starting_tool_num)+\": \"+str(tools_list[starting_tool_num+i][\"description\"])+\"\\n\"\n\t        count+=1\n\t    tools+=\"tool \"+str(count)+\": \"+str(\"Use this tool when the question can be answered without using any tool or if the question is a greeting or a casual conversation.\")+\"\\n\"\n\t    prompt='''\n\tTools we have access to =\n", "tool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\n\ttool 2: Find the driving distance and time to travel between two cities.\n\ttool 3: Find the weather at a location and returns it in celcius.\n\tQ=\"Is the distance between London and Paris larger than the distance between Boston and LA?\"\n\tLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n\tWhat is the distance from London to Paris?, What is the distance from Boston to LA?\n\tTools we have access to =\n\ttool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\n\ttool 2: Find the driving distance and time to travel between two cities.\n\ttool 3: Find the weather at a location and returns it in celcius.\n", "Q=\"Who was the maternal grandfather of george washington?\"\n\tLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n\tWho was george washington's mother?,Who was her father?\n\tTools we have access to =\n\ttool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\n\ttool 2: Find the driving distance and time to travel between two cities.\n\ttool 3: Find the weather at a location and returns it in celcius.\n\tQ:\"What is the currency in India\"\n\tLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n\tTools we have access to =\n", "tool 1: The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\n\ttool 2: Find the driving distance and time to travel between two cities.\n\ttool 3: Find the weather at a location and returns it in celcius.\n\tQ:\"If I am in Miami how far am I from Atlanta?\"\n\tLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n\tTools we have access to =\n\t{tools}\n\tQ= \"{question}\"\n\tLook at the tools we have access to. Split Q into subquestions to answer Q that can each be solved with one use of one tool. Make as few subquestions as possible. Split each subquestion with a comma and have no extra information other than the subquestions.\n\t    '''\n", "    prompt=prompt.format(**{\"question\":question,\"tools\":tools,\"memory\":memory})\n\t    ans = openai.Completion.create(\n\t    model=\"text-davinci-003\",\n\t    max_tokens=256,\n\t    stop=None,\n\t    prompt=prompt,\n\t    temperature=0.2\n\t    )\n\t    ans=ans['choices'][0]['text'].replace(\"\\n\",\"\").split(\",\")\n\t    return (calcCost(prompt),ans)\n", "def memory_check(memory,question):\n\t    prompt='''\n\tQ: \"What's up?\"\n\tIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no. yes\n\tQ: \"What color is the sky\"\n\tIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no. yes\n\tQ: \"What is the temperature in Portland?\"\n\tIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no. no\n\tMemory:\n\t{memory}\n", "Q: \"{question}\"\n\tIs the answer to Q found in the memory or in your knowledge base already? Answer with a yes or no.\n\t    '''\n\t    prompt=prompt.format(**{\"memory\":memory,\"question\":question})\n\t    ans = openai.Completion.create(\n\t                model=\"text-davinci-003\",\n\t                max_tokens=256,\n\t                stop=None,\n\t                prompt=prompt,\n\t                temperature=0.4\n", "            )\n\t    ans=ans['choices'][0]['text']\n\t    if \"yes\" in ans.lower():\n\t        return (calcCost(prompt),True)\n\t    else:\n\t        return (calcCost(prompt),False)\n\tdef replace_variables_for_values(my_dict: dict, dynamic_keys, ignore_key: str = \"_______\"):\n\t    replaced_dict = {}\n\t    for key, value in my_dict.items():\n\t        if (key == ignore_key):\n", "            continue;\n\t        formatted_key = key.format(**dynamic_keys)\n\t        if (isinstance(value, dict)):\n\t            formatted_value = replace_variables_for_values(value, dynamic_keys)\n\t        elif (isinstance(value, list)):\n\t            formatted_value = []\n\t            for item in value:\n\t                formatted_value += replace_variables_for_values(item, dynamic_keys)\n\t        else:\n\t            try:\n", "                formatted_value = value.format(**dynamic_keys)\n\t            except:\n\t                formatted_value = value\n\t        replaced_dict[formatted_key] = formatted_value\n\t    return replaced_dict\n\t#print(question_split(\"\"))\n"]}
{"filename": "src/llm_vm/agents/REBEL/agent.py", "chunked_list": ["\"\"\"\n\tThis Python script runs the REBEL agent, which takes into account of multiple questions within a\n\tsingle user input.\n\tThe user is prompted to enter a question, to which the AI responds.\n\tIf multiple questions are present, the script splits these into different subquestions and retrieves\n\tanswers from the respective APIs.\n\tThe script also takes into account of previous prompts as history, in case the user may\n\tenter related questions later.\n\tThe user will also see messages regarding which API for information was chosen and the\n\tprice of the API call.\n", "\"\"\"\n\timport os\n\timport sys\n\t# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n\t# current_dir = os.path.dirname(os.path.abspath(__file__))\n\t# grandparent_dir = os.path.dirname(os.path.dirname(current_dir))\n\t# utils_dir = os.path.join(grandparent_dir, \"utils/\")\n\t# sys.path.append(utils_dir)\n\tfrom llm_vm.utils.keys import *\n\tfrom llm_vm.utils.labels import *\n", "from llm_vm.utils.print_types import *\n\tfrom llm_vm.utils.tools import *\n\tfrom urllib.parse import urlencode\n\timport urllib.parse as urlparse\n\timport json\n\timport random\n\t# UNCOMMENT THESE IMPORTS SO THAT SUBQUESTION PORTION CAN WORK (NLP CURRENTLY NOT RECOGNIZED)\n\timport spacy\n\tnlp = None #spacy.load(\"en_core_web_md\")\n\tfrom math import sqrt, pow, exp\n", "try:\n\t    from .bothandler import (\n\t        question_split,\n\t        tool_picker,\n\t        memory_check,\n\t        replace_variables_for_values,\n\t    )\n\texcept:\n\t    from bothandler import (\n\t        question_split,\n", "        tool_picker,\n\t        memory_check,\n\t        replace_variables_for_values,\n\t    )\n\ttry:\n\t    from .utils import *\n\texcept:\n\t    from utils import *\n\tdef prepPrintPromptContext(p):\n\t    return \"--> \" + p.replace(\"\\n\", \"\\n--> \") if len(p) > 0 else \"\"\n", "def print_op(*kargs, **kwargs):\n\t    print(*kargs, **kwargs, flush=True)\n\trandom_fixed_seed = random.Random(4)\n\tQUALITY = 0.2\n\tdef buildExampleTools(tools=GENERIC_TOOLS):\n\t    new_tools = tools\n\t    # wolfram\n\t    new_tools[0][\"examples\"] = [\n\t        (\n\t            [\n", "                (\n\t                    \"What's the most popular spot to vacation if you are in Germany?\",\n\t                    \"Mallorca\",\n\t                ),\n\t                (\"What's the date\", \"July 7, 2009\"),\n\t            ],\n\t            \"How crowded is it there now?\",\n\t            '{\"q\": \"How many tourists visit Mallorca each July?\"}',\n\t        ),\n\t        (\n", "            [\n\t                (\"What is the circumference of a basketball?\", \"750mm\"),\n\t            ],\n\t            \"What is the volume of a basketball?\",\n\t            '{\"q\": \"What is the volume of a ball with circumference 750mm?\"}',\n\t        ),\n\t        (\n\t            [\n\t                (\"What's the fastest a ford explorer can drive?\", \"160mph\"),\n\t            ],\n", "            \"What is that multiplied by seventy?\",\n\t            '{\"q\": \"160 x 70\"}',\n\t        ),\n\t        (\n\t            [],\n\t            \"Is 5073 raised to the 3rd power divisible by 73?\",\n\t            '{\"q\": \"Is 5073^3 divisible by 73?\"}',\n\t        ),\n\t    ]\n\t    # geopy\n", "    new_tools[1][\"examples\"] = [\n\t        (\n\t            [\n\t                (\n\t                    \"I feel like taking a drive today to zurich can you help?\",\n\t                    \"Yes!  Where are you now?\",\n\t                ),\n\t            ],\n\t            \"I'm in Paris.\",\n\t            '{\"origins\": \"Paris\", \"destinations\": \"Zurich\"}',\n", "        ),\n\t        (\n\t            [],\n\t            \"How long would it take to get between South Africa and Kenya.\",\n\t            '{\"origins\": \"South Africa\", \"destinations\": \"Kenya\"}',\n\t        ),\n\t        (\n\t            [\n\t                (\"Where do elephant seals mate?\", \"San Luis Obispo\"),\n\t                (\"Thats really cool!\", \"Damn right.\"),\n", "                (\"Where do they migrate each year?\", \"Alaska\"),\n\t            ],\n\t            \"How many miles would they travel while doing that?\",\n\t            '{\"origins\": \"San Luis Obispo\", \"destinations\": \"Alaska\"}',\n\t        ),\n\t    ]\n\t    # weather\n\t    new_tools[2][\"examples\"] = [\n\t        (\n\t            [\n", "                (\"Are you allergic to seafood?\", \"I'm just an AI. I have no body.\"),\n\t                (\"What city is Obama in?\", \"NYC\"),\n\t                (\"What is the latitude of NYC?\", \"40.7128° N\"),\n\t                (\n\t                    \"What is the longitude of NYC?\",\n\t                    \"The longitude is 73° 56' 6.8712'' W\",\n\t                ),\n\t            ],\n\t            \"What is the weather in NYC?\",\n\t            '{\"longitude\": 73.56, \"latitude\": 40.41728}',\n", "        ),\n\t        (\n\t            [\n\t                (\"What is the longitude of Milan?\", \"9.1900° E\"),\n\t                (\"What is the latitude of Milan?\", \"45.4642° N\"),\n\t            ],\n\t            \"What is the weather in Milan?\",\n\t            '{\"longitude\": 45.4642, \"latitude\": 9.1900}',\n\t        ),\n\t    ]\n", "    return new_tools\n\tdef squared_sum(x):\n\t    \"\"\"return 3 rounded square rooted value\"\"\"\n\t    return round(sqrt(sum([a * a for a in x])), 3)\n\tdef cos_similarity(x, y):\n\t    \"\"\"return cosine similarity between two lists\"\"\"\n\t    numerator = sum(a * b for a, b in zip(x, y))\n\t    denominator = squared_sum(x) * squared_sum(y)\n\t    return round(numerator / float(denominator), 3)\n\tclass Agent:\n", "    def __init__(self, openai_key, tools, bot_str=\"\", verbose=4):\n\t        self.verbose = verbose\n\t        self.price = 0\n\t        self.tools = []\n\t        self.set_tools(buildExampleTools()+tools)\n\t        if bot_str == \"\":\n\t            self.bot_str = bot_str\n\t        else:\n\t            self.bot_str = \"<GLOBAL>\" + bot_str + \"<GLOBAL>\"\n\t        self.nlp=spacy.load(\"en_core_web_md\")\n", "        # set all the API resource keys to make calls\n\t        set_api_key(openai_key, \"OPENAI_API_KEY\")\n\t    def makeToolDesc(self, tool_id):\n\t        \"\"\"\n\t        Creates the tool description to contain the relevant tags according to the dynamic params\n\t        Parameters\n\t        ----------\n\t        tool_id\n\t            the tool's enum value as specified in the DefaultTools class\n\t        Returns\n", "        ----------\n\t        String\n\t            a formatted string with tags containing the tool_id, description and params\n\t        \"\"\"\n\t        tool = self.tools[tool_id]\n\t        params = (\n\t            \"{\"\n\t            + \", \".join(\n\t                ['\"' + l + '\": ' + v for l, v in tool[\"dynamic_params\"].items()]\n\t            )\n", "            + \"}\"\n\t            if tool[\"dynamic_params\"] != \"\"\n\t            else \"{}\"\n\t        )\n\t        return f\"\"\"<TOOL>\n\t    <{TOOL_ID}>{str(tool_id)}</{TOOL_ID}>\n\t    <{DESCRIPTION}>{tool['description']}</{DESCRIPTION}>\n\t    <{PARAMS}>{params}</{PARAMS}>\n\t    </TOOL>\"\"\"\n\t    def set_tools(self, tools):\n", "        \"\"\"\n\t        Adds all the available tools when the class is initialized.\n\t        Parameters\n\t        ----------\n\t        tools\n\t            a list of available tools. Each tool is defined within a Dict.\n\t        Returns\n\t        ----------\n\t        None\n\t        \"\"\"\n", "        for tool in tools:\n\t            if not \"args\" in tool:\n\t                tool[\"args\"] = {}\n\t            if not \"method\" in tool:\n\t                tool[\"method\"] = \"GET\"\n\t            if not \"examples\" in tool:\n\t                tool[\"examples\"] = []\n\t            if not \"dynamic_params\" in tool:\n\t                tool[\"dynamic_params\"] = {}\n\t            self.tools += [tool]\n", "    def use_tool(self, tool, gpt_suggested_input, question, memory, facts, query=\"\"):\n\t        \"\"\"\n\t        Calls the appropriate API based on the tool that's in use.\n\t        Parameters\n\t        ----------\n\t        tool\n\t            an integer refencing the selected tool\n\t        gpt_suggested_input\n\t            the input params for the selected tool as specified by the gpt response\n\t        question\n", "            the user's input question\n\t        memory\n\t            a list of tuples containing the conversation history\n\t        facts\n\t            a list containing factual answers generated for each sub-question\n\t        query\n\t            the value of the ai_response_prompt key for the selected tool. If ai_response_prompt key is not available this will have same value as question parameter\n\t        Returns\n\t        ----------\n\t        String\n", "            Response text after calling API tool and passing result into ChatGPT prompt\n\t        \"\"\"\n\t        return tool_api_call(\n\t            self, tool, gpt_suggested_input, question, memory, facts, query=query\n\t        )\n\t    def run(self, question, memory):\n\t        \"\"\"\n\t        Runs the Agent on the given inputs\n\t        Parameters\n\t        ----------\n", "        question\n\t            the user's input question\n\t        memory\n\t            a list of tuples containing the conversation history\n\t        Returns\n\t        ----------\n\t        Tuple\n\t            a tuple containing the Agent's answer and a list of the conversation history\n\t        \"\"\"\n\t        self.price = 0\n", "        thought, facts = self.promptf(\n\t            question,\n\t            memory,\n\t            [],\n\t            0\n\t        )\n\t        if self.verbose > -1:\n\t            print_big(\"GPT-3.5 Price = ~{:.1f} cents\".format(self.price * 100))\n\t        # return (answer, memory + [(question, answer)], calls, debug_return, has_friendly_tags)\n\t        return (thought, memory + [(question, thought)])\n", "    def makeInteraction(self, p, a, Q=\"HUMAN\", A=\"AI\", INTERACTION=INTERACTION):\n\t        \"\"\"\n\t        Formats the tool description to contain the relevant tags according to the dynamic params\n\t        Parameters\n\t        ----------\n\t        p\n\t            the question being asked\n\t        a\n\t            the gpt response\n\t        Q\n", "            the entity asking the question. Could be HUMAN or AI.\n\t        A\n\t            the entity answering the question. Could be HUMAN or AI.\n\t        INTERACTION\n\t            the type of interaction. Could be HUMAN-AI or AI-AI.\n\t        Returns\n\t        ----------\n\t        String\n\t            a formatted string with tags containing the type of interaction, the question and the gpt response\n\t        \"\"\"\n", "        return f\"<>{INTERACTION}:<{Q}>{p}</{Q}>\\n<{A}>\" + (\n\t            f\"{a}</{A}>\\n</>\" if a is not None else \"\"\n\t        )\n\t    def make_sub(\n\t        self,\n\t        tools,\n\t        memory,\n\t        facts,\n\t        question,\n\t        subq,\n", "        answer_label,\n\t        toolEx,\n\t        tool_to_use=None,\n\t        bot_str=\"\",\n\t        quality=\"best\",\n\t        max_tokens=20,\n\t    ):\n\t        \"\"\"\n\t        Formats the tool description to contain the relevant tags according to the dynamic params\n\t        Parameters\n", "        ----------\n\t        tools\n\t            a list of available tools. Each tool is defined within a Dict.\n\t        memory\n\t            a list of tuples containing the conversation history\n\t        facts\n\t            a list containing factual answers generated for each sub-question\n\t        question\n\t            the user's input question\n\t        subq\n", "            a lambda function that returns a question that's used to prompt gpt for the suggested input for the selected tool\n\t        answer_label\n\t            the format of the response\n\t        toolEx\n\t            a lambda function that returns an example for the selected tool\n\t        tool_to_use\n\t            an integer referencing the selected tool\n\t        bot_str\n\t            a string to be appended to the gpt prompt\n\t        quality\n", "            could be either \"okay\"-text-curie-001 or \"best\"-text-davinci-003\n\t        max_tokens\n\t            maximum number of tokens to be generated\n\t        Returns\n\t        ----------\n\t        String\n\t            a gpt text response containing the suggested input for the selected tool\n\t        \"\"\"\n\t        tool_context = \"\".join([self.makeToolDesc(t) for t, _ in tools])\n\t        mem = \"\".join(\n", "            [\n\t                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n\t                for p, a in memory\n\t            ]\n\t        ) + \"\".join(\n\t            [\n\t                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n\t                for p, a in facts\n\t            ]\n\t        )\n", "        def makeQuestion(memory, question, tool=None):\n\t            return (\n\t                f\"\\n\\n<{EXAMPLE}>\\n\"\n\t                + f\"<{QUESTION}>{question}</{QUESTION}>\\n\"\n\t                + f\"<{THOUGHT}><{PROMPT}>{subq(tool)}</{PROMPT}>\\n<{RESPONSE} ty={answer_label}>\\n\"\n\t            )\n\t        examples = []\n\t        for tool_id, tool in tools:\n\t            for tool_example in tool[\"examples\"]:\n\t                examples += [\n", "                    makeQuestion(tool_example[0], tool_example[1], tool_id)\n\t                    + toolEx(tool_id, tool_example)\n\t                    + f\"</{RESPONSE}></{THOUGHT}></{EXAMPLE}>\"\n\t                ]\n\t        random_fixed_seed.shuffle(examples)\n\t        cur_question = f\"<{CONVERSATION}>{mem}</{CONVERSATION}>\" + makeQuestion(\n\t            memory, question, tool=tool_to_use\n\t        )\n\t        prompt = MSG(\"system\", \"You are a good and helpful assistant.\")\n\t        prompt += MSG(\n", "            \"user\",\n\t            \"<TOOLS>\" + tool_context + \"</TOOLS>\" + \"\".join(examples) + cur_question,\n\t        )\n\t        return call_ChatGPT(\n\t            self, prompt, stop=f\"</{RESPONSE}>\", max_tokens=max_tokens\n\t        ).strip()\n\t    def promptf(self, question, memory, facts, level, split_allowed=True, max_level=3):\n\t        \"\"\"\n\t        Formats the gpt prompt to include conversation history, facts and logs responses to the console\n\t        Parameters\n", "        ----------\n\t        question\n\t            the user's input question\n\t        memory\n\t            a list of tuples containing the conversation history\n\t        facts\n\t            a list containing factual answers generated for each sub-question\n\t        level\n\t            param indicating the current recursive level\n\t        split_allowed\n", "            a boolean to allow the question to be split into sub-questions if set to True\n\t        max_level\n\t            param that indicates the maximum recursive level we want to allow.\n\t        Returns\n\t        ----------\n\t        Tuple\n\t            a tuple containing the gpt response and a list with the conversation history\n\t        \"\"\"\n\t        mem = \"\".join(\n\t            [\n", "                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n\t                for p, a in memory\n\t            ]\n\t        ) + \"\".join(\n\t            [\n\t                self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n\t                for p, a in facts\n\t            ]\n\t        )\n\t        if split_allowed:\n", "            subq = question_split(question, self.tools, mem)\n\t            subq_final = []\n\t            if len(subq[1]) == 1:\n\t                split_allowed = False\n\t            else:\n\t                for i in subq[1]:\n\t                    if cos_similarity(self.nlp(question).vector, self.nlp(i).vector) < 0.98:\n\t                        subq_final.append(i)\n\t                    else:\n\t                        split_allowed = False\n", "            if level==max_level:\n\t                split_allowed = False\n\t            self.price += subq[0]\n\t            new_facts = []\n\t            for i in range(len(subq_final)):\n\t                _, new_facts = self.promptf(\n\t                    subq_final[i],\n\t                    memory,\n\t                    facts,\n\t                    level+1,\n", "                    split_allowed=split_allowed,\n\t                )\n\t                facts = facts + new_facts\n\t                mem = \"\".join(\n\t                    [\n\t                        self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n\t                        for p, a in memory\n\t                    ]\n\t                ) + \"\".join(\n\t                    [\n", "                        self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\")\n\t                        for p, a in facts\n\t                    ]\n\t                )\n\t        answer_in_memory = memory_check(mem, question)\n\t        self.price += answer_in_memory[0]\n\t        answer_in_memory = answer_in_memory[1]\n\t        if answer_in_memory:\n\t            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n\t            prompt += MSG(\n", "                \"user\",\n\t                mem + \"\\nQ:\" + question + \"\\nANSWER Q, DO NOT MAKE UP INFORMATION.\",\n\t            )\n\t            a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\t            print_big(\"NO DATA TOOLS USED, ANSWERED FROM MEMORY\")\n\t            return (a.replace(\"\\n\", \"\"), [(question, a)])\n\t        tool_to_use = tool_picker(self.tools, question, 3)\n\t        print_big(\n\t            \"\".join(\n\t                [\n", "                    f'{\"✓\" if self.tools.index(tool) == tool_to_use else \" \"} {self.tools.index(tool)}.- {tool[\"description\"]}\\n'\n\t                    for tool in self.tools[3:0]\n\t                ]\n\t            )\n\t            + f\"\\n> Question: {question}\\n> Raw answer: '{tool_to_use}'\\n> Tool ID: {tool_to_use}\",\n\t            \"LIST OF DATA TOOLS\",\n\t        )\n\t        self.price += tool_to_use[0]\n\t        try:\n\t            tool_to_use = int(tool_to_use[1])\n", "        except:\n\t            tool_to_use = len(self.tools)\n\t        if tool_to_use >= len(self.tools):\n\t            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n\t            prompt += MSG(\n\t                \"user\",\n\t                mem\n\t                + \"\\nQ:\"\n\t                + question\n\t                + \"\\nUsing this information, what is the answer to Q?\",\n", "            )\n\t            a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\t            return (a.replace(\"\\n\", \"\"), [(question, a)])\n\t        tool_input = self.make_sub(\n\t            list(enumerate(self.tools)),\n\t            memory,\n\t            facts,\n\t            question,\n\t            lambda t: \"What should the input for tool \" + str(t) + \" be to answer Q?\",\n\t            \"JSON\",\n", "            lambda t, ex: ex[2],\n\t            tool_to_use=tool_to_use,\n\t            quality=\"best\" if self.price < QUALITY else \"okay\",\n\t            max_tokens=200,\n\t        )\n\t        query = question\n\t        if \"ai_response_prompt\" in self.tools[tool_to_use].keys():\n\t            query = self.tools[tool_to_use][\"ai_response_prompt\"]\n\t        try:\n\t            answer = self.use_tool(\n", "                self.tools[tool_to_use], tool_input, question, memory, facts, query=query\n\t            )\n\t        except:\n\t            prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n\t            prompt += MSG(\n\t                \"user\",\n\t                mem + \"\\nQ:\" + question + \"\\nANSWER Q, DO NOT MAKE UP INFORMATION.\",\n\t            )\n\t            answer = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\t        return (answer, [(question, answer)])\n", "def rebel_main():\n\t    tools = buildExampleTools()\n\t    label = Agent(os.getenv(\"LLM_VM_OPENAI_API_KEY\"), tools, verbose=1)\n\t    conversation_history = []\n\t    last = \"\"\n\t    while True:\n\t        inp = input(last + \"Human: \")\n\t        return_value = label.run(inp, conversation_history)\n\t        conversation_history = return_value[1]\n\t        last = \"AI: \" + str(return_value[0]) + \"\\n\"\n", "if __name__ == \"__main__\":\n\t    rebel_main()\n"]}
{"filename": "src/llm_vm/agents/REBEL/test_agent.py", "chunked_list": ["import json\n\timport openai\n\tfrom agent import Agent\n\timport time\n\timport os, sys\n\tgoogle_tool = {\n\t               'description': \"Returns the result of a google search result about information on the internet. Good for retrieving information about the world and live information.\",\n\t               'dynamic_params': {\"q\": 'The natural language input query'},\n\t               'method': 'GET',\n\t               'args': {'url': \"https://www.googleapis.com/customsearch/v1\",\n", "                         'params': {'key': \"Enter Key Here\",\n\t                                    'cx' : 'Enter CX Here',\n\t                                    'q': '{q}'}\n\t                        }\n\t               }\n\tclass suppress_output:\n\t    def __init__(self, suppress_stdout=False, suppress_stderr=False):\n\t        self.suppress_stdout = suppress_stdout\n\t        self.suppress_stderr = suppress_stderr\n\t        self._stdout = None\n", "        self._stderr = None\n\t    def __enter__(self):\n\t        devnull = open(os.devnull, \"w\")\n\t        if self.suppress_stdout:\n\t            self._stdout = sys.stdout\n\t            sys.stdout = devnull\n\t        if self.suppress_stderr:\n\t            self._stderr = sys.stderr\n\t            sys.stderr = devnull\n\t    def __exit__(self, *args):\n", "        if self.suppress_stdout:\n\t            sys.stdout = self._stdout\n\t        if self.suppress_stderr:\n\t            sys.stderr = self._stderr\n\topenai.api_key = \"\"\n\tagent = Agent(openai.api_key,[], verbose=-1)\n\tagent.set_tools([google_tool])\n\tf = open(\"compositional_celebrities.json\")\n\tdata = json.load(f)\n\tcategory = []\n", "q_and_a={}\n\tfor i in data[\"data\"]:\n\t    if i[\"category\"] in ['birthplace_rounded_lat', 'birthplace_rounded_lng', 'birthplace_tld', 'birthplace_ccn3', 'birthplace_currency', 'birthplace_currency_short', 'birthplace_currency_symbol', 'birthplace_jpn_common_name', 'birthplace_spa_common_name', 'birthplace_rus_common_name', 'birthplace_est_common_name', 'birthplace_urd_common_name', 'birthplace_callingcode', 'birthyear_nobelLiterature', 'birthdate_uspresident', 'birthyear_masterchamp']:\n\t        if i[\"category\"] not in category:\n\t            category.append(i[\"category\"])\n\t            q_and_a.update({i[\"category\"]:[(i[\"Question\"],i[\"Answer\"])]})\n\t        else:\n\t            q_and_a[i[\"category\"]].append((i[\"Question\"],i[\"Answer\"]))\n\top = ['birthplace_rounded_lat', 'birthplace_rounded_lng', 'birthplace_tld', 'birthplace_ccn3', 'birthplace_currency', 'birthplace_currency_short', 'birthplace_currency_symbol', 'birthplace_jpn_common_name', 'birthplace_spa_common_name', 'birthplace_rus_common_name', 'birthplace_est_common_name', 'birthplace_urd_common_name', 'birthplace_callingcode', 'birthyear_nobelLiterature', 'birthdate_uspresident', 'birthyear_masterchamp']\n\tfor j in range(0,len(op)):\n", "    print(j,op[j])\n\tidx=int(input())\n\tchoice=op[idx]\n\tfor i in range(100):\n\t    with suppress_output(suppress_stdout=True, suppress_stderr=True):\n\t        try:\n\t            a=agent.run(q_and_a[choice][i][0],[])\n\t        except Exception as e:\n\t            a = [e,\"\"]\n\t    print(str(i)+\"|\",str(a[0])+\"|\",q_and_a[choice][i][1])\n"]}
{"filename": "src/llm_vm/agents/REBEL/utils.py", "chunked_list": ["import requests\n\timport traceback\n\timport os\n\timport random\n\timport concurrent.futures\n\timport openai\n\timport urllib.request\n\timport json\n\timport string\n\timport re\n", "import sys\n\tdef flatten(a):\n\t    return sum(a, [])\n\tdef print_op(*kargs, **kwargs):\n\t    print(*kargs, **kwargs, flush=True)\n\tdef prepPrintPromptContext(p):\n\t    return \"--> \" + p.replace(\"\\n\", \"\\n--> \") if len(p) > 0 else \"\"\n\tdef MSG(u, c):\n\t    return [{\"role\": u, \"content\": c}]\n\tdef call_ChatGPT(state, cur_prompt, stop=None, max_tokens=20, temperature=0.2):\n", "    if state.verbose > 1:\n\t        print_op(\"\\nGPT input for {\" + str(stop) + \"} \" + str(len(cur_prompt)) + \".\")\n\t    if state.verbose > 2:\n\t        print_op(str(cur_prompt))\n\t    ppt = 0.002\n\t    def calcCost(p):\n\t        chars = sum((len(a[\"content\"]) for a in p))\n\t        if state.verbose > 0:\n\t            print_op(\"ASK_CHARS:\", chars)\n\t        c = (chars / 2700.0) * ppt\n", "        if state.verbose > 2:\n\t            print_op(\"PrePrice: $\" + str(c))\n\t        return c\n\t    cost = calcCost(cur_prompt)\n\t    try:\n\t        ans = openai.ChatCompletion.create(\n\t            model=\"gpt-3.5-turbo-0301\",\n\t            max_tokens=max_tokens,\n\t            stop=stop,\n\t            messages=cur_prompt,\n", "            temperature=temperature,\n\t        )\n\t    except Exception as e:\n\t        state.price += cost\n\t        traceback.print_exc()\n\t        print_op(\"Error:\", e)\n\t        return \"OpenAI is down!\"\n\t    price = ppt * ans[\"usage\"][\"total_tokens\"] / 1000\n\t    if state.verbose > 0:\n\t        print_op(\"Price: $\" + str(price))\n", "    state.price += price\n\t    response_text = ans[\"choices\"][0][\"message\"][\"content\"]\n\t    if state.verbose > 2:\n\t        print_op(\"GPT output:\")\n\t        print_op(prepPrintPromptContext(response_text))\n\t        print_op(\"GPT output fin.\\n\")\n\t    return response_text\n\tdef call_gpt(\n\t    state, cur_prompt: str, stop: str, max_tokens=20, quality=\"best\", temperature=0.0\n\t):\n", "    if state.verbose > 1:\n\t        print_op(\"\\nGPT input for {\" + stop + \"} \" + str(len(cur_prompt)) + \".\")\n\t    if state.verbose > 2:\n\t        print_op(prepPrintPromptContext(cur_prompt))\n\t    ask_tokens = max_tokens + len(cur_prompt) / 2.7\n\t    if state.verbose > 0:\n\t        print_op(\"ASK_TOKENS:\", ask_tokens)\n\t    if (ask_tokens) > 2049:\n\t        quality = \"best\"\n\t    model = {\n", "        \"best\": (\"text-davinci-003\", 0.02),\n\t        \"okay\": (\"text-curie-001\", 0.002),\n\t    }[quality]\n\t    def calcCost(p):\n\t        return (len(p) / 2700.0) * model[1]\n\t    cost = calcCost(cur_prompt)\n\t    try:\n\t        ans = openai.Completion.create(\n\t            model=model[0],\n\t            max_tokens=max_tokens,\n", "            stop=stop,\n\t            prompt=cur_prompt,\n\t            temperature=temperature,\n\t        )\n\t    except Exception as e:\n\t        print_op(\"WTF:\", e)\n\t        state.price += cost\n\t        return \"OpenAI is down!\"\n\t    response_text = ans[\"choices\"][0][\"text\"]\n\t    simpleprice = model[1] * ans[\"usage\"][\"total_tokens\"] / 1000\n", "    if state.verbose > 0:\n\t        print_op(\"SimplePrice: $\" + str(simpleprice))\n\t    state.price += simpleprice\n\t    if state.verbose > 2:\n\t        print_op(\"GPT output:\")\n\t        print_op(prepPrintPromptContext(response_text))\n\t        print_op(\"GPT output fin.\\n\")\n\t    return response_text\n\tdef deep_fmap(lambdaFunc, json_data):\n\t    print_op(json_data)\n", "    if isinstance(json_data, list):\n\t        print_op(\"##LIST\")\n\t        return list(map(lambda listItem: deep_fmap(lambdaFunc, listItem), json_data))\n\t    elif isinstance(json_data, tuple):\n\t        print_op(\"##TUPLE\")\n\t        return tuple(map(lambda tupleItem: deep_fmap(lambdaFunc, tupleItem), json_data))\n\t    elif isinstance(json_data, dict):\n\t        print_op(\"##DICT\")\n\t        return {lambdaFunc(k): deep_fmap(lambdaFunc, v) for k, v in json_data.items()}\n\t    else:\n", "        print_op(\"##SIMPLE\")\n\t        return lambdaFunc(json_data)\n\tdef replace_variables_for_values(\n\t    my_dict: dict, dynamic_keys, ignore_key: str = \"_______\"\n\t):\n\t    replaced_dict = {}\n\t    for key, value in my_dict.items():\n\t        if key == ignore_key:\n\t            continue\n\t        formatted_key = key.format(**dynamic_keys)\n", "        if isinstance(value, dict):\n\t            formatted_value = replace_variables_for_values(value, dynamic_keys)\n\t        elif isinstance(value, list):\n\t            formatted_value = []\n\t            for item in value:\n\t                formatted_value += replace_variables_for_values(item, dynamic_keys)\n\t        else:\n\t            try:\n\t                formatted_value = value.format(**dynamic_keys)\n\t            except:\n", "                formatted_value = value\n\t        replaced_dict[formatted_key] = formatted_value\n\t    return replaced_dict\n\tdef tool_api_call(self, tool, gpt_suggested_input, question, memory, facts, query=\"\"):\n\t    if query == \"\":\n\t        query = question\n\t    if gpt_suggested_input[0] != \"{\":\n\t        gpt_suggested_input = \"{\" + gpt_suggested_input\n\t    if gpt_suggested_input[-1] != \"}\":\n\t        gpt_suggested_input += \"}\"\n", "    if self.verbose > 1:\n\t        print_op(\"GPT SUGGESTED INPUT:\", gpt_suggested_input)\n\t    parsed_gpt_suggested_input = json.loads(gpt_suggested_input)\n\t    # make sure all of the suggested fields exist in the tool desc\n\t    for i in parsed_gpt_suggested_input.keys():\n\t        if i not in tool[\"dynamic_params\"].keys():\n\t            raise Exception(\"Bad Generated Input\")\n\t    tool_args = replace_variables_for_values(tool[\"args\"], parsed_gpt_suggested_input)\n\t    url = tool_args[\"url\"]\n\t    if self.verbose > -1:\n", "        print_op(tool[\"method\"] + \":\", url)\n\t    if \"auth\" in tool_args and isinstance(tool_args[\"auth\"], dict):\n\t        auths = list(tool_args[\"auth\"].items())\n\t        if len(auths) > 0:\n\t            tool_args[\"auth\"] = list(tool_args[\"auth\"].items())[0]\n\t        else:\n\t            del tool_args[\"auth\"]\n\t    # Remove those parameters that are not part of Python's `requests` function.\n\t    tool_args.pop(\"jsonParams\", None)\n\t    tool_args.pop(\"urlParams\", None)\n", "    if self.verbose > -1:\n\t        print_op(\"ARGS: \", tool_args)\n\t    resp = (requests.get if tool[\"method\"] == \"GET\" else requests.post)(**tool_args)\n\t    # print_op(\"FINAL URL: (\" + tool[\"method\"] + \") \", resp.url)\n\t    actual_call = str(tool_args)\n\t    if resp.status_code in [404, 401, 500]:\n\t        er = \" => \" + str(resp.status_code)\n\t        return \"This tool isn't working currently\" + er\n\t    ret = str(resp.text)\n\t    if self.verbose > 4:\n", "        print(ret)\n\t    try:\n\t        ret = str(json.loads(ret))\n\t    except:\n\t        pass\n\t    if len(ret) > 10000:\n\t        ret = ret[0:10000]\n\t    mem = \"\".join(\n\t        [\n\t            self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"Human-AI\")\n", "            for p, a in memory\n\t        ]\n\t    ) + \"\".join(\n\t        [self.makeInteraction(p, a, \"P\", \"AI\", INTERACTION=\"AI-AI\") for p, a in facts]\n\t    )\n\t    prompt = MSG(\"system\", \"You are a good and helpful bot\" + self.bot_str)\n\t    prompt += MSG(\n\t        \"user\",\n\t        mem\n\t        + \"\\nQ:\"\n", "        + query\n\t        + \"\\n An api call about Q returned:\\n\"\n\t        + ret\n\t        + \"\\nUsing this information, what is the answer to Q?\",\n\t    )\n\t    a = call_ChatGPT(self, prompt, stop=\"</AI>\", max_tokens=256).strip()\n\t    return a\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent.py", "chunked_list": ["import os\n\timport sys\n\timport re\n\timport random\n\tfrom llm_vm.agents.FLAT.agent_helper.business_logic import promptf\n\tfrom llm_vm.utils.labels import *\n\tfrom llm_vm.agents.FLAT.agent_helper.utils import *\n\tfrom llm_vm.agents.FLAT.agent_helper.tools import *\n\tfrom llm_vm.utils.keys import *\n\trandom_fixed_seed = random.Random(4)\n", "class Agent:\n\t    def __init__(self, openai_key, tools = None, bot_instructions = \"\", verbose = 4):\n\t        self.verbose = verbose\n\t        self.set_tools((GENERIC_TOOLS + tools) if tools else GENERIC_TOOLS)\n\t        self.bot_instructions = f\"<{L_BOT_INSTRUCTIONS}>{bot_instructions}<{L_BOT_INSTRUCTIONS}>\" if bot_instructions else \"\"\n\t        # set the openai key to make calls to the API\n\t        set_api_key(openai_key)\n\t    def set_tools(self, tools):\n\t        self.tools = []\n\t        for tool in tools:\n", "            if not 'args' in tool:\n\t                tool['args'] = {}\n\t            if not 'method' in tool:\n\t                tool['method'] = \"GET\"\n\t            if not 'examples' in tool:\n\t                tool['examples'] = []\n\t            if not 'dynamic_params' in tool:\n\t                tool['dynamic_params'] = {}\n\t            if not 'id' in tool:\n\t                tool['id'] = len(self.tools) + 1\n", "            self.tools += [tool]\n\t    def run(self, question: str, memory: TupleList) -> Tuple[str, TupleList, list, DebugCallList]:\n\t        try:\n\t            answer, _, calls, debug_return, price = promptf(\n\t                question,\n\t                memory,\n\t                self.tools,\n\t                self.bot_instructions,\n\t                self.verbose\n\t            )\n", "            # Remove xml tags from answer, if any. Important, because this is trade secret.\n\t            answer, has_friendly_tags = remove_tags_from_html_string(answer)\n\t        except Exception as e:\n\t            print(e, flush=True)\n\t            print(\"Main thread exception: \", e, flush=True)\n\t            answer, calls, debug_return, price, has_friendly_tags = \"Error: \" + str(e), [], [], 0, False\n\t        if self.verbose > -1:\n\t            print_big(\"GPT-3.5 Price = ~{:.1f} cents\".format(price * 100))\n\t        return (answer, memory + [(question, answer)], calls, debug_return, has_friendly_tags)\n\tdef flat_main():\n", "    # tools = [{'method': 'GET',\"description\":\"use this tool to find the price of stocks\",'args' : {\"url\":\"https://finnhub.io/api/v1/quote\",'params': { 'token' :'cfi1v29r01qq9nt1nu4gcfi1v29r01qq9nt1nu50'} },\"dynamic_params\":{\"symbol\":\"the symbol of the stock\"}}]\n\t    tools =  [{'method': 'GET', \"dynamic_params\": { 'location': 'This string indicates the geographic area to be used when searching for businesses. \\\n\t    Examples: \"New York City\", \"NYC\", \"350 5th Ave, New York, NY 10118\".', 'term': 'Search term, e.g. \"food\" or \"restaurants\". The \\\n\t    term may also be the business\\'s name, such as \"Starbucks\"', 'price': 'Pricing levels to filter the search result with: 1 = \\\n\t    $, 2 = $$, 3 = $$$, 4 = $$$$. The price filter can be a list of comma delimited pricing levels. e.g., \"1, 2, 3\" will filter the \\\n\t    results to show the ones that are $, $$, or $$$.'}, \"description\":\"This tool searches for a business on yelp.  It's useful for finding restaurants and \\\n\t    whatnot.\", 'args' :{'url': 'https://api.yelp.com/v3/businesses/search', 'cert': '', 'json': {}, 'params': {'limit': '1',\n\t                                                                                                              'open_now': 'true', 'location': '{location}', 'term': '{term}', 'price': '{price}'}, 'data': {},\n\t                       'headers': {'authorization': 'Bearer OaEqVSw9OV6llVnvh9IJo92ZCnseQ9tftnUUVwjYXTNzPxDjxRafYkz99oJKI9WHEwUYkiwULXjoBcLJm7JhHj479Xqv6C0lKVXS7N91ni-nRWpGomaPkZ6Z1T0GZHYx',\n\t                                   'accept': 'application/json'}}}]\n", "    label = Agent(os.getenv(\"LLM_VM_OPENAI_API_KEY\"), tools, verbose=4)\n\t    conversation_history = []\n\t    last = \"\"\n\t    while True:\n\t        inp = input(last+\"Human: \")\n\t        return_value = label.run(inp, conversation_history)\n\t        conversation_history = return_value[1]\n\t        print(return_value[2])\n\t        last = \"AI: \"+str(return_value[0]) + \"\\n\"\n\t# print_op(google(' {\"question\": \"\"}'))\n", "if __name__ == \"__main__\":\n\t    flat_main()\n"]}
{"filename": "src/llm_vm/agents/FLAT/typings.py", "chunked_list": ["from typing import Tuple, List, Dict, TypedDict, Union, Any, Optional\n\tfrom enum import Enum\n\tclass OpenAIModel(Enum):\n\t    CURIE = \"curie\"\n\t    CURIE_TEXT = \"text-curie-001\"\n\t    FAST_DAVINCI = \"code-cushman-002\"\n\t    DAVINCI = \"davinci\"\n\t    DAVINCI_TEXT = \"text-davinci-003\"\n\tclass LLMCallType(Enum):\n\t    OPENAI_COMPLETION = \"openai-completion\"\n", "    OPENAI_CHAT = \"openai-chat\"\n\t    # ... other models\n\tclass DecisionStep(Enum):\n\t    SPLIT = \"split_questions\"\n\t    INPUT = \"guess_api_input\"\n\t    FROM_MEMORY = \"use_memory\"\n\t    TOOL_PICKER = \"tool_picker\"\n\tclass DefaultTools(Enum):\n\t    I_DONT_KNOW = -1\n\t    ANSWER_FROM_MEMORY = 0\n", "    WOLFRAM = 1\n\t    DIRECTIONS = 2\n\t    WEATHER = 3\n\t    ## Always set this to the latest ID. This is an integer, not an enum!\n\t    __LAST_ID__: int = 4\n\tclass DefaultTrainingTools(Enum):\n\t    TRAIN_SEND_EMAIL = 4\n\t    TRAIN_CREATE_DOCUMENT = 5\n\t    TRAIN_SHARE_DOCUMENT = 6\n\t    TRAIN_BOOK_TABLE = 7\n", "    TRAIN_FIND_RESTAURANT = 8\n\t    TRAIN_NEWS = 9\n\t    TRAIN_ORDER_FOOD = 10\n\t    TRAIN_BOOK_APPOINTMENT = 11\n\t    TRAIN_AVAILABLE_APPOINTMENT = 12\n\t    TRAIN_FIND_FLIGHT = 13\n\tTupleList = List[Tuple[str, str]]\n\tclass ToolTypeArgs(TypedDict):\n\t    url: str\n\t    params: Optional[dict]\n", "    json: Optional[dict]\n\t    jsonParams: Optional[dict]\n\t    auth: Optional[dict]\n\t    cert: Optional[str]\n\t    data: Optional[dict]\n\tclass SingleTool(TypedDict):\n\t    description: str\n\t    dynamic_params: dict\n\t    id: int\n\t    args: ToolTypeArgs\n", "    ai_response_prompt: Union[str, None]\n\tToolList = List[SingleTool]\n\t# ToolList = NewType(\"ToolList\", List[SingleTool])\n\tEnumeratedToolList = List[Tuple[int,SingleTool]]\n\tALL_TOOLS_MAP: Dict[str, Union[DefaultTools, DefaultTrainingTools]] = {\n\t    \"dont_know\": DefaultTools.I_DONT_KNOW,\n\t    \"memory\": DefaultTools.ANSWER_FROM_MEMORY,\n\t    \"weather\": DefaultTools.WEATHER,\n\t    \"wolfram\": DefaultTools.WOLFRAM,\n\t    \"directions\": DefaultTools.DIRECTIONS,\n", "    \"send_email\": DefaultTrainingTools.TRAIN_SEND_EMAIL,\n\t    \"create_doc\": DefaultTrainingTools.TRAIN_CREATE_DOCUMENT,\n\t    \"share_doc\": DefaultTrainingTools.TRAIN_SHARE_DOCUMENT,\n\t    \"book_table\": DefaultTrainingTools.TRAIN_BOOK_TABLE,\n\t    \"find_restaurant\": DefaultTrainingTools.TRAIN_FIND_RESTAURANT,\n\t    \"news\": DefaultTrainingTools.TRAIN_NEWS,\n\t    \"order_food\": DefaultTrainingTools.TRAIN_ORDER_FOOD,\n\t    \"book_appointment\": DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT,\n\t    \"check_appointment\": DefaultTrainingTools.TRAIN_AVAILABLE_APPOINTMENT,\n\t    \"find_flight\": DefaultTrainingTools.TRAIN_FIND_FLIGHT\n", "}\n\tclass QuestionSplitModelJSONData(TypedDict):\n\t    mem: List[List[str]]\n\t    question: str\n\t    answer: List[str]\n\tclass QuestionSplitModelData(TypedDict):\n\t    mem: Union[TupleList, None]\n\t    question: str\n\t    answer: Union[List[str], None]\n\t    tools: Union[ToolList, None]\n", "class AnswerFromMemoryModelData(TypedDict):\n\t    mem: Union[TupleList, None]\n\t    question: str\n\t    thought: Union[str, None]\n\t    answer: Union[str, None]\n\tclass QuestionSplitInputModel(TypedDict):\n\t    model_name: str\n\t    openai_model: OpenAIModel\n\t    data: List[QuestionSplitModelData]\n\tclass PromptModelEntry(TypedDict):\n", "    prompt: str\n\t    completion: str\n\tclass LLMCallParams(TypedDict):\n\t    llm: LLMCallType\n\t    model: Tuple[str, Union[str, None]]\n\t    prompt: str\n\t    temperature: float\n\t    max_tokens: int\n\t    stop: str\n\tLLMCallReturnType = Tuple[str, float]\n", "class LLMTrainingResult(TypedDict):\n\t    elapsed_time_s: int\n\t    model_name: str\n\t    model_files: List[str]\n\tclass LLMTrainingResultMeta(TypedDict):\n\t    OPENAI_KEY: str\n\t    DATE: str\n\t    TIMESTAMP: int\n\tLLModels = Union[Dict[str, LLMTrainingResult], LLMTrainingResultMeta]\n\tclass ToolInputModelJSONData(TypedDict):\n", "    mem: Optional[List[str]]\n\t    description: str\n\t    question: str\n\t    answer: dict\n\t    params: dict\n\tclass ToolInputModelData(TypedDict):\n\t    mem: Optional[TupleList]\n\t    description: str\n\t    question: str\n\t    answer: dict\n", "    params: dict\n\tclass ToolInputModel(TypedDict):\n\t    openai_model: OpenAIModel\n\t    data: List[ToolInputModelData]\n\tclass ToolpickerInputModelJSONData(TypedDict):\n\t    mem: Optional[List[str]]\n\t    thought: Optional[str]\n\t    question: str\n\t    answer: str\n\tclass ToolpickerInputModelData(TypedDict):\n", "    mem: Optional[TupleList]\n\t    thought: Optional[str]\n\t    question: str\n\t    answer: int\n\t    tools: Optional[ToolList]\n\tclass ToolpickerInputModel(TypedDict):\n\t    openai_model: OpenAIModel\n\t    data: List[ToolpickerInputModelData]\n\tclass AnswerInMemoryModelData(TypedDict):\n\t    mem: Optional[TupleList]\n", "    facts: Optional[TupleList]\n\t    question: str\n\t    answer: Optional[bool]\n\tclass AnswerInMemoryModel(TypedDict):\n\t    openai_model: OpenAIModel\n\t    data: List[AnswerInMemoryModelData]\n\tclass DebugCalls(TypedDict):\n\t    response: str\n\t    gpt_suggested_params: Any\n\t    url: str\n", "    raw_api_return: str\n\tDebugCallList = List[DebugCalls]\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/bothandler.py", "chunked_list": ["import re\n\timport openai\n\timport random\n\tfrom llm_vm.utils.typings_llm import *\n\tfrom llm_vm.agents.FLAT.agent_helper.requests.call_llm import call_llm\n\tfrom llm_vm.agents.FLAT.agent_helper.tools import *\n\tfrom llm_vm.agents.FLAT.agent_helper.tool_utils import *\n\tfrom llm_vm.agents.FLAT.agent_helper.utils import *\n\tfrom llm_vm.agents.FLAT.models.get_decision_model import get_newest_decision_model\n\tfrom llm_vm.agents.FLAT.models.utils.tool_picker_model.tool_picker_model_data import tool_input_data\n", "from llm_vm.agents.FLAT.models.utils.question_split_model.question_split_model_data import question_splitter_data\n\t# openai.api_key = OPENAI_DEFAULT_KEY\n\tdef question_split(question_to_split, use_fine_tuned_model = True):\n\t    question_data = question_splitter_data[\"data\"].copy()\n\t    random.shuffle(question_data)\n\t    NUMBER_OF_EXAMPLES = 6\n\t    question_data = question_data[:NUMBER_OF_EXAMPLES]\n\t    use_fine_tuned_model = False\n\t    prompt, prompt_stop = splitter_prompt(\n\t        question_data +\n", "        [question_to_split]\n\t    )\n\t    if use_fine_tuned_model:\n\t            custom_model, custom_model_key = get_newest_decision_model(DecisionStep.SPLIT)\n\t    else:\n\t        custom_model, custom_model_key = OpenAIModel.DAVINCI_TEXT.value, None\n\t    sub_questions_str, price = call_llm({\n\t        \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t        \"model\": (custom_model, custom_model_key),\n\t        \"max_tokens\": 256,\n", "        \"stop\": prompt_stop,\n\t        \"prompt\": prompt,\n\t        \"temperature\": 0.2,\n\t    })\n\t    sub_questions, main_question = tidy_up_subquestions(\n\t        sub_questions_str.replace(prompt_stop, \"\"),\n\t        question_to_split[\"question\"]\n\t    )\n\t    return sub_questions, main_question, price\n\tdef pick_tool(tools_list, question, conversation_history, use_fine_tuned_model = True, debug_prompt = False):\n", "    tool_data = tool_input_data[\"data\"].copy()\n\t    random.shuffle(tool_data)\n\t    NUMBER_OF_EXAMPLES = 6\n\t    tool_data = tool_data[:NUMBER_OF_EXAMPLES]\n\t    use_fine_tuned_model = False\n\t    prompt, stop = toolpicker_prompt(\n\t        tool_data + [{\"question\": question, \"mem\": conversation_history}],\n\t        tools_list,\n\t    )\n\t    if debug_prompt:\n", "        print_big(prompt, \"toolpicker prompt\")\n\t    if use_fine_tuned_model:\n\t        custom_model, custom_model_key = get_newest_decision_model(\n\t            DecisionStep.TOOL_PICKER)\n\t    else:\n\t        custom_model, custom_model_key = OpenAIModel.DAVINCI_TEXT.value, None\n\t    suggested_tool_str, price = call_llm({\n\t        \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t        \"model\": (custom_model, custom_model_key),\n\t        \"max_tokens\": 10,\n", "        \"stop\": stop,\n\t        \"prompt\": prompt,\n\t        \"temperature\": 0,\n\t    })\n\t    number_match_regex = \"-?[0-9]+\"\n\t    try:\n\t        valid_tool_ids = [t[\"id\"] for t in tools_list] + \\\n\t            [DefaultTools.ANSWER_FROM_MEMORY.value, DefaultTools.I_DONT_KNOW.value]\n\t        parsed_suggested_tool_str = re.findall(\n\t            number_match_regex, suggested_tool_str)\n", "        if len(parsed_suggested_tool_str) == 0:\n\t            raise Exception(\"No matching ID found in response\")\n\t        suggested_tool_id = int(parsed_suggested_tool_str[0])\n\t        if suggested_tool_id not in valid_tool_ids:\n\t            raise Exception(\n\t                f\"Invalid tool id: {suggested_tool_id} // {','.join([str(v) for v in valid_tool_ids])}\")\n\t        tools_list.sort(key=lambda t: t[\"id\"])\n\t        print_big(\n\t            \"\".join([f'{\"✓\" if tool[\"id\"] == suggested_tool_id else \" \"} {tool[\"id\"]}.- {tool[\"description\"]}\\n' for tool in tools_list]) +\n\t            f\"\\n> Question: {question}\\n> Raw answer: '{suggested_tool_str}'\\n> Tool ID: {suggested_tool_id}\",\n", "            \"LIST OF DATA TOOLS\"\n\t        )\n\t        return suggested_tool_id, price\n\t    except Exception as e:\n\t        print_big(f'Exception parsing tool_id: \"{suggested_tool_str}\"')\n\t        print(e, flush=True)\n\t        return DefaultTools.I_DONT_KNOW.value, price\n\tdef check_can_answer_from_memory(question: str, memory: TupleList = [], facts: TupleList = []) -> Tuple[bool, float]:\n\t    memory_prompt, memory_stop = create_memory_prompt(\n\t        # answer_from_memory_data[\"data\"] +\n", "        [{\"question\": question, \"mem\": memory, \"facts\": facts}]\n\t    )\n\t    ans, price = call_llm({\n\t        \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t        \"model\": get_newest_decision_model(DecisionStep.FROM_MEMORY),\n\t        \"max_tokens\": 10,\n\t        \"stop\": memory_stop,\n\t        \"prompt\": memory_prompt,\n\t        \"temperature\": 0.1,\n\t    })\n", "    if \"yes\" in ans.lower():\n\t        return True, price\n\t    else:\n\t        return False, price\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/replacer.py", "chunked_list": ["import re\n\tfrom llm_vm.utils.keys import DICT_KEY_REGEX_TO_FIND_PURE_INTERPOLATIONS\n\t###\n\t# Use this regex to find if a variable is a pure interpolation.\n\t# For @example, { \"name\": \"{my_name}\" } is pure, since there's nothing else besides the interpolation\n\t# For @example, { \"Ticker\": \"TickerID={ticker_id}|historical=1\" } is NOT pure, since there is extra text\n\tdef __is_pure_interpolation (key: str) -> bool:\n\t    return len(re.findall(DICT_KEY_REGEX_TO_FIND_PURE_INTERPOLATIONS, key)) == 1\n\t###\n\t# Use this to get the key of a pure interpolation ===> '{a}' => 'a', '{abc_def-345}' => 'abc_def-345'\n", "def __get_pure_key (key: str) -> str:\n\t    return key[1:-1]\n\tdef replace_variables_for_values(my_dict: dict, dynamic_keys: dict, ignore_key: str = \"_______\") -> any:\n\t    def format_simple_value(value: str) -> any:\n\t        try:\n\t            if __is_pure_interpolation(value):\n\t                formatted_value = dynamic_keys[__get_pure_key(value)]\n\t            else:\n\t                formatted_value = value.format(**dynamic_keys)\n\t        except Exception as e:\n", "            formatted_value = value\n\t        return formatted_value\n\t    replaced_dict = {}\n\t    for key, value in my_dict.items():\n\t        if (key == ignore_key):\n\t            continue\n\t        formatted_key = key.format(**dynamic_keys)\n\t        if (isinstance(value, dict)):\n\t            if (not isinstance(value, list) and not isinstance(value, dict)):\n\t                formatted_value = format_simple_value(value)\n", "            else:\n\t                formatted_value = replace_variables_for_values(value, dynamic_keys, ignore_key)\n\t        elif (isinstance(value, list)):\n\t            formatted_value = []\n\t            for item in value:\n\t                if (not isinstance(item, list) and not isinstance(item, dict)):\n\t                    formatted_value.append(format_simple_value(item))\n\t                else:\n\t                    formatted_value.append(replace_variables_for_values(item, dynamic_keys, ignore_key))\n\t        else:\n", "            formatted_value = format_simple_value(value)\n\t        replaced_dict[formatted_key] = formatted_value\n\t    return replaced_dict\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/utils.py", "chunked_list": ["import traceback\n\timport os\n\timport sys\n\timport re\n\timport openai\n\t# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n\tcurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\tgrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n\tutils_dir = os.path.join(grandparent_dir, 'utils/')\n\tsys.path.append(utils_dir)\n", "from llm_vm.utils.labels import *\n\timport random\n\tfrom Levenshtein import distance as lev\n\tfrom llm_vm.utils.typings_llm import *\n\tfrom bs4 import BeautifulSoup\n\trandom_fixed_seed = random.Random(4)\n\tdef print_op(*kargs, **kwargs):\n\t    print(*kargs, **kwargs, flush=True)\n\tdef verbose_answer(data, answer):\n\t    return f'''<{L_ANSWER_DATA}>{str(data)}</{L_ANSWER_DATA}><{L_ANSWER_SUMMARY}>{answer}</{L_ANSWER_SUMMARY}>'''\n", "def remove_tags_from_html_string(html_string):\n\t    # if not isinstance(html_string, str):\n\t    #     return \"\"\n\t    # return re.compile(r'<[^>]+>').sub(\"\", html_string), False\n\t    soup = BeautifulSoup(html_string, 'html.parser')\n\t    has_friendly_tags: bool = False\n\t    # Find all tags that are not in the list of tags to preserve\n\t    for tag in soup.find_all(True):\n\t        if tag.name in ['ul', 'ol', 'li', 'a', 'pre', 'code', 'kbd', 'i']:\n\t            has_friendly_tags = True\n", "        elif tag.name in ['style', 'script']:\n\t            tag.extract() # remove tag and content\n\t        else:\n\t            tag.unwrap() # remove tag, but preserve content\n\t    # Get the stripped HTML string\n\t    return str(soup).strip(\"\\n \"), has_friendly_tags\n\tdef print_big(data, label = \"\"):\n\t    def do_format(x) -> str:\n\t        formatted_title = \"======#====== {:20} ======#======\\n\"\n\t        if len(x) >= 20:\n", "            return formatted_title.format(x)\n\t        else:\n\t            return formatted_title.format((int((20 - len(x)) / 2) * \" \") + x)\n\t    try:\n\t        if len(label):\n\t            print(do_format(str(label).upper()), data, flush=True)\n\t        else:\n\t            print(do_format(str(data)), flush=True)\n\t    except:\n\t        print(label, flush=True)\n", "# Tolerance: If string A can be converted into B in more than {tolerance} steps, they are considered different.\n\tdef remove_similars(similars_list, tolerance = 3):\n\t    unique_list = []\n\t    sanitized_list = list(map(lambda item: item.strip(\" \").lower(), similars_list))\n\t    similars_len = len(similars_list)\n\t    for i in range(similars_len):\n\t        has_duplicate: bool = False\n\t        for j in range(i+1, similars_len):\n\t            if lev(sanitized_list[i], sanitized_list[j], score_cutoff=tolerance) < tolerance:\n\t                has_duplicate = True\n", "                break\n\t        if has_duplicate == False:\n\t            unique_list.append(similars_list[i])\n\t    return unique_list\n\tdef make_interaction_request(human_question, ai_response, data, Q = L_QUESTION, A = L_ANSWER, D = L_ANSWER_DATA, INTERACTION = L_INTERACTION):\n\t    interaction_text = f\"<{INTERACTION}>\\n<{Q}>{human_question}</{Q}>\\n\"\n\t    if data and len(data) and isinstance(data, str):\n\t        interaction_text += f'''<{D}>{data}</{D}>'''\n\t    interaction_text += f'''<{A}>{ai_response if ai_response else ''}'''\n\t    stop = f\"</{A}>\\n</{INTERACTION}>\"\n", "    return interaction_text, stop\n\tdef make_interaction(human_question, ai_response, data = '',\n\t                     Q = L_QUESTION,\n\t                     A = L_ANSWER,\n\t                     D = L_ANSWER_DATA,\n\t                     INTERACTION = L_INTERACTION):\n\t    text, stop = make_interaction_request(human_question, ai_response, data, Q, A, D, INTERACTION)\n\t    return text + stop\n\tdef get_tool_by_id(tools, tool_id):\n\t    for tool in tools:\n", "        if tool['id'] == tool_id:\n\t            return tool\n\t    raise Exception(f\"No tool corresponds to id='{tool_id}'\")\n\tdef tidy_up_subquestions(subquestions_str, main_question):\n\t    sub_questions_raw = subquestions_str.replace(\"\\n\", \"\").split(SUBQ_SPLITTER)\n\t    # First, clear the subquestions of extra symbols that might be confusing\n\t    sub_questions_raw = list(filter(lambda q: len(q) > 1, [q.strip() for q in sub_questions_raw]))\n\t    # then, remove similar subquestions\n\t    unique_subquestions = remove_similars(sub_questions_raw)\n\t    # if we only have one sub question, we don't need to \"summarise\" at the end => main question == subquestion.\n", "    if len(unique_subquestions) == 1:\n\t        return [main_question], None\n\t    else:\n\t        # to make sure, remove similar subquestions again because we add the main question\n\t        # sub_questions = remove_similars(sub_questions + [question])\n\t        return unique_subquestions, main_question\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/business_logic.py", "chunked_list": ["import json\n\tfrom llm_vm.utils.typings_llm import *\n\tfrom llm_vm.agents.FLAT.agent_helper.utils import *\n\tfrom llm_vm.agents.FLAT.agent_helper.bothandler import question_split, pick_tool, check_can_answer_from_memory, generate_convo_history, prompt_for_answer, get_newest_decision_model\n\tfrom llm_vm.agents.FLAT.agent_helper.use_tool import use_tool\n\tfrom llm_vm.agents.FLAT.agent_helper.tool_utils import make_tool_input_case\n\t# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n\t# current_dir = os.path.dirname(os.path.abspath(__file__))\n\t# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n\t# utils_dir = os.path.join(grandparent_dir, 'utils/')\n", "# sys.path.append(utils_dir)\n\tfrom labels import *\n\tfrom llm_vm.agents.FLAT.agent_helper.requests.call_llm import call_llm\n\t__MAX_RETRIES_GUESS_INPUT = 3\n\tdef __get_tool_input(\n\t    tool: SingleTool,\n\t    mem: TupleList,\n\t    question: str,\n\t    verbose: int,\n\t    max_tokens: int = 20,\n", "    temperature: float = 0.0\n\t) -> Tuple[dict, float]:\n\t    gpt_prompt, gpt_prompt_stop = make_tool_input_case(\n\t        mem,\n\t        question,\n\t        None,\n\t        tool_descr=tool[\"description\"],\n\t        tool_params=tool[\"dynamic_params\"],\n\t        wrapper_tag=L_INTERACTION\n\t    )\n", "    if verbose > 3:\n\t        print_big(gpt_prompt, f\"GPT PROMPT [tool={tool['id']}]\")\n\t    gpt_suggested_input, price = call_llm({\n\t        \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t        \"model\": get_newest_decision_model(DecisionStep.INPUT),\n\t        \"max_tokens\": max_tokens,\n\t        \"prompt\": gpt_prompt,\n\t        \"stop\": gpt_prompt_stop,\n\t        \"temperature\": temperature,\n\t    })\n", "    try:\n\t        if gpt_suggested_input[0] != \"{\":\n\t            gpt_suggested_input = \"{\" + gpt_suggested_input\n\t        if gpt_suggested_input[-1] != \"}\":\n\t            gpt_suggested_input += \"}\"\n\t        parsed_gpt_suggested_input: dict = json.loads(gpt_suggested_input)\n\t        if verbose > 1:\n\t            _has_keys = len(parsed_gpt_suggested_input.keys()) > 0\n\t            print_big(\n\t                parsed_gpt_suggested_input if _has_keys else \"(( no input ))\", \"GPT SUGGESTED INPUT\")\n", "        return parsed_gpt_suggested_input, price\n\t    except:\n\t        print_big(gpt_suggested_input, \"INVALID GPT INPUT\")\n\t        # {\"$ PARSING FAILED $\": str(e), \"$ REASON $\": \"malformed JSON\"}\n\t        return {}, price\n\tdef promptf(\n\t    question: str,\n\t    factual_memory: TupleList,\n\t    tools: ToolList,\n\t    bot_instructions: str,\n", "    verbose: int,\n\t    split_allowed: bool = True,\n\t    force_answer_from_memory: bool = False\n\t) -> Tuple[str, TupleList, List[str], DebugCallList, float]:\n\t    price_accumulator = 0\n\t    if split_allowed:\n\t        conversation_memory: TupleList = []\n\t        sub_questions, main_question, price = question_split({\n\t            \"mem\": factual_memory,\n\t            \"question\": question,\n", "            # \"tools\": tools\n\t        })\n\t        price_accumulator += price\n\t        if verbose > 0:\n\t            if len(sub_questions) == 1:\n\t                print_big(f\"NO SPLIT REQUIRED\")\n\t            else:\n\t                print_big(f'''Question:\\n- {main_question}\\n\\nSubquestions:\\n''' + (\n\t                    \"- \" + \"\\n- \".join(sub_questions)), f\"SPLIT\")\n\t        calls: List[str] = []\n", "        debug_return: DebugCallList = []\n\t        for i in range(len(sub_questions)):\n\t            answer, new_facts, new_calls, new_debug_return, price = promptf(\n\t                sub_questions[i],\n\t                factual_memory,\n\t                tools,\n\t                bot_instructions,\n\t                verbose,\n\t                split_allowed=False\n\t            )\n", "            calls = calls + new_calls\n\t            factual_memory = factual_memory + new_facts\n\t            debug_return = debug_return + new_debug_return\n\t            price_accumulator += price\n\t            conversation_memory.append((sub_questions[i], answer))\n\t        if main_question:\n\t            print_big(main_question, \"TRYING TO ANSWER MAIN QUESTION\")\n\t            final_answer, _, _, _, price = promptf(\n\t                main_question,\n\t                # last round, we give the non-verbose convo memory.\n", "                conversation_memory,\n\t                [],\n\t                bot_instructions,\n\t                verbose,\n\t                split_allowed=False,\n\t                force_answer_from_memory=True\n\t            )\n\t            price_accumulator += price\n\t            conversation_memory.append((main_question, final_answer))\n\t            answer = final_answer\n", "            print_big(answer, \"FINAL ANSWER\")\n\t            print_big(\"=========#####=========\")\n\t        return answer, [], calls, debug_return, price_accumulator\n\t    if len(tools):\n\t        best_tool_id, price_picker = pick_tool(tools, question, factual_memory)\n\t        price_accumulator += price_picker\n\t        cannot_use_tools_to_answer = best_tool_id in [\n\t            DefaultTools.ANSWER_FROM_MEMORY.value, DefaultTools.I_DONT_KNOW.value]\n\t    else:\n\t        best_tool_id = DefaultTools.ANSWER_FROM_MEMORY.value if force_answer_from_memory else DefaultTools.I_DONT_KNOW.value\n", "        cannot_use_tools_to_answer = True\n\t    # Only in the cases where we DON'T have a tool (answer_from_tool == False) we check if we can answer from memory.\n\t    can_answer_from_memory = force_answer_from_memory\n\t    if cannot_use_tools_to_answer and not force_answer_from_memory:\n\t        can_answer_from_memory, price_check = check_can_answer_from_memory(\n\t            question, memory=factual_memory)\n\t        price_accumulator += price_check\n\t    print_big(\n\t        f\"MEM ({can_answer_from_memory}) / TOOL ({not cannot_use_tools_to_answer})\")\n\t    if cannot_use_tools_to_answer and can_answer_from_memory:\n", "        cur_prompt = (\n\t            bot_instructions +\n\t            generate_convo_history(memory=factual_memory) +\n\t            f\"\\n\\n\" +\n\t            f\"\\n[SYSTEM]: Use <{L_ANSWER_DATA}/> to answer better, but follow the format of <{L_ANSWER_SUMMARY}/>\\n\" +\n\t            prompt_for_answer(question)\n\t        )\n\t        answer, price = call_llm({\n\t            \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t            \"model\": OpenAIModel.DAVINCI_TEXT.value if \"quality\" == \"best\" else OpenAIModel.CURIE_TEXT.value,\n", "            \"max_tokens\": 200,\n\t            \"prompt\": cur_prompt,\n\t            \"stop\": None,\n\t            \"temperature\": 0.1,\n\t        })\n\t        price_accumulator += price\n\t        if verbose > 0:\n\t            print_op(\"Question:  \", question)\n\t            print_op(\"AI Answer: \", answer)\n\t        return answer, [(question, answer)], [], [], price_accumulator\n", "    conversation_history = generate_convo_history(facts=factual_memory)\n\t    # No information in the Conversation memory AND no tool --> Using AI General Knowledge Base.\n\t    if best_tool_id in [DefaultTools.I_DONT_KNOW.value, DefaultTools.ANSWER_FROM_MEMORY.value]:\n\t        if (best_tool_id == DefaultTools.I_DONT_KNOW.value):\n\t            current_prompt = f\"{bot_instructions}\\n\" if bot_instructions else \"\" \\\n\t                + (f\"<{L_CONVERSATION}>{conversation_history}</{L_CONVERSATION}>\\n\" if len(conversation_history) else \"\") \\\n\t                + f\"<{L_QUESTION}>{question}</{L_QUESTION}>\\n\" \\\n\t                + f\"<{L_THOUGHT}>Maybe not enough information to answer. If that's the case, say why.</{L_THOUGHT}>\\n\" \\\n\t                + f\"<{L_ANSWER}>\"\n\t        else:\n", "            current_prompt = f\"{bot_instructions}\\n\" if bot_instructions else \"\" \\\n\t                + (f\"<{L_CONVERSATION}>{conversation_history}</{L_CONVERSATION}>\\n\" if len(conversation_history) else \"\") \\\n\t                + f\"<{L_QUESTION}>{question}</{L_QUESTION}>\\n\" \\\n\t                + f\"<{L_ANSWER}>\"\n\t        a, price = call_llm({\n\t            \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t            \"model\": OpenAIModel.DAVINCI_TEXT.value,\n\t            \"max_tokens\": 500,\n\t            \"prompt\": current_prompt,\n\t            \"stop\": f\"</{L_ANSWER}>\",\n", "            \"temperature\": 0.1,\n\t        })\n\t        price_accumulator += price\n\t        return a, [(question, a)], [], [], price_accumulator\n\t    best_tool: SingleTool = get_tool_by_id(tools, best_tool_id)\n\t    if \"dynamic_params\" not in best_tool:\n\t        best_tool[\"dynamic_params\"] = {}\n\t    dynamic_params_to_fill = len(best_tool[\"dynamic_params\"].keys())\n\t    # only try to guess the \"dynamic_params\" if we have any\n\t    if dynamic_params_to_fill == 0:\n", "        tool_input = {}\n\t    else:\n\t        # Try more than once to get the right input.\n\t        for attempt_to_find_input in range(__MAX_RETRIES_GUESS_INPUT):\n\t            tool_input, price = __get_tool_input(\n\t                best_tool,\n\t                factual_memory,\n\t                question,\n\t                verbose,\n\t                max_tokens=200,\n", "                # higher temperature on subsequent attempts\n\t                temperature=min(0.15 * attempt_to_find_input, 0.5)\n\t            )\n\t            price_accumulator += price\n\t            filled_dynamic_params = len(tool_input.keys())\n\t            if filled_dynamic_params == dynamic_params_to_fill:\n\t                break\n\t        # If, besides all attempts, we still don't have the right input...\n\t        if filled_dynamic_params != dynamic_params_to_fill:\n\t            raise Exception(\n", "                f\"Wrong input! Not enough parameters (actual: {filled_dynamic_params}, expected: {dynamic_params_to_fill})\")\n\t    try:\n\t        response_instructions = question\n\t        if \"ai_response_prompt\" in best_tool.keys():\n\t            response_instructions = best_tool[\"ai_response_prompt\"]\n\t        answer, actual_url, raw_api_return, price = use_tool(\n\t            best_tool,\n\t            tool_input,\n\t            question,\n\t            factual_memory,\n", "            verbose,\n\t            bot_instructions,\n\t            response_instructions\n\t        )\n\t        price_accumulator += price\n\t    except Exception as e:\n\t        print_big(e, \"EXCEPTION CAUGHT! [#1]\")\n\t        cur_prompt_text = f\"\\n\"\\\n\t            f\"{bot_instructions}\\n\" \\\n\t            f\"{conversation_history}\\n\" \\\n", "            f\"--------\\n\" + \\\n\t            prompt_for_answer(question)\n\t        a, price = call_llm({\n\t            \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t            \"model\": OpenAIModel.DAVINCI_TEXT.value,\n\t            \"max_tokens\": 500,\n\t            \"prompt\": cur_prompt_text,\n\t        })\n\t        price_accumulator += price\n\t        return a, [(question, a)], [], [], price_accumulator\n", "    if verbose > 0:\n\t        print_big(\"RESULT OF ANSWER\")\n\t        print_op(\"Question:  \", question)\n\t        print_op(\"Tool Input:\", tool_input)\n\t        print_op(\"AI Answer: \", answer)\n\t    return answer, \\\n\t        [(question, verbose_answer(raw_api_return, answer))], \\\n\t        [actual_url], \\\n\t        [{\"response\": \"\", \"gpt_suggested_params\": tool_input, \"url\": actual_url, \"raw_api_return\": raw_api_return}], \\\n\t        price_accumulator\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/tools.py", "chunked_list": ["import os\n\timport sys\n\t# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n\t# current_dir = os.path.dirname(os.path.abspath(__file__))\n\t# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n\t# utils_dir = os.path.join(grandparent_dir, 'utils/')\n\t# sys.path.append(utils_dir)\n\tfrom keys import *\n\tfrom labels import *\n\tfrom llm_vm.utils.typings_llm import *\n", "from llm_vm.agents.FLAT.agent_helper.utils import verbose_answer\n\tCUSTOM_TOOL_ANSWER_EMBEDDING = \"/answer_embedding\"\n\tdef __get_generic_tools():\n\t    # # wolfram\n\t    WOLFRAM_KEY = os.getenv(\"LLM_VM_WOLFRAM_KEY\")\n\t    GOOGLE_MAPS_KEY = os.getenv(\"LLM_VM_GOOGLE_MAPS_KEY\")\n\t    wolfram_tool = {\n\t        'description': \"Useful to query questions about people, events, anything that can change, complicated math, live data retrieval, current date and other data.\",\n\t        # {'description': \"The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\",\n\t        'id': DefaultTools.WOLFRAM.value,\n", "        'dynamic_params': {\"input\": 'The natural language input query'},\n\t        'method': 'GET',\n\t        'args': {\n\t            'url': \"http://api.wolframalpha.com/v2/query\",\n\t            'params': {'appid': WOLFRAM_KEY, 'input': '{input}'}\n\t        },\n\t    }\n\t    # geopy\n\t    directions_tool = {\n\t        'description': \"Find the driving distance and time to travel between two cities.\",\n", "        'id': DefaultTools.DIRECTIONS.value,\n\t        'dynamic_params': {\"origins\": 'the origin city', \"destinations\": 'the destination city'},\n\t        'method': 'GET',\n\t        'args': {\n\t            'url': \"https://maps.googleapis.com/maps/api/distancematrix/json\",\n\t            'params': {\n\t                'key': GOOGLE_MAPS_KEY,\n\t                'origins': '{origins}',\n\t                'destinations': '{destinations}'\n\t            }\n", "        }\n\t    }\n\t    # weather\n\t    weather_tool = {\n\t        'description': 'Useful to get the weather.',\n\t        'ai_response_prompt': 'Returns temperature in celcius',\n\t        'id': DefaultTools.WEATHER.value,\n\t        'dynamic_params': {\n\t            \"latitude\": 'latitude of the location, a float',\n\t            \"longitude\": 'the longitude of the location, a float'\n", "        },\n\t        \"ai_response_prompt\": '''the value of \"current_weather.weathercode\" means:\n\t0: Clear sky\n\t1, 2, 3: Mainly clear, partly cloudy, and overcast\n\t45, 48: Fog and depositing rime fog\n\t51, 53, 55: Drizzle\n\t56, 57: Freezing Drizzle\n\t61, 63, 65: Rain\n\t66, 67: Freezing Rain\n\t71, 73, 75: Snow fall\n", "77: Snow grains\n\t80, 81, 82: Rain showers\n\t85, 86: Snow showers slight and heavy\n\t95: Thunderstorm\n\t96, 99: Thunderstorm with slight and heavy hail\n\tDo not return the weathercode as a number, instead use the description from list above''',\n\t        'method': 'GET',\n\t        'args': {\n\t            'url': \"https://api.open-meteo.com/v1/forecast\",\n\t            'params': {\n", "                'current_weather': 'true',\n\t                'latitude': '{latitude}',\n\t                'longitude': '{longitude}'\n\t            }\n\t        }\n\t    }\n\t    # return [wolfram_tool, directions_tool, weather_tool]\n\t    return [weather_tool]\n\tGENERIC_TOOLS = __get_generic_tools()\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/use_tool.py", "chunked_list": ["import os\n\timport sys\n\timport requests\n\tfrom llama_index import Document, GPTTreeIndex\n\tfrom llm_vm.agents.FLAT.agent_helper.utils import print_op, make_interaction, print_big\n\tfrom llm_vm.agents.FLAT.agent_helper.requests.call_llm import call_llm\n\tfrom llm_vm.agents.FLAT.agent_helper.replacer import replace_variables_for_values\n\tfrom llm_vm.utils.typings_llm import *\n\tfrom llm_vm.agents.FLAT.agent_helper.bothandler import prompt_for_answer, prompt_for_instructions\n\t# # Get the current file's directory to grab the python files with common functionality in the utils/ folder\n", "# current_dir = os.path.dirname(os.path.abspath(__file__))\n\t# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n\t# utils_dir = os.path.join(grandparent_dir, 'utils/')\n\t# sys.path.append(utils_dir)\n\tfrom llm_vm.utils.labels import *\n\tfrom llm_vm.agents.FLAT.agent_helper.tools import CUSTOM_TOOL_ANSWER_EMBEDDING\n\timport json\n\tdef __format_tool_url (url: str) -> str:\n\t    if CUSTOM_TOOL_ANSWER_EMBEDDING in url:\n\t        return \"CHAT.DEV Embeddings Service\"\n", "    return url\n\tdef use_tool(\n\t    tool: SingleTool,\n\t    gpt_suggested_input: dict,\n\t    question: str,\n\t    memory: TupleList,\n\t    verbose: int,\n\t    bot_instructions: str,\n\t    response_instructions: str = \"\"\n\t) -> Tuple[str, str, Any, float]:\n", "    price = 0\n\t    # make sure all of the suggested fields exist in the tool desc\n\t    for i in gpt_suggested_input.keys():\n\t        if i not in tool[\"dynamic_params\"].keys():\n\t            raise Exception(\"Bad Generated Input\")\n\t    try:\n\t        tool_args = replace_variables_for_values(\n\t            tool[\"args\"],\n\t            gpt_suggested_input\n\t        )\n", "    except Exception as e:\n\t        print_big(e, \"EXCEPTION\");\n\t        tool_args = {}\n\t    url = tool_args['url']\n\t    if 'auth' in tool_args and isinstance(tool_args['auth'], dict):\n\t        auths = list(tool_args['auth'].items())\n\t        if len(auths) > 0:\n\t            tool_args['auth'] = list(tool_args['auth'].items())[0]\n\t        else:\n\t            del tool_args['auth']\n", "    # Remove those parameters that are not part of Python's `requests` function.\n\t    tool_args.pop(\"jsonParams\", None)\n\t    tool_args.pop(\"urlParams\", None)\n\t    # print_big(tool_args, \"ARGS\")\n\t    request_function = None\n\t    method = tool['method'].upper()\n\t    if method == \"PUT\":\n\t        request_function = requests.put\n\t    elif method == \"POST\":\n\t        request_function = requests.post\n", "    elif method == \"PATCH\":\n\t        request_function = requests.patch\n\t    elif method == \"DELETE\":\n\t        request_function = requests.delete\n\t    elif method == \"GET\":\n\t        request_function = requests.get\n\t    else:\n\t        request_function = requests.get\n\t    resp = request_function(**tool_args)\n\t    print_big(resp.url, \"FINAL URL: (\" + tool[\"method\"] + \") \")\n", "    api_call_args = str(tool_args)\n\t    return_value = str(resp.text)\n\t    formatted_url = __format_tool_url(str(url))\n\t    if verbose > -1:\n\t        pass\n\t        #commented out, causes test cases to fail because the return json of some wolfram alpha searches contains character encodings that charmap does not know.\n\t        # leads to error being thrown.\n\t        # print_op(\"URL Response:\", ret)\n\t    try:\n\t        # seems pointless, but it formats the resulting json without spaces.\n", "        # If the response isn't json, it does nothing.\n\t        return_value_json = json.loads(return_value)\n\t        return_value = str(return_value_json)\n\t    except:\n\t        return_value_json = {\"response\": return_value}\n\t        pass\n\t    if resp.status_code in [400, 401, 404, 500]:\n\t        er = \" → \" + str(resp.status_code)\n\t        return \"This tool isn't working currently\" + er, formatted_url + er, return_value_json, price\n\t    summarised_ret: str = \"\"\n", "    if len(return_value) > 8000:\n\t        document = Document(return_value)\n\t        try:\n\t            if verbose > 2:\n\t                print_op(\"### Summarising with GPT Tree Index ...\")\n\t            summarised_ret = GPTTreeIndex([document]).query(\n\t                response_instructions, response_mode=\"tree_summarize\")\n\t        except:\n\t            return \"OpenAI is down!\", formatted_url, None, price\n\t        if verbose > 2:\n", "            print_op(\"SUMMARISED:\", summarised_ret)\n\t        if str(summarised_ret).find(\"ANSWER: \") == 0:\n\t            summarised_ret = str(return_value)[len(\"ANSWER: X.\"):].strip(\" .\")\n\t    else:\n\t        mem = \"\".join([make_interaction(p, a, INTERACTION = L_INT_HUMAN) for p,a in memory]) \\\n\t        # THIS WORKS VERY WELL, but using <XML> tags here does not work.\n\t        summarise_prompt, stop = f\"{bot_instructions}<{L_CONVERSATION}>{mem}</{L_CONVERSATION}>\\n\\\n\tDescription: a JSON object contains:\\n{return_value}\\nUsing this information, what is the [{L_ANSWER}] to [{L_QUESTION}]?\\n \\\n\t{prompt_for_instructions(response_instructions)}{prompt_for_answer(question)}\", None\n\t        if verbose > 3:\n", "            print_big(summarise_prompt, \"SUMMARY OF JSON DECODE\")\n\t        summarised_ret, price = call_llm({\n\t            \"llm\": LLMCallType.OPENAI_COMPLETION,\n\t            \"model\": OpenAIModel.DAVINCI_TEXT.value,\n\t            \"max_tokens\": 256,\n\t            \"stop\": stop,\n\t            \"prompt\": summarise_prompt,\n\t            \"temperature\": 0.1,\n\t        })\n\t        print_big(summarised_ret, \"SUMMARISED ANSWER\")\n", "    return summarised_ret, formatted_url, return_value_json, price\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/tool_utils.py", "chunked_list": ["import json\n\timport os\n\timport sys\n\t# # Get the current file's directory to grab the python files with common functionality in the utils/ folder\n\t# current_dir = os.path.dirname(os.path.abspath(__file__))\n\t# grandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n\t# utils_dir = os.path.join(grandparent_dir, 'utils/')\n\t# sys.path.append(utils_dir)\n\tfrom llm_vm.utils.labels import *\n\tfrom llm_vm.utils.typings_llm import *\n", "from llm_vm.agents.FLAT.agent_helper.utils import make_interaction\n\tfrom datetime import datetime\n\tfrom llm_vm.agents.FLAT.agent_helper.tools import GENERIC_TOOLS\n\tfrom random import sample, shuffle\n\tdef __create_tool_tag(tool: SingleTool) -> str:\n\t    return f'<{L_TOOL} id=\"{tool[\"id\"]}\" description=\"{tool[\"description\"]}\"/>'\n\t__DEFAULT_TOOLS = \"\\n\".join(\n\t    [__create_tool_tag(tool)\n\t     for tool in GENERIC_TOOLS]\n\t)\n", "def __tools_we_have_access_to(tools: str = __DEFAULT_TOOLS) -> str:\n\t    return f\"\"\"<{L_TOOL_LIST}>{tools}</{L_TOOL_LIST}>\"\"\"\n\tdef __split_into_subquestions_instructions(extra: str = \"\") -> str:\n\t    return f\"\"\"<{L_SPLIT_INSTRUCTIONS}>Look at the tools we have access to. If needed, split <{L_QUESTION}/> into subquestions, so each <{L_QUESTION}/> can be answered with one use of one tool. Make as few subquestions as possible, comma-separated, avoid duplicates, and return nothing else.{extra}</{L_SPLIT_INSTRUCTIONS}>\"\"\"\n\tdef make_tool_desc(tool: SingleTool):\n\t    params = \"{\" + \", \".join(['\"'+l + '\": '+v for l, v in tool['dynamic_params'].items()]\n\t                               )+\"}\" if tool['dynamic_params'] != \"\" else \"{}\"\n\t    return f'''<{L_TOOL} id=\"{tool['id']}\">\n\t  <{L_DESCRIPTION}>{tool['description']}</{L_DESCRIPTION}>\n\t  <{L_PARAMS}>{params}</{L_PARAMS}>\n", "</{L_TOOL}>\n\t'''\n\tdef generate_convo_history(memory: TupleList = [], facts: TupleList = []) -> str:\n\t    if len(memory) + len(facts) == 0:\n\t        return \"\"\n\t    return (\n\t        \"\\n\".join([make_interaction(p, a, '', L_QUESTION, L_ANSWER, L_ANSWER_DATA, INTERACTION = L_INT_HUMAN) for p, a in memory]) + \\\n\t        \"\\n\".join([make_interaction(p, a, '', L_QUESTION, L_ANSWER, L_ANSWER_DATA, INTERACTION = L_INT_AI) for p, a in facts])\n\t    )\n\tdef prompt_for_instructions(instructions: str) -> str:\n", "    if instructions and len(instructions):\n\t        return f'''[{L_BOT_INSTRUCTIONS}]: To summarize the JSON, remember:\\n {instructions}\\n\\n\\n'''\n\t    else:\n\t        return \"\"\n\tdef prompt_for_answer(question: str) -> str:\n\t    return f'''[{L_QUESTION}]: {question}\\n[{L_ANSWER}]: ''';\n\t## ONLY for training purposes!\n\tdef get_training_tool_subset (\n\t    list_of_tools: ToolList,\n\t    tool_id_always_returned: Union[int, None],\n", "    max_num_elements: int = 6\n\t) -> ToolList:\n\t    # ONLY when we don't know, we return any elements.\n\t    if tool_id_always_returned == None \\\n\t       or tool_id_always_returned == DefaultTools.I_DONT_KNOW.value \\\n\t       or tool_id_always_returned == DefaultTools.ANSWER_FROM_MEMORY.value:\n\t        return sample(list_of_tools, max_num_elements - 1)\n\t    specific_tool = next(tool for tool in list_of_tools if tool[\"id\"] == tool_id_always_returned)\n\t    subset_of_tools = sample(list_of_tools, max_num_elements - 1)\n\t    # make sure it doesn't have our specific_tool inside\n", "    subset_of_tools = [tool for tool in subset_of_tools if tool[\"id\"] != tool_id_always_returned] + [specific_tool]\n\t    # VERY IMPORTANT - otherwise the model thinks that the last answer is always the correct one.\n\t    shuffle(subset_of_tools)\n\t    return subset_of_tools\n\tdef splitter_prompt(elements: List[QuestionSplitModelData]) -> Tuple[str, str]:\n\t    stopper: str = f'''</{L_ANSWER}>\n\t</{L_SPLIT}>'''\n\t    last_stop = None;\n\t    response: str = f'''\\\n\t{__split_into_subquestions_instructions()}'''\n", "    for element in elements:\n\t        response += f'''\\\n\t<{L_SPLIT}>\n\t{\n\t    __tools_we_have_access_to(\"\".join([__create_tool_tag(tool) for tool in element[\"tools\"]])) if (\"tools\" in element and element[\"tools\"] and isinstance(element[\"tools\"], list)) else \"\"\n\t}{\n\t    f\"\"\"<{L_CONVERSATION}>{\"\".join([make_interaction(q,a) for (q,a) in element[\"mem\"]])}</{L_CONVERSATION}>\"\"\" if (\"mem\" in element and isinstance(element[\"mem\"], list)) else \"\"\n\t}\n\t<{L_QUESTION}>{element[\"question\"]}</{L_QUESTION}>\n\t<{L_ANSWER}>'''\n", "        if \"answer\" in element and element[\"answer\"]:\n\t            response += f'''{SUBQ_SPLITTER.join(element[\"answer\"])}{stopper}\\n'''\n\t            last_stop = None\n\t        else:\n\t            last_stop = stopper\n\t    return response, last_stop\n\tdef toolpicker_prompt(elements: List[ToolpickerInputModelData], tools: ToolList) -> str:\n\t    shuffled_tools = tools.copy()\n\t    shuffle(shuffled_tools)\n\t    tools_html = \"\\n\".join([__create_tool_tag(_tool) for _tool in shuffled_tools])\n", "    last_stop = None\n\t    prompt: str = f'''<{L_BOT_INSTRUCTIONS}><{L_DESCRIPTION}>\n\tWe have a list of tools in <{L_TOOL_LIST}/>, and a question <{L_QUESTION}/> that we want to answer.\n\tWhich of the <{L_TOOL}/> in <{L_TOOL_LIST}/> can be used to best answer <{L_QUESTION}/>?\n\tAnswer only with the 'id' of the most relevant <{L_TOOL}/> (i.e.: {\", \".join([str(t['id']) for t in shuffled_tools])}).\n\tAnswer {DefaultTools.ANSWER_FROM_MEMORY.value} if the answer is in <{L_CONVERSATION}/>.\n\tAnswer {DefaultTools.I_DONT_KNOW.value} if no tool can be used to answer <{L_QUESTION}/>.\n\t</{L_DESCRIPTION}>\n\t<{L_TOOL_LIST}>\n\t{tools_html}\n", "</{L_TOOL_LIST}>\n\t</{L_BOT_INSTRUCTIONS}>\n\t'''\n\t    for element in elements:\n\t        prompt += f'''<{L_INTERACTION}>'''\n\t        prompt += f'''<{L_CONVERSATION}>\n\t            {\"\".join([make_interaction(mem_q, mem_a) for mem_q, mem_a in (element[\"mem\"])\n\t        ])}</{L_CONVERSATION}>''' if \"mem\" in element and isinstance(element[\"mem\"], list) and len(element[\"mem\"]) else \"\"\n\t        prompt += f'''{\n\t      f'<{L_THOUGHT}>{element[\"thought\"]}</{L_THOUGHT}>' if \"thought\" in element and element[\"thought\"] else \"\"\n", "}\n\t<{L_QUESTION}>{element[\"question\"]}</{L_QUESTION}>\n\t<{L_ANSWER}>'''\n\t        stopper = f'''</{L_ANSWER}>\\n</{L_INTERACTION}>'''\n\t        if \"answer\" in element:\n\t            prompt += f'''{element[\"answer\"]}{stopper}\\n'''\n\t            last_stop = None\n\t        else:\n\t            last_stop = stopper\n\t    return prompt, last_stop\n", "def create_memory_prompt(elements: List[AnswerInMemoryModelData]) -> Tuple[str, str]:\n\t    do_you_know_the_answer = f\"<{L_BOT_INSTRUCTIONS}>Is the answer to <{L_QUESTION}/> found in the memory or in your knowledge base already? Answer with a yes or no.</{L_BOT_INSTRUCTIONS}>\"\n\t    stop = f\"</{L_ANSWER}></{L_INTERACTION}>\"\n\t    last_stop = None\n\t    prompt: str = f'''{do_you_know_the_answer}\\n'''\n\t    for element in elements:\n\t        conversation_history = generate_convo_history(\n\t            element[\"mem\"] if \"mem\" in element and element[\"mem\"] else [],\n\t            element[\"facts\"] if \"facts\" in element and element[\"facts\"] else []\n\t        )\n", "        prompt += f'''<{L_INTERACTION}>\\n'''\n\t        if conversation_history:\n\t            prompt += f'''\\\n\t<{L_CONVERSATION}>\n\t    {conversation_history}\n\t</{L_CONVERSATION}>\\n'''\n\t        prompt += f'''<{L_QUESTION}>{element[\"question\"]}</{L_QUESTION}>\\n<{L_ANSWER}>'''\n\t        if \"answer\" in element and isinstance(element[\"answer\"], bool):\n\t            prompt += f'''{\"yes\" if element[\"answer\"] else \"no\"}{stop}'''\n\t            last_stop = None\n", "        else:\n\t            last_stop = stop\n\t            break\n\t    return prompt, last_stop\n\tdef make_tool_input_case(\n\t    q_facts: TupleList,\n\t    q_question: str,\n\t    answer_json_input: Union[dict, None],\n\t    tool_descr: str,\n\t    tool_params: dict,\n", "    answer_label: str = \"JSON\",\n\t    wrapper_tag: str = L_EXAMPLE,\n\t) -> Tuple[str, Union[str, None]]:\n\t    system_instructions: str = f'''\n\tWe have the following {answer_label}: <{answer_label}>{json.dumps(tool_params)}</{answer_label}>. Each key describes what its value is.\\n\n\tThe {answer_label} is used where: <{L_DESCRIPTION}>{tool_descr}</{L_DESCRIPTION}>\\n\n\tWe need a {answer_label} object with the same keys as <{answer_label}/> to answer <{L_QUESTION}/>{f\"\"\" (we can use the <{L_CONVERSATION}/> for contextual info as well)\"\"\" if len(q_facts) else \"\"}. Must be a valid {answer_label}!'''\n\t    stopper: str = f\"</{L_ANSWER}>\\n</{wrapper_tag}>\"\n\t    mem = generate_convo_history(facts=q_facts)\n\t    return f\"\\n<{wrapper_tag}>\\n\" \\\n", "        + f\"  <{L_BOT_INSTRUCTIONS}>{system_instructions}</{L_BOT_INSTRUCTIONS}>\" \\\n\t        + (f\"  <{L_CONVERSATION}>{mem}\\n</{L_CONVERSATION}>\\n\" if mem else \"\") \\\n\t        + f\"  <{L_QUESTION}>{q_question}</{L_QUESTION}>\\n\" \\\n\t        + f\"  <{L_ANSWER} type=\\\"{answer_label}\\\">\"\\\n\t        + (( f\"{json.dumps(answer_json_input)}{stopper}\") if answer_json_input else \"\"), None if answer_json_input else stopper\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/requests/call_llm.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\tfrom llm_vm.agents.FLAT.agent_helper.requests.call_open_ai import call_open_ai\n\tdef call_llm(llm_request: LLMCallParams) -> LLMCallReturnType:\n\t    if llm_request['llm'] == LLMCallType.OPENAI_CHAT or llm_request['llm'] == LLMCallType.OPENAI_COMPLETION:\n\t        try:\n\t            return call_open_ai(llm_request)\n\t        except Exception as e:\n\t            print(\"OpenAI call failed\", e)\n\t            return \"OpenAI is down!\", 0\n"]}
{"filename": "src/llm_vm/agents/FLAT/agent_helper/requests/call_open_ai.py", "chunked_list": ["import openai\n\timport os\n\tfrom llm_vm.utils.typings_llm import *\n\tdef __tokens_to_dollars(usage) -> float:\n\t    return float(usage)/1000*0.02\n\tdef call_open_ai(request: LLMCallParams) -> LLMCallReturnType:\n\t    model = request['model']\n\t    max_tokens = request['max_tokens']\n\t    stop = request['stop'] if 'stop' in request else None\n\t    cur_prompt = request['prompt']\n", "    temperature = request['temperature'] if 'temperature' in request else 0.0\n\t    chat = request['llm']\n\t    if isinstance(model, tuple):\n\t        model, api_key = model[0], model[1]\n\t    else:\n\t        api_key = False\n\t    if api_key:\n\t        if api_key[0] == '$':\n\t            api_key = False\n\t    current_key = None\n", "    if api_key:\n\t        current_key = openai.api_key\n\t        os.environ[\"OPENAI_API_KEY\"] = api_key\n\t        openai.api_key = api_key\n\t    else:\n\t        api_key = os.getenv(\"LLM_VM_OPENAI_API_KEY\")\n\t        openai.api_key = api_key\n\t    if chat == LLMCallType.OPENAI_CHAT:\n\t        response = openai.ChatCompletion.create(\n\t                model=\"gpt-3.5-turbo-0301\",\n", "                max_tokens=max_tokens,\n\t                stop=stop,\n\t                messages=cur_prompt,\n\t                temperature=temperature\n\t            )\n\t        response_text = response['choices'][0]['message']['content']\n\t    elif chat == LLMCallType.OPENAI_COMPLETION:\n\t        response = openai.Completion.create(\n\t                model=model,\n\t                max_tokens=max_tokens,\n", "                stop=stop,\n\t                prompt=cur_prompt,\n\t                temperature=temperature\n\t            )\n\t        response_text = response['choices'][0]['text']\n\t    if current_key:\n\t        openai.api_key = current_key\n\t        os.environ[\"OPENAI_API_KEY\"] = current_key\n\t    usage = response[\"usage\"][\"total_tokens\"]\n\t    price = __tokens_to_dollars(usage)\n", "    return response_text.strip(\"\\n \"), price\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/get_decision_model.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\timport os\n\timport json\n\tdef get_newest_decision_model(model: DecisionStep, default_model = OpenAIModel.DAVINCI_TEXT) -> Tuple[str, Union[str, None]]:\n\t    current_dir = os.path.dirname(os.path.abspath(__file__))\n\t    file_name = os.path.join(current_dir, 'models.json')\n\t    model_name = model.value\n\t    default_model_name = default_model.value\n\t    print(file_name)\n\t    file = open(file_name)\n", "    data: List[LLModels] = json.load(file);\n\t    for llm_model in reversed(data):\n\t        if model.value in llm_model and llm_model[model_name]:\n\t            print(f\"### {model_name} model will be used (version {llm_model['DATE']})\", flush=True)\n\t            return llm_model[model_name]['model_name'], llm_model['OPENAI_KEY']\n\t    print(f\"Model {model_name} not found: will use {default_model_name}\", flush=True)\n\t    return default_model_name, None\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_input_model/get_tool_input_as_jsonl.py", "chunked_list": ["import json\n\tfrom llm_vm.agents.FLAT.agent_helper.tool_utils import create_memory_prompt, make_tool_input_case\n\tfrom llm_vm.agents.FLAT.agent_helper.labels import *\n\tfrom typings_llm import *\n\tfrom random import shuffle\n\tfrom llm_vm.agents.FLAT.models.utils.tool_input_model.tool_input_model_data import tool_input_data\n\tdef __get_tool_input_jsonl(data: List[ToolInputModelData]) -> List[PromptModelEntry]:\n\t    jsonl_entries: List[PromptModelEntry] = []\n\t    for entry in data:\n\t        prompt, stopper = make_tool_input_case(\n", "            entry[\"mem\"],\n\t            entry[\"question\"],\n\t            None,\n\t            entry[\"description\"],\n\t            entry[\"params\"],\n\t        )\n\t        jsonl_entries.append({\n\t            \"prompt\": prompt,\n\t            \"completion\": json.dumps(entry[\"answer\"]) + stopper\n\t        })\n", "    shuffled_examples = jsonl_entries.copy()\n\t    shuffle(shuffled_examples)\n\t    return shuffled_examples\n\tdef tool_input_jsonl() -> Tuple[PromptModelEntry, str]:\n\t    return (\n\t        __get_tool_input_jsonl(tool_input_data[\"data\"]),\n\t        tool_input_data[\"openai_model\"].value\n\t    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_input_model/tool_input_model_data.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\timport os\n\timport json\n\tfrom llm_vm.agents.FLAT.agent_helper.utils import verbose_answer\n\tdef __get_tool_input_model(model: OpenAIModel) -> ToolInputModel:\n\t    current_dir = os.path.dirname(os.path.abspath(__file__))\n\t    file_path = os.path.abspath(os.path.join(current_dir, f'../../raw_data/tool_input_data.json'))\n\t    json_data: List[ToolInputModelJSONData] = json.load(open(file_path, \"r\"))[\"data\"]\n\t    def __get_mem_tuple(json_mem: List[List[Union[str, dict]]]) -> TupleList:\n\t        mem: TupleList = []\n", "        for m in json_mem:\n\t            if len(m) == 3:\n\t                # when the \"mem\" has also the previous JSON response\n\t                mem.append((m[0], verbose_answer(m[2], m[1])))\n\t            elif len(m) == 2:\n\t                # when the \"mem\" has only question/answer\n\t                mem.append((m[0], m[1]))\n\t        return mem\n\t    return {\n\t        \"openai_model\": model,\n", "        \"data\": [{\n\t            \"mem\": __get_mem_tuple(t[\"mem\"]) if \"mem\" in t else [],\n\t            \"question\": t[\"question\"],\n\t            \"answer\": t[\"answer\"],\n\t            \"description\": t[\"description\"],\n\t            \"params\": t[\"params\"]\n\t        } for t in json_data]\n\t    }\n\ttool_input_data: ToolInputModel = __get_tool_input_model(OpenAIModel.DAVINCI)\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/answer_from_memory_model/answer_from_memory_model_data.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\tanswer_from_memory_data: AnswerInMemoryModel = {\n\t    \"openai_model\": OpenAIModel.DAVINCI,\n\t    \"data\": [\n\t        {\n\t            \"question\": \"What's the time?\",\n\t            \"answer\": False\n\t        },\n\t        {\n\t            \"mem\": [\n", "                (\"Whats day is it today?\", \"Today is Sunday, December 15th, 2027. The time is 11:33AM\")\n\t            ],\n\t            \"question\": \"What's the time?\",\n\t            \"answer\": True\n\t        },\n\t        {\n\t            \"question\": \"How are you feeling?\",\n\t            \"answer\": True\n\t        },\n\t        {\n", "            \"question\": \"What color is the sky?\",\n\t            \"answer\": True\n\t        },\n\t        {\n\t            \"question\": \"What is the temperature in Portland?\",\n\t            \"answer\": False\n\t        },\n\t        {\n\t            \"mem\": [\n\t               (\"What is the weather in Portland?\", \"It is sunny, 22C, with clear skies. Theres 40% chance of rain after 18:00hs\")\n", "            ],\n\t            \"question\": \"What is the temperature in Portland?\",\n\t            \"answer\": True\n\t        },\n\t        {\n\t            \"question\": \"What day is it today?\",\n\t            \"answer\": False\n\t        },\n\t        {\n\t            \"question\": \"What's the name of this place?\",\n", "            \"answer\": False\n\t        }\n\t    ]\n\t}\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/answer_from_memory_model/get_asm_as_jsonl.py", "chunked_list": ["from llm_vm.agents.FLAT.agent_helper.tool_utils import create_memory_prompt\n\tfrom llm_vm.agents.FLAT.agent_helper.labels import *\n\tfrom typings_llm import *\n\tfrom llm_vm.agents.FLAT.models.utils.answer_from_memory_model.answer_from_memory_model_data import answer_from_memory_data\n\tdef __construct_answer_from_memory_jsonl(data: List[QuestionSplitModelData]) -> List[PromptModelEntry]:\n\t    jsonl_entries: List[PromptModelEntry] = []\n\t    for entry in data:\n\t        prompt, stopper = create_memory_prompt([{\n\t            \"mem\": entry[\"mem\"] if \"mem\" in entry else None,\n\t            \"question\": entry[\"question\"],\n", "            \"answer\": entry[\"answer\"]\n\t        }])\n\t        jsonl_entries.append({\n\t            \"prompt\": prompt,\n\t            \"completion\": (\"yes\" if entry[\"answer\"] else \"no\") + (stopper if stopper else \"\")\n\t        })\n\t    return jsonl_entries\n\tdef answer_from_memory_jsonl() -> Tuple[PromptModelEntry, str]:\n\t    return (\n\t        __construct_answer_from_memory_jsonl(answer_from_memory_data[\"data\"]),\n", "        answer_from_memory_data[\"openai_model\"].value\n\t    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_picker_model/get_training_tools.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\t# Retuns the tools, in a random order, and with randomised IDs.\n\tdef get_randomised_training_tools (\n\t    generic_tools: ToolList,\n\t    shuffled_by: int = 0,\n\t    shuffle_by_modulo: int = 1e9\n\t) -> ToolList:\n\t    def __shuffled(tool_id: int) -> int:\n\t        return int((tool_id + shuffled_by) % shuffle_by_modulo)\n\t    shuffled_generic_tools: ToolList = [{\"id\": __shuffled(t[\"id\"]), \"description\": t[\"description\"]} for t in generic_tools]\n", "    tools: ToolList = shuffled_generic_tools + [\n\t        {\n\t            \"id\": __shuffled(DefaultTrainingTools.TRAIN_SEND_EMAIL.value),\n\t            \"description\": \"Use this tool to send an email to someone\"\n\t        },\n\t        {\n\t            \"id\": __shuffled(DefaultTrainingTools.TRAIN_BOOK_TABLE.value),\n\t            \"description\": \"Useful to book a table or slot at a restaurant\"\n\t        },\n\t        {\n", "            \"id\": __shuffled(DefaultTrainingTools.TRAIN_CREATE_DOCUMENT.value),\n\t            \"description\": \"Create a document on google drive\"\n\t        },\n\t        {\n\t            \"id\": __shuffled(DefaultTrainingTools.TRAIN_SHARE_DOCUMENT.value),\n\t            \"description\": \"Useful to share an online document with someone. You need the email\"\n\t        },\n\t        {\n\t            \"id\": __shuffled(DefaultTrainingTools.TRAIN_FIND_RESTAURANT.value),\n\t            \"description\": \"Use this tool to find a specific bar, café or restaurant, or to search for places to eat around a certain location.\"\n", "        },\n\t        {\n\t            \"id\": __shuffled(DefaultTrainingTools.TRAIN_NEWS.value),\n\t            \"description\": \"Use this tool to get news and current events\"\n\t        },\n\t        {\n\t            \"id\": __shuffled(DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT.value),\n\t            \"description\": \"Use this tool to book an appointment, meeting or doctor, through a calendar.\"\n\t        },\n\t        {\n", "            \"id\": __shuffled(DefaultTrainingTools.TRAIN_AVAILABLE_APPOINTMENT.value),\n\t            \"description\": \"Use this tool to get the available time slots for meetings, doctors, appointments, etc.\"\n\t        },\n\t        {\n\t            \"id\": __shuffled(DefaultTrainingTools.TRAIN_ORDER_FOOD.value),\n\t            \"description\": \"Use this tool to order food and get it delivered.\"\n\t        }\n\t    ]\n\t    return tools\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_picker_model/get_tp_as_jsonl.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\tfrom llm_vm.agents.FLAT.agent_helper.labels import *\n\tfrom llm_vm.agents.FLAT.agent_helper.tool_utils import toolpicker_prompt\n\tfrom llm_vm.agents.FLAT.models.utils.tool_picker_model.tool_picker_model_data import tool_input_data\n\tdef __construct_tool_picker_jsonl(data: List[ToolpickerInputModelData]) -> List[PromptModelEntry]:\n\t    jsonl_entries: List[PromptModelEntry] = []\n\t    for entry in data:\n\t        prompt, stopper = toolpicker_prompt(\n\t            [{\n\t                \"mem\": entry[\"mem\"] if \"mem\" in entry else None,\n", "                \"question\": entry[\"question\"],\n\t                \"thought\": entry[\"thought\"] if \"thought\" in entry else None\n\t            }],\n\t            entry[\"tools\"]\n\t        )\n\t        jsonl_entries.append({\n\t            \"prompt\": prompt,\n\t            \"completion\": str(entry[\"answer\"]) + stopper\n\t        })\n\t    return jsonl_entries\n", "def tool_picker_jsonl() -> Tuple[PromptModelEntry, str]:\n\t    return (\n\t        __construct_tool_picker_jsonl(tool_input_data[\"data\"]),\n\t        tool_input_data[\"openai_model\"].value\n\t    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/tool_picker_model/tool_picker_model_data.py", "chunked_list": ["from llm_vm.utils.typings_llm import *\n\timport json\n\timport os\n\tfrom llm_vm.agents.FLAT.agent_helper.tool_utils import get_training_tool_subset\n\tfrom llm_vm.agents.FLAT.agent_helper.tools import GENERIC_TOOLS\n\tfrom llm_vm.agents.FLAT.models.utils.tool_picker_model.get_training_tools import get_randomised_training_tools\n\tfrom random import randrange\n\tdef __use_tool(tool_id: int, shuffle_value: int = 0, shuffle_modulo: int = 1e9) -> int:\n\t    return int((tool_id + shuffle_value) % shuffle_modulo)\n\t# Get a random subset of the tools, which WILL INCLUDE the given tool_id.\n", "# -> The tools are shuffled to prevent the model from finding patterns\n\t# -> The ids are changed to prevent the model from remembering tools (i.e. Tool=X is always ID=Y)\n\tdef __get_random_tool_subset(tool_id: int, shuffle_value: int = 0, shuffle_modulo: int = 1e9) -> str:\n\t    return get_training_tool_subset(\n\t        get_randomised_training_tools(GENERIC_TOOLS, shuffle_value, shuffle_modulo),\n\t        tool_id\n\t    )\n\tdef __get_toolpicker_model(tools_input_model: ToolpickerInputModel) -> ToolpickerInputModel:\n\t    current_dir = os.path.dirname(os.path.abspath(__file__))\n\t    file_path = os.path.abspath(os.path.join(current_dir, f'../../raw_data/toolpicker_data.json'))\n", "    json_data: List[QuestionSplitModelJSONData] = json.load(open(file_path, \"r\"))[\"data\"]\n\t    tools_input_model[\"data\"] = json_data\n\t    complete_tools: List[ToolpickerInputModelData] = []\n\t    for t in tools_input_model[\"data\"]:\n\t        rand_value = randrange(0, len(tools_input_model[\"data\"]))\n\t        answer_key = t[\"answer\"]\n\t        answer_value = ALL_TOOLS_MAP[answer_key].value if answer_key in ALL_TOOLS_MAP else None\n\t        if (answer_value == None):\n\t            raise Exception(\"Invalid tool name/value: \" + answer_key)\n\t        # tool_id_to_use = __use_tool(answer_value, rand_value) if answer_value > 0 else answer_value\n", "        tool_id_to_use = 0\n\t        complete_tools.append({\n\t            \"question\": t[\"question\"],\n\t            \"thought\": t[\"thought\"] if \"thought\" in t else \"\",\n\t            \"mem\": [(a[0], a[1]) for a in t[\"mem\"]] if \"mem\" in t else [],\n\t            \"answer\": tool_id_to_use,\n\t            \"tools\": __get_random_tool_subset(tool_id_to_use, rand_value)\n\t        })\n\t    complete_model: ToolpickerInputModel = {\n\t        \"openai_model\": tools_input_model[\"openai_model\"],\n", "        \"data\": complete_tools\n\t    }\n\t    return complete_model;\n\ttool_input_data: ToolpickerInputModel = __get_toolpicker_model({\n\t    \"openai_model\": OpenAIModel.DAVINCI\n\t})\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/question_split_model/question_split_model_data.py", "chunked_list": ["import json\n\timport os\n\tfrom llm_vm.utils.typings_llm import *\n\tdef __question_splitter_data() -> QuestionSplitInputModel:\n\t    current_dir = os.path.dirname(os.path.abspath(__file__))\n\t    file_path = os.path.abspath(os.path.join(current_dir, f'../../raw_data/question_split_data.json'))\n\t    json_data: List[QuestionSplitModelJSONData] = json.load(open(file_path, \"r\"))[\"data\"]\n\t    question_data: List[QuestionSplitModelData] = [\n\t        {\n\t            \"mem\": [(a[0], a[1]) for a in q[\"mem\"]] if \"mem\" in q else [],\n", "            \"question\": q[\"question\"],\n\t            \"answer\": q[\"answer\"]\n\t        }\n\t        for q in json_data\n\t    ]\n\t    return {\n\t        \"openai_model\": OpenAIModel.DAVINCI,\n\t        \"data\": question_data\n\t    }\n\tquestion_splitter_data = __question_splitter_data();\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/utils/question_split_model/get_qs_as_jsonl.py", "chunked_list": ["from llm_vm.agents.FLAT.agent_helper.tool_utils import splitter_prompt\n\tfrom llm_vm.agents.FLAT.agent_helper.labels import *\n\tfrom llm_vm.typings_llm import *\n\tfrom llm_vm.agents.FLAT.models.utils.question_split_model.question_split_model_data import question_splitter_data\n\tdef __construct_question_split_jsonl(data: List[QuestionSplitModelData]) -> List[PromptModelEntry]:\n\t    jsonl_entries: List[PromptModelEntry] = []\n\t    for entry in data:\n\t        prompt, stopper = splitter_prompt([{\n\t            \"mem\": entry[\"mem\"] if \"mem\" in entry else None,\n\t            \"tools\": entry[\"tools\"] if \"tools\" in entry else None,\n", "            \"question\": entry[\"question\"]\n\t        }])\n\t        jsonl_entries.append({\n\t            \"prompt\": prompt,\n\t            \"completion\": SUBQ_SPLITTER.join(entry[\"answer\"]) + stopper\n\t        })\n\t    return jsonl_entries\n\tdef question_splitter_jsonl() -> Tuple[PromptModelEntry, str]:\n\t    return (\n\t        __construct_question_split_jsonl(question_splitter_data[\"data\"]),\n", "        question_splitter_data[\"openai_model\"].value\n\t    )\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/persist_models.py", "chunked_list": ["import json\n\tfrom llm_vm.utils.typings_llm import *\n\timport os\n\timport time\n\tdef persist_models(models: LLModels, openai_key: str, is_test: bool = False) -> None:\n\t    if is_test:\n\t        return\n\t    current_dir = os.path.dirname(os.path.abspath(__file__))\n\t    file_name = os.path.join(current_dir, '../models.json')\n\t    model_json: List[LLModels] = []\n", "    with open(file_name, \"r\") as output_file:\n\t        loaded_json = json.loads(output_file.read() or \"[]\")\n\t        model_json: List[LLModels] = loaded_json if loaded_json else []\n\t    with open(file_name, \"w\") as output_file:\n\t        models[\"TIMESTAMP\"] = int(time.time())\n\t        models[\"DATE\"] = time.asctime()\n\t        models[\"OPENAI_KEY\"] = openai_key\n\t        model_json.append(models)\n\t        json.dump(model_json, output_file, indent=4)\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/delete_model.py", "chunked_list": ["import openai\n\tdef delete_model (model_name: str) -> None:\n\t    ## DELETE MODELS\n\t    try:\n\t        # model = openai.Model.retrieve()\n\t        openai.Model.delete(model_name)\n\t    except Exception as e:\n\t        print(\"Could not delete model because: \" + str(e))\n\t    try:\n\t        # Delete all files:\n", "        files = openai.File.list()[\"data\"]\n\t        [openai.File.delete(file[\"id\"]) for file in files];\n\t    except Exception as e:\n\t        print(\"Could not delete file because: \" + str(e))\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/check_model_status.py", "chunked_list": ["import openai\n\tfrom llm_vm.utils.typings_llm import *\n\tdef check_model_status (\n\t    job_id: str,\n\t    label: str\n\t) -> Dict:\n\t    finetuned_model = openai.FineTune.retrieve(id=job_id)\n\t    print(\n\t        label,\n\t        finetuned_model[\"status\"],\n", "        flush=True\n\t    )\n\t    return finetuned_model\n"]}
{"filename": "src/llm_vm/agents/FLAT/models/helpers/upload_model.py", "chunked_list": ["import json\n\timport openai\n\timport time\n\timport os\n\tfrom llm_vm.utils.typings_llm import *\n\tfrom llm_vm.agents.FLAT.models.helpers.check_model_status import check_model_status\n\t__START_LABEL    = \" 🎛️  Creating model...\"\n\t__PROGRESS_LABEL = \" ⏳ Processing\"\n\t__FINISHED_LABEL = \"\\n 🎉 Finished!\" + \\\n\t                   \"\\n 🔗\"\n", "def upload_model (\n\t    data: PromptModelEntry,\n\t    openai_model: str,\n\t    file_name: str,\n\t    is_test: bool = False\n\t) -> LLMTrainingResult:\n\t    start_of_execution = time.time()\n\t    def progress_label(label: str) -> str:\n\t        ellapsed_s = int(time.time() - start_of_execution)\n\t        return f\"{label} '{file_name}'\\t ({'%dm %02ds' % (ellapsed_s / 60, ellapsed_s % 60)}) \"\n", "    current_dir = os.path.dirname(os.path.abspath(__file__))\n\t    file_path = os.path.join(current_dir, f'./../data/{file_name}.jsonl')\n\t    with open(file_path, \"w\") as output_file:\n\t        output_file.truncate(0)\n\t        for entry in data:\n\t            json.dump(entry, output_file)\n\t            output_file.write(\"\\n\")\n\t    if is_test:\n\t        return {\n\t            \"model_name\": file_name + \"_EXXX\",\n", "            \"model_files\": [],\n\t            \"elapsed_time_s\": int(time.time() - start_of_execution)\n\t        }\n\t    upload_response = openai.File.create(\n\t        file=open(file_path, \"rb\"),\n\t        purpose='fine-tune'\n\t    )\n\t    file_id = upload_response.id\n\t    fine_tune_response = openai.FineTune.create(training_file=file_id, model=openai_model)\n\t    job_id = fine_tune_response[\"id\"]\n", "    print(__START_LABEL, f'''job_id: {job_id}, model: {file_name}''', flush=True)\n\t    finetuned_model = {\"status\": None}\n\t    while finetuned_model[\"status\"] not in [\"succeeded\", \"failed\"]:\n\t        time.sleep(15)\n\t        finetuned_model = check_model_status(job_id, progress_label(__PROGRESS_LABEL))\n\t    print(progress_label(__FINISHED_LABEL), finetuned_model[\"fine_tuned_model\"], flush=True)\n\t    related_files = (\n\t        [file[\"id\"] for file in finetuned_model[\"result_files\"]] + \\\n\t        [file[\"id\"] for file in finetuned_model[\"training_files\"]]\n\t    )\n", "    return {\n\t        \"model_name\": finetuned_model[\"fine_tuned_model\"],\n\t        \"model_files\": related_files,\n\t        \"elapsed_time_s\": int(time.time() - start_of_execution)\n\t    }\n"]}
{"filename": "src/llm_vm/utils/typings_llm.py", "chunked_list": ["\"\"\"\n\tThis file contains various machine learning classes and variables\n\tfor training and API calls to OpenAI.\n\t\"\"\"\n\tfrom typing import Tuple, List, Dict, TypedDict, Union, Any, Optional\n\tfrom enum import Enum\n\tclass OpenAIModel(Enum):\n\t    CURIE = \"curie\"\n\t    CURIE_TEXT = \"text-curie-001\"\n\t    FAST_DAVINCI = \"code-cushman-002\"\n", "    DAVINCI = \"davinci\"\n\t    DAVINCI_TEXT = \"text-davinci-003\"\n\tclass LLMCallType(Enum):\n\t    OPENAI_COMPLETION = \"openai-completion\"\n\t    OPENAI_CHAT = \"openai-chat\"\n\t    # ... other models\n\tclass DecisionStep(Enum):\n\t    SPLIT = \"split_questions\"\n\t    INPUT = \"guess_api_input\"\n\t    FROM_MEMORY = \"use_memory\"\n", "    TOOL_PICKER = \"tool_picker\"\n\tclass DefaultTools(Enum):\n\t    I_DONT_KNOW = -1\n\t    ANSWER_FROM_MEMORY = 0\n\t    WOLFRAM = 1\n\t    DIRECTIONS = 2\n\t    WEATHER = 3\n\t    ## Always set this to the latest ID. This is an integer, not an enum!\n\t    __LAST_ID__: int = 4\n\tclass DefaultTrainingTools(Enum):\n", "    TRAIN_SEND_EMAIL = 4\n\t    TRAIN_CREATE_DOCUMENT = 5\n\t    TRAIN_SHARE_DOCUMENT = 6\n\t    TRAIN_BOOK_TABLE = 7\n\t    TRAIN_FIND_RESTAURANT = 8\n\t    TRAIN_NEWS = 9\n\t    TRAIN_ORDER_FOOD = 10\n\t    TRAIN_BOOK_APPOINTMENT = 11\n\t    TRAIN_AVAILABLE_APPOINTMENT = 12\n\t    TRAIN_FIND_FLIGHT = 13\n", "TupleList = List[Tuple[str, str]]\n\tclass ToolTypeArgs(TypedDict):\n\t    url: str\n\t    params: Optional[dict]\n\t    json: Optional[dict]\n\t    jsonParams: Optional[dict]\n\t    auth: Optional[dict]\n\t    cert: Optional[str]\n\t    data: Optional[dict]\n\tclass SingleTool(TypedDict):\n", "    description: str\n\t    dynamic_params: dict\n\t    id: int\n\t    args: ToolTypeArgs\n\t    ai_response_prompt: Union[str, None]\n\tToolList = List[SingleTool]\n\t# ToolList = NewType(\"ToolList\", List[SingleTool])\n\tEnumeratedToolList = List[Tuple[int,SingleTool]]\n\tALL_TOOLS_MAP: Dict[str, Union[DefaultTools, DefaultTrainingTools]] = {\n\t    \"dont_know\": DefaultTools.I_DONT_KNOW,\n", "    \"memory\": DefaultTools.ANSWER_FROM_MEMORY,\n\t    \"weather\": DefaultTools.WEATHER,\n\t    \"wolfram\": DefaultTools.WOLFRAM,\n\t    \"directions\": DefaultTools.DIRECTIONS,\n\t    \"send_email\": DefaultTrainingTools.TRAIN_SEND_EMAIL,\n\t    \"create_doc\": DefaultTrainingTools.TRAIN_CREATE_DOCUMENT,\n\t    \"share_doc\": DefaultTrainingTools.TRAIN_SHARE_DOCUMENT,\n\t    \"book_table\": DefaultTrainingTools.TRAIN_BOOK_TABLE,\n\t    \"find_restaurant\": DefaultTrainingTools.TRAIN_FIND_RESTAURANT,\n\t    \"news\": DefaultTrainingTools.TRAIN_NEWS,\n", "    \"order_food\": DefaultTrainingTools.TRAIN_ORDER_FOOD,\n\t    \"book_appointment\": DefaultTrainingTools.TRAIN_BOOK_APPOINTMENT,\n\t    \"check_appointment\": DefaultTrainingTools.TRAIN_AVAILABLE_APPOINTMENT,\n\t    \"find_flight\": DefaultTrainingTools.TRAIN_FIND_FLIGHT\n\t}\n\tclass QuestionSplitModelJSONData(TypedDict):\n\t    mem: List[List[str]]\n\t    question: str\n\t    answer: List[str]\n\tclass QuestionSplitModelData(TypedDict):\n", "    mem: Union[TupleList, None]\n\t    question: str\n\t    answer: Union[List[str], None]\n\t    tools: Union[ToolList, None]\n\tclass AnswerFromMemoryModelData(TypedDict):\n\t    mem: Union[TupleList, None]\n\t    question: str\n\t    thought: Union[str, None]\n\t    answer: Union[str, None]\n\tclass QuestionSplitInputModel(TypedDict):\n", "    model_name: str\n\t    openai_model: OpenAIModel\n\t    data: List[QuestionSplitModelData]\n\tclass PromptModelEntry(TypedDict):\n\t    prompt: str\n\t    completion: str\n\tclass LLMCallParams(TypedDict):\n\t    llm: LLMCallType\n\t    model: Tuple[str, Union[str, None]]\n\t    prompt: str\n", "    temperature: float\n\t    max_tokens: int\n\t    stop: str\n\tLLMCallReturnType = Tuple[str, float]\n\tclass LLMTrainingResult(TypedDict):\n\t    elapsed_time_s: int\n\t    model_name: str\n\t    model_files: List[str]\n\tclass LLMTrainingResultMeta(TypedDict):\n\t    OPENAI_KEY: str\n", "    DATE: str\n\t    TIMESTAMP: int\n\tLLModels = Union[Dict[str, LLMTrainingResult], LLMTrainingResultMeta]\n\tclass ToolInputModelJSONData(TypedDict):\n\t    mem: Optional[List[str]]\n\t    description: str\n\t    question: str\n\t    answer: dict\n\t    params: dict\n\tclass ToolInputModelData(TypedDict):\n", "    mem: Optional[TupleList]\n\t    description: str\n\t    question: str\n\t    answer: dict\n\t    params: dict\n\tclass ToolInputModel(TypedDict):\n\t    openai_model: OpenAIModel\n\t    data: List[ToolInputModelData]\n\tclass ToolpickerInputModelJSONData(TypedDict):\n\t    mem: Optional[List[str]]\n", "    thought: Optional[str]\n\t    question: str\n\t    answer: str\n\tclass ToolpickerInputModelData(TypedDict):\n\t    mem: Optional[TupleList]\n\t    thought: Optional[str]\n\t    question: str\n\t    answer: int\n\t    tools: Optional[ToolList]\n\tclass ToolpickerInputModel(TypedDict):\n", "    openai_model: OpenAIModel\n\t    data: List[ToolpickerInputModelData]\n\tclass AnswerInMemoryModelData(TypedDict):\n\t    mem: Optional[TupleList]\n\t    facts: Optional[TupleList]\n\t    question: str\n\t    answer: Optional[bool]\n\tclass AnswerInMemoryModel(TypedDict):\n\t    openai_model: OpenAIModel\n\t    data: List[AnswerInMemoryModelData]\n", "class DebugCalls(TypedDict):\n\t    response: str\n\t    gpt_suggested_params: Any\n\t    url: str\n\t    raw_api_return: str\n\tDebugCallList = List[DebugCalls]\n"]}
{"filename": "src/llm_vm/utils/labels.py", "chunked_list": ["\"\"\"\n\tThis file contains a list of constants that represent tags for JSON objects.\n\t\"\"\"\n\t# REBEL & BACKWARD_CHAINING LABELS\n\tTOOL_ID = \"ID\"\n\tDESCRIPTION =\"DESC\"\n\tPARAMS = \"PARAMS\"\n\tINTERACTION = \"QA\"\n\tEXAMPLE = \"CASE\"\n\tCONVERSATION = \"MEM\"\n", "QUESTION = \"Q\"\n\tTHOUGHT = \"THOUGHT\"\n\tPROMPT = \"P\"\n\tRESPONSE = \"A\"\n\t# FLAT LABELS\n\tL_DESCRIPTION =\"DESC\"\n\tL_PARAMS = \"PARAMS\"\n\tL_BOT_INSTRUCTIONS = \"SYSTEM\"\n\tL_INTERACTION = \"INTERACTION\"\n\tL_INTERACTIONS = \"INTERACTIONS\"\n", "L_EXAMPLE = \"EXAMPLE\"\n\tL_EXAMPLES = \"EXAMPLES\"\n\tL_CONVERSATION = \"CONVERSATION-HISTORY\"\n\tL_THOUGHT = \"THOUGHT\"\n\tL_TOOL_LIST = \"TOOLS\"\n\tL_TOOL = \"TOOL\"\n\tL_TOOL_PICKER = \"PICK-TOOL\"\n\tL_QUESTION = \"QUESTION\"\n\tL_ANSWER = \"ANSWER\"\n\tL_ANSWER_DATA = \"DATA\"\n", "L_ANSWER_SUMMARY = \"SUMMARY\"\n\tL_SPLIT = \"SPLIT\"\n\tL_SPLIT_INSTRUCTIONS = \"INSTRUCTIONS\"\n\tSUBQ_SPLITTER = \"|||\"\n\t# Human-AI interaction\n\tL_INT_HUMAN = \"HUMAN-AI\"\n\t# AI-AI interaction\n\tL_INT_AI = \"AI-AI\"\n\tQUALITY = 0.2\n"]}
{"filename": "src/llm_vm/utils/tools.py", "chunked_list": ["\"\"\"\n\tThis file contains common tools (JSON API request objects) that are used\n\tfor all the agents to retrieve information for questions across various topics.\n\t\"\"\"\n\timport os\n\timport sys\n\t# Get the current file's directory to grab the python files with common functionality in the utils/ folder\n\tcurrent_dir = os.path.dirname(os.path.abspath(__file__))\n\tgrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))\n\tutils_dir = os.path.join(grandparent_dir, 'utils/')\n", "sys.path.append(utils_dir)\n\tfrom llm_vm.utils.keys import *\n\tfrom llm_vm.utils.labels import *\n\tfrom llm_vm.utils.typings_llm import *\n\tCUSTOM_TOOL_ANSWER_EMBEDDING = \"/answer_embedding\"\n\tdef __get_example_tools():\n\t    # wolfram\n\t    google_tool = {\n\t               'description': \"The tool returns the results of free-form queries similar to those used for wolfram alpha. This is useful for complicated math or live data retrieval.  Can be used to get the current date.\",\n\t               'dynamic_params': {\"q\": 'The natural language input query'},\n", "               'method': 'GET',\n\t               'args': {'url': \"https://www.googleapis.com/customsearch/v1\",\n\t                         'params': {'key': \"\",\n\t                                    'cx' : \"\",\n\t                                    'q': '{q}'}\n\t                        }\n\t               }\n\t    # geopy\n\t    directions_tool = {'description': \"Find the driving distance and time to travel between two cities.\",\n\t               'dynamic_params': {\"origins\": 'the origin city', \"destinations\": 'the destination city'},\n", "               'method': 'GET',\n\t               'args': {'url': \"https://maps.googleapis.com/maps/api/distancematrix/json\",\n\t                         'params': {'key': \"\",\n\t                                    'origins': '{origins}',\n\t                                    'destinations': '{destinations}'}\n\t                        }}\n\t    # weather\n\t    weather_tool = {'description': 'Find the weather at a location and returns it in celcius.',\n\t               'dynamic_params': {\"latitude\": 'latitude of as a float',\n\t                                  \"longitude\": 'the longitude as a float'},\n", "               'method': 'GET',\n\t               'args': {'url': \"https://api.open-meteo.com/v1/forecast\",\n\t                         'params': {'current_weather': 'true',\n\t                                    'latitude': '{latitude}',\n\t                                    'longitude': '{longitude}'\n\t                                    }}\n\t               }\n\t    return [google_tool, directions_tool, weather_tool]\n\tGENERIC_TOOLS = __get_example_tools()\n"]}
{"filename": "src/llm_vm/utils/keys.py", "chunked_list": ["\"\"\"\n\tAPI Key Definitions and Regex Pattern for Pure Interpolations.\n\tThis section defines and retrieves the API keys required for various services from the environment variables.\n\tIt also includes a regular expression pattern used to identify pure interpolations.\n\tAPI Key Definitions:\n\t- OPENAI_DEFAULT_KEY: The API key for OPENAI.\n\t- GOOGLE_MAPS_KEY: The API key for Google Maps.\n\t- SERPAPI_KEY: The API key for SERPAPI (Anarchy AI)\n\t- WOLFRAM_KEY: The API key for Wolfram.\n\tNote: The corresponding environment variables should be set before running\n", "any of the agents. Set the environment variables without quotes.\n\tE.g.: WOLFRAM_KEY=an-encypted-key\n\t\"\"\"\n\timport os\n\timport openai\n\t###\n\t# Use this regex to find if a variable is a pure interpolation.\n\t# For @example, { \"name\": \"{my_name}\" } is pure, since there's nothing else besides the interpolation\n\t# For @example, { \"Ticker\": \"TickerID={ticker_id}|historical=1\" } is NOT pure, since there is extra text\n\tDICT_KEY_REGEX_TO_FIND_PURE_INTERPOLATIONS = \"^\\{[a-zA-Z0-9\\.\\-_]+\\}$\"\n", "def set_api_key(key, key_type=\"OPENAI_API_KEY\"):\n\t    \"\"\"\n\t    Set the API key in the environment variable.\n\t    Parameters:\n\t    - key:  The API key to set. Defaults to OPENAI_DEFAULT_KEY.\n\t            There are four options:\n\t                - OPENAI_DEFAULT_KEY\n\t                - GOOGLE_MAPS_KEY\n\t                - GOOGLE_KEY\n\t                - GOOGLE_CX\n", "    - key_type: The type of API key. Defaults to \"OPENAI_API_KEY\".\n\t                There are four options:\n\t                - OPENAI_API_KEY\n\t                - GOOGLE_MAPS_KEY\n\t                - GOOGLE_KEY\n\t                - GOOGLE_CX\n\t    \"\"\"\n\t    if not os.getenv(key_type):\n\t        os.environ[key_type]=key\n\t        if key_type == \"OPENAI_API_KEY\":\n", "            openai.api_key=str(key)\n"]}
{"filename": "src/llm_vm/utils/print_types.py", "chunked_list": ["\"\"\"\n\tThis file contains a list of common functions across\n\tthe different agents that format and print output to the CLI.\n\t\"\"\"\n\tdef print_big(data, label = \"\"):\n\t    def do_format(x) -> str:\n\t        formatted_title = \"======#====== {:20} ======#======\\n\"\n\t        if len(x) >= 20:\n\t            return formatted_title.format(x)\n\t        else:\n", "            return formatted_title.format((int((20 - len(x)) / 2) * \" \") + x)\n\t    try:\n\t        if len(label):\n\t            print(do_format(str(label).upper()), data, flush=True)\n\t        else:\n\t            print(do_format(str(data)), flush=True)\n\t    except:\n\t        print(label, flush=True)\n"]}
{"filename": "src/llm_vm/server/main.py", "chunked_list": ["import flask\n\tfrom flask import request, jsonify\n\timport json\n\timport time\n\timport traceback\n\timport importlib\n\timport openai\n\timport os\n\timport hashlib\n\timport sys\n", "from flask_cors import CORS\n\tfrom contextlib import contextmanager\n\timport llm_vm.server.routes as routes\n\tfrom llm_vm.config import settings\n\tapp = flask.Flask(__name__)\n\tCORS(app)\n\tapp.config[\"DEBUG\"] = True\n\t# Register_blueprint from routes to load API\n\tapp.register_blueprint(routes.bp)\n\tdef server_entry_point(host = '127.0.0.1', port = 3002):\n", "    \"\"\"\n\t    This function launches the server with specified parameters\n\t     Parameters:\n\t         host (str): Network IP address\n\t         port (int): Port Number\n\t     Returns:\n\t         None\n\t     Example:\n\t         >>> server_entry_point(port = 3002)\n\t    \"\"\"\n", "    app.run(host = host,port = port)\n\tdef cli():\n\t    \"\"\"\n\t     This function is the entry point for the project and allows the user to specify an option network address and port number when launching from the cli\n\t     Parameters:\n\t         None\n\t     Returns:\n\t         None\n\t    \"\"\"\n\t    port = settings.port\n", "    if port > 65535:\n\t        print('Port defined out of range, defaulting to 3002')\n\t        port = 3002\n\t    server_entry_point(host = settings.host, port = port)\n\tif __name__ == '__main__':\n\t    cli()\n"]}
{"filename": "src/llm_vm/server/routes.py", "chunked_list": ["from flask import request, Blueprint\n\timport json\n\timport os\n\timport openai\n\tfrom llm_vm.agents.REBEL import agent\n\tfrom llm_vm.client import Client\n\tfrom llm_vm.config import settings\n\t# load optimizer for endpoint use\n\t# optimizer = LocalOptimizer(MIN_TRAIN_EXS=2,openai_key=None)\n\tclient = Client( big_model=settings.big_model, small_model=settings.small_model)\n", "print('optimizer loaded')\n\tbp = Blueprint('bp',__name__)\n\t@bp.route('/', methods=['GET'])\n\tdef home():\n\t    return '''home'''\n\t@bp.route('/v1/complete', methods=['POST'])\n\tdef optimizing_complete():\n\t    rebel_agent = agent.Agent(\"\", [], verbose=1)\n\t    data = json.loads(request.data)\n\t    static_context = data[\"context\"]\n", "    dynamic_prompt = data[\"prompt\"]\n\t    data_synthesis = False\n\t    finetune = False\n\t    use_rebel_agent = False\n\t    kwargs = {}\n\t    if \"openai_key\" not in data.keys():\n\t        return {\"status\":0, \"resp\":\"No OpenAI key provided\"}\n\t    if \"temperature\" in data.keys():\n\t        if type(data[\"temperature\"]) != float and type(data[\"temperature\"]) != int:\n\t            return {\"status\":0, \"resp\":\"Wrong Data Type for temperature\"}\n", "        else:\n\t            kwargs.update({\"temperature\":data[\"temperature\"]})\n\t    if \"stoptoken\" in data.keys():\n\t        if type(data[\"stoptoken\"]) != str and type(data[\"stoptoken\"]) != list:\n\t            # stop can either be a string or array of strings\n\t            return {\"status\":0, \"resp\":\"Wrong Data Type for stop\"}\n\t        elif type(data[\"stoptoken\"]) == list:\n\t            if len(data[\"stoptoken\"]) > 4:\n\t                return {\"status\":0, \"resp\":\"Too many stop tokens in array limit to 4 or less\"}\n\t            # check that every element in the list is a string\n", "            for j in data[\"stoptoken\"]:\n\t                if type(j) != str:\n\t                    return {\"status\":0, \"resp\":\"Wrong Data Type for stop\"}\n\t            kwargs.update({\"stop\": data[\"stoptoken\"]})\n\t        else:\n\t            kwargs.update({\"stop\":data[\"stoptoken\"]})\n\t    if \"data_synthesis\" in data.keys():\n\t        if type(data[\"data_synthesis\"])==bool:\n\t            data_synthesis = data[\"data_synthesis\"]\n\t        else:\n", "            return {\"status\":0, \"resp\":\"Wrong Data Type for data_synthesis\"}\n\t    if \"finetune\" in data.keys():\n\t        if type(data[\"finetune\"])==bool:\n\t            finetune = data[\"finetune\"]\n\t        else:\n\t            return {\"status\":0, \"resp\":\"Wrong Data Type for finetune\"}\n\t    if \"tools\" in data.keys():\n\t        if type(data[\"tools\"]) != list:\n\t            return {\"status\":0, \"resp\":\"Wrong data type for tools list\"}\n\t        else:\n", "            tools=[]\n\t            for i in data[\"tools\"]:\n\t                temp_tool_dict = {}\n\t                temp_args_dict = {}\n\t                temp_tool_dict.update({\"description\":i[\"description\"]})\n\t                temp_tool_dict.update({\"dynamic_params\":i[\"dynamic_params\"]})\n\t                temp_tool_dict.update({\"method\":i[\"method\"]})\n\t                temp_args_dict.update({\"url\":i[\"url\"]})\n\t                temp_args_dict.update({\"params\":{}})\n\t                for j in i[\"static_params\"].keys():\n", "                    temp_args_dict[\"params\"].update({j:i[\"static_params\"][j]})\n\t                for k in i[\"dynamic_params\"].keys():\n\t                    temp_args_dict[\"params\"].update({k:\"{\"+k+\"}\"})\n\t                temp_tool_dict.update({\"args\":temp_args_dict})\n\t                tools.append(temp_tool_dict)\n\t            rebel_agent.set_tools(tools)\n\t            use_rebel_agent = True\n\t    try:\n\t        openai.api_key = data[\"openai_key\"]\n\t    except:\n", "        return  {\"status\":0, \"resp\":\"Issue with OpenAI key\"}\n\t    # optimizer.openai_key = openai.api_key\n\t    agent.set_api_key(openai.api_key,\"OPENAI_API_KEY\")\n\t    try:\n\t        if not use_rebel_agent:\n\t            completion = client.complete(static_context,dynamic_prompt,openai_key=openai.api_key, data_synthesis=data_synthesis,finetune = finetune, **kwargs)\n\t        else:\n\t            completion = rebel_agent.run(static_context+dynamic_prompt,[])[0]\n\t    except Exception as e:\n\t        return {\"status\":0, \"resp\": str(e)}\n", "    # return {\"completion\":completion, \"status\": 200}\n\t    return completion\n"]}
{"filename": "src/llm_vm/completion/data_synthesis.py", "chunked_list": ["import json\n\timport sys\n\tfrom sentence_transformers import SentenceTransformer, util\n\timport openai\n\tclass DataSynthesis:\n\t     def __init__(self, variance, examples_to_generate):\n\t         self.variance = variance\n\t         self.examples_to_generate = examples_to_generate\n\t     def data_synthesis(self, optimizer, prompt, response, example_delim=\"<Datum-Separator/>\", openai_key=None, semantic_sim=True, **kwargs):\n\t        \"\"\"\n", "        This method generates QA pairs using the larger LLM to be used as training data for fine-tuning the smaller LLM.\n\t        Parameters\n\t        ----------\n\t        - optimizer (class): The Optimizer class to use for fine-tuning. Could be either LocalOptimizer or HostedOptimizer.\n\t        - prompt (str | list ): A question to be used as a one-shot QA example for the larger LLM prompt.\n\t        - response (str | list): A verified answer to the provided prompt question to be used in the one-shot QA example.\n\t        - example_delim (str): A unique XML tag used to separate the generated JSON examples. Default value is \"<Datum-Separator/>\".\n\t        - semantic_sim (bool): Option to use semantic similarity to filter duplicate generations. Default value is True.\n\t        - **kwargs: Additional keyword arguments to be passed into the `call_big` method.\n\t        Returns\n", "        ----------\n\t        - List: A list of tuples containing the QA pairs to be used for fine-tuning.\n\t        \"\"\"\n\t        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\t        final_prompt = None\n\t        if type(prompt) is str:\n\t            final_prompt = '{\"prompt\": \"' +prompt+'\"  , \"response\": \"' +response+'\" }'+example_delim\n\t        elif type(prompt) is list:\n\t            json_str = \"\"\n\t            for idx,p in enumerate(prompt):\n", "               example_str = '{\"prompt\": \"' + p +'\"  , \"response\": \"' + str(response[idx]) +'\" }'+example_delim+'\\n'\n\t               json_str += example_str\n\t            final_prompt = json_str\n\t        final_prompt = \"Generate 50 more jsons like the ones below. Use \"+example_delim+\" as a delimeter between JSONs.\\n\" + final_prompt\n\t        print(final_prompt)\n\t        data = None\n\t        openai.api_key=openai_key\n\t        response=openai.Completion.create(prompt=final_prompt,model=\"text-davinci-003\",max_tokens=1000,temperature=1).choices[0].text\n\t        datapoints = []\n\t        print(\"REPLY\"+response,file=sys.stderr)\n", "        split_response = response.split(sep=example_delim)\n\t        print(f\"Generated {len(split_response)}/{self.examples_to_generate} examples.\", file=sys.stderr )\n\t        if semantic_sim:\n\t            num_responses = len(split_response)\n\t            batch_responses = []\n\t            for idx in range(0, len(split_response), 10):\n\t                batch_str = None\n\t                if len(split_response) - idx >= 10:\n\t                    batch_str = \"\".join(split_response[idx : idx + 10])\n\t                else:\n", "                    batch_str = \"\".join(split_response[idx : len(split_response)])\n\t                batch_responses.append(batch_str)\n\t            embeddings = model.encode(batch_responses, convert_to_tensor=True)\n\t            cosine_scores = util.cos_sim(embeddings, embeddings)\n\t            duplicate_idx = []\n\t            for row_idx, row in enumerate(cosine_scores):\n\t                for i, score in enumerate(row):\n\t                    if score >= self.variance and score < 0.99:\n\t                        duplicate_idx.append((row_idx, i))\n\t            deleted_idx = []\n", "            for duplicate in duplicate_idx:\n\t                example_idx, match_idx = duplicate\n\t                if example_idx in deleted_idx or match_idx in deleted_idx:\n\t                    continue\n\t                else:\n\t                    split_idx = (example_idx + 1) * 10\n\t                    if split_idx < len(split_response):\n\t                        del split_response[split_idx - 10 : split_idx]\n\t                    else:\n\t                        del split_response[split_idx - 10 : len(split_response)]\n", "                    deleted_idx.append(example_idx)\n\t            print(\n\t                f\"Found {len(split_response)} valid examples. Removed {num_responses - len(split_response)} duplicate examples.\",\n\t                file=sys.stderr,\n\t            )\n\t        datum_failure = 0\n\t        bad_key_failure =0\n\t        resp_filter = {}\n\t        for d in split_response:\n\t            print(d)\n", "            try:\n\t                the_data = json.loads(d.replace(\"\\n\",\"\"))\n\t                the_tuple = (the_data[\"prompt\"],the_data[\"response\"])\n\t                if the_tuple in resp_filter:\n\t                    continue   # dont save a response if its already happened\n\t                resp_filter[the_tuple]=True  # for now we're treating the (Q,A) pair as a single value\n\t                datapoints.append(the_tuple)\n\t            except json.decoder.JSONDecodeError as err:\n\t                print(F'data_synthesis response parsing failed with: { err } \\nExpected a valid JSON Object but received {type(d)} of length {len(d)}',file=sys.stderr)\n\t                datum_failure+=1\n", "            except LookupError as err : # i have no evidence that this will happen\n\t                print(F'data_synthesis key lookup failed with: { err }',file=sys.stderr)\n\t                bad_key_failure +=1\n\t        print(F'Out of { len(split_response)} response objects, {datum_failure} were not valid json \\n\\\n\t            and {bad_key_failure} were missing a key',file=sys.stderr)\n\t        return datapoints\n"]}
{"filename": "src/llm_vm/completion/optimize.py", "chunked_list": ["import openai\n\timport traceback\n\timport threading\n\timport time\n\timport os\n\timport sys\n\timport signal\n\timport json\n\timport tempfile\n\timport abc\n", "import requests\n\timport hashlib\n\timport pickle\n\t#we need to package-ify so this works\n\timport llm_vm.completion.data_synthesis as data_synthesis\n\timport inspect\n\tjob_id = None # we want to be able to cancel a fine_tune if you kill the program\n\tdef exit_handler(signum, frame):\n\t    if (None != job_id):\n\t        print(\"cancelling fine-tune if applicable\")\n", "        openai.FineTune.cancel(id=job_id)\n\t    print(\"user interrupt, exiting\")\n\t    sys.exit()\n\tsignal.signal(signal.SIGINT, exit_handler)\n\tdef generate_hash(input_string):\n\t    sha256_hash = hashlib.sha256()\n\t    sha256_hash.update(str(input_string).encode('utf-8'))\n\t    return int(sha256_hash.hexdigest(), 16) % 10**18\n\tdef asyncStart(foo):\n\t    t = [None, None]\n", "    def new_thread():\n\t        t[0] = foo()\n\t    t[1] = threading.Thread(target=new_thread)\n\t    t[1].start()\n\t    return t\n\tdef asyncAwait(t):\n\t    t[1].join()\n\t    return t[0]\n\tclass local_ephemeral:\n\t    def __init__(self):\n", "        self.training_store = {}\n\t    def get_data(self, c_id):\n\t        self.init_if_null(c_id)\n\t        return self.training_store[c_id][\"data\"]\n\t    def load_data(self,file):\n\t        #using pickle files right now, in the future we will have to use databases\n\t        self.training_store = pickle.load(file)\n\t    def store_data(self,file):\n\t         #using pickle files right now, in the future we will have to use databases\n\t        pickle.dump(self.training_store,file)\n", "    def add_example(self, c_id, example):\n\t        self.init_if_null(c_id)\n\t        self.training_store[c_id][\"data\"] += [example]\n\t    def set_training_in_progress(self, c_id, is_training):\n\t        self.init_if_null(c_id)\n\t        self.training_store[c_id][\"is_training\"] = is_training\n\t    def get_training_in_progress_set_true(self, c_id):\n\t        self.init_if_null(c_id)\n\t        # TODO: this could cause a concurrency bug when distributed!\n\t        self.training_store[c_id]['lock'].acquire()\n", "        old_val = self.training_store[c_id][\"is_training\"]\n\t        if not old_val:\n\t            self.training_store[c_id][\"is_training\"] = True\n\t        self.training_store[c_id]['lock'].release()\n\t        return old_val\n\t    def set_model(self, c_id, model_id):\n\t        self.init_if_null(c_id)\n\t        self.training_store[c_id][\"model\"] = model_id\n\t    def get_model(self, c_id):\n\t        self.init_if_null(c_id)\n", "        return self.training_store[c_id][\"model\"]\n\t    def init_if_null(self, c_id):\n\t        if not c_id in self.training_store:\n\t            self.training_store[c_id] = { \"is_training\": False,\n\t                                          'lock' : threading.Lock(),\n\t                                          \"data\": [],\n\t                                          \"model\": None }\n\tclass Optimizer:\n\t    @abc.abstractmethod\n\t    def complete(self, stable_context, dynamic_prompt, **kwargs):\n", "        \"\"\"\n\t        Runs a completion using the string stable_context+dynamic_prompt.  Returns an optional training closure to use if the\n\t        caller decides that the completion was particularly good.\n\t        This method first checks if a model exists for the stable_context. If it does, it uses the model to complete the prompt.\n\t        It then checks if the number of training examples is less than the maximum allowable. If it is, or if a model wasn't\n\t        previously found, it retrieves the best completion for the prompt using a larger model, adds a new datapoint for training,\n\t        and potentially fine-tunes a new model using the updated data, storing the new model if successful.\n\t        The function returns the best completion (either generated by the stored model or the larger model).\n\t        This can not handle cases where either stable_context or dynamic_prompt are just whitespace!\n\t        Parameters:\n", "        ----------\n\t        - stable_context (str): Stable contextual data to use as a basis for training.\n\t        - dynamic_prompt (str): The dynamic data to generate a completion for and potentially add to training data.\n\t        - **kwargs: Additional arguments to be passed to the `call_small` and `call_big` methods.\n\t        Returns:\n\t        ----------\n\t        - completion (str): The best completion for the dynamic prompt, as generated by either the stored model or the larger model.\n\t        \"\"\"\n\tclass HostedOptimizer(Optimizer):\n\t    def __init__(self, anarchy_key, openai_key, MIN_TRAIN_EXS=20, MAX_TRAIN_EXS = 2000):\n", "        self.anarchy_key = anarchy_key\n\t        self.openai_key = openai_key\n\t        self.MIN_TRAIN_EXS = MIN_TRAIN_EXS\n\t        self.MAX_TRAIN_EXS = MAX_TRAIN_EXS\n\t    def complete(self, stable_context, dynamic_prompt, **kwargs):\n\t        \"\"\"\n\t        TODO: Runs the optimizing completion process on anarchy's hosted server with persistence.\n\t        Parameters:\n\t        ----------\n\t        - stable_context (str): Stable contextual data to use as a basis for training.\n", "        - dynamic_prompt (str): The dynamic data to generate a completion for and potentially add to training data.\n\t        - **kwargs: Additional arguments to be passed to the `call_small` and `call_big` methods.\n\t        Returns:\n\t        ----------\n\t        - completion (str): The best completion for the dynamic prompt, as generated by either the stored model or the larger model.\n\t        \"\"\"\n\t        url = \"https://api.chat.dev/completion/optimizing\"\n\t        payload = {**kwargs,\n\t                   'stable_context': stable_context,\n\t                   'dynamic_prompt': dynamic_prompt,\n", "                   'anarchy_key' : self.anarchy_key,\n\t                   'openai_key' : self.openai_key,\n\t                   'MIN_TRAIN_EXS' : self.MIN_TRAIN_EXS,\n\t                   'MAX_TRAIN_EXS' : self.MAX_TRAIN_EXS\n\t                   }\n\t        headers = {'Authorization': f'Bearer {self.anarchy_key}'}\n\t        print(\"Payload: \", payload)\n\t        try:\n\t            response = requests.post(url, json=payload, headers=headers)\n\t            response.raise_for_status()  # Raise an exception for 4XX and 5XX status codes\n", "            return response.json()['completion']\n\t        except requests.exceptions.RequestException as e:\n\t            print(\"Error occurred:\", e)\n\tclass LocalOptimizer(Optimizer):\n\t    def __init__(self, storage=local_ephemeral(), MIN_TRAIN_EXS = 1, MAX_TRAIN_EXS = 2000, call_small = None , call_big = None , big_model = None, small_model = None, openai_key=\"\"):\n\t        self.storage = storage\n\t        self.MIN_TRAIN_EXS = MIN_TRAIN_EXS\n\t        self.MAX_TRAIN_EXS = MAX_TRAIN_EXS\n\t        self.call_small = call_small\n\t        self.call_big = call_big\n", "        self.big_model = big_model\n\t        self.small_model = small_model\n\t        self.openai_key = openai_key\n\t        self.data_synthesizer = data_synthesis.DataSynthesis(0.87, 50)\n\t    def complete(self, stable_context, dynamic_prompt, data_synthesis = False, finetune = False, **kwargs):\n\t        openai.api_key = self.openai_key\n\t        completion, train = self.complete_delay_train(stable_context, dynamic_prompt, run_data_synthesis=data_synthesis, **kwargs)\n\t        print(finetune,flush=True)\n\t        if finetune:\n\t            train()\n", "        return completion\n\t    def complete_delay_train(self, stable_context, dynamic_prompt, run_data_synthesis = False, min_examples_for_synthesis = 1 ,c_id = None, **kwargs):\n\t        \"\"\"\n\t        Runs a completion using the string stable_context+dynamic_prompt.  Returns an optional training closure to use if the\n\t        caller decides that the completion was particularly good.\n\t        This method first checks if a model exists for the stable_context. If it does, it uses the model to complete the prompt.\n\t        It then checks if the number of training examples is less than the maximum allowable. If it is, or if a model wasn't\n\t        previously found, it retrieves the best completion for the prompt using a larger model, adds a new datapoint for training,\n\t        and potentially fine-tunes a new model using the updated data, storing the new model if successful.\n\t        The function returns the best completion (either generated by the stored model or the larger model), and a closure\n", "        function that encapsulates the fine-tuning process for potential execution at a later time.\n\t        Parameters:\n\t        ----------\n\t        - stable_context (str): Stable contextual data to use as a basis for training.\n\t        - dynamic_prompt (str): The dynamic data to generate a completion for and potentially add to training data.\n\t        - c_id (str): To be used if multiple users could share the same stable_contexts so that we don't leak data.  If its None, defaults to all possible context.\n\t        - **kwargs: Additional arguments to be passed to the `call_small` and `call_big` methods.\n\t        Returns:\n\t        ----------\n\t        - completion (str): The best completion for the dynamic prompt, as generated by either the stored model or the larger model.\n", "        - succeed_train (function): A closure function that encapsulates the fine-tuning process, ready for\n\t        execution at a later time.  If you pass it a completion, it will use that, otherwise it will use the completion from the \"best\" model.\n\t        \"\"\"\n\t        assert dynamic_prompt.strip() != \"\" or stable_context.strip() != \"\"\n\t        assert self.call_big is not None and self.call_small is not None and self.big_model is not None and self.small_model is not None\n\t        if stable_context.strip() == \"\" :\n\t            print(\"Running with an empty context\")\n\t        prompt = (stable_context + dynamic_prompt).strip()\n\t        c_id_repr = str({'stable_context' : stable_context,\n\t                    'args' : kwargs,\n", "                    'MIN_TRAIN_EXS' : self.MIN_TRAIN_EXS,\n\t                    'MAX_TRAIN_EXS' : self.MAX_TRAIN_EXS,\n\t                    'call_small' : str(self.small_model).split(' ')[0], # HACKS\n\t                    'call_big' : str(self.big_model).split(' ')[0],\n\t                    }) if c_id is None else c_id\n\t        c_id = generate_hash(c_id_repr)\n\t        completion = None\n\t        model = self.storage.get_model(c_id)\n\t        # this gives us the model_id\n\t        if model is not None:\n", "            print(\"Using the new model:\", model, flush=True)\n\t            completion = self.call_small(prompt = dynamic_prompt.strip(), model=model, **kwargs)\n\t        training_exs = self.storage.get_data(c_id)\n\t        best_completion_promise = None\n\t        succeed_train = None\n\t        if len(training_exs) < self.MAX_TRAIN_EXS:\n\t            def promiseCompletion():\n\t                best_completion = self.call_big(prompt, **kwargs)\n\t                def actual_train(use_completion = None):\n\t                    train_completion = best_completion if use_completion is None else use_completion\n", "                    new_datapoint = (dynamic_prompt.strip(), train_completion)\n\t                    self.storage.add_example(c_id, new_datapoint)\n\t                    if run_data_synthesis:\n\t                        if len(self.storage.get_data(c_id)) < min_examples_for_synthesis:\n\t                            print(\"Data synthesis is not available right now, need more examples in storage.\")\n\t                        else:\n\t                            for j in self.data_synthesizer.data_synthesis(self,prompt,best_completion,openai_key=self.openai_key, **kwargs):\n\t                                self.storage.add_example(c_id, j)\n\t                    training_exs = self.storage.get_data(c_id)\n\t                    print(training_exs)\n", "                    print(\"Considering Fine-tuning\", flush=True)\n\t                    if len(training_exs) >= self.MIN_TRAIN_EXS and not self.storage.get_training_in_progress_set_true(c_id):\n\t                        print(\"Actually Fine-tuning\", flush=True)\n\t                        print(\"Training examples:\",str(len(training_exs)))\n\t                        asyncStart(self.small_model.finetune(training_exs,self,c_id))\n\t                return (best_completion, actual_train)\n\t            best_completion_promise = asyncStart(promiseCompletion)\n\t            if completion is None:\n\t                # crazy story: succeed_train gets set before this anyway if it makes sense to set it!\n\t                completion, succeed_train = asyncAwait(best_completion_promise)\n", "            else:\n\t                _, succeed_train = asyncAwait(best_completion_promise)\n\t            print(completion)\n\t        def succeed_train_closure(use_completion = None):\n\t            def promise():\n\t                if succeed_train is not None:\n\t                    return succeed_train(use_completion)\n\t                if best_completion_promise is not None:\n\t                    try:\n\t                        return asyncAwait(best_completion)[1](use_completion)\n", "                    except:\n\t                        return\n\t            return asyncStart(promise)\n\t        return completion, succeed_train_closure\n\tdef create_jsonl_file(data_list):\n\t    out = tempfile.TemporaryFile('w+')\n\t    for a,b in data_list:\n\t        out.write(json.dumps({'prompt': a, 'completion': b}) + \"\\n\")\n\t    out.seek(0)\n\t    return out\n"]}
