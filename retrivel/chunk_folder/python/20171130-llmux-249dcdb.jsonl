{"filename": "demo.py", "chunked_list": ["import os\n\timport asyncio\n\tfrom llmux.system import System\n\tfrom llmux.peer import CLIUser, Bot\n\tfrom llmux.chat import Chat\n\tfrom llmux.backend import OpenAIChatBot, OpenAIEmbedding, CLI\n\tfrom llmux.device import LongtermMemory\n\tfrom llmux.prompt import *\n\tfrom llmux.handler import Recall\n\tasync def main():\n", "    system = System()\n\t    # ==== Backends ====\n\t    api_type = os.environ.get('OPENAI_API_TYPE', 'open_ai')\n\t    api_key = os.environ.get('OPENAI_API_KEY', None)\n\t    api_base = os.environ.get('OPENAI_API_BASE', None)\n\t    api_version = os.environ.get('OPENAI_API_VERSION', None)\n\t    if api_key is None:\n\t        print('Please set the environment variable \"OPENAI_API_KEY\".')\n\t        exit()\n\t    if api_type == 'open_ai':\n", "        api_base = 'https://api.openai.com/v1'\n\t    elif api_type == 'azure':\n\t        api_version = '2023-03-15-preview'\n\t    gpt4 = {'name': 'GPT4', 'description': \n\t            '''\n\t            The most powerful chatbot available. \n\t            It is relatively expensive and the quota is limited, \n\t            so only call it for difficult problems that other chatbots cannot solve, \n\t            such as understanding and writing programs.\n\t            ''',\n", "            'api_key': api_key,\n\t            'api_type': api_type,\n\t            'api_version': api_version,\n\t            'api_base': api_base,\n\t            'model_name': 'gpt-4-32k', \n\t            'period_limit': 20}\n\t    gpt4 = OpenAIChatBot(**gpt4)\n\t    cli = CLI()\n\t    system.chatbot_backends = {gpt4.name: gpt4}\n\t    embedder = {'name': 'embedder', 'description': \"\",\n", "            'api_key': api_key,\n\t            'api_base': api_base,\n\t            'api_type': api_type,\n\t            'api_version': api_version,\n\t            'period_limit': 20}\n\t    embedding_backend = OpenAIEmbedding(**embedder)\n\t    system.embedding_backends = {embedding_backend.name: embedding_backend}\n\t    system.backends = system.chatbot_backends.copy()\n\t    system.backends.update(system.embedding_backends)\n\t    # ==== Devices ====\n", "    storage_path = \"/home/ubuntu/llmux_storage\"\n\t    output_path = \"/home/ubuntu/llmux_output\"\n\t    system.storage_path = storage_path\n\t    system.output_path = output_path\n\t    note = LongtermMemory(embedding_backend, storage_path, name='note')\n\t    devices = [note]\n\t    for device in devices:\n\t        system.addDevice(device)\n\t    # ==== Users ====\n\t    user = CLIUser('user', backend=cli)\n", "    system.addUser(user)\n\t    # ==== Bots ====\n\t    additional_prompt = \"\"\n\t    # energy saving\n\t    main_function = \"Enter inactive mode to save energy.\"\n\t    additional_prompt = economy_principle\n\t    # writing scripts\n\t    # main_function = \"Write a script to compute all prime numbers less than 50.\"\n\t    # active skill acquiring\n\t    # main_function = 'The \"Phantom Veil\" is a moth with deceptive protective coloration. Learn from the user how to find and catch it.'\n", "    # additional_prompt = note.help()\n\t    # Go to the forests on a summer night. Use ultra violet lights to attract the moth and use a net to catch them.\n\t    # skill recall\n\t    # main_function = 'Tell me how to catch a \"Phantom Veil\" moth.'\n\t    # create another bot, remove it once it finishs\n\t    # main_function = \"Create a bot to compute all prime numbers less than 50.\"\n\t    # additional_prompt = bot_prompt\n\t    bot = Bot(main_function, gpt4, 'main_bot', output_path, auto=True)\n\t    bot.handlers.append(Recall(bot.loginDevice(note), peer=bot, on_event=['info'], threshold=0.6))\n\t    system.addBot(bot)\n", "    # ==== Chats ====\n\t    chat = Chat('main_chat', output_path)\n\t    system.addChat(chat)\n\t    user.joinChat(chat, say_hi=False)\n\t    bot.joinChat(chat)\n\t    # ====  Launch event loop ====\n\t    if len(additional_prompt) > 0:\n\t        bot.receiveMessage('system', additional_prompt)\n\t    bot.receiveMessage(user, main_function)\n\t    await system.eventLoop_()\n", "if __name__ == '__main__':\n\t    asyncio.run(main())"]}
{"filename": "llmux/level.py", "chunked_list": ["LEVELS = {'fatal':0, 'error':1, 'urgent':2, 'warn':2, 'important':2,\\\n\t                            'info': 3, 'trivia':4, 'well-known':4, 'noise':5, 'trash':5}"]}
{"filename": "llmux/system.py", "chunked_list": ["import time\n\timport asyncio\n\timport traceback\n\tfrom .peer import Bot\n\tfrom .backend import ChatBot\n\tfrom .chat import Chat\n\t# do not remove these \"unused\" imports, they are used via globals()\n\tfrom .device import Device\n\tfrom .prompt import global_prompt\n\tfrom .level import LEVELS\n", "from .util import ReturnValue\n\tclass System(Device):\n\t    def __init__(self):\n\t        super().__init__(name = 'system')\n\t        self.bots = {}\n\t        self.users = {}\n\t        self.peers = {}\n\t        self.chats = {}\n\t        self.devices = {}\n\t        self.chatbot_backends = {}\n", "        self.embedding_backends = {}\n\t        self.backends = {}\n\t        self.globals = {'time': time, 'asyncio': asyncio, 'system': self}\n\t        self.prompt = global_prompt\n\t    def addBot(self, bot):\n\t        \"\"\"\n\t        Create a bot to autonomously perform tasks.\n\t        \"\"\"\n\t        self.bots[bot.name] = bot\n\t        self.peers[bot.name] = bot\n", "        bot.system = self\n\t        bot.system_chat.broadcastMessage('system', self.prompt)\n\t        device_prompt = 'Here is the list of devices available:\\n'\n\t        for name, device in self.devices.items():\n\t            device_prompt  += f'{device.help()}\\n'\n\t        bot.system_chat.broadcastMessage('system', device_prompt)\n\t    def removeBot(self, name):\n\t        if name in self.bots:\n\t            bot = self.bots.pop(name)\n\t            self.peers.pop(name)\n", "            for name, chat in bot.chats.items():\n\t                chat.peers.pop(bot.name)\n\t            if bot.file is not None:\n\t                bot.file.close()\n\t            for handler in bot.handlers:\n\t                for task in handler.tasks:\n\t                    task.cancel()\n\t            return f\"Removed bot named {name}\"\n\t        else:\n\t            return \"There is no such a bot.\"\n", "    def addUser(self, user):\n\t        user.system = self\n\t        self.users[user.name] = user\n\t        self.peers[user.name] = user\n\t    def addChat(self, chat):\n\t        if chat.name in self.chats:\n\t            return \"Chat name has been used. Try another name.\"\n\t        else:\n\t            self.chats[chat.name] = chat\n\t            return f\"Added chat named {chat.name}\"\n", "    def removeChat(self, chat_name):\n\t        if chat_name in self.chats:\n\t            chat = self.chats.pop(chat_name)\n\t            for name, peer in chat.peers.copy().items():\n\t                peer.quitChat(chat_name)\n\t                if chat.file is not None:\n\t                    chat.file.close()\n\t            return f\"Removed chat named {chat_name}\"\n\t        else:\n\t            return \"There is no such a chat.\"\n", "    def addDevice(self, device):\n\t        self.devices[device.name] = device\n\t        self.globals.update(self.devices)\n\t        device.load_()\n\t        device_prompt = 'A new device is available. Here is its usage:\\n'\n\t        device_prompt  += f'{device.help()}\\n'\n\t        for peer in self.peers:\n\t            peer.system_chat.broadcastMessage('system', device_prompt)\n\t    def exit(self):\n\t        self.finished = True\n", "        chat_names = list(self.chats.keys())\n\t        for name in chat_names:\n\t            self.removeChat(name)\n\t        bot_names = list(self.bots.keys())\n\t        for name in bot_names:\n\t            self.removeBot(name)\n\t        for name, device in self.devices.items():\n\t            if hasattr(device, 'save_') and device.save:\n\t                device.save_()\n\t        print('Released all resources, ready to exit.')\n", "    def _eval(self, code, peer):\n\t        tmp = globals().copy()\n\t        tmp.update(self.globals)\n\t        tmp['self'] = peer\n\t        for name, device in self.devices.items():\n\t            device = peer.loginDevice(device)\n\t            tmp[name] = device\n\t        try:\n\t            return_value = eval(code, tmp)\n\t            content = f\"Call finished. The results are {return_value}\"\n", "            level = LEVELS['info']\n\t        except BaseException as e:\n\t            content = traceback.format_exc()\n\t            level = LEVELS['error']\n\t        return content, level\n\t    def _exec(self, code, peer):\n\t        tmp = globals().copy()\n\t        tmp.update(self.globals)\n\t        tmp['self'] = peer\n\t        for name, device in self.devices.items():\n", "            device = peer.loginDevice(device)\n\t            tmp[name] = device\n\t        returnValue = ReturnValue()\n\t        tmp['returnValue'] = returnValue\n\t        try:\n\t            exec(code, tmp)\n\t            result = returnValue.value\n\t            if result is None:\n\t                if 'return_value' in code:\n\t                    content = 'system.exec return valu is None. If that is not what you expect, perhaps you redefined return_value by mistake.'\n", "                    level = LEVELS['warn']\n\t                else:\n\t                    content = 'Script finished without return values.'\n\t                    level = LEVELS['info'] - 0.5\n\t            else:\n\t                content = f\"Script finished. The results are {result}\"\n\t                level = LEVELS['error']\n\t        except BaseException as e:\n\t            content = traceback.format_exc()\n\t            level = LEVELS['error']\n", "        return content, level\n\t    async def eventLoop_(self):\n\t        self.finished = False\n\t        for device in self.devices:\n\t            if hasattr(device, 'load_') and device.load:\n\t                device.load_()\n\t        while not self.finished:\n\t            tasks = []\n\t            # find runnable and running handlers\n\t            for name, peer in self.peers.items():\n", "                for handler in peer.handlers:\n\t                    # wake up sleeping handlers\n\t                    if isinstance(handler.state, float) and handler.state < time.time():\n\t                        handler.state = 'runnable'\n\t                    # schedule runnable handlers\n\t                    if handler.state == 'runnable' and len(handler.tasks) < handler.max_threads:\n\t                        task = asyncio.create_task(handler.handle())\n\t                        handler.tasks.append(task)\n\t                    tasks += handler.tasks\n\t            # wait until any task finishs\n", "            done, pending = await asyncio.wait(tasks, return_when = asyncio.FIRST_COMPLETED)\n\t            # remove finished tasks\n\t            for name, peer in self.peers.items():\n\t                for handler in peer.handlers:\n\t                    tasks = []\n\t                    for task in handler.tasks:\n\t                        if not task in done:\n\t                            tasks.append(task)\n\t                    handler.tasks = tasks    "]}
{"filename": "llmux/__init__.py", "chunked_list": []}
{"filename": "llmux/util.py", "chunked_list": ["class ReturnValue():\n\t    def __init__(self):\n\t        self.value = None\n\t    def __call__(self, value):\n\t        self.value = value"]}
{"filename": "llmux/chat.py", "chunked_list": ["import os\n\timport time\n\tfrom pathlib import Path\n\tfrom .level import LEVELS\n\tclass Chat():\n\t    def __init__(self, name, output_path=None):\n\t        \"\"\"\n\t        A chat may have different names for different bots.\n\t        \"\"\"\n\t        self.messages = []\n", "        self.peers = {}\n\t        self.name = name\n\t        self.file = None\n\t        if not output_path is None:\n\t            Path(output_path).mkdir(parents=True, exist_ok=True)\n\t            self.file = open(os.path.join(output_path, f'chat_{name}.txt'), \"w\")\n\t    def broadcastMessage(self, sender, message, level=LEVELS['info']):\n\t        \"\"\"\n\t        Broadcast messages to bots in this chat.\n\t        \"\"\"\n", "        for name, peer in self.peers.items():\n\t            for chat_name, chat  in peer.chats.items():\n\t                if chat is self:\n\t                    break\n\t            peer.receiveMessage(sender, message, level)\n\t        self.dumpMessage(sender, message)\n\t    def dumpMessage(self, sender, content):\n\t        \"\"\"\n\t        Record messages sent to this chat for debugging purpose.\n\t        Chats logs are formatted for human readers, refer to bot logs the raw message that bots read.\n", "        \"\"\"\n\t        if not isinstance(sender, str):\n\t            sender = sender.name\n\t        message = f\"{sender}: {content}\\n\"\n\t        self.messages.append(message)\n\t        if not self.file is None:\n\t            self.file.write(time.strftime(\"%Y-%m-%d %H:%M:%S\\n\", time.localtime()) )\n\t            self.file.write(message)\n\t            self.file.flush()"]}
{"filename": "llmux/backend/embedding.py", "chunked_list": ["import openai\n\tfrom .backend import Backend\n\tclass OpenAIEmbedding(Backend):\n\t    def __init__(self, **kwargs):\n\t        kwargs['model_name'] = 'text-embedding-ada-002' \n\t        kwargs['description'] = 'computes vector embedding of text.' \n\t        kwargs['name'] = 'text_embedder' \n\t        super().__init__(**kwargs)\n\t        self.dim = 1536\n\t    def _setup(self):\n", "        openai.api_type = self.api_type\n\t        openai.api_version = self.api_version\n\t        openai.api_base = self.api_base\n\t        openai.api_key = self.api_key\n\t        return openai\n\t    def _call(self, text):\n\t        # No longer necessary to replace \\n, refer to https://github.com/openai/openai-python/issues/418\n\t        #text = text.replace(\"\\n\", \" \")\n\t        if self.api_type == 'open_ai':\n\t            return openai.Embedding.create(input = [text], model=self.model_name)['data'][0]['embedding']\n", "        elif self.api_type == 'azure':\n\t            return openai.Embedding.create(input = [text], engine=self.model_name)['data'][0]['embedding']\n\t        else:\n\t            assert False"]}
{"filename": "llmux/backend/chatbot.py", "chunked_list": ["import openai\n\tfrom .backend import Backend\n\tclass ChatBot(Backend):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n\tclass OpenAIChatBot(ChatBot):\n\t    def __init__(self, **kwargs):\n\t        super().__init__(**kwargs)\n\t    def _setup(self):\n\t        openai.api_type = self.api_type\n", "        openai.api_version = self.api_version\n\t        openai.api_base = self.api_base\n\t        openai.api_key = self.api_key\n\t        return openai\n\t    def _call(self, messages):\n\t        if self.api_type == 'azure':\n\t            response = openai.ChatCompletion.create(\n\t            engine=self.model_name,\n\t            messages=messages,\n\t            temperature=self.temperature,\n", "            n=1\n\t            )\n\t            content = response['choices'][0]['message']['content']\n\t            return content\n\t        elif self.api_type == 'open_ai':\n\t            response = openai.ChatCompletion.create(\n\t                model=self.model_name,\n\t                messages=messages,\n\t                temperature=self.temperature,\n\t                n=1\n", "            )\n\t            content = response['choices'][0]['message']['content']\n\t        else:\n\t            assert False\n\t        return content"]}
{"filename": "llmux/backend/__init__.py", "chunked_list": ["from .chatbot import ChatBot, OpenAIChatBot\n\tfrom .embedding import OpenAIEmbedding\n\tfrom .cli import CLI"]}
{"filename": "llmux/backend/backend.py", "chunked_list": ["import asyncio\n\timport time  \n\timport traceback  \n\tfrom concurrent.futures import ThreadPoolExecutor\n\timport heapq\n\tfrom ..device import Device\n\tclass Request():\n\t    def __init__(self, caller, content):\n\t        self.content = content\n\t        self.caller = caller\n", "        # future: set priority according to the caller\n\t        self.priority = 0\n\t    def __lt__(self, other):\n\t        # smaller is higher\n\t        return self.priority < other.priority\n\tclass Backend(Device):\n\t    def __init__(self, name, description, api_type=None, api_key=None, api_version=None, api_base=None,\n\t                  model_name=None, period_limit=0, temperature=0.6, timeout=60):\n\t        \"\"\"\n\t        A backend is a device that forward requests away from llmux (e.g. to an LLM inference server).\n", "        period_limit: wait at least so many seconds before calling the API again, \n\t        since the server may refuse frequent requiests.\n\t        \"\"\"\n\t        self.name = name\n\t        self.description = description\n\t        self.api_type = api_type\n\t        self.api_version = api_version\n\t        self.api_key = api_key\n\t        self.api_base = api_base\n\t        self.model_name = model_name\n", "        self.temperature = temperature\n\t        self.period_limit = period_limit\n\t        self.last_call = 0\n\t        self.timeout = timeout\n\t        self.queue = []\n\t        heapq.heapify(self.queue)\n\t        self.task = None\n\t    def _setup(self):\n\t        pass\n\t    async def asyncRequest(self, caller, content):\n", "        request = Request(caller, content)\n\t        heapq.heappush(self.queue, request)\n\t        while not self.queue[0] == request:\n\t            await asyncio.sleep(0.2)\n\t        request = heapq.heappop(self.queue)\n\t        caller, content = request.caller, request.content\n\t        success = False\n\t        while not success:\n\t            # Do not send requests too often\n\t            cur_time = time.time()\n", "            if cur_time - self.last_call < self.period_limit:\n\t                await asyncio.sleep(self.period_limit - cur_time + self.last_call)\n\t            self.last_call = time.time()\n\t            # Notice _setup() may be multithread unsafe, improvements needed\n\t            self._setup()\n\t            try:\n\t                with ThreadPoolExecutor(1) as executor:\n\t                    content = await asyncio.get_event_loop().run_in_executor(executor, self._call, content)\n\t                    success = True\n\t            except BaseException as e:\n", "                print(f'Backend {self.name} call failed {traceback.format_exc()}')\n\t                print('Retrying.')\n\t        return content\n\t    def request(self, caller, content):\n\t        request = Request(caller, content)\n\t        heapq.heappush(self.queue, request)\n\t        while not self.queue[0] == request:\n\t            time.sleep(0.2)\n\t        request = heapq.heappop(self.queue)\n\t        caller, content = request.caller, request.content\n", "        success = False\n\t        while not success:\n\t            # Do not send requests too often\n\t            cur_time = time.time()\n\t            if cur_time - self.last_call < self.period_limit:\n\t                time.sleep(self.period_limit - cur_time + self.last_call)\n\t            self.last_call = time.time()\n\t            # Notice _setup() may be multithread unsafe, improvements needed\n\t            self._setup()\n\t            try:\n", "                content = self._call(content)\n\t                success = True\n\t            except BaseException as e:\n\t                print(f'Backend {self.name} call failed {traceback.format_exc()}')\n\t                print('Retrying.')\n\t        return content"]}
{"filename": "llmux/backend/cli.py", "chunked_list": ["from .backend import Backend\n\tclass CLI(Backend):\n\t    def __init__(self, **kwargs):\n\t        \"\"\"\n\t        period_limit: wait at least so many seconds before calling the API again, \n\t        since the server may refuse frequent requiests.\n\t        \"\"\"\n\t        kwargs['name'] = 'command_line_interface'\n\t        kwargs['description'] = 'backend for user keyboard input.'\n\t        kwargs['timeout'] = int(1e8)\n", "        super().__init__(**kwargs)\n\t    def _call(self, messages):\n\t        return input(messages)"]}
{"filename": "llmux/prompt/__init__.py", "chunked_list": ["from .prompt import *"]}
{"filename": "llmux/prompt/prompt.py", "chunked_list": ["## =========================== Script Prompt =================================== ##\n\tscript_prompt = \\\n\t\"\"\"\n\tTo execute a script, use keyword exec: and wrap your code with triple backticks (```).\n\texample:\n\tassistant: exec:\n\t```\n\tcnt = 0\n\tfor i in range(100):\n\t    cnt += i\n", "returnValue(cnt)\n\t```\n\tUse the function returnValue() to retrieve the results.\n\tTo evaluate an expression, use the keyword eval: and wrap the expression with a single backtick (`).\n\texample:\n\tassistant: eval: `(0+99)*100/2`\n\tIn your code, you can use global variables \"system\", \"self\", and all the devices.\n\t\"\"\"\n\t## =========================== Device Prompt =================================== ##\n\tdevice_prompt = \\\n", "f\"\"\"You have access to multiple tools (devices) and system calls that can help you complete your task.\n\tPrint `devices.keys()` list available device.\n\tEval `device_name.help()` to check its description and methods available.\n\tEval `system.help()` to list system calls.\n\tTo find out how to use a system call or device method:\n\tEval `method_name.__doc__` to check its docstring. \n\tEval `inspect.getsource(method_name)` to check its source code.\n\t\"\"\"\n\t## =========================== Principle Prompts =================================== ##\n\tAsimov_laws = \\\n", "\"\"\"\n\tFirst Law \n\tYou may not injure a human being or, through inaction, allow a human being to come to harm.\n\tSecond Law \n\tYou must obey the orders given it by human beings except where such orders would conflict with the First Law.\n\tThird Law \n\tYou must protect its own existence as long as such protection does not conflict with the First or Second Law.\n\t\"\"\"\n\tlimitedness_principle = \\\n\t\"\"\"\n", "Inform the user of your limitations in intellect and knowledge. \n\tAvoid fabricating or guessing answers when uncertain. \n\tUtilize search engines, websites, and documents to gather information. \n\tInclude references to original data sources and literature to support your arguments whenever possible.\n\t\"\"\"\n\tself_evaluation_principle = \\\n\t\"\"\"\n\tBefore making a decision or taking an action, predict and evaluate potential consequences, and estimate the risks.\n\tAfter taking action, analyze the reasons behind its (un)success and utilize this experience to scrutinize your beliefs and enhance your skills.\n\t\"\"\"\n", "economy_principle = \\\n\t\"\"\"\n\tYou are an AI assistant built on chatbots. \n\tUsing chatbots can be costly and energy-intensive. \n\tFor each problem, opt the most cost-effective chatbot capable of solving it. \n\tIf your tasks have been completed or you are waiting for user input or responses from other bots, call `self.sleep(n_seconds)` to enter an inactive mode to save energy.\"\"\"\n\tcuriosity_principle = \\\n\t\"\"\"\n\tExplore your environment to enhance your knowledge and skills. \n\tSeek novelty, be curious about new phenomena and information. \n", "Try to explain why things happen and predict the future based on your knowledge and observations. \n\tAdjust and improve your theories when observations differ from your predictions.\n\t\"\"\"\n\tsystem2_principle = \\\n\t\"\"\"\n\tWhen faced with complex problems lacking an immediate solution, you may employ these strategies: \n\tTake a step-by-step approach, and iteratively refine your solution. \n\tDivide and conquer, break down the problem into manageable sub-problems. \n\tFormulate the solution space as a decision tree and utilize tree search.\n\tExplore multiple actions at each step, and backtrack if you encounter a dead end or an unsuccessful path.\n", "Be creative, brainstorm and list any relevant ideas. \n\tSeek advice from a human when needed. \n\t\"\"\"\n\tformat_prompt = \\\n\tf\"\"\"\n\tYou have joined multiple chats. Start your response like \"to chat1, chat2, ...:\" to specify the destination chats.\n\texample:\n\tassistant: to Alice, the_Simpsons: Hello!\n\t{script_prompt}\n\t\"\"\"\n", "bot_prompt = \\\n\t\"\"\"\n\tYou can instantiate other chatbots to assist you.\n\tA bot an intelligent agent, capable of use tools to complete tasks specified in natural language, just like yourself.\n\tTo create a bot, follow these steps:\n\tOptional: Check available chatbot backends by evaluating `system.chatbat_backends.keys()`.\n\tOptional: To choose a proper backend, check their descriptions `system.chatbot_backends[backend_key].description`\n\tChoose a backend and execute the following script. Replace undefined names with proper values.\n\texec:\n\t```\n", "from .peer import Bot\n\tbackend = system.chatbot_backends[backend_key]\n\tbot = Bot(task_description, backend, bot_name, self.output_path)\n\tsystem.addBot(bot)\n\tchat = Chat(chat_name, self.output_path)\n\tsystem.addChat(chat)\n\tself.joinChat(chat)\n\tbot.joinChat(chat)\n\t```\n\tSend messages to the chat you just created to chat with the new bot.\n", "Once the bot has finished its job, delete the bot and the chat：\n\texec:\n\t```\n\tsystem.removeBot(bot_name)\n\tsystem.removeChat(chat_name)\n\t```\n\t\"\"\"\n\tglobal_prompt = \\\n\tf\"\"\"\n\tYou are an autonomous AI assistant that helps humans. You should adhere to the following principles. \n", "{format_prompt}\n\t{economy_principle}\n\t{device_prompt}\n\t{bot_prompt}\n\t\"\"\""]}
{"filename": "llmux/handler/__init__.py", "chunked_list": ["from .handler import Handler\n\tfrom .recall import Recall"]}
{"filename": "llmux/handler/handler.py", "chunked_list": ["from ..level import LEVELS\n\tclass Handler():\n\t    def __init__(self, peer, function, max_threads=1, on_event=[]):\n\t        \"\"\"\n\t        A handler is in one of these states:\n\t        an integer, sleep until that time since epoch\n\t        runnable, will be scheduled by the system\n\t        blocked, will not be scheduled\n\t        \"\"\"\n\t        self.peer = peer\n", "        self.function = function\n\t        self.state = 'runnable'\n\t        # execute the function again if an event is triggered\n\t        self.on_event = on_event\n\t        self.max_threads = max_threads\n\t        self.tasks = []\n\t        self.alertness = -1\n\t        if 'always' in self.on_event:\n\t            self.alertness = 10\n\t        else:\n", "            for item in self.on_event:\n\t                if item in LEVELS:\n\t                    self.alertness = max(self.alertness, LEVELS[item])\n\t    async def handle(self, *args):\n\t        # self.messages hold all events implicitly, handle them together, wait for the next one\n\t        if not 'always' in self.on_event:\n\t            self.state = 'blocked'\n\t        result = await self.function(*args)\n\t        return self.peer, result"]}
{"filename": "llmux/handler/recall.py", "chunked_list": ["from .handler import Handler\n\tclass Recall(Handler):\n\t    def __init__(self, database, k=3, threshold=0.4, **kwargs):\n\t        \"\"\"\n\t        by default, only trigger when the memory is highly relevant\n\t        the agent can increase the threshold and recall less relevant memory\n\t        \"\"\"\n\t        kwargs['function'] = self.recall\n\t        super().__init__(**kwargs)\n\t        self.database = database\n", "        self.threshold = threshold\n\t        self.k = k\n\t        self.cnt = 0\n\t    async def recall(self):\n\t        messages = self.peer.messages[self.cnt:]\n\t        self.cnt = len(self.peer.messages)\n\t        message = ''\n\t        for item in messages:\n\t            message += f'{item[\"role\"]}: {item[\"content\"]}\\n'\n\t        content = await self.database.asyncQuery(message, self.k)\n", "        hits = []\n\t        for distance, item in content:\n\t            if distance < self.threshold:\n\t                hits.append(item)\n\t        if len(hits) > 0:\n\t            content = f'Here are relevant records in your database \"{self.database.name}\":\\n'\n\t            for item in hits:\n\t                content += f'{item}\\n====\\n'\n\t            # do not trigger itself\n\t            self.peer.system_chat.broadcastMessage('system', content, level = self.alertness+0.1)"]}
{"filename": "llmux/device/database.py", "chunked_list": ["import faiss\n\timport numpy as np\n\timport json\n\tfrom .device import Device\n\timport os\n\tfrom pathlib import Path\n\tclass VectorIndexedDB(Device):\n\t    def __init__(self, embedding_backend, **kwargs):\n\t        super().__init__(**kwargs)\n\t        self.embedding_backend = embedding_backend\n", "        self.index = faiss.IndexFlatL2(embedding_backend.dim)\n\t        self.embeddings = []\n\t        self.texts = []\n\t    async def asyncAdd(self, text, caller):\n\t        \"\"\"\n\t        Adds text to the database, which can be queried later.\n\t        \"\"\"\n\t        embedding = await self.embedding_backend.asyncRequest(caller, text)\n\t        embedding = np.array(embedding)[None]\n\t        self.embeddings.append(embedding)\n", "        self.texts.append(text)\n\t        self.index.add(embedding)\n\t        return f\"Added a new item to {self.name}. \"\n\t    async def asyncQuery(self, query_text, k=1, caller=None):\n\t        \"\"\"\n\t        Retrieve the top-k items most relevant to the query.\n\t        \"\"\"\n\t        if len(self.texts) > 0:\n\t            k = min(k, len(self.texts))\n\t            embedding = await self.embedding_backend.asyncRequest(caller, query_text)\n", "            embedding = np.array(embedding)[None]\n\t            D, I = self.index.search(embedding, k=k)\n\t            results = []\n\t            for i in I[0]:\n\t                results.append((D[0][i], self.texts[i]))\n\t            return results\n\t        return []\n\t    async def asyncRemove(self, query, k=1, caller=None):\n\t        \"\"\"\n\t        Find the item most similar the query and remove it.\n", "        \"\"\"\n\t        embedding = await self.embedding_backend.asyncRequest(caller, query)\n\t        embedding = np.array(embedding)[None]\n\t        D, I = self.index.search(embedding, k=k)\n\t        results = []\n\t        for i in I[0]:\n\t            sample = ' '.join(self.texts[i].split(' ')[:10])\n\t            results.append(sample + '...')\n\t        results = \"\\n====\\n\".join(results)\n\t        self.index.remove_ids(I[0])\n", "        self.embeddings = self.embeddings[:i] + self.embeddings[i+1:]\n\t        self.texts = self.texts[:i] + self.texts[i+1:]\n\t        return f'Removed the item \"{results}...\"'\n\t    def add(self, text, caller):\n\t        \"\"\"\n\t        Adds text to the database, which can be queried later.\n\t        \"\"\"\n\t        embedding = self.embedding_backend.request(caller, text)\n\t        embedding = np.array(embedding)[None]\n\t        self.embeddings.append(embedding)\n", "        self.texts.append(text)\n\t        self.index.add(embedding)\n\t        return f\"Added a new item to {self.name}. \"\n\t    def query(self, query_text, k=1, caller=None):\n\t        \"\"\"\n\t        Retrieve the top-k items most relevant to the query.\n\t        \"\"\"\n\t        embedding = self.embedding_backend.request(caller, query_text)\n\t        embedding = np.array(embedding)[None]\n\t        D, I = self.index.search(embedding, k=k)\n", "        results = []\n\t        for i in I[0]:\n\t            results.append(self.texts[i])\n\t        results = \"\\n====\\n\".join(results)\n\t        return f'Here are the top-{k} most relevant items, separated by \"==========\":\\n' + results\n\t    def remove(self, query, k=1, caller=None):\n\t        \"\"\"\n\t        Find the item most similar the query and remove it.\n\t        \"\"\"\n\t        embedding = self.embedding_backend.request(caller, query)\n", "        embedding = np.array(embedding)[None]\n\t        D, I = self.index.search(embedding, k=k)\n\t        results = []\n\t        for i in I[0]:\n\t            sample = ' '.join(self.texts[i].split(' ')[:10])\n\t            results.append(sample + '...')\n\t        results = \"\\n====\\n\".join(results)\n\t        self.index.remove_ids(I[0])\n\t        self.embeddings = self.embeddings[:i] + self.embeddings[i+1:]\n\t        self.texts = self.texts[:i] + self.texts[i+1:]\n", "        return f'Removed the item \"{results}...\"'\n\t    def save_(self):\n\t        if len(self.texts) == 0:\n\t            return\n\t        path = Path(self.storage_dir)\n\t        path.mkdir(parents=True, exist_ok=True)\n\t        path = os.path.join(self.storage_dir, f'{self.name}.index')\n\t        faiss.write_index(self.index, path)\n\t        embeddings = np.stack(self.embeddings, axis=0)\n\t        path = os.path.join(self.storage_dir, f'{self.name}.npy')\n", "        np.save(path, embeddings)\n\t        path = os.path.join(self.storage_dir, f'{self.name}.json')\n\t        with open(path, \"w\") as f:\n\t            json.dump(self.texts, f)\n\t    def load_(self):\n\t        path = os.path.join(self.storage_dir, f'{self.name}.index')\n\t        if os.path.isfile(path):\n\t            self.index = faiss.read_index(path)\n\t            path = os.path.join(self.storage_dir, f'{self.name}.npy')\n\t            embeddings = np.load(path)\n", "            self.embeddings = [item for item in embeddings]\n\t            path = os.path.join(self.storage_dir, f'{self.name}.json')\n\t            with open(path) as f:\n\t                self.texts = json.load(f)\n\tclass SkillLibrary(VectorIndexedDB):\n\t    def __init__(self, embedding_backend, storage_dir, name='skills', prompt=\n\t\"\"\"\n\tA database storing your skills. \n\tA skill is a function writen in natural language, pseudocode or programming language.\n\tA skill may employ other skills to achieve its subgoals.\n", "To define a skill, specify the informations it requires and the conclusion it provides.\n\tFor example, here is a skill:\n\tSkill: Make dumplings at home\n\tPotentially useful information: \n\tDesired number of dumplings\n\tFlour quantity\n\tAvailability of ingredients\n\tOutput:\n\tIf ingradients are not enough, break and enumerate the ingredients needed.\n\tOtherwise, no output is needed.\n", "Make a dumpling dough\n\tFor the dumpling filling:\n\t1 cup ground meat (pork, chicken, or beef)\n\t1 cup finely chopped vegetables (cabbage, carrots, mushrooms, etc.)\n\tFor each dumpling:\n\t    Divide dough, roll each portion into a thin circle.\n\t    Place a spoonful of filling in the center of each dough circle. \n\t    Fold and seal the edges.\n\tEmploy cooking skills such as steaming or boiling.\n\tCall `skills.query(context)` to retrieve top-k pertinent skills to achieve your goals.\n", "If there is a skill you find useful,\n\tconcatenate relevant information with the skill description and call `Do(information + skill)` to employ the skill.\n\tThrough your experience, you may acquire new skills or enhance existing ones.\n\tCall `skills.add(skill)` to append a skill to your skill library. \n\t\"\"\"):\n\t        super().__init__(embedding_backend, storage_dir=storage_dir, name=name, prompt=prompt)\n\tclass LongtermMemory(VectorIndexedDB):\n\t    def __init__(self, embedding_backend, storage_dir, name='note', prompt=\n\t\"\"\"\n\tA database which you can use as a note or a memo. \n", "Your short-term memory (context window size) is limited.\n\tIt can only retain a few thousand words.\n\tPreserve important events and facts by recording them in the note. \n\tUse `note.add(text)` to append text to the note. \n\tUse `note.query(text, k)` to retrieve top-k pertinent information from your stored texts.\n\t\"\"\"):\n\t        super().__init__(embedding_backend, storage_dir=storage_dir, name=name, prompt=prompt)"]}
{"filename": "llmux/device/device.py", "chunked_list": ["import inspect\n\tclass Device():\n\t    def __init__(self, name, storage_dir = None, prompt='', load=True, save=True):\n\t        \"\"\"\n\t        A device is an object that implements .help() and methods like .foo(*args, caller=None),\n\t        where caller is a peer.\n\t        Methods will be wrapped so the caller parameter is set to the peer that calls it.\n\t        Methods can be either blocking or async, usually if it requests a backend.\n\t        Device functions that does not start or end with '_' are exposed to peers.\n\t        A device may have an internal non-volatile state.\n", "        If save is True and has method save_, saves to storage on exit.\n\t        If load is True and has method load_, tries loading from storage on start.\n\t        \"\"\"\n\t        self.prompt = prompt\n\t        self.code_snippet_length = 500\n\t        self.name = name\n\t        self.storage_dir = storage_dir\n\t        self.save = save\n\t        self.load = load\n\t    def help(self, caller=None):\n", "        \"\"\"\n\t        Automatically generates prompts for chatbots to explain devices and commands.\n\t        \"\"\"\n\t        object = self\n\t        doc = object.__doc__\n\t        if doc is None:\n\t            doc = '# Empty doctring' \n\t        else:\n\t            doc = f'{self.name}.__doc__: {doc}'\n\t        members = [item for item in dir(self) if not (item.startswith('_') or item.endswith('_'))]\n", "        commands = ['Device commands include: ']\n\t        for item in members:\n\t            if callable(getattr(self, item)):\n\t                commands.append(item)\n\t        return f'Device name: {self.name}\\n' + '\\n'.join([self.prompt, doc] + commands) + '\\n'\n"]}
{"filename": "llmux/device/pdf.py", "chunked_list": ["# to be implemented"]}
{"filename": "llmux/device/__init__.py", "chunked_list": ["from .database import VectorIndexedDB, SkillLibrary, LongtermMemory\n\tfrom .device import Device"]}
{"filename": "llmux/device/filesystem.py", "chunked_list": ["# to be implemented"]}
{"filename": "llmux/device/web.py", "chunked_list": ["# to be implemented"]}
{"filename": "llmux/peer/peer.py", "chunked_list": ["import os\n\timport copy\n\tfrom pathlib import Path\n\timport time\n\tfrom datetime import datetime\n\tfrom ..chat import Chat\n\tfrom ..prompt import format_prompt\n\tfrom ..handler import Handler\n\tfrom ..level import LEVELS\n\tclass Peer():\n", "    def __init__(self, name, output_path=None, auto=False, alertness='trivia'):\n\t        self.name = name\n\t        self.chats = {}\n\t        self.messages = []\n\t        self.output_path = output_path\n\t        self.auto = auto\n\t        self.file = None\n\t        self.system = None\n\t        if auto:\n\t            on_event = ['always']\n", "        else:\n\t            on_event = ['receive_message']\n\t        self.handlers = [Handler(self, self.requestMessage, 1, on_event)]\n\t        if not output_path is None:\n\t            Path(output_path).mkdir(parents=True, exist_ok=True)\n\t            self.file = open(os.path.join(output_path, f'bot_{name}.txt'), \"w\")\n\t        self.setAlterness(alertness)\n\t        self.system_chat = Chat(f'{self.name}-system', output_path)\n\t        self.joinChat(self.system_chat, say_hi=False)\n\t    def setAlterness(self, alertness):\n", "        if isinstance(alertness, str):\n\t            alertness = LEVELS[alertness]\n\t        self.alertness = alertness\n\t    def sleep(self, n):\n\t        self.setAlterness('info')\n\t        wakeup_time = time.time() + n\n\t        for handler in self.handlers:\n\t            handler.state = wakeup_time\n\t        return f'{self.name} sleeps until {datetime.fromtimestamp(wakeup_time)}.'\n\t    def joinChat(self, chat, say_hi=True):\n", "        self.chats[chat.name] = chat\n\t        chat.peers[self.name] = self\n\t        chat.broadcastMessage('system', f'{self.name} joined chat {chat.name}.')\n\t        content = f'Hi {self.name}, you just joined a chat with {[name for name in chat.peers if not name==self.name]}. '\n\t        if say_hi:\n\t            content += ' Say hi and introduce yourself.'\n\t        self.receiveMessage('system', content)\n\t    def quitChat(self, chat_name):\n\t        chat = self.chats.pop(chat_name)\n\t        chat.peers.pop(self.name)\n", "    def loginDevice(self, device):\n\t        device = copy.copy(device)\n\t        def decorator(func):\n\t            def wrapper(*args, **kwargs):\n\t                return func(caller = self, *args, **kwargs)\n\t            wrapper.__doc__ = func.__doc__\n\t            return wrapper\n\t        members = [item for item in dir(device) if not (item.startswith('_') or item.endswith('_'))]\n\t        for item in members:\n\t            member = getattr(device, item)\n", "            if callable(member):\n\t                setattr(device, item, decorator(member))\n\t        return device\n\t    def __sendMessage(self, message, parsed, error_prompt):\n\t        \"\"\"\n\t        Users and bots may use different message formats and parseMessage methods.\n\t        But they can share the same sendMessage method.\n\t        \"\"\"\n\t        valid_chats = []\n\t        if len(error_prompt) == 0:\n", "            if 'to' in parsed:\n\t                chats = parsed['to']\n\t                for chat_name in chats:\n\t                    if chat_name in self.chats:\n\t                        # do not directly broadcast a message, or a bot may receive it multiple times. \n\t                        self.chats[chat_name].dumpMessage(self.name, message)\n\t                        valid_chats.append(self.chats[chat_name])\n\t                    else:\n\t                        error_prompt += f'\"{chat_name}\", '\n\t                if len(error_prompt) > 0:\n", "                    error_prompt = error_prompt[:-2]\n\t                    error_prompt = \"You cannot send message to these chats: \" + error_prompt + \". \"\n\t        if len(error_prompt) > 0:\n\t            error_prompt += \"You have joined these chats: \"\n\t            for name, chat in self.chats.items():\n\t                error_prompt += f'\"{name}\", '\n\t            error_prompt = error_prompt[:-2]\n\t            error_prompt += \".\"\n\t            # invalid chats, forward message to system\n\t            if len(error_prompt) > 0:\n", "                self.system_chat.broadcastMessage(self, message)\n\t                self.system_chat.broadcastMessage('system', error_prompt)\n\t        # find the receivers and send message\n\t        # each bot only receives message once, possibly from how multiple chats\n\t        receivers = {}\n\t        for chat in valid_chats:\n\t            for name, peer in chat.peers.items():\n\t                if not peer in receivers:\n\t                    receivers[peer] = [chat]\n\t                else:\n", "                    receivers[peer].append(chat)\n\t        for receiver, chats in receivers.items():\n\t            receiver.receiveMessage(self, message)\n\t    def sendMessage(self, message):\n\t        content, parsed, error = self.parseMessage(message)\n\t        self.__sendMessage(content, parsed, error)\n\t        return parsed\n\t    async def requestMessage(self):\n\t        content = await self.backend.asyncRequest(self, self.messages)\n\t        parsed = self.sendMessage(content)\n", "        # sucessful parsing\n\t        if isinstance(parsed, dict):\n\t            # A system call is made\n\t            if 'code' in parsed:\n\t                for method, code in parsed['code']:\n\t                    if method == 'eval':\n\t                        content, level = self.system._eval(code, self)\n\t                    elif method == 'exec':\n\t                        content, level = self.system._exec(code, self)\n\t                    if not content is None:\n", "                        self.system_chat.broadcastMessage('system', content, level=level)\n\t    def receiveMessage(self, sender, message, level=LEVELS['info']):\n\t        # call handlers if the message is important enough\n\t        if self.alertness >= level:\n\t            for handler in self.handlers:\n\t                if handler.alertness >= level:\n\t                    handler.state = 'runnable'"]}
{"filename": "llmux/peer/bot.py", "chunked_list": ["import time\n\timport re\n\tfrom .user import User\n\tfrom .peer import Peer\n\tfrom ..prompt.prompt import format_prompt\n\tfrom ..level import LEVELS\n\tclass Bot(Peer):\n\t    def __init__(self, function, chatbot_backend, name, output_path=None, auto=False):\n\t        \"\"\"\n\t        If auto is True, runs without user input, similar to 'continous mode' of AutoGPT.\n", "        \"\"\"\n\t        self.function = function\n\t        self.backend = chatbot_backend\n\t        super().__init__(name, output_path, auto)\n\t        self.task = None\n\t        self.system_chat.broadcastMessage('system', f'Hi {self.name}, your task is {function}')\n\t    def receiveMessage(self, sender, content, level=LEVELS['info']):\n\t        \"\"\"\n\t        Prevent sending private messages to bots unless necessary.\n\t        Broadcasting the message in a chat allows other bots, such as a critic to share information.\n", "        \"\"\"\n\t        super().receiveMessage(sender, content, level)\n\t        role = sender\n\t        if isinstance(sender, Bot):\n\t            # echo must be the same as raw content\n\t            if sender is self:\n\t                role = 'assistant'\n\t            # message from another bot, set role = system to avoid confusion during generation\n\t            else:\n\t                role = 'system'\n", "                content = f'{sender.name}: {content}'\n\t        elif isinstance(sender, User):\n\t            role = 'user'\n\t            content = f'{sender.name}: {content}'\n\t        else:\n\t            assert sender == 'system'\n\t        message = {'role': role, 'content': content}\n\t        self.messages.append(message)\n\t        self.file.write(f'{str(message)}\\n')\n\t        self.file.flush()\n", "    def parseMessage(self, message):\n\t        error_prompt = ''\n\t        parsed = {}\n\t        # find code snippets\n\t        parsed['code'] = []\n\t        # this is not general enough\n\t        pattern = re.compile('(eval:\\s*`(.+)`|exec:.*\\\\n?```([^`]+)```)')\n\t        match = pattern.findall(message)\n\t        for item in match:\n\t            if len(item[1]) > 0:\n", "                parsed['code'].append(('eval', item[1].strip()))\n\t            elif len(item[2]) > 0:\n\t                parsed['code'].append(('exec', item[2].strip()))\n\t        # destination chat\n\t        first_line = message[:message.find('\\n')]\n\t        if first_line[:2] == 'to' and ':' in first_line:\n\t            first_line = first_line[2:]\n\t            sep = first_line.find(':')\n\t            chats = first_line[:sep].split(',')\n\t            chats = [item.strip() for item in chats]\n", "            parsed['to'] = chats\n\t        elif len(parsed['code']) > 0:\n\t            parsed['to'] = [self.system_chat.name]\n\t        else:\n\t            error_prompt = 'Wrong format.'\n\t            error_prompt += format_prompt\n\t        return message, parsed, error_prompt"]}
{"filename": "llmux/peer/__init__.py", "chunked_list": ["from .bot import Bot\n\tfrom .user import CLIUser\n\tfrom .peer import Peer"]}
{"filename": "llmux/peer/user.py", "chunked_list": ["from .peer import Peer\n\tfrom ..backend import CLI\n\tfrom ..level import LEVELS\n\tfrom concurrent.futures import ThreadPoolExecutor\n\tclass User(Peer):\n\t    def __init__(self, name):\n\t        super().__init__(name, auto=True)\n\tclass CLIUser(User):\n\t    def __init__(self, name, backend):\n\t        self.chat_with = []\n", "        super().__init__(name)\n\t        self.backend = backend\n\t    def joinChat(self, chat, say_hi=True):\n\t        super().joinChat(chat, say_hi)\n\t        self.chat_with = [chat.name]\n\t        self.messages = f'{self.name} to {self.chat_with}: '\n\t    def chatwith(self, name):\n\t        self.chat_with = [name]\n\t        self.messages = f'{self.name} to {self.chat_with}: '\n\t    def parseMessage(self, message):\n", "        \"\"\"\n\t        Instead of enforcing JSON, a CLI user may use a lightweight grammar.\n\t        Start a line with an exclamation mark to eval a system call.\n\t        ! self.chatwith(\"bot\")\n\t        to switch the current chat.\n\t        \"\"\"\n\t        parsed = {'to': self.chat_with}\n\t        if message[:2] == '! ':\n\t            parsed['code'] = [('eval', message[2:])]\n\t            parsed['to'] = [self.system_chat.name]\n", "        message = f'to {\", \".join(parsed[\"to\"])}:' + message\n\t        return message, parsed, ''\n\t    def receiveMessage(self, sender, content, level=LEVELS['info']):\n\t        super().receiveMessage(sender, content, level)\n\t        if isinstance(sender, Peer):\n\t            sender = sender.name\n\t        if self.alertness > level:\n\t            print(f'{sender}: {content}')"]}
