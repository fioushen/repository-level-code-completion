{"filename": "dev.py", "chunked_list": ["import modules.ui as ui\n\tdemo = ui.create_ui()\n"]}
{"filename": "launch.py", "chunked_list": ["import importlib.util\n\timport os\n\timport shlex\n\timport subprocess\n\timport sys\n\tcommandline_args = os.environ.get(\"COMMANDLINE_ARGS\", \"\")\n\tsys.argv += shlex.split(commandline_args)\n\tpython = sys.executable\n\tgit = os.environ.get(\"GIT\", \"git\")\n\tindex_url = os.environ.get(\"INDEX_URL\", \"\")\n", "stored_commit_hash = None\n\tskip_install = False\n\tdef run(command, desc=None, errdesc=None, custom_env=None):\n\t    if desc is not None:\n\t        print(desc)\n\t    result = subprocess.run(\n\t        command,\n\t        stdout=subprocess.PIPE,\n\t        stderr=subprocess.PIPE,\n\t        shell=True,\n", "        env=os.environ if custom_env is None else custom_env,\n\t    )\n\t    if result.returncode != 0:\n\t        message = f\"\"\"{errdesc or 'Error running command'}.\n\tCommand: {command}\n\tError code: {result.returncode}\n\tstdout: {result.stdout.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stdout)>0 else '<empty>'}\n\tstderr: {result.stderr.decode(encoding=\"utf8\", errors=\"ignore\") if len(result.stderr)>0 else '<empty>'}\n\t\"\"\"\n\t        raise RuntimeError(message)\n", "    return result.stdout.decode(encoding=\"utf8\", errors=\"ignore\")\n\tdef check_run(command):\n\t    result = subprocess.run(\n\t        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True\n\t    )\n\t    return result.returncode == 0\n\tdef is_installed(package):\n\t    try:\n\t        spec = importlib.util.find_spec(package)\n\t    except ModuleNotFoundError:\n", "        return False\n\t    return spec is not None\n\tdef commit_hash():\n\t    global stored_commit_hash\n\t    if stored_commit_hash is not None:\n\t        return stored_commit_hash\n\t    try:\n\t        stored_commit_hash = run(f\"{git} rev-parse HEAD\").strip()\n\t    except Exception:\n\t        stored_commit_hash = \"<none>\"\n", "    return stored_commit_hash\n\tdef run_pip(args, desc=None):\n\t    if skip_install:\n\t        return\n\t    index_url_line = f\" --index-url {index_url}\" if index_url != \"\" else \"\"\n\t    return run(\n\t        f'\"{python}\" -m pip {args} --prefer-binary{index_url_line}',\n\t        desc=f\"Installing {desc}\",\n\t        errdesc=f\"Couldn't install {desc}\",\n\t    )\n", "def run_python(code, desc=None, errdesc=None):\n\t    return run(f'\"{python}\" -c \"{code}\"', desc, errdesc)\n\tdef extract_arg(args, name):\n\t    return [x for x in args if x != name], name in args\n\tdef prepare_environment():\n\t    commit = commit_hash()\n\t    print(f\"Python {sys.version}\")\n\t    print(f\"Commit hash: {commit}\")\n\t    torch_command = os.environ.get(\n\t        \"TORCH_COMMAND\",\n", "        \"pip install torch torchaudio --extra-index-url https://download.pytorch.org/whl/cu118\",\n\t    )\n\t    sys.argv, skip_install = extract_arg(sys.argv, \"--skip-install\")\n\t    if skip_install:\n\t        return\n\t    sys.argv, reinstall_torch = extract_arg(sys.argv, \"--reinstall-torch\")\n\t    ngrok = \"--ngrok\" in sys.argv\n\t    if reinstall_torch or not is_installed(\"torch\") or not is_installed(\"torchaudio\"):\n\t        run(\n\t            f'\"{python}\" -m {torch_command}',\n", "            \"Installing torch and torchaudio\",\n\t            \"Couldn't install torch\",\n\t        )\n\t    if not is_installed(\"pyngrok\") and ngrok:\n\t        run_pip(\"install pyngrok\", \"ngrok\")\n\t    run(\n\t        f'\"{python}\" -m pip install -r requirements.txt',\n\t        desc=f\"Installing requirements\",\n\t        errdesc=f\"Couldn't install requirements\",\n\t    )\n", "def start():\n\t    os.environ[\"PATH\"] = (\n\t        os.path.join(os.path.dirname(__file__), \"bin\")\n\t        + os.pathsep\n\t        + os.environ.get(\"PATH\", \"\")\n\t    )\n\t    subprocess.run(\n\t        [python, \"webui.py\", *sys.argv[1:]],\n\t    )\n\tif __name__ == \"__main__\":\n", "    prepare_environment()\n\t    start()\n"]}
{"filename": "webui.py", "chunked_list": ["import os\n\tfrom modules import cmd_opts, ui\n\t# なんか知らんが湧いて出てくる \".DS_Store\"　を無視する。\n\t# ここにこんなコードを置くべきかはわからないけど…\n\t_list_dir = os.listdir\n\tdef listdir4mac(path):\n\t    return [file for file in _list_dir(path) if not file.startswith(\".\")]\n\tos.listdir = listdir4mac\n\tdef webui():\n\t    app = ui.create_ui()\n", "    app.queue(64)\n\t    app, local_url, share_url = app.launch(\n\t        server_name=cmd_opts.opts.host,\n\t        server_port=cmd_opts.opts.port,\n\t        share=cmd_opts.opts.share,\n\t    )\n\tif __name__ == \"__main__\":\n\t    webui()\n"]}
{"filename": "server.py", "chunked_list": ["import io\n\timport json\n\timport os\n\timport traceback\n\tfrom typing import *\n\timport soundfile as sf\n\tfrom flask import Flask, make_response, request, send_file\n\tfrom scipy.io.wavfile import write\n\tfrom modules.server.model import VoiceServerModel\n\tmodel: Optional[VoiceServerModel] = None\n", "app = Flask(__name__)\n\t@app.route('/ping')\n\tdef ping():\n\t    return make_response(\"server is alive\", 200)\n\t@app.route('/upload_model', methods=['POST'])\n\tdef upload_model():\n\t    \"\"\"\n\t    input:\n\t        json:\n\t            rvc_model_file: str\n", "                specify rvc model's absolute path (.pt, .pth)\n\t            faiss_index_file: Optional[str]\n\t                specify faiss index'S absolute path (.index)\n\t    \"\"\"\n\t    global model\n\t    if request.method == \"POST\":\n\t        rvc_model_file = request.json[\"rvc_model_file\"]\n\t        faiss_index_file =request.json[\"faiss_index_file\"] if \"faiss_index_file\" in request.json else \"\"\n\t        try:\n\t            model = VoiceServerModel(rvc_model_file, faiss_index_file)\n", "            return make_response(\"model is load\", 200)\n\t        except:\n\t            traceback.print_exc()\n\t            return make_response(\"model load error\", 400)\n\t    else:\n\t        return make_response(\"use post method\", 400)\n\t@app.route('/convert_sound', methods=['POST'])\n\tdef convert_sound():\n\t    \"\"\"\n\t    input:\n", "        params: json\n\t            speaker_id: int\n\t                default: 0\n\t            transpose: int\n\t                default: 0\n\t            pitch_extraction_algo: str\n\t                default: dio\n\t                value: [\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"]\n\t            retrieval_feature_ratio: float\n\t                default: 0\n", "                value: 0. ~ 1.\n\t        input_wav: wav file\n\t    output:\n\t        wavfile\n\t    \"\"\"\n\t    global model\n\t    if model is None:\n\t        return make_response(\"please upload model\", 400)\n\t    print(\"start\")\n\t    if request.method == \"POST\":\n", "        input_buffer = io.BytesIO(request.files[\"input_wav\"].stream.read())\n\t        audio, sr = sf.read(input_buffer)\n\t        req_json = json.load(io.BytesIO(request.files[\"params\"].stream.read()))\n\t        sid = int(req_json.get(\"speaker_id\", 0))\n\t        transpose = int(req_json.get(\"transpose\", 0))\n\t        pitch_extraction_algo = req_json.get(\"pitch_extraction_algo\", \"dio\")\n\t        if not pitch_extraction_algo in [\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"]:\n\t            return make_response(\"bad pitch extraction algo\", 400)\n\t        retrieval_feature_ratio = float(req_json.get(\"retrieval_feature_ratio\", 0.))\n\t        out_audio = model(audio, sr, sid, transpose, pitch_extraction_algo, retrieval_feature_ratio)\n", "        output_buffer = io.BytesIO()\n\t        write(output_buffer, rate=model.tgt_sr, data=out_audio)\n\t        output_buffer.seek(0)\n\t        response = make_response(send_file(output_buffer, mimetype=\"audio/wav\"), 200)\n\t        return response\n\t    else:\n\t        return make_response(\"use post method\", 400)\n\tif __name__ == \"__main__\":\n\t    app.run()"]}
{"filename": "lib/rvc/losses.py", "chunked_list": ["import torch\n\tdef feature_loss(fmap_r, fmap_g):\n\t    loss = 0\n\t    for dr, dg in zip(fmap_r, fmap_g):\n\t        for rl, gl in zip(dr, dg):\n\t            rl = rl.float().detach()\n\t            gl = gl.float()\n\t            loss += torch.mean(torch.abs(rl - gl))\n\t    return loss * 2\n\tdef discriminator_loss(disc_real_outputs, disc_generated_outputs):\n", "    loss = 0\n\t    r_losses = []\n\t    g_losses = []\n\t    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n\t        dr = dr.float()\n\t        dg = dg.float()\n\t        r_loss = torch.mean((1 - dr) ** 2)\n\t        g_loss = torch.mean(dg**2)\n\t        loss += r_loss + g_loss\n\t        r_losses.append(r_loss.item())\n", "        g_losses.append(g_loss.item())\n\t    return loss, r_losses, g_losses\n\tdef generator_loss(disc_outputs):\n\t    loss = 0\n\t    gen_losses = []\n\t    for dg in disc_outputs:\n\t        dg = dg.float()\n\t        l = torch.mean((1 - dg) ** 2)\n\t        gen_losses.append(l)\n\t        loss += l\n", "    return loss, gen_losses\n\tdef kl_loss(z_p, logs_q, m_p, logs_p, z_mask):\n\t    \"\"\"\n\t    z_p, logs_q: [b, h, t_t]\n\t    m_p, logs_p: [b, h, t_t]\n\t    \"\"\"\n\t    z_p = z_p.float()\n\t    logs_q = logs_q.float()\n\t    m_p = m_p.float()\n\t    logs_p = logs_p.float()\n", "    z_mask = z_mask.float()\n\t    kl = logs_p - logs_q - 0.5\n\t    kl += 0.5 * ((z_p - m_p) ** 2) * torch.exp(-2.0 * logs_p)\n\t    kl = torch.sum(kl * z_mask)\n\t    l = kl / torch.sum(z_mask)\n\t    return l\n"]}
{"filename": "lib/rvc/attentions.py", "chunked_list": ["import math\n\timport torch\n\tfrom torch import nn\n\tfrom torch.nn import functional as F\n\tfrom . import commons\n\tfrom .modules import LayerNorm\n\tclass Encoder(nn.Module):\n\t    def __init__(\n\t        self,\n\t        hidden_channels,\n", "        filter_channels,\n\t        n_heads,\n\t        n_layers,\n\t        kernel_size=1,\n\t        p_dropout=0.0,\n\t        window_size=10,\n\t        **kwargs\n\t    ):\n\t        super().__init__()\n\t        self.hidden_channels = hidden_channels\n", "        self.filter_channels = filter_channels\n\t        self.n_heads = n_heads\n\t        self.n_layers = n_layers\n\t        self.kernel_size = kernel_size\n\t        self.p_dropout = p_dropout\n\t        self.window_size = window_size\n\t        self.drop = nn.Dropout(p_dropout)\n\t        self.attn_layers = nn.ModuleList()\n\t        self.norm_layers_1 = nn.ModuleList()\n\t        self.ffn_layers = nn.ModuleList()\n", "        self.norm_layers_2 = nn.ModuleList()\n\t        for i in range(self.n_layers):\n\t            self.attn_layers.append(\n\t                MultiHeadAttention(\n\t                    hidden_channels,\n\t                    hidden_channels,\n\t                    n_heads,\n\t                    p_dropout=p_dropout,\n\t                    window_size=window_size,\n\t                )\n", "            )\n\t            self.norm_layers_1.append(LayerNorm(hidden_channels))\n\t            self.ffn_layers.append(\n\t                FFN(\n\t                    hidden_channels,\n\t                    hidden_channels,\n\t                    filter_channels,\n\t                    kernel_size,\n\t                    p_dropout=p_dropout,\n\t                )\n", "            )\n\t            self.norm_layers_2.append(LayerNorm(hidden_channels))\n\t    def forward(self, x, x_mask):\n\t        attn_mask = x_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n\t        x = x * x_mask\n\t        for i in range(self.n_layers):\n\t            y = self.attn_layers[i](x, x, attn_mask)\n\t            y = self.drop(y)\n\t            x = self.norm_layers_1[i](x + y)\n\t            y = self.ffn_layers[i](x, x_mask)\n", "            y = self.drop(y)\n\t            x = self.norm_layers_2[i](x + y)\n\t        x = x * x_mask\n\t        return x\n\tclass Decoder(nn.Module):\n\t    def __init__(\n\t        self,\n\t        hidden_channels,\n\t        filter_channels,\n\t        n_heads,\n", "        n_layers,\n\t        kernel_size=1,\n\t        p_dropout=0.0,\n\t        proximal_bias=False,\n\t        proximal_init=True,\n\t        **kwargs\n\t    ):\n\t        super().__init__()\n\t        self.hidden_channels = hidden_channels\n\t        self.filter_channels = filter_channels\n", "        self.n_heads = n_heads\n\t        self.n_layers = n_layers\n\t        self.kernel_size = kernel_size\n\t        self.p_dropout = p_dropout\n\t        self.proximal_bias = proximal_bias\n\t        self.proximal_init = proximal_init\n\t        self.drop = nn.Dropout(p_dropout)\n\t        self.self_attn_layers = nn.ModuleList()\n\t        self.norm_layers_0 = nn.ModuleList()\n\t        self.encdec_attn_layers = nn.ModuleList()\n", "        self.norm_layers_1 = nn.ModuleList()\n\t        self.ffn_layers = nn.ModuleList()\n\t        self.norm_layers_2 = nn.ModuleList()\n\t        for i in range(self.n_layers):\n\t            self.self_attn_layers.append(\n\t                MultiHeadAttention(\n\t                    hidden_channels,\n\t                    hidden_channels,\n\t                    n_heads,\n\t                    p_dropout=p_dropout,\n", "                    proximal_bias=proximal_bias,\n\t                    proximal_init=proximal_init,\n\t                )\n\t            )\n\t            self.norm_layers_0.append(LayerNorm(hidden_channels))\n\t            self.encdec_attn_layers.append(\n\t                MultiHeadAttention(\n\t                    hidden_channels, hidden_channels, n_heads, p_dropout=p_dropout\n\t                )\n\t            )\n", "            self.norm_layers_1.append(LayerNorm(hidden_channels))\n\t            self.ffn_layers.append(\n\t                FFN(\n\t                    hidden_channels,\n\t                    hidden_channels,\n\t                    filter_channels,\n\t                    kernel_size,\n\t                    p_dropout=p_dropout,\n\t                    causal=True,\n\t                )\n", "            )\n\t            self.norm_layers_2.append(LayerNorm(hidden_channels))\n\t    def forward(self, x, x_mask, h, h_mask):\n\t        \"\"\"\n\t        x: decoder input\n\t        h: encoder output\n\t        \"\"\"\n\t        self_attn_mask = commons.subsequent_mask(x_mask.size(2)).to(\n\t            device=x.device, dtype=x.dtype\n\t        )\n", "        encdec_attn_mask = h_mask.unsqueeze(2) * x_mask.unsqueeze(-1)\n\t        x = x * x_mask\n\t        for i in range(self.n_layers):\n\t            y = self.self_attn_layers[i](x, x, self_attn_mask)\n\t            y = self.drop(y)\n\t            x = self.norm_layers_0[i](x + y)\n\t            y = self.encdec_attn_layers[i](x, h, encdec_attn_mask)\n\t            y = self.drop(y)\n\t            x = self.norm_layers_1[i](x + y)\n\t            y = self.ffn_layers[i](x, x_mask)\n", "            y = self.drop(y)\n\t            x = self.norm_layers_2[i](x + y)\n\t        x = x * x_mask\n\t        return x\n\tclass MultiHeadAttention(nn.Module):\n\t    def __init__(\n\t        self,\n\t        channels,\n\t        out_channels,\n\t        n_heads,\n", "        p_dropout=0.0,\n\t        window_size=None,\n\t        heads_share=True,\n\t        block_length=None,\n\t        proximal_bias=False,\n\t        proximal_init=False,\n\t    ):\n\t        super().__init__()\n\t        assert channels % n_heads == 0\n\t        self.channels = channels\n", "        self.out_channels = out_channels\n\t        self.n_heads = n_heads\n\t        self.p_dropout = p_dropout\n\t        self.window_size = window_size\n\t        self.heads_share = heads_share\n\t        self.block_length = block_length\n\t        self.proximal_bias = proximal_bias\n\t        self.proximal_init = proximal_init\n\t        self.attn = None\n\t        self.k_channels = channels // n_heads\n", "        self.conv_q = nn.Conv1d(channels, channels, 1)\n\t        self.conv_k = nn.Conv1d(channels, channels, 1)\n\t        self.conv_v = nn.Conv1d(channels, channels, 1)\n\t        self.conv_o = nn.Conv1d(channels, out_channels, 1)\n\t        self.drop = nn.Dropout(p_dropout)\n\t        if window_size is not None:\n\t            n_heads_rel = 1 if heads_share else n_heads\n\t            rel_stddev = self.k_channels**-0.5\n\t            self.emb_rel_k = nn.Parameter(\n\t                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n", "                * rel_stddev\n\t            )\n\t            self.emb_rel_v = nn.Parameter(\n\t                torch.randn(n_heads_rel, window_size * 2 + 1, self.k_channels)\n\t                * rel_stddev\n\t            )\n\t        nn.init.xavier_uniform_(self.conv_q.weight)\n\t        nn.init.xavier_uniform_(self.conv_k.weight)\n\t        nn.init.xavier_uniform_(self.conv_v.weight)\n\t        if proximal_init:\n", "            with torch.no_grad():\n\t                self.conv_k.weight.copy_(self.conv_q.weight)\n\t                self.conv_k.bias.copy_(self.conv_q.bias)\n\t    def forward(self, x, c, attn_mask=None):\n\t        q = self.conv_q(x)\n\t        k = self.conv_k(c)\n\t        v = self.conv_v(c)\n\t        x, self.attn = self.attention(q, k, v, mask=attn_mask)\n\t        x = self.conv_o(x)\n\t        return x\n", "    def attention(self, query, key, value, mask=None):\n\t        # reshape [b, d, t] -> [b, n_h, t, d_k]\n\t        b, d, t_s, t_t = (*key.size(), query.size(2))\n\t        query = query.view(b, self.n_heads, self.k_channels, t_t).transpose(2, 3)\n\t        key = key.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n\t        value = value.view(b, self.n_heads, self.k_channels, t_s).transpose(2, 3)\n\t        scores = torch.matmul(query / math.sqrt(self.k_channels), key.transpose(-2, -1))\n\t        if self.window_size is not None:\n\t            assert (\n\t                t_s == t_t\n", "            ), \"Relative attention is only available for self-attention.\"\n\t            key_relative_embeddings = self._get_relative_embeddings(self.emb_rel_k, t_s)\n\t            rel_logits = self._matmul_with_relative_keys(\n\t                query / math.sqrt(self.k_channels), key_relative_embeddings\n\t            )\n\t            scores_local = self._relative_position_to_absolute_position(rel_logits)\n\t            scores = scores + scores_local\n\t        if self.proximal_bias:\n\t            assert t_s == t_t, \"Proximal bias is only available for self-attention.\"\n\t            scores = scores + self._attention_bias_proximal(t_s).to(\n", "                device=scores.device, dtype=scores.dtype\n\t            )\n\t        if mask is not None:\n\t            scores = scores.masked_fill(mask == 0, -1e4)\n\t            if self.block_length is not None:\n\t                assert (\n\t                    t_s == t_t\n\t                ), \"Local attention is only available for self-attention.\"\n\t                block_mask = (\n\t                    torch.ones_like(scores)\n", "                    .triu(-self.block_length)\n\t                    .tril(self.block_length)\n\t                )\n\t                scores = scores.masked_fill(block_mask == 0, -1e4)\n\t        p_attn = F.softmax(scores, dim=-1)  # [b, n_h, t_t, t_s]\n\t        p_attn = self.drop(p_attn)\n\t        output = torch.matmul(p_attn, value)\n\t        if self.window_size is not None:\n\t            relative_weights = self._absolute_position_to_relative_position(p_attn)\n\t            value_relative_embeddings = self._get_relative_embeddings(\n", "                self.emb_rel_v, t_s\n\t            )\n\t            output = output + self._matmul_with_relative_values(\n\t                relative_weights, value_relative_embeddings\n\t            )\n\t        output = (\n\t            output.transpose(2, 3).contiguous().view(b, d, t_t)\n\t        )  # [b, n_h, t_t, d_k] -> [b, d, t_t]\n\t        return output, p_attn\n\t    def _matmul_with_relative_values(self, x, y):\n", "        \"\"\"\n\t        x: [b, h, l, m]\n\t        y: [h or 1, m, d]\n\t        ret: [b, h, l, d]\n\t        \"\"\"\n\t        ret = torch.matmul(x, y.unsqueeze(0))\n\t        return ret\n\t    def _matmul_with_relative_keys(self, x, y):\n\t        \"\"\"\n\t        x: [b, h, l, d]\n", "        y: [h or 1, m, d]\n\t        ret: [b, h, l, m]\n\t        \"\"\"\n\t        ret = torch.matmul(x, y.unsqueeze(0).transpose(-2, -1))\n\t        return ret\n\t    def _get_relative_embeddings(self, relative_embeddings, length):\n\t        max_relative_position = 2 * self.window_size + 1\n\t        # Pad first before slice to avoid using cond ops.\n\t        pad_length = max(length - (self.window_size + 1), 0)\n\t        slice_start_position = max((self.window_size + 1) - length, 0)\n", "        slice_end_position = slice_start_position + 2 * length - 1\n\t        if pad_length > 0:\n\t            padded_relative_embeddings = F.pad(\n\t                relative_embeddings,\n\t                commons.convert_pad_shape([[0, 0], [pad_length, pad_length], [0, 0]]),\n\t            )\n\t        else:\n\t            padded_relative_embeddings = relative_embeddings\n\t        used_relative_embeddings = padded_relative_embeddings[\n\t            :, slice_start_position:slice_end_position\n", "        ]\n\t        return used_relative_embeddings\n\t    def _relative_position_to_absolute_position(self, x):\n\t        \"\"\"\n\t        x: [b, h, l, 2*l-1]\n\t        ret: [b, h, l, l]\n\t        \"\"\"\n\t        batch, heads, length, _ = x.size()\n\t        # Concat columns of pad to shift from relative to absolute indexing.\n\t        x = F.pad(x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, 1]]))\n", "        # Concat extra elements so to add up to shape (len+1, 2*len-1).\n\t        x_flat = x.view([batch, heads, length * 2 * length])\n\t        x_flat = F.pad(\n\t            x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [0, length - 1]])\n\t        )\n\t        # Reshape and slice out the padded elements.\n\t        x_final = x_flat.view([batch, heads, length + 1, 2 * length - 1])[\n\t            :, :, :length, length - 1 :\n\t        ]\n\t        return x_final\n", "    def _absolute_position_to_relative_position(self, x):\n\t        \"\"\"\n\t        x: [b, h, l, l]\n\t        ret: [b, h, l, 2*l-1]\n\t        \"\"\"\n\t        batch, heads, length, _ = x.size()\n\t        # padd along column\n\t        x = F.pad(\n\t            x, commons.convert_pad_shape([[0, 0], [0, 0], [0, 0], [0, length - 1]])\n\t        )\n", "        x_flat = x.view([batch, heads, length**2 + length * (length - 1)])\n\t        # add 0's in the beginning that will skew the elements after reshape\n\t        x_flat = F.pad(x_flat, commons.convert_pad_shape([[0, 0], [0, 0], [length, 0]]))\n\t        x_final = x_flat.view([batch, heads, length, 2 * length])[:, :, :, 1:]\n\t        return x_final\n\t    def _attention_bias_proximal(self, length):\n\t        \"\"\"Bias for self-attention to encourage attention to close positions.\n\t        Args:\n\t          length: an integer scalar.\n\t        Returns:\n", "          a Tensor with shape [1, 1, length, length]\n\t        \"\"\"\n\t        r = torch.arange(length, dtype=torch.float32)\n\t        diff = torch.unsqueeze(r, 0) - torch.unsqueeze(r, 1)\n\t        return torch.unsqueeze(torch.unsqueeze(-torch.log1p(torch.abs(diff)), 0), 0)\n\tclass FFN(nn.Module):\n\t    def __init__(\n\t        self,\n\t        in_channels,\n\t        out_channels,\n", "        filter_channels,\n\t        kernel_size,\n\t        p_dropout=0.0,\n\t        activation=None,\n\t        causal=False,\n\t    ):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.filter_channels = filter_channels\n", "        self.kernel_size = kernel_size\n\t        self.p_dropout = p_dropout\n\t        self.activation = activation\n\t        self.causal = causal\n\t        if causal:\n\t            self.padding = self._causal_padding\n\t        else:\n\t            self.padding = self._same_padding\n\t        self.conv_1 = nn.Conv1d(in_channels, filter_channels, kernel_size)\n\t        self.conv_2 = nn.Conv1d(filter_channels, out_channels, kernel_size)\n", "        self.drop = nn.Dropout(p_dropout)\n\t    def forward(self, x, x_mask):\n\t        x = self.conv_1(self.padding(x * x_mask))\n\t        if self.activation == \"gelu\":\n\t            x = x * torch.sigmoid(1.702 * x)\n\t        else:\n\t            x = torch.relu(x)\n\t        x = self.drop(x)\n\t        x = self.conv_2(self.padding(x * x_mask))\n\t        return x * x_mask\n", "    def _causal_padding(self, x):\n\t        if self.kernel_size == 1:\n\t            return x\n\t        pad_l = self.kernel_size - 1\n\t        pad_r = 0\n\t        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n\t        x = F.pad(x, commons.convert_pad_shape(padding))\n\t        return x\n\t    def _same_padding(self, x):\n\t        if self.kernel_size == 1:\n", "            return x\n\t        pad_l = (self.kernel_size - 1) // 2\n\t        pad_r = self.kernel_size // 2\n\t        padding = [[0, 0], [0, 0], [pad_l, pad_r]]\n\t        x = F.pad(x, commons.convert_pad_shape(padding))\n\t        return x\n"]}
{"filename": "lib/rvc/transforms.py", "chunked_list": ["import numpy as np\n\timport torch\n\tfrom torch.nn import functional as F\n\tDEFAULT_MIN_BIN_WIDTH = 1e-3\n\tDEFAULT_MIN_BIN_HEIGHT = 1e-3\n\tDEFAULT_MIN_DERIVATIVE = 1e-3\n\tdef piecewise_rational_quadratic_transform(\n\t    inputs,\n\t    unnormalized_widths,\n\t    unnormalized_heights,\n", "    unnormalized_derivatives,\n\t    inverse=False,\n\t    tails=None,\n\t    tail_bound=1.0,\n\t    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n\t    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n\t    min_derivative=DEFAULT_MIN_DERIVATIVE,\n\t):\n\t    if tails is None:\n\t        spline_fn = rational_quadratic_spline\n", "        spline_kwargs = {}\n\t    else:\n\t        spline_fn = unconstrained_rational_quadratic_spline\n\t        spline_kwargs = {\"tails\": tails, \"tail_bound\": tail_bound}\n\t    outputs, logabsdet = spline_fn(\n\t        inputs=inputs,\n\t        unnormalized_widths=unnormalized_widths,\n\t        unnormalized_heights=unnormalized_heights,\n\t        unnormalized_derivatives=unnormalized_derivatives,\n\t        inverse=inverse,\n", "        min_bin_width=min_bin_width,\n\t        min_bin_height=min_bin_height,\n\t        min_derivative=min_derivative,\n\t        **spline_kwargs\n\t    )\n\t    return outputs, logabsdet\n\tdef searchsorted(bin_locations, inputs, eps=1e-6):\n\t    bin_locations[..., -1] += eps\n\t    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\n\tdef unconstrained_rational_quadratic_spline(\n", "    inputs,\n\t    unnormalized_widths,\n\t    unnormalized_heights,\n\t    unnormalized_derivatives,\n\t    inverse=False,\n\t    tails=\"linear\",\n\t    tail_bound=1.0,\n\t    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n\t    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n\t    min_derivative=DEFAULT_MIN_DERIVATIVE,\n", "):\n\t    inside_interval_mask = (inputs >= -tail_bound) & (inputs <= tail_bound)\n\t    outside_interval_mask = ~inside_interval_mask\n\t    outputs = torch.zeros_like(inputs)\n\t    logabsdet = torch.zeros_like(inputs)\n\t    if tails == \"linear\":\n\t        unnormalized_derivatives = F.pad(unnormalized_derivatives, pad=(1, 1))\n\t        constant = np.log(np.exp(1 - min_derivative) - 1)\n\t        unnormalized_derivatives[..., 0] = constant\n\t        unnormalized_derivatives[..., -1] = constant\n", "        outputs[outside_interval_mask] = inputs[outside_interval_mask]\n\t        logabsdet[outside_interval_mask] = 0\n\t    else:\n\t        raise RuntimeError(\"{} tails are not implemented.\".format(tails))\n\t    (\n\t        outputs[inside_interval_mask],\n\t        logabsdet[inside_interval_mask],\n\t    ) = rational_quadratic_spline(\n\t        inputs=inputs[inside_interval_mask],\n\t        unnormalized_widths=unnormalized_widths[inside_interval_mask, :],\n", "        unnormalized_heights=unnormalized_heights[inside_interval_mask, :],\n\t        unnormalized_derivatives=unnormalized_derivatives[inside_interval_mask, :],\n\t        inverse=inverse,\n\t        left=-tail_bound,\n\t        right=tail_bound,\n\t        bottom=-tail_bound,\n\t        top=tail_bound,\n\t        min_bin_width=min_bin_width,\n\t        min_bin_height=min_bin_height,\n\t        min_derivative=min_derivative,\n", "    )\n\t    return outputs, logabsdet\n\tdef rational_quadratic_spline(\n\t    inputs,\n\t    unnormalized_widths,\n\t    unnormalized_heights,\n\t    unnormalized_derivatives,\n\t    inverse=False,\n\t    left=0.0,\n\t    right=1.0,\n", "    bottom=0.0,\n\t    top=1.0,\n\t    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n\t    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,\n\t    min_derivative=DEFAULT_MIN_DERIVATIVE,\n\t):\n\t    if torch.min(inputs) < left or torch.max(inputs) > right:\n\t        raise ValueError(\"Input to a transform is not within its domain\")\n\t    num_bins = unnormalized_widths.shape[-1]\n\t    if min_bin_width * num_bins > 1.0:\n", "        raise ValueError(\"Minimal bin width too large for the number of bins\")\n\t    if min_bin_height * num_bins > 1.0:\n\t        raise ValueError(\"Minimal bin height too large for the number of bins\")\n\t    widths = F.softmax(unnormalized_widths, dim=-1)\n\t    widths = min_bin_width + (1 - min_bin_width * num_bins) * widths\n\t    cumwidths = torch.cumsum(widths, dim=-1)\n\t    cumwidths = F.pad(cumwidths, pad=(1, 0), mode=\"constant\", value=0.0)\n\t    cumwidths = (right - left) * cumwidths + left\n\t    cumwidths[..., 0] = left\n\t    cumwidths[..., -1] = right\n", "    widths = cumwidths[..., 1:] - cumwidths[..., :-1]\n\t    derivatives = min_derivative + F.softplus(unnormalized_derivatives)\n\t    heights = F.softmax(unnormalized_heights, dim=-1)\n\t    heights = min_bin_height + (1 - min_bin_height * num_bins) * heights\n\t    cumheights = torch.cumsum(heights, dim=-1)\n\t    cumheights = F.pad(cumheights, pad=(1, 0), mode=\"constant\", value=0.0)\n\t    cumheights = (top - bottom) * cumheights + bottom\n\t    cumheights[..., 0] = bottom\n\t    cumheights[..., -1] = top\n\t    heights = cumheights[..., 1:] - cumheights[..., :-1]\n", "    if inverse:\n\t        bin_idx = searchsorted(cumheights, inputs)[..., None]\n\t    else:\n\t        bin_idx = searchsorted(cumwidths, inputs)[..., None]\n\t    input_cumwidths = cumwidths.gather(-1, bin_idx)[..., 0]\n\t    input_bin_widths = widths.gather(-1, bin_idx)[..., 0]\n\t    input_cumheights = cumheights.gather(-1, bin_idx)[..., 0]\n\t    delta = heights / widths\n\t    input_delta = delta.gather(-1, bin_idx)[..., 0]\n\t    input_derivatives = derivatives.gather(-1, bin_idx)[..., 0]\n", "    input_derivatives_plus_one = derivatives[..., 1:].gather(-1, bin_idx)[..., 0]\n\t    input_heights = heights.gather(-1, bin_idx)[..., 0]\n\t    if inverse:\n\t        a = (inputs - input_cumheights) * (\n\t            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n\t        ) + input_heights * (input_delta - input_derivatives)\n\t        b = input_heights * input_derivatives - (inputs - input_cumheights) * (\n\t            input_derivatives + input_derivatives_plus_one - 2 * input_delta\n\t        )\n\t        c = -input_delta * (inputs - input_cumheights)\n", "        discriminant = b.pow(2) - 4 * a * c\n\t        assert (discriminant >= 0).all()\n\t        root = (2 * c) / (-b - torch.sqrt(discriminant))\n\t        outputs = root * input_bin_widths + input_cumwidths\n\t        theta_one_minus_theta = root * (1 - root)\n\t        denominator = input_delta + (\n\t            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n\t            * theta_one_minus_theta\n\t        )\n\t        derivative_numerator = input_delta.pow(2) * (\n", "            input_derivatives_plus_one * root.pow(2)\n\t            + 2 * input_delta * theta_one_minus_theta\n\t            + input_derivatives * (1 - root).pow(2)\n\t        )\n\t        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\t        return outputs, -logabsdet\n\t    else:\n\t        theta = (inputs - input_cumwidths) / input_bin_widths\n\t        theta_one_minus_theta = theta * (1 - theta)\n\t        numerator = input_heights * (\n", "            input_delta * theta.pow(2) + input_derivatives * theta_one_minus_theta\n\t        )\n\t        denominator = input_delta + (\n\t            (input_derivatives + input_derivatives_plus_one - 2 * input_delta)\n\t            * theta_one_minus_theta\n\t        )\n\t        outputs = input_cumheights + numerator / denominator\n\t        derivative_numerator = input_delta.pow(2) * (\n\t            input_derivatives_plus_one * theta.pow(2)\n\t            + 2 * input_delta * theta_one_minus_theta\n", "            + input_derivatives * (1 - theta).pow(2)\n\t        )\n\t        logabsdet = torch.log(derivative_numerator) - 2 * torch.log(denominator)\n\t        return outputs, logabsdet\n"]}
{"filename": "lib/rvc/train.py", "chunked_list": ["import glob\n\timport json\n\timport operator\n\timport os\n\timport shutil\n\timport time\n\tfrom random import shuffle\n\tfrom typing import *\n\timport faiss\n\timport numpy as np\n", "import torch\n\timport torch.distributed as dist\n\timport torch.multiprocessing as mp\n\timport torchaudio\n\timport tqdm\n\tfrom sklearn.cluster import MiniBatchKMeans\n\tfrom torch.cuda.amp import GradScaler, autocast\n\tfrom torch.nn import functional as F\n\tfrom torch.nn.parallel import DistributedDataParallel as DDP\n\tfrom torch.utils.data import DataLoader\n", "from torch.utils.tensorboard import SummaryWriter\n\tfrom . import commons, utils\n\tfrom .checkpoints import save\n\tfrom .config import DatasetMetadata, TrainConfig\n\tfrom .data_utils import (DistributedBucketSampler, TextAudioCollate,\n\t                         TextAudioCollateMultiNSFsid, TextAudioLoader,\n\t                         TextAudioLoaderMultiNSFsid)\n\tfrom .losses import discriminator_loss, feature_loss, generator_loss, kl_loss\n\tfrom .mel_processing import mel_spectrogram_torch, spec_to_mel_torch\n\tfrom .models import (MultiPeriodDiscriminator, SynthesizerTrnMs256NSFSid,\n", "                     SynthesizerTrnMs256NSFSidNono)\n\tfrom .preprocessing.extract_feature import (MODELS_DIR, get_embedder,\n\t                                            load_embedder)\n\tdef is_audio_file(file: str):\n\t    if \".\" not in file:\n\t        return False\n\t    ext = os.path.splitext(file)[1]\n\t    return ext.lower() in [\n\t        \".wav\",\n\t        \".flac\",\n", "        \".ogg\",\n\t        \".mp3\",\n\t        \".m4a\",\n\t        \".wma\",\n\t        \".aiff\",\n\t    ]\n\tdef glob_dataset(\n\t    glob_str: str,\n\t    speaker_id: int,\n\t    multiple_speakers: bool = False,\n", "    recursive: bool = True,\n\t    training_dir: str = \".\",\n\t):\n\t    globs = glob_str.split(\",\")\n\t    speaker_count = 0\n\t    datasets_speakers = []\n\t    speaker_to_id_mapping = {}\n\t    for glob_str in globs:\n\t        if os.path.isdir(glob_str):\n\t            if multiple_speakers:\n", "                # Multispeaker format:\n\t                # dataset_path/\n\t                # - speakername/\n\t                #     - {wav name here}.wav\n\t                #     - ...\n\t                # - next_speakername/\n\t                #     - {wav name here}.wav\n\t                #     - ...\n\t                # - ...\n\t                print(\"Multispeaker dataset enabled; Processing speakers.\")\n", "                for dir in tqdm.tqdm(os.listdir(glob_str)):\n\t                    print(\"Speaker ID \" + str(speaker_count) + \": \" + dir)\n\t                    speaker_to_id_mapping[dir] = speaker_count\n\t                    speaker_path = glob_str + \"/\" + dir\n\t                    for audio in tqdm.tqdm(os.listdir(speaker_path)):\n\t                        if is_audio_file(glob_str + \"/\" + dir + \"/\" + audio):\n\t                            datasets_speakers.append((glob_str + \"/\" + dir + \"/\" + audio, speaker_count))\n\t                    speaker_count += 1\n\t                with open(os.path.join(training_dir, \"speaker_info.json\"), \"w\") as outfile:\n\t                    print(\"Dumped speaker info to {}\".format(os.path.join(training_dir, \"speaker_info.json\")))\n", "                    json.dump(speaker_to_id_mapping, outfile)\n\t                continue # Skip the normal speaker extend\n\t            glob_str = os.path.join(glob_str, \"**\", \"*\")\n\t        print(\"Single speaker dataset enabled; Processing speaker as ID \" + str(speaker_id) + \".\")\n\t        datasets_speakers.extend(\n\t            [\n\t                (file, speaker_id)\n\t                for file in glob.iglob(glob_str, recursive=recursive)\n\t                if is_audio_file(file)\n\t            ]\n", "        )\n\t    return sorted(datasets_speakers)\n\tdef create_dataset_meta(training_dir: str, f0: bool):\n\t    gt_wavs_dir = os.path.join(training_dir, \"0_gt_wavs\")\n\t    co256_dir = os.path.join(training_dir, \"3_feature256\")\n\t    def list_data(dir: str):\n\t        files = []\n\t        for subdir in os.listdir(dir):\n\t            speaker_dir = os.path.join(dir, subdir)\n\t            for name in os.listdir(speaker_dir):\n", "                files.append(os.path.join(subdir, name.split(\".\")[0]))\n\t        return files\n\t    names = set(list_data(gt_wavs_dir)) & set(list_data(co256_dir))\n\t    if f0:\n\t        f0_dir = os.path.join(training_dir, \"2a_f0\")\n\t        f0nsf_dir = os.path.join(training_dir, \"2b_f0nsf\")\n\t        names = names & set(list_data(f0_dir)) & set(list_data(f0nsf_dir))\n\t    meta = {\n\t        \"files\": {},\n\t    }\n", "    for name in names:\n\t        speaker_id = os.path.dirname(name).split(\"_\")[0]\n\t        speaker_id = int(speaker_id) if speaker_id.isdecimal() else 0\n\t        if f0:\n\t            gt_wav_path = os.path.join(gt_wavs_dir, f\"{name}.wav\")\n\t            co256_path = os.path.join(co256_dir, f\"{name}.npy\")\n\t            f0_path = os.path.join(f0_dir, f\"{name}.wav.npy\")\n\t            f0nsf_path = os.path.join(f0nsf_dir, f\"{name}.wav.npy\")\n\t            meta[\"files\"][name] = {\n\t                \"gt_wav\": gt_wav_path,\n", "                \"co256\": co256_path,\n\t                \"f0\": f0_path,\n\t                \"f0nsf\": f0nsf_path,\n\t                \"speaker_id\": speaker_id,\n\t            }\n\t        else:\n\t            gt_wav_path = os.path.join(gt_wavs_dir, f\"{name}.wav\")\n\t            co256_path = os.path.join(co256_dir, f\"{name}.npy\")\n\t            meta[\"files\"][name] = {\n\t                \"gt_wav\": gt_wav_path,\n", "                \"co256\": co256_path,\n\t                \"speaker_id\": speaker_id,\n\t            }\n\t    with open(os.path.join(training_dir, \"meta.json\"), \"w\") as f:\n\t        json.dump(meta, f, indent=2)\n\tdef change_speaker(net_g, speaker_info, embedder, embedding_output_layer, phone, phone_lengths, pitch, pitchf, spec_lengths):\n\t    \"\"\"\n\t    random change formant\n\t    inspired by https://github.com/auspicious3000/contentvec/blob/d746688a32940f4bee410ed7c87ec9cf8ff04f74/contentvec/data/audio/audio_utils_1.py#L179\n\t    \"\"\"\n", "    N = phone.shape[0]\n\t    device = phone.device\n\t    dtype = phone.dtype\n\t    f0_bin = 256\n\t    f0_max = 1100.0\n\t    f0_min = 50.0\n\t    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n\t    f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\t    pitch_median = torch.median(pitchf, 1).values\n\t    lo = 75. + 25. * (pitch_median >= 200).to(dtype=dtype)\n", "    hi = 250. + 150. * (pitch_median >= 200).to(dtype=dtype)\n\t    pitch_median = torch.clip(pitch_median, lo, hi).unsqueeze(1)\n\t    shift_pitch = torch.exp2((1. - 2. * torch.rand(N)) / 4).unsqueeze(1).to(device, dtype)   # ピッチを半オクターブの範囲でずらす\n\t    new_sid = np.random.choice(np.arange(len(speaker_info))[speaker_info > 0], size=N)\n\t    rel_pitch = pitchf / pitch_median\n\t    new_pitch_median = torch.from_numpy(speaker_info[new_sid]).to(device, dtype).unsqueeze(1) * shift_pitch\n\t    new_pitchf = new_pitch_median * rel_pitch\n\t    new_sid = torch.from_numpy(new_sid).to(device)\n\t    new_pitch = 1127. * torch.log(1. + new_pitchf / 700.)\n\t    new_pitch = (pitch - f0_mel_min) * (f0_bin - 2.) / (f0_mel_max - f0_mel_min) + 1.\n", "    new_pitch = torch.clip(new_pitch, 1, f0_bin - 1).to(dtype=torch.int)\n\t    aug_wave = net_g.infer(phone, phone_lengths, new_pitch, new_pitchf, new_sid)[0]\n\t    aug_wave_16k = torchaudio.functional.resample(aug_wave, net_g.sr, 16000, rolloff=0.99).squeeze(1)\n\t    padding_mask = torch.arange(aug_wave_16k.shape[1]).unsqueeze(0).to(device) > (spec_lengths.unsqueeze(1) * 160).to(device)\n\t    inputs = {\n\t        \"source\": aug_wave_16k.to(device, dtype),\n\t        \"padding_mask\": padding_mask.to(device),\n\t        \"output_layer\": embedding_output_layer\n\t    }\n\t    logits = embedder.extract_features(**inputs)\n", "    if phone.shape[-1] == 768:\n\t        feats = logits[0]\n\t    else:\n\t        feats = embedder.final_proj(logits[0])\n\t    feats = torch.repeat_interleave(feats, 2, 1)\n\t    new_phone = torch.zeros(phone.shape).to(device, dtype)\n\t    new_phone[:, :feats.shape[1]] = feats[:, :phone.shape[1]]\n\t    return new_phone.to(device), aug_wave\n\tdef change_speaker_nono(net_g, embedder, embedding_output_layer, phone, phone_lengths, spec_lengths):\n\t    \"\"\"\n", "    random change formant\n\t    inspired by https://github.com/auspicious3000/contentvec/blob/d746688a32940f4bee410ed7c87ec9cf8ff04f74/contentvec/data/audio/audio_utils_1.py#L179\n\t    \"\"\"\n\t    N = phone.shape[0]\n\t    device = phone.device\n\t    dtype = phone.dtype\n\t    new_sid = np.random.randint(net_g.spk_embed_dim, size=N)\n\t    new_sid = torch.from_numpy(new_sid).to(device)\n\t    aug_wave = net_g.infer(phone, phone_lengths, new_sid)[0]\n\t    aug_wave_16k = torchaudio.functional.resample(aug_wave, net_g.sr, 16000, rolloff=0.99).squeeze(1)\n", "    padding_mask = torch.arange(aug_wave_16k.shape[1]).unsqueeze(0).to(device) > (spec_lengths.unsqueeze(1) * 160).to(device)\n\t    inputs = {\n\t        \"source\": aug_wave_16k.to(device, dtype),\n\t        \"padding_mask\": padding_mask.to(device),\n\t        \"output_layer\": embedding_output_layer\n\t    }\n\t    logits = embedder.extract_features(**inputs)\n\t    if phone.shape[-1] == 768:\n\t        feats = logits[0]\n\t    else:\n", "        feats = embedder.final_proj(logits[0])\n\t    feats = torch.repeat_interleave(feats, 2, 1)\n\t    new_phone = torch.zeros(phone.shape).to(device, dtype)\n\t    new_phone[:, :feats.shape[1]] = feats[:, :phone.shape[1]]\n\t    return new_phone.to(device), aug_wave\n\tdef train_index(\n\t    training_dir: str,\n\t    model_name: str,\n\t    out_dir: str,\n\t    emb_ch: int,\n", "    num_cpu_process: int,\n\t    maximum_index_size: Optional[int],\n\t):\n\t    checkpoint_path = os.path.join(out_dir, model_name)\n\t    feature_256_dir = os.path.join(training_dir, \"3_feature256\")\n\t    index_dir = os.path.join(os.path.dirname(checkpoint_path), f\"{model_name}_index\")\n\t    os.makedirs(index_dir, exist_ok=True)\n\t    for speaker_id in tqdm.tqdm(\n\t        sorted([dir for dir in os.listdir(feature_256_dir) if dir.isdecimal()])\n\t    ):\n", "        feature_256_spk_dir = os.path.join(feature_256_dir, speaker_id)\n\t        speaker_id = int(speaker_id)\n\t        npys = []\n\t        for name in [\n\t            os.path.join(feature_256_spk_dir, file)\n\t            for file in os.listdir(feature_256_spk_dir)\n\t            if file.endswith(\".npy\")\n\t        ]:\n\t            phone = np.load(os.path.join(feature_256_spk_dir, name))\n\t            npys.append(phone)\n", "        # shuffle big_npy to prevent reproducing the sound source\n\t        big_npy = np.concatenate(npys, 0)\n\t        big_npy_idx = np.arange(big_npy.shape[0])\n\t        np.random.shuffle(big_npy_idx)\n\t        big_npy = big_npy[big_npy_idx]\n\t        if not maximum_index_size is None and big_npy.shape[0] > maximum_index_size:\n\t            kmeans = MiniBatchKMeans(\n\t                n_clusters=maximum_index_size,\n\t                batch_size=256 * num_cpu_process,\n\t                init=\"random\",\n", "                compute_labels=False,\n\t            )\n\t            kmeans.fit(big_npy)\n\t            big_npy = kmeans.cluster_centers_\n\t        # recommend parameter in https://github.com/facebookresearch/faiss/wiki/Guidelines-to-choose-an-index\n\t        emb_ch = big_npy.shape[1]\n\t        emb_ch_half = emb_ch // 2\n\t        n_ivf = int(8 * np.sqrt(big_npy.shape[0]))\n\t        if big_npy.shape[0] >= 1_000_000:\n\t            index = faiss.index_factory(\n", "                emb_ch, f\"IVF{n_ivf},PQ{emb_ch_half}x4fsr,RFlat\"\n\t            )\n\t        else:\n\t            index = faiss.index_factory(emb_ch, f\"IVF{n_ivf},Flat\")\n\t        index.train(big_npy)\n\t        batch_size_add = 8192\n\t        for i in range(0, big_npy.shape[0], batch_size_add):\n\t            index.add(big_npy[i : i + batch_size_add])\n\t        np.save(\n\t            os.path.join(index_dir, f\"{model_name}.{speaker_id}.big.npy\"),\n", "            big_npy,\n\t        )\n\t        faiss.write_index(\n\t            index,\n\t            os.path.join(index_dir, f\"{model_name}.{speaker_id}.index\"),\n\t        )\n\tdef train_model(\n\t    gpus: List[int],\n\t    config: TrainConfig,\n\t    training_dir: str,\n", "    model_name: str,\n\t    out_dir: str,\n\t    sample_rate: int,\n\t    f0: bool,\n\t    batch_size: int,\n\t    augment: bool,\n\t    augment_path: Optional[str],\n\t    speaker_info_path: Optional[str],\n\t    cache_batch: bool,\n\t    total_epoch: int,\n", "    save_every_epoch: int,\n\t    save_wav_with_checkpoint: bool,\n\t    pretrain_g: str,\n\t    pretrain_d: str,\n\t    embedder_name: str,\n\t    embedding_output_layer: int,\n\t    save_only_last: bool = False,\n\t    device: Optional[Union[str, torch.device]] = None,\n\t):\n\t    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n", "    os.environ[\"MASTER_PORT\"] = str(utils.find_empty_port())\n\t    deterministic = torch.backends.cudnn.deterministic\n\t    benchmark = torch.backends.cudnn.benchmark\n\t    PREV_CUDA_VISIBLE_DEVICES = os.environ.get(\"CUDA_VISIBLE_DEVICES\", None)\n\t    torch.backends.cudnn.deterministic = False\n\t    torch.backends.cudnn.benchmark = False\n\t    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join([str(gpu) for gpu in gpus])\n\t    start = time.perf_counter()\n\t    # Mac(MPS)でやると、mp.spawnでなんかトラブルが出るので普通にtraining_runnerを呼び出す。\n\t    if device is not None:\n", "        training_runner(\n\t            0,  # rank\n\t            1,  # world size\n\t            config,\n\t            training_dir,\n\t            model_name,\n\t            out_dir,\n\t            sample_rate,\n\t            f0,\n\t            batch_size,\n", "            augment,\n\t            augment_path,\n\t            speaker_info_path,\n\t            cache_batch,\n\t            total_epoch,\n\t            save_every_epoch,\n\t            save_wav_with_checkpoint,\n\t            pretrain_g,\n\t            pretrain_d,\n\t            embedder_name,\n", "            embedding_output_layer,\n\t            save_only_last,\n\t            device,\n\t        )\n\t    else:\n\t        mp.spawn(\n\t            training_runner,\n\t            nprocs=len(gpus),\n\t            args=(\n\t                len(gpus),\n", "                config,\n\t                training_dir,\n\t                model_name,\n\t                out_dir,\n\t                sample_rate,\n\t                f0,\n\t                batch_size,\n\t                augment,\n\t                augment_path,\n\t                speaker_info_path,\n", "                cache_batch,\n\t                total_epoch,\n\t                save_every_epoch,\n\t                save_wav_with_checkpoint,\n\t                pretrain_g,\n\t                pretrain_d,\n\t                embedder_name,\n\t                embedding_output_layer,\n\t                save_only_last,\n\t                device,\n", "            ),\n\t        )\n\t    end = time.perf_counter()\n\t    print(f\"Time: {end - start}\")\n\t    if PREV_CUDA_VISIBLE_DEVICES is None:\n\t        del os.environ[\"CUDA_VISIBLE_DEVICES\"]\n\t    else:\n\t        os.environ[\"CUDA_VISIBLE_DEVICES\"] = PREV_CUDA_VISIBLE_DEVICES\n\t    torch.backends.cudnn.deterministic = deterministic\n\t    torch.backends.cudnn.benchmark = benchmark\n", "def training_runner(\n\t    rank: int,\n\t    world_size: List[int],\n\t    config: TrainConfig,\n\t    training_dir: str,\n\t    model_name: str,\n\t    out_dir: str,\n\t    sample_rate: int,\n\t    f0: bool,\n\t    batch_size: int,\n", "    augment: bool,\n\t    augment_path: Optional[str],\n\t    speaker_info_path: Optional[str],\n\t    cache_in_gpu: bool,\n\t    total_epoch: int,\n\t    save_every_epoch: int,\n\t    save_wav_with_checkpoint: bool,\n\t    pretrain_g: str,\n\t    pretrain_d: str,\n\t    embedder_name: str,\n", "    embedding_output_layer: int,\n\t    save_only_last: bool = False,\n\t    device: Optional[Union[str, torch.device]] = None,\n\t):\n\t    config.train.batch_size = batch_size\n\t    log_dir = os.path.join(training_dir, \"logs\")\n\t    state_dir = os.path.join(training_dir, \"state\")\n\t    training_files_path = os.path.join(training_dir, \"meta.json\")\n\t    training_meta = DatasetMetadata.parse_file(training_files_path)\n\t    embedder_out_channels = config.model.emb_channels\n", "    is_multi_process = world_size > 1\n\t    if device is not None:\n\t        if type(device) == str:\n\t            device = torch.device(device)\n\t    global_step = 0\n\t    is_main_process = rank == 0\n\t    if is_main_process:\n\t        os.makedirs(log_dir, exist_ok=True)\n\t        os.makedirs(state_dir, exist_ok=True)\n\t        writer = SummaryWriter(log_dir=log_dir)\n", "    if torch.cuda.is_available():\n\t        torch.cuda.empty_cache()\n\t    if not dist.is_initialized():\n\t        dist.init_process_group(\n\t            backend=\"gloo\", init_method=\"env://\", rank=rank, world_size=world_size\n\t        )\n\t    if is_multi_process:\n\t        torch.cuda.set_device(rank)\n\t    torch.manual_seed(config.train.seed)\n\t    if f0:\n", "        train_dataset = TextAudioLoaderMultiNSFsid(training_meta, config.data)\n\t    else:\n\t        train_dataset = TextAudioLoader(training_meta, config.data)\n\t    train_sampler = DistributedBucketSampler(\n\t        train_dataset,\n\t        config.train.batch_size * world_size,\n\t        [100, 200, 300, 400, 500, 600, 700, 800, 900],\n\t        num_replicas=world_size,\n\t        rank=rank,\n\t        shuffle=True,\n", "    )\n\t    if f0:\n\t        collate_fn = TextAudioCollateMultiNSFsid()\n\t    else:\n\t        collate_fn = TextAudioCollate()\n\t    train_loader = DataLoader(\n\t        train_dataset,\n\t        num_workers=4,\n\t        shuffle=False,\n\t        pin_memory=True,\n", "        collate_fn=collate_fn,\n\t        batch_sampler=train_sampler,\n\t        persistent_workers=True,\n\t        prefetch_factor=8,\n\t    )\n\t    speaker_info = None\n\t    if os.path.exists(os.path.join(training_dir, \"speaker_info.json\")):\n\t        with open(os.path.join(training_dir, \"speaker_info.json\"), \"r\") as f:\n\t            speaker_info = json.load(f)\n\t            config.model.spk_embed_dim = len(speaker_info)\n", "    if f0:\n\t        net_g = SynthesizerTrnMs256NSFSid(\n\t            config.data.filter_length // 2 + 1,\n\t            config.train.segment_size // config.data.hop_length,\n\t            **config.model.dict(),\n\t            is_half=False, # config.train.fp16_run,\n\t            sr=int(sample_rate[:-1] + \"000\"),\n\t        )\n\t    else:\n\t        net_g = SynthesizerTrnMs256NSFSidNono(\n", "            config.data.filter_length // 2 + 1,\n\t            config.train.segment_size // config.data.hop_length,\n\t            **config.model.dict(),\n\t            is_half=False, # config.train.fp16_run,\n\t            sr=int(sample_rate[:-1] + \"000\"),\n\t        )\n\t    if is_multi_process:\n\t        net_g = net_g.cuda(rank)\n\t    else:\n\t        net_g = net_g.to(device=device)\n", "    if config.version == \"v1\":\n\t        periods = [2, 3, 5, 7, 11, 17]\n\t    elif config.version == \"v2\":\n\t        periods = [2, 3, 5, 7, 11, 17, 23, 37]\n\t    net_d = MultiPeriodDiscriminator(config.model.use_spectral_norm, periods=periods)\n\t    if is_multi_process:\n\t        net_d = net_d.cuda(rank)\n\t    else:\n\t        net_d = net_d.to(device=device)\n\t    optim_g = torch.optim.AdamW(\n", "        net_g.parameters(),\n\t        config.train.learning_rate,\n\t        betas=config.train.betas,\n\t        eps=config.train.eps,\n\t    )\n\t    optim_d = torch.optim.AdamW(\n\t        net_d.parameters(),\n\t        config.train.learning_rate,\n\t        betas=config.train.betas,\n\t        eps=config.train.eps,\n", "    )\n\t    last_d_state = utils.latest_checkpoint_path(state_dir, \"D_*.pth\")\n\t    last_g_state = utils.latest_checkpoint_path(state_dir, \"G_*.pth\")\n\t    if last_d_state is None or last_g_state is None:\n\t        epoch = 1\n\t        global_step = 0\n\t        if os.path.exists(pretrain_g) and os.path.exists(pretrain_d):\n\t            net_g_state = torch.load(pretrain_g, map_location=\"cpu\")[\"model\"]\n\t            emb_spk_size = (config.model.spk_embed_dim, config.model.gin_channels)\n\t            emb_phone_size = (config.model.hidden_channels, config.model.emb_channels)\n", "            if emb_spk_size != net_g_state[\"emb_g.weight\"].size():\n\t                original_weight = net_g_state[\"emb_g.weight\"]\n\t                net_g_state[\"emb_g.weight\"] = original_weight.mean(dim=0, keepdims=True) * torch.ones(emb_spk_size, device=original_weight.device, dtype=original_weight.dtype)\n\t            if emb_phone_size != net_g_state[\"enc_p.emb_phone.weight\"].size():\n\t                # interpolate\n\t                orig_shape = net_g_state[\"enc_p.emb_phone.weight\"].size()\n\t                if net_g_state[\"enc_p.emb_phone.weight\"].dtype == torch.half:\n\t                    net_g_state[\"enc_p.emb_phone.weight\"] = (\n\t                        F.interpolate(\n\t                            net_g_state[\"enc_p.emb_phone.weight\"]\n", "                            .float()\n\t                            .unsqueeze(0)\n\t                            .unsqueeze(0),\n\t                            size=emb_phone_size,\n\t                            mode=\"bilinear\",\n\t                        )\n\t                        .half()\n\t                        .squeeze(0)\n\t                        .squeeze(0)\n\t                    )\n", "                else:\n\t                    net_g_state[\"enc_p.emb_phone.weight\"] = (\n\t                        F.interpolate(\n\t                            net_g_state[\"enc_p.emb_phone.weight\"]\n\t                            .unsqueeze(0)\n\t                            .unsqueeze(0),\n\t                            size=emb_phone_size,\n\t                            mode=\"bilinear\",\n\t                        )\n\t                        .squeeze(0)\n", "                        .squeeze(0)\n\t                    )\n\t                print(\n\t                    \"interpolated pretrained state enc_p.emb_phone from\",\n\t                    orig_shape,\n\t                    \"to\",\n\t                    emb_phone_size,\n\t                )\n\t            if is_multi_process:\n\t                net_g.module.load_state_dict(net_g_state)\n", "            else:\n\t                net_g.load_state_dict(net_g_state)\n\t            del net_g_state\n\t            if is_multi_process:\n\t                net_d.module.load_state_dict(\n\t                    torch.load(pretrain_d, map_location=\"cpu\")[\"model\"]\n\t                )\n\t            else:\n\t                net_d.load_state_dict(\n\t                    torch.load(pretrain_d, map_location=\"cpu\")[\"model\"]\n", "                )\n\t            if is_main_process:\n\t                print(f\"loaded pretrained {pretrain_g} {pretrain_d}\")\n\t    else:\n\t        _, _, _, epoch = utils.load_checkpoint(last_d_state, net_d, optim_d)\n\t        _, _, _, epoch = utils.load_checkpoint(last_g_state, net_g, optim_g)\n\t        if is_main_process:\n\t            print(f\"loaded last state {last_d_state} {last_g_state}\")\n\t        epoch += 1\n\t        global_step = (epoch - 1) * len(train_loader)\n", "    if augment:\n\t        # load embedder\n\t        embedder_filepath, _, embedder_load_from = get_embedder(embedder_name)\n\t        if embedder_load_from == \"local\":\n\t            embedder_filepath = os.path.join(\n\t                MODELS_DIR, \"embeddings\", embedder_filepath\n\t            )\n\t        embedder, _ = load_embedder(embedder_filepath, device)\n\t        if not config.train.fp16_run:\n\t            embedder = embedder.float()\n", "        if (augment_path is not None):\n\t            state_dict = torch.load(augment_path, map_location=\"cpu\")\n\t            if state_dict[\"f0\"] == 1:\n\t                augment_net_g = SynthesizerTrnMs256NSFSid(\n\t                    **state_dict[\"params\"], is_half=config.train.fp16_run\n\t                )\n\t                augment_speaker_info = np.load(speaker_info_path)\n\t            else:\n\t                augment_net_g = SynthesizerTrnMs256NSFSidNono(\n\t                    **state_dict[\"params\"], is_half=config.train.fp16_run\n", "                )\n\t            augment_net_g.load_state_dict(state_dict[\"weight\"], strict=False)\n\t            augment_net_g.eval().to(device)\n\t        else:\n\t            augment_net_g = net_g\n\t            if f0:\n\t                medians = [[] for _ in range(augment_net_g.spk_embed_dim)]\n\t                for file in training_meta.files.values():\n\t                    f0f = np.load(file.f0nsf)\n\t                    if np.any(f0f > 0):\n", "                        medians[file.speaker_id].append(np.median(f0f[f0f > 0]))\n\t                augment_speaker_info = np.array([np.median(x) if len(x) else 0. for x in medians])\n\t                np.save(os.path.join(training_dir, \"speaker_info.npy\"), augment_speaker_info)\n\t    if is_multi_process:\n\t        net_g = DDP(net_g, device_ids=[rank])\n\t        net_d = DDP(net_d, device_ids=[rank])\n\t    scheduler_g = torch.optim.lr_scheduler.ExponentialLR(\n\t        optim_g, gamma=config.train.lr_decay, last_epoch=epoch - 2\n\t    )\n\t    scheduler_d = torch.optim.lr_scheduler.ExponentialLR(\n", "        optim_d, gamma=config.train.lr_decay, last_epoch=epoch - 2\n\t    )\n\t    scaler = GradScaler(enabled=config.train.fp16_run)\n\t    cache = []\n\t    progress_bar = tqdm.tqdm(range((total_epoch - epoch + 1) * len(train_loader)))\n\t    progress_bar.set_postfix(epoch=epoch)\n\t    step = -1 + len(train_loader) * (epoch - 1)\n\t    for epoch in range(epoch, total_epoch + 1):\n\t        train_loader.batch_sampler.set_epoch(epoch)\n\t        net_g.train()\n", "        net_d.train()\n\t        use_cache = len(cache) == len(train_loader)\n\t        data = cache if use_cache else enumerate(train_loader)\n\t        if is_main_process:\n\t            lr = optim_g.param_groups[0][\"lr\"]\n\t        if use_cache:\n\t            shuffle(cache)\n\t        for batch_idx, batch in data:\n\t            step += 1\n\t            progress_bar.update(1)\n", "            if f0:\n\t                (\n\t                    phone,\n\t                    phone_lengths,\n\t                    pitch,\n\t                    pitchf,\n\t                    spec,\n\t                    spec_lengths,\n\t                    wave,\n\t                    wave_lengths,\n", "                    sid,\n\t                ) = batch\n\t            else:\n\t                (\n\t                    phone,\n\t                    phone_lengths,\n\t                    spec,\n\t                    spec_lengths,\n\t                    wave,\n\t                    wave_lengths,\n", "                    sid,\n\t                ) = batch\n\t            if not use_cache:\n\t                phone, phone_lengths = (\n\t                    phone.to(device=device, non_blocking=True),\n\t                    phone_lengths.to(device=device, non_blocking=True),\n\t                )\n\t                if f0:\n\t                    pitch, pitchf = (\n\t                        pitch.to(device=device, non_blocking=True),\n", "                        pitchf.to(device=device, non_blocking=True),\n\t                    )\n\t                sid = sid.to(device=device, non_blocking=True)\n\t                spec, spec_lengths = (\n\t                    spec.to(device=device, non_blocking=True),\n\t                    spec_lengths.to(device=device, non_blocking=True),\n\t                )\n\t                wave, wave_lengths = (\n\t                    wave.to(device=device, non_blocking=True),\n\t                    wave_lengths.to(device=device, non_blocking=True),\n", "                )\n\t                if cache_in_gpu:\n\t                    if f0:\n\t                        cache.append(\n\t                            (\n\t                                batch_idx,\n\t                                (\n\t                                    phone,\n\t                                    phone_lengths,\n\t                                    pitch,\n", "                                    pitchf,\n\t                                    spec,\n\t                                    spec_lengths,\n\t                                    wave,\n\t                                    wave_lengths,\n\t                                    sid,\n\t                                ),\n\t                            )\n\t                        )\n\t                    else:\n", "                        cache.append(\n\t                            (\n\t                                batch_idx,\n\t                                (\n\t                                    phone,\n\t                                    phone_lengths,\n\t                                    spec,\n\t                                    spec_lengths,\n\t                                    wave,\n\t                                    wave_lengths,\n", "                                    sid,\n\t                                ),\n\t                            )\n\t                        )\n\t            with autocast(enabled=config.train.fp16_run):\n\t                if augment:\n\t                    with torch.no_grad():\n\t                        if type(augment_net_g) == SynthesizerTrnMs256NSFSid:\n\t                            new_phone, aug_wave = change_speaker(augment_net_g, augment_speaker_info, embedder, embedding_output_layer, phone, phone_lengths, pitch, pitchf, spec_lengths)\n\t                        else:\n", "                            new_phone, aug_wave = change_speaker_nono(augment_net_g, embedder, embedding_output_layer, phone, phone_lengths, spec_lengths)\n\t                        weight = np.power(.5, step / len(train_loader))  # 学習の初期はそのままのphone embeddingを使う\n\t                        phone = phone * weight + new_phone * (1. - weight)\n\t                if f0:\n\t                    (\n\t                        y_hat,\n\t                        ids_slice,\n\t                        x_mask,\n\t                        z_mask,\n\t                        (z, z_p, m_p, logs_p, m_q, logs_q),\n", "                    ) = net_g(\n\t                        phone, phone_lengths, pitch, pitchf, spec, spec_lengths, sid\n\t                    )\n\t                else:\n\t                    (\n\t                        y_hat,\n\t                        ids_slice,\n\t                        x_mask,\n\t                        z_mask,\n\t                        (z, z_p, m_p, logs_p, m_q, logs_q),\n", "                    ) = net_g(phone, phone_lengths, spec, spec_lengths, sid)\n\t                mel = spec_to_mel_torch(\n\t                    spec,\n\t                    config.data.filter_length,\n\t                    config.data.n_mel_channels,\n\t                    config.data.sampling_rate,\n\t                    config.data.mel_fmin,\n\t                    config.data.mel_fmax,\n\t                )\n\t                y_mel = commons.slice_segments(\n", "                    mel, ids_slice, config.train.segment_size // config.data.hop_length\n\t                )\n\t                with autocast(enabled=False):\n\t                    y_hat_mel = mel_spectrogram_torch(\n\t                        y_hat.float().squeeze(1),\n\t                        config.data.filter_length,\n\t                        config.data.n_mel_channels,\n\t                        config.data.sampling_rate,\n\t                        config.data.hop_length,\n\t                        config.data.win_length,\n", "                        config.data.mel_fmin,\n\t                        config.data.mel_fmax,\n\t                    )\n\t                if config.train.fp16_run == True and device != torch.device(\"mps\"):\n\t                    y_hat_mel = y_hat_mel.half()\n\t                wave_slice = commons.slice_segments(\n\t                    wave, ids_slice * config.data.hop_length, config.train.segment_size\n\t                )  # slice\n\t                # Discriminator\n\t                y_d_hat_r, y_d_hat_g, _, _ = net_d(wave_slice, y_hat.detach())\n", "                with autocast(enabled=False):\n\t                    loss_disc, losses_disc_r, losses_disc_g = discriminator_loss(\n\t                        y_d_hat_r, y_d_hat_g\n\t                    )\n\t            optim_d.zero_grad()\n\t            scaler.scale(loss_disc).backward()\n\t            scaler.unscale_(optim_d)\n\t            grad_norm_d = commons.clip_grad_value_(net_d.parameters(), None)\n\t            scaler.step(optim_d)\n\t            with autocast(enabled=config.train.fp16_run):\n", "                # Generator\n\t                y_d_hat_r, y_d_hat_g, fmap_r, fmap_g = net_d(wave_slice, y_hat)\n\t                with autocast(enabled=False):\n\t                    loss_mel = F.l1_loss(y_mel, y_hat_mel) * config.train.c_mel\n\t                    loss_kl = (\n\t                        kl_loss(z_p, logs_q, m_p, logs_p, z_mask) * config.train.c_kl\n\t                    )\n\t                    loss_fm = feature_loss(fmap_r, fmap_g)\n\t                    loss_gen, losses_gen = generator_loss(y_d_hat_g)\n\t                    loss_gen_all = loss_gen + loss_fm + loss_mel + loss_kl\n", "            optim_g.zero_grad()\n\t            scaler.scale(loss_gen_all).backward()\n\t            scaler.unscale_(optim_g)\n\t            grad_norm_g = commons.clip_grad_value_(net_g.parameters(), None)\n\t            scaler.step(optim_g)\n\t            scaler.update()\n\t            if is_main_process:\n\t                progress_bar.set_postfix(\n\t                    epoch=epoch,\n\t                    loss_g=float(loss_gen_all) if loss_gen_all is not None else 0.0,\n", "                    loss_d=float(loss_disc) if loss_disc is not None else 0.0,\n\t                    lr=float(lr) if lr is not None else 0.0,\n\t                    use_cache=use_cache,\n\t                )\n\t                if global_step % config.train.log_interval == 0:\n\t                    lr = optim_g.param_groups[0][\"lr\"]\n\t                    # Amor For Tensorboard display\n\t                    if loss_mel > 50:\n\t                        loss_mel = 50\n\t                    if loss_kl > 5:\n", "                        loss_kl = 5\n\t                    scalar_dict = {\n\t                        \"loss/g/total\": loss_gen_all,\n\t                        \"loss/d/total\": loss_disc,\n\t                        \"learning_rate\": lr,\n\t                        \"grad_norm_d\": grad_norm_d,\n\t                        \"grad_norm_g\": grad_norm_g,\n\t                    }\n\t                    scalar_dict.update(\n\t                        {\n", "                            \"loss/g/fm\": loss_fm,\n\t                            \"loss/g/mel\": loss_mel,\n\t                            \"loss/g/kl\": loss_kl,\n\t                        }\n\t                    )\n\t                    scalar_dict.update(\n\t                        {\"loss/g/{}\".format(i): v for i, v in enumerate(losses_gen)}\n\t                    )\n\t                    scalar_dict.update(\n\t                        {\n", "                            \"loss/d_r/{}\".format(i): v\n\t                            for i, v in enumerate(losses_disc_r)\n\t                        }\n\t                    )\n\t                    scalar_dict.update(\n\t                        {\n\t                            \"loss/d_g/{}\".format(i): v\n\t                            for i, v in enumerate(losses_disc_g)\n\t                        }\n\t                    )\n", "                    image_dict = {\n\t                        \"slice/mel_org\": utils.plot_spectrogram_to_numpy(\n\t                            y_mel[0].data.cpu().numpy()\n\t                        ),\n\t                        \"slice/mel_gen\": utils.plot_spectrogram_to_numpy(\n\t                            y_hat_mel[0].data.cpu().numpy()\n\t                        ),\n\t                        \"all/mel\": utils.plot_spectrogram_to_numpy(\n\t                            mel[0].data.cpu().numpy()\n\t                        ),\n", "                    }\n\t                    utils.summarize(\n\t                        writer=writer,\n\t                        global_step=global_step,\n\t                        images=image_dict,\n\t                        scalars=scalar_dict,\n\t                    )\n\t            global_step += 1\n\t        if is_main_process and save_every_epoch != 0 and epoch % save_every_epoch == 0:\n\t            if save_only_last:\n", "                old_g_path = os.path.join(\n\t                    state_dir, f\"G_{epoch - save_every_epoch}.pth\"\n\t                )\n\t                old_d_path = os.path.join(\n\t                    state_dir, f\"D_{epoch - save_every_epoch}.pth\"\n\t                )\n\t                old_wav_path = os.path.join(\n\t                    state_dir, f\"wav_sample_{epoch - save_every_epoch}\"\n\t                )\n\t                if os.path.exists(old_g_path):\n", "                    os.remove(old_g_path)\n\t                if os.path.exists(old_d_path):\n\t                    os.remove(old_d_path)\n\t                if os.path.exists(old_wav_path):\n\t                    shutil.rmtree(old_wav_path)\n\t            if save_wav_with_checkpoint:\n\t                with autocast(enabled=config.train.fp16_run):\n\t                    with torch.no_grad():\n\t                        if f0:\n\t                            pred_wave = net_g.infer(phone, phone_lengths, pitch, pitchf, sid)[0]\n", "                        else:\n\t                            pred_wave = net_g.infer(phone, phone_lengths, sid)[0]\n\t                os.makedirs(os.path.join(state_dir, f\"wav_sample_{epoch}\"), exist_ok=True)\n\t                for i in range(pred_wave.shape[0]):\n\t                    torchaudio.save(filepath=os.path.join(state_dir, f\"wav_sample_{epoch}\", f\"{i:02}_y_true.wav\"), src=wave[i].detach().cpu().float(), sample_rate=int(sample_rate[:-1] + \"000\"))\n\t                    torchaudio.save(filepath=os.path.join(state_dir, f\"wav_sample_{epoch}\", f\"{i:02}_y_pred.wav\"), src=pred_wave[i].detach().cpu().float(), sample_rate=int(sample_rate[:-1] + \"000\"))\n\t                    if augment:\n\t                        torchaudio.save(filepath=os.path.join(state_dir, f\"wav_sample_{epoch}\", f\"{i:02}_y_aug.wav\"), src=aug_wave[i].detach().cpu().float(), sample_rate=int(sample_rate[:-1] + \"000\"))\n\t            utils.save_state(\n\t                net_g,\n", "                optim_g,\n\t                config.train.learning_rate,\n\t                epoch,\n\t                os.path.join(state_dir, f\"G_{epoch}.pth\"),\n\t            )\n\t            utils.save_state(\n\t                net_d,\n\t                optim_d,\n\t                config.train.learning_rate,\n\t                epoch,\n", "                os.path.join(state_dir, f\"D_{epoch}.pth\"),\n\t            )\n\t            save(\n\t                net_g,\n\t                config.version,\n\t                sample_rate,\n\t                f0,\n\t                embedder_name,\n\t                embedder_out_channels,\n\t                embedding_output_layer,\n", "                os.path.join(training_dir, \"checkpoints\", f\"{model_name}-{epoch}.pth\"),\n\t                epoch,\n\t                speaker_info\n\t            )\n\t        scheduler_g.step()\n\t        scheduler_d.step()\n\t    if is_main_process:\n\t        print(\"Training is done. The program is closed.\")\n\t        save(\n\t            net_g,\n", "            config.version,\n\t            sample_rate,\n\t            f0,\n\t            embedder_name,\n\t            embedder_out_channels,\n\t            embedding_output_layer,\n\t            os.path.join(out_dir, f\"{model_name}.pth\"),\n\t            epoch,\n\t            speaker_info\n\t        )\n"]}
{"filename": "lib/rvc/config.py", "chunked_list": ["from typing import *\n\tfrom pydantic import BaseModel\n\tclass TrainConfigTrain(BaseModel):\n\t    log_interval: int\n\t    seed: int\n\t    epochs: int\n\t    learning_rate: float\n\t    betas: List[float]\n\t    eps: float\n\t    batch_size: int\n", "    fp16_run: bool\n\t    lr_decay: float\n\t    segment_size: int\n\t    init_lr_ratio: int\n\t    warmup_epochs: int\n\t    c_mel: int\n\t    c_kl: float\n\tclass TrainConfigData(BaseModel):\n\t    max_wav_value: float\n\t    sampling_rate: int\n", "    filter_length: int\n\t    hop_length: int\n\t    win_length: int\n\t    n_mel_channels: int\n\t    mel_fmin: float\n\t    mel_fmax: Any\n\tclass TrainConfigModel(BaseModel):\n\t    inter_channels: int\n\t    hidden_channels: int\n\t    filter_channels: int\n", "    n_heads: int\n\t    n_layers: int\n\t    kernel_size: int\n\t    p_dropout: int\n\t    resblock: str\n\t    resblock_kernel_sizes: List[int]\n\t    resblock_dilation_sizes: List[List[int]]\n\t    upsample_rates: List[int]\n\t    upsample_initial_channel: int\n\t    upsample_kernel_sizes: List[int]\n", "    use_spectral_norm: bool\n\t    gin_channels: int\n\t    emb_channels: int\n\t    spk_embed_dim: int\n\tclass TrainConfig(BaseModel):\n\t    version: Literal[\"v1\", \"v2\"] = \"v2\"\n\t    train: TrainConfigTrain\n\t    data: TrainConfigData\n\t    model: TrainConfigModel\n\tclass DatasetMetaItem(BaseModel):\n", "    gt_wav: str\n\t    co256: str\n\t    f0: Optional[str]\n\t    f0nsf: Optional[str]\n\t    speaker_id: int\n\tclass DatasetMetadata(BaseModel):\n\t    files: Dict[str, DatasetMetaItem]\n\t    # mute: DatasetMetaItem\n"]}
{"filename": "lib/rvc/data_utils.py", "chunked_list": ["import os\n\timport traceback\n\timport numpy as np\n\timport torch\n\timport torch.utils.data\n\tfrom .config import DatasetMetadata, DatasetMetaItem, TrainConfigData\n\tfrom .mel_processing import spectrogram_torch\n\tfrom .utils import load_wav_to_torch\n\tclass TextAudioLoader(torch.utils.data.Dataset):\n\t    \"\"\"\n", "    1) loads audio, text pairs\n\t    2) normalizes text and converts them to sequences of integers\n\t    3) computes spectrograms from audio files.\n\t    \"\"\"\n\t    def __init__(self, dataset_meta: DatasetMetadata, data: TrainConfigData):\n\t        self.dataset_meta = dataset_meta\n\t        self.max_wav_value = data.max_wav_value\n\t        self.sampling_rate = data.sampling_rate\n\t        self.filter_length = data.filter_length\n\t        self.hop_length = data.hop_length\n", "        self.win_length = data.win_length\n\t        self.sampling_rate = data.sampling_rate\n\t        self.min_text_len = getattr(data, \"min_text_len\", 1)\n\t        self.max_text_len = getattr(data, \"max_text_len\", 5000)\n\t        self._filter()\n\t    def _filter(self):\n\t        \"\"\"\n\t        Filter text & store spec lengths\n\t        \"\"\"\n\t        # Store spectrogram lengths for Bucketing\n", "        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n\t        # spec_length = wav_length // hop_length\n\t        lengths = []\n\t        for key, data in self.dataset_meta.files.items():\n\t            if (\n\t                self.min_text_len <= len(data.co256)\n\t                and len(data.co256) <= self.max_text_len\n\t            ):\n\t                lengths.append(os.path.getsize(data.gt_wav) // (2 * self.hop_length))\n\t            else:\n", "                del self.dataset_meta.files[key]\n\t        self.lengths = lengths\n\t    def get_sid(self, sid):\n\t        sid = torch.LongTensor([int(sid)])\n\t        return sid\n\t    def get_audio_text_pair(self, data: DatasetMetaItem):\n\t        # separate filename and text\n\t        file = data.gt_wav\n\t        phone = data.co256\n\t        dv = data.speaker_id\n", "        phone = self.get_labels(phone)\n\t        spec, wav = self.get_audio(file)\n\t        dv = self.get_sid(dv)\n\t        len_phone = phone.size()[0]\n\t        len_spec = spec.size()[-1]\n\t        if len_phone != len_spec:\n\t            len_min = min(len_phone, len_spec)\n\t            len_wav = len_min * self.hop_length\n\t            spec = spec[:, :len_min]\n\t            wav = wav[:, :len_wav]\n", "            phone = phone[:len_min, :]\n\t        return (spec, wav, phone, dv)\n\t    def get_labels(self, phone):\n\t        phone = np.load(phone)\n\t        phone = np.repeat(phone, 2, axis=0)\n\t        n_num = min(phone.shape[0], 900)  # DistributedBucketSampler\n\t        phone = phone[:n_num, :]\n\t        phone = torch.FloatTensor(phone)\n\t        return phone\n\t    def get_audio(self, filename):\n", "        audio, sampling_rate = load_wav_to_torch(filename)\n\t        if sampling_rate != self.sampling_rate:\n\t            raise ValueError(\n\t                \"{} SR doesn't match target {} SR\".format(\n\t                    sampling_rate, self.sampling_rate\n\t                )\n\t            )\n\t        # audio_norm = audio / self.max_wav_value\n\t        audio_norm = audio.unsqueeze(0)\n\t        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n", "        if os.path.exists(spec_filename):\n\t            try:\n\t                spec = torch.load(spec_filename)\n\t            except:\n\t                print(spec_filename, traceback.format_exc())\n\t                spec = spectrogram_torch(\n\t                    audio_norm,\n\t                    self.filter_length,\n\t                    self.sampling_rate,\n\t                    self.hop_length,\n", "                    self.win_length,\n\t                    center=False,\n\t                )\n\t                spec = torch.squeeze(spec, 0)\n\t                torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n\t        else:\n\t            spec = spectrogram_torch(\n\t                audio_norm,\n\t                self.filter_length,\n\t                self.sampling_rate,\n", "                self.hop_length,\n\t                self.win_length,\n\t                center=False,\n\t            )\n\t            spec = torch.squeeze(spec, 0)\n\t            torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n\t        return spec, audio_norm\n\t    def __getitem__(self, index):\n\t        _, data = list(self.dataset_meta.files.items())[index]\n\t        return self.get_audio_text_pair(data)\n", "    def __len__(self):\n\t        return len(self.dataset_meta.files)\n\tclass TextAudioLoaderMultiNSFsid(torch.utils.data.Dataset):\n\t    \"\"\"\n\t    1) loads audio, text pairs\n\t    2) normalizes text and converts them to sequences of integers\n\t    3) computes spectrograms from audio files.\n\t    \"\"\"\n\t    def __init__(self, dataset_meta: DatasetMetadata, data: TrainConfigData):\n\t        self.dataset_meta = dataset_meta\n", "        self.max_wav_value = data.max_wav_value\n\t        self.sampling_rate = data.sampling_rate\n\t        self.filter_length = data.filter_length\n\t        self.hop_length = data.hop_length\n\t        self.win_length = data.win_length\n\t        self.sampling_rate = data.sampling_rate\n\t        self.min_text_len = getattr(data, \"min_text_len\", 1)\n\t        self.max_text_len = getattr(data, \"max_text_len\", 5000)\n\t        self._filter()\n\t    def _filter(self):\n", "        \"\"\"\n\t        Filter text & store spec lengths\n\t        \"\"\"\n\t        # Store spectrogram lengths for Bucketing\n\t        # wav_length ~= file_size / (wav_channels * Bytes per dim) = file_size / (1 * 2)\n\t        # spec_length = wav_length // hop_length\n\t        lengths = []\n\t        for key, data in self.dataset_meta.files.items():\n\t            if (\n\t                self.min_text_len <= len(data.co256)\n", "                and len(data.co256) <= self.max_text_len\n\t            ):\n\t                lengths.append(os.path.getsize(data.gt_wav) // (2 * self.hop_length))\n\t            else:\n\t                del self.dataset_meta.files[key]\n\t        self.lengths = lengths\n\t    def get_sid(self, sid):\n\t        sid = torch.LongTensor([int(sid)])\n\t        return sid\n\t    def get_audio_text_pair(self, data: DatasetMetaItem):\n", "        # separate filename and text\n\t        file = data.gt_wav\n\t        phone = data.co256\n\t        pitch = data.f0\n\t        pitchf = data.f0nsf\n\t        dv = data.speaker_id\n\t        phone, pitch, pitchf = self.get_labels(phone, pitch, pitchf)\n\t        spec, wav = self.get_audio(file)\n\t        dv = self.get_sid(dv)\n\t        len_phone = phone.size()[0]\n", "        len_spec = spec.size()[-1]\n\t        # print(123,phone.shape,pitch.shape,spec.shape)\n\t        if len_phone != len_spec:\n\t            len_min = min(len_phone, len_spec)\n\t            # amor\n\t            len_wav = len_min * self.hop_length\n\t            spec = spec[:, :len_min]\n\t            wav = wav[:, :len_wav]\n\t            phone = phone[:len_min, :]\n\t            pitch = pitch[:len_min]\n", "            pitchf = pitchf[:len_min]\n\t        return (spec, wav, phone, pitch, pitchf, dv)\n\t    def get_labels(self, phone, pitch, pitchf):\n\t        phone = np.load(phone)\n\t        phone = np.repeat(phone, 2, axis=0)\n\t        pitch = np.load(pitch)\n\t        pitchf = np.load(pitchf)\n\t        n_num = min(phone.shape[0], 900)  # DistributedBucketSampler\n\t        # print(234,phone.shape,pitch.shape)\n\t        phone = phone[:n_num, :]\n", "        pitch = pitch[:n_num]\n\t        pitchf = pitchf[:n_num]\n\t        phone = torch.FloatTensor(phone)\n\t        pitch = torch.LongTensor(pitch)\n\t        pitchf = torch.FloatTensor(pitchf)\n\t        return phone, pitch, pitchf\n\t    def get_audio(self, filename):\n\t        audio, sampling_rate = load_wav_to_torch(filename)\n\t        if sampling_rate != self.sampling_rate:\n\t            raise ValueError(\n", "                \"{} SR doesn't match target {} SR\".format(\n\t                    sampling_rate, self.sampling_rate\n\t                )\n\t            )\n\t        # audio_norm = audio / self.max_wav_value\n\t        audio_norm = audio.unsqueeze(0)\n\t        spec_filename = filename.replace(\".wav\", \".spec.pt\")\n\t        if os.path.exists(spec_filename):\n\t            try:\n\t                spec = torch.load(spec_filename)\n", "            except:\n\t                print(spec_filename, traceback.format_exc())\n\t                spec = spectrogram_torch(\n\t                    audio_norm,\n\t                    self.filter_length,\n\t                    self.sampling_rate,\n\t                    self.hop_length,\n\t                    self.win_length,\n\t                    center=False,\n\t                )\n", "                spec = torch.squeeze(spec, 0)\n\t                torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n\t        else:\n\t            spec = spectrogram_torch(\n\t                audio_norm,\n\t                self.filter_length,\n\t                self.sampling_rate,\n\t                self.hop_length,\n\t                self.win_length,\n\t                center=False,\n", "            )\n\t            spec = torch.squeeze(spec, 0)\n\t            torch.save(spec, spec_filename, _use_new_zipfile_serialization=False)\n\t        return spec, audio_norm\n\t    def __getitem__(self, index):\n\t        _, data = list(self.dataset_meta.files.items())[index]\n\t        return self.get_audio_text_pair(data)\n\t    def __len__(self):\n\t        return len(self.dataset_meta.files)\n\tclass TextAudioCollateMultiNSFsid:\n", "    \"\"\"Zero-pads model inputs and targets\"\"\"\n\t    def __init__(self, return_ids=False):\n\t        self.return_ids = return_ids\n\t    def __call__(self, batch):\n\t        \"\"\"Collate's training batch from normalized text and aduio\n\t        PARAMS\n\t        ------\n\t        batch: [text_normalized, spec_normalized, wav_normalized]\n\t        \"\"\"\n\t        # Right zero-pad all one-hot text sequences to max input length\n", "        _, ids_sorted_decreasing = torch.sort(\n\t            torch.LongTensor([x[0].size(1) for x in batch]), dim=0, descending=True\n\t        )\n\t        max_spec_len = max([x[0].size(1) for x in batch])\n\t        max_wave_len = max([x[1].size(1) for x in batch])\n\t        spec_lengths = torch.LongTensor(len(batch))\n\t        wave_lengths = torch.LongTensor(len(batch))\n\t        spec_padded = torch.FloatTensor(len(batch), batch[0][0].size(0), max_spec_len)\n\t        wave_padded = torch.FloatTensor(len(batch), 1, max_wave_len)\n\t        spec_padded.zero_()\n", "        wave_padded.zero_()\n\t        max_phone_len = max([x[2].size(0) for x in batch])\n\t        phone_lengths = torch.LongTensor(len(batch))\n\t        phone_padded = torch.FloatTensor(\n\t            len(batch), max_phone_len, batch[0][2].shape[1]\n\t        )  # (spec, wav, phone, pitch)\n\t        pitch_padded = torch.LongTensor(len(batch), max_phone_len)\n\t        pitchf_padded = torch.FloatTensor(len(batch), max_phone_len)\n\t        phone_padded.zero_()\n\t        pitch_padded.zero_()\n", "        pitchf_padded.zero_()\n\t        # dv = torch.FloatTensor(len(batch), 256)#gin=256\n\t        sid = torch.LongTensor(len(batch))\n\t        for i in range(len(ids_sorted_decreasing)):\n\t            row = batch[ids_sorted_decreasing[i]]\n\t            spec = row[0]\n\t            spec_padded[i, :, : spec.size(1)] = spec\n\t            spec_lengths[i] = spec.size(1)\n\t            wave = row[1]\n\t            wave_padded[i, :, : wave.size(1)] = wave\n", "            wave_lengths[i] = wave.size(1)\n\t            phone = row[2]\n\t            phone_padded[i, : phone.size(0), :] = phone\n\t            phone_lengths[i] = phone.size(0)\n\t            pitch = row[3]\n\t            pitch_padded[i, : pitch.size(0)] = pitch\n\t            pitchf = row[4]\n\t            pitchf_padded[i, : pitchf.size(0)] = pitchf\n\t            # dv[i] = row[5]\n\t            sid[i] = row[5]\n", "        return (\n\t            phone_padded,\n\t            phone_lengths,\n\t            pitch_padded,\n\t            pitchf_padded,\n\t            spec_padded,\n\t            spec_lengths,\n\t            wave_padded,\n\t            wave_lengths,\n\t            # dv\n", "            sid,\n\t        )\n\tclass TextAudioCollate:\n\t    \"\"\"Zero-pads model inputs and targets\"\"\"\n\t    def __init__(self, return_ids=False):\n\t        self.return_ids = return_ids\n\t    def __call__(self, batch):\n\t        \"\"\"Collate's training batch from normalized text and aduio\n\t        PARAMS\n\t        ------\n", "        batch: [text_normalized, spec_normalized, wav_normalized]\n\t        \"\"\"\n\t        # Right zero-pad all one-hot text sequences to max input length\n\t        _, ids_sorted_decreasing = torch.sort(\n\t            torch.LongTensor([x[0].size(1) for x in batch]), dim=0, descending=True\n\t        )\n\t        max_spec_len = max([x[0].size(1) for x in batch])\n\t        max_wave_len = max([x[1].size(1) for x in batch])\n\t        spec_lengths = torch.LongTensor(len(batch))\n\t        wave_lengths = torch.LongTensor(len(batch))\n", "        spec_padded = torch.FloatTensor(len(batch), batch[0][0].size(0), max_spec_len)\n\t        wave_padded = torch.FloatTensor(len(batch), 1, max_wave_len)\n\t        spec_padded.zero_()\n\t        wave_padded.zero_()\n\t        max_phone_len = max([x[2].size(0) for x in batch])\n\t        phone_lengths = torch.LongTensor(len(batch))\n\t        phone_padded = torch.FloatTensor(\n\t            len(batch), max_phone_len, batch[0][2].shape[1]\n\t        )\n\t        phone_padded.zero_()\n", "        sid = torch.LongTensor(len(batch))\n\t        for i in range(len(ids_sorted_decreasing)):\n\t            row = batch[ids_sorted_decreasing[i]]\n\t            spec = row[0]\n\t            spec_padded[i, :, : spec.size(1)] = spec\n\t            spec_lengths[i] = spec.size(1)\n\t            wave = row[1]\n\t            wave_padded[i, :, : wave.size(1)] = wave\n\t            wave_lengths[i] = wave.size(1)\n\t            phone = row[2]\n", "            phone_padded[i, : phone.size(0), :] = phone\n\t            phone_lengths[i] = phone.size(0)\n\t            sid[i] = row[3]\n\t        return (\n\t            phone_padded,\n\t            phone_lengths,\n\t            spec_padded,\n\t            spec_lengths,\n\t            wave_padded,\n\t            wave_lengths,\n", "            sid,\n\t        )\n\tclass DistributedBucketSampler(torch.utils.data.distributed.DistributedSampler):\n\t    \"\"\"\n\t    Maintain similar input lengths in a batch.\n\t    Length groups are specified by boundaries.\n\t    Ex) boundaries = [b1, b2, b3] -> any batch is included either {x | b1 < length(x) <=b2} or {x | b2 < length(x) <= b3}.\n\t    It removes samples which are not included in the boundaries.\n\t    Ex) boundaries = [b1, b2, b3] -> any x s.t. length(x) <= b1 or length(x) > b3 are discarded.\n\t    \"\"\"\n", "    def __init__(\n\t        self,\n\t        dataset,\n\t        batch_size,\n\t        boundaries,\n\t        num_replicas=None,\n\t        rank=None,\n\t        shuffle=True,\n\t    ):\n\t        super().__init__(dataset, num_replicas=num_replicas, rank=rank, shuffle=shuffle)\n", "        self.lengths = dataset.lengths\n\t        self.batch_size = batch_size\n\t        self.boundaries = boundaries\n\t        self.buckets, self.num_samples_per_bucket = self._create_buckets()\n\t        self.total_size = sum(self.num_samples_per_bucket)\n\t        self.num_samples = self.total_size // self.num_replicas\n\t    def _create_buckets(self):\n\t        buckets = [[] for _ in range(len(self.boundaries) - 1)]\n\t        for i in range(len(self.lengths)):\n\t            length = self.lengths[i]\n", "            idx_bucket = self._bisect(length)\n\t            if idx_bucket != -1:\n\t                buckets[idx_bucket].append(i)\n\t        for i in range(len(buckets) - 1, -1, -1):  #\n\t            if len(buckets[i]) == 0:\n\t                buckets.pop(i)\n\t                self.boundaries.pop(i + 1)\n\t        num_samples_per_bucket = []\n\t        for i in range(len(buckets)):\n\t            len_bucket = len(buckets[i])\n", "            total_batch_size = self.num_replicas * self.batch_size\n\t            rem = (\n\t                total_batch_size - (len_bucket % total_batch_size)\n\t            ) % total_batch_size\n\t            num_samples_per_bucket.append(len_bucket + rem)\n\t        return buckets, num_samples_per_bucket\n\t    def __iter__(self):\n\t        # deterministically shuffle based on epoch\n\t        g = torch.Generator()\n\t        g.manual_seed(self.epoch)\n", "        indices = []\n\t        if self.shuffle:\n\t            for bucket in self.buckets:\n\t                indices.append(torch.randperm(len(bucket), generator=g).tolist())\n\t        else:\n\t            for bucket in self.buckets:\n\t                indices.append(list(range(len(bucket))))\n\t        batches = []\n\t        for i in range(len(self.buckets)):\n\t            bucket = self.buckets[i]\n", "            len_bucket = len(bucket)\n\t            ids_bucket = indices[i]\n\t            num_samples_bucket = self.num_samples_per_bucket[i]\n\t            # add extra samples to make it evenly divisible\n\t            rem = num_samples_bucket - len_bucket\n\t            ids_bucket = (\n\t                ids_bucket\n\t                + ids_bucket * (rem // len_bucket)\n\t                + ids_bucket[: (rem % len_bucket)]\n\t            )\n", "            # subsample\n\t            ids_bucket = ids_bucket[self.rank :: self.num_replicas]\n\t            # batching\n\t            for j in range(len(ids_bucket) // self.batch_size):\n\t                batch = [\n\t                    bucket[idx]\n\t                    for idx in ids_bucket[\n\t                        j * self.batch_size : (j + 1) * self.batch_size\n\t                    ]\n\t                ]\n", "                batches.append(batch)\n\t        if self.shuffle:\n\t            batch_ids = torch.randperm(len(batches), generator=g).tolist()\n\t            batches = [batches[i] for i in batch_ids]\n\t        self.batches = batches\n\t        assert len(self.batches) * self.batch_size == self.num_samples\n\t        return iter(self.batches)\n\t    def _bisect(self, x, lo=0, hi=None):\n\t        if hi is None:\n\t            hi = len(self.boundaries) - 1\n", "        if hi > lo:\n\t            mid = (hi + lo) // 2\n\t            if self.boundaries[mid] < x and x <= self.boundaries[mid + 1]:\n\t                return mid\n\t            elif x <= self.boundaries[mid]:\n\t                return self._bisect(x, lo, mid)\n\t            else:\n\t                return self._bisect(x, mid + 1, hi)\n\t        else:\n\t            return -1\n", "    def __len__(self):\n\t        return self.num_samples // self.batch_size\n"]}
{"filename": "lib/rvc/models.py", "chunked_list": ["import math\n\timport numpy as np\n\timport torch\n\tfrom torch import nn\n\tfrom torch.nn import Conv1d, Conv2d, ConvTranspose1d\n\tfrom torch.nn import functional as F\n\tfrom torch.nn.utils import remove_weight_norm, spectral_norm, weight_norm\n\tfrom . import attentions, commons, modules\n\tfrom .commons import get_padding, init_weights\n\tclass TextEncoder(nn.Module):\n", "    def __init__(\n\t        self,\n\t        out_channels: int,\n\t        hidden_channels: int,\n\t        filter_channels: int,\n\t        emb_channels: int,\n\t        n_heads: int,\n\t        n_layers: int,\n\t        kernel_size: int,\n\t        p_dropout: int,\n", "        f0: bool = True,\n\t    ):\n\t        super().__init__()\n\t        self.out_channels = out_channels\n\t        self.hidden_channels = hidden_channels\n\t        self.filter_channels = filter_channels\n\t        self.emb_channels = emb_channels\n\t        self.n_heads = n_heads\n\t        self.n_layers = n_layers\n\t        self.kernel_size = kernel_size\n", "        self.p_dropout = p_dropout\n\t        self.emb_phone = nn.Linear(emb_channels, hidden_channels)\n\t        self.lrelu = nn.LeakyReLU(0.1, inplace=True)\n\t        if f0 == True:\n\t            self.emb_pitch = nn.Embedding(256, hidden_channels)  # pitch 256\n\t        self.encoder = attentions.Encoder(\n\t            hidden_channels, filter_channels, n_heads, n_layers, kernel_size, p_dropout\n\t        )\n\t        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\t    def forward(self, phone, pitch, lengths):\n", "        if pitch == None:\n\t            x = self.emb_phone(phone)\n\t        else:\n\t            x = self.emb_phone(phone) + self.emb_pitch(pitch)\n\t        x = x * math.sqrt(self.hidden_channels)  # [b, t, h]\n\t        x = self.lrelu(x)\n\t        x = torch.transpose(x, 1, -1)  # [b, h, t]\n\t        x_mask = torch.unsqueeze(commons.sequence_mask(lengths, x.size(2)), 1).to(\n\t            x.dtype\n\t        )\n", "        x = self.encoder(x * x_mask, x_mask)\n\t        stats = self.proj(x) * x_mask\n\t        m, logs = torch.split(stats, self.out_channels, dim=1)\n\t        return m, logs, x_mask\n\tclass ResidualCouplingBlock(nn.Module):\n\t    def __init__(\n\t        self,\n\t        channels,\n\t        hidden_channels,\n\t        kernel_size,\n", "        dilation_rate,\n\t        n_layers,\n\t        n_flows=4,\n\t        gin_channels=0,\n\t    ):\n\t        super().__init__()\n\t        self.channels = channels\n\t        self.hidden_channels = hidden_channels\n\t        self.kernel_size = kernel_size\n\t        self.dilation_rate = dilation_rate\n", "        self.n_layers = n_layers\n\t        self.n_flows = n_flows\n\t        self.gin_channels = gin_channels\n\t        self.flows = nn.ModuleList()\n\t        for i in range(n_flows):\n\t            self.flows.append(\n\t                modules.ResidualCouplingLayer(\n\t                    channels,\n\t                    hidden_channels,\n\t                    kernel_size,\n", "                    dilation_rate,\n\t                    n_layers,\n\t                    gin_channels=gin_channels,\n\t                    mean_only=True,\n\t                )\n\t            )\n\t            self.flows.append(modules.Flip())\n\t    def forward(self, x, x_mask, g=None, reverse=False):\n\t        if not reverse:\n\t            for flow in self.flows:\n", "                x, _ = flow(x, x_mask, g=g, reverse=reverse)\n\t        else:\n\t            for flow in reversed(self.flows):\n\t                x = flow(x, x_mask, g=g, reverse=reverse)\n\t        return x\n\t    def remove_weight_norm(self):\n\t        for i in range(self.n_flows):\n\t            self.flows[i * 2].remove_weight_norm()\n\tclass PosteriorEncoder(nn.Module):\n\t    def __init__(\n", "        self,\n\t        in_channels,\n\t        out_channels,\n\t        hidden_channels,\n\t        kernel_size,\n\t        dilation_rate,\n\t        n_layers,\n\t        gin_channels=0,\n\t    ):\n\t        super().__init__()\n", "        self.in_channels = in_channels\n\t        self.out_channels = out_channels\n\t        self.hidden_channels = hidden_channels\n\t        self.kernel_size = kernel_size\n\t        self.dilation_rate = dilation_rate\n\t        self.n_layers = n_layers\n\t        self.gin_channels = gin_channels\n\t        self.pre = nn.Conv1d(in_channels, hidden_channels, 1)\n\t        self.enc = modules.WN(\n\t            hidden_channels,\n", "            kernel_size,\n\t            dilation_rate,\n\t            n_layers,\n\t            gin_channels=gin_channels,\n\t        )\n\t        self.proj = nn.Conv1d(hidden_channels, out_channels * 2, 1)\n\t    def forward(self, x, x_lengths, g=None):\n\t        x_mask = torch.unsqueeze(commons.sequence_mask(x_lengths, x.size(2)), 1).to(\n\t            x.dtype\n\t        )\n", "        x = self.pre(x) * x_mask\n\t        x = self.enc(x, x_mask, g=g)\n\t        stats = self.proj(x) * x_mask\n\t        m, logs = torch.split(stats, self.out_channels, dim=1)\n\t        z = (m + torch.randn_like(m) * torch.exp(logs)) * x_mask\n\t        return z, m, logs, x_mask\n\t    def remove_weight_norm(self):\n\t        self.enc.remove_weight_norm()\n\tclass Generator(torch.nn.Module):\n\t    def __init__(\n", "        self,\n\t        initial_channel,\n\t        resblock,\n\t        resblock_kernel_sizes,\n\t        resblock_dilation_sizes,\n\t        upsample_rates,\n\t        upsample_initial_channel,\n\t        upsample_kernel_sizes,\n\t        gin_channels=0,\n\t    ):\n", "        super(Generator, self).__init__()\n\t        self.num_kernels = len(resblock_kernel_sizes)\n\t        self.num_upsamples = len(upsample_rates)\n\t        self.conv_pre = Conv1d(\n\t            initial_channel, upsample_initial_channel, 7, 1, padding=3\n\t        )\n\t        resblock = modules.ResBlock1 if resblock == \"1\" else modules.ResBlock2\n\t        self.ups = nn.ModuleList()\n\t        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n\t            self.ups.append(\n", "                weight_norm(\n\t                    ConvTranspose1d(\n\t                        upsample_initial_channel // (2**i),\n\t                        upsample_initial_channel // (2 ** (i + 1)),\n\t                        k,\n\t                        u,\n\t                        padding=(k - u) // 2,\n\t                    )\n\t                )\n\t            )\n", "        self.resblocks = nn.ModuleList()\n\t        for i in range(len(self.ups)):\n\t            ch = upsample_initial_channel // (2 ** (i + 1))\n\t            for j, (k, d) in enumerate(\n\t                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n\t            ):\n\t                self.resblocks.append(resblock(ch, k, d))\n\t        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n\t        self.ups.apply(init_weights)\n\t        if gin_channels != 0:\n", "            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\t    def forward(self, x, g=None):\n\t        x = self.conv_pre(x)\n\t        if g is not None:\n\t            x = x + self.cond(g)\n\t        for i in range(self.num_upsamples):\n\t            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n\t            x = self.ups[i](x)\n\t            xs = None\n\t            for j in range(self.num_kernels):\n", "                if xs is None:\n\t                    xs = self.resblocks[i * self.num_kernels + j](x)\n\t                else:\n\t                    xs += self.resblocks[i * self.num_kernels + j](x)\n\t            x = xs / self.num_kernels\n\t        x = F.leaky_relu(x)\n\t        x = self.conv_post(x)\n\t        x = torch.tanh(x)\n\t        return x\n\t    def remove_weight_norm(self):\n", "        for l in self.ups:\n\t            remove_weight_norm(l)\n\t        for l in self.resblocks:\n\t            l.remove_weight_norm()\n\tclass SineGen(torch.nn.Module):\n\t    \"\"\"Definition of sine generator\n\t    SineGen(samp_rate, harmonic_num = 0,\n\t            sine_amp = 0.1, noise_std = 0.003,\n\t            voiced_threshold = 0,\n\t            flag_for_pulse=False)\n", "    samp_rate: sampling rate in Hz\n\t    harmonic_num: number of harmonic overtones (default 0)\n\t    sine_amp: amplitude of sine-wavefrom (default 0.1)\n\t    noise_std: std of Gaussian noise (default 0.003)\n\t    voiced_thoreshold: F0 threshold for U/V classification (default 0)\n\t    flag_for_pulse: this SinGen is used inside PulseGen (default False)\n\t    Note: when flag_for_pulse is True, the first time step of a voiced\n\t        segment is always sin(np.pi) or cos(0)\n\t    \"\"\"\n\t    def __init__(\n", "        self,\n\t        samp_rate,\n\t        harmonic_num=0,\n\t        sine_amp=0.1,\n\t        noise_std=0.003,\n\t        voiced_threshold=0,\n\t        flag_for_pulse=False,\n\t    ):\n\t        super(SineGen, self).__init__()\n\t        self.sine_amp = sine_amp\n", "        self.noise_std = noise_std\n\t        self.harmonic_num = harmonic_num\n\t        self.dim = self.harmonic_num + 1\n\t        self.sampling_rate = samp_rate\n\t        self.voiced_threshold = voiced_threshold\n\t    def _f02uv(self, f0):\n\t        # generate uv signal\n\t        uv = torch.ones_like(f0)\n\t        uv = uv * (f0 > self.voiced_threshold)\n\t        return uv\n", "    def forward(self, f0, upp):\n\t        \"\"\"sine_tensor, uv = forward(f0)\n\t        input F0: tensor(batchsize=1, length, dim=1)\n\t                  f0 for unvoiced steps should be 0\n\t        output sine_tensor: tensor(batchsize=1, length, dim)\n\t        output uv: tensor(batchsize=1, length, 1)\n\t        \"\"\"\n\t        with torch.no_grad():\n\t            f0 = f0[:, None].transpose(1, 2)\n\t            f0_buf = torch.zeros(f0.shape[0], f0.shape[1], self.dim, device=f0.device)\n", "            # fundamental component\n\t            f0_buf[:, :, 0] = f0[:, :, 0]\n\t            for idx in np.arange(self.harmonic_num):\n\t                f0_buf[:, :, idx + 1] = f0_buf[:, :, 0] * (\n\t                    idx + 2\n\t                )  # idx + 2: the (idx+1)-th overtone, (idx+2)-th harmonic\n\t            rad_values = (f0_buf / self.sampling_rate) % 1  ###%1意味着n_har的乘积无法后处理优化\n\t            rand_ini = torch.rand(\n\t                f0_buf.shape[0], f0_buf.shape[2], device=f0_buf.device\n\t            )\n", "            rand_ini[:, 0] = 0\n\t            rad_values[:, 0, :] = rad_values[:, 0, :] + rand_ini\n\t            tmp_over_one = torch.cumsum(rad_values, 1)  # % 1  #####%1意味着后面的cumsum无法再优化\n\t            tmp_over_one *= upp\n\t            tmp_over_one = F.interpolate(\n\t                tmp_over_one.transpose(2, 1),\n\t                scale_factor=upp,\n\t                mode=\"linear\",\n\t                align_corners=True,\n\t            ).transpose(2, 1)\n", "            rad_values = F.interpolate(\n\t                rad_values.transpose(2, 1), scale_factor=upp, mode=\"nearest\"\n\t            ).transpose(\n\t                2, 1\n\t            )  #######\n\t            tmp_over_one %= 1\n\t            tmp_over_one_idx = (tmp_over_one[:, 1:, :] - tmp_over_one[:, :-1, :]) < 0\n\t            cumsum_shift = torch.zeros_like(rad_values)\n\t            cumsum_shift[:, 1:, :] = tmp_over_one_idx * -1.0\n\t            sine_waves = torch.sin(\n", "                torch.cumsum(rad_values + cumsum_shift, dim=1) * 2 * np.pi\n\t            )\n\t            sine_waves = sine_waves * self.sine_amp\n\t            uv = self._f02uv(f0)\n\t            uv = F.interpolate(\n\t                uv.transpose(2, 1), scale_factor=upp, mode=\"nearest\"\n\t            ).transpose(2, 1)\n\t            noise_amp = uv * self.noise_std + (1 - uv) * self.sine_amp / 3\n\t            noise = noise_amp * torch.randn_like(sine_waves)\n\t            sine_waves = sine_waves * uv + noise\n", "        return sine_waves, uv, noise\n\tclass SourceModuleHnNSF(torch.nn.Module):\n\t    \"\"\"SourceModule for hn-nsf\n\t    SourceModule(sampling_rate, harmonic_num=0, sine_amp=0.1,\n\t                 add_noise_std=0.003, voiced_threshod=0)\n\t    sampling_rate: sampling_rate in Hz\n\t    harmonic_num: number of harmonic above F0 (default: 0)\n\t    sine_amp: amplitude of sine source signal (default: 0.1)\n\t    add_noise_std: std of additive Gaussian noise (default: 0.003)\n\t        note that amplitude of noise in unvoiced is decided\n", "        by sine_amp\n\t    voiced_threshold: threhold to set U/V given F0 (default: 0)\n\t    Sine_source, noise_source = SourceModuleHnNSF(F0_sampled)\n\t    F0_sampled (batchsize, length, 1)\n\t    Sine_source (batchsize, length, 1)\n\t    noise_source (batchsize, length 1)\n\t    uv (batchsize, length, 1)\n\t    \"\"\"\n\t    def __init__(\n\t        self,\n", "        sampling_rate,\n\t        harmonic_num=0,\n\t        sine_amp=0.1,\n\t        add_noise_std=0.003,\n\t        voiced_threshod=0,\n\t        is_half=True,\n\t    ):\n\t        super(SourceModuleHnNSF, self).__init__()\n\t        self.sine_amp = sine_amp\n\t        self.noise_std = add_noise_std\n", "        self.is_half = is_half\n\t        # to produce sine waveforms\n\t        self.l_sin_gen = SineGen(\n\t            sampling_rate, harmonic_num, sine_amp, add_noise_std, voiced_threshod\n\t        )\n\t        # to merge source harmonics into a single excitation\n\t        self.l_linear = torch.nn.Linear(harmonic_num + 1, 1)\n\t        self.l_tanh = torch.nn.Tanh()\n\t    def forward(self, x, upp=None):\n\t        sine_wavs, uv, _ = self.l_sin_gen(x, upp)\n", "        if self.is_half == True:\n\t            sine_wavs = sine_wavs.half()\n\t        sine_merge = self.l_tanh(self.l_linear(sine_wavs))\n\t        return sine_merge, None, None  # noise, uv\n\tclass GeneratorNSF(torch.nn.Module):\n\t    def __init__(\n\t        self,\n\t        initial_channel,\n\t        resblock,\n\t        resblock_kernel_sizes,\n", "        resblock_dilation_sizes,\n\t        upsample_rates,\n\t        upsample_initial_channel,\n\t        upsample_kernel_sizes,\n\t        gin_channels,\n\t        sr,\n\t        is_half=False,\n\t    ):\n\t        super(GeneratorNSF, self).__init__()\n\t        self.num_kernels = len(resblock_kernel_sizes)\n", "        self.num_upsamples = len(upsample_rates)\n\t        self.f0_upsamp = torch.nn.Upsample(scale_factor=np.prod(upsample_rates))\n\t        self.m_source = SourceModuleHnNSF(\n\t            sampling_rate=sr, harmonic_num=0, is_half=is_half\n\t        )\n\t        self.noise_convs = nn.ModuleList()\n\t        self.conv_pre = Conv1d(\n\t            initial_channel, upsample_initial_channel, 7, 1, padding=3\n\t        )\n\t        resblock = modules.ResBlock1 if resblock == \"1\" else modules.ResBlock2\n", "        self.ups = nn.ModuleList()\n\t        for i, (u, k) in enumerate(zip(upsample_rates, upsample_kernel_sizes)):\n\t            c_cur = upsample_initial_channel // (2 ** (i + 1))\n\t            self.ups.append(\n\t                weight_norm(\n\t                    ConvTranspose1d(\n\t                        upsample_initial_channel // (2**i),\n\t                        upsample_initial_channel // (2 ** (i + 1)),\n\t                        k,\n\t                        u,\n", "                        padding=(k - u) // 2,\n\t                    )\n\t                )\n\t            )\n\t            if i + 1 < len(upsample_rates):\n\t                stride_f0 = np.prod(upsample_rates[i + 1 :])\n\t                self.noise_convs.append(\n\t                    Conv1d(\n\t                        1,\n\t                        c_cur,\n", "                        kernel_size=stride_f0 * 2,\n\t                        stride=stride_f0,\n\t                        padding=stride_f0 // 2,\n\t                    )\n\t                )\n\t            else:\n\t                self.noise_convs.append(Conv1d(1, c_cur, kernel_size=1))\n\t        self.resblocks = nn.ModuleList()\n\t        for i in range(len(self.ups)):\n\t            ch = upsample_initial_channel // (2 ** (i + 1))\n", "            for j, (k, d) in enumerate(\n\t                zip(resblock_kernel_sizes, resblock_dilation_sizes)\n\t            ):\n\t                self.resblocks.append(resblock(ch, k, d))\n\t        self.conv_post = Conv1d(ch, 1, 7, 1, padding=3, bias=False)\n\t        self.ups.apply(init_weights)\n\t        if gin_channels != 0:\n\t            self.cond = nn.Conv1d(gin_channels, upsample_initial_channel, 1)\n\t        self.upp = np.prod(upsample_rates)\n\t    def forward(self, x, f0, g=None):\n", "        har_source, noi_source, uv = self.m_source(f0, self.upp)\n\t        har_source = har_source.transpose(1, 2)\n\t        x = self.conv_pre(x)\n\t        if g is not None:\n\t            x = x + self.cond(g)\n\t        for i in range(self.num_upsamples):\n\t            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n\t            x = self.ups[i](x)\n\t            x_source = self.noise_convs[i](har_source)\n\t            x = x + x_source\n", "            xs = None\n\t            for j in range(self.num_kernels):\n\t                if xs is None:\n\t                    xs = self.resblocks[i * self.num_kernels + j](x)\n\t                else:\n\t                    xs += self.resblocks[i * self.num_kernels + j](x)\n\t            x = xs / self.num_kernels\n\t        x = F.leaky_relu(x)\n\t        x = self.conv_post(x)\n\t        x = torch.tanh(x)\n", "        return x\n\t    def remove_weight_norm(self):\n\t        for l in self.ups:\n\t            remove_weight_norm(l)\n\t        for l in self.resblocks:\n\t            l.remove_weight_norm()\n\tsr2sr = {\n\t    \"32k\": 32000,\n\t    \"40k\": 40000,\n\t    \"48k\": 48000,\n", "}\n\tclass SynthesizerTrnMs256NSFSid(nn.Module):\n\t    def __init__(\n\t        self,\n\t        spec_channels,\n\t        segment_size,\n\t        inter_channels,\n\t        hidden_channels,\n\t        filter_channels,\n\t        n_heads,\n", "        n_layers,\n\t        kernel_size,\n\t        p_dropout,\n\t        resblock,\n\t        resblock_kernel_sizes,\n\t        resblock_dilation_sizes,\n\t        upsample_rates,\n\t        upsample_initial_channel,\n\t        upsample_kernel_sizes,\n\t        spk_embed_dim,\n", "        gin_channels,\n\t        emb_channels,\n\t        sr,\n\t        **kwargs\n\t    ):\n\t        super().__init__()\n\t        if type(sr) == type(\"strr\"):\n\t            sr = sr2sr[sr]\n\t        self.spec_channels = spec_channels\n\t        self.inter_channels = inter_channels\n", "        self.hidden_channels = hidden_channels\n\t        self.filter_channels = filter_channels\n\t        self.n_heads = n_heads\n\t        self.n_layers = n_layers\n\t        self.kernel_size = kernel_size\n\t        self.p_dropout = p_dropout\n\t        self.resblock = resblock\n\t        self.resblock_kernel_sizes = resblock_kernel_sizes\n\t        self.resblock_dilation_sizes = resblock_dilation_sizes\n\t        self.upsample_rates = upsample_rates\n", "        self.upsample_initial_channel = upsample_initial_channel\n\t        self.upsample_kernel_sizes = upsample_kernel_sizes\n\t        self.segment_size = segment_size\n\t        self.gin_channels = gin_channels\n\t        self.emb_channels = emb_channels\n\t        self.sr = sr\n\t        # self.hop_length = hop_length#\n\t        self.spk_embed_dim = spk_embed_dim\n\t        self.enc_p = TextEncoder(\n\t            inter_channels,\n", "            hidden_channels,\n\t            filter_channels,\n\t            emb_channels,\n\t            n_heads,\n\t            n_layers,\n\t            kernel_size,\n\t            p_dropout,\n\t        )\n\t        self.dec = GeneratorNSF(\n\t            inter_channels,\n", "            resblock,\n\t            resblock_kernel_sizes,\n\t            resblock_dilation_sizes,\n\t            upsample_rates,\n\t            upsample_initial_channel,\n\t            upsample_kernel_sizes,\n\t            gin_channels=gin_channels,\n\t            sr=sr,\n\t            is_half=kwargs[\"is_half\"],\n\t        )\n", "        self.enc_q = PosteriorEncoder(\n\t            spec_channels,\n\t            inter_channels,\n\t            hidden_channels,\n\t            5,\n\t            1,\n\t            16,\n\t            gin_channels=gin_channels,\n\t        )\n\t        self.flow = ResidualCouplingBlock(\n", "            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n\t        )\n\t        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n\t        print(\n\t            \"gin_channels:\",\n\t            gin_channels,\n\t            \"self.spk_embed_dim:\",\n\t            self.spk_embed_dim,\n\t            \"emb_channels:\",\n\t            emb_channels,\n", "        )\n\t    def remove_weight_norm(self):\n\t        self.dec.remove_weight_norm()\n\t        self.flow.remove_weight_norm()\n\t        self.enc_q.remove_weight_norm()\n\t    def forward(\n\t        self, phone, phone_lengths, pitch, pitchf, y, y_lengths, ds\n\t    ):  # 这里ds是id，[bs,1]\n\t        # print(1,pitch.shape)#[bs,t]\n\t        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n", "        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n\t        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n\t        z_p = self.flow(z, y_mask, g=g)\n\t        z_slice, ids_slice = commons.rand_slice_segments(\n\t            z, y_lengths, self.segment_size\n\t        )\n\t        # print(-1,pitchf.shape,ids_slice,self.segment_size,self.hop_length,self.segment_size//self.hop_length)\n\t        pitchf = commons.slice_segments2(pitchf, ids_slice, self.segment_size)\n\t        # print(-2,pitchf.shape,z_slice.shape)\n\t        o = self.dec(z_slice, pitchf, g=g)\n", "        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\t    def infer(self, phone, phone_lengths, pitch, nsff0, sid, max_len=None):\n\t        g = self.emb_g(sid).unsqueeze(-1)\n\t        m_p, logs_p, x_mask = self.enc_p(phone, pitch, phone_lengths)\n\t        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n\t        z = self.flow(z_p, x_mask, g=g, reverse=True)\n\t        o = self.dec((z * x_mask)[:, :, :max_len], nsff0, g=g)\n\t        return o, x_mask, (z, z_p, m_p, logs_p)\n\tclass SynthesizerTrnMs256NSFSidNono(nn.Module):\n\t    def __init__(\n", "        self,\n\t        spec_channels,\n\t        segment_size,\n\t        inter_channels,\n\t        hidden_channels,\n\t        filter_channels,\n\t        n_heads,\n\t        n_layers,\n\t        kernel_size,\n\t        p_dropout,\n", "        resblock,\n\t        resblock_kernel_sizes,\n\t        resblock_dilation_sizes,\n\t        upsample_rates,\n\t        upsample_initial_channel,\n\t        upsample_kernel_sizes,\n\t        spk_embed_dim,\n\t        gin_channels,\n\t        emb_channels,\n\t        sr=None,\n", "        **kwargs\n\t    ):\n\t        super().__init__()\n\t        self.spec_channels = spec_channels\n\t        self.inter_channels = inter_channels\n\t        self.hidden_channels = hidden_channels\n\t        self.filter_channels = filter_channels\n\t        self.n_heads = n_heads\n\t        self.n_layers = n_layers\n\t        self.kernel_size = kernel_size\n", "        self.p_dropout = p_dropout\n\t        self.resblock = resblock\n\t        self.resblock_kernel_sizes = resblock_kernel_sizes\n\t        self.resblock_dilation_sizes = resblock_dilation_sizes\n\t        self.upsample_rates = upsample_rates\n\t        self.upsample_initial_channel = upsample_initial_channel\n\t        self.upsample_kernel_sizes = upsample_kernel_sizes\n\t        self.segment_size = segment_size\n\t        self.gin_channels = gin_channels\n\t        self.emb_channels = emb_channels\n", "        self.sr = sr\n\t        # self.hop_length = hop_length#\n\t        self.spk_embed_dim = spk_embed_dim\n\t        self.enc_p = TextEncoder(\n\t            inter_channels,\n\t            hidden_channels,\n\t            filter_channels,\n\t            emb_channels,\n\t            n_heads,\n\t            n_layers,\n", "            kernel_size,\n\t            p_dropout,\n\t            f0=False,\n\t        )\n\t        self.dec = Generator(\n\t            inter_channels,\n\t            resblock,\n\t            resblock_kernel_sizes,\n\t            resblock_dilation_sizes,\n\t            upsample_rates,\n", "            upsample_initial_channel,\n\t            upsample_kernel_sizes,\n\t            gin_channels=gin_channels,\n\t        )\n\t        self.enc_q = PosteriorEncoder(\n\t            spec_channels,\n\t            inter_channels,\n\t            hidden_channels,\n\t            5,\n\t            1,\n", "            16,\n\t            gin_channels=gin_channels,\n\t        )\n\t        self.flow = ResidualCouplingBlock(\n\t            inter_channels, hidden_channels, 5, 1, 3, gin_channels=gin_channels\n\t        )\n\t        self.emb_g = nn.Embedding(self.spk_embed_dim, gin_channels)\n\t        print(\n\t            \"gin_channels:\",\n\t            gin_channels,\n", "            \"self.spk_embed_dim:\",\n\t            self.spk_embed_dim,\n\t            \"emb_channels:\",\n\t            emb_channels,\n\t        )\n\t    def remove_weight_norm(self):\n\t        self.dec.remove_weight_norm()\n\t        self.flow.remove_weight_norm()\n\t        self.enc_q.remove_weight_norm()\n\t    def forward(self, phone, phone_lengths, y, y_lengths, ds):  # 这里ds是id，[bs,1]\n", "        g = self.emb_g(ds).unsqueeze(-1)  # [b, 256, 1]##1是t，广播的\n\t        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n\t        z, m_q, logs_q, y_mask = self.enc_q(y, y_lengths, g=g)\n\t        z_p = self.flow(z, y_mask, g=g)\n\t        z_slice, ids_slice = commons.rand_slice_segments(\n\t            z, y_lengths, self.segment_size\n\t        )\n\t        o = self.dec(z_slice, g=g)\n\t        return o, ids_slice, x_mask, y_mask, (z, z_p, m_p, logs_p, m_q, logs_q)\n\t    def infer(self, phone, phone_lengths, sid, max_len=None):\n", "        g = self.emb_g(sid).unsqueeze(-1)\n\t        m_p, logs_p, x_mask = self.enc_p(phone, None, phone_lengths)\n\t        z_p = (m_p + torch.exp(logs_p) * torch.randn_like(m_p) * 0.66666) * x_mask\n\t        z = self.flow(z_p, x_mask, g=g, reverse=True)\n\t        o = self.dec((z * x_mask)[:, :, :max_len], g=g)\n\t        return o, x_mask, (z, z_p, m_p, logs_p)\n\tclass DiscriminatorS(torch.nn.Module):\n\t    def __init__(self, use_spectral_norm=False):\n\t        super(DiscriminatorS, self).__init__()\n\t        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n", "        self.convs = nn.ModuleList(\n\t            [\n\t                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n\t                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n\t                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n\t                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),\n\t                norm_f(Conv1d(1024, 1024, 41, 4, groups=256, padding=20)),\n\t                norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n\t            ]\n\t        )\n", "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n\t    def forward(self, x):\n\t        fmap = []\n\t        for l in self.convs:\n\t            x = l(x)\n\t            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n\t            fmap.append(x)\n\t        x = self.conv_post(x)\n\t        fmap.append(x)\n\t        x = torch.flatten(x, 1, -1)\n", "        return x, fmap\n\tclass DiscriminatorP(torch.nn.Module):\n\t    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n\t        super(DiscriminatorP, self).__init__()\n\t        self.period = period\n\t        self.use_spectral_norm = use_spectral_norm\n\t        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n\t        self.convs = nn.ModuleList(\n\t            [\n\t                norm_f(\n", "                    Conv2d(\n\t                        1,\n\t                        32,\n\t                        (kernel_size, 1),\n\t                        (stride, 1),\n\t                        padding=(get_padding(kernel_size, 1), 0),\n\t                    )\n\t                ),\n\t                norm_f(\n\t                    Conv2d(\n", "                        32,\n\t                        128,\n\t                        (kernel_size, 1),\n\t                        (stride, 1),\n\t                        padding=(get_padding(kernel_size, 1), 0),\n\t                    )\n\t                ),\n\t                norm_f(\n\t                    Conv2d(\n\t                        128,\n", "                        512,\n\t                        (kernel_size, 1),\n\t                        (stride, 1),\n\t                        padding=(get_padding(kernel_size, 1), 0),\n\t                    )\n\t                ),\n\t                norm_f(\n\t                    Conv2d(\n\t                        512,\n\t                        1024,\n", "                        (kernel_size, 1),\n\t                        (stride, 1),\n\t                        padding=(get_padding(kernel_size, 1), 0),\n\t                    )\n\t                ),\n\t                norm_f(\n\t                    Conv2d(\n\t                        1024,\n\t                        1024,\n\t                        (kernel_size, 1),\n", "                        1,\n\t                        padding=(get_padding(kernel_size, 1), 0),\n\t                    )\n\t                ),\n\t            ]\n\t        )\n\t        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n\t    def forward(self, x):\n\t        fmap = []\n\t        # 1d to 2d\n", "        b, c, t = x.shape\n\t        if t % self.period != 0:  # pad first\n\t            n_pad = self.period - (t % self.period)\n\t            x = F.pad(x, (0, n_pad), \"reflect\")\n\t            t = t + n_pad\n\t        x = x.view(b, c, t // self.period, self.period)\n\t        for l in self.convs:\n\t            x = l(x)\n\t            x = F.leaky_relu(x, modules.LRELU_SLOPE)\n\t            fmap.append(x)\n", "        x = self.conv_post(x)\n\t        fmap.append(x)\n\t        x = torch.flatten(x, 1, -1)\n\t        return x, fmap\n\tclass MultiPeriodDiscriminator(torch.nn.Module):\n\t    def __init__(self, use_spectral_norm=False, periods=[2, 3, 5, 7, 11, 17]):\n\t        super(MultiPeriodDiscriminator, self).__init__()\n\t        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n\t        discs = discs + [\n\t            DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods\n", "        ]\n\t        self.discriminators = nn.ModuleList(discs)\n\t    def forward(self, y, y_hat):\n\t        y_d_rs = []  #\n\t        y_d_gs = []\n\t        fmap_rs = []\n\t        fmap_gs = []\n\t        for i, d in enumerate(self.discriminators):\n\t            y_d_r, fmap_r = d(y)\n\t            y_d_g, fmap_g = d(y_hat)\n", "            # for j in range(len(fmap_r)):\n\t            #     print(i,j,y.shape,y_hat.shape,fmap_r[j].shape,fmap_g[j].shape)\n\t            y_d_rs.append(y_d_r)\n\t            y_d_gs.append(y_d_g)\n\t            fmap_rs.append(fmap_r)\n\t            fmap_gs.append(fmap_g)\n\t        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n"]}
{"filename": "lib/rvc/pipeline.py", "chunked_list": ["import os\n\timport traceback\n\tfrom typing import *\n\timport faiss\n\timport numpy as np\n\timport pyworld\n\timport scipy.signal as signal\n\timport torch\n\timport torch.nn.functional as F\n\timport torchcrepe\n", "from torch import Tensor\n\t# from faiss.swigfaiss_avx2 import IndexIVFFlat # cause crash on windows' faiss-cpu installed from pip\n\tfrom fairseq.models.hubert import HubertModel\n\tfrom .models import SynthesizerTrnMs256NSFSid\n\tclass VocalConvertPipeline(object):\n\t    def __init__(self, tgt_sr: int, device: Union[str, torch.device], is_half: bool):\n\t        if isinstance(device, str):\n\t            device = torch.device(device)\n\t        if device.type == \"cuda\":\n\t            vram = torch.cuda.get_device_properties(device).total_memory / 1024**3\n", "        else:\n\t            vram = None\n\t        if vram is not None and vram <= 4:\n\t            self.x_pad = 1\n\t            self.x_query = 5\n\t            self.x_center = 30\n\t            self.x_max = 32\n\t        elif vram is not None and vram <= 5:\n\t            self.x_pad = 1\n\t            self.x_query = 6\n", "            self.x_center = 38\n\t            self.x_max = 41\n\t        else:\n\t            self.x_pad = 3\n\t            self.x_query = 10\n\t            self.x_center = 60\n\t            self.x_max = 65\n\t        self.sr = 16000  # hubert input sample rate\n\t        self.window = 160  # hubert input window\n\t        self.t_pad = self.sr * self.x_pad  # padding time for each utterance\n", "        self.t_pad_tgt = tgt_sr * self.x_pad\n\t        self.t_pad2 = self.t_pad * 2\n\t        self.t_query = self.sr * self.x_query  # query time before and after query point\n\t        self.t_center = self.sr * self.x_center  # query cut point position\n\t        self.t_max = self.sr * self.x_max  # max time for no query\n\t        self.device = device\n\t        self.is_half = is_half\n\t    def get_optimal_torch_device(self, index: int = 0) -> torch.device:\n\t        # Get cuda device\n\t        if torch.cuda.is_available():\n", "            return torch.device(f\"cuda:{index % torch.cuda.device_count()}\") # Very fast\n\t        elif torch.backends.mps.is_available():\n\t            return torch.device(\"mps\")\n\t        # Insert an else here to grab \"xla\" devices if available. TO DO later. Requires the torch_xla.core.xla_model library\n\t        # Else wise return the \"cpu\" as a torch device, \n\t        return torch.device(\"cpu\")\n\t    def get_f0_crepe_computation(\n\t            self, \n\t            x, \n\t            f0_min,\n", "            f0_max,\n\t            p_len,\n\t            hop_length=64, # 512 before. Hop length changes the speed that the voice jumps to a different dramatic pitch. Lower hop lengths means more pitch accuracy but longer inference time.\n\t            model=\"full\", # Either use crepe-tiny \"tiny\" or crepe \"full\". Default is full\n\t    ):\n\t        x = x.astype(np.float32) # fixes the F.conv2D exception. We needed to convert double to float.\n\t        x /= np.quantile(np.abs(x), 0.999)\n\t        torch_device = self.get_optimal_torch_device()\n\t        audio = torch.from_numpy(x).to(torch_device, copy=True)\n\t        audio = torch.unsqueeze(audio, dim=0)\n", "        if audio.ndim == 2 and audio.shape[0] > 1:\n\t            audio = torch.mean(audio, dim=0, keepdim=True).detach()\n\t        audio = audio.detach()\n\t        print(\"Initiating prediction with a crepe_hop_length of: \" + str(hop_length))\n\t        pitch: Tensor = torchcrepe.predict(\n\t            audio,\n\t            self.sr,\n\t            hop_length,\n\t            f0_min,\n\t            f0_max,\n", "            model,\n\t            batch_size=hop_length * 2,\n\t            device=torch_device,\n\t            pad=True\n\t        )\n\t        p_len = p_len or x.shape[0] // hop_length\n\t        # Resize the pitch for final f0\n\t        source = np.array(pitch.squeeze(0).cpu().float().numpy())\n\t        source[source < 0.001] = np.nan\n\t        target = np.interp(\n", "            np.arange(0, len(source) * p_len, len(source)) / p_len,\n\t            np.arange(0, len(source)),\n\t            source\n\t        )\n\t        f0 = np.nan_to_num(target)\n\t        return f0 # Resized f0\n\t    def get_f0_official_crepe_computation(\n\t            self,\n\t            x,\n\t            f0_min,\n", "            f0_max,\n\t            model=\"full\",\n\t    ):\n\t        # Pick a batch size that doesn't cause memory errors on your gpu\n\t        batch_size = 512\n\t        # Compute pitch using first gpu\n\t        audio = torch.tensor(np.copy(x))[None].float()\n\t        f0, pd = torchcrepe.predict(\n\t            audio,\n\t            self.sr,\n", "            self.window,\n\t            f0_min,\n\t            f0_max,\n\t            model,\n\t            batch_size=batch_size,\n\t            device=self.device,\n\t            return_periodicity=True,\n\t        )\n\t        pd = torchcrepe.filter.median(pd, 3)\n\t        f0 = torchcrepe.filter.mean(f0, 3)\n", "        f0[pd < 0.1] = 0\n\t        f0 = f0[0].cpu().numpy()\n\t        return f0\n\t    def get_f0(\n\t        self,\n\t        x: np.ndarray,\n\t        p_len: int,\n\t        f0_up_key: int,\n\t        f0_method: str,\n\t        inp_f0: np.ndarray = None,\n", "    ):\n\t        f0_min = 50\n\t        f0_max = 1100\n\t        f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n\t        f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\t        if f0_method == \"harvest\":\n\t            f0, t = pyworld.harvest(\n\t                x.astype(np.double),\n\t                fs=self.sr,\n\t                f0_ceil=f0_max,\n", "                f0_floor=f0_min,\n\t                frame_period=10,\n\t            )\n\t            f0 = pyworld.stonemask(x.astype(np.double), f0, t, self.sr)\n\t            f0 = signal.medfilt(f0, 3)\n\t        elif f0_method == \"dio\":\n\t            f0, t = pyworld.dio(\n\t                x.astype(np.double),\n\t                fs=self.sr,\n\t                f0_ceil=f0_max,\n", "                f0_floor=f0_min,\n\t                frame_period=10,\n\t            )\n\t            f0 = pyworld.stonemask(x.astype(np.double), f0, t, self.sr)\n\t            f0 = signal.medfilt(f0, 3)\n\t        elif f0_method == \"mangio-crepe\":\n\t            f0 = self.get_f0_crepe_computation(x, f0_min, f0_max, p_len, 160, \"full\")\n\t        elif f0_method == \"crepe\":\n\t            f0 = self.get_f0_official_crepe_computation(x, f0_min, f0_max, \"full\")\n\t        f0 *= pow(2, f0_up_key / 12)\n", "        tf0 = self.sr // self.window  # f0 points per second\n\t        if inp_f0 is not None:\n\t            delta_t = np.round(\n\t                (inp_f0[:, 0].max() - inp_f0[:, 0].min()) * tf0 + 1\n\t            ).astype(\"int16\")\n\t            replace_f0 = np.interp(\n\t                list(range(delta_t)), inp_f0[:, 0] * 100, inp_f0[:, 1]\n\t            )\n\t            shape = f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)].shape[0]\n\t            f0[self.x_pad * tf0 : self.x_pad * tf0 + len(replace_f0)] = replace_f0[\n", "                :shape\n\t            ]\n\t        f0bak = f0.copy()\n\t        f0_mel = 1127 * np.log(1 + f0 / 700)\n\t        f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * 254 / (\n\t            f0_mel_max - f0_mel_min\n\t        ) + 1\n\t        f0_mel[f0_mel <= 1] = 1\n\t        f0_mel[f0_mel > 255] = 255\n\t        f0_coarse = np.rint(f0_mel).astype(np.int)\n", "        return f0_coarse, f0bak  # 1-0\n\t    def _convert(\n\t        self,\n\t        model: HubertModel,\n\t        embedding_output_layer: int,\n\t        net_g: SynthesizerTrnMs256NSFSid,\n\t        sid: int,\n\t        audio: np.ndarray,\n\t        pitch: np.ndarray,\n\t        pitchf: np.ndarray,\n", "        index: faiss.IndexIVFFlat,\n\t        big_npy: np.ndarray,\n\t        index_rate: float,\n\t    ):\n\t        feats = torch.from_numpy(audio)\n\t        if self.is_half:\n\t            feats = feats.half()\n\t        else:\n\t            feats = feats.float()\n\t        if feats.dim() == 2:  # double channels\n", "            feats = feats.mean(-1)\n\t        assert feats.dim() == 1, feats.dim()\n\t        feats = feats.view(1, -1)\n\t        padding_mask = torch.BoolTensor(feats.shape).to(self.device).fill_(False)\n\t        half_support = (\n\t            self.device.type == \"cuda\"\n\t            and torch.cuda.get_device_capability(self.device)[0] >= 5.3\n\t        )\n\t        is_feats_dim_768 = net_g.emb_channels == 768\n\t        if isinstance(model, tuple):\n", "            feats = model[0](\n\t                feats.squeeze(0).squeeze(0).to(self.device),\n\t                return_tensors=\"pt\",\n\t                sampling_rate=16000,\n\t            )\n\t            if self.is_half:\n\t                feats = feats.input_values.to(self.device).half()\n\t            else:\n\t                feats = feats.input_values.to(self.device)\n\t            with torch.no_grad():\n", "                if is_feats_dim_768:\n\t                    feats = model[1](feats).last_hidden_state\n\t                else:\n\t                    feats = model[1](feats).extract_features\n\t        else:\n\t            inputs = {\n\t                \"source\": feats.half().to(self.device)\n\t                if half_support\n\t                else feats.to(self.device),\n\t                \"padding_mask\": padding_mask.to(self.device),\n", "                \"output_layer\": embedding_output_layer,\n\t            }\n\t            if not half_support:\n\t                model = model.float()\n\t                inputs[\"source\"] = inputs[\"source\"].float()\n\t            with torch.no_grad():\n\t                logits = model.extract_features(**inputs)\n\t                if is_feats_dim_768:\n\t                    feats = logits[0]\n\t                else:\n", "                    feats = model.final_proj(logits[0])\n\t        if (\n\t            isinstance(index, type(None)) == False\n\t            and isinstance(big_npy, type(None)) == False\n\t            and index_rate != 0\n\t        ):\n\t            npy = feats[0].cpu().numpy()\n\t            if self.is_half:\n\t                npy = npy.astype(\"float32\")\n\t            score, ix = index.search(npy, k=8)\n", "            weight = np.square(1 / score)\n\t            weight /= weight.sum(axis=1, keepdims=True)\n\t            npy = np.sum(big_npy[ix] * np.expand_dims(weight, axis=2), axis=1)\n\t            if self.is_half:\n\t                npy = npy.astype(\"float16\")\n\t            feats = (\n\t                torch.from_numpy(npy).unsqueeze(0).to(self.device) * index_rate\n\t                + (1 - index_rate) * feats\n\t            )\n\t        feats = F.interpolate(feats.permute(0, 2, 1), scale_factor=2).permute(0, 2, 1)\n", "        p_len = audio.shape[0] // self.window\n\t        if feats.shape[1] < p_len:\n\t            p_len = feats.shape[1]\n\t            if pitch != None and pitchf != None:\n\t                pitch = pitch[:, :p_len]\n\t                pitchf = pitchf[:, :p_len]\n\t        p_len = torch.tensor([p_len], device=self.device).long()\n\t        with torch.no_grad():\n\t            if pitch != None and pitchf != None:\n\t                audio1 = (\n", "                    (net_g.infer(feats, p_len, pitch, pitchf, sid)[0][0, 0] * 32768)\n\t                    .data.cpu()\n\t                    .float()\n\t                    .numpy()\n\t                    .astype(np.int16)\n\t                )\n\t            else:\n\t                audio1 = (\n\t                    (net_g.infer(feats, p_len, sid)[0][0, 0] * 32768)\n\t                    .data.cpu()\n", "                    .float()\n\t                    .numpy()\n\t                    .astype(np.int16)\n\t                )\n\t        del feats, p_len, padding_mask\n\t        if torch.cuda.is_available():\n\t            torch.cuda.empty_cache()\n\t        return audio1\n\t    def __call__(\n\t        self,\n", "        model: HubertModel,\n\t        embedding_output_layer: int,\n\t        net_g: SynthesizerTrnMs256NSFSid,\n\t        sid: int,\n\t        audio: np.ndarray,\n\t        transpose: int,\n\t        f0_method: str,\n\t        file_index: str,\n\t        index_rate: float,\n\t        if_f0: bool,\n", "        f0_file: str = None,\n\t    ):\n\t        if file_index != \"\" and os.path.exists(file_index) and index_rate != 0:\n\t            try:\n\t                index = faiss.read_index(file_index)\n\t                # big_npy = np.load(file_big_npy)\n\t                big_npy = index.reconstruct_n(0, index.ntotal)\n\t            except:\n\t                traceback.print_exc()\n\t                index = big_npy = None\n", "        else:\n\t            index = big_npy = None\n\t        bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=16000)\n\t        audio = signal.filtfilt(bh, ah, audio)\n\t        audio_pad = np.pad(audio, (self.window // 2, self.window // 2), mode=\"reflect\")\n\t        opt_ts = []\n\t        if audio_pad.shape[0] > self.t_max:\n\t            audio_sum = np.zeros_like(audio)\n\t            for i in range(self.window):\n\t                audio_sum += audio_pad[i : i - self.window]\n", "            for t in range(self.t_center, audio.shape[0], self.t_center):\n\t                opt_ts.append(\n\t                    t\n\t                    - self.t_query\n\t                    + np.where(\n\t                        np.abs(audio_sum[t - self.t_query : t + self.t_query])\n\t                        == np.abs(audio_sum[t - self.t_query : t + self.t_query]).min()\n\t                    )[0][0]\n\t                )\n\t        audio_pad = np.pad(audio, (self.t_pad, self.t_pad), mode=\"reflect\")\n", "        p_len = audio_pad.shape[0] // self.window\n\t        inp_f0 = None\n\t        if hasattr(f0_file, \"name\"):\n\t            try:\n\t                with open(f0_file.name, \"r\") as f:\n\t                    lines = f.read().strip(\"\\n\").split(\"\\n\")\n\t                inp_f0 = []\n\t                for line in lines:\n\t                    inp_f0.append([float(i) for i in line.split(\",\")])\n\t                inp_f0 = np.array(inp_f0, dtype=\"float32\")\n", "            except:\n\t                traceback.print_exc()\n\t        sid = torch.tensor(sid, device=self.device).unsqueeze(0).long()\n\t        pitch, pitchf = None, None\n\t        if if_f0 == 1:\n\t            pitch, pitchf = self.get_f0(audio_pad, p_len, transpose, f0_method, inp_f0)\n\t            pitch = pitch[:p_len]\n\t            pitchf = pitchf[:p_len]\n\t            if self.device.type == \"mps\":\n\t                pitchf = pitchf.astype(np.float32)\n", "            pitch = torch.tensor(pitch, device=self.device).unsqueeze(0).long()\n\t            pitchf = torch.tensor(pitchf, device=self.device).unsqueeze(0).float()\n\t        audio_opt = []\n\t        s = 0\n\t        t = None\n\t        for t in opt_ts:\n\t            t = t // self.window * self.window\n\t            if if_f0 == 1:\n\t                audio_opt.append(\n\t                    self._convert(\n", "                        model,\n\t                        embedding_output_layer,\n\t                        net_g,\n\t                        sid,\n\t                        audio_pad[s : t + self.t_pad2 + self.window],\n\t                        pitch[:, s // self.window : (t + self.t_pad2) // self.window],\n\t                        pitchf[:, s // self.window : (t + self.t_pad2) // self.window],\n\t                        index,\n\t                        big_npy,\n\t                        index_rate,\n", "                    )[self.t_pad_tgt : -self.t_pad_tgt]\n\t                )\n\t            else:\n\t                audio_opt.append(\n\t                    self._convert(\n\t                        model,\n\t                        embedding_output_layer,\n\t                        net_g,\n\t                        sid,\n\t                        audio_pad[s : t + self.t_pad2 + self.window],\n", "                        None,\n\t                        None,\n\t                        index,\n\t                        big_npy,\n\t                        index_rate,\n\t                    )[self.t_pad_tgt : -self.t_pad_tgt]\n\t                )\n\t            s = t\n\t        if if_f0 == 1:\n\t            audio_opt.append(\n", "                self._convert(\n\t                    model,\n\t                    embedding_output_layer,\n\t                    net_g,\n\t                    sid,\n\t                    audio_pad[t:],\n\t                    pitch[:, t // self.window :] if t is not None else pitch,\n\t                    pitchf[:, t // self.window :] if t is not None else pitchf,\n\t                    index,\n\t                    big_npy,\n", "                    index_rate,\n\t                )[self.t_pad_tgt : -self.t_pad_tgt]\n\t            )\n\t        else:\n\t            audio_opt.append(\n\t                self._convert(\n\t                    model,\n\t                    embedding_output_layer,\n\t                    net_g,\n\t                    sid,\n", "                    audio_pad[t:],\n\t                    None,\n\t                    None,\n\t                    index,\n\t                    big_npy,\n\t                    index_rate,\n\t                )[self.t_pad_tgt : -self.t_pad_tgt]\n\t            )\n\t        audio_opt = np.concatenate(audio_opt)\n\t        del pitch, pitchf, sid\n", "        if torch.cuda.is_available():\n\t            torch.cuda.empty_cache()\n\t        return audio_opt\n"]}
{"filename": "lib/rvc/mel_processing.py", "chunked_list": ["import torch\n\timport torch.utils.data\n\tfrom librosa.filters import mel as librosa_mel_fn\n\tMAX_WAV_VALUE = 32768.0\n\tdef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n\t    \"\"\"\n\t    PARAMS\n\t    ------\n\t    C: compression factor\n\t    \"\"\"\n", "    return torch.log(torch.clamp(x, min=clip_val) * C)\n\tdef dynamic_range_decompression_torch(x, C=1):\n\t    \"\"\"\n\t    PARAMS\n\t    ------\n\t    C: compression factor used to compress\n\t    \"\"\"\n\t    return torch.exp(x) / C\n\tdef spectral_normalize_torch(magnitudes):\n\t    return dynamic_range_compression_torch(magnitudes)\n", "def spectral_de_normalize_torch(magnitudes):\n\t    return dynamic_range_decompression_torch(magnitudes)\n\tmel_basis = {}\n\thann_window = {}\n\tdef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n\t    if torch.min(y) < -1.07:\n\t        print(\"min value is \", torch.min(y))\n\t    if torch.max(y) > 1.07:\n\t        print(\"max value is \", torch.max(y))\n\t    global hann_window\n", "    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n\t    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n\t    if wnsize_dtype_device not in hann_window:\n\t        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(\n\t            dtype=y.dtype, device=y.device\n\t        )\n\t    y = torch.nn.functional.pad(\n\t        y.unsqueeze(1),\n\t        (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n\t        mode=\"reflect\",\n", "    )\n\t    y = y.squeeze(1)\n\t    # mps does not support torch.stft.\n\t    if y.device.type == \"mps\":\n\t        i = y.cpu()\n\t        win = hann_window[wnsize_dtype_device].cpu()\n\t    else:\n\t        i = y\n\t        win = hann_window[wnsize_dtype_device]\n\t    spec = torch.stft(\n", "        i,\n\t        n_fft,\n\t        hop_length=hop_size,\n\t        win_length=win_size,\n\t        window=win,\n\t        center=center,\n\t        pad_mode=\"reflect\",\n\t        normalized=False,\n\t        onesided=True,\n\t        return_complex=False,\n", "    ).to(device=y.device)\n\t    spec = torch.sqrt(spec.pow(2).sum(-1) + 1e-6)\n\t    return spec\n\tdef spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n\t    global mel_basis\n\t    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n\t    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n\t    if fmax_dtype_device not in mel_basis:\n\t        mel = librosa_mel_fn(sampling_rate, n_fft, num_mels, fmin, fmax)\n\t        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(\n", "            dtype=spec.dtype, device=spec.device\n\t        )\n\t    melspec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n\t    melspec = spectral_normalize_torch(melspec)\n\t    return melspec\n\tdef mel_spectrogram_torch(\n\t    y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False\n\t):\n\t    \"\"\"Convert waveform into Mel-frequency Log-amplitude spectrogram.\n\t    Args:\n", "        y       :: (B, T)           - Waveforms\n\t    Returns:\n\t        melspec :: (B, Freq, Frame) - Mel-frequency Log-amplitude spectrogram\n\t    \"\"\"\n\t    # Linear-frequency Linear-amplitude spectrogram :: (B, T) -> (B, Freq, Frame)\n\t    spec = spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center)\n\t    # Mel-frequency Log-amplitude spectrogram :: (B, Freq, Frame) -> (B, Freq=num_mels, Frame)\n\t    melspec = spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax)\n\t    return melspec\n"]}
{"filename": "lib/rvc/commons.py", "chunked_list": ["import math\n\timport torch\n\tfrom torch.nn import functional as F\n\tdef init_weights(m, mean=0.0, std=0.01):\n\t    classname = m.__class__.__name__\n\t    if classname.find(\"Conv\") != -1:\n\t        m.weight.data.normal_(mean, std)\n\tdef get_padding(kernel_size, dilation=1):\n\t    return int((kernel_size * dilation - dilation) / 2)\n\tdef convert_pad_shape(pad_shape):\n", "    l = pad_shape[::-1]\n\t    pad_shape = [item for sublist in l for item in sublist]\n\t    return pad_shape\n\tdef kl_divergence(m_p, logs_p, m_q, logs_q):\n\t    \"\"\"KL(P||Q)\"\"\"\n\t    kl = (logs_q - logs_p) - 0.5\n\t    kl += (\n\t        0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n\t    )\n\t    return kl\n", "def rand_gumbel(shape):\n\t    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n\t    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n\t    return -torch.log(-torch.log(uniform_samples))\n\tdef rand_gumbel_like(x):\n\t    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n\t    return g\n\tdef slice_segments(x, ids_str, segment_size=4):\n\t    ret = torch.zeros_like(x[:, :, :segment_size])\n\t    for i in range(x.size(0)):\n", "        idx_str = ids_str[i]\n\t        idx_end = idx_str + segment_size\n\t        ret[i] = x[i, :, idx_str:idx_end]\n\t    return ret\n\tdef slice_segments2(x, ids_str, segment_size=4):\n\t    ret = torch.zeros_like(x[:, :segment_size])\n\t    for i in range(x.size(0)):\n\t        idx_str = ids_str[i]\n\t        idx_end = idx_str + segment_size\n\t        ret[i] = x[i, idx_str:idx_end]\n", "    return ret\n\tdef rand_slice_segments(x, x_lengths=None, segment_size=4):\n\t    b, d, t = x.size()\n\t    if x_lengths is None:\n\t        x_lengths = t\n\t    ids_str_max = x_lengths - segment_size + 1\n\t    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n\t    ret = slice_segments(x, ids_str, segment_size)\n\t    return ret, ids_str\n\tdef get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n", "    position = torch.arange(length, dtype=torch.float)\n\t    num_timescales = channels // 2\n\t    log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / (\n\t        num_timescales - 1\n\t    )\n\t    inv_timescales = min_timescale * torch.exp(\n\t        torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment\n\t    )\n\t    scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n\t    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n", "    signal = F.pad(signal, [0, 0, 0, channels % 2])\n\t    signal = signal.view(1, channels, length)\n\t    return signal\n\tdef add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n\t    b, channels, length = x.size()\n\t    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n\t    return x + signal.to(dtype=x.dtype, device=x.device)\n\tdef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n\t    b, channels, length = x.size()\n\t    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n", "    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\n\tdef subsequent_mask(length):\n\t    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n\t    return mask\n\t@torch.jit.script\n\tdef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n\t    n_channels_int = n_channels[0]\n\t    in_act = input_a + input_b\n\t    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n\t    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n", "    acts = t_act * s_act\n\t    return acts\n\tdef convert_pad_shape(pad_shape):\n\t    l = pad_shape[::-1]\n\t    pad_shape = [item for sublist in l for item in sublist]\n\t    return pad_shape\n\tdef shift_1d(x):\n\t    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n\t    return x\n\tdef sequence_mask(length, max_length=None):\n", "    if max_length is None:\n\t        max_length = length.max()\n\t    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n\t    return x.unsqueeze(0) < length.unsqueeze(1)\n\tdef generate_path(duration, mask):\n\t    \"\"\"\n\t    duration: [b, 1, t_x]\n\t    mask: [b, 1, t_y, t_x]\n\t    \"\"\"\n\t    b, _, t_y, t_x = mask.shape\n", "    cum_duration = torch.cumsum(duration, -1)\n\t    cum_duration_flat = cum_duration.view(b * t_x)\n\t    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)\n\t    path = path.view(b, t_x, t_y)\n\t    path = path - F.pad(path, convert_pad_shape([[0, 0], [1, 0], [0, 0]]))[:, :-1]\n\t    path = path.unsqueeze(1).transpose(2, 3) * mask\n\t    return path\n\tdef clip_grad_value_(parameters, clip_value, norm_type=2):\n\t    if isinstance(parameters, torch.Tensor):\n\t        parameters = [parameters]\n", "    parameters = list(filter(lambda p: p.grad is not None, parameters))\n\t    norm_type = float(norm_type)\n\t    if clip_value is not None:\n\t        clip_value = float(clip_value)\n\t    total_norm = 0\n\t    for p in parameters:\n\t        param_norm = p.grad.data.norm(norm_type)\n\t        total_norm += param_norm.item() ** norm_type\n\t        if clip_value is not None:\n\t            p.grad.data.clamp_(min=-clip_value, max=clip_value)\n", "    total_norm = total_norm ** (1.0 / norm_type)\n\t    return total_norm\n"]}
{"filename": "lib/rvc/utils.py", "chunked_list": ["import glob\n\timport logging\n\timport os\n\timport shutil\n\timport socket\n\timport sys\n\timport ffmpeg\n\timport matplotlib\n\timport matplotlib.pylab as plt\n\timport numpy as np\n", "import torch\n\tfrom scipy.io.wavfile import read\n\tfrom torch.nn import functional as F\n\tfrom modules.shared import ROOT_DIR\n\tfrom .config import TrainConfig\n\tmatplotlib.use(\"Agg\")\n\tlogging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n\tlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n\tlogger = logging\n\tdef load_audio(file: str, sr):\n", "    try:\n\t        # https://github.com/openai/whisper/blob/main/whisper/audio.py#L26\n\t        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n\t        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n\t        file = (\n\t            file.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n\t        )  # Prevent small white copy path head and tail with spaces and \" and return\n\t        out, _ = (\n\t            ffmpeg.input(file, threads=0)\n\t            .output(\"-\", format=\"f32le\", acodec=\"pcm_f32le\", ac=1, ar=sr)\n", "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n\t        )\n\t    except Exception as e:\n\t        raise RuntimeError(f\"Failed to load audio: {e}\")\n\t    return np.frombuffer(out, np.float32).flatten()\n\tdef find_empty_port():\n\t    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n\t    s.bind((\"\", 0))\n\t    s.listen(1)\n\t    port = s.getsockname()[1]\n", "    s.close()\n\t    return port\n\tdef load_checkpoint(checkpoint_path, model, optimizer=None, load_opt=1):\n\t    assert os.path.isfile(checkpoint_path)\n\t    checkpoint_dict = torch.load(checkpoint_path, map_location=\"cpu\")\n\t    saved_state_dict = checkpoint_dict[\"model\"]\n\t    if hasattr(model, \"module\"):\n\t        state_dict = model.module.state_dict()\n\t    else:\n\t        state_dict = model.state_dict()\n", "    new_state_dict = {}\n\t    for k, v in state_dict.items():  # 模型需要的shape\n\t        try:\n\t            new_state_dict[k] = saved_state_dict[k]\n\t            if saved_state_dict[k].shape != state_dict[k].shape:\n\t                print(\n\t                    f\"shape-{k}-mismatch|need-{state_dict[k].shape}|get-{saved_state_dict[k].shape}\"\n\t                )\n\t                if saved_state_dict[k].dim() == 2:  # NOTE: check is this ok?\n\t                    # for embedded input 256 <==> 768\n", "                    # this achieves we can continue training from original's pretrained checkpoints when using embedder that 768-th dim output etc.\n\t                    if saved_state_dict[k].dtype == torch.half:\n\t                        new_state_dict[k] = (\n\t                            F.interpolate(\n\t                                saved_state_dict[k].float().unsqueeze(0).unsqueeze(0),\n\t                                size=state_dict[k].shape,\n\t                                mode=\"bilinear\",\n\t                            )\n\t                            .half()\n\t                            .squeeze(0)\n", "                            .squeeze(0)\n\t                        )\n\t                    else:\n\t                        new_state_dict[k] = (\n\t                            F.interpolate(\n\t                                saved_state_dict[k].unsqueeze(0).unsqueeze(0),\n\t                                size=state_dict[k].shape,\n\t                                mode=\"bilinear\",\n\t                            )\n\t                            .squeeze(0)\n", "                            .squeeze(0)\n\t                        )\n\t                    print(\n\t                        \"interpolated new_state_dict\",\n\t                        k,\n\t                        \"from\",\n\t                        saved_state_dict[k].shape,\n\t                        \"to\",\n\t                        new_state_dict[k].shape,\n\t                    )\n", "                else:\n\t                    raise KeyError\n\t        except Exception as e:\n\t            # print(traceback.format_exc())\n\t            print(f\"{k} is not in the checkpoint\")\n\t            print(\"error: %s\" % e)\n\t            new_state_dict[k] = v  # 模型自带的随机值\n\t    if hasattr(model, \"module\"):\n\t        model.module.load_state_dict(new_state_dict, strict=False)\n\t    else:\n", "        model.load_state_dict(new_state_dict, strict=False)\n\t    print(\"Loaded model weights\")\n\t    epoch = checkpoint_dict[\"epoch\"]\n\t    learning_rate = checkpoint_dict[\"learning_rate\"]\n\t    if optimizer is not None and load_opt == 1:\n\t        optimizer.load_state_dict(checkpoint_dict[\"optimizer\"])\n\t    print(\"Loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, epoch))\n\t    return model, optimizer, learning_rate, epoch\n\tdef save_state(model, optimizer, learning_rate, epoch, checkpoint_path):\n\t    print(\n", "        \"Saving model and optimizer state at epoch {} to {}\".format(\n\t            epoch, checkpoint_path\n\t        )\n\t    )\n\t    if hasattr(model, \"module\"):\n\t        state_dict = model.module.state_dict()\n\t    else:\n\t        state_dict = model.state_dict()\n\t    torch.save(\n\t        {\n", "            \"model\": state_dict,\n\t            \"epoch\": epoch,\n\t            \"optimizer\": optimizer.state_dict(),\n\t            \"learning_rate\": learning_rate,\n\t        },\n\t        checkpoint_path,\n\t    )\n\tdef summarize(\n\t    writer,\n\t    global_step,\n", "    scalars={},\n\t    histograms={},\n\t    images={},\n\t    audios={},\n\t    audio_sampling_rate=22050,\n\t):\n\t    for k, v in scalars.items():\n\t        writer.add_scalar(k, v, global_step)\n\t    for k, v in histograms.items():\n\t        writer.add_histogram(k, v, global_step)\n", "    for k, v in images.items():\n\t        writer.add_image(k, v, global_step, dataformats=\"HWC\")\n\t    for k, v in audios.items():\n\t        writer.add_audio(k, v, global_step, audio_sampling_rate)\n\tdef latest_checkpoint_path(dir_path, regex=\"G_*.pth\"):\n\t    filelist = glob.glob(os.path.join(dir_path, regex))\n\t    if len(filelist) == 0:\n\t        return None\n\t    filelist.sort(key=lambda f: int(\"\".join(filter(str.isdigit, f))))\n\t    filepath = filelist[-1]\n", "    return filepath\n\tdef plot_spectrogram_to_numpy(spectrogram):\n\t    fig, ax = plt.subplots(figsize=(10, 2))\n\t    im = ax.imshow(spectrogram, aspect=\"auto\", origin=\"lower\", interpolation=\"none\")\n\t    plt.colorbar(im, ax=ax)\n\t    plt.xlabel(\"Frames\")\n\t    plt.ylabel(\"Channels\")\n\t    plt.tight_layout()\n\t    fig.canvas.draw()\n\t    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n", "    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\t    plt.close()\n\t    return data\n\tdef plot_alignment_to_numpy(alignment, info=None):\n\t    fig, ax = plt.subplots(figsize=(6, 4))\n\t    im = ax.imshow(\n\t        alignment.transpose(), aspect=\"auto\", origin=\"lower\", interpolation=\"none\"\n\t    )\n\t    fig.colorbar(im, ax=ax)\n\t    xlabel = \"Decoder timestep\"\n", "    if info is not None:\n\t        xlabel += \"\\n\\n\" + info\n\t    plt.xlabel(xlabel)\n\t    plt.ylabel(\"Encoder timestep\")\n\t    plt.tight_layout()\n\t    fig.canvas.draw()\n\t    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\"\")\n\t    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n\t    plt.close()\n\t    return data\n", "def load_wav_to_torch(full_path):\n\t    sampling_rate, data = read(full_path)\n\t    return torch.FloatTensor(data.astype(np.float32)), sampling_rate\n\tdef load_config(training_dir: str, sample_rate: int, emb_channels: int):\n\t    if emb_channels == 256:\n\t        config_path = os.path.join(ROOT_DIR, \"configs\", f\"{sample_rate}.json\")\n\t    else:\n\t        config_path = os.path.join(\n\t            ROOT_DIR, \"configs\", f\"{sample_rate}-{emb_channels}.json\"\n\t        )\n", "    config_save_path = os.path.join(training_dir, \"config.json\")\n\t    shutil.copyfile(config_path, config_save_path)\n\t    return TrainConfig.parse_file(config_save_path)\n"]}
{"filename": "lib/rvc/checkpoints.py", "chunked_list": ["import os\n\tfrom collections import OrderedDict\n\tfrom typing import *\n\timport torch\n\tdef write_config(state_dict: Dict[str, Any], cfg: Dict[str, Any]):\n\t    state_dict[\"config\"] = []\n\t    for key, x in cfg.items():\n\t        state_dict[\"config\"].append(x)\n\t    state_dict[\"params\"] = cfg\n\tdef create_trained_model(\n", "    weights: Dict[str, Any],\n\t    version: Literal[\"v1\", \"v2\"],\n\t    sr: str,\n\t    f0: bool,\n\t    emb_name: str,\n\t    emb_ch: int,\n\t    emb_output_layer: int,\n\t    epoch: int,\n\t    speaker_info: Optional[dict[str, int]]\n\t):\n", "    state_dict = OrderedDict()\n\t    state_dict[\"weight\"] = {}\n\t    for key in weights.keys():\n\t        if \"enc_q\" in key:\n\t            continue\n\t        state_dict[\"weight\"][key] = weights[key].half()\n\t    if sr == \"40k\":\n\t        write_config(\n\t            state_dict,\n\t            {\n", "                \"spec_channels\": 1025,\n\t                \"segment_size\": 32,\n\t                \"inter_channels\": 192,\n\t                \"hidden_channels\": 192,\n\t                \"filter_channels\": 768,\n\t                \"n_heads\": 2,\n\t                \"n_layers\": 6,\n\t                \"kernel_size\": 3,\n\t                \"p_dropout\": 0,\n\t                \"resblock\": \"1\",\n", "                \"resblock_kernel_sizes\": [3, 7, 11],\n\t                \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n\t                \"upsample_rates\": [10, 10, 2, 2],\n\t                \"upsample_initial_channel\": 512,\n\t                \"upsample_kernel_sizes\": [16, 16, 4, 4],\n\t                \"spk_embed_dim\": 109 if speaker_info is None else len(speaker_info),\n\t                \"gin_channels\": 256,\n\t                \"emb_channels\": emb_ch,\n\t                \"sr\": 40000,\n\t            },\n", "        )\n\t    elif sr == \"48k\":\n\t        write_config(\n\t            state_dict,\n\t            {\n\t                \"spec_channels\": 1025,\n\t                \"segment_size\": 32,\n\t                \"inter_channels\": 192,\n\t                \"hidden_channels\": 192,\n\t                \"filter_channels\": 768,\n", "                \"n_heads\": 2,\n\t                \"n_layers\": 6,\n\t                \"kernel_size\": 3,\n\t                \"p_dropout\": 0,\n\t                \"resblock\": \"1\",\n\t                \"resblock_kernel_sizes\": [3, 7, 11],\n\t                \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n\t                \"upsample_rates\": [10, 6, 2, 2, 2],\n\t                \"upsample_initial_channel\": 512,\n\t                \"upsample_kernel_sizes\": [16, 16, 4, 4, 4],\n", "                \"spk_embed_dim\": 109 if speaker_info is None else len(speaker_info),\n\t                \"gin_channels\": 256,\n\t                \"emb_channels\": emb_ch,\n\t                \"sr\": 48000,\n\t            },\n\t        )\n\t    elif sr == \"32k\":\n\t        write_config(\n\t            state_dict,\n\t            {\n", "                \"spec_channels\": 513,\n\t                \"segment_size\": 32,\n\t                \"inter_channels\": 192,\n\t                \"hidden_channels\": 192,\n\t                \"filter_channels\": 768,\n\t                \"n_heads\": 2,\n\t                \"n_layers\": 6,\n\t                \"kernel_size\": 3,\n\t                \"p_dropout\": 0,\n\t                \"resblock\": \"1\",\n", "                \"resblock_kernel_sizes\": [3, 7, 11],\n\t                \"resblock_dilation_sizes\": [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n\t                \"upsample_rates\": [10, 4, 2, 2, 2],\n\t                \"upsample_initial_channel\": 512,\n\t                \"upsample_kernel_sizes\": [16, 16, 4, 4, 4],\n\t                \"spk_embed_dim\": 109 if speaker_info is None else len(speaker_info),\n\t                \"gin_channels\": 256,\n\t                \"emb_channels\": emb_ch,\n\t                \"sr\": 32000,\n\t            },\n", "        )\n\t    state_dict[\"version\"] = version\n\t    state_dict[\"info\"] = f\"{epoch}epoch\"\n\t    state_dict[\"sr\"] = sr\n\t    state_dict[\"f0\"] = 1 if f0 else 0\n\t    state_dict[\"embedder_name\"] = emb_name\n\t    state_dict[\"embedder_output_layer\"] = emb_output_layer\n\t    if not speaker_info is None:\n\t        state_dict[\"speaker_info\"] = {str(v): str(k) for k, v in speaker_info.items()}\n\t    return state_dict\n", "def save(\n\t    model,\n\t    version: Literal[\"v1\", \"v2\"],\n\t    sr: str,\n\t    f0: bool,\n\t    emb_name: str,\n\t    emb_ch: int,\n\t    emb_output_layer: int,\n\t    filepath: str,\n\t    epoch: int,\n", "    speaker_info: Optional[dict[str, int]]\n\t):\n\t    if hasattr(model, \"module\"):\n\t        state_dict = model.module.state_dict()\n\t    else:\n\t        state_dict = model.state_dict()\n\t    print(f\"save: emb_name: {emb_name} {emb_ch}\")\n\t    state_dict = create_trained_model(\n\t        state_dict,\n\t        version,\n", "        sr,\n\t        f0,\n\t        emb_name,\n\t        emb_ch,\n\t        emb_output_layer,\n\t        epoch,\n\t        speaker_info\n\t    )\n\t    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n\t    torch.save(state_dict, filepath)\n"]}
{"filename": "lib/rvc/modules.py", "chunked_list": ["import math\n\timport torch\n\tfrom torch import nn\n\tfrom torch.nn import Conv1d\n\tfrom torch.nn import functional as F\n\tfrom torch.nn.utils import remove_weight_norm, weight_norm\n\tfrom . import commons\n\tfrom .commons import get_padding, init_weights\n\tfrom .transforms import piecewise_rational_quadratic_transform\n\tLRELU_SLOPE = 0.1\n", "class LayerNorm(nn.Module):\n\t    def __init__(self, channels, eps=1e-5):\n\t        super().__init__()\n\t        self.channels = channels\n\t        self.eps = eps\n\t        self.gamma = nn.Parameter(torch.ones(channels))\n\t        self.beta = nn.Parameter(torch.zeros(channels))\n\t    def forward(self, x):\n\t        x = x.transpose(1, -1)\n\t        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)\n", "        return x.transpose(1, -1)\n\tclass ConvReluNorm(nn.Module):\n\t    def __init__(\n\t        self,\n\t        in_channels,\n\t        hidden_channels,\n\t        out_channels,\n\t        kernel_size,\n\t        n_layers,\n\t        p_dropout,\n", "    ):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.hidden_channels = hidden_channels\n\t        self.out_channels = out_channels\n\t        self.kernel_size = kernel_size\n\t        self.n_layers = n_layers\n\t        self.p_dropout = p_dropout\n\t        assert n_layers > 1, \"Number of layers should be larger than 0.\"\n\t        self.conv_layers = nn.ModuleList()\n", "        self.norm_layers = nn.ModuleList()\n\t        self.conv_layers.append(\n\t            nn.Conv1d(\n\t                in_channels, hidden_channels, kernel_size, padding=kernel_size // 2\n\t            )\n\t        )\n\t        self.norm_layers.append(LayerNorm(hidden_channels))\n\t        self.relu_drop = nn.Sequential(nn.ReLU(), nn.Dropout(p_dropout))\n\t        for _ in range(n_layers - 1):\n\t            self.conv_layers.append(\n", "                nn.Conv1d(\n\t                    hidden_channels,\n\t                    hidden_channels,\n\t                    kernel_size,\n\t                    padding=kernel_size // 2,\n\t                )\n\t            )\n\t            self.norm_layers.append(LayerNorm(hidden_channels))\n\t        self.proj = nn.Conv1d(hidden_channels, out_channels, 1)\n\t        self.proj.weight.data.zero_()\n", "        self.proj.bias.data.zero_()\n\t    def forward(self, x, x_mask):\n\t        x_org = x\n\t        for i in range(self.n_layers):\n\t            x = self.conv_layers[i](x * x_mask)\n\t            x = self.norm_layers[i](x)\n\t            x = self.relu_drop(x)\n\t        x = x_org + self.proj(x)\n\t        return x * x_mask\n\tclass DDSConv(nn.Module):\n", "    \"\"\"\n\t    Dialted and Depth-Separable Convolution\n\t    \"\"\"\n\t    def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):\n\t        super().__init__()\n\t        self.channels = channels\n\t        self.kernel_size = kernel_size\n\t        self.n_layers = n_layers\n\t        self.p_dropout = p_dropout\n\t        self.drop = nn.Dropout(p_dropout)\n", "        self.convs_sep = nn.ModuleList()\n\t        self.convs_1x1 = nn.ModuleList()\n\t        self.norms_1 = nn.ModuleList()\n\t        self.norms_2 = nn.ModuleList()\n\t        for i in range(n_layers):\n\t            dilation = kernel_size**i\n\t            padding = (kernel_size * dilation - dilation) // 2\n\t            self.convs_sep.append(\n\t                nn.Conv1d(\n\t                    channels,\n", "                    channels,\n\t                    kernel_size,\n\t                    groups=channels,\n\t                    dilation=dilation,\n\t                    padding=padding,\n\t                )\n\t            )\n\t            self.convs_1x1.append(nn.Conv1d(channels, channels, 1))\n\t            self.norms_1.append(LayerNorm(channels))\n\t            self.norms_2.append(LayerNorm(channels))\n", "    def forward(self, x, x_mask, g=None):\n\t        if g is not None:\n\t            x = x + g\n\t        for i in range(self.n_layers):\n\t            y = self.convs_sep[i](x * x_mask)\n\t            y = self.norms_1[i](y)\n\t            y = F.gelu(y)\n\t            y = self.convs_1x1[i](y)\n\t            y = self.norms_2[i](y)\n\t            y = F.gelu(y)\n", "            y = self.drop(y)\n\t            x = x + y\n\t        return x * x_mask\n\tclass WN(torch.nn.Module):\n\t    def __init__(\n\t        self,\n\t        hidden_channels,\n\t        kernel_size,\n\t        dilation_rate,\n\t        n_layers,\n", "        gin_channels=0,\n\t        p_dropout=0,\n\t    ):\n\t        super(WN, self).__init__()\n\t        assert kernel_size % 2 == 1\n\t        self.hidden_channels = hidden_channels\n\t        self.kernel_size = (kernel_size,)\n\t        self.dilation_rate = dilation_rate\n\t        self.n_layers = n_layers\n\t        self.gin_channels = gin_channels\n", "        self.p_dropout = p_dropout\n\t        self.in_layers = torch.nn.ModuleList()\n\t        self.res_skip_layers = torch.nn.ModuleList()\n\t        self.drop = nn.Dropout(p_dropout)\n\t        if gin_channels != 0:\n\t            cond_layer = torch.nn.Conv1d(\n\t                gin_channels, 2 * hidden_channels * n_layers, 1\n\t            )\n\t            self.cond_layer = torch.nn.utils.weight_norm(cond_layer, name=\"weight\")\n\t        for i in range(n_layers):\n", "            dilation = dilation_rate**i\n\t            padding = int((kernel_size * dilation - dilation) / 2)\n\t            in_layer = torch.nn.Conv1d(\n\t                hidden_channels,\n\t                2 * hidden_channels,\n\t                kernel_size,\n\t                dilation=dilation,\n\t                padding=padding,\n\t            )\n\t            in_layer = torch.nn.utils.weight_norm(in_layer, name=\"weight\")\n", "            self.in_layers.append(in_layer)\n\t            # last one is not necessary\n\t            if i < n_layers - 1:\n\t                res_skip_channels = 2 * hidden_channels\n\t            else:\n\t                res_skip_channels = hidden_channels\n\t            res_skip_layer = torch.nn.Conv1d(hidden_channels, res_skip_channels, 1)\n\t            res_skip_layer = torch.nn.utils.weight_norm(res_skip_layer, name=\"weight\")\n\t            self.res_skip_layers.append(res_skip_layer)\n\t    def forward(self, x, x_mask, g=None, **kwargs):\n", "        output = torch.zeros_like(x)\n\t        n_channels_tensor = torch.IntTensor([self.hidden_channels])\n\t        if g is not None:\n\t            g = self.cond_layer(g)\n\t        for i in range(self.n_layers):\n\t            x_in = self.in_layers[i](x)\n\t            if g is not None:\n\t                cond_offset = i * 2 * self.hidden_channels\n\t                g_l = g[:, cond_offset : cond_offset + 2 * self.hidden_channels, :]\n\t            else:\n", "                g_l = torch.zeros_like(x_in)\n\t            acts = commons.fused_add_tanh_sigmoid_multiply(x_in, g_l, n_channels_tensor)\n\t            acts = self.drop(acts)\n\t            res_skip_acts = self.res_skip_layers[i](acts)\n\t            if i < self.n_layers - 1:\n\t                res_acts = res_skip_acts[:, : self.hidden_channels, :]\n\t                x = (x + res_acts) * x_mask\n\t                output = output + res_skip_acts[:, self.hidden_channels :, :]\n\t            else:\n\t                output = output + res_skip_acts\n", "        return output * x_mask\n\t    def remove_weight_norm(self):\n\t        if self.gin_channels != 0:\n\t            torch.nn.utils.remove_weight_norm(self.cond_layer)\n\t        for l in self.in_layers:\n\t            torch.nn.utils.remove_weight_norm(l)\n\t        for l in self.res_skip_layers:\n\t            torch.nn.utils.remove_weight_norm(l)\n\tclass ResBlock1(torch.nn.Module):\n\t    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n", "        super(ResBlock1, self).__init__()\n\t        self.convs1 = nn.ModuleList(\n\t            [\n\t                weight_norm(\n\t                    Conv1d(\n\t                        channels,\n\t                        channels,\n\t                        kernel_size,\n\t                        1,\n\t                        dilation=dilation[0],\n", "                        padding=get_padding(kernel_size, dilation[0]),\n\t                    )\n\t                ),\n\t                weight_norm(\n\t                    Conv1d(\n\t                        channels,\n\t                        channels,\n\t                        kernel_size,\n\t                        1,\n\t                        dilation=dilation[1],\n", "                        padding=get_padding(kernel_size, dilation[1]),\n\t                    )\n\t                ),\n\t                weight_norm(\n\t                    Conv1d(\n\t                        channels,\n\t                        channels,\n\t                        kernel_size,\n\t                        1,\n\t                        dilation=dilation[2],\n", "                        padding=get_padding(kernel_size, dilation[2]),\n\t                    )\n\t                ),\n\t            ]\n\t        )\n\t        self.convs1.apply(init_weights)\n\t        self.convs2 = nn.ModuleList(\n\t            [\n\t                weight_norm(\n\t                    Conv1d(\n", "                        channels,\n\t                        channels,\n\t                        kernel_size,\n\t                        1,\n\t                        dilation=1,\n\t                        padding=get_padding(kernel_size, 1),\n\t                    )\n\t                ),\n\t                weight_norm(\n\t                    Conv1d(\n", "                        channels,\n\t                        channels,\n\t                        kernel_size,\n\t                        1,\n\t                        dilation=1,\n\t                        padding=get_padding(kernel_size, 1),\n\t                    )\n\t                ),\n\t                weight_norm(\n\t                    Conv1d(\n", "                        channels,\n\t                        channels,\n\t                        kernel_size,\n\t                        1,\n\t                        dilation=1,\n\t                        padding=get_padding(kernel_size, 1),\n\t                    )\n\t                ),\n\t            ]\n\t        )\n", "        self.convs2.apply(init_weights)\n\t    def forward(self, x, x_mask=None):\n\t        for c1, c2 in zip(self.convs1, self.convs2):\n\t            xt = F.leaky_relu(x, LRELU_SLOPE)\n\t            if x_mask is not None:\n\t                xt = xt * x_mask\n\t            xt = c1(xt)\n\t            xt = F.leaky_relu(xt, LRELU_SLOPE)\n\t            if x_mask is not None:\n\t                xt = xt * x_mask\n", "            xt = c2(xt)\n\t            x = xt + x\n\t        if x_mask is not None:\n\t            x = x * x_mask\n\t        return x\n\t    def remove_weight_norm(self):\n\t        for l in self.convs1:\n\t            remove_weight_norm(l)\n\t        for l in self.convs2:\n\t            remove_weight_norm(l)\n", "class ResBlock2(torch.nn.Module):\n\t    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n\t        super(ResBlock2, self).__init__()\n\t        self.convs = nn.ModuleList(\n\t            [\n\t                weight_norm(\n\t                    Conv1d(\n\t                        channels,\n\t                        channels,\n\t                        kernel_size,\n", "                        1,\n\t                        dilation=dilation[0],\n\t                        padding=get_padding(kernel_size, dilation[0]),\n\t                    )\n\t                ),\n\t                weight_norm(\n\t                    Conv1d(\n\t                        channels,\n\t                        channels,\n\t                        kernel_size,\n", "                        1,\n\t                        dilation=dilation[1],\n\t                        padding=get_padding(kernel_size, dilation[1]),\n\t                    )\n\t                ),\n\t            ]\n\t        )\n\t        self.convs.apply(init_weights)\n\t    def forward(self, x, x_mask=None):\n\t        for c in self.convs:\n", "            xt = F.leaky_relu(x, LRELU_SLOPE)\n\t            if x_mask is not None:\n\t                xt = xt * x_mask\n\t            xt = c(xt)\n\t            x = xt + x\n\t        if x_mask is not None:\n\t            x = x * x_mask\n\t        return x\n\t    def remove_weight_norm(self):\n\t        for l in self.convs:\n", "            remove_weight_norm(l)\n\tclass Log(nn.Module):\n\t    def forward(self, x, x_mask, reverse=False, **kwargs):\n\t        if not reverse:\n\t            y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n\t            logdet = torch.sum(-y, [1, 2])\n\t            return y, logdet\n\t        else:\n\t            x = torch.exp(x) * x_mask\n\t            return x\n", "class Flip(nn.Module):\n\t    def forward(self, x, *args, reverse=False, **kwargs):\n\t        x = torch.flip(x, [1])\n\t        if not reverse:\n\t            logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n\t            return x, logdet\n\t        else:\n\t            return x\n\tclass ElementwiseAffine(nn.Module):\n\t    def __init__(self, channels):\n", "        super().__init__()\n\t        self.channels = channels\n\t        self.m = nn.Parameter(torch.zeros(channels, 1))\n\t        self.logs = nn.Parameter(torch.zeros(channels, 1))\n\t    def forward(self, x, x_mask, reverse=False, **kwargs):\n\t        if not reverse:\n\t            y = self.m + torch.exp(self.logs) * x\n\t            y = y * x_mask\n\t            logdet = torch.sum(self.logs * x_mask, [1, 2])\n\t            return y, logdet\n", "        else:\n\t            x = (x - self.m) * torch.exp(-self.logs) * x_mask\n\t            return x\n\tclass ResidualCouplingLayer(nn.Module):\n\t    def __init__(\n\t        self,\n\t        channels,\n\t        hidden_channels,\n\t        kernel_size,\n\t        dilation_rate,\n", "        n_layers,\n\t        p_dropout=0,\n\t        gin_channels=0,\n\t        mean_only=False,\n\t    ):\n\t        assert channels % 2 == 0, \"channels should be divisible by 2\"\n\t        super().__init__()\n\t        self.channels = channels\n\t        self.hidden_channels = hidden_channels\n\t        self.kernel_size = kernel_size\n", "        self.dilation_rate = dilation_rate\n\t        self.n_layers = n_layers\n\t        self.half_channels = channels // 2\n\t        self.mean_only = mean_only\n\t        self.pre = nn.Conv1d(self.half_channels, hidden_channels, 1)\n\t        self.enc = WN(\n\t            hidden_channels,\n\t            kernel_size,\n\t            dilation_rate,\n\t            n_layers,\n", "            p_dropout=p_dropout,\n\t            gin_channels=gin_channels,\n\t        )\n\t        self.post = nn.Conv1d(hidden_channels, self.half_channels * (2 - mean_only), 1)\n\t        self.post.weight.data.zero_()\n\t        self.post.bias.data.zero_()\n\t    def forward(self, x, x_mask, g=None, reverse=False):\n\t        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n\t        h = self.pre(x0) * x_mask\n\t        h = self.enc(h, x_mask, g=g)\n", "        stats = self.post(h) * x_mask\n\t        if not self.mean_only:\n\t            m, logs = torch.split(stats, [self.half_channels] * 2, 1)\n\t        else:\n\t            m = stats\n\t            logs = torch.zeros_like(m)\n\t        if not reverse:\n\t            x1 = m + x1 * torch.exp(logs) * x_mask\n\t            x = torch.cat([x0, x1], 1)\n\t            logdet = torch.sum(logs, [1, 2])\n", "            return x, logdet\n\t        else:\n\t            x1 = (x1 - m) * torch.exp(-logs) * x_mask\n\t            x = torch.cat([x0, x1], 1)\n\t            return x\n\t    def remove_weight_norm(self):\n\t        self.enc.remove_weight_norm()\n\tclass ConvFlow(nn.Module):\n\t    def __init__(\n\t        self,\n", "        in_channels,\n\t        filter_channels,\n\t        kernel_size,\n\t        n_layers,\n\t        num_bins=10,\n\t        tail_bound=5.0,\n\t    ):\n\t        super().__init__()\n\t        self.in_channels = in_channels\n\t        self.filter_channels = filter_channels\n", "        self.kernel_size = kernel_size\n\t        self.n_layers = n_layers\n\t        self.num_bins = num_bins\n\t        self.tail_bound = tail_bound\n\t        self.half_channels = in_channels // 2\n\t        self.pre = nn.Conv1d(self.half_channels, filter_channels, 1)\n\t        self.convs = DDSConv(filter_channels, kernel_size, n_layers, p_dropout=0.0)\n\t        self.proj = nn.Conv1d(\n\t            filter_channels, self.half_channels * (num_bins * 3 - 1), 1\n\t        )\n", "        self.proj.weight.data.zero_()\n\t        self.proj.bias.data.zero_()\n\t    def forward(self, x, x_mask, g=None, reverse=False):\n\t        x0, x1 = torch.split(x, [self.half_channels] * 2, 1)\n\t        h = self.pre(x0)\n\t        h = self.convs(h, x_mask, g=g)\n\t        h = self.proj(h) * x_mask\n\t        b, c, t = x0.shape\n\t        h = h.reshape(b, c, -1, t).permute(0, 1, 3, 2)  # [b, cx?, t] -> [b, c, t, ?]\n\t        unnormalized_widths = h[..., : self.num_bins] / math.sqrt(self.filter_channels)\n", "        unnormalized_heights = h[..., self.num_bins : 2 * self.num_bins] / math.sqrt(\n\t            self.filter_channels\n\t        )\n\t        unnormalized_derivatives = h[..., 2 * self.num_bins :]\n\t        x1, logabsdet = piecewise_rational_quadratic_transform(\n\t            x1,\n\t            unnormalized_widths,\n\t            unnormalized_heights,\n\t            unnormalized_derivatives,\n\t            inverse=reverse,\n", "            tails=\"linear\",\n\t            tail_bound=self.tail_bound,\n\t        )\n\t        x = torch.cat([x0, x1], 1) * x_mask\n\t        logdet = torch.sum(logabsdet * x_mask, [1, 2])\n\t        if not reverse:\n\t            return x, logdet\n\t        else:\n\t            return x\n"]}
{"filename": "lib/rvc/preprocessing/extract_f0.py", "chunked_list": ["import os\n\timport traceback\n\tfrom concurrent.futures import ProcessPoolExecutor\n\tfrom typing import *\n\timport multiprocessing as mp\n\timport numpy as np\n\timport pyworld\n\timport torch\n\timport torchcrepe\n\tfrom torch import Tensor\n", "from tqdm import tqdm\n\tfrom lib.rvc.utils import load_audio\n\tdef get_optimal_torch_device(index: int = 0) -> torch.device:\n\t    # Get cuda device\n\t    if torch.cuda.is_available():\n\t        return torch.device(f\"cuda:{index % torch.cuda.device_count()}\") # Very fast\n\t    elif torch.backends.mps.is_available():\n\t        return torch.device(\"mps\")\n\t    # Insert an else here to grab \"xla\" devices if available. TO DO later. Requires the torch_xla.core.xla_model library\n\t    # Else wise return the \"cpu\" as a torch device, \n", "    return torch.device(\"cpu\")\n\tdef get_f0_official_crepe_computation(\n\t        x,\n\t        sr,\n\t        f0_min,\n\t        f0_max,\n\t        model=\"full\",\n\t):\n\t    batch_size = 512\n\t    torch_device = get_optimal_torch_device()\n", "    audio = torch.tensor(np.copy(x))[None].float()\n\t    f0, pd = torchcrepe.predict(\n\t        audio,\n\t        sr,\n\t        160,\n\t        f0_min,\n\t        f0_max,\n\t        model,\n\t        batch_size=batch_size,\n\t        device=torch_device,\n", "        return_periodicity=True,\n\t    )\n\t    pd = torchcrepe.filter.median(pd, 3)\n\t    f0 = torchcrepe.filter.mean(f0, 3)\n\t    f0[pd < 0.1] = 0\n\t    f0 = f0[0].cpu().numpy()\n\t    f0 = f0[1:] # Get rid of extra first frame\n\t    return f0\n\tdef get_f0_crepe_computation(\n\t        x, \n", "        sr,\n\t        f0_min,\n\t        f0_max,\n\t        hop_length=160, # 512 before. Hop length changes the speed that the voice jumps to a different dramatic pitch. Lower hop lengths means more pitch accuracy but longer inference time.\n\t        model=\"full\", # Either use crepe-tiny \"tiny\" or crepe \"full\". Default is full\n\t):\n\t    x = x.astype(np.float32) # fixes the F.conv2D exception. We needed to convert double to float.\n\t    x /= np.quantile(np.abs(x), 0.999)\n\t    torch_device = get_optimal_torch_device()\n\t    audio = torch.from_numpy(x).to(torch_device, copy=True)\n", "    audio = torch.unsqueeze(audio, dim=0)\n\t    if audio.ndim == 2 and audio.shape[0] > 1:\n\t        audio = torch.mean(audio, dim=0, keepdim=True).detach()\n\t    audio = audio.detach()\n\t    print(\"Initiating prediction with a crepe_hop_length of: \" + str(hop_length))\n\t    pitch: Tensor = torchcrepe.predict(\n\t        audio,\n\t        sr,\n\t        hop_length,\n\t        f0_min,\n", "        f0_max,\n\t        model,\n\t        batch_size=hop_length * 2,\n\t        device=torch_device,\n\t        pad=True\n\t    )\n\t    p_len = x.shape[0] // hop_length\n\t    # Resize the pitch for final f0\n\t    source = np.array(pitch.squeeze(0).cpu().float().numpy())\n\t    source[source < 0.001] = np.nan\n", "    target = np.interp(\n\t        np.arange(0, len(source) * p_len, len(source)) / p_len,\n\t        np.arange(0, len(source)),\n\t        source\n\t    )\n\t    f0 = np.nan_to_num(target)\n\t    f0 = f0[1:] # Get rid of extra first frame\n\t    return f0 # Resized f0\n\tdef compute_f0(\n\t    path: str,\n", "    f0_method: str,\n\t    fs: int,\n\t    hop: int,\n\t    f0_max: float,\n\t    f0_min: float,\n\t):\n\t    x = load_audio(path, fs)\n\t    if f0_method == \"harvest\":\n\t        f0, t = pyworld.harvest(\n\t            x.astype(np.double),\n", "            fs=fs,\n\t            f0_ceil=f0_max,\n\t            f0_floor=f0_min,\n\t            frame_period=1000 * hop / fs,\n\t        )\n\t        f0 = pyworld.stonemask(x.astype(np.double), f0, t, fs)\n\t    elif f0_method == \"dio\":\n\t        f0, t = pyworld.dio(\n\t            x.astype(np.double),\n\t            fs=fs,\n", "            f0_ceil=f0_max,\n\t            f0_floor=f0_min,\n\t            frame_period=1000 * hop / fs,\n\t        )\n\t        f0 = pyworld.stonemask(x.astype(np.double), f0, t, fs)\n\t    elif f0_method == \"mangio-crepe\":\n\t        f0 = get_f0_crepe_computation(x, fs, f0_min, f0_max, 160, \"full\")\n\t    elif f0_method == \"crepe\":\n\t        f0 = get_f0_official_crepe_computation(x.astype(np.double), fs, f0_min, f0_max, \"full\")\n\t    return f0\n", "def coarse_f0(f0, f0_bin, f0_mel_min, f0_mel_max):\n\t    f0_mel = 1127 * np.log(1 + f0 / 700)\n\t    f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * (f0_bin - 2) / (\n\t        f0_mel_max - f0_mel_min\n\t    ) + 1\n\t    # use 0 or 1\n\t    f0_mel[f0_mel <= 1] = 1\n\t    f0_mel[f0_mel > f0_bin - 1] = f0_bin - 1\n\t    f0_coarse = np.rint(f0_mel).astype(np.int)\n\t    assert f0_coarse.max() <= 255 and f0_coarse.min() >= 1, (\n", "        f0_coarse.max(),\n\t        f0_coarse.min(),\n\t    )\n\t    return f0_coarse\n\tdef processor(paths, f0_method, samplerate=16000, hop_size=160, process_id=0):\n\t    fs = samplerate\n\t    hop = hop_size\n\t    f0_bin = 256\n\t    f0_max = 1100.0\n\t    f0_min = 50.0\n", "    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n\t    f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\t    if len(paths) != 0:\n\t        for idx, (inp_path, opt_path1, opt_path2) in enumerate(\n\t            tqdm(paths, position=1 + process_id)\n\t        ):\n\t            try:\n\t                if (\n\t                    os.path.exists(opt_path1 + \".npy\") == True\n\t                    and os.path.exists(opt_path2 + \".npy\") == True\n", "                ):\n\t                    continue\n\t                featur_pit = compute_f0(inp_path, f0_method, fs, hop, f0_max, f0_min)\n\t                np.save(\n\t                    opt_path2,\n\t                    featur_pit,\n\t                    allow_pickle=False,\n\t                )  # nsf\n\t                coarse_pit = coarse_f0(featur_pit, f0_bin, f0_mel_min, f0_mel_max)\n\t                np.save(\n", "                    opt_path1,\n\t                    coarse_pit,\n\t                    allow_pickle=False,\n\t                )  # ori\n\t            except:\n\t                print(f\"f0 failed {idx}: {inp_path} {traceback.format_exc()}\")\n\tdef run(training_dir: str, num_processes: int, f0_method: str):\n\t    paths = []\n\t    dataset_dir = os.path.join(training_dir, \"1_16k_wavs\")\n\t    opt_dir_f0 = os.path.join(training_dir, \"2a_f0\")\n", "    opt_dir_f0_nsf = os.path.join(training_dir, \"2b_f0nsf\")\n\t    if os.path.exists(opt_dir_f0) and os.path.exists(opt_dir_f0_nsf):\n\t        return\n\t    os.makedirs(opt_dir_f0, exist_ok=True)\n\t    os.makedirs(opt_dir_f0_nsf, exist_ok=True)\n\t    names = []\n\t    for pathname in sorted(list(os.listdir(dataset_dir))):\n\t        if os.path.isdir(os.path.join(dataset_dir, pathname)):\n\t            for f in sorted(list(os.listdir(os.path.join(dataset_dir, pathname)))):\n\t                if \"spec\" in f:\n", "                    continue\n\t                names.append(os.path.join(pathname, f))\n\t        else:\n\t            names.append(pathname)\n\t    for name in names:  # dataset_dir/{05d}/file.ext\n\t        filepath = os.path.join(dataset_dir, name)\n\t        if \"spec\" in filepath:\n\t            continue\n\t        opt_filepath_f0 = os.path.join(opt_dir_f0, name)\n\t        opt_filepath_f0_nsf = os.path.join(opt_dir_f0_nsf, name)\n", "        paths.append([filepath, opt_filepath_f0, opt_filepath_f0_nsf])\n\t    for dir in set([(os.path.dirname(p[1]), os.path.dirname(p[2])) for p in paths]):\n\t        os.makedirs(dir[0], exist_ok=True)\n\t        os.makedirs(dir[1], exist_ok=True)\n\t    with ProcessPoolExecutor(mp_context=mp.get_context(\"spawn\")) as executer:\n\t        for i in range(num_processes):\n\t            executer.submit(processor, paths[i::num_processes], f0_method, process_id=i)\n\t    processor(paths, f0_method)\n"]}
{"filename": "lib/rvc/preprocessing/extract_feature.py", "chunked_list": ["import multiprocessing as mp\n\timport os\n\timport traceback\n\tfrom concurrent.futures import ProcessPoolExecutor\n\tfrom typing import *\n\timport numpy as np\n\timport soundfile as sf\n\timport torch\n\timport torch.nn.functional as F\n\tfrom fairseq import checkpoint_utils\n", "from tqdm import tqdm\n\tROOT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n\tMODELS_DIR = os.path.join(ROOT_DIR, \"models\")\n\tEMBEDDINGS_LIST = {\n\t    \"hubert-base-japanese\": (\n\t        \"rinna_hubert_base_jp.pt\",\n\t        \"hubert-base-japanese\",\n\t        \"local\",\n\t    ),\n\t    \"contentvec\": (\"checkpoint_best_legacy_500.pt\", \"contentvec\", \"local\"),\n", "}\n\tdef get_embedder(embedder_name):\n\t    if embedder_name in EMBEDDINGS_LIST:\n\t        return EMBEDDINGS_LIST[embedder_name]\n\t    return None\n\tdef load_embedder(embedder_path: str, device):\n\t    try:\n\t        models, cfg, _ = checkpoint_utils.load_model_ensemble_and_task(\n\t            [embedder_path],\n\t            suffix=\"\",\n", "        )\n\t        embedder_model = models[0]\n\t        embedder_model = embedder_model.to(device)\n\t        if device != \"cpu\":\n\t            embedder_model = embedder_model.half()\n\t        else:\n\t            embedder_model = embedder_model.float()\n\t        embedder_model.eval()\n\t    except Exception as e:\n\t        print(f\"Error: {e} {embedder_path}\")\n", "        traceback.print_exc()\n\t    return embedder_model, cfg\n\t# wave must be 16k, hop_size=320\n\tdef readwave(wav_path, normalize=False):\n\t    wav, sr = sf.read(wav_path)\n\t    assert sr == 16000\n\t    feats = torch.from_numpy(wav).float()\n\t    if feats.dim() == 2:  # double channels\n\t        feats = feats.mean(-1)\n\t    assert feats.dim() == 1, feats.dim()\n", "    if normalize:\n\t        with torch.no_grad():\n\t            feats = F.layer_norm(feats, feats.shape)\n\t    feats = feats.view(1, -1)\n\t    return feats\n\tdef processor(\n\t    todo: List[str],\n\t    device: torch.device,\n\t    embedder_path: str,\n\t    embedder_load_from: str,\n", "    embedding_channel: bool,\n\t    embedding_output_layer: int,\n\t    wav_dir: str,\n\t    out_dir: str,\n\t    process_id: int,\n\t):\n\t    half_support = (\n\t        device.type == \"cuda\" and torch.cuda.get_device_capability(device)[0] >= 5.3\n\t    )\n\t    is_feats_dim_768 = embedding_channel == 768\n", "    if embedder_load_from == \"local\" and not os.path.exists(embedder_path):\n\t        return f\"Embedder not found: {embedder_path}\"\n\t    model, cfg = load_embedder(embedder_path, device)\n\t    for file in tqdm(todo, position=1 + process_id):\n\t        try:\n\t            if file.endswith(\".wav\"):\n\t                wav_filepath = os.path.join(wav_dir, file)\n\t                out_filepath = os.path.join(out_dir, file.replace(\"wav\", \"npy\"))\n\t                if os.path.exists(out_filepath):\n\t                    continue\n", "                os.makedirs(os.path.dirname(out_filepath), exist_ok=True)\n\t                is_normalize = False if cfg is None else cfg.task.normalize\n\t                feats = readwave(wav_filepath, normalize=is_normalize)\n\t                padding_mask = torch.BoolTensor(feats.shape).fill_(False)\n\t                if isinstance(model, tuple):\n\t                    feats = model[0](\n\t                        feats.squeeze(0).squeeze(0).to(device),\n\t                        return_tensors=\"pt\",\n\t                        sampling_rate=16000,\n\t                    )\n", "                    if half_support:\n\t                        feats = feats.input_values.to(device).half()\n\t                    else:\n\t                        feats = feats.input_values.to(device).float()\n\t                    with torch.no_grad():\n\t                        if half_support:\n\t                            if is_feats_dim_768:\n\t                                feats = model[1](feats).last_hidden_state\n\t                            else:\n\t                                feats = model[1](feats).extract_features\n", "                        else:\n\t                            if is_feats_dim_768:\n\t                                feats = model[1].float()(feats).last_hidden_state\n\t                            else:\n\t                                feats = model[1].float()(feats).extract_features\n\t                else:\n\t                    inputs = {\n\t                        \"source\": feats.half().to(device)\n\t                        if half_support\n\t                        else feats.to(device),\n", "                        \"padding_mask\": padding_mask.to(device),\n\t                        \"output_layer\": embedding_output_layer,\n\t                    }\n\t                    # なんかまだこの時点でfloat16なので改めて変換\n\t                    if not half_support:\n\t                        model = model.float()\n\t                        inputs[\"source\"] = inputs[\"source\"].float()\n\t                    with torch.no_grad():\n\t                        logits = model.extract_features(**inputs)\n\t                        if is_feats_dim_768:\n", "                            feats = logits[0]\n\t                        else:\n\t                            feats = model.final_proj(logits[0])\n\t                feats = feats.squeeze(0).float().cpu().numpy()\n\t                if np.isnan(feats).sum() == 0:\n\t                    np.save(out_filepath, feats, allow_pickle=False)\n\t                else:\n\t                    print(f\"{file} contains nan\")\n\t        except Exception as e:\n\t            print(f\"Error: {e} {file}\")\n", "            traceback.print_exc()\n\tdef run(\n\t    training_dir: str,\n\t    embedder_path: str,\n\t    embedder_load_from: str,\n\t    embedding_channel: int,\n\t    embedding_output_layer: int,\n\t    gpu_ids: List[int],\n\t    device: Optional[Union[torch.device, str]] = None,\n\t):\n", "    wav_dir = os.path.join(training_dir, \"1_16k_wavs\")\n\t    out_dir = os.path.join(training_dir, \"3_feature256\")\n\t    num_gpus = len(gpu_ids)\n\t    for gpu_id in gpu_ids:\n\t        if num_gpus < gpu_id + 1:\n\t            print(f\"GPU {gpu_id} is not available\")\n\t            return\n\t    if os.path.exists(out_dir):\n\t        return\n\t    os.makedirs(out_dir, exist_ok=True)\n", "    todo = [\n\t        os.path.join(dir, f)\n\t        for dir in sorted(list(os.listdir(wav_dir)))\n\t        if os.path.isdir(os.path.join(wav_dir, dir))\n\t        for f in sorted(list(os.listdir(os.path.join(wav_dir, dir))))\n\t    ]\n\t    if device is not None:\n\t        if type(device) == str:\n\t            device = torch.device(device)\n\t        if device.type == \"mps\":\n", "            device = torch.device(\n\t                \"cpu\"\n\t            )  # Mac(MPS) crashes when multiprocess, so change to CPU.\n\t        processor(\n\t            todo,\n\t            device,\n\t            embedder_path,\n\t            embedder_load_from,\n\t            embedding_channel,\n\t            embedding_output_layer,\n", "            wav_dir,\n\t            out_dir,\n\t            process_id=0,\n\t        )\n\t    else:\n\t        with ProcessPoolExecutor(mp_context=mp.get_context(\"spawn\")) as executor:\n\t            for i, id in enumerate(gpu_ids):\n\t                executor.submit(\n\t                    processor,\n\t                    todo[i::num_gpus],\n", "                    torch.device(f\"cuda:{id}\"),\n\t                    embedder_path,\n\t                    embedder_load_from,\n\t                    embedding_channel,\n\t                    embedding_output_layer,\n\t                    wav_dir,\n\t                    out_dir,\n\t                    process_id=i,\n\t                )\n"]}
{"filename": "lib/rvc/preprocessing/split.py", "chunked_list": ["import operator\n\timport os\n\tfrom concurrent.futures import ProcessPoolExecutor\n\tfrom typing import *\n\timport librosa\n\timport numpy as np\n\timport scipy.signal as signal\n\tfrom scipy.io import wavfile\n\tfrom tqdm import tqdm\n\tfrom lib.rvc.utils import load_audio\n", "from .slicer import Slicer\n\tdef norm_write(\n\t    tmp_audio: np.ndarray,\n\t    idx0: int,\n\t    idx1: int,\n\t    speaker_id: int,\n\t    outdir: str,\n\t    outdir_16k: str,\n\t    sampling_rate: int,\n\t    max: float,\n", "    alpha: float,\n\t    is_normalize: bool,\n\t):\n\t    if is_normalize:\n\t        tmp_audio = (tmp_audio / np.abs(tmp_audio).max() * (max * alpha)) + (\n\t            1 - alpha\n\t        ) * tmp_audio\n\t    else:\n\t        # clip level to max (cause sometimes when floating point decoding)\n\t        audio_min = np.min(tmp_audio)\n", "        if audio_min < -max:\n\t            tmp_audio = tmp_audio / -audio_min * max\n\t        audio_max = np.max(tmp_audio)\n\t        if audio_max > max:\n\t            tmp_audio = tmp_audio / audio_max * max\n\t    wavfile.write(\n\t        os.path.join(outdir, f\"{speaker_id:05}\", f\"{idx0}_{idx1}.wav\"),\n\t        sampling_rate,\n\t        tmp_audio.astype(np.float32),\n\t    )\n", "    tmp_audio = librosa.resample(\n\t        tmp_audio, orig_sr=sampling_rate, target_sr=16000, res_type=\"soxr_vhq\"\n\t    )\n\t    wavfile.write(\n\t        os.path.join(outdir_16k, f\"{speaker_id:05}\", f\"{idx0}_{idx1}.wav\"),\n\t        16000,\n\t        tmp_audio.astype(np.float32),\n\t    )\n\tdef write_mute(\n\t    mute_wave_filename: str,\n", "    speaker_id: int,\n\t    outdir: str,\n\t    outdir_16k: str,\n\t    sampling_rate: int,\n\t):\n\t    tmp_audio = load_audio(mute_wave_filename, sampling_rate)\n\t    wavfile.write(\n\t        os.path.join(outdir, f\"{speaker_id:05}\", \"mute.wav\"),\n\t        sampling_rate,\n\t        tmp_audio.astype(np.float32),\n", "    )\n\t    tmp_audio = librosa.resample(\n\t        tmp_audio, orig_sr=sampling_rate, target_sr=16000, res_type=\"soxr_vhq\"\n\t    )\n\t    wavfile.write(\n\t        os.path.join(outdir_16k, f\"{speaker_id:05}\", \"mute.wav\"),\n\t        16000,\n\t        tmp_audio.astype(np.float32),\n\t    )\n\tdef pipeline(\n", "    slicer: Slicer,\n\t    datasets: List[Tuple[str, int]],  # List[(path, speaker_id)]\n\t    outdir: str,\n\t    outdir_16k: str,\n\t    sampling_rate: int,\n\t    is_normalize: bool,\n\t    process_id: int = 0,\n\t):\n\t    per = 3.7\n\t    overlap = 0.3\n", "    tail = per + overlap\n\t    max = 0.95\n\t    alpha = 0.8\n\t    bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=sampling_rate)\n\t    for index, (wave_filename, speaker_id) in tqdm(datasets, position=1 + process_id):\n\t        audio = load_audio(wave_filename, sampling_rate)\n\t        audio = signal.lfilter(bh, ah, audio)\n\t        idx1 = 0\n\t        for audio in slicer.slice(audio):\n\t            i = 0\n", "            while 1:\n\t                start = int(sampling_rate * (per - overlap) * i)\n\t                i += 1\n\t                if len(audio[start:]) > tail * sampling_rate:\n\t                    tmp_audio = audio[start : start + int(per * sampling_rate)]\n\t                    norm_write(\n\t                        tmp_audio,\n\t                        index,\n\t                        idx1,\n\t                        speaker_id,\n", "                        outdir,\n\t                        outdir_16k,\n\t                        sampling_rate,\n\t                        max,\n\t                        alpha,\n\t                        is_normalize,\n\t                    )\n\t                    idx1 += 1\n\t                else:\n\t                    tmp_audio = audio[start:]\n", "                    break\n\t            norm_write(\n\t                tmp_audio,\n\t                index,\n\t                idx1,\n\t                speaker_id,\n\t                outdir,\n\t                outdir_16k,\n\t                sampling_rate,\n\t                max,\n", "                alpha,\n\t                is_normalize,\n\t            )\n\t            idx1 += 1\n\tdef preprocess_audio(\n\t    datasets: List[Tuple[str, int]],  # List[(path, speaker_id)]\n\t    sampling_rate: int,\n\t    num_processes: int,\n\t    training_dir: str,\n\t    is_normalize: bool,\n", "    mute_wav_path: str,\n\t):\n\t    waves_dir = os.path.join(training_dir, \"0_gt_wavs\")\n\t    waves16k_dir = os.path.join(training_dir, \"1_16k_wavs\")\n\t    if os.path.exists(waves_dir) and os.path.exists(waves16k_dir):\n\t        return\n\t    for speaker_id in set([spk for _, spk in datasets]):\n\t        os.makedirs(os.path.join(waves_dir, f\"{speaker_id:05}\"), exist_ok=True)\n\t        os.makedirs(os.path.join(waves16k_dir, f\"{speaker_id:05}\"), exist_ok=True)\n\t    all = [(i, x) for i, x in enumerate(sorted(datasets, key=operator.itemgetter(0)))]\n", "    # n of datasets per process\n\t    process_all_nums = [len(all) // num_processes] * num_processes\n\t    # add residual datasets\n\t    for i in range(len(all) % num_processes):\n\t        process_all_nums[i] += 1\n\t    assert len(all) == sum(process_all_nums), print(\n\t        f\"len(all): {len(all)}, sum(process_all_nums): {sum(process_all_nums)}\"\n\t    )\n\t    with ProcessPoolExecutor(max_workers=num_processes) as executor:\n\t        all_index = 0\n", "        for i in range(num_processes):\n\t            data = all[all_index : all_index + process_all_nums[i]]\n\t            slicer = Slicer(\n\t                sr=sampling_rate,\n\t                threshold=-42,\n\t                min_length=1500,\n\t                min_interval=400,\n\t                hop_size=15,\n\t                max_sil_kept=500,\n\t            )\n", "            executor.submit(\n\t                pipeline,\n\t                slicer,\n\t                data,\n\t                waves_dir,\n\t                waves16k_dir,\n\t                sampling_rate,\n\t                is_normalize,\n\t                process_id=i,\n\t            )\n", "            all_index += process_all_nums[i]\n\t    for speaker_id in set([spk for _, spk in datasets]):\n\t        write_mute(mute_wav_path, speaker_id, waves_dir, waves16k_dir, sampling_rate)\n"]}
{"filename": "lib/rvc/preprocessing/slicer.py", "chunked_list": ["import numpy as np\n\t# This function is obtained from librosa.\n\tdef get_rms(\n\t    y,\n\t    frame_length=2048,\n\t    hop_length=512,\n\t    pad_mode=\"constant\",\n\t):\n\t    padding = (int(frame_length // 2), int(frame_length // 2))\n\t    y = np.pad(y, padding, mode=pad_mode)\n", "    axis = -1\n\t    # put our new within-frame axis at the end for now\n\t    out_strides = y.strides + tuple([y.strides[axis]])\n\t    # Reduce the shape on the framing axis\n\t    x_shape_trimmed = list(y.shape)\n\t    x_shape_trimmed[axis] -= frame_length - 1\n\t    out_shape = tuple(x_shape_trimmed) + tuple([frame_length])\n\t    xw = np.lib.stride_tricks.as_strided(y, shape=out_shape, strides=out_strides)\n\t    if axis < 0:\n\t        target_axis = axis - 1\n", "    else:\n\t        target_axis = axis + 1\n\t    xw = np.moveaxis(xw, -1, target_axis)\n\t    # Downsample along the target axis\n\t    slices = [slice(None)] * xw.ndim\n\t    slices[axis] = slice(0, None, hop_length)\n\t    x = xw[tuple(slices)]\n\t    # Calculate power\n\t    power = np.mean(np.abs(x) ** 2, axis=-2, keepdims=True)\n\t    return np.sqrt(power)\n", "class Slicer:\n\t    def __init__(\n\t        self,\n\t        sr: int,\n\t        threshold: float = -40.0,\n\t        min_length: int = 5000,\n\t        min_interval: int = 300,\n\t        hop_size: int = 20,\n\t        max_sil_kept: int = 5000,\n\t    ):\n", "        if not min_length >= min_interval >= hop_size:\n\t            raise ValueError(\n\t                \"The following condition must be satisfied: min_length >= min_interval >= hop_size\"\n\t            )\n\t        if not max_sil_kept >= hop_size:\n\t            raise ValueError(\n\t                \"The following condition must be satisfied: max_sil_kept >= hop_size\"\n\t            )\n\t        min_interval = sr * min_interval / 1000\n\t        self.threshold = 10 ** (threshold / 20.0)\n", "        self.hop_size = round(sr * hop_size / 1000)\n\t        self.win_size = min(round(min_interval), 4 * self.hop_size)\n\t        self.min_length = round(sr * min_length / 1000 / self.hop_size)\n\t        self.min_interval = round(min_interval / self.hop_size)\n\t        self.max_sil_kept = round(sr * max_sil_kept / 1000 / self.hop_size)\n\t    def _apply_slice(self, waveform, begin, end):\n\t        if len(waveform.shape) > 1:\n\t            return waveform[\n\t                :, begin * self.hop_size : min(waveform.shape[1], end * self.hop_size)\n\t            ]\n", "        else:\n\t            return waveform[\n\t                begin * self.hop_size : min(waveform.shape[0], end * self.hop_size)\n\t            ]\n\t    # @timeit\n\t    def slice(self, waveform):\n\t        if len(waveform.shape) > 1:\n\t            samples = waveform.mean(axis=0)\n\t        else:\n\t            samples = waveform\n", "        if samples.shape[0] <= self.min_length:\n\t            return [waveform]\n\t        rms_list = get_rms(\n\t            y=samples, frame_length=self.win_size, hop_length=self.hop_size\n\t        ).squeeze(0)\n\t        sil_tags = []\n\t        silence_start = None\n\t        clip_start = 0\n\t        for i, rms in enumerate(rms_list):\n\t            # Keep looping while frame is silent.\n", "            if rms < self.threshold:\n\t                # Record start of silent frames.\n\t                if silence_start is None:\n\t                    silence_start = i\n\t                continue\n\t            # Keep looping while frame is not silent and silence start has not been recorded.\n\t            if silence_start is None:\n\t                continue\n\t            # Clear recorded silence start if interval is not enough or clip is too short\n\t            is_leading_silence = silence_start == 0 and i > self.max_sil_kept\n", "            need_slice_middle = (\n\t                i - silence_start >= self.min_interval\n\t                and i - clip_start >= self.min_length\n\t            )\n\t            if not is_leading_silence and not need_slice_middle:\n\t                silence_start = None\n\t                continue\n\t            # Need slicing. Record the range of silent frames to be removed.\n\t            if i - silence_start <= self.max_sil_kept:\n\t                pos = rms_list[silence_start : i + 1].argmin() + silence_start\n", "                if silence_start == 0:\n\t                    sil_tags.append((0, pos))\n\t                else:\n\t                    sil_tags.append((pos, pos))\n\t                clip_start = pos\n\t            elif i - silence_start <= self.max_sil_kept * 2:\n\t                pos = rms_list[\n\t                    i - self.max_sil_kept : silence_start + self.max_sil_kept + 1\n\t                ].argmin()\n\t                pos += i - self.max_sil_kept\n", "                pos_l = (\n\t                    rms_list[\n\t                        silence_start : silence_start + self.max_sil_kept + 1\n\t                    ].argmin()\n\t                    + silence_start\n\t                )\n\t                pos_r = (\n\t                    rms_list[i - self.max_sil_kept : i + 1].argmin()\n\t                    + i\n\t                    - self.max_sil_kept\n", "                )\n\t                if silence_start == 0:\n\t                    sil_tags.append((0, pos_r))\n\t                    clip_start = pos_r\n\t                else:\n\t                    sil_tags.append((min(pos_l, pos), max(pos_r, pos)))\n\t                    clip_start = max(pos_r, pos)\n\t            else:\n\t                pos_l = (\n\t                    rms_list[\n", "                        silence_start : silence_start + self.max_sil_kept + 1\n\t                    ].argmin()\n\t                    + silence_start\n\t                )\n\t                pos_r = (\n\t                    rms_list[i - self.max_sil_kept : i + 1].argmin()\n\t                    + i\n\t                    - self.max_sil_kept\n\t                )\n\t                if silence_start == 0:\n", "                    sil_tags.append((0, pos_r))\n\t                else:\n\t                    sil_tags.append((pos_l, pos_r))\n\t                clip_start = pos_r\n\t            silence_start = None\n\t        # Deal with trailing silence.\n\t        total_frames = rms_list.shape[0]\n\t        if (\n\t            silence_start is not None\n\t            and total_frames - silence_start >= self.min_interval\n", "        ):\n\t            silence_end = min(total_frames, silence_start + self.max_sil_kept)\n\t            pos = rms_list[silence_start : silence_end + 1].argmin() + silence_start\n\t            sil_tags.append((pos, total_frames + 1))\n\t        # Apply and return slices.\n\t        if len(sil_tags) == 0:\n\t            return [waveform]\n\t        else:\n\t            chunks = []\n\t            if sil_tags[0][0] > 0:\n", "                chunks.append(self._apply_slice(waveform, 0, sil_tags[0][0]))\n\t            for i in range(len(sil_tags) - 1):\n\t                chunks.append(\n\t                    self._apply_slice(waveform, sil_tags[i][1], sil_tags[i + 1][0])\n\t                )\n\t            if sil_tags[-1][1] < total_frames:\n\t                chunks.append(\n\t                    self._apply_slice(waveform, sil_tags[-1][1], total_frames)\n\t                )\n\t            return chunks\n"]}
{"filename": "modules/merge.py", "chunked_list": ["from collections import OrderedDict\n\tfrom typing import *\n\timport torch\n\timport tqdm\n\tdef merge(\n\t    path_a: str,\n\t    path_b: str,\n\t    path_c: str,\n\t    alpha: float,\n\t    weights: Dict[str, float],\n", "    method: str,\n\t):\n\t    def extract(ckpt: Dict[str, Any]):\n\t        a = ckpt[\"model\"]\n\t        opt = OrderedDict()\n\t        opt[\"weight\"] = {}\n\t        for key in a.keys():\n\t            if \"enc_q\" in key:\n\t                continue\n\t            opt[\"weight\"][key] = a[key]\n", "        return opt\n\t    def load_weight(path: str):\n\t        print(f\"Loading {path}...\")\n\t        state_dict = torch.load(path, map_location=\"cpu\")\n\t        if \"model\" in state_dict:\n\t            weight = extract(state_dict)\n\t        else:\n\t            weight = state_dict[\"weight\"]\n\t        return weight, state_dict\n\t    def get_alpha(key: str):\n", "        try:\n\t            filtered = sorted(\n\t                [x for x in weights.keys() if key.startswith(x)], key=len, reverse=True\n\t            )\n\t            if len(filtered) < 1:\n\t                return alpha\n\t            return weights[filtered[0]]\n\t        except:\n\t            return alpha\n\t    weight_a, state_dict = load_weight(path_a)\n", "    weight_b, _ = load_weight(path_b)\n\t    if path_c is not None:\n\t        weight_c, _ = load_weight(path_c)\n\t    if sorted(list(weight_a.keys())) != sorted(list(weight_b.keys())):\n\t        raise RuntimeError(\"Failed to merge models.\")\n\t    merged = OrderedDict()\n\t    merged[\"weight\"] = {}\n\t    def merge_weight(a, b, c, alpha):\n\t        if method == \"weight_sum\":\n\t            return (1 - alpha) * a + alpha * b\n", "        elif method == \"add_diff\":\n\t            return a + (b - c) * alpha\n\t    for key in tqdm.tqdm(weight_a.keys()):\n\t        a = get_alpha(key)\n\t        if path_c is not None:\n\t            merged[\"weight\"][key] = merge_weight(\n\t                weight_a[key], weight_b[key], weight_c[key], a\n\t            )\n\t        else:\n\t            merged[\"weight\"][key] = merge_weight(weight_a[key], weight_b[key], None, a)\n", "    merged[\"config\"] = state_dict[\"config\"]\n\t    merged[\"params\"] = state_dict[\"params\"] if \"params\" in state_dict else None\n\t    merged[\"version\"] = state_dict.get(\"version\", \"v1\")\n\t    merged[\"sr\"] = state_dict[\"sr\"]\n\t    merged[\"f0\"] = state_dict[\"f0\"]\n\t    merged[\"info\"] = state_dict[\"info\"]\n\t    merged[\"embedder_name\"] = (\n\t        state_dict[\"embedder_name\"] if \"embedder_name\" in state_dict else None\n\t    )\n\t    merged[\"embedder_output_layer\"] = state_dict.get(\"embedder_output_layer\", \"12\")\n", "    return merged\n"]}
{"filename": "modules/cmd_opts.py", "chunked_list": ["import argparse\n\tparser = argparse.ArgumentParser()\n\tparser.add_argument(\"--host\", help=\"Host to connect to\", type=str, default=\"127.0.0.1\")\n\tparser.add_argument(\"--port\", help=\"Port to connect to\", type=int)\n\tparser.add_argument(\"--share\", help=\"Enable gradio share\", action=\"store_true\")\n\tparser.add_argument(\n\t    \"--models-dir\", help=\"Path to models directory\", type=str, default=None\n\t)\n\tparser.add_argument(\n\t    \"--output-dir\", help=\"Path to output directory\", type=str, default=None\n", ")\n\tparser.add_argument(\n\t    \"--precision\",\n\t    help=\"Precision to use\",\n\t    type=str,\n\t    default=\"fp16\",\n\t    choices=[\"fp32\", \"fp16\"],\n\t)\n\topts, _ = parser.parse_known_args()\n"]}
{"filename": "modules/separate.py", "chunked_list": ["import os\n\tfrom typing import *\n\timport tqdm\n\tfrom pydub import AudioSegment\n\tfrom pydub.silence import split_on_silence\n\tdef separate_audio(\n\t    input: str,\n\t    output: str,\n\t    silence_thresh: int,\n\t    min_silence_len: int = 1000,\n", "    keep_silence: int = 100,\n\t    margin: int = 0,\n\t    padding: bool = False,\n\t    min: Optional[int] = None,\n\t    max: Optional[int] = None,\n\t):\n\t    if os.path.isfile(input):\n\t        input = [input]\n\t    elif os.path.isdir(input):\n\t        input = [os.path.join(input, f) for f in os.listdir(input)]\n", "    else:\n\t        raise ValueError(\"input must be a file or directory\")\n\t    os.makedirs(output, exist_ok=True)\n\t    for file in input:\n\t        if os.path.splitext(file)[1] == \".mp3\":\n\t            audio = AudioSegment.from_mp3(file)\n\t        elif os.path.splitext(file)[1] == \".wav\":\n\t            audio = AudioSegment.from_wav(file)\n\t        elif os.path.splitext(file)[1] == \".flac\":\n\t            audio = AudioSegment.from_file(file, \"flac\")\n", "        else:\n\t            raise ValueError(\n\t                \"Invalid file format. Only MP3 and WAV files are supported.\"\n\t            )\n\t        chunks = split_on_silence(\n\t            audio,\n\t            min_silence_len=min_silence_len,\n\t            silence_thresh=silence_thresh,\n\t            keep_silence=keep_silence,\n\t        )\n", "        output_chunks: List[AudioSegment] = []\n\t        so_short = None\n\t        for chunk in tqdm.tqdm(chunks):\n\t            if so_short is not None:\n\t                chunk = so_short + chunk\n\t                so_short = None\n\t            if min is None or len(chunk) > min:\n\t                if max is not None and len(chunk) > max:\n\t                    sub_chunks = [\n\t                        chunk[i : i + max + margin]\n", "                        for i in range(0, len(chunk) - margin, max)\n\t                    ]\n\t                    if len(sub_chunks[-1]) < min:\n\t                        if padding and len(sub_chunks) > 2:\n\t                            output_chunks.extend(sub_chunks[0:-2])\n\t                            output_chunks.append(sub_chunks[-2] + sub_chunks[-1])\n\t                        else:\n\t                            output_chunks.extend(sub_chunks[0:-1])\n\t                    else:\n\t                        output_chunks.extend(sub_chunks)\n", "                else:\n\t                    output_chunks.append(chunk)\n\t            else:\n\t                if so_short is None:\n\t                    so_short = chunk\n\t                else:\n\t                    so_short += chunk\n\t        basename = os.path.splitext(os.path.basename(file))[0]\n\t        for i, chunk in enumerate(output_chunks):\n\t            filepath = os.path.join(output, f\"{basename}_{i}.wav\")\n", "            chunk.export(filepath, format=\"wav\")\n"]}
{"filename": "modules/models.py", "chunked_list": ["import os\n\timport re\n\tfrom typing import *\n\timport torch\n\tfrom fairseq import checkpoint_utils\n\tfrom fairseq.models.hubert.hubert import HubertModel\n\tfrom pydub import AudioSegment\n\tfrom lib.rvc.models import (SynthesizerTrnMs256NSFSid,\n\t                            SynthesizerTrnMs256NSFSidNono)\n\tfrom lib.rvc.pipeline import VocalConvertPipeline\n", "from .cmd_opts import opts\n\tfrom .shared import ROOT_DIR, device, is_half\n\tfrom .utils import load_audio\n\tAUDIO_OUT_DIR = opts.output_dir or os.path.join(ROOT_DIR, \"outputs\")\n\tEMBEDDINGS_LIST = {\n\t    \"hubert-base-japanese\": (\n\t        \"rinna_hubert_base_jp.pt\",\n\t        \"hubert-base-japanese\",\n\t        \"local\",\n\t    ),\n", "    \"contentvec\": (\"checkpoint_best_legacy_500.pt\", \"contentvec\", \"local\"),\n\t}\n\tdef update_state_dict(state_dict):\n\t    if \"params\" in state_dict and state_dict[\"params\"] is not None:\n\t        return\n\t    keys = [\n\t        \"spec_channels\",\n\t        \"segment_size\",\n\t        \"inter_channels\",\n\t        \"hidden_channels\",\n", "        \"filter_channels\",\n\t        \"n_heads\",\n\t        \"n_layers\",\n\t        \"kernel_size\",\n\t        \"p_dropout\",\n\t        \"resblock\",\n\t        \"resblock_kernel_sizes\",\n\t        \"resblock_dilation_sizes\",\n\t        \"upsample_rates\",\n\t        \"upsample_initial_channel\",\n", "        \"upsample_kernel_sizes\",\n\t        \"spk_embed_dim\",\n\t        \"gin_channels\",\n\t        \"emb_channels\",\n\t        \"sr\",\n\t    ]\n\t    state_dict[\"params\"] = {}\n\t    n = 0\n\t    for i, key in enumerate(keys):\n\t        i = i - n\n", "        if len(state_dict[\"config\"]) != 19 and key == \"emb_channels\":\n\t            # backward compat.\n\t            n += 1\n\t            continue\n\t        state_dict[\"params\"][key] = state_dict[\"config\"][i]\n\t    if not \"emb_channels\" in state_dict[\"params\"]:\n\t        if state_dict.get(\"version\", \"v1\") == \"v1\":\n\t            state_dict[\"params\"][\"emb_channels\"] = 256  # for backward compat.\n\t            state_dict[\"embedder_output_layer\"] = 9\n\t        else:\n", "            state_dict[\"params\"][\"emb_channels\"] = 768  # for backward compat.\n\t            state_dict[\"embedder_output_layer\"] = 12\n\tclass VoiceConvertModel:\n\t    def __init__(self, model_name: str, state_dict: Dict[str, Any]) -> None:\n\t        update_state_dict(state_dict)\n\t        self.model_name = model_name\n\t        self.state_dict = state_dict\n\t        self.tgt_sr = state_dict[\"params\"][\"sr\"]\n\t        f0 = state_dict.get(\"f0\", 1)\n\t        state_dict[\"params\"][\"spk_embed_dim\"] = state_dict[\"weight\"][\n", "            \"emb_g.weight\"\n\t        ].shape[0]\n\t        if not \"emb_channels\" in state_dict[\"params\"]:\n\t            state_dict[\"params\"][\"emb_channels\"] = 256  # for backward compat.\n\t        if f0 == 1:\n\t            self.net_g = SynthesizerTrnMs256NSFSid(\n\t                **state_dict[\"params\"], is_half=is_half\n\t            )\n\t        else:\n\t            self.net_g = SynthesizerTrnMs256NSFSidNono(**state_dict[\"params\"])\n", "        del self.net_g.enc_q\n\t        self.net_g.load_state_dict(state_dict[\"weight\"], strict=False)\n\t        self.net_g.eval().to(device)\n\t        if is_half:\n\t            self.net_g = self.net_g.half()\n\t        else:\n\t            self.net_g = self.net_g.float()\n\t        self.vc = VocalConvertPipeline(self.tgt_sr, device, is_half)\n\t        self.n_spk = state_dict[\"params\"][\"spk_embed_dim\"]\n\t    def single(\n", "        self,\n\t        sid: int,\n\t        input_audio: str,\n\t        embedder_model_name: str,\n\t        embedding_output_layer: str,\n\t        f0_up_key: int,\n\t        f0_file: str,\n\t        f0_method: str,\n\t        auto_load_index: bool,\n\t        faiss_index_file: str,\n", "        index_rate: float,\n\t        output_dir: str = AUDIO_OUT_DIR,\n\t    ):\n\t        if not input_audio:\n\t            raise Exception(\"You need to set Source Audio\")\n\t        f0_up_key = int(f0_up_key)\n\t        audio = load_audio(input_audio, 16000)\n\t        if embedder_model_name == \"auto\":\n\t            embedder_model_name = (\n\t                self.state_dict[\"embedder_name\"]\n", "                if \"embedder_name\" in self.state_dict\n\t                else \"hubert_base\"\n\t            )\n\t            if embedder_model_name.endswith(\"768\"):\n\t                embedder_model_name = embedder_model_name[:-3]\n\t        if embedder_model_name == \"hubert_base\":\n\t            embedder_model_name = \"contentvec\"\n\t        if not embedder_model_name in EMBEDDINGS_LIST.keys():\n\t            raise Exception(f\"Not supported embedder: {embedder_model_name}\")\n\t        if (\n", "            embedder_model == None\n\t            or loaded_embedder_model != EMBEDDINGS_LIST[embedder_model_name][1]\n\t        ):\n\t            print(f\"load {embedder_model_name} embedder\")\n\t            embedder_filename, embedder_name, load_from = get_embedder(\n\t                embedder_model_name\n\t            )\n\t            load_embedder(embedder_filename, embedder_name)\n\t        if embedding_output_layer == \"auto\":\n\t            embedding_output_layer = (\n", "                self.state_dict[\"embedding_output_layer\"]\n\t                if \"embedding_output_layer\" in self.state_dict\n\t                else 12\n\t            )\n\t        else:\n\t            embedding_output_layer = int(embedding_output_layer)\n\t        f0 = self.state_dict.get(\"f0\", 1)\n\t        if not faiss_index_file and auto_load_index:\n\t            faiss_index_file = self.get_index_path(sid)\n\t        audio_opt = self.vc(\n", "            embedder_model,\n\t            embedding_output_layer,\n\t            self.net_g,\n\t            sid,\n\t            audio,\n\t            f0_up_key,\n\t            f0_method,\n\t            faiss_index_file,\n\t            index_rate,\n\t            f0,\n", "            f0_file=f0_file,\n\t        )\n\t        audio = AudioSegment(\n\t            audio_opt,\n\t            frame_rate=self.tgt_sr,\n\t            sample_width=2,\n\t            channels=1,\n\t        )\n\t        os.makedirs(output_dir, exist_ok=True)\n\t        input_audio_splitext = os.path.splitext(os.path.basename(input_audio))[0]\n", "        model_splitext = os.path.splitext(self.model_name)[0]\n\t        index = 0\n\t        existing_files = os.listdir(output_dir)\n\t        for existing_file in existing_files:\n\t            result = re.match(r\"\\d+\", existing_file)\n\t            if result:\n\t                prefix_num = int(result.group(0))\n\t                if index < prefix_num:\n\t                    index = prefix_num\n\t        audio.export(\n", "            os.path.join(\n\t                output_dir, f\"{index+1}-{model_splitext}-{input_audio_splitext}.wav\"\n\t            ),\n\t            format=\"wav\",\n\t        )\n\t        return audio_opt\n\t    def get_index_path(self, speaker_id: int):\n\t        basename = os.path.splitext(self.model_name)[0]\n\t        speaker_index_path = os.path.join(\n\t            MODELS_DIR,\n", "            \"checkpoints\",\n\t            f\"{basename}_index\",\n\t            f\"{basename}.{speaker_id}.index\",\n\t        )\n\t        if os.path.exists(speaker_index_path):\n\t            return speaker_index_path\n\t        return os.path.join(MODELS_DIR, \"checkpoints\", f\"{basename}.index\")\n\tMODELS_DIR = opts.models_dir or os.path.join(ROOT_DIR, \"models\")\n\tvc_model: Optional[VoiceConvertModel] = None\n\tembedder_model: Optional[HubertModel] = None\n", "loaded_embedder_model = \"\"\n\tdef get_models():\n\t    dir = os.path.join(ROOT_DIR, \"models\", \"checkpoints\")\n\t    os.makedirs(dir, exist_ok=True)\n\t    return [\n\t        file\n\t        for file in os.listdir(dir)\n\t        if any([x for x in [\".ckpt\", \".pth\"] if file.endswith(x)])\n\t    ]\n\tdef get_embedder(embedder_name):\n", "    if embedder_name in EMBEDDINGS_LIST:\n\t        return EMBEDDINGS_LIST[embedder_name]\n\t    return None\n\tdef load_embedder(emb_file: str, emb_name: str):\n\t    global embedder_model, loaded_embedder_model\n\t    emb_file = os.path.join(MODELS_DIR, \"embeddings\", emb_file)\n\t    models, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n\t        [emb_file],\n\t        suffix=\"\",\n\t    )\n", "    embedder_model = models[0]\n\t    embedder_model = embedder_model.to(device)\n\t    if is_half:\n\t        embedder_model = embedder_model.half()\n\t    else:\n\t        embedder_model = embedder_model.float()\n\t    embedder_model.eval()\n\t    loaded_embedder_model = emb_name\n\tdef get_vc_model(model_name: str):\n\t    model_path = os.path.join(MODELS_DIR, \"checkpoints\", model_name)\n", "    weight = torch.load(model_path, map_location=\"cpu\")\n\t    return VoiceConvertModel(model_name, weight)\n\tdef load_model(model_name: str):\n\t    global vc_model\n\t    vc_model = get_vc_model(model_name)\n"]}
{"filename": "modules/ui.py", "chunked_list": ["import importlib\n\timport os\n\tfrom typing import *\n\timport gradio as gr\n\timport gradio.routes\n\timport torch\n\tfrom . import models, shared\n\tfrom .core import preload\n\tfrom .shared import ROOT_DIR\n\tclass Tab:\n", "    TABS_DIR = os.path.join(ROOT_DIR, \"modules\", \"tabs\")\n\t    def __init__(self, filepath: str) -> None:\n\t        self.filepath = filepath\n\t    def sort(self):\n\t        return 1\n\t    def title(self):\n\t        return \"\"\n\t    def ui(self, outlet: Callable):\n\t        pass\n\t    def __call__(self):\n", "        children_dir = self.filepath[:-3]\n\t        children = []\n\t        if os.path.isdir(children_dir):\n\t            for file in os.listdir(children_dir):\n\t                if not file.endswith(\".py\"):\n\t                    continue\n\t                module_name = file[:-3]\n\t                parent = os.path.relpath(Tab.TABS_DIR, Tab.TABS_DIR).replace(\"/\", \".\")\n\t                if parent.startswith(\".\"):\n\t                    parent = parent[1:]\n", "                if parent.endswith(\".\"):\n\t                    parent = parent[:-1]\n\t                children.append(\n\t                    importlib.import_module(f\"modules.tabs.{parent}.{module_name}\")\n\t                )\n\t        children = sorted(children, key=lambda x: x.sort())\n\t        tabs = []\n\t        for child in children:\n\t            attrs = child.__dict__\n\t            tab = [x for x in attrs.values() if issubclass(x, Tab)]\n", "            if len(tab) > 0:\n\t                tabs.append(tab[0])\n\t        def outlet():\n\t            with gr.Tabs():\n\t                for tab in tabs:\n\t                    with gr.Tab(tab.title()):\n\t                        tab()\n\t        return self.ui(outlet)\n\tdef load_tabs() -> List[Tab]:\n\t    tabs = []\n", "    files = os.listdir(os.path.join(ROOT_DIR, \"modules\", \"tabs\"))\n\t    for file in files:\n\t        if not file.endswith(\".py\"):\n\t            continue\n\t        module_name = file[:-3]\n\t        module = importlib.import_module(f\"modules.tabs.{module_name}\")\n\t        attrs = module.__dict__\n\t        TabClass = [\n\t            x\n\t            for x in attrs.values()\n", "            if type(x) == type and issubclass(x, Tab) and not x == Tab\n\t        ]\n\t        if len(TabClass) > 0:\n\t            tabs.append((file, TabClass[0]))\n\t    tabs = sorted([TabClass(file) for file, TabClass in tabs], key=lambda x: x.sort())\n\t    return tabs\n\tdef webpath(fn):\n\t    if fn.startswith(ROOT_DIR):\n\t        web_path = os.path.relpath(fn, ROOT_DIR).replace(\"\\\\\", \"/\")\n\t    else:\n", "        web_path = os.path.abspath(fn)\n\t    return f\"file={web_path}?{os.path.getmtime(fn)}\"\n\tdef javascript_html():\n\t    script_js = os.path.join(ROOT_DIR, \"script.js\")\n\t    head = f'<script type=\"text/javascript\" src=\"{webpath(script_js)}\"></script>\\n'\n\t    return head\n\tdef css_html():\n\t    return f'<link rel=\"stylesheet\" property=\"stylesheet\" href=\"{webpath(os.path.join(ROOT_DIR, \"styles.css\"))}\">'\n\tdef create_head():\n\t    head = \"\"\n", "    head += css_html()\n\t    head += javascript_html()\n\t    def template_response(*args, **kwargs):\n\t        res = shared.gradio_template_response_original(*args, **kwargs)\n\t        res.body = res.body.replace(b\"</head>\", f\"{head}</head>\".encode(\"utf8\"))\n\t        res.init_headers()\n\t        return res\n\t    gradio.routes.templates.TemplateResponse = template_response\n\tdef create_ui():\n\t    preload()\n", "    block = gr.Blocks()\n\t    with block:\n\t        with gr.Tabs():\n\t            tabs = load_tabs()\n\t            for tab in tabs:\n\t                with gr.Tab(tab.title()):\n\t                    tab()\n\t    create_head()\n\t    return block\n\tdef create_model_list_ui(speaker_id: bool = True, load: bool = True):\n", "    speaker_id_info = {\n\t        \"visible\": False,\n\t        \"maximum\": 10000,\n\t    }\n\t    def reload_model(raw=False):\n\t        model_list = models.get_models()\n\t        if len(model_list) > 0:\n\t            models.load_model(model_list[0])\n\t        if models.vc_model is not None:\n\t            speaker_id_info[\"visible\"] = True\n", "            speaker_id_info[\"maximum\"] = models.vc_model.n_spk\n\t        return model_list if raw else gr.Dropdown.update(choices=model_list)\n\t    model_list = reload_model(raw=True)\n\t    def load_model(model_name):\n\t        if load:\n\t            models.load_model(model_name)\n\t            speaker_id_info[\"visible\"] = True\n\t            speaker_id_info[\"maximum\"] = models.vc_model.n_spk\n\t        else:\n\t            model = models.get_vc_model(model_name)\n", "            speaker_id_info[\"visible\"] = True\n\t            speaker_id_info[\"maximum\"] = model.n_spk\n\t            del model\n\t            torch.cuda.empty_cache()\n\t        return gr.Slider.update(\n\t            maximum=speaker_id_info[\"maximum\"], visible=speaker_id_info[\"visible\"]\n\t        )\n\t    with gr.Row(equal_height=False):\n\t        model = gr.Dropdown(\n\t            choices=model_list,\n", "            label=\"Model\",\n\t            value=model_list[0] if len(model_list) > 0 else None,\n\t        )\n\t        speaker_id = gr.Slider(\n\t            minimum=0,\n\t            maximum=speaker_id_info[\"maximum\"],\n\t            step=1,\n\t            label=\"Speaker ID\",\n\t            value=0,\n\t            visible=speaker_id and speaker_id_info[\"visible\"],\n", "            interactive=True,\n\t        )\n\t        reload_model_button = gr.Button(\"♻️\")\n\t        model.change(load_model, inputs=[model], outputs=[speaker_id])\n\t        reload_model_button.click(reload_model, outputs=[model])\n\t    return model, speaker_id\n\tif not hasattr(shared, \"gradio_template_response_original\"):\n\t    shared.gradio_template_response_original = gradio.routes.templates.TemplateResponse\n"]}
{"filename": "modules/utils.py", "chunked_list": ["import os\n\tfrom typing import *\n\timport ffmpeg\n\timport numpy as np\n\timport requests\n\timport torch\n\tfrom tqdm import tqdm\n\tfrom lib.rvc.config import TrainConfig\n\tfrom modules.shared import ROOT_DIR\n\tdef load_audio(file: str, sr):\n", "    try:\n\t        # https://github.com/openai/whisper/blob/main/whisper/audio.py#L26\n\t        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n\t        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n\t        file = (\n\t            file.strip(\" \").strip('\"').strip(\"\\n\").strip('\"').strip(\" \")\n\t        )  # Prevent small white copy path head and tail with spaces and \" and return\n\t        out, _ = (\n\t            ffmpeg.input(file, threads=0)\n\t            .output(\"-\", format=\"f32le\", acodec=\"pcm_f32le\", ac=1, ar=sr)\n", "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n\t        )\n\t    except Exception as e:\n\t        raise RuntimeError(f\"Failed to load audio: {e}\")\n\t    return np.frombuffer(out, np.float32).flatten()\n\tdef get_gpus():\n\t    num_gpus = torch.cuda.device_count()\n\t    return [torch.device(f\"cuda:{i}\") for i in range(num_gpus)]\n\tdef download_file(url: str, out: str, position: int = 0, show: bool = True):\n\t    req = requests.get(url, stream=True, allow_redirects=True)\n", "    content_length = req.headers.get(\"content-length\")\n\t    if show:\n\t        progress_bar = tqdm(\n\t            total=int(content_length) if content_length is not None else None,\n\t            leave=False,\n\t            unit=\"B\",\n\t            unit_scale=True,\n\t            unit_divisor=1024,\n\t            position=position,\n\t        )\n", "    # with tqdm\n\t    with open(out, \"wb\") as f:\n\t        for chunk in req.iter_content(chunk_size=1024):\n\t            if chunk:\n\t                if show:\n\t                    progress_bar.update(len(chunk))\n\t                f.write(chunk)\n\tdef load_config(\n\t    version: Literal[\"v1\", \"v2\"],\n\t    training_dir: str,\n", "    sample_rate: str,\n\t    emb_channels: int,\n\t    fp16: bool,\n\t):\n\t    if emb_channels == 256:\n\t        config_path = os.path.join(ROOT_DIR, \"configs\", f\"{sample_rate}.json\")\n\t    else:\n\t        config_path = os.path.join(\n\t            ROOT_DIR, \"configs\", f\"{sample_rate}-{emb_channels}.json\"\n\t        )\n", "    config = TrainConfig.parse_file(config_path)\n\t    config.version = version\n\t    config.train.fp16_run = fp16\n\t    config_save_path = os.path.join(training_dir, \"config.json\")\n\t    with open(config_save_path, \"w\") as f:\n\t        f.write(config.json())\n\t    return config\n"]}
{"filename": "modules/core.py", "chunked_list": ["import hashlib\n\timport os\n\timport shutil\n\timport sys\n\tfrom concurrent.futures import ThreadPoolExecutor\n\timport requests\n\tfrom modules.models import MODELS_DIR\n\tfrom modules.shared import ROOT_DIR\n\tfrom modules.utils import download_file\n\tdef get_hf_etag(url: str):\n", "    r = requests.head(url)\n\t    etag = r.headers[\"X-Linked-ETag\"] if \"X-Linked-ETag\" in r.headers else \"\"\n\t    if etag.startswith('\"') and etag.endswith('\"'):\n\t        etag = etag[1:-1]\n\t    return etag\n\tdef calc_sha256(filepath: str):\n\t    sha256 = hashlib.sha256()\n\t    with open(filepath, \"rb\") as f:\n\t        for chunk in iter(lambda: f.read(4096), b\"\"):\n\t            sha256.update(chunk)\n", "    return sha256.hexdigest()\n\tdef download_models():\n\t    def hash_check(url: str, out: str):\n\t        if not os.path.exists(out):\n\t            return False\n\t        etag = get_hf_etag(url)\n\t        hash = calc_sha256(out)\n\t        return etag == hash\n\t    os.makedirs(os.path.join(MODELS_DIR, \"pretrained\", \"v2\"), exist_ok=True)\n\t    tasks = []\n", "    for template in [\n\t        \"D{}k\",\n\t        \"G{}k\",\n\t        \"f0D{}k\",\n\t        \"f0G{}k\",\n\t    ]:\n\t        basename = template.format(\"40\")\n\t        url = f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/pretrained/v2/{basename}.pth\"\n\t        out = os.path.join(MODELS_DIR, \"pretrained\", \"v2\", f\"{basename}.pth\")\n\t        if hash_check(url, out):\n", "            continue\n\t        tasks.append((url, out))\n\t    for filename in [\n\t        \"checkpoint_best_legacy_500.pt\",\n\t    ]:\n\t        out = os.path.join(MODELS_DIR, \"embeddings\", filename)\n\t        url = f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/embeddings/{filename}\"\n\t        if hash_check(url, out):\n\t            continue\n\t        tasks.append(\n", "            (\n\t                f\"https://huggingface.co/ddPn08/rvc-webui-models/resolve/main/embeddings/{filename}\",\n\t                out,\n\t            )\n\t        )\n\t    # japanese-hubert-base (Fairseq)\n\t    # from official repo\n\t    # NOTE: change filename?\n\t    hubert_jp_url = f\"https://huggingface.co/rinna/japanese-hubert-base/resolve/main/fairseq/model.pt\"\n\t    out = os.path.join(MODELS_DIR, \"embeddings\", \"rinna_hubert_base_jp.pt\")\n", "    if not hash_check(hubert_jp_url, out):\n\t        tasks.append(\n\t            (\n\t                hubert_jp_url,\n\t                out,\n\t            )\n\t        )\n\t    if len(tasks) < 1:\n\t        return\n\t    with ThreadPoolExecutor() as pool:\n", "        pool.map(\n\t            download_file,\n\t            *zip(\n\t                *[(filename, out, i, True) for i, (filename, out) in enumerate(tasks)]\n\t            ),\n\t        )\n\tdef install_ffmpeg():\n\t    if os.path.exists(os.path.join(ROOT_DIR, \"bin\", \"ffmpeg.exe\")):\n\t        return\n\t    tmpdir = os.path.join(ROOT_DIR, \"tmp\")\n", "    url = (\n\t        \"https://www.gyan.dev/ffmpeg/builds/packages/ffmpeg-5.1.2-essentials_build.zip\"\n\t    )\n\t    out = os.path.join(tmpdir, \"ffmpeg.zip\")\n\t    os.makedirs(os.path.dirname(out), exist_ok=True)\n\t    download_file(url, out)\n\t    shutil.unpack_archive(out, os.path.join(tmpdir, \"ffmpeg\"))\n\t    shutil.copyfile(\n\t        os.path.join(\n\t            tmpdir, \"ffmpeg\", \"ffmpeg-5.1.2-essentials_build\", \"bin\", \"ffmpeg.exe\"\n", "        ),\n\t        os.path.join(ROOT_DIR, \"bin\", \"ffmpeg.exe\"),\n\t    )\n\t    os.remove(os.path.join(tmpdir, \"ffmpeg.zip\"))\n\t    shutil.rmtree(os.path.join(tmpdir, \"ffmpeg\"))\n\tdef update_modelnames():\n\t    for sr in [\"32k\", \"40k\", \"48k\"]:\n\t        files = [\n\t            f\"f0G{sr}\",\n\t            f\"f0D{sr}\",\n", "            f\"G{sr}\",\n\t            f\"D{sr}\",\n\t        ]\n\t        for file in files:\n\t            filepath = os.path.join(MODELS_DIR, \"pretrained\", f\"{file}.pth\")\n\t            if os.path.exists(filepath):\n\t                os.rename(\n\t                    filepath,\n\t                    os.path.join(MODELS_DIR, \"pretrained\", f\"{file}256.pth\"),\n\t                )\n", "    if not os.path.exists(os.path.join(MODELS_DIR, \"embeddings\")):\n\t        os.makedirs(os.path.join(MODELS_DIR, \"embeddings\"))\n\t    if os.path.exists(os.path.join(MODELS_DIR, \"hubert_base.pt\")):\n\t        os.rename(\n\t            os.path.join(MODELS_DIR, \"hubert_base.pt\"),\n\t            os.path.join(MODELS_DIR, \"embeddings\", \"hubert_base.pt\"),\n\t        )\n\t    if os.path.exists(os.path.join(MODELS_DIR, \"checkpoint_best_legacy_500.pt\")):\n\t        os.rename(\n\t            os.path.join(MODELS_DIR, \"checkpoint_best_legacy_500.pt\"),\n", "            os.path.join(MODELS_DIR, \"embeddings\", \"checkpoint_best_legacy_500.pt\"),\n\t        )\n\tdef preload():\n\t    update_modelnames()\n\t    download_models()\n\t    if sys.platform == \"win32\":\n\t        install_ffmpeg()\n"]}
{"filename": "modules/shared.py", "chunked_list": ["import os\n\timport sys\n\timport torch\n\tfrom modules.cmd_opts import opts\n\tROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\tMODELS_DIR = os.path.join(ROOT_DIR, \"models\")\n\tdef has_mps():\n\t    if sys.platform != \"darwin\":\n\t        return False\n\t    else:\n", "        if not getattr(torch, \"has_mps\", False):\n\t            return False\n\t        try:\n\t            torch.zeros(1).to(torch.device(\"mps\"))\n\t            return True\n\t        except Exception:\n\t            return False\n\tis_half = opts.precision == \"fp16\"\n\thalf_support = (\n\t    torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 5.3\n", ")\n\tif not half_support:\n\t    print(\"WARNING: FP16 is not supported on this GPU\")\n\t    is_half = False\n\tdevice = \"cuda:0\"\n\tif not torch.cuda.is_available():\n\t    if has_mps():\n\t        print(\"Using MPS\")\n\t        device = \"mps\"\n\t    else:\n", "        print(\"Using CPU\")\n\t        device = \"cpu\"\n\tdevice = torch.device(device)\n"]}
{"filename": "modules/server/model.py", "chunked_list": ["import os\n\timport re\n\tfrom typing import *\n\timport faiss\n\timport numpy as np\n\timport pyworld\n\timport scipy.signal as signal\n\timport torch\n\timport torch.nn.functional as F\n\timport torchaudio\n", "import torchcrepe\n\tfrom fairseq import checkpoint_utils\n\tfrom fairseq.models.hubert.hubert import HubertModel\n\tfrom pydub import AudioSegment\n\tfrom torch import Tensor\n\tfrom lib.rvc.models import (SynthesizerTrnMs256NSFSid,\n\t                            SynthesizerTrnMs256NSFSidNono)\n\tfrom lib.rvc.pipeline import VocalConvertPipeline\n\tfrom modules.cmd_opts import opts\n\tfrom modules.models import (EMBEDDINGS_LIST, MODELS_DIR, get_embedder,\n", "                            get_vc_model, update_state_dict)\n\tfrom modules.shared import ROOT_DIR, device, is_half\n\tMODELS_DIR = opts.models_dir or os.path.join(ROOT_DIR, \"models\")\n\tvc_model: Optional[\"VoiceServerModel\"] = None\n\tembedder_model: Optional[HubertModel] = None\n\tloaded_embedder_model = \"\"\n\tclass VoiceServerModel:\n\t    def __init__(self, rvc_model_file: str, faiss_index_file: str) -> None:\n\t        # setting vram\n\t        global device, is_half\n", "        if isinstance(device, str):\n\t            device = torch.device(device)\n\t        if device.type == \"cuda\":\n\t            vram = torch.cuda.get_device_properties(device).total_memory / 1024**3\n\t        else:\n\t            vram = None\n\t        if vram is not None and vram <= 4:\n\t            self.x_pad = 1\n\t            self.x_query = 5\n\t            self.x_center = 30\n", "            self.x_max = 32\n\t        elif vram is not None and vram <= 5:\n\t            self.x_pad = 1\n\t            self.x_query = 6\n\t            self.x_center = 38\n\t            self.x_max = 41\n\t        else:\n\t            self.x_pad = 3\n\t            self.x_query = 10\n\t            self.x_center = 60\n", "            self.x_max = 65\n\t        # load_model\n\t        state_dict = torch.load(rvc_model_file, map_location=\"cpu\")\n\t        update_state_dict(state_dict)\n\t        self.state_dict = state_dict\n\t        self.tgt_sr = state_dict[\"params\"][\"sr\"]\n\t        self.f0 = state_dict.get(\"f0\", 1)\n\t        state_dict[\"params\"][\"spk_embed_dim\"] = state_dict[\"weight\"][\n\t            \"emb_g.weight\"\n\t        ].shape[0]\n", "        if not \"emb_channels\" in state_dict[\"params\"]:\n\t            if state_dict.get(\"version\", \"v1\") == \"v1\":\n\t                state_dict[\"params\"][\"emb_channels\"] = 256  # for backward compat.\n\t                state_dict[\"embedder_output_layer\"] = 9\n\t            else:\n\t                state_dict[\"params\"][\"emb_channels\"] = 768  # for backward compat.\n\t                state_dict[\"embedder_output_layer\"] = 12\n\t        if self.f0 == 1:\n\t            self.net_g = SynthesizerTrnMs256NSFSid(\n\t                **state_dict[\"params\"], is_half=is_half\n", "            )\n\t        else:\n\t            self.net_g = SynthesizerTrnMs256NSFSidNono(**state_dict[\"params\"])\n\t        del self.net_g.enc_q\n\t        self.net_g.load_state_dict(state_dict[\"weight\"], strict=False)\n\t        self.net_g.eval().to(device)\n\t        if is_half:\n\t            self.net_g = self.net_g.half()\n\t        else:\n\t            self.net_g = self.net_g.float()\n", "        emb_name = state_dict.get(\"embedder_name\", \"contentvec\")\n\t        if emb_name == \"hubert_base\":\n\t            emb_name = \"contentvec\"\n\t        emb_file = os.path.join(MODELS_DIR, \"embeddings\", EMBEDDINGS_LIST[emb_name][0])\n\t        models, _, _ = checkpoint_utils.load_model_ensemble_and_task(\n\t            [emb_file],\n\t            suffix=\"\",\n\t        )\n\t        embedder_model = models[0]\n\t        embedder_model = embedder_model.to(device)\n", "        if is_half:\n\t            embedder_model = embedder_model.half()\n\t        else:\n\t            embedder_model = embedder_model.float()\n\t        embedder_model.eval()\n\t        self.embedder_model = embedder_model\n\t        self.embedder_output_layer = state_dict[\"embedder_output_layer\"]\n\t        self.index = None\n\t        if faiss_index_file != \"\" and os.path.exists(faiss_index_file):\n\t            self.index = faiss.read_index(faiss_index_file)\n", "            self.big_npy = self.index.reconstruct_n(0, self.index.ntotal)\n\t        self.n_spk = state_dict[\"params\"][\"spk_embed_dim\"]\n\t        self.sr = 16000  # hubert input sample rate\n\t        self.window = 160  # hubert input window\n\t        self.t_pad = self.sr * self.x_pad  # padding time for each utterance\n\t        self.t_pad_tgt = self.tgt_sr * self.x_pad\n\t        self.t_pad2 = self.t_pad * 2\n\t        self.t_query = self.sr * self.x_query  # query time before and after query point\n\t        self.t_center = self.sr * self.x_center  # query cut point position\n\t        self.t_max = self.sr * self.x_max  # max time for no query\n", "        self.device = device\n\t        self.is_half = is_half\n\t    def __call__(\n\t        self,\n\t        audio: np.ndarray,\n\t        sr: int,\n\t        sid: int,\n\t        transpose: int,\n\t        f0_method: str,\n\t        index_rate: float,\n", "    ):\n\t        # bh, ah = signal.butter(N=5, Wn=48, btype=\"high\", fs=16000)\n\t        # audio = signal.filtfilt(bh, ah, audio)\n\t        if sr != self.sr:\n\t            audio = torchaudio.functional.resample(torch.from_numpy(audio), sr, self.sr, rolloff=0.99).detach().cpu().numpy()\n\t        audio_pad = np.pad(audio, (self.window // 2, self.window // 2), mode=\"reflect\" if audio.shape[0] > self.window // 2 else \"constant\")\n\t        opt_ts = []\n\t        if audio_pad.shape[0] > self.t_max:\n\t            audio_sum = np.zeros_like(audio)\n\t            for i in range(self.window):\n", "                audio_sum += audio_pad[i : i - self.window]\n\t            for t in range(self.t_center, audio.shape[0], self.t_center):\n\t                opt_ts.append(\n\t                    t\n\t                    - self.t_query\n\t                    + np.where(\n\t                        np.abs(audio_sum[t - self.t_query : t + self.t_query])\n\t                        == np.abs(audio_sum[t - self.t_query : t + self.t_query]).min()\n\t                    )[0][0]\n\t                )\n", "        audio_pad = np.pad(audio, (self.t_pad, self.t_pad), mode=\"reflect\" if audio.shape[0] > self.t_pad else \"constant\")\n\t        p_len = audio_pad.shape[0] // self.window\n\t        sid = torch.tensor(sid, device=self.device).unsqueeze(0).long()\n\t        pitch, pitchf = None, None\n\t        if self.f0 == 1:\n\t            pitch, pitchf = get_f0(audio_pad, self.sr, p_len, transpose, f0_method)\n\t            pitch = pitch[:p_len]\n\t            pitchf = pitchf[:p_len]\n\t            if self.device.type == \"mps\":\n\t                pitchf = pitchf.astype(np.float32)\n", "            pitch = torch.tensor(pitch, device=self.device).unsqueeze(0).long()\n\t            pitchf = torch.tensor(pitchf, device=self.device).unsqueeze(0).float()\n\t        audio_opt = []\n\t        s = 0\n\t        t = None\n\t        for t in opt_ts:\n\t            t = t // self.window * self.window\n\t            if self.f0 == 1:\n\t                audio_opt.append(\n\t                    self._convert(\n", "                        sid,\n\t                        audio_pad[s : t + self.t_pad2 + self.window],\n\t                        pitch[:, s // self.window : (t + self.t_pad2) // self.window],\n\t                        pitchf[:, s // self.window : (t + self.t_pad2) // self.window],\n\t                        index_rate,\n\t                    )[self.t_pad_tgt : -self.t_pad_tgt]\n\t                )\n\t            else:\n\t                audio_opt.append(\n\t                    self._convert(\n", "                        sid,\n\t                        audio_pad[s : t + self.t_pad2 + self.window],\n\t                        None,\n\t                        None,\n\t                        index_rate,\n\t                    )[self.t_pad_tgt : -self.t_pad_tgt]\n\t                )\n\t            s = t\n\t        if self.f0 == 1:\n\t            audio_opt.append(\n", "                self._convert(\n\t                    sid,\n\t                    audio_pad[t:],\n\t                    pitch[:, t // self.window :] if t is not None else pitch,\n\t                    pitchf[:, t // self.window :] if t is not None else pitchf,\n\t                    index_rate,\n\t                )[self.t_pad_tgt : -self.t_pad_tgt]\n\t            )\n\t        else:\n\t            audio_opt.append(\n", "                self._convert(\n\t                    sid,\n\t                    audio_pad[t:],\n\t                    None,\n\t                    None,\n\t                    index_rate,\n\t                )[self.t_pad_tgt : -self.t_pad_tgt]\n\t            )\n\t        audio_opt = np.concatenate(audio_opt)\n\t        del pitch, pitchf, sid\n", "        if torch.cuda.is_available():\n\t            torch.cuda.empty_cache()\n\t        return audio_opt\n\t    def _convert(\n\t        self,\n\t        sid: int,\n\t        audio: np.ndarray,\n\t        pitch: Optional[np.ndarray],\n\t        pitchf: Optional[np.ndarray],\n\t        index_rate: float,\n", "    ):\n\t        feats = torch.from_numpy(audio)\n\t        if self.is_half:\n\t            feats = feats.half()\n\t        else:\n\t            feats = feats.float()\n\t        if feats.dim() == 2:  # double channels\n\t            feats = feats.mean(-1)\n\t        assert feats.dim() == 1, feats.dim()\n\t        feats = feats.view(1, -1)\n", "        padding_mask = torch.BoolTensor(feats.shape).to(self.device).fill_(False)\n\t        half_support = (\n\t            self.device.type == \"cuda\"\n\t            and torch.cuda.get_device_capability(self.device)[0] >= 5.3\n\t        )\n\t        is_feats_dim_768 = self.net_g.emb_channels == 768\n\t        if isinstance(self.embedder_model, tuple):\n\t            feats = self.embedder_model[0](\n\t                feats.squeeze(0).squeeze(0).to(self.device),\n\t                return_tensors=\"pt\",\n", "                sampling_rate=16000,\n\t            )\n\t            if self.is_half:\n\t                feats = feats.input_values.to(self.device).half()\n\t            else:\n\t                feats = feats.input_values.to(self.device)\n\t            with torch.no_grad():\n\t                if is_feats_dim_768:\n\t                    feats = self.embedder_model[1](feats).last_hidden_state\n\t                else:\n", "                    feats = self.embedder_model[1](feats).extract_features\n\t        else:\n\t            inputs = {\n\t                \"source\": feats.half().to(self.device)\n\t                if half_support\n\t                else feats.to(self.device),\n\t                \"padding_mask\": padding_mask.to(self.device),\n\t                \"output_layer\": self.embedder_output_layer,\n\t            }\n\t            if not half_support:\n", "                self.embedder_model = self.embedder_model.float()\n\t                inputs[\"source\"] = inputs[\"source\"].float()\n\t            with torch.no_grad():\n\t                logits = self.embedder_model.extract_features(**inputs)\n\t                if is_feats_dim_768:\n\t                    feats = logits[0]\n\t                else:\n\t                    feats = self.embedder_model.final_proj(logits[0])\n\t        if (\n\t            isinstance(self.index, type(None)) == False\n", "            and isinstance(self.big_npy, type(None)) == False\n\t            and index_rate != 0\n\t        ):\n\t            npy = feats[0].cpu().numpy()\n\t            if self.is_half:\n\t                npy = npy.astype(\"float32\")\n\t            _, ix = self.index.search(npy, k=1)\n\t            npy = self.big_npy[ix[:, 0]]\n\t            if self.is_half:\n\t                npy = npy.astype(\"float16\")\n", "            feats = (\n\t                torch.from_numpy(npy).unsqueeze(0).to(self.device) * index_rate\n\t                + (1 - index_rate) * feats\n\t            )\n\t        feats = F.interpolate(feats.permute(0, 2, 1), scale_factor=2).permute(0, 2, 1)\n\t        p_len = audio.shape[0] // self.window\n\t        if feats.shape[1] < p_len:\n\t            p_len = feats.shape[1]\n\t            if pitch != None and pitchf != None:\n\t                pitch = pitch[:, :p_len]\n", "                pitchf = pitchf[:, :p_len]\n\t        p_len = torch.tensor([p_len], device=self.device).long()\n\t        with torch.no_grad():\n\t            if pitch != None and pitchf != None:\n\t                audio1 = (\n\t                    (self.net_g.infer(feats, p_len, pitch, pitchf, sid)[0][0, 0] * 32768)\n\t                    .data.cpu()\n\t                    .float()\n\t                    .numpy()\n\t                    .astype(np.int16)\n", "                )\n\t            else:\n\t                audio1 = (\n\t                    (self.net_g.infer(feats, p_len, sid)[0][0, 0] * 32768)\n\t                    .data.cpu()\n\t                    .float()\n\t                    .numpy()\n\t                    .astype(np.int16)\n\t                )\n\t        del feats, p_len, padding_mask\n", "        if torch.cuda.is_available():\n\t            torch.cuda.empty_cache()\n\t        return audio1\n\t# F0 computation\n\tdef get_f0_crepe_computation(\n\t        x,\n\t        sr,\n\t        f0_min,\n\t        f0_max,\n\t        p_len,\n", "        model=\"full\", # Either use crepe-tiny \"tiny\" or crepe \"full\". Default is full\n\t):\n\t    hop_length = sr // 100\n\t    x = x.astype(np.float32) # fixes the F.conv2D exception. We needed to convert double to float.\n\t    x /= np.quantile(np.abs(x), 0.999)\n\t    torch_device = self.get_optimal_torch_device()\n\t    audio = torch.from_numpy(x).to(torch_device, copy=True)\n\t    audio = torch.unsqueeze(audio, dim=0)\n\t    if audio.ndim == 2 and audio.shape[0] > 1:\n\t        audio = torch.mean(audio, dim=0, keepdim=True).detach()\n", "    audio = audio.detach()\n\t    print(\"Initiating prediction with a crepe_hop_length of: \" + str(hop_length))\n\t    pitch: Tensor = torchcrepe.predict(\n\t        audio,\n\t        sr,\n\t        sr // 100,\n\t        f0_min,\n\t        f0_max,\n\t        model,\n\t        batch_size=hop_length * 2,\n", "        device=torch_device,\n\t        pad=True\n\t    )\n\t    p_len = p_len or x.shape[0] // hop_length\n\t    # Resize the pitch for final f0\n\t    source = np.array(pitch.squeeze(0).cpu().float().numpy())\n\t    source[source < 0.001] = np.nan\n\t    target = np.interp(\n\t        np.arange(0, len(source) * p_len, len(source)) / p_len,\n\t        np.arange(0, len(source)),\n", "        source\n\t    )\n\t    f0 = np.nan_to_num(target)\n\t    return f0 # Resized f0\n\tdef get_f0_official_crepe_computation(\n\t        x,\n\t        sr,\n\t        f0_min,\n\t        f0_max,\n\t        model=\"full\",\n", "):\n\t    # Pick a batch size that doesn't cause memory errors on your gpu\n\t    batch_size = 512\n\t    # Compute pitch using first gpu\n\t    audio = torch.tensor(np.copy(x))[None].float()\n\t    f0, pd = torchcrepe.predict(\n\t        audio,\n\t        sr,\n\t        sr // 100,\n\t        f0_min,\n", "        f0_max,\n\t        model,\n\t        batch_size=batch_size,\n\t        device=device,\n\t        return_periodicity=True,\n\t    )\n\t    pd = torchcrepe.filter.median(pd, 3)\n\t    f0 = torchcrepe.filter.mean(f0, 3)\n\t    f0[pd < 0.1] = 0\n\t    f0 = f0[0].cpu().numpy()\n", "    return f0\n\tdef get_f0(\n\t    x: np.ndarray,\n\t    sr: int,\n\t    p_len: int,\n\t    f0_up_key: int,\n\t    f0_method: str,\n\t):\n\t    f0_min = 50\n\t    f0_max = 1100\n", "    f0_mel_min = 1127 * np.log(1 + f0_min / 700)\n\t    f0_mel_max = 1127 * np.log(1 + f0_max / 700)\n\t    if f0_method == \"harvest\":\n\t        f0, t = pyworld.harvest(\n\t            x.astype(np.double),\n\t            fs=sr,\n\t            f0_ceil=f0_max,\n\t            f0_floor=f0_min,\n\t            frame_period=10,\n\t        )\n", "        f0 = pyworld.stonemask(x.astype(np.double), f0, t, sr)\n\t        f0 = signal.medfilt(f0, 3)\n\t    elif f0_method == \"dio\":\n\t        f0, t = pyworld.dio(\n\t            x.astype(np.double),\n\t            fs=sr,\n\t            f0_ceil=f0_max,\n\t            f0_floor=f0_min,\n\t            frame_period=10,\n\t        )\n", "        f0 = pyworld.stonemask(x.astype(np.double), f0, t, sr)\n\t        f0 = signal.medfilt(f0, 3)\n\t    elif f0_method == \"mangio-crepe\":\n\t        f0 = get_f0_crepe_computation(x, sr, f0_min, f0_max, p_len, \"full\")\n\t    elif f0_method == \"crepe\":\n\t        f0 = get_f0_official_crepe_computation(x, sr, f0_min, f0_max, \"full\")\n\t    f0 *= pow(2, f0_up_key / 12)\n\t    f0bak = f0.copy()\n\t    f0_mel = 1127 * np.log(1 + f0 / 700)\n\t    f0_mel[f0_mel > 0] = (f0_mel[f0_mel > 0] - f0_mel_min) * 254 / (\n", "        f0_mel_max - f0_mel_min\n\t    ) + 1\n\t    f0_mel[f0_mel <= 1] = 1\n\t    f0_mel[f0_mel > 255] = 255\n\t    f0_coarse = np.rint(f0_mel).astype(np.int32)\n\t    return f0_coarse, f0bak  # 1-0"]}
{"filename": "modules/tabs/merge.py", "chunked_list": ["import json\n\timport os\n\tfrom typing import *\n\timport gradio as gr\n\timport torch\n\tfrom modules import models\n\tfrom modules.merge import merge\n\tfrom modules.tabs.inference import inference_options_ui\n\tfrom modules.ui import Tab\n\tMERGE_METHODS = {\n", "    \"weight_sum\": \"Weight sum:A*(1-alpha)+B*alpha\",\n\t    \"add_diff\": \"Add difference:A+(B-C)*alpha\",\n\t}\n\tclass Merge(Tab):\n\t    def title(self):\n\t        return \"Merge\"\n\t    def sort(self):\n\t        return 3\n\t    def ui(self, outlet):\n\t        def merge_ckpt(model_a, model_b, model_c, weight_text, alpha, each_key, method):\n", "            model_a = model_a if type(model_a) != list and model_a != \"\" else None\n\t            model_b = model_b if type(model_b) != list and model_b != \"\" else None\n\t            model_c = model_c if type(model_c) != list and model_c != \"\" else None\n\t            if each_key:\n\t                weights = json.loads(weight_text)\n\t            else:\n\t                weights = {}\n\t            method = [k for k, v in MERGE_METHODS.items() if v == method][0]\n\t            return merge(\n\t                os.path.join(models.MODELS_DIR, \"checkpoints\", model_a),\n", "                os.path.join(models.MODELS_DIR, \"checkpoints\", model_b),\n\t                os.path.join(models.MODELS_DIR, \"checkpoints\", model_c)\n\t                if model_c\n\t                else None,\n\t                alpha,\n\t                weights,\n\t                method,\n\t            )\n\t        def merge_and_save(\n\t            model_a, model_b, model_c, alpha, each_key, weight_text, method, out_name\n", "        ):\n\t            print(each_key)\n\t            out_path = os.path.join(models.MODELS_DIR, \"checkpoints\", out_name)\n\t            if os.path.exists(out_path):\n\t                return \"Model name already exists.\"\n\t            merged = merge_ckpt(\n\t                model_a, model_b, model_c, weight_text, alpha, each_key, method\n\t            )\n\t            if not out_name.endswith(\".pth\"):\n\t                out_name += \".pth\"\n", "            torch.save(merged, os.path.join(models.MODELS_DIR, \"checkpoints\", out_name))\n\t            return \"Success\"\n\t        def merge_and_gen(\n\t            model_a,\n\t            model_b,\n\t            model_c,\n\t            alpha,\n\t            each_key,\n\t            weight_text,\n\t            method,\n", "            speaker_id,\n\t            source_audio,\n\t            embedder_name,\n\t            embedding_output_layer,\n\t            transpose,\n\t            fo_curve_file,\n\t            pitch_extraction_algo,\n\t            auto_load_index,\n\t            faiss_index_file,\n\t            retrieval_feature_ratio,\n", "        ):\n\t            merged = merge_ckpt(\n\t                model_a, model_b, model_c, weight_text, alpha, each_key, method\n\t            )\n\t            model = models.VoiceConvertModel(\"merge\", merged)\n\t            audio = model.single(\n\t                speaker_id,\n\t                source_audio,\n\t                embedder_name,\n\t                embedding_output_layer,\n", "                transpose,\n\t                fo_curve_file,\n\t                pitch_extraction_algo,\n\t                auto_load_index,\n\t                faiss_index_file,\n\t                retrieval_feature_ratio,\n\t            )\n\t            tgt_sr = model.tgt_sr\n\t            del merged\n\t            del model\n", "            torch.cuda.empty_cache()\n\t            return \"Success\", (tgt_sr, audio)\n\t        def reload_model():\n\t            model_list = models.get_models()\n\t            return (\n\t                gr.Dropdown.update(choices=model_list),\n\t                gr.Dropdown.update(choices=model_list),\n\t                gr.Dropdown.update(choices=model_list),\n\t            )\n\t        def update_speaker_ids(model):\n", "            if model == \"\":\n\t                return gr.Slider.update(\n\t                    maximum=0,\n\t                    visible=False,\n\t                )\n\t            model = torch.load(\n\t                os.path.join(models.MODELS_DIR, \"checkpoints\", model),\n\t                map_location=\"cpu\",\n\t            )\n\t            vc_model = models.VoiceConvertModel(\"merge\", model)\n", "            max = vc_model.n_spk\n\t            del model\n\t            del vc_model\n\t            return gr.Slider.update(\n\t                maximum=max,\n\t                visible=True,\n\t            )\n\t        with gr.Group():\n\t            with gr.Column():\n\t                with gr.Row(equal_height=False):\n", "                    model_a = gr.Dropdown(choices=models.get_models(), label=\"Model A\")\n\t                    model_b = gr.Dropdown(choices=models.get_models(), label=\"Model B\")\n\t                    model_c = gr.Dropdown(choices=models.get_models(), label=\"Model C\")\n\t                    reload_model_button = gr.Button(\"♻️\")\n\t                    reload_model_button.click(\n\t                        reload_model, outputs=[model_a, model_b, model_c]\n\t                    )\n\t                with gr.Row(equal_height=False):\n\t                    method = gr.Radio(\n\t                        label=\"Merge method\",\n", "                        choices=list(MERGE_METHODS.values()),\n\t                        value=\"Weight sum:A*(1-alpha)+B*alpha\",\n\t                    )\n\t                    output_name = gr.Textbox(label=\"Output name\")\n\t                    each_key = gr.Checkbox(label=\"Each key merge\")\n\t                with gr.Row(equal_height=False):\n\t                    base_alpha = gr.Slider(\n\t                        label=\"Base alpha\", minimum=0, maximum=1, value=0.5, step=0.01\n\t                    )\n\t                default_weights = {}\n", "                weights = {}\n\t                def create_weight_ui(name: str, *keys_list: List[List[str]]):\n\t                    with gr.Accordion(label=name, open=False):\n\t                        with gr.Row(equal_height=False):\n\t                            for keys in keys_list:\n\t                                with gr.Column():\n\t                                    for key in keys:\n\t                                        default_weights[key] = 0.5\n\t                                        weights[key] = gr.Slider(\n\t                                            label=key,\n", "                                            minimum=0,\n\t                                            maximum=1,\n\t                                            step=0.01,\n\t                                            value=0.5,\n\t                                        )\n\t                with gr.Box(visible=False) as each_key_ui:\n\t                    with gr.Column():\n\t                        create_weight_ui(\n\t                            \"enc_p\",\n\t                            [\n", "                                \"enc_p.encoder.attn_layers.0\",\n\t                                \"enc_p.encoder.attn_layers.1\",\n\t                                \"enc_p.encoder.attn_layers.2\",\n\t                                \"enc_p.encoder.attn_layers.3\",\n\t                                \"enc_p.encoder.attn_layers.4\",\n\t                                \"enc_p.encoder.attn_layers.5\",\n\t                                \"enc_p.encoder.norm_layers_1.0\",\n\t                                \"enc_p.encoder.norm_layers_1.1\",\n\t                                \"enc_p.encoder.norm_layers_1.2\",\n\t                                \"enc_p.encoder.norm_layers_1.3\",\n", "                                \"enc_p.encoder.norm_layers_1.4\",\n\t                                \"enc_p.encoder.norm_layers_1.5\",\n\t                            ],\n\t                            [\n\t                                \"enc_p.encoder.ffn_layers.0\",\n\t                                \"enc_p.encoder.ffn_layers.1\",\n\t                                \"enc_p.encoder.ffn_layers.2\",\n\t                                \"enc_p.encoder.ffn_layers.3\",\n\t                                \"enc_p.encoder.ffn_layers.4\",\n\t                                \"enc_p.encoder.ffn_layers.5\",\n", "                                \"enc_p.encoder.norm_layers_2.0\",\n\t                                \"enc_p.encoder.norm_layers_2.1\",\n\t                                \"enc_p.encoder.norm_layers_2.2\",\n\t                                \"enc_p.encoder.norm_layers_2.3\",\n\t                                \"enc_p.encoder.norm_layers_2.4\",\n\t                                \"enc_p.encoder.norm_layers_2.5\",\n\t                            ],\n\t                            [\n\t                                \"enc_p.emb_phone\",\n\t                                \"enc_p.emb_pitch\",\n", "                            ],\n\t                        )\n\t                        create_weight_ui(\n\t                            \"dec\",\n\t                            [\n\t                                \"dec.noise_convs.0\",\n\t                                \"dec.noise_convs.1\",\n\t                                \"dec.noise_convs.2\",\n\t                                \"dec.noise_convs.3\",\n\t                                \"dec.noise_convs.4\",\n", "                                \"dec.noise_convs.5\",\n\t                                \"dec.ups.0\",\n\t                                \"dec.ups.1\",\n\t                                \"dec.ups.2\",\n\t                                \"dec.ups.3\",\n\t                            ],\n\t                            [\n\t                                \"dec.resblocks.0\",\n\t                                \"dec.resblocks.1\",\n\t                                \"dec.resblocks.2\",\n", "                                \"dec.resblocks.3\",\n\t                                \"dec.resblocks.4\",\n\t                                \"dec.resblocks.5\",\n\t                                \"dec.resblocks.6\",\n\t                                \"dec.resblocks.7\",\n\t                                \"dec.resblocks.8\",\n\t                                \"dec.resblocks.9\",\n\t                                \"dec.resblocks.10\",\n\t                                \"dec.resblocks.11\",\n\t                            ],\n", "                            [\n\t                                \"dec.m_source.l_linear\",\n\t                                \"dec.conv_pre\",\n\t                                \"dec.conv_post\",\n\t                                \"dec.cond\",\n\t                            ],\n\t                        )\n\t                        create_weight_ui(\n\t                            \"flow\",\n\t                            [\n", "                                \"flow.flows.0\",\n\t                                \"flow.flows.1\",\n\t                                \"flow.flows.2\",\n\t                                \"flow.flows.3\",\n\t                                \"flow.flows.4\",\n\t                                \"flow.flows.5\",\n\t                                \"flow.flows.6\",\n\t                                \"emb_g.weight\",\n\t                            ],\n\t                        )\n", "                        with gr.Accordion(label=\"JSON\", open=False):\n\t                            weights_text = gr.TextArea(\n\t                                value=json.dumps(default_weights),\n\t                            )\n\t                with gr.Accordion(label=\"Inference options\", open=False):\n\t                    with gr.Row(equal_height=False):\n\t                        speaker_id = gr.Slider(\n\t                            minimum=0,\n\t                            maximum=2333,\n\t                            step=1,\n", "                            label=\"Speaker ID\",\n\t                            value=0,\n\t                            visible=True,\n\t                            interactive=True,\n\t                        )\n\t                    (\n\t                        source_audio,\n\t                        _,\n\t                        transpose,\n\t                        embedder_name,\n", "                        embedding_output_layer,\n\t                        pitch_extraction_algo,\n\t                        auto_load_index,\n\t                        faiss_index_file,\n\t                        retrieval_feature_ratio,\n\t                        fo_curve_file,\n\t                    ) = inference_options_ui(show_out_dir=False)\n\t                with gr.Row(equal_height=False):\n\t                    with gr.Column():\n\t                        status = gr.Textbox(value=\"\", label=\"Status\")\n", "                        audio_output = gr.Audio(label=\"Output\", interactive=False)\n\t                with gr.Row(equal_height=False):\n\t                    merge_and_save_button = gr.Button(\n\t                        \"Merge and save\", variant=\"primary\"\n\t                    )\n\t                    merge_and_gen_button = gr.Button(\"Merge and gen\", variant=\"primary\")\n\t                def each_key_on_change(each_key):\n\t                    return gr.update(visible=each_key)\n\t                each_key.change(\n\t                    fn=each_key_on_change,\n", "                    inputs=[each_key],\n\t                    outputs=[each_key_ui],\n\t                )\n\t                def update_weights_text(data):\n\t                    d = {}\n\t                    for key in weights.keys():\n\t                        d[key] = data[weights[key]]\n\t                    return json.dumps(d)\n\t                for w in weights.values():\n\t                    w.change(\n", "                        fn=update_weights_text,\n\t                        inputs={*weights.values()},\n\t                        outputs=[weights_text],\n\t                    )\n\t                merge_data = [\n\t                    model_a,\n\t                    model_b,\n\t                    model_c,\n\t                    base_alpha,\n\t                    each_key,\n", "                    weights_text,\n\t                    method,\n\t                ]\n\t                inference_opts = [\n\t                    speaker_id,\n\t                    source_audio,\n\t                    embedder_name,\n\t                    embedding_output_layer,\n\t                    transpose,\n\t                    fo_curve_file,\n", "                    pitch_extraction_algo,\n\t                    auto_load_index,\n\t                    faiss_index_file,\n\t                    retrieval_feature_ratio,\n\t                ]\n\t                merge_and_save_button.click(\n\t                    fn=merge_and_save,\n\t                    inputs=[\n\t                        *merge_data,\n\t                        output_name,\n", "                    ],\n\t                    outputs=[status],\n\t                )\n\t                merge_and_gen_button.click(\n\t                    fn=merge_and_gen,\n\t                    inputs=[\n\t                        *merge_data,\n\t                        *inference_opts,\n\t                    ],\n\t                    outputs=[status, audio_output],\n", "                )\n\t                model_a.change(\n\t                    update_speaker_ids, inputs=[model_a], outputs=[speaker_id]\n\t                )\n"]}
{"filename": "modules/tabs/inference.py", "chunked_list": ["import glob\n\timport os\n\timport traceback\n\timport gradio as gr\n\tfrom modules import models, ui\n\tfrom modules.ui import Tab\n\tdef inference_options_ui(show_out_dir=True):\n\t    with gr.Row(equal_height=False):\n\t        with gr.Column():\n\t            source_audio = gr.Textbox(label=\"Source Audio\")\n", "            out_dir = gr.Textbox(\n\t                label=\"Out folder\",\n\t                visible=show_out_dir,\n\t                placeholder=models.AUDIO_OUT_DIR,\n\t            )\n\t        with gr.Column():\n\t            transpose = gr.Slider(\n\t                minimum=-20, maximum=20, value=0, step=1, label=\"Transpose\"\n\t            )\n\t            pitch_extraction_algo = gr.Radio(\n", "                choices=[\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"],\n\t                value=\"crepe\",\n\t                label=\"Pitch Extraction Algorithm\",\n\t            )\n\t            embedding_model = gr.Radio(\n\t                choices=[\"auto\", *models.EMBEDDINGS_LIST.keys()],\n\t                value=\"auto\",\n\t                label=\"Embedder Model\",\n\t            )\n\t            embedding_output_layer = gr.Radio(\n", "                choices=[\"auto\", \"9\", \"12\"],\n\t                value=\"auto\",\n\t                label=\"Embedder Output Layer\",\n\t            )\n\t        with gr.Column():\n\t            auto_load_index = gr.Checkbox(value=False, label=\"Auto Load Index\")\n\t            faiss_index_file = gr.Textbox(value=\"\", label=\"Faiss Index File Path\")\n\t            retrieval_feature_ratio = gr.Slider(\n\t                minimum=0,\n\t                maximum=1,\n", "                value=1,\n\t                step=0.01,\n\t                label=\"Retrieval Feature Ratio\",\n\t            )\n\t        with gr.Column():\n\t            fo_curve_file = gr.File(label=\"F0 Curve File\")\n\t    return (\n\t        source_audio,\n\t        out_dir,\n\t        transpose,\n", "        embedding_model,\n\t        embedding_output_layer,\n\t        pitch_extraction_algo,\n\t        auto_load_index,\n\t        faiss_index_file,\n\t        retrieval_feature_ratio,\n\t        fo_curve_file,\n\t    )\n\tclass Inference(Tab):\n\t    def title(self):\n", "        return \"Inference\"\n\t    def sort(self):\n\t        return 1\n\t    def ui(self, outlet):\n\t        def infer(\n\t            sid,\n\t            input_audio,\n\t            out_dir,\n\t            embedder_model,\n\t            embedding_output_layer,\n", "            f0_up_key,\n\t            f0_file,\n\t            f0_method,\n\t            auto_load_index,\n\t            faiss_index_file,\n\t            index_rate,\n\t        ):\n\t            model = models.vc_model\n\t            try:\n\t                yield \"Infering...\", None\n", "                if out_dir == \"\":\n\t                    out_dir = models.AUDIO_OUT_DIR\n\t                if \"*\" in input_audio:\n\t                    assert (\n\t                        out_dir is not None\n\t                    ), \"Out folder is required for batch processing\"\n\t                    files = glob.glob(input_audio, recursive=True)\n\t                elif os.path.isdir(input_audio):\n\t                    assert (\n\t                        out_dir is not None\n", "                    ), \"Out folder is required for batch processing\"\n\t                    files = glob.glob(\n\t                        os.path.join(input_audio, \"**\", \"*.wav\"), recursive=True\n\t                    )\n\t                else:\n\t                    files = [input_audio]\n\t                for file in files:\n\t                    audio = model.single(\n\t                        sid,\n\t                        file,\n", "                        embedder_model,\n\t                        embedding_output_layer,\n\t                        f0_up_key,\n\t                        f0_file,\n\t                        f0_method,\n\t                        auto_load_index,\n\t                        faiss_index_file,\n\t                        index_rate,\n\t                        output_dir=out_dir,\n\t                    )\n", "                yield \"Success\", (model.tgt_sr, audio) if len(files) == 1 else None\n\t            except:\n\t                yield \"Error: \" + traceback.format_exc(), None\n\t        with gr.Group():\n\t            with gr.Box():\n\t                with gr.Column():\n\t                    _, speaker_id = ui.create_model_list_ui()\n\t                    (\n\t                        source_audio,\n\t                        out_dir,\n", "                        transpose,\n\t                        embedder_model,\n\t                        embedding_output_layer,\n\t                        pitch_extraction_algo,\n\t                        auto_load_index,\n\t                        faiss_index_file,\n\t                        retrieval_feature_ratio,\n\t                        f0_curve_file,\n\t                    ) = inference_options_ui()\n\t                    with gr.Row(equal_height=False):\n", "                        with gr.Column():\n\t                            status = gr.Textbox(value=\"\", label=\"Status\")\n\t                            output = gr.Audio(label=\"Output\", interactive=False)\n\t                    with gr.Row():\n\t                        infer_button = gr.Button(\"Infer\", variant=\"primary\")\n\t        infer_button.click(\n\t            infer,\n\t            inputs=[\n\t                speaker_id,\n\t                source_audio,\n", "                out_dir,\n\t                embedder_model,\n\t                embedding_output_layer,\n\t                transpose,\n\t                f0_curve_file,\n\t                pitch_extraction_algo,\n\t                auto_load_index,\n\t                faiss_index_file,\n\t                retrieval_feature_ratio,\n\t            ],\n", "            outputs=[status, output],\n\t            queue=True,\n\t        )\n"]}
{"filename": "modules/tabs/split.py", "chunked_list": ["import gradio as gr\n\tfrom modules.separate import separate_audio\n\tfrom modules.ui import Tab\n\tclass Split(Tab):\n\t    def title(self):\n\t        return \"Split Audio\"\n\t    def sort(self):\n\t        return 5\n\t    def ui(self, outlet):\n\t        def separate(\n", "            input_audio,\n\t            output_dir,\n\t            silence_thresh,\n\t            min_silence_len,\n\t            keep_silence,\n\t            margin,\n\t            padding,\n\t            min,\n\t            max,\n\t        ):\n", "            min = None if min == 0 else min\n\t            max = None if max == 0 else max\n\t            separate_audio(\n\t                input_audio,\n\t                output_dir,\n\t                int(silence_thresh),\n\t                int(min_silence_len),\n\t                int(keep_silence),\n\t                int(margin),\n\t                padding,\n", "                int(min),\n\t                int(max),\n\t            )\n\t            return \"Success\"\n\t        with gr.Group():\n\t            with gr.Column():\n\t                with gr.Row(equal_height=False):\n\t                    input_audio = gr.Textbox(label=\"Input Audio (File or Directory)\")\n\t                    output_dir = gr.Textbox(label=\"Output Directory\")\n\t                with gr.Row(equal_height=False):\n", "                    silence_thresh = gr.Number(value=-40, label=\"Silence Threshold\")\n\t                    min_silence_len = gr.Number(\n\t                        value=750, label=\"Minimum Silence Length\"\n\t                    )\n\t                    keep_silence = gr.Number(value=750, label=\"Keep Silence\")\n\t                    margin = gr.Number(value=0, label=\"Margin\")\n\t                    padding = gr.Checkbox(value=True, label=\"Padding\")\n\t                with gr.Row(equal_height=False):\n\t                    min = gr.Number(value=1000, label=\"Minimum audio length\")\n\t                    max = gr.Number(value=5000, label=\"Maximum audio length\")\n", "                with gr.Row(equal_height=False):\n\t                    status = gr.Textbox(value=\"\", label=\"Status\")\n\t                with gr.Row(equal_height=False):\n\t                    separate_button = gr.Button(\"Separate\", variant=\"primary\")\n\t        separate_button.click(\n\t            separate,\n\t            inputs=[\n\t                input_audio,\n\t                output_dir,\n\t                silence_thresh,\n", "                min_silence_len,\n\t                keep_silence,\n\t                margin,\n\t                padding,\n\t                min,\n\t                max,\n\t            ],\n\t            outputs=[status],\n\t        )\n"]}
{"filename": "modules/tabs/training.py", "chunked_list": ["import math\n\timport os\n\timport shutil\n\tfrom multiprocessing import cpu_count\n\timport gradio as gr\n\tfrom lib.rvc.preprocessing import extract_f0, extract_feature, split\n\tfrom lib.rvc.train import create_dataset_meta, glob_dataset, train_index, train_model\n\tfrom modules import models, utils\n\tfrom modules.shared import MODELS_DIR, device, half_support\n\tfrom modules.ui import Tab\n", "SR_DICT = {\n\t    \"32k\": 32000,\n\t    \"40k\": 40000,\n\t    \"48k\": 48000,\n\t}\n\tclass Training(Tab):\n\t    def title(self):\n\t        return \"Training\"\n\t    def sort(self):\n\t        return 2\n", "    def ui(self, outlet):\n\t        def train_index_only(\n\t            model_name,\n\t            target_sr,\n\t            f0,\n\t            dataset_glob,\n\t            recursive,\n\t            multiple_speakers,\n\t            speaker_id,\n\t            gpu_id,\n", "            num_cpu_process,\n\t            norm_audio_when_preprocess,\n\t            pitch_extraction_algo,\n\t            run_train_index,\n\t            reduce_index_size,\n\t            maximum_index_size,\n\t            embedder_name,\n\t            embedding_channels,\n\t            embedding_output_layer,\n\t            ignore_cache,\n", "        ):\n\t            maximum_index_size = int(maximum_index_size)\n\t            f0 = f0 == \"Yes\"\n\t            norm_audio_when_preprocess = norm_audio_when_preprocess == \"Yes\"\n\t            run_train_index = run_train_index == \"Yes\"\n\t            reduce_index_size = reduce_index_size == \"Yes\"\n\t            training_dir = os.path.join(MODELS_DIR, \"training\", \"models\", model_name)\n\t            gpu_ids = [int(x.strip()) for x in gpu_id.split(\",\")] if gpu_id else []\n\t            yield f\"Training directory: {training_dir}\"\n\t            if os.path.exists(training_dir) and ignore_cache:\n", "                shutil.rmtree(training_dir)\n\t            os.makedirs(training_dir, exist_ok=True)\n\t            datasets = glob_dataset(\n\t                dataset_glob,\n\t                speaker_id,\n\t                multiple_speakers=multiple_speakers,\n\t                recursive=recursive,\n\t            )\n\t            if len(datasets) == 0:\n\t                raise Exception(\"No audio files found\")\n", "            yield \"Preprocessing...\"\n\t            split.preprocess_audio(\n\t                datasets,\n\t                SR_DICT[target_sr],\n\t                num_cpu_process,\n\t                training_dir,\n\t                norm_audio_when_preprocess,\n\t                os.path.join(\n\t                    MODELS_DIR,\n\t                    \"training\",\n", "                    \"mute\",\n\t                    \"0_gt_wavs\",\n\t                    f\"mute{target_sr}.wav\",\n\t                ),\n\t            )\n\t            if f0:\n\t                yield \"Extracting f0...\"\n\t                extract_f0.run(training_dir, num_cpu_process, pitch_extraction_algo)\n\t            yield \"Extracting features...\"\n\t            embedder_filepath, _, embedder_load_from = models.get_embedder(\n", "                embedder_name\n\t            )\n\t            if embedder_load_from == \"local\":\n\t                embedder_filepath = os.path.join(\n\t                    MODELS_DIR, \"embeddings\", embedder_filepath\n\t                )\n\t            extract_feature.run(\n\t                training_dir,\n\t                embedder_filepath,\n\t                embedder_load_from,\n", "                int(embedding_channels),\n\t                int(embedding_output_layer),\n\t                gpu_ids,\n\t            )\n\t            out_dir = os.path.join(MODELS_DIR, \"checkpoints\")\n\t            yield \"Training index...\"\n\t            if run_train_index:\n\t                if not reduce_index_size:\n\t                    maximum_index_size = None\n\t                train_index(\n", "                    training_dir,\n\t                    model_name,\n\t                    out_dir,\n\t                    int(embedding_channels),\n\t                    num_cpu_process,\n\t                    maximum_index_size,\n\t                )\n\t            yield \"Training complete\"\n\t        def train_all(\n\t            model_name,\n", "            version,\n\t            sampling_rate_str,\n\t            f0,\n\t            dataset_glob,\n\t            recursive,\n\t            multiple_speakers,\n\t            speaker_id,\n\t            gpu_id,\n\t            num_cpu_process,\n\t            norm_audio_when_preprocess,\n", "            pitch_extraction_algo,\n\t            batch_size,\n\t            augment,\n\t            augment_from_pretrain,\n\t            augment_path,\n\t            speaker_info_path,\n\t            cache_batch,\n\t            num_epochs,\n\t            save_every_epoch,\n\t            save_wav_with_checkpoint,\n", "            fp16,\n\t            pre_trained_bottom_model_g,\n\t            pre_trained_bottom_model_d,\n\t            run_train_index,\n\t            reduce_index_size,\n\t            maximum_index_size,\n\t            embedder_name,\n\t            embedding_channels,\n\t            embedding_output_layer,\n\t            ignore_cache,\n", "        ):\n\t            batch_size = int(batch_size)\n\t            num_epochs = int(num_epochs)\n\t            maximum_index_size = int(maximum_index_size)\n\t            f0 = f0 == \"Yes\"\n\t            norm_audio_when_preprocess = norm_audio_when_preprocess == \"Yes\"\n\t            run_train_index = run_train_index == \"Yes\"\n\t            reduce_index_size = reduce_index_size == \"Yes\"\n\t            training_dir = os.path.join(MODELS_DIR, \"training\", \"models\", model_name)\n\t            gpu_ids = [int(x.strip()) for x in gpu_id.split(\",\")] if gpu_id else []\n", "            if os.path.exists(training_dir) and ignore_cache:\n\t                shutil.rmtree(training_dir)\n\t            os.makedirs(training_dir, exist_ok=True)\n\t            yield f\"Training directory: {training_dir}\"\n\t            datasets = glob_dataset(\n\t                dataset_glob,\n\t                speaker_id,\n\t                multiple_speakers=multiple_speakers,\n\t                recursive=recursive,\n\t                training_dir=training_dir,\n", "            )\n\t            if len(datasets) == 0:\n\t                raise Exception(\"No audio files found\")\n\t            yield \"Preprocessing...\"\n\t            split.preprocess_audio(\n\t                datasets,\n\t                SR_DICT[sampling_rate_str],\n\t                num_cpu_process,\n\t                training_dir,\n\t                norm_audio_when_preprocess,\n", "                os.path.join(\n\t                    MODELS_DIR,\n\t                    \"training\",\n\t                    \"mute\",\n\t                    \"0_gt_wavs\",\n\t                    f\"mute{sampling_rate_str}.wav\",\n\t                ),\n\t            )\n\t            if f0:\n\t                yield \"Extracting f0...\"\n", "                extract_f0.run(training_dir, num_cpu_process, pitch_extraction_algo)\n\t            yield \"Extracting features...\"\n\t            embedder_filepath, _, embedder_load_from = models.get_embedder(\n\t                embedder_name\n\t            )\n\t            if embedder_load_from == \"local\":\n\t                embedder_filepath = os.path.join(\n\t                    MODELS_DIR, \"embeddings\", embedder_filepath\n\t                )\n\t            extract_feature.run(\n", "                training_dir,\n\t                embedder_filepath,\n\t                embedder_load_from,\n\t                int(embedding_channels),\n\t                int(embedding_output_layer),\n\t                gpu_ids,\n\t                None if len(gpu_ids) > 1 else device,\n\t            )\n\t            create_dataset_meta(training_dir, f0)\n\t            yield \"Training model...\"\n", "            print(f\"train_all: emb_name: {embedder_name}\")\n\t            config = utils.load_config(\n\t                version, training_dir, sampling_rate_str, embedding_channels, fp16\n\t            )\n\t            out_dir = os.path.join(MODELS_DIR, \"checkpoints\")\n\t            if not augment_from_pretrain:\n\t                augment_path = None\n\t                speaker_info_path = None\n\t            train_model(\n\t                gpu_ids,\n", "                config,\n\t                training_dir,\n\t                model_name,\n\t                out_dir,\n\t                sampling_rate_str,\n\t                f0,\n\t                batch_size,\n\t                augment,\n\t                augment_path,\n\t                speaker_info_path,\n", "                cache_batch,\n\t                num_epochs,\n\t                save_every_epoch,\n\t                save_wav_with_checkpoint,\n\t                pre_trained_bottom_model_g,\n\t                pre_trained_bottom_model_d,\n\t                embedder_name,\n\t                int(embedding_output_layer),\n\t                False,\n\t                None if len(gpu_ids) > 1 else device,\n", "            )\n\t            yield \"Training index...\"\n\t            if run_train_index:\n\t                if not reduce_index_size:\n\t                    maximum_index_size = None\n\t                train_index(\n\t                    training_dir,\n\t                    model_name,\n\t                    out_dir,\n\t                    int(embedding_channels),\n", "                    num_cpu_process,\n\t                    maximum_index_size,\n\t                )\n\t            yield \"Training completed\"\n\t        with gr.Group():\n\t            with gr.Box():\n\t                with gr.Column():\n\t                    with gr.Row():\n\t                        with gr.Column():\n\t                            model_name = gr.Textbox(label=\"Model Name\")\n", "                            ignore_cache = gr.Checkbox(label=\"Ignore cache\")\n\t                        with gr.Column():\n\t                            dataset_glob = gr.Textbox(\n\t                                label=\"Dataset glob\", placeholder=\"data/**/*.wav\"\n\t                            )\n\t                            recursive = gr.Checkbox(label=\"Recursive\", value=True)\n\t                            multiple_speakers = gr.Checkbox(\n\t                                label=\"Multiple speakers\", value=False\n\t                            )\n\t                            speaker_id = gr.Slider(\n", "                                maximum=4,\n\t                                minimum=0,\n\t                                value=0,\n\t                                step=1,\n\t                                label=\"Speaker ID\",\n\t                            )\n\t                    with gr.Row(equal_height=False):\n\t                        version = gr.Radio(\n\t                            choices=[\"v1\", \"v2\"],\n\t                            value=\"v2\",\n", "                            label=\"Model version\",\n\t                        )\n\t                        target_sr = gr.Radio(\n\t                            choices=[\"32k\", \"40k\", \"48k\"],\n\t                            value=\"40k\",\n\t                            label=\"Target sampling rate\",\n\t                        )\n\t                        f0 = gr.Radio(\n\t                            choices=[\"Yes\", \"No\"],\n\t                            value=\"Yes\",\n", "                            label=\"f0 Model\",\n\t                        )\n\t                    with gr.Row(equal_height=False):\n\t                        embedding_name = gr.Radio(\n\t                            choices=list(models.EMBEDDINGS_LIST.keys()),\n\t                            value=\"contentvec\",\n\t                            label=\"Using phone embedder\",\n\t                        )\n\t                        embedding_channels = gr.Radio(\n\t                            choices=[\"256\", \"768\"],\n", "                            value=\"768\",\n\t                            label=\"Embedding channels\",\n\t                        )\n\t                        embedding_output_layer = gr.Radio(\n\t                            choices=[\"9\", \"12\"],\n\t                            value=\"12\",\n\t                            label=\"Embedding output layer\",\n\t                        )\n\t                    with gr.Row(equal_height=False):\n\t                        gpu_id = gr.Textbox(\n", "                            label=\"GPU ID\",\n\t                            value=\", \".join([f\"{x.index}\" for x in utils.get_gpus()]),\n\t                        )\n\t                        num_cpu_process = gr.Slider(\n\t                            minimum=0,\n\t                            maximum=cpu_count(),\n\t                            step=1,\n\t                            value=math.ceil(cpu_count() / 2),\n\t                            label=\"Number of CPU processes\",\n\t                        )\n", "                        norm_audio_when_preprocess = gr.Radio(\n\t                            choices=[\"Yes\", \"No\"],\n\t                            value=\"Yes\",\n\t                            label=\"Normalize audio volume when preprocess\",\n\t                        )\n\t                        pitch_extraction_algo = gr.Radio(\n\t                            choices=[\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"],\n\t                            value=\"crepe\",\n\t                            label=\"Pitch extraction algorithm\",\n\t                        )\n", "                    with gr.Row(equal_height=False):\n\t                        batch_size = gr.Number(value=4, label=\"Batch size\")\n\t                        num_epochs = gr.Number(\n\t                            value=30,\n\t                            label=\"Number of epochs\",\n\t                        )\n\t                        save_every_epoch = gr.Slider(\n\t                            minimum=0,\n\t                            maximum=100,\n\t                            value=10,\n", "                            step=1,\n\t                            label=\"Save every epoch\",\n\t                        )\n\t                        save_wav_with_checkpoint = gr.Checkbox(\n\t                            label=\"save_wav_with_checkpoint\", value=False\n\t                        )\n\t                        cache_batch = gr.Checkbox(label=\"Cache batch\", value=True)\n\t                        fp16 = gr.Checkbox(\n\t                            label=\"FP16\", value=half_support, disabled=not half_support\n\t                        )\n", "                    with gr.Row(equal_height=False):\n\t                        augment = gr.Checkbox(label=\"Augment\", value=False)\n\t                        augment_from_pretrain = gr.Checkbox(\n\t                            label=\"Augment From Pretrain\", value=False\n\t                        )\n\t                        augment_path = gr.Textbox(\n\t                            label=\"Pre trained generator path (pth)\",\n\t                            value=\"file is not prepared\",\n\t                        )\n\t                        speaker_info_path = gr.Textbox(\n", "                            label=\"speaker info path (npy)\",\n\t                            value=\"file is not prepared\",\n\t                        )\n\t                    with gr.Row(equal_height=False):\n\t                        pre_trained_generator = gr.Textbox(\n\t                            label=\"Pre trained generator path\",\n\t                            value=os.path.join(\n\t                                MODELS_DIR, \"pretrained\", \"v2\", \"f0G40k.pth\"\n\t                            ),\n\t                        )\n", "                        pre_trained_discriminator = gr.Textbox(\n\t                            label=\"Pre trained discriminator path\",\n\t                            value=os.path.join(\n\t                                MODELS_DIR, \"pretrained\", \"v2\", \"f0D40k.pth\"\n\t                            ),\n\t                        )\n\t                    with gr.Row(equal_height=False):\n\t                        run_train_index = gr.Radio(\n\t                            choices=[\"Yes\", \"No\"],\n\t                            value=\"Yes\",\n", "                            label=\"Train Index\",\n\t                        )\n\t                        reduce_index_size = gr.Radio(\n\t                            choices=[\"Yes\", \"No\"],\n\t                            value=\"No\",\n\t                            label=\"Reduce index size with kmeans\",\n\t                        )\n\t                        maximum_index_size = gr.Number(\n\t                            value=10000, label=\"maximum index size\"\n\t                        )\n", "                    with gr.Row(equal_height=False):\n\t                        status = gr.Textbox(value=\"\", label=\"Status\")\n\t                    with gr.Row(equal_height=False):\n\t                        train_index_button = gr.Button(\"Train Index\", variant=\"primary\")\n\t                        train_all_button = gr.Button(\"Train\", variant=\"primary\")\n\t        train_index_button.click(\n\t            train_index_only,\n\t            inputs=[\n\t                model_name,\n\t                target_sr,\n", "                f0,\n\t                dataset_glob,\n\t                recursive,\n\t                multiple_speakers,\n\t                speaker_id,\n\t                gpu_id,\n\t                num_cpu_process,\n\t                norm_audio_when_preprocess,\n\t                pitch_extraction_algo,\n\t                run_train_index,\n", "                reduce_index_size,\n\t                maximum_index_size,\n\t                embedding_name,\n\t                embedding_channels,\n\t                embedding_output_layer,\n\t                ignore_cache,\n\t            ],\n\t            outputs=[status],\n\t        )\n\t        train_all_button.click(\n", "            train_all,\n\t            inputs=[\n\t                model_name,\n\t                version,\n\t                target_sr,\n\t                f0,\n\t                dataset_glob,\n\t                recursive,\n\t                multiple_speakers,\n\t                speaker_id,\n", "                gpu_id,\n\t                num_cpu_process,\n\t                norm_audio_when_preprocess,\n\t                pitch_extraction_algo,\n\t                batch_size,\n\t                augment,\n\t                augment_from_pretrain,\n\t                augment_path,\n\t                speaker_info_path,\n\t                cache_batch,\n", "                num_epochs,\n\t                save_every_epoch,\n\t                save_wav_with_checkpoint,\n\t                fp16,\n\t                pre_trained_generator,\n\t                pre_trained_discriminator,\n\t                run_train_index,\n\t                reduce_index_size,\n\t                maximum_index_size,\n\t                embedding_name,\n", "                embedding_channels,\n\t                embedding_output_layer,\n\t                ignore_cache,\n\t            ],\n\t            outputs=[status],\n\t        )\n"]}
{"filename": "modules/tabs/server.py", "chunked_list": ["import io\n\timport json\n\timport gradio as gr\n\timport requests\n\timport soundfile as sf\n\timport torch.multiprocessing as multiprocessing\n\tfrom scipy.io.wavfile import write\n\tfrom modules.ui import Tab\n\tfrom server import app\n\tproc = None\n", "def server_options_ui(show_out_dir=True):\n\t    with gr.Row().style(equal_height=False):\n\t        with gr.Row():\n\t            host = gr.Textbox(value=\"127.0.0.1\", label=\"host\")\n\t            port = gr.Textbox(value=\"5001\", label=\"port\")\n\t    with gr.Row().style(equal_height=False):\n\t        with gr.Row():\n\t            rvc_model_file = gr.Textbox(value=\"\", label=\"RVC model file path\")\n\t            faiss_index_file = gr.Textbox(value=\"\", label=\"Faiss index file path\")\n\t    with gr.Row().style(equal_height=False):\n", "        with gr.Row():\n\t            input_voice_file = gr.Textbox(value=\"\", label=\"input voice file path\")\n\t            speaker_id = gr.Number(\n\t                value=0,\n\t                label=\"speaker_id\",\n\t            )\n\t            transpose = gr.Slider(\n\t                minimum=-20, maximum=20, value=0, step=1, label=\"transpose\"\n\t            )\n\t            pitch_extraction_algo = gr.Radio(\n", "                choices=[\"dio\", \"harvest\", \"mangio-crepe\", \"crepe\"],\n\t                value=\"crepe\",\n\t                label=\"pitch_extraction_algo\",\n\t            )\n\t            retrieval_feature_ratio = gr.Slider(\n\t                minimum=0,\n\t                maximum=1,\n\t                value=1,\n\t                step=0.01,\n\t                label=\"retrieval_feature_ratio\",\n", "            )\n\t    return (\n\t        host,\n\t        port,\n\t        rvc_model_file,\n\t        faiss_index_file,\n\t        input_voice_file,\n\t        speaker_id,\n\t        transpose,\n\t        pitch_extraction_algo,\n", "        retrieval_feature_ratio,\n\t    )\n\tdef run(**kwargs):\n\t    app.run(**kwargs)\n\tclass Server(Tab):\n\t    def title(self):\n\t        return \"Server(experimental)\"\n\t    def sort(self):\n\t        return 6\n\t    def ui(self, outlet):\n", "        def start(host, port):\n\t            if multiprocessing.get_start_method() == 'fork':\n\t                multiprocessing.set_start_method('spawn', force=True)\n\t            proc = multiprocessing.Process(target = run, kwargs = {'host': host, 'port': port})\n\t            proc.start()\n\t            yield \"start server\"\n\t        def upload(host, port, rvc_model_file, faiss_index_file):\n\t            file_names = {\"rvc_model_file\": rvc_model_file, \"faiss_index_file\": faiss_index_file}\n\t            res = requests.post(f\"http://{host}:{port}/upload_model\", json=file_names)\n\t            yield res.text\n", "        def convert(host, port, input_voice_file, speaker_id, transpose, pitch_extraction_algo, retrieval_feature_ratio):\n\t            params = {\n\t                \"speaker_id\": speaker_id,\n\t                \"transpose\": transpose,\n\t                \"pitch_extraction_algo\": pitch_extraction_algo,\n\t                \"retrieval_feature_ratio\": retrieval_feature_ratio\n\t            }\n\t            audio, sr = sf.read(input_voice_file)\n\t            audio_buffer = io.BytesIO()\n\t            write(audio_buffer, rate=sr, data=audio)\n", "            json_buffer = io.BytesIO(json.dumps(params).encode('utf-8'))\n\t            files = {\n\t                \"input_wav\": audio_buffer,\n\t                \"params\": json_buffer\n\t            }\n\t            res = requests.post(f\"http://{host}:{port}/convert_sound\", files=files)\n\t            audio, sr = sf.read(io.BytesIO(res.content))\n\t            yield \"convert succeed\", (sr, audio)\n\t        with gr.Group():\n\t            with gr.Box():\n", "                with gr.Column():\n\t                    (\n\t                        host,\n\t                        port,\n\t                        rvc_model_file,\n\t                        faiss_index_file,\n\t                        input_voice_file,\n\t                        speaker_id,\n\t                        transpose,\n\t                        pitch_extraction_algo,\n", "                        retrieval_feature_ratio,\n\t                    ) = server_options_ui()\n\t                    with gr.Row().style(equal_height=False):\n\t                        with gr.Column():\n\t                            status = gr.Textbox(value=\"\", label=\"Status\")\n\t                            output = gr.Audio(label=\"Output\", interactive=False)\n\t                    with gr.Row():\n\t                        start_button = gr.Button(\"Start server\", variant=\"primary\")\n\t                        upload_button = gr.Button(\"Upload Model\")\n\t                        convert_button = gr.Button(\"Convert Voice\")\n", "        start_button.click(\n\t            start,\n\t            inputs=[\n\t                host,\n\t                port\n\t            ],\n\t            outputs=[status],\n\t            queue=True,\n\t        )\n\t        upload_button.click(\n", "            upload,\n\t            inputs=[\n\t                host,\n\t                port,\n\t                rvc_model_file,\n\t                faiss_index_file\n\t            ],\n\t            outputs=[status],\n\t            queue=True,\n\t        )\n", "        convert_button.click(\n\t            convert,\n\t            inputs=[\n\t                host,\n\t                port,\n\t                input_voice_file,\n\t                speaker_id,\n\t                transpose,\n\t                pitch_extraction_algo,\n\t                retrieval_feature_ratio\n", "            ],\n\t            outputs=[status, output],\n\t            queue=True,\n\t        )\n"]}
