{"filename": "setup.py", "chunked_list": ["\"\"\"Setup. Required for the installation in editable mode.\"\"\"\n\tfrom setuptools import setup\n\tsetup()\n"]}
{"filename": "tests/__init__.py", "chunked_list": ["\"\"\"Test configuration.\"\"\"\n"]}
{"filename": "tests/test_lanczos/test_bidiagonal_full_reortho.py", "chunked_list": ["\"\"\"Tests for GKL bidiagonalisation.\"\"\"\n\tfrom matfree import decomp, lanczos, test_util\n\tfrom matfree.backend import linalg, np, prng, testing\n\t@testing.fixture()\n\tdef A(nrows, ncols, num_significant_singular_vals):\n\t    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n\t    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n\t    n = min(nrows, ncols)\n\t    d = np.arange(n) + 10.0\n\t    d = d.at[num_significant_singular_vals:].set(0.001)\n", "    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)\n\t@testing.parametrize(\"nrows\", [50])\n\t@testing.parametrize(\"ncols\", [49])\n\t@testing.parametrize(\"num_significant_singular_vals\", [4])\n\t@testing.parametrize(\"order\", [6])  # ~1.5 * num_significant_eigvals\n\tdef test_bidiagonal_full_reortho(A, order):\n\t    \"\"\"Test that Lanczos tridiagonalisation yields an orthogonal-tridiagonal decomp.\"\"\"\n\t    nrows, ncols = np.shape(A)\n\t    key = prng.prng_key(1)\n\t    v0 = prng.normal(key, shape=(ncols,))\n", "    alg = lanczos.bidiagonal_full_reortho(order, matrix_shape=np.shape(A))\n\t    def Av(v):\n\t        return A @ v\n\t    def vA(v):\n\t        return v @ A\n\t    Us, Bs, Vs, (b, v) = decomp.decompose_fori_loop(v0, Av, vA, algorithm=alg)\n\t    (d_m, e_m) = Bs\n\t    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\t    assert np.shape(Us) == (nrows, order + 1)\n\t    assert np.allclose(Us.T @ Us, np.eye(order + 1), **tols_decomp), Us.T @ Us\n", "    assert np.shape(Vs) == (order + 1, ncols)\n\t    assert np.allclose(Vs @ Vs.T, np.eye(order + 1), **tols_decomp), Vs @ Vs.T\n\t    UAVt = Us.T @ A @ Vs.T\n\t    assert np.allclose(linalg.diagonal(UAVt), d_m, **tols_decomp)\n\t    assert np.allclose(linalg.diagonal(UAVt, 1), e_m, **tols_decomp)\n\t    B = _bidiagonal_dense(d_m, e_m)\n\t    assert np.shape(B) == (order + 1, order + 1)\n\t    assert np.allclose(UAVt, B, **tols_decomp)\n\t    em = np.eye(order + 1)[:, -1]\n\t    AVt = A @ Vs.T\n", "    UtB = Us @ B\n\t    AtUt = A.T @ Us\n\t    VtBtb_plus_bve = Vs.T @ B.T + b * v[:, None] @ em[None, :]\n\t    assert np.allclose(AVt, UtB, **tols_decomp)\n\t    assert np.allclose(AtUt, VtBtb_plus_bve, **tols_decomp)\n\tdef _bidiagonal_dense(d, e):\n\t    diag = linalg.diagonal_matrix(d)\n\t    offdiag = linalg.diagonal_matrix(e, 1)\n\t    return diag + offdiag\n\t@testing.parametrize(\"nrows\", [5])\n", "@testing.parametrize(\"ncols\", [3])\n\t@testing.parametrize(\"num_significant_singular_vals\", [3])\n\tdef test_error_too_high_depth(A):\n\t    \"\"\"Assert that a ValueError is raised when the depth exceeds the matrix size.\"\"\"\n\t    nrows, ncols = np.shape(A)\n\t    max_depth = min(nrows, ncols) - 1\n\t    with testing.raises(ValueError):\n\t        _ = lanczos.bidiagonal_full_reortho(max_depth + 1, matrix_shape=np.shape(A))\n\t@testing.parametrize(\"nrows\", [5])\n\t@testing.parametrize(\"ncols\", [3])\n", "@testing.parametrize(\"num_significant_singular_vals\", [3])\n\tdef test_error_too_low_depth(A):\n\t    \"\"\"Assert that a ValueError is raised when the depth is negative.\"\"\"\n\t    min_depth = 0\n\t    with testing.raises(ValueError):\n\t        _ = lanczos.bidiagonal_full_reortho(min_depth - 1, matrix_shape=np.shape(A))\n\t@testing.parametrize(\"nrows\", [15])\n\t@testing.parametrize(\"ncols\", [3])\n\t@testing.parametrize(\"num_significant_singular_vals\", [3])\n\tdef test_no_error_zero_depth(A):\n", "    \"\"\"Assert the corner case of zero-depth does not raise an error.\"\"\"\n\t    nrows, ncols = np.shape(A)\n\t    algorithm = lanczos.bidiagonal_full_reortho(0, matrix_shape=np.shape(A))\n\t    key = prng.prng_key(1)\n\t    v0 = prng.normal(key, shape=(ncols,))\n\t    def Av(v):\n\t        return A @ v\n\t    def vA(v):\n\t        return v @ A\n\t    Us, Bs, Vs, (b, v) = decomp.decompose_fori_loop(v0, Av, vA, algorithm=algorithm)\n", "    (d_m, e_m) = Bs\n\t    assert np.shape(Us) == (nrows, 1)\n\t    assert np.shape(Vs) == (1, ncols)\n\t    assert np.shape(d_m) == (1,)\n\t    assert np.shape(e_m) == (0,)\n\t    assert np.shape(b) == ()\n\t    assert np.shape(v) == (ncols,)\n"]}
{"filename": "tests/test_lanczos/test_tridiagonal_full_reortho.py", "chunked_list": ["\"\"\"Tests for Lanczos functionality.\"\"\"\n\tfrom matfree import decomp, lanczos, test_util\n\tfrom matfree.backend import linalg, np, prng, testing\n\t@testing.fixture()\n\tdef A(n, num_significant_eigvals):\n\t    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n\t    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n\t    d = np.arange(n) + 10.0\n\t    d = d.at[num_significant_eigvals:].set(0.001)\n\t    return test_util.symmetric_matrix_from_eigenvalues(d)\n", "@testing.parametrize(\"n\", [6])\n\t@testing.parametrize(\"num_significant_eigvals\", [6])\n\tdef test_max_order(A):\n\t    \"\"\"If m == n, the matrix should be equal to the full tridiagonal.\"\"\"\n\t    n, _ = np.shape(A)\n\t    order = n - 1\n\t    key = prng.prng_key(1)\n\t    v0 = prng.normal(key, shape=(n,))\n\t    alg = lanczos.tridiagonal_full_reortho(order)\n\t    Q, (d_m, e_m) = decomp.decompose_fori_loop(v0, lambda v: A @ v, algorithm=alg)\n", "    # Lanczos is not stable.\n\t    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\t    # Since full-order mode: Q must be unitary\n\t    assert np.shape(Q) == (order + 1, n)\n\t    assert np.allclose(Q @ Q.T, np.eye(n), **tols_decomp), Q @ Q.T\n\t    assert np.allclose(Q.T @ Q, np.eye(n), **tols_decomp), Q.T @ Q\n\t    # T = Q A Qt\n\t    T = _sym_tridiagonal_dense(d_m, e_m)\n\t    QAQt = Q @ A @ Q.T\n\t    assert np.shape(T) == (order + 1, order + 1)\n", "    # Fail early if the (off)diagonals don't coincide\n\t    assert np.allclose(linalg.diagonal(QAQt), d_m, **tols_decomp)\n\t    assert np.allclose(linalg.diagonal(QAQt, 1), e_m, **tols_decomp)\n\t    assert np.allclose(linalg.diagonal(QAQt, -1), e_m, **tols_decomp)\n\t    # Test the full decomposition\n\t    # (i.e. assert that the off-tridiagonal elements are actually small)\n\t    # be loose with this test. off-diagonal elements accumulate quickly.\n\t    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\t    assert np.allclose(QAQt, T, **tols_decomp)\n\t    # Since full-order mode: Qt T Q = A\n", "    # Since Q is unitary and T = Q A Qt, this test\n\t    # should always pass.\n\t    assert np.allclose(Q.T @ T @ Q, A, **tols_decomp)\n\t@testing.parametrize(\"n\", [50])\n\t@testing.parametrize(\"num_significant_eigvals\", [4])\n\t@testing.parametrize(\"order\", [6])  # ~1.5 * num_significant_eigvals\n\tdef test_identity(A, order):\n\t    \"\"\"Test that Lanczos tridiagonalisation yields an orthogonal-tridiagonal decomp.\"\"\"\n\t    n, _ = np.shape(A)\n\t    key = prng.prng_key(1)\n", "    v0 = prng.normal(key, shape=(n,))\n\t    alg = lanczos.tridiagonal_full_reortho(order)\n\t    Q, tridiag = decomp.decompose_fori_loop(v0, lambda v: A @ v, algorithm=alg)\n\t    (d_m, e_m) = tridiag\n\t    # Lanczos is not stable.\n\t    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\t    assert np.shape(Q) == (order + 1, n)\n\t    assert np.allclose(Q @ Q.T, np.eye(order + 1), **tols_decomp), Q @ Q.T\n\t    # T = Q A Qt\n\t    T = _sym_tridiagonal_dense(d_m, e_m)\n", "    QAQt = Q @ A @ Q.T\n\t    assert np.shape(T) == (order + 1, order + 1)\n\t    # Fail early if the (off)diagonals don't coincide\n\t    assert np.allclose(linalg.diagonal(QAQt), d_m, **tols_decomp)\n\t    assert np.allclose(linalg.diagonal(QAQt, 1), e_m, **tols_decomp)\n\t    assert np.allclose(linalg.diagonal(QAQt, -1), e_m, **tols_decomp)\n\t    # Test the full decomposition\n\t    assert np.allclose(QAQt, T, **tols_decomp)\n\tdef _sym_tridiagonal_dense(d, e):\n\t    diag = linalg.diagonal_matrix(d)\n", "    offdiag1 = linalg.diagonal_matrix(e, 1)\n\t    offdiag2 = linalg.diagonal_matrix(e, -1)\n\t    return diag + offdiag1 + offdiag2\n"]}
{"filename": "tests/test_lanczos/__init__.py", "chunked_list": ["\"\"\"Decomposition tests.\"\"\"\n"]}
{"filename": "tests/test_slq/test_logdet_spd_autodiff.py", "chunked_list": ["\"\"\"Tests for (selected) autodiff functionality.\"\"\"\n\tfrom matfree import montecarlo, slq, test_util\n\tfrom matfree.backend import np, prng, testing\n\t@testing.fixture()\n\tdef A(n, num_significant_eigvals):\n\t    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n\t    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n\t    d = np.arange(n) + 10.0\n\t    d = d.at[num_significant_eigvals:].set(0.001)\n\t    return test_util.symmetric_matrix_from_eigenvalues(d)\n", "@testing.parametrize(\"n\", [200])\n\t@testing.parametrize(\"num_significant_eigvals\", [30])\n\t@testing.parametrize(\"order\", [10])\n\t# usually: ~1.5 * num_significant_eigvals.\n\t# But logdet seems to converge sooo much faster.\n\tdef test_check_grads(A, order):\n\t    \"\"\"Assert that log-determinant computation admits valid VJPs and JVPs.\"\"\"\n\t    key = prng.prng_key(1)\n\t    def fun(s):\n\t        return _logdet(s, order, key)\n", "    testing.check_grads(fun, (A,), order=1, atol=1e-1, rtol=1e-1)\n\tdef _logdet(A, order, key):\n\t    n, _ = np.shape(A)\n\t    fun = montecarlo.normal(shape=(n,))\n\t    return slq.logdet_spd(\n\t        order,\n\t        lambda v: A @ v,\n\t        key=key,\n\t        num_samples_per_batch=10,\n\t        num_batches=1,\n", "        sample_fun=fun,\n\t    )\n"]}
{"filename": "tests/test_slq/test_schatten_norm.py", "chunked_list": ["\"\"\"Test Schatten norm implementations.\"\"\"\n\tfrom matfree import montecarlo, slq, test_util\n\tfrom matfree.backend import linalg, np, prng, testing\n\t@testing.fixture()\n\tdef A(nrows, ncols, num_significant_singular_vals):\n\t    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n\t    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n\t    n = min(nrows, ncols)\n\t    d = np.arange(n) + 1.0\n\t    d = d.at[num_significant_singular_vals:].set(0.001)\n", "    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)\n\t@testing.parametrize(\"nrows\", [30])\n\t@testing.parametrize(\"ncols\", [30])\n\t@testing.parametrize(\"num_significant_singular_vals\", [30])\n\t@testing.parametrize(\"order\", [20])\n\t@testing.parametrize(\"power\", [1, 2, 5])\n\tdef test_schatten_norm(A, order, power):\n\t    \"\"\"Assert that schatten_norm yields an accurate estimate.\"\"\"\n\t    _, s, _ = linalg.svd(A, full_matrices=False)\n\t    expected = np.sum(s**power) ** (1 / power)\n", "    _, ncols = np.shape(A)\n\t    key = prng.prng_key(1)\n\t    fun = montecarlo.normal(shape=(ncols,))\n\t    received = slq.schatten_norm(\n\t        order,\n\t        lambda v: A @ v,\n\t        lambda v: A.T @ v,\n\t        power=power,\n\t        matrix_shape=np.shape(A),\n\t        key=key,\n", "        num_samples_per_batch=100,\n\t        num_batches=5,\n\t        sample_fun=fun,\n\t    )\n\t    print_if_assert_fails = (\"error\", np.abs(received - expected), \"target:\", expected)\n\t    assert np.allclose(received, expected, atol=1e-2, rtol=1e-2), print_if_assert_fails\n"]}
{"filename": "tests/test_slq/test_logdet_spd.py", "chunked_list": ["\"\"\"Tests for Lanczos functionality.\"\"\"\n\tfrom matfree import montecarlo, slq, test_util\n\tfrom matfree.backend import linalg, np, prng, testing\n\t@testing.fixture()\n\tdef A(n, num_significant_eigvals):\n\t    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n\t    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n\t    d = np.arange(n) / n + 1.0\n\t    d = d.at[num_significant_eigvals:].set(0.001)\n\t    return test_util.symmetric_matrix_from_eigenvalues(d)\n", "@testing.parametrize(\"n\", [200])\n\t@testing.parametrize(\"num_significant_eigvals\", [30])\n\t@testing.parametrize(\"order\", [10])\n\t# usually: ~1.5 * num_significant_eigvals.\n\t# But logdet seems to converge sooo much faster.\n\tdef test_logdet_spd(A, order):\n\t    \"\"\"Assert that the log-determinant estimation matches the true log-determinant.\"\"\"\n\t    n, _ = np.shape(A)\n\t    key = prng.prng_key(1)\n\t    fun = montecarlo.normal(shape=(n,))\n", "    received = slq.logdet_spd(\n\t        order,\n\t        lambda v: A @ v,\n\t        key=key,\n\t        num_samples_per_batch=10,\n\t        num_batches=1,\n\t        sample_fun=fun,\n\t    )\n\t    expected = linalg.slogdet(A)[1]\n\t    print_if_assert_fails = (\"error\", np.abs(received - expected), \"target:\", expected)\n", "    assert np.allclose(received, expected, atol=1e-2, rtol=1e-2), print_if_assert_fails\n"]}
{"filename": "tests/test_slq/test_logdet_product.py", "chunked_list": ["\"\"\"Test slq.logdet_prod().\"\"\"\n\tfrom matfree import montecarlo, slq, test_util\n\tfrom matfree.backend import linalg, np, prng, testing\n\t@testing.fixture()\n\tdef A(nrows, ncols, num_significant_singular_vals):\n\t    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n\t    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n\t    n = min(nrows, ncols)\n\t    d = np.arange(n) + 1.0\n\t    d = d.at[num_significant_singular_vals:].set(0.001)\n", "    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)\n\t@testing.parametrize(\"nrows\", [50])\n\t@testing.parametrize(\"ncols\", [30])\n\t@testing.parametrize(\"num_significant_singular_vals\", [30])\n\t@testing.parametrize(\"order\", [20])\n\tdef test_logdet_product(A, order):\n\t    \"\"\"Assert that logdet_product yields an accurate estimate.\"\"\"\n\t    _, ncols = np.shape(A)\n\t    key = prng.prng_key(3)\n\t    fun = montecarlo.normal(shape=(ncols,))\n", "    received = slq.logdet_product(\n\t        order,\n\t        lambda v: A @ v,\n\t        lambda v: A.T @ v,\n\t        matrix_shape=np.shape(A),\n\t        key=key,\n\t        num_samples_per_batch=200,\n\t        num_batches=2,\n\t        sample_fun=fun,\n\t    )\n", "    expected = linalg.slogdet(A.T @ A)[1]\n\t    print_if_assert_fails = (\"error\", np.abs(received - expected), \"target:\", expected)\n\t    assert np.allclose(received, expected, atol=1e-2, rtol=1e-2), print_if_assert_fails\n"]}
{"filename": "tests/test_slq/__init__.py", "chunked_list": ["\"\"\"Tests for stochastic Lanczos quadrature.\"\"\"\n"]}
{"filename": "tests/test_montecarlo/test_van_der_corput.py", "chunked_list": ["\"\"\"Tests for Monte-Carlo machinery.\"\"\"\n\tfrom matfree import montecarlo\n\tfrom matfree.backend import np\n\tdef test_van_der_corput():\n\t    \"\"\"Assert that the van-der-Corput sequence yields values as expected.\"\"\"\n\t    expected = np.asarray([0, 0.5, 0.25, 0.75, 0.125, 0.625, 0.375, 0.875, 0.0625])\n\t    received = np.asarray([montecarlo.van_der_corput(i) for i in range(9)])\n\t    assert np.allclose(received, expected)\n\t    expected = np.asarray([0.0, 1 / 3, 2 / 3, 1 / 9, 4 / 9, 7 / 9, 2 / 9, 5 / 9, 8 / 9])\n\t    received = np.asarray([montecarlo.van_der_corput(i, base=3) for i in range(9)])\n", "    assert np.allclose(received, expected)\n"]}
{"filename": "tests/test_montecarlo/__init__.py", "chunked_list": ["\"\"\"Monte-Carlo tests.\"\"\"\n"]}
{"filename": "tests/test_montecarlo/test_estimate.py", "chunked_list": ["\"\"\"Tests for Monte-Carlo machinery.\"\"\"\n\tfrom matfree import montecarlo\n\tfrom matfree.backend import np, prng, testing\n\t@testing.parametrize(\"key\", [prng.prng_key(1)])\n\t@testing.parametrize(\"num_batches, num_samples\", [[1, 10_000], [10_000, 1], [100, 100]])\n\tdef test_mean(key, num_batches, num_samples):\n\t    \"\"\"Assert that the mean estimate is accurate.\"\"\"\n\t    def fun(x):\n\t        return x**2\n\t    received = montecarlo.estimate(\n", "        fun,\n\t        num_batches=num_batches,\n\t        num_samples_per_batch=num_samples,\n\t        key=key,\n\t        sample_fun=montecarlo.normal(shape=()),\n\t    )\n\t    assert np.allclose(received, 1.0, rtol=1e-1)\n"]}
{"filename": "tests/test_montecarlo/test_multiestimate.py", "chunked_list": ["\"\"\"Tests for Monte-Carlo machinery.\"\"\"\n\tfrom matfree import montecarlo\n\tfrom matfree.backend import np, prng, testing\n\t@testing.parametrize(\"key\", [prng.prng_key(1)])\n\t@testing.parametrize(\"num_batches, num_samples\", [[1, 10_000], [10_000, 1], [100, 100]])\n\tdef test_mean_and_max(key, num_batches, num_samples):\n\t    \"\"\"Assert that the mean estimate is accurate.\"\"\"\n\t    def fun(x):\n\t        return x**2\n\t    mean, amax = montecarlo.multiestimate(\n", "        fun,\n\t        num_batches=num_batches,\n\t        num_samples_per_batch=num_samples,\n\t        key=key,\n\t        sample_fun=montecarlo.normal(shape=()),\n\t        statistics_batch=[np.mean, np.array_max],\n\t        statistics_combine=[np.mean, np.array_max],\n\t    )\n\t    assert np.allclose(mean, 1.0, rtol=1e-1)\n\t    assert mean < amax\n"]}
{"filename": "tests/test_decomp/__init__.py", "chunked_list": ["\"\"\"Decomposition tests.\"\"\"\n"]}
{"filename": "tests/test_decomp/test_svd.py", "chunked_list": ["\"\"\"Tests for SVD functionality.\"\"\"\n\tfrom matfree import decomp, test_util\n\tfrom matfree.backend import linalg, np, testing\n\t@testing.fixture()\n\t@testing.parametrize(\"nrows\", [10])\n\t@testing.parametrize(\"ncols\", [3])\n\t@testing.parametrize(\"num_significant_singular_vals\", [3])\n\tdef A(nrows, ncols, num_significant_singular_vals):\n\t    \"\"\"Make a positive definite matrix with certain spectrum.\"\"\"\n\t    # 'Invent' a spectrum. Use the number of pre-defined eigenvalues.\n", "    n = min(nrows, ncols)\n\t    d = np.arange(n) + 10.0\n\t    d = d.at[num_significant_singular_vals:].set(0.001)\n\t    return test_util.asymmetric_matrix_from_singular_values(d, nrows=nrows, ncols=ncols)\n\tdef test_equal_to_linalg_svd(A):\n\t    \"\"\"The output of full-depth SVD should be equal (*) to linalg.svd().\n\t    (*) Note: The singular values should be identical,\n\t    and the orthogonal matrices should be orthogonal. They are not unique.\n\t    \"\"\"\n\t    nrows, ncols = np.shape(A)\n", "    depth = min(nrows, ncols) - 1\n\t    def Av(v):\n\t        return A @ v\n\t    def vA(v):\n\t        return v @ A\n\t    v0 = np.ones((ncols,))\n\t    U, S, Vt = decomp.svd(v0, depth, Av, vA, matrix_shape=np.shape(A))\n\t    U_, S_, Vt_ = linalg.svd(A, full_matrices=False)\n\t    tols_decomp = {\"atol\": 1e-5, \"rtol\": 1e-5}\n\t    assert np.allclose(S, S_)  # strict \"allclose\"\n", "    assert np.shape(U) == np.shape(U_)\n\t    assert np.shape(Vt) == np.shape(Vt_)\n\t    assert np.allclose(U @ U.T, U_ @ U_.T, **tols_decomp)\n\t    assert np.allclose(Vt @ Vt.T, Vt_ @ Vt_.T, **tols_decomp)\n"]}
{"filename": "tests/test_hutchinson/test_trace_and_diagonal.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\tfrom matfree import hutchinson, montecarlo\n\tfrom matfree.backend import func, linalg, np, prng, testing\n\t@testing.fixture(name=\"fun\")\n\tdef fixture_fun():\n\t    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\t    def f(x):\n\t        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\t    return f\n\t@testing.fixture(name=\"key\")\n", "def fixture_key():\n\t    \"\"\"Fix a pseudo-random number generator.\"\"\"\n\t    return prng.prng_key(seed=1)\n\t@testing.parametrize(\"num_samples\", [10_000])\n\t@testing.parametrize(\"dim\", [5])\n\t@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\n\tdef test_trace_and_diagonal(fun, key, num_samples, dim, sample_fun):\n\t    \"\"\"Assert that the estimated trace and diagonal approximations are accurate.\"\"\"\n\t    # Linearise function\n\t    x0 = prng.uniform(key, shape=(dim,))\n", "    _, jvp = func.linearize(fun, x0)\n\t    J = func.jacfwd(fun)(x0)\n\t    # Estimate the trace\n\t    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n\t    trace, diag = hutchinson.trace_and_diagonal(\n\t        jvp, key=key, num_levels=num_samples, sample_fun=fun\n\t    )\n\t    # Print errors if test fails\n\t    error_diag = linalg.vector_norm(diag - linalg.diagonal(J))\n\t    error_trace = linalg.vector_norm(trace - linalg.trace(J))\n", "    assert np.allclose(diag, linalg.diagonal(J), rtol=1e-2), error_diag\n\t    assert np.allclose(trace, linalg.trace(J), rtol=1e-2), error_trace\n"]}
{"filename": "tests/test_hutchinson/test_frobeniusnorm_squared.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\tfrom matfree import hutchinson, montecarlo\n\tfrom matfree.backend import func, linalg, np, prng, testing\n\t@testing.fixture(name=\"fun\")\n\tdef fixture_fun():\n\t    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\t    def f(x):\n\t        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\t    return f\n\t@testing.fixture(name=\"key\")\n", "def fixture_key():\n\t    \"\"\"Fix a pseudo-random number generator.\"\"\"\n\t    return prng.prng_key(seed=1)\n\t@testing.parametrize(\"num_batches\", [1_000])\n\t@testing.parametrize(\"num_samples_per_batch\", [1_000])\n\t@testing.parametrize(\"dim\", [1, 10])\n\t@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\n\tdef test_frobeniusnorm_squared(\n\t    fun, key, num_batches, num_samples_per_batch, dim, sample_fun\n\t):\n", "    \"\"\"Assert that the Frobenius norm estimate is accurate.\"\"\"\n\t    # Linearise function\n\t    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n\t    _, jvp = func.linearize(fun, x0)\n\t    J = func.jacfwd(fun)(x0)\n\t    # Estimate the trace\n\t    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n\t    estimate = hutchinson.frobeniusnorm_squared(\n\t        jvp,\n\t        num_batches=num_batches,\n", "        key=key,\n\t        num_samples_per_batch=num_samples_per_batch,\n\t        sample_fun=fun,\n\t    )\n\t    truth = linalg.trace(J.T @ J)\n\t    assert np.allclose(estimate, truth, rtol=1e-2)\n"]}
{"filename": "tests/test_hutchinson/test_trace_moments.py", "chunked_list": ["\"\"\"Tests for estimating traces.\"\"\"\n\tfrom matfree import hutchinson, montecarlo\n\tfrom matfree.backend import func, linalg, np, prng, testing\n\t@testing.fixture(name=\"key\")\n\tdef fixture_key():\n\t    \"\"\"Fix a pseudo-random number generator.\"\"\"\n\t    return prng.prng_key(seed=1)\n\t@testing.fixture(name=\"J_and_jvp\")\n\tdef fixture_J_and_jvp(key, dim):\n\t    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n", "    def fun(x):\n\t        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\t    # Linearise function\n\t    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n\t    _, jvp = func.linearize(fun, x0)\n\t    J = func.jacfwd(fun)(x0)\n\t    return J, jvp\n\t@testing.parametrize(\"num_batches\", [1_000])\n\t@testing.parametrize(\"num_samples_per_batch\", [1_000])\n\t@testing.parametrize(\"dim\", [1, 10])\n", "def test_variance_normal(J_and_jvp, key, num_batches, num_samples_per_batch, dim):\n\t    \"\"\"Assert that the estimated trace approximates the true trace accurately.\"\"\"\n\t    # Estimate the trace\n\t    J, jvp = J_and_jvp\n\t    fun = montecarlo.normal(shape=(dim,), dtype=float)\n\t    first, second = hutchinson.trace_moments(\n\t        jvp,\n\t        key=key,\n\t        num_batches=num_batches,\n\t        num_samples_per_batch=num_samples_per_batch,\n", "        sample_fun=fun,\n\t        moments=(1, 2),\n\t    )\n\t    # Assert the trace is correct\n\t    truth = linalg.trace(J)\n\t    assert np.allclose(first, truth, rtol=1e-2)\n\t    # Assert the variance is correct:\n\t    norm = linalg.matrix_norm(J, which=\"fro\") ** 2\n\t    assert np.allclose(second - first**2, norm * 2, rtol=1e-2)\n\t@testing.parametrize(\"num_batches\", [1_000])\n", "@testing.parametrize(\"num_samples_per_batch\", [1_000])\n\t@testing.parametrize(\"dim\", [1, 10])\n\tdef test_variance_rademacher(J_and_jvp, key, num_batches, num_samples_per_batch, dim):\n\t    \"\"\"Assert that the estimated trace approximates the true trace accurately.\"\"\"\n\t    # Estimate the trace\n\t    J, jvp = J_and_jvp\n\t    fun = montecarlo.rademacher(shape=(dim,), dtype=float)\n\t    first, second = hutchinson.trace_moments(\n\t        jvp,\n\t        key=key,\n", "        num_batches=num_batches,\n\t        num_samples_per_batch=num_samples_per_batch,\n\t        sample_fun=fun,\n\t        moments=(1, 2),\n\t    )\n\t    # Assert the trace is correct\n\t    truth = linalg.trace(J)\n\t    assert np.allclose(first, truth, rtol=1e-2)\n\t    # Assert the variance is correct:\n\t    norm = linalg.matrix_norm(J, which=\"fro\") ** 2\n", "    truth = 2 * (norm - linalg.trace(J**2))\n\t    assert np.allclose(second - first**2, truth, atol=1e-2, rtol=1e-2)\n"]}
{"filename": "tests/test_hutchinson/__init__.py", "chunked_list": ["\"\"\"Trace estimator tests.\"\"\"\n"]}
{"filename": "tests/test_hutchinson/test_diagonal.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\tfrom matfree import hutchinson, montecarlo\n\tfrom matfree.backend import func, linalg, np, prng, testing\n\t@testing.fixture(name=\"fun\")\n\tdef fixture_fun():\n\t    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\t    def f(x):\n\t        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\t    return f\n\t@testing.fixture(name=\"key\")\n", "def fixture_key():\n\t    \"\"\"Fix a pseudo-random number generator.\"\"\"\n\t    return prng.prng_key(seed=1)\n\t@testing.parametrize(\"num_batches\", [1_000])\n\t@testing.parametrize(\"num_samples_per_batch\", [1_000])\n\t@testing.parametrize(\"dim\", [1, 10])\n\t@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\n\tdef test_diagonal(fun, key, num_batches, num_samples_per_batch, dim, sample_fun):\n\t    \"\"\"Assert that the estimated diagonal approximates the true diagonal accurately.\"\"\"\n\t    # Linearise function\n", "    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n\t    _, jvp = func.linearize(fun, x0)\n\t    J = func.jacfwd(fun)(x0)\n\t    # Estimate the trace\n\t    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n\t    estimate = hutchinson.diagonal(\n\t        jvp,\n\t        num_batches=num_batches,\n\t        key=key,\n\t        num_samples_per_batch=num_samples_per_batch,\n", "        sample_fun=fun,\n\t    )\n\t    truth = linalg.diagonal(J)\n\t    assert np.allclose(estimate, truth, rtol=1e-2)\n"]}
{"filename": "tests/test_hutchinson/test_trace.py", "chunked_list": ["\"\"\"Tests for basic trace estimators.\"\"\"\n\tfrom matfree import hutchinson, montecarlo\n\tfrom matfree.backend import func, linalg, np, prng, testing\n\t@testing.fixture(name=\"fun\")\n\tdef fixture_fun():\n\t    \"\"\"Create a nonlinear, to-be-differentiated function.\"\"\"\n\t    def f(x):\n\t        return np.sin(np.flip(np.cos(x)) + 1.0) * np.sin(x) + 1.0\n\t    return f\n\t@testing.fixture(name=\"key\")\n", "def fixture_key():\n\t    \"\"\"Fix a pseudo-random number generator.\"\"\"\n\t    return prng.prng_key(seed=1)\n\t@testing.parametrize(\"num_batches\", [1_000])\n\t@testing.parametrize(\"num_samples_per_batch\", [1_000])\n\t@testing.parametrize(\"dim\", [1, 10])\n\t@testing.parametrize(\"sample_fun\", [montecarlo.normal, montecarlo.rademacher])\n\tdef test_trace(fun, key, num_batches, num_samples_per_batch, dim, sample_fun):\n\t    \"\"\"Assert that the estimated trace approximates the true trace accurately.\"\"\"\n\t    # Linearise function\n", "    x0 = prng.uniform(key, shape=(dim,))  # random lin. point\n\t    _, jvp = func.linearize(fun, x0)\n\t    J = func.jacfwd(fun)(x0)\n\t    # Estimate the trace\n\t    fun = sample_fun(shape=np.shape(x0), dtype=np.dtype(x0))\n\t    estimate = hutchinson.trace(\n\t        jvp,\n\t        num_batches=num_batches,\n\t        key=key,\n\t        num_samples_per_batch=num_samples_per_batch,\n", "        sample_fun=fun,\n\t    )\n\t    truth = linalg.trace(J)\n\t    assert np.allclose(estimate, truth, rtol=1e-2)\n"]}
{"filename": "docs/benchmarks/control_variates.py", "chunked_list": ["\"\"\"Control_variates benchmark.\n\tRuntime: ~10 seconds.\n\t\"\"\"\n\tfrom matfree import benchmark_util, hutchinson, montecarlo\n\tfrom matfree.backend import func, linalg, np, plt, prng, progressbar\n\tdef problem(n):\n\t    \"\"\"Create an example problem.\"\"\"\n\t    # This function has a Jacobian with x-shaped sparsity pattern\n\t    # We expect control variates to do pretty well\n\t    # (But I don't know why)\n", "    def f(x):\n\t        return np.sin(np.roll(np.sin(np.flip(np.cos(x)) + 1) ** 2, 1)) * np.sin(x**2)\n\t    key = prng.prng_key(seed=2)\n\t    x0 = prng.uniform(key, shape=(n,))\n\t    _, jvp = func.linearize(f, x0)\n\t    J = func.jacfwd(f)(x0)\n\t    trace = linalg.trace(J)\n\t    sample_fun = montecarlo.normal(shape=(n,), dtype=float)\n\t    return (jvp, trace, J), (key, sample_fun)\n\tif __name__ == \"__main__\":\n", "    dim = 100\n\t    num_samples = 2 ** np.arange(4, 10)\n\t    num_restarts = 5\n\t    (Av, trace, J), (k, sample_fun) = problem(dim)\n\t    error_fun = func.partial(benchmark_util.rmse_relative, expected=trace)\n\t    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n\t    @func.partial(func.jit, static_argnums=0)\n\t    def fun1(num, key):\n\t        \"\"\"Estimate the trace conventionally.\"\"\"\n\t        return hutchinson.trace(\n", "            Av, key=key, sample_fun=sample_fun, num_batches=num, num_samples_per_batch=1\n\t        )\n\t    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n\t    @func.partial(func.jit, static_argnums=0)\n\t    def fun2(num, key):\n\t        \"\"\"Estimate trace and diagonal jointly and discard the diagonal.\"\"\"\n\t        trace2, _ = hutchinson.trace_and_diagonal(\n\t            Av, key=key, num_levels=num, sample_fun=sample_fun\n\t        )\n\t        return trace2\n", "    errors1, stds1, times1 = [], [], []\n\t    errors2, stds2, times2 = [], [], []\n\t    for n in progressbar.progressbar(num_samples):\n\t        test_keys = prng.split(k, num=num_restarts)\n\t        e1, t1 = zip(*[fun1(int(n), ke) for ke in test_keys])\n\t        e2, t2 = zip(*[fun2(int(n), ke) for ke in test_keys])\n\t        errors1.append(np.mean(np.asarray(e1)))\n\t        stds1.append(np.std(np.asarray(e1)))\n\t        times1.append(min(t1))\n\t        errors2.append(np.mean(np.asarray(e2)))\n", "        stds2.append(np.std(np.asarray(e2)))\n\t        times2.append(min(t2))\n\t    errors1 = np.asarray(errors1)\n\t    stds1 = np.asarray(stds1)\n\t    times1 = np.asarray(times1)\n\t    errors2 = np.asarray(errors2)\n\t    stds2 = np.asarray(stds2)\n\t    times2 = np.asarray(times2)\n\t    fig, axes = plt.subplots(ncols=2, tight_layout=True, figsize=(10, 5), dpi=100)\n\t    ax_wp, ax_sparsity = axes\n", "    ax_wp.set_title(\"Trace estimation\")\n\t    ax_wp.loglog(\n\t        times1,\n\t        errors1,\n\t        \"o-\",\n\t        markeredgecolor=\"k\",\n\t        label=\"Conventional trace estimation\",\n\t    )\n\t    ax_wp.fill_between(\n\t        times1,\n", "        errors1 - stds1,  # type: ignore\n\t        errors1 + stds1,  # type: ignore\n\t        alpha=0.25,\n\t    )\n\t    ax_wp.loglog(\n\t        times2,\n\t        errors2,\n\t        \"^-\",\n\t        markeredgecolor=\"k\",\n\t        label=\"Joint trace and diagonal estimation\",\n", "    )\n\t    ax_wp.fill_between(\n\t        times2,\n\t        errors2 - stds2,  # type: ignore\n\t        errors2 + stds2,  # type: ignore\n\t        alpha=0.25,\n\t    )\n\t    ax_wp.set_ylabel(f\"Relative error (mean & std, {num_restarts} restarts)\")\n\t    ax_wp.set_xlabel(f\"Wall clock time (minimum, {num_restarts} restarts)\")\n\t    ax_wp.legend()\n", "    ax_sparsity.set_title(\"Sparsity pattern of Jacobian\")\n\t    ax_sparsity.spy(J)\n\t    ax_sparsity.set_xticks(())\n\t    ax_sparsity.set_yticks(())\n\t    plt.show()\n"]}
{"filename": "docs/benchmarks/jacobian_squared.py", "chunked_list": ["\"\"\"What is the fastest way of computing trace(A^5).\"\"\"\n\tfrom matfree import benchmark_util, hutchinson, montecarlo, slq\n\tfrom matfree.backend import func, linalg, np, plt, prng\n\tfrom matfree.backend.progressbar import progressbar\n\tdef problem(n):\n\t    \"\"\"Create an example problem.\"\"\"\n\t    # This function has a Jacobian with x-shaped sparsity pattern\n\t    # We expect control variates to do pretty well\n\t    # (But I don't know why)\n\t    def f(x):\n", "        return np.sin(np.roll(np.sin(np.flip(np.cos(x)) + 1) ** 2, 2)) * np.sin(x**2)\n\t    key = prng.prng_key(seed=2)\n\t    x0 = prng.uniform(key, shape=(n,))\n\t    _, jvp = func.linearize(f, x0)\n\t    J = func.jacfwd(f)(x0)\n\t    A = J @ J @ J @ J\n\t    trace = linalg.trace(A)\n\t    sample_fun = montecarlo.normal(shape=(n,), dtype=float)\n\t    def Av(v):\n\t        return jvp(jvp(jvp(jvp(v))))\n", "    return (lambda x: x**4, jvp, Av, trace, A), (key, sample_fun)\n\tdef evaluate_all(fun, outer_loop, inner_loop):\n\t    \"\"\"Evaluate all metrics of a function.\"\"\"\n\t    errors, stds, times = [], [], []\n\t    for n in outer_loop:\n\t        fun_bind = func.partial(fun, int(n))\n\t        err, tim = evaluate(fun_bind, inner_loop)\n\t        errors.append(np.mean(err))\n\t        stds.append(np.std(err))\n\t        times.append(np.array_min(tim))\n", "    return np.asarray(errors), np.asarray(stds), np.asarray(times)\n\tdef evaluate(fun, keys):\n\t    \"\"\"Evaluate all metrics of a function.\"\"\"\n\t    errors, times = zip(*[fun(key) for key in keys])\n\t    return np.asarray(errors), np.asarray(times)\n\tif __name__ == \"__main__\":\n\t    dim = 10\n\t    num_samples = 2 ** np.arange(4, 12)\n\t    num_restarts = 10\n\t    (matfun, jvp, Av, trace, JJ), (k, sample_fun) = problem(dim)\n", "    x = hutchinson.trace(Av, key=k, sample_fun=sample_fun)\n\t    y = slq.trace_of_matfun_spd(matfun, jvp, 5, key=k, sample_fun=sample_fun)\n\t    assert np.allclose(x, trace, atol=1e-1, rtol=1e-1), (x, trace)\n\t    assert np.allclose(y, trace, atol=1e-1, rtol=1e-1), (y, trace)\n\t    error_fun = func.partial(benchmark_util.rmse_relative, expected=trace)\n\t    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n\t    @func.partial(func.jit, static_argnums=0)\n\t    def matvec(num, key):\n\t        \"\"\"Matrix-vector mult.\"\"\"\n\t        return hutchinson.trace(\n", "            Av, key=key, sample_fun=sample_fun, num_samples_per_batch=num\n\t        )\n\t    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n\t    @func.partial(func.jit, static_argnums=0)\n\t    def slq_low(num, key):\n\t        \"\"\"SLQ(1)\"\"\"  # noqa: D400,D415\n\t        return slq.trace_of_matfun_spd(\n\t            matfun,\n\t            jvp,\n\t            1,\n", "            key=key,\n\t            sample_fun=sample_fun,\n\t            num_samples_per_batch=num,\n\t        )\n\t    @func.partial(benchmark_util.error_and_time, error_fun=error_fun)\n\t    @func.partial(func.jit, static_argnums=0)\n\t    def slq_high(num, key):\n\t        \"\"\"SLQ(5)\"\"\"  # noqa: D400,D415\n\t        return slq.trace_of_matfun_spd(\n\t            matfun,\n", "            jvp,\n\t            5,\n\t            key=key,\n\t            sample_fun=sample_fun,\n\t            num_samples_per_batch=num,\n\t        )\n\t    test_keys = prng.split(k, num=num_restarts)\n\t    fig, axes = plt.subplots(ncols=2, tight_layout=True, figsize=(10, 5), dpi=100)\n\t    ax_wp, ax_sparsity = axes\n\t    ax_wp.set_title(\"Estimating trace(A^2)\")\n", "    for fun in [matvec, slq_low, slq_high]:\n\t        errors, stds, times = evaluate_all(fun, progressbar(num_samples), test_keys)\n\t        ax_wp.loglog(\n\t            times,\n\t            errors,\n\t            \"o-\",\n\t            markeredgecolor=\"k\",\n\t            label=fun.__doc__,\n\t        )\n\t        ax_wp.fill_between(times, errors - stds, errors + stds, alpha=0.25)\n", "    ax_wp.set_ylabel(f\"Relative error (mean & std, {num_restarts} restarts)\")\n\t    ax_wp.set_xlabel(f\"Wall clock time (minimum, {num_restarts} restarts)\")\n\t    ax_wp.legend()\n\t    ax_sparsity.set_title(\"Sparsity pattern of A^2\")\n\t    ax_sparsity.spy(JJ)\n\t    ax_sparsity.set_xticks(())\n\t    ax_sparsity.set_yticks(())\n\t    plt.show()\n"]}
{"filename": "matfree/test_util.py", "chunked_list": ["\"\"\"Test utilities.\"\"\"\n\tfrom matfree.backend import linalg, np\n\tdef symmetric_matrix_from_eigenvalues(eigvals, /):\n\t    \"\"\"Generate a symmetric matrix with prescribed eigenvalues.\"\"\"\n\t    assert np.array_min(eigvals) > 0\n\t    (n,) = eigvals.shape\n\t    # Need _some_ matrix to start with\n\t    A = np.reshape(np.arange(1.0, n**2 + 1.0), (n, n))\n\t    A = A / linalg.matrix_norm(A, which=\"fro\")\n\t    X = A.T @ A + np.eye(n)\n", "    # QR decompose. We need the orthogonal matrix.\n\t    # Treat Q as a stack of eigenvectors.\n\t    Q, R = linalg.qr(X)\n\t    # Treat Q as eigenvectors, and 'D' as eigenvalues.\n\t    # return Q D Q.T.\n\t    # This matrix will be dense, symmetric, and have a given spectrum.\n\t    return Q @ (eigvals[:, None] * Q.T)\n\tdef asymmetric_matrix_from_singular_values(vals, /, nrows, ncols):\n\t    \"\"\"Generate an asymmetric matrix with specific singular values.\"\"\"\n\t    assert np.array_min(vals) > 0\n", "    A = np.reshape(np.arange(1.0, nrows * ncols + 1.0), (nrows, ncols))\n\t    A /= nrows * ncols\n\t    U, S, Vt = linalg.svd(A, full_matrices=False)\n\t    return U @ linalg.diagonal(vals) @ Vt\n"]}
{"filename": "matfree/montecarlo.py", "chunked_list": ["\"\"\"Monte-Carlo estimation.\"\"\"\n\tfrom matfree.backend import containers, control_flow, func, np, prng\n\tfrom matfree.backend.typing import Array, Callable, Sequence\n\t# todo: allow a fun() that returns pytrees instead of arrays.\n\t#  why? Because then we rival trace_and_variance as\n\t#  trace_and_frobeniusnorm(): y=Ax; return (x@y, y@y)\n\tdef estimate(\n\t    fun: Callable,\n\t    /,\n\t    *,\n", "    key: Array,\n\t    sample_fun: Callable,\n\t    num_batches: int = 1,\n\t    num_samples_per_batch: int = 10_000,\n\t    statistic_batch: Callable = np.mean,\n\t    statistic_combine: Callable = np.mean,\n\t) -> Array:\n\t    \"\"\"Monte-Carlo estimation: Compute the expected value of a function.\n\t    Parameters\n\t    ----------\n", "    fun:\n\t        Function whose expected value shall be estimated.\n\t    key:\n\t        Pseudo-random number generator key.\n\t    sample_fun:\n\t        Sampling function.\n\t        For trace-estimation, use\n\t        either [montecarlo.normal(...)][matfree.montecarlo.normal]\n\t        or [montecarlo.rademacher(...)][matfree.montecarlo.normal].\n\t    num_batches:\n", "        Number of batches when computing arithmetic means.\n\t    num_samples_per_batch:\n\t        Number of samples per batch.\n\t    statistic_batch:\n\t        The summary statistic to compute on batch-level.\n\t        Usually, this is np.mean. But any other\n\t        statistical function with a signature like\n\t        [one of these functions](https://data-apis.org/array-api/2022.12/API_specification/statistical_functions.html)\n\t        would work.\n\t    statistic_combine:\n", "        The summary statistic to combine batch-results.\n\t        Usually, this is np.mean. But any other\n\t        statistical function with a signature like\n\t        [one of these functions](https://data-apis.org/array-api/2022.12/API_specification/statistical_functions.html)\n\t        would work.\n\t    \"\"\"\n\t    [result] = multiestimate(\n\t        fun,\n\t        key=key,\n\t        sample_fun=sample_fun,\n", "        num_batches=num_batches,\n\t        num_samples_per_batch=num_samples_per_batch,\n\t        statistics_batch=[statistic_batch],\n\t        statistics_combine=[statistic_combine],\n\t    )\n\t    return result\n\tdef multiestimate(\n\t    fun: Callable,\n\t    /,\n\t    *,\n", "    key: Array,\n\t    sample_fun: Callable,\n\t    num_batches: int = 1,\n\t    num_samples_per_batch: int = 10_000,\n\t    statistics_batch: Sequence[Callable] = (np.mean,),\n\t    statistics_combine: Sequence[Callable] = (np.mean,),\n\t) -> Array:\n\t    \"\"\"Compute a Monte-Carlo estimate with multiple summary statistics.\n\t    The signature of this function is almost identical to\n\t    [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n", "    The only difference is that statistics_batch and statistics_combine are iterables\n\t    of summary statistics (of equal lengths).\n\t    The result of this function is an iterable of matching length.\n\t    Parameters\n\t    ----------\n\t    fun:\n\t        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n\t    key:\n\t        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n\t    sample_fun:\n", "        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n\t    num_batches:\n\t        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n\t    num_samples_per_batch:\n\t        Same as in [montecarlo.estimate(...)][matfree.montecarlo.estimate].\n\t    statistics_batch:\n\t        List or tuple of summary statistics to compute on batch-level.\n\t    statistics_combine:\n\t        List or tuple of summary statistics to combine batches.\n\t    \"\"\"\n", "    assert len(statistics_batch) == len(statistics_combine)\n\t    fun_mc = _montecarlo(fun, sample_fun=sample_fun, num_stats=len(statistics_batch))\n\t    fun_single_batch = _stats_via_vmap(fun_mc, num_samples_per_batch, statistics_batch)\n\t    fun_batched = _stats_via_map(fun_single_batch, num_batches, statistics_combine)\n\t    return fun_batched(key)\n\tdef _montecarlo(f, /, sample_fun, num_stats):\n\t    \"\"\"Turn a function into a Monte-Carlo problem.\n\t    More specifically, f(x) becomes g(key) = f(h(key)),\n\t    using a sample function h: key -> x.\n\t    This can then be evaluated and averaged in batches, loops, and compositions thereof.\n", "    \"\"\"\n\t    # todo: what about randomised QMC? How do we best implement this?\n\t    def f_mc(key, /):\n\t        sample = sample_fun(key)\n\t        return [f(sample)] * num_stats\n\t    return f_mc\n\tdef _stats_via_vmap(f, num, /, statistics: Sequence[Callable]):\n\t    \"\"\"Compute summary statistics via jax.vmap.\"\"\"\n\t    def f_mean(key, /):\n\t        subkeys = prng.split(key, num)\n", "        fx_values = func.vmap(f)(subkeys)\n\t        return [stat(fx, axis=0) for stat, fx in zip(statistics, fx_values)]\n\t    return f_mean\n\tdef _stats_via_map(f, num, /, statistics: Sequence[Callable]):\n\t    \"\"\"Compute summary statistics via jax.lax.map.\"\"\"\n\t    def f_mean(key, /):\n\t        subkeys = prng.split(key, num)\n\t        fx_values = control_flow.array_map(f, subkeys)\n\t        return [stat(fx, axis=0) for stat, fx in zip(statistics, fx_values)]\n\t    return f_mean\n", "def normal(*, shape, dtype=float):\n\t    \"\"\"Construct a function that samples from a standard normal distribution.\"\"\"\n\t    def fun(key):\n\t        return prng.normal(key, shape=shape, dtype=dtype)\n\t    return fun\n\tdef rademacher(*, shape, dtype=float):\n\t    \"\"\"Construct a function that samples from a Rademacher distribution.\"\"\"\n\t    def fun(key):\n\t        return prng.rademacher(key, shape=shape, dtype=dtype)\n\t    return fun\n", "class _VDCState(containers.NamedTuple):\n\t    n: int\n\t    vdc: float\n\t    denom: int\n\tdef van_der_corput(n, /, base=2):\n\t    \"\"\"Compute the 'n'th element of the Van-der-Corput sequence.\"\"\"\n\t    state = _VDCState(n, vdc=0, denom=1)\n\t    vdc_modify = func.partial(_van_der_corput_modify, base=base)\n\t    state = control_flow.while_loop(_van_der_corput_cond, vdc_modify, state)\n\t    return state.vdc\n", "def _van_der_corput_cond(state: _VDCState):\n\t    return state.n > 0\n\tdef _van_der_corput_modify(state: _VDCState, *, base):\n\t    denom = state.denom * base\n\t    num, remainder = divmod(state.n, base)\n\t    vdc = state.vdc + remainder / denom\n\t    return _VDCState(num, vdc, denom)\n"]}
{"filename": "matfree/hutchinson.py", "chunked_list": ["\"\"\"Hutchinson-style trace and diagonal estimation.\"\"\"\n\tfrom matfree import montecarlo\n\tfrom matfree.backend import containers, control_flow, func, linalg, np, prng\n\tfrom matfree.backend.typing import Any, Array, Callable, Sequence\n\tdef trace(Av: Callable, /, **kwargs) -> Array:\n\t    \"\"\"Estimate the trace of a matrix stochastically.\n\t    Parameters\n\t    ----------\n\t    Av:\n\t        Matrix-vector product function.\n", "    **kwargs:\n\t        Keyword-arguments to be passed to\n\t        [montecarlo.estimate()][matfree.montecarlo.estimate].\n\t    \"\"\"\n\t    def quadform(vec):\n\t        return linalg.vecdot(vec, Av(vec))\n\t    return montecarlo.estimate(quadform, **kwargs)\n\tdef trace_moments(Av: Callable, /, moments: Sequence[int] = (1, 2), **kwargs) -> Array:\n\t    \"\"\"Estimate the trace of a matrix and the variance of the estimator.\n\t    Parameters\n", "    ----------\n\t    Av:\n\t        Matrix-vector product function.\n\t    moments:\n\t        Which moments to compute. For example, selection `moments=(1,2)` computes\n\t        the first and second moment.\n\t    **kwargs:\n\t        Keyword-arguments to be passed to\n\t        [montecarlo.multiestimate(...)][matfree.montecarlo.multiestimate].\n\t    \"\"\"\n", "    def quadform(vec):\n\t        return linalg.vecdot(vec, Av(vec))\n\t    def moment(x, axis, *, power):\n\t        return np.mean(x**power, axis=axis)\n\t    statistics_batch = [func.partial(moment, power=m) for m in moments]\n\t    statistics_combine = [np.mean] * len(moments)\n\t    return montecarlo.multiestimate(\n\t        quadform,\n\t        statistics_batch=statistics_batch,\n\t        statistics_combine=statistics_combine,\n", "        **kwargs,\n\t    )\n\tdef frobeniusnorm_squared(Av: Callable, /, **kwargs) -> Array:\n\t    r\"\"\"Estimate the squared Frobenius norm of a matrix stochastically.\n\t    The Frobenius norm of a matrix $A$ is defined as\n\t    $$\n\t    \\|A\\|_F^2 = \\text{trace}(A^\\top A)\n\t    $$\n\t    so computing squared Frobenius norms amounts to trace estimation.\n\t    Parameters\n", "    ----------\n\t    Av:\n\t        Matrix-vector product function.\n\t    **kwargs:\n\t        Keyword-arguments to be passed to\n\t        [montecarlo.estimate()][matfree.montecarlo.estimate].\n\t    \"\"\"\n\t    def quadform(vec):\n\t        x = Av(vec)\n\t        return linalg.vecdot(x, x)\n", "    return montecarlo.estimate(quadform, **kwargs)\n\tdef diagonal_with_control_variate(Av: Callable, control: Array, /, **kwargs) -> Array:\n\t    \"\"\"Estimate the diagonal of a matrix stochastically and with a control variate.\n\t    Parameters\n\t    ----------\n\t    Av:\n\t        Matrix-vector product function.\n\t    control:\n\t        Control variate.\n\t        This should be the best-possible estimate of the diagonal of the matrix.\n", "    **kwargs:\n\t        Keyword-arguments to be passed to\n\t        [montecarlo.estimate()][matfree.montecarlo.estimate].\n\t    \"\"\"\n\t    return diagonal(lambda v: Av(v) - control * v, **kwargs) + control\n\tdef diagonal(Av: Callable, /, **kwargs) -> Array:\n\t    \"\"\"Estimate the diagonal of a matrix stochastically.\n\t    Parameters\n\t    ----------\n\t    Av:\n", "        Matrix-vector product function.\n\t    **kwargs:\n\t        Keyword-arguments to be passed to\n\t        [montecarlo.estimate()][matfree.montecarlo.estimate].\n\t    \"\"\"\n\t    def quadform(vec):\n\t        return vec * Av(vec)\n\t    return montecarlo.estimate(quadform, **kwargs)\n\tdef trace_and_diagonal(Av: Callable, /, *, sample_fun: Callable, key: Array, **kwargs):\n\t    \"\"\"Jointly estimate the trace and the diagonal stochastically.\n", "    The advantage of computing both quantities simultaneously is\n\t    that the diagonal estimate\n\t    may serve as a control variate for the trace estimate,\n\t    thus reducing the variance of the estimator\n\t    (and thereby accelerating convergence.)\n\t    Parameters\n\t    ----------\n\t    Av:\n\t        Matrix-vector product function.\n\t    sample_fun:\n", "        Sampling function.\n\t        Usually, either [montecarlo.normal][matfree.montecarlo.normal]\n\t        or [montecarlo.rademacher][matfree.montecarlo.normal].\n\t    key:\n\t        Pseudo-random number generator key.\n\t    **kwargs:\n\t        Keyword-arguments to be passed to\n\t        [diagonal_multilevel()][matfree.hutchinson.diagonal_multilevel].\n\t    See:\n\t    Adams et al., Estimating the Spectral Density of Large Implicit Matrices, 2018.\n", "    \"\"\"\n\t    fx_value = func.eval_shape(sample_fun, key)\n\t    init = np.zeros(shape=fx_value.shape, dtype=fx_value.dtype)\n\t    final = diagonal_multilevel(Av, init, sample_fun=sample_fun, key=key, **kwargs)\n\t    return np.sum(final), final\n\tclass _EstState(containers.NamedTuple):\n\t    diagonal_estimate: Any\n\t    key: Any\n\tdef diagonal_multilevel(\n\t    Av: Callable,\n", "    init: Array,\n\t    /,\n\t    *,\n\t    key: Array,\n\t    sample_fun: Callable,\n\t    num_levels: int,\n\t    num_batches_per_level: int = 1,\n\t    num_samples_per_batch: int = 1,\n\t) -> Array:\n\t    \"\"\"Estimate the diagonal in a multilevel framework.\n", "    The general idea is that a diagonal estimate serves as a control variate\n\t    for the next step's diagonal estimate.\n\t    Parameters\n\t    ----------\n\t    Av:\n\t        Matrix-vector product function.\n\t    init:\n\t        Initial guess.\n\t    key:\n\t        Pseudo-random number generator key.\n", "    sample_fun:\n\t        Sampling function.\n\t        Usually, either [montecarlo.normal][matfree.montecarlo.normal]\n\t        or [montecarlo.rademacher][matfree.montecarlo.normal].\n\t    num_levels:\n\t        Number of levels.\n\t    num_batches_per_level:\n\t        Number of batches per level.\n\t    num_samples_per_batch:\n\t        Number of samples per batch (per level).\n", "    \"\"\"\n\t    kwargs = {\n\t        \"sample_fun\": sample_fun,\n\t        \"num_batches\": num_batches_per_level,\n\t        \"num_samples_per_batch\": num_samples_per_batch,\n\t    }\n\t    def update_fun(level: int, x: _EstState) -> _EstState:\n\t        \"\"\"Update the diagonal estimate.\"\"\"\n\t        diag, k = x\n\t        _, subkey = prng.split(k, num=2)\n", "        update = diagonal_with_control_variate(Av, diag, key=subkey, **kwargs)\n\t        diag = _incr(diag, level, update)\n\t        return _EstState(diag, subkey)\n\t    state = _EstState(diagonal_estimate=init, key=key)\n\t    state = control_flow.fori_loop(0, num_levels, body_fun=update_fun, init_val=state)\n\t    (final, *_) = state\n\t    return final\n\tdef _incr(old, count, incoming):\n\t    return (old * count + incoming) / (count + 1)\n"]}
{"filename": "matfree/lanczos.py", "chunked_list": ["\"\"\"Lanczos-style algorithms.\"\"\"\n\tfrom matfree.backend import containers, control_flow, linalg, np\n\tfrom matfree.backend.typing import Array, Callable, Tuple\n\tclass _Alg(containers.NamedTuple):\n\t    \"\"\"Matrix decomposition algorithm.\"\"\"\n\t    init: Callable\n\t    \"\"\"Initialise the state of the algorithm. Usually, this involves pre-allocation.\"\"\"\n\t    step: Callable\n\t    \"\"\"Compute the next iteration.\"\"\"\n\t    extract: Callable\n", "    \"\"\"Extract the solution from the state of the algorithm.\"\"\"\n\t    lower_upper: Tuple[int, int]\n\t    \"\"\"Range of the for-loop used to decompose a matrix.\"\"\"\n\tdef tridiagonal_full_reortho(depth, /):\n\t    \"\"\"Construct an implementation of **tridiagonalisation**.\n\t    Uses pre-allocation. Fully reorthogonalise vectors at every step.\n\t    This algorithm assumes a **symmetric matrix**.\n\t    Decompose a matrix into a product of orthogonal-**tridiagonal**-orthogonal matrices.\n\t    Use this algorithm for approximate **eigenvalue** decompositions.\n\t    \"\"\"\n", "    class State(containers.NamedTuple):\n\t        i: int\n\t        basis: Array\n\t        tridiag: Tuple[Array, Array]\n\t        q: Array\n\t    def init(init_vec: Array) -> State:\n\t        (ncols,) = np.shape(init_vec)\n\t        if depth >= ncols or depth < 1:\n\t            raise ValueError\n\t        diag = np.zeros((depth + 1,))\n", "        offdiag = np.zeros((depth,))\n\t        basis = np.zeros((depth + 1, ncols))\n\t        return State(0, basis, (diag, offdiag), init_vec)\n\t    def apply(state: State, Av: Callable) -> State:\n\t        i, basis, (diag, offdiag), vec = state\n\t        # Re-orthogonalise against ALL basis elements before storing.\n\t        # Note: we re-orthogonalise against ALL columns of Q, not just\n\t        # the ones we have already computed. This increases the complexity\n\t        # of the whole iteration from n(n+1)/2 to n^2, but has the advantage\n\t        # that the whole computation has static bounds (thus we can JIT it all).\n", "        # Since 'Q' is padded with zeros, the numerical values are identical\n\t        # between both modes of computing.\n\t        vec, length = _normalise(vec)\n\t        vec, _ = _gram_schmidt_orthogonalise_set(vec, basis)\n\t        # I don't know why, but this re-normalisation is soooo crucial\n\t        vec, _ = _normalise(vec)\n\t        basis = basis.at[i, :].set(vec)\n\t        # When i==0, Q[i-1] is Q[-1] and again, we benefit from the fact\n\t        #  that Q is initialised with zeros.\n\t        vec = Av(vec)\n", "        basis_vectors_previous = np.asarray([basis[i], basis[i - 1]])\n\t        vec, (coeff, _) = _gram_schmidt_orthogonalise_set(vec, basis_vectors_previous)\n\t        diag = diag.at[i].set(coeff)\n\t        offdiag = offdiag.at[i - 1].set(length)\n\t        return State(i + 1, basis, (diag, offdiag), vec)\n\t    def extract(state: State, /):\n\t        _, basis, (diag, offdiag), _ = state\n\t        return basis, (diag, offdiag)\n\t    return _Alg(init=init, step=apply, extract=extract, lower_upper=(0, depth + 1))\n\tdef bidiagonal_full_reortho(depth, /, matrix_shape):\n", "    \"\"\"Construct an implementation of **bidiagonalisation**.\n\t    Uses pre-allocation. Fully reorthogonalise vectors at every step.\n\t    Works for **arbitrary matrices**. No symmetry required.\n\t    Decompose a matrix into a product of orthogonal-**bidiagonal**-orthogonal matrices.\n\t    Use this algorithm for approximate **singular value** decompositions.\n\t    \"\"\"\n\t    nrows, ncols = matrix_shape\n\t    max_depth = min(nrows, ncols) - 1\n\t    if depth > max_depth or depth < 0:\n\t        msg1 = f\"Depth {depth} exceeds the matrix' dimensions. \"\n", "        msg2 = f\"Expected: 0 <= depth <= min(nrows, ncols) - 1 = {max_depth} \"\n\t        msg3 = f\"for a matrix with shape {matrix_shape}.\"\n\t        raise ValueError(msg1 + msg2 + msg3)\n\t    class State(containers.NamedTuple):\n\t        i: int\n\t        Us: Array\n\t        Vs: Array\n\t        alphas: Array\n\t        betas: Array\n\t        beta: Array\n", "        vk: Array\n\t    def init(init_vec: Array) -> State:\n\t        nrows, ncols = matrix_shape\n\t        alphas = np.zeros((depth + 1,))\n\t        betas = np.zeros((depth + 1,))\n\t        Us = np.zeros((depth + 1, nrows))\n\t        Vs = np.zeros((depth + 1, ncols))\n\t        v0, _ = _normalise(init_vec)\n\t        return State(0, Us, Vs, alphas, betas, 0.0, v0)\n\t    def apply(state: State, Av: Callable, vA: Callable) -> State:\n", "        i, Us, Vs, alphas, betas, beta, vk = state\n\t        Vs = Vs.at[i].set(vk)\n\t        betas = betas.at[i].set(beta)\n\t        uk = Av(vk) - beta * Us[i - 1]\n\t        uk, alpha = _normalise(uk)\n\t        uk, _ = _gram_schmidt_orthogonalise_set(uk, Us)  # full reorthogonalisation\n\t        uk, _ = _normalise(uk)\n\t        Us = Us.at[i].set(uk)\n\t        alphas = alphas.at[i].set(alpha)\n\t        vk = vA(uk) - alpha * vk\n", "        vk, beta = _normalise(vk)\n\t        vk, _ = _gram_schmidt_orthogonalise_set(vk, Vs)  # full reorthogonalisation\n\t        vk, _ = _normalise(vk)\n\t        return State(i + 1, Us, Vs, alphas, betas, beta, vk)\n\t    def extract(state: State, /):\n\t        _, uk_all, vk_all, alphas, betas, beta, vk = state\n\t        return uk_all.T, (alphas, betas[1:]), vk_all, (beta, vk)\n\t    return _Alg(init=init, step=apply, extract=extract, lower_upper=(0, depth + 1))\n\tdef _normalise(vec):\n\t    length = linalg.vector_norm(vec)\n", "    return vec / length, length\n\tdef _gram_schmidt_orthogonalise_set(vec, vectors):  # Gram-Schmidt\n\t    vec, coeffs = control_flow.scan(_gram_schmidt_orthogonalise, vec, xs=vectors)\n\t    return vec, coeffs\n\tdef _gram_schmidt_orthogonalise(vec1, vec2):\n\t    coeff = linalg.vecdot(vec1, vec2)\n\t    vec_ortho = vec1 - coeff * vec2\n\t    return vec_ortho, coeff\n"]}
{"filename": "matfree/decomp.py", "chunked_list": ["\"\"\"Matrix decomposition algorithms.\"\"\"\n\tfrom matfree import lanczos\n\tfrom matfree.backend import containers, control_flow, linalg\n\tfrom matfree.backend.typing import Array, Callable, Tuple\n\tdef svd(\n\t    v0: Array, depth: int, Av: Callable, vA: Callable, matrix_shape: Tuple[int, ...]\n\t):\n\t    \"\"\"Approximate singular value decomposition.\n\t    Uses GKL with full reorthogonalisation to bi-diagonalise the target matrix\n\t    and computes the full SVD of the (small) bidiagonal matrix.\n", "    Parameters\n\t    ----------\n\t    v0:\n\t        Initial vector for Golub-Kahan-Lanczos bidiagonalisation.\n\t    depth:\n\t        Depth of the Krylov space constructed by Golub-Kahan-Lanczos bidiagonalisation.\n\t        Choosing `depth = min(nrows, ncols) - 1` would yield behaviour similar to\n\t        e.g. `np.linalg.svd`.\n\t    Av:\n\t        Matrix-vector product function.\n", "    vA:\n\t        Vector-matrix product function.\n\t    matrix_shape:\n\t        Shape of the matrix involved in matrix-vector and vector-matrix products.\n\t    \"\"\"\n\t    # Factorise the matrix\n\t    algorithm = lanczos.bidiagonal_full_reortho(depth, matrix_shape=matrix_shape)\n\t    u, (d, e), vt, _ = decompose_fori_loop(v0, Av, vA, algorithm=algorithm)\n\t    # Compute SVD of factorisation\n\t    B = _bidiagonal_dense(d, e)\n", "    U, S, Vt = linalg.svd(B, full_matrices=False)\n\t    # Combine orthogonal transformations\n\t    return u @ U, S, Vt @ vt\n\tdef _bidiagonal_dense(d, e):\n\t    diag = linalg.diagonal_matrix(d)\n\t    offdiag = linalg.diagonal_matrix(e, 1)\n\t    return diag + offdiag\n\tclass _DecompAlg(containers.NamedTuple):\n\t    \"\"\"Matrix decomposition algorithm.\"\"\"\n\t    init: Callable\n", "    \"\"\"Initialise the state of the algorithm. Usually, this involves pre-allocation.\"\"\"\n\t    step: Callable\n\t    \"\"\"Compute the next iteration.\"\"\"\n\t    extract: Callable\n\t    \"\"\"Extract the solution from the state of the algorithm.\"\"\"\n\t    lower_upper: Tuple[int, int]\n\t    \"\"\"Range of the for-loop used to decompose a matrix.\"\"\"\n\tAlgorithmType = Tuple[Callable, Callable, Callable, Tuple[int, int]]\n\t\"\"\"Decomposition algorithm type.\n\tFor example, the output of\n", "[matfree.lanczos.tridiagonal_full_reortho(...)][matfree.lanczos.tridiagonal_full_reortho].\n\t\"\"\"\n\t# all arguments are positional-only because we will rename arguments a lot\n\tdef decompose_fori_loop(v0, *matvec_funs, algorithm: AlgorithmType):\n\t    r\"\"\"Decompose a matrix purely based on matvec-products with A.\n\t    The behaviour of this function is equivalent to\n\t    ```python\n\t    def decompose(v0, *matvec_funs, algorithm):\n\t        init, step, extract, (lower, upper) = algorithm\n\t        state = init(v0)\n", "        for _ in range(lower, upper):\n\t            state = step(state, *matvec_funs)\n\t        return extract(state)\n\t    ```\n\t    but the implementation uses JAX' fori_loop.\n\t    \"\"\"\n\t    # todo: turn the \"practically equivalent\" bit above into a doctest.\n\t    init, step, extract, (lower, upper) = algorithm\n\t    init_val = init(v0)\n\t    def body_fun(_, s):\n", "        return step(s, *matvec_funs)\n\t    result = control_flow.fori_loop(lower, upper, body_fun=body_fun, init_val=init_val)\n\t    return extract(result)\n"]}
{"filename": "matfree/__init__.py", "chunked_list": ["\"\"\"Matrix-free linear algebra.\"\"\"\n"]}
{"filename": "matfree/benchmark_util.py", "chunked_list": ["\"\"\"Benchmark utilities.\"\"\"\n\tfrom matfree.backend import func, linalg, np, time\n\tdef rmse_relative(received, *, expected):\n\t    \"\"\"Compute the relative root-mean-square error.\"\"\"\n\t    return linalg.vector_norm((received - expected) / expected) / np.sqrt(expected.size)\n\tdef error_and_time(fun, error_fun):\n\t    \"\"\"Compute error and runtime of a function with a single outputs.\"\"\"\n\t    @func.wraps(fun)\n\t    def fun_wrapped(*args, **kwargs):\n\t        # Execute once for compilation\n", "        _ = fun(*args, **kwargs)\n\t        # Execute and time\n\t        t0 = time.perf_counter()\n\t        result = fun(*args, **kwargs)\n\t        result.block_until_ready()\n\t        t1 = time.perf_counter()\n\t        return error_fun(result), (t1 - t0)\n\t    return fun_wrapped\n"]}
{"filename": "matfree/slq.py", "chunked_list": ["\"\"\"Stochastic Lanczos quadrature.\"\"\"\n\tfrom matfree import decomp, lanczos, montecarlo\n\tfrom matfree.backend import func, linalg, np\n\tdef logdet_spd(*args, **kwargs):\n\t    \"\"\"Estimate the log-determinant of a symmetric, positive definite matrix.\"\"\"\n\t    return trace_of_matfun_spd(np.log, *args, **kwargs)\n\tdef trace_of_matfun_spd(matfun, order, Av, /, **kwargs):\n\t    \"\"\"Compute the trace of the function of a symmetric matrix.\"\"\"\n\t    quadratic_form = _quadratic_form_slq_spd(matfun, order, Av)\n\t    return montecarlo.estimate(quadratic_form, **kwargs)\n", "def _quadratic_form_slq_spd(matfun, order, Av, /):\n\t    \"\"\"Quadratic form for stochastic Lanczos quadrature.\n\t    Assumes a symmetric, positive definite matrix.\n\t    \"\"\"\n\t    def quadform(v0, /):\n\t        algorithm = lanczos.tridiagonal_full_reortho(order)\n\t        _, tridiag = decomp.decompose_fori_loop(v0, Av, algorithm=algorithm)\n\t        (diag, off_diag) = tridiag\n\t        # todo: once jax supports eigh_tridiagonal(eigvals_only=False),\n\t        #  use it here. Until then: an eigen-decomposition of size (order + 1)\n", "        #  does not hurt too much...\n\t        diag = linalg.diagonal_matrix(diag)\n\t        offdiag1 = linalg.diagonal_matrix(off_diag, -1)\n\t        offdiag2 = linalg.diagonal_matrix(off_diag, 1)\n\t        dense_matrix = diag + offdiag1 + offdiag2\n\t        eigvals, eigvecs = linalg.eigh(dense_matrix)\n\t        # Since Q orthogonal (orthonormal) to v0, Q v = Q[0],\n\t        # and therefore (Q v)^T f(D) (Qv) = Q[0] * f(diag) * Q[0]\n\t        (dim,) = v0.shape\n\t        fx_eigvals = func.vmap(matfun)(eigvals)\n", "        return dim * linalg.vecdot(eigvecs[0, :], fx_eigvals * eigvecs[0, :])\n\t    return quadform\n\tdef logdet_product(*args, **kwargs):\n\t    r\"\"\"Compute the log-determinant of a product of matrices.\n\t    Here, \"product\" refers to $X = A^\\top A$.\n\t    \"\"\"\n\t    return trace_of_matfun_product(np.log, *args, **kwargs)\n\tdef schatten_norm(*args, power, **kwargs):\n\t    r\"\"\"Compute the Schatten-p norm of a matrix via stochastic Lanczos quadrature.\"\"\"\n\t    def matfun(x):\n", "        \"\"\"Matrix-function for Schatten-p norms.\"\"\"\n\t        return x ** (power / 2)\n\t    trace = trace_of_matfun_product(matfun, *args, **kwargs)\n\t    return trace ** (1 / power)\n\tdef trace_of_matfun_product(matfun, order, *matvec_funs, matrix_shape, **kwargs):\n\t    r\"\"\"Compute the trace of a function of a product of matrices.\n\t    Here, \"product\" refers to $X = A^\\top A$.\n\t    \"\"\"\n\t    quadratic_form = _quadratic_form_slq_product(\n\t        matfun, order, *matvec_funs, matrix_shape=matrix_shape\n", "    )\n\t    return montecarlo.estimate(quadratic_form, **kwargs)\n\tdef _quadratic_form_slq_product(matfun, depth, *matvec_funs, matrix_shape):\n\t    r\"\"\"Quadratic form for stochastic Lanczos quadrature.\n\t    Instead of the trace of a function of a matrix,\n\t    compute the trace of a function of the product of matrices.\n\t    Here, \"product\" refers to $X = A^\\top A$.\n\t    \"\"\"\n\t    def quadform(v0, /):\n\t        # Decompose into orthogonal-bidiag-orthogonal\n", "        algorithm = lanczos.bidiagonal_full_reortho(depth, matrix_shape=matrix_shape)\n\t        output = decomp.decompose_fori_loop(v0, *matvec_funs, algorithm=algorithm)\n\t        u, (d, e), vt, _ = output\n\t        # Compute SVD of factorisation\n\t        B = _bidiagonal_dense(d, e)\n\t        _, S, Vt = linalg.svd(B, full_matrices=False)\n\t        # Since Q orthogonal (orthonormal) to v0, Q v = Q[0],\n\t        # and therefore (Q v)^T f(D) (Qv) = Q[0] * f(diag) * Q[0]\n\t        _, ncols = matrix_shape\n\t        eigvals, eigvecs = S**2, Vt.T\n", "        fx_eigvals = func.vmap(matfun)(eigvals)\n\t        return ncols * linalg.vecdot(eigvecs[0, :], fx_eigvals * eigvecs[0, :])\n\t    return quadform\n\tdef _bidiagonal_dense(d, e):\n\t    diag = linalg.diagonal_matrix(d)\n\t    offdiag = linalg.diagonal_matrix(e, 1)\n\t    return diag + offdiag\n"]}
{"filename": "matfree/backend/np.py", "chunked_list": ["\"\"\"NumPy-style API.\"\"\"\n\t# In here, we loosely follow the Array API:\n\t#\n\t# https://data-apis.org/array-api/2022.12/\n\t#\n\t# But deviate in a few points:\n\t# * The functions here do not have all the arguments specified in the API\n\t#   (we only wrap the arguments we need)\n\t# * Our current version of diag/diagonal is slightly different\n\t# * We do not use methods on Array types, e.g. shape(), dtype(). Instead\n", "#   these are functions. (Not all backends might always follow this method interface.)\n\t# * We do not implement any constants (e.g. NaN, Pi). Instead, these are methods.\n\t# * We call max/min/amax/amin array_max and elementwise_max.\n\t#   This is more verbose than what the array API suggests.\n\timport jax.numpy as jnp\n\t# Creation functions:\n\tdef arange(start, /, stop=None, step=1):\n\t    return jnp.arange(start, stop, step)\n\tdef asarray(obj, /):\n\t    return jnp.asarray(obj)\n", "def eye(n_rows):\n\t    return jnp.eye(n_rows)\n\tdef ones_like(x, /):\n\t    return jnp.ones_like(x)\n\tdef zeros(shape, *, dtype=None):\n\t    return jnp.zeros(shape, dtype=dtype)\n\tdef ones(shape, *, dtype=None):\n\t    return jnp.ones(shape, dtype=dtype)\n\t# Element-wise functions\n\tdef abs(x, /):  # noqa: A001\n", "    return jnp.abs(x)\n\tdef log(x, /):\n\t    return jnp.log(x)\n\tdef isnan(x, /):\n\t    return jnp.isnan(x)\n\tdef sin(x, /):\n\t    return jnp.sin(x)\n\tdef cos(x, /):\n\t    return jnp.cos(x)\n\tdef sqrt(x, /):\n", "    return jnp.sqrt(x)\n\tdef sign(x, /):\n\t    return jnp.sign(x)\n\t# Utility functions\n\tdef any(x, /):  # noqa: A001\n\t    return jnp.any(x)\n\tdef allclose(x1, x2, /, *, rtol=1e-5, atol=1e-8):\n\t    return jnp.allclose(x1, x2, rtol=rtol, atol=atol)\n\t# Statistical functions\n\tdef mean(x, /, axis=None):\n", "    return jnp.mean(x, axis)\n\tdef std(x, /, axis=None):\n\t    return jnp.std(x, axis)\n\tdef sum(x, /, axis=None):  # noqa: A001\n\t    return jnp.sum(x, axis)\n\tdef array_min(x, /):\n\t    return jnp.amin(x)\n\tdef array_max(x, /, axis=None):\n\t    return jnp.amax(x, axis=axis)\n\tdef elementwise_max(x1, x2, /):\n", "    return jnp.maximum(x1, x2)\n\tdef nanmean(x, /, axis=None):\n\t    return jnp.nanmean(x, axis)\n\t# Searching functions\n\tdef where(condition, x1, x2, /):\n\t    return jnp.where(condition, x1, x2)\n\t# Manipulation functions\n\tdef reshape(x, /, shape):\n\t    return jnp.reshape(x, shape)\n\tdef flip(x, /):\n", "    return jnp.flip(x)\n\tdef roll(x, /, shift):\n\t    return jnp.roll(x, shift)\n\t# Functional implementation of what are usually array-methods\n\tdef shape(x, /):\n\t    return jnp.shape(x)\n\tdef dtype(x, /):\n\t    return jnp.dtype(x)\n\t# Functional implementation of constants\n\tdef nan():\n", "    return jnp.nan\n\t# Others\n\tdef convolve(a, b, /, mode=\"full\"):\n\t    return jnp.convolve(a, b, mode=mode)\n"]}
{"filename": "matfree/backend/prng.py", "chunked_list": ["\"\"\"Pseudo-random-number utilities.\"\"\"\n\timport jax.random\n\tdef prng_key(seed):\n\t    return jax.random.PRNGKey(seed=seed)\n\tdef split(key, num=2):\n\t    return jax.random.split(key, num=num)\n\tdef normal(key, *, shape, dtype=None):\n\t    if dtype is None:\n\t        return jax.random.normal(key, shape=shape)\n\t    return jax.random.normal(key, shape=shape, dtype=dtype)\n", "def uniform(key, *, shape, dtype=None):\n\t    if dtype is None:\n\t        return jax.random.uniform(key, shape=shape)\n\t    return jax.random.uniform(key, shape=shape, dtype=dtype)\n\tdef rademacher(key, *, shape, dtype=None):\n\t    if dtype is None:\n\t        return jax.random.rademacher(key, shape=shape)\n\t    return jax.random.rademacher(key, shape=shape, dtype=dtype)\n"]}
{"filename": "matfree/backend/plt.py", "chunked_list": ["\"\"\"Plotting functionality.\"\"\"\n\t# Not part of dependencies because not used by default, only for debugging.\n\timport matplotlib.pyplot  # noqa: ICN001\n\tdef subplots(nrows=1, ncols=1, *, figsize, tight_layout=True, dpi=100):\n\t    return matplotlib.pyplot.subplots(\n\t        nrows=nrows, ncols=ncols, figsize=figsize, tight_layout=tight_layout, dpi=dpi\n\t    )\n\tdef show():\n\t    return matplotlib.pyplot.show()\n"]}
{"filename": "matfree/backend/progressbar.py", "chunked_list": ["\"\"\"Progress bars.\"\"\"\n\timport tqdm\n\tdef progressbar(x, /):\n\t    return tqdm.tqdm(x)\n"]}
{"filename": "matfree/backend/control_flow.py", "chunked_list": ["\"\"\"Control flow.\"\"\"\n\timport jax\n\t# API follows JAX, but we liberally work with positional- and keyword-only arguments\n\t# We also rename some arguments for improved consistency:\n\t# For example, we always use 'body_fun' and 'init_val',\n\t#  even though jax.lax.scan uses 'f' and 'init'.\n\tdef scan(body_fun, init_val, /, xs, *, reverse=False):\n\t    return jax.lax.scan(body_fun, init=init_val, xs=xs, reverse=reverse)\n\tdef cond(pred, /, true_fun, false_fun, *operands):\n\t    return jax.lax.cond(pred, true_fun, false_fun, *operands)\n", "def fori_loop(lower, upper, body_fun, init_val):\n\t    return jax.lax.fori_loop(lower, upper, body_fun, init_val)\n\tdef while_loop(cond_fun, body_fun, init_val):\n\t    return jax.lax.while_loop(cond_fun, body_fun, init_val)\n\tdef array_map(fun, /, xs):\n\t    return jax.lax.map(fun, xs)\n"]}
{"filename": "matfree/backend/time.py", "chunked_list": ["\"\"\"Timing.\"\"\"\n\timport time\n\tdef perf_counter():\n\t    return time.perf_counter()\n"]}
{"filename": "matfree/backend/testing.py", "chunked_list": ["\"\"\"Test-utilities.\"\"\"\n\timport jax.test_util\n\timport pytest\n\timport pytest_cases\n\tdef fixture(name=None):\n\t    return pytest_cases.fixture(name=name)\n\tdef parametrize(argnames, argvalues, /):\n\t    return pytest.mark.parametrize(argnames, argvalues)\n\tdef check_grads(fun, /, args, *, order, atol, rtol):\n\t    return jax.test_util.check_grads(fun, args, order=order, atol=atol, rtol=rtol)\n", "def raises(err, /):\n\t    return pytest.raises(err)\n"]}
{"filename": "matfree/backend/__init__.py", "chunked_list": ["\"\"\"MatFree backend.\"\"\"\n"]}
{"filename": "matfree/backend/containers.py", "chunked_list": ["\"\"\"Container types.\"\"\"\n\tfrom typing import NamedTuple  # noqa: F401\n"]}
{"filename": "matfree/backend/typing.py", "chunked_list": ["\"\"\"Types.\"\"\"\n\t# fmt: off\n\tfrom collections.abc import Callable  # noqa: F401\n\tfrom typing import (Any, Generic, Iterable, Sequence,  # noqa: F401, UP035\n\t                    Tuple, TypeVar)\n\tfrom jax import Array  # noqa: F401\n\t# fmt: on\n"]}
{"filename": "matfree/backend/linalg.py", "chunked_list": ["\"\"\"Numerical linear algebra.\"\"\"\n\timport jax.numpy as jnp\n\tdef vector_norm(x, /):\n\t    return jnp.linalg.norm(x)\n\tdef matrix_norm(x, /, which):\n\t    return jnp.linalg.norm(x, ord=which)\n\tdef qr(x, /, *, mode=\"reduced\"):\n\t    return jnp.linalg.qr(x, mode=mode)\n\tdef eigh(x, /):\n\t    return jnp.linalg.eigh(x)\n", "def slogdet(x, /):\n\t    return jnp.linalg.slogdet(x)\n\tdef vecdot(x1, x2, /):\n\t    return jnp.dot(x1, x2)\n\tdef diagonal(x, /, offset=0):\n\t    \"\"\"Extract the diagonal of a matrix.\"\"\"\n\t    return jnp.diag(x, offset)\n\tdef diagonal_matrix(x, /, offset=0):  # not part of array API\n\t    \"\"\"Construct a diagonal matrix.\"\"\"\n\t    return jnp.diag(x, offset)\n", "def trace(x, /):\n\t    return jnp.trace(x)\n\tdef svd(A, /, *, full_matrices=True):\n\t    return jnp.linalg.svd(A, full_matrices=full_matrices)\n"]}
{"filename": "matfree/backend/func.py", "chunked_list": ["\"\"\"Function transformations (algorithmic differentiation, vmap, partial, and so on).\"\"\"\n\t# API-wise, we tend to follow JAX and functools.\n\t# But we only implement those function arguments that we need.\n\t# For example, algorithmic differentiation functions do not offer a 'has_aux' argument\n\t# at the moment, because we don't need it.\n\timport functools\n\timport jax\n\t# Vectorisation\n\tdef vmap(fun, /, in_axes=0, out_axes=0):\n\t    return jax.vmap(fun, in_axes=in_axes, out_axes=out_axes)\n", "# Partial and the like\n\tdef partial(func, /, *args, **kwargs):\n\t    return functools.partial(func, *args, **kwargs)\n\tdef wraps(func, /):\n\t    return functools.wraps(func)\n\t# Algorithmic differentiation\n\tdef linearize(func, /, *args):\n\t    return jax.linearize(func, *args)\n\tdef jacfwd(fun, /, argnums=0):\n\t    return jax.jacfwd(fun, argnums)\n", "# Inferring input and output shapes:\n\tdef eval_shape(func, /, *args, **kwargs):\n\t    return jax.eval_shape(func, *args, **kwargs)\n\t# Compilation (don't use in source!)\n\tdef jit(fun, /, *, static_argnums=None, static_argnames=None):\n\t    return jax.jit(fun, static_argnames=static_argnames, static_argnums=static_argnums)\n"]}
