{"filename": "tsup.config.ts", "chunked_list": ["import { defineConfig } from 'tsup'\n\texport default defineConfig({\n\t  entry: ['src/index.ts'],\n\t  outDir: 'build',\n\t  target: 'es2020',\n\t  format: ['esm'],\n\t  splitting: false,\n\t  sourcemap: true,\n\t  minify: false,\n\t  shims: true,\n", "  dts: false,\n\t})\n"]}
{"filename": "src/const.ts", "chunked_list": ["/// ACCESS_TOKEN START\n\t(globalThis as any).accessToken = ''\n\t/// API ADDR START\n\texport const wechatApiPath = '/wechat/api'\n\texport const promptPath = '/prompt-process'\n\texport const chatPath = '/chat-process'\n\t/// API ADDR END\n\t/// GENERAL START\n\texport const pastMessagesIncluded = 6\n\texport const chatDeploymentName = 'IdeaCreation'\n", "export const promptDeploymentName = 'TextIdeaCreation'\n\texport const chatModel = 'gpt-35-turbo'\n\texport const promptModel = 'text-davinci-003'\n\t/// GENERAL START\n"]}
{"filename": "src/router.ts", "chunked_list": ["import type { FastifyInstance } from 'fastify'\n\timport verify from './wechat/verify'\n\timport wechatAcceptor from './wechat/acceptor'\n\timport openAIAcceptor from './azureopenai/acceptor'\n\tfunction setUpRouter(server: FastifyInstance) {\n\t  // verify wechat request\n\t  verify(server)\n\t  // accept wechat request\n\t  wechatAcceptor(server)\n\t  // Azure openai sample api receive the request\n", "  openAIAcceptor(server)\n\t}\n\texport default setUpRouter\n"]}
{"filename": "src/index.ts", "chunked_list": ["/* eslint-disable no-console */\n\timport Fastify from 'fastify'\n\timport * as dotenv from 'dotenv'\n\timport { isNotEmptyString } from './utils/is'\n\timport parser from './utils/parser'\n\timport router from './router'\n\tdotenv.config()\n\tif (!isNotEmptyString(process.env.OPENAI_API_KEY))\n\t  throw new Error('Missing OPENAI_API_KEY environment variable')\n\tconst server = Fastify({\n", "  logger: true,\n\t})\n\tconst port = 80\n\tfunction main() {\n\t  parser(server)\n\t  router(server)\n\t  // Run the server!\n\t  server.listen({ port, host: '0.0.0.0' }, (err, address) => {\n\t    if (err) {\n\t      server.log.error(err)\n", "      process.exit(1)\n\t    }\n\t    // Server is now listening on ${address}\n\t    console.log('Server listening at', port)\n\t  })\n\t}\n\tmain()\n"]}
{"filename": "src/wechat/acceptor.ts", "chunked_list": ["/* eslint-disable no-console */\n\timport crypto from 'crypto'\n\timport type { FastifyInstance, FastifyReply } from 'fastify'\n\timport { isUUID } from 'src/utils/is'\n\timport type { ChatCompletionRequestMessage } from 'azure-openai'\n\timport { FixedQueue } from 'src/utils/FixedQueue'\n\timport { pastMessagesIncluded, wechatApiPath } from '../const'\n\timport { generateTextResponse } from '../utils/responser'\n\timport { GetAzureOpenAIChatAnswerAsync } from '../azureopenai/chatgpt'\n\tconst cacheMap = new Map()\n", "const checkTimes = new Map()\n\tconst chatHistory = new Map() // cache in memory, should be cleared.\n\tconst queryCacheMap = new Map()\n\tasync function handleMsg(xml: any, reply: FastifyReply) {\n\t  if (xml.MsgType === 'text' || xml.MsgType === 'voice') {\n\t    handleTextMsg(xml, reply)\n\t  }\n\t  else if (xml.MsgType === 'event') {\n\t    if (xml.Event === 'subscribe' || xml.Event === 'SCAN')\n\t      reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '感谢您的支持，我可以作为你的智能助手，我非常智能，虽然我只有鱼的记忆，但是你的问题我一定拼尽全力！！谢谢。'))\n", "    else\n\t      reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '非常遗憾（so sad）'))\n\t  }\n\t  else { reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '只支持文字消息（only support text message）')) }\n\t}\n\t// <xml>\n\t//   <ToUserName><![CDATA[toUser]]></ToUserName>\n\t//   <FromUserName><![CDATA[fromUser]]></FromUserName>\n\t//   <CreateTime>1348831860</CreateTime>\n\t//   <MsgType><![CDATA[text]]></MsgType>\n", "//   <Content><![CDATA[this is a test]]></Content>\n\t//   <MsgId>1234567890123456</MsgId>\n\t//   <MsgDataId>xxxx</MsgDataId>\n\t//   <Idx>xxxx</Idx>\n\t// </xml>\n\tasync function handleTextMsg(xml: any, reply: FastifyReply) {\n\t  const msg = (xml?.Content?.toString() ?? xml?.Recognition?.toString()) as string\n\t  if (msg === undefined || msg.trim().length === 0) {\n\t    reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, '咋回事，不能发送空内容哦（empty text）'))\n\t  }\n", "  else {\n\t    if (isUUID(msg)) {\n\t      if (queryCacheMap.has(msg)) {\n\t        const requestKey = queryCacheMap.get(msg)\n\t        if (cacheMap.has(requestKey)) {\n\t          const resp = cacheMap.get(requestKey)\n\t          if (resp != null) {\n\t            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n\t            return\n\t          }\n", "        }\n\t      }\n\t      else {\n\t        reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName,\n\t          '您输入的标识可能已经过期或者输入过快还没有处理得到结果'))\n\t        return\n\t      }\n\t    }\n\t    const uuidKey = crypto.randomUUID()\n\t    const requestKey = `${xml.FromUserName}_${xml.CreateTime}`\n", "    if (!cacheMap.has(requestKey)) {\n\t      const fq = getChatHistoryForUser(xml.FromUserName.toString())\n\t      fq.enqueue({ role: 'user', content: msg })\n\t      GetAzureOpenAIChatAnswerAsync(fq.toArray(), xml.FromUserName.toString())\n\t        .then((response) => {\n\t          if (!reply.sent) // return the result directly within 5s\n\t            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, response))\n\t          cacheMap.set(requestKey, response)\n\t          fq.enqueue({ role: 'assistant', content: response })\n\t        }).catch((reason) => {\n", "          if (!reply.sent) // return the result directly within 5s\n\t            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, reason))\n\t          cacheMap.set(requestKey, reason)\n\t        })\n\t      cacheMap.set(requestKey, null)\n\t      checkTimes.set(requestKey, 1)\n\t    }\n\t    else {\n\t      const resp = cacheMap.get(requestKey)\n\t      if (resp != null) {\n", "        reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n\t      }\n\t      else {\n\t        const checkTime = checkTimes.get(requestKey) as number\n\t        if (checkTime === 2) { // this is the third retry from wechat, we need to response the result within 5s\n\t          const resp = cacheMap.get(requestKey)\n\t          if (resp != null) {\n\t            reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n\t          }\n\t          else {\n", "            // wait another 3s to get response, so total timeout value is 5 + 5 + 3 = 13s\n\t            setTimeout(() => {\n\t              const resp = cacheMap.get(requestKey)\n\t              if (resp != null) {\n\t                reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName, resp))\n\t              }\n\t              else {\n\t                queryCacheMap.set(uuidKey, requestKey)\n\t                reply.send(generateTextResponse(xml.FromUserName, xml.ToUserName,\n\t                  `哎哟，超时啦！请耐心等待 30 秒后输入下面的标识获取 chatgpt 的回复：\\n\n", "                  <a href=\"weixin://bizmsgmenu?msgmenucontent=${uuidKey}&msgmenuid=1\">${uuidKey}</a>`))\n\t              }\n\t            }, 3 * 1000)\n\t          }\n\t        }\n\t        else {\n\t          checkTimes.set(requestKey, checkTimes.get(requestKey) as number + 1)\n\t          console.log(checkTimes.get(requestKey))\n\t        }\n\t      }\n", "    }\n\t    // cache 3 mins, this can be optimized.\n\t    setTimeout(() => {\n\t      console.log('delete cache')\n\t      cacheMap.delete(requestKey)\n\t      checkTimes.delete(requestKey)\n\t      queryCacheMap.delete(uuidKey)\n\t    }, 180 * 1000)\n\t  }\n\t}\n", "function getChatHistoryForUser(requestKey: string) {\n\t  if (!chatHistory.has(requestKey))\n\t    chatHistory.set(requestKey, new FixedQueue<ChatCompletionRequestMessage>(pastMessagesIncluded))\n\t  return chatHistory.get(requestKey)\n\t}\n\tfunction route(server: FastifyInstance) {\n\t  server.post(wechatApiPath, (request, reply) => {\n\t    const xml = request.body as any\n\t    handleMsg(xml.xml, reply)\n\t  })\n", "}\n\texport default route\n"]}
{"filename": "src/wechat/verify.ts", "chunked_list": ["import type { FastifyInstance, FastifyRequest } from 'fastify'\n\timport { wechatApiPath } from '../const'\n\timport { toSha1 } from '../utils/encryptor'\n\timport type { VerifyQuery } from './types'\n\texport function checkIsWechatRequest(request: FastifyRequest) {\n\t  const q = request.query as VerifyQuery\n\t  try {\n\t    const arr: string[] = [process.env.WECHAT_TOKEN, q.timestamp.toString(), q.nonce.toString()]\n\t    arr.sort()\n\t    const sha1 = toSha1(arr.join(''))\n", "    if (sha1 === q.signature) {\n\t      // Success.\n\t      return true\n\t    }\n\t    else {\n\t      return false\n\t    }\n\t  }\n\t  catch (error) {\n\t    console.error(error)\n", "    return false\n\t  }\n\t}\n\tfunction route(server: FastifyInstance) {\n\t  // setup hook\n\t  server.addHook('onRequest', (request, reply, done) => {\n\t    // TODO: Do we need to add this hook for all wechat request?\n\t    if (request?.url?.startsWith(wechatApiPath)) {\n\t      if (checkIsWechatRequest(request)) {\n\t        // Continue.\n", "        done()\n\t      }\n\t      else {\n\t        reply.send('who are you?')\n\t      }\n\t    }\n\t    else {\n\t      done()\n\t    }\n\t  })\n", "  server.get(wechatApiPath, (request: FastifyRequest, reply) => {\n\t    const q = request.query as VerifyQuery\n\t    if (checkIsWechatRequest(request)) {\n\t      // Success.\n\t      reply.send(q.echostr)\n\t    }\n\t    else {\n\t      reply.send('who are you?')\n\t    }\n\t  })\n", "}\n\texport default route\n"]}
{"filename": "src/wechat/types.ts", "chunked_list": ["export interface VerifyQuery {\n\t  signature: string\n\t  timestamp: number\n\t  nonce: number\n\t  echostr: string\n\t  openid: string\n\t}\n"]}
{"filename": "src/utils/FixedQueue.ts", "chunked_list": ["export class FixedQueue<T> {\n\t  private queue: T[]\n\t  private maxLength: number\n\t  constructor(maxLength: number) {\n\t    this.queue = []\n\t    this.maxLength = maxLength\n\t  }\n\t  enqueue(item: T): void {\n\t    if (this.queue.length >= this.maxLength)\n\t      this.dequeue()\n", "    this.queue.push(item)\n\t  }\n\t  dequeue(): T | undefined {\n\t    return this.queue.shift()\n\t  }\n\t  peek(): T | undefined {\n\t    return this.queue[0]\n\t  }\n\t  clear(): void {\n\t    this.queue = []\n", "  }\n\t  get length(): number {\n\t    return this.queue.length\n\t  }\n\t  toArray(): T[] {\n\t    return Array.from(this.queue)\n\t  }\n\t}\n"]}
{"filename": "src/utils/parser.ts", "chunked_list": ["import { XMLParser } from 'fast-xml-parser'\n\timport type { FastifyInstance } from 'fastify'\n\tconst xmlParser = new XMLParser()\n\texport default function configureParser(server: FastifyInstance) {\n\t  server.addContentTypeParser(['text/xml', 'application/xml'], (\n\t    request, payload, done,\n\t  ) => {\n\t    const chunks: Uint8Array[] = []\n\t    payload.on('data', (chunk) => {\n\t      chunks.push(chunk)\n", "    })\n\t    payload.on('end', () => {\n\t      try {\n\t        const xml = Buffer.concat(chunks).toString()\n\t        const result = xmlParser.parse(xml)\n\t        done(null, result)\n\t      }\n\t      catch (err) {\n\t        done(err as Error)\n\t      }\n", "    })\n\t  })\n\t}\n"]}
{"filename": "src/utils/responser.ts", "chunked_list": ["import { XMLBuilder } from 'fast-xml-parser'\n\texport function generateTextResponse(toUser: string, fromUser: string, text: string) {\n\t  const xmlBuilder = new XMLBuilder({})\n\t  // <xml>\n\t  //   <ToUserName><![CDATA[toUser]]></ToUserName>\n\t  //   <FromUserName><![CDATA[fromUser]]></FromUserName>\n\t  //   <CreateTime>12345678</CreateTime>\n\t  //   <MsgType><![CDATA[text]]></MsgType>\n\t  //   <Content><![CDATA[你好]]></Content>\n\t  // </xml>\n", "  return xmlBuilder.build({\n\t    xml: {\n\t      ToUserName: toUser,\n\t      FromUserName: fromUser,\n\t      CreateTime: Date.now(),\n\t      MsgType: 'text',\n\t      Content: text,\n\t    },\n\t  })\n\t}\n"]}
{"filename": "src/utils/encryptor.ts", "chunked_list": ["import Sha1 from 'sha1'\n\texport function toSha1(str: string) {\n\t  return Sha1(str)\n\t}\n"]}
{"filename": "src/utils/is.ts", "chunked_list": ["export function isNumber<T extends number>(value: T | unknown): value is number {\n\t  return Object.prototype.toString.call(value) === '[object Number]'\n\t}\n\texport function isString<T extends string>(value: T | unknown): value is string {\n\t  return Object.prototype.toString.call(value) === '[object String]'\n\t}\n\texport function isNotEmptyString(value: any): boolean {\n\t  return typeof value === 'string' && value.length > 0\n\t}\n\texport function isBoolean<T extends boolean>(value: T | unknown): value is boolean {\n", "  return Object.prototype.toString.call(value) === '[object Boolean]'\n\t}\n\texport function isFunction<T extends (...args: any[]) => any | void | never>(value: T | unknown): value is T {\n\t  return Object.prototype.toString.call(value) === '[object Function]'\n\t}\n\texport function isUUID(str: string): boolean {\n\t  const uuidRegex = /^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$/i\n\t  return uuidRegex.test(str)\n\t}\n"]}
{"filename": "src/azureopenai/acceptor.ts", "chunked_list": ["import type { FastifyInstance, FastifyReply } from 'fastify'\n\timport { chatPath, promptPath } from '../const'\n\timport { GetAzureOpenAIAnswerAsync, GetAzureOpenAIChatAnswerAsync } from '../azureopenai/chatgpt'\n\tfunction route(server: FastifyInstance) {\n\t  server.post(promptPath, (request, reply) => {\n\t    // eslint-disable-next-line no-console\n\t    console.log('prompt')\n\t    handlePrompt(request.body, reply)\n\t  })\n\t  server.post(chatPath, (request, reply) => {\n", "    handleChat(request.body, reply)\n\t  })\n\t}\n\tasync function handlePrompt(requestBody: any, reply: FastifyReply) {\n\t  GetAzureOpenAIAnswerAsync(requestBody.prompt, 'prompt restapi').then((response) => {\n\t    reply.send(response)\n\t  }).catch((reason) => {\n\t    reply.send(reason)\n\t  })\n\t}\n", "async function handleChat(requestBody: any, reply: FastifyReply) {\n\t  GetAzureOpenAIChatAnswerAsync(requestBody.messages, 'chat restapi').then((response) => {\n\t    reply.send(response)\n\t  }).catch((reason) => {\n\t    reply.send(reason)\n\t  })\n\t}\n\texport default route\n"]}
{"filename": "src/azureopenai/chatgpt.ts", "chunked_list": ["/* eslint-disable no-console */\n\timport type { ChatCompletionRequestMessage } from 'azure-openai'\n\timport { Configuration, OpenAIApi } from 'azure-openai'\n\timport { chatDeploymentName, chatModel, promptDeploymentName, promptModel } from 'src/const'\n\texport async function GetAzureOpenAIAnswerAsync(content: string, user: string) {\n\t  console.log(`user ${user} try to get prompt answer for \\\"${content}\\\".`)\n\t  try {\n\t    const configuration = new Configuration({\n\t      azure: {\n\t        apiKey: process.env.OPENAI_API_KEY,\n", "        endpoint: process.env.OPENAI_API_BASE_URL,\n\t        deploymentName: promptDeploymentName,\n\t      },\n\t    })\n\t    const openai = new OpenAIApi(configuration)\n\t    const completion = await openai.createCompletion({\n\t      model: promptModel,\n\t      prompt: content,\n\t      max_tokens: 800,\n\t    })\n", "    console.log('prompt response-content: ', completion.data.choices[0].text?.trim())\n\t    return completion.data.choices[0].text?.trim()\n\t  }\n\t  catch (error) {\n\t    if (error.response)\n\t      console.log(error.response.status, error.response.data)\n\t    else\n\t      console.log(error.message)\n\t    return '暂不支持 (unsupported temporally)'\n\t  }\n", "}\n\texport async function GetAzureOpenAIChatAnswerAsync(messages: Array<ChatCompletionRequestMessage>, user: string) {\n\t  console.log(`user ${user} try to get chat answer for \\\"${messages[messages.length - 1].content}\\\".`)\n\t  try {\n\t    const configuration = new Configuration({\n\t      azure: {\n\t        apiKey: process.env.OPENAI_API_KEY,\n\t        endpoint: process.env.OPENAI_API_BASE_URL,\n\t        deploymentName: chatDeploymentName,\n\t      },\n", "    })\n\t    const openai = new OpenAIApi(configuration)\n\t    const completion = await openai.createChatCompletion({\n\t      model: chatModel,\n\t      messages,\n\t      max_tokens: 800,\n\t    })\n\t    console.log('chat response-content: ', completion.data.choices[0].message.content?.trim())\n\t    return completion.data.choices[0].message.content?.trim()\n\t  }\n", "  catch (error) {\n\t    if (error.response)\n\t      console.log(error.response.status, error.response.data)\n\t    else\n\t      console.log(error.message)\n\t    return '暂不支持 (unsupported temporally)'\n\t  }\n\t}\n"]}
