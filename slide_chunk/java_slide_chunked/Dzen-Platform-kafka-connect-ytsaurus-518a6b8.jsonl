{"filename": "src/test/java/ru/dzen/kafka/connect/ytsaurus/UtilTest.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus;\n\nimport static org.junit.jupiter.api.Assertions.assertEquals;\nimport static org.junit.jupiter.api.Assertions.assertTrue;\nimport static org.junit.jupiter.api.Assertions.fail;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport java.time.Duration;\nimport java.time.Instant;", "import java.time.Duration;\nimport java.time.Instant;\nimport ru.dzen.kafka.connect.ytsaurus.common.Util;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\nclass UtilTest {\n\n  @org.junit.jupiter.api.Test\n  void parseHumanReadableDuration() {\n    assertEquals(Duration.parse(\"P10D\"), Util.parseHumanReadableDuration(\"10d\"));", "  void parseHumanReadableDuration() {\n    assertEquals(Duration.parse(\"P10D\"), Util.parseHumanReadableDuration(\"10d\"));\n    assertEquals(Duration.parse(\"P3DT5H3S\"), Util.parseHumanReadableDuration(\"3d5h3s\"));\n    assertEquals(Duration.parse(\"P3DT5H3S\"), Util.parseHumanReadableDuration(\"3d 5h 3s\"));\n  }\n\n  @org.junit.jupiter.api.Test\n  void toHumanReadableDuration() {\n    assertEquals(\"240h\", Util.toHumanReadableDuration(Util.parseHumanReadableDuration(\"10d\")));\n    assertEquals(\"77h 3s\", Util.toHumanReadableDuration(Util.parseHumanReadableDuration(\"3d5h3s\")));", "    assertEquals(\"240h\", Util.toHumanReadableDuration(Util.parseHumanReadableDuration(\"10d\")));\n    assertEquals(\"77h 3s\", Util.toHumanReadableDuration(Util.parseHumanReadableDuration(\"3d5h3s\")));\n    assertEquals(\"1200h\", Util.toHumanReadableDuration(Util.parseHumanReadableDuration(\"50d\")));\n  }\n\n  @org.junit.jupiter.api.Test\n  public void testFormatDateTime_dateOnly() {\n    Instant timestamp = Instant.parse(\"2023-03-20T12:34:56Z\");\n    String expectedDate = \"2023-03-20\";\n    String actualDate = Util.formatDateTime(timestamp, true);\n    assertEquals(expectedDate, actualDate);\n  }\n\n  @org.junit.jupiter.api.Test", "  public void testFormatDateTime_dateTime() {\n    Instant timestamp = Instant.parse(\"2023-03-20T12:34:56Z\");\n    String expectedDateTime = \"2023-03-20T12:34:56\";\n    String actualDateTime = Util.formatDateTime(timestamp, false);\n    assertEquals(expectedDateTime, actualDateTime);\n  }\n\n  @org.junit.jupiter.api.Test\n  public void testFloorByDuration_minutes() {\n    Instant timestamp = Instant.parse(\"2023-03-20T12:34:56Z\");\n    Duration duration = Duration.ofMinutes(1);\n    Instant expectedFloor = Instant.parse(\"2023-03-20T12:34:00Z\");\n    Instant actualFloor = Util.floorByDuration(timestamp, duration);\n    assertEquals(expectedFloor, actualFloor);\n  }\n\n  @org.junit.jupiter.api.Test", "  public void testFloorByDuration_minutes() {\n    Instant timestamp = Instant.parse(\"2023-03-20T12:34:56Z\");\n    Duration duration = Duration.ofMinutes(1);\n    Instant expectedFloor = Instant.parse(\"2023-03-20T12:34:00Z\");\n    Instant actualFloor = Util.floorByDuration(timestamp, duration);\n    assertEquals(expectedFloor, actualFloor);\n  }\n\n  @org.junit.jupiter.api.Test\n  public void testFloorByDuration_hours() {\n    Instant timestamp = Instant.parse(\"2023-03-20T12:34:56Z\");\n    Duration duration = Duration.ofHours(1);\n    Instant expectedFloor = Instant.parse(\"2023-03-20T12:00:00Z\");\n    Instant actualFloor = Util.floorByDuration(timestamp, duration);\n    assertEquals(expectedFloor, actualFloor);\n  }\n\n  @org.junit.jupiter.api.Test\n  void convertJsonNodeToYTree() {\n    ObjectMapper objectMapper = new ObjectMapper();\n\n    // Test case 1: Simple JSON object\n    String json1 = \"{\\\"key\\\": \\\"value\\\", \\\"number\\\": 42}\";", "  public void testFloorByDuration_hours() {\n    Instant timestamp = Instant.parse(\"2023-03-20T12:34:56Z\");\n    Duration duration = Duration.ofHours(1);\n    Instant expectedFloor = Instant.parse(\"2023-03-20T12:00:00Z\");\n    Instant actualFloor = Util.floorByDuration(timestamp, duration);\n    assertEquals(expectedFloor, actualFloor);\n  }\n\n  @org.junit.jupiter.api.Test\n  void convertJsonNodeToYTree() {\n    ObjectMapper objectMapper = new ObjectMapper();\n\n    // Test case 1: Simple JSON object\n    String json1 = \"{\\\"key\\\": \\\"value\\\", \\\"number\\\": 42}\";", "    try {\n      JsonNode jsonNode1 = objectMapper.readTree(json1);\n      YTreeNode ytreeNode1 = Util.convertJsonNodeToYTree(jsonNode1);\n\n      // Assertions for Test case 1\n      assertTrue(ytreeNode1.isMapNode());\n      assertEquals(\"value\", ytreeNode1.asMap().get(\"key\").stringValue());\n      assertEquals(42, ytreeNode1.asMap().get(\"number\").integerNode().getInt());\n    } catch (Exception e) {\n      fail(\"Conversion failed for Test case 1\");\n    }\n\n    // Test case 2: JSON object with nested structure\n    String json2 = \"{\\\"key\\\": \\\"value\\\", \\\"nested\\\": {\\\"key2\\\": \\\"value2\\\"}, \\\"array\\\": [1, 2, 3]}\";", "    try {\n      JsonNode jsonNode2 = objectMapper.readTree(json2);\n      YTreeNode ytreeNode2 = Util.convertJsonNodeToYTree(jsonNode2);\n\n      // Assertions for Test case 2\n      assertTrue(ytreeNode2.isMapNode());\n      assertEquals(\"value\", ytreeNode2.asMap().get(\"key\").stringValue());\n      assertTrue(ytreeNode2.asMap().get(\"nested\").isMapNode());\n      assertEquals(\"value2\", ytreeNode2.asMap().get(\"nested\").asMap().get(\"key2\").stringValue());\n      assertTrue(ytreeNode2.asMap().get(\"array\").isListNode());\n      assertEquals(3, ytreeNode2.asMap().get(\"array\").asList().size());\n      assertEquals(1, ytreeNode2.asMap().get(\"array\").asList().get(0).integerNode().getInt());\n      assertEquals(2, ytreeNode2.asMap().get(\"array\").asList().get(1).integerNode().getInt());\n      assertEquals(3, ytreeNode2.asMap().get(\"array\").asList().get(2).integerNode().getInt());\n    } catch (Exception e) {\n      fail(\"Conversion failed for Test case 2\");\n    }\n\n    // Test case 3: Simple JSON array\n    String json3 = \"[1, 2, 3]\";", "    try {\n      JsonNode jsonNode3 = objectMapper.readTree(json3);\n      YTreeNode ytreeNode3 = Util.convertJsonNodeToYTree(jsonNode3);\n\n      // Assertions for Test case 3\n      assertTrue(ytreeNode3.isListNode());\n      assertEquals(3, ytreeNode3.asList().size());\n      assertEquals(1, ytreeNode3.asList().get(0).integerNode().getInt());\n      assertEquals(2, ytreeNode3.asList().get(1).integerNode().getInt());\n      assertEquals(3, ytreeNode3.asList().get(2).integerNode().getInt());\n    } catch (Exception e) {\n      fail(\"Conversion failed for Test case 3\");\n    }\n  }\n}"]}
{"filename": "src/test/java/ru/dzen/kafka/connect/ytsaurus/ConfigTest.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus;\n\nimport static org.junit.jupiter.api.Assertions.assertEquals;\nimport static org.junit.jupiter.api.Assertions.assertTrue;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport org.apache.kafka.common.config.ConfigValue;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig;", "import org.apache.kafka.common.config.ConfigValue;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig;\nimport ru.dzen.kafka.connect.ytsaurus.dynamicTable.DynTableWriterConfig;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriterConfig;\n\nclass ConfigTest {\n\n  static final Map<String, String> correctBaseConfig = Map.ofEntries(\n      Map.entry(\"yt.connection.user\", \"user\"),\n      Map.entry(\"yt.connection.token\", \"token\"),", "      Map.entry(\"yt.connection.user\", \"user\"),\n      Map.entry(\"yt.connection.token\", \"token\"),\n      Map.entry(\"yt.connection.cluster\", \"cluster\"),\n      Map.entry(\"yt.sink.output.type\", \"DYNAMIC_TABLE\"),\n      Map.entry(\"yt.sink.output.table.schema.type\", \"UNSTRUCTURED\"),\n      Map.entry(\"yt.sink.output.key.format\", \"ANY\"),\n      Map.entry(\"yt.sink.output.value.format\", \"ANY\"),\n      Map.entry(\"yt.sink.output.directory\", \"//home/user/output\"),\n      Map.entry(\"yt.sink.output.ttl\", \"30m\"),\n      Map.entry(\"yt.sink.metadata.directory.postfix\", \"_metadata\")", "      Map.entry(\"yt.sink.output.ttl\", \"30m\"),\n      Map.entry(\"yt.sink.metadata.directory.postfix\", \"_metadata\")\n  );\n  static final Map<String, String> correctDynamicConfig = new HashMap<>(correctBaseConfig);\n  static final Map<String, String> correctStaticConfig = new HashMap<>(correctBaseConfig);\n\n  static {\n    correctDynamicConfig.put(\"yt.sink.output.type\", \"DYNAMIC_TABLE\");\n    correctDynamicConfig.put(\"yt.sink.dynamic.queue.postfix\", \"queue\");\n    correctDynamicConfig.put(\"yt.sink.dynamic.queue.auto.create\", \"true\");", "    correctDynamicConfig.put(\"yt.sink.dynamic.queue.postfix\", \"queue\");\n    correctDynamicConfig.put(\"yt.sink.dynamic.queue.auto.create\", \"true\");\n    correctDynamicConfig.put(\"yt.sink.dynamic.queue.tablet.count\", \"10\");\n  }\n\n  static {\n    correctStaticConfig.put(\"yt.sink.output.type\", \"STATIC_TABLES\");\n    correctStaticConfig.put(\"yt.sink.static.rotation.period\", \"10m\");\n    correctStaticConfig.put(\"yt.sink.static.tables.dir.postfix\", \"output\");\n  }", "    correctStaticConfig.put(\"yt.sink.static.tables.dir.postfix\", \"output\");\n  }\n\n  @org.junit.jupiter.api.Test\n  void baseConfig() {\n    List<ConfigValue> configValueList = BaseTableWriterConfig.CONFIG_DEF.validate(\n        correctBaseConfig);\n    for (ConfigValue configValue : configValueList) {\n      assertEquals(0, configValue.errorMessages().size());\n    }\n  }\n\n  @org.junit.jupiter.api.Test\n  void dynamicConfig() {\n    List<ConfigValue> configValueList = DynTableWriterConfig.CONFIG_DEF.validate(\n        correctDynamicConfig);", "    for (ConfigValue configValue : configValueList) {\n      assertEquals(0, configValue.errorMessages().size());\n    }\n  }\n\n  @org.junit.jupiter.api.Test\n  void staticConfig() {\n    List<ConfigValue> configValueList = StaticTableWriterConfig.CONFIG_DEF.validate(\n        correctStaticConfig);\n    for (ConfigValue configValue : configValueList) {\n      assertEquals(0, configValue.errorMessages().size());\n    }\n  }\n\n  @org.junit.jupiter.api.Test\n  void invalidTTL() {\n    Map<String, String> incorrectConfig = new HashMap<>(correctBaseConfig);\n    incorrectConfig.put(\"yt.sink.output.ttl\", \"123123\");\n    List<ConfigValue> configValueList = DynTableWriterConfig.CONFIG_DEF.validate(incorrectConfig);", "    for (ConfigValue configValue : configValueList) {\n      assertEquals(0, configValue.errorMessages().size());\n    }\n  }\n\n  @org.junit.jupiter.api.Test\n  void invalidTTL() {\n    Map<String, String> incorrectConfig = new HashMap<>(correctBaseConfig);\n    incorrectConfig.put(\"yt.sink.output.ttl\", \"123123\");\n    List<ConfigValue> configValueList = DynTableWriterConfig.CONFIG_DEF.validate(incorrectConfig);\n    for (ConfigValue configValue : configValueList) {", "    for (ConfigValue configValue : configValueList) {\n      if (configValue.name() == \"yt.sink.output.ttl\") {\n        assertEquals(1, configValue.errorMessages().size());\n        assertTrue(\n            configValue.errorMessages().get(0).contains(\"Text cannot be parsed to a Duration\"));\n      }\n    }\n  }\n}"]}
{"filename": "src/test/java/ru/dzen/kafka/connect/ytsaurus/statik/InferredSchemaBuilderTest.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.staticTables;\n\nimport static org.junit.jupiter.api.Assertions.assertEquals;\n\nimport java.util.ArrayList;\nimport tech.ytsaurus.core.tables.TableSchema;\nimport tech.ytsaurus.typeinfo.TiType;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeMapNode;\nimport tech.ytsaurus.ysontree.YTreeNode;", "import tech.ytsaurus.ysontree.YTreeMapNode;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\n\npublic class InferredSchemaBuilderTest {\n\n  @org.junit.jupiter.api.Test\n  void testEmptyConstructor() {\n    InferredSchemaBuilder builder = new InferredSchemaBuilder();\n    TableSchema schema = builder.build();\n    assertEquals(0, schema.getColumns().size());\n  }\n\n  @org.junit.jupiter.api.Test\n  void testYTreeNodeConstructor() {\n    // Create schema using TableSchema.builder()\n    TableSchema tableSchema = TableSchema.builder()\n        .addValue(\"column1\", TiType.optional(TiType.string()))\n        .addValue(\"column2\", TiType.optional(TiType.int64()))\n        .build();\n\n    // Convert schema to YTreeNode\n    YTreeNode schemaNode = tableSchema.toYTree();\n\n    InferredSchemaBuilder builder = new InferredSchemaBuilder(schemaNode);\n    TableSchema schema = builder.build();\n    assertEquals(2, schema.getColumns().size());\n    assertEquals(TiType.optional(TiType.string()),\n        schema.getColumnSchema(schema.findColumn(\"column1\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.int64()),\n        schema.getColumnSchema(schema.findColumn(\"column2\")).getTypeV3());\n  }\n\n  @org.junit.jupiter.api.Test\n  void testTableSchemaConstructor() {\n    TableSchema inputSchema = TableSchema.builder()\n        .addValue(\"column1\", TiType.optional(TiType.string()))\n        .addValue(\"column2\", TiType.optional(TiType.int64()))\n        .build();\n    InferredSchemaBuilder builder = new InferredSchemaBuilder(inputSchema);\n    TableSchema schema = builder.build();\n    assertEquals(2, schema.getColumns().size());\n    assertEquals(TiType.optional(TiType.string()),\n        schema.getColumnSchema(schema.findColumn(\"column1\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.int64()),\n        schema.getColumnSchema(schema.findColumn(\"column2\")).getTypeV3());\n  }\n\n  @org.junit.jupiter.api.Test\n  void testListYTreeNodeConstructor() {\n    // Create schema 1 using TableSchema.builder()\n    TableSchema schema1 = TableSchema.builder()\n        .addValue(\"column1\", TiType.optional(TiType.string()))\n        .addValue(\"column2\", TiType.optional(TiType.int64()))\n        .build();\n\n    // Convert schema1 to YTreeNode\n    YTreeNode schemaNode1 = schema1.toYTree();\n\n    // Create schema 2 using TableSchema.builder()\n    TableSchema schema2 = TableSchema.builder()\n        .addValue(\"column2\", TiType.optional(TiType.int64()))\n        .addValue(\"column3\", TiType.optional(TiType.doubleType()))\n        .build();\n\n    // Convert schema2 to YTreeNode\n    YTreeNode schemaNode2 = schema2.toYTree();\n\n    var schemaNodes = new ArrayList<YTreeNode>();\n    schemaNodes.add(schemaNode1);\n    schemaNodes.add(schemaNode2);\n\n    InferredSchemaBuilder builder = new InferredSchemaBuilder(schemaNodes);\n    TableSchema schema = builder.build();\n    assertEquals(3, schema.getColumns().size());\n    assertEquals(TiType.optional(TiType.string()),\n        schema.getColumnSchema(schema.findColumn(\"column1\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.int64()),\n        schema.getColumnSchema(schema.findColumn(\"column2\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.doubleType()),\n        schema.getColumnSchema(schema.findColumn(\"column3\")).getTypeV3());\n  }\n\n  @org.junit.jupiter.api.Test\n  void testUpdateYTreeMapNode() {\n    InferredSchemaBuilder builder = new InferredSchemaBuilder();\n    var mapNodeBuilder = YTree.mapBuilder()\n        .key(\"column1\").value(YTree.stringNode(\"value\"))\n        .key(\"column2\").value(YTree.longNode(42))\n        .key(\"column3\").value(YTree.doubleNode(3.14));\n    YTreeMapNode mapNode = mapNodeBuilder.buildMap();\n\n    builder.update(mapNode);\n    TableSchema schema = builder.build();\n    assertEquals(3, schema.getColumns().size());\n    assertEquals(TiType.optional(TiType.string()),\n        schema.getColumnSchema(schema.findColumn(\"column1\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.int64()),\n        schema.getColumnSchema(schema.findColumn(\"column2\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.doubleType()),\n        schema.getColumnSchema(schema.findColumn(\"column3\")).getTypeV3());\n  }\n\n  @org.junit.jupiter.api.Test\n  void testUpdateListOfYTreeMapNodes() {\n    InferredSchemaBuilder builder = new InferredSchemaBuilder();\n    var mapNodeBuilder1 = YTree.mapBuilder()\n        .key(\"column1\").value(YTree.stringNode(\"value\"))\n        .key(\"column2\").value(YTree.longNode(42));\n    YTreeMapNode mapNode1 = mapNodeBuilder1.buildMap();\n\n    var mapNodeBuilder2 = YTree.mapBuilder()\n        .key(\"column2\").value(YTree.longNode(42))\n        .key(\"column3\").value(YTree.doubleNode(3.14));\n    YTreeMapNode mapNode2 = mapNodeBuilder2.buildMap();\n\n    var mapNodes = new ArrayList<YTreeMapNode>();\n    mapNodes.add(mapNode1);\n    mapNodes.add(mapNode2);\n\n    builder.update(mapNodes);\n    TableSchema schema = builder.build();\n    assertEquals(3, schema.getColumns().size());\n    assertEquals(TiType.optional(TiType.string()),\n        schema.getColumnSchema(schema.findColumn(\"column1\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.int64()),\n        schema.getColumnSchema(schema.findColumn(\"column2\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.doubleType()),\n        schema.getColumnSchema(schema.findColumn(\"column3\")).getTypeV3());\n  }\n\n  @org.junit.jupiter.api.Test\n  void testInferredSchemaUpdate() {\n    InferredSchemaBuilder builder = new InferredSchemaBuilder();\n    var mapNodeBuilder1 = YTree.mapBuilder()\n        .key(\"column1\").value(YTree.stringNode(\"value\"))\n        .key(\"column2\").value(YTree.longNode(42));\n    YTreeMapNode mapNode1 = mapNodeBuilder1.buildMap();\n\n    var mapNodeBuilder2 = YTree.mapBuilder()\n        .key(\"column1\").value(YTree.longNode(1))\n        .key(\"column2\").value(YTree.stringNode(\"text\"))\n        .key(\"column3\").value(YTree.doubleNode(3.14));\n    YTreeMapNode mapNode2 = mapNodeBuilder2.buildMap();\n\n    builder.update(mapNode1);\n    builder.update(mapNode2);\n    TableSchema schema = builder.build();\n    assertEquals(3, schema.getColumns().size());\n    assertEquals(TiType.optional(TiType.yson()),\n        schema.getColumnSchema(schema.findColumn(\"column1\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.yson()),\n        schema.getColumnSchema(schema.findColumn(\"column2\")).getTypeV3());\n    assertEquals(TiType.optional(TiType.doubleType()),\n        schema.getColumnSchema(schema.findColumn(\"column3\")).getTypeV3());\n  }\n}"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/YtTableSinkTask.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Map;\nimport org.apache.kafka.clients.consumer.OffsetAndMetadata;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.connect.errors.RetriableException;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.apache.kafka.connect.sink.SinkTask;", "import org.apache.kafka.connect.sink.SinkRecord;\nimport org.apache.kafka.connect.sink.SinkTask;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig;\nimport ru.dzen.kafka.connect.ytsaurus.dynamicTable.DynTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.dynamicTable.DynTableWriterConfig;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriterConfig;", "import ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriterConfig;\n\npublic class YtTableSinkTask extends SinkTask {\n\n  private static final Logger log = LoggerFactory.getLogger(YtTableSinkTask.class);\n\n  private BaseTableWriter producer;\n\n  public YtTableSinkTask() {\n  }\n\n  @Override", "  public String version() {\n    return new YtTableSinkConnector().version();\n  }\n\n  @Override\n  public void start(Map<String, String> props) {\n    BaseTableWriterConfig config = new BaseTableWriterConfig(props);\n    if (config.getOutputType() == BaseTableWriterConfig.OutputType.DYNAMIC_TABLE) {\n      producer = new DynTableWriter(new DynTableWriterConfig(props));\n    } else if (config.getOutputType() == BaseTableWriterConfig.OutputType.STATIC_TABLES) {\n      producer = new StaticTableWriter(new StaticTableWriterConfig(props));\n    }\n    log.info(\"Started YtTableSinkTask\");\n  }\n\n  @Override", "    } else if (config.getOutputType() == BaseTableWriterConfig.OutputType.STATIC_TABLES) {\n      producer = new StaticTableWriter(new StaticTableWriterConfig(props));\n    }\n    log.info(\"Started YtTableSinkTask\");\n  }\n\n  @Override\n  public void stop() {\n    log.info(\"Stopped YtTableSinkTask\");\n  }\n\n\n  @Override", "  public void put(Collection<SinkRecord> sinkRecords) {\n    try {\n      producer.writeBatch(sinkRecords);\n    } catch (Exception ex) {\n      log.warn(\"Exception in put\", ex);\n      throw new RetriableException(ex);\n    }\n  }\n\n  @Override\n  public Map<TopicPartition, OffsetAndMetadata> preCommit(\n      Map<TopicPartition, OffsetAndMetadata> currentOffsets) {", "    try {\n      return producer.getSafeToCommitOffsets(currentOffsets);\n    } catch (Exception ex) {\n      log.warn(\"Exception in preCommit\", ex);\n      return Collections.emptyMap();\n    }\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/YtTableSinkConnector.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus;\n\nimport java.util.List;\nimport java.util.Map;\nimport java.util.stream.Collectors;\nimport java.util.stream.Stream;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.utils.AppInfoParser;\nimport org.apache.kafka.connect.connector.ConnectorContext;\nimport org.apache.kafka.connect.connector.Task;", "import org.apache.kafka.connect.connector.ConnectorContext;\nimport org.apache.kafka.connect.connector.Task;\nimport org.apache.kafka.connect.errors.ConnectException;\nimport org.apache.kafka.connect.sink.SinkConnector;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig;\nimport ru.dzen.kafka.connect.ytsaurus.common.TableWriterManager;\nimport ru.dzen.kafka.connect.ytsaurus.common.Util;\nimport ru.dzen.kafka.connect.ytsaurus.dynamicTable.DynTableWriter;", "import ru.dzen.kafka.connect.ytsaurus.common.Util;\nimport ru.dzen.kafka.connect.ytsaurus.dynamicTable.DynTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.dynamicTable.DynTableWriterConfig;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriterConfig;\n\npublic class YtTableSinkConnector extends SinkConnector {\n\n  private static final Logger log = LoggerFactory.getLogger(YtTableSinkTask.class);\n  TableWriterManager manager;\n  private Map<String, String> props;\n\n  @Override", "  public String version() {\n    return AppInfoParser.getVersion();\n  }\n\n  @Override\n  public void start(Map<String, String> props) {\n    this.props = props;\n    BaseTableWriterConfig config = new BaseTableWriterConfig(props);\n    if (config.getOutputType() == BaseTableWriterConfig.OutputType.DYNAMIC_TABLE) {\n      manager = new DynTableWriter(new DynTableWriterConfig(props)).getManager();\n    } else if (config.getOutputType() == BaseTableWriterConfig.OutputType.STATIC_TABLES) {\n      manager = new StaticTableWriter(new StaticTableWriterConfig(props)).getManager();\n    }\n", "    if (config.getOutputType() == BaseTableWriterConfig.OutputType.DYNAMIC_TABLE) {\n      manager = new DynTableWriter(new DynTableWriterConfig(props)).getManager();\n    } else if (config.getOutputType() == BaseTableWriterConfig.OutputType.STATIC_TABLES) {\n      manager = new StaticTableWriter(new StaticTableWriterConfig(props)).getManager();\n    }\n\n    try {\n      Util.retryWithBackoff(() -> {\n        try {\n          manager.start();\n        } catch (Exception exc) {\n          log.warn(\"Exception in start: \", exc);\n          throw exc;\n        }\n      }, 10, 1000, 120000);\n    } catch (Exception exc) {\n      throw new ConnectException(exc);\n    }\n\n    log.info(\"Started YtTableSinkConnector\");\n  }\n\n  @Override", "        try {\n          manager.start();\n        } catch (Exception exc) {\n          log.warn(\"Exception in start: \", exc);\n          throw exc;\n        }\n      }, 10, 1000, 120000);\n    } catch (Exception exc) {\n      throw new ConnectException(exc);\n    }\n\n    log.info(\"Started YtTableSinkConnector\");\n  }\n\n  @Override", "  public void initialize(ConnectorContext ctx, List<Map<String, String>> taskConfigs) {\n    super.initialize(ctx, taskConfigs);\n    this.manager.setContext(ctx);\n  }\n\n  @Override\n  public Class<? extends Task> taskClass() {\n    return YtTableSinkTask.class;\n  }\n\n  @Override\n  public List<Map<String, String>> taskConfigs(int maxTasks) {\n    return Stream.generate(() -> props)\n        .limit(maxTasks)\n        .collect(Collectors.toList());\n  }\n\n  @Override", "  public void stop() {\n    manager.stop();\n    log.info(\"Stopped YtTableSinkConnector\");\n  }\n\n  @Override\n  public ConfigDef config() {\n    return DynTableWriterConfig.CONFIG_DEF;\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/dynamicTable/DynTableWriterManager.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.dynamicTable;\n\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Objects;\nimport java.util.Optional;\nimport org.apache.kafka.connect.errors.RetriableException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.TableWriterManager;", "import org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.TableWriterManager;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.EColumn;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.ETableType;\nimport tech.ytsaurus.client.YTsaurusClient;\nimport tech.ytsaurus.client.request.CreateNode;\nimport tech.ytsaurus.client.request.MountTable;\nimport tech.ytsaurus.client.request.ReshardTable;\nimport tech.ytsaurus.core.cypress.CypressNodeType;", "import tech.ytsaurus.client.request.ReshardTable;\nimport tech.ytsaurus.core.cypress.CypressNodeType;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.core.tables.TableSchema;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\npublic class DynTableWriterManager extends DynTableWriter implements TableWriterManager {\n\n  private static final Logger log = LoggerFactory.getLogger(DynTableWriterManager.class);\n\n  public DynTableWriterManager(DynTableWriterConfig config) {\n    super(config);\n  }\n\n  protected void createDynamicTable(YTsaurusClient client, YPath path, TableSchema schema,\n      Map<String, YTreeNode> extraAttributes)\n      throws Exception {\n    var attributes = new HashMap<>(Map.of(\n        \"dynamic\", YTree.booleanNode(true),\n        \"schema\", schema.toYTree(),\n        \"enable_dynamic_store_read\", YTree.booleanNode(true)\n    ));\n    attributes.putAll(extraAttributes);\n    var createNodeBuilder = CreateNode.builder().setPath(path)\n        .setType(CypressNodeType.TABLE).setAttributes(attributes).setIgnoreExisting(true);\n    client.createNode(createNodeBuilder.build()).get();\n    log.info(\"Created table {}\", path);\n  }\n\n  protected void mountDynamicTable(YTsaurusClient client, YPath path) throws Exception {\n    log.info(\"Trying to mount table {}\", path);\n    client.mountTableAndWaitTablets(new MountTable(path)).get();\n    log.info(\"Mounted table {}\", path);\n  }\n\n  protected void reshardQueueAndSetAttributesIfNeeded(YTsaurusClient client) throws Exception {\n    var attributes = client.getNode(config.getDataQueueTablePath() + \"/@\").get().mapNode();", "    for (var entry : config.getExtraQueueAttributes().entrySet()) {\n      if (!Objects.equals(attributes.get(entry.getKey()), Optional.of(entry.getValue()))) {\n        client.setNode(config.getDataQueueTablePath() + \"/@\" + entry.getKey(), entry.getValue());\n        log.info(\n            \"Changed {}/@{} to {}\", config.getDataQueueTablePath(), entry.getKey(),\n            entry.getValue());\n      }\n    }\n    var currentTabletCount = attributes.get(\"tablet_count\").map(YTreeNode::intValue);\n    if (currentTabletCount.equals(Optional.of(config.getTabletCount()))) {\n      log.info(\"No need to reshard table {}: tablet count = desired tablet count\",\n          config.getDataQueueTablePath());\n      return;\n    }\n\n    log.info(\"Unmounting table {}\", config.getDataQueueTablePath());\n    client.unmountTableAndWaitTablets(config.getDataQueueTablePath().toString()).join();\n    log.info(\"Unmounted table {}\", config.getDataQueueTablePath());\n\n    log.info(\"Resharding table {}\", config.getDataQueueTablePath());\n    client.reshardTable(ReshardTable.builder().setPath(config.getDataQueueTablePath())\n        .setTabletCount(config.getTabletCount()).build()).get();\n    log.info(\"Successfully resharded table {}\", config.getDataQueueTablePath());\n  }\n\n  @Override", "    if (currentTabletCount.equals(Optional.of(config.getTabletCount()))) {\n      log.info(\"No need to reshard table {}: tablet count = desired tablet count\",\n          config.getDataQueueTablePath());\n      return;\n    }\n\n    log.info(\"Unmounting table {}\", config.getDataQueueTablePath());\n    client.unmountTableAndWaitTablets(config.getDataQueueTablePath().toString()).join();\n    log.info(\"Unmounted table {}\", config.getDataQueueTablePath());\n\n    log.info(\"Resharding table {}\", config.getDataQueueTablePath());\n    client.reshardTable(ReshardTable.builder().setPath(config.getDataQueueTablePath())\n        .setTabletCount(config.getTabletCount()).build()).get();\n    log.info(\"Successfully resharded table {}\", config.getDataQueueTablePath());\n  }\n\n  @Override", "  public void start() throws RetriableException {\n    if (!config.getAutoCreateTables()) {\n      return;\n    }\n    try {\n      var createNodeBuilder = CreateNode.builder()\n          .setPath(config.getMetadataDirectory())\n          .setType(CypressNodeType.MAP)\n          .setRecursive(true)\n          .setIgnoreExisting(true);\n      client.createNode(createNodeBuilder.build()).get();\n    } catch (Exception e) {\n      throw new RetriableException(e);\n    }", "    try {\n      var dataQueueTableSchema = UnstructuredTableSchema.createDataQueueTableSchema(\n          config.getKeyOutputFormat(), config.getValueOutputFormat(),\n          EColumn.getAllMetadataColumns(ETableType.DYNAMIC));\n\n      var desiredExtraAttributesWithTabletCount = new HashMap<>(config.getExtraQueueAttributes());\n      desiredExtraAttributesWithTabletCount.put(\"tablet_count\",\n          YTree.node(config.getTabletCount()));\n      createDynamicTable(client, config.getDataQueueTablePath(), dataQueueTableSchema,\n          desiredExtraAttributesWithTabletCount);\n\n      reshardQueueAndSetAttributesIfNeeded(client);\n\n      mountDynamicTable(client, config.getDataQueueTablePath());\n      createDynamicTable(client, config.getOffsetsTablePath(),\n          UnstructuredTableSchema.OFFSETS_TABLE_SCHEMA, config.getExtraQueueAttributes());\n      mountDynamicTable(client, config.getOffsetsTablePath());\n    } catch (Exception ex) {\n      log.warn(\"Cannot initialize queue\", ex);\n      throw new RetriableException(ex);\n    }\n  }\n\n  @Override", "  public void stop() {\n\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/dynamicTable/DynTableWriterConfig.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.dynamicTable;\n\nimport java.util.Map;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.connect.errors.ConnectException;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\npublic class DynTableWriterConfig extends BaseTableWriterConfig {\n\n  private static final String QUEUE_POSTFIX = \"yt.sink.dynamic.queue.postfix\";\n  private static final String QUEUE_AUTO_CREATE = \"yt.sink.dynamic.queue.auto.create\";\n  private static final String QUEUE_TABLET_COUNT = \"yt.sink.dynamic.queue.tablet.count\";", "import tech.ytsaurus.ysontree.YTreeNode;\n\npublic class DynTableWriterConfig extends BaseTableWriterConfig {\n\n  private static final String QUEUE_POSTFIX = \"yt.sink.dynamic.queue.postfix\";\n  private static final String QUEUE_AUTO_CREATE = \"yt.sink.dynamic.queue.auto.create\";\n  private static final String QUEUE_TABLET_COUNT = \"yt.sink.dynamic.queue.tablet.count\";\n  public static ConfigDef CONFIG_DEF = new ConfigDef(BaseTableWriterConfig.CONFIG_DEF)\n      .define(QUEUE_POSTFIX, ConfigDef.Type.STRING, \"queue\", ConfigDef.Importance.MEDIUM,\n          \"Postfix for the data queue table name in dynamic output mode\")\n      .define(QUEUE_AUTO_CREATE, ConfigDef.Type.BOOLEAN, true, ConfigDef.Importance.MEDIUM,\n          \"Flag to automatically create and mount dynamic tables if they do not exist\")\n      .define(QUEUE_TABLET_COUNT, ConfigDef.Type.INT, 1, ConfigDef.Range.atLeast(1),\n          ConfigDef.Importance.MEDIUM,\n          \"Number of tablets for the data queue table in dynamic output mode\");\n\n  public DynTableWriterConfig(Map<String, String> originals) throws ConnectException {\n    super(CONFIG_DEF, originals);\n", "    if (!getOutputTableSchemaType().equals(OutputTableSchemaType.UNSTRUCTURED)) {\n      throw new ConnectException(\"Only UNSTRUCTURED schema type is supported by DynTableWriter!\");\n    }\n  }\n\n  public YPath getDataQueueTablePath() {\n    return getOutputDirectory().child(getString(QUEUE_POSTFIX));\n  }\n\n  public YPath getOffsetsTablePath() {\n    return getMetadataDirectory().child(\"offsets\");\n  }\n", "  public YPath getOffsetsTablePath() {\n    return getMetadataDirectory().child(\"offsets\");\n  }\n\n  public boolean getAutoCreateTables() {\n    return getBoolean(QUEUE_AUTO_CREATE);\n  }\n\n  public int getTabletCount() {\n    return getInt(QUEUE_TABLET_COUNT);\n  }\n\n  public Map<String, YTreeNode> getExtraQueueAttributes() {\n    return Map.of(\n        \"min_data_versions\", YTree.integerNode(0),\n        \"max_data_versions\", YTree.integerNode(1),\n        \"min_data_ttl\", YTree.longNode(0),\n        \"max_data_ttl\", YTree.longNode(getOutputTTL().toMillis()));\n  }\n\n}\n", "  public int getTabletCount() {\n    return getInt(QUEUE_TABLET_COUNT);\n  }\n\n  public Map<String, YTreeNode> getExtraQueueAttributes() {\n    return Map.of(\n        \"min_data_versions\", YTree.integerNode(0),\n        \"max_data_versions\", YTree.integerNode(1),\n        \"min_data_ttl\", YTree.longNode(0),\n        \"max_data_ttl\", YTree.longNode(getOutputTTL().toMillis()));\n  }\n\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/dynamicTable/DynTableOffsetsManager.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.dynamicTable;\n\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.ExecutionException;\nimport java.util.stream.Collectors;\nimport org.apache.kafka.clients.consumer.OffsetAndMetadata;\nimport org.apache.kafka.common.TopicPartition;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseOffsetsManager;", "import org.apache.kafka.common.TopicPartition;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseOffsetsManager;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.EColumn;\nimport tech.ytsaurus.client.ApiServiceTransaction;\nimport tech.ytsaurus.client.request.LookupRowsRequest;\nimport tech.ytsaurus.client.request.ModifyRowsRequest;\nimport tech.ytsaurus.core.cypress.YPath;\n\npublic class DynTableOffsetsManager extends BaseOffsetsManager {\n\n  private final YPath pathToOffsetsTable;\n\n  DynTableOffsetsManager(YPath pathToOffsetsTable) {\n    this.pathToOffsetsTable = pathToOffsetsTable;\n  }\n\n  @Override\n  public Map<TopicPartition, OffsetAndMetadata> getPrevOffsets(ApiServiceTransaction trx,\n      Set<TopicPartition> topicPartitions) throws InterruptedException, ExecutionException {\n    var requestBuilder = LookupRowsRequest.builder()\n        .setPath(pathToOffsetsTable.toString())\n        .setSchema(UnstructuredTableSchema.OFFSETS_TABLE_SCHEMA.toLookup())\n        .addLookupColumns(UnstructuredTableSchema.EColumn.TOPIC.name,\n            UnstructuredTableSchema.EColumn.PARTITION.name,\n            UnstructuredTableSchema.EColumn.OFFSET.name);", "\npublic class DynTableOffsetsManager extends BaseOffsetsManager {\n\n  private final YPath pathToOffsetsTable;\n\n  DynTableOffsetsManager(YPath pathToOffsetsTable) {\n    this.pathToOffsetsTable = pathToOffsetsTable;\n  }\n\n  @Override\n  public Map<TopicPartition, OffsetAndMetadata> getPrevOffsets(ApiServiceTransaction trx,\n      Set<TopicPartition> topicPartitions) throws InterruptedException, ExecutionException {\n    var requestBuilder = LookupRowsRequest.builder()\n        .setPath(pathToOffsetsTable.toString())\n        .setSchema(UnstructuredTableSchema.OFFSETS_TABLE_SCHEMA.toLookup())\n        .addLookupColumns(UnstructuredTableSchema.EColumn.TOPIC.name,\n            UnstructuredTableSchema.EColumn.PARTITION.name,\n            UnstructuredTableSchema.EColumn.OFFSET.name);", "    for (var topicPartition : topicPartitions) {\n      requestBuilder = requestBuilder.addFilter(topicPartition.topic(), topicPartition.partition());\n    }\n    var prevOffsetsRows = trx.lookupRows(requestBuilder.build())\n        .get().getRows().stream();\n    return prevOffsetsRows.collect(Collectors.toMap(\n        row -> new TopicPartition(row.getValues().get(0).stringValue(),\n            (int) row.getValues().get(1).longValue()),\n        row -> new OffsetAndMetadata(row.getValues().get(2).longValue()),\n        (v1, v2) -> v1,\n        HashMap::new));\n  }\n\n\n  @Override", "  public void writeOffsets(ApiServiceTransaction trx,\n      Map<TopicPartition, OffsetAndMetadata> offsets)\n      throws InterruptedException, ExecutionException {\n    var modifyRowsRequestBuilder = ModifyRowsRequest.builder()\n        .setPath(pathToOffsetsTable.toString())\n        .setSchema(UnstructuredTableSchema.OFFSETS_TABLE_SCHEMA);\n    offsets.entrySet().stream()\n        .map(entry -> Map.of(\n            EColumn.TOPIC.name, entry.getKey().topic(),\n            EColumn.PARTITION.name, entry.getKey().partition(),\n            EColumn.OFFSET.name, entry.getValue().offset()\n        ))\n        .forEach(update -> modifyRowsRequestBuilder.addUpdate(update));\n    trx.modifyRows(modifyRowsRequestBuilder.build()).get();\n  }\n\n  @Override", "  public void lockPartitions(ApiServiceTransaction trx, Set<TopicPartition> topicPartitions)\n      throws InterruptedException, ExecutionException {\n// Dynamic tables prevent simultaneous modification of one and the same row by default\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/dynamicTable/DynTableWriter.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.dynamicTable;\n\nimport java.util.Collection;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.common.TableWriterManager;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.EColumn;", "import ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.EColumn;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.ETableType;\nimport tech.ytsaurus.client.ApiServiceTransaction;\nimport tech.ytsaurus.client.request.ModifyRowsRequest;\nimport tech.ytsaurus.client.request.StartTransaction;\n\npublic class DynTableWriter extends BaseTableWriter {\n\n  private static final Logger log = LoggerFactory.getLogger(DynTableWriter.class);\n", "  public final DynTableWriterConfig config;\n\n  public DynTableWriter(DynTableWriterConfig config) {\n    super(config, new DynTableOffsetsManager(config.getOffsetsTablePath()));\n    this.config = config;\n  }\n\n  @Override\n  protected void writeRows(ApiServiceTransaction trx, Collection<SinkRecord> records)\n      throws Exception {\n    var modifyRowsRequestBuilder = ModifyRowsRequest.builder()\n        .setPath(config.getDataQueueTablePath().toString())\n        .setSchema(UnstructuredTableSchema.createDataQueueTableSchema(config.getKeyOutputFormat(),\n            config.getValueOutputFormat(), EColumn.getAllMetadataColumns(ETableType.DYNAMIC)));\n    var mapNodesToWrite = recordsToRows(records);\n    mapNodesToWrite.forEach(mapNodeToWrite -> modifyRowsRequestBuilder.addUpdate(mapNodeToWrite));\n    trx.modifyRows(modifyRowsRequestBuilder.build()).get();\n  }\n\n  @Override\n  protected ApiServiceTransaction createTransaction() throws Exception {\n    return client.startTransaction(StartTransaction.tablet()).get();\n  }\n\n  @Override", "  public TableWriterManager getManager() {\n    return new DynTableWriterManager(config);\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/common/BaseTableWriter.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.common;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.node.JsonNodeFactory;\nimport java.nio.charset.StandardCharsets;\nimport java.time.Duration;\nimport java.util.ArrayList;\nimport java.util.Collection;\nimport java.util.Collections;", "import java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.apache.kafka.clients.consumer.OffsetAndMetadata;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.connect.errors.DataException;", "import org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.connect.errors.DataException;\nimport org.apache.kafka.connect.header.Header;\nimport org.apache.kafka.connect.json.JsonConverter;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig.AuthType;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig.OutputTableSchemaType;\nimport tech.ytsaurus.client.ApiServiceTransaction;", "import ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig.OutputTableSchemaType;\nimport tech.ytsaurus.client.ApiServiceTransaction;\nimport tech.ytsaurus.client.YTsaurusClient;\nimport tech.ytsaurus.client.YTsaurusClientConfig;\nimport tech.ytsaurus.client.request.StartTransaction;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\npublic abstract class BaseTableWriter {\n\n  protected static final ObjectMapper objectMapper = new ObjectMapper();\n  private static final Logger log = LoggerFactory.getLogger(BaseTableWriter.class);\n  private static final JsonConverter JSON_CONVERTER;\n\n  static {\n    JSON_CONVERTER = new JsonConverter();\n    JSON_CONVERTER.configure(Collections.singletonMap(\"schemas.enable\", \"false\"), false);\n  }\n\n  protected final YTsaurusClient client;\n  protected final BaseOffsetsManager offsetsManager;\n  protected final BaseTableWriterConfig config;\n\n  protected BaseTableWriter(BaseTableWriterConfig config, BaseOffsetsManager offsetsManager) {\n    this.config = config;\n    this.client = YTsaurusClient.builder()\n        .setConfig(YTsaurusClientConfig.builder()\n            .setTvmOnly(config.getAuthType().equals(AuthType.SERVICE_TICKET)).build())\n        .setCluster(config.getYtCluster()).setAuth(config.getYtClientAuth()).build();\n    this.offsetsManager = offsetsManager;\n  }\n\n  protected ApiServiceTransaction createTransaction() throws Exception {\n    return client.startTransaction(StartTransaction.master()).get();\n  }\n\n  public Map<TopicPartition, OffsetAndMetadata> getSafeToCommitOffsets(\n      Map<TopicPartition, OffsetAndMetadata> unsafeOffsets) throws Exception {\n    return offsetsManager.getPrevOffsets(createTransaction(), unsafeOffsets.keySet()).entrySet()\n        .stream()\n        .filter(entry -> unsafeOffsets.containsKey(entry.getKey()))\n        .map(entry -> {\n          var topicPartition = entry.getKey();\n          var prevOffset = entry.getValue();\n          var unsafeOffset = unsafeOffsets.get(topicPartition);\n          return unsafeOffset.offset() >= prevOffset.offset() ?\n              Map.entry(topicPartition, prevOffset) :\n              Map.entry(topicPartition, unsafeOffset);\n        })\n        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n  }\n\n  protected Object convertRecordKey(SinkRecord record) throws Exception {", "public abstract class BaseTableWriter {\n\n  protected static final ObjectMapper objectMapper = new ObjectMapper();\n  private static final Logger log = LoggerFactory.getLogger(BaseTableWriter.class);\n  private static final JsonConverter JSON_CONVERTER;\n\n  static {\n    JSON_CONVERTER = new JsonConverter();\n    JSON_CONVERTER.configure(Collections.singletonMap(\"schemas.enable\", \"false\"), false);\n  }\n\n  protected final YTsaurusClient client;\n  protected final BaseOffsetsManager offsetsManager;\n  protected final BaseTableWriterConfig config;\n\n  protected BaseTableWriter(BaseTableWriterConfig config, BaseOffsetsManager offsetsManager) {\n    this.config = config;\n    this.client = YTsaurusClient.builder()\n        .setConfig(YTsaurusClientConfig.builder()\n            .setTvmOnly(config.getAuthType().equals(AuthType.SERVICE_TICKET)).build())\n        .setCluster(config.getYtCluster()).setAuth(config.getYtClientAuth()).build();\n    this.offsetsManager = offsetsManager;\n  }\n\n  protected ApiServiceTransaction createTransaction() throws Exception {\n    return client.startTransaction(StartTransaction.master()).get();\n  }\n\n  public Map<TopicPartition, OffsetAndMetadata> getSafeToCommitOffsets(\n      Map<TopicPartition, OffsetAndMetadata> unsafeOffsets) throws Exception {\n    return offsetsManager.getPrevOffsets(createTransaction(), unsafeOffsets.keySet()).entrySet()\n        .stream()\n        .filter(entry -> unsafeOffsets.containsKey(entry.getKey()))\n        .map(entry -> {\n          var topicPartition = entry.getKey();\n          var prevOffset = entry.getValue();\n          var unsafeOffset = unsafeOffsets.get(topicPartition);\n          return unsafeOffset.offset() >= prevOffset.offset() ?\n              Map.entry(topicPartition, prevOffset) :\n              Map.entry(topicPartition, unsafeOffset);\n        })\n        .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue));\n  }\n\n  protected Object convertRecordKey(SinkRecord record) throws Exception {", "    if (record.key() == null) {\n      return JsonNodeFactory.instance.nullNode();\n    }\n    if (record.key() instanceof String) {\n      return record.key();\n    }\n\n    byte[] jsonBytes = JSON_CONVERTER.fromConnectData(record.topic(), record.keySchema(),\n        record.key());\n    var jsonString = new String(jsonBytes, StandardCharsets.UTF_8);\n\n    JsonNode jsonNode = objectMapper.readTree(jsonString);\n\n    return jsonNode;\n  }\n\n  protected YTreeNode convertRecordKeyToNode(SinkRecord record) throws Exception {\n    var recordKey = convertRecordKey(record);\n", "    if (config.getKeyOutputFormat() == BaseTableWriterConfig.OutputFormat.STRING\n        && !(recordKey instanceof String)) {\n      recordKey = objectMapper.writeValueAsString(recordKey);\n    } else if (!(recordKey instanceof String)) {\n      recordKey = Util.convertJsonNodeToYTree((JsonNode) recordKey);\n    }\n\n    return YTree.node(recordKey);\n  }\n\n  protected Object convertRecordValue(SinkRecord record) throws Exception {", "    if (record.value() == null) {\n      return JsonNodeFactory.instance.nullNode();\n    }\n    if (record.value() instanceof String) {\n      return record.value();\n    }\n\n    byte[] jsonBytes = JSON_CONVERTER.fromConnectData(record.topic(), record.valueSchema(),\n        record.value());\n    var jsonString = new String(jsonBytes, StandardCharsets.UTF_8);\n\n    JsonNode jsonNode = objectMapper.readTree(jsonString);\n\n    return jsonNode;\n  }\n\n  protected YTreeNode convertRecordValueToNode(SinkRecord record) throws Exception {\n    var recordValue = convertRecordValue(record);", "    if (config.getValueOutputFormat() == BaseTableWriterConfig.OutputFormat.STRING\n        && !(recordValue instanceof String)) {\n      recordValue = objectMapper.writeValueAsString(recordValue);\n    } else if (!(recordValue instanceof String)) {\n      recordValue = Util.convertJsonNodeToYTree((JsonNode) recordValue);\n    }\n\n    return YTree.node(recordValue);\n  }\n\n  protected List<Map<String, YTreeNode>> recordsToRows(Collection<SinkRecord> records) {\n    var mapNodesToWrite = new ArrayList<Map<String, YTreeNode>>();", "    for (SinkRecord record : records) {\n      var headersBuilder = YTree.builder().beginList();\n      for (Header header : record.headers()) {\n        headersBuilder.value(\n            YTree.builder().beginList().value(header.key()).value(header.value().toString())\n                .buildList());\n      }\n      YTreeNode recordKeyNode;\n      try {\n        recordKeyNode = convertRecordKeyToNode(record);\n      } catch (Exception e) {\n        log.error(\"Exception in convertRecordKeyToNode:\", e);\n        throw new DataException(e);\n      }\n      YTreeNode recordValueNode;", "      try {\n        recordKeyNode = convertRecordKeyToNode(record);\n      } catch (Exception e) {\n        log.error(\"Exception in convertRecordKeyToNode:\", e);\n        throw new DataException(e);\n      }\n      YTreeNode recordValueNode;\n      try {\n        recordValueNode = convertRecordValueToNode(record);\n      } catch (Exception e) {\n        log.error(\"Exception in convertRecordValueToNode:\", e);\n        throw new DataException(e);\n      }\n\n      Map<String, YTreeNode> rowMap = new HashMap<>();", "      if (config.getOutputTableSchemaType().equals(OutputTableSchemaType.UNSTRUCTURED)) {\n        rowMap.put(UnstructuredTableSchema.EColumn.DATA.name, recordValueNode);\n      } else {\n        if (!recordValueNode.isMapNode()) {\n          throw new DataException(\"Record value is not a map: \" + recordValueNode);\n        }\n        rowMap = recordValueNode.asMap();\n      }\n\n      rowMap.put(UnstructuredTableSchema.EColumn.KEY.name, recordKeyNode);\n      rowMap.put(UnstructuredTableSchema.EColumn.TOPIC.name, YTree.stringNode(record.topic()));\n      rowMap.put(UnstructuredTableSchema.EColumn.PARTITION.name,\n          YTree.unsignedLongNode(record.kafkaPartition()));\n      rowMap.put(UnstructuredTableSchema.EColumn.OFFSET.name,\n          YTree.unsignedLongNode(record.kafkaOffset()));\n      rowMap.put(UnstructuredTableSchema.EColumn.TIMESTAMP.name,\n          YTree.unsignedLongNode(System.currentTimeMillis()));\n      rowMap.put(UnstructuredTableSchema.EColumn.HEADERS.name, headersBuilder.buildList());\n      mapNodesToWrite.add(rowMap);\n    }\n    return mapNodesToWrite;\n  }\n\n  protected void writeRows(ApiServiceTransaction trx, Collection<SinkRecord> records)\n      throws Exception {\n\n  }\n\n  protected void writeRows(ApiServiceTransaction trx, Collection<SinkRecord> records,\n      Set<TopicPartition> topicPartitions)\n      throws Exception {\n    writeRows(trx, records);\n  }\n", "  public void writeBatch(Collection<SinkRecord> records) throws Exception {\n    var startTime = System.currentTimeMillis();\n    try (var trx = createTransaction()) {\n      var maxOffsets = offsetsManager.getMaxOffsets(records);\n      offsetsManager.lockPartitions(trx, maxOffsets.keySet());\n      var prevOffsets = offsetsManager.getPrevOffsets(trx,\n          maxOffsets.keySet());\n      var filteredRecords = offsetsManager.filterRecords(records, prevOffsets);\n      if (filteredRecords.isEmpty()) {\n        trx.close();\n      } else {\n        writeRows(trx, filteredRecords, maxOffsets.keySet());\n        offsetsManager.writeOffsets(trx, maxOffsets);\n        trx.commit().get();\n      }\n      var elapsed = Duration.ofMillis(System.currentTimeMillis() - startTime);\n      log.info(\"Done processing batch in {}: {} total, {} written, {} skipped\",\n          Util.toHumanReadableDuration(elapsed), records.size(), filteredRecords.size(),\n          records.size() - filteredRecords.size());\n    } catch (Exception ex) {\n      throw ex;\n    }\n  }\n", "      if (filteredRecords.isEmpty()) {\n        trx.close();\n      } else {\n        writeRows(trx, filteredRecords, maxOffsets.keySet());\n        offsetsManager.writeOffsets(trx, maxOffsets);\n        trx.commit().get();\n      }\n      var elapsed = Duration.ofMillis(System.currentTimeMillis() - startTime);\n      log.info(\"Done processing batch in {}: {} total, {} written, {} skipped\",\n          Util.toHumanReadableDuration(elapsed), records.size(), filteredRecords.size(),\n          records.size() - filteredRecords.size());\n    } catch (Exception ex) {\n      throw ex;\n    }\n  }\n", "  public abstract TableWriterManager getManager();\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/common/HttpServiceTicketAuth.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.common;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.HttpURLConnection;\nimport java.net.URL;\nimport org.slf4j.Logger;", "import java.net.URL;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport tech.ytsaurus.client.rpc.ServiceTicketAuth;\n\npublic class HttpServiceTicketAuth implements ServiceTicketAuth {\n\n  private static final Logger log = LoggerFactory.getLogger(HttpServiceTicketAuth.class);\n  private final String serviceUrl;\n  private final ObjectMapper mapper;\n\n  public HttpServiceTicketAuth(String serviceUrl) {\n    this.serviceUrl = serviceUrl;\n    this.mapper = new ObjectMapper();\n    log.debug(\"HttpServiceTicketAuth initialized.\");\n  }\n\n  @Override", "  public String issueServiceTicket() {\n    StringBuilder response = new StringBuilder();\n    log.debug(\"Requesting service ticket.\");\n\n    try {\n      URL url = new URL(serviceUrl);\n      HttpURLConnection connection = (HttpURLConnection) url.openConnection();\n\n      connection.setRequestMethod(\"GET\");\n      connection.setRequestProperty(\"Accept\", \"application/json\");\n", "      if (connection.getResponseCode() != HttpURLConnection.HTTP_OK) {\n        log.error(\"Failed: HTTP error code : {}\", connection.getResponseCode());\n        throw new RuntimeException(\"Failed : HTTP error code : \" + connection.getResponseCode());\n      }\n\n      BufferedReader br = new BufferedReader(new InputStreamReader((connection.getInputStream())));\n\n      String output;\n      while ((output = br.readLine()) != null) {\n        response.append(output);\n      }\n\n      connection.disconnect();\n    } catch (IOException e) {\n      log.error(\"IOException occurred while making a GET request.\", e);\n      throw new RuntimeException(e);\n    }\n", "      while ((output = br.readLine()) != null) {\n        response.append(output);\n      }\n\n      connection.disconnect();\n    } catch (IOException e) {\n      log.error(\"IOException occurred while making a GET request.\", e);\n      throw new RuntimeException(e);\n    }\n\n    try {\n      JsonNode jsonResponse = mapper.readTree(response.toString());", "    try {\n      JsonNode jsonResponse = mapper.readTree(response.toString());\n      if (jsonResponse.has(\"error\")) {\n        log.error(\"Error received from service: {}\", jsonResponse.get(\"error\").asText());\n        throw new RuntimeException(jsonResponse.get(\"error\").asText());\n      } else if (jsonResponse.has(\"ticket\")) {\n        log.debug(\"Received service ticket from service.\");\n        return jsonResponse.get(\"ticket\").asText();\n      } else {\n        log.error(\"Invalid ticket response: {}\", jsonResponse);\n        throw new RuntimeException(\"Invalid ticket response: \" + jsonResponse);\n      }\n    } catch (IOException e) {\n      log.error(\"Error parsing JSON response from service.\", e);\n      throw new RuntimeException(\"Error parsing JSON response: \" + e.getMessage());\n    }\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/common/Util.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.common;\n\nimport com.fasterxml.jackson.databind.JsonNode;\nimport java.time.Duration;\nimport java.time.Instant;\nimport java.time.LocalDateTime;\nimport java.time.ZoneOffset;\nimport java.time.format.DateTimeFormatter;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.TimeUnit;", "import java.util.concurrent.Callable;\nimport java.util.concurrent.TimeUnit;\nimport java.util.regex.Pattern;\nimport tech.ytsaurus.client.ApiServiceTransaction;\nimport tech.ytsaurus.client.request.LockNode;\nimport tech.ytsaurus.client.request.LockNodeResult;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\npublic class Util {\n", "\npublic class Util {\n\n  public static Duration parseHumanReadableDuration(String durationString) {\n    durationString = durationString.trim().replaceAll(\" \", \"\").toUpperCase();\n    var pattern = Pattern.compile(\"\\\\d*[A-Z]\");\n    var matcher = pattern.matcher(durationString);\n    var isDatePart = true;\n    var formattedDuration = \"P\";\n    while (matcher.find()) {\n      var part = durationString.substring(matcher.start(), matcher.end());", "    while (matcher.find()) {\n      var part = durationString.substring(matcher.start(), matcher.end());\n      if (part.charAt(part.length() - 1) != 'D' && isDatePart) {\n        isDatePart = false;\n        formattedDuration += \"T\";\n      }\n      formattedDuration += part;\n    }\n    return Duration.parse(formattedDuration);\n  }\n", "  public static String toHumanReadableDuration(Duration duration) {\n    return duration.toString()\n        .substring(1)\n        .replace(\"T\", \"\")\n        .replaceAll(\"(\\\\d[HMS])(?!$)\", \"$1 \")\n        .toLowerCase();\n  }\n\n  public static String formatDateTime(Instant timestamp, boolean dateOnly) {\n    var localDateTime = LocalDateTime.ofInstant(timestamp, ZoneOffset.UTC);\n    var formatter = dateOnly ? DateTimeFormatter.ofPattern(\"yyyy-MM-dd\") :\n        DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss\");\n    return localDateTime.format(formatter);\n  }\n", "  public static String formatDateTime(Instant timestamp, boolean dateOnly) {\n    var localDateTime = LocalDateTime.ofInstant(timestamp, ZoneOffset.UTC);\n    var formatter = dateOnly ? DateTimeFormatter.ofPattern(\"yyyy-MM-dd\") :\n        DateTimeFormatter.ofPattern(\"yyyy-MM-dd'T'HH:mm:ss\");\n    return localDateTime.format(formatter);\n  }\n\n  public static Instant floorByDuration(Instant timestamp, Duration duration) {\n    var flooredSeconds =\n        (timestamp.getEpochSecond() / duration.getSeconds()) * duration.getSeconds();\n    return Instant.ofEpochSecond(flooredSeconds);\n  }\n", "  public static YTreeNode convertJsonNodeToYTree(JsonNode jsonNode) {\n    if (jsonNode.isObject()) {\n      var mapBuilder = YTree.mapBuilder();\n      jsonNode.fields().forEachRemaining(entry -> {\n        var key = entry.getKey();\n        var valueNode = entry.getValue();\n        mapBuilder.key(key).value(convertJsonNodeToYTree(valueNode));\n      });\n      return mapBuilder.buildMap();\n    } else if (jsonNode.isArray()) {\n      var listBuilder = YTree.listBuilder();\n      jsonNode.forEach(element -> listBuilder.value(convertJsonNodeToYTree(element)));\n      return listBuilder.buildList();", "    } else if (jsonNode.isArray()) {\n      var listBuilder = YTree.listBuilder();\n      jsonNode.forEach(element -> listBuilder.value(convertJsonNodeToYTree(element)));\n      return listBuilder.buildList();\n    } else if (jsonNode.isTextual()) {\n      return YTree.stringNode(jsonNode.asText());\n    } else if (jsonNode.isNumber()) {\n      if (jsonNode.isIntegralNumber()) {\n        return YTree.longNode(jsonNode.asLong());\n      } else {\n        return YTree.doubleNode(jsonNode.asDouble());\n      }", "    } else if (jsonNode.isBoolean()) {\n      return YTree.booleanNode(jsonNode.asBoolean());\n    } else if (jsonNode.isNull()) {\n      return YTree.nullNode();\n    } else {\n      throw new UnsupportedOperationException(\n          \"Unsupported JsonNode type: \" + jsonNode.getNodeType());\n    }\n  }\n\n  public static LockNodeResult waitAndLock(ApiServiceTransaction trx, LockNode lockNodeReq,\n      Duration timeout) throws Exception {\n    var res = trx.lockNode(lockNodeReq).get();\n    var deadline = Instant.now().plusMillis(timeout.toMillis());", "  public static LockNodeResult waitAndLock(ApiServiceTransaction trx, LockNode lockNodeReq,\n      Duration timeout) throws Exception {\n    var res = trx.lockNode(lockNodeReq).get();\n    var deadline = Instant.now().plusMillis(timeout.toMillis());\n    while ((timeout.toNanos() == 0) || Instant.now().isBefore(deadline)) {\n      var lockStateNodePath = \"#\" + res.lockId + \"/@state\";\n      var state = trx.getNode(lockStateNodePath).get().stringValue();\n      if (state.equals(\"acquired\")) {\n        break;\n      }\n      Thread.sleep(2000);\n    }\n    return res;\n  }\n\n  public static <T> T retryWithBackoff(Callable<T> task, int maxRetries, long initialBackoffMillis,\n      long maxBackoffMillis) throws Exception {\n    Exception lastException = null;\n    long backoffMillis = initialBackoffMillis;\n", "    for (int attempt = 1; attempt <= maxRetries; attempt++) {\n      try {\n        return task.call();\n      } catch (Exception e) {\n        lastException = e;\n        if (attempt < maxRetries) {\n          try {\n            TimeUnit.MILLISECONDS.sleep(backoffMillis);\n          } catch (InterruptedException ie) {\n            // Restore the interrupt status and break the loop\n            Thread.currentThread().interrupt();\n            break;\n          }\n          backoffMillis = Math.min(backoffMillis * 2, maxBackoffMillis);\n        }\n      }\n    }\n", "    if (lastException != null) {\n      throw lastException;\n    } else {\n      throw new Exception(\"Retry operation failed due to interruption.\");\n    }\n  }\n\n  public static void retryWithBackoff(Runnable task, int maxRetries, long initialBackoffMillis,\n      long maxBackoffMillis) throws Exception {\n    Callable<Void> callableTask = () -> {\n      task.run();\n      return null;\n    };\n    retryWithBackoff(callableTask, maxRetries, initialBackoffMillis, maxBackoffMillis);\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/common/UnstructuredTableSchema.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.common;\n\n\nimport java.util.Arrays;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.TreeMap;\nimport java.util.stream.Collectors;\nimport tech.ytsaurus.core.tables.TableSchema;\nimport tech.ytsaurus.typeinfo.TiType;", "import tech.ytsaurus.core.tables.TableSchema;\nimport tech.ytsaurus.typeinfo.TiType;\n\npublic final class UnstructuredTableSchema {\n\n  public static final TableSchema OFFSETS_TABLE_SCHEMA = TableSchema.builder()\n      .addKey(EColumn.TOPIC.name, TiType.string())\n      .addKey(EColumn.PARTITION.name, TiType.uint32())\n      .addValue(EColumn.OFFSET.name, TiType.uint64())\n      .build();\n", "  public static TableSchema createDataQueueTableSchema(\n      BaseTableWriterConfig.OutputFormat keyOutputFormat,\n      BaseTableWriterConfig.OutputFormat valueOutputFormat, Map<EColumn, String> metadataColumns) {\n    var builder = TableSchema.builder()\n        .setUniqueKeys(false);\n    if (keyOutputFormat != null) {\n      builder.addValue(EColumn.KEY.name, keyOutputFormat.toTiType());\n    }\n    if (valueOutputFormat != null) {\n      builder.addValue(EColumn.DATA.name, valueOutputFormat.toTiType());\n    }", "    if (valueOutputFormat != null) {\n      builder.addValue(EColumn.DATA.name, valueOutputFormat.toTiType());\n    }\n    if (metadataColumns.containsKey(EColumn.TOPIC)) {\n      builder.addValue(metadataColumns.get(EColumn.TOPIC), TiType.string());\n    }\n    if (metadataColumns.containsKey(EColumn.PARTITION)) {\n      builder.addValue(metadataColumns.get(EColumn.PARTITION), TiType.uint64());\n    }\n    if (metadataColumns.containsKey(EColumn.OFFSET)) {\n      builder.addValue(metadataColumns.get(EColumn.OFFSET), TiType.uint64());\n    }", "    if (metadataColumns.containsKey(EColumn.OFFSET)) {\n      builder.addValue(metadataColumns.get(EColumn.OFFSET), TiType.uint64());\n    }\n    if (metadataColumns.containsKey(EColumn.TIMESTAMP)) {\n      builder.addValue(metadataColumns.get(EColumn.TIMESTAMP), TiType.uint64());\n    }\n    if (metadataColumns.containsKey(EColumn.HEADERS)) {\n      builder.addValue(metadataColumns.get(EColumn.HEADERS), TiType.optional(TiType.yson()));\n    }\n\n    return builder.build();\n  }\n", "  public enum ETableType {\n    STATIC,\n    DYNAMIC\n  }\n\n  public enum EColumn {\n    DATA(\"data\", TiType.string(), false),\n    KEY(\"_key\", TiType.string(), true),\n    TOPIC(\"_topic\", TiType.string(), true),\n    PARTITION(\"_partition\", TiType.uint64(), true),\n    OFFSET(\"_offset\", TiType.uint64(), true),\n    TIMESTAMP(\"_timestamp\", TiType.uint64(), true),\n    HEADERS(\"_headers\", TiType.optional(TiType.yson()), true),\n    SYSTEM_TIMESTAMP(\"_timestamp\", TiType.uint64(), true, ETableType.DYNAMIC),\n    SYSTEM_CUMULATIVE_DATA_WEIGHT(\"_timestamp\", TiType.uint64(), true, ETableType.DYNAMIC);\n", "    public final String name;\n\n    public final TiType type;\n    public final boolean isMetadata;\n    private final Set<ETableType> tableTypes;\n\n    // Constructor\n    EColumn(String value, TiType type, boolean isMetadata, ETableType... tableTypes) {\n      this.name = value;\n      this.type = type;\n      this.isMetadata = isMetadata;\n      this.tableTypes = Arrays.stream(tableTypes).collect(Collectors.toSet());\n    }\n\n    EColumn(String value, TiType type, boolean isMetadata) {\n      this(value, type, isMetadata, ETableType.STATIC, ETableType.DYNAMIC);\n    }\n", "    public static Map<EColumn, String> getAllMetadataColumns(ETableType tableType) {\n      return Arrays.stream(values())\n          .filter(column -> column.isMetadata && column.tableTypes.contains(tableType))\n          .collect(Collectors.toMap(column -> column, column -> column.name, (c1, c2) -> c1,\n              TreeMap::new));\n    }\n  }\n}"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/common/BaseOffsetsManager.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.common;\n\nimport java.util.Collection;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.apache.kafka.clients.consumer.OffsetAndMetadata;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport tech.ytsaurus.client.ApiServiceTransaction;", "import org.apache.kafka.connect.sink.SinkRecord;\nimport tech.ytsaurus.client.ApiServiceTransaction;\n\npublic abstract class BaseOffsetsManager {\n\n  public Collection<SinkRecord> filterRecords(Collection<SinkRecord> sinkRecords,\n      Map<TopicPartition, OffsetAndMetadata> prevOffsetsMap) {\n    return sinkRecords.stream()\n        .filter(record -> {\n          var topicPartition = new TopicPartition(record.topic(), record.kafkaPartition());\n          return !prevOffsetsMap.containsKey(topicPartition)\n              || prevOffsetsMap.get(topicPartition).offset() < record.kafkaOffset();\n        })\n        .collect(Collectors.toList());\n  }\n\n  public Map<TopicPartition, OffsetAndMetadata> getMaxOffsets(Collection<SinkRecord> sinkRecords) {\n    return sinkRecords.stream()\n        .collect(Collectors.toMap(\n            record -> new TopicPartition(record.topic(), record.kafkaPartition()),\n            record -> new OffsetAndMetadata(record.kafkaOffset()),\n            (prev, curr) -> curr.offset() > prev.offset() ? curr : prev\n        ));\n  }\n", "  public abstract Map<TopicPartition, OffsetAndMetadata> getPrevOffsets(ApiServiceTransaction trx,\n      Set<TopicPartition> topicPartitions)\n      throws InterruptedException, java.util.concurrent.ExecutionException;\n\n  public abstract void writeOffsets(ApiServiceTransaction trx,\n      Map<TopicPartition, OffsetAndMetadata> offsets)\n      throws InterruptedException, java.util.concurrent.ExecutionException;\n\n  public abstract void lockPartitions(ApiServiceTransaction trx,\n      Set<TopicPartition> partitions)\n      throws InterruptedException, java.util.concurrent.ExecutionException;\n}\n", "  public abstract void lockPartitions(ApiServiceTransaction trx,\n      Set<TopicPartition> partitions)\n      throws InterruptedException, java.util.concurrent.ExecutionException;\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/common/TableWriterManager.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.common;\n\nimport org.apache.kafka.connect.connector.ConnectorContext;\nimport org.apache.kafka.connect.errors.RetriableException;\n\npublic interface TableWriterManager {\n\n  void start() throws RetriableException;\n\n  void stop();\n\n  default void setContext(ConnectorContext context) {\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/common/BaseTableWriterConfig.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.common;\n\nimport java.time.Duration;\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.stream.Collectors;\nimport org.apache.kafka.common.config.AbstractConfig;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;", "import org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.common.utils.Utils;\nimport tech.ytsaurus.client.rpc.YTsaurusClientAuth;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.typeinfo.TiType;\n\npublic class BaseTableWriterConfig extends AbstractConfig {\n\n  public static final String AUTH_TYPE = \"yt.connection.auth.type\";\n  public static final String YT_USER = \"yt.connection.user\";", "  public static final String AUTH_TYPE = \"yt.connection.auth.type\";\n  public static final String YT_USER = \"yt.connection.user\";\n  public static final String YT_TOKEN = \"yt.connection.token\";\n  public static final String SERVICE_TICKET_PROVIDER_URL = \"yt.connection.service.ticket.provider.url\";\n  public static final String YT_CLUSTER = \"yt.connection.cluster\";\n  public static final String OUTPUT_TYPE = \"yt.sink.output.type\";\n  public static final String OUTPUT_TABLE_SCHEMA_TYPE = \"yt.sink.output.table.schema.type\";\n  public static final String KEY_OUTPUT_FORMAT = \"yt.sink.output.key.format\";\n  public static final String VALUE_OUTPUT_FORMAT = \"yt.sink.output.value.format\";\n  public static final String OUTPUT_DIRECTORY = \"yt.sink.output.directory\";\n  public static final String OUTPUT_TTL = \"yt.sink.output.ttl\";", "  public static final String VALUE_OUTPUT_FORMAT = \"yt.sink.output.value.format\";\n  public static final String OUTPUT_DIRECTORY = \"yt.sink.output.directory\";\n  public static final String OUTPUT_TTL = \"yt.sink.output.ttl\";\n  public static final String METADATA_DIRECTORY_NAME = \"yt.sink.metadata.directory.name\";\n\n  public static ConfigDef CONFIG_DEF = new ConfigDef()\n      .define(AUTH_TYPE, ConfigDef.Type.STRING, AuthType.TOKEN.name(),\n          ValidUpperString.in(AuthType.TOKEN.name(), AuthType.SERVICE_TICKET.name()),\n          ConfigDef.Importance.HIGH,\n          \"Specifies the auth type: 'token' for token authentication or 'service_ticket' for service ticket authentication\")\n      .define(YT_USER, ConfigDef.Type.STRING, null, ConfigDef.Importance.HIGH,\n          \"Username for the YT API authentication\")\n      .define(YT_TOKEN, ConfigDef.Type.PASSWORD, null, ConfigDef.Importance.HIGH,\n          \"Access token for the YT API authentication\")\n      .define(SERVICE_TICKET_PROVIDER_URL, ConfigDef.Type.PASSWORD, \"\", ConfigDef.Importance.HIGH,\n          \"URL of the service ticket provider, required if 'yt.connection.auth.type' is 'SERVICE_TICKET'\")\n      .define(YT_CLUSTER, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH,\n          \"Identifier of the YT cluster to connect to\")\n      .define(OUTPUT_TYPE, ConfigDef.Type.STRING, OutputType.DYNAMIC_TABLE.name(),\n          ValidUpperString.in(OutputType.DYNAMIC_TABLE.name(),\n              OutputType.STATIC_TABLES.name()),\n          ConfigDef.Importance.HIGH,\n          \"Specifies the output type: 'dynamic_table' for a sharded queue similar to Apache Kafka or 'static_tables' for separate time-based tables\")\n      .define(KEY_OUTPUT_FORMAT, ConfigDef.Type.STRING, OutputFormat.ANY.name(),\n          ValidUpperString.in(OutputFormat.STRING.name(), OutputFormat.ANY.name()),\n          ConfigDef.Importance.HIGH,\n          \"Determines the output format for keys: 'string' for plain string keys or 'any' for keys with no specific format\")\n      .define(VALUE_OUTPUT_FORMAT, ConfigDef.Type.STRING, OutputFormat.ANY.name(),\n          ValidUpperString.in(OutputFormat.STRING.name(), OutputFormat.ANY.name()),\n          ConfigDef.Importance.HIGH,\n          \"Determines the output format for values: 'string' for plain string values or 'any' for values with no specific format\")\n      .define(OUTPUT_TABLE_SCHEMA_TYPE, ConfigDef.Type.STRING,\n          OutputTableSchemaType.UNSTRUCTURED.name(),\n          ValidUpperString.in(OutputTableSchemaType.UNSTRUCTURED.name(),\n              OutputTableSchemaType.STRICT.name(), OutputTableSchemaType.WEAK.name()),\n          ConfigDef.Importance.HIGH,\n          \"Defines the schema type for output tables: 'unstructured' for schema-less tables, 'strict' for tables with a fixed schema, or 'weak' for tables with a flexible schema\")\n      .define(OUTPUT_DIRECTORY, ConfigDef.Type.STRING, ConfigDef.NO_DEFAULT_VALUE,\n          new YPathValidator(), ConfigDef.Importance.HIGH,\n          \"Specifies the directory path for storing the output data\")\n      .define(METADATA_DIRECTORY_NAME, ConfigDef.Type.STRING, \"__connect_sink_metadata__\",\n          ConfigDef.Importance.MEDIUM, \"Suffix for the metadata directory used by the system\")\n      .define(OUTPUT_TTL, ConfigDef.Type.STRING, \"30d\", new DurationValidator(),\n          ConfigDef.Importance.MEDIUM,\n          \"Time-to-live (TTL) for output tables or rows, specified as a duration (e.g., '30d' for 30 days)\");\n\n  public BaseTableWriterConfig(ConfigDef configDef, Map<String, String> originals) {\n    super(configDef, originals);\n", "    if (getAuthType() == AuthType.SERVICE_TICKET && getPassword(SERVICE_TICKET_PROVIDER_URL).value()\n        .isEmpty()) {\n      throw new ConfigException(SERVICE_TICKET_PROVIDER_URL, null,\n          \"Must be set when 'yt.connection.auth.type' is 'SERVICE_TICKET'\");\n    } else if (getAuthType() == AuthType.TOKEN && (get(YT_USER) == null || get(YT_TOKEN) == null)) {\n      throw new ConfigException(\n          \"Both 'yt.connection.user' and 'yt.connection.token' must be set when 'yt.connection.auth.type' is 'TOKEN'\");\n    }\n  }\n\n  public BaseTableWriterConfig(Map<String, String> originals) {\n    super(CONFIG_DEF, originals);\n  }\n", "  public String getYtUser() {\n    return getString(YT_USER);\n  }\n\n  public String getYtToken() {\n    return getPassword(YT_TOKEN).value();\n  }\n\n  public String getYtCluster() {\n    return getString(YT_CLUSTER);\n  }\n", "  public String getYtCluster() {\n    return getString(YT_CLUSTER);\n  }\n\n  public OutputType getOutputType() {\n    return OutputType.valueOf(getString(OUTPUT_TYPE).toUpperCase());\n  }\n\n  public OutputTableSchemaType getOutputTableSchemaType() {\n    return OutputTableSchemaType.valueOf(getString(OUTPUT_TABLE_SCHEMA_TYPE).toUpperCase());\n  }\n", "  public OutputTableSchemaType getOutputTableSchemaType() {\n    return OutputTableSchemaType.valueOf(getString(OUTPUT_TABLE_SCHEMA_TYPE).toUpperCase());\n  }\n\n  public OutputFormat getKeyOutputFormat() {\n    return OutputFormat.valueOf(getString(KEY_OUTPUT_FORMAT).toUpperCase());\n  }\n\n  public OutputFormat getValueOutputFormat() {\n    return OutputFormat.valueOf(getString(VALUE_OUTPUT_FORMAT).toUpperCase());\n  }\n", "  public OutputFormat getValueOutputFormat() {\n    return OutputFormat.valueOf(getString(VALUE_OUTPUT_FORMAT).toUpperCase());\n  }\n\n  public YPath getOutputDirectory() {\n    return YPath.simple(getString(OUTPUT_DIRECTORY));\n  }\n\n  public YPath getMetadataDirectory() {\n    return getOutputDirectory().child(getString(METADATA_DIRECTORY_NAME));\n  }\n", "  public YPath getMetadataDirectory() {\n    return getOutputDirectory().child(getString(METADATA_DIRECTORY_NAME));\n  }\n\n  public Duration getOutputTTL() {\n    return Util.parseHumanReadableDuration(getString(OUTPUT_TTL));\n  }\n\n  public AuthType getAuthType() {\n    return AuthType.valueOf(getString(AUTH_TYPE).toUpperCase());\n  }\n", "  public AuthType getAuthType() {\n    return AuthType.valueOf(getString(AUTH_TYPE).toUpperCase());\n  }\n\n  public String getServiceTicketProviderUrl() {\n    return getPassword(SERVICE_TICKET_PROVIDER_URL).value();\n  }\n\n  public YTsaurusClientAuth getYtClientAuth() {\n    var builder = YTsaurusClientAuth.builder();\n", "  public YTsaurusClientAuth getYtClientAuth() {\n    var builder = YTsaurusClientAuth.builder();\n\n    if (getAuthType() == AuthType.TOKEN) {\n      builder.setUser(getYtUser());\n      builder.setToken(getYtToken());\n    } else if (getAuthType() == AuthType.SERVICE_TICKET) {\n      builder.setServiceTicketAuth(new HttpServiceTicketAuth(getServiceTicketProviderUrl()));\n    } else {\n      throw new RuntimeException(\"invalid AuthType!\");\n    }\n\n    return builder.build();\n  }\n", "  public enum AuthType {\n    TOKEN,\n    SERVICE_TICKET\n  }\n\n  public enum OutputType {\n    DYNAMIC_TABLE,\n    STATIC_TABLES\n  }\n\n  public enum OutputFormat {\n    STRING,\n    ANY;\n", "  public enum OutputFormat {\n    STRING,\n    ANY;\n\n    public TiType toTiType() {\n      switch (this) {\n        case STRING:\n          return TiType.string();\n        case ANY:\n          return TiType.optional(TiType.yson());\n        default:\n          throw new IllegalArgumentException(\"Unsupported output format: \" + this);\n      }\n    }\n  }\n", "  public enum OutputTableSchemaType {\n    UNSTRUCTURED,\n    STRICT,\n    WEAK\n  }\n\n  public static class YPathValidator implements ConfigDef.Validator {\n\n    @Override\n    public void ensureValid(String name, Object value) {\n      try {\n        YPath.simple(value.toString());\n      } catch (Exception ex) {\n        throw new ConfigException(name, value, ex.toString());\n      }\n    }\n  }\n", "    public void ensureValid(String name, Object value) {\n      try {\n        YPath.simple(value.toString());\n      } catch (Exception ex) {\n        throw new ConfigException(name, value, ex.toString());\n      }\n    }\n  }\n\n  public static class DurationValidator implements ConfigDef.Validator {\n\n    @Override", "  public static class DurationValidator implements ConfigDef.Validator {\n\n    @Override\n    public void ensureValid(String name, Object value) {\n      try {\n        Util.parseHumanReadableDuration(value.toString());\n      } catch (Exception ex) {\n        throw new ConfigException(name, value, ex.toString());\n      }\n    }\n  }\n", "  public static class ValidUpperString implements ConfigDef.Validator {\n\n    final List<String> validStrings;\n\n    private ValidUpperString(List<String> validStrings) {\n      this.validStrings = validStrings.stream().map(String::toUpperCase)\n          .collect(Collectors.toList());\n    }\n\n    public static ValidUpperString in(String... validStrings) {\n      return new ValidUpperString(Arrays.asList(validStrings));\n    }\n\n    @Override", "    public static ValidUpperString in(String... validStrings) {\n      return new ValidUpperString(Arrays.asList(validStrings));\n    }\n\n    @Override\n    public void ensureValid(String name, Object o) {\n      String s = ((String) o).toUpperCase();\n      if (!validStrings.contains(s)) {\n        throw new ConfigException(name, o,\n            \"String must be one of: \" + Utils.join(validStrings, \", \"));\n      }\n    }\n", "    public String toString() {\n      return \"[\" + Utils.join(\n          validStrings.stream().map(String::toUpperCase).collect(Collectors.toList()), \", \") + \"]\";\n    }\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/staticTables/StaticTableWriterConfig.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.staticTables;\n\nimport java.time.Duration;\nimport java.util.HashMap;\nimport java.util.Map;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.connect.errors.ConnectException;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig;\nimport ru.dzen.kafka.connect.ytsaurus.common.Util;\nimport tech.ytsaurus.core.cypress.YPath;", "import ru.dzen.kafka.connect.ytsaurus.common.Util;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\npublic class StaticTableWriterConfig extends BaseTableWriterConfig {\n\n  private static final String ROTATION_PERIOD = \"yt.sink.static.rotation.period\";\n  private static final String OUTPUT_TABLES_DIRECTORY_POSTFIX = \"yt.sink.static.tables.dir.postfix\";\n  private static final String MERGE_CHUNKS = \"yt.sink.static.merge.chunks\";\n  private static final String CHUNK_MERGER_ATTRIBUTE_ENABLED = \"yt.sink.static.chunk.merger.attribute.enabled\";\n  private static final String MERGE_DATA_SIZE_PER_JOB = \"yt.sink.static.merge.data.size.per.job\";\n  private static final String SCHEMA_INFERENCE_STRATEGY = \"yt.sink.static.tables.schema.inference.strategy\";\n  private static final String COMPRESSION_CODEC = \"yt.sink.static.tables.compression.codec\";\n  private static final String OPTIMIZE_FOR = \"yt.sink.static.tables.optimize.for\";\n  private static final String REPLICATION_FACTOR = \"yt.sink.static.tables.replication.factor\";\n  private static final String ERASURE_CODEC = \"yt.sink.static.tables.erasure.codec\";\n", "  public static ConfigDef CONFIG_DEF = new ConfigDef(BaseTableWriterConfig.CONFIG_DEF)\n      .define(ROTATION_PERIOD, ConfigDef.Type.STRING, ConfigDef.Importance.HIGH, \"Rotation period\")\n      .define(OUTPUT_TABLES_DIRECTORY_POSTFIX, ConfigDef.Type.STRING, \"output\",\n          ConfigDef.Importance.MEDIUM, \"Output tables directory postfix\")\n      .define(COMPRESSION_CODEC, ConfigDef.Type.STRING, \"zstd_3\", ConfigDef.Importance.MEDIUM,\n          \"Compression codec of the output tables.\")\n      .define(OPTIMIZE_FOR, ConfigDef.Type.STRING, \"lookup\",\n          ConfigDef.ValidString.in(\"lookup\", \"scan\"), ConfigDef.Importance.MEDIUM,\n          \"Specifies the storage optimization strategy for the table. Choose 'lookup' for row-based table storage optimized for point lookups, or 'scan' for column-based table storage optimized for scans and aggregations.\")\n      .define(REPLICATION_FACTOR, ConfigDef.Type.INT, null,\n          ConfigDef.Importance.MEDIUM, \"The replication factor of the output tables.\")\n      .define(ERASURE_CODEC, ConfigDef.Type.STRING, null, ConfigDef.Importance.MEDIUM,\n          \"Erasure coding codec of the output tables.\")\n      .define(MERGE_CHUNKS, ConfigDef.Type.BOOLEAN, false, ConfigDef.Importance.MEDIUM,\n          \"Activate the consolidation of chunks during the table rotation process.\")\n      .define(CHUNK_MERGER_ATTRIBUTE_ENABLED, ConfigDef.Type.BOOLEAN, false,\n          ConfigDef.Importance.LOW, \"Set chunk_merger_mode attribute to auto\")\n      .define(MERGE_DATA_SIZE_PER_JOB, ConfigDef.Type.INT, 100, ConfigDef.Importance.MEDIUM,\n          \"Maximum size of data to be merged per job in MB.\")\n      .define(SCHEMA_INFERENCE_STRATEGY, ConfigDef.Type.STRING,\n          SchemaInferenceStrategy.DISABLED.name(),\n          ValidUpperString.in(SchemaInferenceStrategy.DISABLED.name(),\n              SchemaInferenceStrategy.INFER_FROM_FIRST_BATCH.name(),\n              SchemaInferenceStrategy.INFER_FROM_FINALIZED_TABLE.name()),\n          ConfigDef.Importance.HIGH,\n          \"The strategy for inferring the schema of the output tables. Valid options are DISABLED, INFER_FROM_FIRST_BATCH, and INFER_FROM_FINALIZED_TABLE.\\n\"\n              +\n              \"DISABLED means that the schema will not be inferred at all, and the output tables will have a weak schema that only includes the column names.\\n\"\n              +\n              \"INFER_FROM_FIRST_BATCH means that the table schema will be created from the first batch of data, and will not change after the table is created.\\n\"\n              +\n              \"INFER_FROM_FINALIZED_TABLE means that the weak schema will be used during the writing of rows, and after rotation, the finalized table will be re-merged with the schema based on all rows of the table.\");\n\n  public StaticTableWriterConfig(Map<String, String> originals) throws ConnectException {\n    super(CONFIG_DEF, originals);\n", "    if (getOutputTableSchemaType().equals(OutputTableSchemaType.STRICT)) {\n      if (!getValueOutputFormat().equals(OutputFormat.ANY)) {\n        throw new ConnectException(\n            \"When using the STRICT table schema type, the value format must be set to ANY!\");\n      }\n    } else if (!getSchemaInferenceStrategy().equals(SchemaInferenceStrategy.DISABLED)) {\n      throw new ConnectException(\n          \"Schema inference strategy could be enabled only when using STRICT schema type\");\n    }\n  }\n", "  public YPath getOutputTablesDirectory() {\n    return getOutputDirectory().child(getString(OUTPUT_TABLES_DIRECTORY_POSTFIX));\n  }\n\n  public YPath getOffsetsDirectory() {\n    return getMetadataDirectory().child(\"offsets\");\n  }\n\n  public Duration getRotationPeriod() {\n    return Util.parseHumanReadableDuration(getString(ROTATION_PERIOD));\n  }\n", "  public Duration getRotationPeriod() {\n    return Util.parseHumanReadableDuration(getString(ROTATION_PERIOD));\n  }\n\n  public YPath getSchemasDirectory() {\n    return getMetadataDirectory().child(\"schemas\");\n  }\n\n  public SchemaInferenceStrategy getSchemaInferenceStrategy() {\n    String strategyName = getString(SCHEMA_INFERENCE_STRATEGY);\n    return SchemaInferenceStrategy.valueOf(strategyName);\n  }\n", "  public SchemaInferenceStrategy getSchemaInferenceStrategy() {\n    String strategyName = getString(SCHEMA_INFERENCE_STRATEGY);\n    return SchemaInferenceStrategy.valueOf(strategyName);\n  }\n\n  public boolean getNeedToRunMerge() {\n    return getBoolean(MERGE_CHUNKS) || getSchemaInferenceStrategy().equals(\n        SchemaInferenceStrategy.INFER_FROM_FINALIZED_TABLE);\n  }\n\n  public int getMergeDataSizePerJob() {\n    return getInt(MERGE_DATA_SIZE_PER_JOB);\n  }\n\n  public Map<String, YTreeNode> getExtraTablesAttributes() {\n    Map<String, YTreeNode> extraAttributes = new HashMap<>();\n    extraAttributes.put(\"compression_codec\", YTree.node(getString(COMPRESSION_CODEC)));\n    extraAttributes.put(\"optimize_for\", YTree.node(getString(OPTIMIZE_FOR)));\n\n    Integer replicationFactor = getInt(REPLICATION_FACTOR);", "  public int getMergeDataSizePerJob() {\n    return getInt(MERGE_DATA_SIZE_PER_JOB);\n  }\n\n  public Map<String, YTreeNode> getExtraTablesAttributes() {\n    Map<String, YTreeNode> extraAttributes = new HashMap<>();\n    extraAttributes.put(\"compression_codec\", YTree.node(getString(COMPRESSION_CODEC)));\n    extraAttributes.put(\"optimize_for\", YTree.node(getString(OPTIMIZE_FOR)));\n\n    Integer replicationFactor = getInt(REPLICATION_FACTOR);\n    if (replicationFactor != null) {\n      extraAttributes.put(\"replication_factor\", YTree.node(replicationFactor));\n    }\n\n    String erasureCodec = getString(ERASURE_CODEC);", "    if (replicationFactor != null) {\n      extraAttributes.put(\"replication_factor\", YTree.node(replicationFactor));\n    }\n\n    String erasureCodec = getString(ERASURE_CODEC);\n    if (erasureCodec != null) {\n      extraAttributes.put(\"erasure_codec\", YTree.node(erasureCodec));\n    }\n\n    return extraAttributes;\n  }\n", "  public boolean getChunkMergerAttributeEnabled() {\n    return getBoolean(CHUNK_MERGER_ATTRIBUTE_ENABLED);\n  }\n\n  public enum SchemaInferenceStrategy {\n    DISABLED,\n    INFER_FROM_FIRST_BATCH,\n    INFER_FROM_FINALIZED_TABLE\n  }\n\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/staticTables/DocumentOffsetsManager.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.staticTables;\n\n\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.concurrent.ExecutionException;\nimport org.apache.kafka.clients.consumer.OffsetAndMetadata;\nimport org.apache.kafka.common.TopicPartition;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseOffsetsManager;", "import org.apache.kafka.common.TopicPartition;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseOffsetsManager;\nimport tech.ytsaurus.client.ApiServiceTransaction;\nimport tech.ytsaurus.client.request.CreateNode;\nimport tech.ytsaurus.client.request.LockNode;\nimport tech.ytsaurus.core.cypress.CypressNodeType;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.core.request.LockMode;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeMapNode;", "import tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeMapNode;\n\npublic class DocumentOffsetsManager extends BaseOffsetsManager {\n\n  private final YPath pathToOffsetsDirectory;\n\n  DocumentOffsetsManager(YPath pathToOffsetsDirectory) {\n    this.pathToOffsetsDirectory = pathToOffsetsDirectory;\n  }\n\n  @Override\n  public Map<TopicPartition, OffsetAndMetadata> getPrevOffsets(ApiServiceTransaction trx,\n      Set<TopicPartition> topicPartitions) throws InterruptedException, ExecutionException {\n    var res = new HashMap<TopicPartition, OffsetAndMetadata>();", "    for (var topicPartition : topicPartitions) {\n      var path = pathToOffsetsDirectory + \"/\" + topicPartition.partition();\n      if (trx.existsNode(path).get()) {\n        var mapNode = (YTreeMapNode) trx.getNode(path).get();\n        var offset = mapNode.get(\"offset\").get().longValue();\n        res.put(topicPartition, new OffsetAndMetadata(offset));\n      }\n    }\n    return res;\n  }\n\n  @Override", "  public void writeOffsets(ApiServiceTransaction trx,\n      Map<TopicPartition, OffsetAndMetadata> offsets)\n      throws InterruptedException, ExecutionException {\n    for (var entry : offsets.entrySet()) {\n      var topicPartition = entry.getKey();\n      var offsetAndMetadata = entry.getValue();\n      var path = pathToOffsetsDirectory + \"/\" + topicPartition.partition();\n      trx.createNode(\n          CreateNode.builder().setPath(YPath.simple(path)).setType(CypressNodeType.DOCUMENT)\n              .setIgnoreExisting(true).build()).get();\n      trx.setNode(path,\n          YTree.mapBuilder().key(\"offset\").value(YTree.longNode(offsetAndMetadata.offset()))\n              .buildMap()).get();\n    }\n  }\n\n  @Override", "  public void lockPartitions(ApiServiceTransaction trx, Set<TopicPartition> topicPartitions)\n      throws InterruptedException, ExecutionException {\n    for (var topicPartition : topicPartitions) {\n      trx.lockNode(LockNode.builder().setPath(pathToOffsetsDirectory).setMode(LockMode.Shared)\n          .setChildKey(topicPartition.topic() + \"-\" + topicPartition.partition())\n          .build()).get();\n    }\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/staticTables/SchemaManager.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.staticTables;\n\n\nimport java.util.ArrayList;\nimport java.util.Set;\nimport java.util.concurrent.ExecutionException;\nimport org.apache.kafka.common.TopicPartition;\nimport tech.ytsaurus.client.ApiServiceTransaction;\nimport tech.ytsaurus.client.request.CreateNode;\nimport tech.ytsaurus.core.cypress.CypressNodeType;", "import tech.ytsaurus.client.request.CreateNode;\nimport tech.ytsaurus.core.cypress.CypressNodeType;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.core.tables.TableSchema;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\npublic class SchemaManager {\n\n  private final YPath pathToSchemasDirectory;\n\n  SchemaManager(YPath pathToSchemasDirectory) {\n    this.pathToSchemasDirectory = pathToSchemasDirectory;\n  }\n", "  public TableSchema getPrevSchema(ApiServiceTransaction trx)\n      throws InterruptedException, ExecutionException {\n    var schemas = new ArrayList<YTreeNode>();\n    for (var child : trx.listNode(pathToSchemasDirectory).get().asList()) {\n      var path = pathToSchemasDirectory.child(child.stringValue());\n      if (trx.existsNode(path.toString()).get()) {\n        schemas.add(trx.getNode(path).get());\n      }\n    }\n    return new InferredSchemaBuilder(schemas).build();\n  }\n", "  public void writeSchema(ApiServiceTransaction trx,\n      TableSchema schema, Set<TopicPartition> topicPartitions)\n      throws InterruptedException, ExecutionException {\n    for (var topicPartition : topicPartitions) {\n      var path = pathToSchemasDirectory.child(topicPartition.partition());\n      trx.createNode(\n          CreateNode.builder().setPath(path).setType(CypressNodeType.DOCUMENT)\n              .setIgnoreExisting(true).build()).get();\n      trx.setNode(path.toString(), schema.toYTree()).get();\n    }\n  }\n}"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/staticTables/StaticTableWriter.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.staticTables;\n\nimport java.time.Instant;\nimport java.util.Collection;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.stream.Collectors;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.connect.data.Schema.Type;\nimport org.apache.kafka.connect.sink.SinkRecord;", "import org.apache.kafka.connect.data.Schema.Type;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriter;\nimport ru.dzen.kafka.connect.ytsaurus.common.BaseTableWriterConfig.OutputTableSchemaType;\nimport ru.dzen.kafka.connect.ytsaurus.common.TableWriterManager;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.EColumn;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.ETableType;", "import ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.EColumn;\nimport ru.dzen.kafka.connect.ytsaurus.common.UnstructuredTableSchema.ETableType;\nimport ru.dzen.kafka.connect.ytsaurus.common.Util;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriterConfig.SchemaInferenceStrategy;\nimport tech.ytsaurus.client.ApiServiceTransaction;\nimport tech.ytsaurus.client.request.CreateNode;\nimport tech.ytsaurus.client.request.Format;\nimport tech.ytsaurus.client.request.SerializationContext;\nimport tech.ytsaurus.client.request.WriteTable;\nimport tech.ytsaurus.core.cypress.CypressNodeType;", "import tech.ytsaurus.client.request.WriteTable;\nimport tech.ytsaurus.core.cypress.CypressNodeType;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.core.request.LockMode;\nimport tech.ytsaurus.core.rows.YTreeMapNodeSerializer;\nimport tech.ytsaurus.core.tables.TableSchema;\nimport tech.ytsaurus.typeinfo.TiType;\nimport tech.ytsaurus.ysontree.YTree;\nimport tech.ytsaurus.ysontree.YTreeMapNode;\n\npublic class StaticTableWriter extends BaseTableWriter {\n\n  private static final Logger log = LoggerFactory.getLogger(StaticTableWriter.class);\n", "import tech.ytsaurus.ysontree.YTreeMapNode;\n\npublic class StaticTableWriter extends BaseTableWriter {\n\n  private static final Logger log = LoggerFactory.getLogger(StaticTableWriter.class);\n\n  public final StaticTableWriterConfig config;\n  protected final SchemaManager schemaManager;\n  private final TableSchema unstructuredTableSchema;\n\n  public StaticTableWriter(StaticTableWriterConfig config) {\n    super(config, new DocumentOffsetsManager(config.getOffsetsDirectory()));\n    this.config = config;\n\n    unstructuredTableSchema = UnstructuredTableSchema.createDataQueueTableSchema(\n        config.getKeyOutputFormat(), config.getValueOutputFormat(), EColumn.getAllMetadataColumns(\n            ETableType.STATIC));\n\n    schemaManager = new SchemaManager(config.getSchemasDirectory());\n  }\n\n  protected Instant getNow() throws Exception {\n    return client.generateTimestamps().get().getInstant();\n  }\n\n  protected YPath getOutputTablePath(Instant now) {\n    var rotationPeriod = config.getRotationPeriod();\n    var dateOnly = rotationPeriod.getSeconds() % (60 * 60 * 25) == 0;\n    var tableName = Util.formatDateTime(Util.floorByDuration(now, rotationPeriod), dateOnly);\n    return YPath.simple(config.getOutputTablesDirectory() + \"/\" + tableName);\n  }\n\n  @Override\n  protected void writeRows(ApiServiceTransaction trx, Collection<SinkRecord> records,\n      Set<TopicPartition> topicPartitions)\n      throws Exception {\n    var mapNodesToWrite = recordsToRows(records);\n    var rows = mapNodesToWrite.stream().map(x -> (YTreeMapNode) YTree.node(x))\n        .collect(Collectors.toList());\n\n    var inferredSchemaBuilder = new InferredSchemaBuilder();", "    if (config.getSchemaInferenceStrategy()\n        .equals(SchemaInferenceStrategy.INFER_FROM_FINALIZED_TABLE)) {\n      inferredSchemaBuilder = new InferredSchemaBuilder(schemaManager.getPrevSchema(trx));\n      inferredSchemaBuilder.update(rows);\n    }\n\n    var outputPath = getOutputTablePath(getNow());\n    if (trx.existsNode(outputPath.toString()).get()) {\n      trx.lockNode(outputPath.toString(), LockMode.Shared).get();\n      if (trx.getNode(outputPath.allAttributes().toString()).get().mapNode().get(\"final\")\n          .isPresent()) {\n        throw new RuntimeException(\"Tried to modify finalized table!\");\n      }\n    } else {\n      var createNodeBuilder = CreateNode.builder().setType(CypressNodeType.TABLE)\n          .setPath(outputPath);", "      if (trx.getNode(outputPath.allAttributes().toString()).get().mapNode().get(\"final\")\n          .isPresent()) {\n        throw new RuntimeException(\"Tried to modify finalized table!\");\n      }\n    } else {\n      var createNodeBuilder = CreateNode.builder().setType(CypressNodeType.TABLE)\n          .setPath(outputPath);\n      if (config.getSchemaInferenceStrategy()\n          .equals(SchemaInferenceStrategy.INFER_FROM_FIRST_BATCH)) {\n        inferredSchemaBuilder.update(rows);\n        var inferredSchema = inferredSchemaBuilder.build();\n        log.warn(\n            \"Automatically inferred schema with %d columns from first {} rows: {}\",\n            inferredSchema.getColumns().size(), rows.size(),\n            inferredSchema.toYTree().toString());\n        createNodeBuilder.setAttributes(Map.of(\"schema\", inferredSchema.toYTree()));", "      } else if (config.getOutputTableSchemaType().equals(OutputTableSchemaType.UNSTRUCTURED)) {\n        createNodeBuilder.setAttributes(Map.of(\"schema\", unstructuredTableSchema.toYTree()));\n      } else if (config.getOutputTableSchemaType().equals(OutputTableSchemaType.STRICT)\n          && !config.getSchemaInferenceStrategy()\n          .equals(SchemaInferenceStrategy.INFER_FROM_FINALIZED_TABLE)) {\n        createNodeBuilder.setAttributes(\n            Map.of(\"schema\", createStrictTableSchemaFromRecordsSchema(records).toYTree()));\n      }\n      for (var entry : config.getExtraTablesAttributes().entrySet()) {\n        createNodeBuilder.addAttribute(entry.getKey(), entry.getValue());\n      }\n      trx.createNode(createNodeBuilder.build()).get();\n    }\n\n    var writeTable = WriteTable.<YTreeMapNode>builder()\n        .setPath(outputPath.plusAdditionalAttribute(\"append\", true))\n        .setSerializationContext(\n            new SerializationContext<>(new YTreeMapNodeSerializer(), Format.ysonBinary()))\n        .build();\n    var writer = trx.writeTable(writeTable).get();\n", "      for (var entry : config.getExtraTablesAttributes().entrySet()) {\n        createNodeBuilder.addAttribute(entry.getKey(), entry.getValue());\n      }\n      trx.createNode(createNodeBuilder.build()).get();\n    }\n\n    var writeTable = WriteTable.<YTreeMapNode>builder()\n        .setPath(outputPath.plusAdditionalAttribute(\"append\", true))\n        .setSerializationContext(\n            new SerializationContext<>(new YTreeMapNodeSerializer(), Format.ysonBinary()))\n        .build();\n    var writer = trx.writeTable(writeTable).get();\n", "    try {\n      while (true) {\n        // It is necessary to wait for readyEvent before trying to write.\n        writer.readyEvent().get();\n\n        // If false is returned, then readyEvent must be waited for before trying again.\n        var accepted = config.getOutputTableSchemaType().equals(OutputTableSchemaType.WEAK)\n            ? writer.write(rows)\n            : writer.write(rows, unstructuredTableSchema);\n\n        if (accepted) {\n          break;\n        }\n      }\n    } catch (Exception ex) {\n      throw new RuntimeException(ex);\n    } finally {\n      // Waiting for completion of writing. An exception might be thrown if something goes wrong.\n      writer.close().get();\n    }\n", "        if (accepted) {\n          break;\n        }\n      }\n    } catch (Exception ex) {\n      throw new RuntimeException(ex);\n    } finally {\n      // Waiting for completion of writing. An exception might be thrown if something goes wrong.\n      writer.close().get();\n    }\n", "    if (config.getSchemaInferenceStrategy()\n        .equals(SchemaInferenceStrategy.INFER_FROM_FINALIZED_TABLE)) {\n      schemaManager.writeSchema(trx, inferredSchemaBuilder.build(), topicPartitions);\n    }\n  }\n\n  private TableSchema createStrictTableSchemaFromRecordsSchema(Collection<SinkRecord> records) {\n    var tableSchemaBuilder = UnstructuredTableSchema.createDataQueueTableSchema(\n            config.getKeyOutputFormat(), null, EColumn.getAllMetadataColumns(ETableType.STATIC))\n        .toBuilder();\n    var firstRecord = records.stream().iterator().next();\n    var firstRecordValueSchema = firstRecord.valueSchema();", "    for (var field : firstRecordValueSchema.fields()) {\n      var columnType = recordSchemaTypeToColumnType(field.schema().type());\n      if (field.schema().isOptional() || columnType.isYson()) {\n        columnType = TiType.optional(columnType);\n      }\n      tableSchemaBuilder.addValue(field.name(), columnType);\n    }\n    return tableSchemaBuilder.build();\n  }\n\n  private TiType recordSchemaTypeToColumnType(Type type) {\n    switch (type) {\n      case INT8:\n        return TiType.int8();\n      case INT16:\n        return TiType.int16();\n      case INT32:\n        return TiType.int32();\n      case INT64:\n        return TiType.int64();\n      case FLOAT32:\n        return TiType.floatType();\n      case FLOAT64:\n        return TiType.doubleType();\n      case BOOLEAN:\n        return TiType.bool();\n      case STRING:\n      case BYTES:\n        return TiType.string();\n    }\n    return TiType.yson();\n  }\n\n  @Override", "  public TableWriterManager getManager() {\n    return new StaticTableWriterManager(config);\n  }\n}\n"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/staticTables/StaticTableWriterManager.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.staticTables;\n\nimport java.time.Duration;\nimport java.time.Instant;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.ScheduledExecutorService;\nimport java.util.concurrent.TimeUnit;\nimport org.apache.kafka.connect.connector.ConnectorContext;", "import java.util.concurrent.TimeUnit;\nimport org.apache.kafka.connect.connector.ConnectorContext;\nimport org.apache.kafka.connect.errors.RetriableException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport ru.dzen.kafka.connect.ytsaurus.common.TableWriterManager;\nimport ru.dzen.kafka.connect.ytsaurus.common.Util;\nimport ru.dzen.kafka.connect.ytsaurus.staticTables.StaticTableWriterConfig.SchemaInferenceStrategy;\nimport tech.ytsaurus.client.operations.MergeSpec;\nimport tech.ytsaurus.client.operations.OperationStatus;", "import tech.ytsaurus.client.operations.MergeSpec;\nimport tech.ytsaurus.client.operations.OperationStatus;\nimport tech.ytsaurus.client.request.ColumnFilter;\nimport tech.ytsaurus.client.request.CreateNode;\nimport tech.ytsaurus.client.request.ListNode;\nimport tech.ytsaurus.client.request.LockNode;\nimport tech.ytsaurus.client.request.MergeOperation;\nimport tech.ytsaurus.core.DataSize;\nimport tech.ytsaurus.core.cypress.CypressNodeType;\nimport tech.ytsaurus.core.cypress.YPath;", "import tech.ytsaurus.core.cypress.CypressNodeType;\nimport tech.ytsaurus.core.cypress.YPath;\nimport tech.ytsaurus.core.request.LockMode;\nimport tech.ytsaurus.ysontree.YTree;\n\npublic class StaticTableWriterManager extends StaticTableWriter implements TableWriterManager {\n\n  private static final Logger log = LoggerFactory.getLogger(StaticTableWriterManager.class);\n\n  private final ScheduledExecutorService scheduler;\n  private ConnectorContext context;\n\n\n  public StaticTableWriterManager(StaticTableWriterConfig config) {\n    super(config);\n\n    this.scheduler = Executors.newScheduledThreadPool(1);\n  }\n\n  private void rotate() {", "    try {\n      var now = getNow();\n      var currentTablePath = getOutputTablePath(now);\n      var retriesCount = 5;\n      for (var i = 1; i <= retriesCount; i++) {\n        log.info(\"[Try {} of {}] Freezing tables...\", i, retriesCount);\n        var numberOfTablesToFreeze = 0;\n        try (var trx = this.createTransaction()) {\n          var allTables = trx.listNode(\n                  ListNode.builder()\n                      .setPath(config.getOutputTablesDirectory())\n                      .setAttributes(ColumnFilter.of(\"final\"))\n                      .build())\n              .get().asList();", "          for (var table : allTables) {\n            if (table.getAttribute(\"final\").isPresent() || config.getOutputTablesDirectory()\n                .child(table.stringValue()).toString().compareTo(currentTablePath.toString())\n                >= 0) {\n              continue;\n            }\n            var tablePath = config.getOutputTablesDirectory().child(table.stringValue());\n            log.info(\"Freezing table {}\", tablePath);\n            Util.waitAndLock(trx, LockNode.builder()\n                .setPath(tablePath)\n                .setWaitable(true)\n                .setMode(LockMode.Exclusive)\n                .build(), Duration.ofMinutes(2));\n\n            var outputTableAttributes = new HashMap<>(Map.of(\n                \"final\", YTree.booleanNode(true),\n                \"expiration_time\", YTree.node(now.toEpochMilli() + config.getOutputTTL().toMillis())\n            ));\n            outputTableAttributes.putAll(config.getExtraTablesAttributes());", "            if (config.getNeedToRunMerge()) {\n              if (config.getSchemaInferenceStrategy()\n                  .equals(SchemaInferenceStrategy.INFER_FROM_FINALIZED_TABLE)) {\n                outputTableAttributes.put(\"schema\", schemaManager.getPrevSchema(trx).toYTree());\n              } else {\n                outputTableAttributes.put(\"schema\",\n                    trx.getNode(tablePath.attribute(\"schema\")).get());\n              }\n              log.info(\"Running merge for {}\", tablePath);\n              var oldTablePath = YPath.simple(tablePath + \".old\");\n              trx.moveNode(tablePath.toString(), oldTablePath.toString()).get();\n              trx.createNode(\n                  CreateNode.builder().setPath(tablePath).setAttributes(outputTableAttributes)\n                      .setType(CypressNodeType.TABLE).build()).get();\n              var mergeOperation = trx.startMerge(MergeOperation.builder().setSpec(\n                  MergeSpec.builder().setInputTables(oldTablePath).setOutputTable(tablePath)\n                      .setDataSizePerJob(DataSize.fromMegaBytes(config.getMergeDataSizePerJob()))\n                      .setCombineChunks(true)\n                      .build()).build()).get();\n              log.info(\"Merge operation {} for {} started\", mergeOperation.getId(),\n                  tablePath);\n              mergeOperation.watch().get();\n              var mergeOperationStatus = mergeOperation.getStatus().get();", "              if (mergeOperationStatus.equals(OperationStatus.COMPLETED)) {\n                log.info(\"Completed merge for {}\", tablePath);\n              } else {\n                var mergeOperationResult = mergeOperation.getResult().get();\n                var errorMessage = \"Merge for \" + tablePath + \" failed: \" + mergeOperationResult;\n                log.error(errorMessage);\n                throw new Exception(errorMessage);\n              }\n              trx.removeNode(oldTablePath.toString()).get();\n            } else {\n              for (var entry : outputTableAttributes.entrySet()) {\n                trx.setNode(tablePath.attribute(entry.getKey()).toString(), entry.getValue()).get();\n              }\n            }\n\n            numberOfTablesToFreeze++;\n          }\n          trx.commit().get();\n          log.info(\"Frozen {} tables\", numberOfTablesToFreeze);\n          break;\n        } catch (Exception e) {\n          log.warn(\"Can't freeze tables\", e);", "              for (var entry : outputTableAttributes.entrySet()) {\n                trx.setNode(tablePath.attribute(entry.getKey()).toString(), entry.getValue()).get();\n              }\n            }\n\n            numberOfTablesToFreeze++;\n          }\n          trx.commit().get();\n          log.info(\"Frozen {} tables\", numberOfTablesToFreeze);\n          break;\n        } catch (Exception e) {\n          log.warn(\"Can't freeze tables\", e);", "          if (i == retriesCount) {\n            throw e;\n          }\n          try {\n            Thread.sleep(5000);\n          } catch (InterruptedException ex) {\n            ex.printStackTrace();\n          }\n        }\n      }\n    } catch (Exception e) {\n      this.context.raiseError(e);\n    }\n  }\n\n  @Override", "  public void start() throws RetriableException {\n    // Perform any initialization or setup needed for StaticTablesQueueManager\n    try {\n      var createNodeBuilder = CreateNode.builder()\n          .setPath(config.getOutputTablesDirectory())\n          .setType(CypressNodeType.MAP)\n          .setRecursive(true)\n          .setIgnoreExisting(true);\n      client.createNode(createNodeBuilder.build()).get();\n      log.info(\"Created output tables directory {}\", config.getOutputTablesDirectory());\n      if (config.getChunkMergerAttributeEnabled()) {\n        var chunkModeAttrPath = config.getOutputTablesDirectory().attribute(\"chunk_merger_mode\");\n        client.setNode(chunkModeAttrPath.toString(),\n            YTree.node(\"auto\")).get();\n        log.info(\"Set {} to auto\", chunkModeAttrPath);\n      }\n\n    } catch (Exception e) {\n      throw new RetriableException(e);\n    }", "      if (config.getChunkMergerAttributeEnabled()) {\n        var chunkModeAttrPath = config.getOutputTablesDirectory().attribute(\"chunk_merger_mode\");\n        client.setNode(chunkModeAttrPath.toString(),\n            YTree.node(\"auto\")).get();\n        log.info(\"Set {} to auto\", chunkModeAttrPath);\n      }\n\n    } catch (Exception e) {\n      throw new RetriableException(e);\n    }\n    try {\n      var createNodeBuilder = CreateNode.builder()\n          .setPath(config.getOffsetsDirectory())\n          .setType(CypressNodeType.MAP)\n          .setRecursive(true)\n          .setIgnoreExisting(true);\n      client.createNode(createNodeBuilder.build()).get();\n      log.info(\"Created offsets directory {}\", config.getOffsetsDirectory());\n    } catch (Exception e) {\n      throw new RetriableException(e);\n    }", "    try {\n      var createNodeBuilder = CreateNode.builder()\n          .setPath(config.getOffsetsDirectory())\n          .setType(CypressNodeType.MAP)\n          .setRecursive(true)\n          .setIgnoreExisting(true);\n      client.createNode(createNodeBuilder.build()).get();\n      log.info(\"Created offsets directory {}\", config.getOffsetsDirectory());\n    } catch (Exception e) {\n      throw new RetriableException(e);\n    }", "    if (config.getSchemaInferenceStrategy()\n        .equals(SchemaInferenceStrategy.INFER_FROM_FINALIZED_TABLE)) {\n      try {\n        var createNodeBuilder = CreateNode.builder()\n            .setPath(config.getSchemasDirectory())\n            .setType(CypressNodeType.MAP)\n            .setRecursive(true)\n            .setIgnoreExisting(true);\n        client.createNode(createNodeBuilder.build()).get();\n        log.info(\"Created schemas directory {}\", config.getSchemasDirectory());\n      } catch (Exception e) {\n        throw new RetriableException(e);\n      }\n    }\n    // Start the periodic task\n    var periodMillis = config.getRotationPeriod().toMillis();\n    var delayMillis = periodMillis - (Instant.now().toEpochMilli() % periodMillis);\n    scheduler.scheduleAtFixedRate(this::rotate, delayMillis, periodMillis, TimeUnit.MILLISECONDS);\n  }\n\n  @Override", "  public void stop() {\n    // Perform any cleanup, if necessary\n    scheduler.shutdown();\n  }\n\n  @Override\n  public void setContext(ConnectorContext context) {\n    this.context = context;\n  }\n}"]}
{"filename": "src/main/java/ru/dzen/kafka/connect/ytsaurus/staticTables/InferredSchemaBuilder.java", "chunked_list": ["package ru.dzen.kafka.connect.ytsaurus.staticTables;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport tech.ytsaurus.core.tables.TableSchema;\nimport tech.ytsaurus.typeinfo.TiType;\nimport tech.ytsaurus.ysontree.YTreeIntegerNode;\nimport tech.ytsaurus.ysontree.YTreeMapNode;\nimport tech.ytsaurus.ysontree.YTreeNode;", "import tech.ytsaurus.ysontree.YTreeMapNode;\nimport tech.ytsaurus.ysontree.YTreeNode;\n\npublic class InferredSchemaBuilder {\n\n  private final Map<String, TiType> inferredSchema;\n\n  public InferredSchemaBuilder() {\n    inferredSchema = new HashMap<>();\n  }\n\n  public InferredSchemaBuilder(YTreeNode schemaNode) {\n    inferredSchema = new HashMap<>();", "    for (var columnSchema : TableSchema.fromYTree(schemaNode).getColumns()) {\n      inferredSchema.put(columnSchema.getName(), columnSchema.getTypeV3());\n    }\n  }\n\n  public InferredSchemaBuilder(TableSchema schema) {\n    this(schema.toYTree());\n  }\n\n  public InferredSchemaBuilder(List<YTreeNode> schemaNodes) {\n    inferredSchema = new HashMap<>();", "    for (var node : schemaNodes) {\n      var schemaBuilder = new InferredSchemaBuilder(node);\n      for (var entry : schemaBuilder.inferredSchema.entrySet()) {\n        updateSchema(entry.getKey(), entry.getValue());\n      }\n    }\n  }\n\n  public void update(YTreeMapNode mapNode) {\n    for (String key : mapNode.asMap().keySet()) {\n      var valueType = detectColumnType(mapNode.get(key).get());\n      updateSchema(key, valueType);\n    }\n  }\n", "  public void update(YTreeMapNode mapNode) {\n    for (String key : mapNode.asMap().keySet()) {\n      var valueType = detectColumnType(mapNode.get(key).get());\n      updateSchema(key, valueType);\n    }\n  }\n\n  public void update(List<YTreeMapNode> mapNodes) {\n    for (var mapNode : mapNodes) {\n      update(mapNode);\n    }\n  }\n", "    for (var mapNode : mapNodes) {\n      update(mapNode);\n    }\n  }\n\n  public TableSchema build() {\n    var builder = TableSchema.builder();\n    for (Map.Entry<String, TiType> entry : inferredSchema.entrySet()) {\n      builder.addValue(entry.getKey(), entry.getValue());\n    }\n    return builder.build();\n  }\n\n  private TiType detectColumnType(YTreeNode value) {", "    if (value.isStringNode()) {\n      return TiType.optional(TiType.string());\n    } else if (value.isIntegerNode()) {\n      if (((YTreeIntegerNode) value).isSigned()) {\n        return TiType.optional(TiType.int64());\n      }\n      return TiType.optional(TiType.uint64());\n    } else if (value.isDoubleNode()) {\n      return TiType.optional(TiType.doubleType());\n    } else if (value.isBooleanNode()) {\n      return TiType.optional(TiType.bool());\n    } else {\n      return TiType.optional(TiType.yson());\n    }\n  }\n\n  private void updateSchema(String key, TiType valueType) {\n    var existingType = inferredSchema.get(key);", "    } else if (value.isBooleanNode()) {\n      return TiType.optional(TiType.bool());\n    } else {\n      return TiType.optional(TiType.yson());\n    }\n  }\n\n  private void updateSchema(String key, TiType valueType) {\n    var existingType = inferredSchema.get(key);\n    if (existingType == null) {\n      inferredSchema.put(key, valueType);", "    if (existingType == null) {\n      inferredSchema.put(key, valueType);\n    } else if (!existingType.equals(valueType)) {\n      inferredSchema.put(key, TiType.optional(TiType.yson()));\n    }\n  }\n}\n"]}
