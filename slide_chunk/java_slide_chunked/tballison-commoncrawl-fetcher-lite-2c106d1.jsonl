{"filename": "src/test/java/org/tallison/cc/index/FetcherConfigTest.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index;\n", "package org.tallison.cc.index;\n\nimport static org.junit.jupiter.api.Assertions.assertEquals;\n\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.junit.jupiter.api.Test;\nimport org.tallison.cc.index.extractor.ExtractorConfig;", "import org.junit.jupiter.api.Test;\nimport org.tallison.cc.index.extractor.ExtractorConfig;\nimport org.tallison.cc.index.io.BackoffHttpFetcher;\n\nimport org.apache.tika.pipes.emitter.fs.FileSystemEmitter;\nimport org.apache.tika.pipes.emitter.s3.S3Emitter;\nimport org.apache.tika.pipes.fetcher.s3.S3Fetcher;\n\npublic class FetcherConfigTest {\n\n    @Test", "public class FetcherConfigTest {\n\n    @Test\n    public void testBasic() throws Exception {\n        Path p = Paths.get(getClass().getResource(\"/configs/basic-http.json\").toURI());\n        ExtractorConfig fetcherConfig = new ObjectMapper().readValue(p.toFile(), ExtractorConfig.class);\n\n        assertEquals(BackoffHttpFetcher.class, fetcherConfig.newFetcher().getClass());\n        assertEquals(FileSystemEmitter.class, fetcherConfig.newEmitter().getClass());\n    }\n\n    @Test", "    public void testLocalIndices() throws Exception {\n        Path p = Paths.get(getClass().getResource(\"/configs/basic-local.json\").toURI());\n        ExtractorConfig fetcherConfig = new ObjectMapper().readValue(p.toFile(), ExtractorConfig.class);\n\n        //TODO -- add actual unit test that tests FSFetcher\n        assertEquals(BackoffHttpFetcher.class, fetcherConfig.newFetcher().getClass());\n        assertEquals(FileSystemEmitter.class, fetcherConfig.newEmitter().getClass());\n    }\n\n    @Test\n    public void testS3() throws Exception {\n        Path p = Paths.get(getClass().getResource(\"/configs/basic-s3.json\").toURI());\n        ExtractorConfig fetcherConfig = new ObjectMapper().readValue(p.toFile(), ExtractorConfig.class);\n\n        //TODO -- add actual unit test that tests fetcher and emitter\n        assertEquals(S3Fetcher.class, fetcherConfig.newFetcher().getClass());\n        assertEquals(S3Emitter.class, fetcherConfig.newEmitter().getClass());\n    }\n}\n", "    public void testS3() throws Exception {\n        Path p = Paths.get(getClass().getResource(\"/configs/basic-s3.json\").toURI());\n        ExtractorConfig fetcherConfig = new ObjectMapper().readValue(p.toFile(), ExtractorConfig.class);\n\n        //TODO -- add actual unit test that tests fetcher and emitter\n        assertEquals(S3Fetcher.class, fetcherConfig.newFetcher().getClass());\n        assertEquals(S3Emitter.class, fetcherConfig.newEmitter().getClass());\n    }\n}\n"]}
{"filename": "src/test/java/org/tallison/cc/index/selector/IndexRecordSelectorTest.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.selector;\n", "package org.tallison.cc.index.selector;\n\nimport java.io.BufferedReader;\nimport java.io.InputStreamReader;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.Optional;\nimport java.util.zip.GZIPInputStream;", "import java.util.Optional;\nimport java.util.zip.GZIPInputStream;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.junit.jupiter.api.Disabled;\nimport org.junit.jupiter.api.Test;\nimport org.tallison.cc.index.CCIndexRecord;\n\nimport org.apache.tika.utils.StringUtils;\n", "import org.apache.tika.utils.StringUtils;\n\n\npublic class IndexRecordSelectorTest {\n\n    @Test\n    public void testBasic() throws Exception {\n        ObjectMapper mapper = new ObjectMapper();\n\n        RecordSelector recordSelector =\n                mapper.readValue(getClass().getResourceAsStream(\"/selectors/basic.json\"),\n                        RecordSelector.class);\n    }\n\n    @Test\n    @Disabled(\"for development only\")", "    public void testIndexFile() throws Exception {\n        Path p = Paths.get(\"/Users/allison/data/cc/index-work/cdx-00000.gz\");\n        try (BufferedReader r = new BufferedReader(\n                new InputStreamReader(new GZIPInputStream(Files.newInputStream(p)),\n                        StandardCharsets.UTF_8))) {\n            String line = r.readLine();\n            while (line != null) {\n                Optional<CCIndexRecord> record = CCIndexRecord.parseRecord(line);\n                if (record.isPresent()) {\n                    CCIndexRecord indexRecord = record.get();\n                    if (!indexRecord.getMime().equals(indexRecord.getMimeDetected())) {\n                        System.out.println(line);\n                    }", "                if (record.isPresent()) {\n                    CCIndexRecord indexRecord = record.get();\n                    if (!indexRecord.getMime().equals(indexRecord.getMimeDetected())) {\n                        System.out.println(line);\n                    }\n                    if (!StringUtils.isBlank(indexRecord.getTruncated())) {\n\n                    }\n                }\n                line = r.readLine();\n            }\n        }\n    }\n}\n"]}
{"filename": "src/test/java/org/tallison/cc/index/io/TargetPathRewriterTest.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.io;\n", "package org.tallison.cc.index.io;\n\nimport static org.junit.jupiter.api.Assertions.assertEquals;\n\nimport org.junit.jupiter.api.Test;\n\npublic class TargetPathRewriterTest {\n\n    @Test\n    public void testBasic() throws Exception {\n        String pat = \"xx/xx\";\n        String txt = \"abcdefgh\";\n        TargetPathRewriter targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/abcdefgh\", targetPathRewriter.rewrite(txt));\n\n        pat = \"xx/xx/xx\";\n        targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/cd/abcdefgh\", targetPathRewriter.rewrite(txt));\n\n        pat = \"xx/xx/x/xx\";\n        targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/cd/e/abcdefgh\", targetPathRewriter.rewrite(txt));\n\n        pat = \"xx/xx//xx\";\n        targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/cd//abcdefgh\", targetPathRewriter.rewrite(txt));\n    }\n}\n", "    public void testBasic() throws Exception {\n        String pat = \"xx/xx\";\n        String txt = \"abcdefgh\";\n        TargetPathRewriter targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/abcdefgh\", targetPathRewriter.rewrite(txt));\n\n        pat = \"xx/xx/xx\";\n        targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/cd/abcdefgh\", targetPathRewriter.rewrite(txt));\n\n        pat = \"xx/xx/x/xx\";\n        targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/cd/e/abcdefgh\", targetPathRewriter.rewrite(txt));\n\n        pat = \"xx/xx//xx\";\n        targetPathRewriter = new TargetPathRewriter(pat);\n        assertEquals(\"ab/cd//abcdefgh\", targetPathRewriter.rewrite(txt));\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/AbstractRecordProcessor.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index;\n", "package org.tallison.cc.index;\n\nimport java.util.Locale;\nimport java.util.concurrent.atomic.AtomicInteger;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n\npublic abstract class AbstractRecordProcessor implements IndexRecordProcessor {\n\n    protected static AtomicInteger threadCounter = new AtomicInteger(0);\n\n    private final int threadNumber;\n\n    public AbstractRecordProcessor() {\n        threadNumber = threadCounter.incrementAndGet();\n    }\n\n    //returns \"\" if key is null, otherwise, trims and converts remaining \\r\\n\\t to \" \"\n    protected static String clean(String key) {", "public abstract class AbstractRecordProcessor implements IndexRecordProcessor {\n\n    protected static AtomicInteger threadCounter = new AtomicInteger(0);\n\n    private final int threadNumber;\n\n    public AbstractRecordProcessor() {\n        threadNumber = threadCounter.incrementAndGet();\n    }\n\n    //returns \"\" if key is null, otherwise, trims and converts remaining \\r\\n\\t to \" \"\n    protected static String clean(String key) {", "        if (key == null) {\n            return \"\";\n        }\n        return key.trim().replaceAll(\"[\\r\\n\\t]\", \" \");\n    }\n\n    protected int getThreadNumber() {\n        return threadNumber;\n    }\n\n    String getExtension(String u) {", "        if (u == null || u.length() == 0) {\n            return null;\n        }\n        int i = u.lastIndexOf('.');\n        if (i < 0 || i + 6 < u.length()) {\n            return null;\n        }\n        String ext = u.substring(i + 1);\n        ext = ext.trim();\n        Matcher m = Pattern.compile(\"^\\\\d+$\").matcher(ext);\n        if (m.find()) {\n            return null;\n        }\n        ext = ext.toLowerCase(Locale.ENGLISH);\n        ext = ext.replaceAll(\"\\\\/$\", \"\");\n        return ext;\n    }\n\n}\n", "        if (m.find()) {\n            return null;\n        }\n        ext = ext.toLowerCase(Locale.ENGLISH);\n        ext = ext.replaceAll(\"\\\\/$\", \"\");\n        return ext;\n    }\n\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/CCIndexReaderCounter.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index;\n", "package org.tallison.cc.index;\n\nimport java.util.concurrent.atomic.AtomicLong;\n\npublic class CCIndexReaderCounter {\n    AtomicLong recordsRead = new AtomicLong(0);\n    AtomicLong filesExtracted = new AtomicLong(0);\n    AtomicLong truncated = new AtomicLong(0);\n    AtomicLong emptyPayload = new AtomicLong(0);\n\n    public AtomicLong getRecordsRead() {\n        return recordsRead;\n    }\n", "    public AtomicLong getRecordsRead() {\n        return recordsRead;\n    }\n\n    public AtomicLong getFilesExtracted() {\n        return filesExtracted;\n    }\n\n    public AtomicLong getTruncated() {\n        return truncated;\n    }\n", "    public AtomicLong getTruncated() {\n        return truncated;\n    }\n\n    public AtomicLong getEmptyPayload() {\n        return emptyPayload;\n    }\n\n    @Override\n    public String toString() {\n        return \"CCIndexReaderCounter{\" + \"recordsRead=\" + recordsRead + \", filesExtracted=\" +\n                filesExtracted + \", truncated=\" + truncated + \", emptyPayload=\" + emptyPayload +\n                '}';\n    }\n}\n", "    public String toString() {\n        return \"CCIndexReaderCounter{\" + \"recordsRead=\" + recordsRead + \", filesExtracted=\" +\n                filesExtracted + \", truncated=\" + truncated + \", emptyPayload=\" + emptyPayload +\n                '}';\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/CCIndexRecord.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index;\n", "package org.tallison.cc.index;\n\nimport java.net.MalformedURLException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.net.URL;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.Locale;", "import java.util.List;\nimport java.util.Locale;\nimport java.util.Optional;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport com.fasterxml.jackson.core.JsonProcessingException;\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport com.fasterxml.jackson.databind.PropertyNamingStrategies;\nimport com.fasterxml.jackson.databind.annotation.JsonNaming;", "import com.fasterxml.jackson.databind.PropertyNamingStrategies;\nimport com.fasterxml.jackson.databind.annotation.JsonNaming;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n@JsonNaming(PropertyNamingStrategies.KebabCaseStrategy.class)\npublic class CCIndexRecord {\n\n    private static Pattern INT_PATTERN = Pattern.compile(\"^\\\\d+$\");\n\n    private static Logger LOGGER = LoggerFactory.getLogger(CCIndexRecord.class);\n\n    private static ObjectMapper OBJECT_MAPPER = new ObjectMapper();\n\n    private String url;\n    private String mime;\n    private String mimeDetected;\n    private Integer status;\n    private String digest;\n    private Long length;\n    private Integer offset;\n    private String filename;\n    private String charset;\n    private String languages;\n    private String truncated;\n    private String redirect;\n", "    public static String normalizeMime(String s) {\n        if (s == null) {\n            return null;\n        }\n        s = s.toLowerCase(Locale.ENGLISH);\n        s = s.replaceAll(\"^\\\"|\\\"$\", \"\");\n        s = s.replaceAll(\"\\\\s+\", \" \");\n        return s.trim();\n    }\n\n    /**\n     * @param url\n     * @return \"\" if no tld could be extracted\n     */", "    public static String getTLD(String url) {\n        if (url == null) {\n            return \"\";\n        }\n        Matcher intMatcher = INT_PATTERN.matcher(\"\");\n\n        try {\n            URI uri = new URI(url);\n            String host = uri.getHost();\n            if (host == null) {\n                return \"\";\n            }\n            int i = host.lastIndexOf(\".\");\n            String tld = \"\";", "            if (host == null) {\n                return \"\";\n            }\n            int i = host.lastIndexOf(\".\");\n            String tld = \"\";\n            if (i > -1 && i + 1 < host.length()) {\n                tld = host.substring(i + 1);\n            } else {\n                //bad host...or do we want to handle xyz.com. ?\n                return tld;\n            }", "            if (intMatcher.reset(tld).find()) {\n                return \"\";\n            }\n            return tld;\n\n        } catch (URISyntaxException e) {\n            //swallow\n        }\n        return \"\";\n    }\n", "    public static Optional<CCIndexRecord> parseRecord(String row) {\n        int urlI = row.indexOf(' ');\n        int dateI = row.indexOf(' ', urlI + 1);\n        if (dateI < 0) {\n            LOGGER.warn(\"bad record dateI < 0: {}\", row);\n            return Optional.empty();\n        }\n        String json = row.substring(dateI + 1);\n        try {\n            return Optional.of(OBJECT_MAPPER.readValue(json, CCIndexRecord.class));\n        } catch (JsonProcessingException e) {\n            LOGGER.warn(\"mapping exception, trying repair: {}\", row);\n            return tryRepair(json);\n        }\n    }\n\n    private static Optional<CCIndexRecord> tryRepair(String jsonPart) {\n\n        List<Integer> ends = new ArrayList<>();\n        int end = jsonPart.indexOf('}');\n", "        try {\n            return Optional.of(OBJECT_MAPPER.readValue(json, CCIndexRecord.class));\n        } catch (JsonProcessingException e) {\n            LOGGER.warn(\"mapping exception, trying repair: {}\", row);\n            return tryRepair(json);\n        }\n    }\n\n    private static Optional<CCIndexRecord> tryRepair(String jsonPart) {\n\n        List<Integer> ends = new ArrayList<>();\n        int end = jsonPart.indexOf('}');\n", "        while (end > -1) {\n            ends.add(end);\n            end = jsonPart.indexOf('}', end + 1);\n        }\n        if (ends.size() == 0) {\n            LOGGER.warn(\"bad record: {}\", jsonPart);\n            return Optional.empty();\n        }\n        Collections.reverse(ends);\n        //now try to parse the string ending it at each end\n        //start with the max", "        for (int thisEnd : ends) {\n            String json = jsonPart.substring(0, thisEnd + 1);\n            try {\n                return Optional.of(OBJECT_MAPPER.readValue(json, CCIndexRecord.class));\n            } catch (JsonProcessingException e) {\n                LOGGER.trace(\"mapping exception, trying repair with: {}\", json);\n            }\n        }\n        LOGGER.warn(\"bad record, giving up: {}\", jsonPart);\n        return Optional.empty();\n\n    }\n", "    public String getUrl() {\n        return url;\n    }\n\n    public String getHost() {\n        try {\n            URL u = new URL(url);\n            return u.getHost();\n        } catch (MalformedURLException e) {\n            return \"\";\n        }\n    }\n", "    public String getMime() {\n        return mime;\n    }\n\n    public void setMime(String mime) {\n        this.mime = mime;\n    }\n\n    public String getNormalizedMime() {\n        return CCIndexRecord.normalizeMime(mime);\n    }\n", "    public String getNormalizedMime() {\n        return CCIndexRecord.normalizeMime(mime);\n    }\n\n    public String getNormalizedMimeDetected() {\n        return CCIndexRecord.normalizeMime(mimeDetected);\n    }\n\n    public Integer getStatus() {\n        return status;\n    }\n", "    public Integer getStatus() {\n        return status;\n    }\n\n    public void setStatus(int status) {\n        this.status = status;\n    }\n\n    public String getDigest() {\n        return digest;\n    }\n", "    public String getDigest() {\n        return digest;\n    }\n\n    public void setDigest(String digest) {\n        this.digest = digest;\n    }\n\n    public Long getLength() {\n        return length;\n    }\n", "    public Long getLength() {\n        return length;\n    }\n\n    public void setLength(long length) {\n        this.length = length;\n    }\n\n    public Integer getOffset() {\n        return offset;\n    }\n", "    public Integer getOffset() {\n        return offset;\n    }\n\n    public void setOffset(int offset) {\n        this.offset = offset;\n    }\n\n    public String getFilename() {\n        return filename;\n    }\n", "    public String getFilename() {\n        return filename;\n    }\n\n    public void setFilename(String warcFilename) {\n        this.filename = warcFilename;\n    }\n\n    public String getMimeDetected() {\n        return mimeDetected;\n    }\n", "    public String getMimeDetected() {\n        return mimeDetected;\n    }\n\n    public void setMimeDetected(String mimeDetected) {\n        this.mimeDetected = mimeDetected;\n    }\n\n    public String getCharset() {\n        return charset;\n    }\n", "    public String getCharset() {\n        return charset;\n    }\n\n    public String getLanguages() {\n        return languages;\n    }\n\n    public String getTruncated() {\n        return truncated;\n    }\n", "    public String getTruncated() {\n        return truncated;\n    }\n\n    public void setTruncated(String truncated) {\n        this.truncated = truncated;\n    }\n\n    public String getRedirect() {\n        return redirect;\n    }\n", "    public String getRedirect() {\n        return redirect;\n    }\n\n    public void setRedirect(String redirect) {\n        this.redirect = redirect;\n    }\n\n    public String getOffsetHeader() {\n        return \"bytes=\" + offset + \"-\" + (offset + length - 1);\n    }\n\n    @Override", "    public String getOffsetHeader() {\n        return \"bytes=\" + offset + \"-\" + (offset + length - 1);\n    }\n\n    @Override\n    public String toString() {\n        return \"CCIndexRecord{\" + \"url='\" + url + '\\'' + \", mime='\" + mime + '\\'' +\n                \", mimeDetected='\" + mimeDetected + '\\'' + \", status=\" + status + \", digest='\" +\n                digest + '\\'' + \", length=\" + length + \", offset=\" + offset + \", filename='\" +\n                filename + '\\'' + \", charset='\" + charset + '\\'' + \", languages='\" + languages +\n                '\\'' + \", truncated='\" + truncated + '\\'' + \", redirect='\" + redirect + '\\'' + '}';\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/IndexIterator.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index;\n", "package org.tallison.cc.index;\n\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.util.ArrayList;", "import java.nio.file.Path;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.concurrent.TimeoutException;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\nimport java.util.zip.GZIPInputStream;\n\nimport com.fasterxml.jackson.annotation.JsonCreator;", "\nimport com.fasterxml.jackson.annotation.JsonCreator;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport org.tallison.cc.index.extractor.ExtractorConfig;\nimport org.tallison.cc.index.io.BackoffHttpFetcher;\n\nimport org.apache.tika.config.Initializable;\nimport org.apache.tika.config.Param;\nimport org.apache.tika.exception.TikaConfigException;\nimport org.apache.tika.exception.TikaException;", "import org.apache.tika.exception.TikaConfigException;\nimport org.apache.tika.exception.TikaException;\nimport org.apache.tika.metadata.Metadata;\nimport org.apache.tika.pipes.FetchEmitTuple;\nimport org.apache.tika.pipes.emitter.EmitKey;\nimport org.apache.tika.pipes.fetcher.FetchKey;\nimport org.apache.tika.pipes.fetcher.Fetcher;\nimport org.apache.tika.pipes.fetcher.fs.FileSystemFetcher;\nimport org.apache.tika.pipes.fetcher.s3.S3Fetcher;\nimport org.apache.tika.pipes.pipesiterator.PipesIterator;", "import org.apache.tika.pipes.fetcher.s3.S3Fetcher;\nimport org.apache.tika.pipes.pipesiterator.PipesIterator;\n\npublic class IndexIterator extends PipesIterator implements Initializable {\n\n    //temporary storage of the paths this class was constructed with\n    //during initialization, we'll figure out if these are literal index paths\n    //or paths to index lists.\n    private final List<String> initPaths = new ArrayList<>();\n    private final List<String> indexPaths = new ArrayList<>();\n\n    private Fetcher fetcher = null;\n\n    int maxIndexFiles = -1;\n\n    @JsonCreator\n    public IndexIterator(@JsonProperty(\"profile\") String profile,\n                         @JsonProperty(\"basePath\") String basePath,\n                         @JsonProperty(\"paths\") List<String> indexPaths,\n                         @JsonProperty(\"maxIndexFiles\") Integer maxIndexFiles) {", "        if (profile != null) {\n            fetcher = new S3Fetcher();\n            ((S3Fetcher) fetcher).setProfile(profile);\n            ((S3Fetcher) fetcher).setCredentialsProvider(\"profile\");\n            ((S3Fetcher) fetcher).setBucket(ExtractorConfig.CC_S3_BUCKET);\n            ((S3Fetcher) fetcher).setRegion(ExtractorConfig.CC_REGION);\n        } else if (basePath != null) {\n            fetcher = new FileSystemFetcher();\n            ((FileSystemFetcher) fetcher).setBasePath(basePath);\n        } else {\n            //do nothing\n        }", "        if (indexPaths != null) {\n            initPaths.addAll(indexPaths);\n        }\n\n        if (maxIndexFiles != null) {\n            this.maxIndexFiles = maxIndexFiles;\n        }\n    }\n\n    private static void addIndexPaths(Fetcher fetcher, String path, List<String> indexPaths)\n            throws IOException, TikaException {\n\n        try (InputStream is = fetcher.fetch(path, new Metadata())) {\n            try (BufferedReader reader = getReader(is, path)) {\n                String line = reader.readLine();", "                while (line != null) {\n                    if (line.startsWith(\"#\") || !line.endsWith(\".gz\")) {\n                        //skip comments and paths for index files that do not end in .gz\n                        line = reader.readLine();\n                        continue;\n                    }\n                    indexPaths.add(line);\n                    line = reader.readLine();\n                }\n            }\n        }\n    }\n\n    private static BufferedReader getReader(InputStream is, String path) throws IOException {", "        if (path.endsWith(\".gz\")) {\n            return new BufferedReader(\n                    new InputStreamReader(new GZIPInputStream(is), StandardCharsets.UTF_8));\n        } else {\n            return new BufferedReader(new InputStreamReader(is, StandardCharsets.UTF_8));\n        }\n    }\n\n    @Override\n    protected void enqueue() throws IOException, TimeoutException, InterruptedException {\n        int added = 0;", "        for (String p : indexPaths) {\n            FetchEmitTuple t = new FetchEmitTuple(p, new FetchKey(\"\", p), new EmitKey());\n            tryToAdd(t);\n            if (maxIndexFiles > -1 && ++added >= maxIndexFiles) {\n                break;\n            }\n        }\n        tryToAdd(PipesIterator.COMPLETED_SEMAPHORE);\n    }\n\n    @Override", "    public void initialize(Map<String, Param> params) throws TikaConfigException {\n        if (fetcher == null) {\n            // TODO -- figure out if we actually need to fetch (e.g. to fetch cc-index.paths.gz) or\n            //if these are just the literal paths to the index files\n            fetcher = new BackoffHttpFetcher(new long[]{30, 120});\n        }\n        if (fetcher instanceof Initializable) {\n            ((Initializable) fetcher).initialize(params);\n        }\n        Matcher m = Pattern.compile(\"indexes/cdx-\\\\d{5,5}.gz\\\\Z\").matcher(\"\");\n        if (initPaths.size() == 0) {", "        if (initPaths.size() == 0) {\n            try {\n                loadLocalFiles(fetcher);\n            } catch (IOException e) {\n                throw new TikaConfigException(\"Problem reading from local directory\");\n            }\n        }\n        for (String p : initPaths) {\n            if (p.endsWith(\"cc-index.paths.gz\")) {\n                try {\n                    addIndexPaths(fetcher, p, indexPaths);\n                } catch (IOException | TikaException e) {\n                    throw new TikaConfigException(e.getMessage());\n                }", "            if (p.endsWith(\"cc-index.paths.gz\")) {\n                try {\n                    addIndexPaths(fetcher, p, indexPaths);\n                } catch (IOException | TikaException e) {\n                    throw new TikaConfigException(e.getMessage());\n                }\n            } else if (m.reset(p).find()) {\n                indexPaths.add(p);\n            } else {\n                throw new TikaConfigException(\n                        \"Paths need to be path lists (.../cc-index.paths.gz) \" +\n                                \"or indexes (indexes/cdx-\\\\d\\\\d\\\\d\\\\d\\\\d.gz\");\n            }\n        }\n\n    }\n\n    private void loadLocalFiles(Fetcher fetcher) throws IOException {", "        if (fetcher instanceof FileSystemFetcher) {\n            Path basePath = ((FileSystemFetcher)fetcher).getBasePath();\n            Files.walk(basePath).filter(p -> Files.isRegularFile(p)).forEach(\n                    p -> initPaths.add(basePath.relativize(p).toString())\n            );\n        }\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/IndexRecordProcessor.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.tallison.cc.index;", "\npackage org.tallison.cc.index;\n\n\nimport java.io.IOException;\n\npublic interface IndexRecordProcessor {\n\n    public boolean process(String json) throws IOException, InterruptedException;\n\n    public void close() throws IOException;\n}\n", "    public boolean process(String json) throws IOException, InterruptedException;\n\n    public void close() throws IOException;\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/selector/AbstractSamplingSelector.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.selector;\n", "package org.tallison.cc.index.selector;\n\nimport java.util.Random;\n\npublic abstract class AbstractSamplingSelector implements SelectorClause {\n\n    final Sampler sampler;\n\n    AbstractSamplingSelector(Sampler sampler) {\n        this.sampler = sampler;\n    }\n\n    interface Sampler extends SelectorClause {\n\n    }\n\n    static class SampleAll implements Sampler {\n        @Override", "        public boolean select(String val) {\n            return true;\n        }\n    }\n\n    static class SampleSome implements Sampler {\n        private final double sample;\n        private final Random random = new Random();\n\n        SampleSome(double sample) {\n            this.sample = sample;\n        }\n\n        @Override", "        public boolean select(String val) {\n            if (random.nextDouble() <= sample) {\n                return true;\n            }\n            return false;\n        }\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/selector/RegexSelector.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.selector;\n", "package org.tallison.cc.index.selector;\n\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport com.fasterxml.jackson.annotation.JsonCreator;\nimport com.fasterxml.jackson.annotation.JsonProperty;\n\npublic class RegexSelector extends AbstractSamplingSelector {\n\n    final Pattern pattern;\n\n    @JsonCreator\n    public RegexSelector(@JsonProperty(\"pattern\") String pattern,\n                         @JsonProperty(\"sample\") Double sample) {\n        super(sample == null ? new SampleAll() : new SampleSome(sample));\n        this.pattern = Pattern.compile(pattern);\n    }\n\n    @Override", "public class RegexSelector extends AbstractSamplingSelector {\n\n    final Pattern pattern;\n\n    @JsonCreator\n    public RegexSelector(@JsonProperty(\"pattern\") String pattern,\n                         @JsonProperty(\"sample\") Double sample) {\n        super(sample == null ? new SampleAll() : new SampleSome(sample));\n        this.pattern = Pattern.compile(pattern);\n    }\n\n    @Override", "    public boolean select(String val) {\n        Matcher m = pattern.matcher(val);\n        if (m.find()) {\n            return sampler.select(val);\n        }\n        return false;\n    }\n\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/selector/SelectorClause.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.selector;\n", "package org.tallison.cc.index.selector;\n\nimport com.fasterxml.jackson.annotation.JsonSubTypes;\nimport com.fasterxml.jackson.annotation.JsonTypeInfo;\n\n@JsonTypeInfo(use = JsonTypeInfo.Id.DEDUCTION)\n@JsonSubTypes({@JsonSubTypes.Type(MatchSelector.class), @JsonSubTypes.Type(RegexSelector.class)})\npublic interface SelectorClause {\n\n    boolean select(String val);\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/selector/RecordSelector.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.selector;\n", "package org.tallison.cc.index.selector;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.CCIndexRecord;", "import org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.CCIndexRecord;\n\npublic class RecordSelector {\n\n    private static Logger LOGGER = LoggerFactory.getLogger(RecordSelector.class);\n\n    public static RecordSelector ACCEPT_ALL_RECORDS = new AcceptAllRecords();\n    @JsonProperty\n    Map<String, List<SelectorClause>> must = new HashMap<>();\n    @JsonProperty\n    Map<String, List<SelectorClause>> must_not = new HashMap<>();\n    @JsonProperty\n    Map<String, List<SelectorClause>> should = new HashMap<>();\n", "    public boolean select(CCIndexRecord record) {\n\n        for (Map.Entry<String, List<SelectorClause>> e : must_not.entrySet()) {\n            String val = getStringValue(e.getKey(), record);\n            if (val == null) {\n                LOGGER.warn(\"Value is null for '{}' in the must not clause\", e.getKey());\n                continue;\n            }\n            for (SelectorClause clause : e.getValue()) {\n                if (clause.select(val)) {\n                    return false;\n                }\n            }\n        }\n", "            for (SelectorClause clause : e.getValue()) {\n                if (clause.select(val)) {\n                    return false;\n                }\n            }\n        }\n\n        for (Map.Entry<String, List<SelectorClause>> e : must.entrySet()) {\n            String val = getStringValue(e.getKey(), record);\n            if (val == null) {\n                LOGGER.warn(\"Value is null for '{}' in the must clause. Record not selected.\",\n                        e.getKey());\n                return false;\n            }", "            if (val == null) {\n                LOGGER.warn(\"Value is null for '{}' in the must clause. Record not selected.\",\n                        e.getKey());\n                return false;\n            }\n            for (SelectorClause clause : e.getValue()) {\n                if (!clause.select(val)) {\n                    return false;\n                }\n            }\n        }", "        if (should.size() == 0) {\n            return true;\n        }\n        for (Map.Entry<String, List<SelectorClause>> e : should.entrySet()) {\n            String val = getStringValue(e.getKey(), record);\n            if (val == null) {\n                LOGGER.warn(\"Value is null for '{}' in the should clause. Record not selected\",\n                        e.getKey());\n                continue;\n            }\n            for (SelectorClause clause : e.getValue()) {", "            for (SelectorClause clause : e.getValue()) {\n                if (clause.select(val)) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }\n\n    private String getStringValue(String key, CCIndexRecord record) {\n\n        switch (key) {\n            case \"mime_detected\":\n                return record.getMimeDetected();\n            case \"truncated\":\n                return record.getTruncated();\n            case \"mime\":\n                return record.getMime();\n            case \"status\":\n                return Integer.toString(record.getStatus());\n            case \"url\":\n                return record.getUrl();\n            case \"host\":\n                return record.getHost();\n            case \"digest\":\n                return record.getDigest();\n            default:\n                throw new IllegalArgumentException(\"Don't yet support key \" + key);\n        }\n    }\n\n    private static class AcceptAllRecords extends RecordSelector {\n        @Override", "        public boolean select(CCIndexRecord record) {\n            return true;\n        }\n    }\n\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/selector/MatchSelector.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.selector;\n", "package org.tallison.cc.index.selector;\n\nimport com.fasterxml.jackson.annotation.JsonCreator;\nimport com.fasterxml.jackson.annotation.JsonProperty;\n\n/**\n * match exact string\n */\npublic class MatchSelector extends AbstractSamplingSelector {\n\n    private static final boolean DEFAULT_CASE_SENSITIVE = true;\n    final boolean caseSensitive;\n    final String match;\n\n    @JsonCreator\n    MatchSelector(@JsonProperty(\"match\") String match, @JsonProperty(\"sample\") Double sample,\n                  @JsonProperty(\"case_sensitive\") Boolean caseSensitive) {\n        super(sample == null ? new SampleAll() : new SampleSome(sample));\n        this.match = match;\n        this.caseSensitive = caseSensitive == null ? DEFAULT_CASE_SENSITIVE : caseSensitive;\n    }\n\n    @Override", "public class MatchSelector extends AbstractSamplingSelector {\n\n    private static final boolean DEFAULT_CASE_SENSITIVE = true;\n    final boolean caseSensitive;\n    final String match;\n\n    @JsonCreator\n    MatchSelector(@JsonProperty(\"match\") String match, @JsonProperty(\"sample\") Double sample,\n                  @JsonProperty(\"case_sensitive\") Boolean caseSensitive) {\n        super(sample == null ? new SampleAll() : new SampleSome(sample));\n        this.match = match;\n        this.caseSensitive = caseSensitive == null ? DEFAULT_CASE_SENSITIVE : caseSensitive;\n    }\n\n    @Override", "    public boolean select(String val) {\n        if (caseSensitive) {\n            if (match.equals(val)) {\n                return true;\n            }\n        } else {\n            if (match.equalsIgnoreCase(val)) {\n                return true;\n            }\n        }\n        return false;\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/selector/AcceptAllRecords.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.selector;\n", "package org.tallison.cc.index.selector;\n\nimport org.tallison.cc.index.CCIndexRecord;\n\npublic class AcceptAllRecords extends RecordSelector {\n    @Override\n    public boolean select(CCIndexRecord record) {\n        return true;\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/io/BackoffHttpFetcher.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.io;\n", "package org.tallison.cc.index.io;\n\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.extractor.ExtractorConfig;", "import org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.extractor.ExtractorConfig;\n\nimport org.apache.tika.exception.TikaException;\nimport org.apache.tika.io.TikaInputStream;\nimport org.apache.tika.metadata.Metadata;\nimport org.apache.tika.pipes.fetcher.FetchKey;\nimport org.apache.tika.pipes.fetcher.http.HttpFetcher;\n\n/**", "\n/**\n * We need this because it allows for back-off on 503\n * and it adds the protocol+host base to the paths: {@link ExtractorConfig#CC_HTTPS_BASE}\n */\npublic class BackoffHttpFetcher extends HttpFetcher {\n\n    private static Logger LOGGER = LoggerFactory.getLogger(BackoffHttpFetcher.class);\n\n    //backoff\n    private final long[] throttleSeconds;\n\n    public BackoffHttpFetcher(long[] throttleSeconds) {\n        this.throttleSeconds = throttleSeconds;\n    }\n\n    @Override", "    public InputStream fetch(String fetchKey, Metadata metadata) throws TikaException, IOException {\n        return fetchWithBackOff(new FetchKey(\"name\", getUrl(fetchKey)), metadata);\n    }\n\n    @Override\n    public InputStream fetch(String fetchkey, long rangeStart, long rangeEnd, Metadata metadata)\n            throws IOException {\n        return fetchWithBackOff(new FetchKey(\"name\", getUrl(fetchkey), rangeStart, rangeEnd),\n                metadata);\n    }\n\n    private String getUrl(String fetchKey) {", "        if (!fetchKey.startsWith(\"http\")) {\n            if (fetchKey.startsWith(\"/\")) {\n                return ExtractorConfig.CC_HTTPS_BASE + fetchKey;\n            } else {\n                return ExtractorConfig.CC_HTTPS_BASE + \"/\" + fetchKey;\n            }\n        }\n        return fetchKey;\n    }\n\n    private InputStream fetchWithBackOff(FetchKey fetchKey, Metadata metadata) throws IOException {\n        int tries = 0;", "        while (tries < throttleSeconds.length) {\n            try {\n                return _fetch(fetchKey, metadata);\n            } catch (IOException e) {\n                if (e.getMessage() == null) {\n                    throw e;\n                }\n                Matcher m = Pattern.compile(\"bad status code: (\\\\d+)\").matcher(e.getMessage());\n                if (m.find() && m.group(1).equals(\"503\")) {\n                    long sleepMs = 1000 * throttleSeconds[tries];\n                    LOGGER.warn(\"got backoff warning (#{}) for {}. Will sleep {} seconds. \" +\n                                    \"Message: {}. \",\n                            tries + 1, fetchKey.getFetchKey(),\n                            throttleSeconds[tries],\n                            e.getMessage());\n                    //sleep, back off", "                if (m.find() && m.group(1).equals(\"503\")) {\n                    long sleepMs = 1000 * throttleSeconds[tries];\n                    LOGGER.warn(\"got backoff warning (#{}) for {}. Will sleep {} seconds. \" +\n                                    \"Message: {}. \",\n                            tries + 1, fetchKey.getFetchKey(),\n                            throttleSeconds[tries],\n                            e.getMessage());\n                    //sleep, back off\n                    try {\n                        Thread.sleep(sleepMs);\n                    } catch (InterruptedException ex) {\n                        throw new RuntimeException(ex);\n                    }\n                } else {\n                    throw e;\n                }\n            } catch (TikaException e) {\n                throw new IOException(e);\n            }\n            tries++;\n        }\n        throw new ThrottleException();\n    }\n\n    private TikaInputStream _fetch(FetchKey fetchKey, Metadata metadata)\n            throws IOException, TikaException {", "                    try {\n                        Thread.sleep(sleepMs);\n                    } catch (InterruptedException ex) {\n                        throw new RuntimeException(ex);\n                    }\n                } else {\n                    throw e;\n                }\n            } catch (TikaException e) {\n                throw new IOException(e);\n            }\n            tries++;\n        }\n        throw new ThrottleException();\n    }\n\n    private TikaInputStream _fetch(FetchKey fetchKey, Metadata metadata)\n            throws IOException, TikaException {", "        if (fetchKey.getRangeStart() > 0) {\n            return (TikaInputStream) super.fetch(fetchKey.getFetchKey(), fetchKey.getRangeStart(),\n                    fetchKey.getRangeEnd(), metadata);\n        } else {\n            return (TikaInputStream) super.fetch(fetchKey.getFetchKey(), metadata);\n        }\n\n    }\n\n    @Override\n    public String getName() {\n        return \"backoffHttpFetcher\";\n    }\n\n}\n", "    public String getName() {\n        return \"backoffHttpFetcher\";\n    }\n\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/io/TargetPathRewriter.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.io;\n", "package org.tallison.cc.index.io;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class TargetPathRewriter {\n\n    List<Integer> offsets = new ArrayList<>();\n\n    public TargetPathRewriter(String targetPathPattern) {\n        if (targetPathPattern.startsWith(\"/\")) {\n            throw new IllegalArgumentException(\"targetPathRewriter cannot start with '/'\");\n        }", "        if (targetPathPattern.startsWith(\"/\")) {\n            throw new IllegalArgumentException(\"targetPathRewriter cannot start with '/'\");\n        }\n        if (targetPathPattern.endsWith(\"/\")) {\n            throw new IllegalArgumentException(\"targetPathRewriter cannot end with '/'\");\n        }\n\n        int i = targetPathPattern.indexOf('/');\n        int hits = 0;\n        while (i > -1) {\n            offsets.add(i - hits);\n            hits++;\n            i = targetPathPattern.indexOf('/', i + 1);\n        }\n    }\n", "        while (i > -1) {\n            offsets.add(i - hits);\n            hits++;\n            i = targetPathPattern.indexOf('/', i + 1);\n        }\n    }\n\n    public String rewrite(String originalPath) {\n        if (offsets.size() == 0) {\n            return originalPath;\n        }\n        StringBuilder sb = new StringBuilder();\n        int start = 0;", "        if (offsets.size() == 0) {\n            return originalPath;\n        }\n        StringBuilder sb = new StringBuilder();\n        int start = 0;\n        for (int i : offsets) {\n            sb.append(originalPath.substring(start, i));\n            sb.append('/');\n            start = i;\n        }\n        sb.append(originalPath);\n        return sb.toString();\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/io/ThrottleException.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.io;\n", "package org.tallison.cc.index.io;\n\n/**\n * This is thrown when our throttling strategy fails and aws\n * is still returning 503 -- please slow down\n */\npublic class ThrottleException extends RuntimeException {\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/extractor/CCFetcherCli.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.extractor;\n", "package org.tallison.cc.index.extractor;\n\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\n/**\n * This is a lighter class that doesn't rely on a database\n * to extract files from CC and write a list of truncated urls.\n */\npublic class CCFetcherCli {\n", " */\npublic class CCFetcherCli {\n\n    public static void main(String[] args) throws Exception {\n        String command = args[0];\n\n        if (command.equals(\"Fetch\")) {\n            CCFileExtractor.main(new String[]{args[1]});\n        } else if (command.equals(\"FetchIndices\")) {\n            CCIndexFetcher.main(new String[]{args[1]});\n        } else if (command.equals(\"CountMimes\")) {\n            CCMimeCounter.main(new String[]{args[1]});", "        } else if (command.equals(\"FetchIndices\")) {\n            CCIndexFetcher.main(new String[]{args[1]});\n        } else if (command.equals(\"CountMimes\")) {\n            CCMimeCounter.main(new String[]{args[1]});\n        } else if (Files.isRegularFile(Paths.get(command))) {\n            CCFileExtractor.main(new String[]{args[0]});\n        } else {\n            System.out.println(\"Must start with a command: Fetch, FetchIndices or CountMimes\");\n        }\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/extractor/CCFileExtractor.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.extractor;\n", "package org.tallison.cc.index.extractor;\n\nimport java.io.BufferedInputStream;\nimport java.io.BufferedReader;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.nio.charset.StandardCharsets;\nimport java.util.Collections;", "import java.nio.charset.StandardCharsets;\nimport java.util.Collections;\nimport java.util.Locale;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorCompletionService;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;", "import java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.zip.GZIPInputStream;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.AbstractRecordProcessor;", "import org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.AbstractRecordProcessor;\nimport org.tallison.cc.index.CCIndexReaderCounter;\nimport org.tallison.cc.index.IndexIterator;\n\nimport org.apache.tika.exception.TikaConfigException;\nimport org.apache.tika.exception.TikaException;\nimport org.apache.tika.io.TikaInputStream;\nimport org.apache.tika.metadata.Metadata;\nimport org.apache.tika.pipes.FetchEmitTuple;", "import org.apache.tika.metadata.Metadata;\nimport org.apache.tika.pipes.FetchEmitTuple;\nimport org.apache.tika.pipes.fetcher.Fetcher;\nimport org.apache.tika.pipes.pipesiterator.CallablePipesIterator;\nimport org.apache.tika.pipes.pipesiterator.PipesIterator;\nimport org.apache.tika.utils.StringUtils;\n\n/**\n * This is a lighter class that doesn't rely on a database\n * to extract files from CC and log a list of truncated urls.", " * This is a lighter class that doesn't rely on a database\n * to extract files from CC and log a list of truncated urls.\n */\npublic class CCFileExtractor {\n\n    private static final Long INDEX_WORKER_ID = 1l;\n    private static final Long INDEX_READER_ID = 2l;\n    private static final Logger LOGGER = LoggerFactory.getLogger(CCFileExtractor.class);\n\n    public static void main(String[] args) throws Exception {\n        ExtractorConfig fetcherConfig =\n                new ObjectMapper().readValue(new File(args[0]), ExtractorConfig.class);\n        execute(fetcherConfig);\n    }\n\n    private static void execute(ExtractorConfig fetcherConfig) throws TikaException {\n        ArrayBlockingQueue<FetchEmitTuple> indexPathsList = new ArrayBlockingQueue<>(1000);\n        //IndexPathsReader reads a file containing a list of cc-index.paths files\n        //and writes the literal gz files (cc-index/collections/CC-MAIN-2023-06/indexes/cdx-00000.gz)\n        //to indexPathsList\n\n\n        //IndexWorker reads a single index.gz file at a time and processes each record\n        //It fetches non truncated files and logs truncated files\n        int totalThreads = fetcherConfig.getNumThreads() + 1;\n\n        ExecutorService executorService = Executors.newFixedThreadPool(totalThreads);\n        ExecutorCompletionService<Long> executorCompletionService =\n                new ExecutorCompletionService<>(executorService);\n\n        IndexIterator indexIterator = fetcherConfig.getIndexIterator();\n        indexIterator.initialize(Collections.EMPTY_MAP);\n        executorCompletionService.submit(new CallablePipesIterator(indexIterator, indexPathsList));\n        CCIndexReaderCounter counter = new CCIndexReaderCounter();\n        int finishedWorkers = 0;", "    public static void main(String[] args) throws Exception {\n        ExtractorConfig fetcherConfig =\n                new ObjectMapper().readValue(new File(args[0]), ExtractorConfig.class);\n        execute(fetcherConfig);\n    }\n\n    private static void execute(ExtractorConfig fetcherConfig) throws TikaException {\n        ArrayBlockingQueue<FetchEmitTuple> indexPathsList = new ArrayBlockingQueue<>(1000);\n        //IndexPathsReader reads a file containing a list of cc-index.paths files\n        //and writes the literal gz files (cc-index/collections/CC-MAIN-2023-06/indexes/cdx-00000.gz)\n        //to indexPathsList\n\n\n        //IndexWorker reads a single index.gz file at a time and processes each record\n        //It fetches non truncated files and logs truncated files\n        int totalThreads = fetcherConfig.getNumThreads() + 1;\n\n        ExecutorService executorService = Executors.newFixedThreadPool(totalThreads);\n        ExecutorCompletionService<Long> executorCompletionService =\n                new ExecutorCompletionService<>(executorService);\n\n        IndexIterator indexIterator = fetcherConfig.getIndexIterator();\n        indexIterator.initialize(Collections.EMPTY_MAP);\n        executorCompletionService.submit(new CallablePipesIterator(indexIterator, indexPathsList));\n        CCIndexReaderCounter counter = new CCIndexReaderCounter();\n        int finishedWorkers = 0;", "        try {\n            for (int i = 0; i < fetcherConfig.getNumThreads(); i++) {\n                CCFileExtractorRecordProcessor processor =\n                        new CCFileExtractorRecordProcessor(fetcherConfig, counter);\n                executorCompletionService.submit(\n                        new IndexWorker(fetcherConfig, indexPathsList, processor));\n            }\n\n\n            while (finishedWorkers < fetcherConfig.getNumThreads()) {\n                //blocking\n                Future<Long> future = executorCompletionService.take();", "            while (finishedWorkers < fetcherConfig.getNumThreads()) {\n                //blocking\n                Future<Long> future = executorCompletionService.take();\n                if (future != null) {\n                    Long f = future.get();\n                    LOGGER.debug(\"completed {}\", f);\n                    if (f.equals(INDEX_WORKER_ID)) {\n                        finishedWorkers++;\n                    } else if (f.equals(INDEX_READER_ID)) {\n                        LOGGER.info(\"Index paths reader successfully completed\");\n                    }\n                }\n            }\n        } catch (TikaConfigException | IOException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (ExecutionException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (InterruptedException e) {\n            LOGGER.warn(\"main loop interrupted exception\", e);\n            throw new RuntimeException(e);\n        } finally {\n            executorService.shutdown();\n            executorService.shutdownNow();\n        }\n    }\n\n    private static class IndexWorker implements Callable<Long> {\n\n        private final ArrayBlockingQueue<FetchEmitTuple> indexUrls;\n        private final AbstractRecordProcessor recordProcessor;\n\n        private final Fetcher indexFetcher;\n\n        IndexWorker(ExtractorConfig fetcherConfig, ArrayBlockingQueue<FetchEmitTuple> indexUrls,\n                    AbstractRecordProcessor recordProcessor) throws TikaException {\n            this.indexUrls = indexUrls;\n            this.recordProcessor = recordProcessor;\n            this.indexFetcher = fetcherConfig.newIndexFetcher();\n        }\n\n        @Override", "                    } else if (f.equals(INDEX_READER_ID)) {\n                        LOGGER.info(\"Index paths reader successfully completed\");\n                    }\n                }\n            }\n        } catch (TikaConfigException | IOException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (ExecutionException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (InterruptedException e) {\n            LOGGER.warn(\"main loop interrupted exception\", e);\n            throw new RuntimeException(e);\n        } finally {\n            executorService.shutdown();\n            executorService.shutdownNow();\n        }\n    }\n\n    private static class IndexWorker implements Callable<Long> {\n\n        private final ArrayBlockingQueue<FetchEmitTuple> indexUrls;\n        private final AbstractRecordProcessor recordProcessor;\n\n        private final Fetcher indexFetcher;\n\n        IndexWorker(ExtractorConfig fetcherConfig, ArrayBlockingQueue<FetchEmitTuple> indexUrls,\n                    AbstractRecordProcessor recordProcessor) throws TikaException {\n            this.indexUrls = indexUrls;\n            this.recordProcessor = recordProcessor;\n            this.indexFetcher = fetcherConfig.newIndexFetcher();\n        }\n\n        @Override", "        public Long call() throws Exception {\n            boolean shouldContinue = true;\n            while (shouldContinue) {\n\n                FetchEmitTuple indexUrl = indexUrls.poll(120, TimeUnit.MINUTES);\n                if (indexUrl == null) {\n                    throw new TimeoutException(\"waited 120 minutes for a new record\");\n                }\n\n                if (indexUrl == PipesIterator.COMPLETED_SEMAPHORE) {\n                    recordProcessor.close();\n                    //can hang forever\n                    indexUrls.put(PipesIterator.COMPLETED_SEMAPHORE);\n                    return INDEX_WORKER_ID;\n                }\n                LOGGER.trace(indexUrl.toString());\n                shouldContinue = processFile(indexUrl, recordProcessor);\n            }\n            return INDEX_WORKER_ID;\n        }\n\n        private boolean processFile(FetchEmitTuple fetchEmitTuple,\n                                    AbstractRecordProcessor recordProcessor)\n                throws InterruptedException {\n            long start = System.currentTimeMillis();\n            LOGGER.info(\"starting to fetch index gz: {}\",\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            try (TikaInputStream tis = (TikaInputStream) indexFetcher.fetch(\n                    fetchEmitTuple.getFetchKey().getFetchKey(), new Metadata())) {\n                try (InputStream is = new BufferedInputStream(new GZIPInputStream(tis))) {\n                    try (BufferedReader reader = new BufferedReader(\n                            new InputStreamReader(is, StandardCharsets.UTF_8))) {\n                        String line = reader.readLine();\n                        int lines = 0;\n                        long elapsed = System.currentTimeMillis() - start;\n                        LOGGER.info(\"Finished fetching {} bytes in {} ms for index gz: {}\",\n                                String.format(Locale.US, \"%,d\", tis.getLength()),\n                                String.format(Locale.US, \"%,d\", elapsed),\n                                fetchEmitTuple.getFetchKey().getFetchKey());", "                if (indexUrl == PipesIterator.COMPLETED_SEMAPHORE) {\n                    recordProcessor.close();\n                    //can hang forever\n                    indexUrls.put(PipesIterator.COMPLETED_SEMAPHORE);\n                    return INDEX_WORKER_ID;\n                }\n                LOGGER.trace(indexUrl.toString());\n                shouldContinue = processFile(indexUrl, recordProcessor);\n            }\n            return INDEX_WORKER_ID;\n        }\n\n        private boolean processFile(FetchEmitTuple fetchEmitTuple,\n                                    AbstractRecordProcessor recordProcessor)\n                throws InterruptedException {\n            long start = System.currentTimeMillis();\n            LOGGER.info(\"starting to fetch index gz: {}\",\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            try (TikaInputStream tis = (TikaInputStream) indexFetcher.fetch(\n                    fetchEmitTuple.getFetchKey().getFetchKey(), new Metadata())) {\n                try (InputStream is = new BufferedInputStream(new GZIPInputStream(tis))) {\n                    try (BufferedReader reader = new BufferedReader(\n                            new InputStreamReader(is, StandardCharsets.UTF_8))) {\n                        String line = reader.readLine();\n                        int lines = 0;\n                        long elapsed = System.currentTimeMillis() - start;\n                        LOGGER.info(\"Finished fetching {} bytes in {} ms for index gz: {}\",\n                                String.format(Locale.US, \"%,d\", tis.getLength()),\n                                String.format(Locale.US, \"%,d\", elapsed),\n                                fetchEmitTuple.getFetchKey().getFetchKey());", "                        while (line != null) {\n                            LOGGER.trace(\"about to add a line\");\n                            if (StringUtils.isBlank(line)) {\n                                line = reader.readLine();\n                                continue;\n                            }\n                            try {\n                                boolean shouldContinue = recordProcessor.process(line);\n                                if (!shouldContinue) {\n                                    return shouldContinue;\n                                }\n                            } catch (IOException e) {\n                                LOGGER.warn(\"bad json: \" + line);\n                            }\n                            lines++;\n                            line = reader.readLine();\n                        }\n                    }\n                }\n            } catch (TikaException | IOException e) {\n                LOGGER.error(\n                        \"failed while processing \" + fetchEmitTuple.getFetchKey().getFetchKey(), e);\n            }\n            long elapsed = System.currentTimeMillis() - start;\n            LOGGER.info(\"finished processing index gz in ({}) ms: {}\",\n                    String.format(Locale.US, \"%,d\", elapsed),\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            return true;\n        }\n    }\n}\n", "                                if (!shouldContinue) {\n                                    return shouldContinue;\n                                }\n                            } catch (IOException e) {\n                                LOGGER.warn(\"bad json: \" + line);\n                            }\n                            lines++;\n                            line = reader.readLine();\n                        }\n                    }\n                }\n            } catch (TikaException | IOException e) {\n                LOGGER.error(\n                        \"failed while processing \" + fetchEmitTuple.getFetchKey().getFetchKey(), e);\n            }\n            long elapsed = System.currentTimeMillis() - start;\n            LOGGER.info(\"finished processing index gz in ({}) ms: {}\",\n                    String.format(Locale.US, \"%,d\", elapsed),\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            return true;\n        }\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/extractor/FileFromCCWarcExtractor.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.extractor;\n", "package org.tallison.cc.index.extractor;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.ByteArrayOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.StandardCopyOption;\nimport java.util.Optional;", "import java.nio.file.StandardCopyOption;\nimport java.util.Optional;\nimport java.util.zip.GZIPInputStream;\n\nimport org.apache.commons.codec.binary.Base32;\nimport org.apache.commons.codec.digest.DigestUtils;\nimport org.apache.commons.io.IOUtils;\nimport org.netpreserve.jwarc.MediaType;\nimport org.netpreserve.jwarc.WarcPayload;\nimport org.netpreserve.jwarc.WarcReader;", "import org.netpreserve.jwarc.WarcPayload;\nimport org.netpreserve.jwarc.WarcReader;\nimport org.netpreserve.jwarc.WarcRecord;\nimport org.netpreserve.jwarc.WarcResponse;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.CCIndexReaderCounter;\nimport org.tallison.cc.index.CCIndexRecord;\nimport org.tallison.cc.index.io.TargetPathRewriter;\n", "import org.tallison.cc.index.io.TargetPathRewriter;\n\nimport org.apache.tika.exception.TikaConfigException;\nimport org.apache.tika.exception.TikaException;\nimport org.apache.tika.io.TikaInputStream;\nimport org.apache.tika.metadata.Metadata;\nimport org.apache.tika.pipes.FetchEmitTuple;\nimport org.apache.tika.pipes.emitter.EmitKey;\nimport org.apache.tika.pipes.emitter.StreamEmitter;\nimport org.apache.tika.pipes.fetcher.FetchKey;", "import org.apache.tika.pipes.emitter.StreamEmitter;\nimport org.apache.tika.pipes.fetcher.FetchKey;\nimport org.apache.tika.pipes.fetcher.RangeFetcher;\n\npublic class FileFromCCWarcExtractor {\n    private static Logger LOGGER =\n            LoggerFactory.getLogger(FileFromCCWarcExtractor.class);\n    private static Logger EXTRACTED_LOGGER = LoggerFactory.getLogger(\"extracted-urls\");\n    private static Logger EXTRACTED_ALL_LOGGER = LoggerFactory.getLogger(\"extracted-urls-all\");\n    private final StreamEmitter emitter;\n    private final TargetPathRewriter targetPathRewriter;\n\n    private RangeFetcher fetcher;\n    private final boolean extractTruncated;\n    private Base32 base32 = new Base32();\n\n    private final CCIndexReaderCounter ccIndexReaderCounter;\n    public FileFromCCWarcExtractor(ExtractorConfig fetcherConfig,\n                                   CCIndexReaderCounter ccIndexReaderCounter) throws TikaConfigException {\n        this.emitter = fetcherConfig.newEmitter();\n        this.fetcher = (RangeFetcher) fetcherConfig.newFetcher();\n        this.targetPathRewriter = fetcherConfig.getTargetPathRewriter();\n        this.extractTruncated = fetcherConfig.isExtractTruncated();\n        this.ccIndexReaderCounter = ccIndexReaderCounter;\n    }\n", "    public void fetchToPath(CCIndexRecord record) throws InterruptedException {\n\n        LOGGER.debug(\"going to fetch {} {}->{}\", record.getFilename(), record.getOffset(),\n                record.getLength());\n        FetchEmitTuple t = new FetchEmitTuple(record.getFilename(),\n                new FetchKey(\"\", record.getFilename(), record.getOffset(),\n                        record.getOffset() + record.getLength() - 1), new EmitKey());\n        byte[] warcRecordGZBytes;\n        try {\n            warcRecordGZBytes = fetchWarcBytes(t);\n        } catch (TikaException | IOException e) {\n            LOGGER.warn(\"couldn't get bytes from cc's warc \" + t, e);\n            return;\n        }\n        String id = record.getUrl();", "        try {\n            warcRecordGZBytes = fetchWarcBytes(t);\n        } catch (TikaException | IOException e) {\n            LOGGER.warn(\"couldn't get bytes from cc's warc \" + t, e);\n            return;\n        }\n        String id = record.getUrl();\n        try {\n            parseWarc(id, record, warcRecordGZBytes);\n        } catch (IOException e) {\n            LOGGER.warn(\"problem parsing warc file\", e);\n        }\n    }\n\n\n    private void fetchPayload(String id, CCIndexRecord ccIndexRecord, WarcRecord record)\n            throws IOException {", "        if (!((record instanceof WarcResponse) &&\n                record.contentType().base().equals(MediaType.HTTP))) {\n            return;\n        }\n\n        Optional<WarcPayload> payload = ((WarcResponse) record).payload();\n        if (!payload.isPresent()) {\n            LOGGER.debug(\"no payload {}\", id);\n            ccIndexReaderCounter.getEmptyPayload().incrementAndGet();\n            return;\n        }", "        if (payload.get().body().size() == 0) {\n            LOGGER.debug(\"empty payload id={}\", id);\n            ccIndexReaderCounter.getEmptyPayload().incrementAndGet();\n            return;\n        }\n\n        Path tmp = Files.createTempFile(\"ccfile-fetcher-\", \"\");\n\n        try {\n            Files.copy(payload.get().body().stream(), tmp, StandardCopyOption.REPLACE_EXISTING);\n            String targetDigest = null;\n            String base32Sha1 = \"\";\n            try (InputStream is = Files.newInputStream(tmp)) {\n                base32Sha1 = base32.encodeAsString(DigestUtils.sha1(is));\n            } catch (IOException e) {\n                LOGGER.warn(\"IOException during digesting: \" + tmp.toAbsolutePath());\n                return;\n            }", "        try {\n            Files.copy(payload.get().body().stream(), tmp, StandardCopyOption.REPLACE_EXISTING);\n            String targetDigest = null;\n            String base32Sha1 = \"\";\n            try (InputStream is = Files.newInputStream(tmp)) {\n                base32Sha1 = base32.encodeAsString(DigestUtils.sha1(is));\n            } catch (IOException e) {\n                LOGGER.warn(\"IOException during digesting: \" + tmp.toAbsolutePath());\n                return;\n            }\n            if (!base32Sha1.equals(ccIndexRecord.getDigest())) {\n                LOGGER.warn(\"Bad digest for url={} ccindex={} sha1={}\", id,\n                        ccIndexRecord.getDigest(), base32Sha1);\n            }\n            //TODO: make digest and encoding configurable\n            try (InputStream is = Files.newInputStream(tmp)) {\n                targetDigest = DigestUtils.sha256Hex(is);\n            } catch (IOException e) {\n                LOGGER.warn(\"IOException during digesting: \" + tmp.toAbsolutePath());\n                return;\n            }\n            long length = -1;", "            if (!base32Sha1.equals(ccIndexRecord.getDigest())) {\n                LOGGER.warn(\"Bad digest for url={} ccindex={} sha1={}\", id,\n                        ccIndexRecord.getDigest(), base32Sha1);\n            }\n            //TODO: make digest and encoding configurable\n            try (InputStream is = Files.newInputStream(tmp)) {\n                targetDigest = DigestUtils.sha256Hex(is);\n            } catch (IOException e) {\n                LOGGER.warn(\"IOException during digesting: \" + tmp.toAbsolutePath());\n                return;\n            }\n            long length = -1;", "            try {\n                length = Files.size(tmp);\n            } catch (IOException e) {\n                LOGGER.warn(\"IOException during digesting: \" + tmp.toAbsolutePath());\n                return;\n            }\n            String targetPath = targetPathRewriter.rewrite(targetDigest);\n            Metadata metadata = new Metadata();\n            try (InputStream is = TikaInputStream.get(tmp, metadata)) {\n                emitter.emit(targetPath, is, new Metadata());\n                logSuccess(ccIndexRecord, targetDigest, length, targetPath);\n            } catch (IOException | TikaException e) {\n                LOGGER.warn(\"problem writing id={}\", id, e);\n            }\n        } finally {", "            try {\n                Files.delete(tmp);\n            } catch (IOException e) {\n                LOGGER.warn(\"can't delete \" + tmp.toAbsolutePath(), e);\n            }\n        }\n    }\n\n    private void logSuccess(CCIndexRecord ccIndexRecord, String targetDigest, long length,\n                            String targetPath) {\n        if (extractTruncated) {\n            EXTRACTED_ALL_LOGGER.info(\"\", ccIndexRecord.getUrl(),\n                    ccIndexRecord.getNormalizedMime(),\n                    ccIndexRecord.getNormalizedMimeDetected(),\n                    ccIndexRecord.getFilename(),\n                    ccIndexRecord.getOffset(), ccIndexRecord.getLength(),\n                    ccIndexRecord.getTruncated(), targetDigest, length,\n                    targetPath);\n        } else {\n            //new ObjectArray ?\n            //url,mime_detected,warc_file,warc_offset,warc_length,sha256,length,path\n            EXTRACTED_LOGGER.info(\"\", ccIndexRecord.getUrl(),\n                    ccIndexRecord.getNormalizedMime(),\n                    ccIndexRecord.getNormalizedMimeDetected(),\n                    ccIndexRecord.getFilename(),\n                    ccIndexRecord.getOffset(), ccIndexRecord.getLength(),\n                    targetDigest, length,\n                    targetPath);\n        }\n\n\n    }\n\n    private void parseWarc(String id, CCIndexRecord ccIndexRecord, byte[] warcRecordGZBytes)\n            throws IOException {\n        //need to leave initial inputstream open while parsing warcrecord\n        //can't just parse record and return\n        try (InputStream is = new GZIPInputStream(new ByteArrayInputStream(warcRecordGZBytes))) {\n            try (WarcReader warcreader = new WarcReader(is)) {\n\n                //should be a single warc per file\n                //return the first", "        if (extractTruncated) {\n            EXTRACTED_ALL_LOGGER.info(\"\", ccIndexRecord.getUrl(),\n                    ccIndexRecord.getNormalizedMime(),\n                    ccIndexRecord.getNormalizedMimeDetected(),\n                    ccIndexRecord.getFilename(),\n                    ccIndexRecord.getOffset(), ccIndexRecord.getLength(),\n                    ccIndexRecord.getTruncated(), targetDigest, length,\n                    targetPath);\n        } else {\n            //new ObjectArray ?\n            //url,mime_detected,warc_file,warc_offset,warc_length,sha256,length,path\n            EXTRACTED_LOGGER.info(\"\", ccIndexRecord.getUrl(),\n                    ccIndexRecord.getNormalizedMime(),\n                    ccIndexRecord.getNormalizedMimeDetected(),\n                    ccIndexRecord.getFilename(),\n                    ccIndexRecord.getOffset(), ccIndexRecord.getLength(),\n                    targetDigest, length,\n                    targetPath);\n        }\n\n\n    }\n\n    private void parseWarc(String id, CCIndexRecord ccIndexRecord, byte[] warcRecordGZBytes)\n            throws IOException {\n        //need to leave initial inputstream open while parsing warcrecord\n        //can't just parse record and return\n        try (InputStream is = new GZIPInputStream(new ByteArrayInputStream(warcRecordGZBytes))) {\n            try (WarcReader warcreader = new WarcReader(is)) {\n\n                //should be a single warc per file\n                //return the first", "                for (WarcRecord warcRecord : warcreader) {\n                    fetchPayload(id, ccIndexRecord, warcRecord);\n                    return;\n                }\n            }\n        }\n    }\n\n    private byte[] fetchWarcBytes(FetchEmitTuple t)\n            throws TikaException, InterruptedException, IOException {\n\n        ByteArrayOutputStream bos = new ByteArrayOutputStream();\n        FetchKey k = t.getFetchKey();\n        try (InputStream is = fetcher.fetch(k.getFetchKey(), k.getRangeStart(), k.getRangeEnd(),\n                new Metadata())) {\n            IOUtils.copy(is, bos);\n        }\n        return bos.toByteArray();\n    }\n\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/extractor/ExtractorConfig.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.extractor;\n", "package org.tallison.cc.index.extractor;\n\nimport java.nio.file.Path;\nimport java.util.Collections;\n\nimport com.fasterxml.jackson.annotation.JsonCreator;\nimport com.fasterxml.jackson.annotation.JsonProperty;\nimport org.tallison.cc.index.IndexIterator;\nimport org.tallison.cc.index.io.BackoffHttpFetcher;\nimport org.tallison.cc.index.io.TargetPathRewriter;", "import org.tallison.cc.index.io.BackoffHttpFetcher;\nimport org.tallison.cc.index.io.TargetPathRewriter;\nimport org.tallison.cc.index.selector.RecordSelector;\n\nimport org.apache.tika.config.Initializable;\nimport org.apache.tika.exception.TikaConfigException;\nimport org.apache.tika.pipes.emitter.StreamEmitter;\nimport org.apache.tika.pipes.emitter.fs.FileSystemEmitter;\nimport org.apache.tika.pipes.emitter.s3.S3Emitter;\nimport org.apache.tika.pipes.fetcher.Fetcher;", "import org.apache.tika.pipes.emitter.s3.S3Emitter;\nimport org.apache.tika.pipes.fetcher.Fetcher;\nimport org.apache.tika.pipes.fetcher.fs.FileSystemFetcher;\nimport org.apache.tika.pipes.fetcher.s3.S3Fetcher;\nimport org.apache.tika.utils.StringUtils;\n\npublic class ExtractorConfig {\n\n    public static String CC_HTTPS_BASE = \"https://data.commoncrawl.org\";\n\n    public static String CC_S3_BUCKET = \"commoncrawl\";\n", "    public static String CC_HTTPS_BASE = \"https://data.commoncrawl.org\";\n\n    public static String CC_S3_BUCKET = \"commoncrawl\";\n\n    public static String CC_REGION = \"us-east-1\";\n\n    public static String DEFAULT_FS_DOCS_PATH = \"docs\";\n\n    public static long[] DEFAULT_THROTTLE_SECONDS = new long[]{30, 120, 600, 1800};\n    private int numThreads = 2;\n    //maximum records to read\n    private long maxRecords = -1;\n\n    //maximum files extracted from cc\n    private long maxFilesExtracted = -1;\n    //maximum files written to 'truncated' logger\n    private long maxFilesTruncated = -1;\n\n    private Path indexPathsFile;\n    private String targetPathPattern = \"\";\n\n    private boolean dryRun = false;\n\n    private boolean extractTruncated = false;\n\n    private RecordSelector recordSelector = RecordSelector.ACCEPT_ALL_RECORDS;\n\n    @JsonProperty(\"indices\")\n    private IndexIterator indexIterator;\n\n    @JsonProperty(\"fetcher\")\n    private FetchConfig fetchConfig;\n\n    @JsonProperty(\"indexFetcher\")\n    private FetchConfig indexFetchConfig;\n\n    @JsonProperty(\"docs\")\n    private EmitConfig emitConfig;\n", "    public static long[] DEFAULT_THROTTLE_SECONDS = new long[]{30, 120, 600, 1800};\n    private int numThreads = 2;\n    //maximum records to read\n    private long maxRecords = -1;\n\n    //maximum files extracted from cc\n    private long maxFilesExtracted = -1;\n    //maximum files written to 'truncated' logger\n    private long maxFilesTruncated = -1;\n\n    private Path indexPathsFile;\n    private String targetPathPattern = \"\";\n\n    private boolean dryRun = false;\n\n    private boolean extractTruncated = false;\n\n    private RecordSelector recordSelector = RecordSelector.ACCEPT_ALL_RECORDS;\n\n    @JsonProperty(\"indices\")\n    private IndexIterator indexIterator;\n\n    @JsonProperty(\"fetcher\")\n    private FetchConfig fetchConfig;\n\n    @JsonProperty(\"indexFetcher\")\n    private FetchConfig indexFetchConfig;\n\n    @JsonProperty(\"docs\")\n    private EmitConfig emitConfig;\n", "    public static String getCcHttpsBase() {\n        return CC_HTTPS_BASE;\n    }\n\n    public int getNumThreads() {\n        return numThreads;\n    }\n\n    public void setNumThreads(int numThreads) {\n        this.numThreads = numThreads;\n    }\n", "    public void setNumThreads(int numThreads) {\n        this.numThreads = numThreads;\n    }\n\n    public long getMaxRecords() {\n        return maxRecords;\n    }\n\n    public void setMaxRecords(long maxRecords) {\n        this.maxRecords = maxRecords;\n    }\n", "    public void setMaxRecords(long maxRecords) {\n        this.maxRecords = maxRecords;\n    }\n\n    public long getMaxFilesExtracted() {\n        return maxFilesExtracted;\n    }\n\n    public void setMaxFilesExtracted(long maxFilesExtracted) {\n        this.maxFilesExtracted = maxFilesExtracted;\n    }\n", "    public void setMaxFilesExtracted(long maxFilesExtracted) {\n        this.maxFilesExtracted = maxFilesExtracted;\n    }\n\n    public long getMaxFilesTruncated() {\n        return maxFilesTruncated;\n    }\n\n    public void setMaxFilesTruncated(long maxFilesTruncated) {\n        this.maxFilesTruncated = maxFilesTruncated;\n    }\n", "    public void setMaxFilesTruncated(long maxFilesTruncated) {\n        this.maxFilesTruncated = maxFilesTruncated;\n    }\n\n    public Path getIndexPathsFile() {\n        return indexPathsFile;\n    }\n\n    public void setIndexPathsFile(Path indexPathsFile) {\n        this.indexPathsFile = indexPathsFile;\n    }\n\n", "    public void setIndexPathsFile(Path indexPathsFile) {\n        this.indexPathsFile = indexPathsFile;\n    }\n\n\n    public boolean isDryRun() {\n        return dryRun;\n    }\n\n    public void setDryRun(boolean dryRun) {\n        this.dryRun = dryRun;\n    }\n", "    public void setDryRun(boolean dryRun) {\n        this.dryRun = dryRun;\n    }\n\n    public String getTargetPathPattern() {\n        return targetPathPattern;\n    }\n\n    public void setTargetPathPattern(String pathPattern) {\n        this.targetPathPattern = pathPattern;\n    }\n", "    public void setTargetPathPattern(String pathPattern) {\n        this.targetPathPattern = pathPattern;\n    }\n\n    public TargetPathRewriter getTargetPathRewriter() {\n        return new TargetPathRewriter(targetPathPattern);\n    }\n\n    public RecordSelector getRecordSelector() {\n        return recordSelector;\n    }\n", "    public RecordSelector getRecordSelector() {\n        return recordSelector;\n    }\n\n    public void setRecordSelector(RecordSelector recordSelector) {\n        this.recordSelector = recordSelector;\n    }\n\n    public IndexIterator getIndexIterator() {\n        return indexIterator;\n    }\n", "    public IndexIterator getIndexIterator() {\n        return indexIterator;\n    }\n\n    public Fetcher newFetcher() throws TikaConfigException {\n        if (fetchConfig == null) {\n            fetchConfig = new FetchConfig(null, DEFAULT_THROTTLE_SECONDS, null);\n        }\n        return fetchConfig.newFetcher();\n    }\n", "    public Fetcher newIndexFetcher() throws TikaConfigException {\n        if (indexFetchConfig == null) {\n            indexFetchConfig = new FetchConfig(null, DEFAULT_THROTTLE_SECONDS, null);\n        }\n        return indexFetchConfig.newFetcher();\n    }\n\n    public StreamEmitter newEmitter() throws TikaConfigException {\n        if (emitConfig == null) {\n            emitConfig = new EmitConfig(DEFAULT_FS_DOCS_PATH);\n        }\n        return emitConfig.newEmitter();\n    }\n", "        if (emitConfig == null) {\n            emitConfig = new EmitConfig(DEFAULT_FS_DOCS_PATH);\n        }\n        return emitConfig.newEmitter();\n    }\n\n    public void setExtractTruncated(boolean extractTruncated) {\n        this.extractTruncated = extractTruncated;\n    }\n\n    public boolean isExtractTruncated() {\n        return extractTruncated;\n    }\n\n    private static class FetchConfig {\n        private final String profile;\n        private final long[] throttleSeconds;\n        private final String basePath;\n\n        @JsonCreator\n        public FetchConfig(@JsonProperty(\"profile\") String profile,\n                           @JsonProperty(\"throttleSeconds\") long[] throttleSeconds,\n                           @JsonProperty(\"basePath\") String basePath) {\n            this.profile = profile;\n            this.throttleSeconds =\n                    (throttleSeconds == null) ? DEFAULT_THROTTLE_SECONDS : throttleSeconds;\n            this.basePath = basePath;\n        }\n\n        Fetcher newFetcher() throws TikaConfigException {\n            Fetcher fetcher;", "    public boolean isExtractTruncated() {\n        return extractTruncated;\n    }\n\n    private static class FetchConfig {\n        private final String profile;\n        private final long[] throttleSeconds;\n        private final String basePath;\n\n        @JsonCreator\n        public FetchConfig(@JsonProperty(\"profile\") String profile,\n                           @JsonProperty(\"throttleSeconds\") long[] throttleSeconds,\n                           @JsonProperty(\"basePath\") String basePath) {\n            this.profile = profile;\n            this.throttleSeconds =\n                    (throttleSeconds == null) ? DEFAULT_THROTTLE_SECONDS : throttleSeconds;\n            this.basePath = basePath;\n        }\n\n        Fetcher newFetcher() throws TikaConfigException {\n            Fetcher fetcher;", "            if (profile != null) {\n                fetcher = new S3Fetcher();\n                ((S3Fetcher) fetcher).setProfile(profile);\n                ((S3Fetcher) fetcher).setCredentialsProvider(\"profile\");\n                ((S3Fetcher) fetcher).setBucket(ExtractorConfig.CC_S3_BUCKET);\n                ((S3Fetcher) fetcher).setRegion(ExtractorConfig.CC_REGION);\n                //Update and make configurable once TIKA-3993 is fixed\n                ((S3Fetcher) fetcher).setRetries(3);\n                ((S3Fetcher) fetcher).setSleepBeforeRetryMillis(30000);\n            } else if (basePath != null) {\n                fetcher = new FileSystemFetcher();\n                ((FileSystemFetcher) fetcher).setBasePath(basePath);\n            } else {\n                fetcher = new BackoffHttpFetcher(throttleSeconds);\n            }", "            } else if (basePath != null) {\n                fetcher = new FileSystemFetcher();\n                ((FileSystemFetcher) fetcher).setBasePath(basePath);\n            } else {\n                fetcher = new BackoffHttpFetcher(throttleSeconds);\n            }\n            if (fetcher instanceof Initializable) {\n                ((Initializable) fetcher).initialize(Collections.EMPTY_MAP);\n            }\n            return fetcher;\n        }\n    }\n\n    private static class EmitConfig {\n        private String profile;\n        private String region;\n        private String bucket;\n        private String prefix;\n        private String path;\n\n        private EmitConfig(String path) {\n            this.path = path;\n        }\n\n        //TODO -- clean this up with different classes\n        //for the different fetchers and use jackson's inference\n        @JsonCreator\n        public EmitConfig(@JsonProperty(\"profile\") String profile,\n                          @JsonProperty(\"region\") String region,\n                          @JsonProperty(\"bucket\") String bucket,\n                          @JsonProperty(\"prefix\") String prefix,\n                          @JsonProperty(\"path\") String path) {\n            this.profile = profile;\n            this.region = region;\n            this.bucket = bucket;\n            this.prefix = prefix;\n            this.path = path;\n        }\n", "        public StreamEmitter newEmitter() throws TikaConfigException {\n            if (!StringUtils.isBlank(profile)) {\n                S3Emitter emitter = new S3Emitter();\n                emitter.setCredentialsProvider(\"profile\");\n                emitter.setProfile(profile);\n\n                if (StringUtils.isBlank(bucket)) {\n                    throw new TikaConfigException(\"Must specify a bucket for docs\");\n                }\n                emitter.setBucket(bucket);\n                if (region != null) {\n                    emitter.setRegion(region);\n                } else {\n                    emitter.setRegion(ExtractorConfig.CC_REGION);\n                }", "                if (region != null) {\n                    emitter.setRegion(region);\n                } else {\n                    emitter.setRegion(ExtractorConfig.CC_REGION);\n                }\n                if (!StringUtils.isBlank(prefix)) {\n                    emitter.setPrefix(prefix);\n                }\n                emitter.setFileExtension(\"\");\n                emitter.initialize(Collections.EMPTY_MAP);\n                return emitter;\n            }", "            if (StringUtils.isBlank(path)) {\n                path = DEFAULT_FS_DOCS_PATH;\n            }\n            FileSystemEmitter emitter = new FileSystemEmitter();\n            emitter.setBasePath(path);\n            emitter.setOnExists(\"skip\");\n            emitter.setFileExtension(\"\");\n            return emitter;\n        }\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/extractor/CCFileExtractorRecordProcessor.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.extractor;\n", "package org.tallison.cc.index.extractor;\n\nimport java.io.IOException;\nimport java.util.Optional;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.AbstractRecordProcessor;\nimport org.tallison.cc.index.CCIndexReaderCounter;\nimport org.tallison.cc.index.CCIndexRecord;", "import org.tallison.cc.index.CCIndexReaderCounter;\nimport org.tallison.cc.index.CCIndexRecord;\n\nimport org.apache.tika.exception.TikaConfigException;\nimport org.apache.tika.utils.StringUtils;\n\npublic class CCFileExtractorRecordProcessor extends AbstractRecordProcessor {\n\n    private static Logger LOGGER = LoggerFactory.getLogger(CCFileExtractorRecordProcessor.class);\n    private static Logger TRUNCATED_URLS_LOGGER = LoggerFactory.getLogger(\"truncated-urls\");\n    private static Logger TRUNCATED_URLS_FULL_LOGGER =\n            LoggerFactory.getLogger(\"truncated-urls-full\");\n\n    private final ExtractorConfig fetcherConfig;\n    private final CCIndexReaderCounter counter;\n\n    private final FileFromCCWarcExtractor fileFromCCWarcFetcher;\n\n    private long reportEvery = 100000;\n\n    public CCFileExtractorRecordProcessor(ExtractorConfig fetcherConfig, CCIndexReaderCounter counter)\n            throws TikaConfigException, IOException {\n        this.fetcherConfig = fetcherConfig;\n        this.counter = counter;\n        this.fileFromCCWarcFetcher = new FileFromCCWarcExtractor(fetcherConfig, counter);\n        //completely arbitrary", "        if (fetcherConfig.getNumThreads() > 10) {\n            reportEvery = 1000000;\n        }\n    }\n\n    @Override\n    public boolean process(String json) throws IOException, InterruptedException {\n        //System.out.println(\"JSON: \" + json);\n        long totalRead = counter.getRecordsRead().incrementAndGet();\n        if (totalRead % reportEvery == 0) {\n            LOGGER.info(\"processed: {}\", counter);\n        }", "        if (totalRead % reportEvery == 0) {\n            LOGGER.info(\"processed: {}\", counter);\n        }\n        if (fetcherConfig.getMaxRecords() > -1 && totalRead >= fetcherConfig.getMaxRecords()) {\n            LOGGER.info(\"hit max read\");\n            return false;\n        }\n        //check for hit max\n        //return false;\n\n        Optional<CCIndexRecord> record = CCIndexRecord.parseRecord(json);", "        if (record.isEmpty()) {\n            //problem already logged\n            return true;\n        }\n        CCIndexRecord r = record.get();\n        if (!fetcherConfig.getRecordSelector().select(r)) {\n            return true;\n        }\n        //if truncated, count appropriately and test for limits\n        if (!StringUtils.isBlank(r.getTruncated())) {\n            long truncated = counter.getTruncated().incrementAndGet();", "        if (!StringUtils.isBlank(r.getTruncated())) {\n            long truncated = counter.getTruncated().incrementAndGet();\n            if (fetcherConfig.getMaxFilesTruncated() > -1 &&\n                    truncated >= fetcherConfig.getMaxFilesTruncated()) {\n                LOGGER.info(\"hit max truncated files\");\n                return false;\n            }\n        }\n\n        if (fetcherConfig.isExtractTruncated() || StringUtils.isBlank(r.getTruncated())) {\n            long extracted = counter.getFilesExtracted().incrementAndGet();", "        if (fetcherConfig.isExtractTruncated() || StringUtils.isBlank(r.getTruncated())) {\n            long extracted = counter.getFilesExtracted().incrementAndGet();\n            if (fetcherConfig.getMaxFilesExtracted() > -1 &&\n                    extracted >= fetcherConfig.getMaxFilesExtracted()) {\n                LOGGER.info(\"hit max extracted files\");\n                return false;\n            }\n            if (fetcherConfig.isDryRun()) {\n                LOGGER.info(\"dry run, but would have extracted {}\", r);\n                return true;\n            }\n            fetchBytes(r);\n            return true;\n        } else {\n            String url = r.getUrl();\n            TRUNCATED_URLS_LOGGER.info(\"\", url);\n            //url,mime,mime_detected,warc_file,warc_offset,warc_length,truncated\n            TRUNCATED_URLS_FULL_LOGGER.info(\"\", url,\n                    r.getNormalizedMime(), r.getNormalizedMimeDetected(), r.getFilename(),\n                    r.getOffset(), r.getLength(), r.getTruncated());\n            return true;\n        }\n    }\n\n    private void fetchBytes(CCIndexRecord r) throws InterruptedException {\n        fileFromCCWarcFetcher.fetchToPath(r);\n    }\n\n    @Override", "    public void close() throws IOException {\n\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/extractor/CCMimeCounter.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.extractor;\n", "package org.tallison.cc.index.extractor;\n\nimport java.io.BufferedInputStream;\nimport java.io.BufferedReader;\nimport java.io.BufferedWriter;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.InputStreamReader;\nimport java.nio.charset.StandardCharsets;", "import java.io.InputStreamReader;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Locale;\nimport java.util.Map;", "import java.util.Locale;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorCompletionService;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;", "import java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\nimport java.util.zip.GZIPInputStream;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.apache.commons.csv.CSVFormat;\nimport org.apache.commons.csv.CSVPrinter;\nimport org.apache.commons.lang3.mutable.MutableLong;", "import org.apache.commons.csv.CSVPrinter;\nimport org.apache.commons.lang3.mutable.MutableLong;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.AbstractRecordProcessor;\nimport org.tallison.cc.index.CCIndexReaderCounter;\nimport org.tallison.cc.index.CCIndexRecord;\nimport org.tallison.cc.index.IndexIterator;\n\nimport org.apache.tika.exception.TikaConfigException;", "\nimport org.apache.tika.exception.TikaConfigException;\nimport org.apache.tika.exception.TikaException;\nimport org.apache.tika.io.TikaInputStream;\nimport org.apache.tika.metadata.Metadata;\nimport org.apache.tika.pipes.FetchEmitTuple;\nimport org.apache.tika.pipes.fetcher.Fetcher;\nimport org.apache.tika.pipes.pipesiterator.CallablePipesIterator;\nimport org.apache.tika.pipes.pipesiterator.PipesIterator;\nimport org.apache.tika.utils.StringUtils;", "import org.apache.tika.pipes.pipesiterator.PipesIterator;\nimport org.apache.tika.utils.StringUtils;\n\n/**\n * This counts mime_detected.  Use a regular file selector to include\n * only urls that had a 200, e.g.\n */\npublic class CCMimeCounter {\n\n    private static final Long INDEX_WORKER_ID = 1l;\n    private static final Long INDEX_READER_ID = 2l;\n    private static final Logger LOGGER = LoggerFactory.getLogger(CCMimeCounter.class);\n", "    public static void main(String[] args) throws Exception {\n        ExtractorConfig fetcherConfig =\n                new ObjectMapper().readValue(new File(args[0]), ExtractorConfig.class);\n        execute(fetcherConfig);\n    }\n\n    private static void execute(ExtractorConfig fetcherConfig) throws IOException, TikaException {\n        ArrayBlockingQueue<FetchEmitTuple> indexPathsList = new ArrayBlockingQueue<>(1000);\n        //IndexPathsReader reads a file containing a list of cc-index.paths files\n        //and writes the literal gz files (cc-index/collections/CC-MAIN-2023-06/indexes/cdx-00000.gz)\n        //to indexPathsList\n\n\n        //IndexWorker reads a single index.gz file at a time and processes each record\n        //It fetches non truncated files and logs truncated files\n        int totalThreads = fetcherConfig.getNumThreads() + 1;\n\n        ExecutorService executorService = Executors.newFixedThreadPool(totalThreads);\n        ExecutorCompletionService<Long> executorCompletionService =\n                new ExecutorCompletionService<>(executorService);\n\n        IndexIterator indexIterator = fetcherConfig.getIndexIterator();\n        indexIterator.initialize(Collections.EMPTY_MAP);\n        executorCompletionService.submit(new CallablePipesIterator(indexIterator, indexPathsList));\n        CCIndexReaderCounter counter = new CCIndexReaderCounter();\n        int finishedWorkers = 0;\n        List<DetectedMimeCounter> detectedMimeCounters = new ArrayList<>();", "        try {\n            for (int i = 0; i < fetcherConfig.getNumThreads(); i++) {\n                DetectedMimeCounter processor = new DetectedMimeCounter(fetcherConfig, counter);\n                detectedMimeCounters.add(processor);\n                executorCompletionService.submit(\n                        new IndexWorker(fetcherConfig, indexPathsList, processor));\n            }\n\n\n            while (finishedWorkers < totalThreads) {\n                //blocking\n                Future<Long> future = executorCompletionService.take();", "            while (finishedWorkers < totalThreads) {\n                //blocking\n                Future<Long> future = executorCompletionService.take();\n                if (future != null) {\n                    Long f = future.get();\n                    LOGGER.debug(\"completed {}\", f);\n                    if (f.equals(INDEX_WORKER_ID)) {\n                        finishedWorkers++;\n                    } else if (f.equals(INDEX_READER_ID)) {\n                        LOGGER.info(\"Index paths reader successfully completed\");\n                    }\n                }\n            }\n        } catch (TikaConfigException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (ExecutionException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (InterruptedException e) {\n            LOGGER.warn(\"main loop interrupted exception\", e);\n            throw new RuntimeException(e);\n        } finally {\n            executorService.shutdown();\n            executorService.shutdownNow();\n        }\n\n        summarize(detectedMimeCounters);\n    }\n\n    private static void summarize(List<DetectedMimeCounter> detectedMimeCounters)\n            throws IOException {\n        Map<String, Long> total = new HashMap<>();\n        Map<String, Long> truncated = new HashMap<>();\n        Map<String, Long> nonTruncated = new HashMap<>();", "                    } else if (f.equals(INDEX_READER_ID)) {\n                        LOGGER.info(\"Index paths reader successfully completed\");\n                    }\n                }\n            }\n        } catch (TikaConfigException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (ExecutionException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (InterruptedException e) {\n            LOGGER.warn(\"main loop interrupted exception\", e);\n            throw new RuntimeException(e);\n        } finally {\n            executorService.shutdown();\n            executorService.shutdownNow();\n        }\n\n        summarize(detectedMimeCounters);\n    }\n\n    private static void summarize(List<DetectedMimeCounter> detectedMimeCounters)\n            throws IOException {\n        Map<String, Long> total = new HashMap<>();\n        Map<String, Long> truncated = new HashMap<>();\n        Map<String, Long> nonTruncated = new HashMap<>();", "        for (DetectedMimeCounter c : detectedMimeCounters) {\n            update(c.totalCounts, total);\n            update(c.truncatedCounts, truncated);\n        }\n        calcNonTruncated(truncated, total, nonTruncated);\n        report(\"total\", total);\n        report(\"truncated\", truncated);\n        report(\"non-truncated\", nonTruncated);\n    }\n\n    private static void calcNonTruncated(Map<String, Long> truncated, Map<String, Long> total,\n                                         Map<String, Long> nonTruncated) {", "        for (Map.Entry<String, Long> e : total.entrySet()) {\n            Long val = e.getValue();\n            Long t = truncated.getOrDefault(e.getKey(), 0l);\n            val -= t;\n            nonTruncated.put(e.getKey(), val);\n        }\n    }\n\n    private static void report(String name, Map<String, Long> m) throws IOException {\n        try (BufferedWriter writer = Files.newBufferedWriter(Paths.get(name + \".csv\"),\n                StandardCharsets.UTF_8)) {\n            try (CSVPrinter printer = new CSVPrinter(writer, CSVFormat.EXCEL)) {\n                printer.printRecord(\"mime\", \"count\");\n                m.entrySet().stream().sorted(Collections.reverseOrder(Map.Entry.comparingByValue()))\n                        .forEach(e -> {", "                            try {\n                                printer.printRecord(e.getKey(), e.getValue());\n                            } catch (IOException ex) {\n                                throw new RuntimeException(ex);\n                            }\n                        });\n            }\n        }\n    }\n\n    private static void update(Map<String, MutableLong> from, Map<String, Long> to) {", "        for (Map.Entry<String, MutableLong> e : from.entrySet()) {\n            Long cnt = to.get(e.getKey());\n            if (cnt == null) {\n                cnt = 0l;\n            }\n            cnt += e.getValue().getValue();\n            to.put(e.getKey(), cnt);\n        }\n    }\n\n    private static class IndexWorker implements Callable<Long> {\n\n        private final ArrayBlockingQueue<FetchEmitTuple> indexUrls;\n        private final AbstractRecordProcessor recordProcessor;\n\n        private final Fetcher fetcher;\n\n        IndexWorker(ExtractorConfig fetcherConfig, ArrayBlockingQueue<FetchEmitTuple> indexUrls,\n                    AbstractRecordProcessor recordProcessor) throws TikaException {\n            this.indexUrls = indexUrls;\n            this.recordProcessor = recordProcessor;\n            this.fetcher = fetcherConfig.newFetcher();\n        }\n\n        @Override", "        public Long call() throws Exception {\n            boolean shouldContinue = true;\n            while (shouldContinue) {\n\n                FetchEmitTuple indexUrl = indexUrls.poll(120, TimeUnit.MINUTES);\n                if (indexUrl == null) {\n                    throw new TimeoutException(\"waited 120 minutes for a new record\");\n                }\n\n                if (indexUrl == PipesIterator.COMPLETED_SEMAPHORE) {\n                    recordProcessor.close();\n                    //can hang forever\n                    indexUrls.put(PipesIterator.COMPLETED_SEMAPHORE);\n                    return INDEX_WORKER_ID;\n                }\n                shouldContinue = processFile(indexUrl, recordProcessor);\n            }\n            return INDEX_WORKER_ID;\n        }\n\n        private boolean processFile(FetchEmitTuple fetchEmitTuple,\n                                    AbstractRecordProcessor recordProcessor)\n                throws InterruptedException {\n            long start = System.currentTimeMillis();\n            LOGGER.info(\"starting to fetch index gz: {}\",\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            try (TikaInputStream tis = (TikaInputStream) fetcher.fetch(\n                    fetchEmitTuple.getFetchKey().getFetchKey(), new Metadata())) {\n                try (InputStream is = new BufferedInputStream(new GZIPInputStream(tis))) {\n                    try (BufferedReader reader = new BufferedReader(\n                            new InputStreamReader(is, StandardCharsets.UTF_8))) {\n                        String line = reader.readLine();\n                        int lines = 0;\n                        long elapsed = System.currentTimeMillis() - start;\n                        LOGGER.info(\"Finished fetching {} bytes in {} ms for index gz: {}\",\n                                String.format(Locale.US, \"%,d\", tis.getLength()),\n                                String.format(Locale.US, \"%,d\", elapsed),\n                                fetchEmitTuple.getFetchKey().getFetchKey());", "                if (indexUrl == PipesIterator.COMPLETED_SEMAPHORE) {\n                    recordProcessor.close();\n                    //can hang forever\n                    indexUrls.put(PipesIterator.COMPLETED_SEMAPHORE);\n                    return INDEX_WORKER_ID;\n                }\n                shouldContinue = processFile(indexUrl, recordProcessor);\n            }\n            return INDEX_WORKER_ID;\n        }\n\n        private boolean processFile(FetchEmitTuple fetchEmitTuple,\n                                    AbstractRecordProcessor recordProcessor)\n                throws InterruptedException {\n            long start = System.currentTimeMillis();\n            LOGGER.info(\"starting to fetch index gz: {}\",\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            try (TikaInputStream tis = (TikaInputStream) fetcher.fetch(\n                    fetchEmitTuple.getFetchKey().getFetchKey(), new Metadata())) {\n                try (InputStream is = new BufferedInputStream(new GZIPInputStream(tis))) {\n                    try (BufferedReader reader = new BufferedReader(\n                            new InputStreamReader(is, StandardCharsets.UTF_8))) {\n                        String line = reader.readLine();\n                        int lines = 0;\n                        long elapsed = System.currentTimeMillis() - start;\n                        LOGGER.info(\"Finished fetching {} bytes in {} ms for index gz: {}\",\n                                String.format(Locale.US, \"%,d\", tis.getLength()),\n                                String.format(Locale.US, \"%,d\", elapsed),\n                                fetchEmitTuple.getFetchKey().getFetchKey());", "                        while (line != null) {\n                            LOGGER.trace(\"about to add a line\");\n                            if (StringUtils.isBlank(line)) {\n                                line = reader.readLine();\n                                continue;\n                            }\n                            try {\n                                boolean shouldContinue = recordProcessor.process(line);\n                                if (!shouldContinue) {\n                                    return shouldContinue;\n                                }\n                            } catch (IOException e) {\n                                LOGGER.warn(\"bad json: \" + line);\n                            }\n                            lines++;\n                            line = reader.readLine();\n                        }\n                    }\n                }\n            } catch (TikaException | IOException e) {\n                LOGGER.error(\n                        \"failed while processing \" + fetchEmitTuple.getFetchKey().getFetchKey(), e);\n            }\n            long elapsed = System.currentTimeMillis() - start;\n            LOGGER.info(\"finished processing index gz in ({}) ms: {}\",\n                    String.format(Locale.US, \"%,d\", elapsed),\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            return true;\n        }\n    }\n\n    private static class DetectedMimeCounter extends AbstractRecordProcessor {\n        private final ExtractorConfig fetcherConfig;\n        private final CCIndexReaderCounter counter;\n        private final Map<String, MutableLong> totalCounts = new HashMap<>();\n        private final Map<String, MutableLong> truncatedCounts = new HashMap<>();\n        public DetectedMimeCounter(ExtractorConfig fetcherConfig, CCIndexReaderCounter counter) {\n            this.fetcherConfig = fetcherConfig;\n            this.counter = counter;\n        }\n\n        @Override", "                                if (!shouldContinue) {\n                                    return shouldContinue;\n                                }\n                            } catch (IOException e) {\n                                LOGGER.warn(\"bad json: \" + line);\n                            }\n                            lines++;\n                            line = reader.readLine();\n                        }\n                    }\n                }\n            } catch (TikaException | IOException e) {\n                LOGGER.error(\n                        \"failed while processing \" + fetchEmitTuple.getFetchKey().getFetchKey(), e);\n            }\n            long elapsed = System.currentTimeMillis() - start;\n            LOGGER.info(\"finished processing index gz in ({}) ms: {}\",\n                    String.format(Locale.US, \"%,d\", elapsed),\n                    fetchEmitTuple.getFetchKey().getFetchKey());\n            return true;\n        }\n    }\n\n    private static class DetectedMimeCounter extends AbstractRecordProcessor {\n        private final ExtractorConfig fetcherConfig;\n        private final CCIndexReaderCounter counter;\n        private final Map<String, MutableLong> totalCounts = new HashMap<>();\n        private final Map<String, MutableLong> truncatedCounts = new HashMap<>();\n        public DetectedMimeCounter(ExtractorConfig fetcherConfig, CCIndexReaderCounter counter) {\n            this.fetcherConfig = fetcherConfig;\n            this.counter = counter;\n        }\n\n        @Override", "        public boolean process(String json) throws IOException, InterruptedException {\n            long totalRead = counter.getRecordsRead().incrementAndGet();\n            if (totalRead % 1000000 == 0) {\n                LOGGER.info(\"processed: {}\", counter);\n            }\n            if (fetcherConfig.getMaxRecords() > -1 && totalRead >= fetcherConfig.getMaxRecords()) {\n                LOGGER.info(\"hit max read\");\n                return false;\n            }\n            //check for hit max\n            //return false;\n\n            Optional<CCIndexRecord> record = CCIndexRecord.parseRecord(json);", "            if (record.isEmpty()) {\n                //problem already logged\n                return true;\n            }\n            CCIndexRecord r = record.get();\n            if (!fetcherConfig.getRecordSelector().select(r)) {\n                return true;\n            }\n            increment(totalCounts, r.getNormalizedMimeDetected());\n            if (!StringUtils.isBlank(r.getTruncated())) {\n                long truncated = counter.getTruncated().incrementAndGet();", "            if (!StringUtils.isBlank(r.getTruncated())) {\n                long truncated = counter.getTruncated().incrementAndGet();\n                if (fetcherConfig.getMaxFilesTruncated() > -1 &&\n                        truncated >= fetcherConfig.getMaxFilesTruncated()) {\n                    LOGGER.info(\"hit max truncated files\");\n                    return false;\n                }\n                increment(truncatedCounts, r.getNormalizedMimeDetected());\n                return true;\n            }\n            return true;\n        }\n\n        private void increment(Map<String, MutableLong> m, String k) {\n            MutableLong cnt = m.get(k);", "            if (cnt == null) {\n                cnt = new MutableLong(1);\n                m.put(k, cnt);\n                return;\n            } else {\n                cnt.increment();\n            }\n        }\n\n        @Override\n        public void close() throws IOException {\n\n        }\n    }\n}\n", "        public void close() throws IOException {\n\n        }\n    }\n}\n"]}
{"filename": "src/main/java/org/tallison/cc/index/extractor/CCIndexFetcher.java", "chunked_list": ["/*\n * Licensed to the Apache Software Foundation (ASF) under one or more\n * contributor license agreements.  See the NOTICE file distributed with\n * this work for additional information regarding copyright ownership.\n * The ASF licenses this file to You under the Apache License, Version 2.0\n * (the \"License\"); you may not use this file except in compliance with\n * the License.  You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *", " *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage org.tallison.cc.index.extractor;\n", "package org.tallison.cc.index.extractor;\n\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.Collections;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.Callable;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorCompletionService;", "import java.util.concurrent.ExecutionException;\nimport java.util.concurrent.ExecutorCompletionService;\nimport java.util.concurrent.ExecutorService;\nimport java.util.concurrent.Executors;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.slf4j.Logger;", "import com.fasterxml.jackson.databind.ObjectMapper;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\nimport org.tallison.cc.index.IndexIterator;\n\nimport org.apache.tika.exception.TikaException;\nimport org.apache.tika.metadata.Metadata;\nimport org.apache.tika.pipes.FetchEmitTuple;\nimport org.apache.tika.pipes.emitter.StreamEmitter;\nimport org.apache.tika.pipes.fetcher.Fetcher;", "import org.apache.tika.pipes.emitter.StreamEmitter;\nimport org.apache.tika.pipes.fetcher.Fetcher;\nimport org.apache.tika.pipes.pipesiterator.CallablePipesIterator;\nimport org.apache.tika.pipes.pipesiterator.PipesIterator;\n\n/**\n * This class fetches index files from aws to a local file share.\n * <p>\n * This pulls the index files either via https or s3\n */\npublic class CCIndexFetcher {\n    private static final Logger LOGGER = LoggerFactory.getLogger(CCIndexFetcher.class);\n", " * This pulls the index files either via https or s3\n */\npublic class CCIndexFetcher {\n    private static final Logger LOGGER = LoggerFactory.getLogger(CCIndexFetcher.class);\n\n    public static void main(String[] args) throws Exception {\n        ExtractorConfig fetcherConfig =\n                new ObjectMapper().readValue(new File(args[0]), ExtractorConfig.class);\n        execute(fetcherConfig);\n    }\n\n    private static void execute(ExtractorConfig fetcherConfig) throws Exception {\n        ArrayBlockingQueue<FetchEmitTuple> indexPathsList = new ArrayBlockingQueue<>(1000);\n        //IndexPathsReader reads a file containing a list of cc-index.paths files\n        //and writes the literal gz files (cc-index/collections/CC-MAIN-2023-06/indexes/cdx-00000.gz)\n        //to indexPathsList\n\n        int totalThreads = fetcherConfig.getNumThreads() + 1;\n\n        ExecutorService executorService = Executors.newFixedThreadPool(totalThreads);\n        ExecutorCompletionService<Long> executorCompletionService =\n                new ExecutorCompletionService<>(executorService);\n\n        IndexIterator indexIterator = fetcherConfig.getIndexIterator();\n        indexIterator.initialize(Collections.EMPTY_MAP);\n        executorCompletionService.submit(new CallablePipesIterator(indexIterator, indexPathsList));\n        int finishedWorkers = 0;", "        try {\n            for (int i = 0; i < fetcherConfig.getNumThreads(); i++) {\n                executorCompletionService.submit(new IndexFetcher(fetcherConfig, indexPathsList));\n            }\n\n            while (finishedWorkers < totalThreads) {\n                //blocking\n                Future<Long> future = executorCompletionService.take();\n                if (future != null) {\n                    Long f = future.get();\n                    finishedWorkers++;\n                    LOGGER.debug(\"completed {}: {}\", f, finishedWorkers);\n                }\n            }\n        } catch (ExecutionException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (InterruptedException e) {\n            LOGGER.warn(\"main loop interrupted exception\", e);\n            throw new RuntimeException(e);\n        } finally {\n            executorService.shutdown();\n            executorService.shutdownNow();\n        }\n    }\n\n    private static class IndexFetcher implements Callable<Long> {\n\n        private final ExtractorConfig fetcherConfig;\n        private final ArrayBlockingQueue<FetchEmitTuple> indexPathsList;\n\n        public IndexFetcher(ExtractorConfig fetcherConfig,\n                            ArrayBlockingQueue<FetchEmitTuple> indexPathsList) {\n            this.fetcherConfig = fetcherConfig;\n            this.indexPathsList = indexPathsList;\n        }\n\n        @Override", "                if (future != null) {\n                    Long f = future.get();\n                    finishedWorkers++;\n                    LOGGER.debug(\"completed {}: {}\", f, finishedWorkers);\n                }\n            }\n        } catch (ExecutionException e) {\n            LOGGER.error(\"main loop exception\", e);\n            throw new RuntimeException(e);\n        } catch (InterruptedException e) {\n            LOGGER.warn(\"main loop interrupted exception\", e);\n            throw new RuntimeException(e);\n        } finally {\n            executorService.shutdown();\n            executorService.shutdownNow();\n        }\n    }\n\n    private static class IndexFetcher implements Callable<Long> {\n\n        private final ExtractorConfig fetcherConfig;\n        private final ArrayBlockingQueue<FetchEmitTuple> indexPathsList;\n\n        public IndexFetcher(ExtractorConfig fetcherConfig,\n                            ArrayBlockingQueue<FetchEmitTuple> indexPathsList) {\n            this.fetcherConfig = fetcherConfig;\n            this.indexPathsList = indexPathsList;\n        }\n\n        @Override", "        public Long call() throws Exception {\n            Fetcher fetcher = fetcherConfig.newFetcher();\n            StreamEmitter streamEmitter = fetcherConfig.newEmitter();\n            while (true) {\n                FetchEmitTuple t = indexPathsList.poll(120, TimeUnit.MINUTES);\n                if (t == null) {\n                    throw new TimeoutException(\"waited 120 minutes for a new record\");\n                }\n\n                if (t == PipesIterator.COMPLETED_SEMAPHORE) {\n                    indexPathsList.put(PipesIterator.COMPLETED_SEMAPHORE);\n                    LOGGER.info(\"Index fetcher finished\");\n                    return 1l;\n                }\n                fetch(t, fetcher, streamEmitter);\n            }\n        }\n\n        private void fetch(FetchEmitTuple t, Fetcher fetcher, StreamEmitter streamEmitter) {\n\n            LOGGER.info(\"about to download: \" + t.getFetchKey().getFetchKey());\n            try (InputStream is = fetcher.fetch(t.getFetchKey().getFetchKey(), new Metadata())) {\n                streamEmitter.emit(t.getFetchKey().getFetchKey(), is, new Metadata());\n                LOGGER.info(\"successfully downloaded: \" + t.getFetchKey().getFetchKey());\n            } catch (TikaException | IOException e) {\n                LOGGER.error(\"failed to copy \" + t.getFetchKey().getFetchKey(), e);\n            }\n        }\n    }\n}\n", "                if (t == PipesIterator.COMPLETED_SEMAPHORE) {\n                    indexPathsList.put(PipesIterator.COMPLETED_SEMAPHORE);\n                    LOGGER.info(\"Index fetcher finished\");\n                    return 1l;\n                }\n                fetch(t, fetcher, streamEmitter);\n            }\n        }\n\n        private void fetch(FetchEmitTuple t, Fetcher fetcher, StreamEmitter streamEmitter) {\n\n            LOGGER.info(\"about to download: \" + t.getFetchKey().getFetchKey());\n            try (InputStream is = fetcher.fetch(t.getFetchKey().getFetchKey(), new Metadata())) {\n                streamEmitter.emit(t.getFetchKey().getFetchKey(), is, new Metadata());\n                LOGGER.info(\"successfully downloaded: \" + t.getFetchKey().getFetchKey());\n            } catch (TikaException | IOException e) {\n                LOGGER.error(\"failed to copy \" + t.getFetchKey().getFetchKey(), e);\n            }\n        }\n    }\n}\n"]}
