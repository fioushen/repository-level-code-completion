{"filename": "src/test/java/io/strimzi/kafka/bridge/mqtt/MqttBridgetIT.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt;\n\nimport io.netty.channel.ChannelOption;\nimport io.netty.channel.EventLoopGroup;\nimport io.netty.channel.nio.NioEventLoopGroup;\nimport io.netty.handler.codec.mqtt.MqttQoS;", "import io.netty.channel.nio.NioEventLoopGroup;\nimport io.netty.handler.codec.mqtt.MqttQoS;\nimport io.strimzi.kafka.bridge.mqtt.config.BridgeConfig;\nimport io.strimzi.kafka.bridge.mqtt.config.KafkaConfig;\nimport io.strimzi.kafka.bridge.mqtt.config.MqttConfig;\nimport io.strimzi.kafka.bridge.mqtt.core.MqttServer;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MappingRulesLoader;\nimport io.strimzi.test.container.StrimziKafkaContainer;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;", "import org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.OffsetResetStrategy;\n\nimport org.apache.kafka.common.serialization.StringDeserializer;\nimport org.eclipse.paho.client.mqttv3.IMqttToken;\nimport org.eclipse.paho.client.mqttv3.MqttClient;\nimport org.eclipse.paho.client.mqttv3.MqttConnectOptions;", "import org.eclipse.paho.client.mqttv3.MqttClient;\nimport org.eclipse.paho.client.mqttv3.MqttConnectOptions;\nimport org.eclipse.paho.client.mqttv3.MqttException;\nimport org.eclipse.paho.client.mqttv3.MqttAsyncClient;\n\nimport org.eclipse.paho.client.mqttv3.internal.wire.MqttWireMessage;\nimport org.junit.jupiter.api.AfterAll;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n", "import org.junit.jupiter.api.Test;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport org.testcontainers.shaded.com.google.common.collect.ImmutableMap;\n\nimport java.time.Duration;\nimport java.util.Collections;\nimport java.util.Locale;", "import java.util.Collections;\nimport java.util.Locale;\nimport java.util.Objects;\nimport java.util.Random;\nimport java.util.UUID;\nimport java.util.List;\nimport java.util.ArrayList;\n\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.MatcherAssert.assertThat;", "import static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.MatcherAssert.assertThat;\n\n/**\n * Integration test for MQTT bridge\n */\npublic class MqttBridgetIT {\n    private static final Logger logger = LoggerFactory.getLogger(MqttBridgetIT.class);\n    private static final String MQTT_SERVER_HOST = \"localhost\";\n    private static final int MQTT_SERVER_PORT = 1883;\n    private static final String MQTT_SERVER_URI = \"tcp://\" + MQTT_SERVER_HOST + \":\" + MQTT_SERVER_PORT;\n    private static final String KAFKA_TOPIC = \"devices_bluetooth_data\";\n    private static KafkaConsumer<String, String> kafkaConsumerClient = null;\n    private static MqttServer mqttBridge;\n    private static StrimziKafkaContainer kafkaContainer;\n\n\n    /**\n     * Start the kafka Cluster\n     * Start the MQTT bridge before all tests\n     */\n    @BeforeAll", "    public static void beforeAll() throws InterruptedException {\n        String kafkaBootstrapServers = null;\n        try {\n            kafkaContainer = new StrimziKafkaContainer();\n            kafkaContainer.waitForRunning();\n            kafkaContainer.start();\n            kafkaBootstrapServers = kafkaContainer.getBootstrapServers();\n        } catch (Exception e) {\n            logger.error(\"Exception occurred\", e);\n            throw e;\n        }\n\n        // instantiate the kafka consumer client\n        kafkaConsumerClient = new KafkaConsumer<>(\n                ImmutableMap.of(\n                        ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBootstrapServers,\n                        ConsumerConfig.GROUP_ID_CONFIG, \"gid-\" + UUID.randomUUID(),\n                        ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, OffsetResetStrategy.EARLIEST.name().toLowerCase(Locale.ROOT)\n                ),\n                new StringDeserializer(),\n                new StringDeserializer()\n        );\n\n        // consumer client subscribes to the kafka topic\n        kafkaConsumerClient.subscribe(Collections.singletonList(KAFKA_TOPIC));\n\n        // prepare the configuration for the bridge\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(\n                ImmutableMap.of(\n                        BridgeConfig.BRIDGE_ID, \"my-bridge\",\n                        KafkaConfig.KAFKA_CONFIG_PREFIX + ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, kafkaBootstrapServers,\n                        MqttConfig.MQTT_HOST, MQTT_SERVER_HOST,\n                        MqttConfig.MQTT_PORT, MQTT_SERVER_PORT\n                ));\n\n        // prepare the mapping rules\n        String mappingRulesPath = Objects.requireNonNull(MqttBridgetIT.class.getClassLoader().getResource(\"mapping-rules-regex.json\")).getPath();\n        MappingRulesLoader.getInstance().init(mappingRulesPath);\n\n        // start the MQTT bridge\n        EventLoopGroup bossGroup = new NioEventLoopGroup();\n        EventLoopGroup workerGroup = new NioEventLoopGroup();\n\n        mqttBridge = new MqttServer(bridgeConfig, bossGroup, workerGroup, ChannelOption.SO_KEEPALIVE);\n        mqttBridge.start();\n    }\n\n    /**\n     * Test the client's connection to the Bridge\n     */\n    @Test", "    public void testConnection() throws MqttException {\n        try (MqttClient client = new MqttClient(MQTT_SERVER_URI, getRandomMqttClientId(), null)) {\n            // Set up options for the connection\n            MqttConnectOptions options = new MqttConnectOptions();\n\n            IMqttToken conn = client.connectWithResult(options);\n\n            assertThat(\"The session present flag should be false\",\n                    conn.getSessionPresent(), is(false));\n\n            assertThat(\"The connection's response message type code should be MESSAGE_TYPE_CONNACK\",\n                    conn.getResponse().getType(), is(MqttWireMessage.MESSAGE_TYPE_CONNACK));\n\n            assertThat(\"The client should be connected\",\n                    client.isConnected(), is(true));\n\n            // Disconnect the client\n            client.disconnect();\n        }\n    }\n\n    /**\n     * Test concurrent connections to the Bridge\n     */\n    @Test", "    public void testConcurrentConnections() throws MqttException {\n        List<MqttClient> clients = new ArrayList<>();\n\n        // Create 10 clients\n        for (int i = 0; i < 10; i++) {\n            clients.add(new MqttClient(MQTT_SERVER_URI, getRandomMqttClientId(), null));\n        }\n        MqttConnectOptions options = new MqttConnectOptions();\n\n        // Connect all clients and test the connection\n        clients.stream().unordered().parallel().forEach(client -> {", "            try {\n                IMqttToken conn = client.connectWithResult(options);\n                assertThat(\"The session present flag should be false\",\n                        conn.getSessionPresent(), is(false));\n                assertThat(\"The connection's response message type code should be MESSAGE_TYPE_CONNACK\",\n                        conn.getResponse().getType(), is(MqttWireMessage.MESSAGE_TYPE_CONNACK));\n                assertThat(\"The client should be connected\",\n                        client.isConnected(), is(true));\n            } catch (MqttException e) {\n                logger.error(\"Exception occurred\", e);\n            } finally {", "                try {\n                    client.disconnect();\n                } catch (MqttException e) {\n                    logger.error(\"Exception occurred\", e);\n                }\n            }\n        });\n    }\n\n    /**\n     * Test the client publishing a message to the bridge, and the bridge maps and produce the message to the kafka topic\n     * The kafka consumer client consumes the message from the kafka topic.\n     */\n    @Test", "    public void testPublishAndConsumer() throws MqttException {\n        String mqttTopic = \"devices/bluetooth/type/audio/data\";\n        String kafkaKey = \"devices_audio\";\n\n        try (MqttAsyncClient client = new MqttAsyncClient(MQTT_SERVER_URI, getRandomMqttClientId(), null)) {\n            MqttConnectOptions options = new MqttConnectOptions();\n\n            client.connect(options).waitForCompletion();\n\n            client.publish(mqttTopic, \"Hello world\".getBytes(), MqttQoS.AT_MOST_ONCE.value(), false).waitForCompletion();\n\n            ConsumerRecords<String, String> records = kafkaConsumerClient.poll(Duration.ofSeconds(20));\n\n            assertThat(\"The record should be present\", records.count(), is(1));\n\n            ConsumerRecord<String, String> record = records.iterator().next();\n\n            assertThat(\"The key should be \" + kafkaKey,\n                    record.key(), is(kafkaKey));\n            assertThat(\"The value should be Hello world\",\n                    record.value(), is(\"Hello world\"));\n            assertThat(\"The topic should be \" + KAFKA_TOPIC,\n                    record.topic(), is(KAFKA_TOPIC));\n            assertThat(\"The record headers should contain the MQTT topic \" + mqttTopic,\n                    record.headers().lastHeader(\"mqtt-topic\").value(), is(mqttTopic.getBytes()));\n\n            // Disconnect the client\n            client.disconnect();\n        }\n    }\n\n    /**\n     * Close the kafka consumer client, stop the Bridge and stop the kafka cluster\n     */\n    @AfterAll", "    public static void afterAll() throws InterruptedException {\n        kafkaConsumerClient.close();\n        mqttBridge.stop();\n        kafkaContainer.stop();\n    }\n\n    /**\n     * Randomly generate a new client id before each test\n     */\n    private String getRandomMqttClientId() {\n        return \"mqtt-client-\" + new Random().nextInt(20);\n    }\n}\n"]}
{"filename": "src/test/java/io/strimzi/kafka/bridge/mqtt/config/ConfigRetrieverTest.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport org.junit.jupiter.api.Test;\n\nimport java.io.FileNotFoundException;\nimport java.io.IOException;", "import java.io.FileNotFoundException;\nimport java.io.IOException;\nimport java.util.Map;\nimport java.util.Objects;\n\nimport static org.hamcrest.MatcherAssert.assertThat;\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.junit.jupiter.api.Assertions.assertThrows;\n\n/**", "\n/**\n * Unit tests for {@link ConfigRetriever}.\n */\npublic class ConfigRetrieverTest {\n\n    /**\n     * Test if the application.properties file has all configuration parameters.\n     */\n    @Test\n    public void testApplicationPropertiesFile() throws IOException {\n        String filePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"application.properties\")).getPath();\n        Map<String, Object> config = ConfigRetriever.getConfig(filePath);\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(config);\n\n        // bridge config related tests\n        assertThat(\"Bridge-ID should be 'my-bridge'\",\n                bridgeConfig.getBridgeID(), is(\"my-bridge\"));\n\n        // Mqtt server config related tests\n        assertThat(\"There should be 2 related mqtt server config parameters\",\n                bridgeConfig.getMqttConfig().getConfig().size(), is(2));\n        assertThat(\"Mqtt server host should be 'localhost'\",\n                bridgeConfig.getMqttConfig().getHost(), is(\"localhost\"));\n        assertThat(\"Mqtt server port should be '1883'\",\n                bridgeConfig.getMqttConfig().getPort(), is(1883));\n\n        // Kafka server config related tests\n        assertThat(\"There should be 1 related kafka config parameters\",\n                bridgeConfig.getKafkaConfig().getConfig().size(), is(1));\n        assertThat(\"The address of the kafka bootstrap server should be 'localhost:9092'\",\n                bridgeConfig.getKafkaConfig().getConfig().get(\"bootstrap.servers\"), is(\"localhost:9092\"));\n    }\n\n    /**\n     * Test if the application.properties not exists.\n     */\n    @Test", "    public void testApplicationPropertiesFile() throws IOException {\n        String filePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"application.properties\")).getPath();\n        Map<String, Object> config = ConfigRetriever.getConfig(filePath);\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(config);\n\n        // bridge config related tests\n        assertThat(\"Bridge-ID should be 'my-bridge'\",\n                bridgeConfig.getBridgeID(), is(\"my-bridge\"));\n\n        // Mqtt server config related tests\n        assertThat(\"There should be 2 related mqtt server config parameters\",\n                bridgeConfig.getMqttConfig().getConfig().size(), is(2));\n        assertThat(\"Mqtt server host should be 'localhost'\",\n                bridgeConfig.getMqttConfig().getHost(), is(\"localhost\"));\n        assertThat(\"Mqtt server port should be '1883'\",\n                bridgeConfig.getMqttConfig().getPort(), is(1883));\n\n        // Kafka server config related tests\n        assertThat(\"There should be 1 related kafka config parameters\",\n                bridgeConfig.getKafkaConfig().getConfig().size(), is(1));\n        assertThat(\"The address of the kafka bootstrap server should be 'localhost:9092'\",\n                bridgeConfig.getKafkaConfig().getConfig().get(\"bootstrap.servers\"), is(\"localhost:9092\"));\n    }\n\n    /**\n     * Test if the application.properties not exists.\n     */\n    @Test", "    public void testApplicationPropertiesFileNotExists()  {\n        Exception fileNotFoundException = assertThrows(FileNotFoundException.class, ()-> ConfigRetriever.getConfig(\"not-exists-application.properties\"));\n\n        assertThat(\"File not found exception should be thrown\",\n                fileNotFoundException.getMessage(), is(\"not-exists-application.properties (No such file or directory)\"));\n    }\n\n    /**\n     * Test environment variables override.\n     */\n    @Test", "    public void testEnvironmentEnvOverride() throws IOException {\n        String filePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"application.properties\")).getPath();\n        Map<String, String> envs = new java.util.HashMap<>(Map.of());\n        // add all environment variables\n        envs.putAll(System.getenv());\n\n        // add new environment variable for bridge-id\n        envs.put(BridgeConfig.BRIDGE_ID, \"my-bridge-env\");\n\n        Map<String, Object> config = ConfigRetriever.getConfig(filePath, envs);\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(config);\n\n        assertThat(\"Bridge-ID should be 'my-bridge-env'\",\n                bridgeConfig.getBridgeID(), is(\"my-bridge-env\"));\n    }\n}\n"]}
{"filename": "src/test/java/io/strimzi/kafka/bridge/mqtt/config/ConfigTest.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\n\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport org.apache.kafka.clients.CommonClientConfigs;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.producer.ProducerConfig;", "import org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.junit.jupiter.api.Test;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\nimport static org.hamcrest.MatcherAssert.assertThat;\nimport static org.hamcrest.Matchers.is;\n", "import static org.hamcrest.Matchers.is;\n\n/**\n * Some config related classes unit tests\n */\npublic class ConfigTest {\n\n    @Test\n    public void testConfig() {\n        Map<String, Object> map = new HashMap<>();\n        map.put(\"bridge.id\", \"my-bridge\");\n        map.put(\"kafka.bootstrap.servers\", \"localhost:9092\");\n        map.put(\"kafka.producer.acks\", \"1\");\n        map.put(\"mqtt.host\", \"0.0.0.0\");\n        map.put(\"mqtt.port\", \"1883\");\n\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(map);\n        assertThat(bridgeConfig.getBridgeID(), is(\"my-bridge\"));\n\n        assertThat(bridgeConfig.getKafkaConfig().getConfig().size(), is(1));\n        assertThat(bridgeConfig.getKafkaConfig().getConfig().get(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG), is(\"localhost:9092\"));\n\n        assertThat(bridgeConfig.getKafkaConfig().getProducerConfig().getConfig().size(), is(1));\n        assertThat(bridgeConfig.getKafkaConfig().getProducerConfig().getConfig().get(ProducerConfig.ACKS_CONFIG), is(\"1\"));\n\n        assertThat(bridgeConfig.getMqttConfig().getConfig().size(), is(2));\n        assertThat(bridgeConfig.getMqttConfig().getHost(), is(\"0.0.0.0\"));\n        assertThat(bridgeConfig.getMqttConfig().getPort(), is(1883));\n    }\n\n    @Test", "    public void testConfig() {\n        Map<String, Object> map = new HashMap<>();\n        map.put(\"bridge.id\", \"my-bridge\");\n        map.put(\"kafka.bootstrap.servers\", \"localhost:9092\");\n        map.put(\"kafka.producer.acks\", \"1\");\n        map.put(\"mqtt.host\", \"0.0.0.0\");\n        map.put(\"mqtt.port\", \"1883\");\n\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(map);\n        assertThat(bridgeConfig.getBridgeID(), is(\"my-bridge\"));\n\n        assertThat(bridgeConfig.getKafkaConfig().getConfig().size(), is(1));\n        assertThat(bridgeConfig.getKafkaConfig().getConfig().get(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG), is(\"localhost:9092\"));\n\n        assertThat(bridgeConfig.getKafkaConfig().getProducerConfig().getConfig().size(), is(1));\n        assertThat(bridgeConfig.getKafkaConfig().getProducerConfig().getConfig().get(ProducerConfig.ACKS_CONFIG), is(\"1\"));\n\n        assertThat(bridgeConfig.getMqttConfig().getConfig().size(), is(2));\n        assertThat(bridgeConfig.getMqttConfig().getHost(), is(\"0.0.0.0\"));\n        assertThat(bridgeConfig.getMqttConfig().getPort(), is(1883));\n    }\n\n    @Test", "    public void testHidingPassword() {\n        String storePassword = \"logged-config-should-not-contain-this-password\";\n        Map<String, Object> map = new HashMap<>();\n        map.put(\"kafka.ssl.truststore.location\", \"/tmp/strimzi/bridge.truststore.p12\");\n        map.put(\"kafka.ssl.truststore.password\", storePassword);\n        map.put(\"kafka.ssl.truststore.type\", \"PKCS12\");\n        map.put(\"kafka.ssl.keystore.location\", \"/tmp/strimzi/bridge.keystore.p12\");\n        map.put(\"kafka.ssl.keystore.password\", storePassword);\n        map.put(\"kafka.ssl.keystore.type\", \"PKCS12\");\n\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(map);\n        assertThat(bridgeConfig.getKafkaConfig().getConfig().size(), is(6));\n\n        assertThat(bridgeConfig.getKafkaConfig().toString().contains(\"ssl.truststore.password=\" + storePassword), is(false));\n        assertThat(bridgeConfig.getKafkaConfig().toString().contains(\"ssl.truststore.password=[hidden]\"), is(true));\n    }\n\n    @Test", "    public void testMqttDefaults() {\n        BridgeConfig bridgeConfig = BridgeConfig.fromMap(Map.of());\n\n        assertThat(bridgeConfig.getMqttConfig().getHost(), is(\"0.0.0.0\"));\n        assertThat(bridgeConfig.getMqttConfig().getPort(), is(1883));\n    }\n}\n\n"]}
{"filename": "src/test/java/io/strimzi/kafka/bridge/mqtt/kafka/KafkaBridgeProducerTest.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.kafka;\n\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.TopicPartition;\nimport org.junit.jupiter.api.Test;", "import org.apache.kafka.common.TopicPartition;\nimport org.junit.jupiter.api.Test;\n\nimport java.util.concurrent.CompletableFuture;\n\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.MatcherAssert.assertThat;\n\nimport static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.Mockito.mock;", "import static org.mockito.ArgumentMatchers.any;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\n\n/**\n * Unit tests for {@link KafkaBridgeProducer}\n */\npublic class KafkaBridgeProducerTest {\n\n    /**\n     * Test the {@link KafkaBridgeProducer#send(ProducerRecord)}} method\n     */\n    @Test", "    public void testSend() {\n        // mock the producer\n        KafkaBridgeProducer producer = mock(KafkaBridgeProducer.class);\n\n        String kafkaTopic = \"test-topic\";\n        ProducerRecord<String, byte[]> record = new ProducerRecord<>(kafkaTopic, \"test\".getBytes());\n\n        // simulate the send method with ack\n        when(producer.send(any(ProducerRecord.class)))\n                .thenAnswer(invocation -> {\n                    ProducerRecord<String, byte[]> r = invocation.getArgument(0);\n\n                    assertThat(\"Topic is correct\",\n                            r.topic(), is(kafkaTopic));\n\n                    assertThat(\"Value is correct\",\n                            r.value(), is(\"test\".getBytes()));\n\n                    CompletableFuture<RecordMetadata> promise = new CompletableFuture<>();\n\n                    promise.complete(new RecordMetadata(new TopicPartition(r.topic(), 2), 234L, 0, 1000, 0L, 0, \"test\".getBytes().length));\n                    return promise;\n                });\n\n        // send the record and wait for ack\n        producer.send(record).thenAccept(metadata -> {\n\n            // should have the correct partition\n            assertThat(\"Partition is correct\",\n                    metadata.partition(), is(2));\n\n            // should have the correct offset\n            assertThat(\"Offset is correct\",\n                    metadata.offset(), is(234L));\n\n            // should have the correct timestamp\n            assertThat(\"Timestamp is correct\",\n                    metadata.timestamp(), is(1000L));\n\n            // should have the correct serialized key size\n            assertThat(\"Serialized key size is correct\",\n                    metadata.serializedKeySize(), is(0));\n\n            // should have the correct serialized value size\n            assertThat(\"Serialized value size is correct\",\n                    metadata.serializedValueSize(), is(\"test\".getBytes().length));\n        });\n    }\n}\n"]}
{"filename": "src/test/java/io/strimzi/kafka/bridge/mqtt/mapper/MappingRulesLoaderTest.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\nimport org.junit.jupiter.api.Test;\n\nimport java.nio.file.Path;", "\nimport java.nio.file.Path;\nimport java.util.List;\nimport java.util.Objects;\n\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.CoreMatchers.notNullValue;\nimport static org.hamcrest.MatcherAssert.assertThat;\n\nimport static org.junit.jupiter.api.Assertions.assertThrows;", "\nimport static org.junit.jupiter.api.Assertions.assertThrows;\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\nimport static org.mockito.Mockito.times;\nimport static org.mockito.Mockito.verify;\nimport static org.mockito.Mockito.atMostOnce;\nimport static org.mockito.Mockito.doNothing;\n\n/**", "\n/**\n * Unit tests for {@link MappingRulesLoader}\n */\npublic class MappingRulesLoaderTest {\n\n    /**\n     * Test for loading mapping rules from a file.\n     */\n    @Test\n    public void testLoadRules() throws Exception {\n        String filePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"mapping-rules-regex.json\")).getPath();\n        MappingRulesLoader loader = mock(MappingRulesLoader.class);\n\n        // 1st loadRules call throws exception. 2nd loadRules call returns the rules\n        when(loader.loadRules())\n                .thenThrow(new IllegalStateException(\"MappingRulesLoader is not initialized\"))\n                .thenAnswer(invocationOnMock -> {\n                    ObjectMapper mapper = new ObjectMapper();\n                    // deserialize the JSON array to a list of MappingRule objects\n                    return mapper.readValue(Path.of(filePath).toFile(), mapper.getTypeFactory().constructCollectionType(List.class, MappingRule.class));\n                });\n\n        // the mapping rules loader is not initialized\n        Exception exception = assertThrows(IllegalStateException.class, loader::loadRules);\n        String expectedMessage = \"MappingRulesLoader is not initialized\";\n\n        assertThat(\"Should throw an exception\",\n                exception, notNullValue());\n        assertThat(\"Should throw an exception with the correct message\",\n                exception.getMessage(), is(expectedMessage));\n\n        // init the mapping rules loader\n        loader.init(filePath);\n        List<MappingRule> rules = loader.loadRules();\n\n        assertThat(\"Should load 7 mapping rules\",\n                rules.size(), is(7));\n        assertThat(\"Should not have null values\",\n                rules.stream().anyMatch(rule -> rule.getMqttTopicPattern() == null || rule.getKafkaTopicTemplate() == null), is(false));\n\n        verify(loader, atMostOnce()).init(filePath);\n    }\n\n    /**\n     * Test for initializing mapping rules loader more than once.\n     */\n    @Test", "    public void testLoadRules() throws Exception {\n        String filePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"mapping-rules-regex.json\")).getPath();\n        MappingRulesLoader loader = mock(MappingRulesLoader.class);\n\n        // 1st loadRules call throws exception. 2nd loadRules call returns the rules\n        when(loader.loadRules())\n                .thenThrow(new IllegalStateException(\"MappingRulesLoader is not initialized\"))\n                .thenAnswer(invocationOnMock -> {\n                    ObjectMapper mapper = new ObjectMapper();\n                    // deserialize the JSON array to a list of MappingRule objects\n                    return mapper.readValue(Path.of(filePath).toFile(), mapper.getTypeFactory().constructCollectionType(List.class, MappingRule.class));\n                });\n\n        // the mapping rules loader is not initialized\n        Exception exception = assertThrows(IllegalStateException.class, loader::loadRules);\n        String expectedMessage = \"MappingRulesLoader is not initialized\";\n\n        assertThat(\"Should throw an exception\",\n                exception, notNullValue());\n        assertThat(\"Should throw an exception with the correct message\",\n                exception.getMessage(), is(expectedMessage));\n\n        // init the mapping rules loader\n        loader.init(filePath);\n        List<MappingRule> rules = loader.loadRules();\n\n        assertThat(\"Should load 7 mapping rules\",\n                rules.size(), is(7));\n        assertThat(\"Should not have null values\",\n                rules.stream().anyMatch(rule -> rule.getMqttTopicPattern() == null || rule.getKafkaTopicTemplate() == null), is(false));\n\n        verify(loader, atMostOnce()).init(filePath);\n    }\n\n    /**\n     * Test for initializing mapping rules loader more than once.\n     */\n    @Test", "    public void testInitMoreThanOnce() {\n        String filePath = Objects.requireNonNull(getClass().getClassLoader().getResource(\"mapping-rules-regex.json\")).getPath();\n        MappingRulesLoader loader = mock(MappingRulesLoader.class);\n\n        // 1st init call, do Nothing. 2nd init call throw exception\n        doNothing().doThrow(new IllegalStateException(\"MappingRulesLoader is already initialized\")).when(loader).init(filePath);\n\n        // first init\n        loader.init(filePath);\n\n        // prepare exception, try to init again\n        Exception exception =  assertThrows(IllegalStateException.class,  () -> loader.init(filePath));\n        String expectedMessage = \"MappingRulesLoader is already initialized\";\n\n        assertThat(\"Should throw an exception\",\n                exception, notNullValue());\n        assertThat(\"Should throw an illegal state exception\",\n                exception.getMessage(), is(expectedMessage));\n\n        verify(loader, times(2)).init(filePath);\n    }\n}\n"]}
{"filename": "src/test/java/io/strimzi/kafka/bridge/mqtt/mapper/MqttKafkaSimpleMapperTest.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\nimport org.junit.jupiter.api.Test;\n\nimport java.util.ArrayList;\nimport java.util.List;", "import java.util.ArrayList;\nimport java.util.List;\n\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.CoreMatchers.not;\nimport static org.hamcrest.MatcherAssert.assertThat;\nimport static org.hamcrest.CoreMatchers.nullValue;\nimport static org.junit.jupiter.api.Assertions.assertThrows;\n\n/**", "\n/**\n * Unit tests for @{@link MqttKafkaSimpleMapper}.\n *\n * @see MqttKafkaSimpleMapper\n */\npublic class MqttKafkaSimpleMapperTest {\n\n    /**\n     * Test for default topic.\n     * If the MQTT topic does not match any of the mapping rules, the default topic is used.\n     * E.g. if the Topic Mapping Rules is empty, the default topic is used by default.\n     */\n    @Test", "    public void testDefaultTopic() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        MqttKafkaSimpleMapper mapper = new MqttKafkaSimpleMapper(rules);\n\n        MappingResult result = mapper.map(\"sensor/temperature\");\n        assertThat(\"Should use the default topic when no mapping pattern matches.\",\n                result.kafkaTopic(), is(MqttKafkaSimpleMapper.DEFAULT_KAFKA_TOPIC));\n\n        assertThat(\"The key for the default topic should be null\",\n                result.kafkaKey(), nullValue());\n    }\n\n    /**\n     * Test the mapping of single level topics.\n     */\n    @Test", "    public void testSingleLevel() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        rules.add(new MappingRule(\"sensors/+/data\", \"sensor_data\", null));\n        rules.add(new MappingRule(\"devices/{device}/data\", \"devices_{device}_data\", null));\n        rules.add(new MappingRule(\"fleet/{fleet}/vehicle/{vehicle}\", \"fleet_{fleet}\", \"vehicle_{vehicle}\"));\n        rules.add(new MappingRule(\"building/{building}/floor/{floor}\", \"building.{building}\", \"floor_{floor}\"));\n        rules.add(new MappingRule(\"term/{number}\", \"term{number}\", null));\n\n        MqttKafkaSimpleMapper mapper = new MqttKafkaSimpleMapper(rules);\n\n        // Test sensors/+/data\n        MappingResult mappingResult = mapper.map(\"sensors/8/data\");\n\n        assertThat(\"Mqtt pattern sensors/+/data should be mapped to sensor_data\",\n                mappingResult.kafkaTopic(), is(\"sensor_data\"));\n\n        assertThat(\"The key for sensors/+/data should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test devices/{device}/data\n        mappingResult = mapper.map(\"devices/4/data\");\n\n        assertThat(\"Mqtt pattern devices/{device}/data should be mapped to devices_{device}_data\",\n                mappingResult.kafkaTopic(), is(\"devices_4_data\"));\n\n        assertThat(\"The key for devices/{device}/data should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test fleet/{fleet}/vehicle/{vehicle}\n        mappingResult = mapper.map(\"fleet/4/vehicle/23\");\n\n        assertThat(\"Mqtt pattern fleet/{fleet}/vehicle/{vehicle} should be mapped to fleet_{fleet}\",\n                mappingResult.kafkaTopic(), is(\"fleet_4\"));\n\n        assertThat(\"The key for fleet/{fleet}/vehicle/{vehicle} should be vehicle_{vehicle}\",\n                mappingResult.kafkaKey(), is(\"vehicle_23\"));\n\n        // Test building/{building}/floor/{floor}\n        mappingResult = mapper.map(\"building/40/floor/13\");\n\n        assertThat(\"building/{building}/floor/{floor} should be mapped to building.{building}\",\n                mappingResult.kafkaTopic(), is(\"building.40\"));\n\n        assertThat(\"The key for building/{building}/floor/{floor} should be floor_{floor}\",\n                mappingResult.kafkaKey(), is(\"floor_13\"));\n\n        // Test term/{number}\n        mappingResult = mapper.map(\"term/4\");\n\n        assertThat(\"Mqtt pattern term/{number} should be mapped to term{number}\",\n                mappingResult.kafkaTopic(), is(\"term4\"));\n\n        assertThat(\"The key for term/{number} should be null\",\n                mappingResult.kafkaKey(), nullValue());\n    }\n\n\n    /**\n     * Test the mapping of single level topics.\n     */\n    @Test", "    public void testIllegalPlaceholder() {\n\n        List<MappingRule> rules = new ArrayList<>();\n        rules.add(new MappingRule(\"fleet/{flee}/vehicle/{vehicle}\", \"fleet_{fleet}\", null));\n        rules.add(new MappingRule(\"buildings/+/rooms/+/device/+\", \"buildings_{building}_rooms_{room}_device_{device}\", \"building\"));\n        rules.add(new MappingRule(\"building/{building}/room/{room}\", \"building_{building}_room_{room}_{noexistingplaceholder}\", null));\n\n        MqttKafkaSimpleMapper mapper = new MqttKafkaSimpleMapper(rules);\n\n        Exception exception = assertThrows(IllegalArgumentException.class, () -> mapper.map(\"fleet/4/vehicle/23\"));\n\n        String expectedMessage = \"The placeholder {fleet} was not found assigned any value.\";\n        assertThat(\"The exception message should be: \" + expectedMessage,\n                exception.getMessage(), is(expectedMessage));\n\n        Exception otherException = assertThrows(IllegalArgumentException.class, () -> mapper.map(\"buildings/10/rooms/5/device/3\"));\n\n        String otherExpectedMessage = \"The placeholder {device} was not found assigned any value.\";\n        assertThat(\"The exception message should be: \" + otherExpectedMessage,\n                otherException.getMessage(), is(otherExpectedMessage));\n\n    }\n\n    /**\n     * Test the mapping of multi level topics.\n     */\n    @Test", "    public void testMultiLevel() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        rules.add(new MappingRule(\"building/{building}/room/{room}/#\", \"building_{building}\", \"room_{room}\"));\n        rules.add(new MappingRule(\"building/{building}/#\", \"building_{building}_others\", null));\n        rules.add(new MappingRule(\"building/#\", \"building_others\", null));\n        rules.add(new MappingRule(\"fleet/{fleet}/vehicle/{vehicle}/#\", \"fleet_{fleet}\", \"vehicle_{vehicle}\"));\n        rules.add(new MappingRule(\"sensor/#\", \"sensor_data\", null));\n        rules.add(new MappingRule(\"sport/tennis/{player}/#\", \"sports_{player}\", null));\n        rules.add(new MappingRule(\"+/recipes/#\", \"my_recipes\", null));\n        rules.add(new MappingRule(\"{house}/#\", \"{house}\", null));\n\n        MqttKafkaSimpleMapper mapper = new MqttKafkaSimpleMapper(rules);\n\n        // Test fleet/{fleet}/vehicle/{vehicle}/# pattern\n        MappingResult mappingResult = mapper.map(\"fleet/4/vehicle/23/velocity\");\n\n        assertThat(\"Mqtt topic pattern fleet/{fleet}/vehicle/{vehicle}/# should be mapped to fleet_{fleet}\",\n                mappingResult.kafkaTopic(), is(\"fleet_4\"));\n\n        assertThat(\"The key for fleet/{fleet}/vehicle/{vehicle}/# should be vehicle_{vehicle}\",\n                mappingResult.kafkaKey(), is(\"vehicle_23\"));\n\n        // Test building/{building}/room/{room}/# pattern\n        mappingResult = mapper.map(\"building/4/room/23/temperature\");\n\n        assertThat(\"Mqtt pattern building/{building}/room/{room}/# should be mapped to building_{building}_room_{room}\",\n                mappingResult.kafkaTopic(), is(\"building_4\"));\n\n        assertThat(\"The key for building/{building}/room/{room}/# should be room_{room}\",\n                mappingResult.kafkaKey(), is(\"room_23\"));\n\n        // Test building/{building}/# pattern\n        mappingResult = mapper.map(\"building/405/room\");\n\n        assertThat(\"Mqtt pattern building/{building}/# should be mapped to building_{building}_others\",\n                mappingResult.kafkaTopic(), is(\"building_405_others\"));\n\n        assertThat(\"The key for building/{building}/# should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test building/# pattern\n        mappingResult = mapper.map(\"building\");\n        assertThat(\"Mqtt pattern building/# should be mapped to building_others\",\n                mappingResult.kafkaTopic(), is(\"building_others\"));\n\n        assertThat(\"The key for building/# should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        assertThat(\"Mqtt pattern building/# will be mapped to building_101_others because building/{building}/# was defined before building/#\",\n                mapper.map(\"building/101\").kafkaTopic(), not(\"building_others\"));\n\n        // Test sensor/# pattern\n        mappingResult = mapper.map(\"sensor/temperature\");\n\n        assertThat(\"Mqtt pattern sensor/# should be mapped to sensor_data\",\n                mappingResult.kafkaTopic(), is(\"sensor_data\"));\n\n        assertThat(\"The key for sensor/# should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test sport/tennis/{player}/# pattern\n        mappingResult = mapper.map(\"sport/tennis/player1/ranking\");\n\n        assertThat(\"Mqtt pattern sport/tennis/{player}/# should be mapped to sports_{player}\",\n                mappingResult.kafkaTopic(), is(\"sports_player1\"));\n\n        mappingResult = mapper.map(\"sport/tennis/player123/score/wimbledon\");\n        assertThat(\"Mqtt pattern sport/tennis/{player}/# should be mapped to sports_{player}\",\n                mappingResult.kafkaTopic(), is(\"sports_player123\"));\n\n        assertThat(\"The key for sport/tennis/{player}/# should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test +/recipes/# pattern\n        mappingResult = mapper.map(\"italian/recipes/pizza\");\n\n        assertThat(\"Mqtt pattern +/recipes/# should be mapped to my_recipes\",\n                mappingResult.kafkaTopic(), is(\"my_recipes\"));\n\n        assertThat(\"The key for +/recipes/# should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        mappingResult = mapper.map(\"angolan/recipes/calulu/fish\");\n\n        assertThat(\"Mqtt pattern +/recipes/# should be mapped to my_recipes\",\n                mappingResult.kafkaTopic(), is(\"my_recipes\"));\n\n        // Test {house}/# pattern\n        mappingResult = mapper.map(\"my_house/temperature\");\n        assertThat(\"Mqtt pattern {house}/# should be mapped to {house}\",\n                mappingResult.kafkaTopic(), is(\"my_house\"));\n\n        assertThat(\"The key for {house}/# should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        assertThat(\"Mqtt pattern {house}/# should be mapped to {house}\",\n                mapper.map(\"my_house/temperature/room1\").kafkaTopic(), is(\"my_house\"));\n    }\n}\n"]}
{"filename": "src/test/java/io/strimzi/kafka/bridge/mqtt/mapper/MqttKafkaRegexMapperTest.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\n\nimport org.junit.jupiter.api.Test;\n\nimport java.util.ArrayList;", "\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.regex.PatternSyntaxException;\n\nimport static org.hamcrest.CoreMatchers.is;\nimport static org.hamcrest.CoreMatchers.not;\nimport static org.hamcrest.MatcherAssert.assertThat;\nimport static org.hamcrest.CoreMatchers.nullValue;\nimport static org.hamcrest.CoreMatchers.startsWith;", "import static org.hamcrest.CoreMatchers.nullValue;\nimport static org.hamcrest.CoreMatchers.startsWith;\nimport static org.junit.jupiter.api.Assertions.assertThrows;\n\n/**\n * Unit tests for {@link MqttKafkaRegexMapper}\n */\npublic class MqttKafkaRegexMapperTest {\n\n\n    /**\n     * Test for default topic.\n     * If the MQTT topic does not match any of the mapping rules, the default topic is used.\n     * E.g. if the Topic Mapping Rules is empty, the default topic is used by default.\n     */\n    @Test", "    public void testDefaultTopic() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        MqttKafkaRegexMapper mapper = new MqttKafkaRegexMapper(rules);\n\n        MappingResult result = mapper.map(\"sensor/temperature\");\n        assertThat(\"Should use the default topic when no mapping pattern matches.\",\n                result.kafkaTopic(), is(MqttKafkaSimpleMapper.DEFAULT_KAFKA_TOPIC));\n\n        assertThat(\"The key for the default topic should be null\",\n                result.kafkaKey(), nullValue());\n    }\n\n    /**\n     * Test the mapping of single level topics.\n     */\n    @Test", "    public void testSingleLevel() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        rules.add(new MappingRule(\"sensors/[^/]+/data\", \"sensor_data\", null));\n        rules.add(new MappingRule(\"devices/([^/]+)/data\", \"devices_$1_data\", null));\n        rules.add(new MappingRule(\"fleet/([0-9]+)/vehicle/(\\\\w+)\", \"fleet_$1\", \"vehicle$2\"));\n        rules.add(new MappingRule(\"building/(\\\\d+)/floor/(\\\\d+)\", \"building_$1\", \"floor_$2\"));\n        rules.add(new MappingRule(\"term/(\\\\d+)\", \"term$1\", null));\n        rules.add(new MappingRule(\"devices/([^/]+)/data/(\\\\b(all|new|old)\\\\b)\", \"devices_$1_data\", \"devices_$2\"));\n\n        MqttKafkaRegexMapper mapper = new MqttKafkaRegexMapper(rules);\n\n        // Test building/(\\\\d+)/floor/(\\\\d+)\n        MappingResult mappingResult = mapper.map(\"building/14/floor/25\");\n\n        assertThat(\"building/(\\\\d+)/floor/(\\\\d+) should be mapped to building_$1\",\n                mappingResult.kafkaTopic(), is(\"building_14\"));\n\n        assertThat(\"The key for floor_$2 should be expanded to floor_25\",\n                mappingResult.kafkaKey(), is(\"floor_25\"));\n\n        // Test sensors/[^/]+/data\n        mappingResult = mapper.map(\"sensors/temperature/data\");\n\n        assertThat(\"sensors/[^/]+/data should be mapped to sensor_data\",\n                mappingResult.kafkaTopic(), is(\"sensor_data\"));\n\n        assertThat(\"The key for the rule sensors/[^/]+/data should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test devices/([^/]+)/data\n        mappingResult = mapper.map(\"devices/123/data\");\n\n        assertThat(\"devices/[^/]+/data should be mapped to devices_$1_data\",\n                mappingResult.kafkaTopic(), is(\"devices_123_data\"));\n\n        assertThat(\"The key for devices/[^/]+/data should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test fleet/([0-9]+)/vehicle/(\\\\w+)\n        mappingResult = mapper.map(\"fleet/4/vehicle/23\");\n\n        assertThat(\"fleet/([0-9]+)/vehicle/(\\\\w+) should be mapped to fleet_$1\",\n                mappingResult.kafkaTopic(), is(\"fleet_4\"));\n\n        assertThat(\"The key for fleet/([0-9]+)/vehicle/(\\\\w+) should be vehicle$2\",\n                mappingResult.kafkaKey(), is(\"vehicle23\"));\n\n        // Test term/(\\\\d+)\n        mappingResult = mapper.map(\"term/4\");\n\n        assertThat(\"term/(\\\\d+) should be mapped to term$1\",\n                mappingResult.kafkaTopic(), is(\"term4\"));\n\n        assertThat(\"The key for term/(\\\\d+) should be null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test devices/([^/]+)/data/(\\\\b(all|new|old)\\\\b)\n        mappingResult = mapper.map(\"devices/bluetooth/data/all\");\n\n        assertThat(\"devices/([^/]+)/data/(\\\\b(all|new|old)\\\\b) should be mapped to devices_$1_data\",\n                mappingResult.kafkaTopic(), is(\"devices_bluetooth_data\"));\n\n        assertThat(\"The key for devices/([^/]+)/data/(\\\\b(all|new|old)\\\\b) should be devices_$2\",\n                mappingResult.kafkaKey(), is(\"devices_all\"));\n    }\n\n\n    /**\n     * Test the mapping of single level topics.\n     */\n    @Test", "    public void testIllegalPlaceholder() {\n\n        List<MappingRule> rules = new ArrayList<>();\n        rules.add(new MappingRule(\"fleet/vehicle/(\\\\d+)\", \"fleet_$1\", \"fleet_$2\"));\n        rules.add(new MappingRule(\"buildings/([^/]+)/rooms/([^/]+)/device/([^/]+)\", \"buildings_$0_rooms_$1_device_$2\", \"device_$3\"));\n        rules.add(new MappingRule(\"building/(\\\\d{1,2})/room/(\\\\d{1,4})\", \"building_$1_room_$2_$3\", \"room_$4\"));\n\n        MqttKafkaRegexMapper mapper = new MqttKafkaRegexMapper(rules);\n\n        Exception exception = assertThrows(IllegalArgumentException.class, () -> mapper.map(\"fleet/vehicle/23\"));\n\n        String expectedMessage = \"The placeholder $2 was not found or assigned any value.\";\n        assertThat(\"The exception message should be: \" + expectedMessage,\n                exception.getMessage(), is(expectedMessage));\n\n        Exception otherException = assertThrows(IllegalArgumentException.class, () -> mapper.map(\"buildings/10/rooms/5/device/3\"));\n\n        String otherExpectedMessage = \"The placeholder $0 was not found or assigned any value.\";\n        assertThat(\"The exception message should be: \" + otherExpectedMessage,\n                otherException.getMessage(), is(otherExpectedMessage));\n\n        Exception anotherException = assertThrows(IllegalArgumentException.class, () -> mapper.map(\"building/10/room/403\"));\n\n        String anotherExpectedMessage = \"The placeholder $3 was not found or assigned any value.\";\n        assertThat(\"The exception message should be: \" + anotherExpectedMessage,\n                anotherException.getMessage(), is(anotherExpectedMessage));\n    }\n\n    /**\n     * Test the mapping of multi level topics.\n     */\n    @Test", "    public void testMultiLevel() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        rules.add(new MappingRule(\"building/([^/]+)/room/(\\\\d{1,3}).*\", \"building_$1\", \"room_$2\"));\n        rules.add(new MappingRule(\"building/([^/]+).*\", \"building_$1_others\", null));\n        rules.add(new MappingRule(\"building.*\", \"building_others\", null));\n        rules.add(new MappingRule(\"fleet/([0-9])/vehicle/(\\\\w+)(?:\\\\/.*)?$\", \"fleet_$1\", \"vehicle_$2\"));\n        rules.add(new MappingRule(\"sensor.*\", \"sensor_data\", null));\n        rules.add(new MappingRule(\"sport/tennis/(\\\\w+).*\", \"sports_$1\", null));\n        rules.add(new MappingRule(\"(\\\\w+)/recipes(?:\\\\/.*)?$\", \"$1_recipes\", null));\n        rules.add(new MappingRule(\"([^/]+)/.*\", \"$1\", null));\n\n        MqttKafkaRegexMapper mapper = new MqttKafkaRegexMapper(rules);\n\n\n        // Test for building/([^/]+)/room/(\\\\d{1,3}).* pattern\n        MappingResult mappingResult = mapper.map(\"building/4/room/23/temperature\");\n\n        assertThat(\"Mqtt pattern building/([^/]+)/room/(\\\\d{1,3}).* should be mapped to building_$1\",\n                mappingResult.kafkaTopic(), is(\"building_4\"));\n\n        assertThat(\"The key for building/([^/]+)/room/(\\\\d{1,3}).* should be expanded to room_$2\",\n                mappingResult.kafkaKey(), is(\"room_23\"));\n\n        // Test for building/([^/]+).* pattern\n        mappingResult = mapper.map(\"building/405/room\");\n\n        assertThat(\"Mqtt pattern building/([^/]+).* should be mapped to building_$1_others\",\n                mappingResult.kafkaTopic(), is(\"building_405_others\"));\n\n        assertThat(\"The key for building/([^/]+).* should be expanded to null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test for building.* pattern\n        mappingResult = mapper.map(\"building/101\");\n\n        assertThat(\"Mqtt pattern building.* will be mapped to building_101_others because building/([^/]+).* was defined before building.*\",\n                mappingResult.kafkaTopic(), not(\"building_others\"));\n\n        assertThat(\"Mqtt pattern building.* will be mapped to building_101_others because building/([^/]+).* was defined before building.*\",\n                mappingResult.kafkaTopic(), is(\"building_101_others\"));\n\n        assertThat(\"The key for building.* should be expanded to null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        assertThat(\"building.* should be mapped to building_others\",\n                mapper.map(\"building\").kafkaTopic(), is(\"building_others\"));\n\n        // Test for fleet/([0-9])/vehicle/(\\\\w+)(?:\\/.*)?$ pattern\n        mappingResult = mapper.map(\"fleet/9/vehicle/23/velocity\");\n\n        assertThat(\"Mqtt pattern fleet/([0-9])/vehicle/(\\\\w+)(?:\\\\/.*)?$ should be mapped to fleet_$1\",\n                mappingResult.kafkaTopic(), is(\"fleet_9\"));\n\n        assertThat(\"The key for fleet/([0-9])/vehicle/(\\\\w+)(?:\\\\/.*)?$ should be expanded to fleet$1\",\n                mappingResult.kafkaKey(), is(\"vehicle_23\"));\n\n        // Test for sensor.* pattern\n        mappingResult = mapper.map(\"sensor/temperature/data\");\n\n        assertThat(\"Mqtt pattern sensor.* should be mapped to sensor_data\",\n                mappingResult.kafkaTopic(), is(\"sensor_data\"));\n\n        assertThat(\"The key for sensor.* should be expanded to null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test for sport/tennis/(\\\\w+).* pattern\n        mappingResult = mapper.map(\"sport/tennis/player123/score/wimbledon\");\n\n        assertThat(\"Mqtt pattern sport/tennis/(\\\\w+).* should be mapped to sports_$1\",\n                mappingResult.kafkaTopic(), is(\"sports_player123\"));\n\n        assertThat(\"The key for sport/tennis/(\\\\w+).* should be expanded to null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        assertThat(\"Mqtt pattern sport/tennis/(\\\\w+).* should be mapped to sports_$1\",\n                mapper.map(\"sport/tennis/player1\").kafkaTopic(), is(\"sports_player1\"));\n\n        assertThat(\"Mqtt pattern sport/tennis/(\\\\w+).* should be mapped to sports_$1\",\n                mapper.map(\"sport/tennis/player100/ranking\").kafkaTopic(), is(\"sports_player100\"));\n", "        // Test for ([^/]+)/recipes(?:\\/.*)?$ pattern\n        mappingResult = mapper.map(\"angolan/recipes/caculu/fish\");\n\n        assertThat(\"Mqtt pattern ([^/]+)/recipes(?:\\\\/.*)?$ should be mapped to my_recipes\",\n                mappingResult.kafkaTopic(), is(\"angolan_recipes\"));\n\n        assertThat(\"The key for ([^/]+)/recipes(?:\\\\/.*)?$ should be expanded to null\",\n                mappingResult.kafkaKey(), nullValue());\n\n        // Test for ([^/]+)/.* pattern\n        mappingResult = mapper.map(\"my_house/temperature\");\n\n        assertThat(\"Mqtt pattern ([^/]+)/.* should be mapped to $1\",\n                mappingResult.kafkaTopic(), is(\"my_house\"));\n", "        // Test for ([^/]+)/.* pattern\n        mappingResult = mapper.map(\"my_house/temperature\");\n\n        assertThat(\"Mqtt pattern ([^/]+)/.* should be mapped to $1\",\n                mappingResult.kafkaTopic(), is(\"my_house\"));\n\n        assertThat(\"The key for ([^/]+).* should be expanded to null\",\n                mappingResult.kafkaKey(), nullValue());\n    }\n\n    /**\n     * Test mapping with incorrect regex.\n     */\n    @Test", "    public void testIncorrectRegex() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        rules.add(new MappingRule(\"building/([^/]+)/room/(\\\\d{1,3}).*\", \"building_$1\", \"room_$1\"));\n        rules.add(new MappingRule(\"building/(\\\\p).*\", \"building_$1_others\", null));\n        rules.add(new MappingRule(\"building.*\", \"building_others\", null));\n\n        Exception mapper = assertThrows(PatternSyntaxException.class, () -> new MqttKafkaRegexMapper(rules));\n\n        assertThat(\"Should throw PatternSyntaxException\",\n                mapper.getMessage(), startsWith(\"Unknown character property name {)} near index 12\"));\n\n        // test for .* in capture groups\n        rules.add(0, new MappingRule(\"fleet/([0-9])/vehicle/(\\\\w+)/(.*)\", \"fleet_$1\", \"vehicle_$2\"));\n\n        assertThrows(IllegalArgumentException.class, () -> new MqttKafkaRegexMapper(rules));\n    }\n\n    /**\n     * Test the order of the regex.\n     */\n    @Test", "    public void testRegexOrder() {\n        List<MappingRule> rules = new ArrayList<>();\n\n        rules.add(new MappingRule(\"home/(\\\\w+)/temperature/(sensor\\\\d{1,2})/readings/(\\\\b(all|new|old)\\\\b)\", \"temperature_$2_in_$1\", \"$3\"));\n        rules.add(new MappingRule(\"sports/([^/]+)/league/season/(\\\\d{4}-\\\\d{4})/match/(\\\\d+).*\", \"season_$2_$1\", \"match_$3\"));\n\n        MqttKafkaRegexMapper mapper = new MqttKafkaRegexMapper(rules);\n\n        // Test home/(\\\\w+)/temperature/(sensor\\\\d{1,2})/readings/(\\\\b(all|new|old)\\\\b) pattern\n        MappingResult mappingResult = mapper.map(\"home/bedroom/temperature/sensor01/readings/all\");\n\n        assertThat(\"Mqtt pattern home/(\\\\w+)/temperature/(sensor\\\\d{1,2})/readings/(\\\\b(all|new|old)\\\\b) should be mapped to temperature_$2_in_$1\",\n                mappingResult.kafkaTopic(), is(\"temperature_sensor01_in_bedroom\"));\n\n        assertThat(\"The key for home/(\\\\w+)/temperature/(sensor\\\\d{1,2})/readings/(\\\\b(all|new|old)\\\\b) should be expanded to $3\",\n                mapper.map(\"home/bedroom/temperature/sensor01/readings/all\").kafkaKey(), is(\"all\"));\n\n        // Test sports/([^/]+)/league/season/(\\\\d{4}-\\\\d{4})/match/(\\\\d+).* pattern\n        mappingResult = mapper.map(\"sports/baseball/league/season/2019-2020/match/1/goal/1\");\n\n        assertThat(\"Mqtt pattern sports/([^/]+)/league/season/(\\\\d{4}-\\\\d{4})/match/(\\\\d+)/goal/(\\\\d+).* should be mapped to season_$2_$1\",\n                mappingResult.kafkaTopic(), is(\"season_2019-2020_baseball\"));\n\n        assertThat(\"The key for sports/([^/]+)/league/season/(\\\\d{4}-\\\\d{4})/match/(\\\\d+)/goal/(\\\\d+).* should be expanded to match_$3\",\n                mappingResult.kafkaKey(), is(\"match_1\"));\n\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/Main.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt;\n\nimport io.netty.channel.ChannelOption;\nimport io.netty.channel.EventLoopGroup;\nimport io.netty.channel.nio.NioEventLoopGroup;\nimport io.strimzi.kafka.bridge.mqtt.config.BridgeConfig;", "import io.netty.channel.nio.NioEventLoopGroup;\nimport io.strimzi.kafka.bridge.mqtt.config.BridgeConfig;\nimport io.strimzi.kafka.bridge.mqtt.config.ConfigRetriever;\nimport io.strimzi.kafka.bridge.mqtt.core.MqttServer;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MappingRulesLoader;\nimport org.apache.commons.cli.CommandLine;\nimport org.apache.commons.cli.DefaultParser;\nimport org.apache.commons.cli.Option;\nimport org.apache.commons.cli.Options;\nimport org.slf4j.Logger;", "import org.apache.commons.cli.Options;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.File;\nimport java.util.Map;\nimport java.util.concurrent.CountDownLatch;\n\npublic class Main {\n    private static final Logger logger = LoggerFactory.getLogger(Main.class);\n    private static final String CONFIG_FILE_OPTION = \"config-file\";\n    private static final String MAPPING_RULES_FILE_OPTION = \"mapping-rules\";\n", "public class Main {\n    private static final Logger logger = LoggerFactory.getLogger(Main.class);\n    private static final String CONFIG_FILE_OPTION = \"config-file\";\n    private static final String MAPPING_RULES_FILE_OPTION = \"mapping-rules\";\n\n    public static void main(String[] args) {\n        logger.info(\"Strimzi MQTT Bridge {} is starting\", Main.class.getPackage().getImplementationVersion());\n        try {\n            //prepare the command line options\n            CommandLine cmd = new DefaultParser().parse(generateCommandLineOptions(), args);\n\n            //load the configuration file from the path specified in the command line\n            String configFilePath = getAbsoluteFilePath(cmd.getOptionValue(Main.CONFIG_FILE_OPTION));\n            String mappingRulesFile = getAbsoluteFilePath(cmd.getOptionValue(Main.MAPPING_RULES_FILE_OPTION));\n\n            Map<String, ?> configRetriever = configFilePath != null ? ConfigRetriever.getConfig(configFilePath) : ConfigRetriever.getConfigFromEnv();\n            BridgeConfig bridgeConfig = BridgeConfig.fromMap((Map<String, Object>) configRetriever);\n            logger.info(\"Bridge configuration {}\", bridgeConfig);\n\n            //set the mapping rules file path\n            MappingRulesLoader.getInstance().init(mappingRulesFile);\n\n            //start the MQTT server\n            EventLoopGroup bossGroup = new NioEventLoopGroup();\n            EventLoopGroup workerGroup = new NioEventLoopGroup();\n            MqttServer mqttServer = new MqttServer(bridgeConfig, bossGroup, workerGroup, ChannelOption.SO_KEEPALIVE);\n\n            CountDownLatch latch = new CountDownLatch(1);\n\n            Runtime.getRuntime().addShutdownHook(new Thread(() -> {", "                try {\n                    mqttServer.stop();\n                } catch (Exception e) {\n                    logger.error(\"Error stopping the MQTT server: \", e);\n                } finally {\n                    latch.countDown();\n                }\n            }));\n\n            // start the MQTT server\n            mqttServer.start();\n            latch.await();\n        } catch (Exception e) {\n            logger.error(\"Error starting the MQTT server: \", e);\n            System.exit(1);\n        }\n        System.exit(0);\n    }\n\n    /**\n     * Generate the command line options.\n     * The options are:\n     *      --config-file: the path of the configuration file\n     *      --mapping-rules: the path of the topic mapping rules file\n     * E.g.:\n     *      <application>  --config-file=/path/to/config/file --mapping-rules=/path/to/mapping/rules/file\n     * @return the command line options\n     */\n    private static Options generateCommandLineOptions() {\n\n        Options options = new Options();\n\n        Option optionConfigFile = Option.builder()\n                .longOpt(Main.CONFIG_FILE_OPTION)\n                .hasArg(true)\n                .required()\n                .desc(\"The path to the configuration file\")\n                .build();\n        options.addOption(optionConfigFile);\n\n        Option optionMappingRulesFile = Option.builder()\n                .longOpt(Main.MAPPING_RULES_FILE_OPTION)\n                .hasArg(true)\n                .required()\n                .desc(\"The path to the topic mapping rules file\")\n                .build();\n\n        options.addOption(optionMappingRulesFile);\n        return options;\n    }\n\n    /**\n     * Get the absolute path of the file\n     *\n     * @param arg the path of the file\n     * @return the absolute path of the file\n     */\n    private static String getAbsoluteFilePath(String arg) {", "        if (arg == null) {\n            return null;\n        }\n        return arg.startsWith(File.separator) ? arg : System.getProperty(\"user.dir\") + File.separator + arg;\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/config/KafkaProducerConfig.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport java.util.Map;\nimport java.util.stream.Collectors;\n\n/**", "\n/**\n * Represents configurations related to Kafka\n * @see AbstractConfig\n */\npublic class KafkaProducerConfig extends AbstractConfig {\n\n    // Prefix for all the specific configuration parameters for Kafka producer in the properties file\n    public static final String KAFKA_PRODUCER_CONFIG_PREFIX = KafkaConfig.KAFKA_CONFIG_PREFIX + \"producer.\";\n\n    /**\n     * Constructor\n     *\n     * @param config configuration parameters map\n     */\n    public KafkaProducerConfig(Map<String, Object> config) {\n        super(config);\n    }\n\n    /**\n     * Build a Kafka producer configuration object from a map of configuration parameters\n     *\n     * @param map configuration parameters map\n     * @return a new instance of KafkaProducerConfig\n     */", "    public static final String KAFKA_PRODUCER_CONFIG_PREFIX = KafkaConfig.KAFKA_CONFIG_PREFIX + \"producer.\";\n\n    /**\n     * Constructor\n     *\n     * @param config configuration parameters map\n     */\n    public KafkaProducerConfig(Map<String, Object> config) {\n        super(config);\n    }\n\n    /**\n     * Build a Kafka producer configuration object from a map of configuration parameters\n     *\n     * @param map configuration parameters map\n     * @return a new instance of KafkaProducerConfig\n     */", "    public static KafkaProducerConfig fromMap(Map<String, Object> map) {\n        return new KafkaProducerConfig(map.entrySet().stream()\n                .filter(e -> e.getKey().startsWith(KafkaProducerConfig.KAFKA_PRODUCER_CONFIG_PREFIX))\n                .collect(Collectors.toMap(e -> e.getKey().substring(KafkaProducerConfig.KAFKA_PRODUCER_CONFIG_PREFIX.length()), Map.Entry::getValue)));\n    }\n\n    @Override\n    public String toString() {\n        return \"KafkaProducerConfig(\" +\n                \"config=\" + config +\n                \")\";\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/config/KafkaConfig.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.stream.Collectors;\n", "import java.util.stream.Collectors;\n\n/**\n * Represents configurations related to Kafka\n * @see AbstractConfig\n */\npublic class KafkaConfig extends AbstractConfig {\n\n    // Prefix for all the specific configuration parameters for Kafka in the properties file\n    public static final String KAFKA_CONFIG_PREFIX = \"kafka.\";\n\n    // Kafka bootstrap server configuration. This will be removed when we add the apache kafka client", "    public static final String KAFKA_CONFIG_PREFIX = \"kafka.\";\n\n    // Kafka bootstrap server configuration. This will be removed when we add the apache kafka client\n    public static final String BOOTSTRAP_SERVERS_CONFIG = KAFKA_CONFIG_PREFIX + \"bootstrap.servers\";\n    private final KafkaProducerConfig kafkaProducerConfig;\n\n    /**\n     * Constructor\n     *\n     * @param config configuration parameters map\n     * @param kafkaProducerConfig Kafka producer configuration properties\n     */\n    public KafkaConfig(Map<String, Object> config, KafkaProducerConfig kafkaProducerConfig) {\n        super(config);\n        this.kafkaProducerConfig = kafkaProducerConfig;\n    }\n\n    /**\n     * Build a Kafka configuration object from a map of configuration parameters\n     *\n     * @param config configuration parameters map\n     * @return a new instance of KafkaConfig\n     */", "    public static KafkaConfig fromMap(Map<String, Object> config) {\n        final KafkaProducerConfig kafkaProducerConfig = KafkaProducerConfig.fromMap(config);\n        return new KafkaConfig(config.entrySet().stream()\n                .filter((entry -> entry.getKey().startsWith(KafkaConfig.KAFKA_CONFIG_PREFIX) &&\n                        !entry.getKey().startsWith(KafkaProducerConfig.KAFKA_PRODUCER_CONFIG_PREFIX)))\n                .collect(Collectors.toMap((e) -> e.getKey().substring(KAFKA_CONFIG_PREFIX.length()), Map.Entry::getValue)), kafkaProducerConfig);\n    }\n\n    /**\n     * @return the Kafka producer configuration properties\n     */", "    public KafkaProducerConfig getProducerConfig() {\n        return kafkaProducerConfig;\n    }\n\n    @Override\n    public String toString() {\n        Map<String, Object> configToString = this.hidePasswords(this.config);\n        return \"KafkaConfig(\" +\n                \"config=\" + configToString +\n                \", kafkaProducerConfig=\" + kafkaProducerConfig +\n                \")\";\n    }\n\n    /**\n     * Hides Kafka related password(s) configuration (i.e. truststore and keystore)\n     * by replacing each actual password with [hidden] string\n     *\n     * @param config configuration where to do the replacing\n     * @return updated configuration with hidden password(s)\n     */\n    private Map<String, Object> hidePasswords(Map<String, Object> config) {\n        Map<String, Object> configToString = new HashMap<>();\n        configToString.putAll(this.config);\n        configToString.entrySet().stream()\n                .filter(e -> e.getKey().contains(\"password\"))\n                .forEach(e -> e.setValue(\"[hidden]\"));\n        return configToString;\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/config/MqttConfig.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport java.util.Map;\nimport java.util.stream.Collectors;\n\n/**", "\n/**\n * Represents configurations related to MQTT\n * @see AbstractConfig\n */\npublic class MqttConfig extends AbstractConfig {\n\n    // Prefix for all the specific configuration parameters for MQTT in the properties file\n    public static final String MQTT_CONFIG_PREFIX = \"mqtt.\";\n\n    public static final String MQTT_HOST = MQTT_CONFIG_PREFIX + \"host\";\n", "    public static final String MQTT_CONFIG_PREFIX = \"mqtt.\";\n\n    public static final String MQTT_HOST = MQTT_CONFIG_PREFIX + \"host\";\n\n    public static final String MQTT_PORT = MQTT_CONFIG_PREFIX + \"port\";\n\n    public static final String DEFAULT_MQTT_HOST = \"0.0.0.0\";\n\n    public static final int DEFAULT_MQTT_PORT = 1883;\n\n    /**\n     * Constructor\n     *\n     * @param config configuration parameters map\n     */\n    public MqttConfig(Map<String, Object> config) {\n        super(config);\n    }\n\n    /**\n     * Build a MQTT configuration object from a map of configuration parameters\n     *\n     * @param map configuration parameters map\n     * @return a new instance of MqttConfig\n     */", "    public static final int DEFAULT_MQTT_PORT = 1883;\n\n    /**\n     * Constructor\n     *\n     * @param config configuration parameters map\n     */\n    public MqttConfig(Map<String, Object> config) {\n        super(config);\n    }\n\n    /**\n     * Build a MQTT configuration object from a map of configuration parameters\n     *\n     * @param map configuration parameters map\n     * @return a new instance of MqttConfig\n     */", "    public static MqttConfig fromMap(Map<String, Object> map) {\n        return new MqttConfig(map.entrySet().stream()\n                .filter(entry -> entry.getKey().startsWith(MqttConfig.MQTT_CONFIG_PREFIX))\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)));\n    }\n\n    /**\n     * @return the MQTT server port\n     */\n    public int getPort() {\n        return Integer.parseInt(this.config.getOrDefault(MqttConfig.MQTT_PORT, MqttConfig.DEFAULT_MQTT_PORT).toString());\n    }\n\n    /**\n     * @return the MQTT server host\n     */", "    public int getPort() {\n        return Integer.parseInt(this.config.getOrDefault(MqttConfig.MQTT_PORT, MqttConfig.DEFAULT_MQTT_PORT).toString());\n    }\n\n    /**\n     * @return the MQTT server host\n     */\n    public String getHost() {\n        return this.config.getOrDefault(MqttConfig.MQTT_HOST, MqttConfig.DEFAULT_MQTT_HOST).toString();\n    }\n\n    @Override", "    public String toString(){\n        return \"MqttConfig(\" +\n                \"config=\" + config +\n                \")\";\n    }\n\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/config/AbstractConfig.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport java.util.Map;\n\n/**\n * Base abstract class for configurations related to protocols heads and Kafka", "/**\n * Base abstract class for configurations related to protocols heads and Kafka\n */\npublic abstract class AbstractConfig {\n    protected Map<String, Object> config;\n\n    /**\n     * Constructor\n     *\n     * @param config configuration parameters map\n     */\n    public AbstractConfig(Map<String, Object> config) {\n        this.config = config;\n    }\n\n    /**\n     * @return configuration parameters map\n     */\n    public Map<String, Object> getConfig() {\n        return this.config;\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/config/BridgeConfig.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport java.util.Map;\nimport java.util.stream.Collectors;\n\n/**", "\n/**\n * Represents the bridge configuration properties\n *\n * @see MqttConfig\n * @see KafkaConfig\n */\npublic class BridgeConfig extends AbstractConfig {\n\n    // Prefix for all the specific configuration parameters for the bridge\n    public static final String BRIDGE_CONFIG_PREFIX = \"bridge.\";\n\n    // Bridge identification number", "    public static final String BRIDGE_CONFIG_PREFIX = \"bridge.\";\n\n    // Bridge identification number\n    public static final String BRIDGE_ID = BRIDGE_CONFIG_PREFIX + \"id\";\n\n    private final MqttConfig mqttConfig;\n    private final KafkaConfig kafkaConfig;\n\n    /**\n     * Constructor\n     *\n     * @param config      configuration parameters map\n     * @param mqttConfig  MQTT configuration properties\n     * @param kafkaConfig Kafka configuration properties\n     */\n    public BridgeConfig(Map<String, Object> config, MqttConfig mqttConfig, KafkaConfig kafkaConfig) {\n        super(config);\n        this.mqttConfig = mqttConfig;\n        this.kafkaConfig = kafkaConfig;\n    }\n\n    /**\n     * Build a bridge configuration object from a map of configuration parameters\n     *\n     * @param map configuration parameters map\n     * @return a new instance of BridgeConfig\n     */", "    public static BridgeConfig fromMap(Map<String, Object> map) {\n        final MqttConfig mqttConfig = MqttConfig.fromMap(map);\n        final KafkaConfig kafkaConfig = KafkaConfig.fromMap(map);\n        return new BridgeConfig(map.entrySet().stream()\n                .filter(entry -> entry.getKey().startsWith(BridgeConfig.BRIDGE_CONFIG_PREFIX))\n                .collect(Collectors.toMap(Map.Entry::getKey, Map.Entry::getValue)), mqttConfig, kafkaConfig);\n    }\n\n    /**\n     * @return the Kafka configuration properties\n     */", "    public KafkaConfig getKafkaConfig() {\n        return this.kafkaConfig;\n    }\n\n    /**\n     * @return the MQTT configuration properties\n     */\n    public MqttConfig getMqttConfig() {\n        return this.mqttConfig;\n    }\n\n    /**\n     * @return the bridge identification number\n     */", "    public String getBridgeID() {\n        return this.config.get(BridgeConfig.BRIDGE_ID) == null ? null : this.config.get(BridgeConfig.BRIDGE_ID).toString();\n    }\n\n    /**\n     * @return the bridge configuration properties\n     */\n    @Override\n    public String toString() {\n        return \"BridgeConfig(\" +\n                \"config=\" + this.config +\n                \", mqttConfig=\" + this.mqttConfig +\n                \", kafkaConfig=\" + this.kafkaConfig +\n                \")\";\n    }\n}\n", "    public String toString() {\n        return \"BridgeConfig(\" +\n                \"config=\" + this.config +\n                \", mqttConfig=\" + this.mqttConfig +\n                \", kafkaConfig=\" + this.kafkaConfig +\n                \")\";\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/config/ConfigRetriever.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.config;\n\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.util.HashMap;", "import java.io.InputStream;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Properties;\nimport java.util.stream.Collectors;\n\n/**\n * Retrieve the bridge configuration from properties file and environment variables\n */\npublic class ConfigRetriever {\n\n    /**\n     * Retrieve the bridge configuration from the properties file provided as parameter\n     * and adding environment variables\n     * If a parameter is defined in both properties file and environment variables, the latter wins\n     *\n     * @param path path to the properties file\n     * @return configuration as key-value pairs\n     * @throws IOException when not possible to get the properties file\n     */", " */\npublic class ConfigRetriever {\n\n    /**\n     * Retrieve the bridge configuration from the properties file provided as parameter\n     * and adding environment variables\n     * If a parameter is defined in both properties file and environment variables, the latter wins\n     *\n     * @param path path to the properties file\n     * @return configuration as key-value pairs\n     * @throws IOException when not possible to get the properties file\n     */", "    public static Map<String, Object> getConfig(String path) throws IOException {\n        return getConfig(path, System.getenv());\n    }\n\n    /**\n     * Retrieve the bridge configuration from the environment variables\n     */\n    public static Map<String, String> getConfigFromEnv() {\n        return System.getenv();\n    }\n\n    /**\n     * Retrieve the bridge configuration from the properties file provided as parameter\n     * and adding the additional configuration parameter provided as well\n     * If a parameter is defined in both properties file and additional configuration, the latter wins\n     *\n     * @param path             path to the properties file\n     * @param additionalConfig additional configuration to add\n     * @return configuration as key-value pairs\n     * @throws IOException when not possible to get the properties file\n     */", "    public static Map<String, Object> getConfig(String path, Map<String, String> additionalConfig) throws IOException {\n        Map<String, Object> configuration;\n        try (InputStream is = new FileInputStream(path)) {\n            Properties props = new Properties();\n            props.load(is);\n            configuration =\n                    props.entrySet().stream().collect(\n                            Collectors.toMap(\n                                    e -> String.valueOf(e.getKey()),\n                                    e -> String.valueOf(e.getValue()),\n                                    (prev, next) -> next, HashMap::new\n                            ));\n        }\n        configuration.putAll(additionalConfig);\n        return configuration;\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/core/MqttServerHandler.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.core;\n\nimport io.netty.channel.ChannelHandlerContext;\nimport io.netty.channel.SimpleChannelInboundHandler;\nimport io.netty.handler.codec.mqtt.MqttConnectMessage;\nimport io.netty.handler.codec.mqtt.MqttMessage;", "import io.netty.handler.codec.mqtt.MqttConnectMessage;\nimport io.netty.handler.codec.mqtt.MqttMessage;\nimport io.netty.handler.codec.mqtt.MqttMessageBuilders;\nimport io.netty.handler.codec.mqtt.MqttMessageType;\nimport io.netty.handler.codec.mqtt.MqttPublishMessage;\nimport io.netty.handler.codec.mqtt.MqttConnectReturnCode;\nimport io.netty.handler.codec.mqtt.MqttConnAckMessage;\nimport io.netty.handler.codec.mqtt.MqttQoS;\nimport io.strimzi.kafka.bridge.mqtt.kafka.KafkaBridgeProducer;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MqttKafkaMapper;", "import io.strimzi.kafka.bridge.mqtt.kafka.KafkaBridgeProducer;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MqttKafkaMapper;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MqttKafkaRegexMapper;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MappingRule;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MappingResult;\nimport io.strimzi.kafka.bridge.mqtt.mapper.MappingRulesLoader;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.header.Headers;\nimport org.apache.kafka.common.header.internals.RecordHeader;", "import org.apache.kafka.common.header.Headers;\nimport org.apache.kafka.common.header.internals.RecordHeader;\nimport org.apache.kafka.common.header.internals.RecordHeaders;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\nimport java.util.List;\nimport java.util.concurrent.CompletionStage;\n", "import java.util.concurrent.CompletionStage;\n\nimport static io.netty.channel.ChannelHandler.Sharable;\n\n/**\n * Represents a SimpleChannelInboundHandler. The MqttServerHandler is responsible for: <br>\n * - listen to client connections;<br>\n * - listen to incoming messages; <br>\n *\n * @see io.netty.channel.SimpleChannelInboundHandler", " *\n * @see io.netty.channel.SimpleChannelInboundHandler\n */\n@Sharable\npublic class MqttServerHandler extends SimpleChannelInboundHandler<MqttMessage> {\n    private static final Logger logger = LoggerFactory.getLogger(MqttServerHandler.class);\n    private final KafkaBridgeProducer kafkaBridgeProducer;\n    private MqttKafkaMapper mqttKafkaMapper;\n\n    /**\n     * Constructor\n     */\n    public MqttServerHandler(KafkaBridgeProducer kafkaBridgeProducer) {\n        // auto release reference count to avoid memory leak\n        super(true);", "        try {\n            MappingRulesLoader mappingRulesLoader = MappingRulesLoader.getInstance();\n            List<MappingRule> rules = mappingRulesLoader.loadRules();\n            this.mqttKafkaMapper = new MqttKafkaRegexMapper(rules);\n        } catch (IOException e) {\n            logger.error(\"Error reading mapping file: \", e);\n        }\n        this.kafkaBridgeProducer = kafkaBridgeProducer;\n    }\n\n    /**\n     * Transform a MqttPublishMessage's payload into bytes array\n     */\n    private static byte[] payloadToBytes(MqttPublishMessage msg) {\n        byte[] data = new byte[msg.payload().readableBytes()];\n        msg.payload().readBytes(data);\n        return data;\n    }\n\n    @Override", "    public void channelActive(ChannelHandlerContext ctx) {\n        logger.info(\"Client  {} is trying to connect\", ctx.channel().remoteAddress());\n    }\n\n    @Override\n    protected void channelRead0(ChannelHandlerContext ctx, MqttMessage msg) {\n        try {\n            MqttMessageType messageType = msg.fixedHeader().messageType();\n            logger.debug(\"Got {} message type\", messageType.name());\n            if (messageType == MqttMessageType.CONNECT) {\n                handleConnectMessage(ctx, (MqttConnectMessage) msg);", "            if (messageType == MqttMessageType.CONNECT) {\n                handleConnectMessage(ctx, (MqttConnectMessage) msg);\n            } else if (messageType == MqttMessageType.PUBLISH) {\n                handlePublishMessage(ctx, (MqttPublishMessage) msg);\n            } else {\n                logger.warn(\"Message type {} not handled\", messageType.name());\n            }\n        } catch (Exception e) {\n            logger.error(\"Error reading Channel: \", e);\n            ctx.close();\n        }\n    }\n\n    @Override", "    public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) {\n        cause.printStackTrace();\n        ctx.close();\n    }\n\n    /**\n     * Handle the case when a client sent a MQTT CONNECT message type.\n     *\n     * @param ctx ChannelHandlerContext instance\n     * @param connectMessage incoming MqttConnectMessage\n     */\n    private void handleConnectMessage(ChannelHandlerContext ctx, MqttConnectMessage connectMessage) {\n        MqttConnAckMessage connAckMessage = MqttMessageBuilders.connAck()\n                .sessionPresent(false)\n                .returnCode(MqttConnectReturnCode.CONNECTION_ACCEPTED)\n                .build();\n\n        logger.info(\"Client [{}] connected from {}\", connectMessage.payload().clientIdentifier(), ctx.channel().remoteAddress());\n        ctx.writeAndFlush(connAckMessage);\n    }\n\n    /**\n     * Send a MQTT PUBACK message to the client.\n     *\n     * @param ctx      ChannelHandlerContext instance\n     * @param packetId packet identifier\n     */\n    private void sendPubAckMessage(ChannelHandlerContext ctx, int packetId) {\n        MqttMessage pubAckMessage = MqttMessageBuilders.pubAck()\n                .packetId(packetId)\n                .build();\n\n        ctx.writeAndFlush(pubAckMessage);\n    }\n\n    /**\n     * Handle the case when a client sent a MQTT PUBLISH message type.\n     *\n     * @param ctx            ChannelHandlerContext instance\n     * @param publishMessage represents a MqttPublishMessage\n     */\n    private void handlePublishMessage(ChannelHandlerContext ctx, MqttPublishMessage publishMessage) {\n        // get QoS level from the MqttPublishMessage\n        MqttQoS qos = MqttQoS.valueOf(publishMessage.fixedHeader().qosLevel().value());\n\n        // get the MQTT topic from the MqttPublishMessage\n        String mqttTopic = publishMessage.variableHeader().topicName();\n\n        // perform topic mapping\n        MappingResult mappingResult = mqttKafkaMapper.map(mqttTopic);\n\n        // log the topic mapping\n        logger.info(\"MQTT topic {} mapped to Kafka Topic {} with Key {}\", mqttTopic, mappingResult.kafkaTopic(), mappingResult.kafkaKey());\n\n        byte[] data = payloadToBytes(publishMessage);\n        Headers headers = new RecordHeaders();\n        headers.add(new RecordHeader(\"mqtt-topic\", mqttTopic.getBytes()));\n        // build the Kafka record\n        ProducerRecord<String, byte[]> record = new ProducerRecord<>(mappingResult.kafkaTopic(), null, mappingResult.kafkaKey(),\n                data, headers);\n\n        // send the record to the Kafka topic\n        switch (qos) {\n            case AT_MOST_ONCE -> {\n                kafkaBridgeProducer.sendNoAck(record);\n                logger.info(\"Message sent to Kafka on topic {}\", record.topic());\n            }\n            case AT_LEAST_ONCE -> {\n                CompletionStage<RecordMetadata> result = kafkaBridgeProducer.send(record);\n                // wait for the result of the send operation\n                result.whenComplete((metadata, error) -> {", "                    if (error != null) {\n                        logger.error(\"Error sending message to Kafka: \", error);\n                    } else {\n                        logger.info(\"Message sent to Kafka on topic {} with offset {}\", metadata.topic(), metadata.offset());\n                        // send PUBACK message to the client\n                        sendPubAckMessage(ctx, publishMessage.variableHeader().packetId());\n                    }\n                });\n            }\n            case EXACTLY_ONCE -> logger.warn(\"QoS level EXACTLY_ONCE is not supported\");\n        }\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/core/MqttServerInitializer.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.core;\n\nimport io.netty.channel.ChannelInitializer;\nimport io.netty.channel.socket.SocketChannel;\nimport io.netty.handler.codec.mqtt.MqttDecoder;\nimport io.netty.handler.codec.mqtt.MqttEncoder;", "import io.netty.handler.codec.mqtt.MqttDecoder;\nimport io.netty.handler.codec.mqtt.MqttEncoder;\nimport io.strimzi.kafka.bridge.mqtt.kafka.KafkaBridgeProducer;\n\n/**\n * This helper class help us add necessary Netty pipelines handlers. <br>\n * During the {@link #initChannel(SocketChannel)}, we use MqttDecoder() and MqttEncoder to decode and encode Mqtt messages respectively. <br>\n */\npublic class MqttServerInitializer extends ChannelInitializer<SocketChannel> {\n    private final MqttServerHandler mqttServerHandler;\n\n    public MqttServerInitializer(KafkaBridgeProducer kafkaBridgeProducer) {\n        this.mqttServerHandler = new MqttServerHandler(kafkaBridgeProducer);\n    }\n\n    @Override\n    protected void initChannel(SocketChannel ch) {\n        ch.pipeline().addLast(\"decoder\", new MqttDecoder());\n        ch.pipeline().addLast(\"encoder\", MqttEncoder.INSTANCE);\n        ch.pipeline().addLast(\"handler\", this.mqttServerHandler);\n    }\n}\n", "public class MqttServerInitializer extends ChannelInitializer<SocketChannel> {\n    private final MqttServerHandler mqttServerHandler;\n\n    public MqttServerInitializer(KafkaBridgeProducer kafkaBridgeProducer) {\n        this.mqttServerHandler = new MqttServerHandler(kafkaBridgeProducer);\n    }\n\n    @Override\n    protected void initChannel(SocketChannel ch) {\n        ch.pipeline().addLast(\"decoder\", new MqttDecoder());\n        ch.pipeline().addLast(\"encoder\", MqttEncoder.INSTANCE);\n        ch.pipeline().addLast(\"handler\", this.mqttServerHandler);\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/core/MqttServer.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.core;\n\nimport io.netty.bootstrap.ServerBootstrap;\nimport io.netty.channel.ChannelFuture;\nimport io.netty.channel.ChannelOption;\nimport io.netty.channel.EventLoopGroup;", "import io.netty.channel.ChannelOption;\nimport io.netty.channel.EventLoopGroup;\nimport io.netty.channel.socket.nio.NioServerSocketChannel;\nimport io.netty.handler.logging.LogLevel;\nimport io.netty.handler.logging.LoggingHandler;\nimport io.strimzi.kafka.bridge.mqtt.config.BridgeConfig;\nimport io.strimzi.kafka.bridge.mqtt.config.MqttConfig;\nimport io.strimzi.kafka.bridge.mqtt.kafka.KafkaBridgeProducer;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;", "import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * Represents the MqttServer component.\n */\npublic class MqttServer {\n    private static final Logger logger = LoggerFactory.getLogger(MqttServer.class);\n    private final EventLoopGroup masterGroup;\n    private final EventLoopGroup workerGroup;\n    private final ServerBootstrap serverBootstrap;\n    private final MqttConfig mqttConfig;\n    private final KafkaBridgeProducer kafkaBridgeProducer;\n\n    private ChannelFuture channelFuture;\n\n    /**\n     * Constructor\n     *\n     * @param config      MqttConfig instance with all configuration needed to run the server.\n     * @param masterGroup EventLoopGroup instance for handle incoming connections.\n     * @param workerGroup EventLoopGroup instance for processing I/O.\n     * @param option      ChannelOption<Boolean> instance which allows to configure various channel options, such as SO_KEEPALIVE, SO_BACKLOG etc.\n     * @see BridgeConfig\n     * @see ChannelOption\n     */\n    public MqttServer(BridgeConfig config, EventLoopGroup masterGroup, EventLoopGroup workerGroup, ChannelOption<Boolean> option) {\n        this.masterGroup = masterGroup;\n        this.workerGroup = workerGroup;\n        this.mqttConfig = config.getMqttConfig();\n        this.kafkaBridgeProducer = new KafkaBridgeProducer(config.getKafkaConfig());\n        this.serverBootstrap = new ServerBootstrap();\n        this.serverBootstrap.group(masterGroup, workerGroup)\n                .channel(NioServerSocketChannel.class)\n                .handler(new LoggingHandler(LogLevel.INFO))\n                .childHandler(new MqttServerInitializer(this.kafkaBridgeProducer))\n                .childOption(option, true);\n    }\n\n    /**\n     * Start the server.\n     */", "    public void start() throws InterruptedException {\n        // bind the Netty server and wait synchronously\n        this.channelFuture = this.serverBootstrap.bind(this.mqttConfig.getHost(), this.mqttConfig.getPort()).sync();\n    }\n\n    /**\n     * Stop the server.\n     */\n    public void stop() throws InterruptedException {\n        logger.info(\"Shutting down Netty server...\");\n        this.channelFuture.channel().close().sync();\n        this.channelFuture.channel().closeFuture().sync();\n        this.masterGroup.shutdownGracefully().sync();\n        this.workerGroup.shutdownGracefully().sync();\n        logger.info(\"Netty server shut down\");\n\n        logger.info(\"Closing Kafka producers...\");\n        this.kafkaBridgeProducer.close();\n        logger.info(\"Kafka producers closed\");\n    }\n}\n", "    public void stop() throws InterruptedException {\n        logger.info(\"Shutting down Netty server...\");\n        this.channelFuture.channel().close().sync();\n        this.channelFuture.channel().closeFuture().sync();\n        this.masterGroup.shutdownGracefully().sync();\n        this.workerGroup.shutdownGracefully().sync();\n        logger.info(\"Netty server shut down\");\n\n        logger.info(\"Closing Kafka producers...\");\n        this.kafkaBridgeProducer.close();\n        logger.info(\"Kafka producers closed\");\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/kafka/KafkaBridgeProducer.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.kafka;\n\nimport io.strimzi.kafka.bridge.mqtt.config.KafkaConfig;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;", "import org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.common.serialization.ByteArraySerializer;\nimport org.apache.kafka.common.serialization.StringSerializer;\n\nimport java.util.Properties;\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;", "import java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\n\n/**\n * Represents a Kafka producer for the Bridge.\n */\npublic class KafkaBridgeProducer {\n\n    private final Producer<String, byte[]> noAckProducer;\n    private final Producer<String, byte[]> ackOneProducer;\n\n    /**\n     * Constructor\n     */\n    public KafkaBridgeProducer(KafkaConfig config) {\n        this.noAckProducer = createProducer(config, KafkaProducerAckLevel.ZERO);\n        this.ackOneProducer = createProducer(config, KafkaProducerAckLevel.ONE);\n    }\n\n    /**\n     * Send the given record to the Kafka topic\n     *\n     * @param record record to be sent\n     * @return a future which completes when the record is acknowledged\n     */\n    public CompletionStage<RecordMetadata> send(ProducerRecord<String, byte[]> record) {\n        CompletableFuture<RecordMetadata> promise = new CompletableFuture<>();\n\n        this.ackOneProducer.send(record, (metadata, exception) -> {", "            if (exception != null) {\n                promise.completeExceptionally(exception);\n            } else {\n                promise.complete(metadata);\n            }\n        });\n        return promise;\n    }\n\n    /**\n     * Send the given record to the Kafka topic\n     *\n     * @param record record to be sent\n     */", "    public void sendNoAck(ProducerRecord<String, byte[]> record) {\n        this.noAckProducer.send(record);\n    }\n\n    /**\n     * Create the Kafka producer client with the given configuration\n     */\n    private Producer<String, byte[]> createProducer(KafkaConfig kafkaConfig, KafkaProducerAckLevel producerAckLevel) {\n        Properties props = new Properties();\n        props.putAll(kafkaConfig.getConfig());\n        props.putAll(kafkaConfig.getProducerConfig().getConfig());\n        props.put(ProducerConfig.ACKS_CONFIG, String.valueOf(producerAckLevel.getValue()));\n        return new KafkaProducer<>(props, new StringSerializer(), new ByteArraySerializer());\n    }\n\n    /**\n     * Close the producer\n     */", "    public void close() {\n\n        if (this.noAckProducer != null) {\n            this.noAckProducer.flush();\n            this.noAckProducer.close();\n        }\n\n        if (this.ackOneProducer != null) {\n            this.ackOneProducer.flush();\n            this.ackOneProducer.close();\n        }\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/kafka/KafkaProducerAckLevel.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.kafka;\n\n/**\n * Represents the Kafka producer ack level\n */\npublic enum KafkaProducerAckLevel {\n    // This ack level is used when the kafka producer is expecting no acks\n    ZERO(0),\n    // This ack level is used when the kafka producer is expecting acks from the leader\n    ONE(1),\n    // This ack level is used when the kafka producer is expecting acks from all the replicas\n    ALL(-1);\n\n    private final int value;\n\n    /**\n     * Constructor\n     *\n     * @param value the value of the ack level\n     */\n    KafkaProducerAckLevel(int value) {\n        this.value = value;\n    }\n\n    /**\n     * Get the ack level from the value\n     *\n     * @param value the value of the ack level\n     * @return the ack level\n     */", " */\npublic enum KafkaProducerAckLevel {\n    // This ack level is used when the kafka producer is expecting no acks\n    ZERO(0),\n    // This ack level is used when the kafka producer is expecting acks from the leader\n    ONE(1),\n    // This ack level is used when the kafka producer is expecting acks from all the replicas\n    ALL(-1);\n\n    private final int value;\n\n    /**\n     * Constructor\n     *\n     * @param value the value of the ack level\n     */\n    KafkaProducerAckLevel(int value) {\n        this.value = value;\n    }\n\n    /**\n     * Get the ack level from the value\n     *\n     * @param value the value of the ack level\n     * @return the ack level\n     */", "    public static KafkaProducerAckLevel valueOf(int value) {\n        return switch (value) {\n            case 0 -> ZERO;\n            case 1 -> ONE;\n            case -1 -> ALL;\n            default -> throw new IllegalArgumentException(\"Unknown KafkaProducerAckLevel value: \" + value);\n        };\n    }\n\n    /**\n     * @return the value of the ack level\n     */", "    public int getValue() {\n        return value;\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/mapper/MqttKafkaRegexMapper.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\nimport java.util.List;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n", "import java.util.regex.Pattern;\n\n/**\n * Responsible for handling all the topic mapping rules defined with regular expressions.\n */\npublic class MqttKafkaRegexMapper extends MqttKafkaMapper {\n\n    // used to find any expression starting with a $ followed by upto 2 digits number. E.g. $1, this is known as a placeholder.\n    public static final String MQTT_TOPIC_DOLLAR_PLACEHOLDER_REGEX = \"\\\\$(\\\\d{1,2})\";\n\n    /**\n     * Constructor\n     * Creates a new instance of MqttKafkaRegexMapper.\n     */\n    public MqttKafkaRegexMapper(List<MappingRule> rules) {\n        super(rules, Pattern.compile(MQTT_TOPIC_DOLLAR_PLACEHOLDER_REGEX));\n    }\n\n    @Override", "    public static final String MQTT_TOPIC_DOLLAR_PLACEHOLDER_REGEX = \"\\\\$(\\\\d{1,2})\";\n\n    /**\n     * Constructor\n     * Creates a new instance of MqttKafkaRegexMapper.\n     */\n    public MqttKafkaRegexMapper(List<MappingRule> rules) {\n        super(rules, Pattern.compile(MQTT_TOPIC_DOLLAR_PLACEHOLDER_REGEX));\n    }\n\n    @Override", "    public MappingResult map(String mqttTopic) {\n        for (MappingRule rule : this.rules) {\n            Matcher matcher = this.patterns.get(this.rules.indexOf(rule)).matcher(mqttTopic);\n            if (matcher.matches()) {\n                String mappedKafkaTopic = rule.getKafkaTopicTemplate();\n                String kafkaKey = rule.getKafkaKeyTemplate();\n\n                for (int i = 1; i < matcher.groupCount() + 1; i++) {\n                    mappedKafkaTopic = mappedKafkaTopic.replace(\"$\" + i, matcher.group(i));\n                    kafkaKey = kafkaKey != null ? kafkaKey.replace(\"$\" + i, matcher.group(i)) : null;\n                }\n\n                // check for pending placeholders replacement in the Kafka topic\n                checkPlaceholder(mappedKafkaTopic);\n", "                if (kafkaKey != null) {\n                    // check for pending placeholders replacement in the Kafka key.\n                    checkPlaceholder(kafkaKey);\n                }\n\n                // return the first match\n                return new MappingResult(mappedKafkaTopic, kafkaKey);\n            }\n        }\n        return new MappingResult(MqttKafkaMapper.DEFAULT_KAFKA_TOPIC, null);\n    }\n\n    @Override\n    protected void buildOrCompilePatterns() {\n        this.rules.forEach(rule-> this.patterns.add(Pattern.compile(rule.getMqttTopicPattern())));\n    }\n\n    /**\n     * Checks if there are any pending placeholders in the Kafka topic or Kafka key template.\n     *\n     * @param template the placeholder to check.\n     */\n    private void checkPlaceholder(String template) {\n        Matcher matcher = this.placeholderPattern.matcher(template);", "        if (matcher.find()) {\n            throw new IllegalArgumentException(\"The placeholder \" + matcher.group() + \" was not found or assigned any value.\");\n        }\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/mapper/MqttKafkaSimpleMapper.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.regex.Matcher;", "import java.util.Map;\nimport java.util.regex.Matcher;\nimport java.util.regex.Pattern;\n\n/**\n * Responsible for handling all the topic mapping using named placeholders instead of regular expressions.\n *\n * @see MappingRule\n * @see MqttKafkaMapper\n * @see MqttKafkaRegexMapper", " * @see MqttKafkaMapper\n * @see MqttKafkaRegexMapper\n */\npublic class MqttKafkaSimpleMapper extends MqttKafkaMapper {\n\n    // find any word inside a curly bracket. E.g. {something}, this is known as a placeholder.\n    private static final String MQTT_TOPIC_PLACEHOLDER_REGEX = \"\\\\{\\\\w+\\\\}\";\n\n    // identifies a single level wildcard character in the mqtt pattern. E.g. sensors/+/data\n    private static final String MQTT_TOPIC_SINGLE_LEVEL_WILDCARD_CHARACTER = \"+\";\n\n    // Regex expression used to replace the + in the mqtt pattern.\n    private static final String SINGLE_LEVEL_WILDCARD_REGEX = \"[^/]+\";\n\n    // identifies a multi level wildcard character in the mqtt pattern. E.g. sensors/#\n    private static final String MQTT_TOPIC_MULTI_LEVEL_WILDCARD_CHARACTER = \"#\";\n\n    // used to replace the # in the mqtt pattern.", "    public static final String WILDCARD_REGEX = \"(?:\\\\/.*)?$\";\n\n    /**\n     * Constructor.\n     *\n     * @param rules the list of mapping rules.\n     */\n    public MqttKafkaSimpleMapper(List<MappingRule> rules) {\n        super(rules, Pattern.compile(MQTT_TOPIC_PLACEHOLDER_REGEX));\n    }\n\n    @Override", "    public MappingResult map(String mqttTopic) {\n        for (MappingRule rule : this.rules) {\n            Matcher matcher = this.patterns.get(this.rules.indexOf(rule)).matcher(mqttTopic);\n\n            if (matcher.matches()) {\n                HashMap<String, String> placeholders = new HashMap<>();\n\n                String mappedKafkaTopic = rule.getKafkaTopicTemplate();\n                String kafkaKey = rule.getKafkaKeyTemplate();\n\n                // find MQTT_TOPIC_PLACEHOLDER_REGEX in the kafkaTopicTemplate.\n                Matcher placeholderMatcher = this.placeholderPattern.matcher(rule.getKafkaTopicTemplate());", "                while (placeholderMatcher.find()) {\n                    String placeholderKey = placeholderMatcher.group();\n                    placeholders.put(placeholderKey, null);\n                }\n\n                // find MQTT_TOPIC_PLACEHOLDER_REGEX in the kafkaKey\n                if (kafkaKey != null) {\n                    placeholderMatcher = this.placeholderPattern.matcher(kafkaKey);\n                    while (placeholderMatcher.find()) {\n                        String placeholderKey = placeholderMatcher.group();\n                        placeholders.put(placeholderKey, null);\n                    }\n                }\n", "                    while (placeholderMatcher.find()) {\n                        String placeholderKey = placeholderMatcher.group();\n                        placeholders.put(placeholderKey, null);\n                    }\n                }\n\n                if (!placeholders.isEmpty()) {\n                    Matcher mqttTopicMatcher = this.placeholderPattern.matcher(rule.getMqttTopicPattern());\n\n                    // find the placeholders in the mqtt topic pattern and assign them a value.\n                    while (mqttTopicMatcher.find()) {\n                        String placeholderKey = mqttTopicMatcher.group();\n                        String placeholderValue = matcher.group(removeBrackets(placeholderKey));\n                        placeholders.put(placeholderKey, placeholderValue);\n                    }\n\n                    // build the Kafka topic using the placeholders.", "                    while (mqttTopicMatcher.find()) {\n                        String placeholderKey = mqttTopicMatcher.group();\n                        String placeholderValue = matcher.group(removeBrackets(placeholderKey));\n                        placeholders.put(placeholderKey, placeholderValue);\n                    }\n\n                    // build the Kafka topic using the placeholders.\n                    for (Map.Entry<String, String> entry : placeholders.entrySet()) {\n                        if (entry.getValue() != null) {\n                            mappedKafkaTopic = mappedKafkaTopic.replace(entry.getKey(), entry.getValue());\n                            kafkaKey = kafkaKey != null ? kafkaKey.replace(entry.getKey(), entry.getValue()) : null;\n                        } else {\n                            throw new IllegalArgumentException(\"The placeholder \" + entry.getKey() + \" was not found assigned any value.\");\n                        }\n                    }\n                }\n                return new MappingResult(mappedKafkaTopic, kafkaKey);\n            }\n        }\n        return new MappingResult(MqttKafkaMapper.DEFAULT_KAFKA_TOPIC, null);\n    }\n\n    @Override\n    protected void buildOrCompilePatterns() {\n\n        // convert the mqtt patterns to a valid regex expression.\n        // the mqtt pattern can contain placeholders like {something}, + and #.\n        // if the mqtt topic contains a +, we replace it with @singleLevelWildcardRegex\n        // if the mqtt topic contains a #, we replace it with @multiLevelWildcardRegex\n        // if the mqtt topic contains a placeholder (pattern \\{\\w+\\}), we replace it with @placeholderRegex\n        String[] mqttTopicPatternParts;\n        StringBuilder ruleRegex;", "                        if (entry.getValue() != null) {\n                            mappedKafkaTopic = mappedKafkaTopic.replace(entry.getKey(), entry.getValue());\n                            kafkaKey = kafkaKey != null ? kafkaKey.replace(entry.getKey(), entry.getValue()) : null;\n                        } else {\n                            throw new IllegalArgumentException(\"The placeholder \" + entry.getKey() + \" was not found assigned any value.\");\n                        }\n                    }\n                }\n                return new MappingResult(mappedKafkaTopic, kafkaKey);\n            }\n        }\n        return new MappingResult(MqttKafkaMapper.DEFAULT_KAFKA_TOPIC, null);\n    }\n\n    @Override\n    protected void buildOrCompilePatterns() {\n\n        // convert the mqtt patterns to a valid regex expression.\n        // the mqtt pattern can contain placeholders like {something}, + and #.\n        // if the mqtt topic contains a +, we replace it with @singleLevelWildcardRegex\n        // if the mqtt topic contains a #, we replace it with @multiLevelWildcardRegex\n        // if the mqtt topic contains a placeholder (pattern \\{\\w+\\}), we replace it with @placeholderRegex\n        String[] mqttTopicPatternParts;\n        StringBuilder ruleRegex;", "        for (MappingRule rule : this.rules) {\n            mqttTopicPatternParts = rule.getMqttTopicPattern().split(MQTT_TOPIC_SEPARATOR);\n            ruleRegex = new StringBuilder();\n            for (String part : mqttTopicPatternParts) {\n                if (part.matches(MQTT_TOPIC_PLACEHOLDER_REGEX)) {\n                    ruleRegex.append(buildNamedRegexExpression(part));\n                } else if (part.equals(MQTT_TOPIC_SINGLE_LEVEL_WILDCARD_CHARACTER)) {\n                    ruleRegex.append(SINGLE_LEVEL_WILDCARD_REGEX);\n                } else if (part.equals(MQTT_TOPIC_MULTI_LEVEL_WILDCARD_CHARACTER)) {\n                    if (ruleRegex.length() > 1) {\n                        ruleRegex.deleteCharAt(ruleRegex.length() - 1);\n                    }\n                    ruleRegex.append(WILDCARD_REGEX);\n                } else {\n                    ruleRegex.append(part);\n                }\n                ruleRegex.append(MQTT_TOPIC_SEPARATOR);\n            }\n            // remove the last slash\n            ruleRegex.deleteCharAt(ruleRegex.length() - 1);\n            // compile the regex expression for the rule.\n            patterns.add(Pattern.compile(ruleRegex.toString()));\n        }\n    }\n\n    /**\n     * Helper method for building a named regex expression.\n     * A named regex expression is a regex expression that contains a named capturing group.\n     * E.g. (?<groupName>regexExpression)\n     *\n     * @param placeholder represents a placeholder in the mqtt pattern.\n     * @return a named regex expression.\n     */\n    private String buildNamedRegexExpression(String placeholder) {\n        String groupName = removeBrackets(placeholder);\n        return \"(?<\" + groupName + \">[^/]+)\";\n    }\n\n    /**\n     * Helper method for removing the curly brackets from a placeholder.\n     *\n     * @param placeholder represents a placeholder in the pattern.\n     * @return a placeholder without the curly brackets.\n     */\n    private String removeBrackets(String placeholder) {\n        return placeholder.replaceAll(\"\\\\{+|\\\\}+\", \"\");\n    }\n}\n", "                } else if (part.equals(MQTT_TOPIC_MULTI_LEVEL_WILDCARD_CHARACTER)) {\n                    if (ruleRegex.length() > 1) {\n                        ruleRegex.deleteCharAt(ruleRegex.length() - 1);\n                    }\n                    ruleRegex.append(WILDCARD_REGEX);\n                } else {\n                    ruleRegex.append(part);\n                }\n                ruleRegex.append(MQTT_TOPIC_SEPARATOR);\n            }\n            // remove the last slash\n            ruleRegex.deleteCharAt(ruleRegex.length() - 1);\n            // compile the regex expression for the rule.\n            patterns.add(Pattern.compile(ruleRegex.toString()));\n        }\n    }\n\n    /**\n     * Helper method for building a named regex expression.\n     * A named regex expression is a regex expression that contains a named capturing group.\n     * E.g. (?<groupName>regexExpression)\n     *\n     * @param placeholder represents a placeholder in the mqtt pattern.\n     * @return a named regex expression.\n     */\n    private String buildNamedRegexExpression(String placeholder) {\n        String groupName = removeBrackets(placeholder);\n        return \"(?<\" + groupName + \">[^/]+)\";\n    }\n\n    /**\n     * Helper method for removing the curly brackets from a placeholder.\n     *\n     * @param placeholder represents a placeholder in the pattern.\n     * @return a placeholder without the curly brackets.\n     */\n    private String removeBrackets(String placeholder) {\n        return placeholder.replaceAll(\"\\\\{+|\\\\}+\", \"\");\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/mapper/MqttKafkaMapper.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.regex.Pattern;\n", "import java.util.regex.Pattern;\n\n/**\n * Interface for the MqttKafkaMapper. The MqttKafkaMapper is responsible for mapping MQTT topics to Kafka topics.\n */\npublic abstract class MqttKafkaMapper {\n\n    // default Kafka topic. Used when no mapping rule matches the mqtt topic.\n    public static final String DEFAULT_KAFKA_TOPIC = \"messages_default\";\n\n    // MQTT topic separator", "    public static final String DEFAULT_KAFKA_TOPIC = \"messages_default\";\n\n    // MQTT topic separator\n    public static final String MQTT_TOPIC_SEPARATOR = \"/\";\n\n    protected final List<MappingRule> rules;\n    protected final List<Pattern> patterns = new ArrayList<>();\n    protected final Pattern placeholderPattern;\n\n    /**\n     * Constructor\n     *\n     * @param rules the list of mapping rules.\n     * @param placeholderPattern the pattern used to find placeholders.\n     * @see MappingRule\n     */\n    protected MqttKafkaMapper(List<MappingRule> rules, Pattern placeholderPattern) {\n        this.rules = rules;\n        this.placeholderPattern = placeholderPattern;\n        this.buildOrCompilePatterns();\n    }\n\n    /**\n     * Maps an MQTT topic to a Kafka topic. The topic is mapped according to the defined mapping rules.\n     *\n     * @param mqttTopic\n     * @return a MappingResult object containing the mapped Kafka topic and Kafka key.\n     */", "    public abstract MappingResult map(String mqttTopic);\n\n    /**\n     * Helper method for Building the regex expressions for the mapping rules.\n     */\n    protected abstract void buildOrCompilePatterns();\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/mapper/MappingRulesLoader.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\nimport com.fasterxml.jackson.databind.ObjectMapper;\n\nimport java.io.IOException;\nimport java.nio.file.Path;", "import java.io.IOException;\nimport java.nio.file.Path;\nimport java.util.List;\n\n/**\n * Helper class to Load the rules from the configuration file\n */\npublic class MappingRulesLoader {\n\n    private static final MappingRulesLoader INSTANCE = new MappingRulesLoader();\n    // path of the topic mapping rule file\n    private String mapperRuleFilePath;\n    private boolean initialized = false;\n\n    /**\n     * Initialize the MappingRulesLoader\n     *\n     * @param mapperRuleFilePath the path of the mapper rule file\n     */", "    public void init(String mapperRuleFilePath) {\n        if (!initialized) {\n            this.mapperRuleFilePath = mapperRuleFilePath;\n            initialized = true;\n        } else {\n            throw new IllegalStateException(\"MappingRulesLoader is already initialized\");\n        }\n    }\n\n    /**\n     * Private constructor\n     */\n    private MappingRulesLoader() {\n    }\n\n    /**\n     * Get the singleton instance of the MappingRulesLoader\n     *\n     * @return the singleton instance of the MappingRulesLoader\n     */", "    public static MappingRulesLoader getInstance() {\n        return INSTANCE;\n    }\n\n    /**\n     * Load the mapping rules from the file system and create the mapper instance.\n     *\n     * @see MqttKafkaMapper\n     */\n    public List<MappingRule> loadRules() throws IOException {\n", "        if (!initialized) {\n            throw new IllegalStateException(\"MappingRulesLoader is not initialized\");\n        }\n\n        ObjectMapper mapper = new ObjectMapper();\n\n        // deserialize the JSON array to a list of MappingRule objects\n        return mapper.readValue(Path.of(this.mapperRuleFilePath).toFile(), mapper.getTypeFactory().constructCollectionType(List.class, MappingRule.class));\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/mapper/MappingRule.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\nimport com.fasterxml.jackson.annotation.JsonProperty;\n\n/**\n * Represents a Mapping Rule in the Topic Mapping Rules(ToMaR). Mapping rules are used to define how MQTT topics should be mapped to Kafka topics, and additionally define the record key.", "/**\n * Represents a Mapping Rule in the Topic Mapping Rules(ToMaR). Mapping rules are used to define how MQTT topics should be mapped to Kafka topics, and additionally define the record key.\n * E.g.: a valid mapping rule would look like this in the ToMaR file:\n * {\n *      \"mqttTopic\": \"sensors/(^[0-9])/type/([^/]+)/data\",\n *      \"kafkaTopic\": \"sensors_$1_data\",\n *      \"kafkaKey\": \"sensor_$2\"\n * }\n * and like this in the MappingRule class:\n * MappingRule(mqttTopicPattern=sensors/(^[0-9])/type/([^/]+)/data, kafkaTopicTemplate=sensors_$1_data, kafkaKey=sensor_$2)", " * and like this in the MappingRule class:\n * MappingRule(mqttTopicPattern=sensors/(^[0-9])/type/([^/]+)/data, kafkaTopicTemplate=sensors_$1_data, kafkaKey=sensor_$2)\n */\npublic class MappingRule {\n    @JsonProperty(\"mqttTopic\")\n    private String mqttTopicPattern;\n    @JsonProperty(\"kafkaTopic\")\n    private String kafkaTopicTemplate;\n\n    @JsonProperty(\"kafkaKey\")\n    private String kafkaKeyTemplate;\n\n    /**\n     * Default constructor for MappingRule. Used for deserialization.\n     */\n    public MappingRule() {\n    }\n\n    /**\n     * Constructor for MappingRule.\n     *\n     * @param mqttTopicPattern   the mqtt topic pattern.\n     * @param kafkaTopicTemplate the Kafka topic template.\n     * @param kafkaKeyTemplate   the Kafka key template.\n     */\n    public MappingRule(String mqttTopicPattern, String kafkaTopicTemplate, String kafkaKeyTemplate) {\n        this.mqttTopicPattern = mqttTopicPattern;\n        this.kafkaTopicTemplate = kafkaTopicTemplate;\n        this.kafkaKeyTemplate = kafkaKeyTemplate;\n    }\n\n    /**\n     * Get the Kafka topic template.\n     *\n     * @return the Kafka topic template.\n     */", "    public String getKafkaTopicTemplate() {\n        return kafkaTopicTemplate;\n    }\n\n    /**\n     * Get the mqtt topic pattern.\n     *\n     * @return the mqtt topic pattern.\n     */\n    public String getMqttTopicPattern() {\n        return mqttTopicPattern;\n    }\n\n    /**\n     * Get the record key template.\n     *\n     * @return the record key template.\n     */", "    public String getMqttTopicPattern() {\n        return mqttTopicPattern;\n    }\n\n    /**\n     * Get the record key template.\n     *\n     * @return the record key template.\n     */\n    public String getKafkaKeyTemplate() {\n        return kafkaKeyTemplate;\n    }\n\n    /**\n     * String representation of a MappingRule.\n     *\n     * @return a string containing properties of a MappingRule.\n     */\n    @Override", "    public String getKafkaKeyTemplate() {\n        return kafkaKeyTemplate;\n    }\n\n    /**\n     * String representation of a MappingRule.\n     *\n     * @return a string containing properties of a MappingRule.\n     */\n    @Override\n    public String toString() {\n        return \"MappingRule(\" +\n                \"mqttTopicPattern= \" + this.mqttTopicPattern +\n                \", kafkaTopicTemplate=\" + this.kafkaTopicTemplate +\n                \", kafkaKeyTemplate=\" + this.kafkaKeyTemplate +\n                \")\";\n    }\n}\n", "    public String toString() {\n        return \"MappingRule(\" +\n                \"mqttTopicPattern= \" + this.mqttTopicPattern +\n                \", kafkaTopicTemplate=\" + this.kafkaTopicTemplate +\n                \", kafkaKeyTemplate=\" + this.kafkaKeyTemplate +\n                \")\";\n    }\n}\n"]}
{"filename": "src/main/java/io/strimzi/kafka/bridge/mqtt/mapper/MappingResult.java", "chunked_list": ["/*\n * Copyright Strimzi authors.\n * License: Apache License 2.0 (see the file LICENSE or http://apache.org/licenses/LICENSE-2.0.html).\n */\npackage io.strimzi.kafka.bridge.mqtt.mapper;\n\n/**\n * Represents the result of a mapping operation.\n * It contains the mapped Kafka topic and the Kafka key.\n *", " * It contains the mapped Kafka topic and the Kafka key.\n *\n * @param kafkaTopic the mapped Kafka topic.\n * @param kafkaKey   the Kafka key.\n */\npublic record MappingResult(String kafkaTopic, String kafkaKey) {\n\n    @Override\n    public String toString() {\n        return \"MappingResult(\" +\n                \"kafkaTopic=\" + kafkaTopic +\n                \", kafkaKey=\" + kafkaKey +\n                \")\";\n    }\n}\n", "    public String toString() {\n        return \"MappingResult(\" +\n                \"kafkaTopic=\" + kafkaTopic +\n                \", kafkaKey=\" + kafkaKey +\n                \")\";\n    }\n}\n"]}
