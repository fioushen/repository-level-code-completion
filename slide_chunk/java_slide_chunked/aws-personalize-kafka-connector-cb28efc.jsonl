{"filename": "src/test/java/com/aws/converter/DataConverterTest.java", "chunked_list": ["package com.aws.converter;\n\nimport com.amazonaws.services.personalizeevents.model.Event;\nimport com.amazonaws.services.personalizeevents.model.Item;\nimport com.amazonaws.services.personalizeevents.model.User;\nimport com.aws.util.Constants;\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.Test;", "import org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.Test;\n\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class DataConverterTest {\n\n    @Test\n    public void testEventDataConverter(){\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForEventData);\n        Map<String, String> additionalValues = new HashMap<>();\n        Event eventInfo = DataConverter.convertSinkRecordToEventInfo(sinkRecord, additionalValues);\n        Assertions.assertTrue(eventInfo.getEventType().equals(TestConstants.TEST_EVENT_TYPE_VALUE));\n        Assertions.assertTrue(additionalValues.get(Constants.FIELD_SESSION_ID).equals(TestConstants.TEST_SESSION_ID_VALUE));\n        Assertions.assertTrue(additionalValues.get(Constants.FIELD_USER_ID).equals(TestConstants.TEST_USER_ID_VALUE));\n        Assertions.assertTrue(eventInfo.getItemId().equals(TestConstants.TEST_ITEM_ID_VALUE));\n        Assertions.assertTrue(eventInfo.getSentAt().getTime() == TestConstants.TEST_EVENT_TIME_VALUE);\n    }\n\n    @Test", "    public void testEventDataConverter(){\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForEventData);\n        Map<String, String> additionalValues = new HashMap<>();\n        Event eventInfo = DataConverter.convertSinkRecordToEventInfo(sinkRecord, additionalValues);\n        Assertions.assertTrue(eventInfo.getEventType().equals(TestConstants.TEST_EVENT_TYPE_VALUE));\n        Assertions.assertTrue(additionalValues.get(Constants.FIELD_SESSION_ID).equals(TestConstants.TEST_SESSION_ID_VALUE));\n        Assertions.assertTrue(additionalValues.get(Constants.FIELD_USER_ID).equals(TestConstants.TEST_USER_ID_VALUE));\n        Assertions.assertTrue(eventInfo.getItemId().equals(TestConstants.TEST_ITEM_ID_VALUE));\n        Assertions.assertTrue(eventInfo.getSentAt().getTime() == TestConstants.TEST_EVENT_TIME_VALUE);\n    }\n\n    @Test", "    public void testItemDataConverter(){\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForItemData);\n        Item item = DataConverter.convertSinkRecordToItemInfo(sinkRecord);\n        Assertions.assertTrue(item.getItemId().equals(TestConstants.TEST_ITEM_ID_VALUE));\n        Assertions.assertTrue(item.getProperties().equals(TestConstants.TEST_PROPERTIES_VALUE));\n\n    }\n\n    @Test\n    public void testUserDataConverter(){\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForUserData);\n        User user = DataConverter.convertSinkRecordToUserInfo(sinkRecord);\n        Assertions.assertTrue(user.getUserId().equals(TestConstants.TEST_USER_ID_VALUE));\n        Assertions.assertTrue(user.getProperties().equals(TestConstants.TEST_PROPERTIES_VALUE));\n    }\n\n    private SinkRecord createRecord(Map<String, Object> valueMap) {\n        return new SinkRecord(\"topic\", 0, null, null, null, valueMap, 1);\n    }\n\n}\n", "    public void testUserDataConverter(){\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForUserData);\n        User user = DataConverter.convertSinkRecordToUserInfo(sinkRecord);\n        Assertions.assertTrue(user.getUserId().equals(TestConstants.TEST_USER_ID_VALUE));\n        Assertions.assertTrue(user.getProperties().equals(TestConstants.TEST_PROPERTIES_VALUE));\n    }\n\n    private SinkRecord createRecord(Map<String, Object> valueMap) {\n        return new SinkRecord(\"topic\", 0, null, null, null, valueMap, 1);\n    }\n\n}\n"]}
{"filename": "src/test/java/com/aws/task/PersonalizeSinkTaskTest.java", "chunked_list": ["package com.aws.task;\n\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.connect.sink.ErrantRecordReporter;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.apache.kafka.connect.sink.SinkTaskContext;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.Test;\n", "import org.junit.jupiter.api.Test;\n\nimport java.util.Collections;\nimport java.util.Map;\n\nimport static org.mockito.Mockito.mock;\nimport static org.mockito.Mockito.when;\npublic class PersonalizeSinkTaskTest {\n\n    private final static Map<String, Object> VALUE = TestConstants.sampleValsForEventData;\n    private final static String KEY = \"key\";\n\n    @Test\n    void testTask() {\n        final PersonlizeSinkTask task = createSinkTask();\n        Assertions.assertDoesNotThrow(() -> task.start(TestConstants.configVals));\n        Assertions.assertDoesNotThrow(() -> task.put(Collections.singletonList(createRecord())));\n        Assertions.assertDoesNotThrow(() -> task.stop());\n    }\n\n    @Test\n    void testValidConfig(){\n        final PersonlizeSinkTask task = createSinkTask();\n        Assertions.assertDoesNotThrow(() ->task.start(TestConstants.configVals));\n        Assertions.assertDoesNotThrow(() -> task.stop());\n    }\n\n    @Test\n    void testInValidConfig(){\n        final PersonlizeSinkTask task = createSinkTask();\n        Assertions.assertThrows(ConfigException.class, () ->task.start(TestConstants.invalidConfigVals));\n        Assertions.assertDoesNotThrow(() -> task.stop());\n    }\n\n    private SinkRecord createRecord() {\n        return new SinkRecord(\"topic\", 0, null, KEY, null, VALUE, 1);\n    }\n\n    private PersonlizeSinkTask createSinkTask() {\n        final PersonlizeSinkTask sinkTask = new PersonlizeSinkTask();\n        SinkTaskContext mockContext = mock(SinkTaskContext.class);\n        ErrantRecordReporter reporter = mock(ErrantRecordReporter.class);\n        when(mockContext.errantRecordReporter()).thenReturn(reporter);\n        sinkTask.initialize(mockContext);\n        return sinkTask;\n    }\n}\n"]}
{"filename": "src/test/java/com/aws/transfrom/CombineFieldsToJSONTransformationTest.java", "chunked_list": ["package com.aws.transfrom;\n\nimport com.aws.transform.CombineFieldsToJSONTransformation;\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.connect.connector.ConnectRecord;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.Test;\n", "import org.junit.jupiter.api.Test;\n\nimport java.util.Map;\n\nimport static org.junit.jupiter.api.Assertions.assertFalse;\nimport static org.junit.jupiter.api.Assertions.assertTrue;\n\npublic class CombineFieldsToJSONTransformationTest {\n\n    @Test\n    public void testValidTransformationConfiguration(){\n        CombineFieldsToJSONTransformation.Value value = new CombineFieldsToJSONTransformation.Value();\n        Assertions.assertDoesNotThrow(() -> value.configure(TestConstants.validTransformationConfig));\n    }\n    @Test", "    public void testValidTransformationConfiguration(){\n        CombineFieldsToJSONTransformation.Value value = new CombineFieldsToJSONTransformation.Value();\n        Assertions.assertDoesNotThrow(() -> value.configure(TestConstants.validTransformationConfig));\n    }\n    @Test\n    public void testInValidTransformationConfiguration(){\n        CombineFieldsToJSONTransformation.Value value = new CombineFieldsToJSONTransformation.Value();\n        Assertions.assertThrows(ConfigException.class, () -> value.configure(TestConstants.inValidTransformationConfig));\n    }\n\n    @Test", "    public void testValueTransformation(){\n        CombineFieldsToJSONTransformation.Value value = new CombineFieldsToJSONTransformation.Value();\n        value.configure(TestConstants.validTransformationConfig);\n        ConnectRecord record = value.apply(createRecord(null, TestConstants.sampleValsForTransformation));\n        Map<String, Object> valueMap = (Map<String, Object>) record.value();\n        assertFalse(valueMap.containsKey(TestConstants.FIELD_1));\n        assertFalse(valueMap.containsKey(TestConstants.FIELD_2));\n        assertTrue(valueMap.containsKey(CombineFieldsToJSONTransformation.DEFAULT_FIELD_NAME));\n        assertTrue(valueMap.get(CombineFieldsToJSONTransformation.DEFAULT_FIELD_NAME).equals(TestConstants.PROPERTIES_FIELD_WITH_FIELDS));\n\n    }\n\n    @Test", "    public void testKeyTransformation(){\n        CombineFieldsToJSONTransformation.Key key = new CombineFieldsToJSONTransformation.Key();\n        key.configure(TestConstants.validTransformationConfig);\n        ConnectRecord record = key.apply(createRecord(TestConstants.sampleValsForTransformation, null));\n        Map<String, Object> keyMap = (Map<String, Object>) record.key();\n        assertFalse(keyMap.containsKey(TestConstants.FIELD_1));\n        assertFalse(keyMap.containsKey(TestConstants.FIELD_2));\n        assertTrue(keyMap.containsKey(CombineFieldsToJSONTransformation.DEFAULT_FIELD_NAME));\n        assertTrue(keyMap.get(CombineFieldsToJSONTransformation.DEFAULT_FIELD_NAME).equals(TestConstants.PROPERTIES_FIELD_WITH_FIELDS));\n\n    }\n\n    private ConnectRecord<SinkRecord> createRecord(Object key, Object value) {\n        return new SinkRecord(\"topic\", 0, null, key, null, value, 1);\n    }\n\n}\n"]}
{"filename": "src/test/java/com/aws/connector/PersonalizeSinkConnectorTest.java", "chunked_list": ["package com.aws.connector;\n\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.connect.connector.Connector;\nimport org.apache.kafka.connect.connector.ConnectorContext;\nimport org.apache.kafka.connect.sink.SinkConnector;\nimport org.junit.jupiter.api.Test;\nimport org.mockito.Mock;\n\nimport static com.aws.util.TestConstants.configVals;", "\nimport static com.aws.util.TestConstants.configVals;\nimport static com.aws.util.Constants.AMAZON_PERSONALIZE_EVENT_TRACKING_ID;\nimport static com.aws.util.Constants.AWS_REGION_NAME;\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.data.MapEntry.entry;\nimport static org.junit.jupiter.api.Assertions.*;\n\npublic class PersonalizeSinkConnectorTest {\n\n    @Mock\n    private ConnectorContext context;\n\n    @Test\n    void testStartAndStop() {\n        final PersonalizeSinkConnector connector = createConnector();\n        connector.start(configVals);\n        assertThat(connector.taskConfigs(1)).hasSize(1);\n        assertThat(connector.taskConfigs(10)).hasSize(10);\n    }\n\n    @Test\n    void testConfig() {\n        final PersonalizeSinkConnector connector = createConnector();\n        connector.start(configVals);\n        assertThat(connector.taskConfigs(1).get(0)).contains(\n                entry(AWS_REGION_NAME, TestConstants.TEST_AWS_REGION),\n                entry(AMAZON_PERSONALIZE_EVENT_TRACKING_ID, TestConstants.TEST_EVENT_TRACKING_ID));\n    }\n\n    @Test", "public class PersonalizeSinkConnectorTest {\n\n    @Mock\n    private ConnectorContext context;\n\n    @Test\n    void testStartAndStop() {\n        final PersonalizeSinkConnector connector = createConnector();\n        connector.start(configVals);\n        assertThat(connector.taskConfigs(1)).hasSize(1);\n        assertThat(connector.taskConfigs(10)).hasSize(10);\n    }\n\n    @Test\n    void testConfig() {\n        final PersonalizeSinkConnector connector = createConnector();\n        connector.start(configVals);\n        assertThat(connector.taskConfigs(1).get(0)).contains(\n                entry(AWS_REGION_NAME, TestConstants.TEST_AWS_REGION),\n                entry(AMAZON_PERSONALIZE_EVENT_TRACKING_ID, TestConstants.TEST_EVENT_TRACKING_ID));\n    }\n\n    @Test", "    public void testVersion() {\n        String version = new PersonalizeSinkConnector().version();\n        System.out.println(version);\n        assertNotNull(version);\n        assertFalse(version.isEmpty());\n    }\n\n    @Test\n    public void connectorType() {\n        Connector connector = new PersonalizeSinkConnector();\n        assertTrue(SinkConnector.class.isAssignableFrom(connector.getClass()));\n    }\n\n    private PersonalizeSinkConnector createConnector() {\n        final PersonalizeSinkConnector connector = new PersonalizeSinkConnector();\n        connector.initialize(context);\n        return connector;\n    }\n}\n", "    public void connectorType() {\n        Connector connector = new PersonalizeSinkConnector();\n        assertTrue(SinkConnector.class.isAssignableFrom(connector.getClass()));\n    }\n\n    private PersonalizeSinkConnector createConnector() {\n        final PersonalizeSinkConnector connector = new PersonalizeSinkConnector();\n        connector.initialize(context);\n        return connector;\n    }\n}\n"]}
{"filename": "src/test/java/com/aws/config/PersonalizeSinkConfigTest.java", "chunked_list": ["package com.aws.config;\n\nimport com.aws.util.Constants;\nimport com.aws.util.TestConstants;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.Test;\n\npublic class PersonalizeSinkConfigTest {\n\n    @Test", "\npublic class PersonalizeSinkConfigTest {\n\n    @Test\n    public void testEventConfig(){\n        PersonalizeSinkConfig config = new PersonalizeSinkConfig(TestConstants.configVals);\n        Assertions.assertTrue(StringUtils.isNotBlank(config.getName()));\n        Assertions.assertTrue(config.getName().equals(Constants.DEFAULT_CONNECTOR_NAME));\n        Assertions.assertTrue(config.getDataType().equals(Constants.PUT_EVENTS_REQUEST_TYPE));\n        Assertions.assertTrue(config.getEventTrackingId().equals(TestConstants.TEST_EVENT_TRACKING_ID));\n        Assertions.assertTrue(config.getAwsRegionName().equals(TestConstants.TEST_AWS_REGION));\n        Assertions.assertFalse(config.getDataSetArn().equals(TestConstants.VALID_ITEM_DATASET_ARN));\n    }\n\n    @Test", "    public void testItemConfig(){\n        PersonalizeSinkConfig config = new PersonalizeSinkConfig(TestConstants.itemconfigVals);\n        Assertions.assertTrue(config.getDataType().equals(Constants.PUT_ITEMS_REQUEST_TYPE));\n        Assertions.assertFalse(StringUtils.isNotBlank(config.getEventTrackingId()));\n        Assertions.assertFalse(config.getEventTrackingId().equals(TestConstants.TEST_EVENT_TRACKING_ID));\n        Assertions.assertTrue(config.getAwsRegionName().equals(TestConstants.TEST_AWS_REGION));\n        Assertions.assertTrue(config.getDataSetArn().equals(TestConstants.VALID_ITEM_DATASET_ARN));\n    }\n\n    @Test\n    public void testUserConfig(){\n        PersonalizeSinkConfig config = new PersonalizeSinkConfig(TestConstants.userconfigVals);\n        Assertions.assertTrue(config.getDataType().equals(Constants.PUT_USERS_REQUEST_TYPE));\n        Assertions.assertFalse(StringUtils.isNotBlank(config.getEventTrackingId()));\n        Assertions.assertFalse(config.getEventTrackingId().equals(TestConstants.TEST_EVENT_TRACKING_ID));\n        Assertions.assertTrue(config.getAwsRegionName().equals(TestConstants.TEST_AWS_REGION));\n        Assertions.assertTrue(config.getDataSetArn().equals(TestConstants.VALID_USER_DATASET_ARN));\n    }\n\n    @Test", "    public void testUserConfig(){\n        PersonalizeSinkConfig config = new PersonalizeSinkConfig(TestConstants.userconfigVals);\n        Assertions.assertTrue(config.getDataType().equals(Constants.PUT_USERS_REQUEST_TYPE));\n        Assertions.assertFalse(StringUtils.isNotBlank(config.getEventTrackingId()));\n        Assertions.assertFalse(config.getEventTrackingId().equals(TestConstants.TEST_EVENT_TRACKING_ID));\n        Assertions.assertTrue(config.getAwsRegionName().equals(TestConstants.TEST_AWS_REGION));\n        Assertions.assertTrue(config.getDataSetArn().equals(TestConstants.VALID_USER_DATASET_ARN));\n    }\n\n    @Test\n    public void testInvalidConfig(){\n        Assertions.assertThrows(ConfigException.class,() -> new PersonalizeSinkConfig(TestConstants.invalidConfigVals));\n    }\n}\n", "    public void testInvalidConfig(){\n        Assertions.assertThrows(ConfigException.class,() -> new PersonalizeSinkConfig(TestConstants.invalidConfigVals));\n    }\n}\n"]}
{"filename": "src/test/java/com/aws/config/validators/PersonalizeDataTypeValidatorTest.java", "chunked_list": ["package com.aws.config.validators;\n\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\npublic class PersonalizeDataTypeValidatorTest {\n\n    private static final String DATA_TYPE_CONFIG_NAME = \"amazon.personalize.data.type\";\n\n    private static PersonalizeDataTypeValidator personalizeDataTypeValidator;\n\n    @BeforeAll", "public class PersonalizeDataTypeValidatorTest {\n\n    private static final String DATA_TYPE_CONFIG_NAME = \"amazon.personalize.data.type\";\n\n    private static PersonalizeDataTypeValidator personalizeDataTypeValidator;\n\n    @BeforeAll\n    public static void setUp(){\n        personalizeDataTypeValidator=  new PersonalizeDataTypeValidator();\n    }\n    @Test", "    public void testValidUsersDataType(){\n        // Testing by passing valid User DataSet Type value\n        Assertions.assertDoesNotThrow(() -> personalizeDataTypeValidator.ensureValid(DATA_TYPE_CONFIG_NAME, TestConstants.VALID_USER_DATASET_TYPE));\n    }\n\n    @Test\n    public void testValidItemsDataType(){\n        // Testing by passing valid Items DataSet Type value\n        Assertions.assertDoesNotThrow(() -> personalizeDataTypeValidator.ensureValid(DATA_TYPE_CONFIG_NAME, TestConstants.VALID_ITEMS_DATASET_TYPE));\n    }\n    @Test", "    public void testValidEventsDataType(){\n        // Testing by passing valid Event DataSet Type value\n        Assertions.assertDoesNotThrow(() -> personalizeDataTypeValidator.ensureValid(DATA_TYPE_CONFIG_NAME, TestConstants.VALID_EVENT_DATASET_TYPE));\n    }\n    @Test\n    public void testInvalidDataType(){\n        // Testing by passing Invalid DataSet Type value which is not supported\n        Assertions.assertThrows(ConfigException.class,() -> personalizeDataTypeValidator.ensureValid(DATA_TYPE_CONFIG_NAME, TestConstants.INVALID_DATASET_TYPE));\n    }\n\n}\n"]}
{"filename": "src/test/java/com/aws/config/validators/ArnValidatorTest.java", "chunked_list": ["package com.aws.config.validators;\n\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\npublic class ArnValidatorTest {\n\n    private static final String ARN_CONFIG_NAME = \"amazon.personalize.dataset.arn\";\n    private static ArnValidator arnValidator;\n\n    @BeforeAll", "public class ArnValidatorTest {\n\n    private static final String ARN_CONFIG_NAME = \"amazon.personalize.dataset.arn\";\n    private static ArnValidator arnValidator;\n\n    @BeforeAll\n    public static void setUp(){\n        arnValidator=  new ArnValidator();\n    }\n    @Test\n    public void testValidItemsDataSetArn(){\n        //Testing with passing valid Item DataSet ARN\n        Assertions.assertDoesNotThrow(() -> arnValidator.ensureValid(ARN_CONFIG_NAME, TestConstants.VALID_ITEM_DATASET_ARN));\n    }\n\n    @Test", "    public void testValidItemsDataSetArn(){\n        //Testing with passing valid Item DataSet ARN\n        Assertions.assertDoesNotThrow(() -> arnValidator.ensureValid(ARN_CONFIG_NAME, TestConstants.VALID_ITEM_DATASET_ARN));\n    }\n\n    @Test\n    public void testValidUsersDataSetArn(){\n        //Testing with passing valid User DataSet ARN\n        Assertions.assertDoesNotThrow(() -> arnValidator.ensureValid(ARN_CONFIG_NAME, TestConstants.VALID_USER_DATASET_ARN));\n    }\n\n    @Test", "    public void testInValidArnWithService(){\n        //Testing with passing invalid service name\n        Assertions.assertThrows(ConfigException.class ,() ->arnValidator.ensureValid(ARN_CONFIG_NAME, TestConstants.INVALID_ARN_INVALID_SERVICE));\n    }\n\n    @Test\n    public void testInValidArnWithResource(){\n        //Testing with passing invalid resource name\n        Assertions.assertThrows(ConfigException.class ,() ->arnValidator.ensureValid(ARN_CONFIG_NAME, TestConstants.INVALID_ARN_INVALID_RESOURCE));\n    }\n\n    @Test", "    public void testInValidArnWithDataSetType(){\n        //Testing with passing invalid dataset name\n        Assertions.assertThrows(ConfigException.class ,() ->arnValidator.ensureValid(ARN_CONFIG_NAME, TestConstants.INVALID_ARN_INVALID_DATASET));\n    }\n}\n"]}
{"filename": "src/test/java/com/aws/config/validators/PersonalizeCredentialsProviderValidatorTest.java", "chunked_list": ["package com.aws.config.validators;\n\nimport com.aws.auth.AssumeRoleCredentialsProvider;\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.junit.jupiter.api.Assertions;\nimport org.junit.jupiter.api.BeforeAll;\nimport org.junit.jupiter.api.Test;\n\npublic class PersonalizeCredentialsProviderValidatorTest {\n\n    private static final String CREDENTIALS_PROVIDER_CONFIG_NAME = \"amazon.personalize.credentials.provider.class\";\n    private static PersonalizeCredentialsProviderValidator personalizeCredentialsProviderValidator;\n\n    @BeforeAll", "\npublic class PersonalizeCredentialsProviderValidatorTest {\n\n    private static final String CREDENTIALS_PROVIDER_CONFIG_NAME = \"amazon.personalize.credentials.provider.class\";\n    private static PersonalizeCredentialsProviderValidator personalizeCredentialsProviderValidator;\n\n    @BeforeAll\n    public static void setUp() {\n        personalizeCredentialsProviderValidator = new PersonalizeCredentialsProviderValidator();\n    }\n\n\n    @Test", "    public void testValidClass() {\n        //Testing with valid class with implements AWSCredentialsProvider\n        Assertions.assertDoesNotThrow(() -> personalizeCredentialsProviderValidator.ensureValid(CREDENTIALS_PROVIDER_CONFIG_NAME, AssumeRoleCredentialsProvider.class));\n    }\n\n    @Test\n    public void testInValidClass() {\n        //Testing with invalid class by passing String constant instead of class which implements AWSCredentialsProvider\n        Assertions.assertThrows(ConfigException.class, () -> personalizeCredentialsProviderValidator.ensureValid(CREDENTIALS_PROVIDER_CONFIG_NAME, TestConstants.INVALID_DATASET_TYPE));\n    }\n}\n"]}
{"filename": "src/test/java/com/aws/writer/DataWriterTest.java", "chunked_list": ["package com.aws.writer;\n\nimport com.amazonaws.services.personalizeevents.AmazonPersonalizeEvents;\nimport com.amazonaws.services.personalizeevents.model.*;\nimport com.aws.config.PersonalizeSinkConfig;\nimport com.aws.util.TestConstants;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.junit.jupiter.api.Test;\nimport org.junit.jupiter.api.extension.ExtendWith;\nimport org.mockito.ArgumentCaptor;", "import org.junit.jupiter.api.extension.ExtendWith;\nimport org.mockito.ArgumentCaptor;\nimport org.mockito.Captor;\nimport org.mockito.Mock;\nimport org.mockito.junit.jupiter.MockitoExtension;\n\nimport java.util.Collections;\n\nimport static org.junit.jupiter.api.Assertions.assertTrue;\nimport static org.mockito.Mockito.verify;", "import static org.junit.jupiter.api.Assertions.assertTrue;\nimport static org.mockito.Mockito.verify;\n@ExtendWith(MockitoExtension.class)\npublic class DataWriterTest {\n\n    @Mock\n    AmazonPersonalizeEvents client;\n    @Captor\n    ArgumentCaptor<PutEventsRequest> putEventsRequestArgumentCaptor;\n    @Captor\n    ArgumentCaptor<PutItemsRequest> putItemsRequestArgumentCaptor;\n    @Captor\n    ArgumentCaptor<PutUsersRequest> putUsersRequestArgumentCaptor;\n\n    @Test", "    public void testWriterForEventData() throws Exception {\n        PersonalizeSinkConfig config = new PersonalizeSinkConfig(TestConstants.configVals);\n        DataWriter dataWriter = new DataWriter(config, client);\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForEventData);\n        dataWriter.write(Collections.singletonList(sinkRecord));\n        verify(client).putEvents(putEventsRequestArgumentCaptor.capture());\n        PutEventsRequest putEventsRequest = putEventsRequestArgumentCaptor.getValue();\n        assertTrue(putEventsRequest.getSessionId().equals(TestConstants.TEST_SESSION_ID_VALUE));\n        assertTrue(putEventsRequest.getUserId().equals(TestConstants.TEST_USER_ID_VALUE));\n        assertTrue(!putEventsRequest.getEventList().isEmpty());\n        Event event = putEventsRequest.getEventList().get(0);\n        assertTrue(event.getEventType().equals(TestConstants.TEST_EVENT_TYPE_VALUE));\n        assertTrue(event.getItemId().equals(TestConstants.TEST_ITEM_ID_VALUE));\n        assertTrue(event.getSentAt().getTime() == TestConstants.TEST_EVENT_TIME_VALUE);\n        assertTrue(putEventsRequest.getTrackingId().equals(TestConstants.TEST_EVENT_TRACKING_ID));\n    }\n\n    @Test", "    public void testWriterForItemData() throws Exception {\n        PersonalizeSinkConfig config = new PersonalizeSinkConfig(TestConstants.itemconfigVals);\n        DataWriter dataWriter = new DataWriter(config, client);\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForItemData);\n        dataWriter.write(Collections.singletonList(sinkRecord));\n        verify(client).putItems(putItemsRequestArgumentCaptor.capture());\n        PutItemsRequest putItemsRequest = putItemsRequestArgumentCaptor.getValue();\n        assertTrue(putItemsRequest.getDatasetArn().equals(TestConstants.VALID_ITEM_DATASET_ARN));\n        assertTrue(!putItemsRequest.getItems().isEmpty());\n        Item item = putItemsRequest.getItems().get(0);\n        assertTrue(item.getItemId().equals(TestConstants.TEST_ITEM_ID_VALUE));\n        assertTrue(item.getProperties().equals(TestConstants.TEST_PROPERTIES_VALUE));\n    }\n\n    @Test", "    public void testWriterForUserData() throws Exception {\n        PersonalizeSinkConfig config = new PersonalizeSinkConfig(TestConstants.userconfigVals);\n        DataWriter dataWriter = new DataWriter(config, client);\n        SinkRecord sinkRecord = createRecord(TestConstants.sampleValsForUserData);\n        dataWriter.write(Collections.singletonList(sinkRecord));\n        verify(client).putUsers(putUsersRequestArgumentCaptor.capture());\n        PutUsersRequest putUsersRequest = putUsersRequestArgumentCaptor.getValue();\n        assertTrue(putUsersRequest.getDatasetArn().equals(TestConstants.VALID_USER_DATASET_ARN));\n        assertTrue(!putUsersRequest.getUsers().isEmpty());\n        User user = putUsersRequest.getUsers().get(0);\n        assertTrue(user.getUserId().equals(TestConstants.TEST_USER_ID_VALUE));\n        assertTrue(user.getProperties().equals(TestConstants.TEST_PROPERTIES_VALUE));\n    }\n\n    private SinkRecord createRecord(Object value) {\n        return new SinkRecord(\"topic\", 0, null, null, null, value, 1);\n    }\n\n}\n"]}
{"filename": "src/test/java/com/aws/util/TestConstants.java", "chunked_list": ["package com.aws.util;\n\nimport com.aws.transform.CombineFieldsToJSONTransformation;\n\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class TestConstants {\n", "\npublic class TestConstants {\n\n    public static final Map<String, String> configVals;\n\n    public static final Map<String, String> itemconfigVals;\n\n    public static final Map<String, String> userconfigVals;\n\n    public static final Map<String, String> invalidConfigVals;\n    public static final Map<String, Object> validTransformationConfig;", "    public static final Map<String, String> invalidConfigVals;\n    public static final Map<String, Object> validTransformationConfig;\n    public static final Map<String, Object> inValidTransformationConfig;\n    public static final Map<String, Object> sampleValsForEventData;\n\n    public static final Map<String, Object> sampleValsForUserData;\n\n    public static final Map<String, Object> sampleValsForItemData;\n    public static final Map<String, Object> sampleValsForTransformation;\n    public static final String TEST_USER_ID_VALUE= \"test-user1\";\n    public static final String TEST_ITEM_ID_VALUE = \"test-item1\";", "    public static final Map<String, Object> sampleValsForTransformation;\n    public static final String TEST_USER_ID_VALUE= \"test-user1\";\n    public static final String TEST_ITEM_ID_VALUE = \"test-item1\";\n    public static final String TEST_SESSION_ID_VALUE = \"test-session1\";\n    public static final String TEST_EVENT_TYPE_VALUE = \"test-event1\";\n    public static final Long TEST_EVENT_TIME_VALUE = 123456789L;\n    public static final String TEST_EVENT_TRACKING_ID =\"test-tracking-id\";\n    public static final String TEST_AWS_REGION =\"test-region\";\n    public static final String VALID_ITEM_DATASET_ARN = \"arn:aws:personalize:<REGION>:<ACCOUNT_ID>:dataset/<DATASET_GROUP_NAME>/ITEMS\";\n    public static final String VALID_USER_DATASET_ARN = \"arn:aws:personalize:<REGION>:<ACCOUNT_ID>:dataset/<DATASET_GROUP_NAME>/USERS\";\n    public static final String TEST_PROPERTIES_VALUE = \"{}\";", "    public static final String VALID_ITEM_DATASET_ARN = \"arn:aws:personalize:<REGION>:<ACCOUNT_ID>:dataset/<DATASET_GROUP_NAME>/ITEMS\";\n    public static final String VALID_USER_DATASET_ARN = \"arn:aws:personalize:<REGION>:<ACCOUNT_ID>:dataset/<DATASET_GROUP_NAME>/USERS\";\n    public static final String TEST_PROPERTIES_VALUE = \"{}\";\n    public static final String FIELD_1 =\"field1\";\n    public static final String FIELD_2 =\"field2\";\n    public static final String PROPERTIES_FIELD_WITH_FIELDS = \"{\\\"field1\\\":\\\"field1\\\",\\\"field2\\\":\\\"field2\\\"}\";\n\n    static {\n        Map<String, String> configMap = new HashMap<String, String>();\n        configMap.put(Constants.AMAZON_PERSONALIZE_EVENT_TRACKING_ID,TEST_EVENT_TRACKING_ID);\n        configMap.put(Constants.AWS_REGION_NAME,TEST_AWS_REGION);\n        configMap.put(Constants.AMAZON_PERSONALIZE_DATA_TYPE,Constants.PUT_EVENTS_REQUEST_TYPE);\n        configVals = Collections.unmodifiableMap(configMap);\n\n        configMap = new HashMap<String, String>();\n        configMap.put(Constants.AMAZON_PERSONALIZE_DATASET_ARN, VALID_ITEM_DATASET_ARN);\n        configMap.put(Constants.AWS_REGION_NAME,TEST_AWS_REGION);\n        configMap.put(Constants.AMAZON_PERSONALIZE_DATA_TYPE,Constants.PUT_ITEMS_REQUEST_TYPE);\n        itemconfigVals = Collections.unmodifiableMap(configMap);\n\n        configMap = new HashMap<String, String>();\n        configMap.put(Constants.AMAZON_PERSONALIZE_DATASET_ARN, VALID_USER_DATASET_ARN);\n        configMap.put(Constants.AWS_REGION_NAME,TEST_AWS_REGION);\n        configMap.put(Constants.AMAZON_PERSONALIZE_DATA_TYPE,Constants.PUT_USERS_REQUEST_TYPE);\n        userconfigVals = Collections.unmodifiableMap(configMap);\n\n        Map transformConfig = new HashMap<String, Object>();\n        transformConfig.put(CombineFieldsToJSONTransformation.FIELDS_TO_INCLUDE, Arrays.asList(\"field1\", \"field2\"));\n        transformConfig.put(CombineFieldsToJSONTransformation.TARGET_FIELD_NAME, CombineFieldsToJSONTransformation.DEFAULT_FIELD_NAME);\n        validTransformationConfig = Collections.unmodifiableMap(transformConfig);\n\n        transformConfig = new HashMap<String, Object>();\n        transformConfig.put(Constants.AWS_SECRET_KEY, Arrays.asList(\"field1\", \"field2\"));\n        transformConfig.put(CombineFieldsToJSONTransformation.TARGET_FIELD_NAME, CombineFieldsToJSONTransformation.DEFAULT_FIELD_NAME);\n        inValidTransformationConfig = Collections.unmodifiableMap(transformConfig);\n\n        Map<String, Object> valuesMap = new HashMap<String, Object>();\n        valuesMap.put(Constants.FIELD_USER_ID, TEST_USER_ID_VALUE);\n        valuesMap.put(Constants.FIELD_ITEM_ID, TEST_ITEM_ID_VALUE);\n        valuesMap.put(Constants.FIELD_EVENT_TYPE,TEST_EVENT_TYPE_VALUE);\n        valuesMap.put(Constants.FIELD_SESSION_ID,TEST_SESSION_ID_VALUE);\n        valuesMap.put(Constants.FIELD_EVENT_TIME,TEST_EVENT_TIME_VALUE);\n        sampleValsForEventData = Collections.unmodifiableMap(valuesMap);\n\n        valuesMap = new HashMap<String, Object>();\n        valuesMap.put(Constants.FIELD_ITEM_ID,TEST_ITEM_ID_VALUE);\n        valuesMap.put(Constants.FIELD_PROPERTIES,TEST_PROPERTIES_VALUE);\n        sampleValsForItemData = Collections.unmodifiableMap(valuesMap);\n\n        valuesMap = new HashMap<String, Object>();\n        valuesMap.put(Constants.FIELD_USER_ID,TEST_USER_ID_VALUE);\n        valuesMap.put(Constants.FIELD_PROPERTIES,TEST_PROPERTIES_VALUE);\n        sampleValsForUserData = Collections.unmodifiableMap(valuesMap);\n\n        valuesMap = new HashMap<String, Object>();\n        valuesMap.put(Constants.FIELD_USER_ID,TEST_USER_ID_VALUE);\n        valuesMap.put(FIELD_1,FIELD_1);\n        valuesMap.put(FIELD_2,FIELD_2);\n        sampleValsForTransformation = Collections.unmodifiableMap(valuesMap);\n\n        Map<String, String> invalidConfigMap = new HashMap<String, String>();\n        invalidConfigMap.put(\"amazon.personalize.event.tracking.id.invalid\",\"test-tracking-id\");\n        invalidConfigMap.put(\"amazon.region.name\",\"test-region\");\n        invalidConfigMap.put(\"amazon.personalize.data.type\",\"invalidAction\");\n        invalidConfigVals = Collections.unmodifiableMap(invalidConfigMap);\n    }\n", "    public static final String INVALID_ARN_INVALID_SERVICE = \"arn:aws:ec2:<REGION>:<ACCOUNT_ID>:dataset/<DATASET_GROUP_NAME>/ITEMS\";\n    public static final String INVALID_ARN_INVALID_RESOURCE = \"arn:aws:personalize:<REGION>:<ACCOUNT_ID>:test/<DATASET_GROUP_NAME>/ITEMS\";\n    public static final String INVALID_ARN_INVALID_DATASET = \"arn:aws:personalize:<REGION>:<ACCOUNT_ID>:dataset/<DATASET_GROUP_NAME>/INTERACTIONS\";\n    public static final String VALID_USER_DATASET_TYPE = \"users\";\n    public static final String VALID_EVENT_DATASET_TYPE = \"events\";\n    public static final String VALID_ITEMS_DATASET_TYPE = \"items\";\n    public static final String INVALID_DATASET_TYPE = \"invalid\";\n\n}\n"]}
{"filename": "src/main/java/com/aws/converter/DataConverter.java", "chunked_list": ["package com.aws.converter;\n\nimport com.amazonaws.services.personalizeevents.model.Event;\nimport com.amazonaws.services.personalizeevents.model.Item;\nimport com.amazonaws.services.personalizeevents.model.MetricAttribution;\nimport com.amazonaws.services.personalizeevents.model.User;\nimport com.aws.util.Constants;\nimport org.apache.kafka.connect.data.Struct;\nimport org.apache.kafka.connect.errors.DataException;\nimport org.apache.kafka.connect.sink.SinkRecord;", "import org.apache.kafka.connect.errors.DataException;\nimport org.apache.kafka.connect.sink.SinkRecord;\n\nimport java.util.Collection;\nimport java.util.Date;\nimport java.util.Map;\n\n/*************\n * This class is used for Mapping Kafka records to Personalize Event Model Objects\n */\npublic class DataConverter {\n\n    /************\n     * This methods used to convert Sink Kafka Record into Event Model object.\n     * @param record\n     * @return\n     */", " * This class is used for Mapping Kafka records to Personalize Event Model Objects\n */\npublic class DataConverter {\n\n    /************\n     * This methods used to convert Sink Kafka Record into Event Model object.\n     * @param record\n     * @return\n     */\n    public static Event convertSinkRecordToEventInfo(SinkRecord record, Map<String, String> additionalValues) {\n        Event eventInfo = null;", "    public static Event convertSinkRecordToEventInfo(SinkRecord record, Map<String, String> additionalValues) {\n        Event eventInfo = null;\n        if(record.value() instanceof Struct)\n            eventInfo = prepareEventDataFromStruct((Struct) record.value(), additionalValues);\n        else\n            eventInfo = prepareEventDataFromMap((Map<String, Object>) record.value(), additionalValues);\n\n        return eventInfo;\n    }\n\n    private static Event prepareEventDataFromStruct(Struct recordValue, Map<String, String> additionalValues) {\n        Event eventInfo = new Event();\n        //Processing required fields in request", "        try {\n            eventInfo.setEventType(recordValue.getString(Constants.FIELD_EVENT_TYPE));\n            additionalValues.put(Constants.FIELD_SESSION_ID,recordValue.getString(Constants.FIELD_SESSION_ID));\n            String eventTimeStr = null;\n            if (recordValue.get(Constants.FIELD_EVENT_TIME) instanceof String) {\n                eventTimeStr = recordValue.getString(Constants.FIELD_EVENT_TIME);\n                eventInfo.setSentAt(new Date(Long.parseLong(eventTimeStr)));\n            } else if (recordValue.get(Constants.FIELD_EVENT_TIME) instanceof Long) {\n                eventInfo.setSentAt(new Date((Long) recordValue.get(Constants.FIELD_EVENT_TIME)));\n            }\n        } catch (DataException exception) {\n            throw new DataException(\"Missing Required Data\");\n        }\n\n        //Optional field for Event Object\n        eventInfo.setEventId(null != getValueForOptionalField(recordValue, Constants.FIELD_EVENT_ID) ? (String) getValueForOptionalField(recordValue, Constants.FIELD_EVENT_ID) : null);\n        eventInfo.setEventValue(null != getValueForOptionalField(recordValue, Constants.FIELD_EVENT_VALUE) ? (Float) getValueForOptionalField(recordValue, Constants.FIELD_EVENT_VALUE) : null);\n        eventInfo.setImpression(null != getValueForOptionalField(recordValue, Constants.FIELD_EVENT_IMPRESSION) ? (Collection<String>) getValueForOptionalField(recordValue, Constants.FIELD_EVENT_IMPRESSION) : null);\n        eventInfo.setItemId(null != getValueForOptionalField(recordValue, Constants.FIELD_ITEM_ID) ? (String) getValueForOptionalField(recordValue, Constants.FIELD_ITEM_ID) : null);", "        if(null != getValueForOptionalField(recordValue, Constants.FIELD_EVENT_METRIC_ATTRIBUTION_SOURCE)) {\n            MetricAttribution metricAttribution = new MetricAttribution();\n            metricAttribution.setEventAttributionSource((String) getValueForOptionalField(recordValue, Constants.FIELD_EVENT_METRIC_ATTRIBUTION_SOURCE));\n        }\n        eventInfo.setProperties(null != getValueForOptionalField(recordValue, Constants.FIELD_PROPERTIES) ? (String) getValueForOptionalField(recordValue, Constants.FIELD_PROPERTIES) : null);\n        eventInfo.setRecommendationId(null != getValueForOptionalField(recordValue, Constants.FIELD_EVENT_RECOMMENDATION_ID) ? (String) getValueForOptionalField(recordValue, Constants.FIELD_EVENT_RECOMMENDATION_ID) : null);\n\n        //Another optional field in request\n        if(null != getValueForOptionalField(recordValue, Constants.FIELD_USER_ID))\n            additionalValues.put(Constants.FIELD_USER_ID , (String) getValueForOptionalField(recordValue, Constants.FIELD_USER_ID));\n\n        return eventInfo;\n    }\n\n    private static Event prepareEventDataFromMap(Map<String, Object> recordValue, Map<String, String> additionalValues) {\n        Event eventInfo = new Event();\n        //Processing required fields in request\n", "        if(null != getValueForOptionalField(recordValue, Constants.FIELD_USER_ID))\n            additionalValues.put(Constants.FIELD_USER_ID , (String) getValueForOptionalField(recordValue, Constants.FIELD_USER_ID));\n\n        return eventInfo;\n    }\n\n    private static Event prepareEventDataFromMap(Map<String, Object> recordValue, Map<String, String> additionalValues) {\n        Event eventInfo = new Event();\n        //Processing required fields in request\n\n        if(recordValue.containsKey(Constants.FIELD_EVENT_TYPE))\n            eventInfo.setEventType((String)recordValue.get(Constants.FIELD_EVENT_TYPE));\n        else\n            throw new DataException(\"Missing Required Data\");\n", "        if(recordValue.containsKey(Constants.FIELD_EVENT_TYPE))\n            eventInfo.setEventType((String)recordValue.get(Constants.FIELD_EVENT_TYPE));\n        else\n            throw new DataException(\"Missing Required Data\");\n\n        if(recordValue.containsKey(Constants.FIELD_EVENT_TYPE))\n            additionalValues.put(Constants.FIELD_SESSION_ID, (String) recordValue.get(Constants.FIELD_SESSION_ID));\n        else\n            throw new DataException(\"Missing Required Data\");\n\n        if(recordValue.containsKey(Constants.FIELD_EVENT_TIME)){\n            String eventTimeStr = null;", "        if(recordValue.containsKey(Constants.FIELD_EVENT_TIME)){\n            String eventTimeStr = null;\n            if (recordValue.get(Constants.FIELD_EVENT_TIME) instanceof String) {\n                eventTimeStr = (String) recordValue.get(Constants.FIELD_EVENT_TIME);\n                eventInfo.setSentAt(new Date(Long.parseLong(eventTimeStr)));\n            } else if (recordValue.get(Constants.FIELD_EVENT_TIME) instanceof Long) {\n                eventInfo.setSentAt(new Date((Long) recordValue.get(Constants.FIELD_EVENT_TIME)));\n            }\n        }else\n            throw new DataException(\"Missing Required Data\");\n\n        //Optional field for Event Object\n        eventInfo.setEventId(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_ID) ? (String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_ID) : null);\n        eventInfo.setEventValue (null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_VALUE) ? (Float) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_VALUE) : null);\n        eventInfo.setImpression(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_IMPRESSION) ? (Collection<String>) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_IMPRESSION) : null);\n        eventInfo.setItemId(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_ITEM_ID) ? (String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_ITEM_ID) : null);", "        if(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_METRIC_ATTRIBUTION_SOURCE)) {\n            MetricAttribution metricAttribution = new MetricAttribution();\n            metricAttribution.setEventAttributionSource((String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_METRIC_ATTRIBUTION_SOURCE));\n        }\n\n        eventInfo.setProperties(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_PROPERTIES) ? (String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_PROPERTIES) : null);\n        eventInfo.setRecommendationId(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_RECOMMENDATION_ID) ? (String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_EVENT_RECOMMENDATION_ID) : null);\n\n        //Another optional field in request\n        if(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_USER_ID))\n            additionalValues.put(Constants.FIELD_USER_ID, (String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_USER_ID));\n\n        return eventInfo;\n    }\n\n    private static Object getValueForOptionalFieldFromMap(Map<String, Object> recordValue, String fieldName) {\n        Object valueForField = null;", "        if(null != getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_USER_ID))\n            additionalValues.put(Constants.FIELD_USER_ID, (String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_USER_ID));\n\n        return eventInfo;\n    }\n\n    private static Object getValueForOptionalFieldFromMap(Map<String, Object> recordValue, String fieldName) {\n        Object valueForField = null;\n        if(recordValue != null){\n            try{\n                valueForField = recordValue.get(fieldName);\n            }catch (DataException e){\n                valueForField = null;\n            }\n        }\n        return valueForField;\n    }\n\n    private static Object getValueForOptionalField(Struct recordValue, String fieldName) {\n        Object valueForField = null;", "        if(recordValue != null){\n            try{\n                valueForField = recordValue.get(fieldName);\n            }catch (DataException e){\n                valueForField = null;\n            }\n        }\n        return valueForField;\n    }\n\n    private static Object getValueForOptionalField(Struct recordValue, String fieldName) {\n        Object valueForField = null;", "        if(recordValue != null){\n            try{\n                valueForField = recordValue.get(fieldName);\n            }catch (DataException e){\n                valueForField = null;\n            }\n        }\n        return valueForField;\n    }\n\n    /**************\n     * This method is used to convert Sink record to Item Model object\n     * @param record\n     * @return\n     */", "    public static Item convertSinkRecordToItemInfo(SinkRecord record) {\n        Item itemInfo = null;\n        if(record.value() instanceof Struct)\n            itemInfo =  prepareItemDataFromStruct((Struct) record.value());\n        else\n            itemInfo = prepareItemDataFromMap((Map<String, Object>) record.value());\n        return  itemInfo;\n    }\n\n    private static Item prepareItemDataFromStruct(Struct recordValue) {\n        Item itemInfo = new Item();", "        try {\n            itemInfo.setItemId(recordValue.getString(Constants.FIELD_ITEM_ID));\n        } catch (DataException exception) {\n            throw new DataException(\"Missing Required Data\");\n        }\n        itemInfo.setProperties((String) getValueForOptionalField(recordValue, Constants.FIELD_PROPERTIES));\n        return itemInfo;\n    }\n\n    private static Item prepareItemDataFromMap(Map<String,Object> recordValue) {\n        Item itemInfo = new Item();", "        try {\n            itemInfo.setItemId( (String) recordValue.get(Constants.FIELD_ITEM_ID));\n        } catch (Throwable exception) {\n            throw new DataException(\"Missing Required Data\");\n        }\n        itemInfo.setProperties((String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_PROPERTIES));\n        return itemInfo;\n    }\n\n    /***************\n     * This method is used to convert Sink record to User Model object\n     * @param record\n     * @return\n     */", "    public static User convertSinkRecordToUserInfo(SinkRecord record) {\n        User userInfo = null;\n        if(record.value() instanceof Struct)\n            userInfo = prepareUserDataFromStruct((Struct) record.value());\n        else\n            userInfo = prepareUserDataFromMap((Map<String, Object>) record.value());\n        return  userInfo;\n    }\n\n    private static User prepareUserDataFromMap(Map<String, Object> recordValue) {\n        User userInfo = new User();", "        try {\n            userInfo.setUserId( (String) recordValue.get(Constants.FIELD_USER_ID));\n        } catch (Throwable exception) {\n            throw new DataException(\"Missing Required Data\");\n        }\n        userInfo.setProperties((String) getValueForOptionalFieldFromMap(recordValue, Constants.FIELD_PROPERTIES));\n        return userInfo;\n    }\n\n    private static User prepareUserDataFromStruct(Struct recordValue) {\n        User userInfo = new User();", "        try {\n            userInfo.setUserId(recordValue.getString(Constants.FIELD_USER_ID));\n        }catch (DataException exception) {\n            throw new DataException(\"Missing Required Data\");\n        }\n        userInfo.setProperties( (String) getValueForOptionalField(recordValue, Constants.FIELD_PROPERTIES));\n        return userInfo;\n    }\n}\n"]}
{"filename": "src/main/java/com/aws/task/PersonlizeSinkTask.java", "chunked_list": ["package com.aws.task;\n\nimport com.amazonaws.ClientConfiguration;\nimport com.amazonaws.retry.PredefinedRetryPolicies;\nimport com.amazonaws.retry.RetryPolicy;\nimport com.amazonaws.services.personalizeevents.AmazonPersonalizeEvents;\nimport com.amazonaws.services.personalizeevents.AmazonPersonalizeEventsClient;\nimport com.aws.util.Constants;\nimport com.aws.util.Version;\nimport com.aws.writer.DataWriter;", "import com.aws.util.Version;\nimport com.aws.writer.DataWriter;\nimport com.aws.config.PersonalizeSinkConfig;\nimport org.apache.kafka.connect.sink.ErrantRecordReporter;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.apache.kafka.connect.sink.SinkTask;\nimport org.joda.time.DateTime;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n", "import org.slf4j.LoggerFactory;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Map;\n\n/****************\n * This is main class which used for Sink Task functionality by overriding required methods.\n */\npublic class PersonlizeSinkTask extends SinkTask {\n    private static final String USER_AGENT_PREFIX = \"PersonalizeKafkaSinkConnector\";\n    private static final Logger log = LoggerFactory.getLogger(PersonlizeSinkTask.class);\n\n    ErrantRecordReporter reporter;\n    PersonalizeSinkConfig config;\n    DataWriter writer;\n    int remainingRetries;\n\n    @Override", " */\npublic class PersonlizeSinkTask extends SinkTask {\n    private static final String USER_AGENT_PREFIX = \"PersonalizeKafkaSinkConnector\";\n    private static final Logger log = LoggerFactory.getLogger(PersonlizeSinkTask.class);\n\n    ErrantRecordReporter reporter;\n    PersonalizeSinkConfig config;\n    DataWriter writer;\n    int remainingRetries;\n\n    @Override", "    public String version() {\n        return new Version().getVersion();\n    }\n\n    @Override\n    public void start(Map<String, String> props) {\n        log.info(\"Starting Amazon Personalize Sink task\");\n        config = new PersonalizeSinkConfig(props);\n        initWriter();\n        remainingRetries = config.getMaxRetries();\n        try {\n            reporter = context.errantRecordReporter();\n        } catch (NoSuchMethodError | NoClassDefFoundError e) {\n            // Will occur in Connect runtimes earlier than 2.6\n            reporter = null;\n        }\n    }\n\n    private void initWriter() {\n        ClientConfiguration configuration = new ClientConfiguration()\n                .withRetryPolicy(new RetryPolicy(PredefinedRetryPolicies.DEFAULT_RETRY_CONDITION, PredefinedRetryPolicies.DEFAULT_BACKOFF_STRATEGY,config.getMaxRetries(),true))\n                .withUserAgentPrefix(USER_AGENT_PREFIX)\n                .withMaxErrorRetry(config.getMaxRetries());\n\n        AmazonPersonalizeEvents client = AmazonPersonalizeEventsClient.builder()\n                .withCredentials(config.getCredentionalProvider())\n                .withRegion(null != config.getAwsRegionName() ? config.getAwsRegionName() : Constants.DEFAULT_AWS_REGION_NAME)\n                .withClientConfiguration(configuration)\n                .build();\n\n        writer = new DataWriter(config, client);\n    }\n\n    @Override", "        try {\n            reporter = context.errantRecordReporter();\n        } catch (NoSuchMethodError | NoClassDefFoundError e) {\n            // Will occur in Connect runtimes earlier than 2.6\n            reporter = null;\n        }\n    }\n\n    private void initWriter() {\n        ClientConfiguration configuration = new ClientConfiguration()\n                .withRetryPolicy(new RetryPolicy(PredefinedRetryPolicies.DEFAULT_RETRY_CONDITION, PredefinedRetryPolicies.DEFAULT_BACKOFF_STRATEGY,config.getMaxRetries(),true))\n                .withUserAgentPrefix(USER_AGENT_PREFIX)\n                .withMaxErrorRetry(config.getMaxRetries());\n\n        AmazonPersonalizeEvents client = AmazonPersonalizeEventsClient.builder()\n                .withCredentials(config.getCredentionalProvider())\n                .withRegion(null != config.getAwsRegionName() ? config.getAwsRegionName() : Constants.DEFAULT_AWS_REGION_NAME)\n                .withClientConfiguration(configuration)\n                .build();\n\n        writer = new DataWriter(config, client);\n    }\n\n    @Override", "    public void put(Collection<SinkRecord> records) {\n        if (records.isEmpty()) {\n            return;\n        }\n        final SinkRecord first = records.iterator().next();\n        final int recordsCount = records.size();\n        try {\n            writer.write(records);\n        } catch (Exception ex) {\n            if (reporter != null) {\n                unrollAndRetry(records);\n            } else {\n                log.error(\"Error in writing data:\", ex);\n            }\n        }\n    }\n\n    private void unrollAndRetry(Collection<SinkRecord> records) {\n        int retryAttempts = remainingRetries;", "            if (reporter != null) {\n                unrollAndRetry(records);\n            } else {\n                log.error(\"Error in writing data:\", ex);\n            }\n        }\n    }\n\n    private void unrollAndRetry(Collection<SinkRecord> records) {\n        int retryAttempts = remainingRetries;\n        if (retryAttempts > 0) {\n            writer.closeQuietly();\n            initWriter();", "        if (retryAttempts > 0) {\n            writer.closeQuietly();\n            initWriter();\n            for (SinkRecord record : records) {\n                try {\n                    writer.write(Collections.singletonList(record));\n                } catch (Exception ex) {\n                    reporter.report(record, ex);\n                    writer.closeQuietly();\n                }\n            }\n            retryAttempts--;\n        }\n    }\n\n    @Override", "    public void stop() {\n        log.info(\"Stopping Amazon Personalize Sink task\");\n    }\n}\n"]}
{"filename": "src/main/java/com/aws/connector/PersonalizeSinkConnector.java", "chunked_list": ["package com.aws.connector;\n\nimport com.aws.config.PersonalizeSinkConfig;\nimport com.aws.task.PersonlizeSinkTask;\nimport com.aws.util.Constants;\nimport com.aws.util.Version;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.connect.connector.Task;\nimport org.apache.kafka.connect.sink.SinkConnector;\nimport org.slf4j.Logger;", "import org.apache.kafka.connect.sink.SinkConnector;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\n/****************", "\n/****************\n * This is main class which used for Sink connector functionality by overriding required methods.\n */\npublic class PersonalizeSinkConnector extends SinkConnector {\n\n    private static final Logger log = LoggerFactory.getLogger(PersonalizeSinkConnector.class);\n\n    private Map<String, String> configProps;\n\n    private PersonalizeSinkConfig config;\n\n    @Override", "    public void start(Map<String, String> props) {\n        configProps = new HashMap<>(props);\n        addRecordRateLimitConfiguration(configProps);\n        config = new PersonalizeSinkConfig(props);\n        log.info(\"Starting Personalize connector with name :\", config.getName());\n    }\n\n    private void addRecordRateLimitConfiguration(Map<String, String> configProps) {\n        if (!configProps.containsKey(Constants.RECORD_RATE_LIMIT_PROPERTY_NAME)) {\n            int maxTask = 1;\n            if (configProps.containsKey(Constants.MAX_TASKS_PROPERTY_NAME)) {\n                maxTask = Integer.parseInt(configProps.get(Constants.MAX_TASKS_PROPERTY_NAME));\n            }\n            String dataType = configProps.get(Constants.AMAZON_PERSONALIZE_DATA_TYPE);", "        if (!configProps.containsKey(Constants.RECORD_RATE_LIMIT_PROPERTY_NAME)) {\n            int maxTask = 1;\n            if (configProps.containsKey(Constants.MAX_TASKS_PROPERTY_NAME)) {\n                maxTask = Integer.parseInt(configProps.get(Constants.MAX_TASKS_PROPERTY_NAME));\n            }\n            String dataType = configProps.get(Constants.AMAZON_PERSONALIZE_DATA_TYPE);\n            if (null == dataType || dataType.equals(Constants.PUT_EVENTS_REQUEST_TYPE)) {\n                configProps.put(Constants.RECORD_RATE_LIMIT_PROPERTY_NAME, String.valueOf(Constants.DEFAULT_VALUE_FOR_EVENT_RECORD_RATE / maxTask));\n            } else {\n                configProps.put(Constants.RECORD_RATE_LIMIT_PROPERTY_NAME, String.valueOf(Constants.DEFAULT_VALUE_FOR_ITEM_AND_USER_RECORD_RATE / maxTask));\n            }\n        }\n    }\n\n    @Override\n    public Class<? extends Task> taskClass() {\n        return PersonlizeSinkTask.class;\n    }\n\n    @Override\n    public List<Map<String, String>> taskConfigs(int maxTasks) {\n        log.info(\"Setting task configurations for {} workers.\", maxTasks);\n        final List<Map<String, String>> configs = new ArrayList<>(maxTasks);", "        for (int i = 0; i < maxTasks; ++i) {\n            configs.add(configProps);\n        }\n        return configs;\n    }\n\n    @Override\n    public void stop() {\n        log.info(\"Shutting Down Personalize Connector with name :\" + config.getName());\n    }\n\n    @Override", "    public ConfigDef config() {\n        return PersonalizeSinkConfig.CONFIG_DEF;\n    }\n\n    @Override\n    public String version() {\n        return new Version().getVersion();\n    }\n}\n"]}
{"filename": "src/main/java/com/aws/auth/AssumeRoleCredentialsProvider.java", "chunked_list": ["package com.aws.auth;\n\nimport com.amazonaws.auth.*;\nimport com.amazonaws.services.securitytoken.AWSSecurityTokenServiceClientBuilder;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.kafka.common.Configurable;\nimport org.apache.kafka.common.config.AbstractConfig;\nimport org.apache.kafka.common.config.ConfigDef;\n\n", "\n\nimport java.util.Map;\n\nimport static com.aws.util.Constants.AWS_ACCESS_KEY;\nimport static com.aws.util.Constants.AWS_SECRET_KEY;\n\n/******************\n * This class provide default implementation for if client using trusted account for providing authentication\n */\npublic class AssumeRoleCredentialsProvider implements AWSCredentialsProvider, Configurable {\n", " * This class provide default implementation for if client using trusted account for providing authentication\n */\npublic class AssumeRoleCredentialsProvider implements AWSCredentialsProvider, Configurable {\n\n    public static final String AMAZON_PERSONALIZE_STS_ROLE_EXTERNAL_ID_CONFIG = \"amazon.personalize.sts.role.external.id\";\n    public static final String AMAZON_PERSONALIZE_ROLE_ARN_CONFIG = \"amazon.personalize.sts.role.arn\";\n    public static final String AMAZON_PERSONALIZE_ROLE_SESSION_NAME_CONFIG = \"amazon.personalize.sts.role.session.name\";\n\n    private static final ConfigDef STS_CONFIG_DEF = new ConfigDef()\n            .define(\n                    AMAZON_PERSONALIZE_STS_ROLE_EXTERNAL_ID_CONFIG,\n                    ConfigDef.Type.STRING,\n                    ConfigDef.Importance.MEDIUM,\n                    \"Amazon Personalize external role ID used when retrieving session credentials under an assumed role.\"\n            ).define(\n                    AMAZON_PERSONALIZE_ROLE_ARN_CONFIG,\n                    ConfigDef.Type.STRING,\n                    ConfigDef.Importance.HIGH,\n                    \"Amazon Personalize Role ARN for creating AWS session.\"\n            ).define(\n                    AMAZON_PERSONALIZE_ROLE_SESSION_NAME_CONFIG,\n                    ConfigDef.Type.STRING,\n                    ConfigDef.Importance.HIGH,\n                    \"Amazon Personalize Role session name\"\n            );\n\n    private String roleArn;\n    private String roleExternalId;\n    private String roleSessionName;\n\n    private BasicAWSCredentials basicCredentials;\n\n    private STSAssumeRoleSessionCredentialsProvider stsAssumeRoleCredentialsProvider;\n\n    @Override", "    public void configure(Map<String, ?> configs) {\n        AbstractConfig config = new AbstractConfig(STS_CONFIG_DEF, configs);\n        roleArn = config.getString(AMAZON_PERSONALIZE_ROLE_ARN_CONFIG);\n        roleExternalId = config.getString(AMAZON_PERSONALIZE_STS_ROLE_EXTERNAL_ID_CONFIG);\n        roleSessionName = config.getString(AMAZON_PERSONALIZE_ROLE_SESSION_NAME_CONFIG);\n        final String accessKeyId = (String) configs.get(AWS_ACCESS_KEY);\n        final String secretKey = (String) configs.get(AWS_SECRET_KEY);\n        if (StringUtils.isNotBlank(accessKeyId) && StringUtils.isNotBlank(secretKey)) {\n            BasicAWSCredentials basicCredentials = new BasicAWSCredentials(accessKeyId, secretKey);\n\n            stsAssumeRoleCredentialsProvider = new STSAssumeRoleSessionCredentialsProvider\n                    .Builder(roleArn, roleSessionName)\n                    .withStsClient(AWSSecurityTokenServiceClientBuilder\n                            .standard()\n                            .withCredentials(new AWSStaticCredentialsProvider(basicCredentials)).build()\n                    )\n                    .withExternalId(roleExternalId)\n                    .build();\n        } else {\n            basicCredentials = null;\n            stsAssumeRoleCredentialsProvider = new STSAssumeRoleSessionCredentialsProvider.Builder(roleArn, roleSessionName)\n                    .withStsClient(AWSSecurityTokenServiceClientBuilder.defaultClient())\n                    .withExternalId(roleExternalId).build();\n        }\n    }\n\n\n    @Override", "    public AWSCredentials getCredentials() {\n        return stsAssumeRoleCredentialsProvider.getCredentials();\n    }\n\n    @Override\n    public void refresh() {\n        if(stsAssumeRoleCredentialsProvider != null)\n            stsAssumeRoleCredentialsProvider.refresh();\n    }\n}\n"]}
{"filename": "src/main/java/com/aws/config/PersonalizeSinkConfig.java", "chunked_list": ["package com.aws.config;\n\nimport com.amazonaws.auth.AWSCredentialsProvider;\nimport com.amazonaws.auth.AWSStaticCredentialsProvider;\nimport com.amazonaws.auth.BasicAWSCredentials;\nimport com.aws.config.validators.ArnValidator;\nimport com.aws.config.validators.PersonalizeCredentialsProviderValidator;\nimport com.aws.config.validators.PersonalizeDataTypeValidator;\nimport org.apache.commons.lang3.StringUtils;\nimport org.apache.kafka.common.Configurable;", "import org.apache.commons.lang3.StringUtils;\nimport org.apache.kafka.common.Configurable;\nimport org.apache.kafka.common.config.AbstractConfig;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.types.Password;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.connect.errors.ConnectException;\n\nimport java.util.Map;\n", "import java.util.Map;\n\nimport static com.aws.util.Constants.*;\n\n/****************\n * This class is used for connector specific configuration.\n */\npublic class PersonalizeSinkConfig extends AbstractConfig {\n\n    private final String awsAccessKey;\n    private final String awsSecretKey;\n    private final String awsRegionName;\n    private final String eventTrackingId;\n    private final String dataSetArn;\n    private final String dataType;\n    private final int maxRetries;\n    private final String name;\n\n", "    public static final ConfigDef CONFIG_DEF = new ConfigDef().define(\n            AWS_ACCESS_KEY,\n            ConfigDef.Type.STRING,\n            DEFAULT_STRING_VALUE,\n            ConfigDef.Importance.HIGH,\n            AWS_ACCESS_KEY_DOC,\n            AWS_ACCOUNT_DETAILS_GROUP,\n            1,\n            ConfigDef.Width.LONG,\n            AWS_ACCESS_KEY_DISPLAY\n    ).define(\n            AWS_SECRET_KEY,\n            ConfigDef.Type.PASSWORD,\n            AWS_SECRET_KEY_DEFAULT_VALUE,\n            ConfigDef.Importance.HIGH,\n            AWS_SECRET_KEY_DOC,\n            AWS_ACCOUNT_DETAILS_GROUP,\n            2,\n            ConfigDef.Width.LONG,\n            AWS_SECRET_KEY_DISPLAY\n    ).define(\n            AWS_REGION_NAME,\n            ConfigDef.Type.STRING,\n            ConfigDef.NO_DEFAULT_VALUE,\n            ConfigDef.Importance.HIGH,\n            AWS_REGION_NAME_DOC,\n            AWS_ACCOUNT_DETAILS_GROUP,\n            3,\n            ConfigDef.Width.LONG,\n            AWS_REGION_NAME_DISPLAY\n    ).define(\n            AMAZON_PERSONALIZE_DATA_TYPE,\n            ConfigDef.Type.STRING,\n            PUT_EVENTS_REQUEST_TYPE,\n            new PersonalizeDataTypeValidator(),\n            ConfigDef.Importance.HIGH,\n            AMAZON_PERSONALIZE_EVENT_TYPE_DOC,\n            AWS_ACCOUNT_DETAILS_GROUP,\n            4,\n            ConfigDef.Width.LONG,\n            AMAZON_PERSONALIZE_EVENT_TYPE_DISPLAY\n    ).define(\n            AMAZON_PERSONALIZE_DATASET_ARN,\n            ConfigDef.Type.STRING,\n            DEFAULT_STRING_VALUE,\n            new ArnValidator(),\n            ConfigDef.Importance.HIGH,\n            AMAZON_PERSONALIZE_DATASET_ARN_DOC,\n            AWS_ACCOUNT_DETAILS_GROUP,\n            6,\n            ConfigDef.Width.LONG,\n            AMAZON_PERSONALIZE_DATASET_ARN_DISPLAY\n    ).define(\n            AMAZON_PERSONALIZE_EVENT_TRACKING_ID,\n            ConfigDef.Type.STRING,\n            DEFAULT_STRING_VALUE,\n            ConfigDef.Importance.HIGH,\n            AMAZON_PERSONALIZE_EVENT_TRACKING_ID_DOC,\n            AWS_ACCOUNT_DETAILS_GROUP,\n            5,\n            ConfigDef.Width.LONG,\n            AMAZON_PERSONALIZE_EVENT_TRACKING_ID_DISPLAY\n    ).define(\n            MAX_RETRIES,\n            ConfigDef.Type.INT,\n            2,\n            ConfigDef.Importance.HIGH,\n            MAX_RETRIES,\n            null,\n            8,\n            ConfigDef.Width.LONG,\n            MAX_RETRIES\n    ).define(\n            AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS,\n            ConfigDef.Type.CLASS,\n            AWS_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DEFAULT,\n            new PersonalizeCredentialsProviderValidator(),\n            ConfigDef.Importance.LOW,\n            AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DOC,\n            AWS_ACCOUNT_DETAILS_GROUP,\n            7,\n            ConfigDef.Width.LONG,\n            AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DISPLAY\n    );\n", "    public String awsAccessKey() {\n        return getString(AWS_ACCESS_KEY);\n    }\n\n    public Password awsSecretKey() {\n        return getPassword(AWS_SECRET_KEY);\n    }\n\n    public PersonalizeSinkConfig(Map<?, ?> props) {\n        super(CONFIG_DEF, props);\n        awsAccessKey =awsAccessKey();\n        awsSecretKey = awsSecretKey().value();\n        awsRegionName = getString(AWS_REGION_NAME);\n        maxRetries = getInt(MAX_RETRIES);\n        dataType = getString(AMAZON_PERSONALIZE_DATA_TYPE);", "        if(dataType.equals(PUT_EVENTS_REQUEST_TYPE)) {\n            eventTrackingId = getVaildConfigValue(props, AMAZON_PERSONALIZE_EVENT_TRACKING_ID);\n            dataSetArn = getString(AMAZON_PERSONALIZE_DATASET_ARN);\n        }else {\n            dataSetArn = getVaildConfigValue(props,AMAZON_PERSONALIZE_DATASET_ARN);\n            eventTrackingId = getString(AMAZON_PERSONALIZE_EVENT_TRACKING_ID);\n        }\n        name = getConnectorName(props);\n    }\n\n    /*******\n     * This method is used for getting name for connector which is configured\n     * @param props\n     * @return\n     */\n    private String getConnectorName(Map<?, ?> props) {\n        String connectorName = null;", "        if (props.containsKey(NAME_PROPERTY)) {\n            String nameProp = (String) props.get(NAME_PROPERTY);\n            connectorName = null != nameProp ? nameProp : DEFAULT_CONNECTOR_NAME;\n        } else {\n            connectorName = DEFAULT_CONNECTOR_NAME;\n        }\n        return connectorName;\n    }\n\n    private String getVaildConfigValue(Map<?, ?> props, String configName){\n        if(props.containsKey(configName)) {\n            String validValue = getString(configName) ;", "        if(props.containsKey(configName)) {\n            String validValue = getString(configName) ;\n            if (!StringUtils.isEmpty(validValue)) {\n                return validValue;\n            }else\n                throw new ConfigException(\"You should have non-empty value for Configuration\" + configName);\n        } else\n            throw new ConfigException(\"Missing required configuration \" + configName);\n    }\n\n    public String getName() {\n        return name;\n    }\n", "    public String getName() {\n        return name;\n    }\n\n    public String getAwsRegionName() {\n        return awsRegionName;\n    }\n\n    public String getEventTrackingId() {\n        return eventTrackingId;\n    }\n", "    public String getEventTrackingId() {\n        return eventTrackingId;\n    }\n\n    public String getDataSetArn() {\n        return dataSetArn;\n    }\n\n    public String getDataType() {\n        return dataType;\n    }\n", "    public String getDataType() {\n        return dataType;\n    }\n\n    public int getMaxRetries() {\n        return maxRetries;\n    }\n\n    /**************\n     * This method is used for providing AWS credentials which will be used by personalize client\n     * @return AWS credentials provider\n     */", "    public AWSCredentialsProvider getCredentionalProvider() {\n        try {\n            AWSCredentialsProvider provider = ((Class<? extends AWSCredentialsProvider>) getClass(AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS)).newInstance();\n\n            if (provider instanceof Configurable) {\n                Map<String, Object> configs = originalsWithPrefix(AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_PREFIX);\n                configs.remove(AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS.substring(\n                        AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_PREFIX.length()\n                ));\n\n                configs.put(AWS_ACCESS_KEY, awsAccessKey());\n                configs.put(AWS_SECRET_KEY, awsSecretKey().value());\n\n                ((Configurable) provider).configure(configs);\n            } else {\n                final String accessKeyId = awsAccessKey();\n                final String secretKey = awsSecretKey().value();", "                if (StringUtils.isNotBlank(accessKeyId) && StringUtils.isNotBlank(secretKey)) {\n                    BasicAWSCredentials basicCredentials = new BasicAWSCredentials(accessKeyId, secretKey);\n                    provider = new AWSStaticCredentialsProvider(basicCredentials);\n                }\n            }\n            return provider;\n        } catch (Exception e) {\n            throw new ConnectException(\"Invalid class for: \" + AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS, e);\n        }\n    }\n}"]}
{"filename": "src/main/java/com/aws/config/validators/ArnValidator.java", "chunked_list": ["package com.aws.config.validators;\n\nimport com.amazonaws.arn.Arn;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\n\nimport static com.aws.util.Constants.*;\n\n/*********\n * This is used for validation ARN for Items or Users Dataset", "/*********\n * This is used for validation ARN for Items or Users Dataset\n */\npublic  class ArnValidator implements  ConfigDef.Validator {\n    @Override\n    public void ensureValid(String arnVar, Object value) {\n        if (null != value && ((String) value).isEmpty()) {\n            return;\n        }\n\n        try {\n            String arnValue = (String) value;\n            Arn arn = Arn.fromString(arnValue);\n", "        try {\n            String arnValue = (String) value;\n            Arn arn = Arn.fromString(arnValue);\n\n            if ((arn.getResourceAsString().endsWith(PERSONALIZE_USERS_DATASET_RESOURCE_NAME) || arn.getResourceAsString().endsWith(PERSONALIZE_ITEMS_DATASET_RESOURCE_NAME))\n                    && arn.getResource().getResourceType().equals(PERSONALIZE_DATASET_RESOURCE) && arn.getService().equals(PERSONALIZE_SERVICE_NAME)) {\n                return;\n            } else {\n                throw new ConfigException(\"Invalid Arn for User/Item dataset\");\n            }\n        } catch (Throwable e) {\n            throw new ConfigException(\"Invalid Arn for User/Item dataset\");\n        }\n\n    }\n}"]}
{"filename": "src/main/java/com/aws/config/validators/PersonalizeCredentialsProviderValidator.java", "chunked_list": ["package com.aws.config.validators;\n\nimport com.amazonaws.auth.AWSCredentialsProvider;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\n/*********\n * This is used for validation Credential Provider class\n */\npublic class PersonalizeCredentialsProviderValidator implements ConfigDef.Validator {\n    @Override\n    public void ensureValid(String name, Object provider) {", "public class PersonalizeCredentialsProviderValidator implements ConfigDef.Validator {\n    @Override\n    public void ensureValid(String name, Object provider) {\n        if (provider != null && provider instanceof Class\n                && AWSCredentialsProvider.class.isAssignableFrom((Class<?>) provider)) {\n            return;\n        }\n        throw new ConfigException(\n                name,\n                provider,\n                \"Class must extend: \" + AWSCredentialsProvider.class\n        );\n    }\n}"]}
{"filename": "src/main/java/com/aws/config/validators/PersonalizeDataTypeValidator.java", "chunked_list": ["package com.aws.config.validators;\n\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\n\nimport static com.aws.util.Constants.VALID_DATA_TYPE;\n\n/*************\n * This class is used for validating Personalize Data Type configuration\n */\npublic class PersonalizeDataTypeValidator implements  ConfigDef.Validator {\n\n    @Override", " * This class is used for validating Personalize Data Type configuration\n */\npublic class PersonalizeDataTypeValidator implements  ConfigDef.Validator {\n\n    @Override\n    public void ensureValid(String eventType, Object value) {\n        if (null != value && ((String) value).isEmpty()) {\n            return;\n        }\n        if (!VALID_DATA_TYPE.contains(((String) value).toLowerCase())) {\n            throw new ConfigException(String.format(\n                    \"%s must be one out of %s\",\n                    eventType,\n                    String.join(\",\", VALID_DATA_TYPE)\n            ));\n        }\n    }\n}", "        if (!VALID_DATA_TYPE.contains(((String) value).toLowerCase())) {\n            throw new ConfigException(String.format(\n                    \"%s must be one out of %s\",\n                    eventType,\n                    String.join(\",\", VALID_DATA_TYPE)\n            ));\n        }\n    }\n}"]}
{"filename": "src/main/java/com/aws/writer/DataWriter.java", "chunked_list": ["package com.aws.writer;\n\nimport com.amazonaws.services.personalizeevents.AmazonPersonalizeEvents;\nimport com.amazonaws.services.personalizeevents.model.*;\nimport com.aws.config.PersonalizeSinkConfig;\nimport com.aws.converter.DataConverter;\nimport com.aws.util.Constants;\nimport org.apache.kafka.connect.sink.SinkRecord;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;", "import org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.Map;\n\n/************\n * This class is used for writing Kafka Records into Personalize Event through PersonalizeEvent SDK", "/************\n * This class is used for writing Kafka Records into Personalize Event through PersonalizeEvent SDK\n */\npublic class DataWriter {\n    private static final Logger log = LoggerFactory.getLogger(DataWriter.class);\n    AmazonPersonalizeEvents client;\n    String eventTrackingId;\n    String dataSetArn;\n    String dataType;\n\n    public DataWriter(PersonalizeSinkConfig config, AmazonPersonalizeEvents client) {\n        log.info(\"Initialing data writer for : \" + config.getDataType());\n\n        int maxRetries = config.getMaxRetries();\n\n        this.client = client;\n\n        dataType  = config.getDataType();\n\n        eventTrackingId = config.getEventTrackingId();\n\n        dataSetArn = config.getDataSetArn();\n\n    }\n\n    /**********************\n     * This method is used for writing sink kafka record into personalize.\n     * @param records of Sink records\n     * @throws Exception\n     */", "    public void write(final Collection<SinkRecord> records) throws Exception {\n        log.debug(\"DataWriter started writing record with size :\" + records.size());\n        try{\n            for (SinkRecord record : records) {\n                log.debug(\"Processing record with value:\"+ record.value());\n                if(Constants.PUT_EVENTS_REQUEST_TYPE.equals(dataType)){\n                    Map<String, String> additionalValues = new HashMap<>();\n                    Event eventInfo = DataConverter.convertSinkRecordToEventInfo(record, additionalValues);\n                    if(eventInfo != null)\n                        putEvents(eventInfo, additionalValues);\n                }else if(Constants.PUT_ITEMS_REQUEST_TYPE.equals(dataType)){\n                    Item itemInfo = DataConverter.convertSinkRecordToItemInfo(record);", "                    if(eventInfo != null)\n                        putEvents(eventInfo, additionalValues);\n                }else if(Constants.PUT_ITEMS_REQUEST_TYPE.equals(dataType)){\n                    Item itemInfo = DataConverter.convertSinkRecordToItemInfo(record);\n                    if(itemInfo != null)\n                        putItems(itemInfo);\n                }else if(Constants.PUT_USERS_REQUEST_TYPE.equals(dataType)){\n                    User userInfo = DataConverter.convertSinkRecordToUserInfo(record);\n                    if(userInfo != null)\n                        putUsers(userInfo);\n                }else\n                    log.error(\"Invalid Operation\" );\n            }\n        }catch(Throwable e){\n            log.error(\"Exception occurred while writing data to personalize\", e);\n            throw e;\n        }\n\n    }\n\n    /***************************\n     * This method to call put event APIs\n     * @param eventInfo\n     * @return\n     */\n    private void putEvents(Event eventInfo, Map<String, String> additionalValues) {", "                    if(userInfo != null)\n                        putUsers(userInfo);\n                }else\n                    log.error(\"Invalid Operation\" );\n            }\n        }catch(Throwable e){\n            log.error(\"Exception occurred while writing data to personalize\", e);\n            throw e;\n        }\n\n    }\n\n    /***************************\n     * This method to call put event APIs\n     * @param eventInfo\n     * @return\n     */\n    private void putEvents(Event eventInfo, Map<String, String> additionalValues) {", "        try {\n            PutEventsRequest putEventsRequest = new PutEventsRequest();\n            putEventsRequest.setTrackingId(eventTrackingId);\n            putEventsRequest.setUserId(additionalValues.get(Constants.FIELD_USER_ID));\n            putEventsRequest.setSessionId(additionalValues.get(Constants.FIELD_SESSION_ID));\n            putEventsRequest.setEventList(Collections.singletonList(eventInfo));\n\n            client.putEvents(putEventsRequest);\n\n        } catch (AmazonPersonalizeEventsException e) {\n            log.error(\"Error in Put events API\", e);\n            throw e;\n        }\n    }\n\n\n    /***********\n     * This method to call put user APIs\n     * @param userInfo\n     */\n    private void putUsers(User userInfo) {", "        try {\n            PutUsersRequest putUsersRequest = new PutUsersRequest();\n            putUsersRequest.setUsers(Collections.singletonList(userInfo));\n            putUsersRequest.setDatasetArn(dataSetArn);\n\n            client.putUsers(putUsersRequest);\n        } catch (AmazonPersonalizeEventsException e) {\n            log.error(\"Error in Put Users API\", e);\n            throw e;\n        }\n    }\n\n\n    /*******\n     * This method to call put item APIs\n     * @param itemInfo\n     */\n    private void putItems(Item itemInfo) {", "        try {\n            PutItemsRequest putItemsRequest = new PutItemsRequest();\n            putItemsRequest.setItems(Collections.singletonList(itemInfo));\n            putItemsRequest.setDatasetArn(dataSetArn);\n\n            client.putItems(putItemsRequest);\n        } catch (AmazonPersonalizeEventsException e) {\n            log.error(\"Error in Put Items API\", e);\n            throw e;\n        }\n\n    }\n\n    /*******************\n     * This method is used to close AWS personalize client.\n     */", "    public void closeQuietly() {\n       client.shutdown();\n    }\n}\n"]}
{"filename": "src/main/java/com/aws/util/Constants.java", "chunked_list": ["package com.aws.util;\n\nimport com.amazonaws.auth.AWSCredentialsProvider;\nimport com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\nimport org.apache.kafka.common.config.types.Password;\n\nimport java.util.Arrays;\nimport java.util.HashSet;\nimport java.util.Set;\n", "import java.util.Set;\n\n/***********\n * This class consists of Constants used.\n */\npublic class Constants {\n    //Event Related Fields\n    public static final String FIELD_EVENT_TYPE =\"eventType\";\n    public static final String FIELD_EVENT_ID =\"eventId\";\n    public static final String FIELD_EVENT_VALUE =\"eventValue\";\n    public static final String FIELD_EVENT_IMPRESSION =\"impression\";", "    public static final String FIELD_EVENT_ID =\"eventId\";\n    public static final String FIELD_EVENT_VALUE =\"eventValue\";\n    public static final String FIELD_EVENT_IMPRESSION =\"impression\";\n    public static final String FIELD_ITEM_ID =\"itemId\";\n    public static final String FIELD_EVENT_METRIC_ATTRIBUTION_SOURCE =\"eventAttributionSource\";\n    public static final String FIELD_PROPERTIES = \"properties\";\n    public static final String FIELD_EVENT_RECOMMENDATION_ID = \"recommendationId\";\n    public static final String FIELD_EVENT_TIME =\"sentAt\";\n    public static final String FIELD_SESSION_ID =\"sessionId\";\n    public static final String FIELD_USER_ID =\"userId\";\n    public static final Set<String> VALID_DATA_TYPE = new HashSet<String>(Arrays.asList(\"events\", \"items\",\"users\"));", "    public static final String FIELD_SESSION_ID =\"sessionId\";\n    public static final String FIELD_USER_ID =\"userId\";\n    public static final Set<String> VALID_DATA_TYPE = new HashSet<String>(Arrays.asList(\"events\", \"items\",\"users\"));\n    public static final String PUT_EVENTS_REQUEST_TYPE = \"events\";\n    public static final String PUT_ITEMS_REQUEST_TYPE = \"items\";\n    public static final String PUT_USERS_REQUEST_TYPE = \"users\";\n    public static final String PERSONALIZE_SERVICE_NAME = \"personalize\";\n    public static final String PERSONALIZE_DATASET_RESOURCE = \"dataset\";\n    public static final String PERSONALIZE_USERS_DATASET_RESOURCE_NAME = \"USERS\";\n    public static final String PERSONALIZE_ITEMS_DATASET_RESOURCE_NAME = \"ITEMS\";\n    public static final Class<? extends AWSCredentialsProvider> AWS_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DEFAULT = DefaultAWSCredentialsProviderChain.class;", "    public static final String PERSONALIZE_USERS_DATASET_RESOURCE_NAME = \"USERS\";\n    public static final String PERSONALIZE_ITEMS_DATASET_RESOURCE_NAME = \"ITEMS\";\n    public static final Class<? extends AWSCredentialsProvider> AWS_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DEFAULT = DefaultAWSCredentialsProviderChain.class;\n    public static final String AWS_ACCESS_KEY = \"aws.access.key\";\n    public static final String DEFAULT_STRING_VALUE = \"\";\n    public static final String AWS_ACCESS_KEY_DOC = \"AWS Access key\";\n    public static final String AWS_ACCESS_KEY_DISPLAY = \"AWS Access key\";\n    public static final String AWS_SECRET_KEY = \"aws.secret.key\";\n    public static final Password AWS_SECRET_KEY_DEFAULT_VALUE = new Password(null);\n    public static final String AWS_SECRET_KEY_DOC = \"AWS Secret Access Key\";\n    public static final String AWS_SECRET_KEY_DISPLAY = \"AWS Secret Access Key\";", "    public static final Password AWS_SECRET_KEY_DEFAULT_VALUE = new Password(null);\n    public static final String AWS_SECRET_KEY_DOC = \"AWS Secret Access Key\";\n    public static final String AWS_SECRET_KEY_DISPLAY = \"AWS Secret Access Key\";\n    public static final String AWS_REGION_NAME = \"aws.region.name\";\n    public static final String DEFAULT_AWS_REGION_NAME = \"us-east-1\";\n    public static final String AWS_REGION_NAME_DOC = \"AWS Region\";\n    public static final String AWS_REGION_NAME_DISPLAY = \"AWS Region\";\n    public static final String AMAZON_PERSONALIZE_EVENT_TRACKING_ID = \"amazon.personalize.event.tracking.id\";\n    public static final String AMAZON_PERSONALIZE_EVENT_TRACKING_ID_DOC = \"Amazon Personalize Event tracking ID\";\n    public static final String AMAZON_PERSONALIZE_EVENT_TRACKING_ID_DISPLAY = \"Amazon Personalize Event tracking ID\";\n    public static final String AMAZON_PERSONALIZE_DATA_TYPE = \"amazon.personalize.data.type\";", "    public static final String AMAZON_PERSONALIZE_EVENT_TRACKING_ID_DOC = \"Amazon Personalize Event tracking ID\";\n    public static final String AMAZON_PERSONALIZE_EVENT_TRACKING_ID_DISPLAY = \"Amazon Personalize Event tracking ID\";\n    public static final String AMAZON_PERSONALIZE_DATA_TYPE = \"amazon.personalize.data.type\";\n    public static final String AMAZON_PERSONALIZE_EVENT_TYPE_DISPLAY = \"Amazon Personalize Data Type\";\n    public static final String AMAZON_PERSONALIZE_EVENT_TYPE_DOC = \"Valid data types are events,items and users corresponding to interactions, items and users datasets.\";\n    public static final String AMAZON_PERSONALIZE_DATASET_ARN = \"amazon.personalize.dataset.arn\";\n    public static final String AMAZON_PERSONALIZE_DATASET_ARN_DISPLAY = \"ARN for Users/Items dataset\";\n    public static final String AMAZON_PERSONALIZE_DATASET_ARN_DOC = \"The Amazon Resource Name (ARN) of the Users/Items dataset you are adding the user(s)/item(s) to.\";\n    public static final String AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS = \"amazon.personalize.credentials.provider.class\";\n    public static final String AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_PREFIX = AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS.substring(0, AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS.lastIndexOf(\".\")+1);\n    public static final String AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DOC = \"AWS Credential Provider Class\";", "    public static final String AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS = \"amazon.personalize.credentials.provider.class\";\n    public static final String AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_PREFIX = AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS.substring(0, AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS.lastIndexOf(\".\")+1);\n    public static final String AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DOC = \"AWS Credential Provider Class\";\n    public static final String AMAZON_PERSONALIZE_CREDENTIALS_PROVIDER_CLASS_DISPLAY = \"AWS Credential Provider Class\";\n    public static final String AWS_ACCOUNT_DETAILS_GROUP = \"AWS Account Details\";\n    public static final String MAX_RETRIES = \"max.retries\";\n    public static final String NAME_PROPERTY = \"name\";\n    public static final String DEFAULT_CONNECTOR_NAME = \"Personalize-sink\";\n    public static final String CONNECTOR_VERSION_PROPERTIES_FILE_NAME =\"/kafka-connect-personalize-version.properties\";\n    public static final String CONNECTOR_VERSION_PROPERTY_NAME =\"version\";\n    public static final String RECORD_RATE_LIMIT_PROPERTY_NAME =\"record.rate.limit\";", "    public static final String CONNECTOR_VERSION_PROPERTIES_FILE_NAME =\"/kafka-connect-personalize-version.properties\";\n    public static final String CONNECTOR_VERSION_PROPERTY_NAME =\"version\";\n    public static final String RECORD_RATE_LIMIT_PROPERTY_NAME =\"record.rate.limit\";\n    public static final String MAX_TASKS_PROPERTY_NAME =\"tasks.max\";\n    public static final Integer DEFAULT_VALUE_FOR_EVENT_RECORD_RATE = 1000;\n    public static final Integer DEFAULT_VALUE_FOR_ITEM_AND_USER_RECORD_RATE = 10;\n\n\n}\n"]}
{"filename": "src/main/java/com/aws/util/Version.java", "chunked_list": ["package com.aws.util;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.Properties;\n\n/************\n *  This is used for providing version for Connector\n */\npublic class Version {\n\n    private static final Logger log = LoggerFactory.getLogger(Version.class);\n", " *  This is used for providing version for Connector\n */\npublic class Version {\n\n    private static final Logger log = LoggerFactory.getLogger(Version.class);\n\n    public static final String version;\n\n    static {\n        String versionProperty = \"unknown\";\n", "        try {\n            Properties prop = new Properties();\n            prop.load(Version.class.getResourceAsStream(Constants.CONNECTOR_VERSION_PROPERTIES_FILE_NAME));\n            System.out.println(prop);\n            versionProperty = prop.getProperty(Constants.CONNECTOR_VERSION_PROPERTY_NAME, versionProperty).trim();\n        } catch (Exception e) {\n            log.warn(\"Error in loading connector version\");\n            versionProperty = \"unknown\";\n        }\n        version = versionProperty;\n    }\n\n", "    public static String getVersion() {\n        return version;\n    }\n\n}\n"]}
{"filename": "src/main/java/com/aws/transform/CombineFieldsToJSONTransformation.java", "chunked_list": ["package com.aws.transform;\n\nimport org.apache.kafka.common.cache.Cache;\nimport org.apache.kafka.common.cache.LRUCache;\nimport org.apache.kafka.common.cache.SynchronizedCache;\nimport org.apache.kafka.common.config.ConfigDef;\nimport org.apache.kafka.common.config.ConfigException;\nimport org.apache.kafka.connect.connector.ConnectRecord;\nimport org.apache.kafka.connect.data.*;\nimport org.apache.kafka.connect.transforms.Transformation;", "import org.apache.kafka.connect.data.*;\nimport org.apache.kafka.connect.transforms.Transformation;\nimport org.apache.kafka.connect.transforms.util.SchemaUtil;\nimport org.apache.kafka.connect.transforms.util.SimpleConfig;\nimport org.jose4j.json.internal.json_simple.JSONObject;\n\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;", "import java.util.List;\nimport java.util.Map;\n\nimport static org.apache.kafka.connect.transforms.util.Requirements.requireMap;\nimport static org.apache.kafka.connect.transforms.util.Requirements.requireStruct;\n\n/***********\n * This is custom transformation used for constructing properties field value by including multiple fields in kafka records.\n * @param <R>\n */\npublic abstract class CombineFieldsToJSONTransformation <R extends ConnectRecord<R>> implements Transformation<R> {\n", " * @param <R>\n */\npublic abstract class CombineFieldsToJSONTransformation <R extends ConnectRecord<R>> implements Transformation<R> {\n\n    public static final String FIELDS_TO_INCLUDE = \"fieldsToInclude\";\n    public static final String TARGET_FIELD_NAME = \"targetFieldName\";\n    public static final String DEFAULT_FIELD_NAME = \"properties\";\n\n\n    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n            .define(FIELDS_TO_INCLUDE, ConfigDef.Type.LIST, Collections.emptyList(), ConfigDef.Importance.HIGH,\n                    \"Fields to be included in transformation\")\n            .define(TARGET_FIELD_NAME, ConfigDef.Type.STRING, DEFAULT_FIELD_NAME, ConfigDef.Importance.MEDIUM,\n                    \"Name of the transformed field\");\n\n    private  static  final String PURPOSE = \"Merge Properties Field\";\n    private List<String> include;\n    private String combinedFieldName;\n    private Map<String, List<String>> mappedFieldMap;\n    private Cache<Schema, Schema> schemaUpdateCache;\n\n    @Override", "    public static final ConfigDef CONFIG_DEF = new ConfigDef()\n            .define(FIELDS_TO_INCLUDE, ConfigDef.Type.LIST, Collections.emptyList(), ConfigDef.Importance.HIGH,\n                    \"Fields to be included in transformation\")\n            .define(TARGET_FIELD_NAME, ConfigDef.Type.STRING, DEFAULT_FIELD_NAME, ConfigDef.Importance.MEDIUM,\n                    \"Name of the transformed field\");\n\n    private  static  final String PURPOSE = \"Merge Properties Field\";\n    private List<String> include;\n    private String combinedFieldName;\n    private Map<String, List<String>> mappedFieldMap;\n    private Cache<Schema, Schema> schemaUpdateCache;\n\n    @Override", "    public void configure(Map<String, ?> configs){\n        final SimpleConfig config = new SimpleConfig(CONFIG_DEF, configs);\n        include = config.getList(FIELDS_TO_INCLUDE);\n        combinedFieldName = config.getString(TARGET_FIELD_NAME);\n\n        if(null != include && include.isEmpty())\n            throw new ConfigException(\"Field List cant be empty\");\n\n        mappedFieldMap = prepareMappedFieldMap(combinedFieldName, include);\n        schemaUpdateCache = new SynchronizedCache<>(new LRUCache<>(16));\n    }\n\n    private Map<String, List<String>> prepareMappedFieldMap(String combinedFieldName, List<String> include) {\n        Map<String, List<String>> mappedFieldMap = new HashMap<>();\n        mappedFieldMap.put(combinedFieldName,include);\n        return mappedFieldMap;\n    }\n\n    @Override", "    public R apply(R record){\n        if (getValue(record) == null) {\n            return record;\n        } else if (getSchema(record) == null) {\n            return processDataWithoutSchema(record);\n        } else {\n            return processDataWithSchema(record);\n        }\n    }\n\n    private R processDataWithSchema(R record) {\n        final Struct value = requireStruct(getValue(record), PURPOSE);\n\n        Schema updatedSchema = schemaUpdateCache.get(value.schema());", "        if (updatedSchema == null) {\n            updatedSchema = makeUpdatedSchema(value.schema());\n            schemaUpdateCache.put(value.schema(), updatedSchema);\n        }\n\n        final Struct updatedValue = new Struct(updatedSchema);\n\n        for (Field field : updatedSchema.fields()) {\n            if(field.name().equals(combinedFieldName)){\n                JSONObject jsonObject = new JSONObject();\n                for(String oldFieldName : mappedFieldMap.get(combinedFieldName)){\n                    jsonObject.put(oldFieldName,value.get(oldFieldName));\n                }", "            if(field.name().equals(combinedFieldName)){\n                JSONObject jsonObject = new JSONObject();\n                for(String oldFieldName : mappedFieldMap.get(combinedFieldName)){\n                    jsonObject.put(oldFieldName,value.get(oldFieldName));\n                }\n                if(!jsonObject.isEmpty())\n                    updatedValue.put(field.name(), jsonObject.toJSONString());\n            }else {\n                final Object fieldValue = value.get(field.name());\n                updatedValue.put(field.name(), fieldValue);\n            }\n        }\n\n        return createRecord(record, updatedSchema, updatedValue);\n    }\n\n    private R processDataWithoutSchema(R record) {\n        final Map<String, Object> value = requireMap(getValue(record), PURPOSE);\n\n        final Map<String, Object> updatedValue = new HashMap<>(value.size());\n        JSONObject jsonObject = new JSONObject();", "        for (Map.Entry<String, Object> e : value.entrySet()) {\n            final String fieldName = e.getKey();\n            if(null != include &&  !include.isEmpty()){\n                if(include.contains(fieldName)){\n                    jsonObject.put(fieldName, e.getValue());\n                }else{\n                    updatedValue.put(fieldName,e.getValue());\n                }\n            }else{\n                updatedValue.put(fieldName,e.getValue());\n            }\n        }\n", "        if(!jsonObject.isEmpty())\n            updatedValue.put(combinedFieldName, jsonObject.toJSONString());\n\n        return createRecord(record, null, updatedValue);\n    }\n\n    private Schema makeUpdatedSchema(Schema schema) {\n        final SchemaBuilder builder = SchemaUtil.copySchemaBasics(schema, SchemaBuilder.struct());\n        boolean isCombinedFieldAdded = false;\n        for (Field field : schema.fields()) {\n            if (mappedFieldMap.get(combinedFieldName).contains(field.name())) {", "        for (Field field : schema.fields()) {\n            if (mappedFieldMap.get(combinedFieldName).contains(field.name())) {\n                if(!isCombinedFieldAdded) {\n                    builder.field(combinedFieldName, new ConnectSchema(Schema.Type.STRING));\n                    isCombinedFieldAdded= true;\n                }\n            }else {\n                builder.field(field.name(), field.schema());\n            }\n        }\n        return builder.build();\n    }\n\n    @Override", "    public ConfigDef config() {\n        return CONFIG_DEF;\n    }\n\n    @Override\n    public void close() {\n        schemaUpdateCache = null;\n    }\n\n    protected abstract Schema getSchema(R record);\n\n    protected abstract Object getValue(R record);\n\n    protected abstract R createRecord(R record, Schema updatedSchema, Object updatedValue);\n\n\n    /**************\n     * This class is used for Transformation of Key in Kafka record\n     * @param <R>\n     */", "    public static class Key<R extends ConnectRecord<R>> extends CombineFieldsToJSONTransformation<R> {\n\n        @Override\n        protected Schema getSchema(R record){\n            return  record.keySchema();\n        }\n\n        @Override\n        protected Object getValue(R record) {\n            return record.key();\n        }\n\n        @Override\n        protected R createRecord(R record, Schema updatedSchema, Object updatedValue) {\n            return record.newRecord(record.topic(), record.kafkaPartition(), updatedSchema, updatedValue, record.valueSchema(), record.value(), record.timestamp());\n        }\n    }\n\n    /****************\n     * This class is used for Transformation of Value in Kafka record\n     * @param <R>\n     */", "    public static class Value<R extends ConnectRecord<R>> extends CombineFieldsToJSONTransformation<R> {\n\n        @Override\n        protected Schema getSchema(R record) {\n            return record.valueSchema();\n        }\n\n        @Override\n        protected Object getValue(R record) {\n            return record.value();\n        }\n\n        @Override\n        protected R createRecord(R record, Schema updatedSchema, Object updatedValue) {\n            return record.newRecord(record.topic(), record.kafkaPartition(), record.keySchema(), record.key(), updatedSchema, updatedValue, record.timestamp());\n        }\n    }\n}\n"]}
