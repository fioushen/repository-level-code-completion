{"filename": "vcr_eval.py", "chunked_list": ["from tqdm import tqdm\nimport os\nimport yaml\nimport pdb\nimport argparse\nimport re\nimport pdb\n\ndef parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--result', type=str, default='/home/haoxuan/code/SubGPT/exp_result/vcr_04262356/vcr_val/result', \n                        help='VCR predicted result path')\n    parser.add_argument('--print_multiround', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    parser.add_argument('--print_outlier', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    args = parser.parse_args()\n    return args", "def parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--result', type=str, default='/home/haoxuan/code/SubGPT/exp_result/vcr_04262356/vcr_val/result', \n                        help='VCR predicted result path')\n    parser.add_argument('--print_multiround', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    parser.add_argument('--print_outlier', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    args = parser.parse_args()\n    return args", "\n\nargs = parse()\nblip_gpt_folder = args.result\n\nanswer_map = ['1', '2', '3', '4']\ntotal_correct_count = 0\ntotal_unknown_count = 0\ntotal_count = 0\ntotal_round = 0", "total_count = 0\ntotal_round = 0\ntotal_tokens = 0\ntotal_outlier_count = 0\ncount_round = False\n\nfor i in tqdm(os.listdir(blip_gpt_folder), desc='Analyzing BLIP_GPT result ({})'.format(blip_gpt_folder)):\n    cur_dir = os.path.join(blip_gpt_folder, i)\n    with open(cur_dir, \"r\") as file:\n        data_i = yaml.safe_load(file)\n\n    if 'used_round' in data_i['result']:\n        count_round = True\n        total_round += data_i['result']['used_round']\n        total_tokens += data_i['result']['total_tokens']\n\n    if data_i['result']['predict_answer'] is None:\n        total_unknown_count += 1\n        if args.print_outlier:\n            print('Samples with unknown answer prediction:{}'.format(data_i['setting']['id']))\n    else:\n        if str(data_i['result']['predict_answer']).strip() == str(data_i['setting']['answer_label']).strip():\n            total_correct_count += 1\n            if args.print_multiround and 'used_round' in data_i['result']:\n                if data_i['result']['used_round'] > 1:\n                    print('Samples with multiple rounds and correct:{}'.format(data_i['setting']['id']))\n        elif str(data_i['result']['predict_answer']).strip() not in answer_map:\n            total_outlier_count += 1\n            if args.print_outlier:\n                print('Samples with outlier answer prediction:{}'.format(data_i['setting']['id']))\n\n    total_count += 1", "\nprint('Accuracy of {} samples: {}%'.format(total_count, (total_correct_count*1.0*100/total_count)))\nprint('Ratio of unknown samples: {}%'.format(total_unknown_count*1.0*100/total_count))\nprint('Ratio of outlier samples: {}%'.format(total_outlier_count*1.0*100/total_count))\nif count_round:\n    print('Average rounds used: {}'.format(total_round*1.0/total_count))\n    print('Average tokens used: {}'.format(total_tokens*1.0/total_count))"]}
{"filename": "blip_gpt_main.py", "chunked_list": ["import os\nimport yaml\nimport argparse\nimport torch\n\nimport openai\nfrom tqdm import tqdm\nimport pdb\n\nfrom data import VCRSampler", "\nfrom data import VCRSampler\nfrom data import VESampler\n\nfrom chat import VCRConversationTwoAgent\nfrom chat import VEConversationTwoAgent\nimport random\n\ndef IdealGPT(vqa_model, dataset, data_ids, model, save_path='', max_n_rounds=5, print_mode='no', prompt_setting='v1a', temp_gpt=0.0):\n    \"\"\"\n    Conduct IdealGPT conversation\n\n    Args:\n        vqa_model : vqa model.\n        dataset: the dataset used to caption\n        data_ids (list): a list of sample ids in the dataset\n        model (str or Blip2): the model name used to ask quetion. Valid values are 'gpt3', 'chatgpt', and their concrete model names \n                    including 'text-davinci-003', 'davinci,' and 'gpt-3.5-turbo'.\n                    If passing a Blip2 instance, will use its backend LLM.\n        save_path (str): the path to save caption results. If it is empty, results are not being saved.\n        max_n_rounds (int): the max number of chat rounds\n        n_blip2_context (int): how many previous QA rounds can blip2 see. negative value means blip2 can see all \n        print_mode (str): print mode. 'chat' for printing everying. 'bar' for printing everthing but the chat process. 'no' for no printing\n    \"\"\"\n    if model == 'chatgpt':\n        model = 'gpt-3.5-turbo'\n    elif model =='gpt4':\n        model = 'gpt-4'\n\n    all_predict_answer = []\n    all_answer_label = []\n    all_round_number = 0\n    for data_id in tqdm(data_ids, disable=print_mode!='no'):\n        result_path = os.path.join(save_path, 'result', '{}.yaml'.format(data_id))\n        # Skip if the result file exist.\n        if os.path.isfile(result_path):\n            continue\n        if print_mode != 'no':\n            print('Data ID {}'.format(data_id))\n\n        if type(dataset) == VCRSampler:\n            image_path, qa = dataset.fetch_data(data_id)\n            info = {'setting':\n                        {\n                        'id': data_id,\n                        'question_id': qa['question_id'] if 'question_id' in qa else None,\n                        'question': qa['question'].strip(),\n                        'answer_choices':[answer_i.strip() for answer_i in qa['answer_choices']] if 'answer_choices' in qa else None,\n                        'answer_label': str(qa['answer_label']) if 'answer_label' in qa else None,\n                        'max_n_rounds': max_n_rounds,\n                        'img_path': qa['img_path'] if 'img_path' in qa else None\n                        }\n                }\n            if 'caption' in qa:\n                caption = qa['caption']\n            else:\n                caption = None\n        elif type(dataset) == VESampler:\n            image_path, ve_info = dataset.fetch_data(data_id)\n            info = {'setting':\n                        {\n                        'id': data_id,\n                        'hypothesis': ve_info['hypothesis'].strip(),\n                        'answer_label': str(ve_info['answer_label']) if 'answer_label' in ve_info else None,\n                        'max_n_rounds': max_n_rounds,\n                        'img_path': ve_info['img_path'] if 'img_path' in ve_info else None\n                        }\n                }\n            if 'caption' in ve_info:\n                caption = ve_info['caption']\n            else:\n                caption = None\n        results = {}\n        # Initialize VQA Instance.\n        if type(dataset) == VCRSampler:\n            chat = VCRConversationTwoAgent(img=image_path,\n                                vqa_model=vqa_model,\n                                model=model,\n                                question=info['setting']['question'],\n                                answer_choices=info['setting']['answer_choices'],\n                                prompt_setting=prompt_setting,\n                                caption=caption,\n                                temp_gpt=temp_gpt,\n                                data_id=data_id,)\n        elif type(dataset) == VESampler:\n            chat = VEConversationTwoAgent(img=image_path,\n                                vqa_model=vqa_model,\n                                model=model,\n                                question=info['setting']['hypothesis'],\n                                answer_choices=['entailment', 'neutral', 'contradiction'],\n                                prompt_setting=prompt_setting,\n                                caption=caption,\n                                temp_gpt=temp_gpt,\n                                data_id=data_id)\n\n\n        used_round = chat.chatting(max_n_rounds, print_mode=print_mode)\n        results['predict_answer'] = chat.answer_predict\n        results['sub_questions'] = chat.sub_questions\n        results['sub_answers'] = chat.sub_answers\n        results['chat_history'] = chat.chat_history\n        results['total_tokens'] = chat.total_tokens\n        results['caption'] = chat.catpion\n        results['used_round'] = used_round\n\n        info['result'] = results\n\n        all_predict_answer.append(chat.answer_predict)\n        all_answer_label.append(str(info['setting']['answer_label']))\n        all_round_number += results['used_round']\n        \n        if save_path:\n            with open(result_path, 'w') as f:\n                yaml.dump(info, f)\n\n    # Evaluation:\n    if type(dataset) == VCRSampler or type(dataset) == VESampler:\n        # Evaluate VCR and SNLI-VE by acc.\n        total_correct = 0\n        total_exceed_round = 0\n        for predict_i, gt_i in zip(all_predict_answer, all_answer_label):\n            if predict_i == gt_i:\n                total_correct += 1\n            if predict_i is None:\n                total_exceed_round += 1\n        acc = (total_correct*1.0) / len(data_ids)\n        print('Acc:{}%'.format(acc*100))\n        print('Average number of rounds:{}'.format(all_round_number*1.0/len(data_ids)))\n        exceed_round_ratio = (total_exceed_round*1.0) / len(data_ids)\n        print('Unknown Ratio:{}%'.format(exceed_round_ratio*100))", "def IdealGPT(vqa_model, dataset, data_ids, model, save_path='', max_n_rounds=5, print_mode='no', prompt_setting='v1a', temp_gpt=0.0):\n    \"\"\"\n    Conduct IdealGPT conversation\n\n    Args:\n        vqa_model : vqa model.\n        dataset: the dataset used to caption\n        data_ids (list): a list of sample ids in the dataset\n        model (str or Blip2): the model name used to ask quetion. Valid values are 'gpt3', 'chatgpt', and their concrete model names \n                    including 'text-davinci-003', 'davinci,' and 'gpt-3.5-turbo'.\n                    If passing a Blip2 instance, will use its backend LLM.\n        save_path (str): the path to save caption results. If it is empty, results are not being saved.\n        max_n_rounds (int): the max number of chat rounds\n        n_blip2_context (int): how many previous QA rounds can blip2 see. negative value means blip2 can see all \n        print_mode (str): print mode. 'chat' for printing everying. 'bar' for printing everthing but the chat process. 'no' for no printing\n    \"\"\"\n    if model == 'chatgpt':\n        model = 'gpt-3.5-turbo'\n    elif model =='gpt4':\n        model = 'gpt-4'\n\n    all_predict_answer = []\n    all_answer_label = []\n    all_round_number = 0\n    for data_id in tqdm(data_ids, disable=print_mode!='no'):\n        result_path = os.path.join(save_path, 'result', '{}.yaml'.format(data_id))\n        # Skip if the result file exist.\n        if os.path.isfile(result_path):\n            continue\n        if print_mode != 'no':\n            print('Data ID {}'.format(data_id))\n\n        if type(dataset) == VCRSampler:\n            image_path, qa = dataset.fetch_data(data_id)\n            info = {'setting':\n                        {\n                        'id': data_id,\n                        'question_id': qa['question_id'] if 'question_id' in qa else None,\n                        'question': qa['question'].strip(),\n                        'answer_choices':[answer_i.strip() for answer_i in qa['answer_choices']] if 'answer_choices' in qa else None,\n                        'answer_label': str(qa['answer_label']) if 'answer_label' in qa else None,\n                        'max_n_rounds': max_n_rounds,\n                        'img_path': qa['img_path'] if 'img_path' in qa else None\n                        }\n                }\n            if 'caption' in qa:\n                caption = qa['caption']\n            else:\n                caption = None\n        elif type(dataset) == VESampler:\n            image_path, ve_info = dataset.fetch_data(data_id)\n            info = {'setting':\n                        {\n                        'id': data_id,\n                        'hypothesis': ve_info['hypothesis'].strip(),\n                        'answer_label': str(ve_info['answer_label']) if 'answer_label' in ve_info else None,\n                        'max_n_rounds': max_n_rounds,\n                        'img_path': ve_info['img_path'] if 'img_path' in ve_info else None\n                        }\n                }\n            if 'caption' in ve_info:\n                caption = ve_info['caption']\n            else:\n                caption = None\n        results = {}\n        # Initialize VQA Instance.\n        if type(dataset) == VCRSampler:\n            chat = VCRConversationTwoAgent(img=image_path,\n                                vqa_model=vqa_model,\n                                model=model,\n                                question=info['setting']['question'],\n                                answer_choices=info['setting']['answer_choices'],\n                                prompt_setting=prompt_setting,\n                                caption=caption,\n                                temp_gpt=temp_gpt,\n                                data_id=data_id,)\n        elif type(dataset) == VESampler:\n            chat = VEConversationTwoAgent(img=image_path,\n                                vqa_model=vqa_model,\n                                model=model,\n                                question=info['setting']['hypothesis'],\n                                answer_choices=['entailment', 'neutral', 'contradiction'],\n                                prompt_setting=prompt_setting,\n                                caption=caption,\n                                temp_gpt=temp_gpt,\n                                data_id=data_id)\n\n\n        used_round = chat.chatting(max_n_rounds, print_mode=print_mode)\n        results['predict_answer'] = chat.answer_predict\n        results['sub_questions'] = chat.sub_questions\n        results['sub_answers'] = chat.sub_answers\n        results['chat_history'] = chat.chat_history\n        results['total_tokens'] = chat.total_tokens\n        results['caption'] = chat.catpion\n        results['used_round'] = used_round\n\n        info['result'] = results\n\n        all_predict_answer.append(chat.answer_predict)\n        all_answer_label.append(str(info['setting']['answer_label']))\n        all_round_number += results['used_round']\n        \n        if save_path:\n            with open(result_path, 'w') as f:\n                yaml.dump(info, f)\n\n    # Evaluation:\n    if type(dataset) == VCRSampler or type(dataset) == VESampler:\n        # Evaluate VCR and SNLI-VE by acc.\n        total_correct = 0\n        total_exceed_round = 0\n        for predict_i, gt_i in zip(all_predict_answer, all_answer_label):\n            if predict_i == gt_i:\n                total_correct += 1\n            if predict_i is None:\n                total_exceed_round += 1\n        acc = (total_correct*1.0) / len(data_ids)\n        print('Acc:{}%'.format(acc*100))\n        print('Average number of rounds:{}'.format(all_round_number*1.0/len(data_ids)))\n        exceed_round_ratio = (total_exceed_round*1.0) / len(data_ids)\n        print('Unknown Ratio:{}%'.format(exceed_round_ratio*100))", "\n\ndef parse():\n    parser = argparse.ArgumentParser(description='IdealGPT Args.')\n    parser.add_argument('--data_root', type=str, default='/home/haoxuan/data/vcr1/', \n                        help='root path to the dataset')\n    parser.add_argument('--save_root', type=str, default='./exp_result/', \n                        help='root path for saving results')\n    parser.add_argument(\"--data_subset\", type=str, default=None, help=\"specify the subset of the dataset.\")\n    parser.add_argument('--data_partition', type=str, default=None,\n                        help='range of data used, in the format of numberA_numberB, A<=B')\n    parser.add_argument('--exp_tag', type=str, required=True, \n                        help='tag for this experiment. caption results will be saved in save_root/exp_tag')\n    parser.add_argument('--dataset', type=str, default='vcr_val',\n                        help='Names of the dataset to use in the experiment. Valid datasets include vcr_val, ve_dev. Default is vcr_val')\n    parser.add_argument('--max_n_rounds', type=int, default=4,\n                        help='Nax Number of QA rounds between GPT and BLIP-2. Default is 4.')\n    parser.add_argument('--model', type=str, default='chatgpt', choices=['chatgpt', 'gpt4'],\n                        help='model used to ask question. can be gpt3, chatgpt, or its concrete tags in openai system')\n    parser.add_argument('--vqa_model', type=str, default='blip2_t5_xxl', choices=['blip2_t5_xxl', 'blip2_t5_xl',  'blip2_opt_6.7b', 'blip2_opt_2.7b', 'llava', 'minigpt4'],\n                        help='model as Answerer.')\n    parser.add_argument('--device_id', type=int, default=0, \n                        help='Which GPU to use.')\n    parser.add_argument('--prompt_setting', type=str,  default='v1a', \n                        help='Prompt Setting Version')\n    parser.add_argument('--openai_key', type=str,  default='', \n                        help='OpenAI Key for GPT-3.5/4 API')\n    parser.add_argument('--caption_path', type=str,  default=None, \n                        help='Caption path for images')\n    parser.add_argument('--temp_gpt', type=float,  default=0.0, \n                        help='Temperature for GPT')\n    parser.add_argument('--temp_vqa', type=float,  default=0.001, \n                        help='Temperature for VQA model (LLaVA and MiniGPT4), must be positive')\n    parser.add_argument('--seed', type=int, default=3, help='random seed')\n    args = parser.parse_args()\n    return args", "    \n    \ndef main(args):\n    # Set OpenAI\n    OPENAI_API_KEY = args.openai_key\n    openai.api_key = OPENAI_API_KEY\n\n    random.seed(args.seed)\n\n    # load the dataset\n    if 'vcr' in args.dataset:\n        dataset = VCRSampler(dataset_root=args.data_root, \n                             dataset_name=args.dataset, \n                             data_subset=args.data_subset, \n                             data_partition=args.data_partition, \n                             caption_path=args.caption_path)\n    elif 've' in args.dataset:\n        dataset = VESampler(dataset_root=args.data_root,\n                               dataset_name=args.dataset, \n                               data_subset=args.data_subset,\n                               data_partition=args.data_partition, \n                               caption_path=args.caption_path)\n    print('Finish loading data')\n\n    print('Start loading VQA model')\n    if 'blip2' in args.vqa_model:\n        from lib.blip2_lib import Blip2Lavis\n        if 't5' in args.vqa_model and '_xl' in args.vqa_model:\n            vqa_model = Blip2Lavis(name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", device=torch.device(\"cuda:{}\".format(args.device_id)))\n\n        elif 't5' in args.vqa_model and '_xxl' in args.vqa_model:\n            vqa_model = Blip2Lavis(name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", device=torch.device(\"cuda:{}\".format(args.device_id)))\n\n        elif 'opt' in args.vqa_model and '6.7b' in args.vqa_model:\n            vqa_model = Blip2Lavis(name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", device=torch.device(\"cuda:{}\".format(args.device_id)))\n\n        elif 'opt' in args.vqa_model and '2.7b' in args.vqa_model:\n            vqa_model = Blip2Lavis(name=\"blip2_opt\", model_type=\"pretrain_opt2.7b\", device=torch.device(\"cuda:{}\".format(args.device_id)))\n        else:\n            raise NotImplemented(f'{args.vqa_model} not supported')\n    elif 'llava' in args.vqa_model:\n        from lib.llava_lib import LLAVA\n        vqa_model = LLAVA(temperature=args.temp_vqa)\n    elif 'minigpt4' in args.vqa_model:\n        from lib.minigpt4_lib import MINIGPT4\n        vqa_model = MINIGPT4(gpu_id=args.device_id, temperature=args.temp_vqa)\n\n    print('Finish loading VQA model {}'.format(args.vqa_model))\n\n    question_model = args.model\n\n    # preparing the folder to save results\n    save_path = os.path.join(args.save_root, f'{args.dataset}_{args.exp_tag}')\n    if not os.path.exists(save_path):\n        os.makedirs(os.path.join(save_path, 'result'))\n    with open(os.path.join(save_path, 'args.yaml'), 'w') as f:\n        yaml.dump(vars(args), f)\n\n    # start Conversation\n    IdealGPT(vqa_model,\n                dataset, \n                dataset.ids, \n                save_path=save_path, \n                max_n_rounds=args.max_n_rounds, \n                model=question_model,\n                print_mode='no',\n                prompt_setting=args.prompt_setting,\n                temp_gpt=args.temp_gpt)", "    \n\nif __name__ == '__main__':\n    args = parse()\n    main(args)"]}
{"filename": "ve_eval.py", "chunked_list": ["from tqdm import tqdm\nimport os\nimport yaml\nimport pdb\nimport argparse\nimport re\nimport pdb\n\ndef parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--result', type=str, default='/home/haoxuan/code/MiniGPT-4/result/qa/ve/promptv1_temp0.001_ve_dev_random500_pairid', \n                        help='SNLI-VE predicted result path')\n    parser.add_argument('--print_multiround', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    parser.add_argument('--print_outlier', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    args = parser.parse_args()\n    return args", "def parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--result', type=str, default='/home/haoxuan/code/MiniGPT-4/result/qa/ve/promptv1_temp0.001_ve_dev_random500_pairid', \n                        help='SNLI-VE predicted result path')\n    parser.add_argument('--print_multiround', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    parser.add_argument('--print_outlier', action='store_true', \n                        help='Whether print the annoid of multiple rounds and correct samples.')\n    args = parser.parse_args()\n    return args", "\n\nargs = parse()\nblip_gpt_folder = args.result\n\n\n\nanswer_map_counter = {'entailment':0, 'contradiction':0, 'neutral':0, 'outlier':0}\ntotal_correct_count = 0\ntotal_unknown_count = 0", "total_correct_count = 0\ntotal_unknown_count = 0\ntotal_count = 0\ntotal_round = 0\ntotal_tokens = 0\ntotal_outlier_count = 0\ncount_round = False\n\ncount_entailment = 0\ncount_neutral = 0", "count_entailment = 0\ncount_neutral = 0\ncount_contradiction = 0\ncorrect_count_entailment = 0\ncorrect_count_neutral = 0\ncorrect_count_contradiction = 0\n\nfor i in tqdm(os.listdir(blip_gpt_folder), desc='Analyzing BLIP_GPT result ({})'.format(blip_gpt_folder)):\n    cur_dir = os.path.join(blip_gpt_folder, i)\n    with open(cur_dir, \"r\") as file:\n        data_i = yaml.safe_load(file)\n\n    if 'used_round' in data_i['result']:\n        count_round = True\n        total_round += data_i['result']['used_round']\n        total_tokens += data_i['result']['total_tokens']\n\n    if str(data_i['setting']['answer_label']).strip() == 'entailment':\n        count_entailment += 1\n    elif str(data_i['setting']['answer_label']).strip() == 'neutral':\n        count_neutral += 1\n    elif str(data_i['setting']['answer_label']).strip() == 'contradiction':\n        count_contradiction += 1\n\n    if data_i['result']['predict_answer'] is None:\n        total_unknown_count += 1\n        if args.print_outlier:\n            print('Samples with unknown answer prediction:{}'.format(data_i['setting']['annot_id']))\n    else:\n        data_i['result']['predict_answer'] = str(data_i['result']['predict_answer']).strip().lower()\n        if data_i['result']['predict_answer'] == str(data_i['setting']['answer_label']).strip():\n            total_correct_count += 1\n\n            if str(data_i['setting']['answer_label']).strip() == 'entailment':\n                correct_count_entailment += 1\n            elif str(data_i['setting']['answer_label']).strip() == 'neutral':\n                correct_count_neutral += 1\n            elif str(data_i['setting']['answer_label']).strip() == 'contradiction':\n                correct_count_contradiction += 1\n\n            if args.print_multiround and 'used_round' in data_i['result']:\n                if data_i['result']['used_round'] > 1:\n                    print('Samples with multiple rounds and correct:{}'.format(data_i['setting']['annot_id']))\n        if data_i['result']['predict_answer'] in answer_map_counter:\n            answer_map_counter[data_i['result']['predict_answer']] += 1\n        else:\n            answer_map_counter['outlier'] += 1\n            if args.print_outlier:\n                print('Samples with outlier answer prediction:{}'.format(data_i['setting']['annot_id']))            \n\n        # elif str(data_i['result']['predict_answer']).strip() not in answer_map_counter:\n        #     total_outlier_count += 1\n        #     if args.print_outlier:\n        #         print('Samples with outlier answer prediction:{}'.format(data_i['setting']['id']))\n\n    total_count += 1", "\nanswer_map_counter = {k:v*1.0/(total_count-total_unknown_count) for k,v in answer_map_counter.items()} \n\nprint('Accuracy of {} samples: {}%'.format(total_count, (total_correct_count*1.0*100/total_count)))\nprint('Ratio of unknown samples: {}%'.format(total_unknown_count*1.0*100/total_count))\nprint('Distribution of answers: {}%'.format(answer_map_counter))\n\nprint('Entailment: Accuracy: {}%, Correct Number:{}, Total Number: {}'.format((correct_count_entailment*1.0*100/count_entailment), correct_count_entailment, count_entailment))\nprint('Neutral: Accuracy: {}%, Correct Number:{}, Total Number: {}'.format((correct_count_neutral*1.0*100/count_neutral), correct_count_neutral, count_neutral))\nprint('Contradiction: Accuracy: {}%, Correct Number:{}, Total Number: {}'.format((correct_count_contradiction*1.0*100/count_contradiction), correct_count_contradiction, count_contradiction))", "print('Neutral: Accuracy: {}%, Correct Number:{}, Total Number: {}'.format((correct_count_neutral*1.0*100/count_neutral), correct_count_neutral, count_neutral))\nprint('Contradiction: Accuracy: {}%, Correct Number:{}, Total Number: {}'.format((correct_count_contradiction*1.0*100/count_contradiction), correct_count_contradiction, count_contradiction))\n\nif count_round:\n    print('Average rounds used: {}'.format(total_round*1.0/total_count))\n    print('Average tokens used: {}'.format(total_tokens*1.0/total_count))"]}
{"filename": "chat/vcr_conv.py", "chunked_list": ["from tqdm import tqdm\nimport re\nfrom copy import deepcopy\nfrom .call_gpt import call_gpt\nfrom .vcr_prompt import *\n\n\nclass VCRConversationTwoAgent():\n    def __init__(self, img, vqa_model, model, question, answer_choices, data_id, prompt_setting='v1a', caption=None, temp_gpt=0):\n        if type(self) == VCRConversationTwoAgent:\n            assert prompt_setting in ['v1a']\n\n        self.img = img\n        self.vqa_model = vqa_model\n        self.model = model\n\n        self.question = question\n        self.answer_choices = answer_choices\n        self.answer_predict = None\n\n        self.catpion = caption\n        self.sub_questions = []\n        self.sub_answers = []\n        self.chat_history = []\n\n        self.total_tokens = 0\n        self.temp_gpt=temp_gpt\n\n        self.prompt_setting = prompt_setting\n        self.use_caption = True\n\n\n        self.data_id = data_id\n        self.chat_history = {}\n        self.chat_history_init_asker = []\n        self.chat_history_more_asker = []\n        self.chat_history_reasoner = []\n        # add prompts.\n        if type(self) == VCRConversationTwoAgent:\n            if prompt_setting == 'v1a':\n                self.INIT_ASKER_SYSTEM_PROMPT = INIT_ASKER_SYSTEM_PROMPT_V1\n                self.INIT_ASKER_FIRST_QUESTION = INIT_ASKER_FIRST_QUESTION_V1\n\n                self.MORE_ASKER_SYSTEM_PROMPT = MORE_ASKER_SYSTEM_PROMPT_V1\n                self.MORE_ASKER_FIRST_QUESTION = MORE_ASKER_FIRST_QUESTION_V1\n\n                self.REASONER_SYSTEM_PROMPT = REASONER_SYSTEM_PROMPT_V1A\n                self.REASONER_FIRST_QUESTION = REASONER_FIRST_QUESTION_V1A\n                self.FINAL_REASONER_SYSTEM_PROMPT = FINAL_REASONER_SYSTEM_PROMPT_V1A\n            else:\n                raise NotImplementedError(f'{prompt_setting} not supported in class VCRConversationTwoAgent.')\n\n        blip2_QA_prompt = 'Question: placeholder Answer:'\n        llava_QA_prompt = 'placeholder Reply in short.'\n        minigpt4_QA_prompt = 'placeholder Answer it in one short sentence.'\n\n        if 'llava' in self.vqa_model.model_type:\n            self.vqa_prompt =  llava_QA_prompt\n        elif 'minigpt4' in self.vqa_model.model_type:\n            self.vqa_prompt = minigpt4_QA_prompt\n        elif 't5' in self.vqa_model.model_type or 'opt' in self.vqa_model.model_type:\n            self.vqa_prompt =  blip2_QA_prompt\n        else:\n            raise NotImplementedError(f'Could not find vqa prompt for {self.vqa_model.model_type}.')\n        \n\n    def prepare_init_asker_message(self, prompt, caption, question, answer_choices):\n        answer_prompt = ''\n        for ans_id, ans_str in enumerate(answer_choices):\n            answer_prompt = answer_prompt + 'Answer {}: {}\\n'.format(str(ans_id+1), ans_str)\n\n        if self.prompt_setting in ['v1a']:\n            input_prompt = 'Imperfect Caption: {}\\nMain Question: {}\\nFour choices:\\n{}'.format(caption, question, answer_prompt)\n        else:\n            raise NotImplementedError(f'{self.prompt_setting} Not supported')\n\n        input_prompt = prompt.replace('[placeholder]', input_prompt)\n        \n        messages = {'role': 'user', 'content': input_prompt}\n        \n        return messages\n\n\n    def prepare_more_asker_message(self, prompt, caption, question, answer_choices, sub_questions, sub_answers, analysis):\n        answer_prompt = ''\n        for ans_id, ans_str in enumerate(answer_choices):\n            answer_prompt = answer_prompt + 'Answer {}: {}\\n'.format(str(ans_id+1), ans_str)\n\n        sub_answer_prompt = ''\n        flat_sub_questions = []\n        for sub_questions_i in sub_questions:\n            flat_sub_questions.extend(sub_questions_i)\n        flat_sub_answers = []\n        for sub_answers_i in sub_answers:\n            flat_sub_answers.extend(sub_answers_i)\n\n        assert len(flat_sub_questions) == len(flat_sub_answers)\n        for ans_id, ans_str in enumerate(flat_sub_answers):\n            sub_answer_prompt = sub_answer_prompt + 'Sub-question: {} Answer: {}\\n'.format(flat_sub_questions[ans_id], ans_str)\n            \n        if self.prompt_setting in ['v1a']:\n            input_prompt = 'Imperfect Caption: {}\\nMain Question: {}\\nFour choices: \\n{} Sub-questions and answers: \\n{} Analysis: \\n{}'.format(\n                caption, question, answer_prompt, sub_answer_prompt, analysis)\n        else:\n            raise NotImplementedError(f'{self.prompt_setting} Not supported')\n\n        input_prompt = prompt.replace('[placeholder]', input_prompt)\n        \n        messages = {'role': 'user', 'content': input_prompt}\n        \n        return messages\n\n\n    def prepare_reasoner_message(self, prompt, caption, question, answer_choices, sub_questions, sub_answers):\n        answer_prompt = ''\n        for ans_id, ans_str in enumerate(answer_choices):\n            answer_prompt = answer_prompt + 'Answer {}: {}\\n'.format(str(ans_id+1), ans_str)\n\n        sub_answer_prompt = ''\n        flat_sub_questions = []\n        for sub_questions_i in sub_questions:\n            flat_sub_questions.extend(sub_questions_i)\n        flat_sub_answers = []\n        for sub_answers_i in sub_answers:\n            flat_sub_answers.extend(sub_answers_i)\n\n        assert len(flat_sub_questions) == len(flat_sub_answers)\n        for ans_id, ans_str in enumerate(flat_sub_answers):\n            sub_answer_prompt = sub_answer_prompt + 'Sub-question: {} Answer: {}\\n'.format(flat_sub_questions[ans_id], ans_str)\n            \n        if self.prompt_setting in ['v1a']:\n            input_prompt = 'Imperfect Caption: {}\\nMain Question: {}\\nFour choices: \\n{} Existing Sub-questions and answers: \\n{}'.format(\n                caption, question, answer_prompt, sub_answer_prompt)\n        else:\n            raise NotImplementedError(f'{self.prompt_setting} Not supported')\n\n        input_prompt = prompt.replace('[placeholder]', input_prompt)\n        \n        messages = {'role': 'user', 'content': input_prompt}\n        \n        return messages\n    \n\n    def answer_question(self, cur_sub_questions):\n        # prepare the context for blip2\n        sub_answers = []\n        for sub_question_i in cur_sub_questions:\n            vqa_prompt = self.vqa_prompt.replace('placeholder', sub_question_i)\n            # Feed into VQA model.\n            if 'llava' in self.vqa_model.model_type or 'minigpt4' in self.vqa_model.model_type:\n                answer = self.vqa_model.ask(self.img, vqa_prompt)\n            elif 't5' in self.vqa_model.model_type or 'opt' in self.vqa_model.model_type:\n                answer = self.vqa_model.ask(self.img, vqa_prompt, length_penalty=-1, max_length=10)\n            else:\n                raise NotImplementedError(f'Not support VQA of {self.vqa_model.model_type}.')\n\n            answer = self.answer_trim(answer)\n            sub_answers.append(answer)\n        return sub_answers\n\n\n    def answer_trim(self, answer):\n        answer = answer.split('Question:')[0].replace('\\n', ' ').strip()\n        return answer\n\n    def parse_subquestion(self, gpt_response):\n        gpt_response = gpt_response + '\\n'\n        sub_questions = []\n        while True:\n            result = re.search('Sub-question.{0,3}:(.*)\\n', gpt_response)\n            if result is None:\n                break\n            else:\n                sub_questions.append(result.group(1).strip())\n                gpt_response = gpt_response.split(result.group(1))[1]\n\n        return sub_questions\n\n    def parse_final_answer(self, gpt_response, final_round_flag):\n        # ===== Parse the paragraph starting with analysis. =====\n        analysis_result = re.search('Analysis:(.*)\\n', gpt_response)\n        if analysis_result:\n            analysis_string = analysis_result.group(1).strip()\n        else:\n            print(f'Can not parse analysis from {gpt_response}')\n            raise ValueError\n\n        if self.prompt_setting in ['v1a']:\n            substring = \"More Likely Answer:\"\n\n        pattern = f\"{re.escape(substring)}(.*?)(?=\\n|$)\"\n        matches = re.findall(pattern, gpt_response)\n        if matches:\n            answer_string = matches[-1].strip()\n            # In middle rounds, detect 'not sure' at first.\n            if not final_round_flag:\n                if 'not sure' in answer_string.lower():\n                    answer_string = None\n\n            # If 'not sure' is not detected, detect number.\n            if answer_string is not None:\n                # Search for the first occurrence of a number (continuous digits) in the string\n                number_match = re.search(r'\\d+', answer_string)\n                # If a number is found, return it; otherwise, return None\n                if number_match:\n                    answer_string = number_match.group(0)  \n                else:\n                    answer_string = None\n        else:\n            print(f'Can not parse Predicted Answer: from {gpt_response}')\n            raise ValueError\n\n        return analysis_string, answer_string\n    \n    \n    def parse_final_answer_rerun(self, gpt_response, final_round_flag):\n        try:\n            analysis_string, answer_string = self.parse_final_answer(gpt_response=gpt_response, final_round_flag=final_round_flag)\n            need_rerun = False\n            return analysis_string, answer_string, need_rerun\n        except:\n            need_rerun = True\n            return None, None, need_rerun\n\n\n    def break_condition(self, gpt_answer):\n        if gpt_answer in ANSWER_MAP:\n            return True\n        else:\n            return False\n\n\n    def chatting(self, max_n_rounds, print_mode):\n        # Caption first.\n        if self.catpion is None and self.use_caption:\n            self.catpion = self.vqa_model.caption(self.img)\n        \n        self.chat_history = {'init_asker':[],\n                             'more_asker':[],\n                             'reasoner':[]}\n        for round_i in tqdm(range(max_n_rounds), desc='Chat Rounds', disable=print_mode != 'bar'):\n            if round_i == 0:\n                # Prepare initial gpt input for decomposing into sub-questions, and Update chat_history.\n                assert self.catpion != None if self.use_caption else self.catpion == None\n\n                self.chat_history_init_asker = [{\"role\": \"system\", \"content\": self.INIT_ASKER_SYSTEM_PROMPT}]\n                gpt_input = self.prepare_init_asker_message(prompt=self.INIT_ASKER_FIRST_QUESTION, caption=self.catpion, question=self.question, answer_choices=self.answer_choices)\n                self.chat_history_init_asker.append(gpt_input)\n\n                # Run GPT and update chat_history.\n                gpt_response, n_tokens = call_gpt(self.chat_history_init_asker, model=self.model, temp_gpt=self.temp_gpt)\n                self.chat_history_init_asker.append({'role': 'assistant', 'content': gpt_response})\n                self.total_tokens = self.total_tokens + n_tokens\n\n                # Save history\n                self.chat_history['init_asker'].append(self.chat_history_init_asker)\n\n            else:\n                # GPT is not sure, let GPT ask additional questions, and update chat_history.\n                self.chat_history_more_asker = [{\"role\": \"system\", \"content\": self.MORE_ASKER_SYSTEM_PROMPT}]\n                gpt_input = self.prepare_more_asker_message(prompt=self.MORE_ASKER_FIRST_QUESTION, caption=self.catpion, question=self.question, answer_choices=self.answer_choices, \n                                                            sub_questions=self.sub_questions, sub_answers=self.sub_answers, analysis=cur_analysis)\n                self.chat_history_more_asker.append(gpt_input)\n\n                # Run GPT.\n                gpt_response, n_tokens = call_gpt(self.chat_history_more_asker, model=self.model, temp_gpt=self.temp_gpt)\n                self.chat_history_more_asker.append({'role': 'assistant', 'content': gpt_response})\n                self.total_tokens = self.total_tokens + n_tokens\n\n                # Save history\n                self.chat_history['more_asker'].append(self.chat_history_more_asker)\n\n\n            #  Post process GPT response to get sub-questions.\n            cur_sub_questions = self.parse_subquestion(gpt_response)\n            # if len(cur_sub_questions) != 0:\n            self.sub_questions.append(cur_sub_questions)\n\n            # Use VQA model to answer sub-questions.\n            cur_sub_answers = self.answer_question(cur_sub_questions)\n            self.sub_answers.append(cur_sub_answers) \n\n            # Input sub-questions and sub-answers into a reasoner GPT.\n            if round_i == max_n_rounds - 1:\n                self.chat_history_reasoner = [{\"role\": \"system\", \"content\": self.FINAL_REASONER_SYSTEM_PROMPT}]\n            else:\n                self.chat_history_reasoner = [{\"role\": \"system\", \"content\": self.REASONER_SYSTEM_PROMPT}]\n            gpt_input = self.prepare_reasoner_message(prompt=self.REASONER_FIRST_QUESTION, caption=self.catpion, question=self.question, answer_choices=self.answer_choices,\n                                                      sub_questions=self.sub_questions, sub_answers=self.sub_answers)\n            self.chat_history_reasoner.append(gpt_input)\n\n            # Run GPT.\n            try_num = 0\n            max_try = 15\n            # Parse predicted answer from GPT output if any.\n            if round_i == max_n_rounds - 1:\n                final_round_flag = True\n            else:\n                final_round_flag = False\n            while try_num < max_try:\n                try_num += 1\n                if try_num > max_try/2:\n                    cur_temp_gpt = self.temp_gpt + 0.1 * ((2.0*try_num/max_try)-1)\n                else:\n                    cur_temp_gpt = self.temp_gpt\n                gpt_response, n_tokens = call_gpt(self.chat_history_reasoner, model=self.model, temp_gpt=cur_temp_gpt)\n                self.total_tokens = self.total_tokens + n_tokens\n\n                cur_analysis, gpt_answer, need_rerun = self.parse_final_answer_rerun(gpt_response, final_round_flag=final_round_flag)\n                if not need_rerun:\n                    break\n                else:\n                    if try_num == max_try:\n                        raise ValueError('Rerun too many times, still failed in parsing.')\n                    else:\n                        print(f'Parsing failed, Time {try_num} of Rerun GPT Decision for data {self.data_id}.')\n\n            # Save history\n            self.chat_history_reasoner.append({'role': 'assistant', 'content': gpt_response})\n            self.chat_history['reasoner'].append(self.chat_history_reasoner)\n\n            self.answer_predict = gpt_answer\n\n            # If gpt answer satisfies some condition. Finish current loop.\n            if self.break_condition(gpt_answer=gpt_answer):\n                break\n                  \n        return round_i+1"]}
{"filename": "chat/ve_conv.py", "chunked_list": ["from .vcr_conv import VCRConversationTwoAgent\nfrom .call_gpt import call_gpt\nfrom .ve_prompt import *\nimport re\nfrom tqdm import tqdm\nimport copy\n\nclass VEConversationTwoAgent(VCRConversationTwoAgent):\n    def __init__(self, img, vqa_model, model, question, answer_choices, data_id, prompt_setting='v1a', caption=None, temp_gpt=0):\n        super().__init__(img, vqa_model, model, question, answer_choices, prompt_setting, caption, temp_gpt)\n        if type(self) == VEConversationTwoAgent:\n            assert prompt_setting in ['v1a']\n\n        self.data_id = data_id\n        self.prompt_setting = prompt_setting\n        self.chat_history = {}\n        self.chat_history_init_asker = []\n        self.chat_history_more_asker = []\n        self.chat_history_reasoner = []\n        # add prompts.\n        if type(self) == VEConversationTwoAgent:\n            if prompt_setting == 'v1a':\n                self.INIT_ASKER_SYSTEM_PROMPT = INIT_ASKER_SYSTEM_PROMPT_V1\n                self.INIT_ASKER_FIRST_QUESTION = INIT_ASKER_FIRST_QUESTION_V1\n\n                self.MORE_ASKER_SYSTEM_PROMPT = MORE_ASKER_SYSTEM_PROMPT_V1\n                self.MORE_ASKER_FIRST_QUESTION = MORE_ASKER_FIRST_QUESTION_V1\n\n                self.REASONER_SYSTEM_PROMPT = REASONER_SYSTEM_PROMPT_V1A\n                self.REASONER_FIRST_QUESTION = REASONER_FIRST_QUESTION_V1A\n                self.FINAL_REASONER_SYSTEM_PROMPT = FINAL_REASONER_SYSTEM_PROMPT_V1A\n            else:\n                raise NotImplementedError(f'{prompt_setting} not supported in class VEConversationTwoAgent.')\n\n    def prepare_init_asker_message(self, prompt, caption, question, answer_choices): # ['entailment', 'neutral', 'contradiction']\n        answer_prompt = ''\n        for ans_id, ans_str in enumerate(answer_choices):\n            answer_prompt = answer_prompt + 'Answer {}: {}\\n'.format(str(ans_id+1), ans_str)\n\n        if self.prompt_setting in ['v1a']:\n            input_prompt = 'Imperfect Caption: {}\\nHypothesis: {}\\nThree choices:\\n{}'.format(caption, question, answer_prompt)\n        else:\n            raise NotImplementedError(f'{self.prompt_setting} Not supported')\n\n        input_prompt = prompt.replace('[placeholder]', input_prompt)\n        \n        messages = {'role': 'user', 'content': input_prompt}\n        \n        return messages\n\n\n    def prepare_more_asker_message(self, prompt, caption, question, answer_choices, sub_questions, sub_answers, analysis):\n        answer_prompt = ''\n        for ans_id, ans_str in enumerate(answer_choices):\n            answer_prompt = answer_prompt + 'Answer {}: {}\\n'.format(str(ans_id+1), ans_str)\n\n        sub_answer_prompt = ''\n        flat_sub_questions = []\n        for sub_questions_i in sub_questions:\n            flat_sub_questions.extend(sub_questions_i)\n        flat_sub_answers = []\n        for sub_answers_i in sub_answers:\n            flat_sub_answers.extend(sub_answers_i)\n\n        assert len(flat_sub_questions) == len(flat_sub_answers)\n        for ans_id, ans_str in enumerate(flat_sub_answers):\n            sub_answer_prompt = sub_answer_prompt + 'Sub-question: {} Answer: {}\\n'.format(flat_sub_questions[ans_id], ans_str)\n            \n        if self.prompt_setting in ['v1a']:\n            input_prompt = 'Imperfect Caption: {}\\nHypothesis: {}\\nThree choices: \\n{} Sub-questions and answers: \\n{} Analysis: \\n{}'.format(\n                caption, question, answer_prompt, sub_answer_prompt, analysis)\n        else:\n            raise NotImplementedError(f'{self.prompt_setting} Not supported')\n\n        input_prompt = prompt.replace('[placeholder]', input_prompt)\n        \n        messages = {'role': 'user', 'content': input_prompt}\n        \n        return messages\n\n\n    def prepare_reasoner_message(self, prompt, caption, question, answer_choices, sub_questions, sub_answers):\n        answer_prompt = ''\n        for ans_id, ans_str in enumerate(answer_choices):\n            answer_prompt = answer_prompt + 'Answer {}: {}\\n'.format(str(ans_id+1), ans_str)\n\n        sub_answer_prompt = ''\n        flat_sub_questions = []\n        for sub_questions_i in sub_questions:\n            flat_sub_questions.extend(sub_questions_i)\n        flat_sub_answers = []\n        for sub_answers_i in sub_answers:\n            flat_sub_answers.extend(sub_answers_i)\n\n        assert len(flat_sub_questions) == len(flat_sub_answers)\n        for ans_id, ans_str in enumerate(flat_sub_answers):\n            sub_answer_prompt = sub_answer_prompt + 'Sub-question: {} Answer: {}\\n'.format(flat_sub_questions[ans_id], ans_str)\n            \n        if self.prompt_setting in ['v1a']:\n            input_prompt = 'Imperfect Caption: {}\\nHypothesis: {}\\nThree choices: \\n{} Existing Sub-questions and answers: \\n{}'.format(\n                caption, question, answer_prompt, sub_answer_prompt)\n        else:\n            raise NotImplementedError(f'{self.prompt_setting} Not supported')\n\n        input_prompt = prompt.replace('[placeholder]', input_prompt)\n        \n        messages = {'role': 'user', 'content': input_prompt}\n        \n        return messages\n    \n\n    def parse_final_answer(self, gpt_response, final_round_flag):\n        # ===== Parse the paragraph starting with analysis. =====\n        analysis_result = re.search('Analysis:(.*)\\n', gpt_response)\n        if analysis_result:\n            analysis_string = analysis_result.group(1).strip()\n        else:\n            print(f'Can not parse analysis from {gpt_response}')\n            raise ValueError\n\n        if self.prompt_setting in ['v1a']:\n            substring = \"More Likely Answer:\"\n\n        pattern = f\"{re.escape(substring)}(.*?)(?=\\n|$)\"\n        matches = re.findall(pattern, gpt_response) # [' contradiction.']\n        if matches:\n            answer_string = matches[-1].strip() # contradiction.\n            # In middle rounds, detect 'not sure' at first.\n            if not final_round_flag:\n                if 'not sure' in answer_string.lower():\n                    answer_string = None\n\n            # If 'not sure' is not detected, detect number.\n            if answer_string is not None:\n                answer_string = re.sub(r'[^\\w\\s]', '', answer_string) # contradiction\n                # If a number is found, return it; otherwise, return None\n                if answer_string in self.answer_choices:\n                    answer_string = answer_string  \n                else:\n                    answer_string = None\n        else:\n            print(f'Can not parse Predicted Answer: from {gpt_response}')\n            raise ValueError\n\n        return analysis_string, answer_string\n    \n    \n    def parse_final_answer_rerun(self, gpt_response, final_round_flag):\n        try:\n            analysis_string, answer_string = self.parse_final_answer(gpt_response=gpt_response, final_round_flag=final_round_flag)\n            need_rerun = False\n            return analysis_string, answer_string, need_rerun\n        except:\n            need_rerun = True\n            return None, None, need_rerun\n\n\n    def break_condition(self, gpt_answer):\n        if gpt_answer in self.answer_choices:\n            return True\n        else:\n            return False\n\n\n    def chatting(self, max_n_rounds, print_mode):\n        # Caption first.\n        if self.catpion is None and self.use_caption:\n            self.catpion = self.vqa_model.caption(self.img)\n        \n        self.chat_history = {'init_asker':[],\n                             'more_asker':[],\n                             'reasoner':[]}\n        for round_i in tqdm(range(max_n_rounds), desc='Chat Rounds', disable=print_mode != 'bar'):\n            if round_i == 0:\n                # Prepare initial gpt input for decomposing into sub-questions, and Update chat_history.\n                assert self.catpion != None if self.use_caption else self.catpion == None\n\n                self.chat_history_init_asker = [{\"role\": \"system\", \"content\": self.INIT_ASKER_SYSTEM_PROMPT}]\n                gpt_input = self.prepare_init_asker_message(prompt=self.INIT_ASKER_FIRST_QUESTION, caption=self.catpion, question=self.question, answer_choices=self.answer_choices)\n                self.chat_history_init_asker.append(gpt_input)\n\n                # Run GPT and update chat_history.\n                gpt_response, n_tokens = call_gpt(self.chat_history_init_asker, model=self.model, temp_gpt=self.temp_gpt)\n                self.chat_history_init_asker.append({'role': 'assistant', 'content': gpt_response})\n                self.total_tokens = self.total_tokens + n_tokens\n\n                # Save history\n                self.chat_history['init_asker'].append(self.chat_history_init_asker)\n\n            else:\n                # GPT is not sure, let GPT ask additional questions, and update chat_history.\n                self.chat_history_more_asker = [{\"role\": \"system\", \"content\": self.MORE_ASKER_SYSTEM_PROMPT}]\n                gpt_input = self.prepare_more_asker_message(prompt=self.MORE_ASKER_FIRST_QUESTION, caption=self.catpion, question=self.question, answer_choices=self.answer_choices, \n                                                            sub_questions=self.sub_questions, sub_answers=self.sub_answers, analysis=cur_analysis)\n                self.chat_history_more_asker.append(gpt_input)\n\n                # Run GPT.\n                gpt_response, n_tokens = call_gpt(self.chat_history_more_asker, model=self.model, temp_gpt=self.temp_gpt)\n                self.chat_history_more_asker.append({'role': 'assistant', 'content': gpt_response})\n                self.total_tokens = self.total_tokens + n_tokens\n\n                # Save history\n                self.chat_history['more_asker'].append(self.chat_history_more_asker)\n\n\n            #  Post process GPT response to get sub-questions.\n            cur_sub_questions = self.parse_subquestion(gpt_response)\n            # if len(cur_sub_questions) != 0:\n            self.sub_questions.append(cur_sub_questions)\n\n            # Use VQA model to answer sub-questions.\n            cur_sub_answers = self.answer_question(cur_sub_questions)\n            self.sub_answers.append(cur_sub_answers) \n\n            # Input sub-questions and sub-answers into a reasoner GPT.\n            if round_i == max_n_rounds - 1:\n                self.chat_history_reasoner = [{\"role\": \"system\", \"content\": self.FINAL_REASONER_SYSTEM_PROMPT}]\n            else:\n                self.chat_history_reasoner = [{\"role\": \"system\", \"content\": self.REASONER_SYSTEM_PROMPT}]\n            gpt_input = self.prepare_reasoner_message(prompt=self.REASONER_FIRST_QUESTION, caption=self.catpion, question=self.question, answer_choices=self.answer_choices,\n                                                      sub_questions=self.sub_questions, sub_answers=self.sub_answers)\n            self.chat_history_reasoner.append(gpt_input)\n\n            # Run GPT.\n            try_num = 0\n            max_try = 10\n            # Parse predicted answer from GPT output if any.\n            if round_i == max_n_rounds - 1:\n                final_round_flag = True\n            else:\n                final_round_flag = False\n            while try_num < max_try:\n                try_num += 1\n                gpt_response, n_tokens = call_gpt(self.chat_history_reasoner, model=self.model, temp_gpt=self.temp_gpt)\n                self.total_tokens = self.total_tokens + n_tokens\n\n                cur_analysis, gpt_answer, need_rerun = self.parse_final_answer_rerun(gpt_response, final_round_flag=final_round_flag)\n                if not need_rerun:\n                    break\n                else:\n                    if try_num == max_try:\n                        raise ValueError('Rerun too many times, still failed in parsing.')\n                    else:\n                        print(f'Parsing failed, Time {try_num} of Rerun GPT Decision for data {self.data_id}.')\n\n            # Save history\n            self.chat_history_reasoner.append({'role': 'assistant', 'content': gpt_response})\n            self.chat_history['reasoner'].append(self.chat_history_reasoner)\n\n            self.answer_predict = gpt_answer\n\n            # If gpt answer satisfies some condition. Finish current loop.\n            if self.break_condition(gpt_answer=gpt_answer):\n                break\n\n        return round_i+1"]}
{"filename": "chat/__init__.py", "chunked_list": ["from .vcr_conv import VCRConversationTwoAgent\nfrom .ve_conv import VEConversationTwoAgent\nfrom .call_gpt import call_gpt"]}
{"filename": "chat/call_gpt.py", "chunked_list": ["\nimport openai\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n\n@retry(wait=wait_random_exponential(min=0.1, max=0.2), stop=stop_after_attempt(10))\ndef call_gpt(chatgpt_messages, model=\"gpt-3.5-turbo\", temp_gpt=0.0):\n    response = openai.ChatCompletion.create(model=model, messages=chatgpt_messages, temperature=temp_gpt, max_tokens=512)\n    reply = response['choices'][0]['message']['content']\n    total_tokens = response['usage']['total_tokens']\n    return reply, total_tokens", "@retry(wait=wait_random_exponential(min=0.1, max=0.2), stop=stop_after_attempt(10))\ndef call_gpt(chatgpt_messages, model=\"gpt-3.5-turbo\", temp_gpt=0.0):\n    response = openai.ChatCompletion.create(model=model, messages=chatgpt_messages, temperature=temp_gpt, max_tokens=512)\n    reply = response['choices'][0]['message']['content']\n    total_tokens = response['usage']['total_tokens']\n    return reply, total_tokens\n\n"]}
{"filename": "chat/ve_prompt.py", "chunked_list": ["# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n# Prompt for VEConversation \n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n\n# ================ V1 ================\nINIT_ASKER_SYSTEM_PROMPT_V1 = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.", "1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n\nYour goal is:\nTo effectively predict whether the image semantically entails the textual hypothesis and select the answer from entailment, neutral, and contradiction, you should come up with several sub-questions that address the key aspects of the image.\n\nHere are the rules you should follow when listing the sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Sub-question 1: ...?; Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".", "2. List the sub-questions in the following format: \"Sub-question 1: ...?; Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-questions are necessary to distinguish the correct answer.\n\nExample:\n\nHypothesis: A group of women are walking along the railroad tracks.\nSub-question 1: What objects or subjects are present in the image?\nSub-question 2: What actions or events are the people doing?", "Sub-question 1: What objects or subjects are present in the image?\nSub-question 2: What actions or events are the people doing?\nSub-question 3: What is the location where the people are walking?\nSub-question 4: What is the gender of this group of people? '''\n\nINIT_ASKER_FIRST_QUESTION_V1 = '''[placeholder]\nPlease list the sub-questions following the requirement I mentioned before.\n'''\n\nMORE_ASKER_SYSTEM_PROMPT_V1 = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.", "\nMORE_ASKER_SYSTEM_PROMPT_V1 = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions proposed for predicting whether the image semantically entails the textual hypothesis, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not entirely precise.\n4. An analysis of whether the given sub-questions and sub-answers can help to predict whether the image semantically entails the textual hypothesis.\n\nThe current sub-questions and sub-answers are not sufficient to predict whether the image semantically entails the textual hypothesis. Your goal is:\nBased on existing sub-questions and analysis, you should pose additional questions, that can gather more information and are necessary to predict whether the image semantically entails the textual hypothesis.", "The current sub-questions and sub-answers are not sufficient to predict whether the image semantically entails the textual hypothesis. Your goal is:\nBased on existing sub-questions and analysis, you should pose additional questions, that can gather more information and are necessary to predict whether the image semantically entails the textual hypothesis.\n\nHere are the rules you should follow when listing additional sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Additional Sub-question 1: ...?; Additional Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-questions are necessary to distinguish the correct answer.\n", "5. The sub-questions are necessary to distinguish the correct answer.\n\nFormat Example:\n\nAdditional Sub-question 1: xxxx\nAdditional Sub-question 2: xxxx \nAdditional Sub-question 3: xxxx\nAdditional Sub-question 4: xxxx '''\n\nMORE_ASKER_FIRST_QUESTION_V1 = '''[placeholder]", "\nMORE_ASKER_FIRST_QUESTION_V1 = '''[placeholder]\nPlease list the additional sub-questions following the requirement I mentioned before.\n'''\n\n# ================ V1A ================\nREASONER_SYSTEM_PROMPT_V1A = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.", "1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions proposed for predicting whether the image semantically entails the textual hypothesis, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not entirely precise.\n\nYour goal is:\nBased on sub-questions and corresponding answers, you should find the more likely answer from the three answer candidates. \n\nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. If you have found the more likely answer, conclude the correct answer in the format of \"More Likely Answer: entailment/neutral/contradiction\". Otherwise, conclude with \"More Likely Answer: We are not sure which option is correct\".", "1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. If you have found the more likely answer, conclude the correct answer in the format of \"More Likely Answer: entailment/neutral/contradiction\". Otherwise, conclude with \"More Likely Answer: We are not sure which option is correct\".\n\nResponse Format:\n\nAnalysis: xxxxxx.\n\nMore Likely Answer: entailment/neutral/contradiction.\n'''\n", "'''\n\nREASONER_FIRST_QUESTION_V1A = '''[placeholder]\nPlease follow the above-mentioned instruction to list the Analysis and More Likely Answer.\n'''\n\nFINAL_REASONER_SYSTEM_PROMPT_V1A = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.", "1. A textual hypothesis about an image and three answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions proposed for predicting whether the image semantically entails the textual hypothesis, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not entirely precise.\n\nYour goal is:\nBased on sub-questions and corresponding answers, you must find the more likely answer from the three answer candidates. \n\nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. Tell me the more likely answer in the format of \"More Likely Answer: entailment/neutral/contradiction\". Even if you are not confident, you must give a prediction with educated guessing.", "1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. Tell me the more likely answer in the format of \"More Likely Answer: entailment/neutral/contradiction\". Even if you are not confident, you must give a prediction with educated guessing.\n\nResponse Format:\n\nAnalysis: xxxxxx.\n\nMore Likely Answer: entailment/neutral/contradiction.\n'''", "'''"]}
{"filename": "chat/vcr_prompt.py", "chunked_list": ["# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n# Prompt for VCRConversation \n# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nANSWER_MAP = ['1', '2', '3', '4']\n\n# ================ V1 ================\nINIT_ASKER_SYSTEM_PROMPT_V1 = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.", "1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n\nYour goal is:\nTo effectively analyze the image and select the correct answer for the question, you should break down the main question into several sub-questions that address the key aspects of the image.\n\nHere are the rules you should follow when listing the sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Sub-question 1: ...?; Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".", "2. List the sub-questions in the following format: \"Sub-question 1: ...?; Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-question are necessary to distinguish the correct answer.\n\nExample:\n\nMain question: What is happening in the image?\nSub-question 1: What objects or subjects are present in the image?\nSub-question 2: What actions or events is the person doing?", "Sub-question 1: What objects or subjects are present in the image?\nSub-question 2: What actions or events is the person doing?\nSub-question 3: What are the emotions or expressions of the woman?\nSub-question 4: What is the brand of this car? '''\n\nINIT_ASKER_FIRST_QUESTION_V1 = '''[placeholder]\nPlease list the sub-questions following the requirement I mentioned before.\n'''\n\nMORE_ASKER_SYSTEM_PROMPT_V1 = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.", "\nMORE_ASKER_SYSTEM_PROMPT_V1 = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions decomposed from the main question, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not entirely precise.\n4. An analysis of whether the given sub-questions and sub-answers can help to solve the original main question.  \n\nThe current sub-questions and sub-answers are not sufficient to solve the main question. Your goal is:\nBased on existing sub-questions and analysis, you should pose additional questions, that can gather more information and are necessary to solve the main question.", "The current sub-questions and sub-answers are not sufficient to solve the main question. Your goal is:\nBased on existing sub-questions and analysis, you should pose additional questions, that can gather more information and are necessary to solve the main question.\n\nHere are the rules you should follow when listing additional sub-questions.\n1. Ensure that each sub-question is independent. It means the latter sub-questions shouldn't mention previous sub-questions.\n2. List the sub-questions in the following format: \"Additional Sub-question 1: ...?; Additional Sub-question 2: ...?\".\n3. Each sub-question should start with \"What\".\n4. Each sub-question should be short and easy to understand.\n5. The sub-question are necessary to distinguish the correct answer.\n", "5. The sub-question are necessary to distinguish the correct answer.\n\nFormat Example:\n\nAdditional Sub-question 1: xxxx\nAdditional Sub-question 2: xxxx \nAdditional Sub-question 3: xxxx\nAdditional Sub-question 4: xxxx '''\n\nMORE_ASKER_FIRST_QUESTION_V1 = '''[placeholder]", "\nMORE_ASKER_FIRST_QUESTION_V1 = '''[placeholder]\nPlease list the additional sub-questions following the requirement I mentioned before.\n'''\n\n# ================ V1A ================\nREASONER_SYSTEM_PROMPT_V1A = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.", "1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions decomposed from main question, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not entirely precise.\n\nYour goal is:\nBased on sub-questions and corresponding answers, you should find the more likely answer from the four answer candidates. \n\nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. If you have found the more likely answer, conclude the correct answer id in the format of \"More Likely Answer: 1/2/3/4\". Otherwise, conclude with \"More Likely Answer: We are not sure which option is correct\".", "1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. If you have found the more likely answer, conclude the correct answer id in the format of \"More Likely Answer: 1/2/3/4\". Otherwise, conclude with \"More Likely Answer: We are not sure which option is correct\".\n\nResponse Format:\n\nAnalysis: xxxxxx.\n\nMore Likely Answer: 1/2/3/4.\n'''\n", "'''\n\nREASONER_FIRST_QUESTION_V1A = '''[placeholder]\nPlease follow the above-mentioned instruction to list the Analysis and More Likely Answer.\n'''\n\nFINAL_REASONER_SYSTEM_PROMPT_V1A = '''You are an AI assistant who has rich visual commonsense knowledge and strong reasoning abilities.\nYou will be provided with:\n1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.", "1. A main question about an image and four answer candidates.\n2. Although you won't be able to directly view the image, you will receive a general caption that might not be entirely precise but will provide an overall description.\n3. Some sub-questions decomposed from main question, and the corresponding answers are provided by a visual AI model. It's noted that the answers are not entirely precise.\n\nYour goal is:\nBased on sub-questions and corresponding answers, you must find the more likely answer from the four answer candidates. \n\nHere are the rules you should follow in your response:\n1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. Tell me the more likely answer's id in the format of \"More Likely Answer: 1/2/3/4\". Even if you are not confident, you must give a prediction with educated guessing.", "1. At first, demonstrate your reasoning and inference process within one paragraph. Start with the format of \"Analysis:\".\n2. Tell me the more likely answer's id in the format of \"More Likely Answer: 1/2/3/4\". Even if you are not confident, you must give a prediction with educated guessing.\n\nResponse Format:\n\nAnalysis: xxxxxx.\n\nMore Likely Answer: 1/2/3/4.\n'''", "'''"]}
{"filename": "lib/minigpt4_config.py", "chunked_list": ["\"\"\"\n Copyright (c) 2022, salesforce.com, inc.\n All rights reserved.\n SPDX-License-Identifier: BSD-3-Clause\n For full license text, see the LICENSE_Lavis file in the repo root or https://opensource.org/licenses/BSD-3-Clause\n\"\"\"\n\nimport logging\nimport json\nfrom typing import Dict", "import json\nfrom typing import Dict\n\nfrom omegaconf import OmegaConf\n\n# Add MiniGPT-4 project directory into python path to import minigpt4\nimport sys\nsys.path.append('/home/haoxuan/code/MiniGPT-4')\n\nfrom minigpt4.common.registry import registry", "\nfrom minigpt4.common.registry import registry\n\n\nclass Config:\n    def __init__(self, args):\n        self.config = {}\n\n        self.args = args\n\n        # Register the config and configuration for setup\n        registry.register(\"configuration\", self)\n\n        user_config = self._build_opt_list(self.args.options)\n\n        config = OmegaConf.load(self.args.cfg_path)\n\n        runner_config = self.build_runner_config(config)\n        model_config = self.build_model_config(config, **user_config)\n        dataset_config = self.build_dataset_config(config)\n\n        # Validate the user-provided runner configuration\n        # model and dataset configuration are supposed to be validated by the respective classes\n        # [TODO] validate the model/dataset configuration\n        # self._validate_runner_config(runner_config)\n\n        # Override the default configuration with user options.\n        self.config = OmegaConf.merge(\n            runner_config, model_config, dataset_config, user_config\n        )\n\n    def _validate_runner_config(self, runner_config):\n        \"\"\"\n        This method validates the configuration, such that\n            1) all the user specified options are valid;\n            2) no type mismatches between the user specified options and the config.\n        \"\"\"\n        runner_config_validator = create_runner_config_validator()\n        runner_config_validator.validate(runner_config)\n\n    def _build_opt_list(self, opts):\n        opts_dot_list = self._convert_to_dot_list(opts)\n        return OmegaConf.from_dotlist(opts_dot_list)\n\n    @staticmethod\n    def build_model_config(config, **kwargs):\n        model = config.get(\"model\", None)\n        assert model is not None, \"Missing model configuration file.\"\n\n        model_cls = registry.get_model_class(model.arch)\n        assert model_cls is not None, f\"Model '{model.arch}' has not been registered.\"\n\n        model_type = kwargs.get(\"model.model_type\", None)\n        if not model_type:\n            model_type = model.get(\"model_type\", None)\n        # else use the model type selected by user.\n\n        assert model_type is not None, \"Missing model_type.\"\n\n        model_config_path = model_cls.default_config_path(model_type=model_type)\n\n        model_config = OmegaConf.create()\n        # hierarchy override, customized config > default config\n        model_config = OmegaConf.merge(\n            model_config,\n            OmegaConf.load(model_config_path),\n            {\"model\": config[\"model\"]},\n        )\n\n        return model_config\n\n    @staticmethod\n    def build_runner_config(config):\n        return {\"run\": config.run}\n\n    @staticmethod\n    def build_dataset_config(config):\n        datasets = config.get(\"datasets\", None)\n        if datasets is None:\n            raise KeyError(\n                \"Expecting 'datasets' as the root key for dataset configuration.\"\n            )\n\n        dataset_config = OmegaConf.create()\n\n        for dataset_name in datasets:\n            builder_cls = registry.get_builder_class(dataset_name)\n\n            dataset_config_type = datasets[dataset_name].get(\"type\", \"default\")\n            dataset_config_path = builder_cls.default_config_path(\n                type=dataset_config_type\n            )\n\n            # hierarchy override, customized config > default config\n            dataset_config = OmegaConf.merge(\n                dataset_config,\n                OmegaConf.load(dataset_config_path),\n                {\"datasets\": {dataset_name: config[\"datasets\"][dataset_name]}},\n            )\n\n        return dataset_config\n\n    def _convert_to_dot_list(self, opts):\n        if opts is None:\n            opts = []\n\n        if len(opts) == 0:\n            return opts\n\n        has_equal = opts[0].find(\"=\") != -1\n\n        if has_equal:\n            return opts\n\n        return [(opt + \"=\" + value) for opt, value in zip(opts[0::2], opts[1::2])]\n\n    def get_config(self):\n        return self.config\n\n    @property\n    def run_cfg(self):\n        return self.config.run\n\n    @property\n    def datasets_cfg(self):\n        return self.config.datasets\n\n    @property\n    def model_cfg(self):\n        return self.config.model\n\n    def pretty_print(self):\n        logging.info(\"\\n=====  Running Parameters    =====\")\n        logging.info(self._convert_node_to_json(self.config.run))\n\n        logging.info(\"\\n======  Dataset Attributes  ======\")\n        datasets = self.config.datasets\n\n        for dataset in datasets:\n            if dataset in self.config.datasets:\n                logging.info(f\"\\n======== {dataset} =======\")\n                dataset_config = self.config.datasets[dataset]\n                logging.info(self._convert_node_to_json(dataset_config))\n            else:\n                logging.warning(f\"No dataset named '{dataset}' in config. Skipping\")\n\n        logging.info(f\"\\n======  Model Attributes  ======\")\n        logging.info(self._convert_node_to_json(self.config.model))\n\n    def _convert_node_to_json(self, node):\n        container = OmegaConf.to_container(node, resolve=True)\n        return json.dumps(container, indent=4, sort_keys=True)\n\n    def to_dict(self):\n        return OmegaConf.to_container(self.config)", "\n\ndef node_to_dict(node):\n    return OmegaConf.to_container(node)\n\n\nclass ConfigValidator:\n    \"\"\"\n    This is a preliminary implementation to centralize and validate the configuration.\n    May be altered in the future.\n\n    A helper class to validate configurations from yaml file.\n\n    This serves the following purposes:\n        1. Ensure all the options in the yaml are defined, raise error if not.\n        2. when type mismatches are found, the validator will raise an error.\n        3. a central place to store and display helpful messages for supported configurations.\n\n    \"\"\"\n\n    class _Argument:\n        def __init__(self, name, choices=None, type=None, help=None):\n            self.name = name\n            self.val = None\n            self.choices = choices\n            self.type = type\n            self.help = help\n\n        def __str__(self):\n            s = f\"{self.name}={self.val}\"\n            if self.type is not None:\n                s += f\", ({self.type})\"\n            if self.choices is not None:\n                s += f\", choices: {self.choices}\"\n            if self.help is not None:\n                s += f\", ({self.help})\"\n            return s\n\n    def __init__(self, description):\n        self.description = description\n\n        self.arguments = dict()\n\n        self.parsed_args = None\n\n    def __getitem__(self, key):\n        assert self.parsed_args is not None, \"No arguments parsed yet.\"\n\n        return self.parsed_args[key]\n\n    def __str__(self) -> str:\n        return self.format_help()\n\n    def add_argument(self, *args, **kwargs):\n        \"\"\"\n        Assume the first argument is the name of the argument.\n        \"\"\"\n        self.arguments[args[0]] = self._Argument(*args, **kwargs)\n\n    def validate(self, config=None):\n        \"\"\"\n        Convert yaml config (dict-like) to list, required by argparse.\n        \"\"\"\n        for k, v in config.items():\n            assert (\n                k in self.arguments\n            ), f\"\"\"{k} is not a valid argument. Support arguments are {self.format_arguments()}.\"\"\"\n\n            if self.arguments[k].type is not None:\n                try:\n                    self.arguments[k].val = self.arguments[k].type(v)\n                except ValueError:\n                    raise ValueError(f\"{k} is not a valid {self.arguments[k].type}.\")\n\n            if self.arguments[k].choices is not None:\n                assert (\n                    v in self.arguments[k].choices\n                ), f\"\"\"{k} must be one of {self.arguments[k].choices}.\"\"\"\n\n        return config\n\n    def format_arguments(self):\n        return str([f\"{k}\" for k in sorted(self.arguments.keys())])\n\n    def format_help(self):\n        # description + key-value pair string for each argument\n        help_msg = str(self.description)\n        return help_msg + \", available arguments: \" + self.format_arguments()\n\n    def print_help(self):\n        # display help message\n        print(self.format_help())", "\n\ndef create_runner_config_validator():\n    validator = ConfigValidator(description=\"Runner configurations\")\n\n    validator.add_argument(\n        \"runner\",\n        type=str,\n        choices=[\"runner_base\", \"runner_iter\"],\n        help=\"\"\"Runner to use. The \"runner_base\" uses epoch-based training while iter-based\n            runner runs based on iters. Default: runner_base\"\"\",\n    )\n    # add argumetns for training dataset ratios\n    validator.add_argument(\n        \"train_dataset_ratios\",\n        type=Dict[str, float],\n        help=\"\"\"Ratios of training dataset. This is used in iteration-based runner.\n        Do not support for epoch-based runner because how to define an epoch becomes tricky.\n        Default: None\"\"\",\n    )\n    validator.add_argument(\n        \"max_iters\",\n        type=float,\n        help=\"Maximum number of iterations to run.\",\n    )\n    validator.add_argument(\n        \"max_epoch\",\n        type=int,\n        help=\"Maximum number of epochs to run.\",\n    )\n    # add arguments for iters_per_inner_epoch\n    validator.add_argument(\n        \"iters_per_inner_epoch\",\n        type=float,\n        help=\"Number of iterations per inner epoch. This is required when runner is runner_iter.\",\n    )\n    lr_scheds_choices = registry.list_lr_schedulers()\n    validator.add_argument(\n        \"lr_sched\",\n        type=str,\n        choices=lr_scheds_choices,\n        help=\"Learning rate scheduler to use, from {}\".format(lr_scheds_choices),\n    )\n    task_choices = registry.list_tasks()\n    validator.add_argument(\n        \"task\",\n        type=str,\n        choices=task_choices,\n        help=\"Task to use, from {}\".format(task_choices),\n    )\n    # add arguments for init_lr\n    validator.add_argument(\n        \"init_lr\",\n        type=float,\n        help=\"Initial learning rate. This will be the learning rate after warmup and before decay.\",\n    )\n    # add arguments for min_lr\n    validator.add_argument(\n        \"min_lr\",\n        type=float,\n        help=\"Minimum learning rate (after decay).\",\n    )\n    # add arguments for warmup_lr\n    validator.add_argument(\n        \"warmup_lr\",\n        type=float,\n        help=\"Starting learning rate for warmup.\",\n    )\n    # add arguments for learning rate decay rate\n    validator.add_argument(\n        \"lr_decay_rate\",\n        type=float,\n        help=\"Learning rate decay rate. Required if using a decaying learning rate scheduler.\",\n    )\n    # add arguments for weight decay\n    validator.add_argument(\n        \"weight_decay\",\n        type=float,\n        help=\"Weight decay rate.\",\n    )\n    # add arguments for training batch size\n    validator.add_argument(\n        \"batch_size_train\",\n        type=int,\n        help=\"Training batch size.\",\n    )\n    # add arguments for evaluation batch size\n    validator.add_argument(\n        \"batch_size_eval\",\n        type=int,\n        help=\"Evaluation batch size, including validation and testing.\",\n    )\n    # add arguments for number of workers for data loading\n    validator.add_argument(\n        \"num_workers\",\n        help=\"Number of workers for data loading.\",\n    )\n    # add arguments for warm up steps\n    validator.add_argument(\n        \"warmup_steps\",\n        type=int,\n        help=\"Number of warmup steps. Required if a warmup schedule is used.\",\n    )\n    # add arguments for random seed\n    validator.add_argument(\n        \"seed\",\n        type=int,\n        help=\"Random seed.\",\n    )\n    # add arguments for output directory\n    validator.add_argument(\n        \"output_dir\",\n        type=str,\n        help=\"Output directory to save checkpoints and logs.\",\n    )\n    # add arguments for whether only use evaluation\n    validator.add_argument(\n        \"evaluate\",\n        help=\"Whether to only evaluate the model. If true, training will not be performed.\",\n    )\n    # add arguments for splits used for training, e.g. [\"train\", \"val\"]\n    validator.add_argument(\n        \"train_splits\",\n        type=list,\n        help=\"Splits to use for training.\",\n    )\n    # add arguments for splits used for validation, e.g. [\"val\"]\n    validator.add_argument(\n        \"valid_splits\",\n        type=list,\n        help=\"Splits to use for validation. If not provided, will skip the validation.\",\n    )\n    # add arguments for splits used for testing, e.g. [\"test\"]\n    validator.add_argument(\n        \"test_splits\",\n        type=list,\n        help=\"Splits to use for testing. If not provided, will skip the testing.\",\n    )\n    # add arguments for accumulating gradient for iterations\n    validator.add_argument(\n        \"accum_grad_iters\",\n        type=int,\n        help=\"Number of iterations to accumulate gradient for.\",\n    )\n\n    # ====== distributed training ======\n    validator.add_argument(\n        \"device\",\n        type=str,\n        choices=[\"cpu\", \"cuda\"],\n        help=\"Device to use. Support 'cuda' or 'cpu' as for now.\",\n    )\n    validator.add_argument(\n        \"world_size\",\n        type=int,\n        help=\"Number of processes participating in the job.\",\n    )\n    validator.add_argument(\"dist_url\", type=str)\n    validator.add_argument(\"distributed\", type=bool)\n    # add arguments to opt using distributed sampler during evaluation or not\n    validator.add_argument(\n        \"use_dist_eval_sampler\",\n        type=bool,\n        help=\"Whether to use distributed sampler during evaluation or not.\",\n    )\n\n    # ====== task specific ======\n    # generation task specific arguments\n    # add arguments for maximal length of text output\n    validator.add_argument(\n        \"max_len\",\n        type=int,\n        help=\"Maximal length of text output.\",\n    )\n    # add arguments for minimal length of text output\n    validator.add_argument(\n        \"min_len\",\n        type=int,\n        help=\"Minimal length of text output.\",\n    )\n    # add arguments number of beams\n    validator.add_argument(\n        \"num_beams\",\n        type=int,\n        help=\"Number of beams used for beam search.\",\n    )\n\n    # vqa task specific arguments\n    # add arguments for number of answer candidates\n    validator.add_argument(\n        \"num_ans_candidates\",\n        type=int,\n        help=\"\"\"For ALBEF and BLIP, these models first rank answers according to likelihood to select answer candidates.\"\"\",\n    )\n    # add arguments for inference method\n    validator.add_argument(\n        \"inference_method\",\n        type=str,\n        choices=[\"genearte\", \"rank\"],\n        help=\"\"\"Inference method to use for question answering. If rank, requires a answer list.\"\"\",\n    )\n\n    # ====== model specific ======\n    validator.add_argument(\n        \"k_test\",\n        type=int,\n        help=\"Number of top k most similar samples from ITC/VTC selection to be tested.\",\n    )\n\n    return validator", ""]}
{"filename": "lib/llava_lib.py", "chunked_list": ["import argparse\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\nimport torch\nimport os\nimport json\nfrom tqdm import tqdm\nimport shortuuid\n\nfrom llava.conversation import conv_templates\nfrom llava.utils import disable_torch_init", "from llava.conversation import conv_templates\nfrom llava.utils import disable_torch_init\nfrom transformers import CLIPVisionModel, CLIPImageProcessor, StoppingCriteria\n\nfrom PIL import Image\nimport random\nimport math\n\nDEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"", "DEFAULT_IMAGE_TOKEN = \"<image>\"\nDEFAULT_IMAGE_PATCH_TOKEN = \"<im_patch>\"\nDEFAULT_IM_START_TOKEN = \"<im_start>\"\nDEFAULT_IM_END_TOKEN = \"<im_end>\"\n\ndef patch_config(config):\n    patch_dict = {\n        \"use_mm_proj\": True,\n        \"mm_vision_tower\": \"openai/clip-vit-large-patch14\",\n        \"mm_hidden_size\": 1024\n    }\n\n    cfg = AutoConfig.from_pretrained(config)\n    if not hasattr(cfg, \"mm_vision_tower\"):\n        print(f'`mm_vision_tower` not found in `{config}`, applying patch and save to disk.')\n        for k, v in patch_dict.items():\n            setattr(cfg, k, v)\n        cfg.save_pretrained(config)", "\n\n# new stopping implementation\nclass KeywordsStoppingCriteria(StoppingCriteria):\n    def __init__(self, keywords, tokenizer, input_ids):\n        self.keywords = keywords\n        self.tokenizer = tokenizer\n        self.start_len = None\n        self.input_ids = input_ids\n\n    def __call__(self, output_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n        if self.start_len is None:\n            self.start_len = self.input_ids.shape[1]\n        else:\n            outputs = self.tokenizer.batch_decode(output_ids[:, self.start_len:], skip_special_tokens=True)[0]\n            for keyword in self.keywords:\n                if keyword in outputs:\n                    return True\n        return False", "\ndef disable_torch_init():\n    \"\"\"\n    Disable the redundant torch default initialization to accelerate model creation.\n    \"\"\"\n    import torch\n    setattr(torch.nn.Linear, \"reset_parameters\", lambda self: None)\n    setattr(torch.nn.LayerNorm, \"reset_parameters\", lambda self: None)\n\n\nclass LLAVA():\n    def __init__(self, model_path=\"/home/rui/code/LLaVA/LLaVA-13B-v0\", mm_projector_setting=None, \n                 vision_tower_setting=None, conv_mode='simple', temperature=0.001):\n        disable_torch_init()\n        model_name = os.path.expanduser(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        if mm_projector_setting is None:\n            patch_config(model_name)\n            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()\n            image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=torch.float16)\n\n            mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            if mm_use_im_start_end:\n                tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n\n            vision_tower = model.model.vision_tower[0]\n            vision_tower.to(device='cuda', dtype=torch.float16)\n            vision_config = vision_tower.config\n            vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n            vision_config.use_im_start_end = mm_use_im_start_end\n            if mm_use_im_start_end:\n                vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n            image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\n        else:\n            # in case of using a pretrained model with only a MLP projector weights\n            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()\n\n            vision_tower = CLIPVisionModel.from_pretrained(vision_tower_setting, torch_dtype=torch.float16).cuda()\n            image_processor = CLIPImageProcessor.from_pretrained(vision_tower_setting, torch_dtype=torch.float16)\n\n            mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            if mm_use_im_start_end:\n                tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n\n            vision_config = vision_tower.config\n            vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n            vision_config.use_im_start_end = mm_use_im_start_end\n            if mm_use_im_start_end:\n                vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n\n            image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\n\n            mm_projector = torch.nn.Linear(vision_config.hidden_size, model.config.hidden_size)\n            mm_projector_weights = torch.load(mm_projector_setting, map_location='cpu')\n            mm_projector.load_state_dict({k.split('.')[-1]: v for k, v in mm_projector_weights.items()})\n\n            model.model.mm_projector = mm_projector.cuda().half()\n            model.model.vision_tower = [vision_tower]\n\n        self.mm_use_im_start_end = mm_use_im_start_end\n        self.image_token_len = image_token_len\n        self.conv_mode = conv_mode\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.model = model\n        self.temperature = temperature\n        self.model_type = 'llava'\n\n    def decode_output_text(self, output_ids, input_ids, conv):\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n        if n_diff_input_output > 0:\n            print(f'[Warning] Sample: {n_diff_input_output} output_ids are not the same as the input_ids')\n        outputs = self.tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n        # pdb.set_trace()\n        full_history = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n\n        if self.conv_mode == 'simple_legacy' or self.conv_mode == 'simple':\n            while True:\n                cur_len = len(outputs)\n                outputs = outputs.strip()\n                for pattern in ['###', 'Assistant:', 'Response:']:\n                    if outputs.startswith(pattern):\n                        outputs = outputs[len(pattern):].strip()\n                if len(outputs) == cur_len:\n                    break\n\n        try:\n            index = outputs.index(conv.sep)\n        except ValueError:\n            outputs += conv.sep\n            index = outputs.index(conv.sep)\n\n        outputs_response = outputs[:index].strip()\n        # Return both response and full_chat history.\n        return outputs_response, full_history  \n\n    def ask(self, img_path, text):\n        qs = text\n        if self.mm_use_im_start_end:\n            qs = qs + '\\n' + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * self.image_token_len + DEFAULT_IM_END_TOKEN\n        else:\n            qs = qs + '\\n' + DEFAULT_IMAGE_PATCH_TOKEN * self.image_token_len\n\n        if self.conv_mode == 'simple_legacy':\n            qs += '\\n\\n### Response:'\n\n        conv = conv_templates[self.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        prompt = conv.get_prompt()\n        inputs = self.tokenizer([prompt])\n    \n\n        image = Image.open(img_path)\n        image_tensor = self.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\n\n        keywords = ['###']\n        stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\n\n        with torch.inference_mode():\n            output_ids = self.model.generate(\n                input_ids,\n                images=image_tensor.unsqueeze(0).half().cuda(),\n                do_sample=True,\n                temperature=self.temperature,\n                max_new_tokens=1024,\n                stopping_criteria=[stopping_criteria])\n\n        outputs, full_chat = self.decode_output_text(output_ids, input_ids, conv=conv)\n        return outputs\n\n    def caption(self, img_path):\n        return self.ask(img_path=img_path, text='Give a clear and concise summary of the image below in one paragraph.')", "\n\nclass LLAVA():\n    def __init__(self, model_path=\"/home/rui/code/LLaVA/LLaVA-13B-v0\", mm_projector_setting=None, \n                 vision_tower_setting=None, conv_mode='simple', temperature=0.001):\n        disable_torch_init()\n        model_name = os.path.expanduser(model_path)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n        if mm_projector_setting is None:\n            patch_config(model_name)\n            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()\n            image_processor = CLIPImageProcessor.from_pretrained(model.config.mm_vision_tower, torch_dtype=torch.float16)\n\n            mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            if mm_use_im_start_end:\n                tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n\n            vision_tower = model.model.vision_tower[0]\n            vision_tower.to(device='cuda', dtype=torch.float16)\n            vision_config = vision_tower.config\n            vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n            vision_config.use_im_start_end = mm_use_im_start_end\n            if mm_use_im_start_end:\n                vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n            image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\n        else:\n            # in case of using a pretrained model with only a MLP projector weights\n            model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).cuda()\n\n            vision_tower = CLIPVisionModel.from_pretrained(vision_tower_setting, torch_dtype=torch.float16).cuda()\n            image_processor = CLIPImageProcessor.from_pretrained(vision_tower_setting, torch_dtype=torch.float16)\n\n            mm_use_im_start_end = getattr(model.config, \"mm_use_im_start_end\", False)\n            tokenizer.add_tokens([DEFAULT_IMAGE_PATCH_TOKEN], special_tokens=True)\n            if mm_use_im_start_end:\n                tokenizer.add_tokens([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN], special_tokens=True)\n\n            vision_config = vision_tower.config\n            vision_config.im_patch_token = tokenizer.convert_tokens_to_ids([DEFAULT_IMAGE_PATCH_TOKEN])[0]\n            vision_config.use_im_start_end = mm_use_im_start_end\n            if mm_use_im_start_end:\n                vision_config.im_start_token, vision_config.im_end_token = tokenizer.convert_tokens_to_ids([DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN])\n\n            image_token_len = (vision_config.image_size // vision_config.patch_size) ** 2\n\n            mm_projector = torch.nn.Linear(vision_config.hidden_size, model.config.hidden_size)\n            mm_projector_weights = torch.load(mm_projector_setting, map_location='cpu')\n            mm_projector.load_state_dict({k.split('.')[-1]: v for k, v in mm_projector_weights.items()})\n\n            model.model.mm_projector = mm_projector.cuda().half()\n            model.model.vision_tower = [vision_tower]\n\n        self.mm_use_im_start_end = mm_use_im_start_end\n        self.image_token_len = image_token_len\n        self.conv_mode = conv_mode\n        self.tokenizer = tokenizer\n        self.image_processor = image_processor\n        self.model = model\n        self.temperature = temperature\n        self.model_type = 'llava'\n\n    def decode_output_text(self, output_ids, input_ids, conv):\n        input_token_len = input_ids.shape[1]\n        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()\n        if n_diff_input_output > 0:\n            print(f'[Warning] Sample: {n_diff_input_output} output_ids are not the same as the input_ids')\n        outputs = self.tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]\n        # pdb.set_trace()\n        full_history = self.tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0]\n\n        if self.conv_mode == 'simple_legacy' or self.conv_mode == 'simple':\n            while True:\n                cur_len = len(outputs)\n                outputs = outputs.strip()\n                for pattern in ['###', 'Assistant:', 'Response:']:\n                    if outputs.startswith(pattern):\n                        outputs = outputs[len(pattern):].strip()\n                if len(outputs) == cur_len:\n                    break\n\n        try:\n            index = outputs.index(conv.sep)\n        except ValueError:\n            outputs += conv.sep\n            index = outputs.index(conv.sep)\n\n        outputs_response = outputs[:index].strip()\n        # Return both response and full_chat history.\n        return outputs_response, full_history  \n\n    def ask(self, img_path, text):\n        qs = text\n        if self.mm_use_im_start_end:\n            qs = qs + '\\n' + DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_PATCH_TOKEN * self.image_token_len + DEFAULT_IM_END_TOKEN\n        else:\n            qs = qs + '\\n' + DEFAULT_IMAGE_PATCH_TOKEN * self.image_token_len\n\n        if self.conv_mode == 'simple_legacy':\n            qs += '\\n\\n### Response:'\n\n        conv = conv_templates[self.conv_mode].copy()\n        conv.append_message(conv.roles[0], qs)\n        prompt = conv.get_prompt()\n        inputs = self.tokenizer([prompt])\n    \n\n        image = Image.open(img_path)\n        image_tensor = self.image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]\n\n        input_ids = torch.as_tensor(inputs.input_ids).cuda()\n\n        keywords = ['###']\n        stopping_criteria = KeywordsStoppingCriteria(keywords, self.tokenizer, input_ids)\n\n        with torch.inference_mode():\n            output_ids = self.model.generate(\n                input_ids,\n                images=image_tensor.unsqueeze(0).half().cuda(),\n                do_sample=True,\n                temperature=self.temperature,\n                max_new_tokens=1024,\n                stopping_criteria=[stopping_criteria])\n\n        outputs, full_chat = self.decode_output_text(output_ids, input_ids, conv=conv)\n        return outputs\n\n    def caption(self, img_path):\n        return self.ask(img_path=img_path, text='Give a clear and concise summary of the image below in one paragraph.')", "         \n    \nif __name__ == \"__main__\":\n    llava_model = LLAVA()\n    print('successfully initialized llava model')"]}
{"filename": "lib/blip2_lib.py", "chunked_list": ["from lavis.models import load_model_and_preprocess\nfrom PIL import Image\n\nclass Blip2Lavis():\n    def __init__(self, name=\"blip2_opt\", model_type=\"pretrain_opt6.7b\", device=\"cuda\"):\n        self.model_type = model_type\n        self.blip2, self.blip2_vis_processors, _ = load_model_and_preprocess(\n            name=name, model_type=model_type, is_eval=True, device=device)\n        # if 't5xl' in self.model_type:\n        #     self.blip2 = self.blip2.float()\n        self.device = device\n\n    def ask(self, img_path, question, length_penalty=1.0, max_length=30):\n        raw_image = Image.open(img_path).convert('RGB')\n        image = self.blip2_vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n        if 't5' in self.model_type:\n            answer = self.blip2.predict_answers({\"image\": image, \"text_input\": question}, length_penalty=length_penalty, max_length=max_length)\n        else:\n            answer = self.blip2.generate({\"image\": image, \"prompt\": question}, length_penalty=length_penalty, max_length=max_length)\n        answer = answer[0]\n        return answer\n\n    def caption(self, img_path, prompt='a photo of'):\n        # TODO: Multiple captions\n        raw_image = Image.open(img_path).convert('RGB')\n        image = self.blip2_vis_processors[\"eval\"](raw_image).unsqueeze(0).to(self.device)\n        # caption = self.blip2.generate({\"image\": image})\n        caption = self.blip2.generate({\"image\": image, \"prompt\": prompt})\n        caption = caption[0].replace('\\n', ' ').strip()  # trim caption\n        return caption", ""]}
{"filename": "lib/__init__.py", "chunked_list": ["# from .llava_lib import LLAVA\n# from .blip2_lib import Blip2Lavis\n# from .minigpt4_lib import MINIGPT4"]}
{"filename": "lib/minigpt4_lib.py", "chunked_list": ["from .minigpt4_config import Config\n\n# Add MiniGPT-4 project directory into python path to import minigpt4\nimport sys\nsys.path.append('/home/haoxuan/code/MiniGPT-4')\n\nfrom minigpt4.common.registry import registry\nfrom minigpt4.conversation.conversation import Chat, CONV_VISION\n\nclass DictToClass:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)", "\nclass DictToClass:\n    def __init__(self, **entries):\n        self.__dict__.update(entries)\n\nclass MINIGPT4():\n    def __init__(self, cfg_path='/home/haoxuan/code/MiniGPT-4/eval_configs/minigpt4_eval.yaml', gpu_id=0, \n                 options=None, temperature=0.001):\n        args = {'cfg_path':cfg_path, 'gpu_id':gpu_id, 'options':options}\n        args = DictToClass(**args)\n        cfg = Config(args)\n        model_config = cfg.model_cfg\n        model_config.device_8bit = args.gpu_id\n        model_cls = registry.get_model_class(model_config.arch)\n        model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n\n        vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n        vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n        chat = Chat(model, vis_processor, device='cuda:{}'.format(args.gpu_id))\n        print('Initialization of MiniGPT4 Finished')\n\n        self.chat = chat\n        self.temperature = temperature\n        self.model_type = 'minigpt4'\n\n    def ask(self, img_path, text):\n        chat_state = CONV_VISION.copy()\n        img_list = []\n\n        # Start extracting image feature\n        _ = self.chat.upload_img(img_path, chat_state, img_list)\n\n        # Asking questions\n        self.chat.ask(text, chat_state)\n\n        # Generating Answer\n        llm_message = self.chat.answer(conv=chat_state,\n                                    img_list=img_list,\n                                    num_beams=1,\n                                    temperature=self.temperature,\n                                    max_new_tokens=300,\n                                    max_length=2000)[0]\n        return llm_message\n\n    def caption(self, img_path):\n        return self.ask(img_path=img_path, text='Give a clear and concise summary of the image below in one paragraph.')", "         "]}
{"filename": "data/vcr.py", "chunked_list": ["import jsonlines\nimport os\nfrom tqdm import tqdm\nimport json\nfrom copy import deepcopy\nimport yaml\n\nclass VCRSampler():\n    def __init__(self, dataset_root, dataset_name, data_subset, data_partition, caption_path):\n        self.ann = {}\n        if 'val' in dataset_name:\n            dataset_anno_dir = os.path.join(dataset_root, 'val.jsonl')\n\n        # Obtain subset annot_ids\n        id_partitions = None\n        if data_subset is not None:\n            with open(data_subset, \"r\") as file:\n                id_partitions = yaml.safe_load(file)\n\n        with jsonlines.open(dataset_anno_dir) as reader:\n            for cur_ann in tqdm(reader):\n                annot_id = cur_ann['annot_id']\n                # Only keep the input partition.\n                if id_partitions is not None:\n                    if annot_id not in id_partitions:\n                        continue\n\n                img_path = os.path.join(dataset_root, 'vcr1images', cur_ann['img_fn'])\n                meta_path = os.path.join(dataset_root, 'vcr1images', cur_ann['metadata_fn'])\n\n                with open(meta_path, 'r') as f:\n                    metadata = json.load(f)\n\n                # TODO: which one is correct?\n                obj_names = cur_ann['objects']\n                # obj_names = metadata['names']\n\n                obj_boxes = metadata['boxes']\n                obj_width = metadata['width']\n                new_obj_names = self.add_person_location(cur_ann['question'], cur_ann['answer_choices'], obj_names, obj_boxes, obj_width)\n\n                question_sent = self.transform_list2sent(cur_ann['question'], new_obj_names)\n                answer_sents = []\n                # qa_sent = []\n                for answer_i in cur_ann['answer_choices']:\n                    answer_sents.append(self.transform_list2sent(answer_i, new_obj_names))\n\n                # TODO: Add data loading for rationale when doing QA2R task.\n                self.ann[annot_id] = {'question': question_sent, \n                                      'answer_choices': answer_sents,\n                                    #   'rationale_choices': cur_ann['rationale_choices'],\n                                      'img_path': img_path,\n                                      'meta_path': meta_path\n                                      }\n\n                if 'val' in dataset_name or 'train' in dataset_name:\n                    self.ann[annot_id]['answer_label'] = cur_ann['answer_label'] + 1\n                    # self.ann[annot_id]['rationale_label'] = cur_ann['rationale_label']\n                \n                # Load caption if any.\n                if caption_path is not None:\n                    with open(os.path.join(caption_path, '{}.yaml'.format(annot_id)), 'r') as f:\n                        cur_cap = yaml.safe_load(f)\n                    assert cur_cap['annot_id'] == annot_id\n                    self.ann[annot_id]['caption'] = cur_cap['caption']\n\n        # Only keep samples in partitions.\n        if data_partition is not None:\n            start_data_id, end_data_id = data_partition.split('_')\n            subset_ids = list(self.ann.keys())[int(start_data_id):int(end_data_id)+1]\n            self.ann = {key:self.ann[key] for key in subset_ids}\n\n        self._ids = list(self.ann.keys())\n        \n\n    def add_person_location(self, questions, answers, obj_names, obj_boxes, obj_width):\n        referred_person_id = []\n        referred_person_rename = []\n        referred_person_coor = []\n\n        left_range = [0, obj_width/3]\n        middle_range = [obj_width/3, obj_width*2/3]\n        right_range = [obj_width*2/3, obj_width]\n\n        for ii in questions:\n            if isinstance(ii, list) and obj_names[ii[0]] == 'person':\n                if ii[0] not in referred_person_id:\n                    referred_person_id.append(ii[0])\n                    referred_person_rename.append('person')\n                    referred_person_coor.append((obj_boxes[ii[0]][0] + obj_boxes[ii[0]][2])/2)\n        for ans_i in answers:\n            for ii in ans_i:\n                if isinstance(ii, list) and obj_names[ii[0]] == 'person':\n                    if ii[0] not in referred_person_id:\n                        referred_person_id.append(ii[0])\n                        referred_person_rename.append('person')\n                        referred_person_coor.append((obj_boxes[ii[0]][0] + obj_boxes[ii[0]][2])/2)\n\n        if len(referred_person_id) == 0:\n            # Don't make change.\n            return obj_names\n        else:\n            if len(referred_person_id) == 1:\n                cur_person_id = referred_person_id[0]\n                if left_range[0] <= obj_boxes[cur_person_id][0] <= left_range[1]:\n                    referred_person_rename[0] = 'person on the left'\n                elif middle_range[0] < obj_boxes[cur_person_id][0] <= middle_range[1]:\n                    referred_person_rename[0] = 'person in the middle'\n                elif right_range[0] < obj_boxes[cur_person_id][0] <= right_range[1]:\n                    referred_person_rename[0] = 'person on the right'\n            elif len(referred_person_id) == 2:\n                left_right_id = sorted(range(len(referred_person_coor)), key=lambda k: referred_person_coor[k])\n                referred_person_rename[left_right_id[0]] = 'person on the left'\n                referred_person_rename[left_right_id[1]] = 'person on the right'\n            elif len(referred_person_id) == 3:\n                left_right_id = sorted(range(len(referred_person_coor)), key=lambda k: referred_person_coor[k])\n                referred_person_rename[left_right_id[0]] = 'person on the left'\n                referred_person_rename[left_right_id[1]] = 'person in the middle'\n                referred_person_rename[left_right_id[2]] = 'person on the right'\n            else:\n                for box_id, box_coor in enumerate(referred_person_coor):\n                    if left_range[0] <= box_coor <= left_range[1]:\n                        referred_person_rename[box_id] = 'person on the left' if 'person on the left' not in referred_person_rename else 'another person on the left'\n                    elif middle_range[0] < box_coor <= middle_range[1]:\n                        referred_person_rename[box_id] = 'person in the middle' if 'person in the middle' not in referred_person_rename else 'another person in the middle'\n                    elif right_range[0] < box_coor <= right_range[1]:\n                        referred_person_rename[box_id] = 'person  on the right' if 'person on the right' not in referred_person_rename else 'another person on the right'\n            \n            for person_id, person_real_id in enumerate(referred_person_id):\n                obj_names[person_real_id] = referred_person_rename[person_id]\n            return obj_names\n\n    \n    def transform_list2sent(self, input, objs):\n        try:\n            input_sent = [objs[ii[0]] if isinstance(ii, list) else ii  for ii in input]\n        except:\n            print('???')\n        input_sent = (' ').join(input_sent)\n        input_sent = input_sent.replace(' ,', ',')\n        input_sent = input_sent.replace(' .', '.')\n        input_sent = input_sent.replace(' ?', '?')\n        return input_sent\n\n    @property\n    def ids(self):\n        return deepcopy(self._ids)\n\n    \n    def fetch_data(self, id):\n        ann = self.ann[id]\n        img_path = ann['img_path']\n        # raw_image = Image.open(img_path).convert('RGB')\n        \n        return img_path, ann", "\n"]}
{"filename": "data/__init__.py", "chunked_list": ["from .vcr import VCRSampler\nfrom .ve import VESampler"]}
{"filename": "data/ve.py", "chunked_list": ["from .vcr import VCRSampler\nimport json\nimport os\nfrom tqdm import tqdm\nimport yaml\nimport jsonlines\n\nclass VESampler(VCRSampler):\n    def __init__(self, dataset_root, dataset_name, data_subset, data_partition, caption_path) -> None:\n        self.ann = {}\n        if 'dev' in dataset_name:\n            image_path = '/home/tangtangwzc/flickr_data/flickr30k_images/flickr30k_images/' # image path\n            dataset_anno_dir = os.path.join(dataset_root, 'snli_ve_dev.jsonl')\n        else:\n            raise NotImplementedError('Support dev only')\n\n        # Obtain subset annot_ids\n        id_partitions = None\n        if data_subset is not None:\n            with open(data_subset, \"r\") as file:\n                id_partitions = yaml.safe_load(file)\n\n        start_data_id, end_data_id = data_partition.split('_')\n        start_data_id = int(start_data_id)\n        end_data_id = int(end_data_id)\n\n        with jsonlines.open(dataset_anno_dir) as dataset_anno:\n            for index_i, line in enumerate(tqdm(dataset_anno)):\n\n                pair_id = line['pairID']\n                # Only keep the input partition.\n                if id_partitions is not None:\n                    if pair_id not in id_partitions:\n                        continue\n\n                image_id = line['Flikr30kID'] # '2248275918.jpg'\n                img_path = os.path.join(image_path, image_id)\n                caption_gt = line['sentence1'] # ground-truth caption\n                hypothesis = line['sentence2']\n                answer_label = line['gold_label'] # entailment neutral contradiction\n                self.ann[pair_id] = {'hypothesis': hypothesis,\n                                     'img_path': img_path,\n                                     'answer_label': answer_label}\n                # Load caption if any.\n                if caption_path is not None:\n                    with open(os.path.join(caption_path, '{}.yaml'.format(pair_id)), 'r') as f:\n                        cur_cap = yaml.safe_load(f)\n                    assert cur_cap['annot_id'] == pair_id\n                    self.ann[pair_id]['caption'] = cur_cap['caption']\n\n        # Only keep samples in partitions.\n        if data_partition is not None:\n            start_data_id, end_data_id = data_partition.split('_')\n            subset_ids = list(self.ann.keys())[int(start_data_id):int(end_data_id)+1]\n            self.ann = {key:self.ann[key] for key in subset_ids}\n\n        self._ids = list(self.ann.keys())", ""]}
{"filename": "misc/blip_caption.py", "chunked_list": ["import argparse\nimport torch\nimport yaml\nimport json\nfrom tqdm import tqdm\nimport jsonlines\nimport os\nimport sys\n# Get the absolute path to the parent directory\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))", "# Get the absolute path to the parent directory\nparent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n# Add the parent directory to sys.path\nsys.path.insert(0, parent_dir)\nfrom lib.blip2_lib import Blip2Lavis\n\n\nparser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\nparser.add_argument('--device_id', type=int, default=0, \n                    help='Which GPU to use.')", "parser.add_argument('--device_id', type=int, default=0, \n                    help='Which GPU to use.')\nparser.add_argument(\"--dataset\", type=str, default='vcr_val', help=\"specify the dataset to generate caption.\")\nparser.add_argument(\"--data_subset\", type=str, default=None, help=\"specify the subset of the dataset.\")\nparser.add_argument(\"--blip_model\", type=str, default='pretrain_flant5xl', help=\"specify the BLIP2 model.\")\nparser.add_argument(\"--data_partition\", type=str, default=None, help=\"id_id, specify the partition of the dataset.\")\nparser.add_argument('--prompt_setting', type=str,  default='v1', \n                    help='Prompt Setting Version, currently support v1/v2')\nargs = parser.parse_args()\n\ndef load_vcr_data(vcr_data_path):\n    img_paths = {}\n    vcr_anno_path = os.path.join(vcr_data_path, 'val.jsonl')\n\n    # Obtain subset annot_ids\n    id_partitions = None\n    if args.data_subset is not None:\n        with open(args.data_subset, \"r\") as file:\n            id_partitions = yaml.safe_load(file)\n            \n    with jsonlines.open(vcr_anno_path) as reader:\n        for cur_ann in tqdm(reader):\n            annot_id = cur_ann['annot_id']\n            # Only keep the input partition.\n            if id_partitions is not None:\n                if annot_id not in id_partitions:\n                    continue\n            img_path = os.path.join(vcr_data_path, 'vcr1images', cur_ann['img_fn'])\n            img_paths[annot_id] = img_path\n\n    # Only keep samples in partitions.\n    if args.data_partition is not None:\n        start_data_id, end_data_id = args.data_partition.split('_')\n        _ids = list(img_paths)[int(start_data_id):int(end_data_id)+1]\n        img_paths = {key:img_paths[key] for key in _ids}\n\n    return img_paths", "args = parser.parse_args()\n\ndef load_vcr_data(vcr_data_path):\n    img_paths = {}\n    vcr_anno_path = os.path.join(vcr_data_path, 'val.jsonl')\n\n    # Obtain subset annot_ids\n    id_partitions = None\n    if args.data_subset is not None:\n        with open(args.data_subset, \"r\") as file:\n            id_partitions = yaml.safe_load(file)\n            \n    with jsonlines.open(vcr_anno_path) as reader:\n        for cur_ann in tqdm(reader):\n            annot_id = cur_ann['annot_id']\n            # Only keep the input partition.\n            if id_partitions is not None:\n                if annot_id not in id_partitions:\n                    continue\n            img_path = os.path.join(vcr_data_path, 'vcr1images', cur_ann['img_fn'])\n            img_paths[annot_id] = img_path\n\n    # Only keep samples in partitions.\n    if args.data_partition is not None:\n        start_data_id, end_data_id = args.data_partition.split('_')\n        _ids = list(img_paths)[int(start_data_id):int(end_data_id)+1]\n        img_paths = {key:img_paths[key] for key in _ids}\n\n    return img_paths", "\ndef load_ve_data(ve_data_path, image_path):\n    ve_anno_path = os.path.join(ve_data_path, 'snli_ve_dev.jsonl')\n    img_paths = {}\n\n    # Obtain subset annot_ids\n    id_partitions = None\n    if args.data_subset is not None:\n        with open(args.data_subset, \"r\") as file:\n            id_partitions = yaml.safe_load(file)\n\n    with jsonlines.open(ve_anno_path) as jsonl_file:\n        for line in jsonl_file:\n            pair_id = line['pairID']\n            image_name = line['Flikr30kID']\n            if id_partitions is not None:\n                if pair_id not in id_partitions:\n                    continue\n            img_path = os.path.join(image_path, image_name)\n            img_paths[pair_id] = img_path\n\n    # Only keep samples in partitions.\n    if args.data_partition is not None:\n        start_data_id, end_data_id = args.data_partition.split('_')\n        _ids = list(img_paths)[int(start_data_id):int(end_data_id)+1]\n        img_paths = {key:img_paths[key] for key in _ids}\n\n    return img_paths", "\n# ========================================\n#             Data Loading\n# ========================================\nif 'vcr' in args.dataset:\n    vcr_data_path = '/home/haoxuan/data/vcr1/'\n    img_paths = load_vcr_data(vcr_data_path)\nelif 've' in args.dataset:\n    ve_data_path = '/dvmm-filer3a/users/rui/multi-task/datasets/visual_entailment/'\n    ve_image_path = '/home/tangtangwzc/flickr_data/flickr30k_images/flickr30k_images/'\n    img_paths = load_ve_data(ve_data_path, ve_image_path)\nelse:\n    raise NotImplementedError('Not support other datasets yet.')", "print('{} Samples Loading Finished'.format(len(img_paths)))\n\nif args.data_subset is not None:\n    subset_name = args.data_subset.split('/')[-1].split('.yaml')[0]\nelse:\n    subset_name = 'fullset'\n\nif 't5xl' in args.blip_model:\n    vqa_model = Blip2Lavis(name=\"blip2_t5\", model_type=\"pretrain_flant5xl\", device=torch.device(\"cuda:{}\".format(args.device_id)))\nelif 't5xxl' in args.blip_model:\n    vqa_model = Blip2Lavis(name=\"blip2_t5\", model_type=\"pretrain_flant5xxl\", device=torch.device(\"cuda:{}\".format(args.device_id)))\nelse:\n    raise NotImplementedError(f'Not Support {args.blip_model} yet.')", "\nif args.prompt_setting == 'v1':\n    caption_prompt = 'a photo of'\n    result_folder = os.path.join('./exp_data/caption', args.dataset, f'blip_{args.blip_model}_{subset_name}_prompt_aphotoof')\nelif args.prompt_setting == 'v2':\n    caption_prompt = ''\n    result_folder = os.path.join('./exp_data/caption', args.dataset, f'blip_{args.blip_model}_{subset_name}_prompt_v2_none')\n\nif not os.path.exists(result_folder):\n    os.makedirs(result_folder)", "if not os.path.exists(result_folder):\n    os.makedirs(result_folder)\n\n# ========================================\n#             Generating Captions\n# ========================================\nresult = {}\nfor key, img_path in tqdm(img_paths.items(), desc='Generating Captions'):\n    caption = vqa_model.caption(img_path, caption_prompt)\n    result[key] = {\n        'img_fn':img_path,\n        'caption':caption,\n        'annot_id':key}\n\n    with open(os.path.join(result_folder, '{}.yaml'.format(key)), 'w') as f:\n        yaml.dump(result[key], f)", "\n\nwith open(os.path.join(result_folder, 'all_{}.yaml'.format(args.data_partition)), 'w') as f:\n    yaml.dump(result, f)\n\n"]}
{"filename": "misc/check_sample.py", "chunked_list": ["from tqdm import tqdm\nimport os\nimport yaml\nimport pdb\nimport argparse\nimport re\nimport pdb\nimport random\nimport jsonlines\n\ndef parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--result', type=str, default='/home/haoxuan/code/SubGPT/exp_result/vcr_val_random500_t5xl_captionv2_twoagent_promptv1a_multitry/result', \n                        help='VCR predicted result path')\n    parser.add_argument(\"--data_subset\", type=str, default='/home/haoxuan/code/SubGPT/exp_data/vcr_val_random500_annoid.yaml', help=\"specify the subset of the dataset.\")\n    args = parser.parse_args()\n    return args", "import jsonlines\n\ndef parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--result', type=str, default='/home/haoxuan/code/SubGPT/exp_result/vcr_val_random500_t5xl_captionv2_twoagent_promptv1a_multitry/result', \n                        help='VCR predicted result path')\n    parser.add_argument(\"--data_subset\", type=str, default='/home/haoxuan/code/SubGPT/exp_data/vcr_val_random500_annoid.yaml', help=\"specify the subset of the dataset.\")\n    args = parser.parse_args()\n    return args\n", "\n\nargs = parse()\nblip_gpt_folder = args.result\nrandom.seed(3)\n\nwith open(args.data_subset, \"r\") as file:\n    id_partitions = yaml.safe_load(file)\n\nids_insubset = []", "\nids_insubset = []\ndataset_anno_dir = os.path.join('/home/haoxuan/data/vcr1/', 'val.jsonl')\nwith jsonlines.open(dataset_anno_dir) as reader:\n    for cur_ann in tqdm(reader):\n        annot_id = cur_ann['annot_id']\n        if annot_id in id_partitions:\n            ids_insubset.append(annot_id)\n\nresult_ids = os.listdir(blip_gpt_folder)", "\nresult_ids = os.listdir(blip_gpt_folder)\nresult_ids = [i.split('.yaml')[0] for i in result_ids]\n\nfor i, value in enumerate(tqdm(ids_insubset)):\n    if value not in result_ids:\n        print(f'Miss {i}-th data, with id:{value}')\n"]}
{"filename": "misc/__init__.py", "chunked_list": [""]}
{"filename": "misc/sample_data.py", "chunked_list": ["import yaml\nimport argparse\nimport os\nimport jsonlines\nfrom tqdm import tqdm\nimport random\nimport json\n\ndef parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--save_root', type=str, default='./exp_data/', \n                        help='root path for saving results')\n    parser.add_argument('--data_num', type=int, default=500,\n                        help='number of data sampled')\n    parser.add_argument('--dataset', type=str, default='vcr_val',\n                        help='Names of the dataset to use in the experiment. Valid datasets include vcr_val, ve_dev. Default is vcr_val')\n    args = parser.parse_args()\n    return args", "def parse():\n    parser = argparse.ArgumentParser(description='IdealGPT in test datasets.')\n    parser.add_argument('--save_root', type=str, default='./exp_data/', \n                        help='root path for saving results')\n    parser.add_argument('--data_num', type=int, default=500,\n                        help='number of data sampled')\n    parser.add_argument('--dataset', type=str, default='vcr_val',\n                        help='Names of the dataset to use in the experiment. Valid datasets include vcr_val, ve_dev. Default is vcr_val')\n    args = parser.parse_args()\n    return args", "\ndef Sample_VCR(dataset, data_num):\n    dataset_path = '/home/haoxuan/data/vcr1/'\n    if 'val' in dataset:\n        dataset_anno_dir = os.path.join(dataset_path, 'val.jsonl')\n    else:\n        raise NotImplementedError(f'{dataset} Not Supported')\n\n    all_anno_id = []\n    with jsonlines.open(dataset_anno_dir) as reader:\n        for cur_ann in tqdm(reader):\n            all_anno_id.append(cur_ann['annot_id'])\n\n    sampled_anno_id = random.sample(all_anno_id, data_num)\n    return sampled_anno_id", "\ndef Sample_VE(dataset, data_num):\n    dataset_path = '/dvmm-filer3a/users/rui/multi-task/datasets/visual_entailment/'\n    if 'dev' in dataset:\n        dataset_anno_dir = os.path.join(dataset_path, 'snli_ve_dev.jsonl')\n    else:\n        raise NotImplementedError(f'{dataset} Not Supported')\n\n    all_pair_id = []\n    with jsonlines.open(dataset_anno_dir) as reader:\n        for cur_ann in tqdm(reader):\n            all_pair_id.append(cur_ann['pairID'])\n\n    sampled_anno_id = random.sample(all_pair_id, data_num)\n    return sampled_anno_id", "\n\nargs = parse()\nrandom.seed(10)\n\nif not os.path.exists(args.save_root):\n    os.makedirs(args.save_root)\n\nif 'vcr' in args.dataset:\n    sampled_id = Sample_VCR(dataset=args.dataset, data_num=args.data_num)\nelif 've' in args.dataset:\n    sampled_id = Sample_VE(dataset=args.dataset, data_num=args.data_num)", "if 'vcr' in args.dataset:\n    sampled_id = Sample_VCR(dataset=args.dataset, data_num=args.data_num)\nelif 've' in args.dataset:\n    sampled_id = Sample_VE(dataset=args.dataset, data_num=args.data_num)\nprint(f'Finish Sampling {args.dataset}; Obtained {len(sampled_id)} samples.')\n\nif 'vcr' in args.dataset:\n    result_path = os.path.join(args.save_root, f'{args.dataset}_random{args.data_num}_annoid.yaml')\nelif 've' in args.dataset:\n    result_path = os.path.join(args.save_root, f'{args.dataset}_random{args.data_num}_pairid.yaml')", "\n\nwith open(result_path, 'w') as f:\n    yaml.dump(sampled_id, f)\nprint(f'Finish writing to {result_path}')\n"]}
