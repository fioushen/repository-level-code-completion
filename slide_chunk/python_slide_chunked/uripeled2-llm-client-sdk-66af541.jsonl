{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_utils/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_utils/load_json_resource.py", "chunked_list": ["import json\nimport os\nfrom pathlib import Path\n\n\ndef load_json_resource(path: str) -> dict:\n    project_base_path = Path(__file__).parent.parent\n    with open(os.path.join(f'{project_base_path}/resources/', path), \"r\") as f:\n        return json.load(f)\n", ""]}
{"filename": "tests/llm_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_client/local_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_client/local_client/test_local_client.py", "chunked_list": ["from unittest.mock import call\n\nimport pytest\n\nfrom llm_client.llm_client.local_client import LocalClient, LocalClientConfig\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__sanity(local_client, mock_model, mock_tokenizer, tensors_type, device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]", "async def test_text_completion__sanity(local_client, mock_model, mock_tokenizer, tensors_type, device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n    mock_model.generate.return_value = [1]\n    mock_tokenizer.decode.return_value = \"first completion\"\n\n    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\"first completion\"]\n    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)", "    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n    mock_model.generate.assert_called_once_with([1, 2, 3])\n    mock_tokenizer.decode.assert_called_once_with(1)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__return_multiple_completions(local_client, mock_model, mock_tokenizer, tensors_type,\n                                                            device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]", "                                                            device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n    mock_model.generate.return_value = [2, 3, 4]\n    mock_tokenizer.decode.side_effect = [\"first completion\", \"second completion\", \"third completion\"]\n\n    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\"first completion\", \"second completion\", \"third completion\"]\n    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)", "    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n    mock_model.generate.assert_called_once_with([1, 2, 3])\n    assert mock_tokenizer.decode.call_args_list == [call(2), call(3), call(4)]\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_kwargs(local_client, mock_model, mock_tokenizer, tensors_type, device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n    mock_model.generate.return_value = [1]", "    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n    mock_model.generate.return_value = [1]\n    mock_tokenizer.decode.return_value = \"first completion\"\n\n    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\", max_length=100)\n\n    assert actual == [\"first completion\"]\n    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n    mock_model.generate.assert_called_once_with([1, 2, 3], max_length=100)", "    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n    mock_model.generate.assert_called_once_with([1, 2, 3], max_length=100)\n    mock_tokenizer.decode.assert_called_once_with(1)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_encode_kwargs(mock_model, mock_tokenizer, tensors_type, device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n    mock_model.generate.return_value = [1]\n    mock_tokenizer.decode.return_value = \"first completion\"", "    mock_model.generate.return_value = [1]\n    mock_tokenizer.decode.return_value = \"first completion\"\n    encode_kwargs = {\"add_special_tokens\": False}\n    local_client = LocalClient(LocalClientConfig(mock_model, mock_tokenizer, tensors_type, device, encode_kwargs))\n\n    actual = await local_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\"first completion\"]\n    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type,\n                                                  add_special_tokens=False)", "    mock_tokenizer.encode.assert_called_once_with(\"These are a few of my favorite\", return_tensors=tensors_type,\n                                                  add_special_tokens=False)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n    mock_model.generate.assert_called_once_with([1, 2, 3])\n    mock_tokenizer.decode.assert_called_once_with(1)\n\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__sanity(local_client, mock_tokenizer, tensors_type, device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]", "async def test_get_tokens_count__sanity(local_client, mock_tokenizer, tensors_type, device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n\n    actual = await local_client.get_tokens_count(text=\"This is a test\")\n\n    assert actual == 3\n    mock_tokenizer.encode.assert_called_once_with(\"This is a test\", return_tensors=tensors_type)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n\n", "\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__with_kwargs(mock_model, mock_tokenizer, tensors_type, device):\n    mock_tokenizer.encode.return_value.to.return_value = [1, 2, 3]\n    encode_kwargs = {\"add_special_tokens\": False}\n    local_client = LocalClient(LocalClientConfig(mock_model, mock_tokenizer, tensors_type, device, encode_kwargs))\n\n    actual = await local_client.get_tokens_count(text=\"This is a test\")\n", "    actual = await local_client.get_tokens_count(text=\"This is a test\")\n\n    assert actual == 3\n    mock_tokenizer.encode.assert_called_once_with(\"This is a test\", return_tensors=tensors_type,\n                                                  add_special_tokens=False)\n    mock_tokenizer.encode.return_value.to.assert_called_once_with(device)\n"]}
{"filename": "tests/llm_client/local_client/conftest.py", "chunked_list": ["from unittest.mock import MagicMock\n\nimport pytest\nfrom transformers import PreTrainedTokenizerBase\n\nfrom llm_client import LocalClientConfig, LocalClient\n\n\n@pytest.fixture\ndef mock_model():\n    return MagicMock()", "@pytest.fixture\ndef mock_model():\n    return MagicMock()\n\n\n@pytest.fixture\ndef mock_tokenizer():\n    return MagicMock(PreTrainedTokenizerBase)\n\n", "\n\n@pytest.fixture\ndef tensors_type():\n    return \"pt\"\n\n\n@pytest.fixture\ndef device():\n    return \"cpu\"", "def device():\n    return \"cpu\"\n\n\n@pytest.fixture\ndef local_client(mock_model, mock_tokenizer, tensors_type, device):\n    return LocalClient(LocalClientConfig(mock_model, mock_tokenizer, tensors_type, device))\n"]}
{"filename": "tests/llm_api_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_api_client/test_llm_api_client_factory.py", "chunked_list": ["from unittest.mock import patch\n\nimport pytest\n\nfrom llm_client import LLMAPIClientType, LLMAPIClientFactory, LLMAPIClientConfig\nfrom llm_client.llm_api_client.llm_api_client_factory import get_llm_api_client_class\n\n\ndef test_get_llm_api_client__without_context_manager():\n    llm_api_client_factory = LLMAPIClientFactory()\n    with pytest.raises(ValueError):\n        llm_api_client_factory.get_llm_api_client(LLMAPIClientType.OPEN_AI, api_key=\"super secret key\")", "def test_get_llm_api_client__without_context_manager():\n    llm_api_client_factory = LLMAPIClientFactory()\n    with pytest.raises(ValueError):\n        llm_api_client_factory.get_llm_api_client(LLMAPIClientType.OPEN_AI, api_key=\"super secret key\")\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"client_type,client_patch\",\n                         [(LLMAPIClientType.OPEN_AI, \"OpenAIClient\"), (LLMAPIClientType.AI21, \"AI21Client\"),\n                          (LLMAPIClientType.HUGGING_FACE, \"HuggingFaceClient\"),", "                         [(LLMAPIClientType.OPEN_AI, \"OpenAIClient\"), (LLMAPIClientType.AI21, \"AI21Client\"),\n                          (LLMAPIClientType.HUGGING_FACE, \"HuggingFaceClient\"),\n                          (LLMAPIClientType.ALEPH_ALPHA, \"AlephAlphaClient\"),\n                          (LLMAPIClientType.ANTHROPIC, \"AnthropicClient\"),\n                          (LLMAPIClientType.GOOGLE, \"GoogleClient\")])\nasync def test_get_llm_api_client__with_client_type(client_type, client_patch):\n    assert len(LLMAPIClientType) == 6\n\n    llm_api_client_factory = LLMAPIClientFactory()\n    async with llm_api_client_factory:\n        with patch(f\"llm_client.{client_patch}\") as mock_client:\n            actual = llm_api_client_factory.get_llm_api_client(client_type, api_key=\"super secret key\")\n\n            assert actual is mock_client.return_value\n            mock_client.assert_called_once_with(LLMAPIClientConfig(session=llm_api_client_factory._session,\n                                                                   api_key=\"super secret key\"))", "    llm_api_client_factory = LLMAPIClientFactory()\n    async with llm_api_client_factory:\n        with patch(f\"llm_client.{client_patch}\") as mock_client:\n            actual = llm_api_client_factory.get_llm_api_client(client_type, api_key=\"super secret key\")\n\n            assert actual is mock_client.return_value\n            mock_client.assert_called_once_with(LLMAPIClientConfig(session=llm_api_client_factory._session,\n                                                                   api_key=\"super secret key\"))\n\n", "\n\n@pytest.mark.asyncio\nasync def test_get_llm_api_client__with_unknown_client_type():\n    llm_api_client_factory = LLMAPIClientFactory()\n    async with llm_api_client_factory:\n        with pytest.raises(ValueError):\n            llm_api_client_factory.get_llm_api_client(\"unknown-client-type\", api_key=\"super secret key\")\n\n", "\n\n@pytest.mark.parametrize(\"client_type,client_patch\",\n                         [(LLMAPIClientType.OPEN_AI, \"OpenAIClient\"), (LLMAPIClientType.AI21, \"AI21Client\"),\n                          (LLMAPIClientType.HUGGING_FACE, \"HuggingFaceClient\"),\n                          (LLMAPIClientType.ALEPH_ALPHA, \"AlephAlphaClient\"),\n                          (LLMAPIClientType.ANTHROPIC, \"AnthropicClient\"),\n                          (LLMAPIClientType.GOOGLE, \"GoogleClient\")])\ndef test_get_llm_api_client_class(client_type, client_patch):\n    assert len(LLMAPIClientType) == 6\n\n    with patch(f\"llm_client.{client_patch}\") as mock_client:\n        actual = get_llm_api_client_class(client_type)\n\n        assert actual is mock_client\n        mock_client.assert_not_called()", "def test_get_llm_api_client_class(client_type, client_patch):\n    assert len(LLMAPIClientType) == 6\n\n    with patch(f\"llm_client.{client_patch}\") as mock_client:\n        actual = get_llm_api_client_class(client_type)\n\n        assert actual is mock_client\n        mock_client.assert_not_called()\n\n\ndef test_get_llm_api_client_class__with_unknown_client_type():\n    with pytest.raises(ValueError):\n        get_llm_api_client_class(\"unknown-client-type\")", "\n\ndef test_get_llm_api_client_class__with_unknown_client_type():\n    with pytest.raises(ValueError):\n        get_llm_api_client_class(\"unknown-client-type\")\n"]}
{"filename": "tests/llm_api_client/conftest.py", "chunked_list": ["import pytest\nimport pytest_asyncio\nfrom aiohttp import ClientSession\nfrom aioresponses import aioresponses\n\n\n@pytest.fixture\ndef mock_aioresponse():\n    with aioresponses() as m:\n        yield m", "\n\n@pytest_asyncio.fixture\nasync def client_session():\n    session = ClientSession()\n    yield session\n    await session.close()\n"]}
{"filename": "tests/llm_api_client/google_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_api_client/google_client/test_google_client.py", "chunked_list": ["import pytest\n\nfrom llm_client import LLMAPIClientType, LLMAPIClientFactory\nfrom llm_client.consts import PROMPT_KEY\nfrom llm_client.llm_api_client.google_client import TEXT_KEY, GoogleClient, COMPLETE_PATH, AUTH_PARAM, CHAT_PATH, \\\n    EMBEDDING_PATH, TOKENIZE_PATH, MESSAGES_KEY, MESSAGE_CONTENT_KEY, MAX_TOKENS_KEY\nfrom tests.llm_api_client.google_client.conftest import build_url\nfrom tests.test_utils.load_json_resource import load_json_resource\n\n", "\n\n@pytest.mark.asyncio\nasync def test_get_llm_api_client__with_google_client(config):\n    del config.session\n    async with LLMAPIClientFactory() as llm_api_client_factory:\n        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.GOOGLE, **config.__dict__)\n\n    assert isinstance(actual, GoogleClient)\n", "    assert isinstance(actual, GoogleClient)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__sanity(mock_aioresponse, llm_client, params):\n    url = build_url(llm_client, COMPLETE_PATH)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/text_completion.json\")\n    )", "        payload=load_json_resource(\"google/text_completion.json\")\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == ['Once upon a time, there was a young girl named Lily...',\n                      'Once upon a time, there was a young boy named Billy...']\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={PROMPT_KEY: {TEXT_KEY: 'These are a few of my favorite'},\n                                                   MAX_TOKENS_KEY: 64,", "                                             json={PROMPT_KEY: {TEXT_KEY: 'These are a few of my favorite'},\n                                                   MAX_TOKENS_KEY: 64,\n                                                   'temperature': None},\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__override_model(mock_aioresponse, llm_client, params):", "@pytest.mark.asyncio\nasync def test_text_completion__override_model(mock_aioresponse, llm_client, params):\n    new_model_name = \"text-bison-002\"\n    url = build_url(llm_client, COMPLETE_PATH, new_model_name)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/text_completion.json\")\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)", "\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)\n\n    assert actual == ['Once upon a time, there was a young girl named Lily...',\n                      'Once upon a time, there was a young boy named Billy...']\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={PROMPT_KEY: {TEXT_KEY: 'These are a few of my favorite'},\n                                                   MAX_TOKENS_KEY: 64,\n                                                   'temperature': None},\n                                             headers=llm_client._headers,", "                                                   'temperature': None},\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, params):\n    url = build_url(llm_client, COMPLETE_PATH)\n    mock_aioresponse.post(", "    url = build_url(llm_client, COMPLETE_PATH)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/text_completion.json\")\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10, blabla=\"aaa\", top_p= 0.95)\n\n    assert actual == ['Once upon a time, there was a young girl named Lily...',\n                      'Once upon a time, there was a young boy named Billy...']", "    assert actual == ['Once upon a time, there was a young girl named Lily...',\n                      'Once upon a time, there was a young boy named Billy...']\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={PROMPT_KEY: {TEXT_KEY: 'These are a few of my favorite'},\n                                                   MAX_TOKENS_KEY: 10,\n                                                   'temperature': None,\n                                                   'blabla': 'aaa',\"topP\" : 0.95},\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )", "                                             raise_for_status=True,\n                                             )\n\n\n@pytest.mark.asyncio\nasync def test_embedding__sanity(mock_aioresponse, llm_client, params):\n    url = build_url(llm_client, EMBEDDING_PATH)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/embedding.json\")", "        url + params,\n        payload=load_json_resource(\"google/embedding.json\")\n    )\n\n    actual = await llm_client.embedding(text=\"These are a few of my favorite\")\n\n    assert actual == [0.0011238843, -0.040586308, -0.013174802, 0.015497498]\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={TEXT_KEY: 'These are a few of my favorite'},\n                                             headers=llm_client._headers,", "                                             json={TEXT_KEY: 'These are a few of my favorite'},\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )\n\n\n@pytest.mark.asyncio\nasync def test_embedding__override_model(mock_aioresponse, llm_client, params):\n    new_model_name = \"text-bison-002\"\n    url = build_url(llm_client, EMBEDDING_PATH, new_model_name)", "    new_model_name = \"text-bison-002\"\n    url = build_url(llm_client, EMBEDDING_PATH, new_model_name)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/embedding.json\")\n    )\n\n    actual = await llm_client.embedding(text=\"These are a few of my favorite\", model=new_model_name)\n\n    assert actual == [0.0011238843, -0.040586308, -0.013174802, 0.015497498]", "\n    assert actual == [0.0011238843, -0.040586308, -0.013174802, 0.015497498]\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={TEXT_KEY: 'These are a few of my favorite'},\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_embedding__with_kwargs_not_pass_through(mock_aioresponse, llm_client, params):\n    url = build_url(llm_client, EMBEDDING_PATH)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/embedding.json\")\n    )\n\n    actual = await llm_client.embedding(text=\"These are a few of my favorite\", max_tokens=10)", "\n    actual = await llm_client.embedding(text=\"These are a few of my favorite\", max_tokens=10)\n\n    assert actual == [0.0011238843, -0.040586308, -0.013174802, 0.015497498]\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={TEXT_KEY: 'These are a few of my favorite'},\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )\n", "                                             )\n\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__sanity(mock_aioresponse, llm_client, params):\n    url = build_url(llm_client, TOKENIZE_PATH)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/tokens_count.json\")\n    )", "        payload=load_json_resource(\"google/tokens_count.json\")\n    )\n\n    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite\")\n\n    assert actual == 23\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={\n                                                 PROMPT_KEY: {\n                                                     MESSAGES_KEY: [", "                                                 PROMPT_KEY: {\n                                                     MESSAGES_KEY: [\n                                                         {MESSAGE_CONTENT_KEY: \"These are a few of my favorite\"},\n                                                     ]\n                                                 }\n                                             },\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )\n", "                                             )\n\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__override_model(mock_aioresponse, llm_client, params):\n    new_model_name = \"text-bison-002\"\n    url = build_url(llm_client, TOKENIZE_PATH, new_model_name)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/tokens_count.json\")", "        url + params,\n        payload=load_json_resource(\"google/tokens_count.json\")\n    )\n\n    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite\", model=new_model_name)\n\n    assert actual == 23\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={\n                                                 PROMPT_KEY: {", "                                             json={\n                                                 PROMPT_KEY: {\n                                                     MESSAGES_KEY: [\n                                                         {MESSAGE_CONTENT_KEY: \"These are a few of my favorite\"},\n                                                     ]\n                                                 }\n                                             },\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )", "                                             raise_for_status=True,\n                                             )\n\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__kwargs_not_pass_through(mock_aioresponse, llm_client, params):\n    url = build_url(llm_client, TOKENIZE_PATH)\n    mock_aioresponse.post(\n        url + params,\n        payload=load_json_resource(\"google/tokens_count.json\")", "        url + params,\n        payload=load_json_resource(\"google/tokens_count.json\")\n    )\n\n    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite\", max_tokens=10)\n\n    assert actual == 23\n    mock_aioresponse.assert_called_once_with(url, method='POST', params={AUTH_PARAM: llm_client._api_key},\n                                             json={\n                                                 PROMPT_KEY: {", "                                             json={\n                                                 PROMPT_KEY: {\n                                                     MESSAGES_KEY: [\n                                                         {MESSAGE_CONTENT_KEY: \"These are a few of my favorite\"},\n                                                     ]\n                                                 }\n                                             },\n                                             headers=llm_client._headers,\n                                             raise_for_status=True,\n                                             )", "                                             raise_for_status=True,\n                                             )\n"]}
{"filename": "tests/llm_api_client/google_client/conftest.py", "chunked_list": ["from typing import Optional\n\nimport pytest\n\nfrom llm_client.llm_api_client.google_client import GoogleClient, BASE_URL, AUTH_PARAM\nfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig, BaseLLMAPIClient\n\n\n@pytest.fixture\ndef model_name():\n    return \"text-bison-001\"", "@pytest.fixture\ndef model_name():\n    return \"text-bison-001\"\n\n\n@pytest.fixture\ndef config(client_session, model_name):\n    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n\n", "\n\n@pytest.fixture\ndef llm_client(config):\n    return GoogleClient(config)\n\n\n@pytest.fixture\ndef params(llm_client):\n    return \"?\" + AUTH_PARAM + \"=\" + llm_client._api_key", "def params(llm_client):\n    return \"?\" + AUTH_PARAM + \"=\" + llm_client._api_key\n\n\ndef build_url(llm_client: BaseLLMAPIClient, path: str, model: Optional[str] = None) -> str:\n    model = model or llm_client._default_model\n    return BASE_URL + model + \":\" + path\n"]}
{"filename": "tests/llm_api_client/huggingface_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_api_client/huggingface_client/test_huggingface.py", "chunked_list": ["import pytest\n\nfrom llm_client import LLMAPIClientType, LLMAPIClientFactory\nfrom llm_client.llm_api_client.huggingface_client import AUTH_HEADER, \\\n    BEARER_TOKEN, HuggingFaceClient\nfrom tests.test_utils.load_json_resource import load_json_resource\n\n\n@pytest.mark.asyncio\nasync def test_get_llm_api_client__with_hugging_face(config):", "@pytest.mark.asyncio\nasync def test_get_llm_api_client__with_hugging_face(config):\n    del config.session\n    async with LLMAPIClientFactory() as llm_api_client_factory:\n\n        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.HUGGING_FACE, **config.__dict__)\n\n    assert isinstance(actual, HuggingFaceClient)\n\n", "\n\n@pytest.mark.asyncio\nasync def test_text_completion__sanity(mock_aioresponse, llm_client, url):\n    mock_aioresponse.post(\n        url,\n        payload=load_json_resource(\"huggingface/text_completion.json\")\n    )\n\n    actual = await llm_client.text_completion(prompt=\"who is kobe bryant\")", "\n    actual = await llm_client.text_completion(prompt=\"who is kobe bryant\")\n\n    assert actual == ['Kobe Bryant is a retired professional basketball player who played for the Los Angeles Lakers of']\n    mock_aioresponse.assert_called_once_with(url, method='POST',\n                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n                                             json={'inputs': 'who is kobe bryant',\"max_length\": None, \"temperature\": 1.0, \"top_p\" : None},\n                                             raise_for_status=True)\n\n", "\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, url):\n    mock_aioresponse.post(\n        url,\n        payload=load_json_resource(\"huggingface/text_completion.json\")\n    )\n\n    actual = await llm_client.text_completion(prompt=\"who is kobe bryant\",max_tokens = 10)", "\n    actual = await llm_client.text_completion(prompt=\"who is kobe bryant\",max_tokens = 10)\n\n    assert actual == ['Kobe Bryant is a retired professional basketball player who played for the Los Angeles Lakers of']\n    mock_aioresponse.assert_called_once_with(url, method='POST',\n                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n                                             json={'inputs': 'who is kobe bryant',\"max_length\": 10, \"temperature\": 1.0, \"top_p\" : None},\n                                             raise_for_status=True)\n\n", "\n\n@pytest.mark.asyncio\ndef test_get_tokens_count__sanity(mock_aioresponse, llm_client, url):\n    actual = llm_client.get_tokens_count(text=\"is queen elisabeth alive?\")\n    assert actual == 7\n"]}
{"filename": "tests/llm_api_client/huggingface_client/conftest.py", "chunked_list": ["import pytest\n\nfrom llm_client.llm_api_client.huggingface_client import HuggingFaceClient, BASE_URL\nfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\n\n\n@pytest.fixture\ndef model_name():\n    return \"oasst-sft-4-pythia-12b-epoch-3.5\"\n", "\n\n@pytest.fixture\ndef config(client_session, model_name):\n    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n\n\n@pytest.fixture\ndef llm_client(config):\n    return HuggingFaceClient(config)", "def llm_client(config):\n    return HuggingFaceClient(config)\n\n\n@pytest.fixture\ndef url(model_name):\n    return build_url(model_name)\n\n\ndef build_url(model: str) -> str:\n    return BASE_URL + model + \"/\"", "\ndef build_url(model: str) -> str:\n    return BASE_URL + model + \"/\"\n"]}
{"filename": "tests/llm_api_client/ai21_client/test_ai21.py", "chunked_list": ["import pytest\n\nfrom llm_client import LLMAPIClientType, LLMAPIClientFactory\nfrom llm_client.llm_api_client.ai21_client import BASE_URL, DATA_KEY, TEXT_KEY, TOKENIZE_PATH, AUTH_HEADER, \\\n    BEARER_TOKEN, AI21Client\nfrom tests.llm_api_client.ai21_client.conftest import build_url\nfrom tests.test_utils.load_json_resource import load_json_resource\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_get_llm_api_client__with_ai21(config):\n    del config.session\n    async with LLMAPIClientFactory() as llm_api_client_factory:\n        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.AI21, **config.__dict__)\n\n    assert isinstance(actual, AI21Client)\n\n", "\n\n@pytest.mark.asyncio\nasync def test_text_completion__sanity(mock_aioresponse, llm_client, url):\n    mock_aioresponse.post(\n        url,\n        payload=load_json_resource(\"ai21/text_completion.json\")\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\")", "\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\n        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties']\n    mock_aioresponse.assert_called_once_with(url, method='POST',\n                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key },\n                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 16, \"temperature\" : 0.7, \"topP\" : 1 },\n                                             raise_for_status=True)", "                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 16, \"temperature\" : 0.7, \"topP\" : 1 },\n                                             raise_for_status=True)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__return_multiple_completions(mock_aioresponse, llm_client, url):\n    payload = load_json_resource(\"ai21/text_completion.json\")\n    payload[\"completions\"].append({DATA_KEY: {TEXT_KEY: \"second completion\"}})\n    mock_aioresponse.post(url, payload=payload)\n", "    mock_aioresponse.post(url, payload=payload)\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\n        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties',\n        \"second completion\"\n    ]\n    mock_aioresponse.assert_called_once_with(url, method='POST',", "    ]\n    mock_aioresponse.assert_called_once_with(url, method='POST',\n                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 16, \"temperature\" : 0.7, \"topP\" : 1  },\n                                             raise_for_status=True)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__override_model(mock_aioresponse, llm_client):\n    new_model_name = \"gpt3\"", "async def test_text_completion__override_model(mock_aioresponse, llm_client):\n    new_model_name = \"gpt3\"\n    url = build_url(new_model_name)\n    mock_aioresponse.post(\n        url,\n        payload=load_json_resource(\"ai21/text_completion.json\")\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)\n", "    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)\n\n    assert actual == [\n        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties']\n    mock_aioresponse.assert_called_once_with(url, method='POST',\n                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 16, \"temperature\" : 0.7, \"topP\" : 1 },\n                                             raise_for_status=True)\n", "                                             raise_for_status=True)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, url):\n    mock_aioresponse.post(\n        url,\n        payload=load_json_resource(\"ai21/text_completion.json\")\n    )\n", "    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10)\n\n    assert actual == [\n        ' things!\\n\\nI love entertaining, entertaining and decorating my home, entertaining clients, entertaining '\n        'friends, entertaining family...you get the point! One of my favorite things to do is plan parties']\n    mock_aioresponse.assert_called_once_with(url, method='POST',\n                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 10, \"temperature\" : 0.7 ,\"topP\" : 1},", "                                             headers={AUTH_HEADER: BEARER_TOKEN + llm_client._api_key},\n                                             json={'prompt': 'These are a few of my favorite', \"maxTokens\" : 10, \"temperature\" : 0.7 ,\"topP\" : 1},\n                                             raise_for_status=True)\n\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__sanity(mock_aioresponse, llm_client, url):\n    mock_aioresponse.post(\n        BASE_URL + TOKENIZE_PATH,\n        payload=load_json_resource(\"ai21/tokenize.json\")", "        BASE_URL + TOKENIZE_PATH,\n        payload=load_json_resource(\"ai21/tokenize.json\")\n    )\n\n    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite things!\")\n\n    assert actual == 3\n"]}
{"filename": "tests/llm_api_client/ai21_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_api_client/ai21_client/conftest.py", "chunked_list": ["import pytest\n\nfrom llm_client.llm_api_client.ai21_client import AI21Client, COMPLETE_PATH, BASE_URL\nfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\n\n\n@pytest.fixture\ndef model_name():\n    return \"ada\"\n", "\n\n@pytest.fixture\ndef config(client_session, model_name):\n    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n\n\n@pytest.fixture\ndef llm_client(config):\n    return AI21Client(config)", "def llm_client(config):\n    return AI21Client(config)\n\n\n@pytest.fixture\ndef url(model_name):\n    return build_url(model_name)\n\n\ndef build_url(model: str) -> str:\n    return BASE_URL + model + \"/\" + COMPLETE_PATH", "\ndef build_url(model: str) -> str:\n    return BASE_URL + model + \"/\" + COMPLETE_PATH\n"]}
{"filename": "tests/llm_api_client/openai_client/test_openai.py", "chunked_list": ["from unittest.mock import AsyncMock, MagicMock\n\nimport pytest\nfrom aiohttp import ClientSession\nfrom openai.openai_object import OpenAIObject\n\nfrom llm_client import OpenAIClient, LLMAPIClientType, LLMAPIClientFactory\nfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig, Role\nfrom llm_client.llm_api_client.openai_client import ChatMessage\nfrom tests.test_utils.load_json_resource import load_json_resource", "from llm_client.llm_api_client.openai_client import ChatMessage\nfrom tests.test_utils.load_json_resource import load_json_resource\n\n\n@pytest.mark.asyncio\nasync def test_get_llm_api_client__with_open_ai(config):\n    del config.session\n    async with LLMAPIClientFactory() as llm_api_client_factory:\n        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.OPEN_AI, **config.__dict__)\n", "        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.OPEN_AI, **config.__dict__)\n\n    assert isinstance(actual, OpenAIClient)\n\n\ndef test_init__sanity(openai_mock, client_session):\n    OpenAIClient(LLMAPIClientConfig(\"fake_api_key\", client_session))\n\n    assert openai_mock.api_key == \"fake_api_key\"\n    openai_mock.aiosession.set.assert_called_once()", "\n\n@pytest.mark.asyncio\nasync def test_text_completion__sanity(openai_mock, open_ai_client, model_name):\n    openai_mock.Completion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\"\\n\\nThis is indeed a test\"]\n    openai_mock.Completion.acreate.assert_awaited_once_with(", "    assert actual == [\"\\n\\nThis is indeed a test\"]\n    openai_mock.Completion.acreate.assert_awaited_once_with(\n        model=model_name,\n        prompt=\"These are a few of my favorite\",\n        headers={}, temperature=0, max_tokens=16, top_p=1)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__return_multiple_completions(openai_mock, open_ai_client, model_name):\n    open_ai_object = OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\"))", "async def test_text_completion__return_multiple_completions(openai_mock, open_ai_client, model_name):\n    open_ai_object = OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\"))\n    open_ai_object.choices.append(OpenAIObject.construct_from({\"text\": \"second completion\"}))\n    openai_mock.Completion.acreate = AsyncMock(return_value=open_ai_object)\n\n    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\"\\n\\nThis is indeed a test\", \"second completion\"]\n    openai_mock.Completion.acreate.assert_awaited_once_with(\n        model=model_name,", "    openai_mock.Completion.acreate.assert_awaited_once_with(\n        model=model_name,\n        prompt=\"These are a few of my favorite\",\n        headers={}, temperature=0, max_tokens=16, top_p=1)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__override_model(openai_mock, open_ai_client, model_name):\n    new_model_name = \"gpt3\"\n    openai_mock.Completion.acreate = AsyncMock(", "    new_model_name = \"gpt3\"\n    openai_mock.Completion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n\n    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name)\n\n    assert actual == [\"\\n\\nThis is indeed a test\"]\n    openai_mock.Completion.acreate.assert_awaited_once_with(\n        model=new_model_name,\n        prompt=\"These are a few of my favorite\",", "        model=new_model_name,\n        prompt=\"These are a few of my favorite\",\n        headers={}, temperature=0, max_tokens=16, top_p=1)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_kwargs(openai_mock, open_ai_client, model_name):\n    openai_mock.Completion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n", "        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n\n    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10)\n\n    assert actual == [\"\\n\\nThis is indeed a test\"]\n    openai_mock.Completion.acreate.assert_awaited_once_with(\n        model=model_name,\n        prompt=\"These are a few of my favorite\",\n        temperature=0, max_tokens=10, top_p=1,\n        headers={})", "        temperature=0, max_tokens=10, top_p=1,\n        headers={})\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_headers(openai_mock, model_name):\n    openai_mock.Completion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/text_completion.json\")))\n    open_ai_client = OpenAIClient(LLMAPIClientConfig(\"fake_api_key\", MagicMock(ClientSession), default_model=model_name,\n                                                     headers={\"header_name\": \"header_value\"}))", "    open_ai_client = OpenAIClient(LLMAPIClientConfig(\"fake_api_key\", MagicMock(ClientSession), default_model=model_name,\n                                                     headers={\"header_name\": \"header_value\"}))\n\n    actual = await open_ai_client.text_completion(prompt=\"These are a few of my favorite\")\n\n    assert actual == [\"\\n\\nThis is indeed a test\"]\n    openai_mock.Completion.acreate.assert_awaited_once_with(\n        model=model_name,\n        prompt=\"These are a few of my favorite\",\n        headers={\"header_name\": \"header_value\"}, temperature=0, max_tokens=16, top_p=1)", "        prompt=\"These are a few of my favorite\",\n        headers={\"header_name\": \"header_value\"}, temperature=0, max_tokens=16, top_p=1)\n\n\n@pytest.mark.asyncio\nasync def test_chat_completion__sanity(openai_mock, open_ai_client, model_name):\n    openai_mock.ChatCompletion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n\n    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])", "\n    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])\n\n    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n        model=model_name,\n        messages=[{'content': 'Hello!', 'role': 'user'}],\n        headers={}, temperature=0, max_tokens=16, top_p=1)\n\n", "\n\n@pytest.mark.asyncio\nasync def test_chat_completion__return_multiple_completions(openai_mock, open_ai_client, model_name):\n    open_ai_object = OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\"))\n    open_ai_object.choices.append(OpenAIObject.construct_from({\"message\": {\"content\": \"second completion\"}}))\n    openai_mock.ChatCompletion.acreate = AsyncMock(return_value=open_ai_object)\n\n    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])\n", "    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])\n\n    assert actual == [\"\\n\\nHello there, how may I assist you today?\", \"second completion\"]\n    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n        model=model_name,\n        messages=[{'content': 'Hello!', 'role': 'user'}],\n        headers={}, temperature=0, max_tokens=16, top_p=1)\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_chat_completion__override_model(openai_mock, open_ai_client, model_name):\n    new_model_name = \"gpt3\"\n    openai_mock.ChatCompletion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n\n    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")], model=new_model_name)\n\n    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]", "\n    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n        model=new_model_name,\n        messages=[{'content': 'Hello!', 'role': 'user'}],\n        headers={}, temperature=0, max_tokens=16, top_p=1)\n\n\n@pytest.mark.asyncio\nasync def test_chat_completion__with_kwargs(openai_mock, open_ai_client, model_name):", "@pytest.mark.asyncio\nasync def test_chat_completion__with_kwargs(openai_mock, open_ai_client, model_name):\n    openai_mock.ChatCompletion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n\n    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")], max_tokens=10, top_p=1)\n\n    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n        model=model_name,", "    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n        model=model_name,\n        messages=[{'content': 'Hello!', 'role': 'user'}],\n        max_tokens=10,\n        headers={}, temperature=0, top_p=1)\n\n\n@pytest.mark.asyncio\nasync def test_chat_completion__with_headers(openai_mock, model_name):\n    openai_mock.ChatCompletion.acreate = AsyncMock(", "async def test_chat_completion__with_headers(openai_mock, model_name):\n    openai_mock.ChatCompletion.acreate = AsyncMock(\n        return_value=OpenAIObject.construct_from(load_json_resource(\"openai/chat_completion.json\")))\n    open_ai_client = OpenAIClient(LLMAPIClientConfig(\"fake_api_key\", MagicMock(ClientSession), default_model=model_name,\n                                                     headers={\"header_name\": \"header_value\"}))\n\n    actual = await open_ai_client.chat_completion([ChatMessage(Role.USER, \"Hello!\")])\n\n    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(", "    assert actual == [\"\\n\\nHello there, how may I assist you today?\"]\n    openai_mock.ChatCompletion.acreate.assert_awaited_once_with(\n        model=model_name,\n        messages=[{'content': 'Hello!', 'role': 'user'}],\n        headers={\"header_name\": \"header_value\"}, temperature=0, max_tokens=16, top_p=1)\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"model_name,expected\", [(\"gpt-3.5-turbo-0301\", 127), (\"gpt-3.5-turbo-0613\", 129),\n                                                 (\"gpt-3.5-turbo\", 129), (\"gpt-4-0314\", 129), (\"gpt-4-0613\", 129),", "@pytest.mark.parametrize(\"model_name,expected\", [(\"gpt-3.5-turbo-0301\", 127), (\"gpt-3.5-turbo-0613\", 129),\n                                                 (\"gpt-3.5-turbo\", 129), (\"gpt-4-0314\", 129), (\"gpt-4-0613\", 129),\n                                                 (\"gpt-4\", 129)])\nasync def test_get_chat_tokens_count__with_examples_from_openai_cookbook(model_name, expected, open_ai_client):\n    example_messages = [\n        ChatMessage(Role.SYSTEM,\n                    \"You are a helpful, pattern-following assistant that translates corporate jargon \"\n                    \"into plain English.\"),\n        ChatMessage(Role.SYSTEM, \"New synergies will help drive top-line growth.\", name=\"example_user\"),\n        ChatMessage(Role.SYSTEM, \"Things working well together will increase revenue.\", name=\"example_assistant\"),", "        ChatMessage(Role.SYSTEM, \"New synergies will help drive top-line growth.\", name=\"example_user\"),\n        ChatMessage(Role.SYSTEM, \"Things working well together will increase revenue.\", name=\"example_assistant\"),\n        ChatMessage(Role.SYSTEM,\n                    \"Let's circle back when we have more bandwidth to touch base on opportunities \"\n                    \"for increased leverage.\", name=\"example_user\"),\n        ChatMessage(Role.SYSTEM, \"Let's talk later when we're less busy about how to do better.\",\n                    name=\"example_assistant\"),\n        ChatMessage(Role.USER,\n                    \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"),\n    ]", "                    \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"),\n    ]\n\n    actual = await open_ai_client.get_chat_tokens_count(example_messages, model=model_name)\n\n    assert actual == expected\n\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__sanity(model_name, open_ai_client, tiktoken_mock):", "@pytest.mark.asyncio\nasync def test_get_tokens_count__sanity(model_name, open_ai_client, tiktoken_mock):\n    tokeniser_mock = tiktoken_mock.encoding_for_model.return_value\n    tokeniser_mock.encode.return_value = [123, 456]\n    text = \"This is a test\"\n\n    actual = await open_ai_client.get_tokens_count(text=text)\n\n    assert actual == len(tokeniser_mock.encode.return_value)\n    tiktoken_mock.encoding_for_model.assert_called_once_with(model_name)", "    assert actual == len(tokeniser_mock.encode.return_value)\n    tiktoken_mock.encoding_for_model.assert_called_once_with(model_name)\n    tokeniser_mock.encode.assert_called_once_with(text)\n\n\n@pytest.mark.asyncio\nasync def test_get_tokens_count__override_model(open_ai_client, tiktoken_mock):\n    tokeniser_mock = tiktoken_mock.encoding_for_model.return_value\n    tokeniser_mock.encode.return_value = [123, 456]\n    text = \"This is a test\"", "    tokeniser_mock.encode.return_value = [123, 456]\n    text = \"This is a test\"\n    model_name = \"gpt3\"\n\n    actual = await open_ai_client.get_tokens_count(text=text, model=model_name)\n\n    assert actual == len(tokeniser_mock.encode.return_value)\n    tiktoken_mock.encoding_for_model.assert_called_once_with(model_name)\n    tokeniser_mock.encode.assert_called_once_with(text)\n", "    tokeniser_mock.encode.assert_called_once_with(text)\n"]}
{"filename": "tests/llm_api_client/openai_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_api_client/openai_client/conftest.py", "chunked_list": ["from unittest.mock import MagicMock, patch\n\nimport pytest\nfrom aiohttp import ClientSession\n\nfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\nfrom llm_client.llm_api_client.openai_client import OpenAIClient\n\n\n@pytest.fixture\ndef model_name():\n    return \"ada\"", "\n@pytest.fixture\ndef model_name():\n    return \"ada\"\n\n\n@pytest.fixture\ndef openai_mock():\n    with patch(\"llm_client.llm_api_client.openai_client.openai\") as openai_mock:\n        yield openai_mock", "\n\n@pytest.fixture\ndef config(model_name):\n    return LLMAPIClientConfig(\"fake-api-key\", MagicMock(ClientSession), default_model=model_name)\n\n\n@pytest.fixture\ndef open_ai_client(config):\n    return OpenAIClient(config)", "def open_ai_client(config):\n    return OpenAIClient(config)\n\n\n@pytest.fixture\ndef tiktoken_mock():\n    with patch(\"llm_client.llm_api_client.openai_client.tiktoken\") as tiktoken_mock:\n        yield tiktoken_mock\n", ""]}
{"filename": "tests/llm_api_client/anthropic_client/__init__.py", "chunked_list": [""]}
{"filename": "tests/llm_api_client/anthropic_client/test_anthropic_client.py", "chunked_list": ["from unittest.mock import AsyncMock\n\nimport pytest\n\nfrom llm_client import LLMAPIClientFactory, LLMAPIClientType, ChatMessage\nfrom llm_client.consts import PROMPT_KEY, MODEL_KEY\nfrom llm_client.llm_api_client.anthropic_client import AUTH_HEADER, COMPLETIONS_KEY, MAX_TOKENS_KEY, ACCEPT_HEADER, \\\n    ACCEPT_VALUE, VERSION_HEADER, AnthropicClient, USER_PREFIX, ASSISTANT_PREFIX, START_PREFIX, SYSTEM_START_PREFIX, \\\n    SYSTEM_END_PREFIX\nfrom llm_client.llm_api_client.base_llm_api_client import Role", "    SYSTEM_END_PREFIX\nfrom llm_client.llm_api_client.base_llm_api_client import Role\n\n\n@pytest.mark.asyncio\nasync def test_get_llm_api_client__with_anthropic(config):\n    del config.session\n    async with LLMAPIClientFactory() as llm_api_client_factory:\n        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.ANTHROPIC, **config.__dict__)\n", "        actual = llm_api_client_factory.get_llm_api_client(LLMAPIClientType.ANTHROPIC, **config.__dict__)\n\n    assert isinstance(actual, AnthropicClient)\n\n@pytest.mark.asyncio\nasync def test_chat_completion_sanity(llm_client):\n    text_completion_mock = AsyncMock(return_value=[\"completion text\"])\n    llm_client.text_completion = text_completion_mock\n\n    actual = await llm_client.chat_completion(messages=[ChatMessage(Role.USER, \"Why is the sky blue?\")], max_tokens=10)", "\n    actual = await llm_client.chat_completion(messages=[ChatMessage(Role.USER, \"Why is the sky blue?\")], max_tokens=10)\n\n    assert actual == [\"completion text\"]\n    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} Why is the sky blue?\"\n                                                  f\"{START_PREFIX}{ASSISTANT_PREFIX}\", None, 10, 1)\n\n\n@pytest.mark.asyncio\nasync def test_chat_completion_with_assistant_in_the_end(llm_client):", "@pytest.mark.asyncio\nasync def test_chat_completion_with_assistant_in_the_end(llm_client):\n    text_completion_mock = AsyncMock(return_value=[\"completion text\"])\n    llm_client.text_completion = text_completion_mock\n\n    actual = await llm_client.chat_completion(messages=[ChatMessage(Role.USER, \"Why is the sky blue?\"),\n                                                        ChatMessage(Role.ASSISTANT, \"Answer - \")], temperature=10)\n\n    assert actual == [\"completion text\"]\n    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} Why is the sky blue?\"", "    assert actual == [\"completion text\"]\n    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} Why is the sky blue?\"\n                                                  f\"{START_PREFIX}{ASSISTANT_PREFIX} Answer -\", None, None,\n                                                  10)\n\n\n@pytest.mark.asyncio\nasync def test_chat_completion_with_system(llm_client):\n    text_completion_mock = AsyncMock(return_value=[\"completion text\"])\n    llm_client.text_completion = text_completion_mock", "    text_completion_mock = AsyncMock(return_value=[\"completion text\"])\n    llm_client.text_completion = text_completion_mock\n\n    actual = await llm_client.chat_completion(messages=[ChatMessage(Role.SYSTEM, \"Be nice!\"),\n                                                        ChatMessage(Role.USER, \"Why is the sky blue?\")], max_tokens=10,\n                                              temperature=2)\n\n    assert actual == [\"completion text\"]\n    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} \"\n                                                  f\"{SYSTEM_START_PREFIX}Be nice!{SYSTEM_END_PREFIX}{START_PREFIX}\"", "    text_completion_mock.assert_awaited_once_with(f\"{START_PREFIX}{USER_PREFIX} \"\n                                                  f\"{SYSTEM_START_PREFIX}Be nice!{SYSTEM_END_PREFIX}{START_PREFIX}\"\n                                                  f\"{USER_PREFIX} Why is the sky blue?\"\n                                                  f\"{START_PREFIX}{ASSISTANT_PREFIX}\", None, 10, 2)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__sanity(mock_aioresponse, llm_client, complete_url, anthropic_version):\n    mock_aioresponse.post(\n        complete_url,", "    mock_aioresponse.post(\n        complete_url,\n        payload={COMPLETIONS_KEY: \"completion text\"}\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10,)\n\n    assert actual == [\"completion text\"]\n    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n                                             headers={AUTH_HEADER: llm_client._api_key,", "    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n                                             headers={AUTH_HEADER: llm_client._api_key,\n                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n                                                      VERSION_HEADER: anthropic_version},\n                                             json={PROMPT_KEY: 'These are a few of my favorite',\n                                                   MAX_TOKENS_KEY: 10, \"temperature\": 1,\n                                                   MODEL_KEY: llm_client._default_model},\n                                             raise_for_status=True)\n\n", "\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_version_header(mock_aioresponse, config, complete_url):\n    mock_aioresponse.post(\n        complete_url,\n        payload={COMPLETIONS_KEY: \"completion text\"}\n    )\n    config.headers[VERSION_HEADER] = \"1.0.0\"\n    llm_client = AnthropicClient(config)", "    config.headers[VERSION_HEADER] = \"1.0.0\"\n    llm_client = AnthropicClient(config)\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10)\n\n    assert actual == [\"completion text\"]\n    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n                                             headers={AUTH_HEADER: llm_client._api_key,\n                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n                                                      VERSION_HEADER: \"1.0.0\"},", "                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n                                                      VERSION_HEADER: \"1.0.0\"},\n                                             json={PROMPT_KEY: 'These are a few of my favorite',\n                                                   MAX_TOKENS_KEY: 10, \"temperature\": 1,\n                                                   MODEL_KEY: llm_client._default_model},\n                                             raise_for_status=True)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__without_max_tokens_raise_value_error(mock_aioresponse, llm_client):\n    with pytest.raises(ValueError):\n        await llm_client.text_completion(prompt=\"These are a few of my favorite\")", "@pytest.mark.asyncio\nasync def test_text_completion__without_max_tokens_raise_value_error(mock_aioresponse, llm_client):\n    with pytest.raises(ValueError):\n        await llm_client.text_completion(prompt=\"These are a few of my favorite\")\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__override_model(mock_aioresponse, llm_client, complete_url, anthropic_version):\n    new_model_name = \"claude-instant\"\n    mock_aioresponse.post(", "    new_model_name = \"claude-instant\"\n    mock_aioresponse.post(\n        complete_url,\n        payload={COMPLETIONS_KEY: \"completion text\"}\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", model=new_model_name,\n                                              max_tokens=10)\n\n    assert actual == [\"completion text\"]", "\n    assert actual == [\"completion text\"]\n    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n                                             headers={AUTH_HEADER: llm_client._api_key,\n                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n                                                      VERSION_HEADER: anthropic_version},\n                                             json={PROMPT_KEY: 'These are a few of my favorite',\n                                                   MAX_TOKENS_KEY: 10, \"temperature\": 1,\n                                                   MODEL_KEY: new_model_name},\n                                             raise_for_status=True)", "                                                   MODEL_KEY: new_model_name},\n                                             raise_for_status=True)\n\n\n@pytest.mark.asyncio\nasync def test_text_completion__with_kwargs(mock_aioresponse, llm_client, complete_url, anthropic_version):\n    mock_aioresponse.post(\n        complete_url,\n        payload={COMPLETIONS_KEY: \"completion text\"}\n    )", "        payload={COMPLETIONS_KEY: \"completion text\"}\n    )\n\n    actual = await llm_client.text_completion(prompt=\"These are a few of my favorite\", max_tokens=10, temperature=0.5,top_p=0.5)\n\n    assert actual == [\"completion text\"]\n    mock_aioresponse.assert_called_once_with(complete_url, method='POST',\n                                             headers={AUTH_HEADER: llm_client._api_key,\n                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n                                                      VERSION_HEADER: anthropic_version},", "                                                      ACCEPT_HEADER: ACCEPT_VALUE,\n                                                      VERSION_HEADER: anthropic_version},\n                                             json={PROMPT_KEY: 'These are a few of my favorite',\n                                                   MAX_TOKENS_KEY: 10,\n                                                   MODEL_KEY: llm_client._default_model,\n                                                   \"temperature\": 0.5, \"top_p\" : 0.5},\n                                             raise_for_status=True)\n\n\n@pytest.mark.asyncio", "\n@pytest.mark.asyncio\nasync def test_get_tokens_count__sanity(llm_client, number_of_tokens, mock_anthropic):\n    actual = await llm_client.get_tokens_count(text=\"These are a few of my favorite things!\")\n\n    assert actual == 10\n    mock_anthropic.return_value.count_tokens.assert_awaited_once_with(\"These are a few of my favorite things!\")\n"]}
{"filename": "tests/llm_api_client/anthropic_client/conftest.py", "chunked_list": ["from unittest.mock import patch, AsyncMock\n\nimport pytest\n\nfrom llm_client.llm_api_client.anthropic_client import BASE_URL, COMPLETE_PATH, VERSION_HEADER, AnthropicClient\nfrom llm_client.llm_api_client.base_llm_api_client import LLMAPIClientConfig\n\n\n@pytest.fixture\ndef model_name():\n    return \"claude-v1\"", "@pytest.fixture\ndef model_name():\n    return \"claude-v1\"\n\n\n@pytest.fixture\ndef config(client_session, model_name):\n    return LLMAPIClientConfig(\"top-secret-api-key\", client_session, default_model=model_name)\n\n", "\n\n@pytest.fixture\ndef llm_client(config):\n    return AnthropicClient(config)\n\n\n@pytest.fixture\ndef complete_url():\n    return BASE_URL + COMPLETE_PATH", "def complete_url():\n    return BASE_URL + COMPLETE_PATH\n\n\n@pytest.fixture\ndef number_of_tokens():\n    return 10\n\n\n@pytest.fixture\ndef anthropic_version():\n    return \"2023-06-01\"", "\n@pytest.fixture\ndef anthropic_version():\n    return \"2023-06-01\"\n\n\n@pytest.fixture(autouse=True)\ndef mock_anthropic(number_of_tokens, anthropic_version):\n    with patch(\"llm_client.llm_api_client.anthropic_client.AsyncAnthropic\") as mock_anthropic:\n        mock_anthropic.return_value.count_tokens = AsyncMock(return_value=number_of_tokens)\n        mock_anthropic.return_value.default_headers = {VERSION_HEADER: anthropic_version}\n        yield mock_anthropic", ""]}
{"filename": "llm_client/consts.py", "chunked_list": ["MODEL_KEY = \"model\"\nPROMPT_KEY = \"prompt\"\n"]}
{"filename": "llm_client/__init__.py", "chunked_list": ["__version__ = \"0.8.0\"\n\nfrom llm_client.base_llm_client import BaseLLMClient\n\n# load api clients\ntry:\n    from llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig, ChatMessage, Role\n    from llm_client.llm_api_client.llm_api_client_factory import LLMAPIClientFactory, LLMAPIClientType\n    # load base-api clients\n    try:\n        from llm_client.llm_api_client.ai21_client import AI21Client\n        from llm_client.llm_api_client.aleph_alpha_client import AlephAlphaClient\n        from llm_client.llm_api_client.google_client import GoogleClient, MessagePrompt\n    except ImportError:\n        pass\n    # load apis with different dependencies\n    try:\n        from llm_client.llm_api_client.openai_client import OpenAIClient\n    except ImportError:\n        pass\n    try:\n        from llm_client.llm_api_client.huggingface_client import HuggingFaceClient\n    except ImportError:\n        pass\n    try:\n        from llm_client.llm_api_client.anthropic_client import AnthropicClient\n    except ImportError:\n        pass\nexcept ImportError:\n    pass", "# load local clients\ntry:\n    from llm_client.llm_client.local_client import LocalClient, LocalClientConfig\nexcept ImportError:\n    pass\n# load sync support\ntry:\n    from llm_client.sync.sync_llm_api_client_factory import init_sync_llm_api_client\nexcept ImportError:\n    pass", ""]}
{"filename": "llm_client/base_llm_client.py", "chunked_list": ["from abc import ABC, abstractmethod\n\n\nclass BaseLLMClient(ABC):\n    @abstractmethod\n    async def text_completion(self, prompt: str, **kwargs) -> list[str]:\n        raise NotImplementedError()\n\n    async def get_tokens_count(self, text: str, **kwargs) -> int:\n        raise NotImplementedError()", ""]}
{"filename": "llm_client/llm_client/local_client.py", "chunked_list": ["from dataclasses import dataclass, field\nfrom typing import Any\n\ntry:\n    from transformers import PreTrainedModel, PreTrainedTokenizerBase\nexcept ImportError:\n    PreTrainedModel = Any\n    PreTrainedTokenizerBase = Any\n\nfrom llm_client import BaseLLMClient", "\nfrom llm_client import BaseLLMClient\n\n\n@dataclass\nclass LocalClientConfig:\n    model: PreTrainedModel\n    tokenizer: PreTrainedTokenizerBase\n    tensors_type: str\n    device: str\n    encode_kwargs: dict[str, Any] = field(default_factory=dict)", "\n\nclass LocalClient(BaseLLMClient):\n    def __init__(self, llm_client_config: LocalClientConfig):\n        if not llm_client_config.model.can_generate():\n            raise TypeError(f\"{llm_client_config.model} is not a text generation model\")\n\n        self._model: PreTrainedModel = llm_client_config.model\n        self._tokenizer: PreTrainedTokenizerBase = llm_client_config.tokenizer\n        self._tensors_type: str = llm_client_config.tensors_type\n        self._device: str = llm_client_config.device\n        self._encode_kwargs: dict[str, Any] = llm_client_config.encode_kwargs\n        self._encode_kwargs[\"return_tensors\"] = llm_client_config.tensors_type\n\n    async def text_completion(self, prompt: str, **kwargs) -> list[str]:\n        input_ids = self._encode(prompt)\n        outputs = self._model.generate(input_ids, **kwargs)\n        return [self._tokenizer.decode(output) for output in outputs]\n\n    async def get_tokens_count(self, text: str, **kwargs) -> int:\n        return len(self._encode(text))\n\n    def _encode(self, prompt: str) -> list[int]:\n        return self._tokenizer.encode(prompt, **self._encode_kwargs).to(self._device)", ""]}
{"filename": "llm_client/llm_client/__init__.py", "chunked_list": [""]}
{"filename": "llm_client/sync/sync_llm_api_client_factory.py", "chunked_list": ["import inspect\nfrom functools import wraps\nfrom typing import Callable, Any\n\nimport async_to_sync\nfrom aiohttp import ClientSession\n\nfrom llm_client import LLMAPIClientConfig\nfrom llm_client.llm_api_client.llm_api_client_factory import LLMAPIClientType, get_llm_api_client_class\n", "from llm_client.llm_api_client.llm_api_client_factory import LLMAPIClientType, get_llm_api_client_class\n\n\ndef _create_new_session(f: Callable[..., Any]) -> Callable[..., Any]:\n    @wraps(f)\n    async def func(self, *args, **kwargs):\n        self._session = ClientSession()\n        try:\n            response = await f(self, *args, **kwargs)\n        except Exception as e:\n            await self._session.close()\n            raise e\n        await self._session.close()\n        return response\n\n    return func", "\n\ndef _decorate_all_methods_in_class(decorators):\n    def apply_decorator(cls: Any) -> Any:\n        for k, f in cls.__dict__.items():\n            if inspect.isfunction(f) and not k.startswith(\"_\"):\n                for decorator in decorators:\n                    setattr(cls, k, decorator(cls.__dict__[k]))\n        return cls\n\n    return apply_decorator", "\n\ndef init_sync_llm_api_client(llm_api_client_type: LLMAPIClientType, **config_kwargs) -> \"Sync BaseLLMAPIClient\":\n    llm_api_client_class = get_llm_api_client_class(llm_api_client_type)\n    return async_to_sync.methods(_decorate_all_methods_in_class([_create_new_session])(llm_api_client_class)(\n        LLMAPIClientConfig(session=None, **config_kwargs)))\n"]}
{"filename": "llm_client/sync/__init__.py", "chunked_list": [""]}
{"filename": "llm_client/llm_api_client/aleph_alpha_client.py", "chunked_list": ["from typing import Optional\n\nfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\nfrom llm_client.consts import PROMPT_KEY\n\nCOMPLETE_PATH = \"complete\"\nTOKENIZE_PATH = \"tokenize\"\nEMBEDDING_PATH = \"semantic_embed\"\nBASE_URL = \"https://api.aleph-alpha.com/\"\nCOMPLETIONS_KEY = \"completions\"", "BASE_URL = \"https://api.aleph-alpha.com/\"\nCOMPLETIONS_KEY = \"completions\"\nTEXT_KEY = \"completion\"\nTOKENS_IDS_KEY = \"token_ids\"\nTOKENS_KEY = \"tokens\"\nREPRESENTATION_KEY = \"representation\"\nREPRESENTATION_DEFAULT_VALUE = \"symmetric\"\nEMBEDDING_KEY = \"embedding\"\nAUTH_HEADER = \"Authorization\"\nBEARER_TOKEN = \"Bearer \"", "AUTH_HEADER = \"Authorization\"\nBEARER_TOKEN = \"Bearer \"\nMAX_TOKENS_KEY = \"maximum_tokens\"\n\n\nclass AlephAlphaClient(BaseLLMAPIClient):\n    def __init__(self, config: LLMAPIClientConfig):\n        super().__init__(config)\n        if self._base_url is None:\n            self._base_url = BASE_URL\n        self._headers[AUTH_HEADER] = BEARER_TOKEN + self._api_key\n\n    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = None,\n                              temperature: float = 0,top_p: float = 0, **kwargs) -> \\\n            list[str]:\n        self._set_model_in_kwargs(kwargs, model)\n        if max_tokens is None:\n            raise ValueError(\"max_tokens must be specified\")\n        kwargs[PROMPT_KEY] = prompt\n        kwargs[\"top_p\"] = top_p\n        kwargs[\"maximum_tokens\"] = kwargs.pop(\"maximum_tokens\", max_tokens)\n        kwargs[\"temperature\"] = temperature\n        response = await self._session.post(self._base_url + COMPLETE_PATH,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        completions = response_json[COMPLETIONS_KEY]\n        return [completion[TEXT_KEY] for completion in completions]\n\n    async def embedding(self, text: str, model: Optional[str] = None,\n                        representation: str = REPRESENTATION_DEFAULT_VALUE,\n                        **kwargs) -> list[float]:\n        self._set_model_in_kwargs(kwargs, model)\n        kwargs[REPRESENTATION_KEY] = representation\n        kwargs[PROMPT_KEY] = text\n        response = await self._session.post(self._base_url + EMBEDDING_PATH,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        return response_json[EMBEDDING_KEY]\n\n    async def get_tokens_count(self, text: str, model: Optional[str] = None, **kwargs) -> int:\n        self._set_model_in_kwargs(kwargs, model)\n        kwargs[TOKENS_KEY] = False\n        kwargs[TOKENS_IDS_KEY] = True\n        kwargs[PROMPT_KEY] = text\n        response = await self._session.post(self._base_url + TOKENIZE_PATH,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        return len(response_json[TOKENS_IDS_KEY])", ""]}
{"filename": "llm_client/llm_api_client/anthropic_client.py", "chunked_list": ["from typing import Optional\n\nfrom anthropic import AsyncAnthropic\n\nfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig, ChatMessage, Role\nfrom llm_client.consts import PROMPT_KEY\n\nCOMPLETE_PATH = \"complete\"\nBASE_URL = \"https://api.anthropic.com/v1/\"\nCOMPLETIONS_KEY = \"completion\"", "BASE_URL = \"https://api.anthropic.com/v1/\"\nCOMPLETIONS_KEY = \"completion\"\nAUTH_HEADER = \"x-api-key\"\nACCEPT_HEADER = \"Accept\"\nVERSION_HEADER = \"anthropic-version\"\nACCEPT_VALUE = \"application/json\"\nMAX_TOKENS_KEY = \"max_tokens_to_sample\"\nUSER_PREFIX = \"Human:\"\nASSISTANT_PREFIX = \"Assistant:\"\nSTART_PREFIX = \"\\n\\n\"", "ASSISTANT_PREFIX = \"Assistant:\"\nSTART_PREFIX = \"\\n\\n\"\nSYSTEM_START_PREFIX = \"<admin>\"\nSYSTEM_END_PREFIX = \"</admin>\"\n\n\nclass AnthropicClient(BaseLLMAPIClient):\n    def __init__(self, config: LLMAPIClientConfig):\n        super().__init__(config)\n        if self._base_url is None:\n            self._base_url = BASE_URL\n        self._anthropic = AsyncAnthropic()\n        if self._headers.get(VERSION_HEADER) is None:\n            self._headers[VERSION_HEADER] = self._anthropic.default_headers[VERSION_HEADER]\n        self._headers[ACCEPT_HEADER] = ACCEPT_VALUE\n        self._headers[AUTH_HEADER] = self._api_key\n\n    async def chat_completion(self, messages: list[ChatMessage], model: Optional[str] = None,\n                              max_tokens: Optional[int] = None, temperature: float = 1, **kwargs) -> list[str]:\n        return await self.text_completion(self.messages_to_text(messages), model, max_tokens, temperature, **kwargs)\n\n    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = None,\n                              temperature: float = 1, top_p: Optional[float] = None,\n                              **kwargs) -> \\\n            list[str]:\n        if max_tokens is None and kwargs.get(MAX_TOKENS_KEY) is None:\n            raise ValueError(f\"max_tokens or {MAX_TOKENS_KEY} must be specified\")\n        if top_p:\n            kwargs[\"top_p\"] = top_p\n        self._set_model_in_kwargs(kwargs, model)\n        kwargs[PROMPT_KEY] = prompt\n        kwargs[MAX_TOKENS_KEY] = kwargs.pop(MAX_TOKENS_KEY, max_tokens)\n        kwargs[\"temperature\"] = temperature\n        response = await self._session.post(self._base_url + COMPLETE_PATH,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        return [response_json[COMPLETIONS_KEY]]\n\n    async def get_chat_tokens_count(self, messages: list[ChatMessage], **kwargs) -> int:\n        return await self.get_tokens_count(self.messages_to_text(messages), **kwargs)\n\n    async def get_tokens_count(self, text: str, **kwargs) -> int:\n        return await self._anthropic.count_tokens(text)\n\n    def messages_to_text(self, messages: list[ChatMessage]) -> str:\n        prompt = START_PREFIX\n        prompt += START_PREFIX.join(map(self._message_to_prompt, messages))\n        if messages[-1].role != Role.ASSISTANT:\n            prompt += START_PREFIX\n            prompt += self._message_to_prompt(ChatMessage(role=Role.ASSISTANT, content=\"\"))\n        return prompt.rstrip()\n\n    @staticmethod\n    def _message_to_prompt(message: ChatMessage) -> str:\n        if message.role == Role.USER:\n            return f\"{USER_PREFIX} {message.content}\"\n        if message.role == Role.ASSISTANT:\n            return f\"{ASSISTANT_PREFIX} {message.content}\"\n        if message.role == Role.SYSTEM:\n            return f\"{USER_PREFIX} {SYSTEM_START_PREFIX}{message.content}{SYSTEM_END_PREFIX}\"\n        raise ValueError(f\"Unknown role: {message.role}\")", ""]}
{"filename": "llm_client/llm_api_client/__init__.py", "chunked_list": [""]}
{"filename": "llm_client/llm_api_client/google_client.py", "chunked_list": ["from typing import Any, Optional\n\ntry:\n    from google.ai.generativelanguage_v1beta2 import MessagePrompt\nexcept ImportError:\n    MessagePrompt = Any  # This only needed for runtime chat_completion and chat tokens count\n\nfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\nfrom llm_client.consts import PROMPT_KEY\n", "from llm_client.consts import PROMPT_KEY\n\nCOMPLETE_PATH = \"generateText\"\nCHAT_PATH = \"generateMessage\"\nEMBEDDING_PATH = \"embedText\"\nTOKENIZE_PATH = \"countMessageTokens\"\nBASE_URL = \"https://generativelanguage.googleapis.com/v1beta2/models/\"\nCOMPLETIONS_KEY = \"candidates\"\nTEXT_KEY = \"text\"\nMAX_TOKENS_KEY = \"maxOutputTokens\"", "TEXT_KEY = \"text\"\nMAX_TOKENS_KEY = \"maxOutputTokens\"\nCOMPLETIONS_OUTPUT_KEY = \"output\"\nTOKENS_KEY = \"tokenCount\"\nMESSAGES_KEY = \"messages\"\nMESSAGE_CONTENT_KEY = \"content\"\nEMBEDDING_KEY = \"embedding\"\nEMBEDDING_VALUE_KEY = \"value\"\nAUTH_PARAM = \"key\"\n", "AUTH_PARAM = \"key\"\n\n\nclass GoogleClient(BaseLLMAPIClient):\n    def __init__(self, config: LLMAPIClientConfig):\n        super().__init__(config)\n        if self._base_url is None:\n            self._base_url = BASE_URL\n        self._params = {AUTH_PARAM: self._api_key}\n\n    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = 64,\n                              temperature: Optional[float] = None,top_p: Optional[float] = None, **kwargs) -> list[str]:\n        model = model or self._default_model\n        kwargs[PROMPT_KEY] = {TEXT_KEY: prompt}\n        kwargs[MAX_TOKENS_KEY] = kwargs.pop(MAX_TOKENS_KEY, max_tokens)\n        if top_p:\n            kwargs[\"topP\"] = top_p\n        kwargs[\"temperature\"] = kwargs.pop(\"temperature\", temperature)\n        response = await self._session.post(self._base_url + model + \":\" + COMPLETE_PATH,\n                                            params=self._params,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        completions = response_json[COMPLETIONS_KEY]\n        return [completion[COMPLETIONS_OUTPUT_KEY] for completion in completions]\n\n    async def embedding(self, text: str, model: Optional[str] = None, **kwargs) -> list[float]:\n        model = model or self._default_model\n        response = await self._session.post(self._base_url + model + \":\" + EMBEDDING_PATH,\n                                            params=self._params,\n                                            json={TEXT_KEY: text},\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        return response_json[EMBEDDING_KEY][EMBEDDING_VALUE_KEY]\n\n    async def get_tokens_count(self, text: str, model: Optional[str] = None,\n                               messages: Optional[MessagePrompt] = None, **kwargs) -> int:\n        \"\"\"\n        Retrieves the count of tokens in the given text using the specified model or the default_model.\n\n        :param text: (str) The input text to tokenize and count the tokens. If the keyword argument `${MESSAGES_KEY}` \\\n                       is provided, this parameter will be ignored.\n        :param model: (Optional[str], optional) The name of the model to use for tokenization. If not provided,\n                       the default model will be used. Defaults to `None`.\n        :param messages: (MessagePrompt | None, optional) The messages to tokenize and count the tokens. If provided,\n                            the `text` parameter will be ignored.\n        :param kwargs: Ignored.\n        :return: (int) The count of tokens in the given text.\n        \"\"\"\n\n        model = model or self._default_model\n        if not messages:\n            messages = {MESSAGES_KEY: [{MESSAGE_CONTENT_KEY: text}]}\n        response = await self._session.post(self._base_url + model + \":\" + TOKENIZE_PATH,\n                                            params=self._params,\n                                            json={PROMPT_KEY: messages},\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        return response_json[TOKENS_KEY]", ""]}
{"filename": "llm_client/llm_api_client/ai21_client.py", "chunked_list": ["from typing import Optional\n\nfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\nfrom llm_client.consts import PROMPT_KEY\n\nCOMPLETE_PATH = \"complete\"\nTOKENIZE_PATH = \"tokenize\"\nBASE_URL = \"https://api.ai21.com/studio/v1/\"\nCOMPLETIONS_KEY = \"completions\"\nDATA_KEY = \"data\"", "COMPLETIONS_KEY = \"completions\"\nDATA_KEY = \"data\"\nTEXT_KEY = \"text\"\nTOKENS_KEY = \"tokens\"\nAUTH_HEADER = \"Authorization\"\nBEARER_TOKEN = \"Bearer \"\n\n\nclass AI21Client(BaseLLMAPIClient):\n    def __init__(self, config: LLMAPIClientConfig):\n        super().__init__(config)\n        if self._base_url is None:\n            self._base_url = BASE_URL\n        self._headers[AUTH_HEADER] = BEARER_TOKEN + self._api_key\n\n    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: int = 16,\n                              temperature: float = 0.7, top_p: float = 1,**kwargs) -> list[str]:\n        model = model or self._default_model\n        kwargs[PROMPT_KEY] = prompt\n        kwargs[\"topP\"] = kwargs.pop(\"topP\", top_p)\n        kwargs[\"maxTokens\"] = kwargs.pop(\"maxTokens\", max_tokens)\n        kwargs[\"temperature\"] = temperature\n        response = await self._session.post(self._base_url + model + \"/\" + COMPLETE_PATH,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        completions = response_json[COMPLETIONS_KEY]\n        return [completion[DATA_KEY][TEXT_KEY] for completion in completions]\n\n    async def get_tokens_count(self, text: str, **kwargs) -> int:\n        response = await self._session.post(self._base_url + TOKENIZE_PATH,\n                                            json={TEXT_KEY: text},\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        return len(response_json[TOKENS_KEY])", "class AI21Client(BaseLLMAPIClient):\n    def __init__(self, config: LLMAPIClientConfig):\n        super().__init__(config)\n        if self._base_url is None:\n            self._base_url = BASE_URL\n        self._headers[AUTH_HEADER] = BEARER_TOKEN + self._api_key\n\n    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: int = 16,\n                              temperature: float = 0.7, top_p: float = 1,**kwargs) -> list[str]:\n        model = model or self._default_model\n        kwargs[PROMPT_KEY] = prompt\n        kwargs[\"topP\"] = kwargs.pop(\"topP\", top_p)\n        kwargs[\"maxTokens\"] = kwargs.pop(\"maxTokens\", max_tokens)\n        kwargs[\"temperature\"] = temperature\n        response = await self._session.post(self._base_url + model + \"/\" + COMPLETE_PATH,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        completions = response_json[COMPLETIONS_KEY]\n        return [completion[DATA_KEY][TEXT_KEY] for completion in completions]\n\n    async def get_tokens_count(self, text: str, **kwargs) -> int:\n        response = await self._session.post(self._base_url + TOKENIZE_PATH,\n                                            json={TEXT_KEY: text},\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        return len(response_json[TOKENS_KEY])", ""]}
{"filename": "llm_client/llm_api_client/openai_client.py", "chunked_list": ["from functools import lru_cache\nfrom typing import Optional\n\nimport openai\nimport tiktoken\nfrom tiktoken import Encoding\n\nfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig, ChatMessage\nfrom llm_client.consts import PROMPT_KEY\n", "from llm_client.consts import PROMPT_KEY\n\nINPUT_KEY = \"input\"\nMODEL_NAME_TO_TOKENS_PER_MESSAGE_AND_TOKENS_PER_NAME = {\n    \"gpt-3.5-turbo-0613\": (3, 1),\n    \"gpt-3.5-turbo-16k-0613\": (3, 1),\n    \"gpt-4-0314\": (3, 1),\n    \"gpt-4-32k-0314\": (3, 1),\n    \"gpt-4-0613\": (3, 1),\n    \"gpt-4-32k-0613\": (3, 1),", "    \"gpt-4-0613\": (3, 1),\n    \"gpt-4-32k-0613\": (3, 1),\n    # every message follows <|start|>{role/name}\\n{content}<|end|>\\n, if there's a name, the role is omitted\n    \"gpt-3.5-turbo-0301\": (4, -1),\n}\n\n\nclass OpenAIClient(BaseLLMAPIClient):\n    def __init__(self, config: LLMAPIClientConfig):\n        super().__init__(config)\n        openai.api_key = self._api_key\n        openai.aiosession.set(self._session)\n        self._client = openai\n\n    async def text_completion(self, prompt: str, model: Optional[str] = None, temperature: float = 0,\n                              max_tokens: int = 16, top_p: float = 1, **kwargs) -> list[str]:\n        self._set_model_in_kwargs(kwargs, model)\n        kwargs[PROMPT_KEY] = prompt\n        kwargs[\"top_p\"] = top_p\n        kwargs[\"temperature\"] = temperature\n        kwargs[\"max_tokens\"] = max_tokens\n        completions = await self._client.Completion.acreate(headers=self._headers, **kwargs)\n        return [choice.text for choice in completions.choices]\n\n    async def chat_completion(self, messages: list[ChatMessage], temperature: float = 0,\n                              max_tokens: int = 16, top_p: float = 1, model: Optional[str] = None, **kwargs) \\\n            -> list[str]:\n        self._set_model_in_kwargs(kwargs, model)\n        kwargs[\"messages\"] = [message.to_dict() for message in messages]\n        kwargs[\"temperature\"] = temperature\n        kwargs[\"top_p\"] = top_p\n        kwargs[\"max_tokens\"] = max_tokens\n        completions = await self._client.ChatCompletion.acreate(headers=self._headers, **kwargs)\n        return [choice.message.content for choice in completions.choices]\n\n    async def embedding(self, text: str, model: Optional[str] = None, **kwargs) -> list[float]:\n        self._set_model_in_kwargs(kwargs, model)\n        kwargs[INPUT_KEY] = text\n        embeddings = await openai.Embedding.acreate(**kwargs)\n        return embeddings.data[0].embedding\n\n    async def get_tokens_count(self, text: str, model: Optional[str] = None, **kwargs) -> int:\n        if model is None:\n            model = self._default_model\n        return len(self._get_relevant_tokeniser(model).encode(text))\n\n    async def get_chat_tokens_count(self, messages: list[ChatMessage], model: Optional[str] = None) -> int:\n        \"\"\"\n        This is based on:\n        https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n        \"\"\"\n        model = self._get_model_name_for_tokeniser(model)\n        encoding = self._get_relevant_tokeniser(model)\n        tokens_per_message, tokens_per_name = MODEL_NAME_TO_TOKENS_PER_MESSAGE_AND_TOKENS_PER_NAME[model]\n        num_tokens = 0\n        for message in messages:\n            num_tokens += tokens_per_message\n            num_tokens += len(encoding.encode(message.content))\n            num_tokens += len(encoding.encode(message.role.value))\n            if message.name:\n                num_tokens += len(encoding.encode(message.name))\n                num_tokens += tokens_per_name\n        num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n        return num_tokens\n\n    def _get_model_name_for_tokeniser(self, model: Optional[str] = None) -> str:\n        if model is None:\n            model = self._default_model\n        if model in {\n            \"gpt-3.5-turbo-0613\",\n            \"gpt-3.5-turbo-16k-0613\",\n            \"gpt-4-0314\",\n            \"gpt-4-32k-0314\",\n            \"gpt-4-0613\",\n            \"gpt-4-32k-0613\",\n        }:\n            return model\n        elif model == \"gpt-3.5-turbo-0301\":\n            return model\n        elif \"gpt-3.5-turbo\" in model:\n            print(\"Warning: gpt-3.5-turbo may update over time. Returning tokeniser assuming gpt-3.5-turbo-0613.\")\n            return \"gpt-3.5-turbo-0613\"\n        elif \"gpt-4\" in model:\n            print(\"Warning: gpt-4 may update over time. Returning tokeniser assuming gpt-4-0613.\")\n            return \"gpt-4-0613\"\n        else:\n            raise NotImplementedError(\n                f\"\"\"not implemented for model {model}. \n                See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\"\n            )\n\n    @staticmethod\n    @lru_cache(maxsize=40)\n    def _get_relevant_tokeniser(model: str) -> Encoding:\n        return tiktoken.encoding_for_model(model)", ""]}
{"filename": "llm_client/llm_api_client/base_llm_api_client.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Optional\n\nfrom dataclasses_json import dataclass_json, config\n\ntry:\n    from aiohttp import ClientSession\nexcept ImportError:\n    ClientSession = Any", "\nfrom llm_client import BaseLLMClient\nfrom llm_client.consts import MODEL_KEY\n\n\nclass Role(Enum):\n    SYSTEM = \"system\"\n    USER = \"user\"\n    ASSISTANT = \"assistant\"\n", "\n\n@dataclass_json\n@dataclass\nclass ChatMessage:\n    role: Role = field(metadata=config(encoder=lambda role: role.value, decoder=Role))\n    content: str\n    name: Optional[str] = field(default=None, metadata=config(exclude=lambda name: name is None))\n    example: bool = field(default=False, metadata=config(exclude=lambda _: True))\n", "\n\n@dataclass\nclass LLMAPIClientConfig:\n    api_key: str\n    session: ClientSession\n    base_url: Optional[str] = None\n    default_model: Optional[str] = None\n    headers: dict[str, Any] = field(default_factory=dict)\n", "\n\nclass BaseLLMAPIClient(BaseLLMClient, ABC):\n    def __init__(self, config: LLMAPIClientConfig):\n        self._api_key: str = config.api_key\n        self._session: ClientSession = config.session\n        self._base_url: str = config.base_url\n        self._default_model: str = config.default_model\n        self._headers: dict[str, str] = config.headers\n\n    @abstractmethod\n    async def text_completion(self, prompt: str, model: Optional[str] = None, max_tokens: Optional[int] = None,\n                              temperature: Optional[float] = None, top_p: Optional[float] = None, **kwargs) -> list[str]:\n        raise NotImplementedError()\n\n    async def chat_completion(self, messages: list[ChatMessage], temperature: float = 0,\n                              max_tokens: int = 16, model: Optional[str] = None, **kwargs) -> list[str]:\n        raise NotImplementedError()\n\n    async def embedding(self, text: str, model: Optional[str] = None, **kwargs) -> list[float]:\n        raise NotImplementedError()\n\n    async def get_chat_tokens_count(self, messages: list[ChatMessage], **kwargs) -> int:\n        raise NotImplementedError()\n\n    def _set_model_in_kwargs(self, kwargs, model: Optional[str]) -> None:\n        if model is not None:\n            kwargs[MODEL_KEY] = model\n        kwargs.setdefault(MODEL_KEY, self._default_model)", ""]}
{"filename": "llm_client/llm_api_client/huggingface_client.py", "chunked_list": ["from typing import Optional\n\nfrom transformers import AutoTokenizer\n\nfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\n\nDEFAULT_DIR = \"OpenAssistant\"\nBASE_URL = f\"https://api-inference.huggingface.co/models/{DEFAULT_DIR}/\"\nCOMPLETIONS_KEY = 0\nINPUT_KEY = \"inputs\"", "COMPLETIONS_KEY = 0\nINPUT_KEY = \"inputs\"\nTEXT_KEY = \"generated_text\"\nAUTH_HEADER = \"Authorization\"\nBEARER_TOKEN = \"Bearer \"\nDEFAULT_MODEL = \"oasst-sft-4-pythia-12b-epoch-3.5\"\nCONST_SLASH = '/'\nEMPTY_STR = ''\nNEWLINE = '\\n'\nTEMPERATURE_KEY = \"temperature\"", "NEWLINE = '\\n'\nTEMPERATURE_KEY = \"temperature\"\nTOKENS_KEY = \"max_length\"\n\n\nclass HuggingFaceClient(BaseLLMAPIClient):\n    def __init__(self, config: LLMAPIClientConfig):\n        super().__init__(config)\n        if self._base_url is None:\n            self._base_url = BASE_URL\n        if self._default_model is None:\n            self._default_model = DEFAULT_MODEL\n        self._headers[AUTH_HEADER] = BEARER_TOKEN + self._api_key\n\n    async def text_completion(self, prompt: str, max_tokens: Optional[int] = None, temperature: float = 1.0,\n                              model: Optional[str] = None, top_p: Optional[float] = None, **kwargs) -> list[str]:\n        model = model or self._default_model\n        kwargs[\"top_p\"] = top_p\n        kwargs[INPUT_KEY] = prompt\n        kwargs[TEMPERATURE_KEY] = temperature\n        kwargs[TOKENS_KEY] = kwargs.pop(TOKENS_KEY, max_tokens)\n        response = await self._session.post(self._base_url + model + CONST_SLASH,\n                                            json=kwargs,\n                                            headers=self._headers,\n                                            raise_for_status=True)\n        response_json = await response.json()\n        if isinstance(response_json, list):\n            completions = response_json[COMPLETIONS_KEY][TEXT_KEY]\n        else:\n            completions = response_json[TEXT_KEY]\n        return [completion for completion in completions.split(NEWLINE) if completion != EMPTY_STR][1:]\n\n    def get_tokens_count(self, text: str, **kwargs) -> int:\n        tokenizer = AutoTokenizer.from_pretrained(DEFAULT_DIR + CONST_SLASH + self._default_model)\n        return len(tokenizer.encode(text))", ""]}
{"filename": "llm_client/llm_api_client/llm_api_client_factory.py", "chunked_list": ["from enum import Enum\nfrom typing import Optional\n\nfrom aiohttp import ClientSession\n\nfrom llm_client.llm_api_client.base_llm_api_client import BaseLLMAPIClient, LLMAPIClientConfig\n\n\nclass LLMAPIClientType(Enum):\n    OPEN_AI = \"OpenAI\"\n    AI21 = \"AI21\"\n    HUGGING_FACE = \"HUGGING_FACE\"\n    ALEPH_ALPHA = \"AlephAlpha\"\n    ANTHROPIC = \"ANTHROPIC\"\n    GOOGLE = \"GOOGLE\"", "class LLMAPIClientType(Enum):\n    OPEN_AI = \"OpenAI\"\n    AI21 = \"AI21\"\n    HUGGING_FACE = \"HUGGING_FACE\"\n    ALEPH_ALPHA = \"AlephAlpha\"\n    ANTHROPIC = \"ANTHROPIC\"\n    GOOGLE = \"GOOGLE\"\n\n\nclass LLMAPIClientFactory:\n    def __init__(self):\n        self._session: Optional[ClientSession] = None\n\n    async def __aenter__(self):\n        self._session = ClientSession()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._session.close()\n\n    def get_llm_api_client(self, llm_api_client_type: LLMAPIClientType, **config_kwargs) -> BaseLLMAPIClient:\n        if self._session is None:\n            raise ValueError(\"Must be used as an context manager\")\n        config = LLMAPIClientConfig(session=self._session, **config_kwargs)\n        llm_api_client_class = get_llm_api_client_class(llm_api_client_type)\n        return llm_api_client_class(config)", "\nclass LLMAPIClientFactory:\n    def __init__(self):\n        self._session: Optional[ClientSession] = None\n\n    async def __aenter__(self):\n        self._session = ClientSession()\n        return self\n\n    async def __aexit__(self, exc_type, exc_val, exc_tb):\n        await self._session.close()\n\n    def get_llm_api_client(self, llm_api_client_type: LLMAPIClientType, **config_kwargs) -> BaseLLMAPIClient:\n        if self._session is None:\n            raise ValueError(\"Must be used as an context manager\")\n        config = LLMAPIClientConfig(session=self._session, **config_kwargs)\n        llm_api_client_class = get_llm_api_client_class(llm_api_client_type)\n        return llm_api_client_class(config)", "\n\ndef get_llm_api_client_class(llm_api_client_type: LLMAPIClientType):\n    if llm_api_client_type == LLMAPIClientType.OPEN_AI:\n        from llm_client import OpenAIClient\n        return OpenAIClient\n    elif llm_api_client_type == LLMAPIClientType.AI21:\n        from llm_client import AI21Client\n        return AI21Client\n    elif llm_api_client_type == LLMAPIClientType.HUGGING_FACE:\n        from llm_client import HuggingFaceClient\n        return HuggingFaceClient\n    elif llm_api_client_type == LLMAPIClientType.ALEPH_ALPHA:\n        from llm_client import AlephAlphaClient\n        return AlephAlphaClient\n    elif llm_api_client_type == LLMAPIClientType.ANTHROPIC:\n        from llm_client import AnthropicClient\n        return AnthropicClient\n    elif llm_api_client_type == LLMAPIClientType.GOOGLE:\n        from llm_client import GoogleClient\n        return GoogleClient\n    else:\n        raise ValueError(\"Unknown LLM client type\")", ""]}
