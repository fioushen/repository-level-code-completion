{"filename": "main.py", "chunked_list": ["\"\"\"Main function for this repo\n\n\"\"\"\nimport ast\nimport argparse\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    #data\n    parser.add_argument('--phase', type=str, default='graphtrain', choices=['pretrain', 'finetune',\n                                                                            'prototrain', 'protoeval',\n                                                                            'mptitrain', 'mptieval'])\n    parser.add_argument('--dataset', type=str, default='s3dis', help='Dataset name: s3dis|scannet')\n    parser.add_argument('--cvfold', type=int, default=0, help='Fold left-out for testing in leave-one-out setting '\n                                                              'Options:{0,1}')\n    parser.add_argument('--data_path', type=str, default='./datasets/S3DIS/blocks_bs1_s1',\n                                                    help='Directory to the source data')\n    parser.add_argument('--pretrain_checkpoint_path', type=str, default=None,\n                        help='Path to the checkpoint of pre model for resuming')\n    parser.add_argument('--model_checkpoint_path', type=str, default=None,\n                        help='Path to the checkpoint of model for resuming')\n    parser.add_argument('--save_path', type=str, default='./log_s3dis/',\n                        help='Directory to the save log and checkpoints')\n    parser.add_argument('--eval_interval', type=int, default=1500,\n                        help='iteration/epoch inverval to evaluate model')\n\n    #optimization\n    parser.add_argument('--batch_size', type=int, default=32, help='Number of samples/tasks in one batch')\n    parser.add_argument('--n_workers', type=int, default=16, help='number of workers to load data')\n    parser.add_argument('--n_iters', type=int, default=30000, help='number of iterations/epochs to train')\n\n    parser.add_argument('--lr', type=float, default=0.001,\n                        help='Model (eg. protoNet or MPTI) learning rate [default: 0.001]')\n    parser.add_argument('--step_size', type=int, default=5000, help='Iterations of learning rate decay')\n    parser.add_argument('--gamma', type=float, default=0.5, help='Multiplicative factor of learning rate decay')\n\n    parser.add_argument('--pretrain_lr', type=float, default=0.001, help='pretrain learning rate [default: 0.001]')\n    parser.add_argument('--pretrain_weight_decay', type=float, default=0., help='weight decay for regularization')\n    parser.add_argument('--pretrain_step_size', type=int, default=50, help='Period of learning rate decay')\n    parser.add_argument('--pretrain_gamma', type=float, default=0.5, help='Multiplicative factor of learning rate decay')\n\n    #few-shot episode setting\n    parser.add_argument('--n_way', type=int, default=2, help='Number of classes for each episode: 1|3')\n    parser.add_argument('--k_shot', type=int, default=1, help='Number of samples/shots for each class: 1|5')\n    parser.add_argument('--n_queries', type=int, default=1, help='Number of queries for each class')\n    parser.add_argument('--n_episode_test', type=int, default=100,\n                        help='Number of episode per configuration during testing')\n\n    # Point cloud processing\n    parser.add_argument('--pc_npts', type=int, default=2048, help='Number of input points for PointNet.')\n    parser.add_argument('--pc_attribs', default='xyzrgbXYZ',\n                        help='Point attributes fed to PointNets, if empty then all possible. '\n                             'xyz = coordinates, rgb = color, XYZ = normalized xyz')\n    parser.add_argument('--pc_augm', action='store_true', help='Training augmentation for points in each superpoint')\n    parser.add_argument('--pc_augm_scale', type=float, default=0,\n                        help='Training augmentation: Uniformly random scaling in [1/scale, scale]')\n    parser.add_argument('--pc_augm_rot', type=int, default=1,\n                        help='Training augmentation: Bool, random rotation around z-axis')\n    parser.add_argument('--pc_augm_mirror_prob', type=float, default=0,\n                        help='Training augmentation: Probability of mirroring about x or y axes')\n    parser.add_argument('--pc_augm_jitter', type=int, default=1,\n                        help='Training augmentation: Bool, Gaussian jittering of all attributes')\n    parser.add_argument('--pc_augm_shift', type=float, default=0,\n                        help='Training augmentation: Probability of shifting points')\n    parser.add_argument('--pc_augm_color', type=int, default=0,\n                        help='Training augmentation: Bool, random color of all attributes')\n\n    # feature extraction network configuration\n    parser.add_argument('--dgcnn_k', type=int, default=20, help='Number of nearest neighbors in Edgeconv')\n    parser.add_argument('--edgeconv_widths', default='[[64,64], [64,64], [64,64]]', help='DGCNN Edgeconv widths')\n    parser.add_argument('--dgcnn_mlp_widths', default='[512, 256]', help='DGCNN MLP (following stacked Edgeconv) widths')\n    parser.add_argument('--base_widths', default='[128, 64]', help='BaseLearner widths')\n    parser.add_argument('--output_dim', type=int, default=64,\n                        help='The dimension of the final output of attention learner or linear mapper')\n    parser.add_argument('--use_attention', action='store_true', help='if incorporate attention learner')\n\n    # protoNet configuration\n    parser.add_argument('--dist_method', default='euclidean',\n                        help='Method to compute distance between query feature maps and prototypes.[Option: cosine|euclidean]')\n    # PAPFZS3D configuration\n    parser.add_argument('--use_align', action='store_true', help='if incorporate alignment process')\n    parser.add_argument('--use_high_dgcnn', action='store_true', help='if incorporate another dgcnn')\n    parser.add_argument('--use_supervise_prototype', action='store_true', help='if incorporate self-reconstruction process')\n    parser.add_argument('--use_transformer', action='store_true', help='if incorporate transformer process')\n    parser.add_argument('--use_linear_proj', action='store_true', help='if incorporate linear projection process')\n    parser.add_argument('--embedding_type', type=str, default='word2vec', help='semantic input')\n    parser.add_argument('--use_zero', action='store_true', help='if incorporate zero-shot learning process')\n    parser.add_argument('--trans_lr', type=float, default=0.0001, help='transformer learning rate')\n    parser.add_argument('--generator_lr', type=float, default=0.0002, help='generator learning rate')\n    parser.add_argument('--noise_dim', type=int, default=300, help='noise dim for generator')\n    parser.add_argument('--gmm_dropout', type=float, default=0.1, help='drop out rate for generator')\n    parser.add_argument('--gmm_weight', type=float, default=0.1, help='training weight for generator')\n    parser.add_argument('--train_dim', type=int, default=320, help='training dim for transformer')\n\n    # MPTI configuration\n    parser.add_argument('--n_subprototypes', type=int, default=100,\n                        help='Number of prototypes for each class in support set')\n    parser.add_argument('--k_connect', type=int, default=200,\n                        help='Number of nearest neighbors to construct local-constrained affinity matrix')\n    parser.add_argument('--sigma', type=float, default=1., help='hyeprparameter in gaussian similarity function')\n\n    args = parser.parse_args()\n\n    args.edgeconv_widths = ast.literal_eval(args.edgeconv_widths)\n    args.dgcnn_mlp_widths = ast.literal_eval(args.dgcnn_mlp_widths)\n    args.base_widths = ast.literal_eval(args.base_widths)\n    args.pc_in_dim = len(args.pc_attribs)\n\n    # Start trainer for pre-train, proto-train, proto-eval, mpti-train, mpti-test\n    if args.phase == 'mptitrain':\n        args.log_dir = args.save_path + 'log_mpti_%s_S%d_N%d_K%d_Att%d' % (args.dataset, args.cvfold,\n                                                                             args.n_way, args.k_shot,\n                                                                             args.use_attention)\n        from runs.mpti_train import train\n        train(args)\n    elif args.phase == 'prototrain':\n        args.log_dir = args.save_path + 'log_proto_%s_S%d_N%d_K%d_Att%d' % (args.dataset, args.cvfold,\n                                                                             args.n_way, args.k_shot,\n                                                                             args.use_attention)\n        from runs.proto_train import train\n        train(args)\n    elif args.phase == 'protoeval' or args.phase == 'mptieval':\n        args.log_dir = args.model_checkpoint_path\n        from runs.eval import eval\n        eval(args)\n    elif args.phase == 'pretrain':\n        args.log_dir = args.save_path + 'log_pretrain_%s_S%d' % (args.dataset, args.cvfold)\n        from runs.pre_train import pretrain\n        pretrain(args)\n    elif args.phase == 'finetune':\n        args.log_dir = args.save_path + 'log_finetune_%s_S%d_N%d_K%d' % (args.dataset, args.cvfold,\n                                                                            args.n_way, args.k_shot)\n        from runs.fine_tune import finetune\n        finetune(args)\n    else:\n        raise ValueError('Please set correct phase.')"]}
{"filename": "runs/proto_train.py", "chunked_list": ["\"\"\" Prototypical Network for Few-shot 3D Point Cloud Semantic Segmentation [Baseline]\n\n\n\"\"\"\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom runs.eval import test_few_shot", "\nfrom runs.eval import test_few_shot\nfrom dataloaders.loader import MyDataset, MyTestDataset, batch_test_task_collate\nfrom models.proto_learner import ProtoLearner\nfrom models.proto_learner_FZ import ProtoLearnerFZ\nfrom utils.cuda_util import cast_cuda\nfrom utils.logger import init_logger\n\n\ndef train(args):\n    logger = init_logger(args.log_dir, args)\n\n    # init model and optimizer\n    if args.use_zero:\n        PL = ProtoLearnerFZ(args)\n    else:\n        PL = ProtoLearner(args)\n    #Init datasets, dataloaders, and writer\n    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n                         'rot': args.pc_augm_rot,\n                         'mirror_prob': args.pc_augm_mirror_prob,\n                         'jitter': args.pc_augm_jitter,\n                         'shift': args.pc_augm_shift,\n                         'random_color': args.pc_augm_color,\n                         }\n\n    TRAIN_DATASET = MyDataset(args.data_path, args.dataset, cvfold=args.cvfold, num_episode=args.n_iters,\n                              n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                              phase=args.phase, mode='train',\n                              num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                              pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\n    VALID_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n                                  num_episode_per_comb=args.n_episode_test,\n                                  n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                  num_point=args.pc_npts, pc_attribs=args.pc_attribs)\n    VALID_CLASSES = list(VALID_DATASET.classes)\n\n    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\n    WRITER = SummaryWriter(log_dir=args.log_dir)\n\n    # train\n    best_iou = 0\n    import time\n    for batch_idx, (data, sampled_classes) in enumerate(TRAIN_LOADER):\n\n        if torch.cuda.is_available():\n            data = cast_cuda(data)\n\n        loss, accuracy = PL.train(data, sampled_classes)\n        if (batch_idx+1) % 100 == 0:\n            logger.cprint('=====[Train] Iter: %d | Loss: %.4f | Accuracy: %f =====' % (batch_idx, loss, accuracy))\n            WRITER.add_scalar('Train/loss', loss, batch_idx)\n            WRITER.add_scalar('Train/accuracy', accuracy, batch_idx)\n\n\n        if (batch_idx+1) % args.eval_interval == 0:\n\n            valid_loss, mean_IoU = test_few_shot(VALID_LOADER, PL, logger, VALID_CLASSES, args.use_zero)\n\n            WRITER.add_scalar('Valid/loss', valid_loss, batch_idx)\n            WRITER.add_scalar('Valid/meanIoU', mean_IoU, batch_idx)\n            if mean_IoU > best_iou:\n                best_iou = mean_IoU\n                logger.cprint('*******************Model Saved*******************')\n\n                save_dict = {'iteration': batch_idx + 1,\n                             'model_state_dict': PL.model.state_dict(),\n                             'loss': valid_loss,\n                             'IoU': best_iou\n                             }\n                torch.save(save_dict, os.path.join(args.log_dir, 'checkpoint.tar'))\n            logger.cprint('=====Best IoU Is: %f =====' % (best_iou))\n\n    WRITER.close()", "\ndef train(args):\n    logger = init_logger(args.log_dir, args)\n\n    # init model and optimizer\n    if args.use_zero:\n        PL = ProtoLearnerFZ(args)\n    else:\n        PL = ProtoLearner(args)\n    #Init datasets, dataloaders, and writer\n    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n                         'rot': args.pc_augm_rot,\n                         'mirror_prob': args.pc_augm_mirror_prob,\n                         'jitter': args.pc_augm_jitter,\n                         'shift': args.pc_augm_shift,\n                         'random_color': args.pc_augm_color,\n                         }\n\n    TRAIN_DATASET = MyDataset(args.data_path, args.dataset, cvfold=args.cvfold, num_episode=args.n_iters,\n                              n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                              phase=args.phase, mode='train',\n                              num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                              pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\n    VALID_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n                                  num_episode_per_comb=args.n_episode_test,\n                                  n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                  num_point=args.pc_npts, pc_attribs=args.pc_attribs)\n    VALID_CLASSES = list(VALID_DATASET.classes)\n\n    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\n    WRITER = SummaryWriter(log_dir=args.log_dir)\n\n    # train\n    best_iou = 0\n    import time\n    for batch_idx, (data, sampled_classes) in enumerate(TRAIN_LOADER):\n\n        if torch.cuda.is_available():\n            data = cast_cuda(data)\n\n        loss, accuracy = PL.train(data, sampled_classes)\n        if (batch_idx+1) % 100 == 0:\n            logger.cprint('=====[Train] Iter: %d | Loss: %.4f | Accuracy: %f =====' % (batch_idx, loss, accuracy))\n            WRITER.add_scalar('Train/loss', loss, batch_idx)\n            WRITER.add_scalar('Train/accuracy', accuracy, batch_idx)\n\n\n        if (batch_idx+1) % args.eval_interval == 0:\n\n            valid_loss, mean_IoU = test_few_shot(VALID_LOADER, PL, logger, VALID_CLASSES, args.use_zero)\n\n            WRITER.add_scalar('Valid/loss', valid_loss, batch_idx)\n            WRITER.add_scalar('Valid/meanIoU', mean_IoU, batch_idx)\n            if mean_IoU > best_iou:\n                best_iou = mean_IoU\n                logger.cprint('*******************Model Saved*******************')\n\n                save_dict = {'iteration': batch_idx + 1,\n                             'model_state_dict': PL.model.state_dict(),\n                             'loss': valid_loss,\n                             'IoU': best_iou\n                             }\n                torch.save(save_dict, os.path.join(args.log_dir, 'checkpoint.tar'))\n            logger.cprint('=====Best IoU Is: %f =====' % (best_iou))\n\n    WRITER.close()", ""]}
{"filename": "runs/fine_tune.py", "chunked_list": ["\"\"\" Finetune Baseline for Few-shot 3D Point Cloud Semantic Segmentation\n\n\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter", "from torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom runs.eval import evaluate_metric\nfrom runs.pre_train import DGCNNSeg\nfrom models.dgcnn import DGCNN\nfrom dataloaders.loader import MyTestDataset, batch_test_task_collate, augment_pointcloud\nfrom utils.logger import init_logger\nfrom utils.cuda_util import cast_cuda\nfrom utils.checkpoint_util import load_pretrain_checkpoint", "from utils.cuda_util import cast_cuda\nfrom utils.checkpoint_util import load_pretrain_checkpoint\n\n\nclass FineTuner(object):\n    def __init__(self, args):\n\n        self.n_way = args.n_way\n        self.k_shot = args.k_shot\n        self.n_queries = args.n_queries\n        self.n_points = args.pc_npts\n\n        # init model and optimizer\n        self.model = DGCNNSeg(args, self.n_way+1)\n        print(self.model)\n        if torch.cuda.is_available():\n            self.model.cuda()\n\n        self.optimizer = torch.optim.Adam(self.model.segmenter.parameters(), lr=args.lr)\n\n        # load pretrained model for point cloud encoding\n        self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n\n\n    def train(self, support_x, support_y):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way*k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way*k_shot, num_points), each point \\in {0,..., n_way}\n        \"\"\"\n        support_logits = self.model(support_x)\n\n        train_loss = F.cross_entropy(support_logits, support_y)\n\n        self.optimizer.zero_grad()\n        train_loss.backward()\n        self.optimizer.step()\n\n        return train_loss\n\n    def test(self, query_x, query_y):\n        \"\"\"\n        Args:\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        \"\"\"\n\n        self.model.eval()\n\n        with torch.no_grad():\n            query_logits = self.model(query_x)\n            test_loss = F.cross_entropy(query_logits, query_y)\n\n            pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n            correct = torch.eq(pred, query_y).sum().item()\n            accuracy = correct / (self.n_queries*self.n_points)\n\n        return pred, test_loss, accuracy", "\n\ndef support_mask_to_label(support_masks, n_way, k_shot, num_points):\n    \"\"\"\n    Args:\n        support_masks: binary (foreground/background) masks with shape (n_way, k_shot, num_points)\n    \"\"\"\n    support_masks = support_masks.view(n_way, k_shot*num_points)\n    support_labels = []\n    for n in range(support_masks.shape[0]):\n        support_mask = support_masks[n, :] #(k_shot*num_points)\n        support_label = torch.zeros_like(support_mask)\n        mask_index = torch.nonzero(support_mask).squeeze(1)\n        support_label= support_label.scatter_(0, mask_index, n+1)\n        support_labels.append(support_label)\n\n    support_labels = torch.stack(support_labels, dim=0)\n    support_labels = support_labels.view(n_way, k_shot, num_points)\n\n    return support_labels.long()", "\n\ndef finetune(args):\n    num_iters = args.n_iters\n\n    logger = init_logger(args.log_dir, args)\n\n    #Init datasets, dataloaders, and writer\n    DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n                                 num_episode_per_comb=args.n_episode_test,\n                                 n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                 num_point=args.pc_npts, pc_attribs=args.pc_attribs, mode='test')\n    CLASSES = list(DATASET.classes)\n    DATA_LOADER = DataLoader(DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n    WRITER = SummaryWriter(log_dir=args.log_dir)\n\n    #Init model and optimizer\n    FT = FineTuner(args)\n\n    predicted_label_total = []\n    gt_label_total = []\n    label2class_total = []\n\n    global_iter = 0\n    for batch_idx, (data, sampled_classes) in enumerate(DATA_LOADER):\n        query_label = data[-1]\n        data[1] = support_mask_to_label(data[1], args.n_way, args.k_shot, args.pc_npts)\n\n        if torch.cuda.is_available():\n            data = cast_cuda(data)\n\n        [support_x, support_y, query_x, query_y] = data\n        support_x = support_x.view(args.n_way * args.k_shot, -1, args.pc_npts)\n        support_y = support_y.view(args.n_way * args.k_shot, args.pc_npts)\n\n        # train on support set\n        for i in range(num_iters):\n            train_loss = FT.train(support_x, support_y)\n\n            WRITER.add_scalar('Train/loss', train_loss, global_iter)\n            logger.cprint('=====[Train] Batch_idx: %d | Iter: %d | Loss: %.4f =====' % (batch_idx, i, train_loss.item()))\n\n            global_iter += 1\n\n        # test on query set\n        query_pred, test_loss, accuracy = FT.test(query_x, query_y)\n        WRITER.add_scalar('Test/loss', test_loss, global_iter)\n        WRITER.add_scalar('Test/accuracy', accuracy, global_iter)\n        logger.cprint(\n            '=====[Valid] Batch_idx: %d | Loss: %.4f =====' % (batch_idx, test_loss.item()))\n\n        #compute metric for predictions\n        predicted_label_total.append(query_pred.cpu().detach().numpy())\n        gt_label_total.append(query_label.numpy())\n        label2class_total.append(sampled_classes)\n\n    mean_IoU = evaluate_metric(logger, predicted_label_total, gt_label_total, label2class_total, CLASSES)\n    logger.cprint('\\n=====[Test] Mean IoU: %f =====\\n' % mean_IoU)", ""]}
{"filename": "runs/__init__.py", "chunked_list": [""]}
{"filename": "runs/mpti_train.py", "chunked_list": ["\"\"\" Attetion-aware Multi-Prototype Transductive Inference for Few-shot 3D Point Cloud Semantic Segmentation [Our method]\n\n\n\"\"\"\nimport os\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom runs.eval import test_few_shot", "\nfrom runs.eval import test_few_shot\nfrom dataloaders.loader import MyDataset, MyTestDataset, batch_test_task_collate\nfrom models.mpti_learner import MPTILearner\nfrom utils.cuda_util import cast_cuda\nfrom utils.logger import init_logger\n\n\ndef train(args):\n    logger = init_logger(args.log_dir, args)\n\n    # os.system('cp models/mpti_learner.py %s' % (args.log_dir))\n    # os.system('cp models/mpti.py %s' % (args.log_dir))\n    # os.system('cp models/dgcnn.py %s' % (args.log_dir))\n\n    # init model and optimizer\n    MPTI = MPTILearner(args)\n\n    #Init datasets, dataloaders, and writer\n    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n                         'rot': args.pc_augm_rot,\n                         'mirror_prob': args.pc_augm_mirror_prob,\n                         'shift': args.pc_augm_shift,\n                         'jitter': args.pc_augm_jitter\n                         }\n\n    TRAIN_DATASET = MyDataset(args.data_path, args.dataset, cvfold=args.cvfold, num_episode=args.n_iters,\n                              n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                              phase=args.phase, mode='train',\n                              num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                              pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\n    VALID_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n                                  num_episode_per_comb=args.n_episode_test,\n                                  n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                  num_point=args.pc_npts, pc_attribs=args.pc_attribs)\n    VALID_CLASSES = list(VALID_DATASET.classes)\n\n    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\n    WRITER = SummaryWriter(log_dir=args.log_dir)\n\n    # train\n    best_iou = 0\n    for batch_idx, (data, sampled_classes) in enumerate(TRAIN_LOADER):\n\n        if torch.cuda.is_available():\n            data = cast_cuda(data)\n\n        loss, accuracy = MPTI.train(data)\n        if (batch_idx+1) % 100 == 0:\n            logger.cprint('==[Train] Iter: %d | Loss: %.4f |  Accuracy: %f  ==' % (batch_idx, loss, accuracy))\n            WRITER.add_scalar('Train/loss', loss, batch_idx)\n            WRITER.add_scalar('Train/accuracy', accuracy, batch_idx)\n\n        if (batch_idx+1) % args.eval_interval == 0:\n\n            valid_loss, mean_IoU = test_few_shot(VALID_LOADER, MPTI, logger, VALID_CLASSES)\n            logger.cprint('\\n=====[VALID] Loss: %.4f | Mean IoU: %f  =====\\n' % (valid_loss, mean_IoU))\n            WRITER.add_scalar('Valid/loss', valid_loss, batch_idx)\n            WRITER.add_scalar('Valid/meanIoU', mean_IoU, batch_idx)\n            if mean_IoU > best_iou:\n                best_iou = mean_IoU\n                logger.cprint('*******************Model Saved*******************')\n                save_dict = {'iteration': batch_idx + 1,\n                             'model_state_dict': MPTI.model.state_dict(),\n                             'optimizer_state_dict': MPTI.optimizer.state_dict(),\n                             'loss': valid_loss,\n                             'IoU': best_iou\n                             }\n                torch.save(save_dict, os.path.join(args.log_dir, 'checkpoint.tar'))\n\n    WRITER.close()", "def train(args):\n    logger = init_logger(args.log_dir, args)\n\n    # os.system('cp models/mpti_learner.py %s' % (args.log_dir))\n    # os.system('cp models/mpti.py %s' % (args.log_dir))\n    # os.system('cp models/dgcnn.py %s' % (args.log_dir))\n\n    # init model and optimizer\n    MPTI = MPTILearner(args)\n\n    #Init datasets, dataloaders, and writer\n    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n                         'rot': args.pc_augm_rot,\n                         'mirror_prob': args.pc_augm_mirror_prob,\n                         'shift': args.pc_augm_shift,\n                         'jitter': args.pc_augm_jitter\n                         }\n\n    TRAIN_DATASET = MyDataset(args.data_path, args.dataset, cvfold=args.cvfold, num_episode=args.n_iters,\n                              n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                              phase=args.phase, mode='train',\n                              num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                              pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\n    VALID_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n                                  num_episode_per_comb=args.n_episode_test,\n                                  n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                  num_point=args.pc_npts, pc_attribs=args.pc_attribs)\n    VALID_CLASSES = list(VALID_DATASET.classes)\n\n    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=1, collate_fn=batch_test_task_collate)\n\n    WRITER = SummaryWriter(log_dir=args.log_dir)\n\n    # train\n    best_iou = 0\n    for batch_idx, (data, sampled_classes) in enumerate(TRAIN_LOADER):\n\n        if torch.cuda.is_available():\n            data = cast_cuda(data)\n\n        loss, accuracy = MPTI.train(data)\n        if (batch_idx+1) % 100 == 0:\n            logger.cprint('==[Train] Iter: %d | Loss: %.4f |  Accuracy: %f  ==' % (batch_idx, loss, accuracy))\n            WRITER.add_scalar('Train/loss', loss, batch_idx)\n            WRITER.add_scalar('Train/accuracy', accuracy, batch_idx)\n\n        if (batch_idx+1) % args.eval_interval == 0:\n\n            valid_loss, mean_IoU = test_few_shot(VALID_LOADER, MPTI, logger, VALID_CLASSES)\n            logger.cprint('\\n=====[VALID] Loss: %.4f | Mean IoU: %f  =====\\n' % (valid_loss, mean_IoU))\n            WRITER.add_scalar('Valid/loss', valid_loss, batch_idx)\n            WRITER.add_scalar('Valid/meanIoU', mean_IoU, batch_idx)\n            if mean_IoU > best_iou:\n                best_iou = mean_IoU\n                logger.cprint('*******************Model Saved*******************')\n                save_dict = {'iteration': batch_idx + 1,\n                             'model_state_dict': MPTI.model.state_dict(),\n                             'optimizer_state_dict': MPTI.optimizer.state_dict(),\n                             'loss': valid_loss,\n                             'IoU': best_iou\n                             }\n                torch.save(save_dict, os.path.join(args.log_dir, 'checkpoint.tar'))\n\n    WRITER.close()"]}
{"filename": "runs/eval.py", "chunked_list": ["\"\"\"Evaluating functions for Few-shot 3D Point Cloud Semantic Segmentation\n\n\n\"\"\"\nimport os\nimport numpy as np\nfrom datetime import datetime\n\nimport torch\nfrom torch.utils.data import DataLoader", "import torch\nfrom torch.utils.data import DataLoader\n\nfrom dataloaders.loader import MyTestDataset, batch_test_task_collate\nfrom models.proto_learner import ProtoLearner\nfrom models.proto_learner_FZ import ProtoLearnerFZ\nfrom models.mpti_learner import MPTILearner\nfrom utils.cuda_util import cast_cuda\nfrom utils.logger import init_logger\n", "from utils.logger import init_logger\n\n\ndef evaluate_metric(logger, pred_labels_list, gt_labels_list, label2class_list, test_classes):\n    \"\"\"\n    :param pred_labels_list: a list of np array, each entry with shape (n_queries*n_way, num_points).\n    :param gt_labels_list: a list of np array, each entry with shape (n_queries*n_way, num_points).\n    :param test_classes: a list of np array, each entry with shape (n_way,)\n    :return: iou: scaler\n    \"\"\"\n    assert len(pred_labels_list) == len(gt_labels_list) == len(label2class_list)\n\n    logger.cprint('*****Test Classes: {0}*****'.format(test_classes))\n\n    NUM_CLASS = len(test_classes) + 1  # add 1 to consider background class\n    gt_classes = [0 for _ in range(NUM_CLASS)]\n    positive_classes = [0 for _ in range(NUM_CLASS)]\n    true_positive_classes = [0 for _ in range(NUM_CLASS)]\n\n    for i, batch_gt_labels in enumerate(gt_labels_list):\n        batch_pred_labels = pred_labels_list[i]  # (n_queries*n_way, num_points)\n        label2class = label2class_list[i]  # (n_way,)\n        for j in range(batch_pred_labels.shape[0]):\n            for k in range(batch_pred_labels.shape[1]):\n                gt = int(batch_gt_labels[j, k])\n                pred = int(batch_pred_labels[j, k])\n\n                if gt == 0:  # 0 indicate background class\n                    gt_index = 0\n                else:\n                    gt_class = label2class[gt - 1]  # the ground truth class in the dataset\n                    gt_index = test_classes.index(gt_class) + 1\n                gt_classes[gt_index] += 1\n\n                if pred == 0:\n                    pred_index = 0\n                else:\n                    pred_class = label2class[pred - 1]\n                    pred_index = test_classes.index(pred_class) + 1\n                positive_classes[pred_index] += 1\n\n                true_positive_classes[gt_index] += int(gt == pred)\n\n    iou_list = []\n    for c in range(NUM_CLASS):\n        iou = true_positive_classes[c] / float(gt_classes[c] + positive_classes[c] - true_positive_classes[c])\n        logger.cprint('----- [class %d]  IoU: %f -----' % (c, iou))\n        iou_list.append(iou)\n\n    mean_IoU = np.array(iou_list[1:]).mean()\n\n    return mean_IoU", "\n\ndef test_few_shot(test_loader, learner, logger, test_classes, use_zero=False):\n    total_loss = 0\n\n    predicted_label_total = []\n    gt_label_total = []\n    label2class_total = []\n    start_time = time.time()\n    print(str(datetime.now()))\n    for batch_idx, (data, sampled_classes) in enumerate(test_loader):\n        query_label = data[-1]\n\n        if torch.cuda.is_available():\n            data = cast_cuda(data)\n        if use_zero:\n            query_pred, loss, accuracy = learner.test_semantic(data, sampled_classes)\n        else:\n            query_pred, loss, accuracy = learner.test(data)\n        total_loss += loss.detach().item()\n\n        if (batch_idx + 1) % 100 == 0:\n            logger.cprint('[Eval] Iter: %d | Loss: %.4f | %s' % (batch_idx + 1, loss.detach().item(), str(datetime.now())))\n\n\n        predicted_label_total.append(query_pred.cpu().detach().numpy())\n        gt_label_total.append(query_label.numpy())\n        label2class_total.append(sampled_classes)\n\n    mean_loss = total_loss / len(test_loader)\n    mean_IoU = evaluate_metric(logger, predicted_label_total, gt_label_total, label2class_total, test_classes)\n    return mean_loss, mean_IoU", "\n\nimport time\n\n\ndef eval(args):\n    logger = init_logger(args.log_dir, args)\n\n    if args.phase == 'protoeval':\n        if args.use_zero:\n            learner = ProtoLearnerFZ(args, mode='test')\n        else:\n            learner = ProtoLearner(args, mode='test')\n    elif args.phase == 'mptieval':\n        learner = MPTILearner(args, mode='test')\n\n    # Init dataset, dataloader\n    TEST_DATASET = MyTestDataset(args.data_path, args.dataset, cvfold=args.cvfold,\n                                 num_episode_per_comb=args.n_episode_test,\n                                 n_way=args.n_way, k_shot=args.k_shot, n_queries=args.n_queries,\n                                 num_point=args.pc_npts, pc_attribs=args.pc_attribs, mode='test')\n    TEST_CLASSES = list(TEST_DATASET.classes)\n    TEST_LOADER = DataLoader(TEST_DATASET, batch_size=1, shuffle=False, collate_fn=batch_test_task_collate)\n\n    test_loss, mean_IoU = test_few_shot(TEST_LOADER, learner, logger, TEST_CLASSES, args.use_zero)\n\n\n    logger.cprint('\\n=====[TEST] Loss: %.4f | Mean IoU: %f =====\\n' % (test_loss, mean_IoU))", ""]}
{"filename": "runs/pre_train.py", "chunked_list": ["\"\"\" Pre-train phase\n\n\n\"\"\"\n\nimport os\nimport numpy as np\n\nimport torch\nimport torch.nn as nn", "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom dataloaders.loader import MyPretrainDataset\nfrom models.dgcnn import DGCNN\nfrom models.dgcnn_new import DGCNN_semseg", "from models.dgcnn import DGCNN\nfrom models.dgcnn_new import DGCNN_semseg\nfrom utils.logger import init_logger\nfrom utils.checkpoint_util import save_pretrain_checkpoint\n\n\nclass DGCNNSeg(nn.Module):\n    def __init__(self, args, num_classes):\n        super(DGCNNSeg, self).__init__()\n        if args.use_high_dgcnn:\n            self.encoder = DGCNN_semseg(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n        else:\n            self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n        in_dim = args.dgcnn_mlp_widths[-1]\n        for edgeconv_width in args.edgeconv_widths:\n            in_dim += edgeconv_width[-1]\n        self.segmenter = nn.Sequential(\n                            nn.Conv1d(in_dim, 256, 1, bias=False),\n                            nn.BatchNorm1d(256),\n                            nn.LeakyReLU(0.2),\n                            nn.Conv1d(256, 128, 1),\n                            nn.BatchNorm1d(128),\n                            nn.LeakyReLU(0.2),\n                            nn.Dropout(0.3),\n                            nn.Conv1d(128, num_classes, 1)\n                         )\n\n    def forward(self, pc):\n        num_points = pc.shape[2]\n        edgeconv_feats, point_feat, _ = self.encoder(pc)\n        global_feat = point_feat.max(dim=-1, keepdim=True)[0]\n        edgeconv_feats.append(global_feat.expand(-1,-1,num_points))\n        pc_feat = torch.cat(edgeconv_feats, dim=1)\n\n        logits = self.segmenter(pc_feat)\n        return logits", "\n\ndef metric_evaluate(predicted_label, gt_label, NUM_CLASS):\n    \"\"\"\n    :param predicted_label: (B,N) tensor\n    :param gt_label: (B,N) tensor\n    :return: iou: scaler\n    \"\"\"\n    gt_classes = [0 for _ in range(NUM_CLASS)]\n    positive_classes = [0 for _ in range(NUM_CLASS)]\n    true_positive_classes = [0 for _ in range(NUM_CLASS)]\n\n    for i in range(gt_label.size()[0]):\n        pred_pc = predicted_label[i]\n        gt_pc = gt_label[i]\n\n        for j in range(gt_pc.shape[0]):\n            gt_l = int(gt_pc[j])\n            pred_l = int(pred_pc[j])\n            gt_classes[gt_l] += 1\n            positive_classes[pred_l] += 1\n            true_positive_classes[gt_l] += int(gt_l == pred_l)\n\n    oa = sum(true_positive_classes)/float(sum(positive_classes))\n    print('Overall accuracy: {0}'.format(oa))\n    iou_list = []\n\n    for i in range(NUM_CLASS):\n        iou_class = true_positive_classes[i] / float(gt_classes[i]+positive_classes[i]-true_positive_classes[i])\n        print('Class_%d: iou_class is %f' % (i, iou_class))\n        iou_list.append(iou_class)\n\n    mean_IoU = np.array(iou_list[1:]).mean()\n\n    return oa, mean_IoU, iou_list", "\n\ndef pretrain(args):\n    logger = init_logger(args.log_dir, args)\n\n    # Init datasets, dataloaders, and writer\n    PC_AUGMENT_CONFIG = {'scale': args.pc_augm_scale,\n                         'rot': args.pc_augm_rot,\n                         'mirror_prob': args.pc_augm_mirror_prob,\n                         'jitter': args.pc_augm_jitter,\n                         'shift': args.pc_augm_shift,\n                         'random_color': args.pc_augm_color,\n                         }\n\n    if args.dataset == 's3dis':\n        from dataloaders.s3dis import S3DISDataset\n        DATASET = S3DISDataset(args.cvfold, args.data_path)\n    elif args.dataset == 'scannet':\n        from dataloaders.scannet import ScanNetDataset\n        DATASET = ScanNetDataset(args.cvfold, args.data_path)\n    else:\n        raise NotImplementedError('Unknown dataset %s!' % args.dataset)\n\n    CLASSES = DATASET.train_classes\n    NUM_CLASSES = len(CLASSES) + 1\n    CLASS2SCANS = {c: DATASET.class2scans[c] for c in CLASSES}\n\n    TRAIN_DATASET = MyPretrainDataset(args.data_path, CLASSES, CLASS2SCANS, mode='train',\n                                      num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                                      pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\n    VALID_DATASET = MyPretrainDataset(args.data_path, CLASSES, CLASS2SCANS, mode='test',\n                                      num_point=args.pc_npts, pc_attribs=args.pc_attribs,\n                                      pc_augm=args.pc_augm, pc_augm_config=PC_AUGMENT_CONFIG)\n\n    logger.cprint('=== Pre-train Dataset (classes: {0}) | Train: {1} blocks | Valid: {2} blocks ==='.format(\n                                                     CLASSES, len(TRAIN_DATASET), len(VALID_DATASET)))\n\n    TRAIN_LOADER = DataLoader(TRAIN_DATASET, batch_size=args.batch_size, num_workers=args.n_workers, shuffle=True,\n                              drop_last=True)\n\n    VALID_LOADER = DataLoader(VALID_DATASET, batch_size=args.batch_size, num_workers=args.n_workers, shuffle=False,\n                              drop_last=True)\n\n    WRITER = SummaryWriter(log_dir=args.log_dir)\n\n    # Init model and optimizer\n    model = DGCNNSeg(args, num_classes=NUM_CLASSES)\n    print(model)\n    if torch.cuda.is_available():\n        model.cuda()\n\n    optimizer = optim.Adam([{'params': model.encoder.parameters(), 'lr': args.pretrain_lr}, \\\n                           {'params': model.segmenter.parameters(), 'lr': args.pretrain_lr}], \\\n                            weight_decay=args.pretrain_weight_decay)\n    # Set learning rate scheduler\n    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.pretrain_step_size, gamma=args.pretrain_gamma)\n\n    # train\n    best_iou = 0\n    global_iter = 0\n    for epoch in range(args.n_iters):\n        model.train()\n        for batch_idx, (ptclouds, labels) in enumerate(TRAIN_LOADER):\n            if torch.cuda.is_available():\n                ptclouds = ptclouds.cuda()\n                labels = labels.cuda()\n\n            logits = model(ptclouds)\n            loss = F.cross_entropy(logits, labels)\n\n            # Loss backwards and optimizer updates\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if (batch_idx + 1) % 100 == 0:\n                WRITER.add_scalar('Train/loss', loss, global_iter)\n                logger.cprint('=====[Train] Epoch: %d | Iter: %d | Loss: %.4f =====' % (epoch, batch_idx, loss.item()))\n            global_iter += 1\n\n        lr_scheduler.step()\n\n        if (epoch+1) % args.eval_interval == 0:\n            pred_total = []\n            gt_total = []\n            model.eval()\n            with torch.no_grad():\n                for i, (ptclouds, labels) in enumerate(VALID_LOADER):\n                    gt_total.append(labels.detach())\n\n                    if torch.cuda.is_available():\n                        ptclouds = ptclouds.cuda()\n                        labels = labels.cuda()\n\n                    logits = model(ptclouds)\n                    loss = F.cross_entropy(logits, labels)\n\n                    # \u3000Compute predictions\n                    _, preds = torch.max(logits.detach(), dim=1, keepdim=False)\n                    pred_total.append(preds.cpu().detach())\n\n                    WRITER.add_scalar('Valid/loss', loss, global_iter)\n                    # logger.cprint(\n                    #     '=====[Valid] Epoch: %d | Iter: %d | Loss: %.4f =====' % (epoch, i, loss.item()))\n\n            pred_total = torch.stack(pred_total, dim=0).view(-1, args.pc_npts)\n            gt_total = torch.stack(gt_total, dim=0).view(-1, args.pc_npts)\n            accuracy, mIoU, iou_perclass = metric_evaluate(pred_total, gt_total, NUM_CLASSES)\n            logger.cprint('===== EPOCH [%d]: Accuracy: %f | mIoU: %f =====\\n' % (epoch, accuracy, mIoU))\n            WRITER.add_scalar('Valid/overall_accuracy', accuracy, global_iter)\n            WRITER.add_scalar('Valid/meanIoU', mIoU, global_iter)\n\n            if mIoU > best_iou:\n                best_iou = mIoU\n                logger.cprint('*******************Model Saved*******************')\n                save_pretrain_checkpoint(model, args.log_dir)\n            logger.cprint('=====Best IoU Is: %f =====' % (best_iou))\n    WRITER.close()", ""]}
{"filename": "dataloaders/loader.py", "chunked_list": ["\"\"\" Data Loader for Generating Tasks\n\n\n\"\"\"\nimport os\nimport random\nimport math\nimport glob\nimport numpy as np\nimport h5py as h5", "import numpy as np\nimport h5py as h5\nimport transforms3d\nfrom itertools import combinations\n\nimport torch\nfrom torch.utils.data import Dataset\n\n\ndef sample_K_pointclouds(data_path, num_point, pc_attribs, pc_augm, pc_augm_config,\n                         scan_names, sampled_class, sampled_classes, is_support=False):\n    '''sample K pointclouds and the corresponding labels for one class (one_way)'''\n    ptclouds  = []\n    labels = []\n    for scan_name in scan_names:\n        ptcloud, label = sample_pointcloud(data_path, num_point, pc_attribs, pc_augm, pc_augm_config,\n                                           scan_name, sampled_classes, sampled_class, support=is_support)\n        ptclouds.append(ptcloud)\n        labels.append(label)\n\n    ptclouds = np.stack(ptclouds, axis=0)\n    labels = np.stack(labels, axis=0)\n\n    return ptclouds, labels", "\ndef sample_K_pointclouds(data_path, num_point, pc_attribs, pc_augm, pc_augm_config,\n                         scan_names, sampled_class, sampled_classes, is_support=False):\n    '''sample K pointclouds and the corresponding labels for one class (one_way)'''\n    ptclouds  = []\n    labels = []\n    for scan_name in scan_names:\n        ptcloud, label = sample_pointcloud(data_path, num_point, pc_attribs, pc_augm, pc_augm_config,\n                                           scan_name, sampled_classes, sampled_class, support=is_support)\n        ptclouds.append(ptcloud)\n        labels.append(label)\n\n    ptclouds = np.stack(ptclouds, axis=0)\n    labels = np.stack(labels, axis=0)\n\n    return ptclouds, labels", "\n\ndef sample_pointcloud(data_path, num_point, pc_attribs, pc_augm, pc_augm_config, scan_name,\n                      sampled_classes, sampled_class=0, support=False, random_sample=False):\n    sampled_classes = list(sampled_classes)\n    data = np.load(os.path.join(data_path, 'data', '%s.npy' %scan_name))\n    N = data.shape[0] #number of points in this scan\n\n    if random_sample:\n        sampled_point_inds = np.random.choice(np.arange(N), num_point, replace=(N < num_point))\n    else:\n        # If this point cloud is for support/query set, make sure that the sampled points contain target class\n        valid_point_inds = np.nonzero(data[:,6] == sampled_class)[0]  # indices of points belonging to the sampled class\n\n        if N < num_point:\n            sampled_valid_point_num = len(valid_point_inds)\n        else:\n            valid_ratio = len(valid_point_inds)/float(N)\n            sampled_valid_point_num = int(valid_ratio*num_point)\n\n        sampled_valid_point_inds = np.random.choice(valid_point_inds, sampled_valid_point_num, replace=False)\n        sampled_other_point_inds = np.random.choice(np.arange(N), num_point-sampled_valid_point_num,\n                                                    replace=(N<num_point))\n        sampled_point_inds = np.concatenate([sampled_valid_point_inds, sampled_other_point_inds])\n\n    data = data[sampled_point_inds]\n    xyz = data[:, 0:3]\n    rgb = data[:, 3:6]\n    labels = data[:, 6].astype(np.int)\n\n\n    xyz_min = np.amin(xyz, axis=0)\n    xyz -= xyz_min\n    if pc_augm:\n        xyz = augment_pointcloud(xyz, pc_augm_config)\n    if 'XYZ' in pc_attribs:\n        xyz_min = np.amin(xyz, axis=0)\n        XYZ = xyz - xyz_min\n        xyz_max = np.amax(XYZ, axis=0)\n        XYZ = XYZ/xyz_max\n\n    ptcloud = []\n    if 'xyz' in pc_attribs: ptcloud.append(xyz)\n    if 'rgb' in pc_attribs: ptcloud.append(rgb/255.)\n    if 'XYZ' in pc_attribs: ptcloud.append(XYZ)\n    ptcloud = np.concatenate(ptcloud, axis=1)\n\n    if support:\n        groundtruth = labels == sampled_class\n    else:\n        groundtruth = np.zeros_like(labels)\n        for i, label in enumerate(labels):\n            if label in sampled_classes:\n                groundtruth[i] = sampled_classes.index(label)+1\n\n    return ptcloud, groundtruth", "\n\ndef augment_pointcloud(P, pc_augm_config):\n    \"\"\"\" Augmentation on XYZ and jittering of everything \"\"\"\n    M = transforms3d.zooms.zfdir2mat(1)\n    if pc_augm_config['scale'] > 1:\n        s = random.uniform(1 / pc_augm_config['scale'], pc_augm_config['scale'])\n        M = np.dot(transforms3d.zooms.zfdir2mat(s), M)\n    if pc_augm_config['rot'] == 1:\n        angle = random.uniform(0, 2 * math.pi)\n        M = np.dot(transforms3d.axangles.axangle2mat([0, 0, 1], angle), M)  # z=upright assumption\n    if pc_augm_config['mirror_prob'] > 0:  # mirroring x&y, not z\n        if random.random() < pc_augm_config['mirror_prob'] / 2:\n            M = np.dot(transforms3d.zooms.zfdir2mat(-1, [1, 0, 0]), M)\n        if random.random() < pc_augm_config['mirror_prob'] / 2:\n            M = np.dot(transforms3d.zooms.zfdir2mat(-1, [0, 1, 0]), M)\n    P[:, :3] = np.dot(P[:, :3], M.T)\n    if pc_augm_config['shift'] > 0:\n        shift = np.random.uniform(-pc_augm_config['shift'], pc_augm_config['shift'], 3)\n        P[:, :3] += shift\n    if pc_augm_config['jitter']:\n        sigma, clip = 0.01, 0.05  # https://github.com/charlesq34/pointnet/blob/master/provider.py#L74\n        P = P + np.clip(sigma * np.random.randn(*P.shape), -1 * clip, clip).astype(np.float32)\n    return P", "\n\nclass MyDataset(Dataset):\n    def __init__(self, data_path, dataset_name, cvfold=0, num_episode=50000, n_way=3, k_shot=5, n_queries=1,\n                 phase=None, mode='train', num_point=4096, pc_attribs='xyz', pc_augm=False, pc_augm_config=None):\n        super(MyDataset).__init__()\n        self.data_path = data_path\n        self.n_way = n_way\n        self.k_shot = k_shot\n        self.n_queries = n_queries\n        self.num_episode = num_episode\n        self.phase = phase\n        self.mode = mode\n        self.num_point = num_point\n        self.pc_attribs = pc_attribs\n        self.pc_augm = pc_augm\n        self.pc_augm_config = pc_augm_config\n\n        if dataset_name == 's3dis':\n            from dataloaders.s3dis import S3DISDataset\n            self.dataset = S3DISDataset(cvfold, data_path)\n        elif dataset_name == 'scannet':\n            from dataloaders.scannet import ScanNetDataset\n            self.dataset = ScanNetDataset(cvfold, data_path)\n        else:\n            raise NotImplementedError('Unknown dataset %s!' % dataset_name)\n\n        if mode == 'train':\n            self.classes = np.array(self.dataset.train_classes)\n        elif mode == 'test':\n            self.classes = np.array(self.dataset.test_classes)\n        else:\n            raise NotImplementedError('Unkown mode %s! [Options: train/test]' % mode)\n\n        print('MODE: {0} | Classes: {1}'.format(mode, self.classes))\n        self.class2scans = self.dataset.class2scans\n\n    def __len__(self):\n        return self.num_episode\n\n    def __getitem__(self, index, n_way_classes=None):\n        if n_way_classes is not None:\n            sampled_classes = np.array(n_way_classes)\n        else:\n            sampled_classes = np.random.choice(self.classes, self.n_way, replace=False)\n\n        support_ptclouds, support_masks, query_ptclouds, query_labels = self.generate_one_episode(sampled_classes)\n\n        if self.mode == 'train' and self.phase == 'metatrain':\n            remain_classes = list(set(self.classes) - set(sampled_classes))\n            try:\n                sampled_valid_classes = np.random.choice(np.array(remain_classes), self.n_way, replace=False)\n            except:\n                raise NotImplementedError('Error! The number remaining classes is less than %d_way' %self.n_way)\n\n            valid_support_ptclouds, valid_support_masks, valid_query_ptclouds, \\\n                                            valid_query_labels = self.generate_one_episode(sampled_valid_classes)\n\n            return support_ptclouds.astype(np.float32), \\\n                   support_masks.astype(np.int32), \\\n                   query_ptclouds.astype(np.float32), \\\n                   query_labels.astype(np.int64), \\\n                   valid_support_ptclouds.astype(np.float32), \\\n                   valid_support_masks.astype(np.int32), \\\n                   valid_query_ptclouds.astype(np.float32), \\\n                   valid_query_labels.astype(np.int64)\n        else:\n            return support_ptclouds.astype(np.float32), \\\n                   support_masks.astype(np.int32), \\\n                   query_ptclouds.astype(np.float32), \\\n                   query_labels.astype(np.int64), \\\n                   sampled_classes.astype(np.int32)\n\n\n    def generate_one_episode(self, sampled_classes):\n        support_ptclouds = []\n        support_masks = []\n        query_ptclouds = []\n        query_labels = []\n\n        black_list = []  # to store the sampled scan names, in order to prevent sampling one scan several times...\n        for sampled_class in sampled_classes:\n            all_scannames = self.class2scans[sampled_class].copy()\n            if len(black_list) != 0:\n                all_scannames = [x for x in all_scannames if x not in black_list]\n            selected_scannames = np.random.choice(all_scannames, self.k_shot+self.n_queries, replace=False)\n            black_list.extend(selected_scannames)\n            query_scannames = selected_scannames[:self.n_queries]\n            support_scannames = selected_scannames[self.n_queries:]\n\n            query_ptclouds_one_way, query_labels_one_way = sample_K_pointclouds(self.data_path, self.num_point,\n                                                                                self.pc_attribs, self.pc_augm,\n                                                                                self.pc_augm_config,\n                                                                                query_scannames,\n                                                                                sampled_class,\n                                                                                sampled_classes,\n                                                                                is_support=False)\n\n            support_ptclouds_one_way, support_masks_one_way = sample_K_pointclouds(self.data_path, self.num_point,\n                                                                                self.pc_attribs, self.pc_augm,\n                                                                                self.pc_augm_config,\n                                                                                support_scannames,\n                                                                                sampled_class,\n                                                                                sampled_classes,\n                                                                                is_support=True)\n\n            query_ptclouds.append(query_ptclouds_one_way)\n            query_labels.append(query_labels_one_way)\n            support_ptclouds.append(support_ptclouds_one_way)\n            support_masks.append(support_masks_one_way)\n\n        support_ptclouds = np.stack(support_ptclouds, axis=0)\n        support_masks = np.stack(support_masks, axis=0)\n        query_ptclouds = np.concatenate(query_ptclouds, axis=0)\n        query_labels = np.concatenate(query_labels, axis=0)\n\n        return support_ptclouds, support_masks, query_ptclouds, query_labels", "\n\ndef batch_train_task_collate(batch):\n    task_train_support_ptclouds, task_train_support_masks, task_train_query_ptclouds, task_train_query_labels, \\\n    task_valid_support_ptclouds, task_valid_support_masks, task_valid_query_ptclouds, task_valid_query_labels = list(zip(*batch))\n\n    task_train_support_ptclouds = np.stack(task_train_support_ptclouds)\n    task_train_support_masks = np.stack(task_train_support_masks)\n    task_train_query_ptclouds = np.stack(task_train_query_ptclouds)\n    task_train_query_labels = np.stack(task_train_query_labels)\n    task_valid_support_ptclouds = np.stack(task_valid_support_ptclouds)\n    task_valid_support_masks = np.stack(task_valid_support_masks)\n    task_valid_query_ptclouds = np.array(task_valid_query_ptclouds)\n    task_valid_query_labels = np.stack(task_valid_query_labels)\n\n    data = [torch.from_numpy(task_train_support_ptclouds).transpose(3,4), torch.from_numpy(task_train_support_masks),\n            torch.from_numpy(task_train_query_ptclouds).transpose(2,3), torch.from_numpy(task_train_query_labels),\n            torch.from_numpy(task_valid_support_ptclouds).transpose(3,4), torch.from_numpy(task_valid_support_masks),\n            torch.from_numpy(task_valid_query_ptclouds).transpose(2,3), torch.from_numpy(task_valid_query_labels)]\n\n    return data", "\n\n################################################ Static Testing Dataset ################################################\n\nclass MyTestDataset(Dataset):\n    def __init__(self, data_path, dataset_name, cvfold=0, num_episode_per_comb=100, n_way=3, k_shot=5, n_queries=1,\n                       num_point=4096, pc_attribs='xyz', mode='valid'):\n        super(MyTestDataset).__init__()\n\n        dataset = MyDataset(data_path, dataset_name, cvfold=cvfold, n_way=n_way, k_shot=k_shot, n_queries=n_queries,\n                            mode='test', num_point=num_point, pc_attribs=pc_attribs, pc_augm=False)\n        self.classes = dataset.classes\n\n        if mode == 'valid':\n            test_data_path = os.path.join(data_path, 'S_%d_N_%d_K_%d_episodes_%d_pts_%d' % (\n                                                    cvfold, n_way, k_shot, num_episode_per_comb, num_point))\n        elif mode == 'test':\n            test_data_path = os.path.join(data_path, 'S_%d_N_%d_K_%d_test_episodes_%d_pts_%d' % (\n                                                    cvfold, n_way, k_shot, num_episode_per_comb, num_point))\n        else:\n            raise NotImplementedError('Mode (%s) is unknown!' %mode)\n\n        if os.path.exists(test_data_path):\n            self.file_names = glob.glob(os.path.join(test_data_path, '*.h5'))\n            self.num_episode = len(self.file_names)\n        else:\n            print('Test dataset (%s) does not exist...\\n Constructing...' %test_data_path)\n            os.mkdir(test_data_path)\n\n            class_comb = list(combinations(self.classes, n_way))  # [(),(),(),...]\n            self.num_episode = len(class_comb) * num_episode_per_comb\n\n            episode_ind = 0\n            self.file_names = []\n            for sampled_classes in class_comb:\n                sampled_classes = list(sampled_classes)\n                for i in range(num_episode_per_comb):\n                    data = dataset.__getitem__(episode_ind, sampled_classes)\n                    out_filename = os.path.join(test_data_path, '%d.h5' % episode_ind)\n                    write_episode(out_filename, data)\n                    self.file_names.append(out_filename)\n                    episode_ind += 1\n\n    def __len__(self):\n        return self.num_episode\n\n    def __getitem__(self, index):\n        file_name = self.file_names[index]\n        return read_episode(file_name)", "\n\ndef batch_test_task_collate(batch):\n    batch_support_ptclouds, batch_support_masks, batch_query_ptclouds, batch_query_labels, batch_sampled_classes = batch[0]\n\n    data = [torch.from_numpy(batch_support_ptclouds).transpose(2,3), torch.from_numpy(batch_support_masks),\n            torch.from_numpy(batch_query_ptclouds).transpose(1,2), torch.from_numpy(batch_query_labels.astype(np.int64))]\n\n    return data, batch_sampled_classes\n", "\n\ndef write_episode(out_filename, data):\n    support_ptclouds, support_masks, query_ptclouds, query_labels, sampled_classes = data\n    data_file = h5.File(out_filename, 'w')\n    data_file.create_dataset('support_ptclouds', data=support_ptclouds, dtype='float32')\n    data_file.create_dataset('support_masks', data=support_masks, dtype='int32')\n    data_file.create_dataset('query_ptclouds', data=query_ptclouds, dtype='float32')\n    data_file.create_dataset('query_labels', data=query_labels, dtype='int64')\n    data_file.create_dataset('sampled_classes', data=sampled_classes, dtype='int32')\n    data_file.close()\n\n    print('\\t {0} saved! | classes: {1}'.format(out_filename, sampled_classes))", "\n\ndef read_episode(file_name):\n    data_file = h5.File(file_name, 'r')\n    support_ptclouds = data_file['support_ptclouds'][:]\n    support_masks = data_file['support_masks'][:]\n    query_ptclouds = data_file['query_ptclouds'][:]\n    query_labels = data_file['query_labels'][:]\n    sampled_classes = data_file['sampled_classes'][:]\n\n    return support_ptclouds, support_masks, query_ptclouds, query_labels, sampled_classes", "\n\n\n################################################  Pre-train Dataset ################################################\nclass MyPretrainDataset(Dataset):\n    def __init__(self, data_path, classes, class2scans, mode='train', num_point=4096, pc_attribs='xyz',\n                       pc_augm=False, pc_augm_config=None):\n        super(MyPretrainDataset).__init__()\n        self.data_path = data_path\n        self.classes = classes\n        self.num_point = num_point\n        self.pc_attribs = pc_attribs\n        self.pc_augm = pc_augm\n        self.pc_augm_config = pc_augm_config\n\n        train_block_names = []\n        all_block_names = []\n        for k, v in sorted(class2scans.items()):\n            all_block_names.extend(v)\n            n_blocks = len(v)\n            n_test_blocks = int(n_blocks * 0.1)\n            n_train_blocks = n_blocks - n_test_blocks\n            train_block_names.extend(v[:n_train_blocks])\n\n        if mode == 'train':\n            self.block_names = list(set(train_block_names))\n        elif mode == 'test':\n            self.block_names = list(set(all_block_names) - set(train_block_names))\n        else:\n            raise NotImplementedError('Mode is unknown!')\n\n        print('[Pretrain Dataset] Mode: {0} | Num_blocks: {1}'.format(mode, len(self.block_names)))\n\n    def __len__(self):\n        return len(self.block_names)\n\n    def __getitem__(self, index):\n        block_name = self.block_names[index]\n\n        ptcloud, label = sample_pointcloud(self.data_path, self.num_point, self.pc_attribs, self.pc_augm,\n                                           self.pc_augm_config, block_name, self.classes, random_sample=True)\n\n        return torch.from_numpy(ptcloud.transpose().astype(np.float32)), torch.from_numpy(label.astype(np.int64))"]}
{"filename": "dataloaders/scannet.py", "chunked_list": ["\"\"\" Data Preprocess and Loader for ScanNetV2 Dataset\n\n\n\"\"\"\nimport os\nimport glob\nimport numpy as np\nimport pickle\n\n\nclass ScanNetDataset(object):\n    def __init__(self, cvfold, data_path):\n        self.data_path = data_path\n        self.classes = 21\n        # self.class2type = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk', 6:'bed', 7:'bookshelf',\n        #                    8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet', 12:'curtain', 13:'counter', 14:'door',\n        #                    15:'window', 16:'shower curtain', 17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}\n        class_names = open(os.path.join(os.path.dirname(data_path), 'meta', 'scannet_classnames.txt')).readlines()\n        self.class2type = {i: name.strip() for i, name in enumerate(class_names)}\n        self.type2class = {self.class2type[t]: t for t in self.class2type}\n        self.types = self.type2class.keys()\n\n        self.fold_0 = ['bathtub', 'bed', 'bookshelf', 'cabinet', 'chair','counter', 'curtain', 'desk', 'door', 'floor']\n        self.fold_1 = ['otherfurniture', 'picture', 'refridgerator', 'shower curtain', 'sink', 'sofa', 'table', 'toilet', 'wall', 'window']\n\n        if cvfold == 0:\n            self.test_classes = [self.type2class[i] for i in self.fold_0]\n        elif cvfold == 1:\n            self.test_classes = [self.type2class[i] for i in self.fold_1]\n        else:\n            raise NotImplementedError('Unknown cvfold (%s). [Options: 0,1]' %cvfold)\n\n        all_classes = [i for i in range(1, self.classes)]\n        self.train_classes = [c for c in all_classes if c not in self.test_classes]\n\n        self.class2scans = self.get_class2scans()\n\n    def get_class2scans(self):\n        class2scans_file = os.path.join(self.data_path, 'class2scans.pkl')\n        if os.path.exists(class2scans_file):\n            #load class2scans (dictionary)\n            with open(class2scans_file, 'rb') as f:\n                class2scans = pickle.load(f)\n        else:\n            min_ratio = .05  # to filter out scans with only rare labelled points\n            min_pts = 100  # to filter out scans with only rare labelled points\n            class2scans = {k:[] for k in range(self.classes)}\n\n            for file in glob.glob(os.path.join(self.data_path, 'data', '*.npy')):\n                scan_name = os.path.basename(file)[:-4]\n                data = np.load(file)\n                labels = data[:,6].astype(np.int)\n                classes = np.unique(labels)\n                print('{0} | shape: {1} | classes: {2}'.format(scan_name, data.shape, list(classes)))\n                for class_id in classes:\n                    #if the number of points for the target class is too few, do not add this sample into the dictionary\n                    num_points = np.count_nonzero(labels == class_id)\n                    threshold = max(int(data.shape[0]*min_ratio), min_pts)\n                    if num_points > threshold:\n                        class2scans[class_id].append(scan_name)\n\n            print('==== class to scans mapping is done ====')\n            for class_id in range(self.classes):\n                print('\\t class_id: {0} | min_ratio: {1} | min_pts: {2} | class_name: {3} | num of scans: {4}'.format(\n                          class_id,  min_ratio, min_pts, self.class2type[class_id], len(class2scans[class_id])))\n\n            with open(class2scans_file, 'wb') as f:\n                pickle.dump(class2scans, f, pickle.HIGHEST_PROTOCOL)\n        return class2scans", "\n\nclass ScanNetDataset(object):\n    def __init__(self, cvfold, data_path):\n        self.data_path = data_path\n        self.classes = 21\n        # self.class2type = {0:'unannotated', 1:'wall', 2:'floor', 3:'chair', 4:'table', 5:'desk', 6:'bed', 7:'bookshelf',\n        #                    8:'sofa', 9:'sink', 10:'bathtub', 11:'toilet', 12:'curtain', 13:'counter', 14:'door',\n        #                    15:'window', 16:'shower curtain', 17:'refridgerator', 18:'picture', 19:'cabinet', 20:'otherfurniture'}\n        class_names = open(os.path.join(os.path.dirname(data_path), 'meta', 'scannet_classnames.txt')).readlines()\n        self.class2type = {i: name.strip() for i, name in enumerate(class_names)}\n        self.type2class = {self.class2type[t]: t for t in self.class2type}\n        self.types = self.type2class.keys()\n\n        self.fold_0 = ['bathtub', 'bed', 'bookshelf', 'cabinet', 'chair','counter', 'curtain', 'desk', 'door', 'floor']\n        self.fold_1 = ['otherfurniture', 'picture', 'refridgerator', 'shower curtain', 'sink', 'sofa', 'table', 'toilet', 'wall', 'window']\n\n        if cvfold == 0:\n            self.test_classes = [self.type2class[i] for i in self.fold_0]\n        elif cvfold == 1:\n            self.test_classes = [self.type2class[i] for i in self.fold_1]\n        else:\n            raise NotImplementedError('Unknown cvfold (%s). [Options: 0,1]' %cvfold)\n\n        all_classes = [i for i in range(1, self.classes)]\n        self.train_classes = [c for c in all_classes if c not in self.test_classes]\n\n        self.class2scans = self.get_class2scans()\n\n    def get_class2scans(self):\n        class2scans_file = os.path.join(self.data_path, 'class2scans.pkl')\n        if os.path.exists(class2scans_file):\n            #load class2scans (dictionary)\n            with open(class2scans_file, 'rb') as f:\n                class2scans = pickle.load(f)\n        else:\n            min_ratio = .05  # to filter out scans with only rare labelled points\n            min_pts = 100  # to filter out scans with only rare labelled points\n            class2scans = {k:[] for k in range(self.classes)}\n\n            for file in glob.glob(os.path.join(self.data_path, 'data', '*.npy')):\n                scan_name = os.path.basename(file)[:-4]\n                data = np.load(file)\n                labels = data[:,6].astype(np.int)\n                classes = np.unique(labels)\n                print('{0} | shape: {1} | classes: {2}'.format(scan_name, data.shape, list(classes)))\n                for class_id in classes:\n                    #if the number of points for the target class is too few, do not add this sample into the dictionary\n                    num_points = np.count_nonzero(labels == class_id)\n                    threshold = max(int(data.shape[0]*min_ratio), min_pts)\n                    if num_points > threshold:\n                        class2scans[class_id].append(scan_name)\n\n            print('==== class to scans mapping is done ====')\n            for class_id in range(self.classes):\n                print('\\t class_id: {0} | min_ratio: {1} | min_pts: {2} | class_name: {3} | num of scans: {4}'.format(\n                          class_id,  min_ratio, min_pts, self.class2type[class_id], len(class2scans[class_id])))\n\n            with open(class2scans_file, 'wb') as f:\n                pickle.dump(class2scans, f, pickle.HIGHEST_PROTOCOL)\n        return class2scans", "\n\nif __name__ == '__main__':\n    import argparse\n    parser = argparse.ArgumentParser(description='Pre-training on ShapeNet')\n    parser.add_argument('--cvfold', type=int, default=0, help='Fold left-out for testing in leave-one-out setting '\n                                                              'Options: {0,1}')\n    parser.add_argument('--data_path', type=str, default='../datasets/ScanNet/blocks_bs1_s1', help='Directory to source data')\n    args = parser.parse_args()\n    dataset = ScanNetDataset(args.cvfold, args.data_path)"]}
{"filename": "dataloaders/get_embedding.py", "chunked_list": ["import gensim.downloader as api\nimport numpy as np\nmodel = api.load('glove-wiki-gigaword-300')  # download trained model\ncategories = ['ceiling', 'floor', 'wall', 'beam', 'column', 'window', 'door', 'table', 'chair', 'sofa', 'bookcase', 'board', 'clutter']\n\nw2vectors = []\nfor word in categories:\n    w2vectors.append(model[word])\nw2v = [np.expand_dims(x, axis=0) for x in w2vectors]\nembeddings = np.concatenate(w2v, axis=0)", "w2v = [np.expand_dims(x, axis=0) for x in w2vectors]\nembeddings = np.concatenate(w2v, axis=0)\nnp.save('dataloaders/S3DIS_glove.npy', embeddings)\n# 'word2vec-ruscorpora-300',glove-wiki-gigaword-300, 'word2vec-google-news-300','conceptnet-numberbatch-17-06-300','fasttext-wiki-news-subwords-300'\n\n\ncategories = ['clutter', 'wall', 'floor', 'chair', 'table', 'desk', 'bed', 'bookshelf',\n                   'sofa', 'sink', 'bathtub', 'toilet', 'curtain', 'counter', 'door',\n                   'window', 'curtain', 'refrigerator', 'picture', 'cabinet', 'furniture']\nw2vectors = []", "                   'window', 'curtain', 'refrigerator', 'picture', 'cabinet', 'furniture']\nw2vectors = []\n\nfor word in categories:\n    w2vectors.append(model[word])\n\nw2v = [np.expand_dims(x, axis=0) for x in w2vectors]\nembeddings = np.concatenate(w2v, axis=0)\nnp.save('dataloaders/ScanNet_glove.npy', embeddings)", "np.save('dataloaders/ScanNet_glove.npy', embeddings)"]}
{"filename": "dataloaders/s3dis.py", "chunked_list": ["\"\"\" Data Preprocess and Loader for S3DIS Dataset\n\n\n\"\"\"\nimport os\nimport glob\nimport numpy as np\nimport pickle\n\n\nclass S3DISDataset(object):\n    def __init__(self, cvfold, data_path):\n        self.data_path = data_path\n        self.classes = 13\n        # self.class2type = {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',\n        #                    8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}\n        class_names = open(os.path.join(os.path.dirname(data_path), 'meta', 's3dis_classnames.txt')).readlines()\n        self.class2type = {i: name.strip() for i, name in enumerate(class_names)}\n        print(self.class2type)\n        self.type2class = {self.class2type[t]: t for t in self.class2type}\n        self.types = self.type2class.keys()\n        self.fold_0 = ['beam', 'board', 'bookcase', 'ceiling', 'chair', 'column']\n        self.fold_1 = ['door', 'floor', 'sofa', 'table', 'wall', 'window']\n\n        if cvfold == 0:\n            self.test_classes = [self.type2class[i] for i in self.fold_0]\n        elif cvfold == 1:\n            self.test_classes = [self.type2class[i] for i in self.fold_1]\n        else:\n            raise NotImplementedError('Unknown cvfold (%s). [Options: 0,1]' %cvfold)\n\n        all_classes = [i for i in range(0, self.classes-1)]\n        self.train_classes = [c for c in all_classes if c not in self.test_classes]\n\n        # print('train_class:{0}'.format(self.train_classes))\n        # print('test_class:{0}'.format(self.test_classes))\n\n        self.class2scans = self.get_class2scans()\n\n    def get_class2scans(self):\n        class2scans_file = os.path.join(self.data_path, 'class2scans.pkl')\n        if os.path.exists(class2scans_file):\n            #load class2scans (dictionary)\n            with open(class2scans_file, 'rb') as f:\n                class2scans = pickle.load(f)\n        else:\n            min_ratio = .05  # to filter out scans with only rare labelled points\n            min_pts = 100  # to filter out scans with only rare labelled points\n            class2scans = {k:[] for k in range(self.classes)}\n\n            for file in glob.glob(os.path.join(self.data_path, 'data', '*.npy')):\n                scan_name = os.path.basename(file)[:-4]\n                data = np.load(file)\n                labels = data[:,6].astype(np.int)\n                classes = np.unique(labels)\n                print('{0} | shape: {1} | classes: {2}'.format(scan_name, data.shape, list(classes)))\n                for class_id in classes:\n                    #if the number of points for the target class is too few, do not add this sample into the dictionary\n                    num_points = np.count_nonzero(labels == class_id)\n                    threshold = max(int(data.shape[0]*min_ratio), min_pts)\n                    if num_points > threshold:\n                        class2scans[class_id].append(scan_name)\n\n            print('==== class to scans mapping is done ====')\n            for class_id in range(self.classes):\n                print('\\t class_id: {0} | min_ratio: {1} | min_pts: {2} | class_name: {3} | num of scans: {4}'.format(\n                          class_id,  min_ratio, min_pts, self.class2type[class_id], len(class2scans[class_id])))\n\n            with open(class2scans_file, 'wb') as f:\n                pickle.dump(class2scans, f, pickle.HIGHEST_PROTOCOL)\n        return class2scans", "\n\nclass S3DISDataset(object):\n    def __init__(self, cvfold, data_path):\n        self.data_path = data_path\n        self.classes = 13\n        # self.class2type = {0:'ceiling', 1:'floor', 2:'wall', 3:'beam', 4:'column', 5:'window', 6:'door', 7:'table',\n        #                    8:'chair', 9:'sofa', 10:'bookcase', 11:'board', 12:'clutter'}\n        class_names = open(os.path.join(os.path.dirname(data_path), 'meta', 's3dis_classnames.txt')).readlines()\n        self.class2type = {i: name.strip() for i, name in enumerate(class_names)}\n        print(self.class2type)\n        self.type2class = {self.class2type[t]: t for t in self.class2type}\n        self.types = self.type2class.keys()\n        self.fold_0 = ['beam', 'board', 'bookcase', 'ceiling', 'chair', 'column']\n        self.fold_1 = ['door', 'floor', 'sofa', 'table', 'wall', 'window']\n\n        if cvfold == 0:\n            self.test_classes = [self.type2class[i] for i in self.fold_0]\n        elif cvfold == 1:\n            self.test_classes = [self.type2class[i] for i in self.fold_1]\n        else:\n            raise NotImplementedError('Unknown cvfold (%s). [Options: 0,1]' %cvfold)\n\n        all_classes = [i for i in range(0, self.classes-1)]\n        self.train_classes = [c for c in all_classes if c not in self.test_classes]\n\n        # print('train_class:{0}'.format(self.train_classes))\n        # print('test_class:{0}'.format(self.test_classes))\n\n        self.class2scans = self.get_class2scans()\n\n    def get_class2scans(self):\n        class2scans_file = os.path.join(self.data_path, 'class2scans.pkl')\n        if os.path.exists(class2scans_file):\n            #load class2scans (dictionary)\n            with open(class2scans_file, 'rb') as f:\n                class2scans = pickle.load(f)\n        else:\n            min_ratio = .05  # to filter out scans with only rare labelled points\n            min_pts = 100  # to filter out scans with only rare labelled points\n            class2scans = {k:[] for k in range(self.classes)}\n\n            for file in glob.glob(os.path.join(self.data_path, 'data', '*.npy')):\n                scan_name = os.path.basename(file)[:-4]\n                data = np.load(file)\n                labels = data[:,6].astype(np.int)\n                classes = np.unique(labels)\n                print('{0} | shape: {1} | classes: {2}'.format(scan_name, data.shape, list(classes)))\n                for class_id in classes:\n                    #if the number of points for the target class is too few, do not add this sample into the dictionary\n                    num_points = np.count_nonzero(labels == class_id)\n                    threshold = max(int(data.shape[0]*min_ratio), min_pts)\n                    if num_points > threshold:\n                        class2scans[class_id].append(scan_name)\n\n            print('==== class to scans mapping is done ====')\n            for class_id in range(self.classes):\n                print('\\t class_id: {0} | min_ratio: {1} | min_pts: {2} | class_name: {3} | num of scans: {4}'.format(\n                          class_id,  min_ratio, min_pts, self.class2type[class_id], len(class2scans[class_id])))\n\n            with open(class2scans_file, 'wb') as f:\n                pickle.dump(class2scans, f, pickle.HIGHEST_PROTOCOL)\n        return class2scans"]}
{"filename": "dataloaders/__init__.py", "chunked_list": [""]}
{"filename": "utils/checkpoint_util.py", "chunked_list": ["\"\"\" Util functions for loading and saving checkpoints\n\n\n\"\"\"\nimport os\nimport torch\n\n\ndef load_pretrain_checkpoint(model, pretrain_checkpoint_path):\n    # load pretrained model for point cloud encoding\n    model_dict = model.state_dict()\n    if pretrain_checkpoint_path is not None:\n        print('Load encoder module from pretrained checkpoint...')\n        pretrained_dict = torch.load(os.path.join(pretrain_checkpoint_path, 'checkpoint.tar'))['params']\n        pretrained_dict = {'encoder.' + k: v for k, v in pretrained_dict.items()}\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        model_dict.update(pretrained_dict)\n        model.load_state_dict(model_dict)\n    else:\n        raise ValueError('Pretrained checkpoint must be given.')\n\n    return model", "def load_pretrain_checkpoint(model, pretrain_checkpoint_path):\n    # load pretrained model for point cloud encoding\n    model_dict = model.state_dict()\n    if pretrain_checkpoint_path is not None:\n        print('Load encoder module from pretrained checkpoint...')\n        pretrained_dict = torch.load(os.path.join(pretrain_checkpoint_path, 'checkpoint.tar'))['params']\n        pretrained_dict = {'encoder.' + k: v for k, v in pretrained_dict.items()}\n        pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n        model_dict.update(pretrained_dict)\n        model.load_state_dict(model_dict)\n    else:\n        raise ValueError('Pretrained checkpoint must be given.')\n\n    return model", "\n\ndef load_model_checkpoint(model, model_checkpoint_path, optimizer=None, mode='test'):\n    try:\n        checkpoint = torch.load(os.path.join(model_checkpoint_path, 'checkpoint.tar'))\n        start_iter = checkpoint['iteration']\n        start_iou = checkpoint['IoU']\n    except:\n        raise ValueError('Model checkpoint file must be correctly given (%s).' %model_checkpoint_path)\n\n    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n    if mode == 'test':\n        print('Load model checkpoint at Iteration %d (IoU %f)...' % (start_iter, start_iou))\n        return model\n    else:\n        try:\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        except:\n            print('Checkpoint does not include optimizer state dict...')\n        print('Resume from checkpoint at Iteration %d (IoU %f)...' % (start_iter, start_iou))\n        return model, optimizer", "\n\ndef save_pretrain_checkpoint(model, output_path):\n    torch.save(dict(params=model.encoder.state_dict()), os.path.join(output_path, 'checkpoint.tar'))"]}
{"filename": "utils/logger.py", "chunked_list": ["\"\"\" Util functions for writing logs\n\n\n\"\"\"\nimport os\n\nclass IOStream():\n    def __init__(self, path):\n        self.f = open(path, 'a')\n\n    def cprint(self, text):\n        print(text)\n        self.f.write(text+'\\n')\n        self.f.flush()\n\n    def close(self):\n        self.f.close()", "\n\ndef mkdir(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n\ndef print_args(logger, args):\n    opt = vars(args)\n    logger.cprint('------------ Options -------------')\n    for k, v in sorted(opt.items()):\n        logger.cprint('%s: %s' % (str(k), str(v)))\n    logger.cprint('-------------- End ----------------\\n')", "\n\ndef init_logger(log_dir, args):\n    mkdir(log_dir)\n    log_file = os.path.join(log_dir, 'log_%s.txt' %args.phase)\n    logger = IOStream(log_file)\n    # logger.cprint(str(args))\n    ## print arguments in format\n    print_args(logger, args)\n    return logger"]}
{"filename": "utils/cuda_util.py", "chunked_list": ["\"\"\" Cuda util function\n\n\n\"\"\"\n\ndef cast_cuda(input):\n    if type(input) == type([]):\n        for i in range(len(input)):\n            input[i] = cast_cuda(input[i])\n    else:\n        return input.cuda()\n    return input"]}
{"filename": "utils/__init__.py", "chunked_list": [""]}
{"filename": "preprocess/room2blocks.py", "chunked_list": ["\"\"\" Processing step 1, split room into blocks\n\n\n\"\"\"\n\nimport os\nimport glob\nimport numpy as np\n\n# -----------------------------------------------------------------------------", "\n# -----------------------------------------------------------------------------\n# PREPARE BLOCK DATA FOR SUPERPOINT GRAPH GENERATION\n# -----------------------------------------------------------------------------\n\ndef room2blocks(data, block_size, stride, min_npts):\n    \"\"\" Prepare block data.\n    Args:\n        data: N x 7 numpy array, 012 are XYZ in meters, 345 are RGB in [0,255], 6 is the labels\n            assumes the data is not shifted (min point is not origin),\n        block_size: float, physical size of the block in meters\n        stride: float, stride for block sweeping\n    Returns:\n        blocks_list: a list of blocks, each block is a num_point x 7 np array\n    \"\"\"\n    assert (stride <= block_size)\n\n    xyz = data[:,:3]\n    xyz_min = np.amin(xyz, axis=0)\n    xyz -= xyz_min\n    xyz_max = np.amax(xyz, axis=0)\n\n    # Get the corner location for our sampling blocks\n    xbeg_list = []\n    ybeg_list = []\n    num_block_x = int(np.ceil((xyz_max[0] - block_size) / stride)) + 1\n    num_block_y = int(np.ceil((xyz_max[1] - block_size) / stride)) + 1\n    for i in range(num_block_x):\n        for j in range(num_block_y):\n            xbeg_list.append(i * stride)\n            ybeg_list.append(j * stride)\n\n    # Collect blocks\n    blocks_list = []\n    for idx in range(len(xbeg_list)):\n        xbeg = xbeg_list[idx]\n        ybeg = ybeg_list[idx]\n        xcond = (xyz[:, 0] <= xbeg + block_size) & (xyz[:, 0] >= xbeg)\n        ycond = (xyz[:, 1] <= ybeg + block_size) & (xyz[:, 1] >= ybeg)\n        cond = xcond & ycond\n        if np.sum(cond) < min_npts:  # discard block if there are less than 100 pts.\n            continue\n\n        block = data[cond, :]\n        blocks_list.append(block)\n\n    return blocks_list", "\n\ndef room2blocks_wrapper(room_path, block_size, stride, min_npts):\n    if room_path[-3:] == 'txt':\n        data = np.loadtxt(room_path)\n    elif room_path[-3:] == 'npy':\n        data = np.load(room_path)\n    else:\n        print('Unknown file type! exiting.')\n        exit()\n    return room2blocks(data, block_size, stride, min_npts)", "\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser(description='[Preprocessing] Split rooms into blocks')\n    parser.add_argument('--data_path', default='../datasets/S3DIS/scenes')\n    parser.add_argument('--dataset', default='s3dis', metavar='bs', help='s3dis|scannet')\n    parser.add_argument('--block_size', type=float, default=1, metavar='s', help='size of each block')\n    parser.add_argument('--stride', type=float, default=1, help='stride of sliding window for splitting rooms, '\n                                                                'stride should be not larger than block size')\n    parser.add_argument('--min_npts', type=int, default=1000, help='the minimum number of points in a block,'\n                                                                  'if less than this threshold, the block is discarded')\n\n    args = parser.parse_args()\n\n    DATA_PATH = args.data_path\n    BLOCK_SIZE = args.block_size\n    STRIDE = args.stride\n    MIN_NPTS = args.min_npts\n    SAVE_PATH = os.path.join(os.path.dirname(DATA_PATH), 'blocks_bs{0}_s{1}'.format(BLOCK_SIZE, STRIDE), 'data')\n    if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n\n    file_paths = glob.glob(os.path.join(DATA_PATH, 'data', '*.npy'))\n    print('{} scenes to be split...'.format(len(file_paths)))\n\n    block_cnt = 0\n    for file_path in file_paths:\n        room_name = os.path.basename(file_path)[:-4]\n        blocks_list = room2blocks_wrapper(file_path, block_size=BLOCK_SIZE, stride=STRIDE, min_npts=MIN_NPTS)\n        print('{0} is split into {1} blocks.'.format(room_name, len(blocks_list)))\n        block_cnt += len(blocks_list)\n\n        for i, block_data in enumerate(blocks_list):\n            block_filename = room_name + '_block_' + str(i) + '.npy'\n            np.save(os.path.join(SAVE_PATH, block_filename), block_data)\n\n    print(\"Total samples: {0}\".format(block_cnt))"]}
{"filename": "preprocess/collect_scannet_data.py", "chunked_list": ["\"\"\" Collect point clouds and the corresponding labels from original ScanNetV2 dataset, and save into numpy files.\n\n\n\"\"\"\nimport os\nimport sys\nimport json\nimport numpy as np\nfrom plyfile import PlyData\n", "from plyfile import PlyData\n\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = os.path.dirname(BASE_DIR)\nsys.path.append(ROOT_DIR)\n\n\ndef get_raw2scannet_label_map(label_mapping_file):\n    lines = [line.rstrip() for line in open(label_mapping_file)]\n    lines = lines[1:]\n    raw2scannet = {}\n    label_classes_set = set(CLASS_NAMES)\n    for i in range(len(lines)):\n        elements = lines[i].split('\\t')\n        raw_name = elements[1]\n        nyu40_name = elements[7]\n        if nyu40_name not in label_classes_set:\n            raw2scannet[raw_name] = 'unannotated'\n        else:\n            raw2scannet[raw_name] = nyu40_name\n    return raw2scannet", "\n\ndef read_ply_xyzrgb(filename):\n    \"\"\" read XYZRGB point cloud from filename PLY file \"\"\"\n    assert(os.path.isfile(filename))\n    with open(filename, 'rb') as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata['vertex'].count\n        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n        vertices[:,0] = plydata['vertex'].data['x']\n        vertices[:,1] = plydata['vertex'].data['y']\n        vertices[:,2] = plydata['vertex'].data['z']\n        vertices[:,3] = plydata['vertex'].data['red']\n        vertices[:,4] = plydata['vertex'].data['green']\n        vertices[:,5] = plydata['vertex'].data['blue']\n    return vertices", "\n\ndef collect_point_label(scene_path, scene_name, out_filename):\n    # Over-segmented segments: maps from segment to vertex/point IDs\n    mesh_seg_filename = os.path.join(scene_path, '%s_vh_clean_2.0.010000.segs.json' % (scene_name))\n    # print mesh_seg_filename\n    with open(mesh_seg_filename) as jsondata:\n        d = json.load(jsondata)\n        seg = d['segIndices']\n        # print len(seg)\n    segid_to_pointid = {}\n    for i in range(len(seg)):\n        if seg[i] not in segid_to_pointid:\n            segid_to_pointid[seg[i]] = []\n        segid_to_pointid[seg[i]].append(i)\n\n    # Raw points in XYZRGBA\n    ply_filename = os.path.join(scene_path, '%s_vh_clean_2.ply' % (scene_name))\n    points = read_ply_xyzrgb(ply_filename)\n    print('{0}: {1} points'.format(scene_name, points.shape[0]))\n\n    # Instances over-segmented segment IDs: annotation on segments\n    instance_segids = []\n    labels = []\n    annotation_filename = os.path.join(scene_path, '%s.aggregation.json' % (scene_name))\n    # print annotation_filename\n    with open(annotation_filename) as jsondata:\n        d = json.load(jsondata)\n        for x in d['segGroups']:\n            instance_segids.append(x['segments'])\n            labels.append(x['label'])\n\n    # print len(instance_segids)\n    # print labels\n\n    # Each instance's points\n    instance_points_list = []\n    # instance_labels_list = []\n    semantic_labels_list = []\n    for i in range(len(instance_segids)):\n        segids = instance_segids[i]\n        pointids = []\n        for segid in segids:\n            pointids += segid_to_pointid[segid]\n        instance_points = points[np.array(pointids), :]\n        instance_points_list.append(instance_points)\n        # instance_labels_list.append(np.ones((instance_points.shape[0], 1)) * i)\n        if labels[i] not in RAW2SCANNET:\n            label = 'unannotated'\n        else:\n            label = RAW2SCANNET[labels[i]]\n        label = CLASS_NAMES.index(label)\n        semantic_labels_list.append(np.ones((instance_points.shape[0], 1)) * label)\n\n    # Refactor data format\n    scene_points = np.concatenate(instance_points_list, 0)\n    scene_points = scene_points[:, 0:6]  # XYZRGB, disregarding the A\n    # instance_labels = np.concatenate(instance_labels_list, 0)\n    semantic_labels = np.concatenate(semantic_labels_list, 0)\n    # data = np.concatenate((scene_points, instance_labels, semantic_labels), 1)\n    data = np.concatenate((scene_points, semantic_labels), 1)\n    np.save(out_filename, data)", "\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_path', default='datasets/ScanNet/scans',\n                        help='Directory to dataset')\n    args = parser.parse_args()\n\n    DATA_PATH = args.data_path\n    DST_PATH = os.path.join(ROOT_DIR, 'datasets/ScanNet')\n    SAVE_PATH = os.path.join(DST_PATH, 'scenes', 'data')\n    if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n\n    meta_path = os.path.join(DST_PATH, 'meta')\n    CLASS_NAMES = [x.rstrip() for x in open(os.path.join(meta_path, 'scannet_classnames.txt'))]\n    label_mapping_file = os.path.join(meta_path, 'scannetv2-labels.combined.tsv')\n    RAW2SCANNET = get_raw2scannet_label_map(label_mapping_file)\n\n\n    scene_paths = [os.path.join(DATA_PATH, o) for o in os.listdir(DATA_PATH) if os.path.isdir(os.path.join(DATA_PATH, o))]\n\n    n_scenes = len(scene_paths)\n    if (n_scenes == 0):\n        raise ValueError('%s is empty' % DATA_PATH)\n    else:\n        print('%d scenes to be processed...' % n_scenes)\n\n    for scene_path in scene_paths:\n        scene_name = os.path.basename(scene_path)\n        try:\n            out_filename = scene_name+'.npy' # scene0000_00.npy\n            collect_point_label(scene_path, scene_name, os.path.join(SAVE_PATH, out_filename))\n        except:\n            raise ValueError('ERROR {}!!'.format(scene_path))", ""]}
{"filename": "preprocess/collect_s3dis_data.py", "chunked_list": ["\"\"\" Collect point clouds and the corresponding labels from original S3DID dataset, and save into numpy files.\n\n\n\"\"\"\n\nimport os\nimport glob\nimport numpy as np\nimport sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))", "import sys\nBASE_DIR = os.path.dirname(os.path.abspath(__file__))\nROOT_DIR = os.path.dirname(BASE_DIR)\nsys.path.append(ROOT_DIR)\n\n\ndef collect_point_label(anno_path, out_filename, file_format='numpy'):\n    \"\"\" Convert original dataset files to data_label file (each line is XYZRGBL).\n        We aggregated all the points from each instance in the room.\n    Args:\n        anno_path: path to annotations. e.g. Area_1/office_2/Annotations/\n        out_filename: path to save collected points and labels (each line is XYZRGBL)\n        file_format: txt or numpy, determines what file format to save.\n    Returns:\n        None\n    Note:\n        the points are shifted before save, the most negative point is now at origin.\n    \"\"\"\n    points_list = []\n\n    for f in glob.glob(os.path.join(anno_path, '*.txt')):\n        cls = os.path.basename(f).split('_')[0]\n        if cls not in CLASS_NAMES:  # note: in some room there is 'staris' class..\n            cls = 'clutter'\n        points = np.loadtxt(f)\n        labels = np.ones((points.shape[0], 1)) * CLASS2LABEL[cls]\n        points_list.append(np.concatenate([points, labels], 1))  # Nx7\n\n    data_label = np.concatenate(points_list, 0)\n    # xyz_min = np.amin(data_label, axis=0)[0:3]\n    # data_label[:, 0:3] -= xyz_min\n\n    if file_format == 'txt':\n        fout = open(out_filename, 'w')\n        for i in range(data_label.shape[0]):\n            fout.write('%f %f %f %d %d %d %d\\n' % \\\n                       (data_label[i, 0], data_label[i, 1], data_label[i, 2],\n                        data_label[i, 3], data_label[i, 4], data_label[i, 5],\n                        data_label[i, 6]))\n        fout.close()\n    elif file_format == 'numpy':\n        np.save(out_filename, data_label)\n    else:\n        print('ERROR!! Unknown file format: %s, please use txt or numpy.' % \\\n              (file_format))\n        exit()", "\n\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--data_path', default='datasets/S3DIS/Stanford3dDataset_v1.2_Aligned_Version',\n                        help='Directory to dataset')\n    args = parser.parse_args()\n\n\n    DATA_PATH = args.data_path\n    folders = [\"Area_1\", \"Area_2\", \"Area_3\", \"Area_4\", \"Area_5\", \"Area_6\"]\n    DST_PATH = os.path.join(ROOT_DIR, 'datasets/S3DIS')\n    SAVE_PATH = os.path.join(DST_PATH, 'scenes', 'data')\n    if not os.path.exists(SAVE_PATH): os.makedirs(SAVE_PATH)\n\n    CLASS_NAMES = [x.rstrip() for x in open(os.path.join(ROOT_DIR, 'datasets/S3DIS/meta', 's3dis_classnames.txt'))]\n    CLASS2LABEL = {cls: i for i, cls in enumerate(CLASS_NAMES)}\n\n    for folder in folders:\n        print(\"=================\\n   \" + folder + \"\\n=================\")\n\n        data_folder = os.path.join(DATA_PATH, folder)\n        if not os.path.isdir(data_folder):\n            raise ValueError(\"%s does not exist\" % data_folder)\n\n        # all the scenes in current Area\n        scene_paths = [os.path.join(data_folder, o) for o in os.listdir(data_folder)\n                                                if os.path.isdir(os.path.join(data_folder, o))]\n\n        n_scenes = len(scene_paths)\n        if (n_scenes == 0):\n            raise ValueError('%s is empty' % data_folder)\n        else:\n            print('%d files are under this folder' % n_scenes)\n\n        for scene_path in scene_paths:\n            # Note: there is an extra character in the v1.2 data in Area_5/hallway_6. It's fixed manually.\n            anno_path = os.path.join(scene_path, \"Annotations\")\n            print(anno_path)\n            elements = scene_path.split('/')\n            out_filename = '{}_{}.npy'.format(elements[-2], elements[-1]) # Area_1_hallway_1.npy\n            try:\n                collect_point_label(anno_path, os.path.join(SAVE_PATH, out_filename))\n            except:\n                print(anno_path, 'ERROR!!')", "\n\n"]}
{"filename": "models/protonet_QGPA.py", "chunked_list": ["\"\"\" Prototypical Network \n\n\n\"\"\"\nimport pdb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n", "import torch.nn.functional as F\n\nfrom models.dgcnn import DGCNN\nfrom models.dgcnn_new import DGCNN_semseg\nfrom models.attention import SelfAttention, QGPA\nfrom models.gmmn import GMMNnetwork\n\n\nclass BaseLearner(nn.Module):\n    \"\"\"The class for inner loop.\"\"\"\n    def __init__(self, in_channels, params):\n        super(BaseLearner, self).__init__()\n\n        self.num_convs = len(params)\n        self.convs = nn.ModuleList()\n\n        for i in range(self.num_convs):\n            if i == 0:\n                in_dim = in_channels\n            else:\n                in_dim = params[i-1]\n            self.convs.append(nn.Sequential(\n                              nn.Conv1d(in_dim, params[i], 1),\n                              nn.BatchNorm1d(params[i])))\n\n    def forward(self, x):\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n            if i != self.num_convs-1:\n                x = F.relu(x)\n        return x", "class BaseLearner(nn.Module):\n    \"\"\"The class for inner loop.\"\"\"\n    def __init__(self, in_channels, params):\n        super(BaseLearner, self).__init__()\n\n        self.num_convs = len(params)\n        self.convs = nn.ModuleList()\n\n        for i in range(self.num_convs):\n            if i == 0:\n                in_dim = in_channels\n            else:\n                in_dim = params[i-1]\n            self.convs.append(nn.Sequential(\n                              nn.Conv1d(in_dim, params[i], 1),\n                              nn.BatchNorm1d(params[i])))\n\n    def forward(self, x):\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n            if i != self.num_convs-1:\n                x = F.relu(x)\n        return x", "\n\nclass ProtoNetAlignQGPASR(nn.Module):\n    def __init__(self, args):\n        super(ProtoNetAlignQGPASR, self).__init__()\n        self.n_way = args.n_way\n        self.k_shot = args.k_shot\n        self.dist_method = 'cosine'\n        self.in_channels = args.pc_in_dim\n        self.n_points = args.pc_npts\n        self.use_attention = args.use_attention\n        self.use_align = args.use_align\n        self.use_linear_proj = args.use_linear_proj\n        self.use_supervise_prototype = args.use_supervise_prototype\n        if args.use_high_dgcnn:\n            self.encoder = DGCNN_semseg(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n        else:\n            self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\n        if self.use_attention:\n            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n        else:\n            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\n        if self.use_linear_proj:\n            self.conv_1 = nn.Sequential(nn.Conv1d(args.train_dim, args.train_dim, kernel_size=1, bias=False),\n                                   nn.BatchNorm1d(args.train_dim),\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.use_transformer = args.use_transformer\n        if self.use_transformer:\n            self.transformer = QGPA()\n\n    def forward(self, support_x, support_y, query_x, query_y):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n        Return:\n            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n        \"\"\"\n        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n        support_feat, _ = self.getFeatures(support_x)\n        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points)\n        query_feat, xyz = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n        fg_mask = support_y\n        bg_mask = torch.logical_not(support_y)\n\n        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask)\n        # prototype learning\n        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n        prototypes = [bg_prototype] + fg_prototypes\n\n        self_regulize_loss = 0\n        if self.use_supervise_prototype:\n            self_regulize_loss = self.sup_regulize_Loss(prototypes, support_feat, fg_mask, bg_mask)\n\n        if self.use_transformer:\n            prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0).repeat(query_feat.shape[0], 1, 1)\n            support_feat_ = support_feat.mean(1)\n            prototypes_all_post = self.transformer(query_feat, support_feat_, prototypes_all)\n            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n            query_pred = torch.stack(similarity, dim=1)\n            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n        else:\n            similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n            query_pred = torch.stack(similarity, dim=1)\n            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n        align_loss = 0\n\n        if self.use_align:\n            align_loss_epi = self.alignLoss_trans(query_feat, query_pred, support_feat, fg_mask, bg_mask)\n            align_loss += align_loss_epi\n\n        prototypes_all_post = prototypes_all_post.clone().detach()\n        return query_pred, loss + align_loss + self_regulize_loss, prototypes_all_post\n\n    def forward_test_semantic(self, support_x, support_y, query_x, query_y, embeddings=None):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n        Return:\n            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n        \"\"\"\n\n        query_feat, xyz = self.getFeatures(query_x)\n\n        # prototype learning\n        if self.use_transformer:\n            prototypes_all_post = embeddings\n            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n            query_pred = torch.stack(similarity, dim=1)\n            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\n        return query_pred, loss\n\n    def sup_regulize_Loss(self, prototype_supp, supp_fts, fore_mask, back_mask):\n        \"\"\"\n        Compute the loss for the prototype suppoort self alignment branch\n\n        Args:\n            prototypes: embedding features for query images\n                expect shape: N x C x num_points\n            supp_fts: embedding features for support images\n                expect shape: (Wa x Shot) x C x num_points\n            fore_mask: foreground masks for support images\n                expect shape: (way x shot) x num_points\n            back_mask: background masks for support images\n                expect shape: (way x shot) x num_points\n        \"\"\"\n        n_ways, n_shots = self.n_way, self.k_shot\n\n        # Compute the support loss\n        loss = 0\n        for way in range(n_ways):\n            prototypes = [prototype_supp[0], prototype_supp[way + 1]]\n            for shot in range(n_shots):\n                img_fts = supp_fts[way, shot].unsqueeze(0)\n\n                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes]\n                supp_pred = torch.stack(supp_dist, dim=1)\n                # Construct the support Ground-Truth segmentation\n                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\n                supp_label[fore_mask[way, shot] == 1] = 1\n                supp_label[back_mask[way, shot] == 1] = 0\n                # Compute Loss\n\n                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n        return loss\n\n    def getFeatures(self, x):\n        \"\"\"\n        Forward the input data to network and generate features\n        :param x: input data with shape (B, C_in, L)\n        :return: features with shape (B, C_out, L)\n        \"\"\"\n        if self.use_attention:\n            feat_level1, feat_level2, xyz = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            att_feat = self.att_learner(feat_level2)\n            if self.use_linear_proj:\n                return self.conv_1(torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1)), xyz\n            else:\n                return torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1), xyz\n        else:\n            # return self.base_learner(self.encoder(x))\n            feat_level1, feat_level2 = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            map_feat = self.linear_mapper(feat_level2)\n            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n\n    def getMaskedFeatures(self, feat, mask):\n        \"\"\"\n        Extract foreground and background features via masked average pooling\n\n        Args:\n            feat: input features, shape: (n_way, k_shot, feat_dim, num_points)\n            mask: binary mask, shape: (n_way, k_shot, num_points)\n        Return:\n            masked_feat: masked features, shape: (n_way, k_shot, feat_dim)\n        \"\"\"\n        mask = mask.unsqueeze(2)\n        masked_feat = torch.sum(feat * mask, dim=3) / (mask.sum(dim=3) + 1e-5)\n        return masked_feat\n\n    def getPrototype(self, fg_feat, bg_feat):\n        \"\"\"\n        Average the features to obtain the prototype\n\n        Args:\n            fg_feat: foreground features for each way/shot, shape: (n_way, k_shot, feat_dim)\n            bg_feat: background features for each way/shot, shape: (n_way, k_shot, feat_dim)\n        Returns:\n            fg_prototypes: a list of n_way foreground prototypes, each prototype is a vector with shape (feat_dim,)\n            bg_prototype: background prototype, a vector with shape (feat_dim,)\n        \"\"\"\n        fg_prototypes = [fg_feat[way, ...].sum(dim=0) / self.k_shot for way in range(self.n_way)]\n        bg_prototype = bg_feat.sum(dim=(0,1)) / (self.n_way * self.k_shot)\n        return fg_prototypes, bg_prototype\n\n    def calculateSimilarity(self, feat,  prototype, method='cosine', scaler=10):\n        \"\"\"\n        Calculate the Similarity between query point-level features and prototypes\n\n        Args:\n            feat: input query point-level features\n                  shape: (n_queries, feat_dim, num_points)\n            prototype: prototype of one semantic class\n                       shape: (feat_dim,)\n            method: 'cosine' or 'euclidean', different ways to calculate similarity\n            scaler: used when 'cosine' distance is computed.\n                    By multiplying the factor with cosine distance can achieve comparable performance\n                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n        Return:\n            similarity: similarity between query point to prototype\n                        shape: (n_queries, 1, num_points)\n        \"\"\"\n        if method == 'cosine':\n            similarity = F.cosine_similarity(feat, prototype[None, ..., None], dim=1) * scaler\n        elif method == 'euclidean':\n            similarity = - F.pairwise_distance(feat, prototype[None, ..., None], p=2)**2\n        else:\n            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n        return similarity\n\n    def calculateSimilarity_trans(self, feat,  prototype, method='cosine', scaler=10):\n        \"\"\"\n        Calculate the Similarity between query point-level features and prototypes\n\n        Args:\n            feat: input query point-level features\n                  shape: (n_queries, feat_dim, num_points)\n            prototype: prototype of one semantic class\n                       shape: (feat_dim,)\n            method: 'cosine' or 'euclidean', different ways to calculate similarity\n            scaler: used when 'cosine' distance is computed.\n                    By multiplying the factor with cosine distance can achieve comparable performance\n                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n        Return:\n            similarity: similarity between query point to prototype\n                        shape: (n_queries, 1, num_points)\n        \"\"\"\n        if method == 'cosine':\n            similarity = F.cosine_similarity(feat, prototype[..., None], dim=1) * scaler\n        elif method == 'euclidean':\n            similarity = - F.pairwise_distance(feat, prototype[..., None], p=2)**2\n        else:\n            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n        return similarity\n\n    def computeCrossEntropyLoss(self, query_logits, query_labels):\n        \"\"\" Calculate the CrossEntropy Loss for query set\n        \"\"\"\n        return F.cross_entropy(query_logits, query_labels)\n\n    def alignLoss_trans(self, qry_fts, pred, supp_fts, fore_mask, back_mask):\n        \"\"\"\n        Compute the loss for the prototype alignment branch\n\n        Args:\n            qry_fts: embedding features for query images\n                expect shape: N x C x num_points\n            pred: predicted segmentation score\n                expect shape: N x (1 + Wa) x num_points\n            supp_fts: embedding features for support images\n                expect shape: (Wa x Shot) x C x num_points\n            fore_mask: foreground masks for support images\n                expect shape: (way x shot) x num_points\n            back_mask: background masks for support images\n                expect shape: (way x shot) x num_points\n        \"\"\"\n        n_ways, n_shots = self.n_way, self.k_shot\n\n        # Mask and get query prototype\n        pred_mask = pred.argmax(dim=1, keepdim=True)  # N x 1 x H' x W'\n        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n        pred_mask = torch.stack(binary_masks, dim=1).float()  # N x (1 + Wa) x 1 x H' x W'\n\n        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3)) / (pred_mask.sum(dim=(0, 3)) + 1e-5)\n        # Compute the support loss\n        loss = 0\n        for way in range(n_ways):\n            if way in skip_ways:\n                continue\n            # Get the query prototypes\n            prototypes = [qry_prototypes[0], qry_prototypes[way + 1]]\n            for shot in range(n_shots):\n                img_fts = supp_fts[way, shot].unsqueeze(0)\n                prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0)\n                prototypes_all_post = self.transformer(img_fts, qry_fts.mean(0).unsqueeze(0), prototypes_all)\n                prototypes_new = [prototypes_all_post[0, 0], prototypes_all_post[0, 1]]\n\n                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes_new]\n                supp_pred = torch.stack(supp_dist, dim=1)\n                # Construct the support Ground-Truth segmentation\n                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\n                supp_label[fore_mask[way, shot] == 1] = 1\n                supp_label[back_mask[way, shot] == 1] = 0\n                # Compute Loss\n\n                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n        return loss", ""]}
{"filename": "models/mpti.py", "chunked_list": ["\"\"\" Multi-prototype transductive inference\n\n\n\"\"\"\nimport numpy as np\nimport faiss\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F", "import torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_cluster import fps\n\nfrom models.dgcnn import DGCNN\nfrom models.attention import SelfAttention\n\n\nclass BaseLearner(nn.Module):\n    \"\"\"The class for inner loop.\"\"\"\n    def __init__(self, in_channels, params):\n        super(BaseLearner, self).__init__()\n\n        self.num_convs = len(params)\n        self.convs = nn.ModuleList()\n\n        for i in range(self.num_convs):\n            if i == 0:\n                in_dim = in_channels\n            else:\n                in_dim = params[i-1]\n            self.convs.append(nn.Sequential(\n                              nn.Conv1d(in_dim, params[i], 1),\n                              nn.BatchNorm1d(params[i])))\n\n    def forward(self, x):\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n            if i != self.num_convs-1:\n                x = F.relu(x)\n        return x", "class BaseLearner(nn.Module):\n    \"\"\"The class for inner loop.\"\"\"\n    def __init__(self, in_channels, params):\n        super(BaseLearner, self).__init__()\n\n        self.num_convs = len(params)\n        self.convs = nn.ModuleList()\n\n        for i in range(self.num_convs):\n            if i == 0:\n                in_dim = in_channels\n            else:\n                in_dim = params[i-1]\n            self.convs.append(nn.Sequential(\n                              nn.Conv1d(in_dim, params[i], 1),\n                              nn.BatchNorm1d(params[i])))\n\n    def forward(self, x):\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n            if i != self.num_convs-1:\n                x = F.relu(x)\n        return x", "\n\nclass MultiPrototypeTransductiveInference(nn.Module):\n    def __init__(self, args):\n        super(MultiPrototypeTransductiveInference, self).__init__()\n        # self.gpu_id = args.gpu_id\n        self.n_way = args.n_way\n        self.k_shot = args.k_shot\n        self.in_channels = args.pc_in_dim\n        self.n_points = args.pc_npts\n        self.use_attention = args.use_attention\n        self.n_subprototypes = args.n_subprototypes\n        self.k_connect = args.k_connect\n        self.sigma = args.sigma\n\n        self.n_classes = self.n_way+1\n\n        self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k)\n        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\n        if self.use_attention:\n            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n        else:\n            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\n        self.feat_dim = args.edgeconv_widths[0][-1] + args.output_dim + args.base_widths[-1]\n\n    def forward(self, support_x, support_y, query_x, query_y):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        Return:\n            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n        \"\"\"\n        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n        support_feat = self.getFeatures(support_x)\n        support_feat = support_feat.view(self.n_way, self.k_shot, self.feat_dim, self.n_points)\n        query_feat = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n        query_feat = query_feat.transpose(1,2).contiguous().view(-1, self.feat_dim) #(n_queries*num_points, feat_dim)\n\n        fg_mask = support_y\n        bg_mask = torch.logical_not(support_y)\n\n        fg_prototypes, fg_labels = self.getForegroundPrototypes(support_feat, fg_mask, k=self.n_subprototypes)\n        bg_prototype, bg_labels = self.getBackgroundPrototypes(support_feat, bg_mask, k=self.n_subprototypes)\n\n        # prototype learning\n        if bg_prototype is not None and bg_labels is not None:\n            prototypes = torch.cat((bg_prototype, fg_prototypes), dim=0) #(*, feat_dim)\n            prototype_labels = torch.cat((bg_labels, fg_labels), dim=0) #(*,n_classes)\n        else:\n            prototypes = fg_prototypes\n            prototype_labels = fg_labels\n        self.num_prototypes = prototypes.shape[0]\n\n        # construct label matrix Y, with Y_ij = 1 if x_i is from the support set and labeled as y_i = j, otherwise Y_ij = 0.\n        self.num_nodes = self.num_prototypes + query_feat.shape[0] # number of node of partial observed graph\n        Y = torch.zeros(self.num_nodes, self.n_classes).cuda()\n        Y[:self.num_prototypes] = prototype_labels\n\n        # construct feat matrix F\n        node_feat = torch.cat((prototypes, query_feat), dim=0) #(num_nodes, feat_dim)\n\n        # label propagation\n        A = self.calculateLocalConstrainedAffinity(node_feat, k=self.k_connect)\n        Z = self.label_propagate(A, Y) #(num_nodes, n_way+1)\n\n        query_pred = Z[self.num_prototypes:, :] #(n_queries*num_points, n_way+1)\n        query_pred = query_pred.view(-1, query_y.shape[1], self.n_classes).transpose(1,2) #(n_queries, n_way+1, num_points)\n        loss = self.computeCrossEntropyLoss(query_pred, query_y)\n        return query_pred, loss\n\n    def getFeatures(self, x):\n        \"\"\"\n        Forward the input data to network and generate features\n        :param x: input data with shape (B, C_in, L)\n        :return: features with shape (B, C_out, L)\n        \"\"\"\n        if self.use_attention:\n            feat_level1, feat_level2 = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            att_feat = self.att_learner(feat_level2)\n            return torch.cat((feat_level1, att_feat, feat_level3), dim=1)\n        else:\n            # return self.base_learner(self.encoder(x))\n            feat_level1, feat_level2 = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            map_feat = self.linear_mapper(feat_level2)\n            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n\n    def getMutiplePrototypes(self, feat, k):\n        \"\"\"\n        Extract multiple prototypes by points separation and assembly\n\n        Args:\n            feat: input point features, shape:(n_points, feat_dim)\n        Return:\n            prototypes: output prototypes, shape: (n_prototypes, feat_dim)\n        \"\"\"\n        # sample k seeds as initial centers with Farthest Point Sampling (FPS)\n        n = feat.shape[0]\n        assert n > 0\n        ratio = k / n\n        if ratio < 1:\n            fps_index = fps(feat, None, ratio=ratio, random_start=False).unique()\n            num_prototypes = len(fps_index)\n            farthest_seeds = feat[fps_index]\n\n            # compute the point-to-seed distance\n            distances = F.pairwise_distance(feat[..., None], farthest_seeds.transpose(0, 1)[None, ...],\n                                            p=2)  # (n_points, n_prototypes)\n\n            # hard assignment for each point\n            assignments = torch.argmin(distances, dim=1)  # (n_points,)\n\n            # aggregating each cluster to form prototype\n            prototypes = torch.zeros((num_prototypes, self.feat_dim)).cuda()\n            for i in range(num_prototypes):\n                selected = torch.nonzero(assignments == i).squeeze(1)\n                selected = feat[selected, :]\n                prototypes[i] = selected.mean(0)\n            return prototypes\n        else:\n            return feat\n\n    def getForegroundPrototypes(self, feats, masks, k=100):\n        \"\"\"\n        Extract foreground prototypes for each class via clustering point features within that class\n\n        Args:\n            feats: input support features, shape: (n_way, k_shot, feat_dim, num_points)\n            masks: foreground binary masks, shape: (n_way, k_shot, num_points)\n        Return:\n            prototypes: foreground prototypes, shape: (n_way*k, feat_dim)\n            labels: foreground prototype labels (one-hot), shape: (n_way*k, n_way+1)\n        \"\"\"\n        prototypes = []\n        labels = []\n        for i in range(self.n_way):\n            # extract point features belonging to current foreground class\n            feat = feats[i, ...].transpose(1,2).contiguous().view(-1, self.feat_dim) #(k_shot*num_points, feat_dim)\n            index = torch.nonzero(masks[i, ...].view(-1)).squeeze(1) #(k_shot*num_points,)\n            feat = feat[index]\n            class_prototypes = self.getMutiplePrototypes(feat, k)\n            prototypes.append(class_prototypes)\n\n            # construct label matrix\n            class_labels = torch.zeros(class_prototypes.shape[0], self.n_classes)\n            class_labels[:, i+1] = 1\n            labels.append(class_labels)\n\n        prototypes = torch.cat(prototypes, dim=0)\n        labels = torch.cat(labels, dim=0)\n\n        return prototypes, labels\n\n    def getBackgroundPrototypes(self, feats, masks, k=100):\n        \"\"\"\n        Extract background prototypes via clustering point features within background class\n\n        Args:\n            feats: input support features, shape: (n_way, k_shot, feat_dim, num_points)\n            masks: background binary masks, shape: (n_way, k_shot, num_points)\n        Return:\n            prototypes: background prototypes, shape: (k, feat_dim)\n            labels: background prototype labels (one-hot), shape: (k, n_way+1)\n        \"\"\"\n        feats = feats.transpose(2,3).contiguous().view(-1, self.feat_dim)\n        index = torch.nonzero(masks.view(-1)).squeeze(1)\n        feat = feats[index]\n        # in case this support set does not contain background points..\n        if feat.shape[0] != 0:\n            prototypes = self.getMutiplePrototypes(feat, k)\n\n            labels = torch.zeros(prototypes.shape[0], self.n_classes)\n            labels[:, 0] = 1\n\n            return prototypes, labels\n        else:\n            return None, None\n\n    def calculateLocalConstrainedAffinity(self, node_feat, k=200, method='gaussian'):\n        \"\"\"\n        Calculate the Affinity matrix of the nearest neighbor graph constructed by prototypes and query points,\n        It is a efficient way when the number of nodes in the graph is too large.\n\n        Args:\n            node_feat: input node features\n                  shape: (num_nodes, feat_dim)\n            k: the number of nearest neighbors for each node to compute the similarity\n            method: 'cosine' or 'gaussian', different similarity function\n        Return:\n            A: Affinity matrix with zero diagonal, shape: (num_nodes, num_nodes)\n        \"\"\"\n        # kNN search for the graph\n        X = node_feat.detach().cpu().numpy()\n        # build the index with cpu version\n        index = faiss.IndexFlatL2(self.feat_dim)\n        index.add(X)\n        _, I = index.search(X, k + 1)\n        I = torch.from_numpy(I[:, 1:]).cuda() #(num_nodes, k)\n\n        # create the affinity matrix\n        knn_idx = I.unsqueeze(2).expand(-1, -1, self.feat_dim).contiguous().view(-1, self.feat_dim)\n        knn_feat = torch.gather(node_feat, dim=0, index=knn_idx).contiguous().view(self.num_nodes, k, self.feat_dim)\n\n        if method == 'cosine':\n            knn_similarity = F.cosine_similarity(node_feat[:,None,:], knn_feat, dim=2)\n        elif method == 'gaussian':\n            dist = F.pairwise_distance(node_feat[:,:,None], knn_feat.transpose(1,2), p=2)\n            knn_similarity = torch.exp(-0.5*(dist/self.sigma)**2)\n        else:\n            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n\n        A = torch.zeros(self.num_nodes, self.num_nodes, dtype=torch.float).cuda()\n        A = A.scatter_(1, I, knn_similarity)\n        A = A + A.transpose(0,1)\n\n        identity_matrix = torch.eye(self.num_nodes, requires_grad=False).cuda()\n        A = A * (1 - identity_matrix)\n        return A\n\n\n    def label_propagate(self, A, Y, alpha=0.99):\n        \"\"\" Label Propagation, refer to \"Learning with Local and Global Consistency\" NeurIPs 2003\n        Args:\n            A: Affinity matrix with zero diagonal, shape: (num_nodes, num_nodes)\n            Y: initial label matrix, shape: (num_nodes, n_way+1)\n            alpha: a parameter to control the amount of propagated info.\n        Return:\n            Z: label predictions, shape: (num_nodes, n_way+1)\n        \"\"\"\n        #compute symmetrically normalized matrix S\n        eps = np.finfo(float).eps\n        D = A.sum(1) #(num_nodes,)\n        D_sqrt_inv = torch.sqrt(1.0/(D+eps))\n        D_sqrt_inv = torch.diag_embed(D_sqrt_inv).cuda()\n        S = D_sqrt_inv @ A @ D_sqrt_inv\n\n        #close form solution\n        Z = torch.inverse(torch.eye(self.num_nodes).cuda() - alpha*S + eps) @ Y\n        return Z\n\n    def computeCrossEntropyLoss(self, query_logits, query_labels):\n        \"\"\" Calculate the CrossEntropy Loss for query set\n        \"\"\"\n        return F.cross_entropy(query_logits, query_labels)", ""]}
{"filename": "models/protonet_FZ.py", "chunked_list": ["\"\"\" Prototypical Network \n\n\n\"\"\"\nimport pdb\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n", "import torch.nn.functional as F\n\nfrom models.dgcnn import DGCNN\nfrom models.dgcnn_new import DGCNN_semseg\nfrom models.attention import SelfAttention, QGPA\nfrom models.gmmn import GMMNnetwork\n\n\nclass BaseLearner(nn.Module):\n    \"\"\"The class for inner loop.\"\"\"\n    def __init__(self, in_channels, params):\n        super(BaseLearner, self).__init__()\n\n        self.num_convs = len(params)\n        self.convs = nn.ModuleList()\n\n        for i in range(self.num_convs):\n            if i == 0:\n                in_dim = in_channels\n            else:\n                in_dim = params[i-1]\n            self.convs.append(nn.Sequential(\n                              nn.Conv1d(in_dim, params[i], 1),\n                              nn.BatchNorm1d(params[i])))\n\n    def forward(self, x):\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n            if i != self.num_convs-1:\n                x = F.relu(x)\n        return x", "class BaseLearner(nn.Module):\n    \"\"\"The class for inner loop.\"\"\"\n    def __init__(self, in_channels, params):\n        super(BaseLearner, self).__init__()\n\n        self.num_convs = len(params)\n        self.convs = nn.ModuleList()\n\n        for i in range(self.num_convs):\n            if i == 0:\n                in_dim = in_channels\n            else:\n                in_dim = params[i-1]\n            self.convs.append(nn.Sequential(\n                              nn.Conv1d(in_dim, params[i], 1),\n                              nn.BatchNorm1d(params[i])))\n\n    def forward(self, x):\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n            if i != self.num_convs-1:\n                x = F.relu(x)\n        return x", "\n\nclass ProtoNetAlignFZ(nn.Module):\n    def __init__(self, args):\n        super(ProtoNetAlignFZ, self).__init__()\n        self.n_way = args.n_way\n        self.k_shot = args.k_shot\n        self.dist_method = 'cosine'\n        self.in_channels = args.pc_in_dim\n        self.n_points = args.pc_npts\n        self.use_attention = args.use_attention\n        self.use_align = args.use_align\n        self.use_linear_proj = args.use_linear_proj\n        self.use_supervise_prototype = args.use_supervise_prototype\n        if args.use_high_dgcnn:\n            self.encoder = DGCNN_semseg(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n        else:\n            self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k, return_edgeconvs=True)\n        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\n        if self.use_attention:\n            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n        else:\n            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\n        if self.use_linear_proj:\n            self.conv_1 = nn.Sequential(nn.Conv1d(args.train_dim, args.train_dim, kernel_size=1, bias=False),\n                                   nn.BatchNorm1d(args.train_dim),\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.use_transformer = args.use_transformer\n        if self.use_transformer:\n            self.transformer = QGPA()\n        if args.use_zero:\n            self.generator = GMMNnetwork(args.noise_dim, args.noise_dim, args.train_dim, args.train_dim, args.gmm_dropout)\n\n    def forward(self, support_x, support_y, query_x, query_y):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n        Return:\n            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n        \"\"\"\n        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n        support_feat, _ = self.getFeatures(support_x)\n        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points)\n        query_feat, xyz = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n        fg_mask = support_y\n        bg_mask = torch.logical_not(support_y)\n\n        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask)\n        # prototype learning\n        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n        prototypes = [bg_prototype] + fg_prototypes\n\n        self_regulize_loss = 0\n        if self.use_supervise_prototype:\n            self_regulize_loss = self.sup_regulize_Loss(prototypes, support_feat, fg_mask, bg_mask)\n\n        if self.use_transformer:\n            prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0).repeat(query_feat.shape[0], 1, 1)\n            prototypes_all_post = self.transformer(query_feat, query_feat, prototypes_all)\n            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n            query_pred = torch.stack(similarity, dim=1)\n            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n        else:\n            similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n            query_pred = torch.stack(similarity, dim=1)\n            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n        align_loss = 0\n\n        if self.use_align:\n            align_loss_epi = self.alignLoss_trans(query_feat, query_pred, support_feat, fg_mask, bg_mask)\n            align_loss += align_loss_epi\n\n        prototypes_all = prototypes_all.clone().detach()\n        return query_pred, loss + align_loss + self_regulize_loss, prototypes_all\n\n    def forward_test_semantic(self, support_x, support_y, query_x, query_y, embeddings=None):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points) [2, 9, 2048]\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points) [2, 1, 2048]\n            query_x: query point clouds with shape (n_queries, in_channels, num_points) [2, 9, 2048]\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way} [2, 2048]\n        Return:\n            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n        \"\"\"\n\n        query_feat, xyz = self.getFeatures(query_x) #(n_queries, feat_dim, num_points)\n        # prototype learning\n        if self.use_transformer:\n            prototypes_all = embeddings\n            prototypes_all_post = self.transformer(query_feat, query_feat, prototypes_all)\n            prototypes_new = torch.chunk(prototypes_all_post, prototypes_all_post.shape[1], dim=1)\n            similarity = [self.calculateSimilarity_trans(query_feat, prototype.squeeze(1), self.dist_method) for prototype in prototypes_new]\n            query_pred = torch.stack(similarity, dim=1)\n            loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\n        return query_pred, loss\n\n    def sup_regulize_Loss(self, prototype_supp, supp_fts, fore_mask, back_mask):\n        \"\"\"\n        Compute the loss for the prototype suppoort self alignment branch\n\n        Args:\n            prototypes: embedding features for query images\n                expect shape: N x C x num_points\n            supp_fts: embedding features for support images\n                expect shape: (Wa x Shot) x C x num_points\n            fore_mask: foreground masks for support images\n                expect shape: (way x shot) x num_points\n            back_mask: background masks for support images\n                expect shape: (way x shot) x num_points\n        \"\"\"\n        n_ways, n_shots = self.n_way, self.k_shot\n\n        # Compute the support loss\n        loss = 0\n        for way in range(n_ways):\n            prototypes = [prototype_supp[0], prototype_supp[way + 1]]\n            for shot in range(n_shots):\n                img_fts = supp_fts[way, shot].unsqueeze(0)\n\n                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes]\n                supp_pred = torch.stack(supp_dist, dim=1)\n                # Construct the support Ground-Truth segmentation\n                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\n                supp_label[fore_mask[way, shot] == 1] = 1\n                supp_label[back_mask[way, shot] == 1] = 0\n                # Compute Loss\n\n                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n        return loss\n\n    def getFeatures(self, x):\n        \"\"\"\n        Forward the input data to network and generate features\n        :param x: input data with shape (B, C_in, L)\n        :return: features with shape (B, C_out, L)\n        \"\"\"\n        if self.use_attention:\n            feat_level1, feat_level2, xyz = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            att_feat = self.att_learner(feat_level2)\n            if self.use_linear_proj:\n                return self.conv_1(torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1)), xyz\n            else:\n                return torch.cat((feat_level1[0], feat_level1[1], feat_level1[2], att_feat, feat_level3), dim=1), xyz\n        else:\n            # return self.base_learner(self.encoder(x))\n            feat_level1, feat_level2 = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            map_feat = self.linear_mapper(feat_level2)\n            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n\n    def getMaskedFeatures(self, feat, mask):\n        \"\"\"\n        Extract foreground and background features via masked average pooling\n\n        Args:\n            feat: input features, shape: (n_way, k_shot, feat_dim, num_points)\n            mask: binary mask, shape: (n_way, k_shot, num_points)\n        Return:\n            masked_feat: masked features, shape: (n_way, k_shot, feat_dim)\n        \"\"\"\n        mask = mask.unsqueeze(2)\n        masked_feat = torch.sum(feat * mask, dim=3) / (mask.sum(dim=3) + 1e-5)\n        return masked_feat\n\n    def getPrototype(self, fg_feat, bg_feat):\n        \"\"\"\n        Average the features to obtain the prototype\n\n        Args:\n            fg_feat: foreground features for each way/shot, shape: (n_way, k_shot, feat_dim)\n            bg_feat: background features for each way/shot, shape: (n_way, k_shot, feat_dim)\n        Returns:\n            fg_prototypes: a list of n_way foreground prototypes, each prototype is a vector with shape (feat_dim,)\n            bg_prototype: background prototype, a vector with shape (feat_dim,)\n        \"\"\"\n        fg_prototypes = [fg_feat[way, ...].sum(dim=0) / self.k_shot for way in range(self.n_way)]\n        bg_prototype = bg_feat.sum(dim=(0,1)) / (self.n_way * self.k_shot)\n        return fg_prototypes, bg_prototype\n\n    def calculateSimilarity(self, feat,  prototype, method='cosine', scaler=10):\n        \"\"\"\n        Calculate the Similarity between query point-level features and prototypes\n\n        Args:\n            feat: input query point-level features\n                  shape: (n_queries, feat_dim, num_points)\n            prototype: prototype of one semantic class\n                       shape: (feat_dim,)\n            method: 'cosine' or 'euclidean', different ways to calculate similarity\n            scaler: used when 'cosine' distance is computed.\n                    By multiplying the factor with cosine distance can achieve comparable performance\n                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n        Return:\n            similarity: similarity between query point to prototype\n                        shape: (n_queries, 1, num_points)\n        \"\"\"\n        if method == 'cosine':\n            similarity = F.cosine_similarity(feat, prototype[None, ..., None], dim=1) * scaler\n        elif method == 'euclidean':\n            similarity = - F.pairwise_distance(feat, prototype[None, ..., None], p=2)**2\n        else:\n            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n        return similarity\n\n    def calculateSimilarity_trans(self, feat,  prototype, method='cosine', scaler=10):\n        \"\"\"\n        Calculate the Similarity between query point-level features and prototypes\n\n        Args:\n            feat: input query point-level features\n                  shape: (n_queries, feat_dim, num_points)\n            prototype: prototype of one semantic class\n                       shape: (feat_dim,)\n            method: 'cosine' or 'euclidean', different ways to calculate similarity\n            scaler: used when 'cosine' distance is computed.\n                    By multiplying the factor with cosine distance can achieve comparable performance\n                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n        Return:\n            similarity: similarity between query point to prototype\n                        shape: (n_queries, 1, num_points)\n        \"\"\"\n        if method == 'cosine':\n            similarity = F.cosine_similarity(feat, prototype[..., None], dim=1) * scaler\n        elif method == 'euclidean':\n            similarity = - F.pairwise_distance(feat, prototype[..., None], p=2)**2\n        else:\n            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n        return similarity\n\n    def computeCrossEntropyLoss(self, query_logits, query_labels):\n        \"\"\" Calculate the CrossEntropy Loss for query set\n        \"\"\"\n        return F.cross_entropy(query_logits, query_labels)\n\n    def alignLoss_trans(self, qry_fts, pred, supp_fts, fore_mask, back_mask):\n        \"\"\"\n        Compute the loss for the prototype alignment branch\n\n        Args:\n            qry_fts: embedding features for query images\n                expect shape: N x C x num_points\n            pred: predicted segmentation score\n                expect shape: N x (1 + Wa) x num_points\n            supp_fts: embedding features for support images\n                expect shape: (Wa x Shot) x C x num_points\n            fore_mask: foreground masks for support images\n                expect shape: (way x shot) x num_points\n            back_mask: background masks for support images\n                expect shape: (way x shot) x num_points\n        \"\"\"\n        n_ways, n_shots = self.n_way, self.k_shot\n\n        # Mask and get query prototype\n        pred_mask = pred.argmax(dim=1, keepdim=True)  # N x 1 x H' x W'\n        binary_masks = [pred_mask == i for i in range(1 + n_ways)]\n        skip_ways = [i for i in range(n_ways) if binary_masks[i + 1].sum() == 0]\n        pred_mask = torch.stack(binary_masks, dim=1).float()  # N x (1 + Wa) x 1 x H' x W'\n\n        qry_prototypes = torch.sum(qry_fts.unsqueeze(1) * pred_mask, dim=(0, 3)) / (pred_mask.sum(dim=(0, 3)) + 1e-5)\n        # Compute the support loss\n        loss = 0\n        for way in range(n_ways):\n            if way in skip_ways:\n                continue\n            # Get the query prototypes\n            prototypes = [qry_prototypes[0], qry_prototypes[way + 1]]\n            for shot in range(n_shots):\n                img_fts = supp_fts[way, shot].unsqueeze(0)\n                prototypes_all = torch.stack(prototypes, dim=0).unsqueeze(0)\n                prototypes_all_post = self.transformer(img_fts, img_fts, prototypes_all)\n                # prototypes_all_post = self.transformer(img_fts, qry_fts[0:1], prototypes_all)\n                prototypes_new = [prototypes_all_post[0, 0], prototypes_all_post[0, 1]]\n\n                supp_dist = [self.calculateSimilarity(img_fts, prototype, self.dist_method) for prototype in prototypes_new]\n                supp_pred = torch.stack(supp_dist, dim=1)\n                # Construct the support Ground-Truth segmentation\n                supp_label = torch.full_like(fore_mask[way, shot], 255, device=img_fts.device).long()\n\n                supp_label[fore_mask[way, shot] == 1] = 1\n                supp_label[back_mask[way, shot] == 1] = 0\n                # Compute Loss\n\n                loss = loss + F.cross_entropy(supp_pred, supp_label.unsqueeze(0), ignore_index=255) / n_shots / n_ways\n        return loss", ""]}
{"filename": "models/mpti_learner.py", "chunked_list": ["\"\"\" MPTI with/without attention Learner for Few-shot 3D Point Cloud Semantic Segmentation\n\n\n\"\"\"\nimport os\nimport torch\nfrom torch import optim\nfrom torch.nn import functional as F\n\nfrom models.mpti import MultiPrototypeTransductiveInference", "\nfrom models.mpti import MultiPrototypeTransductiveInference\nfrom utils.checkpoint_util import load_pretrain_checkpoint, load_model_checkpoint\n\n\nclass MPTILearner(object):\n    def __init__(self, args, mode='train'):\n\n        # init model and optimizer\n        self.model = MultiPrototypeTransductiveInference(args)\n        print(self.model)\n        if torch.cuda.is_available():\n            self.model.cuda()\n\n        if mode=='train':\n            if args.use_attention:\n                self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.att_learner.parameters()}], lr=args.lr)\n            else:\n                self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.linear_mapper.parameters()}], lr=args.lr)\n            #set learning rate scheduler\n            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.step_size,\n                                                          gamma=args.gamma)\n            if args.model_checkpoint_path is None:\n                # load pretrained model for point cloud encoding\n                self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n            else:\n                # resume from model checkpoint\n                self.model, self.optimizer = load_model_checkpoint(self.model, args.model_checkpoint_path,\n                                                                   optimizer=self.optimizer, mode='train')\n        elif mode=='test':\n            # Load model checkpoint\n            self.model = load_model_checkpoint(self.model, args.model_checkpoint_path, mode='test')\n        else:\n            raise ValueError('Wrong GraphLearner mode (%s)! Option:train/test' %mode)\n\n    def train(self, data):\n        \"\"\"\n        Args:\n            data: a list of torch tensors wit the following entries.\n            - support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            - support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n            - query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            - query_y: query labels with shape (n_queries, num_points)\n        \"\"\"\n\n        [support_x, support_y, query_x, query_y] = data\n        self.model.train()\n\n        query_logits, loss= self.model(support_x, support_y, query_x, query_y)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.lr_scheduler.step()\n\n        query_pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n        correct = torch.eq(query_pred, query_y).sum().item()  # including background class\n        accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\n        return loss, accuracy\n\n\n    def test(self, data):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points), each point \\in {0,1}.\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        \"\"\"\n        [support_x, support_y, query_x, query_y] = data\n        self.model.eval()\n\n        with torch.no_grad():\n            logits, loss= self.model(support_x, support_y, query_x, query_y)\n            pred = F.softmax(logits, dim=1).argmax(dim=1)\n            correct = torch.eq(pred, query_y).sum().item()\n            accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\n        return pred, loss, accuracy", ""]}
{"filename": "models/proto_learner_FZ.py", "chunked_list": ["\"\"\" ProtoNet with/without attention learner for Few-shot 3D Point Cloud Semantic Segmentation\n\n\n\"\"\"\nimport torch\nfrom torch import optim\nfrom torch.nn import functional as F\n\nfrom models.protonet_FZ import ProtoNetAlignFZ\nfrom utils.checkpoint_util import load_pretrain_checkpoint, load_model_checkpoint", "from models.protonet_FZ import ProtoNetAlignFZ\nfrom utils.checkpoint_util import load_pretrain_checkpoint, load_model_checkpoint\nfrom models.gmmn import GMMNLoss\nimport numpy as np\n\n\nclass ProtoLearnerFZ(object):\n\n    def __init__(self, args, mode='train'):\n\n        self.model = ProtoNetAlignFZ(args)\n\n        print(self.model)\n        if torch.cuda.is_available():\n            self.model.cuda()\n\n        if mode == 'train':\n            if args.use_attention:\n                if args.use_transformer:\n                    self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.transformer.parameters(), 'lr': args.trans_lr},\n                     {'params': self.model.att_learner.parameters()}\n                     ], lr=args.lr)\n                else:\n                    self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.att_learner.parameters()}\n                     ], lr=args.lr)\n            else:\n                self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.linear_mapper.parameters()}], lr=args.lr)\n            #set learning rate scheduler\n            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.step_size,\n                                                          gamma=args.gamma)\n            # load pretrained model for point cloud encoding\n            self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n        elif mode == 'test':\n            # Load model checkpoint\n            self.model = load_model_checkpoint(self.model, args.model_checkpoint_path, mode='test')\n        else:\n            raise ValueError('Wrong GMMLearner mode (%s)! Option:train/test' %mode)\n        self.optimizer_generator = torch.optim.Adam(self.model.generator.parameters(), lr=args.generator_lr)\n        self.criterion_generator = GMMNLoss(\n            sigma=[2, 5, 10, 20, 40, 80], cuda=True).build_loss()\n        self.noise_dim = args.noise_dim\n        self.gmm_weight = args.gmm_weight\n        self.n_way = args.n_way\n        vec_name = 'glove'\n        if args.dataset == 's3dis':\n            self.bg_id = 12\n            self.embeddings = torch.from_numpy(np.load('dataloaders/S3DIS_{}.npy'.format(vec_name))) # S3DIS_fasttext.npy S3DIS_word2vec_google.npy\n        elif args.dataset == 'scannet':\n            self.bg_id = 0\n            self.embeddings = torch.from_numpy(np.load('dataloaders/ScanNet_{}.npy'.format(vec_name))) # S3DIS_fasttext.npy S3DIS_word2vec_google.npy\n        self.embeddings = torch.nn.functional.normalize(self.embeddings, p=2, dim=1)\n\n    def train(self, data, sampled_classes):\n        \"\"\"\n        Args:\n            data: a list of torch tensors wit the following entries.\n            - support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            - support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n            - query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            - query_y: query labels with shape (n_queries, num_points)\n        \"\"\"\n\n        [support_x, support_y, query_x, query_y] = data\n        self.model.train()\n\n        query_logits, loss, prototype_gts = self.model(support_x, support_y, query_x, query_y)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.lr_scheduler.step()\n\n        query_pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n        correct = torch.eq(query_pred, query_y).sum().item()  # including background class\n        accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\n        self.optimizer_generator.zero_grad()\n\n        support_embeddings = torch.cat([self.embeddings[self.bg_id].unsqueeze(0), self.embeddings[sampled_classes]], dim=0).to(query_pred.device)\n        prototype_fakes = []\n\n        for i in range(self.n_way):\n            z_g = torch.rand((support_embeddings.shape[0], self.noise_dim)).cuda()\n            prototype_fakes.append(self.model.generator(support_embeddings, z_g.float()))\n        prototype_fakes = torch.stack(prototype_fakes, dim=0)\n        g_loss_bg = self.criterion_generator(prototype_fakes[:, :1, :].flatten(0, 1), prototype_gts[:, :1, :].flatten(0, 1))\n        g_loss_fg = self.criterion_generator(prototype_fakes[:, 1:, :].flatten(0, 1), prototype_gts[:, 1:, :].flatten(0, 1))\n        g_loss = g_loss_bg * self.gmm_weight + g_loss_fg\n        g_loss.backward()\n        self.optimizer_generator.step()\n\n        return loss, accuracy\n\n    def test_semantic(self, data, sampled_classes):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points), each point \\in {0,1}.\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        \"\"\"\n        [_, _, query_x, query_y] = data\n        self.model.eval()\n\n        support_embeddings = torch.cat([self.embeddings[self.bg_id].unsqueeze(0), self.embeddings[sampled_classes]], dim=0).cuda()\n\n        with torch.no_grad():\n            prototype_fakes = []\n            for i in range(self.n_way):\n                z_g = torch.rand((support_embeddings.shape[0], self.noise_dim)).cuda()\n                prototype_fakes.append(self.model.generator(support_embeddings, z_g.float()))\n\n            prototype_fakes = torch.stack(prototype_fakes, dim=0)\n\n            logits, loss = self.model.forward_test_semantic(_, _, query_x, query_y, prototype_fakes)\n            pred = F.softmax(logits, dim=1).argmax(dim=1)\n            correct = torch.eq(pred, query_y).sum().item()\n            accuracy = correct / (query_y.shape[0] * query_y.shape[1])\n\n        return pred, loss, accuracy"]}
{"filename": "models/protonet.py", "chunked_list": ["\"\"\" Prototypical Network \n\n\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom models.dgcnn import DGCNN\nfrom models.attention import SelfAttention", "from models.dgcnn import DGCNN\nfrom models.attention import SelfAttention\n\n\nclass BaseLearner(nn.Module):\n    \"\"\"The class for inner loop.\"\"\"\n    def __init__(self, in_channels, params):\n        super(BaseLearner, self).__init__()\n\n        self.num_convs = len(params)\n        self.convs = nn.ModuleList()\n\n        for i in range(self.num_convs):\n            if i == 0:\n                in_dim = in_channels\n            else:\n                in_dim = params[i-1]\n            self.convs.append(nn.Sequential(\n                              nn.Conv1d(in_dim, params[i], 1),\n                              nn.BatchNorm1d(params[i])))\n\n    def forward(self, x):\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n            if i != self.num_convs-1:\n                x = F.relu(x)\n        return x", "\n\nclass ProtoNet(nn.Module):\n    def __init__(self, args):\n        super(ProtoNet, self).__init__()\n        self.n_way = args.n_way\n        self.k_shot = args.k_shot\n        self.dist_method = 'cosine'\n        self.in_channels = args.pc_in_dim\n        self.n_points = args.pc_npts\n        self.use_attention = args.use_attention\n\n        self.encoder = DGCNN(args.edgeconv_widths, args.dgcnn_mlp_widths, args.pc_in_dim, k=args.dgcnn_k)\n        self.base_learner = BaseLearner(args.dgcnn_mlp_widths[-1], args.base_widths)\n\n        if self.use_attention:\n            self.att_learner = SelfAttention(args.dgcnn_mlp_widths[-1], args.output_dim)\n        else:\n            self.linear_mapper = nn.Conv1d(args.dgcnn_mlp_widths[-1], args.output_dim, 1, bias=False)\n\n    def forward(self, support_x, support_y, query_x, query_y):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        Return:\n            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n        \"\"\"\n\n        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n        support_feat = self.getFeatures(support_x)\n        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points) # [2, 1, 192, 2048]\n        query_feat = self.getFeatures(query_x) #(n_queries, feat_dim, num_points) # [2, 192, 2048]\n\n        fg_mask = support_y # [2, 1, 2048]\n        bg_mask = torch.logical_not(support_y)\n\n        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask) # [2, 1, 192]\n\n        # prototype learning\n        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n        prototypes = [bg_prototype] + fg_prototypes\n\n        # non-parametric metric learning\n        similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n\n        query_pred = torch.stack(similarity, dim=1) #(n_queries, n_way+1, num_points)\n\n        loss = self.computeCrossEntropyLoss(query_pred, query_y)\n\n        return query_pred, loss\n\n    def forward_dummy(self, support_x):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        Return:\n            query_pred: query point clouds predicted similarity, shape: (n_queries, n_way+1, num_points)\n        \"\"\"\n        support_x = support_x.squeeze(0)\n        query_x = support_x\n        support_y = torch.randn(support_x.shape[0], 1, support_x.shape[2]).cuda()\n        query_y = torch.randn(support_x.shape[0], support_x.shape[2]).cuda()\n        support_x = support_x.view(self.n_way*self.k_shot, self.in_channels, self.n_points)\n        support_feat = self.getFeatures(support_x)\n        support_feat = support_feat.view(self.n_way, self.k_shot, -1, self.n_points) # [2, 1, 192, 2048]\n        query_feat = self.getFeatures(query_x) #(n_queries, feat_dim, num_points) # [2, 192, 2048]\n\n        fg_mask = support_y # [2, 1, 2048]\n        bg_mask = torch.logical_not(support_y)\n\n        support_fg_feat = self.getMaskedFeatures(support_feat, fg_mask)\n        suppoer_bg_feat = self.getMaskedFeatures(support_feat, bg_mask) # [2, 1, 192]\n\n        # prototype learning\n        fg_prototypes, bg_prototype = self.getPrototype(support_fg_feat, suppoer_bg_feat)\n        prototypes = [bg_prototype] + fg_prototypes\n\n        # non-parametric metric learning\n        similarity = [self.calculateSimilarity(query_feat, prototype, self.dist_method) for prototype in prototypes]\n\n        query_pred = torch.stack(similarity, dim=1) #(n_queries, n_way+1, num_points)\n\n\n        return query_pred\n\n    def getFeatures(self, x):\n        \"\"\"\n        Forward the input data to network and generate features\n        :param x: input data with shape (B, C_in, L)\n        :return: features with shape (B, C_out, L)\n        \"\"\"\n        if self.use_attention:\n            feat_level1, feat_level2 = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            att_feat = self.att_learner(feat_level2)\n            return torch.cat((feat_level1, att_feat, feat_level3), dim=1)\n        else:\n            # return self.base_learner(self.encoder(x))\n            feat_level1, feat_level2 = self.encoder(x)\n            feat_level3 = self.base_learner(feat_level2)\n            map_feat = self.linear_mapper(feat_level2)\n            return torch.cat((feat_level1, map_feat, feat_level3), dim=1)\n\n    def getMaskedFeatures(self, feat, mask):\n        \"\"\"\n        Extract foreground and background features via masked average pooling\n\n        Args:\n            feat: input features, shape: (n_way, k_shot, feat_dim, num_points)\n            mask: binary mask, shape: (n_way, k_shot, num_points)\n        Return:\n            masked_feat: masked features, shape: (n_way, k_shot, feat_dim)\n        \"\"\"\n        mask = mask.unsqueeze(2)\n        masked_feat = torch.sum(feat * mask, dim=3) / (mask.sum(dim=3) + 1e-5)\n        return masked_feat\n\n    def getPrototype(self, fg_feat, bg_feat):\n        \"\"\"\n        Average the features to obtain the prototype\n\n        Args:\n            fg_feat: foreground features for each way/shot, shape: (n_way, k_shot, feat_dim)\n            bg_feat: background features for each way/shot, shape: (n_way, k_shot, feat_dim)\n        Returns:\n            fg_prototypes: a list of n_way foreground prototypes, each prototype is a vector with shape (feat_dim,)\n            bg_prototype: background prototype, a vector with shape (feat_dim,)\n        \"\"\"\n        fg_prototypes = [fg_feat[way, ...].sum(dim=0) / self.k_shot for way in range(self.n_way)]\n        bg_prototype = bg_feat.sum(dim=(0,1)) / (self.n_way * self.k_shot)\n        return fg_prototypes, bg_prototype\n\n    def calculateSimilarity(self, feat,  prototype, method='cosine', scaler=10):\n        \"\"\"\n        Calculate the Similarity between query point-level features and prototypes\n\n        Args:\n            feat: input query point-level features\n                  shape: (n_queries, feat_dim, num_points)\n            prototype: prototype of one semantic class\n                       shape: (feat_dim,)\n            method: 'cosine' or 'euclidean', different ways to calculate similarity\n            scaler: used when 'cosine' distance is computed.\n                    By multiplying the factor with cosine distance can achieve comparable performance\n                    as using squared Euclidean distance (refer to PANet [ICCV2019])\n        Return:\n            similarity: similarity between query point to prototype\n                        shape: (n_queries, 1, num_points)\n        \"\"\"\n        if method == 'cosine':\n            similarity = F.cosine_similarity(feat, prototype[None, ..., None], dim=1) * scaler\n        elif method == 'euclidean':\n            similarity = - F.pairwise_distance(feat, prototype[None, ..., None], p=2)**2\n        else:\n            raise NotImplementedError('Error! Distance computation method (%s) is unknown!' %method)\n        return similarity\n\n    def computeCrossEntropyLoss(self, query_logits, query_labels):\n        \"\"\" Calculate the CrossEntropy Loss for query set\n        \"\"\"\n        return F.cross_entropy(query_logits, query_labels)", ""]}
{"filename": "models/__init__.py", "chunked_list": [""]}
{"filename": "models/dgcnn.py", "chunked_list": ["\ufeff\"\"\"DGCNN as Backbone to extract point-level features\n   Adapted from https://github.com/WangYueFt/dgcnn/blob/master/pytorch/model.py\n   \n\"\"\"\n\nimport os\nimport sys\nimport copy\nimport math\nimport numpy as np", "import math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef knn(x, k):\n    inner = -2 * torch.matmul(x.transpose(2, 1), x) #(B,N,N)\n    xx = torch.sum(x ** 2, dim=1, keepdim=True) #(B,1,N)", "    inner = -2 * torch.matmul(x.transpose(2, 1), x) #(B,N,N)\n    xx = torch.sum(x ** 2, dim=1, keepdim=True) #(B,1,N)\n    pairwise_distance = -xx - inner - xx.transpose(2, 1) #(B,N,N)\n\n    idx = pairwise_distance.topk(k=k, dim=-1)[1]  # (B,N,k)\n    return idx\n\n\ndef get_edge_feature(x, K=20, idx=None):\n    \"\"\"Construct edge feature for each point", "def get_edge_feature(x, K=20, idx=None):\n    \"\"\"Construct edge feature for each point\n      Args:\n        x: point clouds (B, C, N)\n        K: int\n        idx: knn index, if not None, the shape is (B, N, K)\n      Returns:\n        edge feat: (B, 2C, N, K)\n    \"\"\"\n    B, C, N = x.size()", "    \"\"\"\n    B, C, N = x.size()\n    if idx is None:\n        idx = knn(x, k=K)  # (batch_size, num_points, k)\n    central_feat = x.unsqueeze(-1).expand(-1,-1,-1,K)\n    idx = idx.unsqueeze(1).expand(-1, C, -1, -1).contiguous().view(B,C,N*K)\n    knn_feat = torch.gather(x, dim=2, index=idx).contiguous().view(B,C,N,K)\n    edge_feat = torch.cat((knn_feat-central_feat, central_feat), dim=1)\n    return edge_feat\n", "    return edge_feat\n\n\nclass conv2d(nn.Module):\n    def __init__(self, in_feat, layer_dims, batch_norm=True, relu=True, bias=False):\n        super().__init__()\n        self.layer_dims = layer_dims\n        layers = []\n        for i in range(len(layer_dims)):\n            in_dim = in_feat if i==0 else layer_dims[i-1]", "        for i in range(len(layer_dims)):\n            in_dim = in_feat if i==0 else layer_dims[i-1]\n            out_dim = layer_dims[i]\n            layers.append(nn.Conv2d(in_dim, out_dim, kernel_size=1, bias=bias))\n            if batch_norm:\n                layers.append(nn.BatchNorm2d(out_dim))\n            if relu:\n                layers.append(nn.LeakyReLU(0.2))\n        self.layer = nn.Sequential(*layers)\n", "        self.layer = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass conv1d(nn.Module):\n    def __init__(self, in_feat, layer_dims, batch_norm=True, relu=True, bias=False):\n        super().__init__()\n        self.layer_dims = layer_dims", "        super().__init__()\n        self.layer_dims = layer_dims\n        layers = []\n        for i in range(len(layer_dims)):\n            in_dim = in_feat if i==0 else layer_dims[i-1]\n            out_dim = layer_dims[i]\n            layers.append(nn.Conv1d(in_dim, out_dim, kernel_size=1, bias=bias))\n            if batch_norm:\n                layers.append(nn.BatchNorm1d(out_dim))\n            if relu:", "                layers.append(nn.BatchNorm1d(out_dim))\n            if relu:\n                layers.append(nn.LeakyReLU(0.2))\n        self.layer = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layer(x)\n\n\nclass DGCNN(nn.Module):", "\nclass DGCNN(nn.Module):\n    \"\"\"\n    DGCNN with only stacked EdgeConv, return intermediate features if use attention\n    Parameters:\n      edgeconv_widths: list of layer widths of edgeconv blocks [[],[],...]\n      mlp_widths: list of layer widths of mlps following Edgeconv blocks\n      nfeat: number of input features\n      k: number of neighbors\n      conv_aggr: neighbor information aggregation method, Option:['add', 'mean', 'max', None]", "      k: number of neighbors\n      conv_aggr: neighbor information aggregation method, Option:['add', 'mean', 'max', None]\n    \"\"\"\n    def __init__(self, edgeconv_widths, mlp_widths, nfeat, k=20, return_edgeconvs=False):\n        super(DGCNN, self).__init__()\n        self.n_edgeconv = len(edgeconv_widths)\n        self.k = k\n        self.return_edgeconvs = return_edgeconvs\n\n        self.edge_convs = nn.ModuleList()", "\n        self.edge_convs = nn.ModuleList()\n        for i in range(self.n_edgeconv):\n            if i==0:\n                in_feat = nfeat*2\n            else:\n                in_feat = edgeconv_widths[i-1][-1]*2\n\n            self.edge_convs.append(conv2d(in_feat, edgeconv_widths[i]))\n", "            self.edge_convs.append(conv2d(in_feat, edgeconv_widths[i]))\n\n        in_dim = 0\n        for edgeconv_width in edgeconv_widths:\n            in_dim += edgeconv_width[-1]\n        self.conv = conv1d(in_dim, mlp_widths)\n\n    def forward(self, x):\n\n        edgeconv_outputs = []", "\n        edgeconv_outputs = []\n        for i in range(self.n_edgeconv):\n            x = get_edge_feature(x, K=self.k)\n            x = self.edge_convs[i](x)\n            x = x.max(dim=-1, keepdim=False)[0]\n            edgeconv_outputs.append(x)\n\n        out = torch.cat(edgeconv_outputs, dim=1) # [16, 192, 2048]\n", "        out = torch.cat(edgeconv_outputs, dim=1) # [16, 192, 2048]\n\n        out = self.conv(out) # [16, 256, 2048]\n\n        if self.return_edgeconvs:\n            return edgeconv_outputs, out\n        else:\n            return edgeconv_outputs[0], out\n", ""]}
{"filename": "models/proto_learner.py", "chunked_list": ["\"\"\" ProtoNet with/without attention learner for Few-shot 3D Point Cloud Semantic Segmentation\n\n\n\"\"\"\nimport torch\nfrom torch import optim\nfrom torch.nn import functional as F\n\nfrom models.protonet_QGPA import ProtoNetAlignQGPASR\nfrom models.protonet import ProtoNet", "from models.protonet_QGPA import ProtoNetAlignQGPASR\nfrom models.protonet import ProtoNet\nfrom utils.checkpoint_util import load_pretrain_checkpoint, load_model_checkpoint\n\n\nclass ProtoLearner(object):\n\n    def __init__(self, args, mode='train'):\n\n        # init model and optimizer\n        if args.use_transformer:\n            self.model = ProtoNetAlignQGPASR(args)\n        else:\n            self.model = ProtoNet(args)\n        print(self.model)\n        if torch.cuda.is_available():\n            self.model.cuda()\n\n        if mode == 'train':\n            if args.use_attention:\n                if args.use_transformer:\n                    self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.transformer.parameters(), 'lr': args.trans_lr},\n                     {'params': self.model.att_learner.parameters()}\n                     ], lr=args.lr)\n                else:\n                    self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.att_learner.parameters()}\n                     ], lr=args.lr)\n            else:\n                self.optimizer = torch.optim.Adam(\n                    [{'params': self.model.encoder.parameters(), 'lr': 0.0001},\n                     {'params': self.model.base_learner.parameters()},\n                     {'params': self.model.linear_mapper.parameters()}], lr=args.lr)\n            #set learning rate scheduler\n            self.lr_scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=args.step_size,\n                                                          gamma=args.gamma)\n            # load pretrained model for point cloud encoding\n            self.model = load_pretrain_checkpoint(self.model, args.pretrain_checkpoint_path)\n        elif mode == 'test':\n            # Load model checkpoint\n            self.model = load_model_checkpoint(self.model, args.model_checkpoint_path, mode='test')\n        else:\n            raise ValueError('Wrong GMMLearner mode (%s)! Option:train/test' %mode)\n\n    def train(self, data, sampled_classes):\n        \"\"\"\n        Args:\n            data: a list of torch tensors wit the following entries.\n            - support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            - support_y: support masks (foreground) with shape (n_way, k_shot, num_points)\n            - query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            - query_y: query labels with shape (n_queries, num_points)\n        \"\"\"\n\n        [support_x, support_y, query_x, query_y] = data\n        self.model.train()\n\n        query_logits, loss, _ = self.model(support_x, support_y, query_x, query_y)\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.lr_scheduler.step()\n\n        query_pred = F.softmax(query_logits, dim=1).argmax(dim=1)\n        correct = torch.eq(query_pred, query_y).sum().item()  # including background class\n        accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\n        return loss, accuracy\n\n    def test(self, data):\n        \"\"\"\n        Args:\n            support_x: support point clouds with shape (n_way, k_shot, in_channels, num_points)\n            support_y: support masks (foreground) with shape (n_way, k_shot, num_points), each point \\in {0,1}.\n            query_x: query point clouds with shape (n_queries, in_channels, num_points)\n            query_y: query labels with shape (n_queries, num_points), each point \\in {0,..., n_way}\n        \"\"\"\n        [support_x, support_y, query_x, query_y] = data\n        self.model.eval()\n\n        with torch.no_grad():\n            logits, loss, fg_prototypes = self.model(support_x, support_y, query_x, query_y)\n            pred = F.softmax(logits, dim=1).argmax(dim=1)\n            correct = torch.eq(pred, query_y).sum().item()\n            accuracy = correct / (query_y.shape[0]*query_y.shape[1])\n\n        return pred, loss, accuracy"]}
{"filename": "models/attention.py", "chunked_list": ["\"\"\"Self Attention Module\n\n\n\"\"\"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channel, out_channel=None, attn_dropout=0.1):\n        \"\"\"\n        :param in_channel: previous layer's output feature dimension\n        :param out_channel: size of output vector, defaults to in_channel\n        \"\"\"\n        super(SelfAttention, self).__init__()\n        self.in_channel = in_channel\n\n        if out_channel is not None:\n            self.out_channel = out_channel\n        else:\n            self.out_channel = in_channel\n\n        self.temperature = self.out_channel ** 0.5\n\n        self.q_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n        self.k_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n        self.v_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: the feature maps from previous layer,\n                      shape: (batch_size, in_channel, num_points)\n        :return: y: attentioned features maps,\n                        shape\uff1a (batch_size, out_channel, num_points)\n        \"\"\"\n        q = self.q_map(x)  # (batch_size, out_channel, num_points)\n        k = self.k_map(x)  # (batch_size, out_channel, num_points)\n        v = self.v_map(x)  # (batch_size, out_channel, num_points)\n\n        attn = torch.matmul(q.transpose(1,2) / self.temperature, k)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        y = torch.matmul(attn, v.transpose(1,2)) # (batch_size, num_points, out_channel)\n\n        return y.transpose(1, 2)", "\nclass SelfAttention(nn.Module):\n    def __init__(self, in_channel, out_channel=None, attn_dropout=0.1):\n        \"\"\"\n        :param in_channel: previous layer's output feature dimension\n        :param out_channel: size of output vector, defaults to in_channel\n        \"\"\"\n        super(SelfAttention, self).__init__()\n        self.in_channel = in_channel\n\n        if out_channel is not None:\n            self.out_channel = out_channel\n        else:\n            self.out_channel = in_channel\n\n        self.temperature = self.out_channel ** 0.5\n\n        self.q_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n        self.k_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n        self.v_map = nn.Conv1d(in_channel, self.out_channel, 1, bias=False)\n\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, x):\n        \"\"\"\n        :param x: the feature maps from previous layer,\n                      shape: (batch_size, in_channel, num_points)\n        :return: y: attentioned features maps,\n                        shape\uff1a (batch_size, out_channel, num_points)\n        \"\"\"\n        q = self.q_map(x)  # (batch_size, out_channel, num_points)\n        k = self.k_map(x)  # (batch_size, out_channel, num_points)\n        v = self.v_map(x)  # (batch_size, out_channel, num_points)\n\n        attn = torch.matmul(q.transpose(1,2) / self.temperature, k)\n\n        attn = self.dropout(F.softmax(attn, dim=-1))\n        y = torch.matmul(attn, v.transpose(1,2)) # (batch_size, num_points, out_channel)\n\n        return y.transpose(1, 2)", "\n\nclass QGPA(nn.Module):\n    def __init__(self, attn_dropout=0.1):\n\n        super(QGPA, self).__init__()\n        self.in_channel = self.out_channel = 320\n\n        self.temperature = self.out_channel ** 0.5\n        self.layer_norm = nn.LayerNorm(self.in_channel)\n        proj_dim = 512\n        self.q_map = nn.Conv1d(2048, proj_dim, 1, bias=False)\n        self.k_map = nn.Conv1d(2048, proj_dim, 1, bias=False)\n\n        self.v_map = nn.Linear(self.in_channel, self.out_channel)\n        self.fc = nn.Conv1d(self.in_channel, self.out_channel, 1, bias=False)\n        self.dropout = nn.Dropout(attn_dropout)\n\n    def forward(self, query, support, prototype):\n\n        batch, dim = query.shape[0], query.shape[1]\n        way = support.shape[0] + 1\n        residual = prototype\n        q = self.q_map(query.transpose(1, 2))\n        if len(support.shape) == 4:\n            support = support.squeeze()\n        support = torch.cat([support.mean(0).unsqueeze(0), support], dim=0)\n        k = self.k_map(support.transpose(1, 2))\n        v = self.v_map(prototype)\n        q = q.view(q.shape[1], q.shape[2] * q.shape[0])\n        k = k.view(k.shape[1], k.shape[2] * k.shape[0])\n\n        attn = torch.matmul(q.transpose(0, 1) / self.temperature, k)\n        attn = attn.reshape(batch, way, dim, dim)\n        attn = F.softmax(attn, dim=-1)\n        v = v.unsqueeze(2)\n        output = torch.matmul(attn, v.transpose(-2, -1)).squeeze(-1).transpose(1, 2)\n        output = self.dropout(self.fc(output)).transpose(1, 2)\n        output = self.layer_norm(output + residual)\n\n        return output", "\n"]}
{"filename": "models/gmmn.py", "chunked_list": ["import torch\n\nfrom torch import nn\n\n\nclass GMMNnetwork(nn.Module):\n    def __init__(\n        self,\n        noise_dim,\n        embed_dim,\n        hidden_size,\n        feature_dim,\n        drop_out_gmm,\n        semantic_reconstruction=False,\n    ):\n        super().__init__()\n\n        def block(in_feat, out_feat):\n            layers = [nn.Linear(in_feat, out_feat)]\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            layers.append(nn.Dropout(p=drop_out_gmm))\n            return layers\n\n        def init_weights(m):\n            if type(m) == nn.Linear:\n                torch.nn.init.xavier_uniform_(m.weight)\n                m.bias.data.fill_(0.01)\n\n        if hidden_size:\n            self.model = nn.Sequential(\n                *block(noise_dim + embed_dim, hidden_size),\n                nn.Linear(hidden_size, feature_dim),\n            )\n        else:\n            self.model = nn.Linear(noise_dim + embed_dim, feature_dim)\n\n        self.model.apply(init_weights)\n        self.semantic_reconstruction = semantic_reconstruction\n        if self.semantic_reconstruction:\n            self.semantic_reconstruction_layer = nn.Linear(\n                feature_dim, noise_dim + embed_dim\n            )\n\n\n    def forward(self, embd, noise):\n        features = self.model(torch.cat((embd, noise), 1))\n        if self.semantic_reconstruction:\n            semantic = self.semantic_reconstruction_layer(features)\n            return features, semantic\n        else:\n            return features", "\n\nclass GMMNLoss:\n    def __init__(self, sigma=[2, 5, 10, 20, 40, 80], cuda=False):\n        self.sigma = sigma\n        self.cuda = cuda\n\n    def build_loss(self):\n        return self.moment_loss\n\n    def get_scale_matrix(self, M, N):\n        s1 = torch.ones((N, 1)) * 1.0 / N\n        s2 = torch.ones((M, 1)) * -1.0 / M\n        if self.cuda:\n            s1, s2 = s1.cuda(), s2.cuda()\n        return torch.cat((s1, s2), 0)\n\n    def moment_loss(self, gen_samples, x):\n        X = torch.cat((gen_samples, x), 0)\n        XX = torch.matmul(X, X.t())\n        X2 = torch.sum(X * X, 1, keepdim=True)\n        exp = XX - 0.5 * X2 - 0.5 * X2.t()\n        M = gen_samples.size()[0]\n        N = x.size()[0]\n        s = self.get_scale_matrix(M, N)\n        S = torch.matmul(s, s.t())\n\n        loss = 0\n        for v in self.sigma:\n            kernel_val = torch.exp(exp / v)\n            loss += torch.sum(S * kernel_val)\n\n        loss = torch.sqrt(loss)\n        return loss", ""]}
{"filename": "models/dgcnn_new.py", "chunked_list": ["#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\"\"\"\n@Author: Yue Wang\n@Contact: yuewangx@mit.edu\n@File: model.py\n@Time: 2018/10/13 6:35 PM\n\nModified by \n@Author: An Tao", "Modified by \n@Author: An Tao\n@Contact: ta19@mails.tsinghua.edu.cn\n@Time: 2020/3/9 9:32 PM\n\"\"\"\n\n\nimport os\nimport sys\nimport copy", "import sys\nimport copy\nimport math\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\n\n\ndef knn(x, k):\n    inner = -2*torch.matmul(x.transpose(2, 1), x)\n    xx = torch.sum(x**2, dim=1, keepdim=True)\n    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n \n    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n    return idx", "\n\ndef knn(x, k):\n    inner = -2*torch.matmul(x.transpose(2, 1), x)\n    xx = torch.sum(x**2, dim=1, keepdim=True)\n    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n \n    idx = pairwise_distance.topk(k=k, dim=-1)[1]   # (batch_size, num_points, k)\n    return idx\n", "\n\ndef get_graph_feature(x, k=20, idx=None, dim9=False):\n    batch_size = x.size(0)\n    num_points = x.size(2)\n    x = x.view(batch_size, -1, num_points)\n    if idx is None:\n        if dim9 == False:\n            idx = knn(x, k=k)   # (batch_size, num_points, k)\n        else:\n            idx = knn(x[:, 6:], k=k)\n    device = torch.device('cuda')\n\n    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n\n    idx = idx + idx_base\n\n    idx = idx.view(-1)\n \n    _, num_dims, _ = x.size()\n\n    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) #   batch_size * num_points * k + range(0, batch_size*num_points)\n    feature = x.view(batch_size*num_points, -1)[idx, :]\n    feature = feature.view(batch_size, num_points, k, num_dims) \n    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n    \n    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n  \n    return feature      # (batch_size, 2*num_dims, num_points, k)", "\n\nclass PointNet(nn.Module):\n    def __init__(self, args, output_channels=40):\n        super(PointNet, self).__init__()\n        self.args = args\n        self.conv1 = nn.Conv1d(3, 64, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n        self.conv3 = nn.Conv1d(64, 64, kernel_size=1, bias=False)\n        self.conv4 = nn.Conv1d(64, 128, kernel_size=1, bias=False)\n        self.conv5 = nn.Conv1d(128, args.emb_dims, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(64)\n        self.bn3 = nn.BatchNorm1d(64)\n        self.bn4 = nn.BatchNorm1d(128)\n        self.bn5 = nn.BatchNorm1d(args.emb_dims)\n        self.linear1 = nn.Linear(args.emb_dims, 512, bias=False)\n        self.bn6 = nn.BatchNorm1d(512)\n        self.dp1 = nn.Dropout()\n        self.linear2 = nn.Linear(512, output_channels)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = F.relu(self.bn4(self.conv4(x)))\n        x = F.relu(self.bn5(self.conv5(x)))\n        x = F.adaptive_max_pool1d(x, 1).squeeze()\n        x = F.relu(self.bn6(self.linear1(x)))\n        x = self.dp1(x)\n        x = self.linear2(x)\n        return x", "\n\nclass DGCNN_cls(nn.Module):\n    def __init__(self, args, output_channels=40):\n        super(DGCNN_cls, self).__init__()\n        self.args = args\n        self.k = args.k\n        \n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(128)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.bn5 = nn.BatchNorm1d(args.emb_dims)\n\n        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n                                   self.bn1,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n                                   self.bn2,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n                                   self.bn3,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n                                   self.bn4,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv5 = nn.Sequential(nn.Conv1d(512, args.emb_dims, kernel_size=1, bias=False),\n                                   self.bn5,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.linear1 = nn.Linear(args.emb_dims*2, 512, bias=False)\n        self.bn6 = nn.BatchNorm1d(512)\n        self.dp1 = nn.Dropout(p=args.dropout)\n        self.linear2 = nn.Linear(512, 256)\n        self.bn7 = nn.BatchNorm1d(256)\n        self.dp2 = nn.Dropout(p=args.dropout)\n        self.linear3 = nn.Linear(256, output_channels)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        x = get_graph_feature(x, k=self.k)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv2(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 128, num_points, k)\n        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n\n        x = get_graph_feature(x3, k=self.k)     # (batch_size, 128, num_points) -> (batch_size, 128*2, num_points, k)\n        x = self.conv4(x)                       # (batch_size, 128*2, num_points, k) -> (batch_size, 256, num_points, k)\n        x4 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 256, num_points, k) -> (batch_size, 256, num_points)\n\n        x = torch.cat((x1, x2, x3, x4), dim=1)  # (batch_size, 64+64+128+256, num_points)\n\n        x = self.conv5(x)                       # (batch_size, 64+64+128+256, num_points) -> (batch_size, emb_dims, num_points)\n        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)           # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims)\n        x = torch.cat((x1, x2), 1)              # (batch_size, emb_dims*2)\n\n        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2) # (batch_size, emb_dims*2) -> (batch_size, 512)\n        x = self.dp1(x)\n        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2) # (batch_size, 512) -> (batch_size, 256)\n        x = self.dp2(x)\n        x = self.linear3(x)                                             # (batch_size, 256) -> (batch_size, output_channels)\n        \n        return x", "\n\nclass Transform_Net(nn.Module):\n    def __init__(self, args):\n        super(Transform_Net, self).__init__()\n        self.args = args\n        self.k = 3\n\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(128)\n        self.bn3 = nn.BatchNorm1d(1024)\n\n        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n                                   self.bn1,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 128, kernel_size=1, bias=False),\n                                   self.bn2,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv3 = nn.Sequential(nn.Conv1d(128, 1024, kernel_size=1, bias=False),\n                                   self.bn3,\n                                   nn.LeakyReLU(negative_slope=0.2))\n\n        self.linear1 = nn.Linear(1024, 512, bias=False)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.linear2 = nn.Linear(512, 256, bias=False)\n        self.bn4 = nn.BatchNorm1d(256)\n\n        self.transform = nn.Linear(256, 3*3)\n        init.constant_(self.transform.weight, 0)\n        init.eye_(self.transform.bias.view(3, 3))\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 128, num_points, k)\n        x = x.max(dim=-1, keepdim=False)[0]     # (batch_size, 128, num_points, k) -> (batch_size, 128, num_points)\n\n        x = self.conv3(x)                       # (batch_size, 128, num_points) -> (batch_size, 1024, num_points)\n        x = x.max(dim=-1, keepdim=False)[0]     # (batch_size, 1024, num_points) -> (batch_size, 1024)\n\n        x = F.leaky_relu(self.bn3(self.linear1(x)), negative_slope=0.2)     # (batch_size, 1024) -> (batch_size, 512)\n        x = F.leaky_relu(self.bn4(self.linear2(x)), negative_slope=0.2)     # (batch_size, 512) -> (batch_size, 256)\n\n        x = self.transform(x)                   # (batch_size, 256) -> (batch_size, 3*3)\n        x = x.view(batch_size, 3, 3)            # (batch_size, 3*3) -> (batch_size, 3, 3)\n\n        return x", "\n\nclass DGCNN_partseg(nn.Module):\n    def __init__(self, args, seg_num_all):\n        super(DGCNN_partseg, self).__init__()\n        self.args = args\n        self.seg_num_all = seg_num_all\n        self.k = args.k\n        self.transform_net = Transform_Net(args)\n        \n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.bn4 = nn.BatchNorm2d(64)\n        self.bn5 = nn.BatchNorm2d(64)\n        self.bn6 = nn.BatchNorm1d(args.emb_dims)\n        self.bn7 = nn.BatchNorm1d(64)\n        self.bn8 = nn.BatchNorm1d(256)\n        self.bn9 = nn.BatchNorm1d(256)\n        self.bn10 = nn.BatchNorm1d(128)\n\n        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n                                   self.bn1,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n                                   self.bn2,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n                                   self.bn3,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n                                   self.bn4,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n                                   self.bn5,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv6 = nn.Sequential(nn.Conv1d(192, args.emb_dims, kernel_size=1, bias=False),\n                                   self.bn6,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv7 = nn.Sequential(nn.Conv1d(16, 64, kernel_size=1, bias=False),\n                                   self.bn7,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv8 = nn.Sequential(nn.Conv1d(1280, 256, kernel_size=1, bias=False),\n                                   self.bn8,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.dp1 = nn.Dropout(p=args.dropout)\n        self.conv9 = nn.Sequential(nn.Conv1d(256, 256, kernel_size=1, bias=False),\n                                   self.bn9,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.dp2 = nn.Dropout(p=args.dropout)\n        self.conv10 = nn.Sequential(nn.Conv1d(256, 128, kernel_size=1, bias=False),\n                                   self.bn10,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv11 = nn.Conv1d(128, self.seg_num_all, kernel_size=1, bias=False)\n        \n\n    def forward(self, x, l):\n        batch_size = x.size(0)\n        num_points = x.size(2)\n\n        x0 = get_graph_feature(x, k=self.k)     # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n        t = self.transform_net(x0)              # (batch_size, 3, 3)\n        x = x.transpose(2, 1)                   # (batch_size, 3, num_points) -> (batch_size, num_points, 3)\n        x = torch.bmm(x, t)                     # (batch_size, num_points, 3) * (batch_size, 3, 3) -> (batch_size, num_points, 3)\n        x = x.transpose(2, 1)                   # (batch_size, num_points, 3) -> (batch_size, 3, num_points)\n\n        x = get_graph_feature(x, k=self.k)      # (batch_size, 3, num_points) -> (batch_size, 3*2, num_points, k)\n        x = self.conv1(x)                       # (batch_size, 3*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv4(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv5(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = torch.cat((x1, x2, x3), dim=1)      # (batch_size, 64*3, num_points)\n\n        x = self.conv6(x)                       # (batch_size, 64*3, num_points) -> (batch_size, emb_dims, num_points)\n        x = x.max(dim=-1, keepdim=True)[0]      # (batch_size, emb_dims, num_points) -> (batch_size, emb_dims, 1)\n\n        l = l.view(batch_size, -1, 1)           # (batch_size, num_categoties, 1)\n        l = self.conv7(l)                       # (batch_size, num_categoties, 1) -> (batch_size, 64, 1)\n\n        x = torch.cat((x, l), dim=1)            # (batch_size, 1088, 1)\n        x = x.repeat(1, 1, num_points)          # (batch_size, 1088, num_points)\n\n        x = torch.cat((x, x1, x2, x3), dim=1)   # (batch_size, 1088+64*3, num_points)\n\n        x = self.conv8(x)                       # (batch_size, 1088+64*3, num_points) -> (batch_size, 256, num_points)\n        x = self.dp1(x)\n        x = self.conv9(x)                       # (batch_size, 256, num_points) -> (batch_size, 256, num_points)\n        x = self.dp2(x)\n        x = self.conv10(x)                      # (batch_size, 256, num_points) -> (batch_size, 128, num_points)\n        x = self.conv11(x)                      # (batch_size, 256, num_points) -> (batch_size, seg_num_all, num_points)\n        \n        return x", "\n\nclass DGCNN_semseg(nn.Module):\n    def __init__(self, edgeconv_widths, mlp_widths, nfeat, k=20, return_edgeconvs=False):\n        super(DGCNN_semseg, self).__init__()\n        self.k = k\n        self.return_edgeconvs = return_edgeconvs\n        self.bn1 = nn.BatchNorm2d(64)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(64)\n        self.bn4 = nn.BatchNorm2d(64)\n        self.bn5 = nn.BatchNorm2d(64)\n        self.bn6 = nn.BatchNorm1d(256)\n        self.bn7 = nn.BatchNorm1d(512)\n        self.bn8 = nn.BatchNorm1d(256)\n\n        self.conv1 = nn.Sequential(nn.Conv2d(18, 64, kernel_size=1, bias=False),\n                                   self.bn1,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n                                   self.bn2,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n                                   self.bn3,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv4 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1, bias=False),\n                                   self.bn4,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv5 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n                                   self.bn5,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        self.conv6 = nn.Sequential(nn.Conv1d(192, 256, kernel_size=1, bias=False),\n                                   self.bn6,\n                                   nn.LeakyReLU(negative_slope=0.2))\n        \n\n    def forward(self, x):\n        batch_size = x.size(0)\n        num_points = x.size(2)\n        xyz = x[:, :3, :]\n        x = get_graph_feature(x, k=self.k, dim9=True)   # (batch_size, 9, num_points) -> (batch_size, 9*2, num_points, k)\n        x = self.conv1(x)                       # (batch_size, 9*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv2(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n        x1 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = get_graph_feature(x1, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv3(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x = self.conv4(x)                       # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points, k)\n        x2 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = get_graph_feature(x2, k=self.k)     # (batch_size, 64, num_points) -> (batch_size, 64*2, num_points, k)\n        x = self.conv5(x)                       # (batch_size, 64*2, num_points, k) -> (batch_size, 64, num_points, k)\n        x3 = x.max(dim=-1, keepdim=False)[0]    # (batch_size, 64, num_points, k) -> (batch_size, 64, num_points)\n\n        x = torch.cat((x1, x2, x3), dim=1)      # (batch_size, 64*3, num_points)\n\n        out = self.conv6(x)                       # (batch_size, 64*3, num_points) -> (batch_size, emb_dims, num_points)\n        if self.return_edgeconvs:\n            return [x1, x2, x3], out, xyz.transpose(1, 2)\n        else:\n            return x1, out, xyz.transpose(1, 2)", ""]}
