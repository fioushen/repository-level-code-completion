{"filename": "scripts/purge_topics.py", "chunked_list": ["import sys\nfrom tinydb import TinyDB\n\n\ndef remove_topic_table(file_path: str):\n    try:\n        db = TinyDB(file_path)\n        if 'topics' in db.tables():\n            db.drop_table('topics')\n            print(\"The 'topics' table has been removed.\")\n        else:\n            print(\"The file does not contain a 'topics' table.\")\n    except Exception as exc:\n        print(f\"Error: {exc}. The file is not a valid TinyDB file or could not be processed.\")", "\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(\"Usage: python remove_topic_table.py <file_path>\")\n        sys.exit(1)\n\n    remove_topic_table(sys.argv[1])\n", ""]}
{"filename": "tests/test_cli_prompt.py", "chunked_list": ["import os\nimport json\nimport pytest\nfrom click.testing import CliRunner\nfrom devchat._cli import main\nfrom devchat.utils import response_tokens\nfrom devchat.utils import check_format, get_content, get_prompt_hash\n\nrunner = CliRunner()\n", "runner = CliRunner()\n\n\ndef test_prompt_no_args(git_repo):  # pylint: disable=W0613\n    result = runner.invoke(main, ['prompt'])\n    assert result.exit_code == 0\n\n\ndef test_prompt_with_content(git_repo):  # pylint: disable=W0613\n    content = \"What is the capital of France?\"\n    result = runner.invoke(main, ['prompt', content])\n    assert result.exit_code == 0\n    assert check_format(result.output)\n    assert \"Paris\" in result.output", "def test_prompt_with_content(git_repo):  # pylint: disable=W0613\n    content = \"What is the capital of France?\"\n    result = runner.invoke(main, ['prompt', content])\n    assert result.exit_code == 0\n    assert check_format(result.output)\n    assert \"Paris\" in result.output\n\n\ndef test_prompt_with_temp_config_file(git_repo):\n    config_data = {\n        'model': 'gpt-3.5-turbo-0301',\n        'provider': 'OpenAI',\n        'tokens-per-prompt': 3000,\n        'OpenAI': {\n            'temperature': 0\n        }\n    }\n\n    chat_dir = os.path.join(git_repo, \".chat\")\n    if not os.path.exists(chat_dir):\n        os.makedirs(chat_dir)\n    temp_config_path = os.path.join(chat_dir, \"config.json\")\n\n    with open(temp_config_path, \"w\", encoding='utf-8') as temp_config_file:\n        json.dump(config_data, temp_config_file)\n\n    content = \"What is the capital of Spain?\"\n    result = runner.invoke(main, ['prompt', content])\n    assert result.exit_code == 0\n    assert check_format(result.output)\n    assert \"Madrid\" in result.output", "def test_prompt_with_temp_config_file(git_repo):\n    config_data = {\n        'model': 'gpt-3.5-turbo-0301',\n        'provider': 'OpenAI',\n        'tokens-per-prompt': 3000,\n        'OpenAI': {\n            'temperature': 0\n        }\n    }\n\n    chat_dir = os.path.join(git_repo, \".chat\")\n    if not os.path.exists(chat_dir):\n        os.makedirs(chat_dir)\n    temp_config_path = os.path.join(chat_dir, \"config.json\")\n\n    with open(temp_config_path, \"w\", encoding='utf-8') as temp_config_file:\n        json.dump(config_data, temp_config_file)\n\n    content = \"What is the capital of Spain?\"\n    result = runner.invoke(main, ['prompt', content])\n    assert result.exit_code == 0\n    assert check_format(result.output)\n    assert \"Madrid\" in result.output", "\n\n@pytest.fixture(name=\"temp_files\")\ndef fixture_temp_files(tmpdir):\n    instruct0 = tmpdir.join('instruct0.txt')\n    instruct0.write(\"Summarize the following user message.\\n\")\n    instruct1 = tmpdir.join('instruct1.txt')\n    instruct1.write(\"The summary must be lowercased under 5 characters without any punctuation.\\n\")\n    instruct2 = tmpdir.join('instruct2.txt')\n    instruct2.write(\"The summary must be lowercased under 10 characters without any punctuation.\"\n                    \"Include the information in <context> to create the summary.\\n\")\n    context = tmpdir.join(\"context.txt\")\n    context.write(\"It is summer.\")\n    return str(instruct0), str(instruct1), str(instruct2), str(context)", "\n\n@pytest.fixture(name=\"functions_file\")\ndef fixture_functions_file(tmpdir):\n    functions_file = tmpdir.join('functions.json')\n    functions_file.write(\"\"\"\n        [\n            {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"celsius\", \"fahrenheit\"]\n                }\n                },\n                \"required\": [\"location\"]\n            }\n            }\n        ]\n        \"\"\")\n    return str(functions_file)", "\n\ndef test_prompt_with_instruct(git_repo, temp_files):  # pylint: disable=W0613\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-4',\n                                  '-i', temp_files[0], '-i', temp_files[1],\n                                  \"It is really scorching.\"])\n    assert result.exit_code == 0\n    assert get_content(result.output).find(\"hot\\n\") >= 0\n\n\ndef test_prompt_with_instruct_and_context(git_repo, temp_files):  # pylint: disable=W0613\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-4',\n                                  '-i', temp_files[0], '-i', temp_files[2],\n                                  '--context', temp_files[3],\n                                  \"It is really scorching.\"])\n    assert result.exit_code == 0\n    assert get_content(result.output).find(\"hot summer\\n\") >= 0", "\n\ndef test_prompt_with_instruct_and_context(git_repo, temp_files):  # pylint: disable=W0613\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-4',\n                                  '-i', temp_files[0], '-i', temp_files[2],\n                                  '--context', temp_files[3],\n                                  \"It is really scorching.\"])\n    assert result.exit_code == 0\n    assert get_content(result.output).find(\"hot summer\\n\") >= 0\n", "\n\ndef test_prompt_with_functions(git_repo, functions_file):  # pylint: disable=W0613\n    # call with -f option\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-3.5-turbo', '-f', functions_file,\n                                  \"What is the weather like in Boston?\"])\n    print(result.output)\n    assert result.exit_code == 0\n    content = get_content(result.output)\n    assert 'finish_reason: function_call' in content\n    assert '```command' in content\n    assert '\"name\": \"get_current_weather\"' in content\n\n    # compare with no -f options\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-3.5-turbo',\n                                  'What is the weather like in Boston?'])\n\n    content = get_content(result.output)\n    assert result.exit_code == 0\n    assert 'finish_reason: stop' not in content\n    assert 'command' not in content", "\n\ndef test_prompt_log_with_functions(git_repo, functions_file):  # pylint: disable=W0613\n    # call with -f option\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-3.5-turbo', '-f', functions_file,\n                                  'What is the weather like in Boston?'])\n    assert result.exit_code == 0\n    prompt_hash = get_prompt_hash(result.output)\n    result = runner.invoke(main, ['log', '-t', prompt_hash])\n\n    result_json = json.loads(result.output)\n    assert result.exit_code == 0\n    assert result_json[0]['request'] == 'What is the weather like in Boston?'\n    assert '```command' in result_json[0]['responses'][0]\n    assert 'get_current_weather' in result_json[0]['responses'][0]", "\n\ndef test_prompt_log_compatibility():\n    # import test!!\n    # Historical Record Compatibility Test\n    # create git repo folder\n    # install old devchat\n    # run prompt, create old version records\n    # run topic -l, expect topic list\n    # uninstall old devchat\n    # install new devchat\n    # run topic -l, expect topic list\n    # run prompt -f ./.chat/functions.json \"list files in porject\", expect function call return\n    # run topic -l, expect function call in topic list\n    assert True", "\n\n# test prompt with function replay\ndef test_prompt_with_function_replay(git_repo, functions_file):  # pylint: disable=W0613\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-3.5-turbo',\n                                  '-f', functions_file,\n                                  '-n', 'get_current_weather',\n                                  '{\"temperature\": \"22\", \"unit\": \"celsius\", \"weather\": \"Sunny\"}'])\n\n    content = get_content(result.output)\n    assert result.exit_code == 0\n    assert '22 degrees Celsius and sunny' in content\n\n    prompt_hash = get_prompt_hash(result.output)\n    result = runner.invoke(main, ['prompt', '-m', 'gpt-3.5-turbo',\n                                  '-p', prompt_hash,\n                                  'what is the GPT function name?'])\n\n    content = get_content(result.output)\n    assert result.exit_code == 0\n    assert 'get_current_weather' in content", "\n\ndef test_prompt_response_tokens_exceed_config(git_repo):  # pylint: disable=W0613\n    config_data = {\n        'model': 'gpt-3.5-turbo',\n        'provider': 'OpenAI',\n        'tokens-per-prompt': 2000,\n        'OpenAI': {\n            'temperature': 0\n        }\n    }\n\n    chat_dir = os.path.join(git_repo, \".chat\")\n    if not os.path.exists(chat_dir):\n        os.makedirs(chat_dir)\n    temp_config_path = os.path.join(chat_dir, \"config.json\")\n\n    with open(temp_config_path, \"w\", encoding='utf-8') as temp_config_file:\n        json.dump(config_data, temp_config_file)\n\n    content = \"\"\n    while response_tokens({\"content\": content}, config_data[\"model\"]) \\\n            < config_data[\"tokens-per-prompt\"]:\n        content += \"This is a test. Ignore what I say.\\n\"\n    result = runner.invoke(main, ['prompt', content])\n    assert result.exit_code != 0\n    assert \"beyond limit\" in result.output", "\n\ndef test_prompt_response_tokens_exceed_config_with_file(git_repo, tmpdir):  # pylint: disable=W0613\n    config_data = {\n        'model': 'gpt-3.5-turbo',\n        'provider': 'OpenAI',\n        'tokens-per-prompt': 2000,\n        'OpenAI': {\n            'temperature': 0\n        }\n    }\n\n    chat_dir = os.path.join(git_repo, \".chat\")\n    if not os.path.exists(chat_dir):\n        os.makedirs(chat_dir)\n    temp_config_path = os.path.join(chat_dir, \"config.json\")\n\n    with open(temp_config_path, \"w\", encoding='utf-8') as temp_config_file:\n        json.dump(config_data, temp_config_file)\n\n    content_file = tmpdir.join(\"content.txt\")\n    content = \"\"\n    while response_tokens({\"content\": content}, config_data[\"model\"]) < \\\n            config_data[\"tokens-per-prompt\"]:\n        content += \"This is a test. Ignore what I say.\\n\"\n    content_file.write(content)\n\n    input_str = \"This is a test. Ignore what I say.\"\n    result = runner.invoke(main, ['prompt', '-c', str(content_file), input_str])\n    assert result.exit_code != 0\n    assert \"beyond limit\" in result.output", ""]}
{"filename": "tests/__init__.py", "chunked_list": [""]}
{"filename": "tests/test_openai_message.py", "chunked_list": ["import pytest\nfrom devchat.openai import OpenAIMessage\n\n\ndef test_valid_message_creation():\n    message = OpenAIMessage(role=\"user\", content=\"Hello, World!\")\n    assert message.role == \"user\"\n    assert message.content == \"Hello, World!\"\n    assert message.name is None\n", "\n\ndef test_valid_message():\n    message = OpenAIMessage(content=\"Hello, World!\", role=\"user\", name=\"John_Doe\")\n    assert message.to_dict() == {\"role\": \"user\", \"content\": \"Hello, World!\", \"name\": \"John_Doe\"}\n\n\ndef test_invalid_role():\n    with pytest.raises(ValueError):\n        OpenAIMessage(content=\"Hello, World!\", role=\"invalid_role\")", "\n\ndef test_none_content():\n    message = OpenAIMessage(role=\"system\", content=None)\n    assert message.content is None\n\n\ndef test_invalid_name():\n    with pytest.raises(ValueError):\n        OpenAIMessage(content=\"Hello, World!\", role=\"user\", name=\"Invalid@Name\")", "\n\ndef test_empty_name():\n    with pytest.raises(ValueError):\n        OpenAIMessage(content=\"Hello, World!\", role=\"user\", name=\"\")\n\n\ndef test_blank_name():\n    with pytest.raises(ValueError):\n        OpenAIMessage(content=\"Hello, World!\", role=\"user\", name=\"  \")", "\n\ndef test_none_name():\n    message = OpenAIMessage(content=\"Hello, World!\", role=\"user\", name=None)\n    assert message.to_dict() == {\"role\": \"user\", \"content\": \"Hello, World!\"}\n\n\ndef test_from_dict():\n    message_data = {\n        \"content\": \"Welcome to the chat.\",\n        \"role\": \"system\"\n    }\n    message = OpenAIMessage.from_dict(message_data)\n    assert message.role == \"system\"\n    assert message.content == \"Welcome to the chat.\"\n    assert message.name is None", "\n\ndef test_from_dict_with_name():\n    message_data = {\n        \"content\": \"Hello, Assistant!\",\n        \"role\": \"user\",\n        \"name\": \"JohnDoe\"\n    }\n    message = OpenAIMessage.from_dict(message_data)\n    assert message.role == \"user\"\n    assert message.content == \"Hello, Assistant!\"\n    assert message.name == \"JohnDoe\"", ""]}
{"filename": "tests/test_openai_prompt.py", "chunked_list": ["# pylint: disable=protected-access\nimport json\nimport os\nimport pytest\nfrom devchat.message import Message\nfrom devchat.openai import OpenAIMessage\nfrom devchat.openai import OpenAIPrompt\nfrom devchat.utils import get_user_info\n\n\ndef test_prompt_init_and_set_response():\n    name, email = get_user_info()\n    prompt = OpenAIPrompt(model=\"gpt-3.5-turbo\", user_name=name, user_email=email)\n    assert prompt.model == \"gpt-3.5-turbo\"\n\n    prompt.set_request(\"Where was the 2020 World Series played?\")\n    response_str = '''\n    {\n      \"id\": \"chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve\",\n      \"object\": \"chat.completion\",\n      \"created\": 1677649420,\n      \"model\": \"gpt-3.5-turbo-0301\",\n      \"usage\": {\"prompt_tokens\": 56, \"completion_tokens\": 31, \"total_tokens\": 87},\n      \"choices\": [\n        {\n          \"message\": {\n            \"role\": \"assistant\",\n            \"content\": \"The 2020 World Series was played in Arlington, Texas.\"\n          },\n          \"finish_reason\": \"stop\",\n          \"index\": 0\n        }\n      ]\n    }\n    '''\n    prompt.set_response(response_str)\n\n    assert prompt.timestamp == 1677649420\n    assert prompt.request_tokens == 56\n    assert prompt.response_tokens == 31\n    assert len(prompt.responses) == 1\n    assert prompt.responses[0].role == \"assistant\"\n    assert prompt.responses[0].content == \"The 2020 World Series was played in Arlington, Texas.\"", "\n\ndef test_prompt_init_and_set_response():\n    name, email = get_user_info()\n    prompt = OpenAIPrompt(model=\"gpt-3.5-turbo\", user_name=name, user_email=email)\n    assert prompt.model == \"gpt-3.5-turbo\"\n\n    prompt.set_request(\"Where was the 2020 World Series played?\")\n    response_str = '''\n    {\n      \"id\": \"chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve\",\n      \"object\": \"chat.completion\",\n      \"created\": 1677649420,\n      \"model\": \"gpt-3.5-turbo-0301\",\n      \"usage\": {\"prompt_tokens\": 56, \"completion_tokens\": 31, \"total_tokens\": 87},\n      \"choices\": [\n        {\n          \"message\": {\n            \"role\": \"assistant\",\n            \"content\": \"The 2020 World Series was played in Arlington, Texas.\"\n          },\n          \"finish_reason\": \"stop\",\n          \"index\": 0\n        }\n      ]\n    }\n    '''\n    prompt.set_response(response_str)\n\n    assert prompt.timestamp == 1677649420\n    assert prompt.request_tokens == 56\n    assert prompt.response_tokens == 31\n    assert len(prompt.responses) == 1\n    assert prompt.responses[0].role == \"assistant\"\n    assert prompt.responses[0].content == \"The 2020 World Series was played in Arlington, Texas.\"", "\n\ndef test_prompt_model_mismatch():\n    name, email = get_user_info()\n    prompt = OpenAIPrompt(model=\"gpt-3.5-turbo\", user_name=name, user_email=email)\n\n    response_str = '''\n    {\n      \"choices\": [\n        {\n          \"finish_reason\": \"stop\",\n          \"index\": 0,\n          \"message\": {\n            \"content\": \"\\\\n\\\\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10.\",\n            \"role\": \"assistant\"\n          }\n        }\n      ],\n      \"created\": 1677825456,\n      \"id\": \"chatcmpl-6ptKqrhgRoVchm58Bby0UvJzq2ZuQ\",\n      \"model\": \"gpt-4\",\n      \"object\": \"chat.completion\",\n      \"usage\": {\n        \"completion_tokens\": 301,\n        \"prompt_tokens\": 36,\n        \"total_tokens\": 337\n      }\n    }\n    '''\n    with pytest.raises(ValueError):\n        prompt.set_response(response_str)", "\n\n@pytest.fixture(scope=\"module\", name='responses')\ndef fixture_responses():\n    current_dir = os.path.dirname(__file__)\n    folder_path = os.path.join(current_dir, \"stream_responses\")\n    stream_responses = []\n\n    file_names = os.listdir(folder_path)\n    sorted_file_names = sorted(file_names, key=lambda x: int(x.split('.')[0][8:]))\n\n    for file_name in sorted_file_names:\n        with open(os.path.join(folder_path, file_name), 'r', encoding='utf-8') as file:\n            response = json.load(file)\n            stream_responses.append(response)\n\n    return stream_responses", "\n\ndef test_append_response(responses):\n    name, email = get_user_info()\n    prompt = OpenAIPrompt(\"gpt-3.5-turbo-0301\", name, email)\n\n    for response in responses:\n        prompt.append_response(json.dumps(response))\n\n    expected_messages = [\n        OpenAIMessage(role='assistant', content='Tomorrow.'),\n        OpenAIMessage(role='assistant', content='Tomorrow!')\n    ]\n\n    assert len(prompt.responses) == len(expected_messages)\n    for index, message in enumerate(prompt.responses):\n        assert message.role == expected_messages[index].role\n        assert message.content == expected_messages[index].content", "\n\ndef test_messages_empty():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    assert prompt.messages == []\n\n\ndef test_messages_instruct():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    instruct_message = OpenAIMessage(content='Instructions', role='system')\n    prompt.append_new(Message.INSTRUCT, 'Instructions')\n    assert prompt.messages == [instruct_message.to_dict()]", "\n\ndef test_messages_context():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    context_message = OpenAIMessage(content='Context', role='system')\n    prompt.append_new(Message.CONTEXT, 'Context')\n    expected_message = context_message.to_dict()\n    expected_message[\"content\"] = \"<context>\\n\" + context_message.content + \"\\n</context>\"\n    assert prompt.messages == [expected_message]\n", "\n\ndef test_messages_record():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    with pytest.raises(ValueError):\n        prompt.append_new(Message.CHAT, 'Record')\n\n\ndef test_messages_request():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    request_message = OpenAIMessage(content='Request', role='user')\n    prompt.set_request('Request')\n    expected_message = request_message.to_dict()\n    assert prompt.messages == [expected_message]", "def test_messages_request():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    request_message = OpenAIMessage(content='Request', role='user')\n    prompt.set_request('Request')\n    expected_message = request_message.to_dict()\n    assert prompt.messages == [expected_message]\n\n\ndef test_messages_combined():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    instruct_message = OpenAIMessage(content='Instructions', role='system')\n    context_message = OpenAIMessage(content='Context', role='system')\n    request_message = OpenAIMessage(content='Request', role='user')\n\n    prompt.append_new(Message.INSTRUCT, 'Instructions')\n    prompt.append_new(Message.CONTEXT, 'Context')\n    prompt.set_request('Request')\n\n    expected_context_message = context_message.to_dict()\n    expected_context_message[\"content\"] = \"<context>\\n\" + context_message.content + \"\\n</context>\"\n\n    expected_request_message = request_message.to_dict()\n    expected_request_message[\"content\"] = request_message.content\n\n    assert prompt.messages == [\n        instruct_message.to_dict(),\n        expected_request_message,\n        expected_context_message\n    ]", "def test_messages_combined():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    instruct_message = OpenAIMessage(content='Instructions', role='system')\n    context_message = OpenAIMessage(content='Context', role='system')\n    request_message = OpenAIMessage(content='Request', role='user')\n\n    prompt.append_new(Message.INSTRUCT, 'Instructions')\n    prompt.append_new(Message.CONTEXT, 'Context')\n    prompt.set_request('Request')\n\n    expected_context_message = context_message.to_dict()\n    expected_context_message[\"content\"] = \"<context>\\n\" + context_message.content + \"\\n</context>\"\n\n    expected_request_message = request_message.to_dict()\n    expected_request_message[\"content\"] = request_message.content\n\n    assert prompt.messages == [\n        instruct_message.to_dict(),\n        expected_request_message,\n        expected_context_message\n    ]", "\n\ndef test_messages_invalid_append():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    with pytest.raises(ValueError):\n        prompt.append_new('invalid', 'Instructions')\n\n\ndef test_messages_invalid_request():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    with pytest.raises(ValueError):\n        prompt.set_request(\"\")", "def test_messages_invalid_request():\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    with pytest.raises(ValueError):\n        prompt.set_request(\"\")\n\n\ndef test_input_messages():\n    # Test case 1: Only request message\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    messages = [{\"role\": \"user\", \"content\": \"request\"}]\n    prompt.input_messages(messages)\n    assert prompt.request.content == \"request\"\n\n    # Test case 2: New INSTRUCT and request messages\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    messages = [\n        {\"role\": \"system\", \"content\": \"instruction\"},\n        {\"role\": \"user\", \"content\": \"request\"}\n    ]\n    prompt.input_messages(messages)\n    assert prompt._new_messages[Message.INSTRUCT][0].content == \"instruction\"\n    assert prompt.request.content == \"request\"\n\n    # Test case 3: New INSTRUCT, history CHAT, and request messages\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    messages = [\n        {\"role\": \"system\", \"content\": \"instruction\"},\n        {\"role\": \"user\", \"content\": \"user1\"},\n        {\"role\": \"assistant\", \"content\": \"assistant1\"},\n        {\"role\": \"user\", \"content\": \"request\"},\n    ]\n    prompt.input_messages(messages)\n    assert prompt._new_messages[Message.INSTRUCT][0].content == \"instruction\"\n    assert prompt._history_messages[Message.CHAT][0].content == \"user1\"\n    assert prompt._history_messages[Message.CHAT][1].content == \"assistant1\"\n    assert prompt.request.content == \"request\"\n\n    # Test case 4: History CONTEXT, history CHAT, and request messages\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    messages = [\n        {\"role\": \"system\", \"content\": \"<context>context1</context>\"},\n        {\"role\": \"user\", \"content\": \"user1\"},\n        {\"role\": \"assistant\", \"content\": \"assistant1\"},\n        {\"role\": \"user\", \"content\": \"request\"},\n    ]\n    prompt.input_messages(messages)\n    assert prompt._history_messages[Message.CONTEXT][0].content == \"context1\"\n    assert prompt._history_messages[Message.CHAT][0].content == \"user1\"\n    assert prompt._history_messages[Message.CHAT][1].content == \"assistant1\"\n    assert prompt.request.content == \"request\"\n\n    # Test case 5: Request and new CONTEXT messages\n    prompt = OpenAIPrompt(\"davinci-codex\", \"John Doe\", \"john.doe@example.com\")\n    messages = [\n        {\"role\": \"user\", \"content\": \"request\"},\n        {\"role\": \"system\", \"content\": \"<context>context1</context>\"}\n    ]\n    prompt.input_messages(messages)\n    assert prompt._new_messages[Message.CONTEXT][0].content == \"context1\"\n    assert prompt.request.content == \"request\"", ""]}
{"filename": "tests/test_cli_log.py", "chunked_list": ["import json\nfrom click.testing import CliRunner\nfrom devchat.utils import get_prompt_hash\nfrom devchat._cli import main\n\nrunner = CliRunner()\n\n\ndef test_log_no_args(git_repo):  # pylint: disable=W0613\n    result = runner.invoke(main, ['log'])\n    assert result.exit_code == 0\n    logs = json.loads(result.output)\n    assert isinstance(logs, list)", "def test_log_no_args(git_repo):  # pylint: disable=W0613\n    result = runner.invoke(main, ['log'])\n    assert result.exit_code == 0\n    logs = json.loads(result.output)\n    assert isinstance(logs, list)\n\n\ndef test_log_with_skip_and_max_count(git_repo):  # pylint: disable=W0613\n    result = runner.invoke(main, ['log', '--skip', '1', '--max-count', '2'])\n    assert result.exit_code == 0\n    logs = json.loads(result.output)\n    assert isinstance(logs, list)\n    assert len(logs) <= 2", "\n\ndef _within_range(num1: int, num2: int) -> bool:\n    \"\"\"\n    Check if two integers are within a 3% difference or within a difference of 3.\n\n    :param num1: First integer\n    :param num2: Second integer\n    :return: True if the numbers are within the specified range, False otherwise\n    \"\"\"\n    diff = abs(num1 - num2)\n    if diff <= 3:\n        return True\n    if diff / ((num1 + num2) / 2) * 100 <= 3:\n        return True\n    return False", "\n\ndef test_tokens_with_log(git_repo):  # pylint: disable=W0613\n    request1 = \"Translate the following paragraph to Chinese. \" \\\n        \"Reply only such translation without any other words: \" \\\n        \"THE SOFTWARE IS PROVIDED \u201cAS IS\u201d, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, \" \\\n        \"INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \" \\\n        \"FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\" \\\n        \"IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR \" \\\n        \"OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \" \\\n        \"OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\"\n    request2 = \"Write a Hello World program in C++. Put the code in a block. \" \\\n        \"Do not reply any other words.\"\n\n    # Group 1\n    config_str = '--config={ \"stream\": false, \"temperature\": 0 }'\n    result = runner.invoke(\n        main,\n        ['prompt', '--model=gpt-3.5-turbo', config_str, request1]\n    )\n    assert result.exit_code == 0\n    parent1 = get_prompt_hash(result.output)\n\n    config_str = '--config={ \"stream\": true, \"temperature\": 0 }'\n    result = runner.invoke(\n        main,\n        ['prompt', '--model=gpt-3.5-turbo', config_str, request1]\n    )\n    assert result.exit_code == 0\n    parent2 = get_prompt_hash(result.output)\n\n    result = runner.invoke(main, ['log', '-n', 2])\n    logs = json.loads(result.output)\n    assert logs[1][\"hash\"] == parent1\n    assert logs[0][\"hash\"] == parent2\n    assert _within_range(logs[1][\"request_tokens\"], logs[0][\"request_tokens\"])\n    assert _within_range(logs[1][\"response_tokens\"], logs[0][\"response_tokens\"])\n\n    # Group 2\n    result = runner.invoke(\n        main,\n        ['prompt', '--model=gpt-3.5-turbo', config_str, '-p', parent1, request2]\n    )\n    assert result.exit_code == 0\n\n    result = runner.invoke(\n        main,\n        ['prompt', '--model=gpt-3.5-turbo', config_str, '-p', parent2, request2]\n    )\n    assert result.exit_code == 0\n\n    result = runner.invoke(main, ['log', '-n', 2])\n    logs = json.loads(result.output)\n    assert _within_range(logs[1][\"request_tokens\"], logs[0][\"request_tokens\"])\n    assert _within_range(logs[1][\"response_tokens\"], logs[0][\"response_tokens\"])", ""]}
{"filename": "tests/test_store.py", "chunked_list": ["from typing import Tuple\nimport pytest\nfrom devchat.openai import OpenAIChatConfig, OpenAIChat\nfrom devchat.store import Store\n\n\n@pytest.fixture(name=\"chat_store\", scope=\"function\")\ndef create_chat_store(tmp_path) -> Tuple[OpenAIChat, Store]:\n    config = OpenAIChatConfig(model=\"gpt-3.5-turbo\")\n    chat = OpenAIChat(config)\n    return chat, Store(tmp_path, chat)", "\n\ndef _create_response_str(cmpl_id: str, created: int, content: str) -> str:\n    \"\"\"Create a response string from the given parameters.\"\"\"\n    return f'''\n    {{\n      \"id\": \"{cmpl_id}\",\n      \"object\": \"chat.completion\",\n      \"created\": {created},\n      \"model\": \"gpt-3.5-turbo-0301\",\n      \"usage\": {{\"prompt_tokens\": 56, \"completion_tokens\": 31, \"total_tokens\": 87}},\n      \"choices\": [\n        {{\n          \"message\": {{\n            \"role\": \"assistant\",\n            \"content\": \"{content}\"\n          }},\n          \"finish_reason\": \"stop\",\n          \"index\": 0\n        }}\n      ]\n    }}\n    '''", "\n\ndef test_get_prompt(chat_store):\n    chat, store = chat_store\n    prompt = chat.init_prompt(\"Where was the 2020 World Series played?\")\n    response_str = _create_response_str(\"chatcmpl-6p9XYPYSTTRi0xEviKjjilqrWU2Ve\", 1577649420,\n                                        \"The 2020 World Series was played in Arlington, Texas.\")\n    prompt.set_response(response_str)\n    store.store_prompt(prompt)\n\n    assert store.get_prompt(prompt.hash).timestamp == prompt.timestamp", "\n\ndef test_select_recent(chat_store):\n    chat, store = chat_store\n\n    # Create and store 5 prompts\n    hashes = []\n    for index in range(5):\n        prompt = chat.init_prompt(f\"Question {index}\")\n        response_str = _create_response_str(f\"chatcmpl-id{index}\", 1577649420 + index,\n                                            f\"Answer {index}\")\n        prompt.set_response(response_str)\n        store.store_prompt(prompt)\n        hashes.append(prompt.hash)\n\n    # Test selecting recent prompts\n    recent_prompts = store.select_prompts(0, 3)\n    assert len(recent_prompts) == 3\n    for index, prompt in enumerate(recent_prompts):\n        assert prompt.hash == hashes[4 - index]", "\n\ndef test_select_topics_no_topics(chat_store):\n    _, store = chat_store\n\n    # Test selecting topics when there are no topics\n    topics = store.select_topics(0, 5)\n    assert len(topics) == 0\n\n\ndef test_select_topics_and_prompts_with_single_root(chat_store):\n    chat, store = chat_store\n\n    # Create and store a root prompt\n    root_prompt = chat.init_prompt(\"Root question\")\n    root_response_str = _create_response_str(\"chatcmpl-root\", 1677649400, \"Root answer\")\n    root_prompt.set_response(root_response_str)\n    store.store_prompt(root_prompt)\n\n    # Create and store 3 child prompts for the root prompt\n    child_hashes = []\n    for index in range(3):\n        child_prompt = chat.init_prompt(f\"Child question {index}\")\n        child_prompt.parent = root_prompt.hash\n        child_response_str = _create_response_str(f\"chatcmpl-child{index}\", 1677649400 + index,\n                                                  f\"Child answer {index}\")\n        child_prompt.set_response(child_response_str)\n        store.store_prompt(child_prompt)\n        child_hashes.append(child_prompt.hash)\n\n    # Test selecting topics\n    topics = store.select_topics(0, 5)\n    assert len(topics) == 1\n    assert topics[0]['root_prompt'].hash == root_prompt.hash\n\n    # Test selecting prompts within the topic\n    recent_prompts = store.select_prompts(0, 2, topic=root_prompt.hash)\n    assert len(recent_prompts) == 2\n    for index, prompt in enumerate(recent_prompts):\n        assert prompt.hash == child_hashes[2 - index]", "\n\ndef test_select_topics_and_prompts_with_single_root(chat_store):\n    chat, store = chat_store\n\n    # Create and store a root prompt\n    root_prompt = chat.init_prompt(\"Root question\")\n    root_response_str = _create_response_str(\"chatcmpl-root\", 1677649400, \"Root answer\")\n    root_prompt.set_response(root_response_str)\n    store.store_prompt(root_prompt)\n\n    # Create and store 3 child prompts for the root prompt\n    child_hashes = []\n    for index in range(3):\n        child_prompt = chat.init_prompt(f\"Child question {index}\")\n        child_prompt.parent = root_prompt.hash\n        child_response_str = _create_response_str(f\"chatcmpl-child{index}\", 1677649400 + index,\n                                                  f\"Child answer {index}\")\n        child_prompt.set_response(child_response_str)\n        store.store_prompt(child_prompt)\n        child_hashes.append(child_prompt.hash)\n\n    # Test selecting topics\n    topics = store.select_topics(0, 5)\n    assert len(topics) == 1\n    assert topics[0]['root_prompt'].hash == root_prompt.hash\n\n    # Test selecting prompts within the topic\n    recent_prompts = store.select_prompts(0, 2, topic=root_prompt.hash)\n    assert len(recent_prompts) == 2\n    for index, prompt in enumerate(recent_prompts):\n        assert prompt.hash == child_hashes[2 - index]", "\n\ndef test_select_recent_with_topic_tree(chat_store):\n    chat, store = chat_store\n\n    # Create and store a root prompt\n    root_prompt = chat.init_prompt(\"Root question\")\n    root_response_str = _create_response_str(\"chatcmpl-root\", 1677649400, \"Root answer\")\n    root_prompt.set_response(root_response_str)\n    store.store_prompt(root_prompt)\n\n    # Create and store a child prompt for the root prompt\n    child_prompt = chat.init_prompt(\"Child question\")\n    child_prompt.parent = root_prompt.hash\n    child_response_str = _create_response_str(\"chatcmpl-child\", 1677649401, \"Child answer\")\n    child_prompt.set_response(child_response_str)\n    store.store_prompt(child_prompt)\n\n    # Create and store 2 grandchild prompts for the child prompt\n    grandchild_hashes = []\n    for index in range(2):\n        grandchild_prompt = chat.init_prompt(f\"Grandchild question {index}\")\n        grandchild_prompt.parent = child_prompt.hash\n        response_str = _create_response_str(f\"chatcmpl-grandchild{index}\", 1677649402 + index,\n                                            f\"Grandchild answer {index}\")\n        grandchild_prompt.set_response(response_str)\n        store.store_prompt(grandchild_prompt)\n        grandchild_hashes.append(grandchild_prompt.hash)\n\n    # Test selecting topics\n    topics = store.select_topics(0, 5)\n    assert len(topics) == 1\n    assert topics[0]['root_prompt'].hash == root_prompt.hash\n\n    # Test selecting recent prompts within the nested topic\n    recent_prompts = store.select_prompts(1, 3, topic=root_prompt.hash)\n    assert len(recent_prompts) == 2\n    assert recent_prompts[0].hash == grandchild_hashes[0]\n    assert recent_prompts[1].hash == child_prompt.hash", "\n\n@pytest.fixture(name=\"prompt_tree\", scope=\"function\")\ndef create_prompt_tree(chat_store):\n    chat, store = chat_store\n\n    # Create and store a root prompt\n    root_prompt = chat.init_prompt(\"Root question\")\n    root_response_str = _create_response_str(\"chatcmpl-root\", 1677649400, \"Root answer\")\n    root_prompt.set_response(root_response_str)\n    store.store_prompt(root_prompt)\n\n    # Create and store a child prompt for the root prompt\n    child_prompt = chat.init_prompt(\"Child question\")\n    child_prompt.parent = root_prompt.hash\n    child_response_str = _create_response_str(\"chatcmpl-child\", 1677649400 + 1, \"Child answer\")\n    child_prompt.set_response(child_response_str)\n    store.store_prompt(child_prompt)\n\n    # Create and store a grandchild prompt for the child prompt\n    grandchild_prompt = chat.init_prompt(\"Grandchild question\")\n    grandchild_prompt.parent = child_prompt.hash\n    grandchild_response_str = _create_response_str(\"chatcmpl-grandchild\", 1677649400 + 2,\n                                                   \"Grandchild answer\")\n    grandchild_prompt.set_response(grandchild_response_str)\n    store.store_prompt(grandchild_prompt)\n\n    # Create and store another root prompt\n    other_root_prompt = chat.init_prompt(\"Other root question\")\n    other_root_response_str = _create_response_str(\"chatcmpl-other-root\", 1677649400 + 3,\n                                                   \"Other root answer\")\n    other_root_prompt.set_response(other_root_response_str)\n    store.store_prompt(other_root_prompt)\n\n    return store, root_prompt, child_prompt, grandchild_prompt, other_root_prompt", "\n\ndef test_delete_prompt_child(prompt_tree):\n    store, _, child_prompt, _, _ = prompt_tree\n\n    # Test deleting the child prompt\n    assert not store.delete_prompt(child_prompt.hash)\n\n\ndef test_delete_prompt_grandchild(prompt_tree):\n    store, root_prompt, _, grandchild_prompt, other_root_prompt = prompt_tree\n\n    # Test deleting the grandchild prompt\n    assert store.delete_prompt(grandchild_prompt.hash)\n\n    # Verify the trees after deletion\n    topics = store.select_topics(0, 5)\n    assert len(topics) == 2\n    assert topics[0]['root_prompt'].hash == other_root_prompt.hash\n    assert topics[1]['root_prompt'].hash == root_prompt.hash", "\ndef test_delete_prompt_grandchild(prompt_tree):\n    store, root_prompt, _, grandchild_prompt, other_root_prompt = prompt_tree\n\n    # Test deleting the grandchild prompt\n    assert store.delete_prompt(grandchild_prompt.hash)\n\n    # Verify the trees after deletion\n    topics = store.select_topics(0, 5)\n    assert len(topics) == 2\n    assert topics[0]['root_prompt'].hash == other_root_prompt.hash\n    assert topics[1]['root_prompt'].hash == root_prompt.hash", "\n\ndef test_delete_prompt_other_root(prompt_tree):\n    store, root_prompt, _, _, other_root_prompt = prompt_tree\n\n    # Test deleting the other root prompt\n    assert store.delete_prompt(other_root_prompt.hash)\n\n    # Verify the trees after deletion\n    topics = store.select_topics(0, 5)\n    assert len(topics) == 1\n    assert topics[0]['root_prompt'].hash == root_prompt.hash", ""]}
{"filename": "tests/test_utils.py", "chunked_list": ["import pytest\nfrom devchat.utils import parse_files\n\n\ndef test_parse_files_empty_input():\n    assert not parse_files([])\n\n\ndef test_parse_files_nonexistent_file():\n    with pytest.raises(ValueError, match=\"File .* does not exist.\"):\n        parse_files([\"nonexistent_file.txt\"])", "def test_parse_files_nonexistent_file():\n    with pytest.raises(ValueError, match=\"File .* does not exist.\"):\n        parse_files([\"nonexistent_file.txt\"])\n\n\ndef test_parse_files_empty_file(tmpdir):\n    empty_file = tmpdir.join(\"empty_file.txt\")\n    empty_file.write(\"\")\n\n    with pytest.raises(ValueError, match=\"File .* is empty.\"):\n        parse_files([str(empty_file)])", "\n\ndef test_parse_files_single_file(tmpdir):\n    file1 = tmpdir.join(\"file1.txt\")\n    file1.write(\"Hello, World!\")\n\n    assert parse_files([str(file1)]) == [\"Hello, World!\"]\n\n\ndef test_parse_files_multiple_files(tmpdir):\n    file1 = tmpdir.join(\"file1.txt\")\n    file1.write(\"Hello, World!\")\n    file2 = tmpdir.join(\"file2.txt\")\n    file2.write(\"This is a test.\")\n\n    assert parse_files([str(file1), str(file2)]) == [\"Hello, World!\", \"This is a test.\"]", "\ndef test_parse_files_multiple_files(tmpdir):\n    file1 = tmpdir.join(\"file1.txt\")\n    file1.write(\"Hello, World!\")\n    file2 = tmpdir.join(\"file2.txt\")\n    file2.write(\"This is a test.\")\n\n    assert parse_files([str(file1), str(file2)]) == [\"Hello, World!\", \"This is a test.\"]\n\n\ndef test_parse_files_invalid_path(tmpdir):\n    file1 = tmpdir.join(\"file1.txt\")\n    file1.write(\"Hello, World!\")\n    invalid_path = \"invalid/path/file2.txt\"\n\n    with pytest.raises(ValueError, match=\"File .* does not exist.\"):\n        parse_files(f\"{file1},{invalid_path}\")", "\n\ndef test_parse_files_invalid_path(tmpdir):\n    file1 = tmpdir.join(\"file1.txt\")\n    file1.write(\"Hello, World!\")\n    invalid_path = \"invalid/path/file2.txt\"\n\n    with pytest.raises(ValueError, match=\"File .* does not exist.\"):\n        parse_files(f\"{file1},{invalid_path}\")\n", ""]}
{"filename": "tests/conftest.py", "chunked_list": ["import os\nimport shutil\nimport tempfile\nimport pytest\nfrom git import Repo\n\n\n@pytest.fixture(name='git_repo', scope='module')\ndef fixture_git_repo(request):\n    # Create a temporary directory\n    repo_dir = tempfile.mkdtemp()\n\n    # Initialize a new Git repository in the temporary directory\n    Repo.init(repo_dir)\n\n    # Change the current working directory to the temporary directory\n    prev_cwd = os.getcwd()\n    os.chdir(repo_dir)\n\n    # Add a cleanup function to remove the temporary directory after the test\n    def cleanup():\n        os.chdir(prev_cwd)\n        shutil.rmtree(repo_dir)\n\n    request.addfinalizer(cleanup)\n\n    return repo_dir", "def fixture_git_repo(request):\n    # Create a temporary directory\n    repo_dir = tempfile.mkdtemp()\n\n    # Initialize a new Git repository in the temporary directory\n    Repo.init(repo_dir)\n\n    # Change the current working directory to the temporary directory\n    prev_cwd = os.getcwd()\n    os.chdir(repo_dir)\n\n    # Add a cleanup function to remove the temporary directory after the test\n    def cleanup():\n        os.chdir(prev_cwd)\n        shutil.rmtree(repo_dir)\n\n    request.addfinalizer(cleanup)\n\n    return repo_dir", ""]}
{"filename": "devchat/store.py", "chunked_list": ["from dataclasses import asdict\nimport os\nfrom typing import List, Dict, Any, Optional\nfrom xml.etree.ElementTree import ParseError\nimport networkx as nx\nfrom tinydb import TinyDB, where, Query\nfrom tinydb.table import Table\nfrom devchat.chat import Chat\nfrom devchat.prompt import Prompt\nfrom devchat.utils import get_logger", "from devchat.prompt import Prompt\nfrom devchat.utils import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass Store:\n    def __init__(self, store_dir: str, chat: Chat):\n        \"\"\"\n        Initializes a Store instance.\n\n        Args:\n            store_dir (str): The folder to store the files containing the store.\n            chat (Chat): The Chat instance.\n        \"\"\"\n        store_dir = os.path.expanduser(store_dir)\n        if not os.path.isdir(store_dir):\n            os.makedirs(store_dir)\n\n        self._graph_path = os.path.join(store_dir, 'prompts.graphml')\n        self._db_path = os.path.join(store_dir, 'prompts.json')\n        self._chat = chat\n\n        if os.path.isfile(self._graph_path):\n            try:\n                self._graph = nx.read_graphml(self._graph_path)\n            except ParseError as error:\n                raise ValueError(f\"Invalid file format for graph: {self._graph_path}\") from error\n        else:\n            self._graph = nx.DiGraph()\n\n        self._db = TinyDB(self._db_path)\n        self._db_meta = self._migrate_db()\n        self._topics_table = self._db.table('topics')\n\n        if not self._topics_table or not self._topics_table.all():\n            self._initialize_topics_table()\n\n    def _migrate_db(self) -> Table:\n        \"\"\"\n        Migrate the database to the latest version.\n        \"\"\"\n        metadata = self._db.table('metadata')\n\n        result = metadata.get(where('version').exists())\n        if not result or result['version'].startswith('0.1.'):\n            def replace_response():\n                def transform(doc):\n                    if '_new_messages' not in doc or 'response' not in doc['_new_messages']:\n                        logger.error(\"Prompt %s does not match '_new_messages.response'\",\n                                     doc['_hash'])\n                    doc['_new_messages']['responses'] = doc['_new_messages'].pop('response')\n                return transform\n\n            logger.info(\"Migrating database from %s to 0.2.0\", result)\n            self._db.update(replace_response(),\n                            Query()._new_messages.response.exists())  # pylint: disable=W0212\n            metadata.insert({'version': '0.2.0'})\n        return metadata\n\n    def _initialize_topics_table(self):\n        roots = [node for node in self._graph.nodes() if self._graph.out_degree(node) == 0]\n        for root in roots:\n            ancestors = nx.ancestors(self._graph, root)\n            if not ancestors:\n                latest_time = self._graph.nodes[root]['timestamp']\n            else:\n                latest_time = max(self._graph.nodes[node]['timestamp'] for node in ancestors)\n            self._topics_table.insert({\n                'root': root,\n                'latest_time': latest_time,\n                'title': None,\n                'hidden': False\n            })\n\n    def _update_topics_table(self, prompt: Prompt):\n        if self._graph.in_degree(prompt.hash):\n            logger.error(\"Prompt %s not a leaf to update topics table\", prompt.hash)\n\n        if prompt.parent:\n            for topic in self._topics_table.all():\n                if topic['root'] not in self._graph:\n                    self._graph.add_node(topic['root'], timestamp=topic['latest_time'])\n                    logger.warning(\"Topic %s not found in graph but added\", topic['root'])\n                if prompt.parent == topic['root'] or \\\n                        prompt.parent in nx.ancestors(self._graph, topic['root']):\n                    topic['latest_time'] = max(topic['latest_time'], prompt.timestamp)\n                    self._topics_table.update(topic, doc_ids=[topic.doc_id])\n                    break\n        else:\n            self._topics_table.insert({\n                'root': prompt.hash,\n                'latest_time': prompt.timestamp,\n                'title': None,\n                'hidden': False\n            })\n\n    def store_prompt(self, prompt: Prompt):\n        \"\"\"\n        Store a prompt in the store.\n\n        Args:\n            prompt (Prompt): The prompt to store.\n        \"\"\"\n        prompt.finalize_hash()\n\n        # Store the prompt object in TinyDB\n        self._db.insert(asdict(prompt))\n\n        # Add the prompt to the graph\n        self._graph.add_node(prompt.hash, timestamp=prompt.timestamp)\n\n        # Add edges for parents and references\n        if prompt.parent:\n            if prompt.parent not in self._graph:\n                logger.error(\"Parent %s not found while Prompt %s is stored to graph store.\",\n                             prompt.parent, prompt.hash)\n            else:\n                self._graph.add_edge(prompt.hash, prompt.parent)\n\n        self._update_topics_table(prompt)\n\n        for reference_hash in prompt.references:\n            if reference_hash not in self._graph:\n                logger.error(\"Reference %s not found while Prompt %s is stored to graph store.\",\n                             reference_hash, prompt.hash)\n\n        nx.write_graphml(self._graph, self._graph_path)\n\n    def get_prompt(self, prompt_hash: str) -> Prompt:\n        \"\"\"\n        Retrieve a prompt from the store.\n\n        Args:\n            prompt_hash (str): The hash of the prompt to retrieve.\n        Returns:\n            Prompt: The retrieved prompt. None if the prompt is not found.\n        \"\"\"\n        if prompt_hash not in self._graph:\n            logger.warning(\"Prompt %s not found while retrieving from graph store.\", prompt_hash)\n            return None\n\n        # Retrieve the prompt object from TinyDB\n        prompt_data = self._db.search(where('_hash') == prompt_hash)\n        if not prompt_data:\n            logger.warning(\"Prompt %s not found while retrieving from object store.\", prompt_hash)\n            return None\n        assert len(prompt_data) == 1\n        return self._chat.load_prompt(prompt_data[0])\n\n    def select_prompts(self, start: int, end: int, topic: Optional[str] = None) -> List[Prompt]:\n        \"\"\"\n        Select recent prompts in reverse chronological order.\n\n        Args:\n            start (int): The start index.\n            end (int): The end index (excluded).\n            topic (Optional[str]): The hash of the root prompt of the topic.\n                If set, select among the prompts of the topic.\n        Returns:\n            List[Prompt]: The list of prompts selected.\n                If end is greater than the number of all prompts,\n                the list will contain prompts from start to the end of the list.\n        \"\"\"\n        if topic:\n            ancestors = nx.ancestors(self._graph, topic)\n            nodes_with_data = [(node, self._graph.nodes[node]) for node in ancestors] + \\\n                [(topic, self._graph.nodes[topic])]\n            sorted_nodes = sorted(nodes_with_data, key=lambda x: x[1]['timestamp'], reverse=True)\n        else:\n            sorted_nodes = sorted(self._graph.nodes(data=True),\n                                  key=lambda x: x[1]['timestamp'],\n                                  reverse=True)\n\n        prompts = []\n        for node in sorted_nodes[start:end]:\n            prompt = self.get_prompt(node[0])\n            if not prompt:\n                logger.error(\"Prompt %s not found while selecting from the store\", node[0])\n                continue\n            prompts.append(prompt)\n        return prompts\n\n    def select_topics(self, start: int, end: int) -> List[Dict[str, Any]]:\n        \"\"\"\n        Select recent topics in reverse chronological order.\n\n        Args:\n            start (int): The start index.\n            end (int): The end index (excluded).\n\n        Returns:\n            List[Dict[str, Any]]: A list of dictionaries containing root prompts\n                with latest_time, and title fields.\n        \"\"\"\n        visible_topics = self._topics_table.search(\n            where('hidden') == False)  # pylint: disable=C0121\n        sorted_topics = sorted(visible_topics, key=lambda x: x['latest_time'], reverse=True)\n\n        topics = []\n        for topic in sorted_topics[start:end]:\n            prompt = self.get_prompt(topic['root'])\n            if not prompt:\n                logger.error(\"Topic %s not found while selecting from the store\", topic['root'])\n                continue\n            topics.append({\n                'root_prompt': prompt,\n                'latest_time': topic['latest_time'],\n                'title': topic['title'],\n                'hidden': topic['hidden'],\n            })\n        return topics\n\n    def delete_prompt(self, prompt_hash: str) -> bool:\n        \"\"\"\n        Delete a prompt from the store if it is a leaf.\n\n        Args:\n            prompt_hash (str): The hash of the prompt to delete.\n\n        Returns:\n            bool: True if the prompt is successfully deleted, False otherwise.\n        \"\"\"\n        # Check if the prompt is a leaf\n        if self._graph.in_degree(prompt_hash) != 0:\n            logger.error(\"Prompt %s is not a leaf, cannot be deleted.\", prompt_hash)\n            return False\n\n        # Remove the prompt from the graph\n        self._graph.remove_node(prompt_hash)\n\n        # Update the topics table\n        self._topics_table.remove(where('root') == prompt_hash)\n\n        # Remove the prompt from the database\n        self._db.remove(where('_hash') == prompt_hash)\n\n        # Save the graph\n        nx.write_graphml(self._graph, self._graph_path)\n\n        return True\n\n    @property\n    def graph_path(self) -> str:\n        \"\"\"\n        The path to the graph store file.\n        \"\"\"\n        return self._graph_path\n\n    @property\n    def db_path(self) -> str:\n        \"\"\"\n        The path to the object store file.\n        \"\"\"\n        return self._db_path", ""]}
{"filename": "devchat/assistant.py", "chunked_list": ["from typing import Optional, List, Iterator\nfrom devchat.message import Message\nfrom devchat.chat import Chat\nfrom devchat.store import Store\nfrom devchat.utils import get_logger\n\n\nlogger = get_logger(__name__)\n\n\nclass Assistant:\n    def __init__(self, chat: Chat, store: Store):\n        \"\"\"\n        Initializes an Assistant object.\n\n        Args:\n            chat (Chat): A Chat object used to communicate with chat APIs.\n        \"\"\"\n        self._chat = chat\n        self._store = store\n        self._prompt = None\n        self.token_limit = 3000\n\n    @property\n    def available_tokens(self) -> int:\n        return self.token_limit - self._prompt.request_tokens\n\n    def _check_limit(self):\n        if self._prompt.request_tokens > self.token_limit:\n            raise ValueError(f\"Prompt tokens {self._prompt.request_tokens} \"\n                             f\"beyond limit {self.token_limit}.\")\n\n    def make_prompt(self, request: str,\n                    instruct_contents: Optional[List[str]], context_contents: Optional[List[str]],\n                    functions: Optional[List[dict]],\n                    parent: Optional[str] = None, references: Optional[List[str]] = None,\n                    function_name: Optional[str] = None):\n        \"\"\"\n        Make a prompt for the chat API.\n\n        Args:\n            request (str): The user request.\n            instruct_contents (Optional[List[str]]): A list of instructions to the prompt.\n            context_contents (Optional[List[str]]): A list of context messages to the prompt.\n            parent (Optional[str]): The parent prompt hash. None means a new topic.\n            references (Optional[List[str]]): The reference prompt hashes.\n        \"\"\"\n        self._prompt = self._chat.init_prompt(request, function_name=function_name)\n        self._check_limit()\n        # Add instructions to the prompt\n        if instruct_contents:\n            combined_instruct = ''.join(instruct_contents)\n            self._prompt.append_new(Message.INSTRUCT, combined_instruct)\n            self._check_limit()\n        # Add context to the prompt\n        if context_contents:\n            for context_content in context_contents:\n                self._prompt.append_new(Message.CONTEXT, context_content)\n                self._check_limit()\n        # Add functions to the prompt\n        if functions:\n            self._prompt.set_functions(functions)\n            self._check_limit()\n\n        # Add history to the prompt\n        for reference_hash in references:\n            prompt = self._store.get_prompt(reference_hash)\n            if not prompt:\n                logger.error(\"Reference %s not retrievable while making prompt.\", reference_hash)\n                continue\n            self._prompt.references.append(reference_hash)\n            self._prompt.prepend_history(prompt, self.token_limit)\n        if parent:\n            self._prompt.parent = parent\n            parent_hash = parent\n            while parent_hash:\n                parent_prompt = self._store.get_prompt(parent_hash)\n                if not parent_prompt:\n                    logger.error(\"Parent %s not retrievable while making prompt.\", parent_hash)\n                    break\n                if not self._prompt.prepend_history(parent_prompt, self.token_limit):\n                    break\n                parent_hash = parent_prompt.parent\n\n    def iterate_response(self) -> Iterator[str]:\n        \"\"\"Get an iterator of response strings from the chat API.\n\n        Returns:\n            Iterator[str]: An iterator over response strings from the chat API.\n        \"\"\"\n        if self._chat.config.stream:\n            first_chunk = True\n            for chunk in self._chat.stream_response(self._prompt):\n                delta = self._prompt.append_response(str(chunk))\n                if first_chunk:\n                    first_chunk = False\n                    yield self._prompt.formatted_header()\n                yield delta\n            self._store.store_prompt(self._prompt)\n            yield self._prompt.formatted_footer(0) + '\\n'\n            for index in range(1, len(self._prompt.responses)):\n                yield self._prompt.formatted_full_response(index) + '\\n'\n        else:\n            response_str = self._chat.complete_response(self._prompt)\n            self._prompt.set_response(response_str)\n            self._store.store_prompt(self._prompt)\n            for index in range(len(self._prompt.responses)):\n                yield self._prompt.formatted_full_response(index) + '\\n'", "\n\nclass Assistant:\n    def __init__(self, chat: Chat, store: Store):\n        \"\"\"\n        Initializes an Assistant object.\n\n        Args:\n            chat (Chat): A Chat object used to communicate with chat APIs.\n        \"\"\"\n        self._chat = chat\n        self._store = store\n        self._prompt = None\n        self.token_limit = 3000\n\n    @property\n    def available_tokens(self) -> int:\n        return self.token_limit - self._prompt.request_tokens\n\n    def _check_limit(self):\n        if self._prompt.request_tokens > self.token_limit:\n            raise ValueError(f\"Prompt tokens {self._prompt.request_tokens} \"\n                             f\"beyond limit {self.token_limit}.\")\n\n    def make_prompt(self, request: str,\n                    instruct_contents: Optional[List[str]], context_contents: Optional[List[str]],\n                    functions: Optional[List[dict]],\n                    parent: Optional[str] = None, references: Optional[List[str]] = None,\n                    function_name: Optional[str] = None):\n        \"\"\"\n        Make a prompt for the chat API.\n\n        Args:\n            request (str): The user request.\n            instruct_contents (Optional[List[str]]): A list of instructions to the prompt.\n            context_contents (Optional[List[str]]): A list of context messages to the prompt.\n            parent (Optional[str]): The parent prompt hash. None means a new topic.\n            references (Optional[List[str]]): The reference prompt hashes.\n        \"\"\"\n        self._prompt = self._chat.init_prompt(request, function_name=function_name)\n        self._check_limit()\n        # Add instructions to the prompt\n        if instruct_contents:\n            combined_instruct = ''.join(instruct_contents)\n            self._prompt.append_new(Message.INSTRUCT, combined_instruct)\n            self._check_limit()\n        # Add context to the prompt\n        if context_contents:\n            for context_content in context_contents:\n                self._prompt.append_new(Message.CONTEXT, context_content)\n                self._check_limit()\n        # Add functions to the prompt\n        if functions:\n            self._prompt.set_functions(functions)\n            self._check_limit()\n\n        # Add history to the prompt\n        for reference_hash in references:\n            prompt = self._store.get_prompt(reference_hash)\n            if not prompt:\n                logger.error(\"Reference %s not retrievable while making prompt.\", reference_hash)\n                continue\n            self._prompt.references.append(reference_hash)\n            self._prompt.prepend_history(prompt, self.token_limit)\n        if parent:\n            self._prompt.parent = parent\n            parent_hash = parent\n            while parent_hash:\n                parent_prompt = self._store.get_prompt(parent_hash)\n                if not parent_prompt:\n                    logger.error(\"Parent %s not retrievable while making prompt.\", parent_hash)\n                    break\n                if not self._prompt.prepend_history(parent_prompt, self.token_limit):\n                    break\n                parent_hash = parent_prompt.parent\n\n    def iterate_response(self) -> Iterator[str]:\n        \"\"\"Get an iterator of response strings from the chat API.\n\n        Returns:\n            Iterator[str]: An iterator over response strings from the chat API.\n        \"\"\"\n        if self._chat.config.stream:\n            first_chunk = True\n            for chunk in self._chat.stream_response(self._prompt):\n                delta = self._prompt.append_response(str(chunk))\n                if first_chunk:\n                    first_chunk = False\n                    yield self._prompt.formatted_header()\n                yield delta\n            self._store.store_prompt(self._prompt)\n            yield self._prompt.formatted_footer(0) + '\\n'\n            for index in range(1, len(self._prompt.responses)):\n                yield self._prompt.formatted_full_response(index) + '\\n'\n        else:\n            response_str = self._chat.complete_response(self._prompt)\n            self._prompt.set_response(response_str)\n            self._store.store_prompt(self._prompt)\n            for index in range(len(self._prompt.responses)):\n                yield self._prompt.formatted_full_response(index) + '\\n'", ""]}
{"filename": "devchat/_cli.py", "chunked_list": ["\"\"\"\nThis module contains the main function for the DevChat CLI.\n\"\"\"\nfrom contextlib import contextmanager\nimport json\nimport os\nimport sys\nfrom typing import List, Optional, Tuple\nimport importlib.metadata\nimport rich_click as click", "import importlib.metadata\nimport rich_click as click\nfrom devchat.store import Store\nfrom devchat.openai import OpenAIChatConfig, OpenAIChat\nfrom devchat.assistant import Assistant\nfrom devchat.utils import find_root_dir, git_ignore, parse_files\nfrom devchat.utils import setup_logger, get_logger\n\nlogger = get_logger(__name__)\nclick.rich_click.USE_MARKDOWN = True", "logger = get_logger(__name__)\nclick.rich_click.USE_MARKDOWN = True\n\n\n@click.group()\n@click.version_option(importlib.metadata.version(\"devchat\"), '--version',\n                      message='DevChat %(version)s')\ndef main():\n    \"\"\"DevChat CLI: A command-line interface for DevChat.\"\"\"\n", "\n\n@contextmanager\ndef handle_errors():\n    \"\"\"Handle errors in the CLI.\"\"\"\n    try:\n        yield\n    except Exception as error:\n        logger.exception(error)\n        click.echo(f\"Error: {error}\", err=True)\n        sys.exit(os.EX_SOFTWARE)", "\n\ndef init_dir() -> Tuple[dict, str]:\n    root_dir = find_root_dir()\n    if not root_dir:\n        click.echo(\"Error: Failed to find home to store .chat\", err=True)\n        sys.exit(os.EX_DATAERR)\n    chat_dir = os.path.join(root_dir, \".chat\")\n    if not os.path.exists(chat_dir):\n        os.makedirs(chat_dir)\n\n    default_config_data = {\n        \"model\": \"gpt-4\",\n        \"tokens-per-prompt\": 6000,\n        \"provider\": \"OpenAI\",\n        \"OpenAI\": {\n            \"temperature\": 0,\n            \"stream\": True\n        }\n    }\n\n    try:\n        with open(os.path.join(chat_dir, 'config.json'), 'r', encoding='utf-8') as file:\n            config_data = json.load(file)\n    except Exception:\n        config_data = default_config_data\n\n    setup_logger(os.path.join(chat_dir, 'error.log'))\n    git_ignore(chat_dir, '*')\n    return config_data, chat_dir", "\n\n@main.command()\n@click.argument('content', required=False)\n@click.option('-p', '--parent', help='Input the parent prompt hash to continue the conversation.')\n@click.option('-r', '--reference', multiple=True,\n              help='Input one or more specific previous prompts to include in the current prompt.')\n@click.option('-i', '--instruct', multiple=True,\n              help='Add one or more files to the prompt as instructions.')\n@click.option('-c', '--context', multiple=True,", "              help='Add one or more files to the prompt as instructions.')\n@click.option('-c', '--context', multiple=True,\n              help='Add one or more files to the prompt as a context.')\n@click.option('-m', '--model', help='Specify the model to temporarily use for the prompt '\n              '(prefer to modify .chat/config.json).')\n@click.option('--config', 'config_str',\n              help='Specify a JSON string to overwrite the configuration for this prompt.')\n@click.option('-f', '--functions', type=click.Path(exists=True),\n              help='Path to a JSON file with functions for the prompt.')\n@click.option('-n', '--function-name',", "              help='Path to a JSON file with functions for the prompt.')\n@click.option('-n', '--function-name',\n              help='Specify the function name when the content is the output of a function.')\ndef prompt(content: Optional[str], parent: Optional[str], reference: Optional[List[str]],\n           instruct: Optional[List[str]], context: Optional[List[str]],\n           model: Optional[str], config_str: Optional[str] = None,\n           functions: Optional[str] = None, function_name: Optional[str] = None):\n    \"\"\"\n    Main function to run the chat application.\n\n    This function initializes the chat system based on the specified large language model (LLM),\n    and performs interactions with the LLM by sending prompts and retrieving responses.\n\n    Examples\n    --------\n\n    To send a single-line message to the LLM, provide the content as an argument:\n\n    ```bash\n    devchat prompt \"What is the capital of France?\"\n    ```\n\n    To send a multi-line message to the LLM, use the here-doc syntax:\n\n    ```bash\n    devchat prompt << 'EOF'\n    What is the capital of France?\n    Can you tell me more about its history?\n    EOF\n    ```\n\n    Note the quotes around EOF in the first line, to prevent the shell from expanding variables.\n\n    Configuration\n    -------------\n\n    DevChat CLI reads its configuration from `.chat/config.json`\n    in your current Git or SVN root directory.\n    If the file is not found, it uses the following default configuration:\n    ```json\n    {\n        \"model\": \"gpt-4\",\n        \"tokens-per-prompt\": 6000,\n        \"provider\": \"OpenAI\",\n        \"OpenAI\": {\n            \"temperature\": 0,\n            \"stream\": true\n        }\n    }\n    ```\n\n    To customize the configuration, create `.chat/config.json`\n    in your current Git or SVN root directory\n    and modify the settings as needed.\n\n    Note: To use OpenAI's APIs, you must have an API key to run the CLI.\n    Run the following command line with your API key:\n\n    ```bash\n    export OPENAI_API_KEY=\"sk-...\"\n    ```\n\n    \"\"\"\n    config, chat_dir = init_dir()\n\n    with handle_errors():\n        if content is None:\n            content = click.get_text_stream('stdin').read()\n\n        if content == '':\n            return\n\n        instruct_contents = parse_files(instruct)\n        context_contents = parse_files(context)\n\n        provider = config.get('provider')\n        if provider == 'OpenAI':\n            if model is None:\n                model = config['model']\n\n            if config_str is not None:\n                config_json = json.loads(config_str)\n                config['OpenAI'].update(config_json)\n\n            openai_config = OpenAIChatConfig(model=model,\n                                             **config['OpenAI'])\n\n            chat = OpenAIChat(openai_config)\n            store = Store(chat_dir, chat)\n\n            assistant = Assistant(chat, store)\n            if 'tokens-per-prompt' in config:\n                assistant.token_limit = config['tokens-per-prompt']\n\n            functions_data = None\n            if functions is not None:\n                with open(functions, 'r', encoding=\"utf-8\") as f_file:\n                    functions_data = json.load(f_file)\n            assistant.make_prompt(content, instruct_contents, context_contents,\n                                  functions_data, parent, reference,\n                                  function_name=function_name)\n\n            for response in assistant.iterate_response():\n                click.echo(response, nl=False)\n        else:\n            click.echo(f\"Error: Invalid LLM in configuration '{provider}'\", err=True)\n            sys.exit(os.EX_DATAERR)", "\n\n@main.command()\n@click.option('--skip', default=0, help='Skip number prompts before showing the prompt history.')\n@click.option('-n', '--max-count', default=1, help='Limit the number of commits to output.')\n@click.option('-t', '--topic', 'topic_root', default=None,\n              help='Hash of the root prompt of the topic to select prompts from.')\n@click.option('--delete', default=None, help='Delete a leaf prompt from the log.')\ndef log(skip, max_count, topic_root, delete):\n    \"\"\"\n    Manage the prompt history.\n    \"\"\"\n    if delete and (skip != 0 or max_count != 1 or topic_root is not None):\n        click.echo(\"Error: The --delete option cannot be used with other options.\", err=True)\n        sys.exit(os.EX_USAGE)\n\n    config, chat_dir = init_dir()\n\n    with handle_errors():\n        provider = config.get('provider')\n        if provider == 'OpenAI':\n            openai_config = OpenAIChatConfig(model=config['model'], **config['OpenAI'])\n            chat = OpenAIChat(openai_config)\n            store = Store(chat_dir, chat)\n        else:\n            click.echo(f\"Error: Invalid LLM in configuration '{provider}'\", err=True)\n            sys.exit(os.EX_DATAERR)\n\n        if delete:\n            success = store.delete_prompt(delete)\n            if success:\n                click.echo(f\"Prompt {delete} deleted successfully.\")\n            else:\n                click.echo(f\"Failed to delete prompt {delete}.\")\n        else:\n            recent_prompts = store.select_prompts(skip, skip + max_count, topic_root)\n            logs = []\n            for record in recent_prompts:\n                try:\n                    logs.append(record.shortlog())\n                except Exception as exc:\n                    logger.exception(exc)\n                    continue\n            click.echo(json.dumps(logs, indent=2))", "def log(skip, max_count, topic_root, delete):\n    \"\"\"\n    Manage the prompt history.\n    \"\"\"\n    if delete and (skip != 0 or max_count != 1 or topic_root is not None):\n        click.echo(\"Error: The --delete option cannot be used with other options.\", err=True)\n        sys.exit(os.EX_USAGE)\n\n    config, chat_dir = init_dir()\n\n    with handle_errors():\n        provider = config.get('provider')\n        if provider == 'OpenAI':\n            openai_config = OpenAIChatConfig(model=config['model'], **config['OpenAI'])\n            chat = OpenAIChat(openai_config)\n            store = Store(chat_dir, chat)\n        else:\n            click.echo(f\"Error: Invalid LLM in configuration '{provider}'\", err=True)\n            sys.exit(os.EX_DATAERR)\n\n        if delete:\n            success = store.delete_prompt(delete)\n            if success:\n                click.echo(f\"Prompt {delete} deleted successfully.\")\n            else:\n                click.echo(f\"Failed to delete prompt {delete}.\")\n        else:\n            recent_prompts = store.select_prompts(skip, skip + max_count, topic_root)\n            logs = []\n            for record in recent_prompts:\n                try:\n                    logs.append(record.shortlog())\n                except Exception as exc:\n                    logger.exception(exc)\n                    continue\n            click.echo(json.dumps(logs, indent=2))", "\n\n@main.command()\n@click.option('--list', '-l', 'list_topics', is_flag=True,\n              help='List topics in reverse chronological order.')\n@click.option('--skip', default=0, help='Skip number of topics before showing the list.')\n@click.option('-n', '--max-count', default=100, help='Limit the number of topics to output.')\ndef topic(list_topics: bool, skip: int, max_count: int):\n    \"\"\"\n    Manage topics.\n    \"\"\"\n    config, chat_dir = init_dir()\n\n    with handle_errors():\n        provider = config.get('provider')\n        if provider == 'OpenAI':\n            openai_config = OpenAIChatConfig(model=config['model'], **config['OpenAI'])\n            chat = OpenAIChat(openai_config)\n            store = Store(chat_dir, chat)\n        else:\n            click.echo(f\"Error: Invalid LLM in configuration '{provider}'\", err=True)\n            sys.exit(os.EX_DATAERR)\n\n        if list_topics:\n            topics = store.select_topics(skip, skip + max_count)\n            for topic_data in topics:\n                try:\n                    topic_data.update({'root_prompt': topic_data['root_prompt'].shortlog()})\n                except Exception as exc:\n                    logger.exception(exc)\n                    continue\n            click.echo(json.dumps(topics, indent=2))", ""]}
{"filename": "devchat/message.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Message(ABC):\n    \"\"\"\n    The basic unit of information in a prompt.\n    \"\"\"\n    content: str = \"\"\n\n    INSTRUCT = \"instruct\"\n    CONTEXT = \"context\"\n    FUNCTION = \"function\"\n    CHAT = \"chat\"\n\n    @abstractmethod\n    def to_dict(self) -> dict:\n        \"\"\"\n        Convert the message to a dictionary.\n        \"\"\"\n\n    @classmethod\n    @abstractmethod\n    def from_dict(cls, message_data: dict) -> 'Message':\n        \"\"\"\n        Convert the message from a dictionary.\n        \"\"\"\n\n    @abstractmethod\n    def stream_from_dict(self, message_data: dict) -> str:\n        \"\"\"\n        Append to the message from a dictionary returned from a streaming chat API.\n        \"\"\"", ""]}
{"filename": "devchat/__init__.py", "chunked_list": [""]}
{"filename": "devchat/utils.py", "chunked_list": ["import json\nimport logging\nimport os\nimport re\nimport getpass\nimport socket\nimport subprocess\nfrom typing import List, Tuple, Optional\nimport datetime\nimport hashlib", "import datetime\nimport hashlib\nimport tiktoken\n\nlog_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n\n\ndef setup_logger(file_path: Optional[str] = None):\n    \"\"\"Utility function to set up a global file log handler.\"\"\"\n    if file_path is None:\n        handler = logging.StreamHandler()\n    else:\n        handler = logging.FileHandler(file_path)\n    handler.setFormatter(log_formatter)\n    logging.root.handlers = [handler]", "\n\ndef get_logger(name: str = None, handler: logging.Handler = None) -> logging.Logger:\n    local_logger = logging.getLogger(name)\n\n    # Default to 'INFO' if 'LOG_LEVEL' env is not set\n    log_level_str = os.getenv('LOG_LEVEL', 'INFO')\n    log_level = getattr(logging, log_level_str.upper(), logging.INFO)\n    local_logger.setLevel(log_level)\n\n    # If a handler is provided, configure and add it to the logger\n    if handler is not None:\n        handler.setLevel(log_level)\n        handler.setFormatter(log_formatter)\n        local_logger.addHandler(handler)\n\n    local_logger.info(\"Get %s\", str(local_logger))\n    return local_logger", "\n\ndef find_root_dir() -> Optional[str]:\n    try:\n        result = subprocess.run([\"git\", \"rev-parse\", \"--show-toplevel\"],\n                                capture_output=True, text=True, check=True, encoding='utf-8')\n        return result.stdout.strip()\n    except Exception:\n        try:\n            result = subprocess.run([\"svn\", \"info\"],\n                                    capture_output=True, text=True, check=True, encoding='utf-8')\n            if result.returncode == 0:\n                for line in result.stdout.splitlines():\n                    if line.startswith(\"Working Copy Root Path: \"):\n                        return line.split(\"Working Copy Root Path: \", 1)[1].strip()\n        except Exception:\n            return os.path.expanduser(\"~\")\n    return None", "\n\ndef git_ignore(target_dir: str, *ignore_entries: str) -> None:\n    gitignore_path = os.path.join(target_dir, '.gitignore')\n\n    if os.path.exists(gitignore_path):\n        with open(gitignore_path, 'r', encoding='utf-8') as gitignore_file:\n            gitignore_content = gitignore_file.read()\n\n        new_entries = []\n        for entry in ignore_entries:\n            if entry not in gitignore_content:\n                new_entries.append(entry)\n\n        if new_entries:\n            with open(gitignore_path, 'a', encoding='utf-8') as gitignore_file:\n                gitignore_file.write('\\n# devchat\\n')\n                for entry in new_entries:\n                    gitignore_file.write(f'{entry}\\n')\n    else:\n        with open(gitignore_path, 'w', encoding='utf-8') as gitignore_file:\n            gitignore_file.write('# devchat\\n')\n            for entry in ignore_entries:\n                gitignore_file.write(f'{entry}\\n')", "\n\ndef unix_to_local_datetime(unix_time) -> datetime.datetime:\n    # Convert the Unix time to a naive datetime object in UTC\n    naive_dt = datetime.datetime.utcfromtimestamp(unix_time).replace(tzinfo=datetime.timezone.utc)\n\n    # Convert the UTC datetime object to the local timezone\n    local_dt = naive_dt.astimezone()\n\n    return local_dt", "\n\ndef get_user_info() -> Tuple[str, str]:\n    try:\n        cmd = ['git', 'config', 'user.name']\n        user_name = subprocess.check_output(cmd, encoding='utf-8').strip()\n    except Exception:\n        user_name = getpass.getuser()\n\n    try:\n        cmd = ['git', 'config', 'user.email']\n        user_email = subprocess.check_output(cmd, encoding='utf-8').strip()\n    except Exception:\n        user_email = user_name + '@' + socket.gethostname()\n\n    return user_name, user_email", "\n\ndef user_id(user_name, user_email) -> Tuple[str, str]:\n    user_str = f\"{user_name} <{user_email}>\"\n    user_hash = hashlib.sha1(user_str.encode('utf-8')).hexdigest()\n    return user_str, user_hash\n\n\ndef parse_files(file_paths: List[str]) -> List[str]:\n    if not file_paths:\n        return []\n\n    for file_path in file_paths:\n        file_path = os.path.expanduser(file_path.strip())\n        if not os.path.isfile(file_path):\n            raise ValueError(f\"File {file_path} does not exist.\")\n\n    contents = []\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            if not content:\n                raise ValueError(f\"File {file_path} is empty.\")\n            contents.append(content)\n    return contents", "def parse_files(file_paths: List[str]) -> List[str]:\n    if not file_paths:\n        return []\n\n    for file_path in file_paths:\n        file_path = os.path.expanduser(file_path.strip())\n        if not os.path.isfile(file_path):\n            raise ValueError(f\"File {file_path} does not exist.\")\n\n    contents = []\n    for file_path in file_paths:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            content = file.read()\n            if not content:\n                raise ValueError(f\"File {file_path} is empty.\")\n            contents.append(content)\n    return contents", "\n\ndef valid_hash(hash_str):\n    \"\"\"Check if a string is a valid hash value.\"\"\"\n    pattern = re.compile(r'^[a-f0-9]{64}$')  # for SHA-256 hash\n    return bool(pattern.match(hash_str))\n\n\ndef check_format(formatted_response) -> bool:\n    pattern = r\"(User: .+ <.+@.+>\\nDate: .+\\n\\n(?:.*\\n)*\\n(?:prompt [a-f0-9]{64}\\n\\n?)+)\"\n    return bool(re.fullmatch(pattern, formatted_response))", "def check_format(formatted_response) -> bool:\n    pattern = r\"(User: .+ <.+@.+>\\nDate: .+\\n\\n(?:.*\\n)*\\n(?:prompt [a-f0-9]{64}\\n\\n?)+)\"\n    return bool(re.fullmatch(pattern, formatted_response))\n\n\ndef get_content(formatted_response) -> str:\n    header_pattern = r\"User: .+ <.+@.+>\\nDate: .+\\n\\n\"\n    footer_pattern = r\"\\n(?:prompt [a-f0-9]{64}\\n\\n?)+\"\n\n    content = re.sub(header_pattern, \"\", formatted_response)\n    content = re.sub(footer_pattern, \"\", content)\n\n    return content", "\n\ndef get_prompt_hash(formatted_response) -> str:\n    if not check_format(formatted_response):\n        raise ValueError(\"Invalid formatted response.\")\n    footer_pattern = r\"\\n(?:prompt [a-f0-9]{64}\\n\\n?)+\"\n    # get the last prompt hash\n    prompt_hash = re.findall(footer_pattern, formatted_response)[-1].strip()\n    prompt_hash = prompt_hash.replace(\"prompt \", \"\")\n    return prompt_hash", "\n\ndef update_dict(dict_to_update, key, value) -> dict:\n    \"\"\"\n    Update a dictionary with a key-value pair and return the dictionary.\n    \"\"\"\n    dict_to_update[key] = value\n    return dict_to_update\n\n\ndef message_tokens(message: dict, model: str) -> int:\n    \"\"\"Returns the number of tokens used by a message.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError as err:\n        raise ValueError(f\"Invalid model {model} for tiktoken.\") from err\n\n    num_tokens = 0\n    if model == \"gpt-3.5-turbo-0301\":\n        num_tokens += 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n        tokens_per_name = -1  # if there's a name, the role is omitted\n    else:\n        num_tokens += 3\n        tokens_per_name = 1\n\n    for key, value in message.items():\n        if key == 'function_call':\n            value = json.dumps(value)\n        num_tokens += len(encoding.encode(value))\n        if key == \"name\":\n            num_tokens += tokens_per_name\n    return num_tokens", "\n\ndef message_tokens(message: dict, model: str) -> int:\n    \"\"\"Returns the number of tokens used by a message.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError as err:\n        raise ValueError(f\"Invalid model {model} for tiktoken.\") from err\n\n    num_tokens = 0\n    if model == \"gpt-3.5-turbo-0301\":\n        num_tokens += 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n        tokens_per_name = -1  # if there's a name, the role is omitted\n    else:\n        num_tokens += 3\n        tokens_per_name = 1\n\n    for key, value in message.items():\n        if key == 'function_call':\n            value = json.dumps(value)\n        num_tokens += len(encoding.encode(value))\n        if key == \"name\":\n            num_tokens += tokens_per_name\n    return num_tokens", "\n\ndef response_tokens(message: dict, model: str) -> int:\n    \"\"\"Returns the number of tokens used by a response.\"\"\"\n    return message_tokens(message, model)\n"]}
{"filename": "devchat/prompt.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom dataclasses import dataclass, field, asdict\nimport hashlib\nimport math\nfrom typing import Dict, List\nfrom devchat.message import Message\nfrom devchat.utils import unix_to_local_datetime, get_logger, user_id\n\n\nlogger = get_logger(__name__)", "\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass Prompt(ABC):\n    \"\"\"\n    A class to represent a prompt and its corresponding responses from the chat API.\n\n    Attributes:\n        model (str): The name of the language model.\n        user_name (str): The name of the user.\n        user_email (str): The email address of the user.\n        _new_messages (dict): The messages for the current round of conversation.\n        _history_messages (dict): The messages for the history of conversation.\n        parent (str): The parent prompt hash.\n        references (List[str]): The hashes of the referenced prompts.\n        _timestamp (int): The timestamp when the response was created.\n        _request_tokens (int): The number of tokens used in the request.\n        _response_tokens (int): The number of tokens used in the response.\n        _hash (str): The hash of the prompt.\n    \"\"\"\n\n    model: str\n    user_name: str\n    user_email: str\n    _new_messages: Dict = field(default_factory=lambda: {\n        Message.INSTRUCT: [],\n        'request': None,\n        Message.CONTEXT: [],\n        'responses': []\n    })\n    _history_messages: Dict[str, Message] = field(default_factory=lambda: {\n        Message.CONTEXT: [],\n        Message.CHAT: []\n    })\n    parent: str = None\n    references: List[str] = field(default_factory=list)\n    _timestamp: int = None\n    _request_tokens: int = 0\n    _response_tokens: int = 0\n    _response_reasons: List[str] = field(default_factory=list)\n    _hash: str = None\n\n    def _check_complete(self) -> bool:\n        \"\"\"\n        Check if the prompt is complete for hashing.\n\n        Returns:\n            bool: Whether the prompt is complete.\n        \"\"\"\n        if not self.request or not self._request_tokens or not self.responses:\n            logger.warning(\"Incomplete prompt: request = %s (%d), response = %s\",\n                           self.request, self._request_tokens, self.responses)\n            return False\n\n        if not self._response_tokens:\n            return False\n\n        return True\n\n    @property\n    def timestamp(self) -> int:\n        return self._timestamp\n\n    @property\n    def new_context(self) -> List[Message]:\n        return self._new_messages[Message.CONTEXT]\n\n    @property\n    def request(self) -> Message:\n        return self._new_messages['request']\n\n    @property\n    def responses(self) -> List[Message]:\n        return self._new_messages['responses']\n\n    @property\n    def request_tokens(self) -> int:\n        return self._request_tokens\n\n    @property\n    def response_tokens(self) -> int:\n        return self._response_tokens\n\n    @abstractmethod\n    def _count_response_tokens(self) -> int:\n        \"\"\"\n        Calculate the number of tokens used in the responses.\n        \"\"\"\n\n    @property\n    def hash(self) -> str:\n        return self._hash\n\n    @property\n    @abstractmethod\n    def messages(self) -> List[dict]:\n        \"\"\"\n        List of messages in the prompt to be sent to the chat API.\n        \"\"\"\n\n    @abstractmethod\n    def input_messages(self, messages: List[dict]):\n        \"\"\"\n        Input the messages from the chat API to new and history messages.\n        The message list should be generated by the `messages` property.\n\n        Args:\n            messages (List[dict]): The messages from the chat API.\n        \"\"\"\n\n    @abstractmethod\n    def append_new(self, message_type: str, content: str,\n                   available_tokens: int = math.inf) -> bool:\n        \"\"\"\n        Append a new message provided by the user to this prompt.\n\n        Args:\n            message_type (str): The type of the message.\n            content (str): The content of the message.\n            available_tokens (int): The number of tokens available for the message.\n\n        Returns:\n            bool: Whether the message is appended.\n        \"\"\"\n\n    @abstractmethod\n    def prepend_history(self, prompt: \"Prompt\", token_limit: int = math.inf) -> bool:\n        \"\"\"\n        Add the prompt to the beginning of the history messages.\n\n        Args:\n            prompt(Prompt): The prompt to prepend.\n            token_limit (int): The max number of tokens for this prompt.\n\n        Returns:\n            bool: Whether the message is prepended.\n        \"\"\"\n\n    @abstractmethod\n    def set_request(self, content: str):\n        \"\"\"\n        Set the request message for the prompt.\n\n        Args:\n            content (str): The request content to set.\n        \"\"\"\n\n    @abstractmethod\n    def set_response(self, response_str: str):\n        \"\"\"\n        Parse the API response string and set the Prompt object's attributes.\n\n        Args:\n            response_str (str): The JSON-formatted response string from the chat API.\n        \"\"\"\n\n    @abstractmethod\n    def append_response(self, delta_str: str) -> str:\n        \"\"\"\n        Append the content of a streaming response to the existing messages.\n\n        Args:\n            delta_str (str): The JSON-formatted delta string from the chat API.\n\n        Returns:\n            str: The delta content with index 0. None when the response is over.\n        \"\"\"\n\n    def finalize_hash(self) -> str:\n        \"\"\"\n        Calculate and set the hash of the prompt.\n\n        Returns:\n            str: The hash of the prompt. None if the prompt is incomplete.\n        \"\"\"\n        if not self._check_complete():\n            self._hash = None\n\n        if self._hash:\n            return self._hash\n\n        self._count_response_tokens()\n\n        data = asdict(self)\n        data.pop('_hash')\n        string = str(tuple(sorted(data.items())))\n        self._hash = hashlib.sha256(string.encode('utf-8')).hexdigest()\n        return self._hash\n\n    def formatted_header(self) -> str:\n        \"\"\"Formatted string header of the prompt.\"\"\"\n        formatted_str = f\"User: {user_id(self.user_name, self.user_email)[0]}\\n\"\n\n        if not self._timestamp:\n            raise ValueError(f\"Prompt lacks timestamp for formatting header: {self.request}\")\n\n        local_time = unix_to_local_datetime(self._timestamp)\n        formatted_str += f\"Date: {local_time.strftime('%a %b %d %H:%M:%S %Y %z')}\\n\\n\"\n\n        return formatted_str\n\n    def formatted_footer(self, index: int) -> str:\n        \"\"\"Formatted string footer of the prompt.\"\"\"\n        if not self.hash:\n            raise ValueError(f\"Prompt lacks hash for formatting footer: {self.request}\")\n\n        note = None\n        formatted_str = \"\\n\\n\"\n        reason = self._response_reasons[index]\n        if reason == 'length':\n            note = \"Incomplete model output due to max_tokens parameter or token limit\"\n        elif reason == 'function_call':\n            formatted_str += self.responses[index].function_call_to_json() + \"\\n\\n\"\n            note = \"The model decided to call a function\"\n        elif reason == 'content_filter':\n            note = \"Omitted content due to a flag from our content filters\"\n\n        if note:\n            formatted_str += f\"Note: {note} (finish_reason: {reason})\\n\\n\"\n\n        return formatted_str + f\"prompt {self.hash}\"\n\n    def formatted_full_response(self, index: int) -> str:\n        \"\"\"\n        Formatted full response of the prompt.\n\n        Args:\n            index (int): The index of the response to format.\n\n        Returns:\n            str: The formatted response string. None if the response is invalid.\n        \"\"\"\n        if index >= len(self.responses) or not self.responses[index]:\n            logger.error(\"Response index %d is invalid to format: request = %s, response = %s\",\n                         index, self.request, self.responses)\n            return None\n\n        formatted_str = self.formatted_header()\n\n        if self.responses[index].content:\n            formatted_str += self.responses[index].content\n\n        return formatted_str + self.formatted_footer(index)\n\n    def shortlog(self) -> List[dict]:\n        \"\"\"Generate a shortlog of the prompt.\"\"\"\n        if not self.request or not self.responses:\n            raise ValueError(\"Prompt is incomplete for shortlog.\")\n\n        responses = []\n        for message in self.responses:\n            responses.append((message.content if message.content else \"\")\n                             + message.function_call_to_json())\n\n        return {\n            \"user\": user_id(self.user_name, self.user_email)[0],\n            \"date\": self._timestamp,\n            \"context\": [msg.to_dict() for msg in self.new_context],\n            \"request\": self.request.content,\n            \"responses\": responses,\n            \"request_tokens\": self._request_tokens,\n            \"response_tokens\": self._response_tokens,\n            \"hash\": self.hash,\n            \"parent\": self.parent\n        }", ""]}
{"filename": "devchat/chat.py", "chunked_list": ["from abc import ABC, abstractmethod\nfrom typing import Iterator\nfrom devchat.prompt import Prompt\n\n\nclass Chat(ABC):\n    \"\"\"\n    Chat interface for managing chat-related interactions.\n\n    This interface defines methods for prompting a chat system with\n    a list of Message objects and retrieving responses, either as a\n    complete response or as a streaming response.\n    \"\"\"\n\n    @abstractmethod\n    def init_prompt(self, request: str) -> Prompt:\n        \"\"\"\n        Initialize a prompt for the chat system.\n\n        Args:\n            request (str): The basic request of the prompt.\n                           The returned prompt can be combined with more instructions and context.\n        \"\"\"\n\n    @abstractmethod\n    def load_prompt(self, data: dict) -> Prompt:\n        \"\"\"\n        Load a prompt from a dictionary.\n\n        Args:\n            data (dict): The dictionary containing the prompt data.\n        \"\"\"\n\n    @abstractmethod\n    def complete_response(self, prompt: Prompt) -> str:\n        \"\"\"\n        Retrieve a complete response JSON string from the chat system.\n\n        Args:\n            prompt (Prompt): A prompt of messages representing the conversation.\n        Returns:\n            str: A JSON string representing the complete response.\n        \"\"\"\n\n    @abstractmethod\n    def stream_response(self, prompt: Prompt) -> Iterator:\n        \"\"\"\n        Retrieve a streaming response as an iterator of JSON strings from the chat system.\n\n        Args:\n            prompt (Prompt): A prompt of messages representing the conversation.\n        Returns:\n            Iterator: An iterator over JSON strings (to be converted to) representing the response.\n        \"\"\"", ""]}
{"filename": "devchat/openai/__init__.py", "chunked_list": ["from .openai_chat import OpenAIChatConfig, OpenAIChat\nfrom .openai_message import OpenAIMessage\nfrom .openai_prompt import OpenAIPrompt\n\n__all__ = ['OpenAIChatConfig',\n           'OpenAIChat',\n           'OpenAIMessage',\n           'OpenAIPrompt']\n", ""]}
{"filename": "devchat/openai/openai_message.py", "chunked_list": ["import ast\nimport json\nfrom dataclasses import dataclass, asdict, field, fields\nfrom typing import Dict, Optional\n\nfrom devchat.message import Message\n\n\n@dataclass\nclass OpenAIMessage(Message):\n    role: str = None\n    name: Optional[str] = None\n    function_call: Dict[str, str] = field(default_factory=dict)\n\n    def __post_init__(self):\n        if not self._validate_role():\n            raise ValueError(\"Invalid role. Must be one of 'system', 'user', or 'assistant'.\")\n\n        if not self._validate_name():\n            raise ValueError(\"Invalid name. Must contain a-z, A-Z, 0-9, and underscores, \"\n                             \"with a maximum length of 64 characters.\")\n\n    def to_dict(self) -> dict:\n        state = asdict(self)\n        if state['name'] is None:\n            del state['name']\n        if not state['function_call'] or len(state['function_call'].keys()) == 0:\n            del state['function_call']\n        return state\n\n    @classmethod\n    def from_dict(cls, message_data: dict) -> 'OpenAIMessage':\n        keys = {f.name for f in fields(cls)}\n        kwargs = {k: v for k, v in message_data.items() if k in keys}\n        return cls(**kwargs)\n\n    def function_call_to_json(self):\n        '''\n        convert function_call to json\n        function_call is like this:\n        {\n            \"name\": function_name,\n            \"arguments\": '{\"key\": \"\"\"value\"\"\"}'\n        }\n        '''\n        if not self.function_call:\n            return ''\n        function_call_copy = self.function_call.copy()\n        if 'arguments' in function_call_copy:\n            # arguments field may be not a json string\n            # we can try parse it by eval\n            try:\n                function_call_copy['arguments'] = ast.literal_eval(function_call_copy['arguments'])\n            except Exception:\n                # if it is not a json string, we can do nothing\n                try:\n                    function_call_copy['arguments'] = json.loads(function_call_copy['arguments'])\n                except Exception:\n                    pass\n        return '```command\\n' + json.dumps(function_call_copy) + '\\n```'\n\n    def stream_from_dict(self, message_data: dict) -> str:\n        \"\"\"Append to the message from a dictionary returned from a streaming chat API.\"\"\"\n        delta = message_data.get('content', '')\n        if self.content:\n            self.content += delta\n        else:\n            self.content = delta\n\n        return delta\n\n    def _validate_role(self) -> bool:\n        \"\"\"Validate the role attribute.\n\n        Returns:\n            bool: True if the role is valid, False otherwise.\n        \"\"\"\n        return self.role in [\"system\", \"user\", \"assistant\", \"function\"]\n\n    def _validate_name(self) -> bool:\n        \"\"\"Validate the name attribute.\n\n        Returns:\n            bool: True if the name is valid or None, False otherwise.\n        \"\"\"\n        return self._validate_string(self.name)\n\n    def _validate_string(self, string: str) -> bool:\n        \"\"\"Validate a string attribute.\n\n        Returns:\n            bool: True if the string is valid or None, False otherwise.\n        \"\"\"\n        if string is None:\n            return True\n        if not string.strip():\n            return False\n        return len(string) <= 64 and string.replace(\"_\", \"\").isalnum()", "@dataclass\nclass OpenAIMessage(Message):\n    role: str = None\n    name: Optional[str] = None\n    function_call: Dict[str, str] = field(default_factory=dict)\n\n    def __post_init__(self):\n        if not self._validate_role():\n            raise ValueError(\"Invalid role. Must be one of 'system', 'user', or 'assistant'.\")\n\n        if not self._validate_name():\n            raise ValueError(\"Invalid name. Must contain a-z, A-Z, 0-9, and underscores, \"\n                             \"with a maximum length of 64 characters.\")\n\n    def to_dict(self) -> dict:\n        state = asdict(self)\n        if state['name'] is None:\n            del state['name']\n        if not state['function_call'] or len(state['function_call'].keys()) == 0:\n            del state['function_call']\n        return state\n\n    @classmethod\n    def from_dict(cls, message_data: dict) -> 'OpenAIMessage':\n        keys = {f.name for f in fields(cls)}\n        kwargs = {k: v for k, v in message_data.items() if k in keys}\n        return cls(**kwargs)\n\n    def function_call_to_json(self):\n        '''\n        convert function_call to json\n        function_call is like this:\n        {\n            \"name\": function_name,\n            \"arguments\": '{\"key\": \"\"\"value\"\"\"}'\n        }\n        '''\n        if not self.function_call:\n            return ''\n        function_call_copy = self.function_call.copy()\n        if 'arguments' in function_call_copy:\n            # arguments field may be not a json string\n            # we can try parse it by eval\n            try:\n                function_call_copy['arguments'] = ast.literal_eval(function_call_copy['arguments'])\n            except Exception:\n                # if it is not a json string, we can do nothing\n                try:\n                    function_call_copy['arguments'] = json.loads(function_call_copy['arguments'])\n                except Exception:\n                    pass\n        return '```command\\n' + json.dumps(function_call_copy) + '\\n```'\n\n    def stream_from_dict(self, message_data: dict) -> str:\n        \"\"\"Append to the message from a dictionary returned from a streaming chat API.\"\"\"\n        delta = message_data.get('content', '')\n        if self.content:\n            self.content += delta\n        else:\n            self.content = delta\n\n        return delta\n\n    def _validate_role(self) -> bool:\n        \"\"\"Validate the role attribute.\n\n        Returns:\n            bool: True if the role is valid, False otherwise.\n        \"\"\"\n        return self.role in [\"system\", \"user\", \"assistant\", \"function\"]\n\n    def _validate_name(self) -> bool:\n        \"\"\"Validate the name attribute.\n\n        Returns:\n            bool: True if the name is valid or None, False otherwise.\n        \"\"\"\n        return self._validate_string(self.name)\n\n    def _validate_string(self, string: str) -> bool:\n        \"\"\"Validate a string attribute.\n\n        Returns:\n            bool: True if the string is valid or None, False otherwise.\n        \"\"\"\n        if string is None:\n            return True\n        if not string.strip():\n            return False\n        return len(string) <= 64 and string.replace(\"_\", \"\").isalnum()", ""]}
{"filename": "devchat/openai/openai_chat.py", "chunked_list": ["from typing import Optional, Union, List, Dict, Iterator\nfrom pydantic import BaseModel, Field, Extra\nimport openai\nfrom devchat.chat import Chat\nfrom devchat.utils import get_user_info, user_id\nfrom .openai_message import OpenAIMessage\nfrom .openai_prompt import OpenAIPrompt\n\n\nclass OpenAIChatConfig(BaseModel):\n    \"\"\"\n    Configuration object for the OpenAIChat class.\n    \"\"\"\n    model: str\n    temperature: Optional[float] = Field(0, ge=0, le=2)\n    top_p: Optional[float] = Field(None, ge=0, le=1)\n    n: Optional[int] = Field(None, ge=1)\n    stream: Optional[bool] = Field(None)\n    stop: Optional[Union[str, List[str]]] = Field(None)\n    max_tokens: Optional[int] = Field(None, ge=1)\n    presence_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)\n    frequency_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)\n    logit_bias: Optional[Dict[int, float]] = Field(None)\n    user: Optional[str] = Field(None)\n    request_timeout: Optional[int] = Field(32, ge=3)\n\n    class Config:\n        \"\"\"\n        Configuration class to forbid extra fields in the model.\n        \"\"\"\n        extra = Extra.forbid", "\nclass OpenAIChatConfig(BaseModel):\n    \"\"\"\n    Configuration object for the OpenAIChat class.\n    \"\"\"\n    model: str\n    temperature: Optional[float] = Field(0, ge=0, le=2)\n    top_p: Optional[float] = Field(None, ge=0, le=1)\n    n: Optional[int] = Field(None, ge=1)\n    stream: Optional[bool] = Field(None)\n    stop: Optional[Union[str, List[str]]] = Field(None)\n    max_tokens: Optional[int] = Field(None, ge=1)\n    presence_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)\n    frequency_penalty: Optional[float] = Field(None, ge=-2.0, le=2.0)\n    logit_bias: Optional[Dict[int, float]] = Field(None)\n    user: Optional[str] = Field(None)\n    request_timeout: Optional[int] = Field(32, ge=3)\n\n    class Config:\n        \"\"\"\n        Configuration class to forbid extra fields in the model.\n        \"\"\"\n        extra = Extra.forbid", "\n\nclass OpenAIChat(Chat):\n    \"\"\"\n    OpenAIChat class that handles communication with the OpenAI Chat API.\n    \"\"\"\n    def __init__(self, config: OpenAIChatConfig):\n        \"\"\"\n        Initialize the OpenAIChat class with a configuration object.\n\n        Args:\n            config (OpenAIChatConfig): Configuration object with parameters for the OpenAI Chat API.\n        \"\"\"\n        self.config = config\n\n    def init_prompt(self, request: str, function_name: Optional[str] = None) -> OpenAIPrompt:\n        user, email = get_user_info()\n        self.config.user = user_id(user, email)[1]\n        prompt = OpenAIPrompt(self.config.model, user, email)\n        prompt.set_request(request, function_name=function_name)\n        return prompt\n\n    def load_prompt(self, data: dict) -> OpenAIPrompt:\n        data['_new_messages'] = {\n            k: [OpenAIMessage.from_dict(m) for m in v]\n            if isinstance(v, list) else OpenAIMessage.from_dict(v)\n            for k, v in data['_new_messages'].items() if k != 'function'\n        }\n        data['_history_messages'] = {k: [OpenAIMessage.from_dict(m) for m in v]\n                                     for k, v in data['_history_messages'].items()}\n        return OpenAIPrompt(**data)\n\n    def complete_response(self, prompt: OpenAIPrompt) -> str:\n        # Filter the config parameters with non-None values\n        config_params = {\n            key: value\n            for key, value in self.config.dict().items() if value is not None\n        }\n        if prompt.get_functions():\n            config_params['functions'] = prompt.get_functions()\n            config_params['function_call'] = 'auto'\n        config_params['stream'] = False\n\n        response = openai.ChatCompletion.create(\n            messages=prompt.messages,\n            **config_params\n        )\n        return str(response)\n\n    def stream_response(self, prompt: OpenAIPrompt) -> Iterator:\n        # Filter the config parameters with non-None values\n        config_params = {\n            key: value\n            for key, value in self.config.dict().items() if value is not None\n        }\n        if prompt.get_functions():\n            config_params['functions'] = prompt.get_functions()\n            config_params['function_call'] = 'auto'\n        config_params['stream'] = True\n\n        response = openai.ChatCompletion.create(\n            messages=prompt.messages,\n            **config_params\n        )\n        return response", ""]}
{"filename": "devchat/openai/openai_prompt.py", "chunked_list": ["from dataclasses import dataclass\nimport json\nimport math\nfrom typing import List, Optional\nfrom devchat.prompt import Prompt\nfrom devchat.message import Message\nfrom devchat.utils import update_dict, get_logger\nfrom devchat.utils import message_tokens, response_tokens\nfrom .openai_message import OpenAIMessage\n", "from .openai_message import OpenAIMessage\n\nlogger = get_logger(__name__)\n\n\n@dataclass\nclass OpenAIPrompt(Prompt):\n    \"\"\"\n    A class to represent a prompt and its corresponding responses from OpenAI APIs.\n    \"\"\"\n\n    _id: str = None\n\n    @property\n    def id(self) -> str:\n        return self._id\n\n    @property\n    def messages(self) -> List[dict]:\n        combined = []\n        # Instruction\n        if self._new_messages[Message.INSTRUCT]:\n            combined += [msg.to_dict() for msg in self._new_messages[Message.INSTRUCT]]\n        # History context\n        if self._history_messages[Message.CONTEXT]:\n            combined += [update_dict(msg.to_dict(), 'content',\n                                     f\"<context>\\n{msg.content}\\n</context>\")\n                         for msg in self._history_messages[Message.CONTEXT]]\n        # History chat\n        if self._history_messages[Message.CHAT]:\n            combined += [msg.to_dict() for msg in self._history_messages[Message.CHAT]]\n        # Request\n        if self.request:\n            combined += [self.request.to_dict()]\n        # New context\n        if self.new_context:\n            combined += [update_dict(msg.to_dict(), 'content',\n                                     f\"<context>\\n{msg.content}\\n</context>\")\n                         for msg in self.new_context]\n        return combined\n\n    def input_messages(self, messages: List[dict]):\n        state = \"new_instruct\"\n        for message_data in messages:\n            message = OpenAIMessage.from_dict(message_data)\n\n            if state == \"new_instruct\":\n                if message.role == \"system\" and not message.content.startswith(\"<context>\"):\n                    self._new_messages[Message.INSTRUCT].append(message)\n                else:\n                    state = \"history_context\"\n\n            if state == \"history_context\":\n                if message.role == \"system\" and message.content.startswith(\"<context>\"):\n                    content = message.content.replace(\"<context>\", \"\").replace(\"</context>\", \"\")\n                    message.content = content.strip()\n                    self._history_messages[Message.CONTEXT].append(message)\n                else:\n                    state = \"history_chat\"\n\n            if state == \"history_chat\":\n                if message.role in (\"user\", \"assistant\"):\n                    self._history_messages[Message.CHAT].append(message)\n                else:\n                    state = \"new_context\"\n\n            if state == \"new_context\":\n                if message.role == \"system\" and message.content.startswith(\"<context>\"):\n                    content = message.content.replace(\"<context>\", \"\").replace(\"</context>\", \"\")\n                    message.content = content.strip()\n                    self._new_messages[Message.CONTEXT].append(message)\n                else:\n                    logger.warning(\"Invalid new context message: %s\", message)\n\n        if not self.request:\n            last_user_message = self._history_messages[Message.CHAT].pop()\n            if last_user_message.role == \"user\":\n                self._new_messages[\"request\"] = last_user_message\n            else:\n                logger.warning(\"Invalid user request: %s\", last_user_message)\n\n    def append_new(self, message_type: str, content: str,\n                   available_tokens: int = math.inf) -> bool:\n        if message_type not in (Message.INSTRUCT, Message.CONTEXT):\n            raise ValueError(f\"Current messages cannot be of type {message_type}.\")\n        # New instructions and context are of the system role\n        message = OpenAIMessage(content=content, role='system')\n\n        num_tokens = message_tokens(message.to_dict(), self.model)\n        if num_tokens > available_tokens:\n            return False\n\n        self._new_messages[message_type].append(message)\n        self._request_tokens += num_tokens\n        return True\n\n    def set_functions(self, functions, available_tokens: int = math.inf):\n        num_tokens = message_tokens({\"functions\": json.dumps(functions)}, self.model)\n        if num_tokens > available_tokens:\n            return False\n\n        self._new_messages[Message.FUNCTION] = functions\n        self._request_tokens += num_tokens\n        return True\n\n    def get_functions(self):\n        return self._new_messages.get(Message.FUNCTION, None)\n\n    def _prepend_history(self, message_type: str, message: Message,\n                         token_limit: int = math.inf) -> bool:\n        if message_type == Message.INSTRUCT:\n            raise ValueError(\"History messages cannot be of type INSTRUCT.\")\n        num_tokens = message_tokens(message.to_dict(), self.model)\n        if num_tokens > token_limit - self._request_tokens:\n            return False\n        self._history_messages[message_type].insert(0, message)\n        self._request_tokens += num_tokens\n        return True\n\n    def prepend_history(self, prompt: 'OpenAIPrompt', token_limit: int = math.inf) -> bool:\n        # Prepend the first response and the request of the prompt\n        if not self._prepend_history(Message.CHAT, prompt.responses[0], token_limit):\n            return False\n        if not self._prepend_history(Message.CHAT, prompt.request, token_limit):\n            return False\n\n        # Append the context messages of the appended prompt\n        for context_message in prompt.new_context:\n            if not self._prepend_history(Message.CONTEXT, context_message, token_limit):\n                return False\n        return True\n\n    def set_request(self, content: str, function_name: Optional[str] = None) -> int:\n        if not content.strip():\n            raise ValueError(\"The request cannot be empty.\")\n        message = OpenAIMessage(content=content,\n                                role=('user' if not function_name else 'function'),\n                                name=function_name)\n        self._new_messages['request'] = message\n        self._request_tokens += message_tokens(message.to_dict(), self.model)\n\n    def set_response(self, response_str: str):\n        \"\"\"\n        Parse the API response string and set the Prompt object's attributes.\n\n        Args:\n            response_str (str): The JSON-formatted response string from the chat API.\n        \"\"\"\n        response_data = json.loads(response_str)\n        self._validate_model(response_data)\n        self._timestamp_from_dict(response_data)\n        self._id_from_dict(response_data)\n\n        self._request_tokens = response_data['usage']['prompt_tokens']\n        self._response_tokens = response_data['usage']['completion_tokens']\n\n        for choice in response_data['choices']:\n            index = choice['index']\n            if index >= len(self.responses):\n                self.responses.extend([None] * (index - len(self.responses) + 1))\n                self._response_reasons.extend([None] * (index - len(self._response_reasons) + 1))\n            self.responses[index] = OpenAIMessage.from_dict(choice['message'])\n            if choice['finish_reason']:\n                self._response_reasons[index] = choice['finish_reason']\n\n    def append_response(self, delta_str: str) -> str:\n        \"\"\"\n        Append the content of a streaming response to the existing messages.\n\n        Args:\n            delta_str (str): The JSON-formatted delta string from the chat API.\n\n        Returns:\n            str: The delta content with index 0. None when the response is over.\n        \"\"\"\n        response_data = json.loads(delta_str)\n        self._validate_model(response_data)\n        self._timestamp_from_dict(response_data)\n        self._id_from_dict(response_data)\n\n        delta_content = ''\n        for choice in response_data['choices']:\n            delta = choice['delta']\n            index = choice['index']\n            finish_reason = choice['finish_reason']\n\n            if index >= len(self.responses):\n                self.responses.extend([None] * (index - len(self.responses) + 1))\n                self._response_reasons.extend([None] * (index - len(self._response_reasons) + 1))\n\n            if not self.responses[index]:\n                self.responses[index] = OpenAIMessage.from_dict(delta)\n                if index == 0:\n                    delta_content = self.responses[0].content if self.responses[0].content else ''\n            else:\n                if index == 0:\n                    delta_content = self.responses[0].stream_from_dict(delta)\n                else:\n                    self.responses[index].stream_from_dict(delta)\n\n                if 'function_call' in delta:\n                    if 'name' in delta['function_call']:\n                        self.responses[index].function_call['name'] = \\\n                            self.responses[index].function_call.get('name', '') + \\\n                            delta['function_call']['name']\n                    if 'arguments' in delta['function_call']:\n                        self.responses[index].function_call['arguments'] = \\\n                            self.responses[index].function_call.get('arguments', '') + \\\n                            delta['function_call']['arguments']\n\n            if finish_reason:\n                self._response_reasons[index] = finish_reason\n        return delta_content\n\n    def _count_response_tokens(self) -> int:\n        if self._response_tokens:\n            return self._response_tokens\n\n        total = 0\n        for response_message in self.responses:\n            total += response_tokens(response_message.to_dict(), self.model)\n        self._response_tokens = total\n        return total\n\n    def _validate_model(self, response_data: dict):\n        if not response_data['model'].startswith(self.model):\n            raise ValueError(f\"Model mismatch: expected '{self.model}', \"\n                             f\"got '{response_data['model']}'\")\n\n    def _timestamp_from_dict(self, response_data: dict):\n        if self._timestamp is None:\n            self._timestamp = response_data['created']\n        elif self._timestamp != response_data['created']:\n            raise ValueError(f\"Time mismatch: expected {self._timestamp}, \"\n                             f\"got {response_data['created']}\")\n\n    def _id_from_dict(self, response_data: dict):\n        if self._id is None:\n            self._id = response_data['id']\n        elif self._id != response_data['id']:\n            raise ValueError(f\"ID mismatch: expected {self._id}, \"\n                             f\"got {response_data['id']}\")", ""]}
