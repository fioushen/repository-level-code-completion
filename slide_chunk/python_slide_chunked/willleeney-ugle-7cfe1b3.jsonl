{"filename": "setup.py", "chunked_list": ["from setuptools import setup, find_packages\n\nsetup(name='ugle', version='1.0', packages=find_packages())\n"]}
{"filename": "main.py", "chunked_list": ["import ugle\nimport ugle.utils as utils\nfrom ugle.logger import log\nfrom ugle.trainer import MyLibrarySniffingClass\nfrom omegaconf import OmegaConf, DictConfig, open_dict\nimport argparse\nimport psutil\nimport time\nfrom os.path import isfile\nimport pickle", "from os.path import isfile\nimport pickle\n\n\ndef neural_run(override_model: str = None,\n               override_dataset: str = None,\n               override_cfg: DictConfig = None) -> dict:\n    \"\"\"\n    runs a GNN neural experiment\n    :param override_model: name of model to override default config\n    :param override_dataset: name of the dataset to override default config\n    :param override_cfg: override config options\n    :return results: results from the study\n    \"\"\"\n\n    # load model config\n    cfg = utils.load_model_config(override_model=override_model, override_cfg=override_cfg)\n    if override_dataset:\n        cfg.dataset = override_dataset\n\n    if cfg.trainer.load_existing_test and 'default' not in override_model:\n        # try load the pickle file from previous study \n        hpo_path = f\"{cfg.trainer.load_hps_path}{cfg.dataset}_{cfg.model}.pkl\"\n        if isfile(hpo_path):\n            log.info(f'loading hpo args: {hpo_path}')\n            previously_found = pickle.load(open(hpo_path, \"rb\"))\n            cfg.previous_results = previously_found.results\n        # if this doesn't exist then just use the default parameters\n        else: \n            log.info(f'loading default args')\n            found_args = OmegaConf.load(f'ugle/configs/models/{cfg.model}/{cfg.model}_default.yaml')\n            with open_dict(cfg):\n                cfg.args = OmegaConf.merge(cfg.args, found_args)\n        cfg.trainer.only_testing = True\n\n    # create trainer object defined in models and init with config\n    Trainer = getattr(getattr(ugle.models, cfg.model), f\"{cfg.model}_trainer\")(cfg)\n\n    start_time = time.time()\n    # memory profiling max memory requires other class\n    if 'memory' in Trainer.cfg.trainer.test_metrics:\n        # train model\n        start_mem = psutil.virtual_memory().active\n        mythread = MyLibrarySniffingClass(Trainer.eval)\n        mythread.start()\n\n        delta_mem = 0\n        max_memory = 0\n        memory_usage_refresh = .001  # Seconds\n\n        while True:\n            time.sleep(memory_usage_refresh)\n            delta_mem = psutil.virtual_memory().active - start_mem\n            if delta_mem > max_memory:\n                max_memory = delta_mem\n                max_percent = psutil.virtual_memory().percent\n\n            # Check to see if the library call is complete\n            if mythread.isShutdown():\n                break\n\n        max_memory /= 1024.0 ** 2\n        log.info(f\"MAX Memory Usage in MB: {max_memory:.2f}\")\n        log.info(f\"Max useage %: {max_percent}\")\n\n        results = mythread.results\n        results['memory'] = max_memory\n\n    else:\n        # train and evaluate model\n        results = Trainer.eval()\n\n    log.info(f\"Total Time for {cfg.model} {cfg.dataset}: {round(time.time() - start_time, 3)}s\")\n\n    return results", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='which model to run')\n    parser.add_argument('--model', type=str, default='daegc',\n                        help='the name of the model to run')\n    parser.add_argument('--dataset', type=str, default='cora',\n                        help='the name of the dataset to run model on')\n    parser.add_argument('--seed', type=str, default=42,\n                        help='the number random seed to train on')\n    parser.add_argument('--gpu', type=str, default=\"0\",\n                        help='the gpu to train on')\n    parser.add_argument('--load_existing_test', action='store_true',\n                        help='load best parameters available')\n    parsed = parser.parse_args()\n    study_cfg = OmegaConf.create({\"args\": {\"random_seed\": int(parsed.seed)},\n                                  \"trainer\": {\"gpu\": int(parsed.gpu), \n                                              \"load_existing_test\": parsed.load_existing_test}})\n    if ugle.utils.is_neural(parsed.model):\n        results = neural_run(override_model=parsed.model,\n                             override_dataset=parsed.dataset,\n                             override_cfg=study_cfg)"]}
{"filename": "model_evaluations.py", "chunked_list": ["from main import neural_run\nfrom omegaconf import OmegaConf, DictConfig\nimport ugle\nimport argparse\nfrom ugle.logger import log\nimport pickle\nfrom os.path import exists\nfrom os import makedirs\nfrom copy import deepcopy\n", "from copy import deepcopy\n\n\ndef run_study(study_override_cfg: DictConfig, algorithm: str, dataset: str, seeds: list):\n    \"\"\"\n    runs a study of one algorithm on one dataset over multiple seeds\n    :param study_override_cfg: study configuration object\n    :param algorithm: string of algorithm to test on\n    :param dataset: string of the dataset to test on\n    :param seeds: list of seeds to test over\n    :return average_results: the results for each metric averaged over the seeds\n    \"\"\"\n    study_cfg = study_override_cfg.copy()\n    average_results = ugle.utils.create_study_tracker(len(seeds), study_cfg.trainer.test_metrics)\n    study_results = OmegaConf.create({'dataset': dataset,\n                                      'model': algorithm,\n                                      'average_results': {},\n                                      'results': []})\n    \n    # repeat training over all seeds\n    for idx, seed in enumerate(seeds):\n        study_cfg.args.random_seed = seed\n        log.info(f'Study -- {algorithm}:{dataset}:Seed({seed})')\n\n        # test results stores the results of one algorithm run\n        if ugle.utils.is_neural(algorithm):\n            results = neural_run(override_model=algorithm,\n                                 override_dataset=dataset,\n                                 override_cfg=study_cfg)\n\n        # save study output\n        study_results.results.append({'seed': seed, 'study_output': results})\n        average_results = ugle.utils.collate_study_results(average_results, results, idx)\n\n        # use first seed hyperparameters and train/test on remaining\n        if idx == 0:\n            study_cfg.previous_results = results\n\n    # average results stores the average calculation of statistics\n    average_results = ugle.utils.calc_average_results(average_results)\n    study_results.average_results = average_results\n\n    # save the result of the study\n    if not exists(study_cfg.trainer.results_path):\n        makedirs(study_cfg.trainer.results_path)\n    save_path = f\"{study_cfg.trainer.results_path}{dataset}_{algorithm}\"\n    pickle.dump(study_results, open(f\"{save_path}.pkl\", \"wb\"))\n\n    # after all results have been collected, get best result seed, retrain and save\n    if study_cfg.trainer.retrain_on_each_dataset:\n        args, best_seed = ugle.utils.get_best_args(study_results.results, study_cfg.trainer.model_resolution_metric)\n        study_cfg.args = args\n        study_cfg.args.random_seed = best_seed\n        study_cfg.trainer.only_testing = True\n        study_cfg.trainer.save_model = True\n        _ = neural_run(override_model=algorithm,\n                        override_dataset=dataset,\n                        override_cfg=study_cfg)\n        \n    return average_results", "\n\ndef run_experiment(exp_cfg_name: str):\n    \"\"\"\n    run experiments which consists of multiple models and datasets\n    :param exp_cfg_name: location of the yaml file containing experiment configuration\n    \"\"\"\n    # load experiment config\n    log.info(f'loading experiment: {exp_cfg_name}')\n    exp_cfg = OmegaConf.load('ugle/configs/experiments/exp_cfg_template.yaml')\n    exp_cfg = ugle.utils.merge_yaml(exp_cfg, exp_cfg_name)\n\n    # iterate\n    if exp_cfg.special_training.split_addition_percentage:\n        log.info('Special Experiment: Removes percentage from whole dataset')\n        saved_cfg = deepcopy(exp_cfg)\n        special_runs = deepcopy(exp_cfg.study_override_cfg.trainer.split_addition_percentage)\n    elif exp_cfg.study_override_cfg.trainer.retrain_on_each_dataset: \n        log.info('Special Experiment: Finetuning on each new dataset given')\n        special_runs = [-1]\n        iterations_before_fine_tuning = len(exp_cfg.algorithms) - 1\n        if not exists(exp_cfg.study_override_cfg.trainer.models_path):\n            makedirs(exp_cfg.study_override_cfg.trainer.models_path)\n\n    elif exp_cfg.study_override_cfg.trainer.same_init_hpo:\n        log.info('Special Experiment: Same initilisation of Parameters')\n        special_runs = [-1]\n        if not exists(exp_cfg.study_override_cfg.trainer.models_path):\n            makedirs(exp_cfg.study_override_cfg.trainer.models_path)\n\n\n    else:\n        special_runs = [-1]\n\n    save_path = exp_cfg.study_override_cfg.trainer.results_path\n    \n    for special_var in special_runs:\n        if special_var != -1:\n            log.info(f'Experiment: {special_var}% of the entire dataset')\n            exp_cfg = deepcopy(saved_cfg)\n            exp_cfg.study_override_cfg.trainer.split_addition_percentage = special_var\n            exp_cfg.study_override_cfg.trainer.results_path += str(special_var).replace('.', '') + '/'\n            if not exists(exp_cfg.study_override_cfg.trainer.results_path):\n                makedirs(exp_cfg.study_override_cfg.trainer.results_path)\n\n        # creating experiment iterator\n        experiment_tracker = ugle.utils.create_experiment_tracker(exp_cfg)\n        experiments_cpu = []\n\n        for exp_num, experiment in enumerate(experiment_tracker):\n            log.debug(f'starting new experiment ... ...')\n            log.debug(f'testing dataset: {experiment.dataset}')\n            log.debug(f'testing algorithm: {experiment.algorithm}')\n            \n            if exp_cfg.study_override_cfg.trainer.retrain_on_each_dataset:\n                if exp_num > iterations_before_fine_tuning:\n                    exp_cfg.study_override_cfg.trainer.finetuning_new_dataset = True\n                    exp_cfg.study_override_cfg.trainer.only_testing = False\n                    exp_cfg.study_override_cfg.trainer.save_model = False\n\n            try:\n                # run experiment\n                experiment_results = run_study(exp_cfg.study_override_cfg,\n                                                experiment.algorithm,\n                                                experiment.dataset,\n                                                exp_cfg.seeds)\n                # save result in experiment tracker\n                experiment.results = experiment_results\n                ugle.utils.save_experiment_tracker(experiment_tracker, save_path)\n\n\n\n            # if breaks then tests on cpu\n            except Exception as e:\n                log.exception(str(e))\n                log.info(f\"adding to cpu fallback test\")\n                experiments_cpu.append(experiment)\n\n        # run all experiments that didn't work on gpu\n        if experiments_cpu and exp_cfg.run_cpu_fallback:\n            log.info(f'launching cpu fallback experiments')\n            exp_cfg.study_override_cfg.trainer.gpu = -1\n\n            for experiment in experiments_cpu:\n                log.debug(f'starting new experiment ...')\n                log.debug(f'testing dataset: {experiment.dataset}')\n                log.debug(f'testing algorithm: {experiment.algorithm}')\n\n                # run experiment\n                experiment_results = run_study(exp_cfg.study_override_cfg,\n                                                experiment.algorithm,\n                                                experiment.dataset,\n                                                exp_cfg.seeds)\n                # save result in experiment tracker\n                experiment.results = experiment_results\n                ugle.utils.save_experiment_tracker(experiment_tracker, save_path)\n        else:\n            if experiments_cpu:\n                log.info('The following combinations lead to OOM')\n                for experiment in experiments_cpu:\n                    log.info(f'{experiment.dataset} : {experiment.algorithm}')\n        \n        ugle.utils.display_evaluation_results(experiment_tracker)\n        \n    return", "\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='parsing the experiment to run')\n    parser.add_argument('-ec', '--experiment_config', type=str, required=True,\n                        help='the location of the experiment config')\n    parsed = parser.parse_args()\n    run_experiment(parsed.experiment_config)\n", ""]}
{"filename": "transfer_results.py", "chunked_list": ["import subprocess\nimport os\nugle_path = os.path.dirname(os.path.realpath(__file__))\n\n\ndef search_results(folder, filename):\n\n    for root, dirs, files in os.walk(f'{ugle_path}/{folder}'):\n        if filename in files:\n            return os.path.join(root, filename)\n    return None", "\n\ndef attempt_file_transfer(filename, folder):\n    command = f\"scp -r blue_server:~/ugle/{folder}{filename} {folder}\"\n\n    res = subprocess.call(command, shell=True)\n    if res != 1:\n        print(f'File Transferred: {filename}')\n    else:\n        print(f'No Result: {filename}')\n\n    return", "\n\ndef update_progress_results(datasets, algorithms, folder):\n    for dataset in datasets:\n        for algo in algorithms:\n            filename = f\"{dataset}_{algo}.pkl\"\n            result = search_results(folder, filename)\n            if result:\n                print(f'Found: {filename}')\n            else:\n                attempt_file_transfer(filename, folder)\n\n    return", "\n\nalgorithms = ['daegc', 'dgi', 'dmon', 'grace', 'mvgrl', 'selfgnn', 'sublime', 'bgrl', 'vgaer', 'cagc']\ndatasets = ['cora', 'citeseer', 'dblp', 'bat', 'eat', 'texas', 'wisc', 'cornell', 'uat', 'amac', 'amap']\nfolder = './updated_results/'\nupdate_progress_results(datasets, algorithms, folder)\n\ndefault_algos = ['daegc_default', 'dgi_default', 'dmon_default', 'grace_default', 'mvgrl_default', 'selfgnn_default',\n                 'sublime_default', 'bgrl_default', 'vgaer_default', 'cagc_default']\ndefault_folder = './new_default/'", "                 'sublime_default', 'bgrl_default', 'vgaer_default', 'cagc_default']\ndefault_folder = './new_default/'\nupdate_progress_results(datasets, default_algos, default_folder)"]}
{"filename": "tests/testing_env.py", "chunked_list": ["import ugle\n\nfrom model_evaluations import run_experiment\nfrom main import neural_run\n\ndef test_loading_real_data():\n    features, label, training_adj, testing_adj = ugle.datasets.load_real_graph_data('cora', 0.5, False)\n    features, label, training_adj, testing_adj = ugle.datasets.load_real_graph_data('facebook', 0.5, False)\n    return\n\ndef test_neural():\n    neural_run(override_model='grace_default')\n    neural_run(override_model='dmon_default')\n    neural_run(override_model='daegc_default')\n    neural_run(override_model='mvgrl_default')\n    neural_run(override_model='bgrl_default')\n    neural_run(override_model='selfgnn_default')\n    neural_run(override_model='sublime_default')\n    neural_run(override_model='vgaer_default')", "\ndef test_neural():\n    neural_run(override_model='grace_default')\n    neural_run(override_model='dmon_default')\n    neural_run(override_model='daegc_default')\n    neural_run(override_model='mvgrl_default')\n    neural_run(override_model='bgrl_default')\n    neural_run(override_model='selfgnn_default')\n    neural_run(override_model='sublime_default')\n    neural_run(override_model='vgaer_default')", "\n\ndef test_pipeline():\n    # test that pipeline fallsback to cpu correctly\n\n    run_experiment('ugle/configs/testing/min_cpu_fallback.yaml')\n    # test that pipeline works\n    run_experiment('ugle/configs/testing/min_working_pipeline_config.yaml')\n\n\ndef test_multi_objective():\n    run_experiment('ugle/configs/testing/min_multi_objective.yaml')\n    run_experiment('ugle/configs/testing/min_multi_hpo_non_neural.yaml')", "\n\ndef test_multi_objective():\n    run_experiment('ugle/configs/testing/min_multi_objective.yaml')\n    run_experiment('ugle/configs/testing/min_multi_hpo_non_neural.yaml')"]}
{"filename": "ugle/gnn_architecture.py", "chunked_list": ["import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass mvgrl_Discriminator(nn.Module):\n    def __init__(self, n_h):\n        super(mvgrl_Discriminator, self).__init__()\n        self.f_k = nn.Bilinear(n_h, n_h, 1)\n\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Bilinear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    def forward(self, c1, c2, h1, h2, h3, h4, s_bias1=None, s_bias2=None):\n        c_x1 = torch.unsqueeze(c1, 1)\n        c_x1 = c_x1.expand_as(h1).contiguous()\n        c_x2 = torch.unsqueeze(c2, 1)\n        c_x2 = c_x2.expand_as(h2).contiguous()\n\n        # positive\n        sc_1 = torch.squeeze(self.f_k(h2, c_x1), 2)\n        sc_2 = torch.squeeze(self.f_k(h1, c_x2), 2)\n\n        # negetive\n        sc_3 = torch.squeeze(self.f_k(h4, c_x1), 2)\n        sc_4 = torch.squeeze(self.f_k(h3, c_x2), 2)\n\n        logits = torch.cat((sc_1, sc_2, sc_3, sc_4), 1)\n        return logits", "\nclass Discriminator(nn.Module):\n    def __init__(self, n_h):\n        super(Discriminator, self).__init__()\n        self.f_k = nn.Bilinear(n_h, n_h, 1)\n\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Bilinear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    def forward(self, c, h_pl, h_mi, s_bias1=None, s_bias2=None):\n        c_x = torch.unsqueeze(c, 1)\n        c_x = c_x.expand_as(h_pl)\n\n        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 2)\n        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 2)\n\n        if s_bias1 is not None:\n            sc_1 += s_bias1\n        if s_bias2 is not None:\n            sc_2 += s_bias2\n\n        logits = torch.cat((sc_1, sc_2), 1)\n\n        return logits", "\n# Applies an average on seq, of shape (batch, nodes, features)\n# While taking into account the masking of msk\nclass AvgReadout(nn.Module):\n    def __init__(self):\n        super(AvgReadout, self).__init__()\n\n    def forward(self, seq, msk):\n        if msk is None:\n            return torch.mean(seq, 1)\n        else:\n            msk = torch.unsqueeze(msk, -1)\n            return torch.sum(seq * msk, 1) / torch.sum(msk)", "\nclass GATLayer(nn.Module):\n    \"\"\"\n    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n    \"\"\"\n\n    def __init__(self, in_features, out_features, alpha=0.2):\n        super(GATLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.alpha = alpha\n\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n\n        self.a_self = nn.Parameter(torch.zeros(size=(out_features, 1)))\n        nn.init.xavier_uniform_(self.a_self.data, gain=1.414)\n\n        self.a_neighs = nn.Parameter(torch.zeros(size=(out_features, 1)))\n        nn.init.xavier_uniform_(self.a_neighs.data, gain=1.414)\n\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n\n    def forward(self, input, adj, M, concat=True):\n        h = torch.mm(input, self.W)\n\n        attn_for_self = torch.mm(h, self.a_self)  # (N,1)\n        attn_for_neighs = torch.mm(h, self.a_neighs)  # (N,1)\n        attn_dense = attn_for_self + torch.transpose(attn_for_neighs, 0, 1)\n        attn_dense = torch.mul(attn_dense, M)\n        attn_dense = self.leakyrelu(attn_dense)  # (N,N)\n\n        zero_vec = -9e15 * torch.ones_like(adj)\n        adj = torch.where(adj > 0, attn_dense, zero_vec)\n        attention = F.softmax(adj, dim=1)\n        h_prime = torch.matmul(attention, h)\n\n        if concat:\n            return F.elu(h_prime)\n        else:\n            return h_prime\n\n    def __repr__(self):\n        return (\n            self.__class__.__name__\n            + \" (\"\n            + str(self.in_features)\n            + \" -> \"\n            + str(self.out_features)\n            + \")\"\n        )", "\nclass GAT(nn.Module):\n    def __init__(self, num_features, hidden_size, embedding_size, alpha):\n        super(GAT, self).__init__()\n        self.hidden_size = hidden_size\n        self.embedding_size = embedding_size\n        self.alpha = alpha\n        self.conv1 = GATLayer(num_features, hidden_size, alpha)\n        self.conv2 = GATLayer(hidden_size, embedding_size, alpha)\n\n    def forward(self, x, adj, M):\n        h = self.conv1(x, adj, M)\n        h = self.conv2(h, adj, M)\n        z = F.normalize(h, p=2, dim=1)\n        A_pred = self.dot_product_decode(z)\n        return A_pred, z\n\n    def dot_product_decode(self, Z):\n        A_pred = torch.sigmoid(torch.matmul(Z, Z.t()))\n        return A_pred", "\nclass GCN(nn.Module):\n    def __init__(self, in_ft, out_ft, act, bias=True, skip=False):\n        super(GCN, self).__init__()\n        self.fc = nn.Linear(in_ft, out_ft, bias=False)\n\n        if act == 'prelu':\n            self.act = nn.PReLU()\n        elif act == 'selu':\n            self.act = nn.SELU()\n        else:\n            self.act = act\n\n        if bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_ft))\n            torch.nn.init.normal_(self.bias, mean=0.0, std=0.1)\n        else:\n            self.register_parameter('bias', None)\n\n        if skip:\n            self.skip = nn.Parameter(torch.FloatTensor(out_ft))\n            torch.nn.init.normal_(self.skip, mean=0.0, std=0.1)\n        else:\n            self.register_parameter('skip', None)\n\n        for m in self.modules():\n            self.weights_init(m)\n\n    def weights_init(self, m):\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight.data)\n            if m.bias is not None:\n                m.bias.data.fill_(0.0)\n\n    # Shape of seq: (batch, nodes, features)\n    def forward(self, seq, adj, sparse=False, skip=False):\n        seq_fts = self.fc(seq)\n\n        if skip:\n            skip_out = seq_fts * self.skip\n\n        if sparse:\n            out = torch.unsqueeze(torch.spmm(adj, torch.squeeze(seq_fts, 0)), 0)\n        else:\n            out = torch.bmm(adj, seq_fts)\n\n        if skip:\n            out += skip_out\n\n        if self.bias is not None:\n            out += self.bias\n\n        return self.act(out)"]}
{"filename": "ugle/process.py", "chunked_list": ["import numpy as np\nimport scipy.sparse as sp\nimport torch\nfrom scipy.linalg import fractional_matrix_power, inv\nfrom sklearn.metrics import f1_score, normalized_mutual_info_score\nfrom scipy.optimize import linear_sum_assignment\nimport math\nimport warnings\n\nSMOOTH_K_TOLERANCE = 1e-5", "\nSMOOTH_K_TOLERANCE = 1e-5\nMIN_K_DIST_SCALE = 1e-3\nNPY_INFINITY = np.inf\n\n\ndef compute_ppr(adj: np.ndarray, alpha: int = 0.2, self_loop: bool = True):\n    \"\"\"\n    computes ppr diffusion\n    \"\"\"\n    if self_loop:\n        adj = adj + np.eye(adj.shape[0])  # A^ = A + I_n\n    d = np.diag(np.sum(adj, 1))  # D^ = Sigma A^_ii\n    dinv = fractional_matrix_power(d, -0.5)  # D^(-1/2)\n    at = np.matmul(np.matmul(dinv, adj), dinv)  # A~ = D^(-1/2) x A^ x D^(-1/2)\n    return alpha * inv((np.eye(adj.shape[0]) - (1 - alpha) * at))  # a(I_n-(1-a)A~)^-1", "\n\ndef modularity(adjacency: np.ndarray, preds: np.ndarray) -> float:\n    \"\"\"\n    computes modularity\n    \"\"\"\n    adjacency = sp.coo_matrix(adjacency).tocsr()\n    degrees = adjacency.sum(axis=0).A1\n    m = degrees.sum()\n    result = 0\n    for cluster_id in np.unique(preds):\n        cluster_indices = np.where(preds == cluster_id)[0]\n        adj_submatrix = adjacency[cluster_indices, :][:, cluster_indices]\n        degrees_submatrix = degrees[cluster_indices]\n        result += np.sum(adj_submatrix) - (np.sum(degrees_submatrix) ** 2) / m\n    return result / m", "\n\ndef conductance(adjacency: np.ndarray, preds: np.ndarray) -> float:\n    \"\"\"\n    computes conductance\n    \"\"\"\n    if len(np.unique(preds)) == 1:\n        return 1.\n    inter = 0\n    intra = 0\n    cluster_idx = np.zeros(adjacency.shape[0], dtype=bool)\n    for cluster_id in np.unique(preds):\n        cluster_idx[:] = 0\n        cluster_idx[np.where(preds == cluster_id)[0]] = 1\n        adj_submatrix = adjacency[cluster_idx, :]\n        inter += np.sum(adj_submatrix[:, cluster_idx])\n        intra += np.sum(adj_submatrix[:, ~cluster_idx])\n    return intra / (inter + intra)", "\n\ndef preds_eval(labels: np.ndarray, preds: np.ndarray, sf=4, adj: np.ndarray = None, metrics=['nmi', 'f1']) -> tuple[\n    dict, np.ndarray]:\n    \"\"\"\n    evaluates predictions given metrics\n    \"\"\"\n    # returns the correct predictions to match labels\n    eval_preds, _ = hungarian_algorithm(labels, preds)\n    results = {}\n\n    if 'nmi' in metrics:\n        nmi = normalized_mutual_info_score(labels, eval_preds)\n        results['nmi'] = float(round(nmi, sf))\n\n    if 'f1' in metrics:\n        f1 = f1_score(labels, eval_preds, average='macro')\n        results['f1'] = float(round(f1, sf))\n\n    if 'snmi' in metrics:\n        true_num_clusters = len(np.unique(labels))\n        found_num_clusters = len(np.unique(preds))\n\n        scaling_k = np.exp(-(np.abs(true_num_clusters - found_num_clusters) / true_num_clusters))\n        snmi = scaling_k * nmi\n        results['snmi'] = float(round(snmi, sf))\n\n    if 'modularity' in metrics:\n        assert adj is not None, 'adj not provided'\n        results['modularity'] = float(round(modularity(adj, eval_preds), sf))\n    if 'conductance' in metrics:\n        assert adj is not None, 'adj not provided'\n        results['conductance'] = float(round(conductance(adj, eval_preds), sf))\n\n    if 'n_clusters' in metrics:\n        results['n_clusters'] = len(np.unique(preds))\n\n    return results, eval_preds", "\n\ndef hungarian_algorithm(labels: np.ndarray, preds: np.ndarray, col_ind=None):\n    \"\"\"\n    hungarian algorithm for prediction reassignment\n    \"\"\"\n    labels = labels.astype(np.int64)\n    assert preds.size == labels.size\n    D = max(preds.max(), labels.max()) + 1\n    w = np.zeros((D, D), dtype=np.int64)\n    for i in range(preds.size):\n        w[preds[i], labels[i]] += 1\n\n    if col_ind is None:\n        row_ind, col_ind = linear_sum_assignment(w.max() - w)\n        preds = col_ind[preds]\n\n    else:\n        preds = [col_ind[int(i)] for i in preds]\n        preds = np.asarray(preds)\n\n    return preds, col_ind", "\n\ndef preprocess_features(features):\n    \"\"\"\n    Row-normalize feature matrix and convert to tuple representation\n    \"\"\"\n    rowsum = np.array(features.sum(1))\n    nonzero_indexes = np.argwhere(rowsum != 0).flatten()\n    r_inv = np.zeros_like(rowsum, dtype=float)\n    r_inv[nonzero_indexes] = np.power(rowsum[nonzero_indexes], -1)\n    r_inv[np.isinf(r_inv)] = 0.\n    r_inv[np.isnan(r_inv)] = 0.\n    r_mat_inv = sp.diags(r_inv)\n    features = r_mat_inv.dot(features)\n    if isinstance(features, np.ndarray):\n        return features\n    else:\n        return features.todense(), sparse_to_tuple(features)", "\n\ndef normalize_adj(adj):\n    \"\"\"\n    Symmetrically normalize adjacency matrix.\n    \"\"\"\n    adj = sp.coo_matrix(adj)\n    rowsum = np.array(adj.sum(1))\n    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()", "\n\ndef sparse_to_tuple(sparse_mx, insert_batch=False):\n    \"\"\"\n    Convert sparse matrix to tuple representation.\n    Set insert_batch=True if you want to insert a batch dimension.\n    \"\"\"\n\n    def to_tuple(mx):\n        if not sp.isspmatrix_coo(mx):\n            mx = mx.tocoo()\n        if insert_batch:\n            coords = np.vstack((np.zeros(mx.row.shape[0]), mx.row, mx.col)).transpose()\n            values = mx.data\n            shape = (1,) + mx.shape\n        else:\n            coords = np.vstack((mx.row, mx.col)).transpose()\n            values = mx.data\n            shape = mx.shape\n        return coords, values, shape\n\n    if isinstance(sparse_mx, list):\n        for i in range(len(sparse_mx)):\n            sparse_mx[i] = to_tuple(sparse_mx[i])\n    else:\n        sparse_mx = to_tuple(sparse_mx)\n\n    return sparse_mx", "\n\ndef sparse_mx_to_torch_sparse_tensor(sparse_mx):\n    \"\"\"\n    Convert a scipy sparse matrix to a torch sparse tensor.\n    \"\"\"\n    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n    indices = torch.from_numpy(\n        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n    values = torch.from_numpy(sparse_mx.data)\n    shape = torch.Size(sparse_mx.shape)\n    return torch.sparse.FloatTensor(indices, values, shape)", "\n\ndef euclidean_distance(point1: np.ndarray, point2: np.ndarray):\n    # calculate the Euclidean distance\n    point_iterable = zip(point1, point2)\n    distance = math.sqrt(sum([(y - x) ** 2 for x, y in point_iterable]))\n    return distance"]}
{"filename": "ugle/logger.py", "chunked_list": ["import numpy as np\nimport os\nimport logging\n\nreal_path = os.path.dirname(os.path.realpath(__file__))\nugle_path, _ = os.path.split(real_path)\n    \n\nclass CustomFormatter(logging.Formatter):\n    COLOR_CODE = {\n        'DEBUG': '\\033[94m',    # Blue\n        'INFO': '\\033[92m',     # Green\n        'WARNING': '\\033[93m',  # Yellow\n        'ERROR': '\\033[91m',    # Red\n        'CRITICAL': '\\033[95m'  # Purple\n    }\n    RESET_CODE = '\\033[0m'  # Reset to default color\n\n    def format(self, record):\n        # Get the original formatted message\n        original_message = super().format(record)\n\n        # Shorten the file path\n        record.pathname = os.path.basename(record.pathname)\n        original_message = super().format(record)\n\n        # Apply color to the log level\n        log_level = record.levelname\n        color_code = self.COLOR_CODE.get(log_level, '')\n        colored_message = f\"{color_code}{original_message}{self.RESET_CODE}\"\n        \n        return colored_message", "class CustomFormatter(logging.Formatter):\n    COLOR_CODE = {\n        'DEBUG': '\\033[94m',    # Blue\n        'INFO': '\\033[92m',     # Green\n        'WARNING': '\\033[93m',  # Yellow\n        'ERROR': '\\033[91m',    # Red\n        'CRITICAL': '\\033[95m'  # Purple\n    }\n    RESET_CODE = '\\033[0m'  # Reset to default color\n\n    def format(self, record):\n        # Get the original formatted message\n        original_message = super().format(record)\n\n        # Shorten the file path\n        record.pathname = os.path.basename(record.pathname)\n        original_message = super().format(record)\n\n        # Apply color to the log level\n        log_level = record.levelname\n        color_code = self.COLOR_CODE.get(log_level, '')\n        colored_message = f\"{color_code}{original_message}{self.RESET_CODE}\"\n        \n        return colored_message", "    \n\ndef create_logger():\n    \"\"\"\n    :return log: the loging object\n    \"\"\"\n    \n    # Create a file handler (logs will be stored in \"my_log_file.log\")\n    pokemon_names = np.load(open(ugle_path + '/data/pokemon_names.npy', 'rb'), allow_pickle=True)\n    pokemon_names = [element for element in pokemon_names if all(char.isalpha() for char in element.lower())]\n    log_path = f'{ugle_path}/logs/'\n\n    if not os.path.exists(log_path):\n        os.mkdir(log_path)\n\n    latest_file_index = len(os.listdir(log_path))\n    poke_name = np.random.choice(pokemon_names)\n    uid = f'{latest_file_index}-{poke_name}'\n\n    # Set up the logger\n    logger = logging.getLogger(uid)\n    logger.setLevel(logging.INFO)\n\n    # Create a stream handler (console output)\n    stream_handler = logging.StreamHandler()\n    stream_handler.setLevel(logging.INFO)\n\n    file_handler = logging.FileHandler(filename= log_path + uid)\n    file_handler.setLevel(logging.DEBUG)\n\n    # Set the custom formatters to the handlers\n    color_formatter = CustomFormatter(fmt=\"[%(levelname)s:%(name)s: %(asctime)s: %(module)s.%(funcName)s] %(message)s\", \n                                      datefmt=\"%d/%m--%I:%M:%S\")\n    stream_handler.setFormatter(color_formatter)\n    file_handler.setFormatter(color_formatter)\n\n    # Add the handlers to the logger\n    logger.addHandler(stream_handler)\n    logger.addHandler(file_handler)\n\n    return logger", "\n\nlog = create_logger()\n"]}
{"filename": "ugle/__init__.py", "chunked_list": ["import importlib\nimport pkgutil\n\n\ndef import_submodules(package, recursive=True):\n    \"\"\" Import all submodules of a module, recursively, including subpackages\n\n    :param package: package (name or actual module)\n    :type package: str | module\n    :rtype: dict[str, types.ModuleType]\n    \"\"\"\n    if isinstance(package, str):\n        package = importlib.import_module(package)\n    results = {}\n    for loader, name, is_pkg in pkgutil.walk_packages(package.__path__):\n        full_name = package.__name__ + '.' + name\n        results[full_name] = importlib.import_module(full_name)\n        if recursive and is_pkg:\n            results.update(import_submodules(full_name))\n    return results", "\n\nimport_submodules(__name__)\n"]}
{"filename": "ugle/utils.py", "chunked_list": ["from omegaconf import OmegaConf, open_dict, DictConfig\nfrom optuna import Study\nfrom typing import Tuple\nimport random\nimport torch\nimport numpy as np\nimport optuna\nimport os\nimport pickle\nfrom ugle.logger import log", "import pickle\nfrom ugle.logger import log\n\nneural_algorithms = ['daegc', 'dgi', 'dmon', 'grace', 'mvgrl', 'selfgnn', 'sublime', 'bgrl', 'vgaer', 'cagc', 'igo']\n\n\ndef load_model_config(override_model: str = None, override_cfg: DictConfig = None) -> DictConfig:\n    \"\"\"\n    loads the base model config, model specific config, then override_cfg, then the umap processing\n    investigation config options if relevant\n    :param override_model: name of the model to load config for\n    :param override_cfg: config to override options on loaded config\n    :return config: config for running experiments\n    \"\"\"\n    config = OmegaConf.load('ugle/configs/config.yaml')\n\n    if override_model:\n        config.model = override_model\n\n    config_name = config.model\n    model_name = config.model\n\n    # model_path structure edit if testing\n    if config_name.__contains__('_'):\n        model_name, name_ext = config_name.split(\"_\")\n        config.model = model_name\n        model_path = f'ugle/configs/models/{model_name}/{config_name}.yaml'\n    else:\n        model_path = f'ugle/configs/models/{model_name}/{model_name}.yaml'\n\n    config = merge_yaml(config, model_path)\n\n    # model_eval override\n    if override_cfg:\n        with open_dict(config):\n            config = OmegaConf.merge(config, override_cfg)\n\n    return config", "\n\ndef process_study_cfg_parameters(config: DictConfig) -> DictConfig:\n    \"\"\"\n    creates optimisation directions for study\n    :param config: config object\n    :return config: process config\n    \"\"\"\n    opt_directions = []\n    if len(config.trainer.valid_metrics) > 1:\n        config.trainer.multi_objective_study = True\n    else:\n        config.trainer.multi_objective_study = False\n\n    for metric in config.trainer.valid_metrics:\n        if metric == 'nmi' or metric == 'f1' or metric == 'modularity':\n            opt_directions.append('maximize')\n        else:\n            opt_directions.append('minimize')\n\n    config.trainer.optimisation_directions = opt_directions\n    return config", "\n\ndef extract_best_trials_info(study: Study, valid_metrics: list) -> Tuple[list, list]:\n    \"\"\"\n    extracts the best trial from a study for each given metric and associated trial\n    :param study: the study object\n    :param valid_metrics: the validation metrics \n    :return best_values: the best values for each metric\n    :return associated_trail: the associated trial with each best value\n    \"\"\"\n    best_values = study.best_trials[0].values\n    associated_trial = [study.best_trials[0].number] * len(valid_metrics)\n    if len(study.best_trials) > 1:\n        for a_best_trial in study.best_trials[1:]:\n            for i, bval in enumerate(a_best_trial.values):\n                if (bval > best_values[i] and study.directions[i].name == 'MAXIMIZE') or (\n                        bval < best_values[i] and study.directions[i].name == 'MINIMIZE'):\n                    best_values[i] = bval\n                    associated_trial[i] = a_best_trial.number\n                \n    return best_values, associated_trial", "\n\ndef merge_yaml(config: DictConfig, yaml_str: str) -> DictConfig:\n    \"\"\"\n    merges config object with config specified in yaml path str\n    :param config: config object to be merged\n    :param yaml_str: path location of .yaml string to be merged\n    :return config: merged config\n    \"\"\"\n    yaml_dict = OmegaConf.load(yaml_str)\n    with open_dict(config):\n        config = OmegaConf.merge(config, yaml_dict)\n    return config", "\n\ndef set_random(random_seed: int):\n    \"\"\"\n    sets the random seed\n    \"\"\"\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n    return", "\n\ndef set_device(gpu: int):\n    \"\"\"\n    returns the correct device\n    \"\"\"\n    if gpu != -1 and torch.cuda.is_available():\n        device = f'cuda:{gpu}'\n    else:\n        device = 'cpu'\n    return device", "\n\ndef sample_hyperparameters(trial: optuna.trial.Trial, args: DictConfig, prune_params=None) -> DictConfig:\n    \"\"\"\n    iterates through the args configuration, if an item is a list then suggests a value based on\n    the optuna trial instance\n    :param trial: instance of trial for suggestion parameters\n    :param args: config dictionary where list\n    :return: new config with values replaced where list is given\n    \"\"\"\n    vars_to_set = []\n    vals_to_set = []\n    for k, v in args.items_ex(resolve=False):\n        if not OmegaConf.is_list(v):\n            continue\n        else:\n            trial_sample = trial.suggest_categorical(k, OmegaConf.to_object(v))\n            setattr(args, k, trial_sample)\n            vars_to_set.append(k)\n            vals_to_set.append(trial_sample)\n\n    if prune_params:\n        repeated = prune_params.check_params()\n    for var, val in zip(vars_to_set, vals_to_set):\n        log.info(f\"args.{var}={val}\")\n\n    return args", "\ndef assign_test_params(config: DictConfig, best_params: dict) -> DictConfig:\n    \"\"\"\n    assigns the best params from the hyperparameter selection and assigns test config settings\n    :param config: original config for training\n    :param best_params: the best hyperparameters from training\n    :return: config for training then testing\n    \"\"\"\n    cfg = config.copy()\n    # assigns the best parameters from hp to config\n    for key, value in cfg.args.items_ex(resolve=False):\n        if not OmegaConf.is_list(value):\n            continue\n        else:\n            cfg.args[key] = best_params[key]\n\n    # assigns parameters for testing\n    cfg.trainer.train_to_valid_split = 1.0\n    cfg.trainer.n_valid_splits = 1\n    cfg.trainer.currently_testing = True\n\n    return cfg", "\n\ndef is_neural(model_name: str) -> bool:\n    \"\"\"\n    checks if an algorithm is neural or non_neural\n    :param model_name: the model name string\n    :return True if neural, False if not\n    \"\"\"\n    if model_name.__contains__('_'):\n        adjusted_model_name, _ = model_name.split(\"_\")\n    else:\n        adjusted_model_name = 'this is definitely not a name of an algorithm'\n\n    if model_name in neural_algorithms or adjusted_model_name in neural_algorithms:\n        return True\n    else:\n        raise ValueError(f\"Algorithm {model_name} is not implemented\")", "\n\ndef collate_study_results(average_results: dict, results: dict, val_idx: int) -> dict:\n    \"\"\"\n    converts study results from results\n    :param average_results: dict of metrics with numpy arrays of results across training runs\n    :param results: dictionary of results\n    :param val_idx: no. training run\n    :return:\n    \"\"\"\n    for trial in results:\n        for metric, value in trial[\"results\"].items():\n            average_results[metric][val_idx] = value\n\n    return average_results", "\n\ndef create_study_tracker(k_splits: int, metrics: list) -> dict:\n    \"\"\"\n    creates a study tracker for multiple seeds\n    :param k_splits: the number of seeds\n    :param metrics: the metrics to create a tracker for\n    :return results: the results tracker\n    \"\"\"\n    results = {}\n    for metric in metrics:\n        results[metric] = np.zeros(k_splits)\n    return results", "\n\ndef create_experiment_tracker(exp_cfg: DictConfig) -> list:\n    \"\"\"\n    creates the experiment tracker to track results\n    :param exp_cfg: experiment config\n    :experiment_tracker: list of results objects that store results from individual experiment\n    \"\"\"\n    experiment_tracker = []\n    for dataset in exp_cfg.datasets:\n        for algorithm in exp_cfg.algorithms:\n            experiment_tracker.append(OmegaConf.create(\n                            {'dataset': dataset,\n                             'algorithm': algorithm,\n                             'seeds': exp_cfg.seeds,\n                             'results': {}\n                             }))\n\n    return experiment_tracker", "\n\ndef display_saved_results(results_path: str = \"./results\"):\n    \"\"\"\n    displays results saved in memory under given directory in latex form\n    :param results_path: path where results are located\n    \"\"\"\n    for subdir, dirs, files in os.walk(results_path):\n        for file in files:\n            log.info(f\"{file}\")\n            exp_result_path = os.path.join(subdir, file)\n            exp_result = pickle.load(open(exp_result_path, \"rb\"))\n            for experiment in exp_result:\n                if hasattr(experiment, 'algorithm'):\n                    log.info(f\"{experiment.dataset}\")\n                    try:\n                        display_latex_results(experiment.algorithm_identifier, experiment.results)\n                    except:\n                        print(experiment.results)\n\n    return", "\n\ndef display_evaluation_results(experiment_tracker: list):\n    \"\"\"\n    displays the evaluation results in table latex form\n    :param experiment_tracker: list of results from experiment\n    \"\"\"\n    for experiment in experiment_tracker:\n        try: \n            display_latex_results(experiment.algorithm, experiment.results, experiment.dataset)\n        except:\n            pass\n\n    return", "\n\ndef display_latex_results(method: str, exp_result: dict, dataset):\n    \"\"\"\n    prints out latex results of experiment\n    :param method: method string\n    :param exp_result: dictionary of results\n    \"\"\"\n    display_str = f'{method} {dataset}'\n\n    if not is_neural(method):\n        n_clusters = int(exp_result['n_clusters_mean'])\n        display_str += f' ({n_clusters}) '\n\n    if 'memory_max' in exp_result.keys():\n        memory = float(exp_result['memory_max'])\n        display_str += f' ({memory}) '\n\n    metric_string = extract_result_values(exp_result)\n    display_str += metric_string\n    print(display_str)\n\n    return", "\n\ndef extract_result_values(exp_result, metrics=['f1', 'nmi', 'modularity', 'conductance']) -> str:\n    \"\"\"\n    extracts the results into a strjing ready to print for latex display\n    if std values exist then they are also used\n    :param exp_result: dictionary of results\n    :param metrics: list of metrics in results\n    :return metric_string: string to print given results\n    \"\"\"\n    metric_string = ''\n    for metric in metrics:\n        metric_string += ' & '\n        metric_mean = f'{metric}_mean'\n        metric_val = exp_result[metric_mean]\n        metric_string += str(metric_val)\n\n        metric_std = f'{metric}_std'\n        if metric_std in exp_result.keys():\n            std = str(exp_result[metric_std])\n            metric_string += f'\\pm {std}'\n\n    metric_string += ' \\\\\\\\'\n    return metric_string", "\n\ndef save_experiment_tracker(result_tracker: list, results_path: str):\n    \"\"\"\n    pickle saves result tracker to results path\n    :param result_tracker: list of experiment results\n    :param results_path: the path to save results in\n    \"\"\"\n    if not os.path.exists(results_path):\n        os.makedirs(results_path)\n    experiment_identifier = log.name\n    log.info(f'saving to {results_path}{experiment_identifier}')\n    pickle.dump(result_tracker, open(f\"{results_path}{experiment_identifier}.pkl\", \"wb\"))\n\n    return", "\n\ndef calc_average_results(results_tracker: dict) -> dict:\n    \"\"\"\n    calculates the average of the performance statistic to an appropriate s.f.\n    :param results_tracker: contains results of over seeds\n    :return average_results: the average of results over all seeds\n    \"\"\"\n    average_results = {}\n    for stat, values in results_tracker.items():\n        if stat != 'memory':\n            average_results[f'{stat}_mean'] = float(np.format_float_positional(np.mean(values, axis=0),\n                                                                               precision=4,\n                                                                               unique=False,\n                                                                               fractional=False))\n            if len(values) > 1:\n                average_results[f'{stat}_std'] = float(np.format_float_positional(np.std(values, axis=0),\n                                                                                  precision=2,\n                                                                                  unique=False,\n                                                                                  fractional=False))\n        else:\n            average_results[f'memory_max'] = float(np.format_float_positional(np.max(values, axis=0),\n                                                                              precision=7,\n                                                                              unique=False,\n                                                                              fractional=False))\n    return average_results", "\n\n\ndef get_best_args(results, model_resolution_metric):\n    best = -1.\n    for seed_res in results: \n        for metric_res in seed_res['study_output']:\n            if model_resolution_metric in metric_res['metrics'] and best < metric_res['results'][model_resolution_metric]: \n                best = metric_res['results'][model_resolution_metric]\n                best_seed = seed_res['seed']\n                args = metric_res['args']\n    return args, best_seed", ""]}
{"filename": "ugle/datasets.py", "chunked_list": ["import os\nfrom omegaconf import OmegaConf, DictConfig\nimport numpy as np\nfrom karateclub.dataset import GraphReader\nimport networkx as nx\nimport zipfile\nimport gdown\nfrom pathlib import Path\nimport shutil\nimport torch", "import shutil\nimport torch\nimport copy\nimport random\nimport scipy.sparse as sp\nimport plotly.graph_objects as go\nfrom typing import Union\nfrom ugle.logger import log, ugle_path\nfrom typing import Tuple\nfrom torch_geometric.utils import to_dense_adj, stochastic_blockmodel_graph", "from typing import Tuple\nfrom torch_geometric.utils import to_dense_adj, stochastic_blockmodel_graph\nimport torch\n\ngoogle_store_datasets = ['acm', 'amac', 'amap', 'bat', 'citeseer', 'cora', 'cocs', 'dblp', 'eat', 'uat', 'pubmed',\n                         'cite', 'corafull', 'texas', 'wisc', 'film', 'cornell']\nkarate_club_datasets = ['facebook', 'twitch', 'wikipedia', 'github', 'lastfm', 'deezer']\nall_datasets = (google_store_datasets + karate_club_datasets)\n\n\ndef check_data_presence(dataset_name: str) -> bool:\n    \"\"\"\n    checks for dataset presence in local memory\n    :param dataset_name: dataset name to check\n    \"\"\"\n    dataset_path = ugle_path + f'/data/{dataset_name}'\n    if not os.path.exists(dataset_path):\n        return False\n    elif not os.path.exists(f'{dataset_path}/{dataset_name}_feat.npy'):\n        return False\n    elif not os.path.exists(f'{dataset_path}/{dataset_name}_label.npy'):\n        return False\n    elif not os.path.exists(f'{dataset_path}/{dataset_name}_adj.npy'):\n        return False\n    else:\n        return True", "\n\ndef check_data_presence(dataset_name: str) -> bool:\n    \"\"\"\n    checks for dataset presence in local memory\n    :param dataset_name: dataset name to check\n    \"\"\"\n    dataset_path = ugle_path + f'/data/{dataset_name}'\n    if not os.path.exists(dataset_path):\n        return False\n    elif not os.path.exists(f'{dataset_path}/{dataset_name}_feat.npy'):\n        return False\n    elif not os.path.exists(f'{dataset_path}/{dataset_name}_label.npy'):\n        return False\n    elif not os.path.exists(f'{dataset_path}/{dataset_name}_adj.npy'):\n        return False\n    else:\n        return True", "\n\ndef download_graph_data(dataset_name: str) -> bool:\n    \"\"\"\n    downloads a graph dataset\n    :param dataset_name: name of the dataset to download\n    :return True if successful\n    \"\"\"\n    log.info(f'downloading {dataset_name}')\n    download_link_path = ugle_path + '/data/download_links.yaml'\n    download_links = OmegaConf.load(download_link_path)\n    url = download_links[dataset_name]\n    dataset_path = ugle_path + f'/data/{dataset_name}'\n    if not os.path.exists(dataset_path):\n        os.mkdir(dataset_path)\n\n    dataset_zip_path = dataset_path + f'/{dataset_name}.zip'\n    gdown.download(url=url, output=dataset_zip_path, quiet=False, fuzzy=True)\n    log.info('finished downloading')\n\n    # extract the zip file\n    log.info('extracting dataset')\n    with zipfile.ZipFile(dataset_zip_path, 'r') as zip_ref:\n        zip_ref.printdir()\n        zip_ref.extractall(dataset_path)\n    log.info('extraction complete')\n\n    # correct the path dir\n    extended_path = f'{dataset_path}/{dataset_name}'\n    dataset_path += '/'\n    if os.path.exists(extended_path):\n        log.info('extraction to wrong location')\n        for subdir, dirs, files in os.walk(extended_path):\n            for file in files:\n                extract_path = os.path.join(subdir, file)\n                file = Path(file)\n                out_path = os.path.join(dataset_path, file)\n                log.info(f'Extracting {extract_path} to ... {out_path} ...')\n                shutil.move(Path(extract_path), Path(out_path))\n\n        shutil.rmtree(extended_path)\n\n    return True", "\n\ndef load_real_graph_data(dataset_name: str, test_split: float = 0.5, split_scheme: str = 'drop_edges',\n                         split_addition=None) -> Tuple[\n    np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    loads the graph dataset and splits the adjacency matrix into two\n    :param dataset_name: name of the dataset\n    :param test_split: percentage of edges to keep\n    :return features, label, train_adj, adjacency: loaded graph data\n    \"\"\"\n    assert dataset_name in all_datasets, f\"{dataset_name} not a real dataset\"\n\n    if dataset_name in google_store_datasets:\n        if not check_data_presence(dataset_name):\n            extraction_success = download_graph_data(dataset_name)\n            assert extraction_success, f'download/extraction of dataset {dataset_name} failed'\n            assert check_data_presence(dataset_name)\n\n        dataset_path = ugle_path + f'/data/{dataset_name}/{dataset_name}'\n        features = np.load(dataset_path + \"_feat.npy\", allow_pickle=True)\n        label = np.load(dataset_path + \"_label.npy\", allow_pickle=True)\n        adjacency = np.load(dataset_path + \"_adj.npy\", allow_pickle=True)\n\n    elif dataset_name in karate_club_datasets:\n        loader = GraphReader(dataset_name)\n        features = loader.get_features().todense()\n        label = loader.get_target()\n        adjacency = nx.to_numpy_matrix(loader.get_graph())\n\n    if split_addition:\n        adjacency, _ = aug_drop_adj(adjacency, drop_percent=1-split_addition, split_adj=False)\n\n    log.debug('splitting dataset into training/testing')\n    train_adj, test_adj = split_adj(adjacency, test_split, split_scheme)\n\n    return features, label, train_adj, test_adj", "\n\ndef compute_datasets_info(dataset_names: list, visualise: bool=False):\n    \"\"\"\n    computes the information about dataset statistics\n    :param dataset_names: list of datasets to look at\n    \"\"\"\n\n    clustering_x_data = []\n    closeness_y_data = []\n\n    for dataset_name in dataset_names:\n        features, label, train_adjacency, test_adjacency = load_real_graph_data(dataset_name, test_split=1.)\n        display_string = dataset_name + ' & '  # name\n        display_string += str(train_adjacency.shape[0]) + ' & '  # n_nodes\n        display_string += str(features.shape[1]) + ' & '  # n_features\n        display_string += str(int(np.nonzero(train_adjacency)[0].shape[0]/2)) + ' & '  # n_edges\n        display_string += str(len(np.unique(label))) + ' & '  # n_classes\n\n        nx_g = nx.Graph(train_adjacency)\n        clustering = nx.average_clustering(nx_g)\n        cercania = nx.closeness_centrality(nx_g)\n        cercania = np.mean(list(cercania.values()))\n\n        clustering_x_data.append(clustering)\n        closeness_y_data.append(cercania)\n\n        display_string += str(round(clustering, 3)) + ' & '  # clustering coefficient\n        display_string += str(round(cercania, 3)) + ' \\\\\\\\'  # closeness centrality\n        if visualise:\n            print(display_string)\n    if visualise:\n        _ = display_figure_dataset_stats(clustering_x_data, closeness_y_data, dataset_names)\n\n    return clustering_x_data, closeness_y_data", "\n\ndef display_figure_dataset_stats(x_data: list, y_data: list, datasets: list):\n    \"\"\"\n    function to display dataset statistics on a graph\n    :param x_data: clustering coefficient data for x-axis\n    :param y_data: closeness centrality data for y-axis\n    :param datasets: list of datasets metrics were computed over\n    :return fig: figure to be displayed\n    \"\"\"\n    fig = go.Figure()\n    fig.add_trace(go.Scatter(\n        x=x_data, y=y_data, text=datasets, textposition=\"top center\",\n        color_discrete_sequence=['red']\n    ))\n\n    # layout options\n    layout = dict(\n                font=dict(\n                    size=18),\n              plot_bgcolor='white',\n              paper_bgcolor='white',\n              margin=dict(t=10, b=10, l=10, r=10, pad=0),\n              xaxis=dict(title='Average Clustering Coefficient',\n                         linecolor='black',\n                         showgrid=False,\n                         showticklabels=False,\n                         mirror=True),\n              yaxis=dict(title='Mean Closeness Centrality',\n                         linecolor='black',\n                         showgrid=False,\n                         showticklabels=False,\n                         mirror=True))\n    fig.update_layout(layout)\n    # save figure\n    if not os.path.exists(\"images\"):\n        os.mkdir(\"images\")\n    fig.write_image(\"images/dataset_stats.png\")\n\n    return fig", "\n\ndef aug_drop_features(input_feature: Union[np.ndarray, torch.Tensor], drop_percent: float = 0.2):\n    \"\"\"\n    augmentation by randomly masking features for every node\n    :param input_feature: feature matrix\n    :param drop_percent: percent that any feature is dropped\n    :return aug_feature: augmented feature matrix\n    \"\"\"\n    node_num = input_feature.shape[1]\n    mask_num = int(node_num * drop_percent)\n    node_idx = [i for i in range(node_num)]\n    mask_idx = random.sample(node_idx, mask_num)\n    aug_feature = copy.deepcopy(input_feature)\n    if type(aug_feature) == np.ndarray:\n        zeros = np.zeros_like(aug_feature[0][0])\n    else:\n        zeros = torch.zeros_like(aug_feature[0][0])\n    for j in mask_idx:\n        aug_feature[0][j] = zeros\n    return aug_feature", "\n\ndef aug_drop_adj(input_adj: np.ndarray, drop_percent: float = 0.2, split_adj: bool = False):\n    \"\"\"\n    augmentation by randomly dropping edges with given probability\n    :param input_adj: input adjacency matrix\n    :param drop_percent: percent that any edge is dropped\n    :return aug_adj: augmented adjacency matrix\n    \"\"\"\n\n    index_list = input_adj.nonzero()\n    row_idx = index_list[0].tolist()\n    col_idx = index_list[1].tolist()\n\n    index_list = []\n    for i in range(len(row_idx)):\n        index_list.append((row_idx[i], col_idx[i]))\n\n    edge_num = int(len(row_idx))\n    add_drop_num = int(edge_num * drop_percent)\n    aug_adj = copy.deepcopy(input_adj.tolist())\n    else_adj = np.zeros_like(input_adj)\n\n    edge_idx = list(np.arange(edge_num))\n    drop_idx = random.sample(edge_idx, add_drop_num)\n    n_drop = len(drop_idx)\n    log.debug(f'dropping {n_drop} edges from {edge_num}')\n\n    for i in drop_idx:\n        aug_adj[index_list[i][0]][index_list[i][1]] = 0\n        aug_adj[index_list[i][1]][index_list[i][0]] = 0\n\n        else_adj[index_list[i][0]][index_list[i][1]] = 1\n        else_adj[index_list[i][1]][index_list[i][0]] = 1\n\n    aug_adj = np.array(aug_adj)\n    if split_adj: \n        return aug_adj, else_adj\n    else:\n        return aug_adj, input_adj", "\n\ndef numpy_to_edge_index(adjacency: np.ndarray):\n    \"\"\"\n    converts adjacency in numpy array form to an array of active edges\n    :param adjacency: input adjacency matrix\n    :return adjacency: adjacency matrix update form\n    \"\"\"\n    adj_label = sp.coo_matrix(adjacency)\n    adj_label = adj_label.todok()\n\n    outwards = [i[0] for i in adj_label.keys()]\n    inwards = [i[1] for i in adj_label.keys()]\n\n    adjacency = np.array([outwards, inwards], dtype=np.int)\n    return adjacency", "\n\nclass Augmentations:\n    \"\"\"\n    A utility for graph data augmentation\n    \"\"\"\n\n    def __init__(self, method='gdc'):\n        methods = {\"split\", \"standardize\", \"katz\"}\n        assert method in methods\n        self.method = method\n\n    @staticmethod\n    def _split(features, permute=True):\n        \"\"\"\n        Data augmentation is build by spliting data along the feature dimension.\n\n        :param data: the data object to be augmented\n        :param permute: Whether to permute along the feature dimension\n\n        \"\"\"\n        perm = np.random.permutation(features.shape[1]) if permute else np.arange(features.shape[1])\n        features = features[:, perm]\n        size = features.shape[1] // 2\n        x1 = features[:, :size]\n        x2 = features[:, size:]\n\n        return x1, x2\n\n    @staticmethod\n    def _standardize(features):\n        \"\"\"\n        Applies a zscore node feature data augmentation.\n\n        :param data: The data to be augmented\n        :return: a new augmented instance of the input data\n        \"\"\"\n        mean, std = features.mean(), features.std()\n        new_data = (features - mean) / (std + 10e-7)\n        return new_data\n\n    @staticmethod\n    def _katz(features, adjacency, beta=0.1, threshold=0.0001):\n        \"\"\"\n        Applies a Katz-index graph topology augmentation\n\n        :param data: The data to be augmented\n        :return: a new augmented instance of the input data\n        \"\"\"\n        n_nodes = features.shape[0]\n\n        a_hat = adjacency + sp.eye(n_nodes)\n        d_hat = sp.diags(\n            np.array(1 / np.sqrt(a_hat.sum(axis=1))).reshape(n_nodes))\n        a_hat = d_hat @ a_hat @ d_hat\n        temp = sp.eye(n_nodes) - beta * a_hat\n        h_katz = (sp.linalg.inv(temp.tocsc()) * beta * a_hat).toarray()\n        mask = (h_katz < threshold)\n        h_katz[mask] = 0.\n        edge_index = np.array(h_katz.nonzero())\n        edge_attr = torch.tensor(h_katz[h_katz.nonzero()], dtype=torch.float32)\n\n        return edge_index\n\n    def __call__(self, features, adjacency):\n        \"\"\"\n        Applies different data augmentation techniques\n        \"\"\"\n\n        if self.method == \"katz\":\n            aug_adjacency = self._katz(features, adjacency)\n            aug_adjacency = np.array([aug_adjacency[0], aug_adjacency[1]], dtype=np.int)\n            adjacency = adjacency.todense()\n            adjacency = numpy_to_edge_index(adjacency)\n            aug_features = features.copy()\n        elif self.method == 'split':\n            features, aug_features = self._split(features)\n            adjacency = adjacency.todense()\n            adjacency = numpy_to_edge_index(adjacency)\n            aug_adjacency = adjacency.copy()\n        elif self.method == \"standardize\":\n            aug_features = self._standardize(features)\n            adjacency = adjacency.todense()\n            adjacency = numpy_to_edge_index(adjacency)\n            aug_adjacency = adjacency.copy()\n\n        return features, adjacency, aug_features, aug_adjacency\n\n    def __str__(self):\n        return self.method.title()", "\n\ndef split(n, k):\n    d, r = divmod(n, k)\n    return [d + 1] * r + [d] * (k - r)\n\n\ndef create_synth_graph(n_nodes: int, n_features: int , n_clusters: int, adj_type: str, feature_type: str = 'random'):\n    if adj_type == 'disjoint':\n        probs = (np.identity(n_clusters)).tolist()\n    elif adj_type == 'random':\n        probs = (np.ones((n_clusters, n_clusters))/n_clusters).tolist()\n    elif adj_type == 'complete':\n        probs = np.ones((n_clusters, n_clusters)).tolist()\n\n    cluster_sizes = split(n_nodes, n_clusters)\n    adj = to_dense_adj(stochastic_blockmodel_graph(cluster_sizes, probs)).squeeze(0).numpy()\n    \n    if feature_type == 'random':\n        features = torch.normal(mean=0, std=1, size=(n_nodes, n_features)).numpy()\n        features = np.where(features > 0., 1, 0)\n    elif feature_type == 'complete': \n        features = np.ones((n_nodes, n_features))\n    elif feature_type == 'disjoint':\n        features = np.zeros((n_nodes, n_features))\n        feature_dims_fo_cluster = split(n_features, n_clusters)\n        start_feat = 0\n        end_feat = feature_dims_fo_cluster[0]\n        start_clus = 0\n        end_clus = cluster_sizes[0]\n        for i in range(len(feature_dims_fo_cluster)):\n            features[start_clus:end_clus, start_feat:end_feat] = np.ones_like(features[start_clus:end_clus, start_feat:end_feat])\n            if i == len(feature_dims_fo_cluster) - 1:\n                break\n            start_feat += feature_dims_fo_cluster[i]\n            end_feat += feature_dims_fo_cluster[i+1]\n            start_clus += cluster_sizes[i]\n            end_clus += cluster_sizes[i+1]\n\n    labels = []\n    for i in range(n_clusters):\n        labels.extend([i] * cluster_sizes[i])\n    labels = np.array(labels)\n\n    return adj, features.astype(float), labels", "\n\ndef split_adj(adj, percent, split_scheme):\n    if split_scheme == 'drop_edges':\n        # drops edges from dataset to form new adj \n        if percent != 1.:\n            train_adjacency, validation_adjacency = aug_drop_adj(adj, drop_percent=1 - percent, split_adj=False)\n        else:\n            train_adjacency = adj\n            validation_adjacency = copy.deepcopy(adj)\n    elif split_scheme == 'split_edges':\n        # splits the adj via the edges so that no edges in both \n        if percent != 1.:\n            train_adjacency, validation_adjacency = aug_drop_adj(adj, drop_percent=1 - percent, split_adj=True)\n        else:\n            train_adjacency = adj\n            validation_adjacency = copy.deepcopy(adj)\n\n    elif split_scheme == 'all_edges':\n        # makes the adj fully connected \n        train_adjacency = np.ones_like(adj)\n        validation_adjacency = np.ones_like(adj)\n\n    elif split_scheme == 'no_edges':\n        # makes the adj completely unconnected \n        train_adjacency = np.zeros_like(adj)\n        validation_adjacency = np.zeros_like(adj)\n    \n    return train_adjacency, validation_adjacency"]}
{"filename": "ugle/helper.py", "chunked_list": ["from omegaconf import OmegaConf\nfrom ugle.process import euclidean_distance\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import wasserstein_distance, gaussian_kde\nimport os\nimport matplotlib\nfrom ugle.logger import ugle_path\nfrom copy import deepcopy", "from ugle.logger import ugle_path\nfrom copy import deepcopy\n\ndef search_results(folder, filename):\n\n    for root, dirs, files in os.walk(f'{ugle_path}/{folder}'):\n        if filename in files:\n            return os.path.join(root, filename)\n    return None\n", "\n\ndef get_all_results_from_storage(datasets: list, algorithms: list, folder: str, empty: str = 'minus_ten',\n                                 collect_all: bool = False):\n    if empty == 'minus_ten':\n        empty_result = OmegaConf.create(\n            {'conductance_mean': -10.0,\n             'conductance_std': -10.0,\n             'f1_mean': -10.0,\n             'f1_std': -10.0,\n             'modularity_mean': -10.0,\n             'modularity_std': -10.0,\n             'nmi_mean': -10.0,\n             'nmi_std': -10.0,\n             })\n    elif empty == 'zeros':\n        empty_result = OmegaConf.create(\n            {'conductance_mean': 0.0,\n             'conductance_std': 0.0,\n             'f1_mean': 0.0,\n             'f1_std': 0.0,\n             'modularity_mean': 0.0,\n             'modularity_std': 0.0,\n             'nmi_mean': 0.0,\n             'nmi_std': 0.0,\n             })\n\n    result_holder = OmegaConf.create({})\n\n    for dataset in datasets:\n        for algo in algorithms:\n            filename = f\"{dataset}_{algo}.pkl\"\n            file_found = search_results(folder, filename)\n            if file_found:\n                result = pickle.load(open(file_found, \"rb\"))\n                if collect_all:\n                    # parse seed\n                    return_results = np.zeros(shape=(4, 10))\n                    # go thru every seed\n                    for i, seed_result in enumerate(result.results):\n                        # go thru every best hps configuration\n                        for metric_result in seed_result.study_output:\n                            # go thru each best metric in configuration\n                            for metric in metric_result.metrics:\n                                # add result to correct place\n                                if metric == 'f1':\n                                    return_results[0, i] = metric_result.results[metric]\n                                elif metric == 'nmi':\n                                    return_results[1, i] = metric_result.results[metric]\n                                elif metric == 'modularity':\n                                    return_results[2, i] = metric_result.results[metric]\n                                elif metric == 'conductance':\n                                    return_results[3, i] = metric_result.results[metric]\n\n                    result_holder[f\"{dataset}_{algo}\"] = return_results.tolist()\n\n                else:\n                    result_holder[f\"{dataset}_{algo}\"] = result.average_results\n            else:\n                if collect_all:\n                    if empty == 'minus_ten':\n                        place_holder = np.ones(shape=(4, 10)) * -10\n                        result_holder[f\"{dataset}_{algo}\"] = place_holder.tolist()\n                    elif empty == 'zeros':\n                        result_holder[f\"{dataset}_{algo}\"] = np.zeros(shape=(4, 10))\n                else:\n                    result_holder[f\"{dataset}_{algo}\"] = empty_result\n\n    return result_holder", "\n\ndef get_values_from_results_holder(result_holder, dataset_name, metric_name, return_std=False):\n    metric_values = []\n    if return_std:\n        std_values = []\n    for k, v in result_holder.items():\n        if k.__contains__(dataset_name):\n            metric_values.append(v[f'{metric_name}_mean'])\n            if return_std:\n                std_values.append(v[f'{metric_name}_std'])\n\n    if return_std:\n        return metric_values, std_values\n    else:\n        return metric_values", "\n\ndef make_test_performance_object(datasets, algorithms, metrics, seeds, folder):\n    # get results object\n    result_object = np.zeros(shape=(len(datasets), len(algorithms), len(metrics), len(seeds)))\n    try:\n        result_holder = get_all_results_from_storage(datasets, algorithms, folder, collect_all=True)\n    except:\n        return result_object\n    for d, dataset in enumerate(datasets):\n        for a, algo in enumerate(algorithms):\n            result_object[d, a] = result_holder[f\"{dataset}_{algo}\"]\n    return result_object", "\n\ndef calculate_abs_std(result_object, datasets, metrics):\n    # calculate deviation over each seed\n    std_object = np.zeros(shape=np.shape(result_object)[:-1])\n    for d, _ in enumerate(datasets):\n        for m, _ in enumerate(metrics):\n            std_object[d, :, m] = np.std(result_object[d, :, m, :] , axis=1)\n    return std_object\n", "\n\ndef calculate_ranking_performance(result_object, datasets, metrics, seeds, scale_metrics=False, calc_ave_first=False):\n    if calc_ave_first:\n        # calculate ranking on each seed\n        ranking_object = np.zeros(shape=np.shape(result_object)[:-1])\n        for d, _ in enumerate(datasets):\n            for m, metric_name in enumerate(metrics):\n                metric_values = np.mean(result_object[d, :, m, :] , axis=1)\n                last_place_zero = np.argwhere(np.array(metric_values) == -10).flatten()\n                if metric_name != 'conductance':\n                    ranking_of_algorithms = np.flip(np.argsort(metric_values)) + 1\n                else:\n                    ranking_of_algorithms = np.argsort(metric_values) + 1\n                ranking_of_algorithms[last_place_zero] = len(ranking_of_algorithms)\n                if scale_metrics:\n                    ranking_of_algorithms = scale_metric_values(ranking_of_algorithms, metric_values, metric_name)\n                ranking_object[d, :, m] = ranking_of_algorithms\n    else:\n        # calculate ranking on each seed\n        ranking_object = np.zeros_like(result_object)\n        for d, _ in enumerate(datasets):\n            for m, metric_name in enumerate(metrics):\n                for s, _, in enumerate(seeds):\n                    metric_values = result_object[d, :, m, s]\n                    last_place_zero = np.argwhere(np.array(metric_values) == -10).flatten()\n                    if metric_name != 'conductance':\n                        ranking_of_algorithms = np.flip(np.argsort(metric_values)) + 1\n                    else:\n                        ranking_of_algorithms = np.argsort(metric_values) + 1\n                    ranking_of_algorithms[last_place_zero] = len(ranking_of_algorithms)\n                    if scale_metrics:\n                        ranking_of_algorithms = scale_metric_values(ranking_of_algorithms, metric_values, metric_name)\n                    ranking_object[d, :, m, s] = ranking_of_algorithms\n    \n    return ranking_object", "\n\ndef create_result_bar_chart(dataset_name, algorithms, folder, default_algos, default_folder, ax=None):\n    \"\"\"\n    displays the results in matplotlib with dashed borders for original comparison on single dataset\n    :param hpo_results: hyperparameter results\n    :param default_results: default parameter results\n    :param dataset_name: the dataset on which the results were gathered\n    :param ax: optional input axis\n    :return ax: axis on which figure is displayed\n\n    \"\"\"\n    if not ax:\n        fig, ax = plt.subplots(figsize=(10, 5))\n\n    #alt_colours = ['#dc143c', '#0bb5ff', '#2ca02c', '#800080']\n    alt_colours = ['C2', 'C0', 'C1', 'C3']\n\n    # extract key arrays for results\n    result_holder = get_all_results_from_storage([dataset_name], algorithms, folder, empty='zeros')\n    default_result_holder = get_all_results_from_storage([dataset_name], default_algos, default_folder, empty='zeros')\n\n    f1, f1_std = get_values_from_results_holder(result_holder, dataset_name, 'f1', return_std=True)\n    nmi, nmi_std = get_values_from_results_holder(result_holder, dataset_name, 'nmi', return_std=True)\n    modularity, modularity_std = get_values_from_results_holder(result_holder, dataset_name, 'modularity',\n                                                                return_std=True)\n    conductance, conductance_std = get_values_from_results_holder(result_holder, dataset_name, 'conductance',\n                                                                  return_std=True)\n\n    default_f1, default_f1_std = get_values_from_results_holder(default_result_holder, dataset_name, 'f1',\n                                                                return_std=True)\n    default_nmi, default_nmi_std = get_values_from_results_holder(default_result_holder, dataset_name, 'nmi',\n                                                                  return_std=True)\n    default_modularity, default_modularity_std = get_values_from_results_holder(default_result_holder, dataset_name,\n                                                                                'modularity',\n                                                                                return_std=True)\n    default_conductance, default_conductance_std = get_values_from_results_holder(default_result_holder, dataset_name,\n                                                                                  'conductance',\n                                                                                  return_std=True)\n\n    bar_width = 1 / 4\n    x_axis_names = np.arange(len(algorithms))\n\n    # plot hyperparameter results in full colour\n    ax.bar(x_axis_names, f1, yerr=f1_std,\n           width=bar_width, facecolor=alt_colours[0], alpha=0.9, linewidth=0, label='f1')\n    ax.bar(x_axis_names + bar_width, nmi, yerr=nmi_std,\n           width=bar_width, facecolor=alt_colours[1], alpha=0.9, linewidth=0, label='nmi')\n    ax.bar(x_axis_names + (2 * bar_width), modularity, yerr=modularity_std,\n           width=bar_width, facecolor=alt_colours[2], alpha=0.9, linewidth=0, label='modularity')\n    ax.bar(x_axis_names + (3 * bar_width), conductance, yerr=conductance_std,\n           width=bar_width, facecolor=alt_colours[3], alpha=0.9, linewidth=0, label='conductance')\n\n    # plot default parameters bars in dashed lines\n    blank_colours = np.zeros(4)\n    ax.bar(x_axis_names, default_f1, width=bar_width,\n           facecolor=blank_colours, edgecolor='black', linewidth=2, linestyle='--', label='default values')\n    ax.bar(x_axis_names + bar_width, default_nmi, width=bar_width,\n           facecolor=blank_colours, edgecolor='black', linewidth=2, linestyle='--')\n    ax.bar(x_axis_names + (2 * bar_width), default_modularity, width=bar_width,\n           facecolor=blank_colours, edgecolor='black', linewidth=2, linestyle='--')\n    ax.bar(x_axis_names + (3 * bar_width), default_conductance,\n           facecolor=blank_colours, width=bar_width, edgecolor='black', linewidth=2, linestyle='--')\n\n    # create the tick labels for axis\n    ax.set_xticks(x_axis_names - 0.5 * bar_width)\n    ax.set_xticklabels(algorithms, ha='left', rotation=-45, position=(-0.3, 0))\n    ax.set_axisbelow(True)\n\n    # Axis styling.\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_color('#DDDDDD')\n    ax.yaxis.grid(True, color='#EEEEEE')\n    ax.xaxis.grid(False)\n\n    # tighten the layout\n    ax.set_title(dataset_name, y=0.95, fontsize=98)\n    for item in ([ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n        item.set_fontsize(42)\n\n    return ax", "\n\ndef create_dataset_landscape_fig(ax, title, datasets, algorithms, metrics, seeds, folder, scale_metrics=False, calc_ave_first=False):\n    # get datasest information\n    try:\n        clustering = pickle.load(open(f\"{ugle_path}/dataset_stats/clustering.pkl\", \"rb\"))\n        closeness = pickle.load(open(f\"{ugle_path}/dataset_stats/closeness.pkl\", \"rb\"))\n    except:\n        from ugle.datasets import compute_datasets_info\n        clustering, closeness = compute_datasets_info(datasets)\n        pickle.dump(clustering, open(f\"{ugle_path}/dataset_stats/clustering.pkl\", \"wb\"))\n        pickle.dump(closeness, open(f\"{ugle_path}/dataset_stats/closeness.pkl\", \"wb\"))\n\n    # make data landscape\n    dataset_points = np.array([clustering, closeness])\n    n_dataset = len(clustering)\n    n_points = 512\n    x_range = 0.8\n    y_range = 0.6\n    X, Y = np.meshgrid(np.linspace(0, x_range, n_points), np.linspace(0, y_range, n_points))\n    xy = np.vstack((X.flatten(), Y.flatten())).T\n    # find the idx of the dataset point closest in the landscape\n    all_distances = np.zeros((n_dataset, np.shape(xy)[0]))\n    for i in range(n_dataset):\n        all_distances[i, :] = [euclidean_distance(x, dataset_points[:, i]) for x in xy]\n    closest_point = np.argsort(all_distances, axis=0)[0]\n    Z = closest_point.reshape((n_points, n_points))\n\n    # get results\n    result_object = make_test_performance_object(datasets, algorithms, metrics, seeds, folder)\n    ranking_object = calculate_ranking_performance(result_object, datasets, metrics, seeds, scale_metrics=scale_metrics, calc_ave_first=calc_ave_first)\n    if calc_ave_first:\n        # calculate the ranking averages \n        ranking_over_datasets = np.mean(ranking_object, axis=2)\n    else:\n        # calculate the ranking averages \n        ranking_over_datasets = np.mean(ranking_object, axis=(2, 3))\n    \n    # get best algorithm for each dataset\n    best_algo_idx_over_datasets = np.argsort(np.array(ranking_over_datasets), axis=1)[:, 0]\n    # work out best algorithm for any point on dataset landscape\n    best_algo_over_space = best_algo_idx_over_datasets[Z]\n    n_best_algos = len(np.unique(best_algo_over_space))\n    # scale the best algorithm space to work nicely with visualisation\n    dataset_replace = np.array(list(range(n_dataset)))\n    dataset_replace[np.unique(best_algo_over_space)] = np.array(list(range(n_best_algos)))\n    scaled_best_algo_over_space = dataset_replace[best_algo_over_space]\n    # get colouring for the visualisation\n    cmap = plt.get_cmap('Blues', n_best_algos)\n    cax = ax.imshow(np.flip(scaled_best_algo_over_space, axis=0), cmap=cmap, interpolation='nearest',\n                    extent=[0, x_range, 0, y_range])\n    # add color bar\n    best_algo_list = [algorithms[i] for i in np.unique(best_algo_over_space)]\n    best_algo_list = [albel.split(\"_\")[0] for albel in best_algo_list]\n    if len(best_algo_list) == 2:\n        tick_spacing = [0.25, 0.75]\n    elif len(best_algo_list) == 3:\n        tick_spacing = [0.33, 1., 1.66]\n    else:\n        tick_spacing = np.linspace(0.5, n_best_algos - 1.5, n_best_algos)\n    cbar = plt.colorbar(cax, ticks=range(n_best_algos), orientation=\"vertical\", shrink=0.8)\n    cbar.set_ticks(tick_spacing)\n    cbar.ax.set_yticklabels(best_algo_list, fontsize=14, ha='left', rotation=-40)\n    # add dataset locations\n    ax.scatter(clustering, closeness, marker='x', s=20, color='black', label='datasets')\n    # add nice visual elements\n    ax.set_xlabel('Clustering Coefficient', fontsize=16)\n    ax.set_ylabel('Closeness Centrality', fontsize=16)\n    ax.set_title(title, fontsize=22, pad=15)\n    ax.tick_params(axis='y', labelsize=18)\n    ax.tick_params(axis='x', labelsize=18)\n    return ax", "\n\ndef create_ranking_charts(datasets: list, algorithms: list, metrics: list, seeds: list, folder: str, title_name: str, \n    scale_metrics: bool=True, calc_ave_first: bool=False, set_legend: bool=True, ax=None, ax1=None, fig=None, fig1=None):\n\n    # init axis\n    if not ax and not fig:\n        fig, ax = plt.subplots(figsize=(6, 6))\n    elif (ax and not fig) or (fig and not ax):\n        print('please provide both fig and ax or neither')\n\n    if not ax1:\n        fig1, ax1 = plt.subplots(figsize=(6, 6))\n    elif (ax1 and not fig1) or (fig1 and not ax1):\n        print('please provide both fig1 and ax1 or neither')\n\n    # fetch results\n    result_object = make_test_performance_object(datasets, algorithms, metrics, seeds, folder)\n    # calculate ranking \n    ranking_object = calculate_ranking_performance(result_object, datasets, metrics, seeds, scale_metrics=scale_metrics, calc_ave_first=calc_ave_first)\n    # calculate overrall ranking and sensitivity + order\n    if calc_ave_first:\n        overrall_ranking = np.mean(ranking_object, axis=(0, 2))\n        # calculate the ranking averages \n        ranking_over_metrics = np.mean(ranking_object, axis=0)\n        ranking_over_datasets = np.mean(ranking_object, axis=2)\n    else:\n        overrall_ranking = np.mean(ranking_object, axis=(0, 2, 3))\n        # calculate the ranking averages \n        ranking_over_metrics = np.mean(ranking_object, axis=(0, 3))\n        ranking_over_datasets = np.mean(ranking_object, axis=(2, 3))\n\n    algorithms = np.array(algorithms)\n    ranking_order = np.argsort(overrall_ranking)\n    algo_labels = algorithms[ranking_order]\n    algo_labels = [albel.split(\"_\")[0] for albel in algo_labels]\n\n    # plot the by metric ranking\n    for rank_on_metric_ave_over_dataset, metric in zip(ranking_over_metrics.T, metrics):\n        ax.plot(algo_labels, rank_on_metric_ave_over_dataset[ranking_order],\n                marker=\"o\", label=metric, alpha=0.5, zorder=20)\n\n    # plot the overrall ranking\n    ax.scatter(algo_labels, overrall_ranking[ranking_order],\n               marker=\"x\", c='black', s=20, label='average \\noverall rank', zorder=100)\n\n    # set legend\n    if set_legend:\n        ax.legend(loc='upper center', fontsize=14, ncol=2, bbox_to_anchor=(0.5, -0.35))\n    # configure y axis\n    ax.set_ylim(0, 10.5)\n    ax.set_ylabel('Algorithm ranking\\naveraged over dataset', fontsize=14)\n    ax.tick_params(axis='y', labelsize=18)\n    # configure x axis\n    plt.setp(ax.xaxis.get_majorticklabels(), ha='left', rotation=-40, fontsize=14)\n    # create offset transform by 5 points in x direction\n    dx = 5 / 72.\n    dy = 0 / 72.\n    offset = matplotlib.transforms.ScaledTranslation(dx, dy, fig.dpi_scale_trans)\n    # apply offset transform to all x ticklabels.\n    for label in ax.xaxis.get_majorticklabels():\n        label.set_transform(label.get_transform() - offset)\n    # set title\n    ax.set_title(f'{title_name}', fontsize=20)\n    plt.tight_layout()\n\n    # plot the by dataset ranking\n    colours = plt.get_cmap('tab20').colors\n    for rank_on_dataset_ave_over_metric, dataset, c in zip(ranking_over_datasets, datasets, colours):\n        ax1.plot(algo_labels, rank_on_dataset_ave_over_metric[ranking_order],\n                 marker=\"o\", label=dataset, alpha=0.5, zorder=20, c=c)\n    # plot the overrall ranking\n    ax1.scatter(algo_labels, overrall_ranking[ranking_order],\n                marker=\"x\", c='black', s=20, label='average \\noverall rank', zorder=100)\n    # set legend\n    if set_legend:\n        ax1.legend(loc='upper center', fontsize=14, ncol=4, bbox_to_anchor=(0.5, -0.35))\n    # set y axis\n    ax1.set_ylim(0, 10.5)\n    ax1.set_ylabel('Algorithm ranking\\naveraged over metric', fontsize=14)\n    ax1.tick_params(axis='y', labelsize=20)\n    # set x axis\n    plt.setp(ax1.xaxis.get_majorticklabels(), ha='left', rotation=-40, fontsize=14)\n    # create offset transform by 5 points in x direction\n    dx = 5 / 72.\n    dy = 0 / 72.\n    offset = matplotlib.transforms.ScaledTranslation(dx, dy, fig1.dpi_scale_trans)\n    # apply offset transform to all x ticklabels.\n    for label in ax1.xaxis.get_majorticklabels():\n        label.set_transform(label.get_transform() - offset)\n    # set title\n    ax1.set_title(f'{title_name}', fontsize=20)\n    plt.tight_layout()\n\n    return ax, ax1", "\n\ndef create_rand_dist_fig(ax, title, datasets, algorithms, metrics, seeds, folder, scale_metrics=False, calc_ave_first=False, set_legend=False):\n    if not ax:\n        fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n\n    x_axis = np.arange(0, len(algorithms), 0.001)\n\n    result_object = make_test_performance_object(datasets, algorithms, metrics, seeds, folder)\n    ranking_object = calculate_ranking_performance(result_object, datasets, metrics, seeds, calc_ave_first=calc_ave_first, scale_metrics=scale_metrics)\n\n    if calc_ave_first:\n        ave_axis = [0, 2]\n    else:\n        ave_axis = [0, 2, 3]\n\n    n_ranks = 1\n    for axis in ave_axis:\n        n_ranks *= ranking_object.shape[axis]\n\n    # average rank over dataset and metric + sensitivity over seed\n    all_ranks_per_algo = np.zeros(shape=(len(algorithms), n_ranks))\n    for i, algo in enumerate(algorithms):\n        all_ranks_per_algo[i] = ranking_object[:, i, :].flatten()\n\n    # calculate amount of overlap\n    out = 0.\n    for i, _ in enumerate(algorithms):\n        for j, _ in enumerate(algorithms):\n            if i != j:\n                out += wasserstein_distance(all_ranks_per_algo[i], all_ranks_per_algo[j])\n                \n    # calculate averages\n    div_out = (len(algorithms) * (len(algorithms) -1))/2\n    ave_overlap = str(round(out/div_out, 3))\n\n    with plt.xkcd():\n        # plot the lines\n        for j, algo_ranks in enumerate(all_ranks_per_algo):\n            kde = gaussian_kde(algo_ranks)\n            y_axis = kde.evaluate(x_axis)\n            ax.plot(x_axis, y_axis, label=algorithms[j], zorder=10)\n\n        if set_legend:\n            ax.legend(loc='best', fontsize=16, ncol=3)\n            #ax.legend(loc='upper center', fontsize=14, ncol=3, bbox_to_anchor=(0.5, -0.35))\n            #ax.set_xlabel('algorithm rank distribution over all tests', fontsize=18)\n\n        #ax.set_ybound(0, 3)\n        ax.set_xbound(1, 10)\n        ax.set_ylabel('probability density', fontsize=18)\n        ax.set_xlabel(r'kde estimatation of rank distribution $f_{j}(r)$', fontsize=18)\n\n        #ax.text(0.4, 0.85, ave_overlap_text, fontsize=20, transform=ax.transAxes, zorder=1000)\n        ax.tick_params(axis='x', labelsize=18)\n        ax.tick_params(axis='y', labelsize=18)\n\n        ax.set_title(title + ave_overlap, fontsize=20)\n    return ax", "\n\ndef create_big_figure(datasets, algorithms, folder, default_algos, default_folder):\n    \"\"\"\n    creates figure for all datasets tested comparing default and hpo results\n    \"\"\"\n    # create holder figure\n    nrows, ncols = 4, 3\n    col_n, row_n = 0, 0\n    fig, axs = plt.subplots(nrows, ncols, figsize=(54, 78))\n\n    for dataset_name in datasets:\n        # create a figure on the axis\n        if row_n == nrows:\n            col_n += 1\n            row_n = 0\n\n        axs[row_n, col_n] = create_result_bar_chart(dataset_name, algorithms, folder, default_algos, default_folder, axs[row_n, col_n])\n        row_n += 1\n\n    axs[row_n, col_n].spines['top'].set_visible(False)\n    axs[row_n, col_n].spines['bottom'].set_visible(False)\n    axs[row_n, col_n].spines['left'].set_visible(False)\n    axs[row_n, col_n].spines['right'].set_visible(False)\n    axs[row_n, col_n].set_xticks([])\n    axs[row_n, col_n].set_yticks([])\n\n    axs[0, 0].legend()\n    for item in axs[0, 0].get_legend().get_texts():\n        item.set_fontsize(48)\n\n    fig.tight_layout()\n    fig.savefig(f\"{ugle_path}/figures/new_hpo_investigation.png\", bbox_inches='tight')\n    return", "\n\ndef create_algo_selection_on_dataset_landscape(datasets, algorithms, default_algos, metrics, seeds, folder, default_folder, titles):\n\n    fig, ax = plt.subplots(2, 2, figsize=(15, 7.5))\n\n    titles[0] += r' $(\\mathcal{\\hat{T}}_{(dhp)})$'\n    titles[1] += r' $(\\mathcal{\\hat{T}}_{(hpo)})$'\n    titles[2] += r' $(\\mathcal{T}_{(dhp)})$'\n    titles[3] += r' $(\\mathcal{T}_{(hpo)})$'\n\n    ax[0, 0] = create_dataset_landscape_fig(ax[0, 0], titles[0], datasets, default_algos, metrics, seeds, default_folder, calc_ave_first=True)\n    ax[0, 1] = create_dataset_landscape_fig(ax[0, 1], titles[1], datasets, algorithms, metrics, seeds, folder, calc_ave_first=True)\n    ax[1, 0] = create_dataset_landscape_fig(ax[1, 0], titles[2], datasets, default_algos, metrics, seeds, default_folder)\n    ax[1, 1] = create_dataset_landscape_fig(ax[1, 1], titles[3], datasets, algorithms, metrics, seeds, folder)\n    \n    fig.tight_layout()\n    fig.savefig(f\"{ugle_path}/figures/new_dataset_landscape_comparison.png\")\n    return", "\n\ndef create_comparison_figures(datasets: list, algorithms: list, metrics: list, seeds: list, folder: str,\n                              default_algos: list, default_folder: str, titles: list):\n    # create holder figure\n    nrows, ncols = 2, 2\n    fig0, axs0 = plt.subplots(nrows, ncols, figsize=(15, 7.5))\n\n    # create holder figure\n    nrows, ncols = 2, 2\n    fig1, axs1 = plt.subplots(nrows, ncols, figsize=(15, 7.5))\n\n    titles[0] += r' $(\\mathcal{\\hat{T}}_{(dhp)})$'\n    titles[1] += r' $(\\mathcal{\\hat{T}}_{(hpo)})$'\n    titles[2] += r' $(\\mathcal{T}_{(dhp)})$'\n    titles[3] += r' $(\\mathcal{T}_{(hpo)})$'\n\n    axs0[0, 0], axs1[0, 0] = create_ranking_charts(datasets, default_algos, metrics, seeds, default_folder,\n                                           title_name=titles[0],\n                                           scale_metrics=False, calc_ave_first=True, set_legend=False, ax=axs0[0, 0], ax1=axs1[0, 0], fig=fig0,\n                                           fig1=fig1)\n    axs0[0, 1], axs1[0, 1] = create_ranking_charts(datasets, algorithms, metrics, seeds, folder, title_name=titles[1],\n                                           scale_metrics=False, calc_ave_first=True, set_legend=False, ax=axs0[0, 1], ax1=axs1[0, 1], fig=fig0,\n                                           fig1=fig1)\n    axs0[1, 0], axs1[1, 0] = create_ranking_charts(datasets, default_algos, metrics, seeds, default_folder,\n                                           title_name=titles[2],\n                                           scale_metrics=False, set_legend=False, ax=axs0[1, 0], ax1=axs1[1, 0], fig=fig0,\n                                           fig1=fig1)\n    axs0[1, 1], axs1[1, 1] = create_ranking_charts(datasets, algorithms, metrics, seeds, folder, title_name=titles[3],\n                                           scale_metrics=False, set_legend=True, ax=axs0[1, 1], ax1=axs1[1, 1], fig=fig0,\n                                           fig1=fig1)\n\n    fig0.tight_layout()\n    fig1.tight_layout()\n\n    fig0.savefig(f\"{ugle_path}/figures/new_ranking_comparison_metrics.png\", bbox_inches='tight')\n    fig1.savefig(f\"{ugle_path}/figures/new_ranking_comparison_datasets.png\", bbox_inches='tight')\n    return", "\n\ndef create_rand_dist_comparison(datasets: list, algorithms: list, metrics: list, seeds: list, folder: str,\n                                default_algos: list, default_folder: str, titles: list):\n\n    # create holder figure\n    nrows, ncols = 2, 2\n    fig, ax = plt.subplots(nrows, ncols, figsize=(15, 7.5))\n\n    titles[0] += r' $\\Omega(\\mathcal{\\hat{T}}_{(dhp)})$ : '\n    titles[1] += r' $\\Omega(\\mathcal{\\hat{T}}_{(hpo)})$ : '\n    titles[2] += r' $\\Omega(\\mathcal{T}_{(dhp)})$ : '\n    titles[3] += r' $\\Omega(\\mathcal{T}_{(hpo)})$ : '\n\n    ax[0, 0] = create_rand_dist_fig(ax[0, 0], titles[0], datasets, default_algos, metrics, seeds, default_folder, calc_ave_first=True)\n    ax[0, 1] = create_rand_dist_fig(ax[0, 1], titles[1], datasets, algorithms, metrics, seeds, folder, calc_ave_first=True)\n    ax[1, 0] = create_rand_dist_fig(ax[1, 0], titles[2], datasets, default_algos, metrics, seeds, default_folder)\n    ax[1, 1] = create_rand_dist_fig(ax[1, 1], titles[3], datasets, algorithms, metrics, seeds, folder, set_legend=True)\n\n    fig.tight_layout()\n    fig.savefig(f'{ugle_path}/figures/new_rand_dist_comparison.png', bbox_inches='tight')\n    return", "\n\ndef create_all_paper_figures(datasets, algorithms, metrics, seeds, folder, default_folder, default_algos):\n    titles = ['a) Default HPs w/ AveSeed', 'b) HPO w/ AveSeed', 'c) Default HPs w/ SeedRanking', 'd) HPO w/ SeedRanking']\n    create_rand_dist_comparison(datasets, algorithms, metrics, seeds, folder, default_algos, default_folder, deepcopy(titles))\n    create_algo_selection_on_dataset_landscape(datasets, algorithms, default_algos, metrics, seeds, folder, default_folder, deepcopy(titles))\n    create_comparison_figures(datasets, algorithms, metrics, seeds, folder, default_algos, default_folder, deepcopy(titles))\n    create_big_figure(datasets, algorithms, folder, default_algos, default_folder)\n\n    return", "\n\n#algorithms = ['daegc', 'dgi', 'dmon', 'grace', 'mvgrl', 'selfgnn', 'sublime', 'bgrl', 'vgaer', 'cagc']\n#datasets = ['cora', 'citeseer', 'dblp', 'bat', 'eat', 'texas', 'wisc', 'cornell', 'uat', 'amac', 'amap']\n#metrics = ['nmi', 'modularity', 'f1', 'conductance']\n#folder = './progress_results/'\n#seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n#default_algos = ['daegc_default', 'dgi_default', 'dmon_default', 'grace_default', 'mvgrl_default', 'selfgnn_default',\n#                 'sublime_default', 'bgrl_default', 'vgaer_default', 'cagc_default']\n#default_folder = './new_default/'", "#                 'sublime_default', 'bgrl_default', 'vgaer_default', 'cagc_default']\n#default_folder = './new_default/'\n\n#create_all_paper_figures(datasets, algorithms, metrics, seeds, folder, default_folder, default_algos)\n\n# creates a tikz figure to show someone\n#fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n#ax = create_rand_dist_fig(ax, r'Framework Rank Distinction Coefficient $\\Omega(\\mathcal{\\hat{T}}_{(hpo)}) : $', datasets, algorithms, metrics, seeds, folder, calc_ave_first=True, set_legend=True)\n#fig.tight_layout()\n#fig.savefig(f'{ugle_path}/figures/tkiz_fig.png', bbox_inches='tight')", "#fig.tight_layout()\n#fig.savefig(f'{ugle_path}/figures/tkiz_fig.png', bbox_inches='tight')\n\ndef rank_values(scores):\n    ranks = []\n    for score in scores:\n       ranks.append(scores.index(score) + 1)\n    return ranks\n\n\ndef calculate_ranking_coefficient_over_dataset(result_object, algorithms, dataset_idx, metrics):\n    ranking_object = np.mean(result_object, axis=-1)[dataset_idx, :, :]\n    n_ranks = len(metrics)\n    all_ranks_per_algo = np.zeros(shape=(len(algorithms), n_ranks))\n\n    for m, metric_name in enumerate(metrics):\n        if metric_name != 'conductance':\n            all_ranks_per_algo[:, m] = rank_values(np.flip(np.sort(ranking_object[:, m])).tolist())\n        else:\n            all_ranks_per_algo[:, m] = rank_values(np.sort(ranking_object[:, m]).tolist())\n       \n    coeff = 0.\n    for i, _ in enumerate(algorithms):\n        for j, _ in enumerate(algorithms):\n            if i != j:\n                coeff += wasserstein_distance(all_ranks_per_algo[i], all_ranks_per_algo[j])\n    \n    # calculate averages\n    div_out = (len(algorithms) * (len(algorithms) -1 ))/2\n    coeff = round(coeff/div_out, 3)  \n\n    return coeff ", "\n\ndef calculate_ranking_coefficient_over_dataset(result_object, algorithms, dataset_idx, metrics):\n    ranking_object = np.mean(result_object, axis=-1)[dataset_idx, :, :]\n    n_ranks = len(metrics)\n    all_ranks_per_algo = np.zeros(shape=(len(algorithms), n_ranks))\n\n    for m, metric_name in enumerate(metrics):\n        if metric_name != 'conductance':\n            all_ranks_per_algo[:, m] = rank_values(np.flip(np.sort(ranking_object[:, m])).tolist())\n        else:\n            all_ranks_per_algo[:, m] = rank_values(np.sort(ranking_object[:, m]).tolist())\n       \n    coeff = 0.\n    for i, _ in enumerate(algorithms):\n        for j, _ in enumerate(algorithms):\n            if i != j:\n                coeff += wasserstein_distance(all_ranks_per_algo[i], all_ranks_per_algo[j])\n    \n    # calculate averages\n    div_out = (len(algorithms) * (len(algorithms) -1 ))/2\n    coeff = round(coeff/div_out, 3)  \n\n    return coeff ", "\n\ndef create_synth_figure():\n    synth_datasets_2 = ['synth_disjoint_disjoint_2', 'synth_disjoint_random_2', 'synth_disjoint_complete_2',\n                    'synth_random_disjoint_2', 'synth_random_random_2', 'synth_random_complete_2',\n                    'synth_complete_disjoint_2', 'synth_complete_random_2', 'synth_complete_complete_2']\n    \n    synth_datasets = ['synth_disjoint_disjoint_4', 'synth_disjoint_random_4', 'synth_disjoint_complete_4',\n                    'synth_random_disjoint_4', 'synth_random_random_4', 'synth_random_complete_4',\n                    'synth_complete_disjoint_4', 'synth_complete_random_4', 'synth_complete_complete_4']\n    synth_algorithms = ['dgi_default', 'daegc_default', 'dmon_default', 'grace_default', 'sublime_default', 'bgrl_default', 'vgaer_default']\n    synth_folder = './synth_defaults/'\n    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n    # fetch results\n    result_object = make_test_performance_object(synth_datasets, synth_algorithms, metrics, seeds, synth_folder)\n\n    # create graph subfigures \n    nrows, ncols = 3, 3\n    fig, axes = plt.subplots(nrows, ncols, figsize=(9, 9))\n    for i, ax in enumerate(axes.flat):\n        coeff = calculate_ranking_coefficient_over_dataset(result_object, synth_algorithms, i, metrics)\n\n        title = (\" \").join(synth_datasets[i].split(\"_\")[1:3])\n        title = 'adj: ' + synth_datasets[i].split(\"_\")[1] + \" feat: \" + synth_datasets[i].split(\"_\")[2]\n        ax.set_title(f\"{title}: {str(coeff)}\" )\n\n        alt_colours = ['C3', 'C0', 'C2', 'C1']\n\n        bar_width = 1 / 4\n        x_axis_names = np.arange(len(synth_algorithms))\n\n        # plot results\n        for m, metric in enumerate(metrics):\n            metric_vals = []\n            metric_std = []\n            for a, algo in enumerate(synth_algorithms):\n                metric_vals.append(np.mean(result_object[i, a, m, :]))\n                metric_std.append(np.std(result_object[i, a, m, :]))\n            ax.bar(x_axis_names + (m * bar_width), metric_vals, yerr=metric_std, \n                   width=bar_width, facecolor=alt_colours[m], alpha=0.9, linewidth=0, label=metric)\n            \n        ax.set_xticks(x_axis_names - 0.5 * bar_width)\n        ax.set_xticklabels(synth_algorithms, ha='left', rotation=-45, position=(-0.5, 0.0))\n        ax.set_axisbelow(True)\n        ax.set_ylim(0.0, 1.0)\n        ax.axhline(y=0.5, color='k', linestyle='-') \n\n\n    plt.tight_layout()\n    plt.savefig('synth_4_default.png', bbox_inches='tight')\n    print('done')", "\n\n\ndef create_trainpercent_figure():\n    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n    datasets = ['citeseer', 'cora', 'dblp', 'texas', 'wisc', 'cornell', 'amac']\n    algorithms = ['dgi', 'daegc', 'dmon', 'grace', 'sublime', 'bgrl', 'vgaer']\n    percents = ['01', '03', '05', '07', '09']\n    percents_labels = ['0.1', '0.3', '0.5', '0.7', '0.9']\n    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n    folder = './revamp_train_percent/'\n\n    nrows, ncols = len(algorithms), len(datasets)\n    fig, axes = plt.subplots(nrows, ncols, figsize=(7.5, 10))\n\n    res_objs = []\n    for pct in percents:\n        res_objs.append(make_test_performance_object(datasets, algorithms, metrics, seeds, folder + pct + '/'))\n    res_objs = np.stack(res_objs, axis=0)\n\n    for d, dataset in enumerate(datasets):\n        for a, algo in enumerate(algorithms):\n            ax = axes[a, d]\n            ax.set_title(f'{algo} -- {dataset}')\n\n            alt_colours = ['C3', 'C0', 'C2', 'C1']\n            bar_width = 1 / 4\n            x_axis_names = np.arange(len(percents))\n\n            # plot results\n            for m, metric in enumerate(metrics):\n                metric_vals = []\n                metric_std = []\n                for p in range(len(percents)):\n                    metric_vals.append(np.mean(res_objs[p, d, a, m, :]))\n                    metric_std.append(np.std(res_objs[p, d, a, m, :]))\n                ax.bar(x_axis_names + (m * bar_width), metric_vals, yerr=metric_std, \n                    width=bar_width, facecolor=alt_colours[m], alpha=0.9, linewidth=0, label=metric)\n                \n            ax.set_xticks(x_axis_names - 0.5 * bar_width)\n            ax.set_xticklabels(percents_labels, ha='left', rotation=-45, position=(-0.5, 0.0))\n            ax.set_axisbelow(True)\n            ax.set_ylim(0.0, 1.0)\n                \n\n    plt.tight_layout()\n    plt.show()\n    print('done')", "\n\n#create_trainpercent_figure()\n#create_synth_figure()\n\ndef calc_correlation():\n    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n    datasets = ['citeseer', 'cora', 'dblp', 'texas', 'wisc', 'cornell']\n    algorithms = ['dgi_default', 'daegc_default', 'dmon_default', 'grace_default', 'sublime_default', 'bgrl_default', 'vgaer_default']\n    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n    folder = './results/q1_default_predict_super/'\n\n    mod_results = []\n    con_results = []\n\n    for dataset in datasets:\n        for algo in algorithms:\n            filename = f\"{dataset}_{algo}.pkl\"\n            file_found = search_results(folder, filename)\n            if file_found:\n                result = pickle.load(open(file_found, \"rb\"))\n            \n            for seed_result in result.results:\n                for metric_result in seed_result.study_output:\n                    if 'modularity' in metric_result.metrics:\n                       mod_results.append([metric_result.results['modularity'], metric_result.results['f1'], metric_result.results['nmi']])\n\n                    if 'conductance' in metric_result.metrics:\n                       con_results.append([metric_result.results['conductance'], metric_result.results['f1'], metric_result.results['nmi']])\n\n    mod_results = np.asarray(mod_results)\n    con_results = np.asarray(con_results)\n    mod_f1 = np.corrcoef(mod_results[:, 0], mod_results[:, 1])[0,1]\n    mod_nmi = np.corrcoef(mod_results[:, 0], mod_results[:, 2])[0,1]\n    con_f1 = np.corrcoef(con_results[:, 0], con_results[:, 1])[0,1]\n    con_nmi = np.corrcoef(con_results[:, 0], con_results[:, 2])[0,1]\n    print('Correlation Coefficients: ')\n    print(f'Modularity --> F1: {mod_f1:.3f}')\n    print(f'Modularity --> NMI: {mod_nmi:.3f}')\n    print(f'Conductance --> F1: {con_f1:.3f}')\n    print(f'Conductance --> NMI: {con_nmi:.3f}')", "\n\ndef calc_synth_results():\n    seeds = [42, 24, 976, 12345, 98765, 7, 856, 90, 672, 785]\n    datasets = ['synth_disjoint_disjoint_2', 'synth_disjoint_random_2', 'synth_disjoint_complete_2',\n                    'synth_random_disjoint_2', 'synth_random_random_2', 'synth_random_complete_2',\n                    'synth_complete_disjoint_2', 'synth_complete_random_2', 'synth_complete_complete_2']\n    algorithms = ['dgi_default', 'daegc_default', 'dmon_default', 'grace_default', 'sublime_default', 'bgrl_default', 'vgaer_default']\n    metrics = ['nmi', 'modularity', 'f1', 'conductance']\n    folder = './results/synth_defaults/'\n    \n    mod_results = []\n    con_results = []\n\n    for dataset in datasets:\n        for algo in algorithms:\n            filename = f\"{dataset}_{algo}.pkl\"\n            file_found = search_results(folder, filename)\n            if file_found:\n                result = pickle.load(open(file_found, \"rb\"))\n\n            for seed_result in result.results:\n                for metric_result in seed_result.study_output:\n                    if 'modularity' in metric_result.metrics:\n                       mod_results.append([metric_result.results['modularity'], metric_result.results['f1'], metric_result.results['nmi']])\n\n                    if 'conductance' in metric_result.metrics:\n                       con_results.append([metric_result.results['conductance'], metric_result.results['f1'], metric_result.results['nmi']])\n\n    mod_results = np.asarray(mod_results)\n    con_results = np.asarray(con_results)", "\n\n#calc_synth_results()"]}
{"filename": "ugle/trainer.py", "chunked_list": ["import ugle\nimport ugle.utils as utils\nimport ugle.datasets as datasets\nfrom ugle.logger import log\nimport time\nfrom omegaconf import OmegaConf, DictConfig, ListConfig\nimport numpy as np\nimport optuna\nfrom optuna import Trial, Study\nfrom optuna.samplers import TPESampler", "from optuna import Trial, Study\nfrom optuna.samplers import TPESampler\nfrom optuna.pruners import HyperbandPruner\nfrom optuna.trial import TrialState\nimport threading\nimport time\nfrom alive_progress import alive_it\nimport copy\nimport torch\nfrom typing import Dict, List, Optional, Tuple", "import torch\nfrom typing import Dict, List, Optional, Tuple\nfrom collections import defaultdict\nimport optuna\nimport warnings\nfrom os.path import exists\nfrom os import makedirs\n\nfrom optuna.exceptions import ExperimentalWarning\nwarnings.filterwarnings(\"ignore\", category=ExperimentalWarning)", "from optuna.exceptions import ExperimentalWarning\nwarnings.filterwarnings(\"ignore\", category=ExperimentalWarning)\noptuna.logging.set_verbosity(optuna.logging.CRITICAL)\n\n# https://stackoverflow.com/questions/9850995/tracking-maximum-memory-usage-by-a-python-function\n\nclass StoppableThread(threading.Thread):\n    def __init__(self):\n        super(StoppableThread, self).__init__()\n        self.daemon = True\n        self.__monitor = threading.Event()\n        self.__monitor.set()\n        self.__has_shutdown = False\n\n    def run(self):\n        '''Overloads the threading.Thread.run'''\n        # Call the User's Startup functions\n        self.startup()\n\n        # Loop until the thread is stopped\n        while self.isRunning():\n            self.mainloop()\n\n        # Clean up\n        self.cleanup()\n\n        # Flag to the outside world that the thread has exited\n        # AND that the cleanup is complete\n        self.__has_shutdown = True\n\n    def stop(self):\n        self.__monitor.clear()\n\n    def isRunning(self):\n        return self.__monitor.isSet()\n\n    def isShutdown(self):\n        return self.__has_shutdown\n\n    ###############################\n    ### User Defined Functions ####\n    ###############################\n\n    def mainloop(self):\n        '''\n        Expected to be overwritten in a subclass!!\n        Note that Stoppable while(1) is handled in the built in \"run\".\n        '''\n        pass\n\n    def startup(self):\n        '''Expected to be overwritten in a subclass!!'''\n        pass\n\n    def cleanup(self):\n        '''Expected to be overwritten in a subclass!!'''\n        pass", "\n\nclass MyLibrarySniffingClass(StoppableThread):\n    def __init__(self, target_lib_call):\n        super(MyLibrarySniffingClass, self).__init__()\n        self.target_function = target_lib_call\n        self.results = None\n\n    def startup(self):\n        # Overload the startup function\n        log.info(\"Starting Memory Calculation\")\n\n    def cleanup(self):\n        # Overload the cleanup function\n        log.info(\"Ending Memory Tracking\")\n\n    def mainloop(self):\n        # Start the library Call\n        self.results = self.target_function()\n\n        # Kill the thread when complete\n        self.stop()", "\n\nclass StopWhenMaxTrialsHit:\n    def __init__(self, max_n_trials: int, max_n_pruned: int):\n        self.max_n_trials = max_n_trials\n        self.max_pruned = max_n_pruned\n        self.completed_trials = 0\n        self.pruned_in_a_row = 0\n\n    def __call__(self, study: optuna.study.Study, trial: optuna.trial.FrozenTrial) -> None:\n        if trial.state == optuna.trial.TrialState.COMPLETE:\n            self.completed_trials += 1\n            self.pruned_in_a_row = 0\n        elif trial.state == optuna.trial.TrialState.PRUNED:\n            self.pruned_in_a_row += 1\n\n        if self.completed_trials >= self.max_n_trials:\n            log.info('Stopping Study for Reaching Max Number of Trials')\n            study.stop()\n\n        if self.pruned_in_a_row >= self.max_pruned:\n            log.info('Stopping Study for Reaching Max Number of Pruned Trials in a Row')\n            study.stop()", "\n\nclass ParamRepeatPruner:\n    \"\"\"Prunes repeated trials, which means trials with the same parameters won't waste time/resources.\"\"\"\n\n    def __init__(\n        self,\n        study: optuna.study.Study,\n        repeats_max: int = 0,\n        should_compare_states: List[TrialState] = [TrialState.COMPLETE],\n        compare_unfinished: bool = True,\n    ):\n        \"\"\"\n        Args:\n            study (optuna.study.Study): Study of the trials.\n            repeats_max (int, optional): Instead of prunning all of them (not repeating trials at all, repeats_max=0) you can choose to repeat them up to a certain number of times, useful if your optimization function is not deterministic and gives slightly different results for the same params. Defaults to 0.\n            should_compare_states (List[TrialState], optional): By default it only skips the trial if the paremeters are equal to existing COMPLETE trials, so it repeats possible existing FAILed and PRUNED trials. If you also want to skip these trials then use [TrialState.COMPLETE,TrialState.FAIL,TrialState.PRUNED] for example. Defaults to [TrialState.COMPLETE].\n            compare_unfinished (bool, optional): Unfinished trials (e.g. `RUNNING`) are treated like COMPLETE ones, if you don't want this behavior change this to False. Defaults to True.\n        \"\"\"\n        self.should_compare_states = should_compare_states\n        self.repeats_max = repeats_max\n        self.repeats: Dict[int, List[int]] = defaultdict(lambda: [], {})\n        self.unfinished_repeats: Dict[int, List[int]] = defaultdict(lambda: [], {})\n        self.compare_unfinished = compare_unfinished\n        self.study = study\n\n    @property\n    def study(self) -> Optional[optuna.study.Study]:\n        return self._study\n\n    @study.setter\n    def study(self, study):\n        self._study = study\n        if self.study is not None:\n            self.register_existing_trials()\n\n    def register_existing_trials(self):\n        \"\"\"In case of studies with existing trials, it counts existing repeats\"\"\"\n        trials = self.study.trials\n        trial_n = len(trials)\n        for trial_idx, trial_past in enumerate(self.study.trials[1:]):\n            self.check_params(trial_past, False, -trial_n + trial_idx)\n\n    def prune(self):\n        self.check_params()\n\n    def should_compare(self, state):\n        return any(state == state_comp for state_comp in self.should_compare_states)\n\n    def clean_unfinised_trials(self):\n        trials = self.study.trials\n        finished = []\n        for key, value in self.unfinished_repeats.items():\n            if self.should_compare(trials[key].state):\n                for t in value:\n                    self.repeats[key].append(t)\n                finished.append(key)\n\n        for f in finished:\n            del self.unfinished_repeats[f]\n\n    def check_params(\n        self,\n        trial: Optional[optuna.trial.BaseTrial] = None,\n        prune_existing=True,\n        ignore_last_trial: Optional[int] = None,\n    ):\n        if self.study is None:\n            return\n        trials = self.study.trials\n        if trial is None:\n            trial = trials[-1]\n            ignore_last_trial = -1\n\n        self.clean_unfinised_trials()\n\n        self.repeated_idx = -1\n        self.repeated_number = -1\n        for idx_p, trial_past in enumerate(trials[:ignore_last_trial]):\n            should_compare = self.should_compare(trial_past.state)\n            should_compare |= (\n                self.compare_unfinished and not trial_past.state.is_finished()\n            )\n            if should_compare and trial.params == trial_past.params:\n                if not trial_past.state.is_finished():\n                    self.unfinished_repeats[trial_past.number].append(trial.number)\n                    continue\n                self.repeated_idx = idx_p\n                self.repeated_number = trial_past.number\n                break\n\n        if self.repeated_number > -1:\n            self.repeats[self.repeated_number].append(trial.number)\n        if len(self.repeats[self.repeated_number]) > self.repeats_max:\n            if prune_existing:\n                log.info('Pruning Trial for Suggesting Duplicate Parameters')\n                raise optuna.exceptions.TrialPruned()\n\n        return self.repeated_number\n\n    def get_value_of_repeats(\n        self, repeated_number: int, func=lambda value_list: np.mean(value_list)\n    ):\n        if self.study is None:\n            raise ValueError(\"No study registered.\")\n        trials = self.study.trials\n        values = (\n            trials[repeated_number].value,\n            *(\n                trials[tn].value\n                for tn in self.repeats[repeated_number]\n                if trials[tn].value is not None\n            ),\n        )\n        return func(values)", "    \n\ndef log_trial_result(trial: Trial, results: dict, valid_metrics: list, multi_objective_study: bool):\n    \"\"\"\n    logs the results for a trial\n    :param trial: trial object\n    :param results: result dictionary for trial\n    :param valid_metrics: validation metrics used\n    :param multi_objective_study: boolean whether under multi-objective study or not\n    \"\"\"\n    if not multi_objective_study:\n        # log validation results\n        trial_value = results[valid_metrics[0]]\n        log.info(f'Trial {trial.number} finished. Validation result || {valid_metrics[0]}: {trial_value} ||')\n        # log best trial comparison\n        new_best_message = f'New best trial {trial.number}'\n        if trial.number > 0:\n            if trial_value > trial.study.best_value and trial.study.direction.name == 'MAXIMIZE':\n                log.info(new_best_message)\n            elif trial_value < trial.study.best_value and trial.study.direction.name == 'MINIMIZE':\n                log.info(new_best_message)\n            else:\n                log.info(\n                    f'Trial {trial.number} finished. Best trial is {trial.study.best_trial.number} with {valid_metrics[0]}: {trial.study.best_value}')\n        else:\n            log.info(new_best_message)\n\n    else:\n        # log trial results\n        right_order_results = [results[k] for k in valid_metrics]\n        to_log_trial_values = ''.join(\n            f'| {metric}: {right_order_results[i]} |' for i, metric in enumerate(valid_metrics))\n        log.info(f'Trial {trial.number} finished. Validation result |{to_log_trial_values}|')\n        # log best trial comparison\n        if trial.number > 0:\n            # get best values for each metric across the best trials\n            best_values, associated_trial = ugle.utils.extract_best_trials_info(trial.study, valid_metrics)\n            # compare best values for each trial with new values found\n            improved_metrics = []\n            for i, metric_result in enumerate(right_order_results):\n                if (metric_result > best_values[i] and trial.study.directions[i].name == 'MAXIMIZE') or \\\n                        (metric_result < best_values[i] and trial.study.directions[i].name == 'MINIMIZE'):\n                    best_values[i] = metric_result\n                    improved_metrics.append(valid_metrics[i])\n                    associated_trial[i] = trial.number\n\n            # log best trial and value for each metric\n            if improved_metrics:\n                improved_metrics_str = ''.join(f'{metric}, ' for metric in improved_metrics)\n                improved_metrics_str = improved_metrics_str[:improved_metrics_str.rfind(',')]\n                log.info(f'New Best trial for metrics: {improved_metrics_str}')\n            else:\n                log.info('Trial worse than existing across all metrics')\n\n            best_so_far = ''.join(\n                f'trial {associated_trial[i]} ({metric}: {best_values[i]}), ' for i, metric in enumerate(valid_metrics))\n            best_so_far = best_so_far[:best_so_far.rfind(',')]\n            log.info(f'Best results so far: {best_so_far}')\n\n    return", "\n\nclass ugleTrainer:\n    def __init__(self, cfg: DictConfig):\n        super(ugleTrainer, self).__init__()\n\n        _device = ugle.utils.set_device(cfg.trainer.gpu)\n        if _device == 'cpu':\n            self.device_name = _device\n        else:\n            self.device_name = cfg.trainer.gpu\n        self.device = torch.device(_device)\n\n        utils.set_random(cfg.args.random_seed)\n        if cfg.trainer.show_config:\n            log.info(OmegaConf.to_yaml(cfg.args))\n\n        cfg = ugle.utils.process_study_cfg_parameters(cfg)\n\n        self.cfg = cfg\n        if not exists(cfg.trainer.models_path):\n            makedirs(cfg.trainer.models_path)\n\n        self.progress_bar = None\n        self.model = None\n        self.loss_function = None\n        self.scheduler = None\n        self.optimizers = None\n        self.patience_wait = 0\n        self.best_loss = 1e9\n        self.current_epoch = 0\n\n\n    def load_database(self):\n        log.info('loading dataset')\n        if 'synth' not in self.cfg.dataset:\n            # loads and splits the dataset\n            features, label, train_adjacency, test_adjacency = datasets.load_real_graph_data(\n                self.cfg.dataset,\n                self.cfg.trainer.training_to_testing_split,\n                self.cfg.trainer.split_scheme,\n                self.cfg.trainer.split_addition_percentage)\n        else:\n            _, adj_type, feature_type, n_clusters = self.cfg.dataset.split(\"_\")\n            n_clusters = int(n_clusters)\n            adjacency, features, label = datasets.create_synth_graph(n_nodes=1000, n_features=500, n_clusters=n_clusters, \n                                                                     adj_type=adj_type, feature_type=feature_type)\n            train_adjacency, test_adjacency = datasets.split_adj(adjacency,\n                                                                 self.cfg.trainer.training_to_testing_split,\n                                                                 self.cfg.trainer.split_scheme)\n\n        # extract database relevant info\n        if not self.cfg.args.n_clusters:\n            self.cfg.args.n_clusters = np.unique(label).shape[0]\n        self.cfg.args.n_nodes = features.shape[0]\n        self.cfg.args.n_features = features.shape[1]\n\n        return features, label, train_adjacency, test_adjacency\n\n    def eval(self):\n        # loads the database to train on\n        features, label, validation_adjacency, test_adjacency = self.load_database()\n\n        # creates store for range of hyperparameters optimised over\n        self.cfg.hypersaved_args = copy.deepcopy(self.cfg.args)\n\n        log.debug('splitting dataset into train/validation')\n        train_adjacency, validation_adjacency = datasets.split_adj(validation_adjacency, \n                                                          self.cfg.trainer.train_to_valid_split, \n                                                          self.cfg.trainer.split_scheme)\n\n        # process data for training\n        processed_data = self.preprocess_data(features, train_adjacency)\n        processed_valid_data = self.preprocess_data(features, validation_adjacency)\n\n        if not self.cfg.trainer.only_testing:\n            optuna.logging.disable_default_handler()\n            # creates the hpo study\n            if self.cfg.trainer.multi_objective_study:\n                study = optuna.create_study(study_name=f'{self.cfg.model}_{self.cfg.dataset}',\n                                            directions=self.cfg.trainer.optimisation_directions,\n                                            sampler=TPESampler(seed=self.cfg.args.random_seed, multivariate=True, group=True))\n            else:\n                study = optuna.create_study(study_name=f'{self.cfg.model}_{self.cfg.dataset}',\n                                            direction=self.cfg.trainer.optimisation_directions[0],\n                                            sampler=TPESampler(seed=self.cfg.args.random_seed))\n            log.info(f\"A new hyperparameter study created: {study.study_name}\")\n\n            study_stop_cb = StopWhenMaxTrialsHit(self.cfg.trainer.n_trials_hyperopt, self.cfg.trainer.max_n_pruned)\n            prune_params = ParamRepeatPruner(study)\n            study.optimize(lambda trial: self.train(trial, self.cfg.args,\n                                                    label,\n                                                    features,\n                                                    processed_data,\n                                                    validation_adjacency,\n                                                    processed_valid_data,\n                                                    prune_params=prune_params),\n                                n_trials=2*self.cfg.trainer.n_trials_hyperopt,\n                                callbacks=[study_stop_cb])\n\n            # assigns test parameters found in the study\n            if not self.cfg.trainer.multi_objective_study:\n                params_to_assign = study.best_trial.params\n                if params_to_assign != {}:\n                    log.info('Best Hyperparameters found: ')\n                    for hp_key, hp_val in params_to_assign.items():\n                        log.info(f'{hp_key} : {hp_val}')\n                self.cfg = utils.assign_test_params(self.cfg, params_to_assign)\n\n        processed_test_data = self.preprocess_data(features, test_adjacency)\n\n        # retrains the model on the validation adj and evaluates test performance\n        if not self.cfg.trainer.multi_objective_study:\n            self.cfg.trainer.calc_time = False\n            log.debug('Retraining model')\n            self.train(None, self.cfg.args, label, features, processed_data, validation_adjacency, processed_valid_data)\n            results = self.testing_loop(label, features, test_adjacency, processed_test_data,\n                                        self.cfg.trainer.test_metrics)\n\n            # log test results\n            right_order_results = [results[k] for k in self.cfg.trainer.valid_metrics]\n            to_log_trial_values = ''.join(f'| {metric}: {right_order_results[i]} |' for i, metric in\n                                          enumerate(self.cfg.trainer.valid_metrics))\n            log.info(f'Test results |{to_log_trial_values}|')\n\n            objective_results = {'metrics': self.cfg.trainer.valid_metrics[0],\n                                 'results': results,\n                                 'args': params_to_assign}\n\n        else:\n            if not self.cfg.trainer.only_testing:\n                # best hp found for metrics\n                best_values, associated_trial = ugle.utils.extract_best_trials_info(study, self.cfg.trainer.valid_metrics)\n                unique_trials = list(np.unique(np.array(associated_trial)))\n            elif not self.cfg.get(\"previous_results\", False):\n                unique_trials = [-1]\n            else:\n                unique_trials = list(range(len(self.cfg.previous_results)))\n\n            objective_results = []\n            for idx, best_trial_id in enumerate(unique_trials):\n                if not self.cfg.trainer.only_testing:\n                    # find the metrics for which trial is best at\n                    best_at_metrics = [metric for i, metric in enumerate(self.cfg.trainer.valid_metrics) if\n                                       associated_trial[i] == best_trial_id]\n                    best_hp_params = [trial for trial in study.best_trials if trial.number == best_trial_id][0].params\n                elif best_trial_id != -1:\n                    best_hp_params = self.cfg.previous_results[idx].args\n                    best_at_metrics = self.cfg.previous_results[idx].metrics\n                else:\n                    best_at_metrics = self.cfg.trainer.test_metrics\n                    best_hp_params = self.cfg.args\n\n                if best_trial_id != -1:\n                    # log the best hyperparameters and metrics at which they are best\n                    best_at_metrics_str = ''.join(f'{metric}, ' for metric in best_at_metrics)\n                    best_at_metrics_str = best_at_metrics_str[:best_at_metrics_str.rfind(',')]\n                    test_metrics = ''.join(f'{metric}_' for metric in best_at_metrics)\n                    test_metrics = test_metrics[:test_metrics.rfind(',')]\n                    log.info(f'Using best hyperparameters for metrics {best_at_metrics_str}: ')\n                    if not self.cfg.trainer.only_testing:\n                        for hp_key, hp_val in best_hp_params.items():\n                            log.info(f'{hp_key} : {hp_val}')\n\n                    # assign hyperparameters for the metric optimised over\n                    if self.cfg.trainer.finetuning_new_dataset or self.cfg.trainer.same_init_hpo:\n                        args_to_overwrite = list(set(self.cfg.args.keys()).intersection(self.cfg.trainer.args_cant_finetune))\n                        saved_args = OmegaConf.load(f'ugle/configs/models/{self.cfg.model}/{self.cfg.model}_default.yaml')\n                        for k in args_to_overwrite:\n                            best_hp_params[k] = saved_args.args[k]\n                        \n                    self.cfg = utils.assign_test_params(self.cfg, best_hp_params)\n\n                # do testing\n                self.cfg.trainer.calc_time = False\n                log.debug('Retraining model')\n                validation_results = self.train(None, self.cfg.args, label, features, processed_data, validation_adjacency,\n                           processed_valid_data)\n                results = self.testing_loop(label, features, test_adjacency, processed_test_data,\n                                            self.cfg.trainer.test_metrics)\n                \n                # log test results\n                right_order_results = [results[k] for k in self.cfg.trainer.test_metrics]\n                to_log_trial_values = ''.join(f'| {metric}: {right_order_results[i]} |' for i, metric in\n                                              enumerate(self.cfg.trainer.test_metrics))\n                log.info(f'Test results |{to_log_trial_values}|')\n\n                objective_results.append({'metrics': best_at_metrics,\n                                          'results': results,\n                                          'args': best_hp_params})\n                \n                if self.cfg.trainer.save_validation:\n                    objective_results[-1]['validation_results'] = validation_results\n\n                # re init the args object for assign_test params\n                self.cfg.args = copy.deepcopy(self.cfg.hypersaved_args)\n\n                if self.cfg.trainer.save_model and self.cfg.trainer.model_resolution_metric in best_at_metrics:\n                    log.info(f'Saving best version of model for {best_at_metrics}')\n                    torch.save({\"model\": self.model.state_dict(),\n                                \"args\": best_hp_params},\n                               f\"{self.cfg.trainer.models_path}{self.cfg.model}_{test_metrics}.pt\")\n\n\n        return objective_results\n\n    def train(self, trial: Trial, args: DictConfig, label: np.ndarray, features: np.ndarray, processed_data: tuple,\n              validation_adjacency: np.ndarray, processed_valid_data: tuple, prune_params=None):\n        \n        timings = np.zeros(2)\n        start = time.time()\n        # configuration of args if hyperparameter optimisation phase\n        if trial is not None:\n            log.info(f'Launching Trial {trial.number}')\n            # if finetuning or reusing init procedure then just use default architecture sizes\n            if self.cfg.trainer.finetuning_new_dataset or self.cfg.trainer.same_init_hpo:\n                args_to_overwrite = list(set(args.keys()).intersection(self.cfg.trainer.args_cant_finetune))\n                saved_args = OmegaConf.load(f'ugle/configs/models/{self.cfg.model}/{self.cfg.model}_default.yaml')\n                for k in args_to_overwrite:\n                    args[k] = saved_args.args[k]\n            self.cfg.args = ugle.utils.sample_hyperparameters(trial, args, prune_params)\n\n        # process model creation\n        processed_data = self.move_to_activedevice(processed_data)\n        self.training_preprocessing(self.cfg.args, processed_data)\n        self.model.train()\n\n        if self.cfg.trainer.finetuning_new_dataset:\n            log.info('Loading pretrained model')\n            self.model.load_state_dict(torch.load(f\"{self.cfg.trainer.models_path}{self.cfg.model}.pt\")['model'])\n            self.model.to(self.device)\n\n        if trial is not None:\n            if self.cfg.trainer.same_init_hpo and trial.number == 0:\n                log.info(\"Saving initilisation of parameters\")\n                torch.save(self.model.state_dict(), f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_init.pt\")\n\n            if self.cfg.trainer.same_init_hpo: \n                log.info(\"Loading same initilisation of parameters\")\n                self.model.load_state_dict(torch.load(f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_init.pt\"))\n                self.model.to(self.device)\n\n        # create training loop\n        self.progress_bar = alive_it(range(self.cfg.args.max_epoch))\n        best_so_far = np.zeros(len((self.cfg.trainer.valid_metrics)))\n        best_epochs = np.zeros(len((self.cfg.trainer.valid_metrics)), dtype=int)\n        best_so_far[self.cfg.trainer.valid_metrics.index('conductance')] = 1.\n        patience_waiting = np.zeros((len(self.cfg.trainer.valid_metrics)), dtype=int)\n\n        for self.current_epoch in self.progress_bar:\n            timings[0] += time.time() - start\n            start = time.time()\n            # check if validation time\n            if self.current_epoch % self.cfg.trainer.validate_every_nepochs == 0:\n                # put in validation mode \n                processed_data = self.move_to_cpudevice(processed_data)\n                results = self.testing_loop(label, features, validation_adjacency, processed_valid_data,\n                                    self.cfg.trainer.valid_metrics)\n                # put data back into training mode\n                processed_data = self.move_to_activedevice(processed_data)\n                # check better for each metric\n                for m, metric in enumerate(self.cfg.trainer.valid_metrics):\n                    if (results[metric] > best_so_far[m] and metric != 'conductance') or (results[metric] < best_so_far[m] and metric == 'conductance'):\n                        best_so_far[m] = results[metric]\n                        best_epochs[m] = self.current_epoch\n                        # save model for this metric\n                        torch.save({\"model\": self.model.state_dict(),\n                                    \"args\": self.cfg.args},\n                                    f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_{metric}.pt\")\n                        patience_waiting[m] = 0\n                    else:\n                        patience_waiting[m] += 1\n            else: \n                patience_waiting += 1\n            \n            timings[1] += time.time() - start\n            start = time.time()\n                \n            # compute training iteration \n            loss, data_returned = self.training_epoch_iter(self.cfg.args, processed_data)\n            if data_returned:\n                processed_data = data_returned\n            # optimise\n            for opt in self.optimizers:\n                opt.zero_grad()\n            loss.backward()\n            for opt in self.optimizers:\n                opt.step()\n            if self.scheduler:\n                self.scheduler.step()\n            # update progress bar\n            self.progress_bar.title = f'Training {self.cfg.model} on {self.cfg.dataset} ... Epoch {self.current_epoch}: Loss={loss.item():.4f}'\n            # if all metrics hit patience then end\n            if patience_waiting.all() >= self.cfg.args.patience:\n                log.info(f'Early stopping at epoch {self.current_epoch}!')\n                break\n\n       \n        timings[0] += time.time() - start\n        start = time.time()    \n\n        # compute final validation \n        processed_data = self.move_to_cpudevice(processed_data)\n        return_results = {}\n        if self.cfg.trainer.save_validation:\n            valid_results = {}\n        for m, metric in enumerate(self.cfg.trainer.valid_metrics):\n            log.info(f'Best model for {metric} at epoch {best_epochs[m]}')\n            self.model.load_state_dict(torch.load(f\"{self.cfg.trainer.models_path}{self.cfg.model}_{self.device_name}_{metric}.pt\")['model'])\n            self.model.to(self.device)\n            results = self.testing_loop(label, features, validation_adjacency, processed_valid_data,\n                                        self.cfg.trainer.valid_metrics)\n            return_results[metric] = results[metric]\n            if self.cfg.trainer.save_validation:\n                results = self.testing_loop(label, features, validation_adjacency, processed_valid_data,\n                                        self.cfg.trainer.test_metrics)\n                valid_results[metric] = results\n\n        timings[1] += time.time() - start\n        start = time.time()\n\n        log.info(f\"Time Training {round(timings[0], 3)}s\")\n        log.info(f\"Time Validating {round(timings[1], 3)}s\")\n\n        if trial is None:\n            if not self.cfg.trainer.save_validation:\n                return\n            else:\n                return valid_results\n        else:\n            self.cfg.args = copy.deepcopy(self.cfg.hypersaved_args)\n            log_trial_result(trial, return_results, self.cfg.trainer.valid_metrics, self.cfg.trainer.multi_objective_study)\n\n            if not self.cfg.trainer.multi_objective_study:\n                return return_results[self.cfg.trainer.valid_metrics[0]]\n            else:\n                right_order_results = [return_results[k] for k in self.cfg.trainer.valid_metrics]\n                return tuple(right_order_results)\n            \n\n    def testing_loop(self, label: np.ndarray, features: np.ndarray, adjacency: np.ndarray, processed_data: tuple, eval_metrics: ListConfig):\n        \"\"\"\n        testing loop which processes the testing data, run through the model to get predictions\n        evaluate those predictions\n        \"\"\"\n        self.model.eval()\n        processed_data = self.move_to_activedevice(processed_data)\n        preds = self.test(processed_data)\n        processed_data = self.move_to_cpudevice(processed_data)\n        results, eval_preds = ugle.process.preds_eval(label,\n                                                      preds,\n                                                      sf=4,\n                                                      adj=adjacency,\n                                                      metrics=eval_metrics)\n        self.model.train()\n        return results\n\n    def preprocess_data(self, features: np.ndarray, adjacency: np.ndarray) -> tuple:\n        log.error('NO PROCESSING STEP IMPLEMENTED FOR MODEL')\n        pass\n\n    def test(self, processed_data: tuple) -> np.ndarray:\n        log.error('NO TESTING STEP IMPLEMENTED')\n        pass\n\n    def training_preprocessing(self, args: DictConfig, processed_data: tuple):\n        \"\"\"\n        preparation steps of training\n        \"\"\"\n        log.error('NO TRAINING PREPROCESSING PROCEDURE')\n        pass\n\n    def training_epoch_iter(self, args: DictConfig, processed_data: tuple):\n        \"\"\"\n        this is what happens inside the iteration of epochs\n        \"\"\"\n        log.error('NO TRAINING ITERATION LOOP')\n        pass\n\n    def move_to_cpudevice(self, data):\n        return tuple(databite.to(torch.device(\"cpu\"), non_blocking=True) if torch.is_tensor(databite) else databite for databite in data)\n\n    def move_to_activedevice(self, data):\n        return tuple(databite.to(self.device, non_blocking=True)  if torch.is_tensor(databite) else databite for databite in data)", "\n\n"]}
{"filename": "ugle/models/selfgnn.py", "chunked_list": ["# https://github.com/zekarias-tilahun/SelfGNN\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom fast_pytorch_kmeans import KMeans\nimport scipy.sparse as sp\nfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv\nfrom functools import wraps\nimport copy\nimport ugle", "import copy\nimport ugle\nfrom ugle.trainer import ugleTrainer\n\n\nclass EMA:\n    def __init__(self, beta):\n        super().__init__()\n        self.beta = beta\n\n    def update_average(self, old, new):\n        if old is None:\n            return new\n        return old * self.beta + (1 - self.beta) * new", "\n\ndef loss_fn(x, y):\n    x = F.normalize(x, dim=-1, p=2)\n    y = F.normalize(y, dim=-1, p=2)\n    return 2 - 2 * (x * y).sum(dim=-1)\n\n\ndef singleton(cache_key):\n    def inner_fn(fn):\n        @wraps(fn)\n        def wrapper(self, *args, **kwargs):\n            instance = getattr(self, cache_key)\n            if instance is not None:\n                return instance\n\n            instance = fn(self, *args, **kwargs)\n            setattr(self, cache_key, instance)\n            return instance\n\n        return wrapper\n\n    return inner_fn", "def singleton(cache_key):\n    def inner_fn(fn):\n        @wraps(fn)\n        def wrapper(self, *args, **kwargs):\n            instance = getattr(self, cache_key)\n            if instance is not None:\n                return instance\n\n            instance = fn(self, *args, **kwargs)\n            setattr(self, cache_key, instance)\n            return instance\n\n        return wrapper\n\n    return inner_fn", "\n\ndef update_moving_average(ema_updater, ma_model, current_model):\n    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n        old_weight, up_weight = ma_params.data, current_params.data\n        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n\n\ndef set_requires_grad(model, val):\n    for p in model.parameters():\n        p.requires_grad = val", "def set_requires_grad(model, val):\n    for p in model.parameters():\n        p.requires_grad = val\n\n\nclass Normalize(nn.Module):\n    def __init__(self, dim=None, method=\"batch\"):\n        super().__init__()\n        method = None if dim is None else method\n        if method == \"batch\":\n            self.norm = nn.BatchNorm1d(dim)\n        elif method == \"layer\":\n            self.norm = nn.LayerNorm(dim)\n        else:  # No norm => identity\n            self.norm = lambda x: x\n\n    def forward(self, x):\n        return self.norm(x)", "\n\nclass Encoder(nn.Module):\n\n    def __init__(self, args, layers, heads, dropout=None):\n        super().__init__()\n        rep_dim = layers[-1]\n        self.gnn_type = args.gnn_type\n        self.dropout = dropout\n        self.project = args.prj_head_norm\n\n        self.stacked_gnn = get_encoder(args.gnn_type, layers, heads, args.concat)\n        self.encoder_norm = Normalize(dim=rep_dim, method=args.encoder_norm)\n        if self.project != \"no\":\n            self.projection_head = nn.Sequential(\n                nn.Linear(rep_dim, rep_dim),\n                Normalize(dim=rep_dim, method=args.prj_head_norm),\n                nn.ReLU(inplace=True), nn.Dropout(dropout))\n\n    def forward(self, x, edge_index, edge_weight=None):\n        for i, gnn in enumerate(self.stacked_gnn):\n            if self.gnn_type == \"gat\" or self.gnn_type == \"sage\":\n                x = gnn(x, edge_index)\n            else:\n                x = gnn(x, edge_index, edge_weight=edge_weight)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.encoder_norm(x)\n        return x, (self.projection_head(x) if self.project != \"no\" else None)", "\n\nclass SelfGNN(nn.Module):\n\n    def __init__(self, args, layers, heads, dropout=0.0, moving_average_decay=0.99):\n        super().__init__()\n        self.student_encoder = Encoder(args, layers=layers, heads=heads, dropout=dropout)\n        self.teacher_encoder = None\n        self.teacher_ema_updater = EMA(moving_average_decay)\n        rep_dim = layers[-1]\n        self.student_predictor = nn.Sequential(\n            nn.Linear(rep_dim, rep_dim),\n            Normalize(dim=rep_dim, method=args.prd_head_norm),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout))\n\n    @singleton('teacher_encoder')\n    def _get_teacher_encoder(self):\n        teacher_encoder = copy.deepcopy(self.student_encoder)\n        set_requires_grad(teacher_encoder, False)\n        return teacher_encoder\n\n    def reset_moving_average(self):\n        del self.teacher_encoder\n        self.teacher_encoder = None\n\n    def update_moving_average(self):\n        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n\n    def encode(self, x, edge_index, edge_weight=None, encoder=None):\n        encoder = self.student_encoder if encoder is None else encoder\n        encoder.train(self.training)\n        return encoder(x, edge_index, edge_weight)\n\n    def forward(self, x1, x2, edge_index_v1, edge_index_v2, edge_weight_v1=None, edge_weight_v2=None):\n        \"\"\"\n        Apply student network on both views\n\n        v<x>_rep is the output of the stacked GNN\n        v<x>_student is the output of the student projection head, if used, otherwise is just a reference to v<x>_rep\n        \"\"\"\n        v1_enc = self.encode(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n        v1_rep, v1_student = v1_enc if v1_enc[1] is not None else (v1_enc[0], v1_enc[0])\n        v2_enc = self.encode(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n        v2_rep, v2_student = v2_enc if v2_enc[1] is not None else (v2_enc[0], v2_enc[0])\n\n        \"\"\"\n        Apply the student predictor both views using the outputs from the previous phase \n        (after the stacked GNN or projection head - if there is one)\n        \"\"\"\n        v1_pred = self.student_predictor(v1_student)\n        v2_pred = self.student_predictor(v2_student)\n\n        \"\"\"\n        Apply the same procedure on the teacher network as in the student network except the predictor.\n        \"\"\"\n        with torch.no_grad():\n            teacher_encoder = self._get_teacher_encoder()\n            v1_enc = self.encode(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1, encoder=teacher_encoder)\n            v1_teacher = v1_enc[1] if v1_enc[1] is not None else v1_enc[0]\n            v2_enc = self.encode(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2, encoder=teacher_encoder)\n            v2_teacher = v2_enc[1] if v2_enc[1] is not None else v2_enc[0]\n\n        \"\"\"\n        Compute symmetric loss (once based on view1 (v1) as input to the student and then using view2 (v2))\n        \"\"\"\n        loss1 = loss_fn(v1_pred, v2_teacher.detach())\n        loss2 = loss_fn(v2_pred, v1_teacher.detach())\n\n        loss = loss1 + loss2\n        return v1_rep, v2_rep, loss.mean()", "\n\ndef get_encoder(gnn_type, layers, heads, concat):\n    \"\"\"\n    Builds the GNN backbone as required\n    \"\"\"\n    if gnn_type == \"gcn\":\n        return nn.ModuleList([GCNConv(layers[i - 1], layers[i]) for i in range(1, len(layers))])\n    elif gnn_type == \"sage\":\n        return nn.ModuleList([SAGEConv(layers[i - 1], layers[i]) for i in range(1, len(layers))])\n    elif gnn_type == \"gat\":\n        return nn.ModuleList(\n            [GATConv(layers[i - 1], layers[i] // heads[i - 1], heads=heads[i - 1], concat=concat)\n             for i in range(1, len(layers))])", "\n\nclass selfgnn_trainer(ugleTrainer):\n    def preprocess_data(self, features, adjacency):\n\n        adjacency = sp.csr_matrix(adjacency)\n\n        augmentation = ugle.datasets.Augmentations(method=self.cfg.args.aug)\n        features, adjacency, aug_features, aug_adjacency = augmentation(features, adjacency)\n\n        features = torch.FloatTensor(features)\n        adjacency = torch.LongTensor(adjacency)\n        aug_features = torch.FloatTensor(aug_features)\n        aug_adjacency = torch.LongTensor(aug_adjacency)\n\n        diff = abs(aug_features.shape[1] - features.shape[1])\n        if diff > 0:\n            \"\"\"\n            Data augmentation on the features could lead to mismatch between the shape of the two views,\n            hence the smaller view should be padded with zero. (smaller_data is a reference, changes will\n            reflect on the original data)\n            \"\"\"\n            which_small = 'features' if features.shape[1] < aug_features.shape[1] else 'aug_features'\n            smaller_data = features if which_small == 'features' else aug_features\n            smaller_data = F.pad(smaller_data, pad=(0, diff))\n            if which_small == 'features':\n                features = smaller_data\n            else:\n                aug_features = smaller_data\n            features = F.normalize(features)\n            aug_features = F.normalize(aug_features)\n            self.cfg.args.n_features = features.shape[1]\n\n        return features, adjacency, aug_features, aug_adjacency\n\n    def training_preprocessing(self, args, processed_data):\n\n        layers = [args.n_features, args.layer1, args.layer2]\n        heads = [args.head1, args.head2]\n        self.model = SelfGNN(args=args, layers=layers, heads=heads).to(self.device)\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        self.optimizers = [optimizer]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        features, adjacency, aug_features, aug_adjacency = processed_data\n        v1_output, v2_output, loss = self.model(\n            x1=features, x2=aug_features, edge_index_v1=adjacency, edge_index_v2=aug_adjacency)\n        self.model.update_moving_average()\n        return loss, None\n\n    def test(self, processed_data):\n        features, adjacency, aug_features, aug_adjacency = processed_data\n\n        self.model.eval()\n\n        v1_output, v2_output, _ = self.model(\n            x1=features, x2=aug_features, edge_index_v1=adjacency,\n            edge_index_v2=aug_adjacency)\n\n        emb = torch.cat([v1_output, v2_output], dim=1).detach()\n        emb = emb.squeeze(0)\n        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n        preds = kmeans.fit_predict(emb).cpu().numpy()\n\n        return preds", "\n"]}
{"filename": "ugle/models/cagc.py", "chunked_list": ["# https://github.com/wangtong627/CAGC/\nimport torch\nimport torch.nn as nn\nfrom torch_geometric.nn import GATConv\nfrom ugle.trainer import ugleTrainer\nimport numpy as np\nfrom sklearn import cluster\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import svds\nfrom sklearn.preprocessing import normalize", "from scipy.sparse.linalg import svds\nfrom sklearn.preprocessing import normalize\nimport torch.nn.functional as F\n\n\ndef knn_fast(X, k):\n    X = F.normalize(X, dim=1, p=2)\n    similarities = torch.mm(X, X.t())\n    vals, inds = similarities.topk(k=k + 1, dim=-1)\n    return inds", "\n\ndef sim(z1: torch.Tensor, z2: torch.Tensor):\n    z1 = F.normalize(z1)\n    z2 = F.normalize(z2)\n    return torch.mm(z1, z2.t())\n\n\ndef semi_loss(z1: torch.Tensor, z2: torch.Tensor, tau: float):\n    f = lambda x: torch.exp(x / tau)\n    refl_sim = f(sim(z1, z1))\n    between_sim = f(sim(z1, z2))\n    return -torch.log(\n        between_sim.diag()\n        / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))", "def semi_loss(z1: torch.Tensor, z2: torch.Tensor, tau: float):\n    f = lambda x: torch.exp(x / tau)\n    refl_sim = f(sim(z1, z1))\n    between_sim = f(sim(z1, z2))\n    return -torch.log(\n        between_sim.diag()\n        / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))\n\n\ndef instanceloss(z1: torch.Tensor, z2: torch.Tensor, tau: float, mean: bool = True):\n    l1 = semi_loss(z1, z2, tau)\n    l2 = semi_loss(z2, z1, tau)\n    ret = (l1 + l2) * 0.5\n    ret = ret.mean() if mean else ret.sum()\n    return ret", "\ndef instanceloss(z1: torch.Tensor, z2: torch.Tensor, tau: float, mean: bool = True):\n    l1 = semi_loss(z1, z2, tau)\n    l2 = semi_loss(z2, z1, tau)\n    ret = (l1 + l2) * 0.5\n    ret = ret.mean() if mean else ret.sum()\n    return ret\n\n\ndef knbrsloss(H, k, n_nodes, tau_knbrs, device):\n    indices = knn_fast(H, k)\n    f = lambda x: torch.exp(x / tau_knbrs)\n    refl_sim = f(sim(H, H))\n    ind2 = indices[:, 1:]\n    V = torch.gather(refl_sim, 1, ind2)\n    ret = -torch.log(\n        V.sum(1) / (refl_sim.sum(1) - refl_sim.diag()))\n    ret = ret.mean()\n    return ret", "\ndef knbrsloss(H, k, n_nodes, tau_knbrs, device):\n    indices = knn_fast(H, k)\n    f = lambda x: torch.exp(x / tau_knbrs)\n    refl_sim = f(sim(H, H))\n    ind2 = indices[:, 1:]\n    V = torch.gather(refl_sim, 1, ind2)\n    ret = -torch.log(\n        V.sum(1) / (refl_sim.sum(1) - refl_sim.diag()))\n    ret = ret.mean()\n    return ret", "\n\ndef post_proC(C, K, d=6, alpha=8):\n    # C: coefficient matrix, K: number of clusters, d: dimension of each subspace\n    C = 0.5 * (C + C.T)\n    r = d * K + 1\n    U, S, _ = svds(C, r, v0=np.ones(C.shape[0]))\n    U = U[:, ::-1]\n    S = np.sqrt(S[::-1])\n    S = np.diag(S)\n    U = U.dot(S)\n    U = normalize(U, norm='l2', axis=1)\n    Z = U.dot(U.T)\n    Z = Z * (Z > 0)\n    L = np.abs(Z ** alpha)\n    L = L / L.max()\n    L = 0.5 * (L + L.T)\n    spectral = cluster.SpectralClustering(n_clusters=K, eigen_solver='arpack', affinity='precomputed',\n                                          assign_labels='discretize')\n    spectral.fit(L)\n    grp = spectral.fit_predict(L) + 1\n    return grp, L", "\n\ndef thrC(C, ro):\n    if ro < 1:\n        N = C.shape[1]\n        Cp = np.zeros((N, N))\n        S = np.abs(np.sort(-np.abs(C), axis=0))\n        Ind = np.argsort(-np.abs(C), axis=0)\n        for i in range(N):\n            cL1 = np.sum(S[:, i]).astype(float)\n            stop = False\n            csum = 0\n            t = 0\n            while stop == False:\n                csum = csum + S[t, i]\n                if csum > ro * cL1:\n                    stop = True\n                    Cp[Ind[0:t + 1, i], i] = C[Ind[0:t + 1, i], i]\n                t = t + 1\n    else:\n        Cp = C\n\n    return Cp", "\n\nclass Encoder(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, activation, base_model, k: int = 2, skip=False):\n        super(Encoder, self).__init__()\n        self.base_model = base_model\n\n        assert k >= 2\n        self.k = k\n        self.skip = skip\n        if not self.skip:\n            self.conv = [base_model(in_channels, 2 * out_channels).jittable()]\n            for _ in range(1, k - 1):\n                self.conv.append(base_model(1 * out_channels, 1 * out_channels))\n            self.conv.append(base_model(2 * out_channels, out_channels))\n            self.conv = nn.ModuleList(self.conv)\n\n            self.activation = activation\n        else:\n            self.fc_skip = nn.Linear(in_channels, out_channels)\n            self.conv = [base_model(in_channels, out_channels)]\n            for _ in range(1, k):\n                self.conv.append(base_model(out_channels, out_channels))\n            self.conv = nn.ModuleList(self.conv)\n\n            self.activation = activation\n\n    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n        if not self.skip:\n            for i in range(self.k):\n                x = self.activation(self.conv[i](x, edge_index))\n            return x\n        else:\n            h = self.activation(self.conv[0](x, edge_index))\n            hs = [self.fc_skip(x), h]\n            for i in range(1, self.k):\n                u = sum(hs)\n                hs.append(self.activation(self.conv[i](u, edge_index)))\n            return hs[-1]", "\n\nclass Decoder(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, activation, base_model, k: int = 2, skip=False):\n        super(Decoder, self).__init__()\n        self.base_model = base_model\n\n        assert k >= 2\n        self.k = k\n        self.skip = skip\n        if not self.skip:\n            self.conv = [base_model(in_channels, 2 * in_channels).jittable()]\n            for _ in range(1, k - 1):\n                self.conv.append(base_model(1 * in_channels, 1 * in_channels))\n            self.conv.append(base_model(2 * in_channels, out_channels))\n            self.conv = nn.ModuleList(self.conv)\n\n            self.activation = activation\n        else:\n            self.fc_skip = nn.Linear(in_channels, out_channels)\n            self.conv = [base_model(in_channels, in_channels)]\n            for _ in range(1, k - 1):\n                self.conv = [base_model(in_channels, in_channels)]\n            self.conv.append(base_model(in_channels, out_channels))\n            self.conv = nn.ModuleList(self.conv)\n            self.activation = activation\n\n    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n        if not self.skip:\n            for i in range(self.k):\n                x = self.activation(self.conv[i](x, edge_index))\n            return x\n        else:\n            h = self.activation(self.conv[0](x, edge_index))\n            hs = [self.fc_skip(x), h]\n            for i in range(1, self.k):\n                u = sum(hs)\n                hs.append(self.activation(self.conv[i](u, edge_index)))\n            return hs[-1]", "\n\nclass Model(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, num_sample: int, device):\n        super(Model, self).__init__()\n        self.device = device\n        self.n = num_sample\n        self.encoder: Encoder = encoder\n        self.decoder: Decoder = decoder\n        self.Coefficient = nn.Parameter(1.0e-8 * torch.ones(self.n, self.n, dtype=torch.float32),\n                                        requires_grad=True).to(self.device)\n\n    def forward(self, x, edge_index):\n        # self expression layer, reshape to vectors, multiply Coefficient, then reshape back\n        H = self.encoder(x, edge_index)\n        CH = torch.matmul(self.Coefficient, H)\n        X_ = self.decoder(CH, edge_index)\n\n        return H, CH, self.Coefficient, X_", "\n\nclass cagc_trainer(ugleTrainer):\n    def preprocess_data(self, features, adjacency):\n        adj_label = sp.coo_matrix(adjacency)\n        adj_label = adj_label.todok()\n\n        outwards = [i[0] for i in adj_label.keys()]\n        inwards = [i[1] for i in adj_label.keys()]\n\n        adj = torch.from_numpy(np.array([outwards, inwards], dtype=np.int))\n\n        data = torch.FloatTensor(features)\n\n        self.cfg.args.alpha = max(0.4 - (self.cfg.args.n_clusters - 1) / 10 * 0.1, 0.1)\n        self.cfg.hypersaved_args.alpha = self.cfg.args.alpha\n\n        return data, adj\n\n    def training_preprocessing(self, args, processed_data):\n        activation = nn.PReLU()\n        encoder = Encoder(args.n_features, args.num_hidden, activation,\n                          base_model=GATConv, k=args.num_layers).to(self.device)\n\n        decoder = Decoder(args.num_hidden, args.n_features, activation,\n                          base_model=GATConv, k=args.num_layers).to(self.device)\n\n        self.model = Model(encoder, decoder, args.n_nodes, self.device).to(self.device)\n\n        optimizer = torch.optim.Adam(\n            self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\n        self.optimizers = [optimizer]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        data, adj = processed_data\n        H, CH, Coefficient, X_ = self.model(data, adj)\n        loss_knbrs = knbrsloss(H, 10, args.n_nodes, args.tau_knbrs, self.device)\n        rec_loss = torch.sum(torch.pow(data - X_, 2))\n        loss_instance = instanceloss(H, CH, args.tau)\n        loss_coef = torch.sum(torch.pow(Coefficient, 2))\n        loss = (args.loss_instance * loss_instance) + (args.loss_knbrs * loss_knbrs) + (args.loss_coef * loss_coef) \\\n               + (args.rec_loss * rec_loss)\n\n        return loss, None\n\n    def test(self, processed_data):\n        data, adj = processed_data\n        self.model.eval()\n        _ , _, Coefficient, _ = self.model(data, adj)\n        # get C\n        C = Coefficient.detach().to('cpu').numpy()\n        commonZ = thrC(C, self.cfg.args.alpha)\n        preds, _ = post_proC(commonZ, self.cfg.args.n_clusters)\n\n        return preds", ""]}
{"filename": "ugle/models/dmon.py", "chunked_list": ["# https://github.com/google-research/google-research/blob/master/graph_embedding/dmon/dmon.py\nimport ugle\nimport scipy.sparse as sp\nfrom ugle.trainer import ugleTrainer\nimport torch.nn as nn\nimport torch\nfrom ugle.gnn_architecture import GCN\nimport math\nfrom collections import OrderedDict\n", "from collections import OrderedDict\n\n\nclass DMoN(nn.Module):\n    def __init__(self,\n                 args,\n                 act='selu',\n                 do_unpooling=False):\n        \"\"\"Initializes the layer with specified parameters.\"\"\"\n        super(DMoN, self).__init__()\n        self.args = args\n        self.n_clusters = args.n_clusters\n        self.orthogonality_regularization = args.orthogonality_regularization\n        self.cluster_size_regularization = args.cluster_size_regularization\n        self.dropout_rate = args.dropout_rate\n        self.do_unpooling = do_unpooling\n        self.gcn = GCN(args.n_features, args.architecture, act=act, skip=True)\n        self.transform = nn.Sequential(OrderedDict([\n            ('layer1', nn.Linear(args.architecture, args.n_clusters)),\n            ('dropout', nn.Dropout(args.dropout_rate)),\n        ]))\n\n        def init_weights(m):\n            if isinstance(m, nn.Linear):\n                nn.init.orthogonal_(m.weight.data, gain=math.sqrt(2))\n                if m.bias is not None:\n                    m.bias.data.fill_(0.0)\n\n        self.transform.apply(init_weights)\n\n        return\n\n\n    def forward(self, graph, graph_normalised, features, extra_loss=False):\n\n        gcn_out = self.gcn(features, graph_normalised, sparse=True, skip=True)\n\n        assignments = self.transform(gcn_out).squeeze(0)\n        assignments = nn.functional.softmax(assignments, dim=1)\n\n        n_edges = graph._nnz()\n        degrees = torch.sparse.sum(graph, dim=0)._values().unsqueeze(1)\n        graph_pooled = torch.spmm(torch.spmm(graph, assignments).T, assignments)\n        normalizer_left = torch.spmm(assignments.T, degrees)\n        normalizer_right = torch.spmm(assignments.T, degrees).T\n        normalizer = torch.spmm(normalizer_left, normalizer_right) / 2 / n_edges\n        spectral_loss = - torch.trace(graph_pooled - normalizer) / 2 / n_edges\n        loss = spectral_loss\n\n        if extra_loss:\n            pairwise = torch.spmm(assignments.T, assignments)\n            identity = torch.eye(self.n_clusters).to(pairwise.device)\n            orthogonality_loss = torch.norm(pairwise / torch.norm(pairwise) -\n                                         identity / math.sqrt(float(self.n_clusters)))\n            orthogonality_loss *= self.orthogonality_regularization\n            loss += orthogonality_loss\n\n            cluster_loss = torch.norm(torch.sum(pairwise, dim=1)) / self.args.n_nodes * math.sqrt(float(self.n_clusters)) - 1\n            cluster_loss *= self.cluster_size_regularization\n            loss += cluster_loss\n\n        else:\n            cluster_sizes = torch.sum(assignments, dim=0)\n            cluster_loss = torch.norm(cluster_sizes) / self.args.n_nodes * math.sqrt(float(self.n_clusters)) - 1\n            cluster_loss *= self.cluster_size_regularization\n            loss += cluster_loss\n\n        return loss\n\n    def embed(self, graph, graph_normalised, features):\n        gcn_out = self.gcn(features, graph_normalised, sparse=True)\n        assignments = self.transform(gcn_out).squeeze(0)\n        assignments = nn.functional.softmax(assignments, dim=1)\n\n        return assignments", "\n\nclass dmon_trainer(ugleTrainer):\n\n    def preprocess_data(self, features, adjacency):\n        features = torch.FloatTensor(features)\n\n        adjacency = adjacency + sp.eye(adjacency.shape[0])\n        adj_label = adjacency.copy()\n        adjacency = ugle.process.normalize_adj(adjacency)\n\n        graph_normalised = ugle.process.sparse_mx_to_torch_sparse_tensor(adjacency)\n\n        adj_label = sp.coo_matrix(adj_label)\n        graph = ugle.process.sparse_mx_to_torch_sparse_tensor(adj_label)\n\n        return graph, graph_normalised, features\n\n    def training_preprocessing(self, args, processed_data):\n\n        self.model = DMoN(args).to(self.device)\n        optimiser = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        self.optimizers = [optimiser]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        graph, graph_normalised, features = processed_data\n        loss = self.model(graph, graph_normalised, features)\n\n        return loss, None\n\n    def test(self, processed_data):\n        graph, graph_normalised, features = processed_data\n        with torch.no_grad():\n            assignments = self.model.embed(graph, graph_normalised, features)\n            preds = assignments.detach().cpu().numpy().argmax(axis=1)\n\n        return preds", "\n\n\n\n"]}
{"filename": "ugle/models/sublime.py", "chunked_list": ["# https://github.com/GRAND-Lab/SUBLIME\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Sequential, Linear, ReLU\nimport torch.nn.functional as F\nimport numpy as np\nimport copy\nfrom fast_pytorch_kmeans import KMeans\nfrom ugle.trainer import ugleTrainer\nEOS = 1e-10", "from ugle.trainer import ugleTrainer\nEOS = 1e-10\n\nclass GCNConv_dense(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(GCNConv_dense, self).__init__()\n        self.linear = nn.Linear(input_size, output_size)\n\n    def init_para(self):\n        self.linear.reset_parameters()\n\n    def forward(self, input, A, sparse=False):\n        hidden = self.linear(input)\n        output = torch.matmul(A, hidden)\n        return output", "\n\nclass GCN(nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, num_layers, dropout, dropout_adj, Adj, sparse):\n        super(GCN, self).__init__()\n        self.layers = nn.ModuleList()\n\n        self.layers.append(GCNConv_dense(in_channels, hidden_channels))\n        for i in range(num_layers - 2):\n            self.layers.append(GCNConv_dense(hidden_channels, hidden_channels))\n        self.layers.append(GCNConv_dense(hidden_channels, out_channels))\n\n        self.dropout = dropout\n        self.dropout_adj_p = dropout_adj\n        self.Adj = Adj\n        self.Adj.requires_grad = False\n\n        self.dropout_adj = nn.Dropout(p=dropout_adj)\n\n    def forward(self, x):\n\n        Adj = self.dropout_adj(self.Adj)\n\n        for i, conv in enumerate(self.layers[:-1]):\n            x = conv(x, Adj)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.layers[-1](x, Adj)\n        return x", "\n\nclass GraphEncoder(nn.Module):\n    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj):\n\n        super(GraphEncoder, self).__init__()\n        self.dropout = dropout\n        self.dropout_adj_p = dropout_adj\n\n        self.gnn_encoder_layers = nn.ModuleList()\n\n        self.gnn_encoder_layers.append(GCNConv_dense(in_dim, hidden_dim))\n        for _ in range(nlayers - 2):\n            self.gnn_encoder_layers.append(GCNConv_dense(hidden_dim, hidden_dim))\n        self.gnn_encoder_layers.append(GCNConv_dense(hidden_dim, emb_dim))\n\n        self.dropout_adj = nn.Dropout(p=dropout_adj)\n\n        self.proj_head = Sequential(Linear(emb_dim, proj_dim), ReLU(inplace=True),\n                                    Linear(proj_dim, proj_dim))\n\n    def forward(self, x, Adj_, branch=None):\n\n        Adj = self.dropout_adj(Adj_)\n\n        for conv in self.gnn_encoder_layers[:-1]:\n            x = conv(x, Adj)\n            x = F.relu(x)\n            x = F.dropout(x, p=self.dropout, training=self.training)\n        x = self.gnn_encoder_layers[-1](x, Adj)\n        z = self.proj_head(x)\n        return z, x", "\n\nclass GCL(nn.Module):\n    def __init__(self, nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj, sparse=False):\n        super(GCL, self).__init__()\n\n        self.encoder = GraphEncoder(nlayers, in_dim, hidden_dim, emb_dim, proj_dim, dropout, dropout_adj)\n\n    def forward(self, x, Adj_, branch=None):\n        z, embedding = self.encoder(x, Adj_, branch)\n        return z, embedding\n\n    @staticmethod\n    def calc_loss(x, x_aug, temperature=0.2, sym=True):\n        batch_size, _ = x.size()\n        x_abs = x.norm(dim=1)\n        x_aug_abs = x_aug.norm(dim=1)\n\n        sim_matrix = torch.einsum('ik,jk->ij', x, x_aug) / torch.einsum('i,j->ij', x_abs, x_aug_abs)\n        sim_matrix = torch.exp(sim_matrix / temperature)\n        pos_sim = sim_matrix[range(batch_size), range(batch_size)]\n        if sym:\n            loss_0 = pos_sim / (sim_matrix.sum(dim=0) - pos_sim)\n            loss_1 = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n\n            loss_0 = - torch.log(loss_0).mean()\n            loss_1 = - torch.log(loss_1).mean()\n            loss = (loss_0 + loss_1) / 2.0\n            return loss\n        else:\n            loss_1 = pos_sim / (sim_matrix.sum(dim=1) - pos_sim)\n            loss_1 = - torch.log(loss_1).mean()\n            return loss_1", "\n\nclass MLP_learner(nn.Module):\n    def __init__(self, nlayers, isize, k, knn_metric, i, sparse, act):\n        super(MLP_learner, self).__init__()\n\n        self.layers = nn.ModuleList()\n        if nlayers == 1:\n            self.layers.append(nn.Linear(isize, isize))\n        else:\n            self.layers.append(nn.Linear(isize, isize))\n            for _ in range(nlayers - 2):\n                self.layers.append(nn.Linear(isize, isize))\n            self.layers.append(nn.Linear(isize, isize))\n\n        self.input_dim = isize\n        self.output_dim = isize\n        self.k = k\n        self.knn_metric = knn_metric\n        self.non_linearity = 'relu'\n        self.param_init()\n        self.i = i\n        self.act = act\n\n    def internal_forward(self, h):\n        for i, layer in enumerate(self.layers):\n            h = layer(h)\n            if i != (len(self.layers) - 1):\n                if self.act == \"relu\":\n                    h = F.relu(h)\n                elif self.act == \"tanh\":\n                    h = F.tanh(h)\n        return h\n\n    def param_init(self):\n        for layer in self.layers:\n            layer.weight = nn.Parameter(torch.eye(self.input_dim))", "\ndef cal_similarity_graph(node_embeddings):\n    similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n    return similarity_graph\n\n\ndef apply_non_linearity(tensor, non_linearity, i):\n    if non_linearity == 'elu':\n        return F.elu(tensor * i - i) + 1\n    elif non_linearity == 'relu':\n        return F.relu(tensor)\n    elif non_linearity == 'none':\n        return tensor\n    else:\n        raise NameError('We dont support the non-linearity yet')", "\n\ndef symmetrize(adj):  # only for non-sparse\n    return (adj + adj.T) / 2\n\n\ndef normalize(adj, mode, sparse=False):\n\n    if mode == \"sym\":\n        inv_sqrt_degree = 1. / (torch.sqrt(adj.sum(dim=1, keepdim=False)) + EOS)\n        return inv_sqrt_degree[:, None] * adj * inv_sqrt_degree[None, :]\n    elif mode == \"row\":\n        inv_degree = 1. / (adj.sum(dim=1, keepdim=False) + EOS)\n        return inv_degree[:, None] * adj\n    else:\n        exit(\"wrong norm mode\")", "\n\n\ndef split_batch(init_list, batch_size):\n    groups = zip(*(iter(init_list),) * batch_size)\n    end_list = [list(i) for i in groups]\n    count = len(init_list) % batch_size\n    end_list.append(init_list[-count:]) if count != 0 else end_list\n    return end_list\n", "\n\ndef get_feat_mask(features, mask_rate):\n    feat_node = features.shape[1]\n    mask = torch.zeros(features.shape)\n    samples = np.random.choice(feat_node, size=int(feat_node * mask_rate), replace=False)\n    mask[:, samples] = 1\n    return mask, samples\n\n\nclass sublime_trainer(ugleTrainer):\n    def knn_fast(self, X, k, b):\n        X = F.normalize(X, dim=1, p=2)\n        index = 0\n        values = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n        rows = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n        cols = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n        norm_row = torch.zeros(X.shape[0]).to(self.device)\n        norm_col = torch.zeros(X.shape[0]).to(self.device)\n        while index < X.shape[0]:\n            if (index + b) > (X.shape[0]):\n                end = X.shape[0]\n            else:\n                end = index + b\n            sub_tensor = X[index:index + b]\n            similarities = torch.mm(sub_tensor, X.t())\n            vals, inds = similarities.topk(k=k + 1, dim=-1)\n            values[index * (k + 1):(end) * (k + 1)] = vals.view(-1)\n            cols[index * (k + 1):(end) * (k + 1)] = inds.view(-1)\n            rows[index * (k + 1):(end) * (k + 1)] = torch.arange(index, end).view(-1, 1).repeat(1, k + 1).view(-1)\n            norm_row[index: end] = torch.sum(vals, dim=1)\n            norm_col.index_add_(-1, inds.view(-1), vals.view(-1))\n            index += b\n        norm = norm_row + norm_col\n        rows = rows.long()\n        cols = cols.long()\n        values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))\n        return rows, cols, values\n\n    def top_k(self, raw_graph, K):\n        values, indices = raw_graph.topk(k=int(K), dim=-1)\n        assert torch.max(indices) < raw_graph.shape[1]\n        mask = torch.zeros(raw_graph.shape).to(self.device)\n        mask[torch.arange(raw_graph.shape[0]).view(-1, 1), indices] = 1.\n\n        mask.requires_grad = False\n        sparse_graph = raw_graph * mask\n        return sparse_graph\n\n    def graph_learner_forward(self, features):\n        embeddings = self.graph_learner.internal_forward(features)\n        embeddings = F.normalize(embeddings, dim=1, p=2)\n        similarities = cal_similarity_graph(embeddings)\n        similarities = self.top_k(similarities, self.graph_learner.k + 1)\n        similarities = apply_non_linearity(similarities, self.graph_learner.non_linearity, self.graph_learner.i)\n        return similarities\n\n    def loss_gcl(self, model, features, anchor_adj):\n\n        # view 1: anchor graph\n        if self.cfg.args.maskfeat_rate_anchor:\n            mask_v1, _ = get_feat_mask(features, self.cfg.args.maskfeat_rate_anchor)\n            mask_v1 = mask_v1.to(self.device)\n            features_v1 = features * (1 - mask_v1)\n        else:\n            features_v1 = copy.deepcopy(features)\n\n        features_v1 = features_v1.to(self.device)\n        z1, _ = model(features_v1, anchor_adj, 'anchor')\n\n        # view 2: learned graph\n        if self.cfg.args.maskfeat_rate_learner:\n            mask, _ = get_feat_mask(features, self.cfg.args.maskfeat_rate_learner)\n            mask = mask.to(self.device)\n            features_v2 = features * (1 - mask)\n        else:\n            features_v2 = copy.deepcopy(features)\n\n        learned_adj = self.graph_learner_forward(features)\n\n\n        learned_adj = symmetrize(learned_adj)\n        learned_adj = normalize(learned_adj, 'sym')\n\n        z2, _ = model(features_v2, learned_adj, 'learner')\n\n        # compute loss\n        if self.cfg.args.contrast_batch_size:\n            node_idxs = list(range(self.cfg.agrs.n_nodes))\n            # random.shuffle(node_idxs)\n            batches = split_batch(node_idxs, self.cfg.args.contrast_batch_size)\n            loss = 0\n            for batch in batches:\n                weight = len(batch) / self.cfg.agrs.n_nodes\n                loss += model.calc_loss(z1[batch], z2[batch]) * weight\n        else:\n            loss = model.calc_loss(z1, z2)\n\n        return loss, learned_adj\n\n    def preprocess_data(self, features, adjacency):\n\n        anchor_adj_raw = torch.from_numpy(adjacency)\n\n        anchor_adj = normalize(anchor_adj_raw, 'sym')\n\n        features = torch.FloatTensor(features)\n        anchor_adj = anchor_adj.float()\n\n        return features, anchor_adj\n\n    def training_preprocessing(self, args, processed_data):\n        if args.learner_type == 'mlp':\n            self.graph_learner = MLP_learner(args.nlayers, args.n_features, args.k, args.sim_function, args.i, args.sparse,\n                                        args.activation_learner).to(self.device)\n\n        self.model = GCL(nlayers=args.nlayers, in_dim=args.n_features, hidden_dim=args.hidden_dim,\n                    emb_dim=args.rep_dim, proj_dim=args.proj_dim,\n                    dropout=args.dropout, dropout_adj=args.dropedge_rate).to(self.device)\n\n        optimizer_cl = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        optimizer_learner = torch.optim.Adam(self.graph_learner.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        self.optimizers = [optimizer_cl, optimizer_learner]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        features, anchor_adj = processed_data\n\n        loss, Adj = self.loss_gcl(self.model, features, anchor_adj)\n\n        # Structure Bootstrapping\n        if (1 - args.tau) and (args.c == 0 or self.current_epoch % args.c == 0):\n            anchor_adj = anchor_adj * args.tau + Adj.detach() * (1 - args.tau)\n\n        processed_data = (features, anchor_adj)\n\n        return loss, processed_data\n\n    def test(self, processed_data):\n        features, anchor_adj = processed_data\n\n        self.model.eval()\n        self.graph_learner.eval()\n        with torch.no_grad():\n            _, Adj = self.loss_gcl(self.model, features, anchor_adj)\n            _, embedding = self.model(features, Adj)\n            embedding = embedding.squeeze(0)\n\n            kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n            preds = kmeans.fit_predict(embedding).cpu().numpy()\n\n        return preds", "\n\nclass sublime_trainer(ugleTrainer):\n    def knn_fast(self, X, k, b):\n        X = F.normalize(X, dim=1, p=2)\n        index = 0\n        values = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n        rows = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n        cols = torch.zeros(X.shape[0] * (k + 1)).to(self.device)\n        norm_row = torch.zeros(X.shape[0]).to(self.device)\n        norm_col = torch.zeros(X.shape[0]).to(self.device)\n        while index < X.shape[0]:\n            if (index + b) > (X.shape[0]):\n                end = X.shape[0]\n            else:\n                end = index + b\n            sub_tensor = X[index:index + b]\n            similarities = torch.mm(sub_tensor, X.t())\n            vals, inds = similarities.topk(k=k + 1, dim=-1)\n            values[index * (k + 1):(end) * (k + 1)] = vals.view(-1)\n            cols[index * (k + 1):(end) * (k + 1)] = inds.view(-1)\n            rows[index * (k + 1):(end) * (k + 1)] = torch.arange(index, end).view(-1, 1).repeat(1, k + 1).view(-1)\n            norm_row[index: end] = torch.sum(vals, dim=1)\n            norm_col.index_add_(-1, inds.view(-1), vals.view(-1))\n            index += b\n        norm = norm_row + norm_col\n        rows = rows.long()\n        cols = cols.long()\n        values *= (torch.pow(norm[rows], -0.5) * torch.pow(norm[cols], -0.5))\n        return rows, cols, values\n\n    def top_k(self, raw_graph, K):\n        values, indices = raw_graph.topk(k=int(K), dim=-1)\n        assert torch.max(indices) < raw_graph.shape[1]\n        mask = torch.zeros(raw_graph.shape).to(self.device)\n        mask[torch.arange(raw_graph.shape[0]).view(-1, 1), indices] = 1.\n\n        mask.requires_grad = False\n        sparse_graph = raw_graph * mask\n        return sparse_graph\n\n    def graph_learner_forward(self, features):\n        embeddings = self.graph_learner.internal_forward(features)\n        embeddings = F.normalize(embeddings, dim=1, p=2)\n        similarities = cal_similarity_graph(embeddings)\n        similarities = self.top_k(similarities, self.graph_learner.k + 1)\n        similarities = apply_non_linearity(similarities, self.graph_learner.non_linearity, self.graph_learner.i)\n        return similarities\n\n    def loss_gcl(self, model, features, anchor_adj):\n\n        # view 1: anchor graph\n        if self.cfg.args.maskfeat_rate_anchor:\n            mask_v1, _ = get_feat_mask(features, self.cfg.args.maskfeat_rate_anchor)\n            mask_v1 = mask_v1.to(self.device)\n            features_v1 = features * (1 - mask_v1)\n        else:\n            features_v1 = copy.deepcopy(features)\n\n        features_v1 = features_v1.to(self.device)\n        z1, _ = model(features_v1, anchor_adj, 'anchor')\n\n        # view 2: learned graph\n        if self.cfg.args.maskfeat_rate_learner:\n            mask, _ = get_feat_mask(features, self.cfg.args.maskfeat_rate_learner)\n            mask = mask.to(self.device)\n            features_v2 = features * (1 - mask)\n        else:\n            features_v2 = copy.deepcopy(features)\n\n        learned_adj = self.graph_learner_forward(features)\n\n\n        learned_adj = symmetrize(learned_adj)\n        learned_adj = normalize(learned_adj, 'sym')\n\n        z2, _ = model(features_v2, learned_adj, 'learner')\n\n        # compute loss\n        if self.cfg.args.contrast_batch_size:\n            node_idxs = list(range(self.cfg.agrs.n_nodes))\n            # random.shuffle(node_idxs)\n            batches = split_batch(node_idxs, self.cfg.args.contrast_batch_size)\n            loss = 0\n            for batch in batches:\n                weight = len(batch) / self.cfg.agrs.n_nodes\n                loss += model.calc_loss(z1[batch], z2[batch]) * weight\n        else:\n            loss = model.calc_loss(z1, z2)\n\n        return loss, learned_adj\n\n    def preprocess_data(self, features, adjacency):\n\n        anchor_adj_raw = torch.from_numpy(adjacency)\n\n        anchor_adj = normalize(anchor_adj_raw, 'sym')\n\n        features = torch.FloatTensor(features)\n        anchor_adj = anchor_adj.float()\n\n        return features, anchor_adj\n\n    def training_preprocessing(self, args, processed_data):\n        if args.learner_type == 'mlp':\n            self.graph_learner = MLP_learner(args.nlayers, args.n_features, args.k, args.sim_function, args.i, args.sparse,\n                                        args.activation_learner).to(self.device)\n\n        self.model = GCL(nlayers=args.nlayers, in_dim=args.n_features, hidden_dim=args.hidden_dim,\n                    emb_dim=args.rep_dim, proj_dim=args.proj_dim,\n                    dropout=args.dropout, dropout_adj=args.dropedge_rate).to(self.device)\n\n        optimizer_cl = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        optimizer_learner = torch.optim.Adam(self.graph_learner.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        self.optimizers = [optimizer_cl, optimizer_learner]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        features, anchor_adj = processed_data\n\n        loss, Adj = self.loss_gcl(self.model, features, anchor_adj)\n\n        # Structure Bootstrapping\n        if (1 - args.tau) and (args.c == 0 or self.current_epoch % args.c == 0):\n            anchor_adj = anchor_adj * args.tau + Adj.detach() * (1 - args.tau)\n\n        processed_data = (features, anchor_adj)\n\n        return loss, processed_data\n\n    def test(self, processed_data):\n        features, anchor_adj = processed_data\n\n        self.model.eval()\n        self.graph_learner.eval()\n        with torch.no_grad():\n            _, Adj = self.loss_gcl(self.model, features, anchor_adj)\n            _, embedding = self.model(features, Adj)\n            embedding = embedding.squeeze(0)\n\n            kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n            preds = kmeans.fit_predict(embedding).cpu().numpy()\n\n        return preds", ""]}
{"filename": "ugle/models/mvgrl.py", "chunked_list": ["# https://github.com/kavehhassani/mvgrl\nimport torch\nimport torch.nn as nn\nimport ugle\nimport scipy.sparse as sp\nimport numpy as np\nfrom fast_pytorch_kmeans import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom ugle.trainer import ugleTrainer\nfrom ugle.gnn_architecture import GCN, AvgReadout, mvgrl_Discriminator", "from ugle.trainer import ugleTrainer\nfrom ugle.gnn_architecture import GCN, AvgReadout, mvgrl_Discriminator\nfrom ugle.process import sparse_mx_to_torch_sparse_tensor\n\n\nclass Model(nn.Module):\n    def __init__(self, n_in, n_h, act):\n        super(Model, self).__init__()\n        self.gcn1 = GCN(n_in, n_h, act)\n        self.gcn2 = GCN(n_in, n_h, act)\n        self.read = AvgReadout()\n\n        self.sigm = nn.Sigmoid()\n\n        self.disc = mvgrl_Discriminator(n_h)\n\n    def forward(self, seq1, seq2, adj, diff, sparse, msk, samp_bias1, samp_bias2):\n        h_1 = self.gcn1(seq1, adj, sparse)\n        c_1 = self.read(h_1, msk)\n        c_1 = self.sigm(c_1)\n\n        h_2 = self.gcn2(seq1, diff, sparse)\n        c_2 = self.read(h_2, msk)\n        c_2 = self.sigm(c_2)\n\n        h_3 = self.gcn1(seq2, adj, sparse)\n        h_4 = self.gcn2(seq2, diff, sparse)\n\n        ret = self.disc(c_1, c_2, h_1, h_2, h_3, h_4, samp_bias1, samp_bias2)\n\n        return ret, h_1, h_2\n\n    def embed(self, seq, adj, diff, sparse, msk):\n        h_1 = self.gcn1(seq, adj, sparse)\n        c = self.read(h_1, msk)\n\n        h_2 = self.gcn2(seq, diff, sparse)\n        return (h_1 + h_2).detach(), c.detach()", "\n\nclass mvgrl_trainer(ugleTrainer):\n    def preprocess_data(self, features, adjacency):\n        epsilons = [1e-5, 1e-4, 1e-3, 1e-2]\n\n        diff_adj = ugle.process.compute_ppr(adjacency)\n        avg_degree = np.sum(adjacency) / adjacency.shape[0]\n        epsilon = epsilons[np.argmin([abs(avg_degree - np.argwhere(diff_adj >= e).shape[0] / diff_adj.shape[0])\n                                      for e in epsilons])]\n\n        diff_adj[diff_adj < epsilon] = 0.0\n        scaler = MinMaxScaler()\n        scaler.fit(diff_adj)\n        diff_adj = scaler.transform(diff_adj)\n\n        features = ugle.process.preprocess_features(features)\n        adjacency = ugle.process.normalize_adj(adjacency + sp.eye(adjacency.shape[0])).toarray()\n\n        return features, adjacency, diff_adj\n\n    def training_preprocessing(self, args, processed_data):\n        features, adj, diff_adj = processed_data\n\n        if adj.shape[-1] < args.sample_size:\n            args.sample_size = int(np.floor(adj.shape[-1] / 100.0) * 100)\n\n        self.model = Model(args.n_features, args.hid_units, args.activation).to(self.device)\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        self.optimizers = [optimizer]\n        self.loss_function = nn.BCEWithLogitsLoss()\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n\n        features, adj, diff_adj = processed_data\n\n        if adj.shape[-1] < self.cfg.args.sample_size:\n            self.cfg.args.sample_size = int(np.floor(adj.shape[-1] / 100.0) * 100)\n\n        lbl_1 = torch.ones(self.cfg.args.batch_size, self.cfg.args.sample_size * 2)\n        lbl_2 = torch.zeros(self.cfg.args.batch_size, self.cfg.args.sample_size * 2)\n        lbl = torch.cat((lbl_1, lbl_2), 1).to(self.device)\n        idx = np.random.randint(0, adj.shape[-1] - args.sample_size + 1, args.batch_size)\n\n        ba, bd, bf = [], [], []\n        for i in idx:\n            ba.append(adj[i: i + args.sample_size, i: i + args.sample_size])\n            bd.append(diff_adj[i: i + args.sample_size, i: i + args.sample_size])\n            bf.append(features[i: i + args.sample_size])\n\n        ba = np.asarray(ba).reshape(args.batch_size, args.sample_size, args.sample_size)\n        bd = np.array(bd).reshape(args.batch_size, args.sample_size, args.sample_size)\n        bf = np.array(bf).reshape(args.batch_size, args.sample_size, args.n_features)\n\n        if args.sparse:\n            ba = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(ba))\n            bd = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(bd))\n        else:\n            ba = torch.FloatTensor(ba)\n            bd = torch.FloatTensor(bd)\n\n        bf = torch.FloatTensor(bf)\n        idx = np.random.permutation(args.sample_size)\n        shuf_fts = bf[:, idx, :].to(self.device)\n        ba = ba.to(self.device)\n        bd = bd.to(self.device)\n        bf = bf.to(self.device)\n\n        logits, _, _ = self.model(bf, shuf_fts, ba, bd, args.sparse, None, None, None)\n\n        loss = self.loss_function(logits, lbl)\n\n        return loss, None\n\n    def test(self, processed_data):\n        features, adj, diff_adj = processed_data\n\n        if self.cfg.args.sparse:\n            adj = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(adj))\n            diff_adj = sparse_mx_to_torch_sparse_tensor(sp.coo_matrix(diff_adj))\n\n        features = torch.FloatTensor(features[np.newaxis]).to(self.device)\n        adj = torch.FloatTensor(adj[np.newaxis]).to(self.device)\n        diff_adj = torch.FloatTensor(diff_adj[np.newaxis]).to(self.device)\n\n        embeds, _ = self.model.embed(features, adj, diff_adj, self.cfg.args.sparse, None)\n        embeds = embeds.squeeze(0)\n\n        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n        preds = kmeans.fit_predict(embeds).cpu().numpy()\n\n        return preds", ""]}
{"filename": "ugle/models/__init__.py", "chunked_list": [""]}
{"filename": "ugle/models/daegc.py", "chunked_list": ["# code insipred from https://github.com/Tiger101010/DAEGC\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom torch.optim import Adam\nfrom fast_pytorch_kmeans import KMeans\nimport scipy.sparse as sp\nimport ugle", "import scipy.sparse as sp\nimport ugle\nfrom ugle.logger import log\nfrom ugle.trainer import ugleTrainer\nfrom ugle.gnn_architecture import GAT\n\nclass DAEGC(nn.Module):\n    def __init__(self, num_features, hidden_size, embedding_size, alpha, num_clusters, v=1):\n        super(DAEGC, self).__init__()\n        self.num_clusters = num_clusters\n        self.v = v\n\n        # get pretrain model\n        self.gat = GAT(num_features, hidden_size, embedding_size, alpha)\n        #self.gat.load_state_dict(torch.load(args.pretrain_path, map_location='cpu'))\n\n        # cluster layer\n        self.cluster_layer = Parameter(torch.Tensor(num_clusters, embedding_size))\n        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n\n    def forward(self, x, adj, M):\n        A_pred, z = self.gat(x, adj, M)\n        q = self.get_Q(z)\n\n        return A_pred, z, q\n\n    def get_Q(self, z):\n        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n        q = q.pow((self.v + 1.0) / 2.0)\n        q = (q.t() / torch.sum(q, 1)).t()\n        return q", "\n\ndef target_distribution(q):\n    weight = q ** 2 / q.sum(0)\n    return (weight.t() / weight.sum(1)).t()\n\ndef get_M(adj):\n    adj_numpy = adj.todense()\n    # t_order\n    t=2\n    row_l1_norms = np.linalg.norm(adj_numpy, ord=1, axis=1)\n    tran_prob = adj_numpy / row_l1_norms[:, None]\n    M_numpy = sum([np.linalg.matrix_power(tran_prob, i) for i in range(1, t + 1)]) / t\n    return torch.Tensor(M_numpy)", "\n\nclass daegc_trainer(ugleTrainer):\n\n    def preprocess_data(self, features, adjacency):\n        adjacency = adjacency + sp.eye(adjacency.shape[0])\n        adj_label = adjacency.copy()\n        adjacency = ugle.process.normalize_adj(adjacency)\n        M = get_M(adjacency)\n        adj = torch.FloatTensor(adjacency.todense())\n        adj_label = torch.FloatTensor(adj_label)\n        features = torch.FloatTensor(features)\n        features[features != 0.] = 1.\n        return features, adj, adj_label, M\n\n    def training_preprocessing(self, args, processed_data):\n        features, adj, adj_label, M = processed_data\n\n        log.debug('creating model')\n        self.model = DAEGC(num_features=args.n_features, hidden_size=args.hidden_size,\n                      embedding_size=args.embedding_size, alpha=args.alpha, num_clusters=args.n_clusters).to(\n            self.device)\n        optimizer = Adam(self.model.gat.parameters(), lr=args.pre_lr, weight_decay=args.weight_decay)\n\n        log.debug('pretraining')\n        best_nmi = 0.\n        for pre_epoch in range(args.pre_epoch):\n            # training pass\n            self.model.train()\n            A_pred, z = self.model.gat(features, adj, M)\n            loss = F.binary_cross_entropy(A_pred.view(-1), adj_label.view(-1))\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        log.debug('kmeans init estimate')\n        with torch.no_grad():\n            _, z = self.model.gat(features, adj, M)\n\n        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n        _ = kmeans.fit_predict(z).cpu().numpy()\n        \n        self.model.cluster_layer.data = kmeans.centroids.clone().detach().to(self.device)\n\n        log.debug('model training')\n        optimizer = Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\n        self.optimizers = [optimizer]\n        return\n\n\n    def training_epoch_iter(self, args, processed_data):\n        if len(processed_data) == 4:\n            features, adj, adj_label, M = processed_data\n        else:\n            features, adj, adj_label, M, Q = processed_data\n\n\n        if self.current_epoch % args.update_interval == 0:\n            # update_interval\n            A_pred, z, Q = self.model(features, adj, M)\n            q = Q.detach().data.cpu().numpy().argmax(1)\n\n        A_pred, z, q = self.model(features, adj, M)\n        p = target_distribution(Q.detach())\n\n        kl_loss = F.kl_div(q.log(), p, reduction='batchmean')\n        re_loss = F.binary_cross_entropy(A_pred.view(-1), adj_label.view(-1))\n\n        loss = (args.kl_loss_const * kl_loss) + re_loss\n\n        processed_data = (features, adj, adj_label, M, Q)\n\n        return loss, processed_data\n\n    def test(self, processed_data):\n\n        with torch.no_grad():\n            features, adj, adj_label, M = processed_data\n            _, z, Q = self.model(features, adj, M)\n            preds = Q.detach().data.cpu().numpy().argmax(1)\n\n        return preds"]}
{"filename": "ugle/models/dgi.py", "chunked_list": ["# inspired by https://github.com/PetarV-/DGI\nimport numpy as np\nimport scipy.sparse as sp\nimport torch\nimport torch.nn as nn\nfrom fast_pytorch_kmeans import KMeans\nimport ugle\nfrom ugle.trainer import ugleTrainer\nfrom ugle.gnn_architecture import GCN, AvgReadout, Discriminator\n\nclass DGI(nn.Module):\n    def __init__(self, n_in, n_h, activation):\n        super(DGI, self).__init__()\n        self.gcn = GCN(n_in, n_h, activation)\n        self.read = AvgReadout()\n\n        self.sigm = nn.Sigmoid()\n\n        self.disc = Discriminator(n_h)\n\n    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2):\n        h_1 = self.gcn(seq1, adj, sparse=True)\n\n        c = self.read(h_1, msk)\n        c = self.sigm(c)\n\n        h_2 = self.gcn(seq2, adj, sparse=True)\n\n        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n\n        return ret\n\n    # Detach the return variables\n    def embed(self, seq, adj, msk):\n        h_1 = self.gcn(seq, adj, sparse=True)\n        c = self.read(h_1, msk)\n\n        return h_1.detach(), c.detach()", "from ugle.gnn_architecture import GCN, AvgReadout, Discriminator\n\nclass DGI(nn.Module):\n    def __init__(self, n_in, n_h, activation):\n        super(DGI, self).__init__()\n        self.gcn = GCN(n_in, n_h, activation)\n        self.read = AvgReadout()\n\n        self.sigm = nn.Sigmoid()\n\n        self.disc = Discriminator(n_h)\n\n    def forward(self, seq1, seq2, adj, msk, samp_bias1, samp_bias2):\n        h_1 = self.gcn(seq1, adj, sparse=True)\n\n        c = self.read(h_1, msk)\n        c = self.sigm(c)\n\n        h_2 = self.gcn(seq2, adj, sparse=True)\n\n        ret = self.disc(c, h_1, h_2, samp_bias1, samp_bias2)\n\n        return ret\n\n    # Detach the return variables\n    def embed(self, seq, adj, msk):\n        h_1 = self.gcn(seq, adj, sparse=True)\n        c = self.read(h_1, msk)\n\n        return h_1.detach(), c.detach()", "\n\nclass dgi_trainer(ugleTrainer):\n\n    def preprocess_data(self, features, adjacency):\n        adjacency = adjacency + sp.eye(adjacency.shape[0])\n        adjacency = ugle.process.normalize_adj(adjacency)\n        adj = ugle.process.sparse_mx_to_torch_sparse_tensor(adjacency)\n        features = ugle.process.preprocess_features(features)\n        features = torch.FloatTensor(features[np.newaxis])\n\n        return adj, features\n\n    def training_preprocessing(self, args, processed_data):\n        self.model = DGI(args.n_features, args.hid_units, args.nonlinearity).to(self.device)\n        optimiser = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n        self.loss_function = nn.BCEWithLogitsLoss()\n        self.optimizers = [optimiser]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        adj, features = processed_data\n        idx = np.random.permutation(args.n_nodes)\n        shuf_fts = features[:, idx, :]\n        lbl_1 = torch.ones(args.batch_size, args.n_nodes)\n        lbl_2 = torch.zeros(args.batch_size, args.n_nodes)\n        lbl = torch.cat((lbl_1, lbl_2), 1)\n        shuf_fts = shuf_fts.to(self.device)\n        lbl = lbl.to(self.device)\n        logits = self.model(features, shuf_fts, adj, None, None, None)\n        loss = self.loss_function(logits, lbl)\n\n        return loss, None\n\n    def test(self, processed_data):\n        adj, features = processed_data\n        with torch.no_grad():\n            embeds, _ = self.model.embed(features, adj, None)\n            embeds = embeds.squeeze(0)\n\n        kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n        preds = kmeans.fit_predict(embeds).cpu().numpy()\n\n        return preds", ""]}
{"filename": "ugle/models/grace.py", "chunked_list": ["# https://github.com/CRIPAC-DIG/GRACE\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport ugle\nimport scipy.sparse as sp\nimport numpy as np\nfrom fast_pytorch_kmeans import KMeans\nfrom torch_geometric.utils import dropout_adj\nfrom torch_geometric.nn import GCNConv", "from torch_geometric.utils import dropout_adj\nfrom torch_geometric.nn import GCNConv\nfrom ugle.trainer import ugleTrainer\n\n\nclass Encoder(torch.nn.Module):\n    def __init__(self, in_channels: int, out_channels: int, activation,\n                 base_model, k: int = 2):\n        super(Encoder, self).__init__()\n        self.base_model = base_model\n\n        assert k >= 2\n        self.k = k\n        self.conv = [base_model(in_channels, 2 * out_channels)]\n        for _ in range(1, k-1):\n            self.conv.append(base_model(2 * out_channels, 2 * out_channels))\n        self.conv.append(base_model(2 * out_channels, out_channels))\n        self.conv = nn.ModuleList(self.conv)\n\n        self.activation = activation\n\n    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n        for i in range(self.k):\n            x = self.activation(self.conv[i](x, edge_index))\n        return x", "\n\nclass Model(torch.nn.Module):\n    def __init__(self, encoder: Encoder, num_hidden: int, num_proj_hidden: int,\n                 tau: float = 0.5):\n        super(Model, self).__init__()\n        self.encoder: Encoder = encoder\n        self.tau: float = tau\n\n        self.fc1 = torch.nn.Linear(num_hidden, num_proj_hidden)\n        self.fc2 = torch.nn.Linear(num_proj_hidden, num_hidden)\n\n    def forward(self, x: torch.Tensor,\n                edge_index: torch.Tensor) -> torch.Tensor:\n        return self.encoder(x, edge_index)\n\n    def projection(self, z: torch.Tensor) -> torch.Tensor:\n        z = F.elu(self.fc1(z))\n        return self.fc2(z)\n\n    def sim(self, z1: torch.Tensor, z2: torch.Tensor):\n        z1 = F.normalize(z1)\n        z2 = F.normalize(z2)\n        return torch.mm(z1, z2.t())\n\n    def semi_loss(self, z1: torch.Tensor, z2: torch.Tensor):\n        f = lambda x: torch.exp(x / self.tau)\n        refl_sim = f(self.sim(z1, z1))\n        between_sim = f(self.sim(z1, z2))\n\n        return -torch.log(\n            between_sim.diag()\n            / (refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()))\n\n    def batched_semi_loss(self, z1: torch.Tensor, z2: torch.Tensor,\n                          batch_size: int):\n        # Space complexity: O(BN) (semi_loss: O(N^2))\n        device = z1.device\n        num_nodes = z1.size(0)\n        num_batches = (num_nodes - 1) // batch_size + 1\n        f = lambda x: torch.exp(x / self.tau)\n        indices = torch.arange(0, num_nodes).to(device)\n        losses = []\n\n        for i in range(num_batches):\n            mask = indices[i * batch_size:(i + 1) * batch_size]\n            refl_sim = f(self.sim(z1[mask], z1))  # [B, N]\n            between_sim = f(self.sim(z1[mask], z2))  # [B, N]\n\n            losses.append(-torch.log(\n                between_sim[:, i * batch_size:(i + 1) * batch_size].diag()\n                / (refl_sim.sum(1) + between_sim.sum(1)\n                   - refl_sim[:, i * batch_size:(i + 1) * batch_size].diag())))\n\n        return torch.cat(losses)\n\n    def loss(self, z1: torch.Tensor, z2: torch.Tensor,\n             mean: bool = True, batch_size: int = 0):\n        h1 = self.projection(z1)\n        h2 = self.projection(z2)\n\n        if batch_size == 0:\n            l1 = self.semi_loss(h1, h2)\n            l2 = self.semi_loss(h2, h1)\n        else:\n            l1 = self.batched_semi_loss(h1, h2, batch_size)\n            l2 = self.batched_semi_loss(h2, h1, batch_size)\n\n        ret = (l1 + l2) * 0.5\n        ret = ret.mean() if mean else ret.sum()\n\n        return ret", "\n\ndef drop_feature(x, drop_prob):\n    drop_mask = torch.empty(\n        (x.size(1), ),\n        dtype=torch.float32,\n        device=x.device).uniform_(0, 1) < drop_prob\n    x = x.clone()\n    x[:, drop_mask] = 0\n\n    return x", "\nclass grace_trainer(ugleTrainer):\n    def preprocess_data(self, features, adjacency):\n        adj_label = sp.coo_matrix(adjacency)\n        adj_label = adj_label.todok()\n\n        outwards = [i[0] for i in adj_label.keys()]\n        inwards = [i[1] for i in adj_label.keys()]\n\n        adj = torch.from_numpy(np.array([outwards, inwards], dtype=np.int))\n\n        data = torch.FloatTensor(features)\n\n        return data, adj\n\n    def training_preprocessing(self, args, processed_data):\n\n        activation = ({'relu': F.relu, 'prelu': nn.PReLU()})[args.activation]\n        base_model = ({'GCNConv': GCNConv})[args.base_model]\n\n        encoder = Encoder(args.n_features, args.num_hidden, activation,\n                          base_model=base_model, k=args.num_layers).to(self.device)\n        self.model = Model(encoder, args.num_hidden, args.num_proj_hidden, args.tau).to(self.device)\n        optimizer = torch.optim.Adam(\n            self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\n        self.optimizers = [optimizer]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        data, adj = processed_data\n\n        adj_1 = dropout_adj(adj, p=args.drop_edge_rate_1, force_undirected=True)[0]\n        adj_2 = dropout_adj(adj, p=args.drop_edge_rate_2, force_undirected=True)[0]\n\n        x_1 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_1)\n        x_2 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_2)\n\n        z1 = self.model(x_1, adj_1)\n        z2 = self.model(x_2, adj_2)\n\n        loss = self.model.loss(z1, z2, batch_size=0)\n\n        return loss, None\n\n    def test(self, processed_data):\n        data, adj = processed_data\n        self.model.eval()\n        z = self.model(data, adj)\n        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n        preds = kmeans.fit_predict(z).cpu().numpy()\n        return preds", "\n\n\n"]}
{"filename": "ugle/models/bgrl.py", "chunked_list": ["# https://github.com/Namkyeong/BGRL_Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom fast_pytorch_kmeans import KMeans\nimport scipy.sparse as sp\nfrom torch_geometric.nn import GCNConv\nimport copy\nfrom torch_geometric.utils import dropout_adj", "import copy\nfrom torch_geometric.utils import dropout_adj\nimport ugle\nfrom ugle.trainer import ugleTrainer\n\nclass EMA:\n    def __init__(self, beta, epochs):\n        super().__init__()\n        self.beta = beta\n        self.step = 0\n        self.total_steps = epochs\n\n    def update_average(self, old, new):\n        if old is None:\n            return new\n        beta = 1 - (1 - self.beta) * (np.cos(np.pi * self.step / self.total_steps) + 1) / 2.0\n        self.step += 1\n        return old * beta + (1 - beta) * new", "\n\ndef loss_fn(x, y):\n    x = F.normalize(x, dim=-1, p=2)\n    y = F.normalize(y, dim=-1, p=2)\n    return 2 - 2 * (x * y).sum(dim=-1)\n\n\ndef update_moving_average(ema_updater, ma_model, current_model):\n    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n        old_weight, up_weight = ma_params.data, current_params.data\n        ma_params.data = ema_updater.update_average(old_weight, up_weight)", "def update_moving_average(ema_updater, ma_model, current_model):\n    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n        old_weight, up_weight = ma_params.data, current_params.data\n        ma_params.data = ema_updater.update_average(old_weight, up_weight)\n\n\ndef set_requires_grad(model, val):\n    for p in model.parameters():\n        p.requires_grad = val\n", "\n\nclass Encoder(nn.Module):\n\n    def __init__(self, layer_config, dropout=None, project=False, **kwargs):\n        super().__init__()\n\n        self.conv1 = GCNConv(layer_config[0], layer_config[1])\n        self.bn1 = nn.BatchNorm1d(layer_config[1], momentum=0.01)\n        self.prelu1 = nn.PReLU()\n        self.conv2 = GCNConv(layer_config[1], layer_config[2])\n        self.bn2 = nn.BatchNorm1d(layer_config[2], momentum=0.01)\n        self.prelu2 = nn.PReLU()\n\n    def forward(self, x, edge_index, edge_weight=None):\n        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n        x = self.prelu1(self.bn1(x))\n        x = self.conv2(x, edge_index, edge_weight=edge_weight)\n        x = self.prelu2(self.bn2(x))\n\n        return x", "\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform_(m.weight)\n        m.bias.data.fill_(0.01)\n\n\nclass BGRL(nn.Module):\n\n    def __init__(self, layer_config, pred_hid, dropout=0.0, moving_average_decay=0.99, epochs=1000, **kwargs):\n        super().__init__()\n        self.student_encoder = Encoder(layer_config=layer_config, dropout=dropout, **kwargs)\n        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n        set_requires_grad(self.teacher_encoder, False)\n        self.teacher_ema_updater = EMA(moving_average_decay, epochs)\n        rep_dim = layer_config[-1]\n        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, pred_hid), nn.PReLU(), nn.Linear(pred_hid, rep_dim))\n        self.student_predictor.apply(init_weights)\n\n    def reset_moving_average(self):\n        del self.teacher_encoder\n        self.teacher_encoder = None\n\n    def update_moving_average(self):\n        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n\n    def forward(self, x1, x2, edge_index_v1, edge_index_v2, edge_weight_v1=None, edge_weight_v2=None):\n        v1_student = self.student_encoder(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n        v2_student = self.student_encoder(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n\n        v1_pred = self.student_predictor(v1_student)\n        v2_pred = self.student_predictor(v2_student)\n\n        with torch.no_grad():\n            v1_teacher = self.teacher_encoder(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n            v2_teacher = self.teacher_encoder(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n\n        loss1 = loss_fn(v1_pred, v2_teacher.detach())\n        loss2 = loss_fn(v2_pred, v1_teacher.detach())\n\n        loss = loss1 + loss2\n        return v1_student, v2_student, loss.mean()", "class BGRL(nn.Module):\n\n    def __init__(self, layer_config, pred_hid, dropout=0.0, moving_average_decay=0.99, epochs=1000, **kwargs):\n        super().__init__()\n        self.student_encoder = Encoder(layer_config=layer_config, dropout=dropout, **kwargs)\n        self.teacher_encoder = copy.deepcopy(self.student_encoder)\n        set_requires_grad(self.teacher_encoder, False)\n        self.teacher_ema_updater = EMA(moving_average_decay, epochs)\n        rep_dim = layer_config[-1]\n        self.student_predictor = nn.Sequential(nn.Linear(rep_dim, pred_hid), nn.PReLU(), nn.Linear(pred_hid, rep_dim))\n        self.student_predictor.apply(init_weights)\n\n    def reset_moving_average(self):\n        del self.teacher_encoder\n        self.teacher_encoder = None\n\n    def update_moving_average(self):\n        assert self.teacher_encoder is not None, 'teacher encoder has not been created yet'\n        update_moving_average(self.teacher_ema_updater, self.teacher_encoder, self.student_encoder)\n\n    def forward(self, x1, x2, edge_index_v1, edge_index_v2, edge_weight_v1=None, edge_weight_v2=None):\n        v1_student = self.student_encoder(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n        v2_student = self.student_encoder(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n\n        v1_pred = self.student_predictor(v1_student)\n        v2_pred = self.student_predictor(v2_student)\n\n        with torch.no_grad():\n            v1_teacher = self.teacher_encoder(x=x1, edge_index=edge_index_v1, edge_weight=edge_weight_v1)\n            v2_teacher = self.teacher_encoder(x=x2, edge_index=edge_index_v2, edge_weight=edge_weight_v2)\n\n        loss1 = loss_fn(v1_pred, v2_teacher.detach())\n        loss2 = loss_fn(v2_pred, v1_teacher.detach())\n\n        loss = loss1 + loss2\n        return v1_student, v2_student, loss.mean()", "\n\nclass bgrl_trainer(ugleTrainer):\n    def preprocess_data(self, features, adjacency):\n        adj_label = sp.coo_matrix(adjacency)\n        adj_label = adj_label.todok()\n\n        outwards = [i[0] for i in adj_label.keys()]\n        inwards = [i[1] for i in adj_label.keys()]\n\n        adj = torch.from_numpy(np.array([outwards, inwards], dtype=int))\n\n        data = torch.FloatTensor(features)\n\n        return data, adj\n\n    def training_preprocessing(self, args, processed_data):\n\n        layers = [args.n_features, args.layer1, args.layer2]\n        self.model = BGRL(layer_config=layers, pred_hid=args.pred_hidden,\n                     dropout=args.dropout, epochs=args.max_epoch).to(self.device)\n\n        optimizer = torch.optim.Adam(params=self.model.parameters(), lr=args.learning_rate, weight_decay=1e-5)\n        if args.max_epoch == 1000:\n            args.max_epoch += 1\n        scheduler = lambda epoch: epoch / 1000 if epoch < 1000 else (1 + np.cos((epoch - 1000) * np.pi / (args.max_epoch - 1000))) * 0.5\n\n        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n\n        self.scheduler = scheduler\n        self.optimizers = [optimizer]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n\n        data, adj = processed_data\n        self.model.update_moving_average()\n        adj_1 = dropout_adj(adj, p=args.drop_edge_rate_1, force_undirected=True)[0]\n        adj_2 = dropout_adj(adj, p=args.drop_edge_rate_2, force_undirected=True)[0]\n\n        x_1 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_1)\n        x_2 = ugle.datasets.aug_drop_features(data, drop_percent=args.drop_feature_rate_2)\n\n        v1_output, v2_output, loss = self.model(\n            x1=x_1, x2=x_2, edge_index_v1=adj_1, edge_index_v2=adj_2,\n            edge_weight_v1=None, edge_weight_v2=None)\n\n        return loss, None\n\n    def test(self, processed_data):\n        data, adj = processed_data\n        self.model.eval()\n        v1_output, v2_output, _ = self.model(\n                x1=data, x2=data, edge_index_v1=adj, edge_index_v2=adj,\n            edge_weight_v1=None,\n            edge_weight_v2=None)\n\n        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n        preds = kmeans.fit_predict(v1_output).cpu().numpy()\n\n\n        return preds"]}
{"filename": "ugle/models/vgaer.py", "chunked_list": ["# https://github.com/qcydm/VGAER/tree/main/VGAER_codes\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.parameter import Parameter\nfrom fast_pytorch_kmeans import KMeans\nimport math\nfrom ugle.trainer import ugleTrainer\nfrom copy import deepcopy\n\nclass GraphConvolution(nn.Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True, act=\"tanh\"):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if act == \"tanh\":\n            self.act = nn.Tanh()\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, adj, input):\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n\n        if self.bias is not None:\n            output += self.bias\n\n        return self.act(output)\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'", "from copy import deepcopy\n\nclass GraphConvolution(nn.Module):\n    \"\"\"\n    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n    \"\"\"\n\n    def __init__(self, in_features, out_features, bias=True, act=\"tanh\"):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n        if act == \"tanh\":\n            self.act = nn.Tanh()\n        if bias:\n            self.bias = Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. / math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, adj, input):\n        support = torch.mm(input, self.weight)\n        output = torch.spmm(adj, support)\n\n        if self.bias is not None:\n            output += self.bias\n\n        return self.act(output)\n\n    def __repr__(self):\n        return self.__class__.__name__ + ' (' \\\n               + str(self.in_features) + ' -> ' \\\n               + str(self.out_features) + ')'", "\n\nclass InnerProductDecoder(nn.Module):\n    \"\"\"Decoder for using inner product for prediction.\"\"\"\n    def __init__(self, dropout, act=torch.sigmoid):\n        super(InnerProductDecoder, self).__init__()\n        self.dropout = dropout\n        self.act = act\n\n    def forward(self, z):\n        z = F.dropout(z, self.dropout, training=self.training)\n        B_hat = z @ z.t()\n        B_hat = F.sigmoid(B_hat)\n        return B_hat", "\n\nclass VGAERModel(nn.Module):\n    def __init__(self, in_dim, hidden1_dim, hidden2_dim, device):\n        super(VGAERModel, self).__init__()\n        self.in_dim = in_dim\n        self.hidden1_dim = hidden1_dim\n        self.hidden2_dim = hidden2_dim\n\n        layers = [GraphConvolution(self.in_dim, self.hidden1_dim, act=\"tanh\"),\n                  GraphConvolution(self.hidden1_dim, self.hidden2_dim, act=\"tanh\"),\n                  GraphConvolution(self.hidden1_dim, self.hidden2_dim, act=\"tanh\")]\n        self.layers = nn.ModuleList(layers)\n        self.device = device\n\n    def encoder(self, a_hat, features):\n\n        h = self.layers[0](a_hat, features)\n        self.mean = self.layers[1](a_hat, h)\n        self.log_std = self.layers[2](a_hat, h)\n        gaussian_noise = torch.randn(features.size(0), self.hidden2_dim).to(self.device)\n        sampled_z = self.mean + gaussian_noise * torch.exp(self.log_std).to(self.device)\n        return sampled_z\n\n    def decoder(self, z):\n        adj_rec = torch.sigmoid(torch.matmul(z, z.t()))\n        return adj_rec\n\n    def forward(self, a_hat, features):\n        z = self.encoder(a_hat, features)\n        adj_rec = self.decoder(z)\n        return adj_rec, z", "\n\ndef compute_loss_para(adj):\n    try: \n        pos_weight = ((adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum())\n        norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n        weight_mask = adj.view(-1) == 1\n        weight_tensor = torch.ones(weight_mask.size(0))\n        weight_tensor[weight_mask] = pos_weight\n    except:\n        weight_tensor = torch.ones_like(adj.view(-1))\n        norm = 1.\n    return weight_tensor, norm", "\n\nclass vgaer_trainer(ugleTrainer):\n    def preprocess_data(self, features, adjacency):\n        A = torch.FloatTensor(adjacency)\n        A[A != 0] = 1\n        A_orig_ten = deepcopy(A)\n\n        # compute B matrix\n        K = 1 / (A.sum().item()) * (A.sum(dim=1).reshape(A.shape[0], 1) @ A.sum(dim=1).reshape(1, A.shape[0]))\n        feats = A - K\n\n        # compute A_hat matrix\n        A = A + torch.eye(A.shape[0])\n        D = torch.diag(torch.pow(A.sum(dim=1), -0.5))  # D = D^-1/2\n        A_hat = D @ A @ D\n\n        weight_tensor, norm = compute_loss_para(A)\n        weight_tensor = weight_tensor\n        A_hat = A_hat\n        feats = feats\n\n        return A_orig_ten, A_hat, feats, weight_tensor, norm\n\n\n    def training_preprocessing(self, args, processed_data):\n\n        self.model = VGAERModel(args.n_nodes, args.hidden1, args.hidden2, self.device).to(self.device)\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n\n        self.optimizers = [optimizer]\n\n        return\n\n    def training_epoch_iter(self, args, processed_data):\n        A_orig_ten, A_hat, feats, weight_tensor, norm = processed_data\n        recovered = self.model.forward(A_hat, feats)\n        logits = recovered[0]\n        hidemb = recovered[1]\n\n        logits = logits.clamp(min=0., max=1.)\n\n        loss = norm * F.binary_cross_entropy(logits.view(-1), A_orig_ten.view(-1), weight=weight_tensor)\n        kl_divergence = 0.5 / logits.size(0) * (\n                1 + 2 * self.model.log_std - self.model.mean ** 2 - torch.exp(self.model.log_std) ** 2).sum(\n            1).mean()\n        loss -= kl_divergence\n\n        return loss, None\n\n    def test(self, processed_data):\n        A_orig_ten, A_hat, feats, weight_tensor, norm = processed_data\n\n        self.model.eval()\n        recovered = self.model.forward(A_hat, feats)\n        emb = recovered[1]\n        emb = emb.float().clamp(torch.finfo(torch.float32).min, torch.finfo(torch.float32).max)\n\n        kmeans = kmeans = KMeans(n_clusters=self.cfg.args.n_clusters)\n        preds = kmeans.fit_predict(emb).cpu().numpy()\n      \n        return preds"]}
