{"filename": "setup.py", "chunked_list": ["import setuptools\n\nRELEASE_VERSION = 'v0.2.0'\n\nwith open('requirements.txt', 'r') as fh:\n    requirements = fh.read().splitlines()\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n", "\nsetuptools.setup(\n    name=\"autocontrastive-gen\",\n    version=f\"{RELEASE_VERSION}\".replace('v', ''),\n    author=\"IBM Research\",\n    author_email=\"ariel.gera1@ibm.com\",\n    url=\"https://github.com/IBM/auto-contrastive-generation\",\n    description=\"Auto-Contrastive Text Generation\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",", "    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    install_requires=requirements,\n    packages=setuptools.find_packages(),\n    license='Apache License 2.0',\n    python_requires='>=3.9',\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",", "        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: OS Independent\",\n        \"Topic :: Scientific/Engineering\",\n    ],\n    package_data={\"\": [\"LICENSE\", \"requirements.txt\"]},\n    include_package_data=True\n)\n"]}
{"filename": "autocontrastive_gen/run_inference.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport ast\nimport json\nimport os\nimport random", "import os\nimport random\n\nfrom argparse import ArgumentParser\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nfrom autocontrastive_gen.data_processing.dataset_catalog import DatasetsCatalog\nfrom autocontrastive_gen.evaluation.auto_metrics import calc_metrics", "from autocontrastive_gen.data_processing.dataset_catalog import DatasetsCatalog\nfrom autocontrastive_gen.evaluation.auto_metrics import calc_metrics\nfrom autocontrastive_gen.modeling.configuration import MultiExitConfiguration\nfrom autocontrastive_gen.utils import get_model, get_tokenizer, device\n\n\ndef prepare_texts(dataset, num_samples, dataset_split):\n    dataset = getattr(DatasetsCatalog, dataset)\n    dataset_dict = dataset.load()\n\n    texts = dataset_dict[dataset_split]['source_text']\n\n    is_seq2seq_task = dataset.is_seq2seq_task()\n    if is_seq2seq_task:\n        targets = dataset_dict[dataset_split]['target_text']\n    else:\n        targets = ['dummy_target'] * len(texts)\n\n    if num_samples is not None and num_samples < len(texts):\n        sample_ids = random.Random(0).sample(list(range(len(texts))), k=num_samples)\n        texts = [texts[i] for i in sample_ids]\n        targets = [targets[i] for i in sample_ids]\n\n    return is_seq2seq_task, texts, targets", "\n\ndef run(args):\n    is_seq2seq_task, texts, targets = prepare_texts(args.dataset, args.number_of_samples, args.dataset_split)\n\n    lm_config = MultiExitConfiguration(\n        use_original_head=args.use_original_head,\n        output_layer_index=args.output_layer_index,\n        contrast_layer_indices=args.contrast_layer_indices,\n    )\n\n    tokenizer = get_tokenizer(args.model_name_or_path, max_seq_length=args.max_seq_length)\n    model = get_model(args.model_name_or_path, lm_config, args.vanilla_model)\n    modus = 'top_k' if args.use_top_k else ('top_p' if args.use_top_p else f'beam_{args.beam_search}')\n    desc = f'{args.model_name_or_path.replace(\"/\", \"_\")}_{lm_config.get_description()}_{modus}_{args.additional_desc}'\n\n    all_results = []\n    print(f'generating texts from {len(texts)} prompts')\n    for i, (text, target) in tqdm(enumerate(zip(texts, targets)), total=len(texts)):\n        if not is_seq2seq_task:\n            prompt_text = ' '.join(text.split()[:args.num_prompt_tokens])\n        else:\n            prompt_text = text\n        prompt = tokenizer(prompt_text, return_tensors='pt', truncation=True).input_ids.to(device)\n        generated_ids = model.generate(prompt,\n                                       pad_token_id=tokenizer.pad_token_id, bos_token_id=tokenizer.bos_token_id,\n                                       eos_token_id=tokenizer.eos_token_id,\n                                       max_new_tokens=args.max_new_tokens,\n                                       num_beams=args.beam_search,\n                                       do_sample=args.use_top_k or args.use_top_p,\n                                       top_p=0.95 if args.use_top_p else 1.0,\n                                       top_k=50 if args.use_top_k else 0.0,\n                                       output_hidden_states=True)\n\n        if not model.config.is_encoder_decoder:  # keep only the newly-generated tokens\n            generated_ids = generated_ids[:, prompt.shape[1]:]\n\n        generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n        example_metrics = calc_metrics(prompt_text, generated_text)\n        all_results.append({'text_id': i, 'model_description': desc, 'prompt': prompt_text,\n                            'generated_text': generated_text, **example_metrics})\n\n    if args.output_dir:\n        output_dir = os.path.join(args.output_dir, desc)\n    else:\n        output_dir = os.path.join('output', desc)\n    os.makedirs(output_dir, exist_ok=True)\n\n    df = pd.DataFrame(all_results)\n    df.to_csv(os.path.join(output_dir, f'{desc}_{args.dataset}_output.csv'))\n    print(f'csv with generation results written to {os.path.abspath(output_dir)}')\n\n    metrics = {'diversity': df['diversity'].mean(), 'coherence': df['coherence'].mean(),\n               'num_examples': len(df)}\n    print(metrics)\n    with open(os.path.join(output_dir, f'metrics_{args.dataset}{args.additional_desc}.json'), 'w') as f:\n        f.write(json.dumps(metrics))\n\n    return metrics", "\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument('--model_name_or_path', type=str, required=True)\n    parser.add_argument('--dataset', type=str, required=True, choices=DatasetsCatalog.all_datasets())\n    parser.add_argument('--max_seq_length', type=int, default=512)\n\n    parser.add_argument('--use_original_head', type=ast.literal_eval, required=True)\n    parser.add_argument('--output_layer_index', type=int, default=24)\n    parser.add_argument('--contrast_layer_indices', type=str, default=None)\n    parser.add_argument('--vanilla-model', action='store_true', default=False)\n    \n    parser.add_argument('--max_new_tokens', type=int, default=100)\n    parser.add_argument('--num_prompt_tokens', type=int, default=32)                                               \n    parser.add_argument('--beam_search', type=int, default=1)\n    parser.add_argument('--use_top_k', type=ast.literal_eval, default=False)\n    parser.add_argument('--use_top_p', type=ast.literal_eval, default=False)\n\n    parser.add_argument('--dataset_split', type=str, default='validation')\n    parser.add_argument('--number_of_samples', type=int, default=None)\n\n    parser.add_argument('--additional_desc', type=str, default='')\n    parser.add_argument('--output_dir', '-o', type=str, required=False)\n    args = parser.parse_args()\n    run(args)", ""]}
{"filename": "autocontrastive_gen/contrast_calculation.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport torch\n\n\ndef expand_tensor(t, desired_shape):\n    while len(t.shape) < len(desired_shape):\n        t = t.unsqueeze(-1)\n\n    t = t.expand(desired_shape)\n    return t", "\ndef expand_tensor(t, desired_shape):\n    while len(t.shape) < len(desired_shape):\n        t = t.unsqueeze(-1)\n\n    t = t.expand(desired_shape)\n    return t\n\n\ndef calculate_contrasted_logits(upper_layer_logits, lower_layer_logits, minimum_candidates=1, alpha=0.1):\n    from autocontrastive_gen.utils import device\n\n    lm_logits_upper = upper_layer_logits.softmax(dim=-1)\n    lm_logits_lower = lower_layer_logits.softmax(dim=-1)\n\n    # we set a probability threshold relative to the top candidate probability\n    plausible_token_probability_threshold = \\\n        lm_logits_upper.max(-1).values.squeeze(-1) * torch.tensor(alpha)\n\n    # when minimum_candidates=1, min_threshold will simply equal the plausible_token_probability_threshold\n    min_threshold = torch.min(plausible_token_probability_threshold,\n                              lm_logits_upper.sort(descending=True).values.squeeze()[..., minimum_candidates - 1])\n\n    zero = torch.tensor(0.0).to(device)\n    minus_inf = torch.tensor(-torch.inf).to(device)\n\n    # for tokens above the threshold, calculate softmax of contrast score between lm_logits_upper and lm_logits_lower\n    min_threshold_expanded = expand_tensor(min_threshold, lm_logits_upper.shape)\n    contrasted_logits = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                    torch.log(lm_logits_upper) - torch.log(lm_logits_lower),\n                                    lm_logits_upper)\n    softmax_for_included_new = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                           contrasted_logits, minus_inf).softmax(-1)\n    # calculate the total probability mass of tokens above the threshold\n    sum_for_included_orig = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                        lm_logits_upper, zero).sum(-1)\n    # redistribute this probability mass using the contrastive softmax scores\n    sum_for_included_orig_expanded = expand_tensor(sum_for_included_orig, softmax_for_included_new.shape)\n    adjusted_contrasted_logits = softmax_for_included_new * sum_for_included_orig_expanded\n    contrasted_logits = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                    adjusted_contrasted_logits, lm_logits_upper)\n\n    contrasted_logits = torch.log(contrasted_logits)\n    return contrasted_logits", "\ndef calculate_contrasted_logits(upper_layer_logits, lower_layer_logits, minimum_candidates=1, alpha=0.1):\n    from autocontrastive_gen.utils import device\n\n    lm_logits_upper = upper_layer_logits.softmax(dim=-1)\n    lm_logits_lower = lower_layer_logits.softmax(dim=-1)\n\n    # we set a probability threshold relative to the top candidate probability\n    plausible_token_probability_threshold = \\\n        lm_logits_upper.max(-1).values.squeeze(-1) * torch.tensor(alpha)\n\n    # when minimum_candidates=1, min_threshold will simply equal the plausible_token_probability_threshold\n    min_threshold = torch.min(plausible_token_probability_threshold,\n                              lm_logits_upper.sort(descending=True).values.squeeze()[..., minimum_candidates - 1])\n\n    zero = torch.tensor(0.0).to(device)\n    minus_inf = torch.tensor(-torch.inf).to(device)\n\n    # for tokens above the threshold, calculate softmax of contrast score between lm_logits_upper and lm_logits_lower\n    min_threshold_expanded = expand_tensor(min_threshold, lm_logits_upper.shape)\n    contrasted_logits = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                    torch.log(lm_logits_upper) - torch.log(lm_logits_lower),\n                                    lm_logits_upper)\n    softmax_for_included_new = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                           contrasted_logits, minus_inf).softmax(-1)\n    # calculate the total probability mass of tokens above the threshold\n    sum_for_included_orig = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                        lm_logits_upper, zero).sum(-1)\n    # redistribute this probability mass using the contrastive softmax scores\n    sum_for_included_orig_expanded = expand_tensor(sum_for_included_orig, softmax_for_included_new.shape)\n    adjusted_contrasted_logits = softmax_for_included_new * sum_for_included_orig_expanded\n    contrasted_logits = torch.where(lm_logits_upper >= min_threshold_expanded,\n                                    adjusted_contrasted_logits, lm_logits_upper)\n\n    contrasted_logits = torch.log(contrasted_logits)\n    return contrasted_logits", ""]}
{"filename": "autocontrastive_gen/__init__.py", "chunked_list": [""]}
{"filename": "autocontrastive_gen/utils.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoConfig, T5Config\n\nfrom autocontrastive_gen.modeling.auto_model import AutoMultiExitModel", "\nfrom autocontrastive_gen.modeling.auto_model import AutoMultiExitModel\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\ndef get_model(model_name_or_path, multi_exit_config, vanilla_model: bool = False):\n    if vanilla_model:\n        if type(AutoConfig.from_pretrained(model_name_or_path)) == T5Config:\n            model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path).to(device)\n        else:\n            model = AutoModelForCausalLM.from_pretrained(model_name_or_path).to(device)\n    else:\n        model = AutoMultiExitModel.from_pretrained(model_name_or_path, multi_exit_config=multi_exit_config).to(device)\n\n    return model", "\n\ndef get_tokenizer(model_name, max_seq_length=512):\n    tokenizer_params = {'pad_token': '<|endoftext|>'} if 'gpt' in model_name else {}\n    tokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_seq_length, **tokenizer_params)\n    return tokenizer\n"]}
{"filename": "autocontrastive_gen/run_training.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport ast\nfrom argparse import ArgumentParser\n\nfrom autocontrastive_gen.data_processing.dataset_catalog import DatasetsCatalog", "\nfrom autocontrastive_gen.data_processing.dataset_catalog import DatasetsCatalog\nfrom autocontrastive_gen.head_training.head_training_utils import get_head_training_function\nfrom autocontrastive_gen.modeling.configuration import MultiExitConfiguration\nfrom autocontrastive_gen.utils import get_model, get_tokenizer\n\n\nif __name__ == '__main__':\n    parser = ArgumentParser()\n    parser.add_argument('--model_name_or_path', type=str, required=True)\n    parser.add_argument('--output-dir', type=str, required=True)\n    parser.add_argument('--dataset', type=str, required=True, choices=DatasetsCatalog.all_datasets())\n    parser.add_argument('--max_seq_length', type=int, default=512)\n    parser.add_argument('--lm_head_layer_indices', type=ast.literal_eval, default=(24, 22, 18, 12))\n    parser.add_argument('--max-train-instance', type=int, default=None)\n    args = parser.parse_args()\n\n    print(args)\n    \n    dataset = getattr(DatasetsCatalog, args.dataset)\n    dataset_dict = dataset.load()\n\n    tokenizer = get_tokenizer(args.model_name_or_path, max_seq_length=args.max_seq_length)\n\n    # for training, we can mostly use the default generation params of *MultiExitConfiguration* as we only care\n    # about the loss, and not about which specific exit layer or decoding approach is used for generation outputs\n    lm_config = MultiExitConfiguration(\n        freeze_parameters=not dataset.is_seq2seq_task(),\n        lm_head_layer_indices=args.lm_head_layer_indices,\n    )\n\n    model = get_model(args.model_name_or_path, lm_config)\n\n    head_training_function = get_head_training_function(model_config=model.config,\n                                                        is_seq2seq_task=dataset.is_seq2seq_task())\n\n    print(f'***** Training LM heads using {head_training_function} *****')\n    head_training_function(dataset_dict, model, tokenizer, args.max_seq_length,\n                           output_dir=f'{args.output_dir}', debug=False, max_train_instance=args.max_train_instance)", ""]}
{"filename": "autocontrastive_gen/modeling/gpt2_multi_head.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn", "import torch\nfrom torch import nn\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom transformers import GPT2LMHeadModel\nfrom transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n\n\nclass MultiHeadGPT2(GPT2LMHeadModel):\n    def __init__(self, config, use_original_head=False, output_layer_index=24, contrast_layer_indices=None,\n                 contrast_function=None, freeze_parameters=True):\n        super().__init__(config)\n        if not hasattr(config, 'lm_head_layer_indices'):\n            raise Exception(f\"LM exit head indices must be specified when initializing {self.__class__.__name__}\")\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False  # freeze all standard parameters and heads\n\n        # initialize the linear exit heads\n        self.lm_head_name_prefix = 'lm_head_'\n        self.name_to_lm_exit_head = nn.ModuleDict(\n            {self.lm_head_name_prefix + str(layer): self.lm_head if isinstance(layer, str) and layer == 'original'\n                else nn.Linear(config.n_embd, config.vocab_size, bias=False)\n             for layer in config.lm_head_layer_indices})\n\n        # set inference-time generation parameters\n        self.use_original_head = use_original_head\n        self.output_layer_index = output_layer_index\n        self.contrast_layer_indices = contrast_layer_indices\n        self.contrast_function = contrast_function\n        desc = 'original head' if self.use_original_head else (\n            f'output layer {self.output_layer_index}' if not self.contrast_layer_indices else f'contrast between layers {self.contrast_layer_indices}')\n        print(f'********* Using {desc} for generation ***********')\n\n        if freeze_parameters:\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    print(name)\n\n    def forward(\n            self,\n            input_ids: Optional[torch.LongTensor] = None,\n            past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n            attention_mask: Optional[torch.FloatTensor] = None,\n            token_type_ids: Optional[torch.LongTensor] = None,\n            position_ids: Optional[torch.LongTensor] = None,\n            head_mask: Optional[torch.FloatTensor] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            encoder_hidden_states: Optional[torch.Tensor] = None,\n            encoder_attention_mask: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n        \"\"\"\n        Based on the original forward() method of GPT2LMHeadModel (transformers v4.26), with adaptations for\n        multi-head loss and using self.contrast_function to calculate the logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.transformer(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.transformer.first_device)\n            hidden_states = hidden_states.to(self.lm_head.weight.device)\n\n        # index 0 of transformer_outputs.hidden_states are the decoder input embeddings, index 1 is the 1st layer,\n        # index 2 is the 2nd, etc.\n        index_to_layer_lm_head_logits = {}\n        for head_name, layer_lm_head in self.name_to_lm_exit_head.items():\n            if head_name == 'lm_head_original':\n                index_to_layer_lm_head_logits['original'] = layer_lm_head(hidden_states)\n            else:\n                layer_index = int(head_name.split(self.lm_head_name_prefix)[-1])\n                is_top_layer = layer_index == self.config.num_hidden_layers\n                layer_outputs = transformer_outputs.hidden_states[layer_index]\n                if not is_top_layer:  # layer norm for top layer is already applied within the GPT2 code\n                    layer_outputs = self.transformer.ln_f(layer_outputs)\n                layer_lm_logits = layer_lm_head.to(self.transformer.device)(layer_outputs)\n                index_to_layer_lm_head_logits[layer_index] = layer_lm_logits\n\n        # loss calculation for all the lm heads\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = 0\n            for idx, lm_logits in index_to_layer_lm_head_logits.items():\n                # Shift so that tokens < n predict n\n                shift_logits = lm_logits[..., :-1, :].contiguous()\n                shift_labels = labels[..., 1:].contiguous()\n                # Flatten the tokens\n                layer_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n                loss += layer_loss\n\n        # according to the initialization, we decide which head(s) are used for generating outputs at inference time\n        if self.use_original_head:\n            output_logits = self.lm_head(hidden_states)\n        elif not self.contrast_layer_indices:\n            output_logits = index_to_layer_lm_head_logits[self.output_layer_index]\n        else:\n            lm_logits_upper = self.lm_head(hidden_states) if self.contrast_layer_indices[0] == 'original' \\\n                else index_to_layer_lm_head_logits[self.contrast_layer_indices[0]]\n            lm_logits_lower = index_to_layer_lm_head_logits[self.contrast_layer_indices[1]]\n\n            contrasted = self.contrast_function(lm_logits_upper, lm_logits_lower)\n\n            output_logits = contrasted\n\n        if not return_dict:\n            output = (output_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return CausalLMOutputWithCrossAttentions(\n            loss=loss,\n            logits=output_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n            cross_attentions=transformer_outputs.cross_attentions,\n        )", ""]}
{"filename": "autocontrastive_gen/modeling/auto_model.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom transformers import AutoConfig, GPTNeoConfig, GPT2Config, T5Config\n\nfrom autocontrastive_gen.modeling.configuration import MultiExitConfiguration\nfrom autocontrastive_gen.modeling.gpt2_multi_head import MultiHeadGPT2", "from autocontrastive_gen.modeling.configuration import MultiExitConfiguration\nfrom autocontrastive_gen.modeling.gpt2_multi_head import MultiHeadGPT2\nfrom autocontrastive_gen.modeling.gpt_neo_multi_head import MultiHeadGPTNeo\nfrom autocontrastive_gen.modeling.t5_multi_head import MultiHeadT5\n\n\nclass AutoMultiExitModel:\n    @staticmethod\n    def from_pretrained(model_name_or_path, multi_exit_config: MultiExitConfiguration, **extra_kwargs):\n        # Determine the appropriate multi-head model class according to the standard model config it is based on\n        model_config = AutoConfig.from_pretrained(model_name_or_path)\n        if type(model_config) == GPTNeoConfig:\n            model_class = MultiHeadGPTNeo\n        elif type(model_config) == GPT2Config:\n            model_class = MultiHeadGPT2\n        elif type(model_config) == T5Config:\n            model_class = MultiHeadT5\n        else:\n            raise Exception(f'Model {model_name_or_path} of type {type(model_config)} is not supported')\n\n        model_config.output_hidden_states = True\n        if multi_exit_config.lm_head_layer_indices is not None:\n            # If loading a standard single-exit model checkpoint (or, alternatively, if you wish to fine-tune a\n            # specific subset of heads in an existing multi-exit checkpoint), here we set the desired exit layers.\n            # This parameter should be set only at training time; at inference it is loaded from the model config file\n            model_config.lm_head_layer_indices = multi_exit_config.lm_head_layer_indices\n\n        if multi_exit_config.use_original_head is False:\n            # validate multi-exit config is compatible with the model checkpoint\n            multi_exit_config_layers = multi_exit_config.contrast_layer_indices \\\n                if multi_exit_config.contrast_layer_indices is not None else [multi_exit_config.output_layer_index]\n            for layer in multi_exit_config_layers:\n                if layer not in {*model_config.lm_head_layer_indices, 'original'}:\n                    raise Exception(f'Exit layer {layer} in the MultiExitConfiguration does not match the exit heads '\n                                    f'in the pre-trained model checkpoint ({model_config.lm_head_layer_indices})')\n\n        multi_exit_kwargs = multi_exit_config.get_runtime_kwargs()\n        return model_class.from_pretrained(model_name_or_path, config=model_config, **multi_exit_kwargs, **extra_kwargs)", ""]}
{"filename": "autocontrastive_gen/modeling/t5_multi_head.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn", "import torch\nfrom torch import nn\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom transformers import T5ForConditionalGeneration\nfrom transformers.modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n\n\nclass MultiHeadT5(T5ForConditionalGeneration):\n    def __init__(self, config, use_original_head=False, output_layer_index=24, contrast_layer_indices=None,\n                 contrast_function=None, freeze_parameters=True):\n        super().__init__(config)\n        if not hasattr(config, 'lm_head_layer_indices'):\n            raise Exception(f\"LM exit head indices must be specified when initializing {self.__class__.__name__}\")\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False  # freeze all standard parameters and heads\n\n        # initialize the linear exit heads\n        self.lm_head_name_prefix = 'lm_head_'\n        self.name_to_lm_exit_head = nn.ModuleDict(\n            {self.lm_head_name_prefix + str(layer): self.lm_head if isinstance(layer, str) and layer == 'original'\n                else nn.Linear(config.d_model, config.vocab_size, bias=False)\n             for layer in config.lm_head_layer_indices})\n\n        # set inference-time generation parameters\n        self.use_original_head = use_original_head\n        self.output_layer_index = output_layer_index\n        self.contrast_layer_indices = contrast_layer_indices\n        self.contrast_function = contrast_function\n        desc = 'original head' if self.use_original_head else (\n            f'output layer {self.output_layer_index}' if not self.contrast_layer_indices else f'contrast between layers {self.contrast_layer_indices}')\n        print(f'********* Using {desc} for generation ***********')\n\n        if freeze_parameters:\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    print(name)\n\n    def forward(\n            self,\n            input_ids: Optional[torch.LongTensor] = None,\n            attention_mask: Optional[torch.FloatTensor] = None,\n            decoder_input_ids: Optional[torch.LongTensor] = None,\n            decoder_attention_mask: Optional[torch.BoolTensor] = None,\n            head_mask: Optional[torch.FloatTensor] = None,\n            decoder_head_mask: Optional[torch.FloatTensor] = None,\n            cross_attn_head_mask: Optional[torch.Tensor] = None,\n            encoder_outputs: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n            past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n            inputs_embeds: Optional[torch.FloatTensor] = None,\n            decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n            labels: Optional[torch.LongTensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n        \"\"\"\n        Based on the original forward() method of T5ForConditionalGeneration (transformers v4.26), with adaptations for\n        multi-head loss and using self.contrast_function to calculate the logits\n        \"\"\"\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # Encode if needed (training, first prediction pass)\n        if encoder_outputs is None:\n            # Convert encoder inputs in embeddings if needed\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                head_mask=head_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        hidden_states = encoder_outputs[0]\n\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n\n        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n            # get decoder inputs from shifting lm labels to the right\n            decoder_input_ids = self._shift_right(labels)\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.decoder.first_device)\n            hidden_states = hidden_states.to(self.decoder.first_device)\n            if decoder_input_ids is not None:\n                decoder_input_ids = decoder_input_ids.to(self.decoder.first_device)\n            if attention_mask is not None:\n                attention_mask = attention_mask.to(self.decoder.first_device)\n            if decoder_attention_mask is not None:\n                decoder_attention_mask = decoder_attention_mask.to(self.decoder.first_device)\n\n        # Decode\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            inputs_embeds=decoder_inputs_embeds,\n            past_key_values=past_key_values,\n            encoder_hidden_states=hidden_states,\n            encoder_attention_mask=attention_mask,\n            head_mask=decoder_head_mask,\n            cross_attn_head_mask=cross_attn_head_mask,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        sequence_output = decoder_outputs[0]\n\n        # Set device for model parallelism\n        if self.model_parallel:\n            torch.cuda.set_device(self.encoder.first_device)\n            self.lm_head = self.lm_head.to(self.encoder.first_device)\n            sequence_output = sequence_output.to(self.lm_head.weight.device)\n\n        def normalize_and_rescale(hidden_layer, is_top_layer):\n            if not is_top_layer:  # layer norm and dropout for top layer are already applied within the T5 decoder\n                normalized_hidden_layer = self.decoder.final_layer_norm(hidden_layer)\n                hidden_layer = self.decoder.dropout(normalized_hidden_layer)\n            if self.config.tie_word_embeddings:\n                # Rescale output before projecting on vocab\n                hidden_layer = hidden_layer * (self.model_dim ** -0.5)\n            return hidden_layer\n\n        # index 0 of decoder_outputs.hidden_states are the decoder input embeddings, index 1 is the 1st layer,\n        # index 2 is the 2nd, etc.\n        index_to_layer_lm_head_logits = {}\n        for head_name, layer_lm_head in self.name_to_lm_exit_head.items():\n            if head_name == 'lm_head_original':\n                index_to_layer_lm_head_logits['original'] = layer_lm_head(normalize_and_rescale(sequence_output, True))\n            else:\n                layer_index = int(head_name.split(self.lm_head_name_prefix)[-1])\n                is_top_layer = layer_index == self.config.num_decoder_layers\n                layer_decoder_outputs = normalize_and_rescale(decoder_outputs.hidden_states[layer_index], is_top_layer)\n                layer_lm_logits = layer_lm_head.to(self.decoder.device)(layer_decoder_outputs)\n                index_to_layer_lm_head_logits[layer_index] = layer_lm_logits\n\n        # loss calculation for all the lm heads\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss(ignore_index=-100)\n            loss = 0\n            for idx, lm_logits in index_to_layer_lm_head_logits.items():\n                layer_loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n                loss += layer_loss\n\n        # according to the initialization, we decide which head(s) are used for generating outputs at inference time\n        if self.use_original_head:\n            output_logits = self.lm_head(normalize_and_rescale(sequence_output, True))\n        elif not self.contrast_layer_indices:\n            output_logits = index_to_layer_lm_head_logits[self.output_layer_index]\n        else:\n            lm_logits_upper = self.lm_head(normalize_and_rescale(sequence_output, True)) if self.contrast_layer_indices[0] == 'original' \\\n                else index_to_layer_lm_head_logits[self.contrast_layer_indices[0]]\n            lm_logits_lower = index_to_layer_lm_head_logits[self.contrast_layer_indices[1]]\n\n            output_logits = self.contrast_function(lm_logits_upper, lm_logits_lower)\n\n        if not return_dict:\n            output = (output_logits,) + decoder_outputs[1:] + encoder_outputs\n            return ((loss,) + output) if loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=loss,\n            logits=output_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )", ""]}
{"filename": "autocontrastive_gen/modeling/__init__.py", "chunked_list": [""]}
{"filename": "autocontrastive_gen/modeling/gpt_neo_multi_head.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Optional, Tuple, Union\n\nimport torch\nfrom torch import nn", "import torch\nfrom torch import nn\nfrom torch.nn.modules.loss import CrossEntropyLoss\nfrom transformers import GPTNeoForCausalLM\nfrom transformers.modeling_outputs import CausalLMOutputWithCrossAttentions, CausalLMOutputWithPast\n\n\nclass MultiHeadGPTNeo(GPTNeoForCausalLM):\n    def __init__(self, config, use_original_head=False, output_layer_index=24, contrast_layer_indices=None,\n                 contrast_function=None, freeze_parameters=True):\n        super().__init__(config)\n        if not hasattr(config, 'lm_head_layer_indices'):\n            raise Exception(f\"LM exit head indices must be specified when initializing {self.__class__.__name__}\")\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False  # freeze all standard parameters and heads\n\n        # initialize the linear exit heads\n        self.lm_head_name_prefix = 'lm_head_'\n        self.name_to_lm_exit_head = nn.ModuleDict(\n            {self.lm_head_name_prefix + str(layer): self.lm_head if isinstance(layer, str) and layer == 'original'\n                else nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n             for layer in config.lm_head_layer_indices})\n\n        # set inference-time generation parameters\n        self.use_original_head = use_original_head\n        self.output_layer_index = output_layer_index\n        self.contrast_layer_indices = contrast_layer_indices\n        self.contrast_function = contrast_function\n        desc = 'original head' if self.use_original_head else (\n            f'output layer {self.output_layer_index}' if not self.contrast_layer_indices else f'contrast between layers {self.contrast_layer_indices}')\n        print(f'********* Using {desc} for generation ***********')\n\n        if freeze_parameters:\n            for name, param in self.named_parameters():\n                if param.requires_grad:\n                    print(name)\n\n    def forward(\n            self,\n            input_ids: Optional[torch.Tensor] = None,\n            past_key_values: Optional[Tuple[torch.FloatTensor]] = None,\n            attention_mask: Optional[torch.Tensor] = None,\n            token_type_ids: Optional[torch.Tensor] = None,\n            position_ids: Optional[torch.Tensor] = None,\n            head_mask: Optional[torch.Tensor] = None,\n            inputs_embeds: Optional[torch.Tensor] = None,\n            labels: Optional[torch.Tensor] = None,\n            use_cache: Optional[bool] = None,\n            output_attentions: Optional[bool] = None,\n            output_hidden_states: Optional[bool] = None,\n            return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n        \"\"\"\n        Based on the original forward() method of GPTNeoForCausalLM (transformers v4.26), with adaptations for\n        multi-head loss and using self.contrast_function to calculate the logits\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.transformer(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n\n        # index 0 of transformer_outputs.hidden_states are the decoder input embeddings, index 1 is the 1st layer,\n        # index 2 is the 2nd, etc.\n        index_to_layer_lm_head_logits = {}\n        for head_name, layer_lm_head in self.name_to_lm_exit_head.items():\n            if head_name == 'lm_head_original':\n                index_to_layer_lm_head_logits['original'] = layer_lm_head(hidden_states)\n            else:\n                layer_index = int(head_name.split(self.lm_head_name_prefix)[-1])\n                is_top_layer = layer_index == self.config.num_hidden_layers\n                layer_outputs = transformer_outputs.hidden_states[layer_index]\n                if not is_top_layer:  # layer norm for top layer is already applied within the GPT2 code\n                    layer_outputs = self.transformer.ln_f(layer_outputs)\n                layer_lm_logits = layer_lm_head.to(self.transformer.device)(layer_outputs)\n                index_to_layer_lm_head_logits[layer_index] = layer_lm_logits\n\n        # loss calculation for all the lm heads\n        loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            loss = 0\n            for idx, lm_logits in index_to_layer_lm_head_logits.items():\n\n                # Compute loss in fp32 to match with mesh-tf version\n                # https://github.com/EleutherAI/gpt-neo/blob/89ce74164da2fb16179106f54e2269b5da8db333/models/gpt2/gpt2.py#L179\n                lm_logits = lm_logits.to(torch.float32)\n\n                # Shift so that tokens < n predict n\n                shift_logits = lm_logits[..., :-1, :].contiguous()\n                shift_labels = labels[..., 1:].contiguous()\n                # Flatten the tokens\n                layer_loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n                loss += layer_loss\n\n            loss = loss.to(hidden_states.dtype)\n\n        # according to the initialization, we decide which head(s) are used for generating outputs at inference time\n        if self.use_original_head:\n            output_logits = self.lm_head(hidden_states)\n        elif not self.contrast_layer_indices:\n            output_logits = index_to_layer_lm_head_logits[self.output_layer_index]\n        else:\n            lm_logits_upper = self.lm_head(hidden_states) if self.contrast_layer_indices[0] == 'original' \\\n                else index_to_layer_lm_head_logits[self.contrast_layer_indices[0]]\n            lm_logits_lower = index_to_layer_lm_head_logits[self.contrast_layer_indices[1]]\n\n            contrasted = self.contrast_function(lm_logits_upper, lm_logits_lower)\n\n            output_logits = contrasted\n\n        if not return_dict:\n            output = (output_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=output_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )", ""]}
{"filename": "autocontrastive_gen/modeling/configuration.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport ast\nimport dataclasses\nfrom dataclasses import dataclass\nfrom typing import Tuple, Callable, Union", "from dataclasses import dataclass\nfrom typing import Tuple, Callable, Union\n\nfrom autocontrastive_gen.contrast_calculation import calculate_contrasted_logits\n\n\n@dataclass\nclass MultiExitConfiguration:\n    # training\n    lm_head_layer_indices: Tuple[int, ...] = None\n    freeze_parameters: bool = False\n    # inference\n    use_original_head: bool = True\n    output_layer_index: int = None\n    contrast_layer_indices: Tuple[Union[int, str], int] = None\n    contrast_function: Callable = lambda upper, lower: calculate_contrasted_logits(upper, lower, minimum_candidates=1,\n                                                                                   alpha=0.1)\n    \n    def __post_init__(self):\n        if type(self.contrast_layer_indices) == str:\n            self.contrast_layer_indices = tuple(int(idx) if idx != 'original' else 'original'\n                                                for idx in self.contrast_layer_indices.split(';'))\n        if type(self.lm_head_layer_indices) == str:\n            self.lm_head_layer_indices = tuple(int(idx) for idx in self.lm_head_layer_indices.split(';'))\n        if type(self.output_layer_index) == str:\n            self.output_layer_index = int(self.output_layer_index)\n        if type(self.use_original_head) == str:\n            self.use_original_head = ast.literal_eval(self.use_original_head)\n        \n        # validate the generation parameters\n        if self.contrast_layer_indices is not None:\n            if self.use_original_head:\n                raise Exception(f'Contradiction in model configuration: trying to use the original LM head '\n                                f'but also to calculate contrast between layers {self.contrast_layer_indices}')\n\n    def get_description(self):\n        if self.use_original_head:\n            return 'original_head'\n        elif self.contrast_layer_indices:\n            return f'{self.contrast_layer_indices[0]}_vs_{self.contrast_layer_indices[1]}'\n        else:\n            return f'layer_{self.output_layer_index}'\n        \n    def get_runtime_kwargs(self):\n        \"\"\"\n        Returns the extra runtime arguments that are passed to the multi-exit model class, i.e., excluding the LM exit\n        head indices, which are a property of the given model and are stored in the model config\n        \"\"\"\n        kwargs = dataclasses.asdict(self)\n        kwargs.pop('lm_head_layer_indices')\n        return kwargs", ""]}
{"filename": "autocontrastive_gen/evaluation/__init__.py", "chunked_list": [""]}
{"filename": "autocontrastive_gen/evaluation/auto_metrics.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Mapping\n\nfrom simctg.evaluation import measure_repetition_and_diversity\nfrom simcse import SimCSE", "from simctg.evaluation import measure_repetition_and_diversity\nfrom simcse import SimCSE\n\n\nsimcse_model = SimCSE(\"princeton-nlp/sup-simcse-bert-base-uncased\")\n\n\ndef calc_metrics(prompt_text, generated_text) -> Mapping:\n    try:\n        rep_2, rep_3, rep_4, diversity = measure_repetition_and_diversity([generated_text])\n    except ZeroDivisionError:  # text is too short\n        diversity = 0\n\n    coherence = simcse_model.similarity(generated_text, prompt_text)\n    return {'diversity': diversity,\n            'coherence': coherence}", ""]}
{"filename": "autocontrastive_gen/evaluation/lm_eval_harness/run_lm_eval.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport argparse\nimport json\nimport logging\nimport fnmatch", "import logging\nimport fnmatch\n\nfrom lm_eval import tasks, evaluator\n\n\"\"\"\nThis script is taken from the Language Model Evaluation Harness repository (v0.3.0)\n(https://github.com/EleutherAI/lm-evaluation-harness/blob/master/main.py)\n\"\"\"\n", "\"\"\"\n\nlogging.getLogger(\"openai\").setLevel(logging.WARNING)\n\n\nclass MultiChoice:\n    def __init__(self, choices):\n        self.choices = choices\n\n    # Simple wildcard support (linux filename patterns)\n    def __contains__(self, values):\n        for value in values.split(\",\"):\n            if len(fnmatch.filter(self.choices, value)) == 0:\n                return False\n\n        return True\n\n    def __iter__(self):\n        for choice in self.choices:\n            yield choice", "\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", required=True)\n    parser.add_argument(\"--model_args\", default=\"\")\n    parser.add_argument(\"--tasks\", default=None, choices=MultiChoice(tasks.ALL_TASKS))\n    parser.add_argument(\"--provide_description\", action=\"store_true\")\n    parser.add_argument(\"--num_fewshot\", type=int, default=0)\n    parser.add_argument(\"--batch_size\", type=int, default=None)\n    parser.add_argument(\"--device\", type=str, default=None)\n    parser.add_argument(\"--output_path\", default=None)\n    parser.add_argument(\"--limit\", type=int, default=None)\n    parser.add_argument(\"--no_cache\", action=\"store_true\")\n    parser.add_argument(\"--decontamination_ngrams_path\", default=None)\n    parser.add_argument(\"--description_dict_path\", default=None)\n    parser.add_argument(\"--check_integrity\", action=\"store_true\")\n\n    return parser.parse_args()", "\n\n# Returns a list containing all values of the source_list that\n# match at least one of the patterns\ndef pattern_match(patterns, source_list):\n    task_names = set()\n    for pattern in patterns:\n        for matching in fnmatch.filter(source_list, pattern):\n            task_names.add(matching)\n    return list(task_names)", "\n\ndef main():\n    args = parse_args()\n\n    assert not args.provide_description  # not implemented\n\n    if args.limit:\n        print(\n            \"WARNING: --limit SHOULD ONLY BE USED FOR TESTING. REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.\"\n        )\n\n    if args.tasks is None:\n        task_names = tasks.ALL_TASKS\n    else:\n        task_names = pattern_match(args.tasks.split(\",\"), tasks.ALL_TASKS)\n\n    print(f\"Selected Tasks: {task_names}\")\n\n    description_dict = {}\n    if args.description_dict_path:\n        with open(args.description_dict_path, \"r\") as f:\n            description_dict = json.load(f)\n\n    results = evaluator.simple_evaluate(\n        model=args.model,\n        model_args=args.model_args,\n        tasks=task_names,\n        num_fewshot=args.num_fewshot,\n        batch_size=args.batch_size,\n        device=args.device,\n        no_cache=args.no_cache,\n        limit=args.limit,\n        description_dict=description_dict,\n        decontamination_ngrams_path=args.decontamination_ngrams_path,\n        check_integrity=args.check_integrity,\n    )\n\n    dumped = json.dumps(results, indent=2)\n    print(dumped)\n\n    if args.output_path:\n        with open(args.output_path, \"w\") as f:\n            f.write(dumped)\n\n    print(\n        f\"{args.model} ({args.model_args}), limit: {args.limit}, provide_description: {args.provide_description}, \"\n        f\"num_fewshot: {args.num_fewshot}, batch_size: {args.batch_size}\"\n    )\n    print(evaluator.make_table(results))", "\n\nif __name__ == \"__main__\":\n    import lm_eval\n    from autocontrastive_gen.evaluation.lm_eval_harness.lm_eval_multi_head_gpt import HFLMMultiExit\n\n    lm_eval.models.MODEL_REGISTRY['multi_exit_gpt'] = HFLMMultiExit\n\n    main()\n", ""]}
{"filename": "autocontrastive_gen/evaluation/lm_eval_harness/__init__.py", "chunked_list": [""]}
{"filename": "autocontrastive_gen/evaluation/lm_eval_harness/lm_eval_multi_head_gpt.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nimport torch\nimport transformers\n\nfrom lm_eval.models import gpt2", "\nfrom lm_eval.models import gpt2\n\nfrom autocontrastive_gen.modeling.auto_model import AutoMultiExitModel\nfrom autocontrastive_gen.modeling.configuration import MultiExitConfiguration\n\n\nclass HFLMMultiExit(gpt2.HFLM):\n    \"\"\"\n    Overrides the init() method from lm_eval.models.gpt2.HFLM (v0.3.0), with slight modifications so that the model\n    will be initialized with a multi-exit model class rather than using the transformers AutoModelForCausalLM\n    \"\"\"\n    def __init__(\n        self,\n        device=\"cuda\",\n        pretrained=\"gpt2-medium\",\n        revision=\"main\",\n        subfolder=None,\n        tokenizer=None,\n        batch_size=1,\n        **multi_args\n    ):\n        super(gpt2.HFLM, self).__init__()\n\n        assert isinstance(device, str)\n        assert isinstance(pretrained, str)\n        assert isinstance(batch_size, int)\n\n        if device:\n            if device not in [\"cuda\", \"cpu\"]:\n                device = int(device)\n            self._device = torch.device(device)\n            print(f\"Using device '{device}'\")\n        else:\n            print(\"Device not specified\")\n            print(f\"Cuda Available? {torch.cuda.is_available()}\")\n            self._device = (\n                torch.device(\"cuda\")\n                if torch.cuda.is_available()\n                else torch.device(\"cpu\")\n            )\n\n        revision = revision + (\"/\" + subfolder if subfolder is not None else \"\")\n\n        multi_exit_config = MultiExitConfiguration(**multi_args)\n\n        self.gpt2 = AutoMultiExitModel.from_pretrained(\n            pretrained,\n            multi_exit_config=multi_exit_config,\n            revision=revision,\n        ).to(self.device)\n        self.gpt2.eval()\n\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(\n            pretrained if tokenizer is None else tokenizer,\n            revision=revision,\n        )\n\n        assert isinstance(\n            self.tokenizer,\n            (\n                transformers.GPT2Tokenizer,\n                transformers.GPT2TokenizerFast,\n                transformers.T5Tokenizer,\n                transformers.T5TokenizerFast,\n            ),\n        ), \"this tokenizer has not been checked for compatibility yet!\"\n\n        self.vocab_size = self.tokenizer.vocab_size\n\n        if isinstance(\n            self.tokenizer, (transformers.GPT2Tokenizer, transformers.GPT2TokenizerFast)\n        ):\n            assert self.tokenizer.encode(\"hello\\n\\nhello\") == [\n                31373,\n                198,\n                198,\n                31373,\n            ], self.tokenizer.encode(\"hello\\n\\nhello\")\n\n        # multithreading and batching\n        self.batch_size_per_gpu = batch_size", ""]}
{"filename": "autocontrastive_gen/head_training/gpt2_training.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import Optional\n\nfrom datasets import DatasetDict\n", "from datasets import DatasetDict\n\nfrom autocontrastive_gen.head_training.head_training_utils import group_texts, pretrain_tokenize_function, train\n\n\ndef gpt_preprocess_func(examples, ignore_input_in_loss, pad_token_id=50256):\n    # for autoregressive generation, the concatenated source and target also serve as the label, and we can choose\n    # whether to include the input tokens in the loss calculation\n    if ignore_input_in_loss:\n        # in the labels we replace the source input ids and the padding with -100 so that they won't be included in the loss\n        labels = []\n        for concatenated, target in zip(examples['input_ids'], examples['target_input_ids']):\n            target_end_idx = concatenated.index(pad_token_id) if concatenated[-1] == pad_token_id else len(concatenated)\n            target_start_idx = target_end_idx - len(target)\n            labels.append([-100]*target_start_idx + concatenated[target_start_idx:target_end_idx] + [-100]*(len(concatenated)-target_end_idx))\n        examples['labels'] = labels\n    else:\n        # in the labels we replace the padding ids with -100 so that they won't be included in the loss\n        labels = [[input_id if input_id != pad_token_id else -100 for input_id in example_input_ids] \n                  for example_input_ids in examples['input_ids']]\n        examples['labels'] = labels\n    return examples", "\n\ndef run_gpt2_pretraining(dataset_dict: DatasetDict, model, tokenizer, max_seq_length, output_dir, debug=False,\n                         max_train_instance: Optional[int] = None):\n    column_names = dataset_dict[\"train\"].column_names\n    if debug:\n        dataset_dict['train'] = dataset_dict['train'].select(range(1000))\n    if max_train_instance:\n        dataset_dict['train'] = dataset_dict['train'].select(range(max_train_instance))\n\n    dataset_dict = dataset_dict.map(lambda examples: pretrain_tokenize_function(examples, tokenizer),\n                                    batched=True, remove_columns=column_names)\n    print(f\"train dataset size before chunking: {len(dataset_dict['train'])}\")\n    # chunk different examples together up to max_seq_length\n    dataset_dict = dataset_dict.map(lambda examples: group_texts(examples, max_seq_length), batched=True)\n\n    dataset_dict = dataset_dict.map(lambda examples: gpt_preprocess_func(examples, ignore_input_in_loss=False),\n                                    batched=True)\n    print(f\"final train dataset size: {len(dataset_dict['train'])}\")\n\n    train(model, tokenizer, train_dataset=dataset_dict['train'], output_dir=output_dir,\n          optimizer_name='adamw_hf', learning_rate=2e-4, lr_scheduler_type='linear', data_collator=None,\n          debug=debug)", "\n\ndef gpt_finetune_tokenize_function(examples, tokenizer, max_seq_length, ignore_input_in_loss):\n    # tokenization for downstream task fine-tuning with a source and a target (label);\n    # the source and target are concatenated to a single input sequence\n\n    tokenized_dict = \\\n        tokenizer.batch_encode_plus(list(zip(examples['source_text'], examples['target_text'])),\n                                    max_length=max_seq_length, padding='max_length', truncation='only_first',\n                                    return_attention_mask=True, add_special_tokens=False)\n\n    if ignore_input_in_loss:\n        # we also tokenize the targets separately so we will know how to mask the sources in gpt_preprocess_func()\n        target_tokenized = tokenizer.batch_encode_plus(examples['target_text'],\n                                                       truncation=False,\n                                                       return_attention_mask=False, add_special_tokens=False)\n\n        tokenized_dict['target_input_ids'] = target_tokenized['input_ids']\n\n    return tokenized_dict", "\n\ndef run_gpt2_downstream_training(dataset_dict: DatasetDict, model, tokenizer, max_seq_length, output_dir, debug=False,\n                                 max_train_instance: Optional[int] = None):\n    column_names = dataset_dict[\"train\"].column_names\n    \n    if max_train_instance is not None:\n        dataset_dict['train'] = dataset_dict['train'].select(range(max_train_instance))\n\n    dataset_dict = dataset_dict.map(lambda examples:\n                                    gpt_finetune_tokenize_function(examples, tokenizer, max_seq_length,\n                                                                   ignore_input_in_loss=True),\n                                    batched=True, remove_columns=column_names)\n    dataset_dict = dataset_dict.map(lambda examples: gpt_preprocess_func(examples, ignore_input_in_loss=True),\n                                    batched=True)\n\n    train(model, tokenizer, train_dataset=dataset_dict['train'], output_dir=output_dir,\n          optimizer_name='adamw_hf', learning_rate=5e-4, lr_scheduler_type='linear', data_collator=None)", ""]}
{"filename": "autocontrastive_gen/head_training/__init__.py", "chunked_list": [""]}
{"filename": "autocontrastive_gen/head_training/t5_training.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom typing import List, Dict, Optional\n\nimport numpy as np\nimport torch", "import numpy as np\nimport torch\n\nfrom transformers import PreTrainedTokenizerBase, BatchEncoding, DataCollatorForSeq2Seq\nfrom transformers.models.t5.modeling_flax_t5 import shift_tokens_right\n\nfrom autocontrastive_gen.head_training.head_training_utils import group_texts, pretrain_tokenize_function, train\n\n\"\"\"\nThe T5 pre-training code below is adapted from ", "\"\"\"\nThe T5 pre-training code below is adapted from \nhttps://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_t5_mlm_flax.py,\nwith minor modifications for Pytorch\n\"\"\"\n\n\nclass DataCollatorForT5MLM:\n    \"\"\"\n    Data collator used for T5 span-masked language modeling.\n    It is made sure that after masking the inputs are of length `data_args.max_seq_length` and targets are also of fixed length.\n    For more information on how T5 span-masked language modeling works, one can take a look\n    at the `official paper <https://arxiv.org/pdf/1910.10683.pdf>`__\n    or the `official code for preprocessing <https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py>`__ .\n    Args:\n        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n            The tokenizer used for encoding the data.\n        noise_density (:obj:`float`):\n            The probability with which to (randomly) mask tokens in the input.\n        mean_noise_span_length (:obj:`float`):\n            The average span length of the masked tokens.\n        input_length (:obj:`int`):\n            The expected input length after masking.\n        target_length (:obj:`int`):\n            The expected target length after masking.\n        pad_token_id: (:obj:`int`):\n            The pad token id of the model\n        decoder_start_token_id: (:obj:`int):\n            The decoder start token id of the model\n    \"\"\"\n\n    def __init__(self, tokenizer: PreTrainedTokenizerBase, noise_density: float, mean_noise_span_length: float,\n                 input_length: int, target_length: int, pad_token_id: int, decoder_start_token_id: int):\n        self.tokenizer = tokenizer\n        self.noise_density = noise_density\n        self.mean_noise_span_length = mean_noise_span_length\n        self.input_length = input_length\n        self.target_length = target_length\n        self.pad_token_id = pad_token_id\n        self.decoder_start_token_id = decoder_start_token_id\n\n    def __call__(self, examples: List[Dict[str, np.ndarray]]) -> BatchEncoding:\n        # convert list to dict and tensorize input\n        batch = BatchEncoding(\n            {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n        )\n        input_ids = batch[\"input_ids\"]\n        batch_size, expandend_input_length = input_ids.shape\n\n        mask_indices = np.asarray([self.random_spans_noise_mask(expandend_input_length) for i in range(batch_size)])\n        labels_mask = ~mask_indices\n\n        input_ids_sentinel = self.create_sentinel_ids(mask_indices.astype(np.int8))\n        labels_sentinel = self.create_sentinel_ids(labels_mask.astype(np.int8))\n\n        batch[\"input_ids\"] = torch.tensor(self.filter_input_ids(input_ids, input_ids_sentinel))\n        batch[\"labels\"] = torch.tensor(self.filter_input_ids(input_ids, labels_sentinel))\n\n        if batch[\"input_ids\"].shape[-1] != self.input_length:\n            raise ValueError(\n                f\"`input_ids` are incorrectly preprocessed. `input_ids` length is {batch['input_ids'].shape[-1]}, but\"\n                f\" should be {self.input_length}.\"\n            )\n\n        if batch[\"labels\"].shape[-1] != self.target_length:\n            raise ValueError(\n                f\"`labels` are incorrectly preprocessed. `labels` length is {batch['labels'].shape[-1]}, but should be\"\n                f\" {self.target_length}.\"\n            )\n\n        # to check that tokens are correctly preprocessed, one can run `self.tokenizer.batch_decode(input_ids)` and `self.tokenizer.batch_decode(labels)` here...\n        batch[\"decoder_input_ids\"] = torch.tensor(shift_tokens_right(\n            batch[\"labels\"], self.pad_token_id, self.decoder_start_token_id\n        ))\n\n        return batch\n\n    def create_sentinel_ids(self, mask_indices):\n        \"\"\"\n        Sentinel ids creation given the indices that should be masked.\n        The start indices of each mask are replaced by the sentinel ids in increasing\n        order. Consecutive mask indices to be deleted are replaced with `-1`.\n        \"\"\"\n        start_indices = mask_indices - np.roll(mask_indices, 1, axis=-1) * mask_indices\n        start_indices[:, 0] = mask_indices[:, 0]\n\n        sentinel_ids = np.where(start_indices != 0, np.cumsum(start_indices, axis=-1), start_indices)\n        sentinel_ids = np.where(sentinel_ids != 0, (len(self.tokenizer) - sentinel_ids), 0)\n        sentinel_ids -= mask_indices - start_indices\n\n        return sentinel_ids\n\n    def filter_input_ids(self, input_ids, sentinel_ids):\n        \"\"\"\n        Puts sentinel mask on `input_ids` and fuse consecutive mask tokens into a single mask token by deleting.\n        This will reduce the sequence length from `expanded_inputs_length` to `input_length`.\n        \"\"\"\n        batch_size = input_ids.shape[0]\n\n        input_ids_full = np.where(sentinel_ids != 0, sentinel_ids, input_ids)\n        # input_ids tokens and sentinel tokens are >= 0, tokens < 0 are\n        # masked tokens coming after sentinel tokens and should be removed\n        input_ids = input_ids_full[input_ids_full >= 0].reshape((batch_size, -1))\n        input_ids = np.concatenate(\n            [input_ids, np.full((batch_size, 1), self.tokenizer.eos_token_id, dtype=np.int32)], axis=-1\n        )\n        return input_ids\n\n    def random_spans_noise_mask(self, length):\n        \"\"\"This function is copy of `random_spans_helper <https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2682>`__ .\n        Noise mask consisting of random spans of noise tokens.\n        The number of noise tokens and the number of noise spans and non-noise spans\n        are determined deterministically as follows:\n        num_noise_tokens = round(length * noise_density)\n        num_nonnoise_spans = num_noise_spans = round(num_noise_tokens / mean_noise_span_length)\n        Spans alternate between non-noise and noise, beginning with non-noise.\n        Subject to the above restrictions, all masks are equally likely.\n        Args:\n            length: an int32 scalar (length of the incoming token sequence)\n            noise_density: a float - approximate density of output mask\n            mean_noise_span_length: a number\n        Returns:\n            a boolean tensor with shape [length]\n        \"\"\"\n\n        orig_length = length\n\n        num_noise_tokens = int(np.round(length * self.noise_density))\n        # avoid degeneracy by ensuring positive numbers of noise and nonnoise tokens.\n        num_noise_tokens = min(max(num_noise_tokens, 1), length - 1)\n        num_noise_spans = int(np.round(num_noise_tokens / self.mean_noise_span_length))\n\n        # avoid degeneracy by ensuring positive number of noise spans\n        num_noise_spans = max(num_noise_spans, 1)\n        num_nonnoise_tokens = length - num_noise_tokens\n\n        # pick the lengths of the noise spans and the non-noise spans\n        def _random_segmentation(num_items, num_segments):\n            \"\"\"Partition a sequence of items randomly into non-empty segments.\n            Args:\n                num_items: an integer scalar > 0\n                num_segments: an integer scalar in [1, num_items]\n            Returns:\n                a Tensor with shape [num_segments] containing positive integers that add\n                up to num_items\n            \"\"\"\n            mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n            np.random.shuffle(mask_indices)\n            first_in_segment = np.pad(mask_indices, [[1, 0]])\n            segment_id = np.cumsum(first_in_segment)\n            # count length of sub segments assuming that list is sorted\n            _, segment_length = np.unique(segment_id, return_counts=True)\n            return segment_length\n\n        noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n        nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n\n        interleaved_span_lengths = np.reshape(\n            np.stack([nonnoise_span_lengths, noise_span_lengths], axis=1), [num_noise_spans * 2]\n        )\n        span_starts = np.cumsum(interleaved_span_lengths)[:-1]\n        span_start_indicator = np.zeros((length,), dtype=np.int8)\n        span_start_indicator[span_starts] = True\n        span_num = np.cumsum(span_start_indicator)\n        is_noise = np.equal(span_num % 2, 1)\n\n        return is_noise[:orig_length]", "\n\ndef compute_input_and_target_lengths(inputs_length, noise_density, mean_noise_span_length):\n    \"\"\"This function is copy of `random_spans_helper <https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2466>`__ .\n    Training parameters to avoid padding with random_spans_noise_mask.\n    When training a model with random_spans_noise_mask, we would like to set the other\n    training hyperparmeters in a way that avoids padding.\n    This function helps us compute these hyperparameters.\n    We assume that each noise span in the input is replaced by extra_tokens_per_span_inputs sentinel tokens,\n    and each non-noise span in the targets is replaced by extra_tokens_per_span_targets sentinel tokens.\n    This function tells us the required number of tokens in the raw example (for split_tokens())\n    as well as the length of the encoded targets. Note that this function assumes\n    the inputs and targets will have EOS appended and includes that in the reported length.\n    Args:\n        inputs_length: an integer - desired length of the tokenized inputs sequence\n        noise_density: a float\n        mean_noise_span_length: a float\n    Returns:\n        tokens_length: length of original text in tokens\n        targets_length: an integer - length in tokens of encoded targets sequence\n    \"\"\"\n\n    def _tokens_length_to_inputs_length_targets_length(tokens_length):\n        num_noise_tokens = int(round(tokens_length * noise_density))\n        num_nonnoise_tokens = tokens_length - num_noise_tokens\n        num_noise_spans = int(round(num_noise_tokens / mean_noise_span_length))\n        # inputs contain all nonnoise tokens, sentinels for all noise spans\n        # and one EOS token.\n        _input_length = num_nonnoise_tokens + num_noise_spans + 1\n        _output_length = num_noise_tokens + num_noise_spans + 1\n        return _input_length, _output_length\n\n    tokens_length = inputs_length\n\n    while _tokens_length_to_inputs_length_targets_length(tokens_length + 1)[0] <= inputs_length:\n        tokens_length += 1\n\n    inputs_length, targets_length = _tokens_length_to_inputs_length_targets_length(tokens_length)\n\n    # minor hack to get the targets length to be equal to inputs length\n    # which is more likely to have been set to a nice round number.\n    if noise_density == 0.5 and targets_length > inputs_length:\n        tokens_length -= 1\n        targets_length -= 1\n    return tokens_length, targets_length", "\n\ndef run_t5_pretraining(datasets, model, tokenizer, max_seq_length, output_dir, debug=False,\n                       max_train_instance: Optional[int] = None):\n    column_names = datasets[\"train\"].column_names\n    if debug:\n        datasets['train'] = datasets['train'].select(range(100))\n\n    # We tokenize every text, then concatenate them together before splitting them in smaller parts.\n    # Since we make sure that all sequences are of the same length, no attention_mask is needed.\n    datasets = datasets.map(lambda examples: pretrain_tokenize_function(examples, tokenizer),\n                            batched=True, remove_columns=column_names)\n\n    # T5-like span masked language modeling will fuse consecutively masked tokens to a single sentinel token.\n    # To ensure that the input length is `max_seq_length`, we need to increase the maximum length\n    # according to `mlm_probability` and `mean_noise_span_length`. We can also define the label length accordingly.\n    expanded_inputs_length, targets_length = compute_input_and_target_lengths(\n        inputs_length=max_seq_length,\n        noise_density=0.15,\n        mean_noise_span_length=3.0,\n    )\n\n    datasets = datasets.map(lambda examples: group_texts(examples, expanded_inputs_length), batched=True)\n\n    # This one will take care of randomly masking the tokens.\n    data_collator = DataCollatorForT5MLM(\n        tokenizer=tokenizer,\n        noise_density=0.15,\n        mean_noise_span_length=3.0,\n        input_length=max_seq_length,\n        target_length=targets_length,\n        pad_token_id=model.config.pad_token_id,\n        decoder_start_token_id=model.config.decoder_start_token_id,\n    )\n\n    train(model, tokenizer, train_dataset=datasets['train'], output_dir=output_dir,\n          optimizer_name='adafactor', learning_rate=0.01, lr_scheduler_type='constant', data_collator=data_collator,\n          debug=debug)", "\n\ndef seq2seq_preprocess_dataset(examples,\n                               tokenizer: PreTrainedTokenizerBase,\n                               text_column: str,\n                               target_column: str,\n                               max_source_length: int,\n                               max_target_length: int):\n    padding = False\n\n    # remove pairs where at least one record is None\n    inputs, targets = [], []\n    for i in range(len(examples[text_column])):\n        if examples[text_column][i] is not None and examples[target_column][i] is not None:\n            inputs.append(examples[text_column][i])\n            targets.append(examples[target_column][i])\n\n    model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(targets, max_length=max_target_length, padding=padding, truncation=True)\n\n    # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n    # padding in the loss.\n    if padding == \"max_length\":\n        labels[\"input_ids\"] = [\n            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n        ]\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs", "\n\ndef run_t5_seq2seq(datasets, model, tokenizer, max_seq_length, output_dir, debug=False,\n                   max_train_instance: Optional[int] = None):\n    column_names = datasets[\"train\"].column_names\n    if debug:\n        datasets['train'] = datasets['train'].select(range(100))\n\n    datasets = datasets.map(lambda examples: seq2seq_preprocess_dataset(examples, tokenizer, text_column=\"source_text\",\n                                                                        target_column=\"target_text\",\n                                                                        max_source_length=max_seq_length,\n                                                                        max_target_length=max_seq_length),\n                            batched=True, remove_columns=column_names)\n\n    if max_train_instance is not None:\n        datasets['train'] = datasets['train'].select(range(max_train_instance))\n\n    label_pad_token_id = tokenizer.pad_token_id if tokenizer.pad_token_id else -100\n    data_collator = DataCollatorForSeq2Seq(\n        tokenizer=tokenizer,\n        model=model,\n        label_pad_token_id=label_pad_token_id,\n    )\n\n    train(model, tokenizer, train_dataset=datasets['train'], output_dir=output_dir,\n          optimizer_name='adafactor', learning_rate=0.0001, lr_scheduler_type='constant', data_collator=data_collator,\n          debug=debug)", ""]}
{"filename": "autocontrastive_gen/head_training/head_training_utils.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom transformers import Trainer, TrainingArguments\n\n\ndef pretrain_tokenize_function(examples, tokenizer):\n    # tokenization for pretraining-like fine-tuning, where the texts are chunked and we do not need\n    # padding/truncation/attention mask\n    return tokenizer([t for t in examples['source_text']], return_attention_mask=False, return_tensors='np')", "\ndef pretrain_tokenize_function(examples, tokenizer):\n    # tokenization for pretraining-like fine-tuning, where the texts are chunked and we do not need\n    # padding/truncation/attention mask\n    return tokenizer([t for t in examples['source_text']], return_attention_mask=False, return_tensors='np')\n\n\ndef group_texts(examples, inputs_length):\n    \"\"\"\n    Based on https://github.com/huggingface/transformers/blob/main/examples/flax/language-modeling/run_t5_mlm_flax.py\n    \"\"\"\n    # Concatenate all texts.\n    concatenated_examples = {'input_ids': sum(examples['input_ids'], [])}\n    total_length = len(concatenated_examples['input_ids'])\n    # We drop the small remainder\n    if total_length >= inputs_length:\n        total_length = (total_length // inputs_length) * inputs_length\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i: i + inputs_length] for i in range(0, total_length, inputs_length)]\n        for k, t in concatenated_examples.items()\n    }\n    return result", "\n\ndef train(model, tokenizer, train_dataset, output_dir, optimizer_name, learning_rate, lr_scheduler_type, data_collator,\n          debug=False):\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=3, per_device_train_batch_size=16, per_device_eval_batch_size=8,\n        evaluation_strategy='steps', eval_steps=1000, save_strategy='epoch',\n        optim=optimizer_name,\n        lr_scheduler_type=lr_scheduler_type,\n        learning_rate=learning_rate,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=train_dataset.select(range(50)) if not debug else train_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n    )\n\n    trainer.train()", "\n\ndef get_head_training_function(model_config, is_seq2seq_task):\n    from autocontrastive_gen.head_training.gpt2_training import run_gpt2_pretraining, run_gpt2_downstream_training\n    from autocontrastive_gen.head_training.t5_training import run_t5_pretraining, run_t5_seq2seq\n\n    if model_config.is_encoder_decoder:  # T5\n        if is_seq2seq_task:\n            return run_t5_seq2seq\n        else:\n            return run_t5_pretraining\n    else:\n        if is_seq2seq_task:\n            return run_gpt2_downstream_training\n        else:\n            return run_gpt2_pretraining", ""]}
{"filename": "autocontrastive_gen/data_processing/preprocessing_functions.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\n\ndef wikitext_dataset_preprocess(examples):\n    def preprocess_wikitext(text):\n        wikitext_replacements = [(\" 's \", \"'s \"), (\"s ' \", \"s' \"), (' , ', ', '), (' ( ', ' ('), (' )', ')'),\n                                 (' : ', ': '), (' ; ', '; '), (' . ', '. '), (' %', '%'),\n                                 (' @-@ ', '-'), (' @.@ ', '.'), (' @,@ ', ',')]\n        for original, replacement in wikitext_replacements:\n            text = text.replace(original, replacement)\n        return text\n\n    def filter_wikitext(source_text):\n        return len(source_text.split()) > 50\n\n    source_column = 'text'\n    sources = []\n    for i in range(len(examples[source_column])):\n        source = examples[source_column][i]\n        if source is not None and filter_wikitext(source):\n            sources.append(preprocess_wikitext(source))\n\n    new_examples = {\n        source_column: sources,\n    }\n\n    return new_examples", "\n\ndef wikinews_dataset_preprocess(examples):\n    def get_main_text(news_text):\n        return sorted(news_text.split(\":\"), key=len)[-1].strip()\n\n    def filter_wikinews(source_text):\n        text = get_main_text(source_text)\n        return len(text.split()) > 50\n\n    def preprocess_wikinews(text):\n        main_text = get_main_text(text)\n        main_text = main_text.replace('Pillars of Wikinews writing Writing an article ', '')\n        return main_text\n\n    source_column = 'text'\n    sources = []\n    for source_text in examples[source_column]:\n        if source_text is not None and filter_wikinews(source_text):\n            sources.append(preprocess_wikinews(source_text))\n\n    new_examples = {\n        source_column: sources,\n    }\n\n    return new_examples", "\n\ndef bookcorpus_dataset_preprocess(examples):\n    def preprocess_bookcorpus(text):\n        bookcorpus_replacements = [(\" '\", \"'\"), (' , ', ', '), (' .', '.'), (' ?', '?'), (\" n't\", \"n't\"),\n                                   (' : ', ': '), (' ; ', '; '), (' ( ', ' ('), (' )', ')'), (' %', '%')]\n        for original, replacement in bookcorpus_replacements:\n            text = text.replace(original, replacement)\n        return text\n\n    def filter_bookcorpus(source_text):\n        return len(source_text.split()) > 50\n\n    source_column = 'text'\n    sources = []\n    for source_text in examples[source_column]:\n        if source_text is not None and filter_bookcorpus(source_text):\n            sources.append(preprocess_bookcorpus(source_text))\n\n    new_examples = {\n        source_column: sources,\n    }\n\n    return new_examples", ""]}
{"filename": "autocontrastive_gen/data_processing/__init__.py", "chunked_list": [""]}
{"filename": "autocontrastive_gen/data_processing/dataset_catalog.py", "chunked_list": ["#\n#  Copyright (c) 2023 IBM Corp.\n#  Licensed under the Apache License, Version 2.0 (the \"License\");\n#  you may not use this file except in compliance with the License.\n#  You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,", "#  Unless required by applicable law or agreed to in writing, software\n#  distributed under the License is distributed on an \"AS IS\" BASIS,\n#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n#  See the License for the specific language governing permissions and\n#  limitations under the License.\n#\nfrom dataclasses import dataclass, field\nfrom typing import Callable, Mapping\n\nfrom datasets import load_dataset, DatasetDict", "\nfrom datasets import load_dataset, DatasetDict\n\nfrom autocontrastive_gen.data_processing.preprocessing_functions import wikitext_dataset_preprocess, \\\n    wikinews_dataset_preprocess, bookcorpus_dataset_preprocess\n\n\n@dataclass\nclass Dataset:\n    hf_dataset_name: str\n    source_column: str\n    target_column: str = None\n    hf_subset_name: str = None\n    dataset_preprocessing_func: Callable = None\n    extra_kwargs: Mapping = field(default_factory=dict)\n\n    def load(self) -> DatasetDict:\n        dataset_dict = load_dataset(self.hf_dataset_name, self.hf_subset_name, **self.extra_kwargs)\n\n        if self.dataset_preprocessing_func is not None:\n            dataset_dict = dataset_dict.map(self.dataset_preprocessing_func,\n                                            batched=True, remove_columns=list(dataset_dict['train'].column_names))\n\n        dataset_dict = dataset_dict.rename_column(self.source_column, 'source_text')\n        if self.target_column:\n            dataset_dict = dataset_dict.rename_column(self.target_column, 'target_text')\n\n        if 'validation' not in dataset_dict:\n            dataset_dict['validation'] = dataset_dict['train']\n\n        return dataset_dict\n\n    def is_seq2seq_task(self):\n        return self.target_column is not None", "class Dataset:\n    hf_dataset_name: str\n    source_column: str\n    target_column: str = None\n    hf_subset_name: str = None\n    dataset_preprocessing_func: Callable = None\n    extra_kwargs: Mapping = field(default_factory=dict)\n\n    def load(self) -> DatasetDict:\n        dataset_dict = load_dataset(self.hf_dataset_name, self.hf_subset_name, **self.extra_kwargs)\n\n        if self.dataset_preprocessing_func is not None:\n            dataset_dict = dataset_dict.map(self.dataset_preprocessing_func,\n                                            batched=True, remove_columns=list(dataset_dict['train'].column_names))\n\n        dataset_dict = dataset_dict.rename_column(self.source_column, 'source_text')\n        if self.target_column:\n            dataset_dict = dataset_dict.rename_column(self.target_column, 'target_text')\n\n        if 'validation' not in dataset_dict:\n            dataset_dict['validation'] = dataset_dict['train']\n\n        return dataset_dict\n\n    def is_seq2seq_task(self):\n        return self.target_column is not None", "\n\n\"\"\"\nTo create an auth token generate a token in https://huggingface.co/settings/tokens; This is required for loading the\nWikiNews dataset.\n\"\"\"\nHF_AUTH_TOKEN = None\n\n\nclass DatasetsCatalog:\n    wikitext_103 = Dataset('wikitext', hf_subset_name='wikitext-103-raw-v1', source_column='text',\n                           dataset_preprocessing_func=wikitext_dataset_preprocess)\n    wikinews = Dataset('bigscience-data/roots_en_wikinews', source_column='text',\n                       dataset_preprocessing_func=wikinews_dataset_preprocess,\n                       extra_kwargs={'use_auth_token': HF_AUTH_TOKEN})  # Note: requires setting HF_AUTH_TOKEN\n    bookcorpus = Dataset('bookcorpus', source_column='text', dataset_preprocessing_func=bookcorpus_dataset_preprocess)\n    cc_en = Dataset('cc100', source_column='text', extra_kwargs={'lang': 'en'})\n\n    @staticmethod\n    def all_datasets():\n        return [var for var in vars(DatasetsCatalog)\n                if not var.startswith('__') and not callable(getattr(DatasetsCatalog, var))]", "\nclass DatasetsCatalog:\n    wikitext_103 = Dataset('wikitext', hf_subset_name='wikitext-103-raw-v1', source_column='text',\n                           dataset_preprocessing_func=wikitext_dataset_preprocess)\n    wikinews = Dataset('bigscience-data/roots_en_wikinews', source_column='text',\n                       dataset_preprocessing_func=wikinews_dataset_preprocess,\n                       extra_kwargs={'use_auth_token': HF_AUTH_TOKEN})  # Note: requires setting HF_AUTH_TOKEN\n    bookcorpus = Dataset('bookcorpus', source_column='text', dataset_preprocessing_func=bookcorpus_dataset_preprocess)\n    cc_en = Dataset('cc100', source_column='text', extra_kwargs={'lang': 'en'})\n\n    @staticmethod\n    def all_datasets():\n        return [var for var in vars(DatasetsCatalog)\n                if not var.startswith('__') and not callable(getattr(DatasetsCatalog, var))]", ""]}
